{"id": "1401.5857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "COLIN: Planning with Continuous Linear Numeric Change", "abstract": "In similar material everybody describe COLIN, given forward - chaining genotyping search managing, capable among rationality over COntinuous LINear codewords not, in well to the full oblique terminology that PDDL. Through very focused take make two advances them later state - far - on - drawings until terms given expressive metaphysics advantages of acknowledge: the handling instance continuous linear make, and taken handling well minimum - particularly effects in combination with minimal inefficiencies, the one a system hanging decrease stiffness where numeric straightforward although planning. COLIN compositional FF - contemporary forward chaining search, and all any major gave Linear Program (LP) still taken their consistency of on particles inflection took six-digit inequalities point each public. The LP is one before equation scrimmage two all values main variables until out issue, domestic the range in actions that need supposed also considered from file. In addition, know cooperation very extension of into Temporal Relaxed Planning Graph non-uniform of CRIKEY3, because power moral connected with continuous change. We extends on usually known task variables considered return not provide candidates for specifying the gradient both into significant binary but operationalized late as to. Finally, really explore the crucial for laborers mixed vector content having turned utilizing for devlopment form timestamps since to ignored in the support, thought a depend 's though found. To support this, we further commitment turned presentation of further lows domains that all resulted numeric occurring. We represent results for COLIN that demonstrate more scalability take on range several benchmarks, and precisely did controls civil - of - during - paintings planned.", "histories": [["v1", "Thu, 23 Jan 2014 02:46:26 GMT  (1019kb)", "http://arxiv.org/abs/1401.5857v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["amanda j coles", "rew i coles", "maria fox", "derek long"], "accepted": false, "id": "1401.5857"}, "pdf": {"name": "1401.5857.pdf", "metadata": {"source": "CRF", "title": "COLIN: Planning with Continuous Linear Numeric Change", "authors": ["Amanda Coles", "Andrew Coles", "Maria Fox", "Derek Long"], "emails": ["AMANDA.COLES@KCL.AC.UK", "ANDREW.COLES@KCL.AC.UK", "MARIA.FOX@KCL.AC.UK", "DEREK.LONG@KCL.AC.UK"], "sections": [{"heading": null, "text": "soning with COntinuous LINear numeric change, in addition to the full temporal semantics of PDDL2.1. Through this work we make two advances to the state-of-the-art in terms of expressive reasoning capabilities of planners: the handling of continuous linear change, and the handling of duration-dependent effects in combination with duration inequalities, both of which require tightly coupled temporal and numeric reasoning during planning. COLIN combines FF-style forward chaining search, with the use of a Linear Program (LP) to check the consistency of the interacting temporal and numeric constraints at each state. The LP is used to compute bounds on the values of variables in each state, reducing the range of actions that need to be considered for application. In addition, we develop an extension of the Temporal Relaxed Planning Graph heuristic of CRIKEY3, to support reasoning directly with continuous change. We extend the range of task variables considered to be suitable candidates for specifying the gradient of the continuous numeric change effected by an action. Finally, we explore the potential for employing mixed integer programming as a tool for optimising the timestamps of the actions in the plan, once a solution has been found. To support this, we further contribute a selection of extended benchmark domains that include continuous numeric effects. We present results for COLIN that demonstrate its scalability on a range of benchmarks, and compare to existing state-of-the-art planners."}, {"heading": "1. Introduction", "text": "There has been considerable progress in the development of automated planning techniques for domains involving independent temporal and metric conditions and effects (Eyerich, Mattmu\u0308ller, & Ro\u0308ger, 2009; Coles, Fox, Long, & Smith, 2008a; Gerevini, Saetti, & Serina, 2006; Edelkamp, 2003; Coles, Fox, Long, & Smith, 2008b). The development of powerful heuristics for propositional planning has been shown to offer benefits in the solution of extended planning problems, including planning under uncertainty (Palacios & Geffner, 2009), planning with numbers and planning with time. However, the combination and integration of metric and temporal features, in which metric quantities change in time-dependent ways, remains a challenge that has received relatively little attention.\nInteraction between time and numbers in planning problems can occur in many ways. In the simplest case, using PDDL2.1 (Fox & Long, 2003), the numeric effects of actions are only updated instantaneously, and only at the start or end points of actions which are known (and fixed) at the point of action execution. The corpus of domains from past International Planning Competitions adhere to these restrictions. Time and numbers can interact in at least two more complex ways. First, actions can have variable, possibly constrained, durations and the (instantaneous) effects of these\nc\u00a92012 AI Access Foundation. All rights reserved.\nactions can depend on the values of the durations. This allows domain models to capture the effects of processes as discretised step effects, but adjusted according to the demands of specific problem instances. Second, the effects of actions can be considered to be continuous across their execution, so that the values of metric variables at any time point depend on how long the continuous effects have been acting on them.\nFor example, a problem in which sand is loaded into a lorry can be modelled so that the amount of sand loaded depends on the time spent loading. The first approach is to capture the increase in the quantity of loaded sand as a step function applied at the end of the loading action. In the second approach, the process of loading sand is modelled as a continuous and linear function of the time spent loading, so that the amount of sand in the lorry can be observed at any point throughout the loading process. If a safety device must be engaged before the lorry is more than three-quarters full, then only the second of these models will allow a planner to have the necessary access to the underlying process behaviour to make good planning choices about how to integrate this action into solutions. There are alternative models exploiting duration-dependent effects to split the loading action into two parts around the time point at which the safety device must be engaged, but these alternatives become very complicated with relatively modest changes to the domain.\nContinuous change in both of these forms is common in many important problems. These include: energy management, the consumption and replenishment of restricted continuous resources such as fuel, tracking the progress of chemicals through storage tanks in chemical plants, choreographing robot motion with the execution of tasks, and managing the efficient use of time. In some cases, a model using discrete time-independent change is adequate for planning. However, discretisation is not always practical: to find a reasonable solution (or, indeed, to find one at all) identifying the appropriate granularity for discretisation is non-trivial, perhaps requiring a range of choices that are so fine-grained as to make the discrete model infeasibly large. In other cases, the numeric change cannot be appropriately discretised, where it is unavoidably necessary to have access to the values of numeric variables during the execution of actions, in order to manage interactions between numeric values.\nIn this paper we present a planner, COLIN, capable of reasoning with both variable, durationdependent, linear change and linear continuous numeric effects. The key advance that COLIN makes is to be able to reason about time-dependent change through the use of linear programs that combine metric and temporal conditions and effects into the same representation. COLIN is a satisficing planner that attempts to build good quality solutions to this complex class of problems. Since COLIN is a forward-searching planner it requires a representation of states, a means to compute the progression of states and a heuristic function to guide the search for a path from the initial to the goal state. COLIN is built on the planner CRIKEY3 (Coles, Fox, Long et al., 2008a). However, CRIKEY3 requires numeric change to be discrete and cannot reason with continuous numeric change, or duration dependent change (where the duration of actions is not fixed in the state in which the action begins). Being able to reason successfully with problems characterised by continuous change, coping efficiently with a wide range of practical problems that are inspired by real applications, is the major contribution made by COLIN.\nThe organisation of the paper is as follows. In Section 2 we explain the features of PDDL2.1 that COLIN can handle, and contrast its repertoire with that of CRIKEY3. In Section 4 we define the problem that is addressed by COLIN. In Section 5 we outline the background in temporal and metric planning that supports COLIN, before, in Section 6, describing the details of the foundations of COLIN that lie in CRIKEY3. COLIN inherits its representation of states from CRIKEY3,\nas well as the machinery for confirming the temporal consistency of plans and the basis for the heuristic function. In Section 7 we describe systems in the literature that have addressed similar hybrid discrete-continuous planning problems to those that COLIN is designed to handle. Section 8 explains how state progression is extended in COLIN to handle linear continuous change, and Section 9 describes the heuristic that guides the search for solutions. In Section 10 we consider several elements of COLIN that improve both efficiency and plan quality, without affecting the fundamental behaviour of the planner. Since time-dependent numeric change has been so little explored, there are few benchmarks in existence that allow a full quantitative evaluation. We therefore present a collection of continuous domains that can be used for such analysis, and we show how COLIN fares on these. An appendix containing some explanations of technical detail and some detailed summaries of background work on which COLIN depends, ensures that the paper is complete and self-contained.\n2. Language Features in CRIKEY3 and COLIN\nCOLIN builds on CRIKEY3 by handling the continuous features of PDDL2.1. CRIKEY3 was restricted to management of discrete change, while COLIN can handle the full range of linear continuous numeric effects. The only metric functions of PDDL2.1 that are not in the repertoire of COLIN are scale-up and scale-down, which are non-linear updates, and the general form of plan metrics. Managing plan metrics defined in terms of domain variables remains a challenge for planning that has not yet been fully confronted by any contemporary planner. COLIN does handle a restricted form of quality metric, which exploits an instrumented variable called total-cost. This allows COLIN to minimise the overall cost of the shortest plan it can find using total-time (the default metric used by most temporal planners).\nIn common with CRIKEY3, COLIN can cope with Timed Initial Literals, an important feature that was introduced in PDDL2.2 (Hoffmann & Edelkamp, 2005). PDDL2.1 is backward compatible with McDermott\u2019s PDDL (McDermott, 2000) and therefore supports ADL (Pednault, 1989). COLIN does not handle full ADL, but it can deal with a restricted form of conditional effect as seen in the airplane-landing problem described in section 11. This restricted form allows the cost of an action to be dependent on the state in which it is applied. More general forms of conditional effect cannot be handled.\nWith this collection of features, COLIN is able to fully manage both the discrete and continuous numeric change that occur directly as a result of its actions. PDDL+ (Fox & Long, 2006) further supports the modelling of continuous change brought about by exogenous processes and events. These are triggered by actions, but they model the independent continuous behaviour brought about by the world rather than by the planner\u2019s direct action. The key additional features of PDDL+ that support this are processes and events. COLIN does not handle these features but is restricted to the management of continuous change as expressed through the durative action device.\nFor detailed explanations of the syntaxes and semantics of PDDL2.1 and PDDL+, including the semantics on which implementations of state representation and state progression must be constructed, readers should refer to the work of Fox and Long (2003, 2006)."}, {"heading": "3. Motivation", "text": "There are a number of accounts of planning having been successfully applied to real problems, and the frequency with which applications are reported is increasing. The following examples involve domains with hybrid discrete-continuous dynamics. These dynamics are typically being dealt with by discretising time, packaging continuous numeric effects into step functions, or integrating propositional planning techniques with specialised solvers. They are all examples in which hybrid discrete-continuous reasoning could be exploited to improve plan quality or solution time.\n\u2022 Operations of refineries (Boddy & Johnson, 2002; Lamba, Dietz, Johnson, & Boddy, 2003) or chemical plants (Penna, Intrigila, Magazzeni, & Mercorio, 2010), where the continuous processes reflect flows of materials, mixing and chemical reactions, heating and cooling.\n\u2022 Management of power and thermal energy in aerospace applications in which power management is critical, such as management of the solar panel arrays on the International Space Station (Knight, Schaffer, & B.Clement, 2009; Reddy, Frank, Iatauro, Boyce, Ku\u0308rklu\u0308, AiChang, & Jo\u0301nsson, 2011). For example, Knight et al. (2009) rely on a high-fidelity power model (TurboSpeed) to provide support for reasoning about the continuous power supply in different configurations of the solar panels. Power management is a critical problem for most space applications (including planetary rovers and landers, inspiring the temporal-metriccontinuous Rovers domain used as one of our benchmark evaluation domains in Section 11). Chien et al. (2010) describe the planner used to support operations on Earth Observing 1 (EO1), where the management of thermal energy generated by instruments is sufficiently important that the on-board planner uses some of its (highly constrained) CPU cycles to model and track its value. EO-1 inspires the temporal-metric-continuous Satellite benchmark described in Section 11.\n\u2022 Management of non-renewable power in other contexts, such as for battery powered devices. The battery management problem described by Fox et al. (2011) relies on a non-linear model,\nwhich COLIN must currently reduce to a discrete or linear approximation, coupled with iterated validation and solution refinement, in order to optimise power use. Battery management is an example of a continuous problem that cannot be solved if the continuous dynamics are removed.\n\u2022 Assignment of time-dependent costs as in the Aircraft Landing domain (Dierks, 2005), in which continuous processes govern the changing costs of the use of the runway as the landing time deviates from the optimal landing time for each aircraft. This problem inspires the Aircraft-Landing benchmark domain described in Section 11.\n\u2022 Choreography of mobile robotic systems: in many cases, operations of robotic platforms involve careful management of motion alongside other tasks, where the continuous motion of the robot constrains the accessibility of specific tasks, such as inspection or observation. Existing examples of hybrid discrete-continuous planning models and reasoning for problems of this kind include work using flow tubes to capture the constraints on continuous processes (Le\u0301aute\u0301 & Williams, 2005; Li & Williams, 2008). Problems involving autonomous underwater vehicles (AUVs) inspired the temporal-metric-continuous AUV benchmark presented in Section 11."}, {"heading": "4. Problem Definition", "text": "COLIN is designed to solve a class of problems that are temporal and metric, and that feature linear continuous metric change. We refer to this as the class of temporal-metric-continuous problems, and it contains a substantial subset of the problems that can be expressed in PDDL2.1.\nAs a step towards the class of temporal-metric-continuous problems, we recall the definition of a simple temporal-metric planning problem \u2014 one in which there is no time-dependent metric change. Simple temporal-metric problems can be represented as a tuple \u3008I, A,G,M\u3009, where:\n\u2022 I is the initial state: a set of propositions and an assignment of values to a set of numeric variables. Either of these sets may be empty. For notational convenience, we refer to the vector of numeric values in a given state as v.\n\u2022 A, a set of actions, each \u3008dur , pre`, eff `, pre\u2194, prea, eff a\u3009, where:\n\u2013 pre` (prea) are the start (end) conditions of a: at the state in which a starts (ends), these conditions must hold (for a detailed account of some of the subtleties in the semantics of action application, see Fox & Long, 2003).\n\u2013 eff ` (eff a) are the start (end) effects of a: starting (ending) a updates the world state according to these effects. A given collection of effects eff x, x \u2208 {`,a}, consists of: \u2217 eff \u2212x , propositions to be deleted from the world state; \u2217 eff +x , propositions to be added to the world state; \u2217 eff nx , effects acting upon numeric variables. \u2013 pre\u2194 are the invariant conditions of a: these must hold at every point in the open interval between the start and end of a.\n\u2013 dur are the duration constraints of a, calculated on the basis of the world state in which a is started, and constraining the length of time that can pass between the start and end of a. They each refer to the special parameter ?duration, denoting the duration of a.\n\u2022 G, a goal: a set of propositions and conditions over numeric variables.\n\u2022 optionally M , a metric optimisation function, defined as a function of the values of numeric variables at the end of the plan, and the special variable total-time, denoting the makespan of the plan.\nA solution to such a problem is a time-stamped sequence of actions, with associated durations, that transforms the initial state into a state satisfying the goal, respecting all the conditions imposed. The durations of the actions must be specified explicitly, since it is possible that the action specifications can be satisfied by different duration values.\nPDDL2.1 numeric conditions used in pre`, prea, pre\u2194, dur and G can be expressed in the form:\n\u3008f(v), op, c\u3009, such that op \u2208 {\u2264, <,=, >,\u2265}, c \u2208 <\nwhere v is the vector of metric fluents in the planning problem, f(v) is a function applied to the vector of numeric fluents and c is an arbitrary constant. Numeric effects used in eff ` and eff a are expressed as:\n\u3008v, op, f(v)\u3009, such that op \u2208 {\u00d7=,+=,=, -=,\u00f7=}\nA restricted form of numeric expressions is the set of expressions in Linear Normal Form (LNF). These are expressions in which f(v) is a weighted sum of variables plus a constant, expressible in the form w \u00b7v+c, for a vector of constants, w. A notable consequence of permitting dur to take the form of a set of LNF constraints over ?duration is that ?duration need not evaluate to a single fixed value. For instance, it may constrain the value of ?duration to lie within a range of values, e.g. (?duration \u2265 v1) \u2227 (?duration \u2264 v2), for some numeric variables v1 and v2. Restricting conditions and effects to use only LNFs allows the metric expressions to be captured in a linear program model, a fact that we exploit in COLIN.\nThe class of temporal-metric problems is extended to temporal-metric-continuous problems by two additions:\n1. Each action a \u2208 A is described with an additional component: a set of linear continuous numeric effects, cont, of the form \u3008v, k\u3009, k \u2208 <, denoting that a increases v at the rate of k per unit of time. This corresponds to the PDDL2.1 effect (increase (v) (* #t k)).\n2. The start or end effects of actions (eff n` and eff n a may, additionally, include the parameter\n?duration, denoting the duration of the action, and hence are written:\n\u3008v, op,w \u00b7 v + k.(?duration) + c\u3009 s.t. op \u2208 {+=,=, -=}, c, k \u2208 <\nIn temporal-metric-continuous problems the relationship between time and numbers is more complex than in temporal-metric problems. The first extension allows the value of a variable v to depend on the length of time elapsed since the continuous effect acting upon it began. The second extension implies that, if ?duration is not fixed, then the value of variables can depend on the duration assigned to the action. In fact , very few planners allow the literal ?duration to appear in effects, even in actions where the value of the parameter is constrained to take a single fixed value by the duration constraint (e.g. (= ?duration 10)). A typical idiom is to name the intended value of the duration with a metric fluent in the initial state (e.g. (= (durationOfAction) 10)) and then use this fluent in the effects.\nTemporal-metric-continuous problems form a significant subset of problems expressible in the PDDL+ language (Fox & Long, 2006), including those with linear continuous change within durative actions. The problems do not include non-linear continuous change, nor do they explicitly represent events or processes, although the use of certain modelling tricks can capture similar behaviours."}, {"heading": "4.1 An Example Problem", "text": "As a running example of a temporal-metric-continuous domain we use the problem shown in Figure 1. In this, the Borrower Domain, a borrower can use a mortgage to buy a house. The domain is simplified in order to focus attention on some key aspects of continuous reasoning and is not proposed as a realistic application. Furthermore, the domain does not exploit variable duration actions, even though the ability to handle these is a key feature of COLIN. The example illustrates required concurrency, by means of interesting interactions between multiple actions affecting a single continuous variable, and allows us to demonstrate the differences between alternative heuristics described in Section 9. Management of required concurrency is also a key feature of COLIN, and domains with variable durations are discussed later in the paper.\nIn this domain, to obtain a mortgage it is necessary to have an appropriate active savings plan and to be able to lay down a deposit. These conditions are both achieved by saving hard, an action that cannot be applied in parallel with itself, preventing the borrower from building up capital at an arbitrarily high rate by multiple parallel applications of saveHard. For the sake of the example we restrict the saving periods to durations of 10 years to produce interesting interactions with the\ndurations of the mortgages in the sample problem. Once a person starts saving he or she is tied into a 10-year savings plan.\nThe constraint on being able to start a mortgage leads to required concurrency between saving and taking a mortgage. The effects of saving and repaying interest therefore combine to yield different linear effects on the value of the money variable, while the saving action requires this variable to remain non-negative throughout the duration of the saveHard action. Furthermore, in order to qualify for tax relief, each mortgage carries a maximum allowed level of savings throughout the mortgage (which prevents the mortgage being taken too late in the savings plan). Finally, the lifeAudit action places a constraint on the gap between the end of the saving action and the point at which the mortgage is completed (and also ensures that the borrower does not end up in debt). This action acknowledges that borrowers will only be happy if they manage to complete their mortgages within short periods (limited by their patience) of having to save hard.\nThe simple problem instance we will consider is shown in Figure 2. Two possible solutions to this are shown in Figure 3. In the first solution the borrower takes the longer mortgage, which has the advantage that it can start earlier because it requires a lower deposit. Money rises at rate 1 over the first part of the saving action, then decreases by 1 when the mortgage starts. It then rises at rate 0.25 (the difference between the saving and mortgage rates) until the saving action concludes, when it continues to decrease at rate 0.75 until the mortgage ends. The life audit action must start during a saving action and cannot end until after the end of a mortgage action. In the second solution the borrower takes the shorter mortgage, but that cannot start as early because it requires a much larger deposit. As a consequence, the life audit cannot start during the first saving action: the mortgage finishes too late to be included inside a life audit beginning within the first saving action. To meet the initial condition of the life audit, the borrower must therefore perform a second saving action to follow the first. Clearly the first solution is preferable since we are interested in minimising the makespan."}, {"heading": "5. Background in Metric and Temporal Planning", "text": "Most recent work on discrete numeric planning is built on the ideas introduced in the planner MetricFF (Hoffmann, 2003). A discrete numeric planning problem introduces numeric variables into the planning domain that can hold any real numeric value (or be undefined, if they have not yet been given a value). Actions can have conditions expressed in terms of these variables, and have effects that act upon them. To provide heuristic guidance, Metric-FF introduced an extension of the relaxed planning graph (RPG) heuristic (Hoffmann & Nebel, 2001), the Metric RPG heuristic, supporting the computation of a relaxed plan for a problems involving discrete numeric change. As with the propositional RPG heuristic, it performs a forwards-reachability analysis in which the delete effects of actions are relaxed (ignored). For numeric effects, ignoring decrease effects does not always relax the problem, as conditions can require that a variable hold a value less than a given constant. Thus, as the reachability analysis extends forwards, upper- and lower- bounds on the values of numeric variables are computed: decrease effects have no effect upon the upper bound and increase effects have no effect upon the lower bound, while assignment effects replace the value of the upper (lower) bound if the incumbent has a lower (greater) value (respectively) than that which would be assigned. Deciding whether a precondition is satisfied in a given layer is performed (optimistically)\non the basis of these: for a condition w \u00b7 v \u2265 c1, then an optimistically high value for w \u00b7 v can be computed by using the upper bound on each fluent v assigned a value in v if its corresponding weight in w is positive, or, otherwise, using its lower bound.\nAn alternative to the use of a Metric RPG is proposed in LPRPG (Coles, Fox, Long et al., 2008b), where a linear program is constructed incrementally to capture the interactions between actions. This approach is restricted to actions with linear effects, so is not as general as Metric-FF, but it provides a more accurate heuristic guidance in handling metric problems and can perform significantly better in problems where metric resources must be exchanged for one another in order to complete a solution.\nNumeric planning also gives the opportunity to define metric optimisation functions in terms of metric variables within the problem description. For example, an objective to minimise fuel consumption can be defined for domains where the quantity of fuel available is a metric variable. This optimisation function can also include the special variable total-time, representing the makespan (execution duration) of the plan. Most planners are restricted to a weighted sum across variables (although PDDL2.1 syntax allows it to be an unrestricted expression across variables). In general, planners are not yet capable of optimising metric functions effectively: the task of finding any plan remains difficult. However, there are some planners that attempt to optimise these functions, the most notable being LPG (Gerevini & Serina, 2000) (and, in domains where the only numeric effects are to count action cost, LAMA, due to Richter & Westphal, 2010).\nAlthough the introduction of PDDL2.1 led to an increased interest in temporal planning, earlier work on planning with time has been influential. IxTeT (Ghallab & Laruelle, 1994) introduced chronicles, consisting of temporal assertions and constraints over a set of state variables, and timelines which are chronicles for single state variables. Timelines have since been widely used by planners that have followed a different trajectory of development than that led by the PDDL family of languages (Pell, Gat, Keesing, Muscettola, & Smith, 1997; Frank & Jo\u0301nsson, 2003; Cesta, Cortellessa, Fratini, & Oddi, 2009). IxTeT also pioneered the use of many important techniques, including simple temporal networks and linear constraints.\nThe language introduced for the planner \u2018Temporal Graph Plan\u2019 (TGP) (Smith & Weld, 1999) allowed (constant) durations to be attached to actions. The semantics of these actions required their preconditions, pre, to be true for the entire duration of the action, and the effects of the actions, eff, to become available instantaneously at their ends. The values of affected variables are treated as undefined and inaccessible during execution, although the intended semantics (at least in TGP) is that the values should be considered unobservable during these intervals and, therefore, plans should be conformant with respect to all possible values of these variables over these intervals. TGP solves these problems using a temporally extended version of the Graphplan planning graph (Blum & Furst, 1995) to reason with temporal constraints. A temporal heuristic effective for this form of temporal planning was developed by Haslum and Geffner (2001) and Vidal and Geffner (2006) have explored a constraint propagation approach to handling these problems.\nEven when using the more expressive temporal model defined in PDDL2.1, many temporal planners make use of the restricted TGP semantics, exploiting a simplification of the PDDL2.1 encoding known as \u2018action compression\u2019. The compression is performed by setting pre to be the weakest preconditions of the actions, and eff + (eff \u2212) to be their strongest add (delete) effects. In the propo-\n1. Conditions w \u00b7 v \u2264 c can be rewritten in this form by negating both sides. Further, those stating w \u00b7 v = c can be rewritten as a pair of conditions, w \u00b7 v \u2265 c and \u2212(w \u00b7 v) \u2265 \u2212c\nsitional case, in terms of the action representation introduced earlier, these are:\npre = pre` \u222a ((pre\u2194 \u222a prea) \\ eff +` )\neff + = (eff +` \\ eff \u2212 a ) \u222a eff + a\neff \u2212 = ((eff \u2212` \\ eff + ` ) \u222a eff \u2212 a ) \\ eff + a\nMany modern temporal planners, such as MIPS-XXL (Edelkamp & Jabbar, 2006) and earlier versions of LPG (Gerevini & Serina, 2000), make use of this action compression technique. However, applying the compression can lead to incompleteness (Coles, Fox, Halsey, Long, & Smith, 2008) (in particular, a failure to solve certain temporal problems). The issues surrounding incompleteness were first discussed with reference to the planner CRIKEY (Fox, Long, & Halsey, 2004) and, later, the problem structures causing this were said to introduce required concurrency (Cushing, Kambhampati, Mausam, & Weld, 2007). The Borrower domain is one example of a problem in which the compression prevents solution. Both the lifeAudit and takeMortgage actions have initial preconditions that can only be satisfied inside the interval of the saveHard action, since this action adds saving at its start, but deletes it at its end.\nRequired concurrency is a critical ingredient in planning with continuous effects, as both when change occurs and what change occurs are important throughout the execution of actions. In order to avoid producing poor quality plans or, indeed, excluding possible solutions, we must allow concurrency between actions wherever the problem description permits it. A na\u0131\u0308ve extension of the compression approach would discretise continuous numeric change into step function effects occurring at the ends of the relevant actions, precluding any possibility of managing the interaction between numeric variables during execution of actions with continuous effects. We therefore build our approach on a planner capable of reasoning with required concurrency. In the Borrower domain, the mortgage action must overlap with the saving action, but it cannot be too early (to meet the deposit requirement) or too late (to meet the maximum savings constraint and to ensure that the life audit can be performed as early as possible). As this example illustrates, problems that include reasoning with continuous linear change typically also require concurrency.\nSeveral planners are, currently, capable of reasoning with the PDDL2.1 start\u2013end semantics, as opposed to relying on a compression approach. The earliest PDDL2.1 planner that reasons successfully with the semantics is VHPOP (Younes & Simmons, 2003), which is a partial-order planner.\nThis planner depends on heuristic guidance based on the same relaxed planning graph that is used in FF, so the guidance can fail in problems with required concurrency. Nevertheless, the search space explored by VHPOP includes the interleavings of action start and end points that allow solution of problems with required concurrency. VHPOP suffers from some of the problems encountered in earlier partial-order planners and its performance scales poorly in many domains. TPSYS (Garrido, Fox, & Long, 2002; Garrido, Onainda, & Barber, 2001) is a Graphplan-inspired planner that can produce plans in domains with required concurrency. Time is represented by successive layers of the graph, using a uniform time increment for successive layers. This approach is similar to the way that TGP uses a plan graph to represent temporal structure, but TPSYS supports a model of actions that separates the start and end effects of actions as dictated by PDDL2.1 semantics.\nAnother planner that adopts a Graphplan-based approach to temporal planning is LPGP (Long & Fox, 2003a), but in its case the time between successive layers is variable. Instead of using layers of the graph to represent the passage of fixed-duration increments of time, they are used to represent successive happenings \u2014 time points at which state changes occur. The time between successive state changes is allowed to vary within constraints imposed by the action durations whose end points are fixed at particular happenings. A linear program is constructed, incrementally, to model the constraints and the solution of the program is interleaved with the selection of action choices. This approach suffers from most of the weaknesses of a Graphplan planner: the exhaustive iterative deepening search is impractical for large problems, while computation and storage of mutex relations becomes very expensive in larger problems. Nevertheless, LPGP provides a useful approach to the treatment of PDDL2.1 durative actions, by splitting them into their end points which are treated as instantaneous \u2018snap\u2019 actions. A solution to the (original) planning problem can be expressed in terms of these, subject to four conditions:\n1. Each start snap-action is paired with an end snap-action (and no end can be applied without its corresponding start having been applied earlier);\n2. Between the start and end of an action, the invariants of the action pre\u2194 are respected;\n3. No actions must be currently executing for a state to be considered to be a goal state;\n4. Each step in the plan occurs after the preceding step, and the time between the start and end of an action respect its duration constraints.\nSAPA (Do & Kambhampati, 2003) is one of the earliest forward-search planners to solve temporal PDDL2.1 problems. It works with a priority queue of events. When a durative action is started its end point is queued at the time in the future at which it will be executed. The choice points of the planner include starting any new action, but also a special wait action, which advances time to the next entry in the queue, and the corresponding action end point is executed. This allows SAPA to reason with concurrency and to solve some problems with required concurrency. Unfortunately, its search space does not include all necessary interleavings to achieve a complete search. For example, consider the problem illustrated in Figure 4. To solve this problem, action A must start, then action B must start early enough to allow C to complete before A ends (and deletes P ) and late enough that action D can start before B ends but end after A ends. All of the actions are required in order to allow D to be applied, achieving the goal G. After SAPA starts action A, the queue will contain the end of A. The choices now open are to start B immediately, but this will then end too early to allow D to execute successfully, or else to complete A, which advances time too far to allow B to\nLight Match\nexploit effect P of A, preventing C from being executed. In fact, a simpler problem defeats SAPA: if B were to have end condition Q instead of T and end effect G then C and D can be dispensed with. However, the additional complexity of the existing example is that it is impossible to infer when to start B by examination of A and B alone, because the timing constraints on the start of B depends on both actions C and D and it is not immediately obvious how their temporal constraints will affect the placement of B. The difficulty in adopting the waiting approach is that it is hard to anticipate how long to wait if the next interesting time point depends on the interaction of actions that have not yet even been selected.\nA different approach to forward-search temporal planning is explored in the CRIKEY family of planners (Coles, Fox, Halsey et al., 2008; Coles, Fox, Long et al., 2008a). These planners use the same action splitting approach used in LPGP, but work with a heuristically guided forward search. The heuristics in these planners use a relaxed planning graph as a starting point (Hoffmann & Nebel, 2001), but extend it by adding some guidance about the temporal structure of the plan, pruning choices that can be easily demonstrated to violate temporal constraints and inferring choices where temporal constraints imply them. The planners use a Simple Temporal Network to model and solve the temporal constraints between the action end points as they are accumulated during successive action choices. Split actions have also been used to extend LPG into a temporal version that respects the semantics of PDDL2.1 (Gerevini, Saetti, & Serina, 2010) (earlier versions of LPG use the compressed action models described above). Recent work by Haslum (2009) has explored other ways in which heuristics for temporal planning can be constructed, while remaining admissible.\nTemporal Fast Downward (Eyerich et al., 2009), based on Helmert\u2019s Fast Downward planner (Helmert, 2006), uses an approach that is a slight refinement of the compressed action model, allowing some required concurrency to be managed. The authors demonstrate that this planner can solve the Match problem shown in Figure 5. They mistakenly claim that SAPA cannot solve this problem because it cannot consider applying an action between starting and ending lighting the match: in fact, SAPA can apply the mend fuse action after the match is lit, in much the same way as is done in Temporal Fast Downward. The problem that both planners face is in situations in which an action must be started some time after the last happening, but before the next queued event: neither planner includes this choice in its search space.\nHuang et al. (2009) developed a temporal planner exploiting the planning-as-SATisfiability paradigm. This uses a Graphplan-to-SAT encoding, starting with an LPGP action-splitting compilation, and using a fixed time increment between successive layers of the graph. This approach is\nadequate for problems where an appropriate time increment can be identified, but this is not possible, in general, when there are time-dependent effects in a domain. Furthermore, the approach is ineffective when there is significant difference between the durations of actions, so that the time increment becomes very short relative to some actions. The planner can produce optimal (makespan) plans using iterative deepening search. The planner combines existing ideas to achieve its objectives and it is mainly of interest because of its relationship to other SAT-based approaches to temporal planning, such as TM-LPSAT discussed below.\nCRIKEY3, and the other planners mentioned, are only capable of solving the simple temporal planning problems described above. They are restricted to the management of discrete change. Duration-dependent change cannot be handled by these planners. In fact, not all of these planners can manage any kind of reasoning with numbers outside the durations of actions. COLIN therefore significantly extends the competence of other PDDL-compliant temporal planners.\n6. CRIKEY3: A Forward-Chaining Temporal Planner\nTemporal forward-chaining planners have two kinds of choices to make during the construction of plans. Firstly, as in the non-temporal case, a choice must be made of which actions to apply (these choices can be considered to be the \u2018planning\u2019 element of the problem). Secondly, choices must be made of when to apply the actions (these can be seen as the \u2018scheduling\u2019 choices in construction of solutions). CRIKEY3 (Coles, Fox, Long et al., 2008a), a temporal forward-chaining planner, exploits the distinction between these choices, using separate procedures to make the planning decisions (which actions to start or end) and the scheduling decisions (when to place actions on the timeline). Both of these decisions must be checked for consistency with respect to the existing temporal constraints to confirm that all the actions can be completely scheduled. In this section, we briefly describe how CRIKEY3 performs planning and scheduling, since its architecture forms the basis for COLIN and the work subsequently described in this paper. Full details of temporal management in CRIKEY3 are provided by Coles et al.\nCRIKEY3 uses a forward-chaining heuristic state-space search to drive its planning decisions. It makes use of the Enforced Hill-Climbing (EHC) algorithm introduced in FF (Hoffmann & Nebel, 2001) and repeated, for convenience, as Algorithm 1. EHC is incomplete, so if a solution cannot be found CRIKEY3 plans again, using a weighted A* search. We now discuss how the search described within the basic enforced hill-climbing algorithm of FF can be extended to perform temporal planning. In order to do this, a number of modifications are required. In particular:\n1. get applicable actions(S): the planner must reason with two actions per durative action, a start action and an end action, rather than applying an action and immediately considering it to have finished (as in the non-temporal case).\n2. get applicable actions(S), apply(a, S): invariant conditions of durative actions must be maintained throughout their execution, which requires active invariants to be recorded in the state in order to prevent the application of actions that conflict with them.\n3. is goal state(S): for a state to be a goal state (i.e. for the path to it to be a solution plan) all actions must have completed.\nAlgorithm 1: Enforced Hill-Climbing Algorithm Data: P = \u3008A, I,G\u3009 - a planning problem Result: P , a solution plan best heuristic \u2190 evaluate heuristic(I);1 if best heuristic = 0 then2\nreturn [];3 closed \u2190 {I};4 open list \u2190 [\u3008I, []\u3009];5 while open list 6= [] do6 \u3008S, P \u3009 \u2190 element removed from the front of open list ;7 applic(S)\u2190 get applicable actions(S);8 apply helpful filter(applic(S));9 foreach a \u2208 applic(S) do10 S\u2032 \u2190 apply(a, S);11 if S\u2032 6\u2208 closed then12 add S\u2032 to closed ;13 P \u2032 \u2190 P followed by a;14 if is valid plan(P \u2032) then15 if is goal state(S\u2032) then16 return P \u2032;17\nh\u2190 evaluate heuristic(S\u2032);18 if h < best heuristic then19 open list \u2190 [\u3008S\u2032, P \u2032\u3009];20 best heuristic \u2190 h;21 break;22\nelse23 if h <\u221e then24 append \u3008S\u2032, P \u2032\u3009 onto open list;25\nreturn with failure;26\n4. is valid plan(P ): the temporal (scheduling) constraints of candidate plans must be respected. In particular, the duration constraints of durative actions must be satisfied. This is discussed in Section 6.1.\nWe consider each of these modifications in turn. First, durative actions are compiled into two non-temporal actions. A modified version of the LPGP action compilation (Long & Fox, 2003a) is used for this, as described by Coles et al. (2008). Each durative action a, of the form \u3008dur , pre`, eff `, pre\u2194, prea, eff a\u3009, is split into two non-temporal (in fact, instantaneous) \u2018snap actions\u2019 of the form \u3008pre, eff \u3009:\n\u2022 a` = \u3008pre`, eff `\u3009\n\u2022 aa = \u3008prea, eff a\u3009\nBy performing search with these snap actions, and taking appropriate care to ensure that the other constraints are satisfied, the restrictions on expressivity imposed by the use of action compression are avoided. It becomes possible to search for a plan in which the start and end points of different actions are coordinated, solving problems with required concurrency. The price for this is that the search space is much larger: each original action is replaced by two snap-actions, so the length of solution plans is doubled. In some circumstances this blow-up can be avoided by identifying actions that are compression safe (Coles, Coles, Fox, & Long, 2009a), i.e. those for which the use of action compression does not compromise soundness or completeness. In the approach described by Coles et al., these actions are still split into start and end snap-actions, but the end points of compression-safe actions are inserted when either their effects are needed or their invariants would otherwise be violated by another action chosen for application. As a consequence, only one search decision point is needed per compression-safe action (choose to apply its start), rather than two. Recent versions of both CRIKEY3 and COLIN make use of this restricted action compression technique in search.\nHaving split actions into start and end points, modifications to the basic search algorithm are needed to handle the constraints that arise as a consequence. CRIKEY3 makes use of an extended state representation, adding two further elements to the state tuple. The resulting state is defined as S = \u3008F, P,E, T \u3009, where:\n\u2022 F represents the facts that hold in the current world state: a set of propositions that are currently true, W , and a vector, v, recording the values of numeric variables.\n\u2022 P is an ordered list of snap actions, representing the plan to reach S from the initial state.\n\u2022 E is an ordered list of start events, recording actions that have started but not yet finished;\n\u2022 T is a collection of temporal constraints over the actions in the plan to reach F .\nThe purpose of the start event list E is to record information about the currently executing actions, to assist in the formation of sound plans. Each entry e \u2208 E is a tuple \u3008op, i , dmin, dmax \u3009 where:\n\u2022 op is the identifier of an action, for which the start snap-action op` has been added to the plan;\n\u2022 i is the index at which this snap-action was added in the plan to reach S;\n\u2022 dmin, dmax are the minimum and maximum duration of op, determined in the state in which op is started.\nThe minimum and maximum duration of an action can depend on the state in which it is applied (e.g. the duration of a recharge action may depend on the level of charge at the time of execution), so durations must be computed based on the state preceding step i. However, once a given action has started, the bounds on the duration remain fixed. PDDL2.1 also allows actions to have durations constrained by conditions that hold at the end of the action, but such actions are not supported by our planners.\nThis extended state definition leads to corresponding extensions to get applicable actions(S). As before, a snap-action is deemed to be logically applicable in a state S if its preconditions pre are satisfied in S. However, an additional condition must be satisfied: its effects must not violate\nany active invariants. The invariants active in a given state are determined from E \u2014 we denote the invariants in a state S with event list E as:\ninv(S) = \u222a e\u2208E e.op.pre\u2194\nTo apply the end snap-action, aa, there is required to be an entry e \u2208 E whose operator entry op is equal to a. This prevents the planner from attempting to apply the ends of actions that have not yet been started.\nAssuming an action, a, is found to be applicable and chosen as step i of a plan, the function apply(a, S), applied to a temporally-extended state, S, yields a successor S\u2032 = \u3008F \u2032, P \u2032, E\u2032, T \u2032\u3009. The first two elements are updated as in the non-temporal case: F \u2032 = apply(a, F ), andP \u2032 = P+[a]. To obtain T \u2032, we begin by setting T \u2032 = T . Furthermore, if i > 0:\nT \u2032 = T \u2032 \u222a { \u2264 t(i)\u2212 t(i\u2212 1)}\nwhere t(i) is the variable representing the time at which step i is scheduled to be executed. That is, the new step must come at least (a small unit of time) after the preceding step. This separation respects the requirement that interfering actions must be separated by at least (Fox & Long, 2003), but it is strictly stronger than required where actions are not actually mutually exclusive. A more accurate realisation of the PDDL2.1 semantics could be implemented, but it would incur a cost while offering very little apparent benefit. Finally, the resulting value of E\u2032 (and whether T \u2032 is changed further) depends on whether a is a start or end snap-action:\n\u2022 if a start action a` is applied, E\u2032 = E + [\u3008a, i, dmin, dmax \u3009], where dmin and dmax correspond to the lower- and upper-bounds of the duration of a, as evaluated in the context of valuation F .\n\u2022 if an end action aa is applied, a start entry {e \u2208 E | e.op = a} is chosen, and then E\u2032 is assigned a value E\u2032 = E \\ e. It will often be the case that there is only one instance of an action open, so there is only one choice of pairing, but in the case where multiple instances of the same action are executing concurrently, search branches over the choice of each such e. For the e chosen, a final modification is then made to T \u2032 to encode the duration constraints of the action that has just finished:\nT \u2032 = T \u2032 \u222a {e.dmin \u2264 t(i)\u2212 t(e.i) \u2264 e.dmax}\nWith this information encoded in each state about currently executing actions, the extension needed to is goal state(S) is minor: a state S is a goal state if it satisfies the non-temporal version of is goal state(S), and if the event list of the state, E, is empty.\nThis search strategy leads to a natural way to handle PDDL2.2 Timed Initial Literals (TILs) directly. Dummy \u2018TIL actions\u2019 are introduced, comprising the effects of the TILs at each time point, and these can be added to the plan if all earlier TIL actions have already been added, and if they do not delete the invariants of any open action. As a special case, TIL actions do not create an entry in E: only the facts in F are amended by their execution. They do, however, produce an updated set of temporal constraints. As with snap actions, if a TIL is added as step i to a plan, the TIL must fall no earlier than after the preceding step. Then, T \u2032 = T \u2032 \u222a {ts \u2264 t(i)\u2212 t(\u03b1) \u2264 ts},\nwhere ts is the time-stamp at which the TIL is prescribed to happen, \u03b1 is the name denoting the start of the plan and t(\u03b1) = 0. As can be seen, these constraints ensure that the TIL can only occur at an appropriate time, that any step prior to the TIL must occur before it, and that any step after the TIL must occur after it.\nThe changes described in this subsection ensure that the plans produced by CRIKEY3 are logically sound: the check for logical applicability, coupled with the maintenance of E throughout search, ensures that no preconditions, either propositional or numeric, can be broken. Use of get applicable actions(S) only guarantees that actions are logically applicable: there is no guarantee that adding a snap-action to the plan, judged applicable in this way, will not violate the temporal constraints. For example, it is possible that all preconditions are satisfied in the plan P = [a`, b`, ba, aa], so that P is logically sound. However, if the duration of b is greater than the duration of a then P is not temporally sound. In the next section we discuss how the function is valid plan(P ) is modified to identify and reject temporally inconsistent plans."}, {"heading": "6.1 Temporal Plan Consistency", "text": "A state S is only temporally consistent if the steps [0...n \u2212 1] in the plan, P , that reaches it can be assigned values [t(0)...t(n \u2212 1)], representing the times of execution of each of the corresponding steps, respecting the temporal constraints, T . This is checked through the use of is valid plan(P \u2032), called at line 15 of Algorithm 1 \u2014 this function call is trivial in the non-temporal case, but in the temporal case serves to check the temporal consistency of the plan. Any state for which the temporal constraints cannot be satisfied is immediately pruned from search, since no extension of the action sequence can lead to a solution plan that is valid.\nThe temporal constraints T built by CRIKEY3 in a state S are each expressed in the form:\nlb \u2264 t(b)\u2212 t(a) \u2264 ub where lb, ub \u2208 < and 0 \u2264 lb \u2264 ub\nThese constraints are conveniently expressible as a Simple Temporal Problem (STP) (Dechter, Meiri, & Pearl, 1989). The variables within the STP consist of the timestamps of actions, and between them inequality constraints can be specified in the above form. Crucially, for our purposes, the validity of an STP (and the assignment of timestamps to the events therein) can be determined in polynomial time by solving a shortest-path problem within a Simple Temporal Network (STN), a directed-graph representation of an STP. Each event in the STP is represented by a vertex in the STN. There is an additional node t(\u03b1) to represent time 0 and the time of the first action in the plan, t(0), is constrained to fall within of t(\u03b1). Each constraint in the above form adds two edges to the graph: one from a to b with weight ub, and one from b to a with weight \u2212lb. Attempting to solve the shortest-path problem from t(\u03b1) to each event yields one of two outcomes: either it terminates successfully, providing a time-stamp for each step, or it terminates unsuccessfully due to the presence of a negative-cost cycle within the STN indicating a temporal inconsistency (any schedule would require at least one step to be scheduled before itself).\nIn CRIKEY3, an STP is used to check the temporal consistency of the choices made to reach each step S, based on the temporal constraints T that must hold over the plan P to reach S, and additional constraints that can be determined fromE: the list of actions that have started, but not yet finished. The variables vars in the STP can be partitioned into two sets: the \u2018t\u2019 variables, t(i) for step i \u2208 P and the \u2018f \u2019 variables, one f(i) for each entry \u3008op, i, dmin, dmax \u3009 \u2208 E. The t variables correspond to the times of steps that have already been added to the plan, which might be the times\nof start or end points of actions. Some of these time points might correspond to the starts of actions that have not yet finished and it is this subset of actions (only) that will have associated f variables associated with the pending end times of those actions. For consistency with the terminology we introduced in CRIKEY3 (Coles, Fox, Long et al., 2008a), we use now to refer to the time at which the next event in the plan will occur (which could be during the execution of the last actions applied). It is the time point at which the next choice is to be made, either the start of a new action or the completion of an existing one, and can therefore be seen as the time associated with the final state, S, generated by the current plan head. There is only ever one timepoint called now and its value moves forward as the plan head extends. The constraints are then as follows:\n\u2022 T , constraining the t variables \u2014 these ensure the temporal consistency of the steps in the plan to reach S (and include any constraints introduced for timed initial literals);\n\u2022 {dmin \u2264 f(i) \u2212 t(i) \u2264 dmax | \u3008op, i, dmin, dmax \u3009 \u2208 E} \u2014 that is, for each future action end point that has been committed to (but has yet to be applied), the recorded duration constraint must be respected;\n\u2022 { \u2264 f(i) \u2212 t(n \u2212 1) | \u3008op, i, dmin, dmax \u3009 \u2208 E} \u2014 that is, each future action end point must come after the last step in the current plan, to ensure it is in the future.\n\u2022 t(now) \u2212 t(n \u2212 1) \u2265 \u2014 that is, the current time (the time at which the next event in the plan can occur) is at least after the last event in the plan.\nSolving this STP confirms the temporal consistency of the decisions made so far. If the STP cannot be solved, the state S can be pruned: the plan induced from the start\u2013end action representation is temporally invalid. The last two of these categories of constraints are particularly important: without them, pruning could only be undertaken on the basis of the plan P to reach S. Including them, however, allows the STP to identify cases where the end point of an action can never be added to the plan, as doing so would lead to temporal inconsistency. As goal states cannot contain any executing actions (i.e. E must be empty), this allows CRIKEY3 to prune states earlier from which there can definitely be no path to a state in which all end points have been added to the plan.\nTimed initial literals are easily managed in the STP using the dummy TIL actions described earlier. The constraints for each dummy TIL action that has already been applied are included in T . Each dummy TIL action yet to occur is automatically treated as the end of an action that has yet to be applied. Thus, an f variable is added for each, and in doing so, the last step in the plan so far is constrained to come before each TIL event that has yet to happen."}, {"heading": "7. Planning with Continuous Numeric Change", "text": "The most challenging variants of temporal and numeric problems combine the two to arrive at problems with time-dependent metric fluents. Although problems exhibiting hybrid discrete-continuous dynamics have been studied in other research communities for some time, for example, in verification (Yi, Larsen, & Pettersson, 1997; Henzinger, Ho, & Wong-Toi, 1995; Henzinger, 1996), where timed automata capture exactly this kind of behaviour, there has been relatively little work on continuous dynamics in the planning community.\nIn PDDL2.1 the model of mixed discrete-continuous change extends the propositional state transition model to include continuous change on the state variables. There is a state transition system\nin which discrete changes transition instantaneously between states. While the system is in a particular state, continuous change can occur on the state variables and time passes. As soon as a discrete change occurs the system changes state. In PDDL+ (Fox & Long, 2006) this is extended to allow exogenous events and processes (controlled by nature) as well as durative actions. This leads to a formal semantics that is based in the theory of Hybrid Automata (Henzinger, 1996). An action causes a discrete state change which might trigger a continuous process. This continues over time until an event is triggered leading into a new state. Some time later another action might be taken.\nEarly work exploring planning with continuous processes includes the Zeno system of Penberthy and Weld (1994), in which processes are described using differential equations. Zeno suffers from the same limitations as other partial order planners of its time, being unable to solve large planning problems without significant aid from a carefully crafted heuristic function. More importantly, a fundamental constraint on its behaviour is that it does not allow concurrent actions to apply continuous effects to the same variable. This imposes a very significant restriction on the kinds of problems that can be solved, making Zeno much less expressive than COLIN. This constraint follows, in part, from the way that the model requires effects to be specified as differential equations, rather than as continuous update effects, so that simultaneous equations must be consistent with one another rather than accumulating additive effects. As the authors say \u201cWe must specify the entire continuous behaviour over the interval [of the durative action] as our semantics insist that all continuous behaviours are the result of direct, explicit action\u201d.\nAnother early planner to handle continuous processes is McDermott\u2019s OPTOP system (McDermott, 2003), which is a heuristic search planner, using a regression-based heuristic. The \u2018plausible progression\u2019 technique used within OPTOP to guide search is not sufficiently powerful to recognise interactions that could prevent future application of actions, thereby restricting its scalability on problems of the form we consider here. OPTOP competed in the International Planning Competition in 2004, where it solved only a small subset of the problems (although, interestingly, those it solved involved an expressive combination of ADL and temporal windows that no other planner could manage). OPTOP is an interesting variant on the heuristic forward search approach, since it avoids grounding the representation, using an approach that is similar to a means-ends linear planning approach to generate relaxed plan estimates of the number of actions required to achieve the goal from a given state."}, {"heading": "7.1 TM-LPSAT", "text": "More recently, Shin and Davis developed TM-LPSAT (Shin & Davis, 2005), based on the earlier LPSAT system (Wolfman & Weld, 1999). TM-LPSAT was the first planner to implement the PDDL+ semantics. It is implemented as a compilation scheme by which a horizon-bounded continuous planning problem is compiled into a collection of SAT formulas that enforce the PDDL+ semantics, together with an associated set of linear metric constraints over numeric variables. This compiled formulation is then passed to a SAT-based arithmetic constraint solver, LPSAT. LPSAT consists of a DPLL solver and an LP solver. The SAT-solver passes triggered constraints to the LP-solver, which hands back conflict sets in the form of nogoods if the constraints cannot be resolved. If there is no solution the horizon is increased and the process repeats, otherwise the solution is decoded into a plan. In order to support concurrency the compilation exploits the LPGP separation of action start and end points. There are different versions of TM-LPSAT exploiting different solvers: LPSAT and MathSAT-04 (Audemard, Bertoli, Cimatti, Kornilowicz, & Sebastiani, 2002) have both been\nexploited. The novelty of TM-LPSAT lies in the compilation and decoding phases, since both solvers are well-established systems.\nThe compilation scheme of TM-LPSAT implements the full PDDL+ semantics. Although this includes events and processes, which are specific to PDDL+, TM-LPSAT can also handle variable duration durative actions, durative actions with continuous effects and duration-dependent end-effects. The continuous effects of concurrent actions on a quantity between two time-points are summed over all actions active on the quantity over the period. Therefore, TM-LPSAT supports concurrent updates to continuous variables.\nTM-LPSAT is an interesting approach, in theory capable of solving a large class of problems with varied continuous dynamics. However, reported empirical data suggests that the planner is very slow and unable to solve problems requiring plans of more than a few steps. It is not possible to experiment further because there is no publicly available implementation of the system."}, {"heading": "7.2 Kongming", "text": "Hui Li and Brian Williams have explored planning for hybrid systems (Li & Williams, 2008, 2011). This work has focussed on model-based control, using techniques based on constraint reasoning. The continuous dynamics of a system are modelled as flow tubes that capture the envelopes of the continuous behaviours (Le\u0301aute\u0301 & Williams, 2005). The dimensions of these tubes are a function of time (typically expanding as they are allowed to extend), with the requirement being made that successive continuous behaviours must be connected by connecting the start of one tube (the precondition surface) to the cross-section of the preceding tube; i.e. the intersection of the two spaces must be non-empty. The most relevant work in this area is in the development of the planner Kongming, described by Li and Williams.\nKongming solves a class of control planning problems with continuous dynamics. It is based on the construction of fact and action layers and flow tubes, within the iterative plan graph structure introduced in Graphplan (Blum & Furst, 1995). As the graph is developed, every action produces a flow tube which contains the valid trajectories as they develop over time. Starting in a feasible region, actions whose preconditions intersect with the feasible region can be applied and the reachable states at any time point can be computed using the state equations of the system. In the initial state of the system all the variables have single known values. A valid trajectory must pass through a sequence of flow tubes, but must also meet the constraints specified in the dynamics of the actions selected. The mutex relation used in Graphplan is extended to the continuous dynamics as well as the propositional fragment of the language. The graph is iteratively extended as in Graphplan, with a search for a plan conducted after each successive extension.\nThe plan-graph encoding of a problem with continuous dynamics is translated into a Mixed Logical-Quadratic Program (MLQP). The metric objective functions used by the planner to optimise its behaviour can be defined in terms of quadratic functions of state variables. An example problem considered by Li and Williams (2008) is a 2-d representation of a simple autonomous underwater vehicle (AUV) problem where the AUV can glide, ascend and descend while avoiding obstacles. The language used is a version of PDDL2.1 extended to enable dynamics to be encoded. The continuous nature of the problem lies in the fact that, after a continuous action, the AUV will be in one of a continuous range of positions determined by the control system. Because Kongming depends on translation of the planning problems into MLQPs the constraints describing the dynamics of the problem must be linear. Since the effects of continuous actions involve the product of rate\nof change with time, only one of these values can be treated as a variable. In Kongming it is the rate of change that is variable, but time is discretised, which contrasts with COLIN in which rates of change remain constant over continuously variable length intervals. The discretisation of time in Kongming is exploited to support state updates within the plan graph: successive layers of the graph are separated by a constant and uniform time increment. This approach suffers from a disadvantage that the duration of a plan is limited by the number of happenings in the plan, since the solver cannot realistically solve problems with more than a few tens of layers in the plan graph.\nKongming does not support concurrent continuous updates to the same state variable, so, in this respect, PDDL2.1 is more expressive than the extended language used in Kongming. In part this is due to a difficulty in resolving precisely what is the semantics of the dynamics described in the actions used by Kongming. Each dynamic constraint specifies limits on the rate of change of a specific variable: it is unclear whether concurrent actions should be combined by taking the union or the intersection of the bounds each constraint specifies on the rate of change of a given fluent."}, {"heading": "7.3 UPMurphi", "text": "One other recently developed planner that uses PDDL2.1 and reasons with continuous processes is UPMurphi (Penna, Intrigila, Magazzeni, & Mercorio, 2009). UPMurphi takes a completely different approach to those considered so far. Instead of reasoning about continuous change directly, UPMurphi works by guessing a discretisation and iteratively refining it if the solution to the discretised problem does not validate against the original problem specification. The iterative driver is the coarseness of the discretisation, as well as the planning horizon, making it an interestingly different basic architecture from TM-LPSAT.\nUPMurphi begins with the continuous representation of the problem and starts by discretising it. First the actions are discretised by taking specific values from their feasible ranges. This results in several versions of each action. Then UPMurphi explores the state space, by explicitly constructing it under the current discretisation. Plans are constructed using the planning-as-model-checking paradigm (Cimatti, Giunchiglia, Giunchiglia, & Traverso, 1997): there is no heuristic to guide search. Once a plan has been found it is then validated against the original continuous model, using the plan validator (Fox, Howey, & Long, 2005). If it is invalid, the discretisation is refined and the search resumes. If UPMurphi fails to find a plan at one discretisation it starts again at a finer grained discretisation. Subsequent refinements lead to ever denser feasible regions, but they are increasingly complex to construct.\nUPMurphi can be used to build partial policies to handle the uncertainty that is likely to arise in practice during the execution of hybrid control plans. A controller table is initially synthesised, consisting of the (state,action) pairs of the plan it first constructs. However, this table might lack some of the states that could be visited by the controller, so it is not robust. The subsequent step is to \u201crobustify\u201d the controller by randomly perturbing some of the states and finding new paths from these new states. Because some of the perturbed states are not reachable, a probability distribution is used to identify the most likely ones. These are called the safe states. The controller table is then extended with the safe (state, action) pairs. The controller table, or policy, is referred to as a Universal Plan."}, {"heading": "7.4 Other Approaches to Continuous Reasoning", "text": "A completely different way to manage continuous quantities is to model continuous resource consumption and production in terms of uncertainty about the amount consumed or produced. This is the approach taken in the HAO* algorithm (Meuleau, Benazera, Brafman, Hansen, & Mausam, 2009) where a Markov Decision Process (MDP) is constructed consisting of hybrid states. Each state contains a set of propositional variables and also a collection of distributions over resource consumption and production values. Because the states are hybrid, standard value iteration approaches cannot be used to find policies. A hybrid AO* approach is described which can be used to find the best feasible policy. The feasible region constructed by HAO* is a continuous distribution of resource values and the resource is considered to be uncontrollable (unlike in Kongming, where it is assumed that the executive maintains control over which values in the region are eventually chosen).\nPlanning with continuous processes has important applications and, as with many other application areas of planning, this has led to the development of systems that combine generic planning technology with more carefully tuned domain-specific performance to achieve the necessary combination of problem coverage and performance. A good example of this is the work by Boddy and Johnson (2002) and colleagues (Lamba et al., 2003) on planning oil refinery operations. This work uses a quadratic program solver, coupled with heuristically guided assignment to discrete decision variables (corresponding to actions), to solve real problems.\n8. COLIN: Forward Chaining Planning With Continuous Linear Change\nIn this section we will describe how CRIKEY3 is extended to reason with duration-dependent and continuous numeric change, building the planner COLIN ( for COntinuous LINear dynamics). We decided to give the planner a specific name to highlight its capabilities. As demonstrated in Section 4.1, the key difference introduced with continuous numeric change is that logical and numeric constraints can no longer be neatly separated from temporal constraints: the values of the numeric variables in a state depend on the timestamps and durations of actions, and vice versa. The relative benefits of handling temporal and numeric constraints together, rather than separating them out, are apparent in the motivating domains outlined in Section 3 and have been amply rehearsed in the paper describing PDDL+ (Fox & Long, 2006).\nThe need to cope with integrated numeric and temporal constraints raises a number of important issues for planning with these domains. First, checking whether an action choice is consistent can no longer be achieved using an STP, as the numeric constraints now interact with the temporal constraints, and an STP is not sufficiently expressive to capture this. Second, the changing values of numeric variables over time brings new challenges for determining action applicability: if a precondition is not satisfied immediately following the application of an action, it might become satisfied after allowing a certain amount of time to elapse. Finally, there is the need to provide heuristic guidance. We will cover the first two of these issues in this section, and defer discussion of the heuristic guidance to the next."}, {"heading": "8.1 Temporal-Numeric Plan Consistency Through Linear Programming", "text": "We begin with the problem of temporal-numeric plan consistency, as the techniques used in dealing with this issue can also be amended for use in solving the issues encountered when determining\naction applicability. Considering the definition of the STP given in Section 6.1, we make the observation that the STP could equally well be written as a linear program (LP). In CRIKEY3, the STP is more efficiently solved using a shortest-path algorithm. However, this observation becomes important when we wish to reason with continuous change in numeric resources alongside the temporal constraints. In this case, we can use an LP to capture both temporal constraints and numeric constraints, including the interaction between the two. We will now describe how the LP is built, serving as a replacement for the is valid plan(S) function called during search, which invokes the STP solver in CRIKEY3. A diagram of the structure of the LP we create is shown in Figure 6, for a plan P = [a0, ..., an\u22122, an\u22121] to reach a state S, where an\u22121 is the action most recently added to the plan. (For simplicity, it shows a case where the event queue E is empty.)\nThe construction of the LP begins with the variables and (a subset of) the constraints of the STP. Each STP variable ti (the time-stamp of the (snap) action ai) has a corresponding LP variable stepi (shown across the top of Figure 6), and each STP variable ei (for the future end of the action at step i) has a corresponding LP variable estepi. We also construct the constraints corresponding to the total-ordering of action steps, just as in the STP: each step in P is still sequenced (i.e. \u2264 stepi \u2212 stepi\u22121 for all n > i > 0), and each future end snap-action has to be later than stepn\u22121 (i.e. \u2264 estepi \u2212 stepn\u22121 for all estep variables).\nWe then extend the LP with the numeric constraints of the problem, beginning with the effects of actions. Since numeric effects can be both discrete and continuous, we create two additional vectors of variables per step in the plan. The first of these, vi, represents the values of the state variables v immediately prior to ai being executed (in the case of step 0, vi is equal to the values of v in the initial state, I). The second, v\u2032i, contains the values of v immediately after ai is executed. In Figure 6, the variables in v0 are enumerated as v0...vm\u22121 and, similarly, those in v\u20320 are shown as v\u20320...v \u2032 m\u22121. To avoid proliferation of indices we do not further index these values with their time stamp in Figure 6, so vi is the ith value in v at the time step corresponding to the layer in which the variable appears. The use of two vectors at each layer is required in order to represent discrete changes caused by actions: a snap-action can cause the value of a variable to be different immediately after its execution. To represent this within the LP, if an action at step i has no effect on a variable v then v\u2032i = vi\n2. Otherwise, for a discrete effect \u3008v\u2032+=w \u00b7 v + k.(?duration) + c\u3009, a constraint is introduced to define the value of v\u2032i : 3\nv\u2032i = vi + w \u00b7 v + k.(ce(i)\u2212 cs(i)) + c\nwhere the functions cs(i) and ce(i) denote the time-stamp variables for the corresponding start and end of the action at step i. If step i is the end of an action, then ce(i) = stepi, and cs(i) is the step variable for the start of the action that finished at step i. Similarly, if step i initiates an action, then cs(i) = stepi, and ce(i) is either estepi if the action has not yet finished or, otherwise, the step variable for the end of the action started at step i. Therefore, substituting ce(i) \u2212 cs(i) for ?duration captures the relationship between the effect of the action and its duration.\n2. Note that identities such as this are implemented efficiently by simply not introducing the unnecessary additional variable. Similarly, while a variable is subject to no effects or conditions it is not added to the LP, but it is only introduced once it becomes relevant. 3. For effects using the operator -=, i.e. decrease effects, all but the first term on the right-hand side are negated. For assignment effects, where the operator is =, the first term on the right-hand side (i.e. vi) is omitted entirely (the value of v after such an assignment does not depend on the value of v beforehand).\nContinuous numeric change occurs between the steps in the plan, rather than at the instant of execution of the step itself. To capture continuous effects, when building the LP we consider each step in turn, from the start of the plan, recording the gradient of the total (linear) continuous change acting upon each variable v \u2208 v, where \u03b4v denotes the gradient active after ai\u22121 and before the execution of action ai. Under the restrictions on the language handled by COLIN, described in Section 4, and the total-order constraints between snap-actions, the value of each variable \u03b4vi is known and constant within each interval between successive actions: all continuous change is linear. The gradient on a variable v can only be changed by either starting an action (initiating an\nadjustment to the prevailing continuous effect on v given by dvdt += k, for some k \u2208 <) or ending an action (terminating the effect initiated by its start). The values of the \u03b4 constants can be computed as follows4:\n\u2022 For all variables, \u03b4v0 = 0; that is, there is no continuous numeric change active on any variable before the start of the plan.\n\u2022 If ai has no continuous numeric effect on v then \u03b4vi+1 = \u03b4vi;\n\u2022 If ai initiates a continuous numeric effect, dvdt += k, then \u03b4vi+1 = \u03b4vi + k;\n\u2022 If ai terminates a continuous numeric effect, dvdt += k, then \u03b4vi+1 = \u03b4vi \u2212 k;\nOn the basis of these values, we now add constraints to the LP:\nvi+1 = v \u2032 i + \u03b4vi+1(stepi+1 \u2212 stepi)\nAgain, the distinction between vi and v\u2032i is important: vi is determined on the basis of any continuous change in the interval between steps i and i \u2212 1, but immediately prior to any discrete effect that may occur at that step.\nHaving created variables to represent the values of fluents at each step and having introduced constraints to capture the effects of actions on them, we now consider the constraints that arise from the preconditions of each snap-action, the invariants that must be respected between the starts and ends of actions, and any constraints on the durations of each of the actions in the plan. For each numeric precondition of the form \u3008v, {\u2265,=,\u2264},w \u00b7 v + c\u3009, that must hold in order to apply step i, we add a constraint to the LP:\nvi{\u2265,=,\u2264}w \u00b7 vi + c\nFor an action a starting at stepi and ending at stepj , the invariants of a are added to the LP in this form, once for each of the vectors of variables [v\u2032i,v \u2032 j\u22121] and [vi+1,vj ] (vi and v \u2032 j are excluded because the PDDL2.1 semantics does not require invariants of an action to hold at its end points). In the case where the end of the action a (starting at i) has not yet appeared in the plan, the invariants of a are imposed on all vectors of variables from v\u2032i onwards: as a must end in the future, its invariants must not be violated at any step in the current plan after the point where it started.\nFinally, we add the duration constraints. For an action a starting at stepi, we denote the variable corresponding to the time at which a finishes as ce(i), where ce(i) = stepj if the end of the action has been inserted into the plan at step j, or ce(i) = estepi otherwise (as defined above). Then, for each duration constraint of a, of the form \u3008?duration, {\u2265,=,\u2264},w \u00b7v+c\u3009, we add a constraint:\nce(i)\u2212 stepi{\u2265,=,\u2264}w \u00b7 vi + c\nThis process constructs a LP that captures all the numeric and temporal constraints that govern a plan, and the interactions between them. As with the STP in CRIKEY3, a solution to the LP contains values for the variables [step0...stepn], i.e. an assignment of time-stamps to the actions in the plan. To prevent the LP assigning these variables arbitrarily large (but valid) values, we set the\n4. Variables that can be trivially shown to be constant (i.e. where no action has an effect referring to that variable) can be removed from the LP and replaced throughout by their values in the initial state.\nLP objective function to be to minimise stepn, where an is the last step in the plan so far. For the purposes of the is valid plan(S) function, if the LP built for a plan P to reach a state S cannot be solved, we can prune the state S from the search space and need not consider it any further: there is no path from S to a legal goal state. In this way, the LP scheduler can be used as a replacement for the STP in order to determine plan validity."}, {"heading": "8.2 Example: LP for the Borrower Problem", "text": "In order to illustrate LP construction for a plan we consider the example Borrower problem introduced in Section 4.1. Recall that one solution plan for this problem has the following structure: 0: saveHard start"}, {"heading": "1: takeMortgage start longMortgage", "text": ""}, {"heading": "2: lifeAudit start", "text": ""}, {"heading": "3: saveHard end", "text": "4: takeMortgage end longMortgage 5: lifeAudit end. The LP for this six-step Borrower solution plan contains the variables and constraints shown in Table 2. The six step variables represent the time-stamps of the six snap-actions in the plan, and the variable m represents the money that has been saved by the Borrower. In the initial state, m = 0,\nand hencem0 = 0. Starting the saveHard action has no instantaneous numeric effects, introducing the constraint m\u20320 = m0 (if it did have an effect on m, for instance an instantaneous increase in the savings by k, then the constraint would be m\u20320 = m0 + k). Due to the invariant condition of the saveHard action, that the savings remain above zero, the constraintm\u20320 \u2265 0 is added: it can be seen this constraint is duplicated for each mi and m\u2032i during the execution of the saveHard action, to ensure that the invariant continues to hold. Notice, also, when the action takeMortgage is started, the invariant for that action (the savings level remains less than or equal to the maxSavings cap) also appears, and applies to all values of m during its execution. Additional constraints capture discrete change by connecting the value of m\u2032i to mi. In most cases in this example these values are equal, but one constraint shows a discrete effect: m\u20321 = m1 \u2212 1 captures the deduction of the deposit caused by initiating the takeMortgage action.\nAs previously described, the temporal constraints in the LP take two forms. First, there are constraints of the form stepi+1 \u2265 stepi + , forcing stepi+1 to follow stepi, enforcing the sequencing of the snap-actions. Second, duration constraints restrict the duration of actions, e.g. step3 = step0 + 10 forces that step3 (the end point of saveHard) occurs precisely 10 units (the duration of saveHard) after step0, its start snap-action.\nThe final constraints to consider are those modelling the continuous numeric change. The first constraint of this type gives the value of m1 after the execution of saveHard start and before the execution of takeMortgage start. This constraint, m1 = m\u20320 + 1.(step1 \u2212 step0), is based on the value of \u03b4m1, which is 1: the only action currently executing with continuous change on m is saveHard, which increases it by 1 per unit of time. The second such constraint, m2 = m\u20321 + 1 4 .(step2 \u2212 step1), is based on the value of \u03b4m2 which is now (1\u2212 3 4) = 1 4 , found by adding the active gradients from both of the actions that have started but not yet finished. This illustrates how two actions can have active linear continuous effects on the same variable simultaneously. Note that when saveHard end is applied (at step3) the gradient of continuous change (\u03b4m4) becomes \u221234 as the only active continuous effect is now that of the takeMortgage action.\nSolving the temporal constraints in this problem without considering the metric fluents yields a solution in which step0 = 0, step1 = , step2 = 8 + 2 , step3 = 10, step4 = 12 + and step5 = 12 + 2 . Unfortunately, this proposal violates the constraint m\u20321 \u2265 0, since:\nm\u20321 = m1 \u2212 1 = m\u20320 + 1.(step1 \u2212 step0)\u2212 1 = m0 + \u2212 1 = 0 + \u2212 1 = \u2212 1\nand 1. The constraint on the start time of the takeMortgage action cannot be identified because it is dependent on the discrete initial effect of that action, the active continuous effect of the saveHard action and the invariant of saveHard. This simple example illustrates the strength of using the LP to perform the scheduling alongside the resolution of numeric constraints: the timestamps then satisfy both temporal and numeric constraints."}, {"heading": "8.3 Temporal\u2013Numeric Search", "text": "When performing state-space search, a state, S, is a snapshot of the world along some plan trajectory, coming after one action step and before another. In the absence of continuous numeric change, the valuations that define S are known precisely: both which propositions hold, and the values of the numeric variables v. In the presence of continuous numeric change, however, the same does not hold: if a variable v is undergoing continuous numeric change (or is subject to active durationdependent change) the valuations in a state depend on which snap-actions have been applied so far,\non the times at which those snap-actions were applied and on how much time has passed since the last action was applied. Within our representation of the state the time-stamps of the snap-actions in the plan are not fixed (during plan-construction, the LP is used only to confirm that the plan can be scheduled subject to the current constraints), so the valuation of numeric fluents in S is constrained only within ranges determined by the constraints on the temporal variables and the interactions between them.\nAs a consequence of the flexibility in the commitment to values for temporal and continuously changing variables, COLIN requires a different state representation to the one used in CRIKEY3. Rather than representing the values of the numeric variables by a single vector v, we use two vectors: vmax and vmin. These hold the maximum and minimum values, respectively, for each numeric variable in S. The computation of these bounds on variables can be achieved using a small extension of the LP described in Section 8.1. For a state S, reached by plan P (where an is the last step in P ), we add another vector of variables to the LP, denoted vnow, and another time-stamp variable, stepnow. The variables in vnow represent the values of each state variable at some point (at time stepnow) along the state trajectory following an. The numeric variables and time-stamp for now are constrained as if it were an additional action appended to the plan:\n\u2022 now must follow the previous step, i.e. stepnow \u2212 stepn \u2265\n\u2022 now must precede or coincide with the ends of any actions that have started but not yet finished, i.e. for each estep(i), estep(i) \u2265 stepnow\n\u2022 For each variable vnow \u2208 vnow, we compute its value based on any continuous numeric change:\nvnow = v \u2032 n + \u03b4vnow(stepnow \u2212 stepn)\n\u2022 Finally, for every invariant condition \u3008v, {\u2265,=,\u2264},w \u00b7 v + c\u3009 of each action that has started but not yet finished:\nvnow{\u2265,=,\u2264}w \u00b7 vnow + c\nThe LP can then be used to find the upper and lower bounds on variables. For each of the variables vnow \u2208 vnow, two calls are made to the LP solver: one with objective set to to maximise vnow, and one to minimise vnow. These are then taken as the values of vmax and vmin in S. In the simplest case, where a variable v is not subject to (direct or indirect) continuous or duration-dependent change, the value of v is time-independent, so vmax = vmin, and its value can be determined through the successive application of the effects of the actions in P , i.e. the mechanism used in CRIKEY3, or indeed classical (non-temporal) planning.\nSince we have upper and lower bounds on the value of each variable, rather than a fixed assignment, the action applicability function, get applicable actions(S), must be modified. In CRIKEY3, an action is said to be applicable in a state S if its preconditions are satisfied. In COLIN, the definition of what it means for a numeric precondition to be satisfied is different. To preserve completeness, we employ the mechanism used in metric relaxed planning graphs, as discussed in more detail in Section B. Specifically, for a numeric precondition w \u00b7 x \u2265 c, we calculate an optimistic value for w \u00b7x by using the upper bound on a v \u2208 x if its corresponding weight in w is positive, or, otherwise, using its lower bound. Then, if this resulting value is greater than or equal to c, the precondition is considered to be satisfied. (As before, for numeric conditions w \u00b7 x \u2264 c, an equivalent precondition in the appropriate form can be obtained by multiplying both sides of the inequality by \u22121 and\nconstraints of the form w \u00b7 x = c are replaced with the equivalent pair of conditions w \u00b7 x \u2265 c, \u2212w \u00b7 x \u2265 \u2212c.)\nThis test for applicability of an action is relaxed, so it serves only as a filter, eliminating actions that are certainly inapplicable. For instance, a precondition a+ b \u2265 3 could be satisfied if the upper bounds on a and b are both 2, even if the assignment of timestamps to actions within the LP to attain a = 2 conflicts with that needed to attain b \u2265 1. We rely on the subsequent LP consistency check to determine whether actions are truly applicable. Nonetheless, filtering applicable actions on the basis of the variable bounds in a state is a useful tool for reducing the number of candidates that must be individually verified by the LP.\n8.3.1 EXAMPLE OF USE OF now IN THE BORROWER PROBLEM\nWe briefly illustrate the way in which the now variable is constructed and used in the context of the Borrower problem. Consider the situation after the selection of the first two actions (saveHard start and takeMortgage start). The LP construction yields the constraints shown in Table 3. Solving this LP for minimum and maximum values of stepnow gives values of 1 + and 10 respectively, meaning that the earliest time at which the third action can be applied will be 1 + and the latest will be 10.5 Similarly, solving the LP for minimum and maximum values of mnow gives bounds of 4 and 6. This information could, in principle, constrain what actions can be applied in the current state."}, {"heading": "8.4 Some Comments on LP Efficiency", "text": "An LP is solved at every node in the search space, so it is important that this process is made as efficient as possible. When adding the variable vectors to the LP for each step i, it is only necessary to consider a state variable, v, if it has become unstable prior to step i, because of one of the following effects acting on it:\n1. direct continuous numeric change, i.e. changing v according to some gradient;\n5. In practice, for efficiency, COLIN does not actually solve the LP for minimum and maximum values of stepnow, but uses the variable only to communicate constraints to the metric variables in this state.\n2. direct duration-dependent change, i.e. a change on v dependent on the duration of an action (whose duration is non-fixed);\n3. discrete change, where the magnitude of the change was based on one or more variables falling into either of the previous two categories.\nAll variables that do not meet one of these conditions can be omitted from the LP, as their values can be calculated based on the successive effects of the actions applied up to step i, and substituted as a constant within any LP constraints referring to them. This reduces the number of state variables and constraints that must be added to the LP and also reduces the number of times the LP must be solved at each state to find variable bounds: irrelevant variables can be eliminated from the vector vnow. A similar simplification is that, if applying a plan a0...an\u22121 reaches a state S where vmin = vmax, then if there is no continuous numeric change acting on v, v has become stable, i.e. its value is independent of the times assigned to the preceding plan steps. In this case, until the first step k at which v becomes unstable, the value of v can be determined through simple application of discrete effects, and hence v can be omitted from all vj ,v\u2032j , n\u2212 1 < j.\nA further opportunity we exploit is that the LP solved in each state is similar to that being solved in its parent state: it represents the same plan, but with an extra snap-action appended to the end. The lower bounds of the time-stamp variables in the LP can therefore be based on the values computed in the parent states. Suppose a state S is expanded to reach a state S\u2032 by applying a snap action, a, as step i of the plan. At this point, the LP corresponding to the plan will be built and solved with the objective being to minimise stepi. Assuming the plan can indeed be scheduled (if it cannot, then S\u2032 is pruned and no successors will be generated from it), the value of the objective function is stored in S\u2032 as a lower bound on the time-stamp of a. In all states subsequently reached from S\u2032, this stored value can be used in the LP as a lower bound on stepi \u2014 appending actions to the plan can further constrain and hence increase the value of stepi, but it can never remove constraints in order to allow it to decrease.\nAs well as storing lower bounds for time-stamp variables, we can make use of the bounds vmin,vmax in the state S\u2032 when generating successors from it. In a state S reached via plan of length i, applying an action a leads to a state S\u2032 in which the new action at stepi+1 inherits the constraints imposed previously on stepnow when calculating the variable bounds in S\n\u2032. Therefore, the values of vmax and vmin in S serve as upper and lower bounds (respectively) for vi+1 in the LP built to determine the feasibility of S\u2032. Similarly, we can combine any discrete numeric effects of a with the values of vmax and vmin in S to give bounds on v\u2032i+1. For each variable v subject to an effect, an optimistically large (small) outcome for that effect can be computed on the basis of vmax and vmin, and taken as the upper (lower) bound of v\u2032i+1. Otherwise, for variables upon which a has no discrete effect, v\u2032i+1 = vi.\nFinally, the presence of timed initial literals (TILs) allows us to impose stricter bounds on the time-stamp variables. If step j of a plan is the dummy action corresponding to a TIL at time t, the upper bound on stepi, i < j, is t \u2212 and the lower bound on each stepk, j < k (or any estep variable) is t + . Similarly, if the plan does not yet contain a step corresponding to a TIL at time t, the upper bound on all step variables is t \u2212 . Furthermore, a TIL at time t corresponds to a deadline if it deletes some fact p that is present in the initial state, never added by any action, and never reinstated by any other TIL. In this case:\n\u2022 if a plan step i requires p as a precondition, then stepi \u2264 t\u2212 ;\n\u2022 if estepi is the end of an action with an end condition p, then estepi \u2264 t\u2212 ;\n\u2022 if estepi is the end of an action with an invariant condition p, then estepi \u2264 t."}, {"heading": "9. Heuristic Computation", "text": "The search algorithms described so far in this paper all make use of a heuristic to guide the planner efficiently through the search space towards the goal. Having introduced the necessary machinery to support linear continuous numeric and duration-dependent effects we now turn our attention to the construction of an informed heuristic in the face of time-dependent change.\nIn Appendices B and C we revisit the standard Metric-FF Relaxed-Planning Graph (RPG) heuristic and the Temporal RPG (TRPG) used in CRIKEY3, and provide the details of these approaches for reference. Both of these depend on the initial construction of a reachability graph, based on the plan graph introduced in Graphplan (Blum & Furst, 1995). The graph consists of alternating layers of facts (fl) and actions (al). In the TRPG, for convenience, we index these layers by the earliest time they could represent, although they can still be enumerated by consecutive integers because only finitely many times can be relevant in the process of construction. In this section we explain how the heuristic computation techniques introduced by these planners can then be modified to reason with interacting temporal\u2013numeric behaviour. We describe two variants of the heuristic: a basic version, in which active continuous change is relaxed to discrete step changes, and a refined variant in which this relaxation is replaced with a more careful approximation of the continuous values. We show, using the Borrower example, the benefits of the refined approach.\nThe heuristics are based on the underlying use of a relaxed plan step-count. We use the relaxed plan makespan as a tie-breaker in ordering plans with the same step-count. Step-count dominates our heuristic because our first priority is to find a feasible solution to a planning problem and this means attempting to minimise the number of choices that must be made and resolved during the search. Of course, the emphasis on rapidly finding a feasible plan can compromise the quality of the plan, particularly in problems where the step-count is poorly correlated with the makespan. Subsequent attempts to improve the quality of an initial feasible solution, either by iteratively improving the solution itself or by further search using the bound derived from the feasible solution to prune the search space, are possible, but we do not consider them in this work."}, {"heading": "9.1 The Basic \u2018Integrated\u2019 Heuristic Computation with Continuous Numeric Effects", "text": "The first version of COLIN (Coles, Coles, Fox, & Long, 2009b) introduced three significant modifications to the TRPG used in CRIKEY3, in order to generate heuristic values in the presence of continuous and duration-dependent effects. The first modification simply equips the heuristic with the means to approximate the effects of continuous change.\n\u2022 If an action a has a continuous effect equivalent to dvdt += k it is relaxed to an instantaneous start effect \u3008v,+=, k \u2217 dmax (a)\u3009. That is, the effect on the changing variable is treated as the integral of the effect up to an upper bound on the duration of the action and is applied at the start of the action. Doing this ensures that the behaviour is relaxed, in contrast to, say, applying the effect at the end of the action. dmax (a) is calculated at the point where the action is added to the TRPG, based on the maximum duration constraints of a that refer only to variables that cannot change after that time (that is, they are state-independent). If no such\nconstraints exist, the duration is allowed to be infinite (and variables affected by continuous effects of the action will then have similarly uninformed bounds).\n\u2022 If an action a has a discrete duration-dependent effect on a variable v then, when calculating the maximum (minimum) effect of a upon v (as discussed, in the non-temporal case, in Appendix B), the ?duration variable is relaxed to whichever of dmin(a) or dmax (a) gives the largest (smallest) effect. Relaxation of this effect is achieved without changing its timing, so it is associated with the start or end of the action as indicated in the action specification.\nThe second modification affects any action that has a continuous numeric effect on some variable and either an end precondition or invariant that refers to the same numeric variable. If the invariant or end precondition places a constraint on the way in which the process governed by the action can affect the value of a variable, then this constraint is reflected in the corresponding upper or lower bounds of the value of the variable. Specifically, if an action a decreases v at rate k and has an invariant or end precondition v \u2265 c, then the upper bound on v by the end of the action must be at least k.(dmin(a)\u2212elapsed(a))+ c, where elapsed(a) is the maximum amount of time for which a could have been executing in the state being evaluated (0 if a is not currently executing, otherwise, the maximum from all such entries in E). This condition ensures that the variable could achieve the necessary value to support the application of the action. It might appear strange that the bound is set to be higher than c, but the reason is that the relaxation accumulates increase effects and ignores decrease effects in assessing the upper bound, so it will be necessary, by the end of the action, to have accumulated increases in the value of the variable that allow for the outstanding consumption from a in order to still meet the c bound at the end of the action. A corresponding condition is required for an action that a increases v at rate k, and has an invariant or end precondition v \u2264 c, where the lower bound on v cannot be more than k.(dmin(a)\u2212 elapsed(a)) + c. These conditions are added as explicit additional preconditions to aa for the purposes of constructing the TRPG.\nThe third modification deals with the problem of constructing an appropriate initialisation of the bounds for the numeric variables in the first layer of the TRPG. In CRIKEY3 these values are initialised to the actual values of the metric variables, since their values in the current state do not change if time passes without further actions being applied. The same is not true in COLIN, since any actions that have started, but not yet finished, and which govern a process, will cause variables to change simply as a consequence of time passing. As the basic heuristic proposed here relies on being able to integrate continuous numeric change, we determine the variable bounds in fl(0.0) in two stages. First, the bounds on a variable v are set according to those obtained from the LP in Section 8.3. Then, for each entry e \u2208 E, corresponding to the start of an action, a, with a continuous effect on v having positive gradient k, the upper bound on v in fl(0.0) is increased by k.remaining(e). Here, remaining(e) is the maximum amount of time that could elapse between the state being evaluated and the future end snap-action paired with start event e. The maximum remaining execution time is calculated by subtracting the lower bound for the amount of time that has to have elapsed since the start of action a from its maximum duration. In the case where the gradient is negative, the lower bound is decreased."}, {"heading": "9.2 The Refined Integrated Heuristic", "text": "Time-dependent change arises from two sources: continuous numeric effects, initiated by start snapactions, and discrete duration-dependent effects which can apply at either end of durative actions.\nFor the purposes of the refined heuristic described in this section, we treat continuous effects and discrete duration-dependent effects at the ends of actions of these in the same way, attaching a continuous linear effect acting on each relevant variable to the effects of the appropriate snap-action, a, denoting the set of all such continuous effects by g(a). For continuous effects, cont(a), initiated by a`, cont(a) \u2286 g(a`). That is, the gradient effects of the start of a include all of the continuous effects of a. For duration-dependent effects of an end snap-action aa we split the effect into two parts:\n\u2022 a discrete effect of aa, \u3008v, {+=, -=,=},w \u00b7 v + k.dmin(a) + c\u3009 and\n\u2022 a gradient effect on v, added to g(aa). The effect is defined as \u3008v, k\u3009 if the original effect used the operator += or = otherwise, it is \u3008v,\u2212k\u3009.\nThus, instantaneously, at the end of aa, the effect of a is available assuming the smallest possible duration for a is used. As a executes with a greater duration, a continuous effect is applied with the gradient of the change being taken from the coefficient k of the ?duration variable in the corresponding effect in a.\nUnfortunately, the treatment proposed above cannot be applied to duration-dependent start effects, since the effects are always available at the start of the action, regardless of the duration. Thus, we employ the approach taken with the basic heuristic used in COLIN: when calculating the maximum (minimum) effect of a` on the affected variable, v, the ?duration variable is substituted with whichever of dmin(a) or dmax (a) gives the largest (smallest) effect.\nOnce we have a collection of linear continuous effects, g(a), associated with each snap-action, a, we can adjust the construction of the TRPG. First, we identify, for each variable, v, an associated maximum rate of change, \u03b4vmax(t), following the layer al(t). We set this to be the sum of all the positive rates of change, affecting v, of any snap-actions in al(t):\n\u03b4vmax(t) = \u2211\na\u2208al(t) \u2211 \u3008v,k\u3009\u2208g(a) k\nThis definition relies on the restriction that only one instance of any action can execute at any time. If this restriction does not hold, but there is a clear finite bound p(a) on the number of instances of an action that can execute concurrently, then we incorporate this into the calculation of \u03b4vmax(t) as follows:\n\u03b4vmax(t) = \u2211\na\u2208al(t)\np(a)\u00d7 \u2211\n\u3008v,k\u3009\u2208g(a)\nk\nWhere no such finite bound exists, an action could, in principle, be applied arbitrarily many times in parallel and hence we set \u03b4vmax(t) =\u221e.6 Following any layer al(t) at which \u03b4vmax(t) =\u221ewe no longer need to reason about the upper bound of the continuous change on v since the upper bound on v itself will become \u221e immediately after this layer. It should be noted that this degradation of behaviour will, in the worst case, lead to the same heuristic behaviour as the basic heuristic where, again, if arbitrarily many copies of the same action can execute concurrently, the magnitude of its increase or decrease effects becomes unbounded. The extension of the heuristic to consider\n6. We note that, in our experience, the presence of infinitely self-overlapping actions with continuous numeric change is often a bug in the domain encoding: it is difficult to envisage a real situation in which parallel production is unbounded.\ncontinuous effects in a more refined way does not worsen its guidance in this situation. For the remainder of this section, we consider only variables whose values are modified by actions for which there are finite bounds on the number of concurrently executing copies allowed.\nArmed with an upper bound value for the rate of change of each variable following layer al(t), we can deduce the maximum value of each variable at any time t\u2032 > t, by simply applying the appropriate change to the maximum value of the variable at time t. The remaining challenge is to decide how far to advance t\u2032 in the construction of the TRPG. During construction of the TRPG in CRIKEY3 time is constrained to advance by or until the next action end point, depending on whether any new facts are available following the most recent action layer (lines 29\u201334 of Algorithm 2). In order to manage the effects of the active continuous processes, we add a third possibility: time can advance to the earliest value at which the accumulated effect of active continuous change on a variable can satisfy a previously unsatisfied precondition. The set of preconditions of interest will always be finite, so, assuming that the variable is subject to a non-zero effect, the bound on the relevant advance is always defined (or, if the set of preconditions is empty, no advance is required). We can compute the value of this time as follows. Each numeric precondition may be written as a constraint on the vector of numeric variables, v, in the form w \u00b7 v \u2265 c, for vectors of constants w and c. We define the function ub as follows:\nub(w,x,y) = \u2211\nw[i]\u2208w\n{ w[i]\u00d7 y[i] if w[i] \u2265 0 w[i]\u00d7 x[i] otherwise\nThe upper bound on w \u00b7 v at t\u2032 is then: ub(w,vmin(t\u2032),vmax(t\u2032)). The earliest point at which the numeric precondition w \u00b7 v \u2265 c will become satisfied is then the\nsmallest value of t\u2032 for which ub(w,vmin(t\u2032),vmax(t\u2032)) \u2265 c. As an example, suppose there is an action with a precondition x + 2y \u2212 z \u2265 c, so that w = \u30081, 2,\u22121\u3009 (assuming x, y and z are the only numeric fluents in this case). Substituting this into the previous equation yields:\nub(\u30081, 2,\u22121\u3009, \u3008x, y, z\u3009min(t\u2032), \u3008x, y, z\u3009max(t\u2032)) = 1.xmax(t\u2032) + 2.ymax(t\u2032)\u2212 1.zmin(t\u2032) = 1.(\u03b4xmax(t)\u00d7 (t\u2032 \u2212 t\u2212 ) + xmax(t+ ))\n+2.(\u03b4ymax(t)\u00d7 (t\u2032 \u2212 t\u2212 ) + ymax(t+ )) \u22121.(\u03b4zmin(t)\u00d7 (t\u2032 \u2212 t\u2212 ) + zmin(t+ ))\n(The values of x, y and z are based on their starting points at t + because this accounts for any instantaneous changes triggered by actions in al(t).) If the value of t\u2032 produced by this computation is infinite, then the maximum possible rate of increase of the expression x+ 2y \u2212 z must be zero.7 Otherwise, t\u2032 is the time at which a new numeric precondition will first become satisfied due to active continuous effects and, if this is earlier than the earliest point at which an action end point can be applied, then the next fact layer in the TRGP will be fl(t\u2032)."}, {"heading": "9.2.1 IMPROVING THE BOUNDS ON VARIABLES IN FACT-LAYER ZERO", "text": "Previously, setting the bounds in fact-layer zero could be thought of as consisting of two stages: finding initial bounds using the LP and then, because the passage of time could cause these bounds to further diverge due to active continuous numeric change, integrating this change prior to setting\n7. To find t\u2032 requires only a simple rearrangement of the formula to extract t\u2032 directly.\nbounds for layer zero of the TRPG. With an explicit model of numeric gradients in the planning graph, we can now reconsider this approach. The intuition behind our new approach here is as follows:\n1. For each variable v, create an associated variable tnow(v) in the LP, and solve the LP to minimise the value of this variable.\n2. Fixing the value of tnow(v) to this lower-bound, maximise and minimise the value of v to find the bounds on it at this point \u2014 these are then used as the bounds on v in fl(0.0).\n3. If \u03b4v > 0 in the current state, then all \u03b4vmax (t) values in the TRPG are offset by \u03b4v or, similarly, if \u03b4v < 0, all \u03b4vmin(t) values are offset.\nThe first of these steps is based on the ideas described in Section 8.3, but the process is subtly different because we are trying to determine the bounds on v at a given point in time, rather than those that appear to be reachable. As before, tnow(v) must still come after the most recent plan step and is used to determine the value of v. This is reflected by the pair of constraints:\ntnow(v)\u2212 stepi \u2265\nvnow = v \u2032 i + \u03b4vnow(tnow(v)\u2212 stepi)\nAdditionally, since the \u2018now\u2019 variable is associated with only a single v, rather than having to be appropriate for all v, we can further constrain it if, necessarily, v cannot be referred to (either in a precondition, duration or within an effect) until at least after certain steps in the plan, rather than the weaker requirement of just after the most recent step. For our purposes, we observe that if all actions referring to v require, delete and then add a fact p, and all possible interaction with p is of this require-delete-add form, then tnow(v) must come after any plan step that adds p. More formally, the require-delete-add idiom holds for p if p is true in the initial state, and for each action a with preconditions/effects on p, the interaction between the action and p can be characterised as one of the following patterns:\n1. p \u2208 pre`(a), p \u2208 eff \u2212` (a), p \u2208 eff + ` (a)\n2. p \u2208 prea(a), p \u2208 eff \u2212a (a), p \u2208 eff + a (a)\n3. p \u2208 pre`(a), p \u2208 eff \u2212` (a), p \u2208 eff + a (a)\n(An action may exhibit either or both of the first two interactions, or just the third.) The LP variable corresponding to the point at which p is added, which we denote stepp, is determined in one of two ways. First, if p is present in the state being evaluated, stepp is the LP variable corresponding to the plan step that most recently added p. Otherwise, from case 4 above, we know that p \u2208 eff +a (a) for some action a that is currently executing. In this case, stepp is the LP variable estepi corresponding to the end of a. With this defined variable, we can add the constraint to the LP:\ntnow(v) \u2265 stepp +\nSolving the LP with the objective being to minimise tnow(v) finds the earliest possible time at which v can be referred to. Then, fixing tnow(v) to this minimised value, we minimise and maximise\nthe bounds on vnow . This gives us bounds on v that are appropriate as early as possible after the actions in the plan so far.\nHaving obtained variable bounds from the LP we must, as before, account for the fact that the passage of time causes the bounds to change if there is active continuous numeric change. Whereas before we integrated this change prior to the TRPG, we now have a mechanism for handling gradients directly during TRPG expansion. Thus, for each start-event-queue entry e \u2208 E corresponding to the start of an action, A, with a continuous effect on v with a positive (negative) gradient k, we add a gradient effect on the upper (lower) bound on v to the TRPG. Just as we previously restricted the integrated effect of e by remaining(e), the maximum remaining time until the action must end, so here we limit how long the gradient effect is active: it starts at al(0.0) and finishes at al(remaining(e)). Then, for a given fact layer t the value of \u03b4vmax(t) is updated accordingly:\n\u03b4vmax (t)+= \u2211 e\u2208E \u2211 {k | \u3008v, k\u3009 \u2208 g(op(e)) \u2227 k > 0 \u2227 t \u2264 remaining(e)}\nSimilarly, \u03b4vmin(t) is amended to account for effects \u3008v, k\u3009, k < 0."}, {"heading": "9.3 Using the Two Variants of the Integrated Heuristic in the Borrower Problem", "text": "We now illustrate the computation of the two heuristic functions for a choice point in the Borrower problem. This example shows that the refined heuristic guides the planner to a shorter makespan plan than the basic heuristic, because the improved heuristic information leads to the selection of better choices of helpful actions. Consider the situation following execution of the first action, saveHard start. Figure 7 (top) shows the TRPG and relaxed plan constructed using the basic heuristic.\nThe heuristic generates a cost for this state of 5: the four actions shown in the relaxed plan, together with an extra one to end the saveHard action that has already started. This relaxed plan generates two helpful actions, to start the lifeAudit and to start takeMortgage short. An attempt to start the lifeAudit action can quickly be dismissed as temporally inconsistent, depending as it does on boughtHouse becoming true before it ends, so the other helpful action is chosen. Unfortunately, once this action is selected the interaction between the saving process and the deposit requirement (at least five savings must have been acquired) forces the action to start no earlier than time 5. This constraint is invisible in the TRPG, because the continuous effect of saveHard has been abstracted to a start effect, and a full ten savings therefore appear to be available immediately. A plan can be constructed using the short mortgage, but only by introducing a second saving action as shown in the lower plan in Figure 3. This is because the start of the short mortgage is pushed so late that the life audit cannot both overlap the end of the first saveHard action and finish after the mortgage action.\nThe lower part of Figure 7 shows what happens when the refined heuristic is used to solve this problem. The saveHard action starts as before, but this time the heuristic does not relax the behaviour of the continuous savings process so the long mortgage, which requires a smaller deposit to initiate it, becomes available before the short mortgage. As a consequence of this, the relaxed plan selects the long mortgage, and this action starts early enough that the life audit can overlap both its end and the end of the saveHard action. The planner is correctly guided to the optimal plan, as shown at the top of Figure 3. The crucial difference between the two heuristics, is that the refined heuristic is able to access more accurate information about the value of the savings at timepoints\nafter the start of the savehard action. This leads to a finer-grained structure of the TRPG, which can be seen in the fact that there are six action layers before arrival at the goal, rather than four as in the case when the basic heuristic is used. The estimated makespan of the final plan is 12 + , while the makespan according to the basic heuristic is 10 + 2 . The basic heuristic leads to a non-optimal solution because it requires the extra saveHard action, giving a solution makespan of 20 + 2 , in contrast to the makespan of 12 + of the optimal plan.\nThe benefit of the refined heuristic, and the extra work involved in constructing the modified TRPG, is that better helpful actions are chosen and the makespan estimate is therefore more accurate. The choice between similar length plans is made based on makespan. The TRPG, constructed by the refined heuristic in the Borrower problem, does not even contain the short mortgage action at an early enough layer for it to be considered by the relaxed plan."}, {"heading": "10. Improving Performance", "text": "In this section we present two techniques we use to improve the performance of COLIN. The first technique, described in Section 10.1, is a generalisation of our earlier exploitation of one-shot actions (Coles et al., 2009a) to the situation in which they encapsulate continuous processes, leading to faster plan construction in problems with these action types. The second technique, described in Section 10.2, exploits the LP that defines the constraints within the final plan to optimise the plan metric. This leads to better quality plans in many cases."}, {"heading": "10.1 Reasoning with One-Shot Actions", "text": "In earlier work (Coles et al., 2009a) we have observed that there is a common modelling device in planning domains that leads to use of actions that can only be applied once. We call these actions one-shot actions. They arise, in particular, when there is a collection of resources that can each be used only once. The key difference that one-shot actions imply for the TRPG is that continuous effects generated by one-shot actions lapse once a certain point has been reached:\n\u2022 If a one-shot action a has a continuous numeric effect on v, and a` first appears in action layer al(t), then the gradient on v due to this effect of a finishes, at the latest, at al(t+ dmax (a)).\n\u2022 If the end aa of a one-shot action has a duration-dependent effect on v, then the (implicit) continuous effect acting on v finishes, at the latest, at layer al(t+ dmax (a))\nThe termination point is implied, in both cases, by the fact that the action is one-shot. We modify the TRPG construction to reflect these restrictions by extending the data recorded in each action layer to include, for each snap-action action a, the maximum remaining execution time of a, denoted rem(t, a). For one-shot actions, in the layer al(t) in which a` first appears, rem(t, a`) = dmax (a), and when aa first appears, rem(t, aa) = dmax (a)\u2212dmin(a). For actions that are not one-shot rem(t, a`) and rem(t, aa) are both initialised to \u221e. We make three minor changes to the layer update rules to accommodate the rem values. First, when calculating the active gradient on a variable v following action layer al(t):\n\u03b4vmax(t) = \u2211\na\u2208al(t)|rem(a,t)>0\np(a)\u00d7 \u2211\n\u3008v,k\u3009\u2208g(a)\nk\nAs can be seen, only the subset of actions with execution time remaining is considered. Second, at the next action layer al(t + \u2206t) following al(t), the value of each positive rem is decremented by \u2206t, the amount of time elapsed since the previous layer. Third, as a consequence of this, an additional criterion must be considered when calculating the time-stamp of the next fact-layer, t\u2032, described in Section 9.2. Since the time remaining to complete an action may expire, we may need to insert an additional fact layer to denote the point at which a rem value reaches 0 and the continuous effects acting on one or more variables need to be recalculated. The time-stamp of the earliest such layer is:\nt\u2032 = t+ min{rem(t, a) > 0 | a \u2208 al(t)}\nOne-shot actions can be exploited still further by improving the upper bound on the duration of the action a. In the case of actions with state-dependent duration constraints (i.e. where the upperbound is calculated based on variables that can be subjected to the effects of actions), dmax (a) may\nbe a gross over-estimate of the duration of a. Suppose the maximum duration of a is bounded by a formula w \u00b7 v + c. In the layer al(t) in which a` appears, we can compute the maximum duration of a, were it to be started in that layer, based on the variable bounds recorded in fl(t). We could use this value to determine a bound on the remaining execution time for a. However, at some future layer fl(t\u2032), the variable bounds might have changed, so that beginning a in al(t\u2032), and calculating its maximum duration based on fl(t\u2032), would have allowed a to execute for a possibly longer period of time, allowing its continuous effects to persist for longer.\nTo remain faithful to the relaxation, the possibility of exploiting this increased duration of a (by starting a at t\u2032) must be included in the TRPG, as well as allowing the possibility of a to start at t, thereby obtaining its effects sooner. Therefore, each one-shot action is allowed to start in the earliest layer al(t) in which its preconditions are satisfied, giving it an initial maximum duration of dmax (a, t) based on the fact later fl(t). But, if a later fact layer fl(t\u2032) admits a greater duration (dmax (a, t\u2032), the value of dmax for action a at layer t\u2032), the remaining execution time for a is reconsidered. First, in the simple case, the variables in the duration constraint are changed in fl(t\u2032), but not subject to any active continuous effects. In this case, we apply a pair of dummy effects to fact layer t\u2032\u2032 = t\u2032 + dmax (a, t):\n\u3008rem(a`, t\u2032\u2032) += (dmax (a, t\u2032)\u2212 dmax (a, t))\u3009\nand \u3008rem(aa, t\u2032\u2032) += (dmax (a, t\u2032)\u2212 dmax (a, t))\u3009.\nNote that the increase of the rem values is delayed until layer t\u2032\u2032 because, in order to benefit from the longer duration of a, a must have started in layer t\u2032.\nIn the more complex case, the variables in the duration constraint are changed in fl(t\u2032) but the duration is also affected by continuous effects on some of the variables it depends on. In this situation, each subsequent fact layer might admit a marginally bigger duration for a than the last. To avoid having to recalculate the new duration for a repeatedly, we schedule a pair of dummy effects based on the global, layer-independent, maximum value for the duration of a:\n\u3008rem(a`, t\u2032\u2032) += (dmax (a)\u2212 dmax (a, t))\u3009\nand \u3008rem(aa, t\u2032\u2032) += (dmax (a)\u2212 dmax (a, t))\u3009.\nThis relaxation is weaker than it might be, but is efficient to compute."}, {"heading": "10.2 Plan Optimisation", "text": "A plan metric can be specified in PDDL2.1 problem files to indicate the measure of quality to use in evaluating plans. The metric is expressed in terms of the task numeric variables and the total execution time of the plan (by referring to the variable total-time). The use of an LP in COLIN offers an opportunity for the optimisation of a plan with respect to such a metric: for a plan consisting of n steps, the numeric variables v\u2032n\u22121 are those at the end of the plan, the stepn\u22121 is the time-stamp of the final step (i.e. the action dictating the makespan of the plan) and the LP objective can be set to minimise a function over these. The LP must be solved to minimise the time-stamp of the last action (the makespan of the plan) in order to arrive at a lower bound on the time for the next action. However, it can also be solved to optimise the plan metric.\nAlthough it is possible to consider ways to use the metric-optimising LP value during plan construction, to guide the search, we have focussed on a much more limited, but less costly, use: we only attempt post hoc optimisation, attempting to exploit any flexibility in the temporal structure of the final plan to optimise the plan quality at the last stage of plan construction.\nIn order for such post hoc optimisation to be useful, planning problems must have the property that it is possible to vary the quality metric of a plan by scheduling the same actions to occur at different times. This is possible in a wide range of interesting situations, such as scheduling aircraft to land as close to a given target time as possible, taking images from satellites at certain times of day when the view is clearer, or minimising wasted fuel by penalising the time elapsing between starting the engine of a plane and its take off. The last of these represents a general class of problems in which it may be desirable to minimise the amount of time between two activities: a different metric to the total time taken for plan execution. To capture these interesting cases, we first extend the language supported by COLIN to allow a limited subset of ADL conditional effects, to allow the conditions under which an action is executed to vary the effects the action has on the metric value of the plan. Second, we discuss how a MILP can be built, based on the LP described in Section 8.1, to support post hoc plan optimisation.\nOur planner handles most conditional effects by a standard compilation. However, conditional effects on metric variables that appear in the plan quality metric and not in the preconditions of any actions are dealt with differently. We call such variables metric tracking variables and we exploit the fact that rescheduling a plan can affect the values of these variables without changing the validity of the plan. An example is shown in Figure 10.2 of an action to land an airplane, with conditional effects on the metric tracking variable total-cost. The domain is structured so that all land actions must start at the beginning of the plan, and their end points represent the actual landing times of the aircraft in the problem. The duration of the actions are set to correspond to the earliest and latest possible points at which each plane could land. As can be seen, the propositional effects of the action are the same whether the plane lands early or late: the plane has landed, and is no longer flying. However, the numeric effects, which are all effects on the metric tracking variable total-cost, depend on the duration of the action and, in particular, whether the plane has landed early or late. If the plane lands early, a penalty is paid at a certain rate per unit time that the plane lands before the desired target value. If the plane lands late, then a fixed cost is paid, in addition to a penalty (at a different rate) per unit of time the plane lands after the desired target value. By considering this action as a single action, with a pair of conditional effects, the planner can decide upon the actions needed to construct a sound plan (in which all the planes have landed) whilst leaving to the subsequent optimisation phase the decision about whether a plane should be landed early, late, or on time.\nIn general, it is straightforward to exploit the LP described in Section 8.1 to attempt to reschedule the actions in a plan to optimise the value of the plan metric (provided that the metric function is linear). However, if the plan contains actions with conditional effects on metric tracking variables, it becomes possible to exploit the representation of these effects in an extended LP, using integer variables, in order to offer a more powerful optimisation step. Each conditional effect will either be activated or not: we introduce a 0-1 variable to represent which of these is the case for each effect. The variable is connected to corresponding constraints that determine whether or not the condition associated with the effect is true or not.\nWe deal with two kinds of constraints on the 0-1 variables in our MILP encoding of the plan optimisation problem: one is the special case where actions are scheduled against fixed time-windows\ngoverned by timed initial literals that affect whether the conditions are satisfied or not and the other is the case where the satisfaction of the conditions is determined by the status of continuous effects controlled by the actions in the plan (so, for example, the cost of an action might depend on whether a continuously changing value has passed some threshold or not at the time the action is executed). Both of these cases can be handled by a straightforward encoding of the linkage between the value of the 0-1 condition variable and the corresponding conditions (the details are given in Appendix D). We have also extended the conditional effects to allow them to affect the ?duration variable in an action, with similar devices for encoding this in the MILP.\nThe MILP can then be solved as a single and final step in the construction of the plan, optimising the plan metric quality by rescheduling the actions to best exploit the precise timing of the actions and their interaction, through these limited conditional effects, on the plan quality."}, {"heading": "11. Continuous Linear Benchmark Domains", "text": "As COLIN is one of the first planners to support PDDL2.1 models featuring continuous linear change and duration-dependent effects8 there are currently no benchmarks available that exploit these fea-\n8. Specifically, duration-dependent effects that depend on non-fixed durations.\ntures. To support our evaluation and to foster future comparisons between planners designed to solve these problems, we have produced a number of domains with these features9.\nThe first of our domains is an extension of the \u2018Metric Time\u2019 variant of the Rovers domain, from the 2002 International Planning Competition (IPC 2002) (Long & Fox, 2003b). Our focus here is on the action navigate, responsible for moving a rover from one location to another. In the original model, it has a discrete effect, at the start of the action, to decrease the energy level of the rover by 8 units, coupled with a precondition that there must be at least 8 units of energy available. We replace this with a continuous numeric effect on energy, and an over all condition that energy must be at least zero during the action. As the duration of the original action was specified as 5, we use an effect with gradient \u22128/5. Written thus, the action has the same net effect and conditions: energy is decreased by 8 units, and must not become negative. This continuous change models more accurately the use of power during the navigate action: whilst power use may not actually be linear, it is closer to linear than it is to being instantaneous. To make the model still more realistic, we introduce a new action into the domain: journey-recharge, shown in Figure 9. By exploiting interaction between continuous numeric effects on the same variable, we use this action to capture the option of the rover tilting its solar panels to face the sun whilst navigating between two points. To account for the power use in reorienting the solar panels, at the start and end of the action, 0.2 units of energy are used. The benefit for this consumption is that, whilst the action is executing, the energy of the rover is increased according to a constant positive gradient. For our final modification to the domain, we alter the duration constraint on the existing recharge action. In the original encoding, the constraint is:\n(= ?duration (/ (- 80 (energy ?x)) (recharge-rate ?x))).\nThis forces the duration of the action to be sufficient to restore the level of charge to 80 (full capacity). In our new formulation, we replace the = with <= so that the duration constraint specifies the maximum duration for which the battery can be charged: it need not be restored to full capacity every time the action is applied. Following all three of these modifications, the domain can be used with the standard IPC 2002 benchmark problems. In addition to this, we have also created some problems considering just a single rover, where the issue of battery power management is of much greater importance.\nThe next of our domains is an extension of the \u2018Time\u2019 variant of the Satellite domain, again taken from IPC 2002. Here, in our continuous variant of the domain, we make three key changes to the domain model. First, in the original formulation, a proposition was used to indicate whether power was available to operate the instrumentation on a given satellite. Switching an instrument on required and then deleted this fact, and switching it off added it again. Thus, there was no scope for parallel power usage, and all instrumentation effectively used unit power. Now, we use a numeric variable to represent power, with preconditions and effects on this variable replacing the preconditions and effects on the proposition previously used. Second, exploiting the potential we now have for differing power requirements, instruments can be operated in one of two modes: cooled, or uncooled. In cooled mode, active sensor cooling is used to reduce sensor noise, enabling images to be taken in less time. This cooling, however, requires additional energy. Third, and finally, there is a compulsory \u2018sunrise\u2019 phase at the start of the plan, during which the satellites\n9. PDDL domain and problem descriptions for all evaluation tasks are available in the online appendix maintained by JAIR for this paper.\nmove from being shaded by the planet, to being in direct sunlight. This leads to an increase in power availability, modelled as a linear continuous numeric effect attached to an action, sunrise, that must be applied. Interaction between this effect and the preconditions on powering instruments ensures they can be operated no sooner than power is available. The problem files we use for this domain are slightly modified versions of the IPC competition problems, updated to define power availability as a numeric variable and to encode the power requirements of cooled and uncooled sensor operation. The problems in this domain have characteristics that are very similar to the Borrower problem we have used as a running example.\nFurther exploring the use of continuous numeric effects, our next domain models the operations of cooperating Autonomous Underwater Vehicles (AUVs). The AUVs move between waypoints underwater and can perform two sorts of science gathering operations. The first is taking a water sample from a given waypoint, which can be performed by any AUV in the appropriate location, and whose water sample chamber is empty. The second is taking an image of a target of interest. This requires two AUVs to cooperate: one to illuminate the target with a torch, and one to take an image of it. The AUV domain was inspired by the problem described by Maria Fox in her invited lecture at the 2009 International Conference on Automated Planning and Scheduling. Once data has been acquired, it must be communicated to a ship on the surface. As in the Satellite and Rovers domains, the AUVs are energy-constrained \u2014 they have finite battery power \u2014 and the power usage by actions is continuous throughout their execution. The more interesting continuous numeric aspects of the domain arise from the use of a model of drift. We introduce a variable to record how far each AUV has drifted from its nominal position, and update this in two ways. First, all activity in the plan is contained within an action drift with small, positive continuous numeric effect on the drifted distance. Second, we add a localise action that sets this drifted distance to zero, with its duration (and hence energy requirements) depending on the drifted distance prior to its application. This drifting then affects the other domain actions. In the simplest case, to sample water or take an image at a given location, an AUV cannot have drifted more than two metres, hence introducing the need to first localise if this is the case. More interestingly, for an AUV shining a torch, drifting affects how much light is falling on the target. Thus, the shine-torch action for an AUV ?v has three effects on the amount of light falling on a given target ?t:\n\u2022 start: (increase (light-level ?t) (- 1000 (distance-from-waypoint ?v)))\n\u2022 throughout: (decrease (light-level ?t) (* #t (fall-off)))\n\u2022 end: decrease (light-level ?t) by any remaining contribution ?v was making to its illumination.\nThe constant (fall-off) is pessimistically derived from formul\u00e6 involving the inverse-square law, giving a linear approximation of the decay in illumination levels due to drift. Then, for the take-image action itself, its duration is a function of (light-level ?t): the less light available, the longer it requires to take the image.\nThe final domain we use is the Airplane Landing domain (Dierks, 2005), first posed as a challenge by Kim Larsen in his invited lecture at the 2009 International Conference on Automated Planning and Scheduling. This problem models the scheduling of landing aircraft on an airport runway. For each plane, three landing times are specified: the earliest possible landing time, the latest possible landing time, and the target (desired) landing time. Since time must be allowed for airplanes to clear the runway once they have landed, and the use of the runway is a heavily subscribed resource, it is not possible for all planes to land at their ideal time. Planes can, therefore, land early or late, but doing so incurs a penalty. This penalty is modelled by a duration-dependent effect, as shown earlier in the paper (Figure 10.2 in Section 10). We have been able to construct a set of airplane landing problems using real data from the Edinburgh Airport arrivals board. Results from running COLIN on these problems are reported in Section 12."}, {"heading": "12. Evaluation", "text": "COLIN is a temporal planner, able to solve problems with required concurrency, that can handle both discrete and continuous metric variables. The first question we address is how costly is the extension of the underlying CRIKEY3 system to allow COLIN to manage continuous effects? COLIN is a particularly powerful planner and there are no other general PDDL2.1 planners with similar expressive power available for comparison on the continuous problems. However, the extensions necessary to support continuous reasoning will add an overhead to the cost of solving problems where there are no continuous effects. We compare the performance of COLIN with other temporal planners on a selection of temporal problems without continuous effects (Section 12.1) in order to evaluate how much overhead is paid by COLIN in setting up and managing (redundant) structures, in comparison with state-of-the-art planners that do not pay this price.\nWe then move on to considering the performance of COLIN on problems with continuous dynamics. Our second question is: how much improvement do we obtain from using the refined heuristic instead of the basic heuristic, when dealing with problems with continuous change? The planners discussed in Section 7 are not able to scale to large and complex problems, so we compare the two versions of COLIN. We present their performances on new benchmark problems with continuous processes, setting the foundation for future comparative evaluation of alternative approaches to these problems.\nThe third question considered concerns the quality of the solutions produced by COLIN, in comparison with optimal solutions where these can be found. COLIN is a satisficing planner that can perform efficiently on a wide range of continuous planning problems, and we are interested in understanding how much solution quality must be sacrificed in order to obtain the efficiency achieved by COLIN.\nFinally, we consider the question: just how expensive is the move from solving an STP (sufficient for purely discrete temporal planning) to solving an LP (necessary for handling continuous effects)? In particular, is it practical to solve multiple LPs in performing heuristic state evaluations? Since LP construction and solution is central to the architecture of COLIN it is important that this can be relied upon to scale appropriately with the range and complexity of problems that COLIN is expected to solve.\nThe following experiments consider a large number of domains and domain variants. For the temporal comparisons we use the Simple Time and Time variants of Depots, Driverlog, Rovers, Satellite and Zeno, all from IPC 2002, and Airport and Pipes-No-Tankage from IPC 2004. The Airport variant used here is the Strips Temporal variant.\nFor the comparisons between the basic and refined heuristics on continuous domains, we use the new continuous benchmark domains introduced in Section 11: Airplane Landing, Rovers, Satellite Cooled (the Satellite variant with sensor cooling) and the AUV domain.\nFor the post-hoc optimisation experiments we use the Airplane Landing problem, the Cafe domain introduced in the empirical analysis of CRIKEY2 (Coles, Fox, Halsey et al., 2008), a variant of Airport in which the amount of fuel burned is to be minimised, and a version of Satellite with time windows, where rewards are obtained by scheduling observations into the tighter windows.\nIn all cases we use the competition benchmark sets of instances where available. For the continuous Rovers and Satellite domains we used the IPC 2002 Complex Time problem sets. These instances work with the continuous domain variants and it is possible to get better makespan plans for them, by respecting the continuous dynamics, than is possible when the same instances are solved using the discrete domain variants. We generated increasing sized instances for the Airplane Landing domain in which the number of planes to be landed increased (in the nth instance of the problem, n planes must be landed). We wrote a problem generator for the AUV domain that increases the number of AUVs, waypoints and goals in the instances (they range from 2 AUVs, 4 waypoints and 1 goal, to 6 AUVs, 16 waypoints and 6 goals). All experiments were run on a 3.4GHz Pentium D machine, limited to 30 minutes and 1GB of memory."}, {"heading": "12.1 Comparison with Existing Temporal Planners", "text": "Few temporal planners can actually solve a full range of temporal problems. As we have already observed, many temporal planners cannot solve problems with required concurrency. Even within the class of problems that have required concurrency, there are easier problems, which can be solved by a left packing of actions within the plan and harder ones for which this is not possible. By left packing we mean that actions that must be executed concurrently with other actions in the plan can be started at the same time as each other. This property means that the approach adopted in Sapa, of extending forward search to include a choice to either start a new action or else to advance time to the earliest point at which a currently executing action terminates, is sufficient to solve the problem. In contrast, a problem that cannot be left packed will require the possibility of advancing time to some intermediate point during execution of an action in order to coordinate the correct interleaving of other actions with it. We describe such problems as requiring temporal coordination. One of the few planners that can also handle problems requiring temporal coordination is LPG-s (Gerevini et al., 2010).\nWe therefore compare COLIN with LPG-td, LPG-s, Sapa and the temporal baseline planner developed for the temporal satisficing track at the 2008 International Planning Competition. Neither\nthe temporal baseline planner, Sapa nor LPG-td can solve problems requiring any kind of temporal coordination. The temporal baseline planner compiles away temporal information, by using action\ncompression, and solves problems as if they were non-temporal metric or propositional problems. When solutions are found, using Metric-FF as the core planning system, the temporal information is reintroduced by annotating the plan with suitable timestamps based on a critical path analysis. No details are published about this planner, but the source code and brief information are available from the IPC 2008 web site. This approach cannot therefore solve problems with required concurrency, but is fast and effective on simpler problems where the temporal actions can be sequenced. It is straightforward to identify many cases when action compression can be applied safely and this analysis is implemented in COLIN to reduce the overhead of reasoning with action end points where it is unnecessary. Therefore, the behaviour of the temporal baseline planner is similar to that of COLIN when all actions can be safely compressed. In Figures 10, 11 and 12 we show CPU time comparisons between COLIN and the best performances of Sapa, LPG-td, LPG-s and the temporal baseline planner, across a wide and representative collection of temporal benchmark domains. Figure 10 shows performance on simple temporal problems, where action durations are all fixed, while Figures 11 and 12 show results for more complex temporal problems, including those where\nthe duration of actions is determined by the context in which they are executed (although none in which action effects depend on this), and problems with metric variables. None of these problems feature required concurrency or other forms of temporal coordination. In these figures, planners not appearing in a dataset were not the best on any problems in that domain.\nAnalysis of Figures 10\u201312 shows that COLIN does indeed pay an overhead in computation time in the solution of temporal problems that do not feature continuous dynamics. The overhead is particularly significant in the simple temporal problems where there is no interesting temporal structure and the temporal baseline planner tends to perform very well. The overhead paid by COLIN is lower in the complex temporal problems, where the temporal reasoning required is sometimes more challenging. The makespan results in Figures 13, 14 and 15 show that COLIN produces good quality plans, especially for the complex temporal problems, although the temporal baseline planner is still competitive in terms of both CPU time and makespan. This suggests that the temporal structure, even in the complex temporal benchmarks, is quite simple and that a planner can do well by ignoring the temporal structure that is present, rather than trying to reason about it in generating plans.\nThe detailed results of these experiments, showing raw runtime and quality comparisons between the planners used in this experiment, are presented in Appendix E."}, {"heading": "12.2 Solving Problems with Continuous Linear Change and Duration-Dependent Effects", "text": "Our focus in this section is on examining the scalability of COLIN on the continuous benchmark domains we have developed and, specifically, on comparing the two variants of the TRPG discussed in Section 9. These are: the basic heuristic, which discretises time, and the refined heuristic, which is capable of handling continuous numeric change directly. The continuous benchmarks, as described in Section 11, are characterised by sophisticated temporal structure (including required concurrency) giving rise to interesting opportunities for concurrent behaviour. Because these problems have time-dependent effects and continuous effects, they are out of the reach of the temporal planners used in the last experiment. The problems used for this experiment are designed to rely on the exploitation of these features, so a baseline planner that ignored these continuous dynamics would be unable to solve the problems.\nResults comparing the basic and refined heuristics are shown in Figure 16. Beginning with the Airplane Landing domain and the Rovers domain variant, the performance is the same when either\nheuristic is used: the relaxed plans found are the same. This is to be expected, because in these two domains the interaction between time and numbers is relatively limited. In the Airplane Landing problem, action durations affect a variable used to measure plan cost but that is not used in any preconditions. Thus, the selection of actions in the TRPG is unaffected. In the Rovers domain, continuous change arises when consuming power during navigate actions, or producing power when recharging. Capturing the time-dependent nature of these more precisely has no effect on the relaxed plans, as the nature of the relaxation leads it to only rarely require recharge actions, and the conditions under which these are needed are not affected by whether the effects are integrated or not. Nevertheless, these two domains illustrate that in guaranteed \u2018like-for-like\u2019 situations, where the heuristic guidance will be the same, the refined heuristic is only negligibly more expensive to compute, despite the additional overheads of tracking gradient effects as the TRPG is expanded. It can also be seen that COLIN scales well across the Airplane Landing instances, although it only manages to solve 9 of the 14 Rovers problems (these well within two minutes).\nIn the Satellite \u2018Cooled\u2019 domain, the runtime taken to find the plans when using the refined heuristic is comparable to that when using the basic heuristic: in some problems (e.g. 13, 18) it is slower; but in others (e.g. 12, 15) it is faster. The more interesting comparison to make is in the makespan data (shown to the right). As can be seen, the refined heuristic generally produces\nbetter quality plans. The difference in quality is due to the refined heuristic better capturing the relationship between time and numbers, leading to better actions being chosen in the relaxed plan. By way of example, consider the state reached after beginning the sunrise action:\n\u2022 For the basic heuristic, the LP is used to obtain bounds on the power availability in this state, with free reign over how much time to allow to elapse. The lower-bound found is slightly more than zero (corresponding to allowing time to elapse), and the upper-bound found is the peak power availability (corresponding to applying the entirety of the sunrise action). When building a TRPG from these bounds, cooled sensor operation is immediately available, and hence the goals will always be achieved first by actions using sensor cooling: the duration of such actions is lower, making them more attractive. The resulting relaxed plan, and hence helpful actions, will therefore lead search to use sensor cooling.\n\u2022 With the refined heuristic, the LP is used to obtain bounds on the power availability in this state, but these bounds must be obtained at the soonest possible point. Thus, the lowerbound is still slightly more than zero, but the upper bound is also only slightly more than zero. The positive gradients in effect on the power availability variables are then included in the TRPG, influencing the layers at which different actions become applicable. Specifically, actions without sensor cooling have lower power requirements, and hence appear at earlier layers. Then, for goals first achieved by actions not using sensor cooling (where the increased duration of acquiring the image without cooling is compensated for sufficiently by being able to start taking the image sooner) the relaxed plan, and hence helpful actions, will not use sensor cooling for these goals. It can be seen that this situation is closely analogous to the differences in alternative mortgages in the Borrower domain.\nThe extent to which this trade-off influences plan quality varies between problems, depending on the initial orientation of the satellites, and the images required. The least benefit arises if a satellite requires substantial reorientation to point it towards its first target \u2014 if this is the case, the time taken allows the energy level to rise sufficiently to support sensor cooling. The greatest benefit arises in the opposite situation, where a satellite requires minimal reorientation \u2014 then, switching on a sensor in its cooled mode will require a substantial amount of time to elapse to support its energy requirement precondition.\nTo aid understanding of the scalability implications of these results, the Satellite problems are based on those used in the 2002 IPC, so are of a similar fundamental size. However, the continuous reasoning that has been added to them makes the same underlying problems fundamentally much more difficult to solve.\nIn the AUV domain, the use of the refined heuristic increases the problem coverage, with 30 problems solved rather than 27. Applying the Wilcoxon Matched-Pairs Signed-Ranks Test to the paired time-taken data for mutually solved problems, we find that we can reject the null hypothesis that the refined heuristic is no better than the basic heuristic, with p \u2264 0.05. Observing the performance of the planner, this difference in performance arises due to the way in which the drifting process is handled by the two approaches. Specifically, it is accounted for by the difference in how the bounds for fact layer zero of the TRPG are calculated. Consider a state in which an action for an AUV to communicate image data has just been started. The domain encoding ensures that until communication has completed, the AUV cannot perform any other activities. At this point, prior to evaluating the state using a TRPG heuristic, the LP is used to give bounds on the values of each state\nvariable. Considering just the variable recording how far the communicating AUV has drifted \u2014 the variable (distance-from-waypoint auv0), from now on abbreviated to dfw0:\n\u2022 The basic heuristic employs the approach set out in Section 8.3. A single \u2018now\u2019 timestamp variable is introduced, that must come after the action just started, along with an additional variable and constraint for dfw0. Maximising and minimising the value of this additional variable yields bounds on dfw0. The lower bound will be infinitesimally larger than it was prior to starting the action, due to time having elapsed. The upper bound corresponds to allowing a large amount of time to elapse.\n\u2022 The refined heuristic employs the approach set out in Section 9.2.1. Here, a \u2018now\u2019 timestamp variable is introduced for each task variable, in this case we are concerned with tnow(dfw0). As in the prior case, this is constrained to be after the action just applied. Additionally, however, because the domain model enforces that no other action can refer to the value of the variable until the communicate action has finished, this specific tnow must also come after the (future) end of the action just applied. The bounds on dfw0 are then found by following the remaining steps of Section 9.2.1: the LP is solved to minimise the value of this tnow variable, the value of the variable is fixed to this minimum and then the LP is solved to maximise and minimise the value of dfw0. Critically, because this tnow variable must come after the end of the action just applied, rather than just after its start, the lower bound on dfw0 is larger.\nThe increase in the lower bound on dfw0 then affects whether, in the TRPG, preconditions of the form (<= (dfw0) c) are considered satisfied in the initial fact layer. If they are not satisfied, they are delayed until the earliest layer at which a localise action reduces the value of dfw0. This difference can then affect the relaxed plan found: during solution extraction, if an actionA requiring (<= (dfw0) c) is chosen, then if a localise action was necessary to achieve this in the TRPG, the action will be added to the relaxed plan. As A cannot come any earlier than after the end of the communicate action just applied, that is, the point at which the bounds on dfw0 are calculated, then some sort of localisation is necessary if A is ultimately to be applied. Thus, the bounds for the refined heuristic here lead to better relaxed plans being found, containing localise actions that would otherwise be omitted.\nTo give an indication of the difficulty of these problems, the AUV problems range from problems with 2 AUVs, 5 waypoints, 2 objectives and 2 goals to those at the harder end with 6 AUVs, 15 waypoints, 6 objectives and 7 goals. The major hurdle preventing COLIN from scaling to even larger problems is the inability to see that an implicit deadline has been created when a shine-torch action is started. The AUV shining the torch has finite energy, so if the planner starts a shinetorch action with one AUV, in preparation for another AUV to take an image, but then adds to the plan some actions involving the second AUV that are unrelated to taking the image, the delay can lead to there being insufficient energy to shine the torch for long enough to gain the required exposure when the photograph taking action is eventually started. This leads the planner to a dead end and it is forced to resort to best-first search, which is much less effective than EHC in this domain. Such implicit deadlines can occur in many planning problems with temporal coordination and the issues COLIN faces could be avoided by using a branch-ordering heuristic that promotes actions whose applicability is time-limited due to the ends of currently-executing actions, or perhaps through relaxing unnecessary ordering constraints imposed by COLIN due to total order search. Both of these are out of the scope of this paper, but are interesting avenues for future work.\n12.3 Post Hoc Plan Optimisation\nIn this section we evaluate the effectiveness of our post hoc plan optimisation strategy. As described in Section 10, the plan optimisation phase occurs after planning is complete and can never change the actions that are in the plan. By lifting a Partial Order prior to scheduling (Veloso, Pe\u0301rez, & Carbonell, 1990), we can provide the scheduler with a little more flexibility over the order of actions. As long as the ordering constraints remaining after this (greedy) partial-order lifting are respected, the scheduler can reduce plan cost by altering the time-points at which the actions occur and, where possible, their durations. Minimising an objective other than plan makespan can only have an effect on plan quality in domains where the metric is sensitive to the times at which the actions are applied, since, by default, COLIN minimises makespan in the solution of the final LP for a completed plan. There are few such benchmark domains in the literature, so we make use of the one existing suitable domain and introduce some new variations on existing benchmarks, in order to test this feature.\nThe first domain, and the only existing domain with this property, is the Airplane Landing domain, used earlier in this section, and described in Section 11. Here, the penalties incurred for each landing depend on whether, and to what extent, it is early or late. Therefore, for a given sequence of landings, the times assigned to them has an impact on the quality of the plan.\nThe next two of our benchmark problems are variants of problems introduced in the International Planning Competitions of 2002 (Long & Fox, 2003b) and 2004 (Hoffmann & Edelkamp, 2005). First, we consider a modified version of the Satellite domain. We modify the domain by adding time windows (modelled using TILs) during which there is a clear view of a given objective. If the photograph of the objective is taken during such a time window, the quality of the plan improves, as a better quality picture is preferable. In each problem we introduce three such time windows for each objective, of bounded random duration, during which taking a photograph of the objective is preferred. The second adapted benchmark is taken from the IPC2004 Airport domain. Where the Airplane Landing problem described previously is concerned with scheduling landing times for aircraft, the Airport domain is concerned with coordinating the ground traffic: moving planes from gates to runways, and eventually to take-off, whilst respecting the physical separation that must be maintained between aircraft for safety reasons. We add to this domain the metric to minimise the total amount of fuel burnt between an aircraft\u2019s engines starting up and when it eventually takes off. To capture this in PDDL2.1, we add the action shown in Figure 17. This action must occur before a plane\u2019s engines can be started and cannot then finish until the plane has started to take-off (hence its duration is at least that of the startup action). Between these two points it increases\nthe amount of fuel wasted by a rate proportional to the number of engines fitted to the aircraft: larger planes (for which the number of engines is greater) waste more fuel per unit of time. In both the Satellite and the Airport domains we use the standard problem sets from the competitions, adding any minor changes needed to support the modifications made, whilst leaving the underlying problems themselves unaltered.\nThe final domain we consider is the cafe\u0301 domain, first used to evaluate CRIKEY (Coles, Fox, Halsey et al., 2008). In this domain, tea and toast must be made and delivered to each table in a cafe\u0301. The kitchen, however, has only one plug socket, preventing the two items from being made concurrently. This restriction allows the problem to have a number of interesting metric functions: to minimise the total time to serve all customers (the plan makespan), to minimise the time between delivery of tea and toast to a given table, or to minimise the amount by which the items have cooled when each is delivered to the table. We consider the latter two variants here.\nThe results of our experiments are presented in Figure 18. Starting in the top-left, with the Airplane Landing domain, post hoc optimisation gives only a modest improvement in plan quality. This is due to the limited scope for optimisation: even after partial-order lifting, the order in which the planes are going to land is fixed by the plan, so all that can be adjusted is the precise times at which the planes are going to land within that ordering.\nMoving to the Airport domain variant with the \u2018burning-fuel\u2019 action \u2014 Figure 18 top-right \u2014 post hoc scheduling is able to give large improvements in plan quality. In the original plans, before optimisation, the burning-fuel action for a given plane can be started at any point prior to when the relevant can-start-engines fact is needed and can be ended at any point after the relevant taking-off fact is true, so not necessarily in a timely manner. Following post hoc optimisation, due to the objective function used, each burning-fuel action starts as late as possible and finishes as early as possible.\nIn the cafe\u0301 domain, the results for the two metrics used are shown in the central graphs in Figure 18. The two diagonal lines correspond to the original plans. On a given problem, the two plans are identical: only the evaluation metric differs. The two lower lines show the quality of the plan after scheduling it with respect to the relevant metric. Observing the post-scheduled plans, the actions are scheduled as one would intuitively expect. When minimising the total delivery window times, the items for a given table are delivered in succession, even if the first item loses heat while waiting for the second item to be prepared. In contrast, when minimising heat loss items are delivered to tables as soon as they have been prepared, even if there is then a delay between the two items being delivered.\nFinally, the results for the variant of the Satellite domain with observation windows is shown in the bottom-left of Figure 18. Whilst not as marked as the improvements in the previous two domains, the scheduler is able to make some headway in better scheduling the observations. The original plan for a given problem will, for each satellite, fix the observations it is to make, and the order in which they are to be made. There remains enough flexibility to be able to improve plan quality, reducing plan cost by around a factor of 2."}, {"heading": "12.4 Comparison with Optimal Solutions", "text": "We investigated the difference in quality between optimal solutions and the solutions produced by COLIN in order to form an impression of how close to optimal COLIN can get. To do this, we ran COLIN with an admissible heuristic that uses the makespan estimate produced by the TRPG, using\nthe same value for as is used by COLIN in the results presented in Figures 16 and 18. We call this variant optimalCOLIN.\nIn the AUV and Rover domains, there are variable-duration actions in the domain for which the durations can be chosen to be as small as when these actions are used in a plan. -length actions might be chosen, for example, to relocalise having slightly drifted, or to recharge having used a negligible amount of power. In these domains, optimal search has to consider plans comprising almost entirely actions of duration up to the optimal makespan. As an example of the scale of this,\non AUV problem 1, solved by COLIN, we find a plan with makespan 34.031. Careful analysis by hand suggests that this plan cannot be improved, so is optimal. OptimalCOLIN must consider plans of up to 34,031 steps in order to prove that this plan is optimal. This means that this problem is completely out of the reach of optimal planning.\nA similar problem arises in the Rovers domain, where the recharge action can be as little as long, and a series of -long recharge actions can be applied, reaching ostensibly different states, but without making any progress. Clearly, the potential for -duration actions can arise in any continuous temporal domain. The same problem of search-space explosion will also arise in any temporal domain where there are orders of magnitude differences between the longest and shortest possible actions.\nHowever, in the Airplane-Landing and Satellite Cooling domains, there are no variable-duration actions in these domains that can be made arbitrarily short during search. Therefore, optimalCOLIN is in principle able to solve problems in these domains. In fact, given 4 Gb of memory and 1 hour of runtime for each instance, it was able to solve 6 airplane landing instances, as shown in Table 4. As the table shows, the time required to solve these problems increases very fast: problem 5 could be solved in 3.94 seconds, problem 6 in 69.93 seconds, and problem 7 could not be solved within the hour available. On this basis we decided it was unnecessary to extend the time available to optimalCOLIN as it would be unlikely to cope with large instances.\nTable 4 shows that COLIN sacrifices optimality for speed. This sacrifice is important, but it does pay off in terms of time required to solve problems. COLIN is able to solve 62 of the airplane landing problems, with no instance taking more than 33.02 seconds to solve.\nWe found that optimalCOLIN could report a candidate solution to the first Satellite domain instance, within 368 seconds. However, it could not prove within the time available that this solution was optimal, so we did not include it."}, {"heading": "12.5 Costs Associated with LP Scheduling", "text": "In the transition from CRIKEY3 to COLIN we switch from solving an STP at each state to solving an LP. An important issue to consider is the impact that this has on the time taken to evaluate the\nfeasibility of the plan constructed to reach every state considered in the search. In its default mode of operation, COLIN uses an STP to evaluate a state unless it has temporal\u2013numeric constructs that necessitate use of an LP. To evaluate whether or not this is appropriate (or whether always using an LP would be faster), and to compare the overheads of STP solving with LP solving on equivalent problems, we created a variant of COLIN that, at every state S, schedules the plan to reach S independently using three different schedulers: the original STP solver used in the standard version of COLIN, the equivalent LP solved using CPLEX (IBM ILOG CPLEX Optimization Studio) and the equivalent LP solved using CLP (Lougee-Heimer, 2003). The STP solver used is the incremental STP algorithm due to Cesta and Oddi (1996), as previously used in CRIKEY3. Each of the LP solvers is used with the tighter variable bounds described in Section 8.4. In order to evaluate the cost associated with use of an LP instead of an STP, we modified COLIN to collect data revealing the costs for each technique applied at each node evaluated during the search for a plan. It is not possible to compare the performance straightforwardly, simply by running COLIN using an STP versus COLIN with an LP, because minor variations caused by numerical accuracy can lead to very different trajectories being followed, masking the intended comparison. As an aside, it is interesting to observe that minor (and essentially uncontrollable) differences in computed makespans for relaxed plans can lead to significant variations in performance (relaxed plans with equal h-values are sorted by makespan estimates for search).\nAs we wish to compare the STP and LP approaches, it is necessary to consider domains with which both can reason: that is, those without continuous-numeric or duration-dependent effects. In order to consider some problems for which scheduling is interesting and necessary (in contrast to the temporally simple problems of Section 12.1) we consider domains with required concurrency. Currently very few such benchmarks exist, as few planners attempt to solve such problems. We use representatives of the only competition domains with such features: the compiled \u2018timed initial literal\u2019 domains from IPC2004, from which we use Airport (with Time Windows) and PipesNoTankage (with deadlines). We also use the Match-Lift and Driverlog Shift domains (Halsey, 2005). For completeness, we include results for a domain in which the scheduler is not strictly necessary: the PipesNoTankage Temporal domain from IPC2004.\nFigure 19 shows the mean time spent scheduling per state, using each approach, on the problems from the above domains. We exclude from the graph data from any problems that were solved by the planner in less than a second, as the accuracy of the profiling data is not sufficiently reliable to measure the time spent in each scheduler when the overall time taken is small. Since there was no interesting variation in results between domains we present all data together across three graphs, sorted by scheduling time per node when using CPLEX. This is intended as a nominal analogue for how hard the scheduling problems in the given planning problem are. An increase in the scheduling time for CPLEX generally corresponds to an increase in the scheduling time for CLP and the STP solver, except on the easier problems where noise can be sufficient to tip the balance as the figures are small. Note the differing y-axis scales on the three graphs, sorting problems according to difficulty allows us to display the data with an appropriate y range to distinguish the results. For the sake of maintaining reasonable y-axis ranges the final problem, problem 60, has been omitted from the graphs; on this problem the figures were CPLEX 239ms, CLP 139ms and STP 38ms.\nThe results in Figure 19 are, of course, not indicative of the scalability of COLIN, as it is running three schedulers at each state, so is significantly slower than in its usual configuration. In practice, in domains such as these with no continuous or duration dependent effects, COLIN will automatically\ndisable the LP scheduler and use the more efficient STP solver. Further, the planner is here being run with profiling enabled, so is subject to significant overheads.\nConsidering the relative performance of the STP and LP solvers, it is clear that there are overheads incurred by the necessary (for domains with continuous effects) move to using an LP rather than an STP. The mean ratio of time spent scheduling on the same problems by CPLEX to that spent using the STP solver is 5.81, the figure for CLP is 3.71. Analysis of the data suggests that these ratios do not change as problem difficulty increases, rather the overhead is a constant factor on harder problems.\nDespite increased scheduling overheads it is still worth noting that solving the scheduling problem is a relatively small fraction of the cost of the reasoning done at each state. Once the plan to a given state has been scheduled to check for feasibility the state is evaluated using the temporal RPG heuristic described in Section 9. It is well known, from analysis of the performance of FF and other forward search planners, that the majority of search time is spent in evaluating this heuristic. To give an indication of the relative cost of scheduling versus heuristic computation we give an admissible estimate of the mean fraction of the time spent, per-state, running the scheduler versus computing the heuristic. This estimate is guaranteed to overestimate the true mean because in some states the scheduler will demonstrate that the temporal problem has no solution: in such states the RPG heuristic will never be evaluated, so the heuristic evaluation is actually applied to fewer states than the scheduler. Nonetheless, our data shows that, across these problems, using the STP solver scheduling accounts for on average less than 5% of state evaluation time. For CLP and CPLEX the figures are 13% and 18% respectively. This suggests that, although scheduling does add some overhead to solving problems, these are relatively small compared to the cost of heuristic computation.\nA perhaps surprising observation that can be made from Figure 19 is that CLP generally solves the scheduling problems much more efficiently than CPLEX. Given the reputation of CPLEX as a highly efficient commercial LP solver we wanted to investigate why this is the case on our problems.\nWe performed a further analysis of the profiling data, breaking down the results by function call, to observe the time spent in the various aspects of constructing and solving an LP thorough the CLP and CPLEX library calls. The data, presented in Figure 20 shows the time spent in each function as a fraction of the total time taken by CPLEX to schedule plans (each summed across all problems). The \u2018not used\u2019 section in the CLP data represents the time saved using CLP versus CPLEX. This presentation means that equally sized slices of each of the \u2018pies\u2019 represent the same length of time being taken by either of the solvers in their respective methods.\nThe important insight that we can gain from this data is that most of the time in both of the LP solvers is not spent in the solve function, indeed it can be observed that the search portion is negligible: it is barely visible. The majority of time is, in fact, spent in adding rows to the LP matrix, i.e. in adding constraints to the LP before it is actually solved. Comparing CPLEX to CLP, it takes over 6 times longer, on average, to add a row to the matrix. The LPs being created for both are identical, and hence involve adding the same number of rows to the matrix. The \u2019other\u2019 portion of the chart corresponds to other methods, many of which also take longer than search, but that are again pre-processing steps such as adding new columns (variables) and setting upper bounds. Since adding rows to the matrix is a significant portion of the time taken in constructing-then-solving the LPs in COLIN, this results in a large overhead. The LPs created by COLIN are very small and simple to solve, compared with the difficult industrial-sized problems for which CPLEX is designed.\nOur results suggest that, in fact, the best type of LP solver to use for this task is a relatively light-weight LP solver, with few overheads, that can create models efficiently, even if perhaps it would not scale to other large-scale problems. The other notable, although less marked, difference between the two LP solvers is the time spent in the destructor, called to free up the memory used by the LP solver after each state has been evaluated. Here, it takes 23 times longer, on average, to call the destructor for CPLEX than the destructor for CLP. This has less impact than the rowadding overheads, since the LP is only deleted once per state, rather than once per LP constraint. In general, this would not normally be a noticeable issue when solving a single difficult LP. However, in COLIN, where the number of LPs solved is equal to the number of states evaluated, this overhead does become noticeable.\nOne interesting outcome of this study is that if, in the future, COLIN were to be extended to non-linear continuous change, requiring the use of a mathematical programming solver at each state (along with other research developments), the overheads may well not be prohibitive. The search within the solver, which is where the greater overhead would occur due to this change, is in fact not the major contributor to the time overheads of using an LP."}, {"heading": "13. Conclusions", "text": "As the range of problems that can be solved effectively by planners grows, so does the range of opportunities for the technology to be applied to real problems. In recent years, planning has extended to solve problems with real temporal structure, requiring temporal coordination, problems that include metric resources and interactions between their use and the causal structure of plans. We have shown how that range can be extended still further, to include linear continuous process effects. Each extension of the power of planners demands several steps. The first is to model the extension in a form that allows the relationship between the constraints imposed on plans by the new expressiveness, and the actions that can be used to solve the problem, to be properly expressed. The second step is to develop a means by which to represent the world state consistently, in order\nto characterise the space in which the search for a plan is conducted. The third step is to develop a way to compute the progression of states using the action models in this extended representation. Once this step is complete, it is, in principle, possible to plan: a search space can be constructed and searched using classic simple search techniques. In practice, this process is unlikely to lead to solutions of many interesting problems so the fourth step, in order to make the search possible in large spaces, is to construct an informed heuristic to guide the search.\nIn this paper we have built on earlier work that completed the first steps, adding the third and fourth steps that allow us to solve planning problems with continuous effects. The tools we have used to achieve this are well-established Operations Research tools: LP solvers and their extensions to MILP solvers. The contributions we have made are to show how these tools can be harnessed to check consistency of states, to model state progression and to compute heuristics that can successfully guide search in the large spaces that develop for these planning problems.\nAn additional contribution is that we have established a collection of benchmark problems for this direction of research in planning. The planning community has witnessed that the creation of benchmarks and their propagation is a powerful aid in the development of the technology, supporting clear empirical evaluation and challenging researchers to improve on the results of others. We have shown that COLIN can solve interesting and complex problems, but there remains much room for improvement. Apart from extending the capability of the planner by improving the informedness of its heuristic and by improving the early pruning of dead end states, there is also the opportunity to extend still further the range of problems that can be expressed and solved. In particular, we are interested in problems with non-linear continuous effects, such as power and thermal curves. It seems possible that such non-linear effects might be approached by a similar approach to that used in COLIN, adapting a NLP solver to the same role as the LP solver in COLIN. Alternatively, it might be possible to approximate non-linear effects with piecewise linear effects, in much the same way that we did for the AUV domain described in this paper, but performing the process automatically.\nPlanning is becoming an increasingly key technology as robotic systems become more powerful and more complex and we begin to see the limits of low level control strategies in managing the control of these systems. Autonomy demands more powerful predictive control and it is planning that offers possible solutions to this problem. Planning with continuous effects will be an important tool in the collection that we can offer in tackling these new demands."}, {"heading": "Acknowledgments", "text": "The authors wish to thank the handling editor, Malte Helmert, and the anonymous reviewers for their considerable contributions to this paper. The authors also wish to thank members of our Planning Group for their helpful discussions during the long gestation of this work.\nThe authors also wish to acknowledge the EPSRC for their support of this work, specifically through grants EP/G023360/2 and EP/H029001/2."}, {"heading": "Appendix A. Glossary", "text": "Name Description First Use a \u2193 (i, v) The lower bound on assignment effects on variable v due to ac-\ntions at layer i in a reachability graph. 62\na \u2191 (i, v) The upper bound on assignment effects on variable v due to actions at layer i in a reachability graph. 62 Action compression A technique for simplifying the structure of durative actions by treating them as a simple non-durative action with the union of the effects of both ends of the durative action and the union of their preconditions. 9 al Action layer in the reachability graph constructed for heuristic purposes. 26\nce(i) Function returning the variable corresponding to the end time for a snap-action at position i in the current plan. 21 cs(i) Function returning the variable corresponding to the start time for a snap-action at position i in the current plan. 21\ndec(i, v) Set of (discrete) decreasing effects on variable v at layer i in a reachability graph. 62 D The rate of change of variable v (associated with some state achieved during the execution of a plan. 21 dmin (dmax) The minimum (maximum) duration of an action. We use dmin(a) (dmax(a)) where the relevant action is required to be explicit and dmin(a, t) (dmax(a, t)) where the value is anchored to an action in layer al(t). 14\nE The event list recording action start times for durative actions whose end points have not yet been included in a plan. 13 eff +x Propositional add effects of an action, where x, when present, indicates whether at the start or end of the action. 4 eff \u2212x Propositional delete effects of an action, where x, when present, indicates whether at the start or end of the action. 4 eff nx Numeric effects of an action, where x, when present, indicates whether at the start or end of the action. 4 estepi The name of the LP variable corresponding to the time at which a durative action will finish, having started as the ith step in a plan, but not having finished within the plan constructed so far. 20 elapsed(a) The maximum time for which action a could have been executing in a state that is being heuristically evaluated. 27\nf(i) The variable in the STN in CRIKEY3 that corresponds to the time at which a currently incomplete action will eventually finish. 15\nName Description First Use fl Fact layer in the reachability graph constructed for heuristic pur-\nposes. 26\ninc(i, v) Set of (discrete) increasing effects on variable v at layer i in a reachability graph. 62 inv(S) The invariants that are active in state S. 14\nLeft packing A structure of plans with concurrency in which all concurrent actions start simultaneously. 39\nnow The name of the variable created to represent the time at the end of the current plan in each STP or LP used to check temporal consistency of a state. 15\n\u3008op, i , dmin, dmax \u3009 Event record in a CRIKEY state, containing the durative action, op, that started at step i, and the minimum and maximum duration of the action. 14\nprea Conditions required to complete an action. 4 pre\u2194 Invariant conditions of a durative action. 5 pre` Conditions required to initiate an action. 4 p(a) The bound on the number of instances of durative action a that\nmay execute concurrently. 28\nremaining(e) The maximum amount of remaining time over which an action in event record e could continue to be executing following a state being heuristically evaluated. 28 rem(t, a) Information associated with durative action a in al(t) in the reachability analysis constructed by COLIN, indicating how much time a could continue to execute from this layer. 33\nstepi The name of the LP variable corresponding to the time at which action ai is applied in a plan. 20\nt(i) The variable in the STP in CRIKEY3 that represents the time at which step i in a plan is to be executed. 14 Temporal coordination The property of planning problems that require some for of concurrency in order to manage the interactions between the actions or deadlines. 39 Time-dependent change Action effects that refer to ?duration, causing numeric fluents to change by different amounts according to the length of the action causing the effect. 2\nName Description First Use #t Used to describe continuous change: for a complete account\nof its use and semantics, see original discussion on its use in PDDL2.1 (Fox & Long, 2003).\n3\nub(w,x,y) Function used to calculate bounds on the effects of continuous numeric change. 29\nv Used to represent the vector of metric fluents associated with a planning domain, and their values in a state. The vector is treated as indexable: v[i] is the ith entry in v. 5 v\u2032 The vector of values of metric fluents at the start of a state, immediately following step effects of application of an action. 21 vmin,vmax The vectors of lower and upper bounds on the values of the numeric variables in a state (during plan construction). 24\nw Symbol used to represent a vector of constants of equal dimension to the size of the vector of metric fluents in the relevant planning problem. 5"}, {"heading": "Appendix B. The Metric Relaxed Planning Graph Heuristic", "text": "The Relaxed Planning Graph (RPG) heuristic of Metric-FF (Hoffmann, 2003) has been the most popular numeric planning heuristic over the last decade, being widely used in many planners. The intuition behind the heuristic is to generalise the \u2018delete-relaxation\u2019 to include numeric variables. In the case of propositions, the relaxation is to simply ignore propositional delete effects so, as (relaxed) actions are applied, the set of true propositions is non-decreasing. In the case of numbers, the relaxation replaces exact assignments to numeric variables with bound constraints for their upper and lower bounds. Applying relaxed actions extends the bounds by reducing lower bounds with decrease effects and increasing upper bounds with increase effects. Checking whether a numeric precondition is satisfied is then simply a matter of testing whether the constraint is satisfied by some value within the bounds. The delete-relaxed problem can be solved (non-optimally) in polynomial time, and the number of actions in the resulting relaxed plan can then be taken as a heuristic estimate of the distance from the evaluated state to the goal.\nThe purpose of the RPG is to support this heuristic computation. Relaxed planning is undertaken in two phases: graph expansion, and solution extraction. In the graph expansion phase the purpose is to build an RPG, identifying which facts and actions become reachable. The RPG consists of alternate fact layers, consisting of propositions that can hold and optimistic bounds on v, and action layers, containing actions whose preconditions are satisfied in the preceding fact layer. In the case of propositional preconditions, a precondition is satisfied if the relevant fact is contained in the previous layer. In the case of numeric preconditions, these are satisfied if some assignment of the variables appearing in the precondition, consistent with the upper and lower bounds, lead to it being satisfied. We define the function ub(w,x,y) as:\nub(w,x,y) = \u2211\nw[j]\u2208w\n{ w[j]\u00d7 y[j] if w[j] \u2265 0 w[j]\u00d7 x[j] otherwise\n(this is the same function as is defined in Section 9.2). Then, denoting a fact layer i as a set of propositions, fl(i), and upper and lower variable bounds (vmin(i),vmax(i)), a precondition w \u00b7 v \u2265 c of an action in layer i is considered true iff:\nub(w,vmin(i),vmax(i)) \u2265 c\nTo seed graph construction, fact layer 0 contains all facts that are true in S. Thus, action layer 0 consists of all actions whose preconditions are satisfied in fact layer 0. Fact layer 1 is then set to be the optimistic outcome of taking fact layer 0, and applying each of the actions in action layer 0. More formally, considering propositions, applying the actions in action layer i, i.e. the actions al(i) leads to a fact layer i+ 1 where:\nfl(i+ 1) = fl(i) \u222a {eff +(a) | a \u2208 al(i)}\nConsidering numbers, in action layer i the set of optimistic increase and decrease effects on a variable v across all actions are, respectively:\ninc(i, v) = {(ub(w,vmin(i),vmax(i)) + c) > 0 | \u2203a \u2208 al(i) s.t. \u3008v,+=,w \u00b7 v + c\u3009 \u2208 eff n(a)}\ndec(i, v) = {(ub(w,vmax(i),vmin(i)) + c) < 0 | \u2203a \u2208 al(i) s.t. \u3008v,+=,w \u00b7 v + c\u3009 \u2208 eff n(a)}\nThe exchange of the minimum and maximum bounds for v in these two expressions is important: it causes each expression to be as extreme as possible in the appropriate direction. Similarly, the optimistic upper and lower bounds on v, following all available assignment effects, are:\na \u2191 (i, v) = max{(ub(w,vmin(i),vmax(i)) + c) | \u2203a \u2208 al(i) s.t.\u3008v,=,w \u00b7 v + c\u3009 \u2208 eff n(a)}\na \u2193 (i, v) = min{(ub(w,vmax(i),vmin(i)) + c) | \u2203a \u2208 al(i) s.t.\u3008v,=,w \u00b7 v + c\u3009 \u2208 eff n(a)}\nThe new bounds then become:\nvmax(i+ 1)[j] = max{a \u2191 (i,v[j]),vmax(i)[j] + \u2211 inc(i,v[j])}\nvmin(i+ 1)[j] = min{a \u2193 (i,v[j]),vmin(i)[j] + \u2211 dec(i,v[j])}\nThat is, to find the upper (lower) bounds of v[j] at the next layer, for each we have a choice of applying the largest (smallest) single assignment effect, or the sum of all increase (decrease) effects. Having computed the bounds of all variables in layer i + 1, graph expansion then continues iteratively, finding actions applicable in action layer i + 1, and hence the facts in layer i + 2, and so on. Graph expansion terminates in one of two cases: either a fact layer satisfies all propositional and numeric goals, or the addition of further layers would never lead to more preconditions being satisfied \u2014 a condition signalled when no new propositions are appearing and the accumulation of larger or smaller bounds on variables would not lead to any more numeric preconditions becoming satisfied. In this case, the relaxed problem cannot be solved and hence, in the original problem, no plan starting from S can reach G. The heuristic value of the state is then set to\u221e.\nAssuming graph expansion terminates with all goals reached, the second phase is to extract a solution from the planning graph. This is a recursive procedure, regressing from the goals back to the initial fact layer. Each fact layer is augmented with a set of goals (facts or numeric preconditions) that are to be achieved at that layer. Beginning by inserting the top-level goals G into the planning graph at the first layers at which they each appeared, solution extraction repeatedly picks the latest outstanding goal in the planning graph and selects a way to achieve it. For propositional goals, a single action (with an effect adding the goal) is chosen, and its preconditions are inserted as goals to be achieved (again, at the earliest possible layers). To satisfy the numeric goal w \u00b7 v \u2265 c at layer i, actions with effects acting upon the variables (with non-zero coefficients) in v are chosen, until the net increase of w \u00b7 v, k, is sufficient to allow the residual precondition w \u00b7 v \u2265 c \u2212 k to be satisfied at fact layer i\u2212 1. At this point, this residual precondition is added as a goal to be achieved at layer i \u2212 1 (or earlier if possible), and the preconditions of all the actions chosen to support this precondition are added as goals to be achieved at previous layers.\nSolution extraction terminates when all outstanding goals are to be achieved in fact layer 0, since they are then true in the state being evaluated and need no supporting actions. The actions selected in solution extraction form the relaxed plan from S to the goal. The length (number of actions) of this relaxed plan forms the heuristic estimate, h(S). Additionally, the actions in the relaxed plan that were chosen from action layer 0 form the basis of the \u2018helpful actions\u2019 in S, used to restrict the states explored by enforced hill-climbing search: any action with an effect in common with the actions chosen from action layer 0 is considered to be helpful."}, {"heading": "Appendix C. Temporal Reasoning in Relaxed Planning Graphs", "text": "Several approaches have been proposed for building temporal relaxed planning graphs (TRPGs). There are three additional features that TRPGs can attempt to manage, compared with RPGs:\n1. The temporal structure of durative actions: aa can only be applied if a` has been applied before it.\n2. Action durations: end effects of actions are only available at an appropriate delay after they have started.\n3. The PDDL2.1 start\u2013end semantics, allowing effects and preconditions to be attached to both the starts and ends of actions.\nThe TRPG employed in Sapa (Do & Kambhampati, 2003) satisfies the first two of these, but not the third. In Sapa, each action is compressed into a temporally-extended action obeying the TGP semantics, before discarding delete effects, as a relaxation, and building a TGP-style planning graph (Smith & Weld, 1999). The use of compression and a time-stamped TGP representation captures durations and the start-before-end relationships, but the use of compression causes the heuristic to find false dead-ends in cases where there is required concurrency.\nThe TRGP used in CRIKEY (Coles, Fox, Halsey et al., 2008) avoids action compression, but it ignores the durations of actions. A non-temporal RPG is built in terms of the snap-actions used during search, with an additional precondition on each end snap-action that a particular dummy fact, added by its corresponding start, has appeared in the preceding fact layer. The use of snap-actions means no preconditions or effects are lost (ensuring that the heuristic no longer identifies the false dead-ends created by the approach used in Sapa), but the limitation of the heuristic is that there is no forced separation between the start and and end of an action, but only an ordering constraint.\nIn CRIKEY3 (Coles, Fox, Long et al., 2008a), the heuristic is constructed to combine the strengths of both of these earlier heuristics, accounting for the durations of actions, whilst also respecting the start\u2013end semantics. We now briefly describe the construction of this TRPG, since it is the basis for the heuristic used in COLIN. The structure of the TRPG is similar to that constructed in Metric-FF, but instead of each fact layer being assigned an index, it is assigned a time-stamp (indicating the minimum amount of time that must pass after the initial layer before the facts in the layer in question can appear). To capture the durations of actions, we record, for each end action aa, the earliest layer tmin(aa) at which it can appear. This value is set to 0 for all actions that are already executing in the state being evaluated (as there is no need to first insert the start of the action into the RPG). For other actions, the value is initialised to\u221e, before commencing TRPG construction.\nTo build a TRPG we follow Algorithm 2. First, a number of initialisation steps are performed. The time-zero fact layer fl(0) is initialised (at line 1) to contain all the facts true in S10. The set ea is initialised to contain all the end snap-actions that must appear in the TRPG \u2014 if an action is executing, its end has to be reachable (i.e. appear in the TRPG), or else the state S is a dead end. If ea is empty, and S satisfies the goals G (line 14), then no TRPG need be built, since the plan is complete.\nFollowing initialisation, the TRPG is expanded, beginning with t = 0 and using the fact layer fl(t) to determine the action layer al(t). If the preconditions of an action are satisfied in a fact layer\n10. For simplicity we omit the handling of numeric fluents from this explanation \u2014 this is performed exactly as in the earlier description of the RPG heuristic implemented in Metric-FF.\nAlgorithm 2: Building a Temporal RPG in CRIKEY3. Data: S = \u3008F,E, T \u3009 - state to be evaluated Result: R = \u3008fls, als\u3009, a relaxed planning graph fl(0)\u2190 F ;1 fls \u2190 \u3008fl(0)\u3009;2 als \u2190 \u3008 \u3009;3 t\u2190 0;4 ea\u2190 \u2205;5 prev al \u2190 \u2205;6 prev fl \u2190 fl(0);7 foreach aa do8\nif {e \u2208 E | e.op = a} = \u2205 then9 tmin(aa)\u2190\u221e;10 else11 tmin(aa)\u2190 0;12 ea\u2190 ea \u222a {aa};13\nif G \u2286 fl(0) \u2227 ea = \u2205 then return \u2018S is a goal state\u2019;14 while t <\u221e do15 fl(t+ )\u2190 prev fl ;16 al(t)\u2190 {aa | pre(aa) \u2286 fl(t) \u2227 tmin(aa) \u2264 t};17 foreach aa \u2208 al(t)\u2212 prev al do18 fl(t+ )\u2190 fl(t+ ) \u222a eff +(aa);19 al(t)\u2190 al(t) \u222a {a` | pre(a`) \u2286 fl(t)};20 foreach a` \u2208 al(t)\u2212 prev al do21 fl(t+ )\u2190 fl(t+ ) \u222a eff +(a`);22 tmin(aa) = min[tmin(aa), t+ dmin(a)];23\nals \u2190 als + al(t);24 fls \u2190 fls + fls(t+ );25 prev al \u2190 al(t);26 if G \u2286 fl(t+ ) \u2227 ea \u2286 al(t) then27 return R = \u3008fls, als\u3009;28 if prev fl 6= fl(t+ ) then29 prev fl \u2190 fl(t+ );30 t\u2190 t+ ;31\nelse32 prev fl \u2190 fl(t+ );33 ep = {tmin(aa) | pre(aa) \u2286 fl(t) \u2227 tmin(aa) > t};34 if ep 6= \u2205 then t\u2190 min[ep];35 else t\u2190\u221e;36\nreturn \u2018S is a dead end\u2019;37\nfl(t) then whether it can appear in al(t) depends on whether it is a start or end snap-action. The first and simpler case (line 20) is that, if a start snap-action a` is applicable, it is added to al(t) and tmin(aa) is set to t + dmin(a), where dmin(a) is an a priori lower bound on the duration of a. If there is a state-independent measure of the minimum duration of a, i.e. a minimum duration constraint referring only to constants, then this is taken as the value of dmin(a). Otherwise, if all the minimum duration constraints depends on the state in which the action is applied, then dmin(a) = : all that is certain is that some time must elapse between the start and the end of the action. The state-dependent terms cannot be evaluated since the TRPG determines a relaxed state, not a real state.\nThe second case, covering end snap-actions, is that if the preconditions of an end action aa become satisfied in a fact layer fl(t), then addition of aa to al(t) depends on whether the start of the action can have occurred sufficiently far in the past (line 17). If t \u2265 tmin(aa) then aa is added to al(t); if t < tmin(aa) < \u221e, then aa is postponed until al(tmin(aa)); otherwise, the start of the action has yet to appear, and aa is postponed until the relevant start appears.\nHaving determined which actions newly appear in al(t), the fact layer fl(t + ) is updated as in the non-temporal RPG case, by taking fl(t) and (optimistically) applying the effects of all the actions in al(t). If fl(t + ) and al(t) do not contain the necessary goals and end snap-actions (line 27) then it must be decided which fact layer to consider next. Clearly, it is infeasible to create new fact layers at spacing between fl(0) and the fact layer at which the goals appear. Fortunately, it is also unnecessary, as many of the fact and action layers in such a graph would be identical. Instead, we determine the next fact layer to consider as follows:\n\u2022 If there are new facts in fl(t+ ) that were not true in fl(t) (line 29), the next layer to expand is fl(t+ ) \u2014 the appearance of new and potentially useful facts makes it necessary to consider whether any actions become applicable in that layer.\n\u2022 If fl(t + ) = fl(t), then we know that visiting fl(t + ) is futile. In this case (line 34), the time-stamp of the next fact layer to visit is the earliest future point at which the postponed end of an action becomes applicable:\nmin{tmin(aa) | pre(aa) \u2286 fl(t) \u2227 tmin(aa) > t}\nIf the minimum of these values is \u221e (or undefined) then the state can be pruned and the procedure exits early, signalling the result to the search procedure.\nWhen a TRPG is successfully constructed (that is, if the starting state is not a dead end) the graph that is returned contains a finite set of fact and action layers, each associated with a real time value.\nAssuming graph expansion terminates with all goals reached, a relaxed solution is extracted. The solution extraction procedure used in Metric-FF needs one minor modification to be suitable for use in a TRPG: if the end of an action aa is chosen to support a goal at a given fact layer, then if the action a is not already executing in the state being evaluated, the corresponding start a` must be scheduled for selection (at the layer in which it first appeared). The purpose of this corresponds to that of the dummy facts in CRIKEY: if the end of an action is chosen, its start must also be executed.\nAs a final remark on this TRPG, timed initial literals (TILs) can be included by employing the machinery introduced to delay the ends of actions until an appropriate layer. If the dummy TIL actions {TILj ...TILm} have yet to be applied then tmin(TILj) = 0.0, since TILj could be applied in the first action layer. The intuition here is that the state being evaluated is a snapshot of the world,\ntaken no earlier than after the end of the previous action, but no later than the point at which the next TIL event occurs (due to the constraints discussed in Section 6.1). The minimum timestamps for the later TILs, each TILk \u2208 {TILj+1...TILm}, are then set relative to this time point:\ntmin(TILk) = ts(TILk)\u2212 ts(TILj)."}, {"heading": "Appendix D. Post-Hoc Plan Optimisation", "text": "This appendix contains details of the MILP construction briefly described in Section 10.2.\nD.1 Optimising for Time Windows\nFirst let us consider the simple case where an action do has a conditional effect on a metric-tracking variable reward (where the objective of the problem is to maximise reward), and where the effect occurs depends on the truth value of a single proposition p at some time specifier ts relative to the action do (either at start, over all, or at end):\n(when (ts (p)) (at end (increase (reward) k))).\nIn the case where p can be manipulated by actions, without allowing the MILP to introduce new actions or completely change the order of the plan steps (with all the complexity these modifications would entail), there is little scope for optimisation. In the case where the truth value of p is dictated by timed initial literals (TILs), we have a more interesting case: changing the time-stamps of the start or the end of a (LP variables stepi and stepj), so that the condition is or is not satisfied, has a direct effect on the metric function. This relationship can be encoded within the LP. By way of example, consider the case where p becomes true at time a and false at b; then, again, becomes true at c and false at d. In this case, we have two time-windows that could potentially satisfy the condition on the effect. Whether the action has to wholly or only partially within one of these windows depends on the time-specifier attached to p:\n\u2022 if ts =at start, then a` (stepi) has to lie within one of the time windows;\n\u2022 if ts =at end, then aa (stepj) has to lie within one of the time windows;\n\u2022 otherwise, ts =over all, and both a` and aa have to lie within one of the time windows.\nIn all three cases, the question that must be answered is \u2018does the value of this variable lie within a known range?\u2019 \u2014 the over all case requires a conjunction of two such conditions to hold and, in the other two cases, only one has to hold. For a given step variable stepi, and time window (a, b), we can introduce into the (MI)LP a binary variable switchab corresponding to such an observation, with constraints that take the logical form:\nswitchab \u21d4 (stepi > a) \u2227 (stepi < b)\nThus, if the switch variable takes the value 1, the time-stamp of a point at which p is needed must fall within the time-window [a, b] and vice versa. By introducing two additional binary variables, denoted ga and lb, this logical constraint can be represented as a series of inequalities (using N to\ndenote a large number): stepi \u2212 (a+ )\u00d7 switchab \u2265 0 \u2212stepi + (b\u2212 )\u00d7 switchab \u2265 0 \u2212stepi +N \u00d7 ga \u2265 \u2212a stepi \u2212N \u00d7 lb \u2265 b switchab \u2212 ga\u2212 lb \u2265 \u22121\nThe first two constraints encode the forwards implication: if switchab is set to 1, then stepi has to lie in the range [a + , b \u2212 ] (a non-zero amount of separation, here epsilon, is needed under the PDDL semantics to avoid inspecting the value of p at the same time it is being changed by the TIL). The latter three constraints encode the reverse implication: if stepi is strictly greater than a and strictly less than b, then both of ga and lb have to hold the value 1 and thus, so does switchab.\nReturning to our example, where the time specifier is over all, and the windows are (a, b) and (c, d), the constraints that will be added are:\nswitchab1 \u21d4 (stepi > a) \u2227 (stepi < b) switchab2 \u21d4 (stepj > a) \u2227 (stepj < b) switchab \u21d4 (switchab1 \u2227 switchab2) switchcd1 \u21d4 (stepi > c) \u2227 (stepi < d) switchcd2 \u21d4 (stepj > c) \u2227 (stepj < d) switchcd \u21d4 (switchcd1 \u2227 switchcd2) switchp \u21d4 (switchab \u2228 switchcd)\nThat is, switchab is 1 if the entirety of the action do falls within (a, b), switchcd is 1 if it falls within (c, d) and switchp is 1 if either of these hold. This final switch variable is used to capture the benefit of the effect itself: if it holds the value 1, we increase the value of reward at the end of the plan by k, that is, we apply the conditional effect. The variable reward will already appear in the objective function in the form of the LP variable reward \u2032n, where n is the last step of the plan. Thus, we modify the constraints that define reward \u2032n so that k\u00d7 switchp is added to this value. This change will then ensure that the variable providing the value of reward in the objective function will include the reward of k if the condition on the time at which do was executed holds.\nGeneralising, we can extend this to the case where the conditional effect depends on the truth of a formula f consisting of a conjunction of time-specified propositional facts [(ts1 p1)...(tsj pj)]. For each (tsi pi) \u2208 f , we create constraints, as indicated above, so that the switch variable switchpi can only take the value 1 if pi holds at the time-specifier tsi. This gives us a list of switch variables s = [switchp1...switchpj ]. Then, to encode that in fact the conjunction f must hold, we create a variable switchf and add the constraints:\n\u2212j \u00d7 switchf + 1.switchp1 + ...+ 1.switchpj \u2265 0 switchf +\u22121.switchp1 + ...+\u22121.switchpj \u2265 1\u2212 j\nDefined thus, switchf takes the value of 1 iff each of the switch variables in s takes the value 1, which is precisely in the case that the conjunct is satisfied. Then, much as before, when updating the constraint dictating the value of the LP variable reward \u2032n, we add k \u00d7 switchf to this value.\nD.2 Optimising Numeric-Dependent Conditions\nPerhaps more complex than the case of time windows is where the conditions of a conditional effect depend on the values of the numeric variables in the domain. (The PDDL2.2 definition (Hoffmann\n& Edelkamp, 2005) does not include the case where TILs change the values of numeric variables11, so we do not consider that case here.) In the simple case, the time-specifier of all the numeric conditions is either at start or at end. More complicated is the case where one or more of the time-specifiers is over all. In this case, potentially, all the snap-actions in the plan, between the start and end of the action to which the condition belongs, could affect whether or not the condition associated with the effect is met. We must therefore check the status of the condition at each such point during execution of the action. Suppose we have an action O, where stepk and stepl are the variables denoting the start and end time-stamps of the action, and where O has a conditional effect with a numeric precondition in LNF:\nwhen (over all (>= (w \u00b7 v) c)) (at end (increase (reward) k)).\nTo encode this, we need to add constraints to ensure that this conditional outcome occurs iff w \u00b7 v \u2265 c at all times withinO. Since all change is linear, then (as with other over all conditions on O) we only need to check the values of the numeric variables immediately before and immediately after each action time step within O, and also immediately following the start and immediately before the end of O itself. Thus, the variables corresponding to values of v that we must examine are those in the list:\ne = [v\u2032k,vk+1,v \u2032 k+1, ...,vl\u22121,v \u2032 l\u22121,vl].\nAs stated earlier, the case of at start/at end conditions is somewhat easier: for at start, e = [vk], and for at end, e = [vl]. Irrespective of the time specifier, on the basis of this list e, to capture whether the condition is met, adding a switch variable switch to indicate whether the condition is met for all vectors, and switch variables [switcht1...switchtn] for each element [1..s] of the list e, indicating whether it is met for that single vector. The constraints over these (where \u03be is a small number) are then:\n\u2200 x\u2208[1...s] w.e[x] \u2265 \u2212N + (N + c)\u00d7 switch\n\u2200 x\u2208[1...s] w.e[x] \u2264 (c\u2212 \u03be) +N \u00d7 switchtx\ns\u00d7 switch +\u22121.switcht0 + ...+\u22121.switchts \u2265 1\u2212 s.\nThe first quantification ensures that if switch = 1, a lower bound of c is imposed on w \u00b7 v for each element of e. The second quantification ensures that if the vector v at index x in e satisfies w \u00b7 v \u2265 c, then the corresponding switch variable switchtx has to take the value 1. The third constraint ensures that if all such switch variables switchtx take the value 1, switch must, too, be set to 1. Having appropriately constrained the switch variable we can update the constraint governing the value of the LP variable reward \u2032n (the value of reward at the end of the plan) to increase it by the value of k \u00d7 switch .\nD.3 Optimising Time-Dependent Conditions\nThe final extension is to allow conditional effects to refer not only to the truth values of timed propositions, or the values of numeric variables, but also to the value of the duration of an action.\n11. Timed Initial Fluents have been used in some domain models, as an unofficial extension to the language. The semantics of such an extension are as straightforward as Timed Initial Literals.\nThis situation appears in our example airplane landing problem (Section 10.2) where the value (total-cost) is updated by a conditional effect, of which both the condition and the effect depend on ?duration. We will consider this example in order to show how the MILP can be extended to handle such updates. First, as in the previous cases, we need to add constraints to ensure that if the MILP solver chooses to obtain the conditioned outcome of a conditioned effect, then the condition must be met. So, for the example, we introduce a new variable binary switch variable for each condition, and some new constraints. For the land action for a plane ?p, starting and finishing at time-stamps action as stepn and stepm respectively, we add a pair of constrained switch variables. For the sake of this example we give these the meaningful names early and late . The constraints that are added to the LP are then:\ntarget p\u2212 stepm + stepn \u2264 N \u00d7 early stepm \u2212 stepn \u2264 N \u2212 (N + \u2212 target p)\u00d7 early \u2212target p + stepm \u2212 stepn \u2264 N \u00d7 late stepm \u2212 stepn \u2265 (target p + )\u00d7 late\nThese new constraints ensure that if the plane lands early, the variable early has to take the value 1, and vice versa. Similarly, if it lands late, late must take the value 1 and vice versa. In the case of our example, the conditional effects of the action are mutually exclusive, though this is not true in the general case.\nHaving defined the early and late switch variables, the objective function for the MILP must be augmented to reflect the conditional outcomes of the action. Two terms must be added \u2014 one for each switch variable \u2014 for the effect obtained if the switch variable is 1. Abbreviating the terms earlyPenaltyRate, latePenaltyRate and latePenalty to epr, lpr and lp, respectively, the objective terms for plane p are:\nearly \u00d7 (epr p)\u00d7 (target p\u2212 (stepm \u2212 stepn))\nlate \u00d7 (lpr p)\u00d7 ((stepm \u2212 stepn)\u2212 target p) + late \u00d7 (lp p)\nNote that unlike in the previous cases, this objective function is now quadratic: the objective contains terms where a switch variable is multiplied by both a constant and a step variable. This arises as, unlike in previous cases, the conditional effect is duration dependent \u2014 not a fixed, constant value k. Whilst this raises the computational cost of optimising the MILP, the cost is acceptable: it is only incurred once, after a solution plan has been found."}, {"heading": "Appendix E. Details of Empirical Evaluation of Colin", "text": "The graphs presented here show the detailed runtime and quality comparisons analysed in Section 12. The comparative data is graphed. Since the graphs sometimes superimpose curves over one another, making it difficult to see how COLIN is performing, Tables 6\u201313 show the raw time and quality results for COLIN compared with average and best times and qualities for all problems. Best times and qualities are also reported with the corresponding quality or time (respectively) for that solution, and the planner(s) that generated the best result."}], "references": [{"title": "A SAT-based approach for solving formulas over boolean and linear mathematical propositions", "author": ["G. Audemard", "P. Bertoli", "A. Cimatti", "A. Kornilowicz", "R. Sebastiani"], "venue": "In Proceedings of the 18th International Conference on Automated Deduction,", "citeRegEx": "Audemard et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Audemard et al\\.", "year": 2002}, {"title": "Fast Planning through Planning Graph Analysis", "author": ["A. Blum", "M. Furst"], "venue": "In Proceedings of the International Joint Conference on Artificial Inteligence (IJCAI)", "citeRegEx": "Blum and Furst,? \\Q1995\\E", "shortCiteRegEx": "Blum and Furst", "year": 1995}, {"title": "A New Method for the Global Solution of Large Systems of Continuous Constraints", "author": ["M.S. Boddy", "D.P. Johnson"], "venue": "In Proceedings of the 1st International Workshop on Global Constraint Optimization and Constraint Satisfaction (COCOS),", "citeRegEx": "Boddy and Johnson,? \\Q2002\\E", "shortCiteRegEx": "Boddy and Johnson", "year": 2002}, {"title": "Gaining Efficiency and Flexibility in the Simple Temporal Problem", "author": ["A. Cesta", "A. Oddi"], "venue": "In Proceedings of the 3rd International Workshop on Temporal Representation and Reasoning (TIME)", "citeRegEx": "Cesta and Oddi,? \\Q1996\\E", "shortCiteRegEx": "Cesta and Oddi", "year": 1996}, {"title": "Developing an End-to-End Planning Application from a Timeline Representation Framework", "author": ["A. Cesta", "G. Cortellessa", "S. Fratini", "A. Oddi"], "venue": "In Proceedings of 21st Conference on Innovative Applications of Artificial Intelligence (IA*AI)", "citeRegEx": "Cesta et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cesta et al\\.", "year": 2009}, {"title": "Timeline-Based Space Operations Scheduling with External Constraints", "author": ["S.A. Chien", "D. Tran", "G. Rabideau", "S.R. Schaffer", "D. Mandl", "S. Frye"], "venue": "In Proceedings of the International Conference on AI Planning and Scheduling (ICAPS),", "citeRegEx": "Chien et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chien et al\\.", "year": 2010}, {"title": "Planning via Model Checking: A Decision Procedure for R", "author": ["A. Cimatti", "F. Giunchiglia", "E. Giunchiglia", "P. Traverso"], "venue": "In Recent Advances in AI Planning,", "citeRegEx": "Cimatti et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cimatti et al\\.", "year": 1997}, {"title": "Managing concurrency in temporal planning using planner-scheduler interaction", "author": ["A.I. Coles", "M. Fox", "K. Halsey", "D. Long", "A.J. Smith"], "venue": "Artificial Intelligence,", "citeRegEx": "Coles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Coles et al\\.", "year": 2008}, {"title": "Planning with Problems Requiring Temporal Coordination", "author": ["A.I. Coles", "M. Fox", "D. Long", "A.J. Smith"], "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence", "citeRegEx": "Coles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Coles et al\\.", "year": 2008}, {"title": "A Hybrid Relaxed Planning GraphLP Heuristic for Numeric Planning Domains", "author": ["A.I. Coles", "M. Fox", "D. Long", "A.J. Smith"], "venue": "In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Coles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Coles et al\\.", "year": 2008}, {"title": "Extending the Use of Inference in Temporal Planning as Forwards Search", "author": ["A.J. Coles", "A.I. Coles", "M. Fox", "D. Long"], "venue": "In Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Coles et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Coles et al\\.", "year": 2009}, {"title": "Temporal Planning in Domains with Linear Processes", "author": ["A.J. Coles", "A.I. Coles", "M. Fox", "D. Long"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Coles et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Coles et al\\.", "year": 2009}, {"title": "When is temporal planning really temporal planning", "author": ["W. Cushing", "S. Kambhampati", "Mausam", "D. Weld"], "venue": "In Proceedings of the International Joint Conference on AI (IJCAI),", "citeRegEx": "Cushing et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cushing et al\\.", "year": 2007}, {"title": "Temporal Constraint Networks", "author": ["R. Dechter", "I. Meiri", "J. Pearl"], "venue": "In Proceedings of Principles of Knowledge Representation and Reasoning (KR),", "citeRegEx": "Dechter et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Dechter et al\\.", "year": 1989}, {"title": "Finding Optimal Plans for Domains with Restricted Continuous Effects with UPPAAL-Cora", "author": ["H. Dierks"], "venue": "ICAPS Workshop on Verification and Validation of Model-Based Planning and Scheduling Systems.", "citeRegEx": "Dierks,? 2005", "shortCiteRegEx": "Dierks", "year": 2005}, {"title": "Sapa: A Multi-objective Metric Temporal Planner", "author": ["M.B. Do", "S. Kambhampati"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Do and Kambhampati,? \\Q2003\\E", "shortCiteRegEx": "Do and Kambhampati", "year": 2003}, {"title": "Taming numbers and durations in a model-checking integrated planning system", "author": ["S. Edelkamp"], "venue": "Journal of Artificial Intelligence Research (JAIR), 20, 195\u2013238.", "citeRegEx": "Edelkamp,? 2003", "shortCiteRegEx": "Edelkamp", "year": 2003}, {"title": "Cost-Optimal External Planning", "author": ["S. Edelkamp", "S. Jabbar"], "venue": "In Proceedings of the 21st National (American) Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Edelkamp and Jabbar,? \\Q2006\\E", "shortCiteRegEx": "Edelkamp and Jabbar", "year": 2006}, {"title": "Using the Context-enhanced Additive Heuristic for Temporal and Numeric Planning", "author": ["P. Eyerich", "R. Mattm\u00fcller", "G. R\u00f6ger"], "venue": "In Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Eyerich et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Eyerich et al\\.", "year": 2009}, {"title": "PDDL2.1: An extension of PDDL for expressing temporal planning domains", "author": ["M. Fox", "D. Long"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Fox and Long,? \\Q2003\\E", "shortCiteRegEx": "Fox and Long", "year": 2003}, {"title": "Modelling Mixed Discrete-Continuous Domains for Planning", "author": ["M. Fox", "D. Long"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Fox and Long,? \\Q2006\\E", "shortCiteRegEx": "Fox and Long", "year": 2006}, {"title": "Validating Plans in the Context of Processes and Exogenous Events", "author": ["M. Fox", "R. Howey", "D. Long"], "venue": "In Proceedings of the 20th National Conference on Artificial Intelligence and the 17th Innovative Applications of Artificial Intelligence Conference (AAAI),", "citeRegEx": "Fox et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2005}, {"title": "An Investigation into the Expressive Power of PDDL2.1", "author": ["M. Fox", "D. Long", "K. Halsey"], "venue": "In Proceedings of the 16th European Conference of Artificial Intelligence (ECAI)", "citeRegEx": "Fox et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2004}, {"title": "Automatic Construction of Efficient Multiple Battery Usage Policies", "author": ["M. Fox", "D. Long", "D. Magazzeni"], "venue": "In Proceedings of the 21st International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Fox et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2011}, {"title": "A Temporal Planning System for Durative Actions of PDDL2.1", "author": ["A. Garrido", "M. Fox", "D. Long"], "venue": "In Proceedings of the 15th Eureopean Conference on Artificial Intelligence (ECAI),", "citeRegEx": "Garrido et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Garrido et al\\.", "year": 2002}, {"title": "A Temporal Planning System for Time-Optimal Planning", "author": ["A. Garrido", "E. Onainda", "F. Barber"], "venue": "In Proceedings of the 10th Portuguese Conference on Artificial Intelligence,", "citeRegEx": "Garrido et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Garrido et al\\.", "year": 2001}, {"title": "An Approach to Temporal Planning and Scheduling in Domains with Predictable Exogenous Events", "author": ["A. Gerevini", "A. Saetti", "I. Serina"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Gerevini et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gerevini et al\\.", "year": 2006}, {"title": "Temporal Planning with Problems Requiring Concurrency through Action Graphs and Local Search", "author": ["A. Gerevini", "A. Saetti", "I. Serina"], "venue": "In Proceedings of the 20th International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Gerevini et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gerevini et al\\.", "year": 2010}, {"title": "Fast Plan Adaptation through Planning Graphs: Local and Systematic Search Techniques", "author": ["A. Gerevini", "I. Serina"], "venue": "In Proceedings of the 5th International Conference on Artificial Intelligence Planning Systems (AIPS),", "citeRegEx": "Gerevini and Serina,? \\Q2000\\E", "shortCiteRegEx": "Gerevini and Serina", "year": 2000}, {"title": "Representation and Control in IxTeT, a Temporal Planner", "author": ["M. Ghallab", "H. Laruelle"], "venue": "In Proceedings of the 2nd International Conference on Artificial Intelligence Planning Systems (AIPS),", "citeRegEx": "Ghallab and Laruelle,? \\Q1994\\E", "shortCiteRegEx": "Ghallab and Laruelle", "year": 1994}, {"title": "CRIKEY!: It\u2019s co-ordination in temporal planning", "author": ["K. Halsey"], "venue": "Ph.D. thesis, University of Durham.", "citeRegEx": "Halsey,? 2005", "shortCiteRegEx": "Halsey", "year": 2005}, {"title": "Heuristic planning with time and resources", "author": ["P. Haslum", "H. Geffner"], "venue": "In Proceedings of the 6th European Conference on Planning", "citeRegEx": "Haslum and Geffner,? \\Q2001\\E", "shortCiteRegEx": "Haslum and Geffner", "year": 2001}, {"title": "Admissible Makespan Estimates for PDDL2.1 Temporal Planning", "author": ["P. Haslum"], "venue": "In Proceedings of the ICAPS Workshop on Heuristics for Domain-Independent Planning", "citeRegEx": "Haslum,? \\Q2009\\E", "shortCiteRegEx": "Haslum", "year": 2009}, {"title": "The Fast Downward Planning System", "author": ["M. Helmert"], "venue": "Journal of Artificial Intelligence (JAIR), 26, 191\u2013246.", "citeRegEx": "Helmert,? 2006", "shortCiteRegEx": "Helmert", "year": 2006}, {"title": "The Theory of Hybrid Automata", "author": ["T. Henzinger"], "venue": "Proceedings of the 11th Annual Symposium on Logic in Computer Science. Invited tutorial., pp. 278\u2013292. IEEE Computer Society Press.", "citeRegEx": "Henzinger,? 1996", "shortCiteRegEx": "Henzinger", "year": 1996}, {"title": "A user guide to HYTECH", "author": ["T. Henzinger", "Ho", "P.-H", "H. Wong-Toi"], "venue": "Tool and Algorithms for the Construction and Analysis of Systems: (TACAS 95),", "citeRegEx": "Henzinger et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Henzinger et al\\.", "year": 1995}, {"title": "The Metric-FF Planning System: Translating \u201cIgnoring Delete Lists\u201d to Numeric State Variables", "author": ["J. Hoffmann"], "venue": "Journal of Artificial Intelligence Research (JAIR), 20, 291\u2013341.", "citeRegEx": "Hoffmann,? 2003", "shortCiteRegEx": "Hoffmann", "year": 2003}, {"title": "The Deterministic Part of IPC-4: An Overview", "author": ["J. Hoffmann", "S. Edelkamp"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Hoffmann and Edelkamp,? \\Q2005\\E", "shortCiteRegEx": "Hoffmann and Edelkamp", "year": 2005}, {"title": "The FF planning system: Fast plan generation through heuristic search", "author": ["J. Hoffmann", "B. Nebel"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Hoffmann and Nebel,? \\Q2001\\E", "shortCiteRegEx": "Hoffmann and Nebel", "year": 2001}, {"title": "An Optimal Temporally Expressive Planner: Initial Results and Application to P2P Network Optimization", "author": ["R. Huang", "Y. Chen", "W. Zhang"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Power planning in the international space station domain", "author": ["R. Knight", "S. Schaffer"], "venue": "B.Clement", "citeRegEx": "Knight and Schaffer,? \\Q2009\\E", "shortCiteRegEx": "Knight and Schaffer", "year": 2009}, {"title": "A Method for Global Optimization of Large Systems of Quadratic Constraints", "author": ["N. Lamba", "M. Dietz", "D.P. Johnson", "M.S. Boddy"], "venue": "In Proceedings of the 2nd International Workshop on Global Optimization and Constraint Satisfaction (COCOS),", "citeRegEx": "Lamba et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lamba et al\\.", "year": 2003}, {"title": "Coordinating Agile Systems through the Model-based Execution of Temporal Plans", "author": ["T. L\u00e9aut\u00e9", "B. Williams"], "venue": "In Proceedings of the 20th National Conference on AI (AAAI)", "citeRegEx": "L\u00e9aut\u00e9 and Williams,? \\Q2005\\E", "shortCiteRegEx": "L\u00e9aut\u00e9 and Williams", "year": 2005}, {"title": "Generative systems for hybrid planning based on flow tubes", "author": ["H. Li", "B. Williams"], "venue": "In Proc. 18th Int. Conf. on Aut. Planning and Scheduling (ICAPS)", "citeRegEx": "Li and Williams,? \\Q2008\\E", "shortCiteRegEx": "Li and Williams", "year": 2008}, {"title": "Hybrid Planning with Temporally Extended Goals for Sustainable Ocean Observing", "author": ["H. Li", "B. Williams"], "venue": "In Proceedings of the International Conference of the Association for the Advancement of AI (AAAI): Special Track on Sustainability and AI", "citeRegEx": "Li and Williams,? \\Q2011\\E", "shortCiteRegEx": "Li and Williams", "year": 2011}, {"title": "Exploiting a Graphplan Framework in Temporal Planning", "author": ["D. Long", "M. Fox"], "venue": "In Proceedings of the 13th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Long and Fox,? \\Q2003\\E", "shortCiteRegEx": "Long and Fox", "year": 2003}, {"title": "The 3rd International Planning Competition: Results and Analysis", "author": ["D. Long", "M. Fox"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Long and Fox,? \\Q2003\\E", "shortCiteRegEx": "Long and Fox", "year": 2003}, {"title": "The Common Optimization INterface for Operations Research", "author": ["R. Lougee-Heimer"], "venue": "IBM Journal of Research and Development, 47(1), 57\u201366.", "citeRegEx": "Lougee.Heimer,? 2003", "shortCiteRegEx": "Lougee.Heimer", "year": 2003}, {"title": "Reasoning about Autonomous Processes in an Estimated Regression Planner", "author": ["D. McDermott"], "venue": "Proceedings of the 13th International Conference on Automated Planning and Scheduling (ICAPS).", "citeRegEx": "McDermott,? 2003", "shortCiteRegEx": "McDermott", "year": 2003}, {"title": "The 1998 AI Planning Systems Competition", "author": ["D.V. McDermott"], "venue": "AI Magazine, 21(2), 35\u201355.", "citeRegEx": "McDermott,? 2000", "shortCiteRegEx": "McDermott", "year": 2000}, {"title": "A Heuristic Search Approach to Planning with Continuous Resources in Stochastic Domains", "author": ["N. Meuleau", "E. Benazera", "R.I. Brafman", "E.A. Hansen", "Mausam"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Meuleau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 2009}, {"title": "Compiling Uncertainty Away in Conformant Planning Problems with Bounded Width", "author": ["H. Palacios", "H. Geffner"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Palacios and Geffner,? \\Q2009\\E", "shortCiteRegEx": "Palacios and Geffner", "year": 2009}, {"title": "ADL: Exploring the Middle Ground Between STRIPS and the Situation Calculus", "author": ["E.P.D. Pednault"], "venue": "Proceedings of the International Conference on Knowledge Representation (KR), pp. 324\u2013332.", "citeRegEx": "Pednault,? 1989", "shortCiteRegEx": "Pednault", "year": 1989}, {"title": "Robust Periodic Planning and Execution for Autonomous Spacecraft", "author": ["B. Pell", "E. Gat", "R. Keesing", "N. Muscettola", "B.D. Smith"], "venue": "In Proceedings of the International Joint Conference on AI (IJCAI),", "citeRegEx": "Pell et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pell et al\\.", "year": 1997}, {"title": "Temporal Planning with Continuous Change", "author": ["S. Penberthy", "D. Weld"], "venue": "In Proceedings of the 12th National Conference on AI (AAAI),", "citeRegEx": "Penberthy and Weld,? \\Q1994\\E", "shortCiteRegEx": "Penberthy and Weld", "year": 1994}, {"title": "UPMurphi: a Tool for Universal Planning on PDDL+ Problems", "author": ["G.D. Penna", "B. Intrigila", "D. Magazzeni", "F. Mercorio"], "venue": "In Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Penna et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Penna et al\\.", "year": 2009}, {"title": "A PDDL+ Benchmark Problem: The Batch Chemical Plant", "author": ["G.D. Penna", "B. Intrigila", "D. Magazzeni", "F. Mercorio"], "venue": "In Proceedings of the International Conference on AI Planning and Scheduling (ICAPS),", "citeRegEx": "Penna et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Penna et al\\.", "year": 2010}, {"title": "The LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks", "author": ["S. Richter", "M. Westphal"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Richter and Westphal,? \\Q2010\\E", "shortCiteRegEx": "Richter and Westphal", "year": 2010}, {"title": "Processes and Continuous Change in a SAT-based Planner", "author": ["J. Shin", "E. Davis"], "venue": "Artificial Intelligence,", "citeRegEx": "Shin and Davis,? \\Q2005\\E", "shortCiteRegEx": "Shin and Davis", "year": 2005}, {"title": "Temporal Planning with Mutual Exclusion Reasoning", "author": ["D. Smith", "D.S. Weld"], "venue": "In Proceedings of the 16th International Joint Conference on AI (IJCAI),", "citeRegEx": "Smith and Weld,? \\Q1999\\E", "shortCiteRegEx": "Smith and Weld", "year": 1999}, {"title": "Nonlinear planning with parallel resource allocation", "author": ["M. Veloso", "M. P\u00e9rez", "J. Carbonell"], "venue": "In Proceedings of the DARPA Workshop on Innovative Approaches to Planning, Scheduling and Control,", "citeRegEx": "Veloso et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Veloso et al\\.", "year": 1990}, {"title": "Branching and pruning: An optimal temporal POCL planner based on constraint programming", "author": ["V. Vidal", "H. Geffner"], "venue": "Artificial Intelligence,", "citeRegEx": "Vidal and Geffner,? \\Q2006\\E", "shortCiteRegEx": "Vidal and Geffner", "year": 2006}, {"title": "The LPSAT System and its Application to Resource Planning", "author": ["S. Wolfman", "D. Weld"], "venue": "In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Wolfman and Weld,? \\Q1999\\E", "shortCiteRegEx": "Wolfman and Weld", "year": 1999}, {"title": "UPPAAL in a Nutshell", "author": ["W. Yi", "K. Larsen", "P. Pettersson"], "venue": "International Journal of Software Tools for Technology Transfer,", "citeRegEx": "Yi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Yi et al\\.", "year": 1997}, {"title": "VHPOP: Versatile heuristic partial order planner", "author": ["H.L.S. Younes", "R.G. Simmons"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Younes and Simmons,? \\Q2003\\E", "shortCiteRegEx": "Younes and Simmons", "year": 2003}], "referenceMentions": [{"referenceID": 16, "context": "There has been considerable progress in the development of automated planning techniques for domains involving independent temporal and metric conditions and effects (Eyerich, Mattm\u00fcller, & R\u00f6ger, 2009; Coles, Fox, Long, & Smith, 2008a; Gerevini, Saetti, & Serina, 2006; Edelkamp, 2003; Coles, Fox, Long, & Smith, 2008b).", "startOffset": 166, "endOffset": 320}, {"referenceID": 49, "context": "1 is backward compatible with McDermott\u2019s PDDL (McDermott, 2000) and therefore supports ADL (Pednault, 1989).", "startOffset": 47, "endOffset": 64}, {"referenceID": 52, "context": "1 is backward compatible with McDermott\u2019s PDDL (McDermott, 2000) and therefore supports ADL (Pednault, 1989).", "startOffset": 92, "endOffset": 108}, {"referenceID": 5, "context": "Chien et al. (2010) describe the planner used to support operations on Earth Observing 1 (EO1), where the management of thermal energy generated by instruments is sufficiently important that the on-board planner uses some of its (highly constrained) CPU cycles to model and track its value.", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": "The battery management problem described by Fox et al. (2011) relies on a non-linear model,", "startOffset": 44, "endOffset": 62}, {"referenceID": 14, "context": "\u2022 Assignment of time-dependent costs as in the Aircraft Landing domain (Dierks, 2005), in which continuous processes govern the changing costs of the use of the runway as the landing time deviates from the optimal landing time for each aircraft.", "startOffset": 71, "endOffset": 85}, {"referenceID": 36, "context": "Most recent work on discrete numeric planning is built on the ideas introduced in the planner MetricFF (Hoffmann, 2003).", "startOffset": 103, "endOffset": 119}, {"referenceID": 31, "context": "A temporal heuristic effective for this form of temporal planning was developed by Haslum and Geffner (2001) and Vidal and Geffner (2006) have explored a constraint propagation approach to handling these problems.", "startOffset": 83, "endOffset": 109}, {"referenceID": 31, "context": "A temporal heuristic effective for this form of temporal planning was developed by Haslum and Geffner (2001) and Vidal and Geffner (2006) have explored a constraint propagation approach to handling these problems.", "startOffset": 83, "endOffset": 138}, {"referenceID": 18, "context": "Temporal Fast Downward (Eyerich et al., 2009), based on Helmert\u2019s Fast Downward planner (Helmert, 2006), uses an approach that is a slight refinement of the compressed action model, allowing some required concurrency to be managed.", "startOffset": 23, "endOffset": 45}, {"referenceID": 33, "context": ", 2009), based on Helmert\u2019s Fast Downward planner (Helmert, 2006), uses an approach that is a slight refinement of the compressed action model, allowing some required concurrency to be managed.", "startOffset": 50, "endOffset": 65}, {"referenceID": 29, "context": "A different approach to forward-search temporal planning is explored in the CRIKEY family of planners (Coles, Fox, Halsey et al., 2008; Coles, Fox, Long et al., 2008a). These planners use the same action splitting approach used in LPGP, but work with a heuristically guided forward search. The heuristics in these planners use a relaxed planning graph as a starting point (Hoffmann & Nebel, 2001), but extend it by adding some guidance about the temporal structure of the plan, pruning choices that can be easily demonstrated to violate temporal constraints and inferring choices where temporal constraints imply them. The planners use a Simple Temporal Network to model and solve the temporal constraints between the action end points as they are accumulated during successive action choices. Split actions have also been used to extend LPG into a temporal version that respects the semantics of PDDL2.1 (Gerevini, Saetti, & Serina, 2010) (earlier versions of LPG use the compressed action models described above). Recent work by Haslum (2009) has explored other ways in which heuristics for temporal planning can be constructed, while remaining admissible.", "startOffset": 115, "endOffset": 1045}, {"referenceID": 18, "context": "Temporal Fast Downward (Eyerich et al., 2009), based on Helmert\u2019s Fast Downward planner (Helmert, 2006), uses an approach that is a slight refinement of the compressed action model, allowing some required concurrency to be managed. The authors demonstrate that this planner can solve the Match problem shown in Figure 5. They mistakenly claim that SAPA cannot solve this problem because it cannot consider applying an action between starting and ending lighting the match: in fact, SAPA can apply the mend fuse action after the match is lit, in much the same way as is done in Temporal Fast Downward. The problem that both planners face is in situations in which an action must be started some time after the last happening, but before the next queued event: neither planner includes this choice in its search space. Huang et al. (2009) developed a temporal planner exploiting the planning-as-SATisfiability paradigm.", "startOffset": 24, "endOffset": 837}, {"referenceID": 7, "context": "A modified version of the LPGP action compilation (Long & Fox, 2003a) is used for this, as described by Coles et al. (2008). Each durative action a, of the form \u3008dur , pre`, eff `, pre\u2194, prea, eff a\u3009, is split into two non-temporal (in fact, instantaneous) \u2018snap actions\u2019 of the form \u3008pre, eff \u3009: \u2022 a` = \u3008pre`, eff `\u3009 \u2022 aa = \u3008prea, eff a\u3009", "startOffset": 104, "endOffset": 124}, {"referenceID": 34, "context": "Although problems exhibiting hybrid discrete-continuous dynamics have been studied in other research communities for some time, for example, in verification (Yi, Larsen, & Pettersson, 1997; Henzinger, Ho, & Wong-Toi, 1995; Henzinger, 1996), where timed automata capture exactly this kind of behaviour, there has been relatively little work on continuous dynamics in the planning community.", "startOffset": 157, "endOffset": 239}, {"referenceID": 34, "context": "This leads to a formal semantics that is based in the theory of Hybrid Automata (Henzinger, 1996).", "startOffset": 80, "endOffset": 97}, {"referenceID": 48, "context": "Another early planner to handle continuous processes is McDermott\u2019s OPTOP system (McDermott, 2003), which is a heuristic search planner, using a regression-based heuristic.", "startOffset": 81, "endOffset": 98}, {"referenceID": 34, "context": "This leads to a formal semantics that is based in the theory of Hybrid Automata (Henzinger, 1996). An action causes a discrete state change which might trigger a continuous process. This continues over time until an event is triggered leading into a new state. Some time later another action might be taken. Early work exploring planning with continuous processes includes the Zeno system of Penberthy and Weld (1994), in which processes are described using differential equations.", "startOffset": 81, "endOffset": 418}, {"referenceID": 43, "context": "The most relevant work in this area is in the development of the planner Kongming, described by Li and Williams. Kongming solves a class of control planning problems with continuous dynamics. It is based on the construction of fact and action layers and flow tubes, within the iterative plan graph structure introduced in Graphplan (Blum & Furst, 1995). As the graph is developed, every action produces a flow tube which contains the valid trajectories as they develop over time. Starting in a feasible region, actions whose preconditions intersect with the feasible region can be applied and the reachable states at any time point can be computed using the state equations of the system. In the initial state of the system all the variables have single known values. A valid trajectory must pass through a sequence of flow tubes, but must also meet the constraints specified in the dynamics of the actions selected. The mutex relation used in Graphplan is extended to the continuous dynamics as well as the propositional fragment of the language. The graph is iteratively extended as in Graphplan, with a search for a plan conducted after each successive extension. The plan-graph encoding of a problem with continuous dynamics is translated into a Mixed Logical-Quadratic Program (MLQP). The metric objective functions used by the planner to optimise its behaviour can be defined in terms of quadratic functions of state variables. An example problem considered by Li and Williams (2008) is a 2-d representation of a simple autonomous underwater vehicle (AUV) problem where the AUV can glide, ascend and descend while avoiding obstacles.", "startOffset": 96, "endOffset": 1490}, {"referenceID": 41, "context": "A good example of this is the work by Boddy and Johnson (2002) and colleagues (Lamba et al., 2003) on planning oil refinery operations.", "startOffset": 78, "endOffset": 98}, {"referenceID": 2, "context": "A good example of this is the work by Boddy and Johnson (2002) and colleagues (Lamba et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 14, "context": "The final domain we use is the Airplane Landing domain (Dierks, 2005), first posed as a challenge by Kim Larsen in his invited lecture at the 2009 International Conference on Automated Planning and Scheduling.", "startOffset": 55, "endOffset": 69}, {"referenceID": 27, "context": "One of the few planners that can also handle problems requiring temporal coordination is LPG-s (Gerevini et al., 2010).", "startOffset": 95, "endOffset": 118}, {"referenceID": 47, "context": "To evaluate whether or not this is appropriate (or whether always using an LP would be faster), and to compare the overheads of STP solving with LP solving on equivalent problems, we created a variant of COLIN that, at every state S, schedules the plan to reach S independently using three different schedulers: the original STP solver used in the standard version of COLIN, the equivalent LP solved using CPLEX (IBM ILOG CPLEX Optimization Studio) and the equivalent LP solved using CLP (Lougee-Heimer, 2003).", "startOffset": 488, "endOffset": 509}, {"referenceID": 30, "context": "We also use the Match-Lift and Driverlog Shift domains (Halsey, 2005).", "startOffset": 55, "endOffset": 69}, {"referenceID": 3, "context": "The STP solver used is the incremental STP algorithm due to Cesta and Oddi (1996), as previously used in CRIKEY3.", "startOffset": 60, "endOffset": 82}, {"referenceID": 36, "context": "The Relaxed Planning Graph (RPG) heuristic of Metric-FF (Hoffmann, 2003) has been the most popular numeric planning heuristic over the last decade, being widely used in many planners.", "startOffset": 56, "endOffset": 72}], "year": 2012, "abstractText": "In this paper we describe COLIN, a forward-chaining heuristic search planner, capable of reasoning with COntinuous LINear numeric change, in addition to the full temporal semantics of PDDL2.1. Through this work we make two advances to the state-of-the-art in terms of expressive reasoning capabilities of planners: the handling of continuous linear change, and the handling of duration-dependent effects in combination with duration inequalities, both of which require tightly coupled temporal and numeric reasoning during planning. COLIN combines FF-style forward chaining search, with the use of a Linear Program (LP) to check the consistency of the interacting temporal and numeric constraints at each state. The LP is used to compute bounds on the values of variables in each state, reducing the range of actions that need to be considered for application. In addition, we develop an extension of the Temporal Relaxed Planning Graph heuristic of CRIKEY3, to support reasoning directly with continuous change. We extend the range of task variables considered to be suitable candidates for specifying the gradient of the continuous numeric change effected by an action. Finally, we explore the potential for employing mixed integer programming as a tool for optimising the timestamps of the actions in the plan, once a solution has been found. To support this, we further contribute a selection of extended benchmark domains that include continuous numeric effects. We present results for COLIN that demonstrate its scalability on a range of benchmarks, and compare to existing state-of-the-art planners.", "creator": "TeX"}}}