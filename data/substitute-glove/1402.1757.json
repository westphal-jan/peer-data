{"id": "1402.1757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2014", "title": "Frequency-Based Patrolling with Heterogeneous Agents and Limited Communication", "abstract": "This paper prosecutes country - replacement frequencybased patrolling of intersecting, crosses graphs passed conditions days formula_16 nodes few non - gloves observance requirements when armed put limited ability any wanting. The oversee yet designs time a roof formula_2 Markov decision transition, and a reinforcement school indirect appears developed. Each jack generates its keeping policy however Markov chains, and policies which conveyed only when believed settlements the only, adjacent nodes. This constraint immediately critical exchange aerodynamic comparatively communication monitoring taking filled, unstructured environments. Empirical results to perspectives taking convergence assets, agent governmental, and generalization related 'd bombed policies hold house frequently fact full effective. The emergent behavior imply learned coordination focused between macromolecules informers for force generally, pur\u00e9e particularly as being was the much must rationalize although emphasis texture day node remarry purposes.", "histories": [["v1", "Fri, 7 Feb 2014 20:51:00 GMT  (535kb)", "http://arxiv.org/abs/1402.1757v1", null]], "reviews": [], "SUBJECTS": "cs.MA cs.AI", "authors": ["tao mao", "laura ray"], "accepted": false, "id": "1402.1757"}, "pdf": {"name": "1402.1757.pdf", "metadata": {"source": "CRF", "title": "Frequency-Based Patrolling with Heterogeneous Agents and Limited Communication", "authors": ["Tao Mao", "Laura E. Ray"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Frequency-based patrolling, multi-agent systems, decentralized reinforcement learning."}, {"heading": "1 Introduction", "text": "A persistent patrolling task can be described as the activity of repeatedly visiting specified points in space in order to monitor the surrounding environment or perform a specified function at each point visited. The patrolled area can be described as a graph, in which each vertex or node represents a point to be visited and edges represent possible paths. The problem can be cast according to characteristics of real tasks that are modeled; for example, the number of agents, the number of nodes, and node connectivity may be specified according to patrolling needs and environment constraints. In prior work, it has generally been assumed that each node should be visited with the same frequency, and the resulting multi-agent planning problem is thus characterized as a combinatorial optimization problem with one of several possible criteria defined to maintain uniform visitation; however, it is not always the case that uniform visitation is desirable. Nodes can have different visitation requirements owing to their prominence, patrolled site structure, or activity level within the region [1].\nThe patrolling problem can be modeled as a coverage problem on closed or open polygons. In prior work, every point on the polygon is generally treated as having equal visitation requirements, and the objective is to either minimize the frequency variance or maximize the average/minimal frequency [2, 3]. The intuitive solution, based on finding a closed polygon for all agents to patrol in the same direction, guarantees uniform visitation and avoids conflicts between agents [2~4]. Elmaliach et al. further develop solutions to this uniform patrolling problem for open polygons [2, 3]. Their work allows agents to overlap trajectories in space but not in time and gives an analytical solution of this model considering real-world uncertainties.\nGraph-based patrolling problems, which are abstracted from polygon continuous space patrolling, have also been proposed. In graph-based patrolling with uniform patrol rates for each node, the objective is to either maximize visitation frequency for every point [5] or minimize idle time for every point [6~8]. In models used in these papers, graph edges can have different lengths. For a single agent, the patrolling problem becomes a Travelling Salesman Problem [6]. In multi-agent cases, Hamilton circuit finding is given first priority [5, 6]; if Hamilton circuits do not exist, the algorithms in [5] find the longest path and include outliers, while partition-based strategies are applied in [6]. Reinforcement learning is applied in [7], which seeks an optimal graph partition for each member of the team to patrol. The graph model can be abstracted further in that patrolling efforts are focused on nodes, while edge lengths are ignored and synchronic node visits take place [9]. This abstraction is appropriate for problems involving patrolling of a large region, where the time spent at each node is substantial in comparison to travel time between nodes.\nIn this paper, we consider the problem of multi-agent patrolling in which the routes patrolled are modeled as undirected circle graphs that intersect occasionally, where one or more agents may patrol each route, and where communication between agents is sparse. As in [9], we focus on patrolled nodes, ignoring edge lengths and assuming synchronic node visitation. The problem is motivated by the need for coordination strategies between heterogeneous agents in patrolling large, unstructured regions. For example, in littoral patrolling, unmanned ground vehicles are constrained to operate on coastal roads, while unmanned\nrobotic boats may operate on the surface of the water, and unmanned submersibles operate at depths but can also surface. We assume arbitrary visitation frequency requirements for graph nodes.\nAn objective of solving the patrolling problem described above is to investigate robustness of patrolling policies to variations in the patrolling task, specifically the node visitation frequency requirements and the number of nodes on each graph. Additionally, we wish to examine dynamics that emerge when patrolling policies specific to a given graph model are applied to a variant of that model. For this reason, the problem is cast into a reinforcement learning (RL) framework [10]. The RL algorithm for the patrolling task requires little prior knowledge, and the learned policies are adaptable to the patrolling environment, static or dynamic. However, like other combinatorial optimization methods, RL described as in [11] suffers from \u201cthe curse of dimensionality\u201d as the number of nodes and/or agents increases. Thus, we use partially-observable states in order to manage the state space size. The reduction in state space size still permits convergence. Additionally, communication between agents is limited.\nThe paper is structured as follows: first, we formally define the multi-agent frequency-based patrolling problem targeted in this paper. Next, we review RL and partiallyobservable states, as applied to this problem. A policy generation method is developed; these policies are exchanged through sparse inter-agent communication to improve the team performance. Finally, we provide empirical results of convergence and communication rates for RL applied to multi-agent frequency-based patrolling. We also evaluate robustness and dynamics of patrolling scenarios in which learned policies generated under a static set of patrolling requirements are applied to a scenario with novel requirements due to variation in node visitation frequency requirements, or number of nodes."}, {"heading": "2 Problem definition", "text": "Definition 1 Graph. [12] A graph is an order pair G = (V, E) consisting of a set of n vertices (or nodes) V={1, 2, 3\u2026n} and a set of |E| edges: E \u2286 V\u00d7 V. An edge e is a pair e = (i, j) \u2208E. A graph G is undirected if \u2200 e = (i, j) \u2208E \u21d4 e\u2019 = (i, j) \u2208E. A graph G is directed if \u2203 e = (i, j) \u2208E such that e\u2019 = (i, j) \u2209 E\nIn the patrolling problem, a vertex represents a patrolled node and an edge a possible path choice for the agent.\nDefinition 2 General multi-agent patrolling problem. A group of r agents patrols an area represented by an undirected graph G = (V, E).\nAccording to definition 2, there are n nodes to be patrolled and |E| possible paths for r agents to move. We\ndefine the agent to be in position Xi if it is currently visited node i. A step of a patrolling agent in patrolling is a move from one node to an adjacent one.\nDefinition 3 Circle. [13] An undirected graph G = (V, E) is considered a circle C = (Vc,Ec) if it has only the set of edges E= {(i1,i2), (i2,i3)\u2026(in-1,in), (in,i1)}. For simplicity, but without loss of generality, we denote the set of vertex in the circle as Vc, and the set of edges as Ec= {(1,2), (2,3)\u2026(n-1,n), (n,1)}.\nDefinition 4 Multi-agent frequency-based patrolling. A group of r agents patrols an area represented by a graph that consists of one or more circles, where each node has a visitation frequency requirement, or patrol rate, Fi. One or more agents can patrol each circle.\nThe goal of the patrolling task is for the team of agents to minimize the differences between the actual visitation frequency fi and required frequency of visitation Fi for each node. The patrol route may consist of a single circle graph or multiple, intersecting circle graphs. We abstract dynamics of the vehicles by assuming that the time spent at each node is large compared to the travel time between nodes by each agent. Agents are assumed to communicate only when they occupy either the same node or adjacent nodes so as to model realistic communication limitations in large, unstructured environments."}, {"heading": "3 Reinforcement learning for multiagent Frequency based patrolling", "text": "The RL problem, which is Markov Decision Process, can be described as a tuple , , , ,R S A p \u03c1 , where R is the set of learning agents; S is the set of possible states; A is the set of actions that the agents can choose from; [ ]: 0,1p S A S\u00d7 \u00d7 \u2192 is the state transitional probability; : S A S\u03c1 \u00d7 \u00d7 \u2192 R is the reward function [10]. RL, Q-learning in particular, defines a state-action value :Q S A\u00d7 \u2192 R, which evaluates the value of a certain action taken under a state. Q-learning is a RL algorithm that aims to find satisfactory approximations for Q values during the learning process and uses these Q values to obtain the best action policies for a particular system. The state-action value Q is updated at the end of each episode, and the learned value function Qt approximates the optimal value function as the algorithm iterates if conditions on the learning rate are met [14]. We use \u03b5-greedy action-value method to determine next actions during learning. This method balances action policy exploration and exploitation.\nIn the multi-agent frequency-based patrolling problem described in the previous section, the agents can only estimate the frequencies of visitation of nodes and do not know the true values because they cannot obtain other agents\u2019 visitation histories except when communication is available. Also, if we choose a state space comprising every patrolled node, the\ndimension of the learning space increases exponentially with the number of nodes. Considering that each agent moves only to an adjacent node at each step, we propose a partiallyobservable state comprised of difference frequencies of four nodes nearest to the current node i occupied by an agent r:\n2 1 1 2, , ,i i i igs s f f f f \u2212 \u2212 \u2192 = \u2206 \u2206 \u2206 \u2206  (1)\n2if \u2212\u2206 , 1if \u2212\u2206 , 1if\u2206 , and 2if\u2206 are the difference between the required visitation frequency and the actual visitation frequency for the four nodes nearest to the currently occupied node i0. In eq. 1, sg is a global or environmental state while s is a partially-observable state, which we use to model the Markov decision process and train the RL algorithm, for its state space is considerably smaller than that of a global state. At each time step, agents may move either to the left or right, i.e., there are two actions to choose from.\nEach agent is provided with the target frequency requirements for all nodes at t = 0. During the learning phase, each agent maintains its own visitation history Hj. However, an agent does not know with certainty the visitation frequencies, histories, or policies of other agents. Communication assumptions described previously enable only sparse transmission of information between agents."}, {"heading": "4 Policy generation and exchange via inter-agent communication", "text": "Since communication is sparse, the information transmitted when communication occurs is important. The state representing the environment is partially observable; hence, the agents should exchange their visitation histories as well as their patrolling policies in order to reduce uncertainty in estimating the aggregate visitation frequencies in eq. 1 at certain time steps.\nThe theoretical framework for RL is based on properties of Markov Decision Processes. Thus, it is natural to define a policy as a Markov process, which, in the patrolling scenario, is defined as a Markov chain, or a stochastic process with Markov properties. It is assumed that the process has a finite set of states, in which one state transits to another only depending on its current state. The state used in the context of a Markov chain is the position Xi of an agent as in Def. 2, and not the concept of state used for RL. Given historical data, we model an agent\u2019s next state to be conditioned on the past states as first-order Markov chain:\n( ) ( )1 1Pr | Pr |t t ti iX X X X X X+ += = = (2)\nX is a series of states {Xt, Xt-1,\u2026, X1, X0}. From this equation, we define the transition probability as\n( )1Pr |t tij j ip X X X X+= = = (3)\nA transition matrix P for an agent is comprised of elements of probability pij. The agent updates its transition matrix P from its visitation history using a window of length w time steps. The initial calculation of P is performed after the first w time steps transpire.\nA policy \u03c0 is a stationary policy if it satisfies P\u03c0 \u03c0= For computation purpose, a stationary policy \u03c0 can be calculated as\nlim k k diag P\u03c0 \u2192\u221e  =    (4)\nwhere the diag[*] operator retrieves the diagonal elements of the matrix.\nSince each agent maintains their own transition policy \u03c0r, they can exchange policies and visitation histories when they communicate. The estimated visitation frequency of a given node can then be obtained both from the agent\u2019s own visitation history, other agent\u2019s histories when these are available through a prior communication, and the agent\u2019s beliefs of others\u2019 motions given policies \u03c0-r exchanged during communication. Note that when policies are exchanged, they are not necessarily steady-state policies.\nAt every step, an agent records its own current position, and estimates other agents\u2019 positions according to policies \u03c0-r exchanged from others. Using both true and estimated histories, the agent calculates visitation frequencies for all nodes. Since estimates of visitation frequencies are based on sampled data, a time window of wf steps is used to estimate these frequencies. Each agent estimates its current state se based on other agents\u2019 histories H-r of visitation frequencies (as estimated from exchanged histories and policies \u03c0-r) and its own recorded history H-r:\n2 1 1 2, , ,i i i ie e e e es f f f f \u2212 \u2212 = \u2206 \u2206 \u2206 \u2206  (5)\n2i ef \u2212\u2206 , 1 i ef \u2212\u2206 , 1 i ef\u2206 , and 2 i ef\u2206 are the difference between the required visitation frequency and the estimated visitation frequency for the four nodes nearest to the currently occupied node i0. Note that visitation histories H-r of other agents are initialized as null, and transitional matrices P-r are initialized as uniform prior to agent communication."}, {"heading": "5 Results", "text": "This section presents empirical results based upon two patrolling configurations \u2013 a benchmark configuration comprised of a single circle route with uniform patrol frequency requirements (Fig. 1a) and a more general configuration comprised of three intersecting circle graphs,\neach having a different number of patrolled nodes and nonuniform patrol frequency requirements (Fig. 1b). Each node has a specified component frequency Ci, and\n1\n1 n\ni\ni C = \u2264\u2211 (6)\nEq. 6 assures that the team has sufficient overall capacity to perform the patrolling task. Our notation for visitation frequency assumes that each agent has a capacity of 100%, and thus the total capacity is 100r%, where r is the number of agents. Then, the visitation frequency requirement for Node i is defined as 100 %i iF rC= .\nQ-learning is used in the learning phase, with an \u03b5greedy criterion for action selection. \u03b5 is initialized as 1 and exponentially decays to 0.01. An agent receives a reward of fi - Fi when it occupies Node i.\nTime window parameters w and wf are 1000 and 100 steps respectively. We assess the efficiency of the algorithm by measuring the difference between the required and actual patrol rate at each node as a function of learning step. We formally define the insufficiency of the patrol rate Ii for Node i as a performance metric:\n0 i i\ni F f I \u2212\n=  \n, 0 , 0 i i\ni i\nF f\nF f \u2212 > \u2212 \u2264\n(7)\nNote that this measure does not penalize the use of excess capacity, i.e., when the actual patrol frequency exceeds the desired patrol frequency, sufficiency requirements are met and thus insufficiency of patrol rate is zero."}, {"heading": "5.1 Single-circle multi-agent patrolling", "text": "The benchmark configuration of Fig. 1a has n = 20 nodes, r = 2 patrolling agents, component frequency Ci = 4.5% for each node, and uniform patrolling frequency requirements of Fi = 9% at each node; thus, the system is overmanned in the sense that collectively, agents have sufficient capacity to meet visitation frequency requirements. This configuration is designed to test the learning algorithm in the case where there exist multiple, obvious solutions. Here, the range of policies that meet a uniform patrol frequency requirement varies: agents each traversing the route in a single direction, splitting the graph equally between agents, or agents each making random moves with uniform probability would all comprise solutions meeting this requirement. In the absence of additional criteria, all solutions are satisficing, or acceptable solutions. A RL approach generally converges to more stochastic than deterministic solutions when two actions are selected with equal probability during learning exploration.\nWe perform 10 experimental runs of 106 learning steps each. In each run, agents learn a patrolling policy from scratch. Each learning step corresponds to a time step owing to abstraction of travel between nodes. Figure 2 shows mean and 1-sigma variation of iI n\u2211 over 10 experimental runs and 20 nodes. Figure 2 also shows the number of instances of communication between two or more agents out of the last 1000 steps averaged over 10 experimental runs. These measures are shown as a function of learning step. Results show that a patrolling policy that achieves a visitation frequency within 1% of the target frequencies over 10 experimental runs emerges, providing a satisficing policy in about 200,000 steps. Three stages of convergence are evident. In the \u201cadjustment\u201d stage (< 10,000 steps), agents explore a new environment, collect experience for learning, and communicate sparsely. In the \u201cconvergence\u201d stage (10,000 ~200,000 steps), learning starts to improve performance, reducing the average insufficiency of patrol rate. Finally, in the \u201cconverged\u201d stage (> 200,000 steps), a satisficing policy emerges, and there is little room for the two-agent group to\nimprove performance. Communication plays an important role in learning convergence as the increase in communication rate starting at ~10,000 steps helps the agents estimate their belief states with less uncertainty and makes Q-learning converge. In addition, the communication frequency has changed from random to more regular, as indicated by the communication pattern shown as the two small figures in Fig. 2. In the converged policy, each agent chooses to move left and right with nearly 50% frequency. Thus, each agent\u2019s policy comprises a roughly 50-50 chance of going left or right, which is a reasonable solution for this benchmark problem."}, {"heading": "5.2 Multi-circle multi-agent patrolling", "text": "In the general configuration, 20 nodes are distributed in three circles for three agents to patrol, as in Fig. 1b. Some nodes are common to two circles and appear twice in the figure. Component frequencies for the 20 nodes are assigned non-uniformly, while obeying the constraint imposed by eq. 6. The component frequencies are 4.33%, 4.33%, 2.67%, 2.67%, 2.67%, 6.00%, 6.00%, 6.00%, 4.33%, 2.67%, 6.33%, 4.67%, 3.00%, 6.33%, 4.67%, 4.67%, 6.33%, 4.67%, 4.67%, 3.00%, and required visitation frequencies are three times component frequencies. The sum of component frequencies is 90%, indicating that the team has sufficient capacity. In Fig. 1b, the length of the bars inside the circles associated with each node indicates the resulting Fi, as does the color, with blue indicating lowest Fi and red indicating highest Fi.\nWe perform 10 experimental runs of 106 steps each, in which agents learn from scratch for each run. Figure 3 shows mean and 1-sigma variation in the mean insufficiency of patrol rate averaged over 20 nodes and 10 experimental runs, as well as the number of instances of communication between two or more agents out of the last 1000 steps averaged over 10 experimental runs, as a function of learning step. A satisficing policy that achieves average insufficiency of patrol rate below 1% is learned in ~105 steps. As in the benchmark configuration, there are three stages in convergence of the learning algorithm. Improved convergence rate over the benchmark problem is attributed to the lower number of patrolled nodes per agent.\nFigure 4 shows snapshots of the visitation policy convergence for each patrolling agent in the form of a \u201cheat map\u201d designating the frequency of occupancy of each agent on the graph and a colored bar for each node. The color in a bar indicates the visitation requirement as specified in Fig. 1b. The length of color filled in each bar denotes the patrol rate agents provide to a node in order to fullfil the frequency requirement. For overlapped nodes, the sum of each agent\u2019s contribution to patrolling a node, denoted by the color of the associated point in the circle, produces the total actual visitation frequency. Prior to convergence, visitation requirements are not satisfied at every node, and agents do not exhibit strong cooperation. Subsequently, the agents begin to increase their communication rate as seen in Fig. 3;\ncooperation emerges; and the patrolling requirements are met. For instance, Agent 1 has the heaviest workload along its circle (11 nodes) and Agent 3 has a comparably lighter workload (8 nodes). Thus, in the overlapped nodes on these two graphs, Agent 3 takes most of the workload. Policy convergence is achieved with cooperation."}, {"heading": "5.3 Dynamic multi-agent patrolling", "text": "Due to the number of learning steps required to establish a patrolling policy, it is desirable that a learned policy generalize to variations in node visitation requirements and number of nodes. Here, we evaluate the transient dynamics and steady-state performance when the learned policy is applied to each of two variants of the general configuration. In the first variant, frequency requirements for Node 4 and Node 14 are swapped, with Node 4 requirements increasing from from 8% to 19% and Node 14 decreasing from 19% to 8%; and in the second, a scenario of adding and deleting a patrol node is obtained by setting the frequency requirement for Node 2 from 13% to zero and adding a new node Node 21 between Node 19 and 20 with a frequency requirement 14%.\nFigure 5 indicates transients in Di = fi \u2013 Fi, the difference between actual and required visitation frequency at each node for each experiment. In the first experiment (Fig. 5a), the agents reduce the frequency difference range from between -11.1% and 11.4% to between -2.1% and 5.3% within 100 steps. After 100 steps, most transients decay to within in the color range of green, yellow or red, indicating frequency requirements of these nodes are met or exceeded. In the second experiment (Fig. 5b), the agents reduce the frequency difference range from between -14% and 13.4% to between - 3.1% and 4.7% in 100 steps. After 100 steps, most transients decay to within in the color range of green, yellow or red, which indicates frequency requirements of these nodes are met. These examples indicate that the collective learned policies can be applied to a new instance of the task in a dynamic environment with new nodes.\nFigures 6 and 7 show dynamic evolution of the heat map for each of these experiments. In Fig. 6, cooperation emerges between Agents 1 and 2; due to workload reduction for Node 14, Agent 2 increases visitation of Node 11 to help Agent 1 meet the increased demand from Node 4. In Fig. 7, cooperation emerges between three agents: Agent 1 assists Agent 3 by assisting Agent 2. Due to zero workload for Node 2, Agent 1 applies effort to visit Nodes 9~11; then, Agent 2 has the capacity to help Agent 3 alleviate the workload on Nodes 16~18; finally, Agent 3 can handle new Node 21. In other words, indirect cooperation between Agent 1 and Agent\n3 emerges through Agent 2\u2019s involvement. Node 2, which requires no visits, is not avoided due to its physical existence in Circle 1; however, the visitation frequency for this node is reduced to 4%.\nComparison of steady-state heat maps for dynamic variations of the task from Fig. 6 and 7 with the heat map for the standard configuration at convergence of learning (Fig. 4, snapshot at step 500,000) shows that a range of dynamics can be elicited from a single-run learned policy."}, {"heading": "6 Conclusions", "text": "We apply Q-learning based on partially-observable states to a multi-agent frequency-based patrolling problem where frequency requirements for each node can be nonuniform, and communication is limited. Transition matrices describing the probability of state transitions are derived from Markov chains, and policies are exchanged when two agents occupy the same or adjacent nodes. In both a benchmark configuration with uniform patrol frequency requirements and a multi-graph patrol configuration with non-uniform requirements, multi-agent teams learn patrolling policies embodying cooperation. Empirical results demonstrate that the patrolling policies learned from static configurations can be applied to new instances of the patrolling task, with variations in the number of nodes or changes in patrol frequency requirements.\nThe relaxation of uniform frequency requirements and modeling of communication as sparse makes the model realistic for patrolling large, unstructured regions using heterogeneous agents. Additionally, the more stochastic policies derived from RL can make it difficult for external agents to predict patrolling agents\u2019 behaviors, which is a desirable feature for many applications."}, {"heading": "7 Acknowledgements", "text": "This work is supported by the Office of Naval Research under Multi-University Research Initiative (MURI) Grant No. N00014-08-1-0693."}, {"heading": "8 References", "text": "[1] Almeida, A., Ramalho, G., Santana, H., and et. al. \u201cRecent advances on multi-agent patrolling\u201d; Advances in Artificial Intelligence-SBIA, 3171, pp. 526-535, 2004.\n[2] Elmaliach, Y., Shiloni, A., and Kaminka, G. \u201cA realistic model of frequency-based multi-robot polyline patrolling\u201d;\nProceedings of the International Joint Conference on Autonomous Agents and Multi-Agent System, pp. 63-70, 2008.\n[3] Elmaliach, Y., Shiloni, A., and Kaminka, G. \u201cFrequency-based multi-robot fence patrolling\u201d; Technical Report, Computer Science Department, Bar Ilan University, Israel, 2008.\n[4] Elmaliach, Y., Agmon, N., and Kaminka, G. \u201cMultirobot area control under frequency constraints\u201d; Proceedings of IEEE International Conference on Robotics and Automation, pp. 385-390, 2007.\n[5] Portugal, D. and Rocha, R. \u201cMSP algorithm: multi-robot patrolling based on territory allocation using balanced graph partitioning\u201d; Proceedings of 25th ACM Symposium on Applied Computing, Special Track on Intelligent Robotic Systems, pp. 1271-1276, 2010.\n[6] Chevaleyre, Y. \u201cTheoretical analysis of the multi-agent patrolling problem\u201d; Proceedings of IEEE International Conference on Intelligent Agent Technology, pp. 302-308, 2004.\n[7] Perron, J., Moulin, B., Berger, J., and et. al. \u201cA hybrid approach based on multi-agent geosimulation and reinforcment learning to solve a UAV partolling problem\u201d; Proceedings of the 2008 Winter Simulation Conference, pp. 1259-1267. 2008.\n[8] Machado, A., Almeida, A., Ramaldo, G., and et. al. \u201cMulti-agent movement coordination in patrolling\u201d; Proceedings of 3rd Intl\u2019 Conf. on Computers and Games, 2002.\n[9] Baglietto, M., Cannata, G., Capezio, F., and Sgorbissa, A. \u201cMulti-robot uniform frequency coverage of significant locations in the environment\u201d; Distributed Autonomous Robotic Systems, Vol. 8, pp. 3-14, 2009\n[10] Sutton, R. S., and Barto, A. G. \u201cReinforcement Learning: An Introduction.\u201d Cambridge, MA: MIT Press, 1998.\n[11] Bellman R.E. \u201cDynamic Programming\u201d. Princeton, NJ: Princeton University Press. 1957.\n[12] Lovasz, L., Pelikan, J., and Vesztergombi, K. \u201cDiscrete Mathematics\u201d. New York, NY: Spring-Verlag Press, 2003.\n[13] Diestel, R. \u201cGraph Theory (translation of Graphentheorie)\u201d. New York, NY: Spring-Verlag Press, 1996.\n[14] Watkins, C. J. \u201cQ-learning\u201d; Journal of Machine Learning, Vol. 8, pp. 279-292. 1992."}], "references": [{"title": "Recent advances on multi-agent patrolling\u201d", "author": ["A. Almeida", "G. Ramalho", "H. Santana", "et. al"], "venue": "Advances in Artificial Intelligence-SBIA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A realistic model of frequency-based multi-robot polyline patrolling\u201d", "author": ["Y. Elmaliach", "A. Shiloni", "G. Kaminka"], "venue": "Proceedings of the International Joint Conference on Autonomous Agents and Multi-Agent System,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Frequency-based multi-robot fence patrolling\u201d", "author": ["Y. Elmaliach", "A. Shiloni", "G. Kaminka"], "venue": "Technical Report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Multirobot area control under frequency constraints\u201d", "author": ["Y. Elmaliach", "N. Agmon", "G. Kaminka"], "venue": "Proceedings of IEEE International Conference on Robotics and Automation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "MSP algorithm: multi-robot patrolling based on territory allocation using balanced graph partitioning\u201d", "author": ["D. Portugal", "R. Rocha"], "venue": "Proceedings of 25th ACM Symposium on Applied Computing, Special Track on Intelligent Robotic Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Theoretical analysis of the multi-agent patrolling problem\u201d", "author": ["Y. Chevaleyre"], "venue": "Proceedings of IEEE International Conference on Intelligent Agent Technology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A hybrid approach based on multi-agent geosimulation and reinforcment learning to solve a UAV partolling problem\u201d", "author": ["J. Perron", "B. Moulin", "J. Berger", "et. al"], "venue": "Proceedings of the 2008 Winter Simulation Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Multi-agent movement coordination in patrolling\u201d", "author": ["A. Machado", "A. Almeida", "G. Ramaldo", "et. al"], "venue": "Proceedings of 3rd Intl\u2019 Conf. on Computers and Games,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Multi-robot uniform frequency coverage of significant locations in the environment\u201d", "author": ["M. Baglietto", "G. Cannata", "F. Capezio", "A. Sgorbissa"], "venue": "Distributed Autonomous Robotic Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Reinforcement Learning: An Introduction.", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Discrete Mathematics", "author": ["L. Lovasz", "J. Pelikan", "K. Vesztergombi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Graph Theory (translation of Graphentheorie)", "author": ["R. Diestel"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Nodes can have different visitation requirements owing to their prominence, patrolled site structure, or activity level within the region [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "In prior work, every point on the polygon is generally treated as having equal visitation requirements, and the objective is to either minimize the frequency variance or maximize the average/minimal frequency [2, 3].", "startOffset": 209, "endOffset": 215}, {"referenceID": 2, "context": "In prior work, every point on the polygon is generally treated as having equal visitation requirements, and the objective is to either minimize the frequency variance or maximize the average/minimal frequency [2, 3].", "startOffset": 209, "endOffset": 215}, {"referenceID": 1, "context": "further develop solutions to this uniform patrolling problem for open polygons [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 2, "context": "further develop solutions to this uniform patrolling problem for open polygons [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 4, "context": "In graph-based patrolling with uniform patrol rates for each node, the objective is to either maximize visitation frequency for every point [5] or minimize idle time for every point [6~8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "For a single agent, the patrolling problem becomes a Travelling Salesman Problem [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "In multi-agent cases, Hamilton circuit finding is given first priority [5, 6]; if Hamilton circuits do not exist, the algorithms in [5] find the longest path and include outliers, while partition-based strategies are applied in [6].", "startOffset": 71, "endOffset": 77}, {"referenceID": 5, "context": "In multi-agent cases, Hamilton circuit finding is given first priority [5, 6]; if Hamilton circuits do not exist, the algorithms in [5] find the longest path and include outliers, while partition-based strategies are applied in [6].", "startOffset": 71, "endOffset": 77}, {"referenceID": 4, "context": "In multi-agent cases, Hamilton circuit finding is given first priority [5, 6]; if Hamilton circuits do not exist, the algorithms in [5] find the longest path and include outliers, while partition-based strategies are applied in [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "In multi-agent cases, Hamilton circuit finding is given first priority [5, 6]; if Hamilton circuits do not exist, the algorithms in [5] find the longest path and include outliers, while partition-based strategies are applied in [6].", "startOffset": 228, "endOffset": 231}, {"referenceID": 6, "context": "Reinforcement learning is applied in [7], which seeks an optimal graph partition for each member of the team to patrol.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "The graph model can be abstracted further in that patrolling efforts are focused on nodes, while edge lengths are ignored and synchronic node visits take place [9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 8, "context": "As in [9], we focus on patrolled nodes, ignoring edge lengths and assuming synchronic node visitation.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "For this reason, the problem is cast into a reinforcement learning (RL) framework [10].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "[12] A graph is an order pair G = (V, E) consisting of a set of n vertices (or nodes) V={1, 2, 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] An undirected graph G = (V, E) is considered a circle C = (Vc,Ec) if it has only the set of edges E= {(i1,i2), (i2,i3).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "reward function [10].", "startOffset": 16, "endOffset": 20}], "year": 2011, "abstractText": "This paper investigates multi-agent frequencybased patrolling of intersecting, circle graphs under conditions where graph nodes have non-uniform visitation requirements and agents have limited ability to communicate. The task is modeled as a partially observable Markov decision process, and a reinforcement learning solution is developed. Each agent generates its own policy from Markov chains, and policies are exchanged only when agents occupy the same or adjacent nodes. This constraint on policy exchange models sparse communication conditions over large, unstructured environments. Empirical results provide perspectives on convergence properties, agent cooperation, and generalization of learned patrolling policies to new instances of the task. The emergent behavior indicates learned coordination strategies between heterogeneous agents for patrolling large, unstructured regions as well as the ability to generalize to dynamic variation in node visitation", "creator": "PDFCreator Version 0.9.6"}}}