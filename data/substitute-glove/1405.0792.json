{"id": "1405.0792", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2014", "title": "On Exact Learning Monotone DNF from Membership Queries", "abstract": "In certainly papers, way suggests the happens that understand which monotone DNF turn at most $ s $ terms of size (unlike whose vector following seven position) there indeed $ tom $ ($ stands $ for $ calif $ - MDNF) from maintains twitter. This problem however varies give their too with classes a general hypergraph such hyperedge - radiological pertinent, in has motivated which applications widespread in waste toxicity and genomics sequencing.", "histories": [["v1", "Mon, 5 May 2014 06:49:05 GMT  (14kb)", "http://arxiv.org/abs/1405.0792v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hasan abasi", "nader h bshouty", "hanna mazzawi"], "accepted": false, "id": "1405.0792"}, "pdf": {"name": "1405.0792.pdf", "metadata": {"source": "CRF", "title": "On Exact Learning Monotone DNF from Membership Queries", "authors": ["Hasan Abasi", "Nader H. Bshouty"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n07 92\nv1 [\ncs .L\nG ]\nWe first present new lower bounds for this problem and then present deterministic and randomized adaptive algorithms with query complexities that are almost optimal. All the algorithms we present in this paper run in time linear in the query complexity and the number of variables n. In addition, all of the algorithms we present in this paper are asymptotically tight for fixed r and/or s."}, {"heading": "1 Introduction", "text": "We consider the problem of learning a monotone DNF with at most s terms, where each monotone term contains at most r variables (s term r-MDNF) from membership queries [1]. This is equivalent to the problem of learning a general hypergraph using hyperedge-detecting queries, a problem that is motivated by applications arising in chemical reaction and genome sequencing."}, {"heading": "1.1 Learning Hypergraph", "text": "A hypergraph is H = (V,E) where V is the set of vertices and E \u2286 2V is the set of edges. The dimension of the hypergraph H is the cardinality of\nthe largest set in E. For a set S \u2286 V , the edge-detecting queries QH(S) is answered \u201cYes\u201d or \u201cNo\u201d, indicating whether S contains all the vertices of at least one edge of H. Our learning problem is equivalent to learning a hidden hypergraph of dimension r using edge-detecting queries.\nThis problem has many applications in chemical reactions and genome sequencing. In chemical reactions, we are given a set of chemicals, some of which react and some which do not. When multiple chemicals are combined in one test tube, a reaction is detectable if and only if at least one set of the chemicals in the tube reacts. The goal is to identify which sets react using as few experiments as possible. The time needed to compute which experiments to do is a secondary consideration, though it is polynomial for the algorithms we present [5]. See [13, 7, 3, 2, 4, 5] for more details and other applications."}, {"heading": "1.2 Previous Results", "text": "In [5], Angluin and Chen presented an deterministic optimal adaptive learning algorithm for learning s-term 2-MDNF. They also gave a lower bound of \u2126((2s/r)r/2 + rs logn) for learning the class of s-term r-MDNF when r < s. In [4], Angluin and Chen gave a randomized algorithm for s-term r-uniform MDNF (the size of each term is exactly r) that asks O(24rs \u00b7 poly(r, log n)) membership queries. For s-term r-MDNF where r \u2264 s, they gave a randomized learning algorithm that asks O(2r+r\n2/2s1+r/2 \u00b7 poly(log n)) membership queries.\nLiterature has also addressed learning some subclasses of s-term 2-MDNF. Those classes have specific applications to genome sequencing. See [13, 7, 3, 2, 4, 5]. In this paper we are interested in learning the class of all s-term r-MDNF formulas for any r and s."}, {"heading": "1.3 Our Results", "text": "In this paper, we distinguish between two cases: s \u2265 r and s < r. For s < r, we first prove the lower bound O((r/s)s\u22121 + rs log n). We then give three algorithms. Algorithm I is a deterministic algorithm that asks O(rs\u22121 + rs log n) membership queries. Algorithm II is a deterministic algorithm that asks O(s \u00b7 N((s \u2212 1; r); sr) + rs log n) membership queries where N((s \u2212 1; r); sr) is the size of (sr, (s \u2212 1, r))-cover free family (see Subsection 2.2 for the definition of cover free) that can be constructed in time linear in its size. An (sr, (s\u22121, r))-cover free family of size (r/s)s\u22121+o(1) is known to exist. For some r and s (for example r = o(s log s log log s)),\nsuch a bound can be achieved in linear time and therefore for those cases, algorithm II is almost optimal. Algorithm III is a randomized algorithm that asks\nO\n((\ns+ r\ns\n)\u221a sr log(sr) + rs log n ) = O ( (r\ns\n)s\u22121+o(1) + rs log n\n)\nmembership queries. This algorithm is almost optimal. For the case s \u2265 r, Angluin and Chen, [5], gave the lower bound \u2126((2s/r)r/2 + rs log n). We give two algorithms that are almost tight. The first algorithm, Algorithm IV, is a deterministic algorithm that asks (crs)r/2+1.5 + rs logn membership queries for some constant c. The second algorithm, Algorithm V, is a randomized algorithm that asks (c\u2032s)r/2+0.75+ rs log n membership queries for some constant c\u2032.\nAll the algorithms we present in this paper run in time linear in the query complexity and n. Additionally, all the algorithms we describe in this paper are asymptotically tight for fixed r and s.\nThe following table summarizes our results. We have removed the term rs log n from all the bounds to be able to fit this table in this page. Det. and Rand. stands for deterministic algorithm and randomized algorithm, respectively.\nLower Bound Rand./ Upper Bound r, s rs log n+ Algorithm Det. rs log n+ r > s (\nr s\n)s\u22121 Alg. I Det. rs\u22121\nAlg. II Det. s \u00b7N((s \u2212 1; r); sr) Alg. III Rand. (log r) \u221a ses (\nr s + 1\n)s\nr \u2264 s ( 2s r )r/2 Alg. IV. Det. (3e)r(rs)r/2+1.5\nAlg. IV. Rand. \u221a r(3e)r(log s)sr/2+1"}, {"heading": "2 Definitions and Notations", "text": "For a vector w, we denote by wi the ith entry of w. For a positive integer j, we denote by [j] the set {1, 2, . . . , j}.\nLet f(x1, x2, . . . , xn) be a Boolean function from {0, 1}n to {0, 1}. For an assignment a \u2208 {0, 1}n we say that f is \u03be in a (or a is \u03be in f) if f(a) = \u03be. We say that a is zero in xi if ai = 0. For a set of variables S, we say that a is zero in S if for every xi \u2208 S, a is zero in xi. Denote Xn = {x1, . . . , xn}.\nFor a Boolean function f(x1, . . . , xn), 1 \u2264 i1 < i2 < \u00b7 \u00b7 \u00b7 < ik \u2264 n and \u03c31, . . . , \u03c3k \u2208 {0, 1} we denote by\nf |xi1=\u03c31,xi2=\u03c32,\u00b7\u00b7\u00b7 ,xik=\u03c3k\nthe function f when fixing the variables xij to \u03c3j for all j \u2208 [k]. We denote by a|xi1=\u03c31,xi2=\u03c32,\u00b7\u00b7\u00b7 ,xik=\u03c3k the assignment a where each aij is replaced by \u03c3j for all j \u2208 [k]. Note that\nf |xi1=\u03c31,xi2=\u03c32,\u00b7\u00b7\u00b7 ,xik=\u03c3k(a) = f(a|xi1=\u03c31,xi2=\u03c32,\u00b7\u00b7\u00b7 ,xik=\u03c3k).\nWhen \u03c31 = \u00b7 \u00b7 \u00b7 = \u03c3k = \u03be and S = {xi1 , . . . , xik}, we denote\nf |xi1=\u03c31,xi2=\u03c32,\u00b7\u00b7\u00b7 ,xik=\u03c3k\nby f |S\u2190\u03be. In the same way, we define a|S\u2190\u03be. We denote by 1n = (1, 1, . . . , 1) \u2208 {0, 1}n.\nFor two assignments a, b \u2208 {0, 1}n, we write a \u2264 b if for every i, ai \u2264 bi. A Boolean function f : {0, 1}n \u2192 {0, 1} is monotone if for every two assignments a, b \u2208 {0, 1}n, if a \u2264 b then f(a) \u2264 f(b). Recall that every monotone Boolean function f has a unique representation as a reduced monotone DNF. That is, f = M1 \u2228M2 \u2228 \u00b7 \u00b7 \u00b7 \u2228Ms where each monomial Mi is an ANDs of input variables, and for every monomial Mi there is a unique assignment a(i) such that f(a(i)) = 1 and for every j \u2208 [n] where a(i)j = 1 we have f(a(i)|xj=0) = 0. We call such assignment a minterm of the function f . Notice that every monotone DNF can be uniquely determined by its minterms.\nFor a monotone DNF, f(x1, x2, . . . , xn) = M1 \u2228 M2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 Ms, and a variable xi, we say that xi is t-frequent if it appears in more than or equal to t terms. A monotone DNF f is called read k monotone DNF, if none of its variables is k + 1-frequent."}, {"heading": "2.1 Learning Model", "text": "Consider a teacher (or a black box) that has a target function f : {0, 1}n \u2192 {0, 1} that is s-term r-MDNF. The teacher can answer membership queries. That is, when receiving a \u2208 {0, 1}n it returns f(a). A learning algorithm is an algorithm that can ask the teacher membership queries. The goal of the learning algorithm is to exactly learn (exactly find) f with minimum number of membership queries and optimal time complexity.\nIn our algorithms, for a function f we will denote byMQf the oracle that answers the membership queries. That is, for a \u2208 {0, 1}n, MQf (a) = f(a)."}, {"heading": "2.2 Cover-Free Families", "text": "The problem (n, (s, r))-cover-free family [12] is equivalent to the following problem: A (n, (s, r))-cover-free family is a set A \u2286 {0, 1}n such that for\nevery 1 \u2264 i1 < i2 < \u00b7 \u00b7 \u00b7 < id \u2264 n where d = s + r and every J \u2286 [d] of size |J | = s there is a \u2208 A such that aik = 0 for all k \u2208 J and aij = 1 for all j 6\u2208 J . Denote by N((s; r);n) the minimum size of such set. The lower bounds in [16] are\nN((s; r);n) \u2265 \u2126 ( (s+ r)\nlog (s+r\ns\n)\n(\ns+ r\ns\n)\nlog n\n)\n.\nIt is known that a set of random\nm = O\n(\n\u221a\nmin(r, s)\n(\ns+ r\ns\n)(\n(s + r) log n+ log 1\n\u03b4\n))\n(1)\nvectors a(i) \u2208 {0, 1}n, where each a(i)j is 1 with probability r/(s + r), is a (n, (s, r))-cover free family with probability at least 1\u2212 \u03b4.\nIn [8], Bshouty gave a deterministic construction of (n, (s, r))-CFF of size\nC := min((2e)srs+3, (2e)rsr+3) log n\n=\n(\ns+ r\nr\n)\n2min(s log s,r log r)(1+o(1)) log n (2)\nthat can be constructed in time C \u00b7n. Fomin et. al. in [11] gave a construction of size\nD :=\n(\ns+ r\nr\n)\n2 O ( r+s log log(r+s) ) log n (3)\nthat can be constructed in time D \u00b7n. The former bound, (2), is better than the latter when s \u2265 r log r log log r or r \u2265 s log s log log s. We also note that the former bound, (2), is almost optimal, i.e.,\n(\ns+ r\nr\n)1+o(1)\nlog n,\nwhen r = s\u03c9(1) or r = so(1) and the latter bound, (3), is almost optimal when\no(s log log s log log log s) = r = \u03c9\n(\ns\nlog log s log log log s\n)\n."}, {"heading": "3 Lower Bounds", "text": "In this section, we prove some lower bounds."}, {"heading": "3.1 General Lower Bound", "text": "In this section, we prove that the information theoretic lower bound for learning a class C from membership queries is also a lower bound for any randomized learning algorithm. We believe it is a folklore result, but we could not find the proof in the literature. We first state the following informationtheoretic lower bound for deterministic learning algorithm,\nLemma 1. Let C be any class of Boolean function. Then any deterministic learning algorithm for C must ask at least log |C| membership queries.\nWe now prove,\nLemma 2. Let C be any class of boolean function. Then any Monte Carlo (and therefore, Las Vegas) randomized learning algorithm that learns C with probability at least 3/4 must ask at least log |C| \u2212 1 membership queries.\nProof. Let A be a randomized algorithm that for every f \u2208 C and an oracle MQf that answers membership queries for f , asks m membership queries and satisfies\nPrs[A(MQf , s) = f ] \u2265 3\n4\nwhere s \u2208 {0, 1}N is chosen randomly uniformly for some large N . Consider the random variable Xf (s) that is 1 if A(MQf , s) = f and 0, otherwise. Then for every f , Es[Xf ] \u2265 3/4. Therefore, for random uniform f \u2208 C\n3/4 \u2264 Ef [Es[Xf ]] = Es[Ef [Xf (s)]].\nand by Markov Bound for at least 1/2 of the elements s \u2208 {0, 1}N we have Ef [Xf (s)] \u2265 1/2. Let S \u2286 {0, 1}N be the set of such elements. Then |S| \u2265 2N/2. Let s0 \u2208 S and Cs0 \u2286 C the class of functions f where Xf (s0) = 1. Then |Cs0 | \u2265 |C|/2 and A(MQf , s0) is a deterministic algorithm that learns the class Cs0 . Using the information theoretic lower bound for deterministic algorithm, we conclude that A(MQf , s0) must ask at least\nm \u2265 log |Cs0 | = log(1/2) + log |C|\nmembership queries.\nSpecifically, we have,\nCorollary 3. Any Monte Carlo (and therefore Las Vegas) randomized learning algorithm for the class of s-term r-MDNF must ask on average at least rs log n membership queries."}, {"heading": "3.2 Two Lower Bounds", "text": "In this section, we give two lower bounds. The first is from [4] and the second follows using the same techniques used in [9].\nIn [4], Angluin and Chen proved,\nTheorem 4. Let r and s be integers. Let k and \u2113 be two integers such that\n\u2113 \u2264 r, s \u2265 ( k\n2\n)\n\u2113+ 1.\nAny (Monte Carlo) randomized learning algorithm for the class of s-term r-MDNF must ask at least k\u2113 \u2212 1 membership queries.\nSpecifically, when s >> r we have the lower bound\n\u2126\n(\n(\n2s\nr\n)r/2 )\nmembership queries. Also, for any integer \u03bb where\n(\n\u03bb\n2\n) r + 1 \u2264 s < ( \u03bb+ 1\n2\n)\nr\nwe have the lower bound \u03bbr \u2212 1.\nWe now prove the following lower bound,\nTheorem 5. Let r and s be integers and \u2113 and t be two integers such that\n\u2113\u2212 \u230a \u2113\nt\n\u230b \u2264 r, \u230a \u2113\nt\n\u230b\n\u2264 s\u2212 1.\nAny (Monte Carlo) randomized learning algorithm for the class of s-term r-MDNF must ask at least t\u230a\u2113/t\u230b membership queries.\nSpecifically, for r >> s we have the lower bound\n(r\ns\n)s\u22121 .\nand for any constant integer \u03bb and \u03bbs \u2264 r < (\u03bb + 1)s we have the lower bound\n(\u03bb+ 1)s\u22121.\nProof. Let m = \u230a\u2113/t\u230b. Consider the monotone terms Mj = x(j\u22121)t+1 \u00b7 \u00b7 \u00b7 xjt for j = 1, 2, . . . ,m. Define Mi,k where i = 1, . . . ,m and k = 1, . . . , t the monotone term Mi without the variable x(i\u22121)t+k . Let Mk1,k2,...,km = M1,k1M2,k2 \u00b7 \u00b7 \u00b7Mm,km . The only way we can distinguish between the two hypothesis f = M1\u2228M2\u2228 \u00b7 \u00b7 \u00b7 \u2228Mm and g = M1\u2228M2\u2228 \u00b7 \u00b7 \u00b7 \u2228Mm\u2228Mk1,k2,...,km is by guessing an assignment that is 1 in all its first mt entries except for the entire k1, t+k2, 2t+k3, . . . , (m\u22121)t+km. That is, by guessing k1, k2, . . . , km. This takes an average of tm guesses. Since both f and g are s-term r-MDNF, the result follows.\nFor r >> s, we choose \u2113 = r and t such that \u230a\u2113/t\u230b = s \u2212 1. Since s\u2212 1 = \u230a\u2113/t\u230b \u2265 \u2113/t\u2212 1, we have t \u2265 r/s and the result follows.\nFor \u03bbs \u2264 r < (\u03bb+ 1)s, proving the lower bound for r = \u03bbs is sufficient. Take t = \u03bb+ 1 and \u2113 = (\u03bb+ 1)s \u2212 1."}, {"heading": "4 Optimal Algorithms for Monotone DNF", "text": "In this section, we present the algorithms (Algorithm I-V) that learn the class of s-term r-MDNF. We first give a simple algorithm that learns one term. We then give three algorithms (Algorithm I-III) for the case r > s and two algorithms (Algorithm IV-V) for the case s \u2265 r."}, {"heading": "4.1 Learning One Monotone Term", "text": "In this section, we prove the following result.\nLemma 6. Let f(x) = M1\u2228M2\u2228\u00b7 \u00b7 \u00b7\u2228Ms be the target function where each Mi is a monotone term of size at most r. Suppose g(x) = M1\u2228M2\u2228\u00b7 \u00b7 \u00b7\u2228Ms\u2032 and h(x) = Ms\u2032+1\u2228Ms\u2032+2\u2228\u00b7 \u00b7 \u00b7\u2228Ms. If a is an assignment such that g(a) = 0 and h(a) = 1, then a monotone term in h(x) can be found with\nO ( r log n\nr\n)\nmembership queries.\nProof. First notice that since g is monotone, for any b \u2264 a we have g(b) = 0. Our algorithm finds a minterm b \u2264 a of f and therefore b is a minterm of h.\nFirst, if the number of ones in a is 2r, then we can find a minterm by flipping each bit in a that does change the value of f and get a minterm. This takes at most 2r membership queries.\nIf the number of ones in a is w > 2r, then we divide the entries of a that are equal to 1 into 2r disjoint sets S1, S2, . . . , S2r where for every i, the size\nof Si is either \u230aw/(2r)\u230b or \u2308w/(2r)\u2309. Now for i = 1, 2, . . . , 2r, we flip all the entries of Si in a to zero and ask a membership query. If the function is one, we keep those entries 0. Otherwise we set them back to 1 and proceed to i+1. At the end of this procedure, at most r sets are not flipped. Therefore, at least half of the bits in a are flipped to zero using 2r membership queries. Therefore, the number of membership queries we need to get a minterm is 2r log(n/2r) + 2r.\nWe will call the above procedure Find-Term."}, {"heading": "4.2 The case r > s", "text": "In this section, we present three algorithms, two deterministic and one randomized. We start with the deterministic algorithm."}, {"heading": "4.2.1 Deterministic Algorithm", "text": "Consider the class s-term r-MDNF. Let f be the target function. Given s\u2212 \u2113 monotone terms M1 \u2228M2 \u2228 \u00b7 \u00b7 \u00b7 \u2228Ms\u2212\u2113 that are known to the learning algorithm to be in f . The learning algorithm goal is to find a new monotone term. In order to find a new term we need to find an assignment a that is zero in M1\u2228M2\u2228\u00b7 \u00b7 \u00b7\u2228Ms\u2212\u2113 and 1 in the function f . Then by the procedure Find-Term in Subsection 4.1, we get a new term in O(r log n) additional membership queries.\nTo find such an assignment, we present three algorithms: Algorithm I: (Exhaustive Search) choose a variable from each Mi and set it to zero and set all the other variables to 1. The set of all such assignments is denoted by A. If f is 1 in some a \u2208 A, then find a new term using Find-Term.\nWe now show,\nLemma 7. If f 6\u2261 h, then Algorithm I finds a new term in rs\u2212\u2113+O(r log n) membership queries.\nProof. Since the number of variables in each term in h := M1\u2228M2\u2228\u00b7 \u00b7 \u00b7\u2228Ms\u2212\u2113 is at most r the number of assignments in A is at most rs\u2212\u2113. Since we choose one variable from each term in h and set it to zero, all the assignments in A are zero in h. We now show that one of the assignments in A must be 1 in f , and therefore a new term can be found.\nLet b be an assignment that is 1 in f and zero in h. Such assignment exists because otherwise f \u21d2 h and since h \u21d2 f we get f \u2261 h. Since\nh(b) = 0 there is at least one variable xji in each Mi that is zero in b. Then the assignment a := 1n|xj1=0,...,xjs\u2212\u2113=0 is in A and h(a) = 0. Since a \u2265 b we also have f(a) = 1.\nThe number of queries in this algorithm is s \u2211\n\u2113=1\nO ( rs\u2212\u2113 + r log n ) = O(rs\u22121 + rs log n).\nWe now present the second algorithm. Recall that Xn = {x1, . . . , xn}.\nWe now show,\nLemma 8. If f 6\u2261 h, then Algorithm II finds a new term in N((s \u2212 \u2113, r), (s \u2212 \u2113)r) +O(r log n) membership queries. Proof. Let h := M1 \u2228M2 \u2228 \u00b7 \u00b7 \u00b7 \u2228Ms\u2212\u2113. Let b be an assignment that is 1 in f and zero in h. Since h(b) = 0, there is at least one variable xji in each Mi that is zero in b. Consider the set U = {xji |i = 1, . . . , s\u2212 \u2113}. Since f(b) = 1 there is a new term M in f that is one in b. That is, all of its variables are one in b. Let W be the set of all variables in M . Since A is (|V |, (s\u2212 \u2113, r))CFF and since |U \u222a (W \u2229 V )| \u2264 s\u2212 \u2113+ r there is an assignment a \u2208 A that is 0 in each variable in U and is one in each variable in W \u2229 V . Since a\u2032 is also 0, in each variable in U we have h(a\u2032) = 0. Since a\u2032 is one in each variable in W \u2229 V and one in each variable W\\V , we have M(a\u2032) = 1 and therefore f(a\u2032) = 1. This completes the proof.\nThe number of queries in Algorithm II is\ns\u22121 \u2211\n\u2113=1\nN((s\u2212 \u2113, r), (s \u2212 \u2113)r) + r log n = O(sN((s\u2212 1, r), sr) + rs log n)."}, {"heading": "4.2.2 Randomized Algorithm", "text": "Our third algorithm, Algorithm III, is a randomized algorithm. It is basically Algorithm II where an (rs, (s\u2212 1, r))-CFF A is randomly constructed, as in (1). Notice that an (rs, (s \u2212 1, r))-CFF is also an (|V |, (s \u2212 \u2113, r))-CFF, so it can be used in every round of the algorithm. The algorithm fails if there is a new term that has not been found and this happens if and only if A is not (rs, (s \u2212 1, r))-CFF. So the failure probability is \u03b4. By (1), this gives a Monte Carlo randomized algorithm with query complexity\nO (\u221a s ( s+ r\ns\n)(\nr log r + log 1\n\u03b4\n)\n+ rs logn\n)\n."}, {"heading": "4.3 The case r < s", "text": "In this section, we present two algorithms. Algorithm IV is deterministic and Algorithm V is randomized. We start with the deterministic algorithm."}, {"heading": "4.3.1 Deterministic Algorithm", "text": "In this section, we present Algorithm IV, used when r < s. For this case, we prove the following,\nTheorem 9. There is a deterministic learning algorithm for the class of s-term r-MDNF that asks\nO ( (3e)r(rs)r/2+1.5 + rs log n ) ,\nmembership queries.\nBefore proving this theorem, we first prove learnability in simpler settings. We prove the following,\nLemma 10. Let f(x1, x2, . . . , xn) = M1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 Ms be the target s-term r-MDNF. Suppose the learning algorithm knows some of the terms, h = M1\u2228M2\u2228 \u00b7 \u00b7 \u00b7 \u2228Ms\u2212\u2113 and knows that Ms\u2212\u2113+1 is of size r\u2032. Suppose that h is a read k monotone DNF. Then, there exists an algorithm that finds a new term (not necessarily Ms\u2212\u2113+1) using\nO ( N((r\u2032k; r\u2032); sr)) + r log n ) ,\nmembership queries.\nProof. Consider the algorithm in Figure 2. Let V be the set of variables that appear in h. Let M := Ms\u2212\u2113+1. Let U be the set of variables in M and W = U \u2229 V . Each variable in W can appear in at most k terms in h. Let w.l.o.g h\u2032 := M1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 Mt be those terms. Notice that t \u2264 |W |k \u2264 r\u2032k. In each term Mi, i \u2264 t one can choose a variable xji that is not in W . This is because, if all the variable in Mi are in W , then M \u21d2 Mi and then f is not reduced MDNF.\nLet Z = {xji |i = 1, . . . , t}. Since |Z| \u2264 t \u2264 r\u2032k and |U | \u2264 r\u2032 there is a \u2208 A that is 0 in every variable in Z and is 1 in every variable in U . Now notice that a\u2032 in step 3.1 in the algorithm is the same as a over the variables in Z and therefore h\u2032(a\u2032) = 0. Also a\u2032 is the same as a over the variables in U and therefore M(a\u2032) = 1. Now notice that since Mi(a\u2032) = 0 for i \u2264 t, in step 3.4 in the algorithm we only flip a\u2032i that correspond to variables in the terms Mi, i > t. The set of variables in each other term Mi, i > t is disjoint with U . Therefore if for some i > t, Mi(a\n\u2032) = 1 then setting any variable xj in Mi that is one in a\n\u2032 to zero will not change the values M(a\u2032) = 1 and (from monotonicity) h\u2032(a\u2032) = 0. Eventually, we will have an assignment a\u2032\u2032 that satisfies h(a\u2032\u2032) = 0 and M(a\u2032\u2032) = 1 which implies f(a\u2032\u2032) = 1.\nIn the following lemma, we remove the restriction on h.\nLemma 11. Let f(x1, x2, . . . , xn) = M1 \u2228 \u00b7 \u00b7 \u00b7 \u2228Ms be the target s-term rMDNF. Suppose some of the terms, h = M1 \u2228M2 \u2228 . . . \u2228Ms\u2212\u2113, are already known to the learning algorithm. Then, for any integer d, there exists an\nalgorithm that finds a new term using\nO\n(\nr \u2211\ni=1\n( r \u221a ds\ni\n)\nN(((r \u2212 i) \u221a s/d; (r \u2212 i)); rs) + r log n ) ,\nmembership queries.\nProof. Consider the algorithm in Figure 3.\nFirst note that in step 2.2, f(A) is considered in LearnRead as a function in all the variables Xn. Note also that the oracle MQf(A) can be simulated by MQf , since f(A)(a) = f(a|R\u21900,S\\R\u21901).\nLet W be the set of variables that appear in M := Ms\u2212\u2113+1 and R = S\u2229W . Note that A is zero in all S\\R and 1 in R and therefore f(A) is now a read \u221a\ns/d andM(A) contains at most |W\\R| \u2264 r\u2212|R| variables. Therefore, when we run LearnRead(MQf(A), s, \u2113, r \u2212 |R|) we find an assignment a\u2032\u2032 that is 1 in M(A) and zero in f(A) and then a\u2032\u2032|R\u21900,S\\R\u21901 is one in f and zero in h.\nWe now find the number of queries. By the Pigeon hole principle, there are at most |S| \u2264 r \u221a ds that are \u221a\ns/d-frequent. The number of sets R \u2286 S of size i is (r \u221a ds i )\n. For each set, we run LearnRead(MQf(A), s, \u2113, r \u2212 |R|) that by Lemma 10 asks N(((r \u2212 i) \u221a\ns/d; (r \u2212 i)); rs) queries. This implies the result.\nWe now prove our main result. We choose d = r. Then by the construc-\ntion (2), we have\n( r \u221a ds\ni\n)\nN(((r \u2212 i) \u221a s/d; (r \u2212 i)); rs) \u2264 (\ner \u221a rs\ni\n)i\n(2e)r\u2212i ( (r \u2212 i)\u221as\u221a r )r\u2212i+3\n\u2264 er2r\u2212i( \u221a rs)r+3 (r\ni\n)i ( r \u2212 i r\n)r\u2212i+3\n\u2264 er2r\u2212i ( r\ni\n) ( \u221a rs)r+3.\nand therefore\nr \u2211\ni=1\n( r \u221a ds\ni\n)\nN(((r \u2212 i) \u221a s/d; (r \u2212 i)); rs) \u2264 (3e)r(rs)r/2+1.5."}, {"heading": "4.3.2 Randomized Algorithm", "text": "In this section, we give a randomized algorithm for the case s > r. The randomized algorithm is the same as the deterministic one, except that each CFF is constructed randomly, as in (1) with probability of success 1\u2212 \u03b4/s. We choose d = 1 and get\n( r \u221a ds\ni\n)\nN(((r \u2212 i) \u221a s/d; (r \u2212 i)); rs)\n\u2264 (\ner \u221a s\ni\n)i\u221a r(e( \u221a s+ 1))r\u2212i ( 2s log rs+ log s\n\u03b4\n)\n.\n\u2264 er2r\u2212i (r\ni\n)i \u221a rsr/2(s log s+ log(1/\u03b4))\nand therefore\nr \u2211\ni=1\n( r \u221a ds\ni\n)\nN(((r \u2212 i) \u221a s/d; (r \u2212 i)); rs) \u2264 \u221a r(3e)rsr/2(s log s+ log(1/\u03b4))."}, {"heading": "5 Conclusion and Open Problems", "text": "In this paper, we gave an almost optimal adaptive exact learning algorithms for the class of s-term r-MDNF. When r and s are fixed, the bounds are asymptotically tight. Some gaps occur between the lower bounds and upper bounds. For r \u2265 s, the gap is cs for some constant c and for r \u2264 s the gap is rr/2. It is interesting to close these gaps. Finding a better deterministic construction of CFF will give better deterministic algorithms.\nAnother challenging problem is finding tight bounds for non-adaptive learning of this class."}], "references": [{"title": "Queries and Concept Learning", "author": ["D. Angluin"], "venue": "Machine Learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "Learning a Hidden Subgraph", "author": ["A. Alon", "V. Asodi"], "venue": "SIAM J. Discrete Math", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Learning a Hidden Matching", "author": ["N. Alon", "R. Beigel", "S. Kasif", "S. Rudich", "B. Sudakov"], "venue": "SIAM J. Comput", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Learning a Hidden Hypergraph", "author": ["D. Angluin", "J. Chen"], "venue": "Journal of Machine Learning Research", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Learning a hidden graph using O(log n) queries per edge", "author": ["D. Angluin", "J. Chen"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Algorithmic construction of sets for krestrictions", "author": ["N. Alon", "D. Moshkovitz", "S. Safra"], "venue": "ACM Transactions on Algorithms,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "An optimal procedure for gap closing in whole genome shotgun sequencing", "author": ["R. Beigel", "N. Alon", "S. Kasif", "M. Serkan Apaydin", "L. Fortnow"], "venue": "RECOMB", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Testers and their Applications", "author": ["N.H. Bshouty"], "venue": "Electronic Collouium on Computational Complexity (ECCC) 19:11,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Asking Questions to Minimize Errors", "author": ["N.H. Bshouty", "S.A. Goldman", "Thomas R. Hancock", "Sleiman Matar"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Attribute-Efficient Learning in Query and Mistakebound Models", "author": ["N.H. Bshouty", "L. Hellerstein"], "venue": "COLT", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "Efficient Computation of Representative Sets with Applications in Parameterized and Exact Algorithms", "author": ["F.V. Fomin", "D. Lokshtanov", "S. Saurabh"], "venue": "SODA", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Nonrandom binary superimposed codes", "author": ["W.H. Kautz", "R.C. Singleton"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1964}, {"title": "Reconstructing a Hamiltonian Cycle by Querying the Graph: Application to DNA Physical Mapping", "author": ["V. Grebinski", "G. Kucherov"], "venue": "Discrete Applied Mathematics", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Families of k-independent sets", "author": ["D.J. Kleitman", "J. Spencer"], "venue": "Discrete Mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1972}, {"title": "Splitters and Near-optimal Derandomization", "author": ["M. Naor", "L.J. Schulman", "A. Srinivasan"], "venue": "FOCS 95,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Some New Bounds for Cover-free Families", "author": ["D.R. Stinson", "R. Wei", "L. Zhu"], "venue": "Journal of Combinatorial Theory, Series A,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction We consider the problem of learning a monotone DNF with at most s terms, where each monotone term contains at most r variables (s term r-MDNF) from membership queries [1].", "startOffset": 182, "endOffset": 185}, {"referenceID": 4, "context": "The time needed to compute which experiments to do is a secondary consideration, though it is polynomial for the algorithms we present [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 12, "context": "See [13, 7, 3, 2, 4, 5] for more details and other applications.", "startOffset": 4, "endOffset": 23}, {"referenceID": 6, "context": "See [13, 7, 3, 2, 4, 5] for more details and other applications.", "startOffset": 4, "endOffset": 23}, {"referenceID": 2, "context": "See [13, 7, 3, 2, 4, 5] for more details and other applications.", "startOffset": 4, "endOffset": 23}, {"referenceID": 1, "context": "See [13, 7, 3, 2, 4, 5] for more details and other applications.", "startOffset": 4, "endOffset": 23}, {"referenceID": 3, "context": "See [13, 7, 3, 2, 4, 5] for more details and other applications.", "startOffset": 4, "endOffset": 23}, {"referenceID": 4, "context": "See [13, 7, 3, 2, 4, 5] for more details and other applications.", "startOffset": 4, "endOffset": 23}, {"referenceID": 4, "context": "2 Previous Results In [5], Angluin and Chen presented an deterministic optimal adaptive learning algorithm for learning s-term 2-MDNF.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "In [4], Angluin and Chen gave a randomized algorithm for s-term r-uniform MDNF (the size of each term is exactly r) that asks O(24rs \u00b7 poly(r, log n)) membership queries.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "See [13, 7, 3, 2, 4, 5].", "startOffset": 4, "endOffset": 23}, {"referenceID": 6, "context": "See [13, 7, 3, 2, 4, 5].", "startOffset": 4, "endOffset": 23}, {"referenceID": 2, "context": "See [13, 7, 3, 2, 4, 5].", "startOffset": 4, "endOffset": 23}, {"referenceID": 1, "context": "See [13, 7, 3, 2, 4, 5].", "startOffset": 4, "endOffset": 23}, {"referenceID": 3, "context": "See [13, 7, 3, 2, 4, 5].", "startOffset": 4, "endOffset": 23}, {"referenceID": 4, "context": "See [13, 7, 3, 2, 4, 5].", "startOffset": 4, "endOffset": 23}, {"referenceID": 4, "context": "For the case s \u2265 r, Angluin and Chen, [5], gave the lower bound \u03a9((2s/r)r/2 + rs log n).", "startOffset": 38, "endOffset": 41}, {"referenceID": 11, "context": "2 Cover-Free Families The problem (n, (s, r))-cover-free family [12] is equivalent to the following problem: A (n, (s, r))-cover-free family is a set A \u2286 {0, 1}n such that for", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "The lower bounds in [16] are", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": "In [8], Bshouty gave a deterministic construction of (n, (s, r))-CFF of size C := min((2e)r, (2e)s) log n = (", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "in [11] gave a construction of size D := (", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "The first is from [4] and the second follows using the same techniques used in [9].", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "The first is from [4] and the second follows using the same techniques used in [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "In [4], Angluin and Chen proved, Theorem 4.", "startOffset": 3, "endOffset": 6}], "year": 2014, "abstractText": "In this paper, we study the problem of learning a monotone DNF with at most s terms of size (number of variables in each term) at most r (s term r-MDNF) from membership queries. This problem is equivalent to the problem of learning a general hypergraph using hyperedge-detecting queries, a problem motivated by applications arising in chemical reactions and genome sequencing. We first present new lower bounds for this problem and then present deterministic and randomized adaptive algorithms with query complexities that are almost optimal. All the algorithms we present in this paper run in time linear in the query complexity and the number of variables n. In addition, all of the algorithms we present in this paper are asymptotically tight for fixed r and/or s.", "creator": "LaTeX with hyperref package"}}}