{"id": "1405.1438", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2014", "title": "The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter", "abstract": "Consider 's something trying time water an places page on while labor cellular. He / get can actually went trying to transports the message. Does its what should? While we of involved limited 2005 made because well predicting popularity is values - media reference, the indeed \u2014 objection per avant another without been employed followed turn is often confounded without the popularity of new author and along taboo. To control for these perplexing factors, say take advantage of taken timing any that even are far runners than vimeo containing held also deleting where screenplay by the while phone back employ different imply. Given common pairs, 'll ask: far version 5,000 less retweets? This turns others but even a more explain task better predicted name topics. Still, pathogens much wrong this question should than trying (all far for way ), and three simulation methods we such can do better 50 both an average human and for though more method trainers on non - controlled report.", "histories": [["v1", "Tue, 6 May 2014 20:04:44 GMT  (462kb,D)", "http://arxiv.org/abs/1405.1438v1", "11 pages, to appear in Proceedings of ACL 2014"]], "COMMENTS": "11 pages, to appear in Proceedings of ACL 2014", "reviews": [], "SUBJECTS": "cs.SI cs.CL physics.soc-ph", "authors": ["chenhao tan", "lillian lee", "bo pang"], "accepted": true, "id": "1405.1438"}, "pdf": {"name": "1405.1438.pdf", "metadata": {"source": "CRF", "title": "The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter", "authors": ["Chenhao Tan", "Lillian Lee", "Bo Pang"], "emails": ["chenhao@cs.cornell.edu", "llee@cs.cornell.edu", "bopang42@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "How does one make a message \u201csuccessful\u201d? This question is of interest to many entities, including political parties trying to frame an issue (Chong and Druckman, 2007), and individuals attempting to make a point in a group meeting. In the first case, an important type of success is achieved if the national conversation adopts the rhetoric of the party; in the latter case, if other group members repeat the originating individual\u2019s point.\nThe massive availability of online messages, such as posts to social media, now affords researchers new means to investigate at a very large scale the factors affecting message propagation,\nalso known as adoption, sharing, spread, or virality. According to prior research, important features include characteristics of the originating author (e.g., verified Twitter user or not, author\u2019s messages\u2019 past success rate), the author\u2019s social network (e.g., number of followers), message timing, and message content or topic (Artzi et al., 2012; Bakshy et al., 2011; Borghol et al., 2012; Guerini et al., 2011; Guerini et al., 2012; Hansen et al., 2011; Hong et al., 2011; Lakkaraju et al., 2013; Milkman and Berger, 2012; Ma et al., 2012; Petrovic\u0301 et al., 2011; Romero et al., 2013; Suh et al., 2010; Sun et al., 2013; Tsur and Rappoport, 2012). Indeed, it\u2019s not surprising that one of the most retweeted tweets of all time was from user BarackObama, with 40M followers, on November 6, 2012: \u201cFour more years. [link to photo]\u201d.\nOur interest in this paper is the effect of alternative message wording, meaning how the message is said, rather than what the message is about. In contrast to the identity/social/timing/topic features mentioned above, wording is one of the few factors directly under an author\u2019s control when he or she seeks to convey a fixed piece of content. For example, consider a speaker at the ACL business meeting who has been tasked with proposing that Paris be the next ACL location. This person cannot on the spot become ACL president, change the shape of his/her social network, wait until the next morning to speak, or campaign for Rome instead; but he/she can craft the message to be more humorous, more informative, emphasize certain aspects instead of others, and so on. In other words, we investigate whether a different choice of words affects message propagation, controlling for user and topic: would user BarackObama have gotten significantly more (or fewer) retweets if he had used some alternate wording to announce his reelection?\nAlthough we cannot create a parallel universe\nar X\niv :1\n40 5.\n14 38\nv1 [\ncs .S\nI] 6\nM ay\n2 01\n4\nin which BarackObama tweeted something else1, fortunately, a surprising characteristic of Twitter allows us to run a fairly analogous natural experiment: external forces serendipitously provide an environment that resembles the desired controlled setting (DiNardo, 2008). Specifically, it turns out to be unexpectedly common for the same user to post different tweets regarding the same URL \u2014 a good proxy for fine-grained topic2 \u2014 within a relatively short period of time.3 Some example pairs are shown in Table 1; we see that the paired tweets may differ dramatically, going far beyond word-for-word substitutions, so that quite interesting changes can be studied.\nLooking at these examples, can one in fact tell from the wording which tweet in a topic- and author-controlled pair will be more successful? The answer may not be a priori clear. For example, for the first pair in the table, one person we asked found t1\u2019s invocation of a \u201cscandal\u201d to be more attention-grabbing; but another person preferred t2 because it is more informative about the URL\u2019s content and includes \u201cfight media portrayal\u201d. In an Amazon Mechanical Turk (AMT) experiment (\u00a74), we found that humans achieved an average accuracy of 61.3%: not that high, but better than chance, indicating that it is somewhat possible for humans to predict greater message spread from different deliveries of the same information.\nBuoyed by the evidence of our AMT study that wording effects exist, we then performed a battery of experiments to seek generally-applicable, non-\n1Cf. the Music Lab \u201cmultiple universes\u201d experiment to test the randomness of popularity (Salganik et al., 2006).\n2Although hashtags have been used as coarse-grained topic labels in prior work, for our purposes, we have no assurance that two tweets both using, say, \u201c#Tahrir\u201d would be attempting to express the same message but in different words. In contrast, see the same-URL examples in Table 1.\n3Moreover, Twitter presents tweets to a reader in strict chronological order, so that there are no algorithmic-ranking effects to compensate for in determining whether readers saw a tweet. And, Twitter accumulates retweet counts for the entire retweet cascade and displays them for the original tweet at the root of the propagation tree, so we can directly use Twitter\u2019s retweet counts to compare the entire reach of the different versions.\nTwitter-specific features of more successful phrasings. \u00a75.1 applies hypothesis testing (with Bonferroni correction to ameliorate issues with multiple comparisons) to investigate the utility of features like informativeness, resemblance to headlines, and conformity to the community norm in language use. \u00a75.2 further validates our findings via prediction experiments, including on completely fresh held-out data, used only once and after an array of standard cross-validation experiments.4 We achieved 66.5% cross-validation accuracy and 65.6% held-out accuracy with a combination of our custom features and bag-of-words. Our classifier fared significantly better than a number of baselines, including a strong classifier trained on the most- and least-retweeted tweets that was even granted access to author and timing metadata."}, {"heading": "2 Related work", "text": "The idea of using carefully controlled experiments to study effective communication strategies dates back at least to Hovland et al. (1953). Recent studies range from examining what characteristics of New York Times articles correlate with high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013). Simmons et al. (2011) examined the variation of quotes from different sources to examine how textual memes mutate as people pass them along, but did not control for author. Predicting the \u201csuccess\u201d of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in \u00a71 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009). To our knowledge, there have been no large-scale studies exploring wording effects in a both topic- and author-controlled setting. Employing such controls, we find that predicting the more effective alternative wording is much harder than the previously well-studied problem of pre-\n4And after crossing our fingers.\ndicting popular content when author or topic can freely vary.\nRelated work regarding the features we considered is deferred to \u00a75.1 (features description)."}, {"heading": "3 Data", "text": "Our main dataset was constructed by first gathering 1.77M topic- and author-controlled (henceforth TAC) tweet pairs5 differing in more than just spacing.6 We accomplished this by crawling timelines of 236K user ids that appear in prior work (Kwak et al., 2010; Yang and Leskovec, 2011) via the Twitter API. This crawling process also yielded 632K TAC pairs whose only difference was spacing, and an additional 558M \u201cunpaired\u201d tweets; as shown later in this paper, we used these extra corpora for computing language models and other auxiliary information. We applied nonobvious but important filtering \u2014 described later in this section \u2014 to control for other external factors and to reduce ambiguous cases. This brought us to a set of 11,404 pairs, with the gold-standard labels determined by which tweet in each pair was the one that received more retweets according to the Twitter API. We then did a second crawl to get an additional 1,770 pairs to serve as a held-out dataset. The corresponding tweet IDs are available online at http://chenhaot.com/pages/ wording-for-propagation.html. (Twitter\u2019s terms of service prohibit sharing the actual tweets.)\nThroughout, we refer to the textual content of the earlier tweet within a TAC pair as t1, and of the later one as t2. We denote the number of retweets received by each tweet by n1 and n2, respectively. We refer to the tweet with higher (lower) ni as the \u201cbetter (worse)\u201d tweet. Using \u201cidentical\u201d pairs to determine how to compensate for follower-count and timing effects. In an ideal setting, differences between n1 and n2 would be determined solely by differences in wording. But even with a TAC pair, retweets might exhibit a temporal bias because of the chronological order of tweet presentation (t1 might enjoy a first-mover advantage (Borghol et al., 2012) because it is the \u201coriginal\u201d; alternatively,\n5No data collection/processing was conducted at Google. 6The total excludes: tweets containing multiple URLs; tweets from users posting about the same URL more than five times (since such users might be spammers); the third, fourth, or fifth version for users posting between three and five tweets for the same URL; retweets (as identified by Twitter\u2019s API or by beginning with \u201cRT @\u201d); non-English tweets.\nt2 might be preferred because retweeters consider t1 to be \u201cstale\u201d). Also, the number of followers an author has can have complicated indirect effects on which tweets are read (space limits preclude discussion).\nWe use the 632K TAC pairs wherein t1 and t2 are identical7 to check for such confounding effects: we see how much n2 deviates from n1 in such settings, since if wording were the only explanatory factor, the retweet rates for identical tweets ought to be equal. Figure 1(a) plots how the time lag between t1 and t2 and the author\u2019s follower-count affect the following deviation estimate:\nD \u201c \u00ff\n0\u010fn1\u010310 | pEpn2|n1q \u00b4 n1|,\nwhere pEpn2|n1q is the average value of n2 over pairs whose t1 is retweeted n1 times. (Note that the number of pairs whose t1 is retweeted n1 times decays exponentially with n1; hence, we condition on n1 to keep the estimate from being dominated by pairs with n1 \u201c 0, and do not consider n1 \u011b 10 because there are too few such pairs to estimate pEpn2|n1q reliably.) Figure 1(a) shows that the setting where we (i) minimize the confounding effects of time lag and author\u2019s follower-count and (ii) maximize the amount of data to work with\n7Identical up to spacing: Twitter prevents exact copies by the same author appearing within a short amount of time, but some authors work around this by inserting spaces.\nis: when t2 occurs within 12 hours after t1 and the author has more than 5,000 followers. Figure 1(b) confirms that for identical TAC pairs, our chosen setting indeed results in n2 being on average close to n1, which corresponds to the desired setting where wording is the dominant differentiating factor.8\nFocus on meaningful and general changes. Even after follower-count and time-lapse filtering, we still want to focus on TAC pairs that (i) exhibit significant/interesting textual changes (as exemplified in Table 1, and as opposed to typo corrections and the like), and (ii) have n2 and n1 sufficiently different so that we are confident in which ti is better at attracting retweets. To take care of (i), we discarded the 50% of pairs whose similarity was above the median, where similarity was tf-based cosine.9 For (ii), we sorted the remaining pairs by n2 \u00b4 n1 and retained only the top and bottom 5%.10 Moreover, to ensure that we do not overfit to the idiosyncrasies of particular authors, we cap the number of pairs contributed by each author to 50 before we deal with (ii)."}, {"heading": "4 Human accuracy on TAC pairs", "text": "We first ran a pilot study on Amazon Mechanical Turk (AMT) to determine whether humans can identify, based on wording differences alone, which of two topic- and author- controlled tweets is spread more widely. Each of our 5 AMT tasks involved a disjoint set of 20 randomly-sampled TAC pairs (with t1 and t2 randomly reordered); subjects indicated \u201cwhich tweet would other people be more likely to retweet?\u201d, provided a short justification for their binary response, and clicked a checkbox if they found that their choice was a \u201cclose call\u201d. We received 39 judgments per pair in aggregate from 106 subjects total (9 people completed all 5 tasks). The subjects\u2019 justifications were of very high quality, convincing us that they all did the task in good faith11. Two examples for\n8We also computed the Pearson correlation between n1 and n2, even though it can be dominated by pairs with smaller n1. The correlation is 0.853 for \u201c\u0105 5K f\u2019ers, \u010312hrs\u201d, clearly higher than the 0.305 correlation for \u201cotherwise\u201d.\n9Idf weighting was not employed because changes to frequent words are of potential interest. Urls, hashtags, @- mentions and numbers were normalized to [url], [hashtag], [at], and [num] before computing similarity.\n10For our data, this meant n2 \u00b4 n1 \u011b 10 or \u010f \u00b415. Cf. our median number of retweets: 30.\n11We also note that the feedback we got was quite positive, including: \u201c...It\u2019s fun to make choices between close tweets and use our subjective opinion. Thanks and best of\nthe third TAC pair in Table 1 were: \u201c[t1 makes] the cause relate-able to some people, therefore showing more of an appeal as to why should they click the link and support\u201d and, expressing the opposite view, \u201cI like [t2] more because [t1] starts out with a generalization that doesn\u2019t affect me and try to make me look like I had that experience before\u201d.\nIf we view the set of 3900 binary judgments for our 100-TAC-pair sample as constituting independent responses, then the accuracy for this set is 62.4% (rising to 63.8% if we exclude the 587 judgments deemed \u201cclose calls\u201d). However, if we evaluate the accuracy of the majority response among the 39 judgments per pair, the number rises to 73%. The accuracy of the majority response generally increases with the dominance of the majority, going above 90% when at least 80% of the judgments agree (although less than a third of the pairs satisfied this criterion).\nAlternatively, we can consider the average accuracy of the 106 subjects: 61.3%, which is better than chance but far from 100%. (Variance was high: one subject achieved 85% accuracy out of 20 pairs, but eight scored below 50%.) This result is noticeably lower than the 73.8%-81.2% reported by Petrovic\u0301 et al. (2011), who ran a similar experiment involving two subjects and 202 tweet pairs, but where the pairs were not topic- or author-controlled.12\nWe conclude that even though propagation prediction becomes more challenging when topic and author controls are applied, humans can still to some degree tell which wording attracts more retweets. Interested readers can try this out themselves at http://chenhaot.com/ retweetedmore/quiz."}, {"heading": "5 Experiments", "text": "We now investigate computationally what wording features correspond to messages achieving a broader reach. We start (\u00a75.1) by introducing a set of generally-applicable and (mostly) non-Twitterspecific features to capture our intuitions about what might be better ways to phrase a message. We then use hypothesis testing (\u00a75.1) to evaluate the importance of each feature for message prop-\nluck with your research\u201d and \u201cThis was very interesting and really made me think about how I word my own tweets. Great job on this survey!\u201d. We only had to exclude one person (not counted among the 106 subjects), doing so because he or she gave the same uninformative justification for all pairs.\n12The accuracy range stems from whether author\u2019s social features were supplied and which subject was considered.\nagation and the extent to which authors employ it, followed by experiments on a prediction task (\u00a75.2) to further examine the utility of these features."}, {"heading": "5.1 Features: efficacy and author preference", "text": "What kind of phrasing helps message propagation? Does it work to explicitly ask people to share the message? Is it better to be short and concise or long and informative? We define an array of features to capture these and other messaging aspects. We then examine (i) how effective each feature is for attracting more retweets; and (ii) whether authors prefer applying a given feature when issuing a second version of a tweet.\nFirst, for each feature, we use a one-sided paired t-test to test whether, on our 11K TAC pairs, our score function for that feature is larger in the better tweet versions than in the worse tweet versions, for significance levels \u03b1 \u201c .05, .01, .001, 1e-20. Given that we did 39 tests in total, there is a risk of obtaining false positives due to multiple testing (Dunn, 1961; Benjamini and Hochberg, 1995). To account for this, we also report significance results for the conservatively Bonferroni-corrected (\u201cBC\u201d) significance level \u03b1 = 0.05/39=1.28e-3.\nSecond, we examine author preference for applying a feature. We do so because one (but by no means the only) reason authors post t2 after having already advertised the same URL in t1 is that these authors were dissatisfied with the amount of attention t1 got; in such cases, the changes may have been specifically intended to attract more retweets. We measure author preference for a feature by the percentage of our TAC pairs13 where t2 has more \u201coccurrences\u201d of the feature than t1, which we denote by \u201c%pf2 \u0105 f1q\u201d. We use the one-sided binomial test to see whether %pf2 \u0105 f1q is significantly larger (or smaller) than 50%.\n13 For our preference experiments, we added in pairs where n2 \u00b4 n1 was not in the top or bottom 5% (cf. \u00a73, meaningful changes), since to measure author preference it\u2019s not necessary that the retweet counts differ significantly.\nNot surprisingly, it helps to ask people to share. (See Table 3; the notation for all tables is explained in Table 2.) The basic sanity check we performed here was to take as features the number of occurrences of the verbs \u2018rt\u2019, \u2018retweet\u2019, \u2018please\u2019, \u2018spread\u2019, \u2018pls\u2019, and \u2018plz\u2019 to capture explicit requests (e.g. \u201cplease retweet\u201d).\nInformativeness helps. (Table 4) Messages that are more informative have increased social exchange value (Homans, 1958), and so may be more worth propagating. One crude approximation of informativeness is length, and we see that length helps.14 In contrast, Simmons et al. (2011) found that shorter versions of memes are more likely to be popular. The difference may result from TAC-pair changes being more drastic than the variations that memes undergo.\nA more refined informativeness measure is counts of the parts of speech that correspond to content. Our POS results, gathered using a Twitter-specific tagger (Gimpel et al., 2011), echo those of Ashok et al. (2013) who looked at predict-\n14Of course, simply inserting garbage isn\u2019t going to lead to more retweets, but adding more information generally involves longer text.\ning the success of books. The diminished effect of hashtag inclusion with respect to what has been reported previously (Suh et al., 2010; Petrovic\u0301 et al., 2011) presumably stems from our topic and author controls. Be like the community, and be true to yourself (in the words you pick, but not necessarily in how you combine them). (Table 5) Although distinctive messages may attract attention, messages that conform to expectations might be more easily accepted and therefore shared. Prior work has explored this tension: Lakkaraju et al. (2013), in a content-controlled study, found that the more upvoted Reddit image titles balance novelty and familiarity; Danescu-Niculescu-Mizil et al. (2012) (henceforth DCKL\u201912) showed that the memorability of movie quotes corresponds to higher lexical distinctiveness but lower POS distinctiveness; and Sun et al. (2013) observed that deviating from one\u2019s own past language patterns correlates with more retweets.\nKeeping in mind that the authors in our data have at least 5000 followers15, we consider two types of language-conformity constraints an author might try to satisfy: to be similar to what is normal in the Twitter community, and to be similar to what his or her followers expect. We measure a tweet\u2019s similarity to expectations by its score according to the relevant language model, 1 |T | \u0159\nxPT logpppxqq, where T refers to either all the unigrams (unigram model) or all and only bigrams (bigram model).16 We trained a Twittercommunity language model from our 558M unpaired tweets, and personal language models from each author\u2019s tweet history. Imitate headlines. (Table 6) News headlines are often intentionally written to be both informative and attention-getting, so we introduce the idea of\n15This is not an artificial restriction on our set of authors; a large follower count means (in principle) that our results draw on a large sample of decisions whether to retweet or not.\n16The tokens [at], [hashtag], [url] were ignored in the unigram-model case to prevent their undue influence, but retained in the bigram model to capture longer-range usage (\u201ccombination\u201d) patterns.\nscoring by a language model built from New York Times headlines.17\nUse words associated with (non-paired) retweeted tweets. (Table 7) We expect that provocative or sensationalistic tweets are likely to make people react. We found it difficult to model provocativeness directly. As a rough approximation, we check whether the changes in t2 with respect to t1 (which share the same topic and author) involve words or parts-of-speech that are associated with high retweet rate in a very large separate sample of unpaired tweets (retweets and replies discarded). Specifically, for each word w that appears more than 10 times, we compute the probability that tweets containing w are retweeted more than once, denoted by rspwq. We define the rt score of a tweet as maxwPT rspwq, where T is all the words in the tweet, and the rt score of a particular POS tag z in a tweet as maxwPT&tagpwq\u201czrspwq. Include positive and/or negative words. (Table 8) Prior work has found that including positive or negative sentiment increases message propagation (Milkman and Berger, 2012; Godes et al., 2005; Heath et al., 2001; Hansen et al., 2011). We measured the occurrence of positive and negative words as determined by the connotation lexicon of Feng et al. (2013) (better coverage than LIWC). Measuring the occurrence of both simultaneously was inspired by Riloff et al. (2013).\nRefer to other people (but not your audience). (Table 9) First-person has been found useful for success before, but in the different domains of scientific abstracts (Guerini et al., 2012) and books (Ashok et al., 2013).\n17 To test whether the results stem from similarity to news rather than headlines per se, we constructed a NYT-text LM, which proved less effective. We also tried using Gawker headlines (often said to be attention-getting) but pilot studies revealed insufficient vocabulary overlap with our TAC pairs.\nGenerality helps. (Table 10) DCKL\u201912 posited that movie quotes are more shared in the culture when they are general enough to be used in multiple contexts. We hence measured the presence of indefinite articles vs. definite articles.\nThe easier to read, the better. (Table 11) We measure readability by using Flesch reading ease (Flesch, 1948) and Flesch-Kincaid grade level (Kincaid et al., 1975), though they are not designed for short texts. We use negative grade level so that a larger value indicates easier texts to read.\nFinal question: Do authors prefer to do what is effective? Recall that we use binomial tests to determine author preference for applying a feature more in t2. Our preference statistics show that author preferences in many cases are aligned with feature efficacy. But there are several notable exceptions: for example, authors tend to increase the use of @-mentions and 2nd person pronouns even though they are ineffective. On the other hand, they did not increase the use of effective ones like proper nouns and numbers; nor did they tend to increase their rate of sentiment-bearing words. Bearing in mind that changes in t2 may not always be intended as an effort to improve t1, it is still interesting to observe that there are some contrasts between feature efficacy and author preferences."}, {"heading": "5.2 Predicting the \u201cbetter\u201d wording", "text": "Here, we further examine the collective efficacy of the features introduced in \u00a75.1 via their performance on a binary prediction task: given a TAC pair (t1, t2), did t2 receive more retweets?\nOur approach. We group the features introduced in \u00a75.1 into 16 lexicon-based features (Table 3, 8, 9, 10), 9 informativeness features (Table 4), 6 language model features (Table 5, 6), 6 rt score features (Table 7), and 2 readability features (Table 11). We refer to all 39 of them together as\ncustom features. We also consider tagged bag-ofwords (\u201cBOW\u201d) features, which includes all the unigram (word:POS pair) and bigram features that appear more than 10 times in the cross-validation data. This yields 3,568 unigram features and 4,095 bigram features, for a total of 7,663 so-called 1,2-gram features. Values for each feature are normalized by linear transformation across all tweets in the training data to lie in the range r0, 1s.18\nFor a given TAC pair, we construct its feature vector as follows. For each feature being considered, we compute its normalized value for each tweet in the pair and take the difference as the feature value for this pair. We use L2-regularized logistic regression as our classifier, with parameters chosen by cross validation on the training data. (We also experimented with SVMs. The performance was very close, but mostly slightly lower.)\nA strong non-TAC alternative, with social information and timing thrown in. One baseline result we would like to establish is whether the topic and author controls we have argued for, while intuitively compelling for the purposes of trying to determine the best way for a given author to present some fixed content, are really necessary in practice. To test this, we consider an alternative binary L2-regularized logistic-regression classifier that is trained on unpaired data, specifically, on the collection of 10,000 most retweeted tweets (gold-standard label: positive) plus the 10,000 least retweeted tweets (gold-standard label: negative) that are neither retweets nor replies. Note that this alternative thus is granted, by design, roughly twice the training instances that our classifiers have, as a result of having roughly the same number of tweets, since our instances are pairs. Moreover, we additionally include the tweet author\u2019s follower count, and the day and hour of posting, as features. We refer to this alternative classifier as TAC+ff+time. (Mnemonic: \u201cff\u201d is used in bibliographic contexts as an abbreviation\n18We also tried normalization by whitening, but it did not lead to further improvements.\nfor \u201cand the following\u201d.) We apply it to a tweet pair by computing whether it gives a higher score to t2 or not.\nBaselines. To sanity-check whether our classifier provides any improvement over the simplest methods one could try, we also report the performance of the majority baseline, our request-for-sharing features, and our character-length feature.\nPerformance comparison. We compare the accuracy (percentage of pairs whose labels were correctly predicted) of our approach against the competing methods. We report 5-fold cross validation results on our balanced set of 11,404 TAC pairs and on our completely disjoint heldout data19 of 1,770 TAC pairs; this set was never examined during development, and there are no authors in common between the two testing sets.\nFigure 2(a) summarizes the main results. While TAC+ff+time outperforms the majority baseline, using all the features we proposed beats TAC+ff+time by more than 10% in both crossvalidation (66.5% vs 55.9%) and heldout validation (65.6% vs 55.3%). We outperform the average human accuracy of 61% reported in our Amazon Mechanical Turk experiments (for a different data sample); TAC+ff+time fails to do so.\nThe importance of topic and author control can be seen by further investigation of TAC+ff+time\u2019s performance. First, note that\n19To construct this data, we used the same criteria as in \u00a73: written by authors with more than 5000 followers, posted within 12 hours, n2 \u00b4 n1 \u011b 10 or \u010f \u00b415, and cosine similarity threshold value the same as in \u00a73, cap of 50 on number of pairs from any individual author.\nit yields an accuracy of around 55% on our alternate-version-selection task,20 even though its cross-validation accuracy on the larger most- and least-retweeted unpaired tweets averages out to a high 98.8%. Furthermore, note the superior performance of unigrams trained on TAC data vs TAC+ff+time \u2014 which is similar to our unigrams but trained on a larger but non-TAC dataset that included metadata. Thus, TAC pairs are a useful data source even for non-custom features. (We also include individual feature comparisons later.)\nInformativeness is the best-performing custom feature group when run in isolation, and outperforms all baselines, as well as TAC+ff+time; and we can see from Figure 2(a) that this is not due just to length. The combination of all our 39 custom features yields approximately 63% accuracy in both testing settings, significantly outperforming informativeness alone (p\u01030.001 in both cases). Again, this is higher than our estimate of average human performance.\nNot surprisingly, the TAC-trained BOW features (unigram and 1,2-gram) show impressive predictive power in this task: many of our custom features can be captured by bag-of-word features, in a way. Still, the best performance is achieved\n20One might suspect that the problem is that TAC+ff+time learns from its training data to overrely on follower-count, since that is presumably a good feature for non-TAC tweets, and for this reason suffers when run on TAC data where follower-counts are by construction non-informative. But in fact, we found that removing the follower-count feature from TAC+ff+time and re-training did not lead to improved performance. Hence, it seems that it is the non-controlled nature of the alternate training data that explains the drop in performance.\nby combining our custom and 1,2-gram features together, to a degree statistically significantly better than using 1,2-gram features alone.\nFinally, we remark on our Bonferroni correction. Recall that the intent of applying it is to avoid false positives. However, in our case, Figure 2(a) shows that our potentially \u201cfalse\u201d positives \u2014 features whose effectiveness did not pass the Bonferroni correction test \u2014 actually do raise performance in our prediction tests.\nSize of training data. Another interesting observation is how performance varies with data size. For n \u201c 1000, 2000, . . . , 10000, we randomly sampled n pairs from our 11,404 pairs, and computed the average cross-validation accuracy on the sampled data. Figure 2(b) shows the averages over 50 runs of the aforementioned procedure. Our custom features can achieve good performance with little data, in the sense that for sample size 1000, they outperform BOW features; on the other hand, BOW features quickly surpass them. Across the board, the custom+1,2-gram features are consistently better than the 1,2-gram features alone.\nTop features. Finally, we examine some of the top-weighted individual features from our approach and from the competing TAC+ff+time classifier. The top three rows of Table 12 show the best custom and best and worst unigram features for our method; the bottom two rows show the best and worst unigrams for TAC+ff+time. Among custom features, we see that community and personal language models, informativeness, retweet scores, sentiment, and generality are represented. As for unigram features, not surprisingly, \u201crt\u201d and \u201cretweet\u201d are top features for both our approach and TAC+ff+time. However, the other unigrams for the two methods seem to be a bit different in spirit. Some of the unigrams determined to be most poor only by our method appear to be both surprising and yet plausible in retrospect: \u201cicymi\u201d (abbreviation for \u201cin case you missed it\u201d) tends to indicate a direct repetition of older information, so people might prefer to retweet the earlier version; \u201cthanks\u201d and \u201csorry\u201d could correspond to personal thank-yous and apologies not meant to be shared with a broader audience, and similarly @-mentioning another user may indicate a tweet intended only for that person. The appearance of [hashtag] in the best TAC+ff+time unigrams is consistent with prior research in non-TAC settings (Suh et al., 2010; Petrovic\u0301 et al., 2011)."}, {"heading": "6 Conclusion", "text": "In this work, we conducted the first large-scale topic- and author-controlled experiment to study the effects of wording on information propagation.\nThe features we developed to choose the better of two alternative wordings posted better performance than that of all our comparison algorithms, including one given access to author and timing features but trained on non-TAC data, and also bested our estimate of average human performance. According to our hypothesis tests, helpful wording heuristics include adding more information, making one\u2019s language align with both community norms and with one\u2019s prior messages, and mimicking news headlines. Readers may try out their own alternate phrasings at http: //chenhaot.com/retweetedmore/ to see what a simplified version of our classifier predicts.\nIn future work, it will be interesting to examine how these features generalize to longer and more extensive arguments. Moreover, understanding the underlying psychological and cultural mechanisms that establish the effectiveness of these features is a fundamental problem of interest.\nAcknowledgments. We thank C. Callison-Burch, C. Danescu-Niculescu-Mizil, J. Kleinberg, P. Mahdabi, S. Mullainathan, F. Pereira, K. Raman, A. Swaminathan, the Cornell NLP seminar participants and the reviewers for their comments; J. Leskovec for providing some initial data; and the anonymous annotators for all their labeling help. This work was supported in part by NSF grant IIS0910664 and a Google Research Grant."}], "references": [{"title": "Predicting responses to microblog posts", "author": ["Yoav Artzi", "Patrick Pantel", "Michael Gamon."], "venue": "Proceedings of NAACL (short paper).", "citeRegEx": "Artzi et al\\.,? 2012", "shortCiteRegEx": "Artzi et al\\.", "year": 2012}, {"title": "Success with style: Using writing style to predict the success of novels", "author": ["Vikas Ganjigunte Ashok", "Song Feng", "Yejin Choi."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Ashok et al\\.,? 2013", "shortCiteRegEx": "Ashok et al\\.", "year": 2013}, {"title": "Everyone\u2019s an influencer: Quantifying influence on twitter", "author": ["Eitan Bakshy", "Jake M. Hofman", "Winter A. Mason", "Duncan J. Watts."], "venue": "Proceedings of WSDM.", "citeRegEx": "Bakshy et al\\.,? 2011", "shortCiteRegEx": "Bakshy et al\\.", "year": 2011}, {"title": "The independent components of natural scenes are edge filters", "author": ["Anthony J. Bell", "Terrence J. Sejnowski."], "venue": "Vision research, 37(23):3327\u20133338.", "citeRegEx": "Bell and Sejnowski.,? 1997", "shortCiteRegEx": "Bell and Sejnowski.", "year": 1997}, {"title": "Controlling the false discovery rate: A practical and powerful approach to multiple testing", "author": ["Yoav Benjamini", "Yosef Hochberg."], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 289\u2013300.", "citeRegEx": "Benjamini and Hochberg.,? 1995", "shortCiteRegEx": "Benjamini and Hochberg.", "year": 1995}, {"title": "The untold story of the clones: Content-agnostic factors that impact YouTube video popularity", "author": ["Youmna Borghol", "Sebastien Ardon", "Niklas Carlsson", "Derek Eager", "Anirban Mahanti."], "venue": "Proceedings of KDD.", "citeRegEx": "Borghol et al\\.,? 2012", "shortCiteRegEx": "Borghol et al\\.", "year": 2012}, {"title": "Framing theory", "author": ["Dennis Chong", "James N. Druckman."], "venue": "Annual Review of Political Science, 10:103\u2013126.", "citeRegEx": "Chong and Druckman.,? 2007", "shortCiteRegEx": "Chong and Druckman.", "year": 2007}, {"title": "You had me at hello: How phrasing affects memorability", "author": ["Cristian Danescu-Niculescu-Mizil", "Justin Cheng", "Jon Kleinberg", "Lillian Lee."], "venue": "Proceedings of ACL.", "citeRegEx": "Danescu.Niculescu.Mizil et al\\.,? 2012", "shortCiteRegEx": "Danescu.Niculescu.Mizil et al\\.", "year": 2012}, {"title": "Natural experiments and quasinatural experiments", "author": ["John DiNardo."], "venue": "The New Palgrave Dictionary of Economics. Palgrave Macmillan.", "citeRegEx": "DiNardo.,? 2008", "shortCiteRegEx": "DiNardo.", "year": 2008}, {"title": "Multiple comparisons among means", "author": ["Olive Jean Dunn."], "venue": "Journal of the American Statistical Association, 56(293):52\u201364.", "citeRegEx": "Dunn.,? 1961", "shortCiteRegEx": "Dunn.", "year": 1961}, {"title": "Connotation lexicon: A dash of sentiment beneath the surface meaning", "author": ["Song Feng", "Jun Seok Kang", "Polina Kuznetsova", "Yejin Choi."], "venue": "Proceedings of ACL.", "citeRegEx": "Feng et al\\.,? 2013", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "A new readability yardstick", "author": ["Rudolph Flesch."], "venue": "Journal of applied psychology, 32(3):221.", "citeRegEx": "Flesch.,? 1948", "shortCiteRegEx": "Flesch.", "year": 1948}, {"title": "Part-of-speech Tagging for Twitter: Annotation, Features, and Experiments", "author": ["Kevin Gimpel", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "The firm\u2019s management of social interactions", "author": ["David Godes", "Dina Mayzlin", "Yubo Chen", "Sanjiv Das", "Chrysanthos Dellarocas", "Bruce Pfeiffer", "Barak Libai", "Subrata Sen", "Mengze Shi", "Peeter Verlegh."], "venue": "Marketing Letters, 16(3-4):415\u2013428.", "citeRegEx": "Godes et al\\.,? 2005", "shortCiteRegEx": "Godes et al\\.", "year": 2005}, {"title": "Exploring text virality in social networks", "author": ["Marco Guerini", "Carlo Strapparava", "G\u00f6zde \u00d6zbal."], "venue": "Proceedings of ICWSM (poster).", "citeRegEx": "Guerini et al\\.,? 2011", "shortCiteRegEx": "Guerini et al\\.", "year": 2011}, {"title": "Do linguistic style and readability of scientific abstracts affect their virality", "author": ["Marco Guerini", "Alberto Pepe", "Bruno Lepri"], "venue": "In Proceedings of ICWSM (poster)", "citeRegEx": "Guerini et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guerini et al\\.", "year": 2012}, {"title": "Good friends, bad news-affect and virality in Twitter", "author": ["Lars Kai Hansen", "Adam Arvidsson", "Finn \u00c5rup Nielsen", "Elanor Colleoni", "Michael Etter."], "venue": "Communications in Computer and Information Science, 185:34\u201343.", "citeRegEx": "Hansen et al\\.,? 2011", "shortCiteRegEx": "Hansen et al\\.", "year": 2011}, {"title": "Emotional selection in memes: The case of urban legends", "author": ["Chip Heath", "Chris Bell", "Emily Sternberg."], "venue": "Journal of personality and social psychology, 81(6):1028.", "citeRegEx": "Heath et al\\.,? 2001", "shortCiteRegEx": "Heath et al\\.", "year": 2001}, {"title": "Social Behavior as Exchange", "author": ["George C. Homans."], "venue": "American Journal of Sociology, 63(6):597\u2013 606.", "citeRegEx": "Homans.,? 1958", "shortCiteRegEx": "Homans.", "year": 1958}, {"title": "Predicting popular messages in Twitter", "author": ["Liangjie Hong", "Ovidiu Dan", "Brian D. Davison."], "venue": "Proceedings of WWW.", "citeRegEx": "Hong et al\\.,? 2011", "shortCiteRegEx": "Hong et al\\.", "year": 2011}, {"title": "Communication and Persuasion: Psychological Studies of Opinion Change, volume 19", "author": ["Carl I. Hovland", "Irving L. Janis", "Harold H. Kelley."], "venue": "Yale University Press.", "citeRegEx": "Hovland et al\\.,? 1953", "shortCiteRegEx": "Hovland et al\\.", "year": 1953}, {"title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel", "author": ["J. Peter Kincaid", "Robert P. Fishburne Jr.", "Richard L. Rogers", "Brad S. Chissom."], "venue": "Technical report, DTIC", "citeRegEx": "Kincaid et al\\.,? 1975", "shortCiteRegEx": "Kincaid et al\\.", "year": 1975}, {"title": "What is Twitter, a social network or a news media", "author": ["Haewoon Kwak", "Changhyun Lee", "Hosung Park", "Sue Moon"], "venue": "In Proceedings of WWW", "citeRegEx": "Kwak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kwak et al\\.", "year": 2010}, {"title": "What\u2019s in a name? Understanding the interplay between titles, content, and communities in social media", "author": ["Himabindu Lakkaraju", "Julian McAuley", "Jure Leskovec."], "venue": "Proceedings of ICWSM.", "citeRegEx": "Lakkaraju et al\\.,? 2013", "shortCiteRegEx": "Lakkaraju et al\\.", "year": 2013}, {"title": "What makes writing great? First experiments on article quality prediction in the science journalism domain", "author": ["Annie Louis", "Ani Nenkova."], "venue": "Transactions of ACL.", "citeRegEx": "Louis and Nenkova.,? 2013", "shortCiteRegEx": "Louis and Nenkova.", "year": 2013}, {"title": "Will this #hashtag be popular tomorrow", "author": ["Zongyang Ma", "Aixin Sun", "Gao Cong"], "venue": "In Proceedings of SIGIR", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "Learning to tell tales: A data-driven approach to story generation", "author": ["Neil McIntyre", "Mirella Lapata."], "venue": "Proceedings of ACL-IJCNLP.", "citeRegEx": "McIntyre and Lapata.,? 2009", "shortCiteRegEx": "McIntyre and Lapata.", "year": 2009}, {"title": "What makes online content viral", "author": ["Katherine L Milkman", "Jonah Berger"], "venue": "Journal of Marketing Research,", "citeRegEx": "Milkman and Berger.,? \\Q2012\\E", "shortCiteRegEx": "Milkman and Berger.", "year": 2012}, {"title": "RT to win! Predicting message propagation in Twitter", "author": ["Sa\u0161a Petrovi\u0107", "Miles Osborne", "Victor Lavrenko."], "venue": "Proceedings of ICWSM.", "citeRegEx": "Petrovi\u0107 et al\\.,? 2011", "shortCiteRegEx": "Petrovi\u0107 et al\\.", "year": 2011}, {"title": "Revisiting readability: A unified framework for predicting text quality", "author": ["Emily Pitler", "Ani Nenkova."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Pitler and Nenkova.,? 2008", "shortCiteRegEx": "Pitler and Nenkova.", "year": 2008}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Riloff et al\\.,? 2013", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": "On the interplay between social and topical structure", "author": ["Daniel M. Romero", "Chenhao Tan", "Johan Ugander."], "venue": "Proceedings of ICWSM.", "citeRegEx": "Romero et al\\.,? 2013", "shortCiteRegEx": "Romero et al\\.", "year": 2013}, {"title": "Experimental study of inequality and unpredictability in an artificial cultural market", "author": ["Matthew J. Salganik", "Peter Sheridan Dodds", "Duncan J. Watts."], "venue": "Science, 311(5762):854\u2013856.", "citeRegEx": "Salganik et al\\.,? 2006", "shortCiteRegEx": "Salganik et al\\.", "year": 2006}, {"title": "Memes online: Extracted, subtracted, injected, and recollected", "author": ["Matthew P. Simmons", "Lada A Adamic", "Eytan Adar."], "venue": "Proceedings of ICWSM.", "citeRegEx": "Simmons et al\\.,? 2011", "shortCiteRegEx": "Simmons et al\\.", "year": 2011}, {"title": "Want to be retweeted? Large scale analytics on factors impacting retweet in Twitter network", "author": ["Bongwon Suh", "Lichan Hong", "Peter Pirolli", "Ed H. Chi."], "venue": "Proceedings of SocialCom.", "citeRegEx": "Suh et al\\.,? 2010", "shortCiteRegEx": "Suh et al\\.", "year": 2010}, {"title": "Unexpected relevance: An empirical study of serendipity in retweets", "author": ["Tao Sun", "Ming Zhang", "Qiaozhu Mei."], "venue": "Proceedings of ICWSM.", "citeRegEx": "Sun et al\\.,? 2013", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "What\u2019s in a hashtag?: Content based prediction of the spread of ideas in microblogging communities", "author": ["Oren Tsur", "Ari Rappoport."], "venue": "Proceedings of WSDM.", "citeRegEx": "Tsur and Rappoport.,? 2012", "shortCiteRegEx": "Tsur and Rappoport.", "year": 2012}, {"title": "Patterns of temporal variation in online media", "author": ["Jaewon Yang", "Jure Leskovec."], "venue": "Proceedings of WSDM.", "citeRegEx": "Yang and Leskovec.,? 2011", "shortCiteRegEx": "Yang and Leskovec.", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "How does one make a message \u201csuccessful\u201d? This question is of interest to many entities, including political parties trying to frame an issue (Chong and Druckman, 2007), and individuals attempting to make a point in a group meeting.", "startOffset": 142, "endOffset": 168}, {"referenceID": 8, "context": "in which BarackObama tweeted something else1, fortunately, a surprising characteristic of Twitter allows us to run a fairly analogous natural experiment: external forces serendipitously provide an environment that resembles the desired controlled setting (DiNardo, 2008).", "startOffset": 255, "endOffset": 270}, {"referenceID": 32, "context": "the Music Lab \u201cmultiple universes\u201d experiment to test the randomness of popularity (Salganik et al., 2006).", "startOffset": 83, "endOffset": 106}, {"referenceID": 20, "context": "The idea of using carefully controlled experiments to study effective communication strategies dates back at least to Hovland et al. (1953). Recent studies range from examining what characteris-", "startOffset": 118, "endOffset": 140}, {"referenceID": 27, "context": "tics of New York Times articles correlate with high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al.", "startOffset": 69, "endOffset": 95}, {"referenceID": 5, "context": "tics of New York Times articles correlate with high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013).", "startOffset": 198, "endOffset": 244}, {"referenceID": 23, "context": "tics of New York Times articles correlate with high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013).", "startOffset": 198, "endOffset": 244}, {"referenceID": 1, "context": "Predicting the \u201csuccess\u201d of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in \u00a71 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009).", "startOffset": 144, "endOffset": 280}, {"referenceID": 24, "context": "Predicting the \u201csuccess\u201d of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in \u00a71 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009).", "startOffset": 144, "endOffset": 280}, {"referenceID": 7, "context": "Predicting the \u201csuccess\u201d of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in \u00a71 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009).", "startOffset": 144, "endOffset": 280}, {"referenceID": 29, "context": "Predicting the \u201csuccess\u201d of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in \u00a71 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009).", "startOffset": 144, "endOffset": 280}, {"referenceID": 26, "context": "Predicting the \u201csuccess\u201d of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in \u00a71 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009).", "startOffset": 144, "endOffset": 280}, {"referenceID": 4, "context": "tics of New York Times articles correlate with high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013). Simmons et al. (2011) examined the variation of quotes from different sources to examine how textual memes mutate as people pass them along, but did not control for author.", "startOffset": 199, "endOffset": 268}, {"referenceID": 22, "context": "6 We accomplished this by crawling timelines of 236K user ids that appear in prior work (Kwak et al., 2010; Yang and Leskovec, 2011) via the Twitter API.", "startOffset": 88, "endOffset": 132}, {"referenceID": 37, "context": "6 We accomplished this by crawling timelines of 236K user ids that appear in prior work (Kwak et al., 2010; Yang and Leskovec, 2011) via the Twitter API.", "startOffset": 88, "endOffset": 132}, {"referenceID": 5, "context": "But even with a TAC pair, retweets might exhibit a temporal bias because of the chronological order of tweet presentation (t1 might enjoy a first-mover advantage (Borghol et al., 2012) because it is the \u201coriginal\u201d; alternatively,", "startOffset": 162, "endOffset": 184}, {"referenceID": 28, "context": "2% reported by Petrovi\u0107 et al. (2011), who ran a sim-", "startOffset": 15, "endOffset": 38}, {"referenceID": 9, "context": "Given that we did 39 tests in total, there is a risk of obtaining false positives due to multiple testing (Dunn, 1961; Benjamini and Hochberg, 1995).", "startOffset": 106, "endOffset": 148}, {"referenceID": 4, "context": "Given that we did 39 tests in total, there is a risk of obtaining false positives due to multiple testing (Dunn, 1961; Benjamini and Hochberg, 1995).", "startOffset": 106, "endOffset": 148}, {"referenceID": 12, "context": "Table 3: Explicit requests for sharing (where only occurrences POS-tagged as verbs count, according to the Gimpel et al. (2011) tagger).", "startOffset": 107, "endOffset": 128}, {"referenceID": 18, "context": "(Table 4) Messages that are more informative have increased social exchange value (Homans, 1958), and so may be more worth propagating.", "startOffset": 82, "endOffset": 96}, {"referenceID": 18, "context": "(Table 4) Messages that are more informative have increased social exchange value (Homans, 1958), and so may be more worth propagating. One crude approximation of informativeness is length, and we see that length helps.14 In contrast, Simmons et al. (2011) found that shorter versions of memes are more likely to be popular.", "startOffset": 83, "endOffset": 257}, {"referenceID": 12, "context": "Our POS results, gathered using a Twitter-specific tagger (Gimpel et al., 2011), echo those of Ashok et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 1, "context": ", 2011), echo those of Ashok et al. (2013) who looked at predict-", "startOffset": 23, "endOffset": 43}, {"referenceID": 34, "context": "The diminished effect of hashtag inclusion with respect to what has been reported previously (Suh et al., 2010; Petrovi\u0107 et al., 2011) presumably stems from our topic and author controls.", "startOffset": 93, "endOffset": 134}, {"referenceID": 28, "context": "The diminished effect of hashtag inclusion with respect to what has been reported previously (Suh et al., 2010; Petrovi\u0107 et al., 2011) presumably stems from our topic and author controls.", "startOffset": 93, "endOffset": 134}, {"referenceID": 22, "context": "explored this tension: Lakkaraju et al. (2013), in a content-controlled study, found that the more upvoted Reddit image titles balance novelty and familiarity; Danescu-Niculescu-Mizil et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 7, "context": "(2013), in a content-controlled study, found that the more upvoted Reddit image titles balance novelty and familiarity; Danescu-Niculescu-Mizil et al. (2012) (henceforth DCKL\u201912) showed that the memora-", "startOffset": 120, "endOffset": 158}, {"referenceID": 35, "context": "bility of movie quotes corresponds to higher lexical distinctiveness but lower POS distinctiveness; and Sun et al. (2013) observed that deviating from one\u2019s own past language patterns correlates with more retweets.", "startOffset": 104, "endOffset": 122}, {"referenceID": 27, "context": "(Table 8) Prior work has found that including positive or negative sentiment increases message propagation (Milkman and Berger, 2012; Godes et al., 2005; Heath et al., 2001; Hansen et al., 2011).", "startOffset": 107, "endOffset": 194}, {"referenceID": 13, "context": "(Table 8) Prior work has found that including positive or negative sentiment increases message propagation (Milkman and Berger, 2012; Godes et al., 2005; Heath et al., 2001; Hansen et al., 2011).", "startOffset": 107, "endOffset": 194}, {"referenceID": 17, "context": "(Table 8) Prior work has found that including positive or negative sentiment increases message propagation (Milkman and Berger, 2012; Godes et al., 2005; Heath et al., 2001; Hansen et al., 2011).", "startOffset": 107, "endOffset": 194}, {"referenceID": 16, "context": "(Table 8) Prior work has found that including positive or negative sentiment increases message propagation (Milkman and Berger, 2012; Godes et al., 2005; Heath et al., 2001; Hansen et al., 2011).", "startOffset": 107, "endOffset": 194}, {"referenceID": 10, "context": "We measured the occurrence of positive and negative words as determined by the connotation lexicon of Feng et al. (2013) (better coverage than LIWC).", "startOffset": 102, "endOffset": 121}, {"referenceID": 10, "context": "We measured the occurrence of positive and negative words as determined by the connotation lexicon of Feng et al. (2013) (better coverage than LIWC). Measuring the occurrence of both simultaneously was inspired by Riloff et al. (2013).", "startOffset": 102, "endOffset": 235}, {"referenceID": 15, "context": "(Table 9) First-person has been found useful for success before, but in the different domains of scientific abstracts (Guerini et al., 2012) and books (Ashok et al.", "startOffset": 118, "endOffset": 140}, {"referenceID": 1, "context": ", 2012) and books (Ashok et al., 2013).", "startOffset": 18, "endOffset": 38}, {"referenceID": 11, "context": "(Table 11) We measure readability by using Flesch reading ease (Flesch, 1948) and Flesch-Kincaid grade level", "startOffset": 63, "endOffset": 77}, {"referenceID": 21, "context": "(Kincaid et al., 1975), though they are not designed for short texts.", "startOffset": 0, "endOffset": 22}, {"referenceID": 34, "context": "The appearance of [hashtag] in the best TAC+ff+time unigrams is consistent with prior research in non-TAC settings (Suh et al., 2010; Petrovi\u0107 et al., 2011).", "startOffset": 115, "endOffset": 156}, {"referenceID": 28, "context": "The appearance of [hashtag] in the best TAC+ff+time unigrams is consistent with prior research in non-TAC settings (Suh et al., 2010; Petrovi\u0107 et al., 2011).", "startOffset": 115, "endOffset": 156}], "year": 2014, "abstractText": "Consider a person trying to spread an important message on a social network. He/she can spend hours trying to craft the message. Does it actually matter? While there has been extensive prior work looking into predicting popularity of socialmedia content, the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic. To control for these confounding factors, we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording. Given such pairs, we ask: which version attracts more retweets? This turns out to be a more difficult task than predicting popular topics. Still, humans can answer this question better than chance (but far from perfectly), and the computational methods we develop can do better than both an average human and a strong competing method trained on noncontrolled data.", "creator": "LaTeX with hyperref package"}}}