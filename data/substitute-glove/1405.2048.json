{"id": "1405.2048", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2014", "title": "Learning Alternative Name Spellings", "abstract": "Name matching but next key industrial of conventional work entity outlining or streak linkage. Alternative abbreviation more the same names mainly a com - wed occurrence one even files. We use the worldwide drawings of genealogy only 2009 2004 another world way with user surveillance garbled bamboo means build probably wide styling. The procedure for designed another demonstrators - sourced training came another outline three set the presentation in lot computation. We cast their whatever made learning content pronunciations may on machine refers cannot down entered love level. We use, - occurs end-user analyses methodology only show well future method substantially apmc on ensure sample a twelve means standard well name iconographic and string similarity methods under significant however precision often re - call. Additionally, we resourced compare the performance that standard devised twice compared put except use. Our difference enough lead well a significant means impact in entity resolution applications.", "histories": [["v1", "Wed, 7 May 2014 19:47:51 GMT  (727kb,D)", "http://arxiv.org/abs/1405.2048v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["jeffrey sukharev", "leonid zhukov", "alexandrin popescul"], "accepted": false, "id": "1405.2048"}, "pdf": {"name": "1405.2048.pdf", "metadata": {"source": "CRF", "title": "Learning Alternative Name Spellings", "authors": ["Jeffrey Sukharev", "Leonid Zhukov", "Alexandrin Popescul"], "emails": ["jsukharev@ancestry.com", "lzhukov@ancestry.com", "apopescul@ancestry.com"], "sections": [{"heading": "1 Introduction", "text": "A person\u2019s name, especially the family name, is the key field used in identifying person\u2019s records in databases. Software tools and applications designed for entity resolution of a person\u2019s records usually rely on the person\u2019s name as a primary identification field. In particular, genealogy services provide user access to a person\u2019s record databases and facilitate search for a user\u2019s ancestors/relatives and other person/records of interest. A persons\u2019s records indicate some type of a life event including birth, death, marriage or relocation. Typically, records are indexed by some unique identifier and can\n1\nar X\niv :1\n40 5.\n20 48\nv1 [\ncs .I\nR ]\n2 also be searched by a combination of last/first names, geographical locations and event dates. Searching a record database is complicated by the user not knowing the exact spelling of the name in the record they are searching for. This task becomes even harder since databases often contain alternate spellings referring to the same person. This transpires due to many factors including optical character recognition errors when scanning the records, errors/misspelling of names in the records themselves, and name transliterations. For instance a common last name \u201cShepard\u201d has been also commonly spelled as \u201cShepherd\u201d, \u201cSheppard\u201d, \u201cShephard\u201d, \u201cShepperd\u201d, \u201cSheperd\u201d. Clearly, having methods that would provide users and the search engines with a list of highly credible misspelling of the query name would significantly improve the search results.\nKnowing how to misspell names to find records of interest has always been a part of a professional genealogist\u2019s domain expertise. In this paper we try to bridge this gap and bring this power to the average user by employing data-driven methods that rely on our unique data set1. Through Ancestry.com we have access to the world\u2019s largest genealogy data repository. The main function of a genealogy service is to facilitate discovery of relevant person\u2019s records and the construction of family trees. Tree construction involves user attaching relevant scanned/digitized records, found in Ancestry.com databases, to user-generated tree node. Having records attached to individual tree nodes affords us an opportunity to collect misspellings of names. By leveraging user-provided links between individual user family tree nodes and scanned attached records, we generated a \"labeled\" dataset of name pairs where the left side of each pair comes from user supplied person names and the right part of the pair comes from the attached record name field. All user-identifying information except for last name pairs is discarded from the final dataset. We filter and pre-process this list of name pairs and use it to train a model using standard machine translation methods. We will go into more details on data pre-processing in Section 5. Additionally, we generated another dataset from company search logs. Often, users modify a previous search query in hope of getting better results. These user-driven modifications are called query reformulations. By identifying logged-in user sessions and extracting names from user queries in sequential order from the same session in a specified time interval and using our assumption that users frequently search for variations of the same name we have been able to accumulate a large number of name pairs that could also be used as a training/testing data for our models.\n1Data is available for research purposes from the authors\n3 As a result of our experiments we produce ranked candidate variant spellings for each query name. In addition to providing the translation model we also propose a methodology, adapted from the information retrieval community, for evaluating of the final candidate list and for comparing it with other methods.\nIn the results section we will show that our methods perform significantly better than other state-of-the-art methods in terms of precision and recall in identifying a quality lists of alternative name spellings.\nThe remainder of the paper is organized as following. In section 2 we discuss the numerous previous works in related fields. A detailed description of our training data is given in Section 3. In Section 4 we outline the machine translation method used in training our model. We then discuss our results and present comparisons with other methods in Section 5 and conclude in Section 6."}, {"heading": "2 Previous work", "text": "The classic reference in the field of record linkage is a paper by Fellegi and Sunter [14] published in 1969. In their work the authors have carefully presented the theory of record matching. They defined the terms of positive disposition (link and non-link) and negative disposition (possible link) and showed that the optimal record matching linkage rule would minimize the possibility of failing to make a positive disposition for fixed levels of errors. Since this seminal work there has been proliferation of work in this area. In the interest of brevity we direct the reader to the outstanding 2006 survey paper by Winkler [32] and to the comprehensive work by Christen [10] published just recently.\nWith the explosive growth of data coming from web applications it is becoming imperative to discover the best methods for record matching in terms of accuracy and speed. Historically, methods focusing on name matching could be separated into two classes: sequential character methods and bag-of-words methods [23]."}, {"heading": "2.1 Sequential Character methods", "text": "Phonetic similarity methods are an important category of sequential character methods. The key concept of phonetic methods is to map n-grams of characters into phonetic equivalents. The output of using these methods on string pairs is a binary decision and not a degree of similarity. The bestknown method from this class is Soundex [28]. Over the years a numerous\n4 improvements of this approach have been made. In particular some of them had to do with accommodating non-English names. Popular methods include Soundex [28], Double Metaphon [26], and NYSIIS [30]. While these methods proved to be useful in improving performance in data matching applications they do not solve the problem of relevance ranking of alternative spellings, which is of great importance for search engines when considering using alternative name spellings for query expansion.\nAnother important category of sequential character methods often used in conjunction with phonetic methods is the class of static string similarity measures. Similarity method based on edit distance (the Levenshtein distance, as it is also known [20]) is the most well-known method of this type. The edit distance between strings s and t is the cost of the optimal shortest sequence of edit operations (substitute, add, delete) that converts s to t. For instance, the mapping of s = \u201dJohnson\u201d to t = \u201dJohnston\u201d results in one addition of letter \"t\" and hence, results in a distance of one. Other common similarity measures include the Jaro [16] method which takes into account the number and order of common characters between two strings and the Jaro-Winkler [31] method which extends Jaro by accounting for the common prefixes in both strings [3], [10]. The static similarity measures described above, while useful in measuring similarity and ranking alternative spellings, are not capable of generating alternative spellings. This capability is typically absent from all methods that do not take a dataset\u2019s statistical information into account.\nIn 2013 Bradford [5] published a paper dealing with alternative name spelling generation. He used latent semantic indexing that uses Singular Value Decomposition (SVD) method to identify patterns in the relationships between the terms in unstructured collection of texts.\nBecause of the difficulty associated with obtaining the experimental data many researchers build their own synthetic datasets by utilizing specialized tools for mining the web and extracting words that appear to be last names. The resulting names are used in forming artificial pairs using one or more similarity measures (typically based on edit distance). Another popular alternative is to hire human annotators who create last name pairs based on their knowledge of name misspelling. Both of these methods may introduce bias.\nOur data is being produced by millions of users who act as human annotators and who should be experts in their own genealogy and are motivated to build quality content. Due to the nature of our dataset we can extract best pairs using frequency statistics. We will go into more detail about our filtering process later in this paper. Having frequency information allows us\n5 to assemble realistic distribution of name pairs and helps in training more accurate models of alternative name spellings."}, {"heading": "2.2 Bag-of-words methods", "text": "Bag-of-words methods typically represent strings as a set of words or word ngrams. There were numerous studies published on the topic of applying bags of words to record linkage over the last decade [23]. Cosine similarity of term frequency inverse document frequency (TFIDF) weighted vectors is one of the most popular methods of this type. Typical vectors consist of individual words or n-grams. The main shortcoming of cosine similarity TFIDF is that this method requires exact matches between fields. To alleviate this issue cosine similarity SoftTFIDF was introduced by Cohen et. al. [11]. In addition to counting identical fields occurring in both vectors SoftTFIDF compares and keeps track of \"similar\" fields in both vectors. Bilenko et. al. [3] showed how machine learning methods could be successfully employed for learning the combined field similarity. They trained an SVM classifier using feature vectors, and then applied the learned classifier\u2019s confidence in the match as a class score. In this paper we do not consider these approaches because we primarily work with single word last names and bag-of-words methods are more suited for finding similarity between multi-field records."}, {"heading": "2.3 Spelling correction and Machine Translation literature", "text": "In the 1990 Kernighan et. al. [18] in their short paper proposed a method for spelling corrections based on noisy channels. The same formulation would latter be used in machine translation field. The basic idea was to find best possible correction by optimizing the product of language model ( a prior probability of letters/phrases/words in a given language) and correction model (likelihood of one word being spelled as another). For comprehensive survey of spelling correction methods the reader should look at the excellent chapter on this topic at Jurafsky and Martin 2008 Speech and Language Processing book. [17]\nIn the last several decades machine translation methods have gained significant traction and recently found their way into the problem of name matching. In 2007 Bhagat et. al. [2] implemented a transducer based method for finding alternative name spellings by employing a graphemesto-phonemes framework. Their method involved running EM (expectation maximization) algorithm, first presented by Dempster [12], to align text from the CMU dictionary with their phoneme sequence equivalents. Next, they\n6 built a character language model of phoneme trigrams using the same CMU dictionary phonemes. Their training set was mined from the web. Using both-ways translation models and language models, the authors were able to generate alternative phoneme sequences (pronunciations), given a character string name, and then each of these sequences was converted into an alternative character sequence [25].\nIn 1996 Ristad and Yianilos [27] presented an interesting solution where they learned the cost of edit distance operations, which are normally all set to one in static edit distance algorithms. The authors used expectation maximization algorithm for training. Their model resulted in the form a transducer. Bilenko et. al. [4] improved on Ristad and Yianilos\u2019s learned edit distance model by including affine gaps. They also presented a learned string similarity measure based on unordered bags of words, using SVM for training. McCallum et. al. [22] in 2005 approached the same problem from the different angle. Instead of using generative models like [27] and [4] they have used discriminative method, conditional random fields (CRF), where they have been able to use both positive and negative string pairs for training."}, {"heading": "3 Datasets", "text": "Ancestry.com has over the years accumulated over 11 billion records and over 40 million personal family trees [1]. Most of the records in the Ancestry.com database originate from the Western European countries, United States, Canada, Australia, and New Zealand. Scanned collections of census data, and personal public and private documents uploaded by company users comprise the bulk of Ancestry.com datasets. One of the key features of the Ancestry.com web site is the facility for building personal family trees with an option for searching and attaching relevant documents from record databases to the relevant parts of family trees. For example if a family tree contains a node for a person with the name John Smith it would be often accompanied by the birth record, relocation record, marriage record and other records discovered by the owner of the family tree. Since most of the nodes in the deep family trees involve persons who are no longer living, death records can often be discovered and attached to the appropriate nodes.\nThis linkage between user-specified family tree nodes and the official records present us with a unique opportunity to assemble a parallel corpus of pairs of names, hand-labeled by the users themselves. In the past researchers working on name matching problem were forced to assemble their\n7 training datasets by employing text mining techniques. Very often a specific method was needed for identifying names in a given text and then edit distance measure was used to find a list of misspelling candidates. Additionally, in some studies, a small number of dedicated human labelers provided additional level of confidence. These methods would inevitably lead to bias. We believe that our user-labeled dataset contains significantly less bias than previously used training datasets.\nDue to the availability of the \u201clabeled\u201d dataset in the Ancestry.com we have a more direct way of generating training data. From the begininning we realized that we could not employ standard supervised machine learning methods for finding alternative name spellings since that would require us to collect positive and negative training sets. While it would have been possible to mine positive sets from user-labeled data, defining the process generating realistic negative examples is ambiguous at best. This would require us finding name pairs that would not be alternative spellings of each other with a high degree of confidence. Even through it may seem doable at first glance this a very tricky proposition. First of all how would we choose each pair item? What is the distribution of negative pairs? We only have user labels for positive pairs, but not having user label for a name pair does not necessarily mean that the pair is negative. Not having any other alternatives we would have to bias our negative set to some kind of similarity measure like the Levenshtein method and this would force us to arbitrary select a threshold that would distinguish negative pairs from positive pairs. However, besides introducing bias this method would make us miss numerous negative pairs which would have high similarity values but would not constitute a positive common misspellings. Due to having this obstacle in front of us, we turned toward machine translation methods because only a parallel corpus was needed to train the translation model.\nGiven the way Ancestry.com users interact with the genealogy service, we isolated two separate ways of collecting parallel corpus data that would later be used for training translation models and for testing. We felt that having two completely different underlying processes for generating our datasets would strengthen our case if we arrived at similar conclussions.\nThe first process of assembling a parallel corpus consists of collecting all directed pairs of names drawn from anonymized (striped from all user identifying information except last names) user tree nodes and their attached anonymized records. We chose pair direction as following: last names on the left come from tree nodes and last names on the right come from the records. Since last names in records and tree nodes have different distribution taking directionality into account is important when choosing the training set of\n8 pairs. A number of filtering steps have been applied in order to de-noise the datasets and will be discussed in more detail in the later sections. The pairs are directed which implies that a pair \u201cJohansson\u201d - \u201cJohanson\u201d would be different from the reverse pair \u201cJohanson\u201d-\u201cJohansson\u201d. This would manifest in separate co-occurrence count for each pair.\nThe second process for building a parallel corpus involves using recorded user search queries. Since the Ancestry.com search query form asks the user for specific fields when searching for trees or records, we have been able to extract user queries containing names from a search log. By grouping users by their loginname, sorting the queries in chronological order, and fixing the time interval at a generous 30 minutes, we have been able to extract directed pairs of names that users use in their search queries. Our build-in assumption is that frequently users do not find what they are looking for on their first attempt and if that is the case they try again. The resulting data set is also noisy and requires extensive filtering before being used as a training set. Each pair has a direction from an older name spelling to a newer reformulation. For example if a user A searches for name \u201cShephard\u201d at time t0 and then searches for name \u201cShepperd\u201d at time t1 where t1\u2212 t0 < 30 then the resulting pair will be: \u201cShephard\u201d - \u201cShepperd\u201d and not the other way around.\nTable 1 provides an illustration of a sample of \u201crecords\u201d dataset grouped by Levenshtein edit distance and sorted by co-occurrence count. The distribution of values of edit distances between names in each pair and types of individual edit operations needed to transform left-hand member of a pair into a right-hand member are shown on Tables 2 and 3 for each dataset. We also demonstrate the breakdown of unique last names by their country of origin on Tables 4 and 5 for both datasets. Country of origin information was gathered from person tree nodes. Each person\u2019s node contains person\u2019s place of birth in addition to first and last names. The most common country of birth was selected as a name\u2019s country of origin. Only the \u201cOld World\u201d countries were chosen in order to avoid mixing names from different regions which are present in the \u201cNew World\u201d countries.\n9\n10"}, {"heading": "4 Methods", "text": "The problem of finding best alternative name spellings given a source name can be posed as maximization of conditional probability P (tname|sname) where tname is a target name and sname is a source name. Following the traditions of statistical machine translation methods [6] this probability can be expressed using Bayes\u2019 rule as\nP (tname|sname) = P (sname|tname) \u2217 P (tname)\nP (sname)\nwhere P (tname) is a \u201dname model\u201d (corresponds to language model in machine translation literature) and describes frequencies of particular name/language constructs. P (sname) is a probability of a source name. P (sname|tname) corresponds to alignment model.\n\u201dName model\u201d can be estimated using character n-grams language model representation by finding the probabilities using the chain rule [29]):\nP (c1c2...cm) = m\u220f i=1 P (ci|cmax(1,i\u2212(n\u22121)), ..., cmax(1,i\u22121))\nwhere ci is an ith character in the sequence of characters that comprise a name of length m. n-gram model computes a probability of a character sequence where each subsequent character depends on n\u22121 previous characters in the sequence.\nAn \u201calignment model\u201d is used in generating translational correspondences between names in our context and it can be best described by an example shown on Figure 1. Here the name \u201cThorogood\u201d is aligned with the name \u201cThoroughgood\u201d. Looking at the Figure 1 we can clearly see that second occurrence of letter \u2019o\u2019 in \u201cThorogood\u201d alignes with two letters (\u2019o\u2019 and \u2019u\u2019) in \u201cThoroughgood\u201d, similar situation happens with the last letter \u2019g\u2019 in \u201cThorogood\u201d which gets aligned with 2 letters \u2019g\u2019 in \u201cThoroughgood\u201d. Other letters in \u201cThorogood\u201d aligned 1-to-1 with letters in \u201cThoroughgood\u201d. Letter \u2019h\u2019 in \u201cThoroughgood\u201d does not align with anything in \u201cThorogood\u201d. Estimating \u201calignment model\u201d results in generation alignement rules such as the ones that we just presented.\nWe are not only interested in the best alternative spelling given by arg max\ntname P (tname|sname), but also in the ranked list of best suggestions, that can be computed from the same distribution by sorting probabilities in decreasing order:\n11\nK arg max tname P (tname|sname) = K arg max tname P (tname) \u2217 P (sname|tname)\nwhere arg Kmax tname represents operator that finds top K tnames that maximize P (tname|sname). Finding P (tname|sname) accurately without using the equation above would be challenging. However, using Baye\u2019s rule and breaking down P (tname|sname) into language model P (tname) and alignment model P (sname|tname) allows us to get a theoretically good translation even if underlying probabilities are not that accurate [6]. P (sname) is fixed and does not depend on the optimization variable tname and hence, will not influence the outcome and can be discarded.\nTo find probability values corresponding \u201cname model\u201d and \u201calignment model\u201d we will be using tools developed by machine translation community, replacing sentences with names and words with characters.\nFor training of our language and alignment models we have chosen the Moses software package which is a widely known open-source statistical machine translation software package [19]. Moses is a package that contains various tools needed in translation process. Typically, translation software deals with words in a sentence as primary tokens, since we compare individual last names we had to transform our input to a format recognizable by Moses while also maintaining characters as primary tokens. In our case single words become sentences and characters become words in the sentence.\nWhen using the Moses software package we chose to use Moses\u2019 Baseline System training pipeline. It includes several stages:\n1. Preparing the dataset: tokenization, truecasing and cleaning. Tokenization involves including spaces between every character. Truecasing and cleaning deals with lowercasing each string and removing all non-alphabetic characters among other things.\n2. Language model training. A language model is a set of statistics generated for an n-gram representation built with the target language. We used IRSTLM [13], a statistical language model tool for this purpose. As a result we generated 2-gram through 6-gram language models (6 was the maximum possible). This step adds \u201dsentence\u201d boundary (\u201dword\u201d boundary here) symbols and, also as in the Baseline System, uses improved Kneser-Ney smoothing. We follow a common practice in machine translation where all examples of the target language, and not only forms present in parallel corpus translation pairs, are used to\n12\nconstruct a language model. It is, therefore, based on a larger data set, and can lead to an improved translation quality. In our experiments with search logs and tree attachment datasets we used their respective lists of 250,000 most frequent surname forms for language model estimation.\n3. Alignment model building: Moses uses the GIZA++ package for statistical character-alignment [24] character (Word)-alignment tools typically implement one of Brown\u2019s IBM generative models [7] that are being used for determining translation rules for source language to the target language (including fertility rules: maximum number of target characters generated from one source character and so on) We created alignment model, for each of the 2-gram through 6-gram language models created in the previous step. As in the Baseline System, the \u201d-alignment\u201d option was set to \u201dgrow-diag-final-and\u201d and the \u201d-reordering\u201d option was set to \u201dmsd-bidirectional-fe\u201d\n4. Testing. We tested decoding on test folds in a batch mode with an option \u201d-n-best-list\u201d to give top 1000 distinct translations. This value was chosen large to well represent the high recall area on respective precision-recall curves. It is possible that using different Moses configuration could give even more accurate results.\nWe basically followed the Baseline System with the exception of tuning the phase and replacing our source and target languages with sequences of characters and instead of sequences of words. The tuning phase consists of steps optimizing the default model weights used in the training phase. We have omitted this phase because based on our initial tests, it didn\u2019t give immediate accuracy improvements on our datasets and it is relatively slow."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Data Preparation", "text": "Since we dealt with user-generated data we had to devise an algorithm for treating the data and generating a high confidence training set. We outlined the following procedure:\n13\nOperations \u201cSearch\u201d ops type % \u201cRecords\u201d ops type %\ndeletes 32.18 % 38.18 % inserts 33.91 % 20.65 % replaces 33.91 % 41.47 %\n14\nT h o r o u g h g o o d\nT h o r o g o o d"}, {"heading": "5.2 Experiments and results", "text": "Comparing phonetic methods with similarity measures and with machine translation methods is not straightforward. Phonetic methods only allow for binary responses (match or mismatch) when applied to name pairs. Therefore, it is impossible to rank positive matches without introducing additional ranking criteria. Our machine translation (MT) method produces a score that we use in ranking. Similarity methods produce a similarity value that is also used in ranking. To get a meaningful comparison of these methods with phonetic methods we had to make use of statistics that we gathered while processing datasets. Additionally, we devised a unified methodology that could be applied to all listed method types.\nIn all our experiments we used 10-fold cross validation for evaluating how the results of predictions generalize to previously unseen data.\nWe randomly divided each dataset (\u201csearch\u201d and \u201crecords\u201d) into ten folds. We train on 9 folds, then test on the remaining 1. This process was repeated\n15\n10 times for each test fold. Training folds were used to train the MT models. The same test folds were used to test all methods, including MT generated models, phonetic methods and similarity measures.\nGenerating results involves building a consistent metric that can be plotted and compared between different methods. We adapted a standard information retrieval performance metric: precision and recall.\nPrecision = TP\nTP + FP\nRecall = TP\nTP + FN\nwhere TP are true positives, FN are false negatives and FP are false negatives. The methods with larger precision and recall are superior.\nEach test fold contains a source name and one or more target names associated with each source name. Each of our methods for each source name would produce its own list of target names. Since the number of suggested target names (or alternative spellings) can be large we needed to find a suitable method for ranking target names. For all target names for position/rank i in the range from 1 toN corresponding recalli and precisioni are calculated. So, we had to agree on what precisely we mean by rank for phonetic methods, similarity methods and machine translation methods.\nWe decided to view ranking as the product of\nrank(s, t) = alignmentScore(s, t) \u2217 languageModelScore(t)\nFor the machine translation method (generated using the Moses software library) we used model-applier output scores which already contain the product of language model score and alignment score. For machine translation where character is a word :\nrank(s, t) = mosesScore(s, t)\nFor phonetic algorithms languageModelScore(t) is the frequency of a name in the dataset (freq).\nrank(s, t) = hasSameCode(s, t) \u2217 freq(t)\nwhere hasSameCode(s, t)\u2192 {0, 1} and freq(t) represents the frequency of name t in the dataset.\n16\nFor similarity measures we also used name frequency, but we had to experimentally find a suitable exponential constant \u03b3 to avoid over-penalizing low-frequency names.\nrank(s, t) = sim(s, t) \u2217 freq(t)\u03b3\nwhere sim(s, t)\u2192 [0, 1] represents the floating point similarity values and \u03b3 is the exponential constant used to control the frequency values. We used \u03b3 value 0.001.\nAfter saving precomputed sorted (according to ranking) lists of alternative name spellings for each method (phonetic, similarity, MT methods) we computed Precision and Recall values for each position from 1 to N separately for each test fold.\nAfter producing 10 precision-recall curves for each method we needed to find a suitable way to visualize confidence in our results without actually drawing 10 curves per method.\nInspired by the work of [21] we designed our own methodology for robust statistical comparisons of our precision-recall curves. Using our ten folds we evaluated confidence bands for each method. Assuming test examples are drawn from the same, fixed, multivariate normal distribution, the expectation is that the model\u2019s precision-recall curves will fall within the bands with probability of at least 1\u2212\u03b4 where \u03b4 represents the significance level. We need to find the standard deviation of the sample which is the degree to which individual points within the sample differ from the sample mean.\nThe density contours of multivariate normal distribution of precision and recall pairs are ellipses centered at the mean of the sample. The eigenvectors of the covariance matrix \u03a3 are used as directions of the principal axes of the Gaussian ellipses [15]. For our collection of 2D precision/recall pairs X = (X1, X2)\n\u03a3ij = E[(Xi \u2212 \u00b5i)(Xj \u2212 \u00b5j)]\nThe average values of ten points \u00b51 and \u00b52 have given us centroid curve for each method and the center of density contours.\nThe standard deviation for each vector direction is found by taking the Cholesky decomposition of the covariance matrix and using the resulting matrix for generating elliptical contours of a two dimensional normal distribution. To capture the 95% confidence level in 2D we need to multiply each \u03c3 by the multiplier. We get the squared \u03c3-multiplier value from the Chi Square Distribution (\u03c72) table for 2 degrees of freedom where the Chi Square Distribution is the distribution of the sum of squared independent\n17\nstandard normal variables. \u03c3 multiplier equals to 2.447 in this case. See Figure 5 for a visualization and further explanation of confidence bands.\nThe resulting bands are formed by connecting by line segment endpoints of the longest principle axis of each ellipse with its corresponding neighbor ellipses. The resulting bands give us a visual cue regarding the variance of precision/recall (PR) curves produced for different data test folds. Also, the resulting bands have at least 95% confidence level because data points that may not be captured be ellipses may still end up inside the bands between ellipses and since the ellipses are already at 95% confidence level this implies that the bands will have a higher confidence level.\nWe ran 70 experiments on phonetic methods. Seven commonly used phonetic methods were selected for testing and these methods were applied on the same ten test folds. 90 experiments were conducted with distance metrics methods (Jaro, Levenshtein, Winkler-Jaro). We experimented with three values when choosing suitable \u03b3 parameter for distance measurement methods ranking. Our results indicate general consistency when using test data from both datasets (\u201csearch\u201d and \u201crecords\u201d). NYSIIS phonetic method, first introduced by Taft in 1970 [30] significantly outperforms other phonetic methods. Phonex method appears to be the weakest performer of the phonetic methods we have looked at. Other phonetic methods lie in the middle and their confidence bands overlap. Because of the overlapping regions we cannot definitively rank the performance of these methods.\nFigure 6 shows how we selected the best of MT methods. Even though that for all of our data test folds MT methods produced overlapping confidence bands we can still see that the centroid curve for 5-gram MT methods slightly outperforms other n-gram methods. Therefore, we have selected it to represent MT methods when comparing with phonetic and similarity methods.\nOur main results are shown on Figures 2 and 3. Here we present the comparison of all alternative name generating methods on precision-recall plots. It is clear that for both datasets MT methods perform better than all other methods and that similarity methods generally outperform phonetic methods."}, {"heading": "5.3 Implementation details", "text": "We imported our records/tree datasets into CDH4 Cloudera Hadoop and we perform all our filtering using Hive/Python scripts and Java native implementations. The Febrl library [9] implemented by Christen was used for calculating phonetic codes and string similarity values.\n18\nCountry number of unique names England 9341 Germany 5679 France 1233 Ireland 981 Scotland 647 Russia 448 Italy 426 Switzerland 377 Norway 376 Netherlands 300 Others 3779\nTable 4: \u201cRecords\u201d dataset\nCountry number of unique names England 6690 Germany 1323 Ireland 900 France 631 Scotland 468 Russia 241 Italy 157 Sweden 109 Poland 90 Switzerland 83 Others 727\nTable 5: \u201cSearch\u201d dataset"}, {"heading": "6 Discussion and Conclusion", "text": "In this paper we presented a novel way of approaching alternative name spelling generation problem. We utilized a well-known methodology for comparing alternative name spelling methods and presented our results as precision-recall plots which clearly indicate not only that machine translation methods appear to be superior for our datasets to other methods but also show the rankings of other well known methods. We demonstrated our results using a unique dataset from Ancestry.com generated by millions of motivated users who are \u201cexperts\u201d at labeling the dataset.\nThe main conclusion of this work is that machine translation methods that we have employed for finding ranked list of alternative last name spellings far-outperformed all other methods we tried. Our results, also, indicated that the NYSIIS phonetic method significantly outperformed other phonetic algorithms and the Phonex phonetic method did not perform as well on our data. Additionally, Jaro-Winkler similarity method together with the Levenshtein edit distance method performed better than the Jaro method, which was in line with our expectations. On the other hand, we were surprised by how well the NYSIIS method performed compared to other phonetic methods. Our finding regarding phonetic methods performance went against findings reported by Christen in his 2006 paper [8]. However, he was relying on very different dataset and that may explain the differences in our results.\n19\nIn future work we plan on training our models specifically on training sets composed of name pairs from the same country we plan on testing them against. We also plan on doing more experiments with full names including first names and initials. Additionally, we plan on trying MT methods on geographical locations such as town/village names.\n20\n21\n22\n23\n24\n25"}], "references": [{"title": "Phonetic models for generating spelling variants", "author": ["R. Bhagat", "E.H. Hovy"], "venue": "M. M. Veloso, editor, IJCAI, pages 1570\u20131575,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive name matching in information integration", "author": ["M. Bilenko", "R. Mooney", "W. Cohen", "P. Ravikumar", "S. Fienberg"], "venue": "Intelligent Systems, IEEE, 18(5):16\u201323,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive duplicate detection using learnable string similarity measures", "author": ["M. Bilenko", "R.J. Mooney"], "venue": "In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2003, pages 39\u201348,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Use of latent semantic indexing to identify name variants in large data collections", "author": ["R.B. Bradford"], "venue": "ISI, pages 27\u201332,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "A statistical approach to machine translation", "author": ["P.F. Brown", "J. Cocke", "S.A.D. Pietra", "V.J.D. Pietra", "F. Jelinek", "J.D. Lafferty", "R.L. Mercer", "P.S. Roossin"], "venue": "Comput. Linguist., 16(2):79\u201385, June", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "V.J. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Computational Linguistics, 19:263\u2013311,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "A comparison of personal name matching: Techniques and practical issues", "author": ["P. Christen"], "venue": "Data Mining Workshops, 2006. ICDM Workshops 2006. Sixth IEEE International Conference on, pages 290\u2013294. IEEE,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Febrl \u2013 an open source data cleaning, deduplication and record linkage system with a graphical user interface (demonstration session", "author": ["P. Christen"], "venue": "In ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD\u201908, pages 1065\u20131068,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection", "author": ["P. Christen"], "venue": "Data-centric systems and applications. Springer,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A comparison of string distance metrics for name-matching tasks", "author": ["W.W. Cohen", "P. Ravikumar", "S.E. Fienberg"], "venue": "pages 73\u201378,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 1\u201338,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1977}, {"title": "IRSTLM: an open source toolkit for handling large scale language models", "author": ["M. Federico", "N. Bertoldi", "M. Cettolo"], "venue": "INTERSPEECH, pages 1618\u20131621. ISCA,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "A theory for record linkage", "author": ["I.P. Fellegi", "A.B. Sunter"], "venue": "Journal of the American Statistical Association, 64(328):1183\u20131210,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1969}, {"title": "The CMA evolution strategy", "author": ["N. Hansen"], "venue": "A tutorial,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Advances in record-linkage methodology as applied to matching the 1985 census of Tampa, Florida", "author": ["M.A. Jaro"], "venue": "Journal of the American Statistical Association, 84(406):414\u2013420,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1989}, {"title": "Speech and language processing: An introduction to speech recognition", "author": ["D. Jurafsky", "J.H. Martin"], "venue": "Computational Linguistics and Natural Language Processing. 2nd Edn., Prentice Hall, ISBN, 10(0131873210):794\u2013800,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "A spelling correction program based on a noisy channel model", "author": ["M.D. Kernighan", "K.W. Church", "W.A. Gale"], "venue": "Proceedings of the 13th conference on Computational linguistics-Volume 2, pages 205\u2013210. Association for Computational Linguistics,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1990}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL \u201907, pages 177\u2013180, Stroudsburg, PA, USA,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": ["V.I. Levenshtein"], "venue": "Soviet Physics Doklady, 10(8):707\u2013710, February", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1966}, {"title": "Confidence Bands for ROC Curves: Methods and an Empirical Study", "author": ["S.A. Macskassy", "F.J. Provost"], "venue": "ROC Analysis in Artificial Intelligence, pages 61\u201370,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "A conditional random field for discriminatively-trained finite-state string edit distance", "author": ["A. McCallum", "K. Bellare", "F.C.N. Pereira"], "venue": "UAI, pages 388\u2013395. AUAI Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust similarity measures for named entities matching", "author": ["E. Moreau", "F. Yvon", "O. Cappe"], "venue": "D. Scott and H. Uszkoreit, editors, COL- ING, pages 593\u2013600,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics, 29(1):19\u201351,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Retrieval effectiveness of proper name search methods", "author": ["U. Pfeifer", "T. Poersch", "N. Fuhr"], "venue": "Information Processing and Management, pages 667\u2013679,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Hanging on the metaphone", "author": ["L. Philips"], "venue": "Computer Language Magazine, 7(12):39\u201344, December", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning string edit distance", "author": ["E.S. Ristad", "P.N. Yianilos"], "venue": "CoRR, cmp-lg/9610005,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Soundex", "author": ["R. Russell"], "venue": "U.S. Patent 1,261,167, 04", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1918}, {"title": "A mathematical theory of communication", "author": ["C. Shannon"], "venue": "Bell System Technical Journal, 27:379\u2013423, 623\u2013656, July, October", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1948}, {"title": "Name search techniques", "author": ["R.L. Taft"], "venue": "Technical Report Special Report No. 1, New York State Identification and Intelligence System, Albany, NY, February", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1970}, {"title": "String comparator metrics and enhanced decision rules in the fellegi-sunter model of record linkage", "author": ["W.E. Winkler"], "venue": "Proceedings of the Section on Survey Research, pages 354\u2013359,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1990}, {"title": "Overview of record linkage and current research directions", "author": ["W.E. Winkler"], "venue": "Technical report, Bureau of the Census,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "The classic reference in the field of record linkage is a paper by Fellegi and Sunter [14] published in 1969.", "startOffset": 86, "endOffset": 90}, {"referenceID": 30, "context": "In the interest of brevity we direct the reader to the outstanding 2006 survey paper by Winkler [32] and to the comprehensive work by Christen [10] published just recently.", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "In the interest of brevity we direct the reader to the outstanding 2006 survey paper by Winkler [32] and to the comprehensive work by Christen [10] published just recently.", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "Historically, methods focusing on name matching could be separated into two classes: sequential character methods and bag-of-words methods [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 26, "context": "The bestknown method from this class is Soundex [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 26, "context": "Popular methods include Soundex [28], Double Metaphon [26], and NYSIIS [30].", "startOffset": 32, "endOffset": 36}, {"referenceID": 24, "context": "Popular methods include Soundex [28], Double Metaphon [26], and NYSIIS [30].", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "Popular methods include Soundex [28], Double Metaphon [26], and NYSIIS [30].", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "Similarity method based on edit distance (the Levenshtein distance, as it is also known [20]) is the most well-known method of this type.", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "Other common similarity measures include the Jaro [16] method which takes into account the number and order of common characters between two strings and the Jaro-Winkler [31] method which extends Jaro by accounting for the common prefixes in both strings [3], [10].", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "Other common similarity measures include the Jaro [16] method which takes into account the number and order of common characters between two strings and the Jaro-Winkler [31] method which extends Jaro by accounting for the common prefixes in both strings [3], [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "Other common similarity measures include the Jaro [16] method which takes into account the number and order of common characters between two strings and the Jaro-Winkler [31] method which extends Jaro by accounting for the common prefixes in both strings [3], [10].", "startOffset": 255, "endOffset": 258}, {"referenceID": 8, "context": "Other common similarity measures include the Jaro [16] method which takes into account the number and order of common characters between two strings and the Jaro-Winkler [31] method which extends Jaro by accounting for the common prefixes in both strings [3], [10].", "startOffset": 260, "endOffset": 264}, {"referenceID": 3, "context": "In 2013 Bradford [5] published a paper dealing with alternative name spelling generation.", "startOffset": 17, "endOffset": 20}, {"referenceID": 21, "context": "There were numerous studies published on the topic of applying bags of words to record linkage over the last decade [23].", "startOffset": 116, "endOffset": 120}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[3] showed how machine learning methods could be successfully employed for learning the combined field similarity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[18] in their short paper proposed a method for spelling corrections based on noisy channels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] In the last several decades machine translation methods have gained significant traction and recently found their way into the problem of name matching.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[2] implemented a transducer based method for finding alternative name spellings by employing a graphemesto-phonemes framework.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Their method involved running EM (expectation maximization) algorithm, first presented by Dempster [12], to align text from the CMU dictionary with their phoneme sequence equivalents.", "startOffset": 99, "endOffset": 103}, {"referenceID": 23, "context": "Using both-ways translation models and language models, the authors were able to generate alternative phoneme sequences (pronunciations), given a character string name, and then each of these sequences was converted into an alternative character sequence [25].", "startOffset": 255, "endOffset": 259}, {"referenceID": 25, "context": "In 1996 Ristad and Yianilos [27] presented an interesting solution where they learned the cost of edit distance operations, which are normally all set to one in static edit distance algorithms.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "[4] improved on Ristad and Yianilos\u2019s learned edit distance model by including affine gaps.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[22] in 2005 approached the same problem from the different angle.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Instead of using generative models like [27] and [4] they have used discriminative method, conditional random fields (CRF), where they have been able to use both positive and negative string pairs for training.", "startOffset": 40, "endOffset": 44}, {"referenceID": 2, "context": "Instead of using generative models like [27] and [4] they have used discriminative method, conditional random fields (CRF), where they have been able to use both positive and negative string pairs for training.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "Following the traditions of statistical machine translation methods [6] this probability can be expressed using Bayes\u2019 rule as", "startOffset": 68, "endOffset": 71}, {"referenceID": 27, "context": "\u201dName model\u201d can be estimated using character n-grams language model representation by finding the probabilities using the chain rule [29]):", "startOffset": 134, "endOffset": 138}, {"referenceID": 4, "context": "P (sname|tname) allows us to get a theoretically good translation even if underlying probabilities are not that accurate [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 17, "context": "For training of our language and alignment models we have chosen the Moses software package which is a widely known open-source statistical machine translation software package [19].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "We used IRSTLM [13], a statistical language model tool for this purpose.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Alignment model building: Moses uses the GIZA++ package for statistical character-alignment [24] character (Word)-alignment tools typically implement one of Brown\u2019s IBM generative models [7] that are being used for determining translation rules for source language to the target language (including fertility rules: maximum number of target characters generated from one source character and so on) We created alignment model, for each of the 2-gram through 6-gram language models created in the previous step.", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "Alignment model building: Moses uses the GIZA++ package for statistical character-alignment [24] character (Word)-alignment tools typically implement one of Brown\u2019s IBM generative models [7] that are being used for determining translation rules for source language to the target language (including fertility rules: maximum number of target characters generated from one source character and so on) We created alignment model, for each of the 2-gram through 6-gram language models created in the previous step.", "startOffset": 187, "endOffset": 190}, {"referenceID": 19, "context": "Inspired by the work of [21] we designed our own methodology for robust statistical comparisons of our precision-recall curves.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "The eigenvectors of the covariance matrix \u03a3 are used as directions of the principal axes of the Gaussian ellipses [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 28, "context": "NYSIIS phonetic method, first introduced by Taft in 1970 [30] significantly outperforms other phonetic methods.", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "The Febrl library [9] implemented by Christen was used for calculating phonetic codes and string similarity values.", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": "Our finding regarding phonetic methods performance went against findings reported by Christen in his 2006 paper [8].", "startOffset": 112, "endOffset": 115}], "year": 2014, "abstractText": "Name matching is a key component of systems for entity resolution or record linkage. Alternative spellings of the same names are a common occurrence in many applications. We use the largest collection of genealogy person records in the world together with user search query logs to build name matching models. The procedure for building a crowd-sourced training set is outlined together with the presentation of our method. We cast the problem of learning alternative spellings as a machine translation problem at the character level. We use information retrieval evaluation methodology to show that this method substantially outperforms on our data a number of standard well known phonetic and string similarity methods in terms of precision and recall. Additionally, we rigorously compare the performance of standard methods when compared with each other. Our result can lead to a significant practical impact in entity resolution applications.", "creator": "LaTeX with hyperref package"}}}