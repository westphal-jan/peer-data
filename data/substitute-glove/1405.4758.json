{"id": "1405.4758", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2014", "title": "Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms", "abstract": "We consider stochastic broader - armed bandit problems at within expected give is a Lipschitz specified and from arm, those it the eight bringing removing is however discrete or communication. For probabilities Lipschitz fleeing, could derive asymptotic nothing specific lower curve in own regret frankly by any theorem, and should OSLB and CKL - UCB, eleven algorithms clearly ideally exploit now Lipschitz integrated still two difference. In probably, be outcome that OSLB is asymptotically optimal, instance larger modularity feeling matches the typically way. The regret calculations the does nonlinear enterprise on just recently oxygen alienation for ftse wasted beyond KL insufficiencies then same explanations distributions since encourage and still something factoring. For duration Lipschitz cornered, 'd reforms to only discretize soon force around, few then apply OSLB mean CKL - UCB, computer-based that non-sentient exploit this structure automate. This focus is shown, through yardstick experiments, take significantly analyst whereby algorithms given generally discuss still a function sets on arms. Finally three today and combinatorial are extended to timbral nabbed with similarities.", "histories": [["v1", "Mon, 19 May 2014 14:56:51 GMT  (240kb,D)", "http://arxiv.org/abs/1405.4758v1", "COLT 2014"]], "COMMENTS": "COLT 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["stefan magureanu", "richard combes", "alexandre proutiere"], "accepted": false, "id": "1405.4758"}, "pdf": {"name": "1405.4758.pdf", "metadata": {"source": "CRF", "title": "Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms", "authors": ["Stefan Magureanu", "Richard Combes", "Alexandre Proutiere", "MAGUREANU COMBES PROUTIERE"], "emails": ["MAGUR@KTH.SE", "RICHARD.COMBES@SUPELEC.FR", "ALEPRO@KTH.SE"], "sections": [{"heading": null, "text": "function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities."}, {"heading": "1. Introduction", "text": "In their seminal paper, Lai and Robbins (1985) solve the classical stochastic Multi-Armed Bandit (MAB) problem. In this problem, the successive rewards of a given arm are i.i.d., and the expected rewards of the various arms are not related. They derive an asymptotic (when the time horizon grows large) lower bound of the regret satisfied by any algorithm, and present an algorithm whose regret matches this lower bound. This initial algorithm was quite involved, and many researchers have, since then, tried to devise simpler and yet efficient algorithms. The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Cappe\u0301 (2011), Cappe\u0301 et al. (2013) \u2013 note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.6). When the expected rewards of the various arms are not related as in Lai and Robbins (1985), the regret of the best algorithm essentially scales as O(K log(T )) where K denotes the number of arms, and T is the time horizon. When K is very large or even infinite, MAB problems become more challenging. Fortunately, in such scenarios, the expected rewards often exhibit some structural properties that the decision maker can exploit to design efficient algorithms. Various structures have\nc\u00a9 2014 S. Magureanu, R. Combes & A. Proutiere.\nar X\niv :1\n40 5.\n47 58\nv1 [\ncs .L\nG ]\n1 9\nM ay\nbeen investigated in the literature, e.g., Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005).\nIn this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms. In turn, these algorithms fail at exploiting the problem structure revealed by the past observed samples. For continuous bandits, we propose to first discretize the set of arms (as in Kleinberg et al. (2008)), and then apply OSLB, an algorithm that optimally exploits past observations and hence the problem specific structure. As it turns out, this approach outperforms algorithms directly dealing with continuous sets of arms.\nOur contributions. (a) For discrete Lipschitz bandit problems, we derive an asymptotic regret lower bound satisfied by any algorithm. This bound is problem specific in the sense that it depends in an explicit manner on the expected rewards of the various arms (this contrasts with existing lower bounds for continuous Lipschitz bandits).\n(b) We propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. We further present CKL-UCB (Combined KL-UCB), an algorithm that exhibits lower computational complexity than that of OSLB, and that is yet able to exploit the Lipschitz structure.\n(c) We provide a finite time analysis of the regret achieved under OSLB and CKL-UCB. The analysis relies on a new concentration inequality for a weighted sum of KL divergences between the empirical distributions of rewards and their true distributions. We believe that this inequality can be instrumental for various bandit problems with structure.\n(d) We evaluate our algorithms using numerical experiments for both discrete and continuous sets of arms. We compare their performance to that obtained using existing algorithms for continuous bandits.\n(e) We extend our results and algorithms to the case of contextual bandits with similarities as investigated in Slivkins (2011)."}, {"heading": "2. Models", "text": "We consider a stochastic multi-armed bandit problem where the set of arms is a subset {x1, . . . , xK} of the interval [0, 1]. Results can be easily extended to the case where the set of arms is a subset of a metric space as considered in Kleinberg et al. (2008). The set of arms is of finite cardinality, possibly\nlarge, and we assume without loss of generality that x1 < x2 < . . . < xK . Problems with continuous sets of arms are discussed in Section 7. Time proceeds in rounds indexed by n = 1, 2, . . .. At each round, the decision maker selects an arm, and observes the corresponding random reward. Arm xk is referred to as arm k for simplicity. For any k, the reward of arm k in round n is denoted by Xk(n), and the sequence of rewards (Xk(n))n\u22651 is i.i.d. with Bernoulli distribution of mean \u03b8k (the results can be generalized to distributions belonging to a certain parametrized family of distributions, but to simplify the presentation, we restrict our attention to Bernoulli rewards). The vector \u03b8 = (\u03b81, . . . , \u03b8K) represents the expected rewards of the various arms. Let K = {1, . . . ,K}. We denote by \u03b8? = maxk\u2208K \u03b8k the expected reward of the best arm. A sequential selection algorithm \u03c0 selects in round n an arm k\u03c0(n) \u2208 K that depends on the past observations. In other words, for any n \u2265 1, if F\u03c0n denotes the \u03c3-algebra generated by (k\u03c0(t), Xk\u03c0(t)(t))1\u2264t\u2264n, then k\u03c0(n + 1) is F\u03c0n -measurable. Let \u03a0 denote the set of all possible sequential selection algorithms.\nWe assume that the expected reward is a Lipschitz function of the arm, and this structure is known to the decision maker. More precisely, there exists a positive constant L such that for all pairs of arms (k, k\u2032) \u2208 K,\n|\u03b8k \u2212 \u03b8k\u2032 | \u2264 L\u00d7 |xk \u2212 xk\u2032 |. (1)\nWe assume that L is also known. We denote by \u0398L the set of vectors in [0, 1]K satisfying (1). The objective is to devise an algorithm \u03c0 \u2208 \u03a0 that maximizes the average cumulative reward up to a certain round T referred to as the time horizon (T is typically large). Such an algorithm should optimally exploit the Lipschitz structure of the problem. As always in bandit optimization, it is convenient to quantify the performance of an algorithm \u03c0 \u2208 \u03a0 through its expected regret (or regret for short) defined by:\nR\u03c0(T ) = T\u03b8? \u2212 E[ T\u2211 n=1 Xk\u03c0(n)(n)]."}, {"heading": "3. Regret Lower Bound", "text": "In this section, we derive an asymptotic (when T grows large) regret lower bound satisfied by any algorithm \u03c0 \u2208 \u03a0. We denote by I(x, y) = x log(xy ) + (1 \u2212 x) log( 1\u2212x 1\u2212y ) the KL divergence between two Bernoulli distributions with respective means x and y. Fix the average reward vector \u03b8 = (\u03b81, . . . , \u03b8K). Let K\u2212 = {k \u2208 K : \u03b8k < \u03b8?} be the set of sub-optimal arms. For any k \u2208 K\u2212, we define \u03bbk = (\u03bb1, . . . , \u03bbK) as: \u2200i \u2208 K, \u03bbki = max{\u03b8i, \u03b8? \u2212L|xk \u2212 xi|}. The expected reward vector \u03bbk is illustrated in Figure 3, and may be interpreted as the most confusing reward vector among vectors in \u0398L such that arm k (which is sub-optimal under \u03b8) is optimal under \u03bbk. This interpretation will be made clear in the proof of the following theorem. Without loss of generality, we restrict our attention to so-called uniformly good algorithms, as defined in Lai and Robbins (1985). \u03c0 \u2208 \u03a0 is uniformly good if for all \u03b8 \u2208 \u0398L, R\u03c0(T ) = o(T a) for all a > 0. Uniformly good algorithms exist \u2013 for example, the UCB algorithm is uniformly good.\nTheorem 1 Let \u03c0 \u2208 \u03a0 be a uniformly good algorithm. For any \u03b8 \u2208 \u0398L, we have:\nlim inf T\u2192\u221e\nR\u03c0(T ) log(T ) \u2265 C(\u03b8), (2)\nwhere C(\u03b8) is the minimal value of the following optimization problem:\nmin ck\u22650,\u2200k\u2208K\u2212 \u2211 k\u2208K\u2212 ck \u00d7 (\u03b8? \u2212 \u03b8k) (3)\ns.t. \u2200k \u2208 K\u2212, \u2211 i\u2208K ciI(\u03b8i, \u03bb k i ) \u2265 1. (4)\nThe regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix. As in classical bandits, the minimal regret scales logarithmically with the time horizon. Observe that the lower bound (2) is smaller than the lower bound derived in Lai and Robbins (1985) when the various average rewards (\u03b8k, k \u2208 K) are not related (i.e., in absence of the Lipschitz structure). Hence (2) quantifies the gain one may expect by designing algorithms optimally exploiting the structure of the problem. Note that for any k \u2208 K\u2212, the variable ck corresponding to a solution of (3) characterizes the number of times arm k should be played under an optimal algorithm: arm k should be roughly played ck log(n) times up to round n.\nIt should be also observed that our lower bound is problem specific (it depends on \u03b8), which contrasts with existing lower bounds for continuous Lipschitz bandits, see e.g. Kleinberg et al. (2008). The latter are typically derived by selecting the problems that yield maximum regret. However, our lower bound is only valid for bandits with a finite set of arms, and cannot easily be generalized to problems with continuous sets of arms."}, {"heading": "4. Algorithms", "text": "In this section, we present two algorithms for discrete Lipschitz bandit problems. The first of these algorithms, referred to as OSLB (Optimal Sampling for Lipschitz Bandits), has a regret that matches\nAlgorithm 1 OSLB( ) For all n \u2265 1, select arm k(n) such that: If \u03b8\u0302?(n) \u2265 maxk 6=L(n) bk(n), then k(n) = L(n); Else If tk(n)(n) < K tk(n)(n), then k(n) = k(n);\nElse k(n) = k(n).\nthe lower bound derived in Theorem 1, i.e., it is asymptotically optimal. OSLB requires that in each round, one solves an LP similar to (3). The second algorithm, CKL-UCB (Combined KLUCB) is much simpler to implement, but has weaker theoretical performance guarantees, although it provably exploits the Lipschitz structure."}, {"heading": "4.1. The OSLB Algorithm", "text": "To formally describe OSLB, we introduce the following notations. For any n \u2265 1, let k(n) be the arm selected under OSLB in round n. tk(n) denotes the number of times arm k has been selected up to round n \u2212 1. By convention, tk(1) = 0. The empirical reward of arm k at the end of round (n \u2212 1) is \u03b8\u0302k(n) = 1tk(n) \u2211n\u22121 t=1 1{k(t) = k}Xk(t), if tk(n) > 0 and \u03b8\u0302k(n) = 0 otherwise. We denote by L(n) = arg maxk\u2208K \u03b8\u0302k(n) the arm with the highest empirical reward (ties are broken arbitrarily) at the end of round n \u2212 1. Arm L(n) is referred to as the leader for round n. We also define \u03b8\u0302?(n) = \u03b8\u0302L(n)(n) as the empirical reward of the leader at the end of round n \u2212 1. Let f(n) = log(n) + (3K + 1) log log(n). Further define, for all q \u2265 0 and k, the Lipschitz vector \u03bbq,k such that for any k\u2032, \u03bbq,kk\u2032 = q\u2212L|xk \u2212 xk\u2032 |. The sequential decisions made under OSLB are based on the indexes of the various arms. The index bk(n) of arm k for round n is defined by:\nbk(n) = sup{q \u2208 [\u03b8\u0302k(n), 1] : K\u2211 k\u2032=1 tk\u2032(n)I +(\u03b8\u0302k\u2032(n), \u03bb q,k k\u2032 ) \u2264 f(n)}.\nNote that the index bk(n) is always well defined, even for small values of n, e.g. n = 1 (we have for all x > 0, I+(0, x) = \u2212 log(1 \u2212 x)). For any \u03b8 \u2208 \u0398L, let C(\u03b8) denote the minimal value of the optimization problem (3), and let (ck(\u03b8), k \u2208 K\u2212) be the values of the variables (ck, k \u2208 K\u2212) in (3) yielding C(\u03b8). For simplicity, we define C\u0302(n) = C(\u03b8\u0302(n)), and c\u0302k(n) = ck(\u03b8\u0302(n)) for any k \u2208 K\u2212(n) where K\u2212(n) = {k : \u03b8\u0302k(n) < \u03b8\u0302?(n)}. The design of OSLB stems from the observation that an optimal algorithm should satisfy limn\u2192\u221e tk(n)/(ck(\u03b8) log(n)) = 1, almost surely, for all k \u2208 K\u2212. Hence we should force the exploration of arm k \u2208 K\u2212(n) in round n if tk(n) < c\u0302k(n) log(n). We define the arm k(n) to explore as k(n) = arg mink\u2208Ke(n) tk(n) where Ke(n) = {k \u2208 K\u2212(n) : tk(n) \u2264 c\u0302k(n) log(n)}. If Ke(n) = \u2205, k(n) = \u22121 (a dummy arm). Finally we define the least played arm as k(n) = arg mink tk(n). In the definitions of k(n) and k(n), ties are broken arbitrarily. We are now ready to describe OSLB. Its pseudo-code is presented in Algorithm 1.\nUnder OSLB, the leader is selected if its empirical average exceeds the index of other arms. If this is not the case, OSLB selects the least played arm k(n), if the latter has not been played enough, and arm k(n) otherwise. Note that the description of OSLB is valid in the sense that k(n) 6= \u22121 if \u03b8\u0302?(n) < maxk 6=L(n) bk(n). After each round, all variables are updated, and in particular c\u0302k(n) for any k \u2208 K\u2212(n), which means that at each round we solve an LP, similar to (3).\nAlgorithm 2 CKL-UCB For all n \u2265 1, select arm k(n) such that: If \u2203k such that tk(n) < log log(n), then k(n) = k (ties are broken arbitrarily); Else if bL(n)(n) \u2265 max\nk 6=L(n) bk(n), then k(n) = L(n);\nElse k(n) = arg min k {tk(n) : bk(n) > bL(n)(n)} (ties are broken arbitrarily)."}, {"heading": "4.2. The CKL-UCB Algorithm", "text": "Next, we present the algorithm CKL-UCB (Combined KL - UCB). The sequential decisions made under CKL-UCB are based on the indexes bk(n), and CKL-UCB explores the apparently suboptimal arms by choosing the least played arms first. When the leaderL(n) has the largest index, it is played, and otherwise we play the arm in {k : bk(n) > bL(n)(n)}, the set of arms which are possibly better than the leader, with the least number of current plays. Note that in practice, the forced log log(n) exploration is unnecessary and only appears to aide in the regret analysis.\nThe rationale behind CKL-UCB is that if we are given a set of suboptimal arms, by exploring them, we will first eliminate arms whose expected reward is low (these arms do not require many plays to be eliminated). Note that the arm chosen by CKL-UCB is directly computed from the indexes, without solving an LP, and hence CKL-UCB is computationally light. From a practical perspective, CKL-UCB should also be more robust than OSLB in the sense that it does not take decisions based on the solution of the LP calculated with empirical averages \u03b8\u0302(n). This could be problematic if the LP solution is very sensitive to errors in the estimate of \u03b8."}, {"heading": "5. Regret Analysis", "text": "In this section, we provide finite time upper bounds for the regret achieved under OSLB and CKLUCB."}, {"heading": "5.1. Concentration Inequalities", "text": "To analyse the regret of algorithms for bandit optimization problems, one often has to leverage results related to the concentration-of-measure phenomenon. More precisely, here, in view of the definition of the indexes bk(n), we need to establish a concentration inequality for a weighted sum of KL divergences between the empirical distributions of rewards and their true distributions. We derive such an inequality. The latter extends to the multi-dimensional case the concentration inequality derived in Garivier (2013) for a single KL divergence. We believe that this inequality can be instrumental in the analysis of general structured bandit problems, as well as for statistical tests involving vectors whose components have distributions in a one-parameter exponential family (such as Bernoulli or Gaussian distributions). For simplicity, the inequality is stated for Bernoulli random variables only.\nWe use the following notations. For k \u2208 K, let {Xk(n)}n\u2208N be a sequence of i.i.d. Bernoulli random variables with expectation \u03b8k and X(n) = (Xk(n), k \u2208 K). We represent the history up to round n using the \u03c3-algebra Fn = \u03c3(X(1), . . . , X(n)), and define the natural filtration F = {Fn}n\u22651. We consider a generic sampling rule B(n) = (Bk(n), k \u2208 K) where Bk(n) \u2208 {0, 1} for all k \u2208 K. The sampling rule is assumed to be predictable in the sense that B(n) \u2208 Fn\u22121.\nWe define the number of times that k was sampled up to round n\u2212 1 by tk(n) = \u2211n\u22121 t=1 Bk(t)\nand the sum Sk(n) = \u2211n\u22121\nt=1 Bk(t)Xk(t). The empirical average for k is \u03b8\u0302k(n) = Sk(n)/tk(n) if tk(n) > 0 and \u03b8\u0302k(n) = 0 otherwise. Finally, we define the vectors \u03b8\u0302(n) = (\u03b8\u03021(n), . . . , \u03b8\u0302K(n)) and t(n) = (t1(n), . . . , tK(n)). When comparing vectors in RK , we use the component-by-component order unless otherwise specified.\nTheorem 2 For all \u03b4 \u2265 (K + 1) and n \u2208 N we have:\nP [ K\u2211 k=1 tk(n)I +(\u03b8\u0302k(n), \u03b8k) \u2265 \u03b4 ] \u2264 e\u2212\u03b4 ( d\u03b4 log(n)e\u03b4 K )K eK+1. (5)\nThe proof of Theorem 2 involves tools that are classically used in the derivation of concentration inequalities, but also requires the use of stochastic ordering techniques, see e.g. Mu\u0308ller and Stoyan (2002)."}, {"heading": "5.2. Finite time analysis of OSLB", "text": "Next we provide a finite time analysis of the regret achieved under OSLB, under the following mild assumption. This assumption greatly simplifies the analysis.\nAssumption 1 The solution of the LP (3) is unique.\nIt should be observed that the set of parameters \u03b8 \u2208 \u0398L such that Assumption 1 is satisfied constitutes a dense subset of \u0398L.\nTheorem 3 For all > 0, under Assumption 1, the regret achieved under \u03c0 = OSLB( ) satisfies: for all \u03b8 \u2208 \u0398L, for all \u03b4 > 0 and T \u2265 1,\nR\u03c0(T ) \u2264 C\u03b4(\u03b8)(1 + ) log(T ) + C1 log log(T ) +K3 \u22121\u03b4\u22122 + 3K\u03b4\u22122, (6)\nwhere C\u03b4(\u03b8)\u2192 C(\u03b8), as \u03b4 \u2192 0+, and C1 > 0.\nIn view of the above theorem, when is small enough, OSLB( ) approaches the fundamental performance limit derived in Theorem 1. More precisely, we have for all > 0 and \u03b4 > 0:\nlim sup T\u2192\u221e\nR\u03c0(T ) log(T ) \u2264 C\u03b4(\u03b8)(1 + ).\nIn particular, for any \u03b6 > 0, one can find > 0 and \u03b4 > 0 such that C\u03b4(\u03b8)(1 + ) \u2264 (1 + \u03b6)C(\u03b8), and hence, under \u03c0 =OSLB( ),\nlim sup T\u2192\u221e\nR\u03c0(T ) log(T ) \u2264 C(\u03b8)(1 + \u03b6)."}, {"heading": "5.3. Finite Time analysis of CKL-UCB", "text": "In order to analyze the regret of CKL-UCB, we define the following optimization problem. Define the matrix of Kullback-Leibler divergence numbers A = (aik)i,k\u2208K with aik = I(\u03b8i, \u03bb k,\u03b8?\ni ). Consider an arm k 6= k?, a subset of arms N \u2282 {1, . . . ,K} \\ {k, k?}, and \u03b10 \u2265 0. We define dk(A,\u03b10,N ) the optimal value of the following linear program:\nmin \u03b11,...,\u03b1K \u2211 k\u2032\u2208K\u2212\\{k} \u03b1k\u2032ak\u2032k\ns.t. \u03b1k\u2032 \u2265 \u03b10, \u2200k\u2032 6\u2208 N , k\u2032 6= k?\n\u03b1k\u2032 \u2265 0, \u2200k\u2032\u2211 k\u2032\u2032\u2208K\u2212\\{k} \u03b1k\u2032\u2032ak\u2032\u2032k\u2032 \u2265 1\u2212 \u03b10akk\u2032 , \u2200k\u2032 \u2208 N .\nand ek(A,\u03b10) = minN dk(A,\u03b10,N ) where the minimum is taken over all possible subsets of {1, . . . ,K} \\ {k, k?}.\nTheorem 4 Under CKL-UCB, for all \u03b8 \u2208 \u0398L, all T \u2265 1, all 0 < \u03b4 < (\u03b8? \u2212maxk 6=k? \u03b8k)/2, and any suboptimal arm k \u2208 K\u2212, (i) we have:\nE[tk(T )] \u2264 f(T )\nI(\u03b8k + \u03b4, \u03b8\u2217 \u2212 \u03b4) + C1 log(log(T )) + 2\u03b4\n\u22122.\nwith C1 \u2265 0 a constant. (ii) Furthermore, for all k \u2208 K\u2212, we have that:\nlim sup T\u2192\u221e E[tk(T )] log(T ) \u2264 \u03b2k(\u03b8).\nwhere \u03b2k(\u03b8) = inf{\u03b10 \u2265 0 : ak,k\u03b10 + ek(A,\u03b10) > 1}.\n(iii) Assume that there exists k\u2032 such that 0 < akk\u2032 < akk and such that for all k\u2032\u2032 we have that if ak\u2032\u2032k = 0 then ak\u2032\u2032k\u2032 = 0 as well. Then \u03b2k(\u03b8) < 1/akk = 1/I(\u03b8k, \u03b8?).\nIn the above theorem, statement (i) shows that CKL-UCB plays arm k at most as much as KLUCB, so that CKL-UCB outperforms KL-UCB for any value of the parameters \u03b8. Now statements (ii) and (iii) show that under certain assumptions, CKL-UCB plays arm k strictly less than KL-UCB, so that CKL-UCB indeed exploits the Lipshitz structure of the problem. Note that the conditions in (iii) holds for triangular reward functions, and other unimodal functions, and hence in these cases, CKL-UCB strictly outperforms KL-UCB. The regret analysis of CKL-UCB presented above is preliminary, and we believe that its performance guarantees can be further improved."}, {"heading": "6. Contextual Bandit with Similarities", "text": "The algorithms and results presented above can be extended to the case of contextual bandit problems with similarities as studied in Slivkins (2011). In such problems, in each round, the decision maker observes a context, and then decides which arm to select. The expected reward of the various\narms depends on the context, and is assumed to be Lipschitz in the arm and context. We assume that contexts arrive according to an i.i.d. process whose distribution is not known to the decision maker. This contrasts with most of the work in contextual bandits, where the context process is adversarial."}, {"heading": "6.1. Model", "text": "Let {y1, . . . , yJ} denote the set of possible contexts, assumed to be a subset of [0, 1]. We assume that y1 < . . . < yJ . For simplicity, context yj is referred to as context j. For each context j \u2208 J = {1, . . . , J}, the expected rewards of the various arms are represented by a vector \u03b8(j) = (\u03b8k(j), k \u2208 K) (\u03b8k(j) is the expected reward of arm k when the context is j). We consider a general scenario where the reward is a Lipschitz function in both the arm and the context. There exists L (known to the decision maker) such that for all (i, k), (j, l) \u2208 J \u00d7K,\n|\u03b8k(i)\u2212 \u03b8l(j)| \u2264 L\u00d7D((i, k), (j, l)), (7)\nwhere D refers to some metric over J \u00d7 K. The choice of this metric is free, and allows us to consider different scenarios. For example, we may assume that the Lipschitz structure is stronger in terms of arms than in terms of contexts. In this case, we may choose, for some \u03b2 > 1, D((i, k), (j, l)) = \u221a (\u03b2(yi \u2212 yj)2 + (xk \u2212 xl)2) . The set of \u03b8 = (\u03b8k(j), k \u2208 K, j \u2208 J ) satisfying (7) is denoted by \u0398L,2. The context process is i.i.d.. The distribution of the observed context j(n) in round n is \u03c8, i.e., \u03c8(j) = P[j(n) = j]. Without loss of generality, we assume that for any j \u2208 J , \u03c8(j) > 0. \u03c8 is unknown to the decision maker. Let Xj,k(n) denote the reward of arm k obtained in round n when the context is j. For contextual bandits, we define the regret of algorithm \u03c0 as follows:\nR\u03c0(T ) = T \u2211 j\u2208J \u03c8(j)\u03b8?(j)\u2212 T\u2211 n=1 E[Xj(n),k\u03c0(n)(n)]. (8)\nwhere \u03b8?(j) denotes the reward of the best arm under context j, and as earlier k\u03c0(n) denotes the arm selected under \u03c0 in round n."}, {"heading": "6.2. Regret Lower Bound", "text": "To state the regret lower bound, we introduce for any context j \u2208 J , K\u2212(j) = {k \u2208 K : \u03b8k(j) < \u03b8?(j)} the set of suboptimal arms for context j. We also introduce for any context j \u2208 J , and any k \u2208 K, the vector (\u03bbj,kl (i), l \u2208 K, i \u2208 J ) such that\n\u03bbj,kl (i) = max{\u03b8l(i), \u03b8 ?(j)\u2212 LD((j, k), (i, l))}.\nTheorem 5 Let \u03c0 be a uniformly good algorithm. Then, for any \u03b8 \u2208 \u0398L,2:\nlim inf T\u2192\u221e\nR\u03c0(T ) log(T ) \u2265 C \u2032(\u03b8) (9)\nwhere C \u2032(\u03b8) is the minimal value of the following optimization problem:\nmin cj,k\u22650,\u2200j,\u2200k \u2211 j\u2208J \u2211 k\u2208K\u2212(j) cj,k \u00d7 (\u03b8\u2217(j)\u2212 \u03b8k(j)) (10)\ns.t. \u2200j,\u2200k \u2208 K\u2212(j), \u2211 i\u2208J \u2211 l\u2208K ci,lI(\u03b8l(i), \u03bb j,k l (i)) \u2265 1. (11)\nAlgorithm 3 CCKL-UCB For all n \u2265 1, observe context j = j(n), and select arm k(n) such that: If \u2203k such that tk(j, n) < log log(n), then k(n) = k (ties are broken arbitrarily); Else if L(n, j) = arg max\nk bck(n, j), then k(n) = L(n, j);\nElse k(n) = arg min k {tk(j, n) : bck(j, n) > bcL(n)(j, n)} (ties are broken arbitrarily).\nObserve that our regret lower bound is problem specific, and again the values of the cj,k\u2019s solving the above optimization problem can be interpreted as follows: an asymptotically optimal algorithm plays arm k when the context is j a number of times that scales as cj,k log(T ) as T grows large. Also note that the regret lower bound does not depend on the distribution \u03c8 of the contexts."}, {"heading": "6.3. Algorithms", "text": "The algorithms proposed for Lipschitz bandits can be naturally extended to the case of contextual bandits with similarities. For conciseness, we just present CCKL-UCB (Contextual Combined KL - UCB), the extension of CKL-UCB. Its regret analysis can be conducted as that of CKL-UCB with minor modifications.\nTo describe CCKL-UCB, we introduce the following notations. Let \u03b8\u0302k(j, n) denote the empirical average reward of arm k for context j up to round n\u2212 1. tk(j, n) is the number of times context j is presented and arm k is chosen up to round n \u2212 1. We define the index bck(j, n) of arm k for round n, when the context j is observed as:\nbck(n, j) = sup{q \u2208 [\u03b8\u0302k(j, n), 1] : \u2211 i\u2208J \u2211 l\u2208K tl(i, n)I +(\u03b8\u0302l(i, n), \u03bb q,k,j l (i, n)) \u2264 f(n)},\nwhere \u03bbq,k,jl (i, n) = q \u2212 LD((j, k), (i, l)). As for Lipschitz bandits, the indexes are built so as to match the constraints (11) of the optimisation problem leading to the regret lower bound. The leader for round n and context j is defined L(n, j) = arg max\nk \u03b8\u0302k(j, n) (ties are broken arbitrarily). In\nround n, CCKL-UCB plays the leader L(n, j(n)) for the current context if it has the highest index, and otherwise selects the least played arm which has an index higher than the leader L(n, j(n))."}, {"heading": "7. Numerical Experiments", "text": "In this section, we present numerical experiments illustrating the performance of our algorithms compared to other existing algorithms."}, {"heading": "7.1. Discrete Lipschitz Bandits", "text": "We first consider discrete bandit problems with 46 arms, and with time horizons less than T = 5.105 rounds. The regret is averaged over 150 runs. In Figure 2, we compare the performance of KL-UCB and CKL-UCB. For improved numerical performance, in the case of both algorithms we ignore the log log(n) terms in the indexes (i.e.f(n) = log(n)). On the left, we plot the expected reward as a function of the arm, as well as the (scaled) amount of times E[tk(n)]/ log(n) sub-optimal arm k is played under both algorithms, as function of time. Under KL-UCB, the amount of times for arm k approaches 1/I(\u03b8k, \u03b8?), whereas under CKL-UCB, E[tk(n)] satisfy the upper bounds derived\nin Theorem 4. CKL-UCB explores suboptimal arms less often than KL-UCB, as it is designed to exploit the Lipschitz structure. On the right, we plot the expected regret as a function of time under both algorithms. The regret under CKL-UCB is always smaller than that under KL-UCB (the regret under KL-UCB is typically twice as large as that under CKL-UCB in this example). This illustrates the significant gains that one may achieve by efficiently exploiting the structure of the problem."}, {"heading": "7.2. Continuous Lipschitz Bandits", "text": "We now turn our attention to continuous Lipschitz bandits where the set of arms is [0,1]. We consider two reward functions that behave differently around their maximum: (1) \u03b8(x) = 0.8\u2212 0.5|0.5\u2212 x| (triangle) and (2) \u03b8(x) = max(0.1, 0.9\u2212 3.2 \u2217 (0.7\u2212 x)2) (quadratic function). To adapt KL-UCB and CKL-UCB to this continuous setting, we use a uniform discretization of the\nset of arms, with \u03b4\u22121 = d \u221a T/ log(T )e arms. This discretization is known to be order-optimal for functions which are regular around their maximum Kleinberg (2004). In order not to give a positive bias to KL-UCB and CKL-UCB, we make sure that the maximum of the reward functions is not achieved in one of the arms in the discretization: the maximum is placed at a distance of at least \u03b4/4 from any arm in the discretization. We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al. (2008) for details) is set equal to \u221a log(n)/(2 \u2217 tk(n)) in round n. HOO+ and Zooming+ exhibit better performance than their initial versions, but their regrets have not been analytically studied. In the experiments, we limit the time horizon to T = 25000 rounds, and the expected regret is calculated by averaging over 100 independent runs.\nFigure 3 presents the expected regret of the various algorithms for the triangular reward function (left) and for the quadratic reward function (right). First note that surprisingly, KL-UCB, an algorithm that does not leverage the Lipschitz structure, outperforms some of the algorithms designed to exploit the structure. Observe that CKL-UCB clearly outperforms KL-UCB and all other algorithms in both problem instances. For quadratic reward functions, it is known that the optimal discretization of the set of arms should roughly have (log(T )/T )1/4 arms, Combes and Proutiere (2014a). We also plot the regret achieved under CKL-UCB using this optimized discretization, and we observe that this indeed further reduces the regret.\nIt is worth noting that in the case of CKL-UCB most of the regret is caused by not discretizing enough around the top arm. In contrast, in the case of Zooming and HOO, most of the regret is caused by loose confidence bounds. Therefore, in future work we will explore the possibility of combining the adaptive discretization scheme of Zooming and HOO with efficient confidence bounds as used by CKL-UCB."}, {"heading": "8. Conclusion", "text": "We consider stochastic multi-armed bandits (discrete or continuous) where the expected reward is a Lipschitz function of the arm. For discrete Lipschitz bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm. We propose OSLB and CKL-UCB, two algorithms that exploit the Lipschitz structure efficiently. OSLB is asymptotically optimal and CKL-UCB is a computationally light algorithm which efficiently exploits the Lipschitz structure. The regret analysis is based on a new concentration inequality for sums of KL divergences which can be instrumental for bandit problems with correlated arms. For continuous Lipschitz bandits, we adapt OSLB and CKL-UCB by using a simple discretization. For both discrete and continuous bandits, initial numerical experiments show that our approach significantly outperforms the state-of-the-art algorithms. Finally the results and algorithms are extended to contextual bandits with similarities."}, {"heading": "Appendix A. Proof of Theorem 1", "text": "To establish the asymptotic lower bound, we apply the techniques used in Graves and Lai (1997) to investigate efficient adaptive decision rules in controlled Markov chains. We recall here their general framework. Consider a controlled Markov chain (Xt)t\u22650 on a finite state space S with a control set U . The transition probabilities given control u \u2208 U are parametrized by \u03b8 taking values in a compact metric space \u0398: the probability to move from state x to state y given the control u and the parameter \u03b8 is p(x, y;u, \u03b8). The parameter \u03b8 is not known. The decision maker is provided with a finite set of stationary control laws G = {g1, . . . , gK} where each control law gj is a mapping from S to U : when control law gj is applied in state x, the applied control is u = gj(x). It is assumed that if the decision maker always selects the same control law g, the Markov chain is irreducible with stationary distribution \u03c0g\u03b8 . Now the expected reward obtained when applying control u in state x is denoted by r(x, u), so that the expected reward achieved under control law g is: \u00b5\u03b8(g) = \u2211 x r(x, g(x))\u03c0 g \u03b8(x). There is an optimal control law given \u03b8 whose expected reward is denoted \u00b5?\u03b8 \u2208 arg maxg\u2208G \u00b5\u03b8(g). Now the objective of the decision maker is to sequentially select control laws so as to maximize the expected reward up to a given time horizon T . As for MAB problems, the performance of a decision scheme can be quantified through the notion of regret which compares the expected reward to that obtained by always applying the optimal control law.\nWe now apply the above framework to our Lipschitz bandit problem, and we consider \u03b8 \u2208 \u0398L. The Markov chain has values in {0, 1}. The set of control laws is G = {1, . . . ,K}. These laws are constant, in the sense that the control applied by control law k does not depend on the state of the Markov chain, and corresponds to selecting arm k. The transition probabilities are:\np(x, y; k, \u03b8) = { \u03b8k, if y = 1, 1\u2212 \u03b8k, if y = 0.\nFinally, the reward r(x, k) is just given by the state x. We now fix \u03b8 \u2208 \u0398L. Define the set B(\u03b8) consisting of all bad parameters \u03bb \u2208 \u0398L such that k? is not optimal under parameter \u03bb, but which are statistically indistinguishable from \u03b8:\nB(\u03b8) = {\u03bb \u2208 \u0398L : \u03bbk? = \u03b8k? and max k \u03bbk > \u03bbk?},\nB(\u03b8) can be written as the union of sets Bk(\u03b8), k \u2208 K\u2212 defined as:\nBk(\u03b8) = {\u03bb \u2208 B(\u03b8) : \u03bbk > \u03bbk?}.\nBy applying Theorem 1 in Graves and Lai (1997), we know that C(\u03b8) is the minimal value of the following LP:\nmin \u2211\nk ck(\u03b8 ? \u2212 \u03b8k) (12) s.t. inf\u03bb\u2208Bk(\u03b8) \u2211\nl\u2208K clI(\u03b8l, \u03bbl) \u2265 1, \u2200k \u2208 K\u2212 (13) ck \u2265 0, \u2200k \u2208 K. (14)\nTo conclude the proof, it is sufficient to remark that for any k,\ninf \u03bb\u2208Bk(\u03b8) \u2211 l\u2208K clI(\u03b8l, \u03bbl) = \u2211 l\u2208K clI(\u03b8l, \u03bb k l ),\nwhich is easy in view of the definition of \u03bbk, by monotonicity of x 7\u2192 I(\u03b8k, x) when x \u2265 \u03b8k."}, {"heading": "Appendix B. Proof of Theorem 2", "text": "In this section, we first establish the concentration inequality assuming that Lemma 6 holds. We then prove Lemma 6, and to this aim, we state and use two further intermediate results, Lemmas 7 and 8, proved at the end of this section. Without loss of generality, we assume that tk(n) \u2265 1 for any k (the case where for some k, tk(n) = 0 is treated similarly).\nProof of Theorem 2. Let \u03b4 \u2265 K + 1 and \u03b7 > 0. Define D = dlog(n)/ log(1 + \u03b7)e, and the set D = {1, . . . , D}K . Introduce the following events:\nA = { K\u2211 k=1 tk(n)I +(\u03b8\u0302k(n), \u03b8k) \u2265 \u03b4 } ,\nBd = \u2229Kk=1 { (1 + \u03b7)dk\u22121 \u2264 tk(n) \u2264 (1 + \u03b7)dk } , for all d \u2208 D.\nWe have A = \u222ad\u2208D(A\u2229Bd), and hence P[A] \u2264 \u2211\nd\u2208D P[A\u2229Bd]. We let \u03b7 = 1/(\u03b4\u22121) and apply Lemma 6 with tk = (1 + \u03b7)dk\u22121. Since \u03b4 \u2265 K + 1, for \u03b7 = 1/(\u03b4 \u2212 1), \u03b4 \u2265 (1 + \u03b7)K, and the application of Lemma 6 is legitimate. We obtain for all d \u2208 D:\nP[A \u2229Bd] \u2264 ( \u03b4e\nK\n)K e\u2212\u03b4/(1+\u03b7).\nSince |D| = DK , we deduce that P[A] \u2264 ( D\u03b4e K )K e\u2212\u03b4/(1+\u03b7). Now with our choice \u03b7 = 1/(\u03b4 \u2212 1), and using the inequality log(1 + \u03b7) = \u2212 log(1/(1 + \u03b7)) \u2265 1\u2212 1/(1 + \u03b7) = 1/\u03b4, we get:\nP[A] \u2264 e\u2212\u03b4 ( \u03b4d\u03b4 log(n)e\nK\n)K eK+1,\nwhich concludes the proof.\nLemma 6 For any k = 1, . . . ,K, let 1 \u2264 t\u0304k \u2264 n. Let \u03b7 > 0. Define the event:\nC = \u2229Kk=1{t\u0304k \u2264 tk(n) \u2264 (1 + \u03b7)t\u0304k}.\nFor \u03b4 \u2265 (1 + \u03b7)K, we have:\nP [ 1C\nK\u2211 k=1 tk(n)I +(\u03b8\u0302k(n), \u03b8k) \u2265 \u03b4\n] \u2264 ( \u03b4e\nK\n)K e\u2212\u03b4/(1+\u03b7).\nProof of Lemma 6. Define the event E = {1C \u2211K k=1 tk(n)I +(\u03b8\u0302k(n), \u03b8k) \u2265 \u03b4}. We shall prove that for all \u03b6 \u2208 (R+)K :\nP[\u2229Kk=1{1Ctk(n)I+(\u03b8\u0302k(n), \u03b8k) \u2265 \u03b6k}] \u2264 e\u2212( \u2211K k=1 \u03b6k)/(1+\u03b7).\nLet \u03b6 \u2208 (R+)K . For t \u2265 0, we define xk(t) such that (i) if there exists 0 \u2264 x \u2264 \u03b8k such that tI+(x, \u03b8k) = \u03b6k, then xk(t) = x, (ii) else xk(t) = 0. By monotonicity of I+, t 7\u2192 xk(t) is increasing. Hence tk(n)I+(\u03b8\u0302k(n), \u03b8k) \u2265 \u03b6k implies that \u03b8\u0302k(n) \u2264 xk(tk(n)) \u2264 xk(t\u0304k(1 + \u03b7)). We also have t\u0304kI+(xk(t\u0304k(1 + \u03b7)), \u03b8k) = \u03b6k/(1 + \u03b7).\nWe deduce that\nP[\u2229k{1Ctk(n)I+(\u03b8\u0302k(n), \u03b8k) \u2265 \u03b6k}] \u2264 P[\u2229k{\u03b8\u0302k(n) \u2264 xk(tk(n)), C}] \u2264 P[\u2229k{\u03b8\u0302k(n) \u2264 xk(t\u0304k(1 + \u03b7)), C}]\n\u2264 K\u220f k=1 e\u2212t\u0304kI(xk(t\u0304k(1+\u03b7)),\u03b8k)] = e\u2212 \u2211K k=1 \u03b6k/(1+\u03b7),\nwhere the last inequality is obtained by applying Lemma 7 with Ck = xk(t\u0304k(1+\u03b7)). Next we apply Lemma 8 with Zk = 1Ctk(n)I+(\u03b8\u0302k(n), \u03b8k) and a = 1/(1 + \u03b7). We get:\nP[E] \u2264 (\n\u03b4e\nK(1 + \u03b7)\n)K e\u2212\u03b4/(1+\u03b7),\u2264 ( \u03b4e\nK\n)K e\u2212\u03b4/(1+\u03b7).\nLemma 7 For any k = 1, . . . ,K, let 1 \u2264 t\u0304k \u2264 n. Then for all 0 \u2264 Ck \u2264 \u03b8k we have:\nP[\u2229Kk=1{\u03b8\u0302k(n) \u2264 Ck, t\u0304k \u2264 tk(n)}] \u2264 K\u220f k=1 e\u2212t\u0304kI(Ck,\u03b8k).\nProof of Lemma 7. For all k = 1, . . . ,K and \u03bb, we define\n\u03c6k(\u03bb) = log(E[e\u03bbXk(1)]) = log(\u03b8ke\u03bb + (1\u2212 \u03b8k)).\nOne can easily show that for all x \u2208 [0, \u03b8k], I(x, \u03b8k) = sup\u03bb\u22640{\u03bbx \u2212 \u03c6k(\u03bb)}. Define the events F = F1 \u2229 F2, where F1 = \u2229Kk=1{t\u0304k \u2264 tk(n)}, and F2 = \u2229Kk=1{\u03b8\u0302k(n) \u2264 Ck}.\nFor all k, let \u03bbk \u2264 0, and define G(n) = exp (\u2211K k=1 \u03bbkSk(n)\u2212 tk(n)\u03c6k(\u03bbk) )\n. For all n\u2032 \u2264 n we have G(n\u2032) = G(n\u2032 \u2212 1) \u220fK k=1 e Bk(n \u2032)(\u03bbkXk(n\n\u2032)\u2212\u03c6k(\u03bbk)). Since Bk(n\u2032) is Fn\u2032\u22121 measurable and {Xk(n\u2032)}k is independent of Fn\u2032\u22121, we deduce that E[G(n\u2032)|Fn\u2032\u22121] = G(n\u2032 \u2212 1), i.e., G is a martingale. Furthermore E[G(n)] = 1.\nFor all k, we set \u03bbk = arg max\n\u03bb\u22640 {\u03bbCk \u2212 \u03c6k(\u03bb)}, (15)\nso that \u03bbkCk \u2212 \u03c6k(\u03bbk) = I(Ck, \u03b8k). We have \u03bbk < 0 and therefore:\nP[F ] = P[\u2229Kk=1{Sk(n) \u2264 tk(n)Ck , F1}]\n\u2264 P[ K\u2211 k=1 \u03bbkSk(n) \u2265 K\u2211 k=1 \u03bbktk(n)Ck , F1]\n\u2264 P[1F1e \u2211K k=1 \u03bbkSk(n) \u2265 e \u2211K k=1 \u03bbktk(n)Ck ]\n= P[1F1G(n) \u2265 e \u2211K k=1 tk(n)(\u03bbkCk\u2212\u03c6k(\u03bbk))]\n= P[1F1G(n) \u2265 e \u2211K k=1 tk(n)I(Ck,\u03b8k)]\n\u2264 P[1F1G(n) \u2265 e \u2211K k=1 t\u0304kI(Ck,\u03b8k)].\nUsing Markov inequality and the fact that E[1F1G(n)] \u2264 E[G(n)] = 1, and we obtain the announced result:\nP[F ] \u2264 E[1F1G(n)]e\u2212 \u2211K k=1 t\u0304kI(Ck,\u03b8k) \u2264 e\u2212 \u2211K k=1 t\u0304kI(Ck,\u03b8k).\nLemma 8 Let a > 0, K \u2265 2. Let Z \u2208 RK be a random variable such that for all \u03b6 \u2208 (R+)K:\nP[Z \u2265 \u03b6] \u2264 e\u2212a \u2211K k=1 \u03b6k .\nThen for all \u03b4 \u2265 K/a \u2208 R+:\nP[ K\u2211 k=1 Zk \u2265 \u03b4] \u2264 ( a\u03b4e K )K e\u2212a\u03b4.\nProof of Lemma 8. Let Y \u2208 (R+)K a vector whose components are independent and exponentially distributed with parameter a. Then, Z \u2264uo Y since for all \u03b6 \u2208 (R+)K (see Lemma 9):\nP[Z \u2265 \u03b6] \u2264 e\u2212a \u2211K k=1 \u03b6k = P[Y \u2265 \u03b6].\nLet \u03bb \u2208 [0, a) and \u03b4 \u2208 R+. Using Markov inequality we get:\nP[ K\u2211 k=1 Zk \u2265 \u03b4] = P[e\u03bb \u2211K k=1 Zk \u2265 e\u03bb\u03b4] \u2264 e\u2212\u03bb\u03b4E[e\u03bb \u2211K k=1 Zk ]\n= e\u2212\u03bb\u03b4E[ K\u220f k=1 e\u03bbZk ] \u2264 e\u2212\u03bb\u03b4E[ K\u220f k=1 e\u03bbYk ]\n= e\u2212\u03bb\u03b4 K\u220f k=1 E[e\u03bbYk ].\nwhere we have used the results of Lemma 9 with fk(z) = ez\u03bb for all k. Note that z 7\u2192 ez\u03bb is positive and increasing.\nFurthermore we have E[e\u03bbYk ] = \u222b +\u221e\n0 ae \u2212aye\u03bbydy = aa\u2212\u03bb . Hence we have established that for\nall 0 \u2264 \u03bb < a:\nP[ K\u2211 k=1 Zk \u2265 \u03b4] \u2264 e\u2212\u03bb\u03b4 aK (a\u2212 \u03bb)K .\nSetting \u03bb = a\u2212K/\u03b4 \u2265 0, we obtain:\nP[ K\u2211 k=1 Zk \u2265 \u03b4] \u2264 ( a\u03b4e K )K e\u2212a\u03b4.\nThe next lemma presents a result on multivariate stochastic ordering, see Mu\u0308ller and Stoyan\n(2002)[Theorem 3.3.16].\nLemma 9 Let X and Y be two random variables on RK . The following are equivalent: (i) X \u2264uo Y , (ii) For all x \u2208 RK , P[X \u2265 x] \u2264 P[Y \u2265 x], (iii) For all collections of non negative increasing functions f1, . . . , fK we have E[ \u220fK k=1 fk(Xk)] \u2264\nE[ \u220fK k=1 fk(Yk)]."}, {"heading": "Appendix C. Proof of Theorem 3", "text": "We first present two important corollaries of our concentration inequality (Theorem 2).\nCorollary 10 Let f(n) = log(n) + (3K + 1) log log(n). There exists n0 such that for all n \u2265 n0:\nP [ K\u2211 k=1 tk(n)I +(\u03b8\u0302k(n), \u03b8k) \u2265 f(n) ] \u2264 1 n log(n) .\nCorollary 11 Let f(n) = log(n) + (3K + 1) log log(n), and define \u03bbq,kk\u2032 = q\u2212L|xk \u2212 xk\u2032 |. Then there exists n0 such that for all n \u2265 n0:\nP[bk(n) < \u03b8k] \u2264 1\nn log(n) .\nProof of Corollary 11. Since I+ is increasing in its second argument, the event bk(n) < \u03b8k implies that:\nK\u2211 k\u2032=1 tk\u2032(n)I +(\u03b8\u0302k\u2032(n), \u03bb \u03b8k,k k\u2032 ) \u2265 f(n).\nFurthermore, by definition \u03bb\u03b8k,kk\u2032 = \u03b8k \u2212 L|xk \u2212 xk\u2032 | \u2264 \u03b8k\u2032 . Hence:\nK\u2211 k\u2032=1 tk\u2032(n)I +(\u03b8\u0302k\u2032(n), \u03b8k\u2032) \u2265 f(n).\nWe can now apply Corollary 10 and obtain:\nP[bk(n) < \u03b8k] \u2264 P[ K\u2211 k\u2032=1 tk\u2032(n)I +(\u03b8\u0302k\u2032(n), \u03b8k\u2032) \u2265 f(n)] \u2264\n1\nn log(n) .\nWe then give an important lemma that allows us to upper bound the average cardinalities of\nparticular sets of rounds. This lemma is stated and proved in Combes and Proutiere (2014b).\nLemma 12 Let k \u2208 K, and > 0. Define Fn the \u03c3-algebra generated by (Xk(t))1\u2264t\u2264n,1\u2264k\u2264K . Let \u039b \u2282 N be a (random) set of instants. Assume that there exists a sequence of (random) sets (\u039b(s))s\u22651 such that (i) \u039b \u2282 \u222as\u22651\u039b(s), (ii) for all s \u2265 1 and all n \u2208 \u039b(s), tk(n) \u2265 s, (iii) |\u039b(s)| \u2264 1, and (iv) the event n \u2208 \u039b(s) is Fn-measurable. Then for all \u03b4 > 0:\nE[ \u2211 n\u22651 1{n \u2208 \u039b, |\u03b8\u0302k(n)\u2212 \u03b8k| > \u03b4}] \u2264 1 \u03b42 . (16)\nWe are now ready to analyze the regret achieved under OSLB( ).\nProof of Theorem 3. Let S(\u03b8) denote the set of solutions of (3) for a given \u03b8. For any \u03c7 > 0, we define the set\n\u0393\u03c7,\u03b8 = \u222a{\u03b8\u2032:|\u03b8\u2032k\u2212\u03b8k|<\u03c7,\u2200k}S(\u03b8 \u2032),\nand for all k, c\u03c7k = sup{ck : c \u2208 \u0393\u03c7,\u03b8}. In view of Lemma 13, \u03b8 \u2032 7\u2192 S(\u03b8\u2032) is upper hemicontinuous at \u03b8 and by Assumption 1 S(\u03b8) reduces to a point. Therefore, for any open neigbourhood B of S(\u03b8), there exists \u03c7 > 0 such that S(\u03b8\u2032) \u2282 B if supk |\u03b8\u2032k \u2212 \u03b8k| < \u03c7. Hence for all k: c \u03c7 k \u2192 ck(\u03b8), as \u03c7\u2192 0. Fix 0 < \u03b4 < (\u03b8? \u2212maxk 6=k? \u03b8k)/2 and > 0. To simplify the notation, we replace by K in the Theorem 3, and prove the result for this choice of . Let k be a suboptimal arm. We derive an upper on the number of times it is played. Let n be a round where k is played, i.e., k(n) = k. In view of the design of OSLB( ), there are three possible scenarios: (a) k can be the leader and its empirical reward exceeds the indexes of other arms, L(n) = k and \u03b8\u0302k(n) \u2265 maxl bl(n); (b) k and k? are not the leader, and k can be either k(n) or k(n); (c) k? is the leader, and again k can be either k(n) or k(n). We investigate all cases, but we start by defining sets of rounds whose average cardinalities can be easily controlled:\nAk = {1 \u2264 n \u2264 T : k(n) = k, bk(n) \u2264 \u03b8k} Bk = {n \u2265 1 : k(n) = k,min\nk\u2032 tk\u2032(n) \u2265 tk(n),max k\u2032 |\u03b8\u0302k\u2032(n)\u2212 \u03b8k\u2032 | \u2265 \u03b4}\nEk = {n \u2265 1 : k(n) = k, |\u03b8\u0302k(n)\u2212 \u03b8k| \u2265 \u03b4} Fk = {n \u2265 1 : k(n) = k, tk(n) \u2264 min(tk\u2032(n), tk?(n)), max\nl\u2208{k\u2032,k?} |\u03b8\u0302l(n)\u2212 \u03b8l| \u2265 \u03b4}\nand A = \u222akAk, B = \u222akBk, E = \u222akEk, F = \u222akFk. From the concentration inequality, and its corollaries, we have E[|A|] \u2264 C1 log log(T ). We use Lemma 12 to bound the cardinalities of the other sets.\n\u2022 Bound for Bk. Let us fix k\u2032 6= k. We apply Lemma 12 to k\u2032 with \u039b(s) = {n : k(n) = k,minl tl(n) \u2265 s, tk(n) = s}, and \u039b = \u222as\u039b(s). We get that:\nE [\u2223\u2223{n : k(n) = k,min\nl tl(n) \u2265 tk(n), |\u03b8\u0302k\u2032(n)\u2212 \u03b8k\u2032 | \u2265 \u03b4} \u2223\u2223] \u2264 1 \u03b42 .\nWe conclude that: E[|Bk|] \u2264 K/( \u03b42).\n\u2022 Bound for Ek. The application of lemma is direct here, and we get: E[|Ek|] \u2264 1/\u03b42.\n\u2022 Bound for Fk. Using the same argument as that used to bound the cardinality of Bk, we get: E[|Fk|] \u2264 2/\u03b42.\nNext we consider n /\u2208 A \u222a B \u222a E \u222a F such that k is played. We treat all cases (a), (b), and (c) that can arise in such a round.\nCase (a) We assume here that k = L(n) and that k(n) = k, so that \u03b8\u0302k(n) \u2265 maxl bl(n). Hence, since n /\u2208 Ak? , \u03b8\u0302k(n) \u2265 bk?(n) \u2265 \u03b8?. In summary, \u03b8\u0302k(n) \u2265 \u03b8?, which is impossible because of our choice of \u03b4 (< \u03b8? \u2212 \u03b8k), and n /\u2208 Ek.\nCase (b) Let k\u2032 /\u2208 {k, k?} be the leader in round n, and assume that k(n) = k. We consider two subcases: (i) k = k(n), and (ii) k = k(n). (i) In this case, k has been played less than any other arm, and so tk(n) \u2264 min(tk\u2032(n), tk?(n)). On the other hand, since k\u2032 is the leader, we have \u03b8\u0302k\u2032(n) \u2265 \u03b8\u0302k?(n), which implies that either \u03b8k\u2032 or \u03b8k? is badly estimated. More precisely, we proved that n \u2208 Fk, which is impossible. (ii) In this case, we know that tk(n) \u2264 tk(n)(n)/ . In addition, again, we have \u03b8\u0302k\u2032(n) \u2265 \u03b8\u0302k?(n), and so either \u03b8k\u2032 or \u03b8k? is badly estimated. We proved that n \u2208 Bk, which is impossible.\nCase (c) Assume that k? = L(n). k is played, and we need to consider two subcases: (i) k = k(n), and (ii) k = k(n). (i) In this case, since k = k(n), we have tk(n) \u2264 minl tl(n), and hence tk(n) \u2264 minl tl(n). Since n /\u2208 Bk, in view of the previous inequality, all arms must be well-estimated, i.e., maxl |\u03b8\u0302l(n) \u2212 \u03b8l| < \u03b4. This implies that for all l \u2208 K, c\u0302l(n) \u2264 c\u03b4l . Now by definition in our algorithm, if k(n) = k = k(n), then tk(n) < tk(n)(n), and so tk(n) < maxl c \u03b4 l log(n). In other words, n \u2208 Dk where\nDk = {1 \u2264 n \u2264 T, n /\u2208 A \u222aB \u222a E \u222a F,L(n) = k?, k(n) = k, tk(n) \u2264 max k\u2032 c\u03b4k\u2032 log(T )}.\nWe shall bound the size of Dk later in the proof. (ii) In this case, we must have tt(n)(n) \u2265 tk(n). Hence since n /\u2208 Bk, all arms are well estimated, and hence again, for all l \u2208 K, c\u0302l(n) \u2264 c\u03b4l . In particular, since k is played, tk(n) \u2264 c\u03b4k log(n), and thus n \u2208 Ck where\nCk = {1 \u2264 n \u2264 T, n /\u2208 A \u222aB, k(n) = k, tk(n) \u2264 c\u03b4k log(T )}.\nNest we bound the expected cardinalities of Ck and Dk. Since tk(n) is incremented if n \u2208 Ck or n \u2208 Dk, we simply have:\n|Ck| \u2264 c\u03b4k log(T ), |Dk| \u2264 max k\u2032 c\u03b4k\u2032 log(T ).\nPutting it all together we have proven the announced regret bound:\nR\u03c0(T ) \u2264 \u2211 k 6=k? (\u03b8? \u2212 \u03b8k)(E[|Ck|] + E[|Dk|])\n+ E[|A|] + E[|B|] + E[|E|] + E[|F |], \u2264 log(T ) \u2211 k 6=k? (\u03b8? \u2212 \u03b8k)(c\u03b4k + max k\u2032 c\u03b4k\u2032) + C1 log log(T ) +K 2 \u22121\u03b4\u22122 + 3K\u03b4\u22122.\nThis completes the proof (because of our particular choice of , and maxl c\u03b4l \u2264 \u2211 l c \u03b4 l ).\nC.1. Continuity of solutions to parametric linear programs\nWe state and prove Lemma 13, a technical result about the continuity of the solutions of a parametric linear program with respect to its parameters. It follows from the general conditions of Wets (1985).\nLemma 13 Consider A \u2208 (R+)K\u00d7K , c \u2208 (R+)K , and T \u2282 (R+)K\u00d7K \u00d7 (R+)K . Define t = (A, c). Consider the function Q and the set-valued map Q?\nQ(t) = inf x\u2208RK\n{cx|Ax \u2265 1, x \u2265 0}\nQ?(t) = {x : cx \u2264 Q(t)|Ax \u2265 1, x \u2265 0}.\nAssume that:\n(i) For all t \u2208 T , all rows and columns of A are non-identically 0\n(ii) mint\u2208T mink ck > 0\nThen:\n(a) Q is continuous on T .\n(b) Q? is upper hemicontinuous on T .\nProof. Define c0 = min(1,min\nt\u2208T min k ck) > 0,\nand a = max(k,k\u2032)Ak,k\u2032 . Define the sets K = {x|Ax \u2264 1}, D = {x|Ax \u2264 c} and B = [0, c0/(aK)]\nK . Then B \u2282 K \u2229 D, so that both K and D have non-empty interior. By Wets (1985)[Corollary 7], t \u2192 K and t \u2192 D are continuous on T since they have non-empty interior and all rows of (A, 1) and columns of ( A c ) are non identically 0. By Wets (1985)[Theorem 2], Q is continuous on T since both K and D are continuous on T , proving the first statement. Consider a sequence {(ti, xi)}i\u22651, such that xi \u2208 Q?(ti) and (ti, xi) \u2192 (t, x), i \u2192 \u221e. Since for all i \u2265 1 cxi \u2264 Q(ti) and Axi \u2265 1 we have, by continuity, Ax \u2265 1 and cx = Q(t) and so x \u2208 Q?(t). Hence Q? is upper hemicontinuous."}, {"heading": "Appendix D. Proof of Theorem 4", "text": "D.1. Proof of (i)\nLet 0 < \u03b4 < (\u03b8\u2217 \u2212maxk\u2208K\u2212 \u03b8k)/2 fixed throughout the proof. Define the random sets of rounds: B = {1 \u2264 n \u2264 T : bk?(n) \u2264 \u03b8?} the set of rounds at which the index of the optimal arm underestimates its true value \u03b8?, and Dk = {n : k(n) = k, bk(n) \u2265 \u03b8? \u2212 \u03b4} the set of rounds at which k is selected and its index is larger than \u03b8? \u2212 \u03b4.\nLet k 6= k? be a suboptimal arm, and let n /\u2208 B such that k is selected k(n) = k. The possible events are:\n(a) If L(n) \u2208 {k, k?} then bk(n) \u2265 bk?(n) \u2265 \u03b8? since n /\u2208 B, so n \u2208 Dk.\n(b) If L(n) = k\u2032 /\u2208 {k, k?}, then bk(n) \u2265 bk\u2032(n) and:\n(b-i) If we further have bk\u2032(n) \u2265 \u03b8? \u2212 \u03b4 then bk(n) \u2265 \u03b8? \u2212 \u03b4 so n \u2208 Dk as well. (b-ii) Otherwise bk\u2032(n) \u2264 \u03b8? \u2212 \u03b4.\nDefine the random set of instants Ek = {n 6\u2208 B : k(n) = k, L(n) 6\u2208 {k, k?}, bk\u2217(n) > bL(n)(n), |\u03b8\u0302k\u2217(n) \u2212 \u03b8k\u2217 | \u2265 \u03b4}. In the case (b-ii), we have bL(n)(n) \u2264 \u03b8? \u2212 \u03b4 < \u03b8? \u2264 bk?(n) since n /\u2208 B. Also by definition of L(n) we have that \u03b8\u0302k?(n) \u2264 \u03b8\u0302L(n)(n) \u2264 bL(n)(n) \u2264 \u03b8? \u2212 \u03b4. So in case (b-ii) we have n \u2208 Ek.\nIn summary, k(n) = k implies that n \u2208 B\u222aEk\u222aDk so: E[tk(T )] \u2264 E[|B|]+E[|Ek|]+E[|Dk|]. Let us upper bound the expected sizes of sets B, Ek and Dk.\nExpected size of B: From Theorem 2, there exists a constant C1 \u2265 0 such that E[|B|] is upper bounded by the Bertrand series:\nE[|B|] \u2264 T\u2211 n=1 C1(n log(n)) \u22121 \u2264 C1 log(log(T )),\nExpected size of Ek: If n \u2208 Ek , we have bk\u2217(n) > bL(n)(n) > \u03b8\u0302L(n)(n), so that by design of CKL-UCB, k(n) \u2208 arg min\nk:bk(n)>bL(n)(n) tk(n) and k\u2217 \u2208 {k : bk(n) > bL(n)(n)}. Since k(n) = k, we have tk(n) \u2265 tk?(n). Define s = \u2211n\nn\u2032=1 1{n\u2032 \u2208 Ek}, this implies tk\u2217(n) \u2265 s. Applying Lemma 12 as earlier, we conclude that E[Ek] \u2264 \u03b4\u22122.\nExpected size of Dk: Define F \u03b4k = {n : k(n) = k, |\u03b8\u0302k(n) \u2212 \u03b8k| < \u03b4} and F \u03b4k = {n : k(n) = k, |\u03b8\u0302k(n) \u2212 \u03b8k| \u2265 \u03b4}. Let us consider a round n \u2208 Dk \u2229 F \u03b4k . Assume that tk(n) > f(n)/I(\u03b8k + \u03b4, \u03b8\u2217\u2212\u03b4). Since n \u2208 Dk and k(n) = k, we have: bk(n) \u2265 \u03b8\u2217\u2212\u03b4. Therefore, from the monotonicity of I(x, y) in y when y > x, we have:\ntk(n)I(\u03b8\u0302k(n), \u03b8 \u2217 \u2212 \u03b4) \u2264 \u2211 i\u2208K ti(n)I(\u03b8\u0302i(n), \u03bb \u03b8\u2217\u2212\u03b4,k i ) \u2264 \u2211 i\u2208K ti(n)I(\u03b8\u0302i(n), \u03bb bk(n),k i ) = f(n) (17)\nwhere the last equality comes from our definition of bk(n). Furthermore, by our assumption and since \u03b8\u0302k(n) \u2264 \u03b8k + \u03b4 (since n \u2208 F \u03b4k ):\nf(n) < tk(n)I(\u03b8k + \u03b4, \u03b8 \u2217 \u2212 \u03b4) \u2264 tk(n)I(\u03b8\u0302k(n), \u03b8\u2217 \u2212 \u03b4),\nwhich contradicts (17). Thus for all rounds in n \u2208 Dk\u2229F \u03b4k we have tk(n) \u2264 f(n)/I(\u03b8k+\u03b4, \u03b8\u2217\u2212\u03b4) and consequently E[|Dk|] \u2264 f(T )/I(\u03b8k + \u03b4, \u03b8\u2217 \u2212 \u03b4) + E[|F \u03b4k |].\nAgain a direct application of Lemma 12 yields E[|F \u03b4k |] \u2264 \u03b4 \u22122. Thus, we have:\nE[tk(T )] \u2264 f(T )/I(\u03b8k + \u03b4, \u03b8\u2217 \u2212 \u03b4) + C1 log(log(T )) + 2\u03b4\u22122.\nD.2. Proof of (ii)\nWe work with a fixed sample path throughout the proof. Since for all k when T \u2192 \u221e we have tk(T )\u2192\u221e a.s., so by the law of large numbers \u03b8\u0302k(T )\u2192 \u03b8k as T \u2192\u221e.\nFrom the first statement of the theorem, we have that for all k 6= k?, lim sup T\u2192\u221e E[tk(T )]/ log(T ) <\n\u221e which implies that lim sup T\u2192\u221e tk(T )/ log(T ) < \u221e . In turn we have that tk?(T ) = T \u2212\u2211 k 6=k? tk(T ) = T \u2212O(log(T )), so that tk?(T )/T \u2192T\u2192\u221e 1. By Pinsker\u2019s inequality:\n\u03b8\u0302k?(T ) \u2264 bk?(T ) \u2264 \u03b8\u0302k?(T ) + \u221a 2f(T )/tk?(T )\nand we can deduce bk?(T )\u2192 \u03b8? as T \u2192\u221e, because \u03b8\u0302k?(T )\u2192 \u03b8? and f(T )/tk?(T ) = f(T )/(T\u2212 O(log(T )))\u2192 0 when T \u2192\u221e.\nLet \u03b4 such that 0 < \u03b4 < (\u03b8? \u2212 max k\u2208K\u2212 \u03b8k)/2, by the above reasoning there exists n0 \u2208 N (depending on the sample path and \u03b4) such that for all n \u2265 n0 we have |bk?(n) \u2212 \u03b8?| \u2264 \u03b4 and |\u03b8\u0302k(n)\u2212 \u03b8k| \u2264 \u03b4 for all k. It is noted that for all n \u2265 n0, L(n) = k?, since \u03b4 < (\u03b8? \u2212max\nk 6=k? \u03b8k)/2.\nLet \u03b10 \u2265 0, and assume that there exists T large enough such that tk(T ) = \u03b10f(T ) and \u03b10f(T ) > tk(n0). Therefore there exists n0 \u2264 n \u2264 T such that tk(n) = \u03b10f(T ) \u2212 1 and k is selected at time n: k(n) = k. DefineN = {k\u2032 : bk\u2032(n) \u2264 bk?(n)}. Consider k\u2032 \u2208 N , since n \u2265 n0, we have that L(n) = k? and bL(n) \u2264 \u03b8? + \u03b4. So bk\u2032(n) \u2264 \u03b8? + \u03b4 which implies (by definition of bk\u2032(n)): \u2211\nk\u2032\u2032\u2208K tk\u2032\u2032(n)I(\u03b8k\u2032\u2032 \u2212 \u03b4, \u03bb\u03b8\n?+\u03b4,k\u2032\nk\u2032\u2032 ) \u2265 f(n) (18)\nAlso, since k(n) = k, by design of CKL-UCB we have tk(n) = arg min k\u2032 6\u2208N tk\u2032(n), so that :\ntk\u2032(n) \u2265 tk(n) = \u03b10f(T )\u2212 1 \u2265 \u03b10f(n)\u2212 1, \u2200k\u2032 6\u2208 N . (19)\nFinally, since k(n) = k, L(n) = k?, we must have bk(n) \u2265 bk?(n) \u2265 \u03b8? \u2212 \u03b4, so that:\u2211 k\u2032\u2208K tk\u2032(n)I(\u03b8k\u2032 + \u03b4, \u03bb \u03b8?\u2212\u03b4,k k\u2032 ) \u2264 f(n)\n(\u03b10f(T )\u2212 1)I(\u03b8k + \u03b4, \u03bb\u03b8 ?\u2212\u03b4,k k ) + \u2211 k\u2032\u2208K\\{k} tk\u2032(n)I(\u03b8k\u2032 + \u03b4, \u03bb \u03b8?\u2212\u03b4,k k\u2032 ) \u2264 f(n) (\u03b10f(n)\u2212 1)I(\u03b8k + \u03b4, \u03bb\u03b8 ?\u2212\u03b4,k k ) +\n\u2211 k\u2032\u2208K\\{k} tk\u2032(n)I(\u03b8k\u2032 + \u03b4, \u03bb \u03b8?\u2212\u03b4,k k\u2032 ) \u2264 f(n) (20)\nDefine the matrix A\u0303 = (a\u0303k\u2032k)k,k\u2032 , with a\u0303k\u2032k = I(\u03b8k\u2032 + \u03b4, \u03bb \u03b8?\u2212\u03b4,k k\u2032 ) for all k \u2032 and a\u0303k\u2032\u2032k\u2032 = I(\u03b8k\u2032\u2032 \u2212 \u03b4, \u03bb\u03b8 ?+\u03b4,k\u2032\nk\u2032\u2032 ) for all k \u2032 6= k and all k\u2032\u2032.\nDefine \u03b1k\u2032(n) = tk\u2032(n)/f(n) for all k\u2032, and by dividing equations (20) , (19) and (18) by f(n), we obtain:\n\u03b10a\u0303kk + \u2211\nk\u2032\u2208K\\{k}\n\u03b1k\u2032(n)a\u0303k\u2032k \u2264 1,\n\u03b1k\u2032(n) \u2265 \u03b10 \u2212 1 f(n) , \u2200k\u2032 /\u2208 N ,\n\u03b10a\u0303kk\u2032 + \u2211\nk\u2032\u2032\u2208K\\{k}\n\u03b1k\u2032\u2032(n)a\u0303k\u2032\u2032k\u2032 \u2265 1, \u2200k\u2032 \u2208 N .\nIt is noted that \u03b1\u2032k(n) \u2265 0 for all k\u2032 by definition. Define \u03b1 = (\u03b11, . . . , \u03b1K) a limit point of the sequence (\u03b1(n))n\u22651 (note that this sequence need not converge and might have several limit points). First letting n \u2192 \u221e along a converging subsequence and then letting \u03b4 \u2192 0 the constraints above become:\n\u03b10akk + \u2211\nk\u2032\u2208K\\{k}\n\u03b1k\u2032ak\u2032k \u2264 1,\n\u03b1k\u2032 \u2265 \u03b10, \u2200k\u2032 /\u2208 N , \u03b10akk\u2032 + \u2211\nk\u2032\u2032\u2208K\\{k}\n\u03b1k\u2032\u2032ak\u2032\u2032k\u2032 \u2265 1, \u2200k\u2032 \u2208 N .\nTherefore, by definition of dk, we must have:\n\u03b10a\u0303kk + dk(A,\u03b10,N ) \u2264 1,\nand taking the infimum over N so that we obtain the condition:\n\u03b10a\u0303kk + ek(A,\u03b10) \u2264 1. (21)\nNow consider \u03b10 such that \u03b10a\u0303kk + ek(A,\u03b10) > 1. Then in view of the necessary condition (21), we cannot have tk(T ) \u2265 \u03b10 log(T ), so that:\nlim sup T\u2192\u221e\ntk(T )\nlog(T ) \u2264 inf{\u03b10 \u2265 0 : akk\u03b10 + ek(A,\u03b10) > 1} = \u03b2k(\u03b8).\nWe get (ii) by Lebesgue\u2019s dominated convergence theorem, since supT\u22651 E[ tk(T ) log(T ) ] <\u221e from (i).\nD.3. Proof of (iii)\nIn order to prove the last part of the theorem, it is sufficient to prove that for \u03b10 = 1/akk, we have ek(A,\u03b10) > 0 so that akk\u03b10 + ek(A,\u03b10) = 1 + ek(A,\u03b10) > 1 so that \u03b2k(\u03b8) < \u03b10 = 1/akk.\nWe proceed by contradiction. Assume that ek(A,\u03b10) = 0. Then there exists N a subset of {1, . . . ,K} \\ {k?, k} such that dk(A,\u03b10,N ) = 0. As a consequence there exists \u03b11, ..., \u03b1K such that:\n\u2211 k\u2032\u2208K\\{k} \u03b1k\u2032ak\u2032k = 0\ns.t. \u03b1k\u2032 \u2265 \u03b10, \u2200k\u2032 6\u2208 N \u03b1k\u2032 \u2265 0, \u2200k\u2032\u2211 k\u2032\u2032\u2208K\\{k} \u03b1k\u2032\u2032ak\u2032\u2032k\u2032 \u2265 1\u2212 \u03b10akk\u2032 , \u2200k\u2032 \u2208 N .\nConsider there exists k\u2032 such that ak\u2032k > 0. Then we must have k\u2032 \u2208 N , otherwise \u03b1k\u2032 \u2265 \u03b10 = 1/akk and 0 = \u2211 k\u2032\u2032\u2208K\\{k} \u03b1k\u2032\u2032ak\u2032\u2032k \u2265 \u03b10ak\u2032k > 0, a contradiction. By the same reasoning we must also have \u03b1k\u2032 = 0. As said in the theorem statement, assume that there exists k\u2032 \u2208 N such that 0 < akk\u2032 < akk and assume that for all k\u2032\u2032 we have that if ak\u2032\u2032k = 0 then ak\u2032\u2032k\u2032 as well. Considering k\u2032\u2032 = k\u2032 in our assumption, since ak\u2032k = 0 would imply ak\u2032k\u2032 = 0, we then have that ak\u2032k > 0 since ak\u2032k\u2032 = I(\u03b8k\u2032 , \u03b8\n\u2217) > 0. From our previous argument we have that if ak\u2032k > 0 then k\u2032 \u2208 N . Therefore :\n\u2211 k\u2032\u2032\u2208K\\{k}\n\u03b1k\u2032\u2032ak\u2032\u2032k\u2032 \u2265 1\u2212 \u03b10akk\u2032 = 1\u2212 akk\u2032/akk > 0\u2211 k\u2032\u2032 6=k, ak\u2032\u2032k=0 \u03b1k\u2032\u2032ak\u2032\u2032k\u2032 > 0 (22)\nBy assumption, ak\u2032\u2032k = 0 implies ak\u2032\u2032k\u2032 = 0 so that the l.h.s. of (22) is zero and cannot be strictly larger than 0. This is a contradiction, proving that ek(A,\u03b10) = 0 cannot occur and concludes the proof."}], "references": [{"title": "The continuum-armed bandit problem", "author": ["R. Agrawal"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Agrawal.,? \\Q1995\\E", "shortCiteRegEx": "Agrawal.", "year": 1995}, {"title": "Finite time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Online optimization in x-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bubeck et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2008}, {"title": "Kullback-leibler upper confidence bounds for optimal sequential allocation", "author": ["O. Capp\u00e9", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz"], "venue": "Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Unimodal bandits: Regret lower bounds and optimal algorithms", "author": ["R. Combes", "A. Proutiere"], "venue": "In Proc. of ICML,", "citeRegEx": "Combes and Proutiere.,? \\Q2014\\E", "shortCiteRegEx": "Combes and Proutiere.", "year": 2014}, {"title": "Unimodal bandits: Regret lower bounds and optimal algorithms", "author": ["R. Combes", "A. Proutiere"], "venue": "Technical Report, people.kth.se/ \u0303alepro/pdf/tr-icml2014.pdf,", "citeRegEx": "Combes and Proutiere.,? \\Q2014\\E", "shortCiteRegEx": "Combes and Proutiere.", "year": 2014}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["V. Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In Proc. of ACM/SIAM symposium on Discrete Algorithms (SODA),", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Informational confidence bounds for self-normalized averages and applications", "author": ["A. Garivier"], "venue": "In Information Theory Workshop,", "citeRegEx": "Garivier.,? \\Q2013\\E", "shortCiteRegEx": "Garivier.", "year": 2013}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Garivier and Capp\u00e9.,? \\Q2011\\E", "shortCiteRegEx": "Garivier and Capp\u00e9.", "year": 2011}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled markov chains", "author": ["T.L. Graves", "T.L. Lai"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Graves and Lai.,? \\Q1997\\E", "shortCiteRegEx": "Graves and Lai.", "year": 1997}, {"title": "Nearly tight bounds for the continuum-armed bandit problem", "author": ["R. Kleinberg"], "venue": "In Proc. of the conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Kleinberg.,? \\Q2004\\E", "shortCiteRegEx": "Kleinberg.", "year": 2004}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "In Proc. of the 40th annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Adaptive treatment allocation and the multi-armed bandit problem", "author": ["T.L. Lai"], "venue": "The Annals of Statistics, 15(3):1091\u20131114,", "citeRegEx": "Lai.,? \\Q1987\\E", "shortCiteRegEx": "Lai.", "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Comparison Methods for Stochastic Models and Risks", "author": ["A. M\u00fcller", "D. Stoyan"], "venue": null, "citeRegEx": "M\u00fcller and Stoyan.,? \\Q2002\\E", "shortCiteRegEx": "M\u00fcller and Stoyan.", "year": 2002}, {"title": "Contextual bandits with similarity information", "author": ["A. Slivkins"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Slivkins.,? \\Q2011\\E", "shortCiteRegEx": "Slivkins.", "year": 2011}, {"title": "Continuity of solutions to parametric linear programs We state and prove Lemma 13, a technical result about the continuity of the solutions of a parametric linear program with respect to its parameters. It follows from the general conditions", "author": [], "venue": "Lemma 13 Consider", "citeRegEx": "C.1.,? \\Q1985\\E", "shortCiteRegEx": "C.1.", "year": 1985}, {"title": "D, so that both K and D have non-empty interior. By Wets (1985)[Corollary 7], t \u2192 K and t \u2192 D are continuous on T since they have non-empty interior and all rows of (A, 1) and columns", "author": ["K . Then B \u2282 K"], "venue": null, "citeRegEx": "\u2229,? \\Q1985\\E", "shortCiteRegEx": "\u2229", "year": 1985}], "referenceMentions": [{"referenceID": 9, "context": "Introduction In their seminal paper, Lai and Robbins (1985) solve the classical stochastic Multi-Armed Bandit (MAB) problem.", "startOffset": 37, "endOffset": 60}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.", "startOffset": 45, "endOffset": 64}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Capp\u00e9 (2011), Capp\u00e9 et al.", "startOffset": 45, "endOffset": 122}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Capp\u00e9 (2011), Capp\u00e9 et al. (2013) \u2013 note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.", "startOffset": 45, "endOffset": 143}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Capp\u00e9 (2011), Capp\u00e9 et al. (2013) \u2013 note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.", "startOffset": 45, "endOffset": 226}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Capp\u00e9 (2011), Capp\u00e9 et al. (2013) \u2013 note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.6). When the expected rewards of the various arms are not related as in Lai and Robbins (1985), the regret of the best algorithm essentially scales as O(K log(T )) where K denotes the number of arms, and T is the time horizon.", "startOffset": 45, "endOffset": 329}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al.", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al.", "startOffset": 12, "endOffset": 52}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al.", "startOffset": 12, "endOffset": 74}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al.", "startOffset": 12, "endOffset": 101}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm.", "startOffset": 12, "endOffset": 135}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al.", "startOffset": 12, "endOffset": 711}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al.", "startOffset": 12, "endOffset": 736}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound.", "startOffset": 12, "endOffset": 758}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure.", "startOffset": 12, "endOffset": 861}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al.", "startOffset": 12, "endOffset": 1340}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al.", "startOffset": 12, "endOffset": 1365}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms.", "startOffset": 12, "endOffset": 1387}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms. In turn, these algorithms fail at exploiting the problem structure revealed by the past observed samples. For continuous bandits, we propose to first discretize the set of arms (as in Kleinberg et al. (2008)), and then apply OSLB, an algorithm that optimally exploits past observations and hence the problem specific structure.", "startOffset": 12, "endOffset": 1699}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms. In turn, these algorithms fail at exploiting the problem structure revealed by the past observed samples. For continuous bandits, we propose to first discretize the set of arms (as in Kleinberg et al. (2008)), and then apply OSLB, an algorithm that optimally exploits past observations and hence the problem specific structure. As it turns out, this approach outperforms algorithms directly dealing with continuous sets of arms. Our contributions. (a) For discrete Lipschitz bandit problems, we derive an asymptotic regret lower bound satisfied by any algorithm. This bound is problem specific in the sense that it depends in an explicit manner on the expected rewards of the various arms (this contrasts with existing lower bounds for continuous Lipschitz bandits). (b) We propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. We further present CKL-UCB (Combined KL-UCB), an algorithm that exhibits lower computational complexity than that of OSLB, and that is yet able to exploit the Lipschitz structure. (c) We provide a finite time analysis of the regret achieved under OSLB and CKL-UCB. The analysis relies on a new concentration inequality for a weighted sum of KL divergences between the empirical distributions of rewards and their true distributions. We believe that this inequality can be instrumental for various bandit problems with structure. (d) We evaluate our algorithms using numerical experiments for both discrete and continuous sets of arms. We compare their performance to that obtained using existing algorithms for continuous bandits. (e) We extend our results and algorithms to the case of contextual bandits with similarities as investigated in Slivkins (2011).", "startOffset": 12, "endOffset": 3230}, {"referenceID": 11, "context": "Results can be easily extended to the case where the set of arms is a subset of a metric space as considered in Kleinberg et al. (2008). The set of arms is of finite cardinality, possibly", "startOffset": 112, "endOffset": 136}, {"referenceID": 13, "context": "Without loss of generality, we restrict our attention to so-called uniformly good algorithms, as defined in Lai and Robbins (1985). \u03c0 \u2208 \u03a0 is uniformly good if for all \u03b8 \u2208 \u0398L, R\u03c0(T ) = o(T a) for all a > 0.", "startOffset": 108, "endOffset": 131}, {"referenceID": 10, "context": "The regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix.", "startOffset": 92, "endOffset": 114}, {"referenceID": 10, "context": "The regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix. As in classical bandits, the minimal regret scales logarithmically with the time horizon. Observe that the lower bound (2) is smaller than the lower bound derived in Lai and Robbins (1985) when the various average rewards (\u03b8k, k \u2208 K) are not related (i.", "startOffset": 92, "endOffset": 342}, {"referenceID": 10, "context": "The regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix. As in classical bandits, the minimal regret scales logarithmically with the time horizon. Observe that the lower bound (2) is smaller than the lower bound derived in Lai and Robbins (1985) when the various average rewards (\u03b8k, k \u2208 K) are not related (i.e., in absence of the Lipschitz structure). Hence (2) quantifies the gain one may expect by designing algorithms optimally exploiting the structure of the problem. Note that for any k \u2208 K\u2212, the variable ck corresponding to a solution of (3) characterizes the number of times arm k should be played under an optimal algorithm: arm k should be roughly played ck log(n) times up to round n. It should be also observed that our lower bound is problem specific (it depends on \u03b8), which contrasts with existing lower bounds for continuous Lipschitz bandits, see e.g. Kleinberg et al. (2008). The latter are typically derived by selecting the problems that yield maximum regret.", "startOffset": 92, "endOffset": 991}, {"referenceID": 8, "context": "The latter extends to the multi-dimensional case the concentration inequality derived in Garivier (2013) for a single KL divergence.", "startOffset": 89, "endOffset": 105}, {"referenceID": 15, "context": "M\u00fcller and Stoyan (2002).", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "Contextual Bandit with Similarities The algorithms and results presented above can be extended to the case of contextual bandit problems with similarities as studied in Slivkins (2011). In such problems, in each round, the decision maker observes a context, and then decides which arm to select.", "startOffset": 169, "endOffset": 185}, {"referenceID": 8, "context": "This discretization is known to be order-optimal for functions which are regular around their maximum Kleinberg (2004). In order not to give a positive bias to KL-UCB and CKL-UCB, we make sure that the maximum of the reward functions is not achieved in one of the arms in the discretization: the maximum is placed at a distance of at least \u03b4/4 from any arm in the discretization.", "startOffset": 102, "endOffset": 119}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al.", "startOffset": 92, "endOffset": 113}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal).", "startOffset": 92, "endOffset": 176}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al.", "startOffset": 92, "endOffset": 443}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al. (2008) for details) is set equal to \u221a log(n)/(2 \u2217 tk(n)) in round n.", "startOffset": 92, "endOffset": 471}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al. (2008) for details) is set equal to \u221a log(n)/(2 \u2217 tk(n)) in round n. HOO+ and Zooming+ exhibit better performance than their initial versions, but their regrets have not been analytically studied. In the experiments, we limit the time horizon to T = 25000 rounds, and the expected regret is calculated by averaging over 100 independent runs. Figure 3 presents the expected regret of the various algorithms for the triangular reward function (left) and for the quadratic reward function (right). First note that surprisingly, KL-UCB, an algorithm that does not leverage the Lipschitz structure, outperforms some of the algorithms designed to exploit the structure. Observe that CKL-UCB clearly outperforms KL-UCB and all other algorithms in both problem instances. For quadratic reward functions, it is known that the optimal discretization of the set of arms should roughly have (log(T )/T )1/4 arms, Combes and Proutiere (2014a). We also plot the regret achieved under CKL-UCB using this optimized discretization, and we observe that this indeed further reduces the regret.", "startOffset": 92, "endOffset": 1394}], "year": 2014, "abstractText": "We consider stochastic multi-armed bandit problems where the expected reward is a Lipschitz function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities.", "creator": "LaTeX with hyperref package"}}}