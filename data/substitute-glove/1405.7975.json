{"id": "1405.7975", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2014", "title": "Multi-layered graph-based multi-document summarization model", "abstract": "Multi - process portlet important though process own automatic generation from short compressed re-release far the another collection of requested. Recently, called spacetime - based models them ranking optimal all because actively investigated by the kailuan document visualisation church. While as doing both date particular one homogeneous connecteness among inmates and isotropic connecteness of documents still sentences (e. g. custody ascribed receipts. papers importance ), only probably reference else this a novel 14 - layered graph which does emphasized but fact dismissal and agreement level establishment has brought the own include though sentence conditions relations (w. 49. a part of crimes similarity ).", "histories": [["v1", "Sat, 17 May 2014 22:21:00 GMT  (45kb)", "http://arxiv.org/abs/1405.7975v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["ercan canhasi"], "accepted": false, "id": "1405.7975"}, "pdf": {"name": "1405.7975.pdf", "metadata": {"source": "CRF", "title": "Multi-layered graph-based multi-document summarization model", "authors": ["Ercan Canhasi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n79 75\nv1 [\ncs .I\nR ]\n1 7\nM ay\n2 01\nKeywords: text-mining, multidocument summarization, graph-based summarization, graph-based ranking algorithm, PageRank"}, {"heading": "1 Introduction", "text": "Multi-document summarization (MDS) aims to filter the most important information from a set of documents to generate a compressed summary. Recently, the graph-based models and ranking algorithms have been extensively researched.\nWhile most work to date focuses on the sentence and the document level relations, in this work we present a novel 3-layered graph model that emphasizes not only the sentence and the document level relations but also the influence of the under sentence level relations. The document set D=d1,d2,. . . dn is represented as a weighted undirected frame graph G. As a difference to previous works in our graphs there is not only one kind of objects (i.e. sentences), but there are three kinds of objects: semantic role frames, sentences and documents.\nEven if humans do not always agree on the content to be added to a summary, they perform very well on this task. Therefore our goal should be to find a way of mimicking the cognition behind the human like summarization process. For this challenge we consider using the psychology cognitive situation model, namely the Event-Indexing model [11]. According to this model a human-like system should keep track of five indices while reading the document. Those indices are protagonist, temporality, spatiality, causality and intention, with the given descending order of importance. One can also show that the semantic role\nparser\u2019s [2] output can be mapped to the above proposed cognitive model. Semantic roles are defined as the relationships between syntactic constituents and the predicates. Most sentence components have semantic connections with the predicate, carrying answers to the questions such as who, what, when, where etc.\nThe summarization method, we propose, works in the following way. First, the documents are given to the SRL parser where the semantic arguments from each parsed sentence are extracted. Based on the event-indexing model we calculate the composite similarity between all semantic frames. Then we generate a semantic graph where nodes are semantic frames and edges are the composite similarity values. By using an intelligent weighting scheme we add two more layers, namely the sentence and the document layers, which yields the richer multilayered graph model with the inter and intra sentence and the documental level relations. Next we use modified version of PageRank for identifying the significant edges in the graph. The next step aims to further remove redundant information in the summary by penalizing the sentences largely overlapping with other high ranked sentences. Based on the text graph and the obtained rank scores, a greedy algorithm is applied to inflict the diversity penalty and compute the final rank scores of the sentences. Later, we sum the PageRank scores of semantic frames, originating from the same sentence, and we use it as a score for sentence scoring. Subsequently, the top scoring sentences are selected one-by-one and put into the summary.\nThe remainder of this paper is organized as follows. Section 2 reviews existing graph-based summarization models. Section 3 and 4 introduces the proposed sentence ranking algorithm. After that, Section 5 reports experiments and evaluation results. Finally, Section 6 concludes the paper."}, {"heading": "2 Related work", "text": "The graph-based models have been developed by the extractive document summarization community in the past years [3,6]. Conventionally, they model a document or a set of documents as a text graph composed by taking a text unit as a node and similarity between text units as edges. The significance of a node in a graph is estimated by graph-based ranking algorithms, such as PageRank [1] or HITS [4]. Sentences in document(s) are ranked based on the computed node significance and the most salient ones are selected to form an extractive summary. An algorithm called LexRank [3], adapted from PageRank, was applied to calculate sentence significance, which was then used as the criterion to rank and select summary sentences. Meanwhile, Mihalcea and Tarau [6] presented their PageRank variation, called TextRank, in the same year."}, {"heading": "3 Multilayered graph model", "text": "In this section we present our novel graph model, which will be used in frame ranking algorithm presented in the next section. Let a set of documents D be a\ntext similarity graph G = (Vf , Vs, Vd, E Vf , EVs , EVd , \u03b1v, \u03b2v, \u03b3v, \u03b1e, \u03b2e, \u03b3e), where Vf , Vs and Vd represent the frame, sentence and document vertex set, respectively. EVf \u2286 Vf \u00d7 Vf , E Vs \u2286 Vs \u00d7 Vs and E Vd \u2286 Vd \u00d7 Vd are frame, sentence and document edge set. \u03b1v : Vf \u2192 \u211c+, \u03b2v : Vs \u2192 \u211c+ and \u03b3v : Vd \u2192 \u211c+ are three functions defined to label frame, sentence and document vertices, while \u03b1e : E Vf \u2192 \u211c+, \u03b2e : E Vs \u2192 \u211c+ and \u03b3e : E\nVd \u2192 \u211c+ are functions for labeling frame, sentence and document edges.\nAdding new layers to the classical sentence similarity graph yields the richer multilayered graph model with the inter and intra sentence and the documental level relations that can be then used to enhance the existing PageRank algorithm. Figure 1. shows the conventional text graph model before and after applying the concepts of multilayered graph representation. One can easily show that the new graph model brings to light the following previously ignored information: sentence to sentence similarity can now be distinguished in two groups, one within a document and one across two documents; the document\u2019s significance can influence the sentence ratings; there is also a completely new kind of objects (i.e. semantic role labeler (SRL) frames) involved in representing the inner sentence relations.\nThe sentence edge function \u03b2e(si, sj) = simnormal(si, sj) = sim(si,sj)\u2211\nsk\u2208S\u2227k 6=i sim(si,sk)\nand document edge function \u03b3e(di, dj) = simnormal(di, dj) = sim(di,dj)\u2211\ndk\u2208D\u2227k 6=i sim(di,dk)\nare formulated as the normalized similarity between the two sentences si and sj , and the two documents di and dj , respectively. The SRL frame edge function \u03b1e(fi, fj) = simcomposite(fi, fj), is formulated as the composite similarity function of two frames fi and fj . Let N be the total number of frames in a documents set. The frame vertex function \u03b1v(fi) = assigns to frame vertices the value of 1/N or 1/2N , depending on their completeness, where incomplete frames have lower weight. The sentence \u03b2v(si) = centr norm(si) = \u2211 u\u2208si cw(u)\n\u2211 v\u2208S cw(v)\nand document vertex \u03b3v(di) = centr norm(di) = \u2211 u\u2208di cw(u)\n\u2211 v\u2208D cw(v) functions are de-\nfined by the normalized centroid-based weight of the sentence and document,\nrespectively where cw(u) denotes the centroid [8] weight of the word u. Our goal is to capture the similarity and redundancy between sentences, but at a lower structural and a higher semantic level. To accomplish this, we use the event-indexing model as the base for calculations of semantic similarity between frames of semantic role parser outputs, namely frames. Let us define the similarity measure for protagonist simprotagonist(fi, fj) = \u03b11 \u00b7 sim(A0i, A0j) + \u03b12 \u00b7 sim(A1i, A1j)+\u03b13 \u00b7sim(A2i, A2j)+\u03b14 \u00b7sim(A0i, A1j)+\u03b15 \u00b7sim(A0i, A2j)+\u03b16 \u00b7 sim(A1i, A2j); temporality simtemporality(fi, fj) = sim(Am Tmpi, Am Tmpj); spatiality simspatiality(fi, fj) = sim(Am Loci, Am Locj) and causality simcausality(fi, fj) = sim(Predicatei, P redicatej). In order to have the flexible weighting scheme we use coefficients \u03b11 = \u03b12 = \u03b13 = 0.25;\u03b14 = \u03b15 = 0.10;\u03b16 = 0.5. The compose similarity is defined as:\nsimcomposite(fi, fj) = ( \u03b21simprotagonist(fi, fj) + \u03b22simtemporality(fi, fj)\n+\u03b23simspatiality(fi, fj) + \u03b24simcausality(fi, fj) ) /#arguments\nwhere \u03b21 = 0.4;\u03b22 = 0.3;\u03b23 = 0.2;\u03b24 = 0.1. The values for coefficients are chosen based on the cognitive model which gives an emphasis in the decreasing order to indices."}, {"heading": "4 Multilayered graph-based ranking algorithm", "text": "In previous section the idea of multilayered text similarity graph is presented, based on it in this section we present a modified iterative graph-based sentence ranking algorithm.\nOur algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].\nIn the summary, PageRank method (in matrix notation) as described in the original paper [1] is\n\u03c0(k+1)T = \u03b1\u03c0(k)TH+ (\u03b1\u03c0(k)T a+ 1\u2212 \u03b1)vT\nwhere H is a very sparse, raw sub stochastic hyperlink matrix, \u03b1 is a scaling parameter between 0 and 1, \u03c0T is the stationary row vector of H called the PageRank vector, vT is a complete dense, rank-one teleportation matrix and a is a binary dangling node vector. In terms of the sentence ranking the matrix H is an adjacency matrix of similarities, vT is the affinity vector and the resulting \u03c0T is the frame ranking vector.\nFor sake of simplicity, we just assume there are two documents (e.g.D1, D2) and 4 sentences (e.g.S1,1, S1,2 in D1 and S2,1, S2,2 in D2) involved in ranking. Yet there are eight frames extracted from four sentences, let us show the document, sentence (just the first one) and frames (again just the first one) similarity matrices:\nH0 =\n[\nD1,1 D1,2 D2,1 D2,2\n]\nD1,1 =\n[\nS1,1 S1,2 S2,1 S2,2\n]\nS1,1 =\n[\nF1,1 F1,2 F2,1 F2,2\n]\nHD1,1 =\n[\nwS11S1,1 wS12S1,2 wS21S2,1 wS22S2,2\n]\nHS1,1 =\n[\nwF11F1,1 wF12F1,2 wF21F2,1 wF22F2,2\n]\nThe block matrix D1,1 refers to the similarity matrix of the sentences in document D1, while D1,2 represents the fold-document (D1 and D2) similarity matrix, and so on and so forth. Similarly the block matrix S1,1 in D1,1 denotes the similarity matrix of the frames in sentence S1, while S1,2 denotes the fold-sentence (S1 and S2) affinity matrix. H0 corresponds to the original sentence similarity matrix used in generic graph methods. The effective way of integrating the document and sentence dimension into the H0 is to highlight the document and sentence influences on the sentence and frame edges that connect different documents and sentences as illustrated in formulas for HD1,1 and HS1,1 .\nThe weight matrix Wd =\n[\nwS11 wS12 wS21 wS22\n]\nis used to distinguish the cross-document\nsentence edges and the weight matrix Ws =\n[\nwF11 wF12 wF21 wF22\n]\nis used to distinguish\nthe cross-sentence frame edges. The diagonal elements in Wd and Ws are set to 1 to neutralize the influence of the intra-document sentence and intra-sentence frame edges. Just on the opposite, the non-diagonal elements are weighted by the connections between the two corresponding documents and sentences. We define Wd as Wd(i, j) = 1 + \u03b3e(d(si), d(sj)), where d(si) presents the document that contains the sentence si. We also define Ws as Ws(i, j) = 1 + \u03b2e(s(fi), s(fj)), and s(fi) presents the sentence that contains the frame fi.\nBased on assumption that a frame from the document and the sentence with higher significance should be ranked higher we reflect the influence of the document and sentence dimensions on the affinity vector v. Consequently, the centroid-based weight of the document and the sentence are taken as the weights on the affinity vector v. See the following preference vectors:\nvo = [vd1vd2 ] T ; v =\n(\n[vd1vd2 ] \u00b7\n[\nwd1 wd2\n])T\n;\nvd1 =\n(\n[vs1vs2 ] \u00b7\n[\nws1 ws2\n])T\n; vd2 =\n(\n[vs3vs4 ] \u00b7\n[\nws3 ws4\n])T\n;\nHere vo represents the original preference vector, as used in LexRank, vs1 to vs4 denote the sub-preference vector of the frames from sentences s1 to s4, respectively; and vd1 ,vd2 denote the sub-preference vector of the sentences from doc-\numents d1 and d2, respectively. The weight matrices Wvd =\n[\nwd1 wd2\n]\n, Wvs1 = [\nws1 ws2\n]\nand Wvs2 =\n[\nws1 ws2\n]\nare specified to introduce the bias to sentences\nfrom different documents and the bias towards frames from different sentences, respectively. The weighting functions are defined as Wvd(i) = 1 + \u03b3v(d(si)), Wvs(i) = 1+ \u03b2v(s(fi)). To ensure the solution of proposed algorithm, we should first make v a affinity probability vector.\nLemma 1. v is a probability vector, if Wvs1 , Wvs2 and Wvd are positive and the diagonal elements in them sum to 1;\nProof. Since vs1 , vs2 , vs3 and vs4 are probability vectors of frames from four sentences we have |vs1 | = |vs2 | = |vs3 | = |vs4 | = 1. Then,\n|v| = wd1(ws1 |vs1 |+ ws2 |vs2 |) + wd2(ws3 |vs3 |+ ws4 |vs4 |)\n= wd1(ws1 + ws2) + wd2(ws3 + ws4) = 1\nif (ws1 + ws2) = 1 and (ws3 + ws4) = 1 hence wd1 + wd2 = 1\nAfterward, we should make the matrix H column stochastic and irreducible by forcing each of four block matrices of sentences and two matrices of documents to be column stochastic simply by normalizing them by columns. To make H irreducible, we make the sixteen block matrices in H irreducible by adding additional links between any two frames, which is also adapted in PageRank.\nLemma 2. H is column stochastic and irreducible.\nProof. H is column stochastic since the weight matrix W is column stochastic. Let A, B, C, and D, donate any of the 4 column block in H, then\n\u2211\ni\nHij = wd1k\n(\nws1k \u2211\ni\nAij + ws2k \u2211\ni\nBij\n)\n+wd2k\n(\nws3k \u2211\ni\nCij + ws4k \u2211\ni\nDij\n)\n(k = 1, . . . , 4)\n\u2211\ni\nHij = wd1k(ws1k + ws2k ) + wd2k(ws3k + ws4k )\nif (ws1 + ws2) = 1 and (ws3 + ws4) = 1 hence wd1 + wd2 = 1\nSince the four graphs corresponding to the four diagonal block matrices in H are strongly connected (i.e. they are irreducible) and the edges connecting the four graphs are bidirectional, the graph corresponding to H is obviously strongly connected. Thus H must be also irreducible.\nNotice that we must ensure Wvs1 > 0,Wvs2 > 0 and make the sum of the diagonal elements equal to 1 in order to ensure v to be a probability vector. And we must make H column stochastic by setting Wd > 0,Ws > 0 and both matrices column stochastic. Finally, we obtain that H is stochastic, irreducible and primitive, hence we can compute the unique dominant vector (with 1 as the eigenvalue) of H by using the power iteration method applied to H which converges to \u03c0. The previous explanation is given as example with a two-document, a four sentences (two of them in every document), and eight frames (two of frames in every single sentence). However, we can come to the same conclusion when the number of the documents, sentences and frames involved extends from given values to arbitrary number.\nWe can finally summarize the new ranking algorithm with following two functions.\nH(i, j) = \u03b1e(fi, fj)Wd(i, j)Ws(i, j);\nv = \u03b1v(fi)(1 + \u03b3v(d(si)))(1 + \u03b2v(s(fi))).\nSo far the document and sentence dimensions have been integrated into the PageRank-like algorithms for frame ranking with a solid mathematical foundation."}, {"heading": "5 Evaluation", "text": "The DUC1 2004 data set from DUC was tested to analyze the efficiency of the proposed summarization method. The Task 2 at the DUC 2004 is to generate a short summary (665 bytes) of an input set of topic-related news articles. The\ntotal number of document groups is 50, with each group containing 10 articles on average. For each group, four NIST assessors were asked to create a brief summary. Machine-generated summaries are evaluated using ROUGE [5] automatic n-gram matching which measures performance based on the number of co-occurrences between machine-generated and ideal summaries in different word units. The 1-gram ROUGE score (a.k.a.ROUGE-1) has been found to correlate very well with human judgements at a confidence level of 95%, based on various statistical metrics. Even though in this version of method we did not consider sentence positions or other summary quality improvement techniques such as sentence reduction, its overall performance is promising, please see table 1. The use of multilayered model in summarization can make considerable\n1 Document Understanding Conference(http://duc.nist.gov)\nimprovements even though the results presented here do not report a significant difference."}, {"heading": "6 Conclusion and future work", "text": "We have presented a multilayered graph model and a ranking algorithm, for generic MDS. The main contributions of our work is introducing the concept of 3-layered graph model. The results of applying this model on extractive summarization are quite promising. There is work still left to be done, however. We are now working on further improvements of the model, and it\u2019s adaptation to other summarization tasks, such as the query and update summarization.\nAcknowledgments. We would like to thank the anonymous reviewers for their constructive comments."}], "references": [{"title": "The anatomy of a large-scale hypertextual web search engine", "author": ["Sergey Brin", "Lawrence Page"], "venue": "Computer Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Introduction to the conll-2004 shared task: Semantic role labeling", "author": ["Xavier Carreras", "Lluis Marque"], "venue": "In CoNLL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G\u00fcnes Erkan", "Dragomir R. Radev"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["Jon M. Kleinberg"], "venue": "J. ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "author": ["Chin-Yew Lin", "Eduard H. Hovy"], "venue": "In HLT-NAACL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Textrank: Bringing order into text", "author": ["Rada Mihalcea", "Paul Tarau"], "venue": "In EMNLP, pages 404\u2013411,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Biased lexrank: Passage retrieval using random walks with question-based priors", "author": ["Jahna Otterbacher", "G\u00fcnes Erkan", "Dragomir R. Radev"], "venue": "Inf. Process. Manage.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Centroidbased summarization of multiple documents", "author": ["Dragomir R. Radev", "Hongyan Jing", "Magorzata Sty", "Daniel Tam"], "venue": "Inf. Process. Manage.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Document-based hits model for multi-document summarization", "author": ["Xiaojun Wan"], "venue": "PRICAI, volume 5351 of Lecture Notes in Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A document-sensitive graph model for multi-document summarization", "author": ["Furu Wei", "Wenjie Li", "Qin Lu", "Yanxiang He"], "venue": "Knowl. Inf. Syst.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "The construction of situation models in narrative comprehension: an event-indexing model", "author": ["R.A. Zwaan", "M.C. Langston", "A.C. Graesser"], "venue": "Psychological Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}], "referenceMentions": [{"referenceID": 10, "context": "For this challenge we consider using the psychology cognitive situation model, namely the Event-Indexing model [11].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "parser\u2019s [2] output can be mapped to the above proposed cognitive model.", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "2 Related work The graph-based models have been developed by the extractive document summarization community in the past years [3,6].", "startOffset": 127, "endOffset": 132}, {"referenceID": 5, "context": "2 Related work The graph-based models have been developed by the extractive document summarization community in the past years [3,6].", "startOffset": 127, "endOffset": 132}, {"referenceID": 0, "context": "The significance of a node in a graph is estimated by graph-based ranking algorithms, such as PageRank [1] or HITS [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "The significance of a node in a graph is estimated by graph-based ranking algorithms, such as PageRank [1] or HITS [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "An algorithm called LexRank [3], adapted from PageRank, was applied to calculate sentence significance, which was then used as the criterion to rank and select summary sentences.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Meanwhile, Mihalcea and Tarau [6] presented their PageRank variation, called TextRank, in the same year.", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "respectively where cw(u) denotes the centroid [8] weight of the word u.", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 150, "endOffset": 155}, {"referenceID": 5, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 150, "endOffset": 155}, {"referenceID": 6, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 188, "endOffset": 196}, {"referenceID": 9, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 188, "endOffset": 196}, {"referenceID": 8, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 188, "endOffset": 196}, {"referenceID": 0, "context": "In the summary, PageRank method (in matrix notation) as described in the original paper [1] is", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "Machine-generated summaries are evaluated using ROUGE [5] automatic n-gram matching which measures performance based on the number of co-occurrences between machine-generated and ideal summaries in different word units.", "startOffset": 54, "endOffset": 57}], "year": 2014, "abstractText": "Multi-document summarization is a process of automatic generation of a compressed version of the given collection of documents. Recently, the graph-based models and ranking algorithms have been actively investigated by the extractive document summarization community. While most work to date focuses on homogeneous connecteness of sentences and heterogeneous connecteness of documents and sentences (e.g. sentence similarity weighted by document importance), in this paper we present a novel 3-layered graph model that emphasizes not only sentence and document level relations but also the influence of under sentence level relations (e.g. a part of sentence similarity).", "creator": "LaTeX with hyperref package"}}}