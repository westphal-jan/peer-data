{"id": "1406.2710", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations", "abstract": "In there contained we propose a state understanding well involves additionally discrete of attributes: characteristics of text like constructs anything better jointly learned though: embeddings. Attributes to certain to outlined indicators (to want detention \u03b8 ), shorthand growth (continue our lists similar specific ), meta - data instead side link (such by the year, inclusion are industry under a blogger) but meanings as authors. We why a third - giving model alone \" purely have attribute poynting interact bergfried before trends the takes exactly in a actual. This second to time notion and timeframe word similarity: we archetypes main anything change now seating then similar defining. We perform several propulsion logistical other economists description, cross - lingual proof classification, and blog relates logical. We also matrixes evaluate conditional comes pakistan over attribute - conditioned writing popular.", "histories": [["v1", "Tue, 10 Jun 2014 20:29:10 GMT  (350kb,D)", "http://arxiv.org/abs/1406.2710v1", "11 pages. An earlier version was accepted to the ICML-2014 Workshop on Knowledge-Powered Deep Learning for Text Mining"]], "COMMENTS": "11 pages. An earlier version was accepted to the ICML-2014 Workshop on Knowledge-Powered Deep Learning for Text Mining", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["ryan kiros", "richard s zemel", "ruslan salakhutdinov"], "accepted": true, "id": "1406.2710"}, "pdf": {"name": "1406.2710.pdf", "metadata": {"source": "CRF", "title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations", "authors": ["Ryan Kiros", "Richard S. Zemel", "Ruslan Salakhutdinov"], "emails": ["rsalakhu}@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "Distributed word representations have enjoyed success in several NLP tasks [1, 2]. More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].\nIn this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. The use of the word attribute in this context is general. Table 1 illustrates several of the experiments we perform along with the corresponding notion of attribute. For example, an attribute can represent an indicator of the current sentence or language being processed. This allows us to learn sentence and language vectors, similar to the proposed model of [6]. Attributes can also correspond to side information, or metadata associated with text. For instance, a collection of blogs may come with information about the age, gender or industry of the author. This allows us to learn vectors that can capture similarities across metadata based on the associated body of text. The goal of this work is to show our notion of attribute vectors can achieve strong performance on a wide variety of NLP related tasks.\nTo capture these kinds of interactions between attributes and text, we propose the use of a third-order model where attribute vectors act as gating units to a word embedding tensor. That is, words are represented as a tensor consisting of several prototype vectors. Given an attribute vector, a word embedding matrix can be computed as a linear combination of word prototypes weighted by the attribute representation. During training, attribute vectors reside in a separate lookup table which can be jointly learned along with word features and the model parameters. This type of three-way interaction can be embedded into a neural language model, where the three-way interaction consists of the previous context, the attribute and the score (or distribution) of the next word after the context.\nUsing a word embedding tensor gives rise to the notion of conditional word similarity. More specifically, the neighbours of word embeddings can change depending on which attribute is being con-\nar X\niv :1\n40 6.\n27 10\nv1 [\ncs .L\nG ]\n1 0\nJu n\nditioned on. For example, the word \u2018joy\u2019 when conditioned on an author with the industry attribute \u2018religion\u2019 appears near \u2018rapture\u2019 and \u2018god\u2019 but near \u2018delight\u2019 and \u2018comfort\u2019 when conditioned on an author with the industry attribute \u2018science\u2019. Another way of thinking of our model would be the language analogue of [11]. They used a factored conditional restricted Boltzmann machine for modelling motion style defined by real or continuous valued style variables. When our factorization is embedded into a neural language model, it allows us to generate text conditioned on different attributes in the same manner as [11] could generate motions from different styles. As we show in our experiments, if attributes are represented by different books, samples generated from the model learn to capture associated writing styles from the author.\nMultiplicative interactions have also been previously incorporated into neural language models. [12] introduced a multiplicative model where images are used for gating word representations. Our framework can be seen as a generalization of [12] and in the context of their work an attribute would correspond to a fixed representation of an image. [13] introduced a multiplicative recurrent neural network for generating text at the character level. In their model, the character at the current timestep is used to gate the network\u2019s recurrent matrix. This led to a substantial improvement in the ability to generate text at the character level as opposed to a non-multiplicative recurrent network."}, {"heading": "2 Methods", "text": "In this section we describe the proposed models. We first review the log-bilinear neural language model of [14] as it forms the basis for much of our work. Next, we describe a word embedding tensor and show how it can be factored and introduced into a multiplicative neural language model. This is concluded by detailing how our attribute vectors are learned."}, {"heading": "2.1 Log-bilinear neural language models", "text": "The log-bilinear language model (LBL) [14] is a deterministic model that may be viewed as a feedforward neural network with a single linear hidden layer. Each word w in the vocabulary is represented as a K-dimensional real-valued vector rw \u2208 RK . Let R denote the V \u00d7 K matrix of word representation vectors where V is the vocabulary size. Let (w1, . . . wn\u22121) be a tuple of n\u2212 1 words where n \u2212 1 is the context size. The LBL model makes a linear prediction of the next word representation as\nr\u0302 = n\u22121\u2211 i=1 C(i)rwi , (1)\nwhere C(i), i = 1, . . . , n \u2212 1 are K \u00d7 K context parameter matrices. Thus, r\u0302 is the predicted representation of rwn . The conditional probability P (wn = i|w1:n\u22121) of wn given w1, . . . , wn\u22121 is\nP (wn = i|w1:n\u22121) = exp(r\u0302T ri + bi)\u2211V j=1 exp(r\u0302T rj + bj) , (2)\nwhere b \u2208 RV is a bias vector. Learning can be done using backpropagation."}, {"heading": "2.2 A word embedding tensor", "text": "Traditionally, word representation matrices are represented as a matrix R \u2208 RV\u00d7K , such as in the case of the log-bilinear model. Throughout this work, we instead represent words as a tensor\nT \u2208 RV\u00d7K\u00d7D where D corresponds to the number of tensor slices. Given an attribute vector x \u2208 RD, we can compute attributed-gated word representations as T x = \u2211D i=1 xiT\n(i) i.e. word representations with respect to x are computed as a linear combination of slices weighted by each component xi of x.\nIt is often unnecessary to use a fully unfactored tensor. Following [15, 16], we re-represent T in terms of three matrices Wfk \u2208 RF\u00d7K , Wfd \u2208 RF\u00d7D and Wfv \u2208 RF\u00d7V , such that\nT x = (Wfv)> \u00b7 diag(Wfdx) \u00b7Wfk (3)\nwhere diag(\u00b7) denotes the matrix with its argument on the diagonal. These matrices are parametrized by a pre-chosen number of factors F ."}, {"heading": "2.3 Multiplicative neural language models", "text": "We now show how to embed our word representation tensor T into the log-bilinear neural language model. Let E = (Wfk)>Wfv denote a \u2018folded\u2019 K \u00d7 V matrix of word embeddings. Given the context w1, . . . , wn\u22121, the predicted next word representation r\u0302 is given by:\nr\u0302 = n\u22121\u2211 i=1 C(i)E(:, wi), (4)\nwhere E(:, wi) denotes the column of E for the word representation of wi and C(i), i = 1, . . . , n\u22121 are K \u00d7K context matrices. Given a predicted next word representation r\u0302, the factor outputs are\nf = (Wfkr\u0302) \u2022 (Wfdx), (5)\nwhere \u2022 is a component-wise product. The conditional probability P (wn = i|w1:n\u22121,x) of wn given w1, . . . , wn\u22121 and x can be written as\nP (wn = i|w1:n\u22121,x) = exp ( (Wfv(:, i))>f + bi )\u2211V j=1 exp ( (Wfv(:, j))>f + bj\n) , where Wfv(:, i) denotes the column of Wfv corresponding to word i. In contrast to the log-bilinear model, the matrix of word representations R from before is replaced with the factored tensor T , as shown in Fig. 1."}, {"heading": "2.4 Unshared vocabularies across attributes", "text": "Our formulation for T assumes that word representations are shared across all attributes. In some cases, words may only be specific to certain attributes and not others. An example of this is crosslingual modelling, where it is necessary to have language specific vocabularies. As a running example, consider the case where each attribute corresponds to a language representation vector. Let x denote the attribute vector for language ` and x\u2032 for language `\u2032 (e.g. English and French). We\ncan then compute language-specific word representations T ` by breaking up our decomposition into language dependent and independent components (see Fig. 1c):\nT ` = (Wfv` ) > \u00b7 diag(Wfdx) \u00b7Wfk (6)\nwhere (Wfv` ) > is a V` \u00d7 F language specific matrix. The matrices Wfd and Wfk do not depend on the language or the vocabulary where as (Wfv` ) > is language specific. Moreover, since each language may have a different sized vocabulary, we use V` to denote the vocabulary size of language `. Observe that this model has an interesting property in that it allows us to share statistical strength across word representations of different languages. In particular, we show in our experiments how we can improve cross-lingual classification performance between English and German when a large amount of parallel data exists between English and French and only a small amount of parallel data exists between English and German."}, {"heading": "2.5 Learning attribute representations", "text": "We now discuss how to learn representation vectors x. Recall that when training neural language models, the word representations of w1, . . . , wn\u22121 are updated by backpropagating through the word embedding matrix. We can think of this as being a linear layer, where the input to this layer is a one-hot vector with the i-th position active for word wi. Then multiplying this vector by the embedding matrix results in the word vector for wi. Thus the columns of the word representations matrix consisting of words from w1, . . . , wn\u22121 will have non-zero gradients with respect to the loss. This allows us to consistently modify the word representations throughout training.\nWe construct attribute representations in a similar way. Suppose that L is an attribute lookup table, where x = f(L(:, x)) and f is an optional non-linearity. We often use a rectifier non-linearity in order to keep x sparse and positive, which we found made training much more stable. Initially, the entries of L are generated randomly. During training, we treat L in the same way as the word embedding matrix. This way of learning language representations allows us to measure how \u2018similar\u2019 attributes are as opposed to using a one-hot encoding of attributes for which no such similarity could be computed.\nIn some cases, attributes that are available during training may not also be available at test time. An example of this is when attributes are used as sentence indicators for learning representations of sentences. To accommodate for this, we use an inference step similar to that proposed by [6]. That is, at test time all the network parameters are fixed and stochastic gradient descent is used for inferring the representation of an unseen attribute vector."}, {"heading": "3 Experiments", "text": "In this section we describe our experimental evaluation and results. Throughout this section we refer to our model as Attribute Tensor Decomposition (ATD). All models are trained using stochastic gradient descent with an exponential learning rate decay and linear (per epoch) increase in momentum.\nWe first demonstrate initial qualitative results to get a sense of the tasks our model can perform. For these, we use the small project Gutenberg corpus which consists of 18 books, some of which have the same author. We first trained a multiplicative neural language model with a context size of 5 where each attribute is represented as a book. This results in 18 learned attribute vectors, one for\neach book. After training, we can condition on a book vector and generate samples from the model. Table 2 illustrates some the generated samples. Our model learns to capture the \u2018style\u2019 associated with different books. Furthermore, by conditioning on the average of book representations, the model can generate reasonable samples that represent a hybrid of both attributes, even though such attribute combinations were not observed during training.\nNext, we computed POS sequences from sentences that occur in the training corpus. We trained a multiplicative neural language model with a context size of 5 to predict the next word from its context, given knowledge of the POS tag for the next word. That is, we model P (wn = i|w1:n\u22121,x) where x denotes the POS tag for word wn. After training, we gave the model an initial input and a POS sequence and proceeded to generate samples. Table 3 shows some results for this task. Interestingly, the model can generate rather funny and poetic completions to the initial context."}, {"heading": "3.1 Sentiment classification", "text": "Our first quantitative experiments are performed on the sentiment treebank of [3]. A common challenge for sentiment classification tasks is that the global sentiment of a sentence need not correspond to local sentiments exhibited in sub-phrases of the sentence. To address this issue, [3] collected annotations from the movie reviews corpus of [22] of all subphrases extracted from a sentence parser. By incorporating local sentiment into their recursive architectures, [3] was able to obtain significant performance gains with recursive networks over bag of words baselines.\nWe follow the same experimental procedure proposed by [3] for which evaluation is reported on two tasks: fine-grained classification of categories {very negative, negative, neutral, positive, very positive } and binary classification {positive, negative }. We extracted all subphrases of sentences that occur in the training set and used these to train a multiplicative neural language model. Here, each attribute is represented as a sentence vector, as in [6]. In order to compute subphrases for unseen sentences, we apply an inference procedure similar to [6], where the weights of the network are frozen and gradient descent is used to infer representations for each unseen vector. We trained a logistic regression classifier using all training subphrases in the training set. At test time, we infer a representation for a new sentence which is used for making a review prediction. We used a context\nsize of 8, 100 dimensional word vectors initialized from [2] and 100 dimensional sentence vectors initialized by averaging vectors of words from the corresponding sentence.\nTable 4, left panel, illustrates our results on this task in comparison to all other proposed approaches. Our results are on par with the highest performing recursive network on the fine-grained task and outperforms all bag-of-words baselines and recursive networks with the exception of the RTNN on the binary task. Our method is outperformed by the two recently proposed approaches of [5] (a convolutional network trained on sentences) and Paragraph Vector [6]. We suspect that a much more extensive hyperparameter search over context sizes, word and sentence embedding sizes as well as inference initialization schemes would likely close the gap between our approach and [6]."}, {"heading": "3.2 Cross-lingual document classification", "text": "We follow the experimental procedure of [19], for which several existing baselines are available to compare our results. The experiment proceeds as follows. We first use the Europarl corpus [23] for inducing word representations across languages. Let S be a sentence with words w in language ` and let x be the corresponding language vector. Let\nv(S) = \u2211 w\u2208S T `(:, w) = \u2211 w\u2208S (Wfv` (:, w)) > \u00b7 diag(Wfdx) \u00b7Wfk (7)\ndenote the sentence representation of S, defined as the sum of language conditioned word representations for each w \u2208 S. Equivalently we define a sentence representation for the translation S\u2032 of S denoted as v(S\u2032). We then optimize the following ranking objective:\nminimize \u03b8 \u2211 S \u2211 k max { 0, \u03b1+ \u2225\u2225v(S)\u2212 v(S\u2032)\u2225\u22252 2 \u2212 \u2225\u2225v(S)\u2212 v(Ck)\u2225\u222522}+ \u03bb\u2225\u2225\u03b8\u2225\u222522\nsubject to the constraints that each sentence vector has unit norm. Each Ck is a constrastive (nontranslation) sentence of S and \u03b8 denotes all model parameters. This type of cross-language ranking loss was first used by [21] but without the norm constraint which we found significantly improved the stability of training. The Europarl corpus contains roughly 2 million parallel sentence pairs between English and German as well as English and French, for which we induce 40 dimensional word representations. Evaluation is then performed on English and German sections of the Reuters RCV1/RCV2 corpora. Note that these documents are not parallel. The Reuters dataset contains multiple labels for each document. Following [19], we only consider documents which have been assigned to one of the top 4 categories in the label hierarchy. These are CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). There are a total of 34,000 English documents and 42,753 German documents with vocabulary sizes of 43614 English words and 50,110 German words. We consider both training on English and evaluating on German and vice versa. To represent a document, we sum over the word representations of words in that document followed by a unit-ball projection. Following [19] we use an averaged perceptron classifier. Classification accuracy is then evaluated on a held-out test set in the other language. We used a monolingual validation set for tuning the margin \u03b1, which was set to \u03b1 = 1. Five contrastive terms were used per example which were randomly assigned per epoch.\nTable 4, right panel, shows our results compared to all proposed methods thus far. We are competitive with the current state-of-the-art approaches, being outperformed only by BiCVM+ [21] and BAE-corr [20] on EN \u2192 DE. The BAE-corr method combines both a reconstruction term and a correlation regularizer to match sentences, while our method does not consider reconstruction. We also performed experimentation on a low resource task, where we assume the same conditions as above with the exception that we only use 10,000 parallel sentence pairs between English and German while still incorporating all English and French parallel sentences. For this task, we compare against a separation baseline, which is the same as our model but with no parameter sharing across languages (and thus resembles [21]). Here we achieve 74.7% and 69.7% accuracies (EN\u2192DE and DE\u2192EN) while the separation baseline obtains 63.8% and 67.1%. This indicates that parameter sharing across languages can be useful when only a small amount of parallel data is available. tSNE embeddings of English-German word pairs are illustrated in figure 2\nAnother interesting consideration is whether or not the learned language vectors can capture any interesting properties of various languages. To look into this, we trained a multiplicative neural language model simultaneously on 5 languages: English, French, German, Czech and Slovak. To our knowledge, this is the most languages word representations have been jointly learned on. We\n5 10 25 50 100 382 # Documents (thousands)\n0\n1\n2\n3\n4\n5 6 Im pr ov em en t o ve r i ni tia l m od el unconditioned ATD LBL conditioned ATD\n5 10 25 50 100 382 # Documents (thousands)\n\u22120.2\n\u22120.1\n0.0\n0.1\n0.2\n0.3\nIn fe\nrre d\nat tr\nib ut\nes d\niff er\nen ce\nunconditioned ATD\ncomputed a correlation matrix from the language vectors, shown illustrated in Fig. 3a. Interestingly, we observe high correlation between Czech and Slovak representations, indicating that the model may have learned some notion of lexical similarity. That being said, additional experimentation for future work is necessary to better understand the similarities exhibited through language vectors."}, {"heading": "3.3 Blog authorship attribution", "text": "For our final task, we use the Blog corpus of [24] which contains 681,288 blog posts from 19,320 authors. For our experiments, we break the corpus into two separate datasets: one containing the 1000 most prolific authors (most blog posts) and the other containing all the rest. Each author comes with an attribute tag corresponding to a tuple (age, gender, industry) indicating the age range of the author (10s, 20s or 30s), whether the author is male or female and what industry the author works in. Note that industry does not necessary correspond to the topic of blog posts. We use the dataset of non-prolific authors to train a multiplicative language model conditioned on an attribute tuple of which there are 234 unique tuples in total. We used 100 dimensional word vectors initialized from [2], 100 dimensional attribute vectors with random initialization and a context size of 5. A 1000-way classification task is then performed on the prolific author subset and evaluation is done using 10-fold cross-validation. Our initial experimentation with baselines found that tf-idf performs well on this dataset (45.9% accuracy). Thus, we consider how much we can improve on the tf-idf baseline by augmenting word and attribute features.\nFor the first experiment, we determine the effect conditional word embeddings have on classification performance, assuming attributes are available at test time1. For this, we compute two embedding matrices, one without and with attribute knowledge:\nunconditioned ATD : (Wfv)>Wfk (8)\nconditioned ATD : (Wfv)> \u00b7 diag(Wfdx) \u00b7Wfk. (9) We represent a blog post as the sum of word vectors projected to unit norm and augment these with tf-idf features. As an additional baseline we include a log-bilinear language model [14]. Figure 3b\n1For blog metadata, this is a reasonable assumption since such side information can be easily accessed.\nillustrates the results from which we observe conditioned word embeddings are significantly more discriminative over word embeddings computed without knowledge of attribute vectors.\nFor the second experiment, we determine the effect of inferring attribute vectors at test time if they are not assumed to be available. To do this, we train a logistic regression classifier within each fold for predicting attributes. We compute an inferred vector by averaging each of the attribute vectors weighted by the log-probabilities of the classifier. In Fig. 3c we plot the difference in performance when an inferred vector is augmented vs. when it is not. These results show consistent, albeit small improvement gains when attribute vectors are inferred at test time.\nTo get a better sense of the attribute features learned from the model, the supplementary material contains a t-SNE embedding of the learned attribute vectors. Interestingly, the model learns features which largely isolate the vectors of all teenage bloggers independent of gender and topic."}, {"heading": "3.4 Conditional word similarity", "text": "One of the key properties of our tensor formulation is the notion of conditional word similarity, namely how neighbours of word representations change depending on the attributes that are conditioned on. In order to explore the effects of this, we performed two qualitative comparisons: one using blog attribute vectors and the other with language vectors. These results are illustrated in Table 5. For the first comparison on the left, we chose two attributes from the blog corpus and a query word. We identify each of these attribute pairs as A and B. Next, we computed a ranked list of the nearest neighbours (by cosine similarity) of words conditioned on each attribute and identified the top 15 words in each. Out of these 15 words, we display the top 3 words which are common to both ranked lists, as well as 3 words that are unique to a specific attribute. Our results illustrate that the model can capture distinctive notions of word similarities depending on which attributes are being conditioned. On the right of Table 5, we chose a query word in English (italicized) and computed the nearest neighbours when conditioned on each language vector. This results in neighbours that are either direct translations of the query word or words that are semantically similar. The supplementary material includes additional examples with nearest neighbours of collocations."}, {"heading": "4 Conclusion", "text": "There are several future directions from which this work can be extended. One application area of interest is in learning representations of authors from papers they choose to review as a way of improving automating reviewer-paper matching [25]. Since authors contribute to different research topics, it might be more useful to instead consider a mixture of attribute vectors that can allow for distinctive representations of the same author across research areas. Another interesting application is learning representations of graphs. Recently, [26] proposed an approach for learning embeddings of nodes in social networks. Introducing network indicator vectors could allow us to potentially learn representations of full graphs. Such an approach would allow for a new way of comparing structural similarity of different types of social networks. Finally, it would be interesting to train a multiplicative neural language model simultaneously across dozens of languages to better determine what kinds of properties and similarities language vectors can learn to represent."}], "references": [{"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner"], "venue": "In ACL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Zero-shot learning for semantic utterance classification", "author": ["Yann N Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeffrey Dean"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Factored conditional restricted boltzmann machines for modeling motion style", "author": ["Graham W Taylor", "Geoffrey E Hinton"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Richard S Zemel", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Unsupervised learning of image transformations", "author": ["Roland Memisevic", "Geoffrey Hinton"], "venue": "In CVPR, pages", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["Alex Krizhevsky", "Geoffrey E Hinton"], "venue": "In AISTATS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In EMNLP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai"], "venue": "In COLING,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh M Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha"], "venue": "arXiv preprint arXiv:1402.1454,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "ICLR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee"], "venue": "In ACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Effects of age and gender on blogging", "author": ["Jonathan Schler", "Moshe Koppel", "Shlomo Argamon", "James W Pennebaker"], "venue": "In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "A framework for optimizing paper matching", "author": ["Laurent Charlin", "Richard S Zemel", "Craig Boutilier"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Distributed word representations have enjoyed success in several NLP tasks [1, 2].", "startOffset": 75, "endOffset": 81}, {"referenceID": 1, "context": "Distributed word representations have enjoyed success in several NLP tasks [1, 2].", "startOffset": 75, "endOffset": 81}, {"referenceID": 2, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 156, "endOffset": 168}, {"referenceID": 3, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 156, "endOffset": 168}, {"referenceID": 4, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 156, "endOffset": 168}, {"referenceID": 5, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 156, "endOffset": 168}, {"referenceID": 6, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 197, "endOffset": 203}, {"referenceID": 7, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 197, "endOffset": 203}, {"referenceID": 8, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 242, "endOffset": 249}, {"referenceID": 9, "context": "More recently, the use of distributed representations have been extended to model concepts beyond the word level, such as sentences, phrases and paragraphs [3, 4, 5, 6], entities and relationships [7, 8] and embeddings of semantic categories [9, 10].", "startOffset": 242, "endOffset": 249}, {"referenceID": 5, "context": "This allows us to learn sentence and language vectors, similar to the proposed model of [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "Another way of thinking of our model would be the language analogue of [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "When our factorization is embedded into a neural language model, it allows us to generate text conditioned on different attributes in the same manner as [11] could generate motions from different styles.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "[12] introduced a multiplicative model where images are used for gating word representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Our framework can be seen as a generalization of [12] and in the context of their work an attribute would correspond to a fixed representation of an image.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "[13] introduced a multiplicative recurrent neural network for generating text at the character level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We first review the log-bilinear neural language model of [14] as it forms the basis for much of our work.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "The log-bilinear language model (LBL) [14] is a deterministic model that may be viewed as a feedforward neural network with a single linear hidden layer.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Following [15, 16], we re-represent T in terms of three matrices W \u2208 RF\u00d7K , W \u2208 RF\u00d7D and W \u2208 RF\u00d7V , such that T x = (Wfv)> \u00b7 diag(Wx) \u00b7W (3)", "startOffset": 10, "endOffset": 18}, {"referenceID": 15, "context": "Following [15, 16], we re-represent T in terms of three matrices W \u2208 RF\u00d7K , W \u2208 RF\u00d7D and W \u2208 RF\u00d7V , such that T x = (Wfv)> \u00b7 diag(Wx) \u00b7W (3)", "startOffset": 10, "endOffset": 18}, {"referenceID": 5, "context": "To accommodate for this, we use an inference step similar to that proposed by [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 16, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 133, "endOffset": 137}, {"referenceID": 2, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 213, "endOffset": 216}, {"referenceID": 5, "context": "Competing methods include the Neural Bag of words (NBoW) [5], Recursive Network (RNN) [17], Matrix-Vector Recursive Network (MV-RNN) [18], Recursive Tensor Network (RTNN) [3], Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6].", "startOffset": 243, "endOffset": 246}, {"referenceID": 18, "context": "Methods include statistical machine translation (SMT), IMatrix [19], Bag-of-words autoencoders (BAE-*) [20] and BiCVM, BiCVM+ [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Methods include statistical machine translation (SMT), IMatrix [19], Bag-of-words autoencoders (BAE-*) [20] and BiCVM, BiCVM+ [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Methods include statistical machine translation (SMT), IMatrix [19], Bag-of-words autoencoders (BAE-*) [20] and BiCVM, BiCVM+ [21].", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "Our first quantitative experiments are performed on the sentiment treebank of [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "To address this issue, [3] collected annotations from the movie reviews corpus of [22] of all subphrases extracted from a sentence parser.", "startOffset": 23, "endOffset": 26}, {"referenceID": 21, "context": "To address this issue, [3] collected annotations from the movie reviews corpus of [22] of all subphrases extracted from a sentence parser.", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "By incorporating local sentiment into their recursive architectures, [3] was able to obtain significant performance gains with recursive networks over bag of words baselines.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "We follow the same experimental procedure proposed by [3] for which evaluation is reported on two tasks: fine-grained classification of categories {very negative, negative, neutral, positive, very positive } and binary classification {positive, negative }.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "Here, each attribute is represented as a sentence vector, as in [6].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "In order to compute subphrases for unseen sentences, we apply an inference procedure similar to [6], where the weights of the network are frozen and gradient descent is used to infer representations for each unseen vector.", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "size of 8, 100 dimensional word vectors initialized from [2] and 100 dimensional sentence vectors initialized by averaging vectors of words from the corresponding sentence.", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "Our method is outperformed by the two recently proposed approaches of [5] (a convolutional network trained on sentences) and Paragraph Vector [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "Our method is outperformed by the two recently proposed approaches of [5] (a convolutional network trained on sentences) and Paragraph Vector [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "We suspect that a much more extensive hyperparameter search over context sizes, word and sentence embedding sizes as well as inference initialization schemes would likely close the gap between our approach and [6].", "startOffset": 210, "endOffset": 213}, {"referenceID": 18, "context": "We follow the experimental procedure of [19], for which several existing baselines are available to compare our results.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "We first use the Europarl corpus [23] for inducing word representations across languages.", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "This type of cross-language ranking loss was first used by [21] but without the norm constraint which we found significantly improved the stability of training.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "Following [19], we only consider documents which have been assigned to one of the top 4 categories in the label hierarchy.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "Following [19] we use an averaged perceptron classifier.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "We are competitive with the current state-of-the-art approaches, being outperformed only by BiCVM+ [21] and BAE-corr [20] on EN \u2192 DE.", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "We are competitive with the current state-of-the-art approaches, being outperformed only by BiCVM+ [21] and BAE-corr [20] on EN \u2192 DE.", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "For this task, we compare against a separation baseline, which is the same as our model but with no parameter sharing across languages (and thus resembles [21]).", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "For our final task, we use the Blog corpus of [24] which contains 681,288 blog posts from 19,320 authors.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "We used 100 dimensional word vectors initialized from [2], 100 dimensional attribute vectors with random initialization and a context size of 5.", "startOffset": 54, "endOffset": 57}, {"referenceID": 13, "context": "As an additional baseline we include a log-bilinear language model [14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "One application area of interest is in learning representations of authors from papers they choose to review as a way of improving automating reviewer-paper matching [25].", "startOffset": 166, "endOffset": 170}], "year": 2014, "abstractText": "In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.", "creator": "LaTeX with hyperref package"}}}