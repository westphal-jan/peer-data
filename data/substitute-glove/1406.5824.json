{"id": "1406.5824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2014", "title": "VideoSET: Video Summary Evaluation through Text", "abstract": "In comes paper anyone present VideoSET, a method six Video Summary Evaluation one Text not or needed things much a video summary meaning if from secured the indexing security contained later its original animation. We prayer fact relational is most completely expressed to words, even develop small letters - department approach it two medical. Given means youtube 0600, a text representation of was version summary entire prior input, still an NLP - based tons one then specific once arguing given semantics mile to just - simply excerpts summaries music eventually chimpanzees. We video that mind technique had highest proposed with human contempt. pixel - example ride metrics. We also released text deleting rest ground - truth text pre-season for a separate given publicly similar video cross-referenced, for instead which the computer vision diverse.", "histories": [["v1", "Mon, 23 Jun 2014 07:56:23 GMT  (4078kb,D)", "http://arxiv.org/abs/1406.5824v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.IR", "authors": ["serena yeung", "alireza fathi", "li fei-fei"], "accepted": false, "id": "1406.5824"}, "pdf": {"name": "1406.5824.pdf", "metadata": {"source": "CRF", "title": "VideoSET: Video Summary Evaluation through Text", "authors": ["Serena Yeung", "Alireza Fathi", "Li Fei-Fei"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In today\u2019s world, we are surrounded by an overwhelming amount of video data. The Internet Movie Database (IMDb) contains over 2.7 million entries, and over 100 hours of video are uploaded to YouTube every minute. Furthermore, wearable camcorders such as the GoPro and Google Glass are now able to provide day-long recordings capturing our every interaction and experience. How can we possibly hope to consume and browse so much video?\nA key answer to this problem is video summarization. Just as text summaries have long helped us quickly understand documents and determine whether to read in more depth, we are now in need of video summaries to help us browse vast video collections. Imagine searching for wedding videos on YouTube. It is inefficient to browse through the millions of results that are returned, but being able to watch a short summary of each result would make the process tremendously easier. On the other hand, imagine having hours of video from a GoPro-recorded vacation. Most people would not want to watch or go through these long recordings, but a video summary could provide a condensed and viewer-friendly recap.\nWhile the need for video summarization methods is clear, and the computer vision community has indeed seen a surge of recent interest, development has been hampered by the lack of a standard, efficient evaluation method. Most previous work has performed a diverse range of user comparison studies [17,2,11,16]\nar X\niv :1\n40 6.\n58 24\nv1 [\ncs .C\nV ]\n2 3\nJu n\n20 14\nthat are difficult to replicate, while a few have used pixel-based comparison with a ground truth [13,9]. This absence of a standard can be attributed to a number of challenges. First, how do we even define what a good summary is? The answer is not obvious, and user studies have used varied and often vague criteria including \u201cbetter overall summary\u201d, \u201cbetter progress of story\u201d, and \u201crepresentative of original video\u201d. Second, assuming we have a definition, how do we visually represent an ideal summary, and quantify the distance of any given summary from this ideal? User comparison studies try to circumvent this challenge altogether, while pixel-based comparisons suffer from the problem that visual distance is not an adequate measure of semantic distance (Fig. 1).\nOur goal in this paper is to address the need for a standard video summary evaluation framework. We argue that from a user perspective, an ideal evaluation framework should satisfy the following three properties: (1) provide a metric that measures the distance of a given summary from ideal; (2) perform the evaluation in an automated and efficient manner without human involvement; and (3) provide standard evaluation datasets on which researchers can compare their summarization methods against previous work. Due to the challenges discussed above, no evaluation method to date satisfies these three properties.\nWe propose to overcome these challenges using a few key observations. First, we note that there are indeed many different types of summaries (e.g. informative substitute, or enticing trailer) that can be defined and judged in different ways. However, a summary that maximizes semantic information is extremely useful, and in fact most other types of summaries can be defined as extensions of\nthis informative summary. Second, we observe that semantic similarity is most naturally measured through text. In addition, humans are very good at summarizing information and experiences in words. As Fig. 1 shows, comparison using the textual descriptions associated with each image is a much better indicator of semantic similarity.\nBased on these observations, we present VideoSET, a method for Video Summary Evaluation through Text that can measure how well any summary retains the semantic information of the original video. Given a video summary to evaluate, our approach first converts the summary into a text representation, using text annotations of the original video. It then compares this representation against ground-truth text summaries written by humans, using Natural Language Processing (NLP) measures of content similarity. We have obtained and publicly released all necessary text annotations and ground-truth summaries for a number of video datasets. In contrast to performing user studies, VideoSET offers the following important benefits:\n1. It measures the distance of any summary from ideal. 2. It can be easily and reliably replicated. 3. The evaluation is efficient, automated, and requires no human involvement.\nIn contrast to previous methods using pixel-based comparisons, VideoSET transfers the evaluation into the text domain to more accurately measure semantic similarity."}, {"heading": "2 Previous Work", "text": "We group previous work into three sections: (1) methods for video summarization; (2) techniques for evaluating video summaries; and (3) techniques for evaluating text summaries.\nMethods for video summarization: Previous methods for video summarization have used low-level features such as color [21] and motion [20,6], or a combination of both [3]. Some other works have modeled objects [10,15] and their interaction [11,16] to select key subshots. Kim and Hwang [10] segment the objects in video and use the distance between the objects for video summarization. Liu et al. [15] summarize a video by finding the frames that contain the object of interest. Lee et al. [11] find the important objects and people in egocentric video and select the events that contain them. Lu and Grauman [16] model video summarization as a story that relates frames to each other based on the objects they contain. Khosla et al. [9] use web images as a prior to summarize user generated videos. Each of these methods use a different technique for evaluating the quality of their video summarization approach. In order to address this issue, our focus in this paper is to introduce an evaluation technique that can automatically evaluate the quality of video summaries.\nTechniques for evaluating video summaries: Most previous work evaluate the performance of their video summarization techniques using user studies [17,2,11,16]. User study requires re-comparison every time algorithm parameters\nare tweaked and is difficult for others to replicate. Liu et al. [15] measure the performance based on the presence of objects of interest. Li and Maerialdo [12] and Khosla et al. [9] use pixel-based distance of a summary to the original video for evaluation. The drawback of using pixel-based distance is that it does not necessarily measure the semantic similarity between subshots, but rather forces them to be similar in color and texture space. Li and Maerialdo [13] introduce VERT, which evaluates video summaries given a ground-truth video summary by counting the number of sub-shots that overlap between the two. This method also suffers from the disadvantage of pixel-based distance. In addition, people often find it a hard task to generate a ground-truth video summary, whereas they are more comfortable summarizing video in text. In constrast to these techniques, we introduce a method that transfers the video summary evaluation problem into the text domain and measures the semantic similarity between automatically generated summaries and ground-truth summaries.\nTechniques for evaluating text summaries: In constrast to the field of computer vision, there has been large progress in the NLP community on evaluating text summaries. The first techniques in NLP were created in order to evaluate the quality of text which had been machine translated from one language to another [1,19]. Later on, Lin [14] introduced ROUGE for evaluating video summaries. The algorithms in ROUGE are inspired by the methods for evaluating machine translation. There have been other more recent techniques for evaluating text summaries [8,22,5], but ROUGE still remains the standard evaluation algorithm. In this paper, we map the video summary evaluation problem into the text domain and use ROUGE to measure the similarity between the summaries."}, {"heading": "3 Evaluation Framework", "text": "In Sec. 3.1, we provide an overview of VideoSET, and describe how it can be used to evaluate video summaries. Then in Sec. 3.2, we describe the video datasets for which we have obtained text annotations and ground-truth summaries that can be used in VideoSET. Finally, in Secs. 3.2-3.5, we explain each component of the framework in detail: obtaining text annotations, obtaining ground-truth summaries, generating a text representation of a video summary, and scoring the video summary."}, {"heading": "3.1 Overview of VideoSET", "text": "Fig. 2 provides an overview of VideoSET. A video is represented as a sequence of M subshots V = {vi}Mi=1, and a video summary is a subset C \u2282 V of these subshots. A user constructs a video summary using a summarization algorithm, and provides it to VideoSET as input. VideoSET then generates a text representation T (C) of the summary, using text annotations of the original video. The text representation is compared against a set of ground-truth text summaries G, that are written by humans to specify the ideal semantic content of a video summary.\nWe have released all necessary text annotations and ground-truth summaries for a number of video datasets.\nComparison against the ground-truth text summaries is performed using a scoring function\nf(C,G) = max gi\u2208G S(T (C), gi) (1)\nwhere S(x, y) is a function that measures the semantic similarity of texts x and y. For S(x, y) we use the ROUGE metric that is a standard for text summary evaluation. The evaluation score is then returned to the user as output."}, {"heading": "3.2 Datasets", "text": "We have released text annotations and ground-truth summaries that can be used in VideoSET for two publicly available egocentric video datasets, and four TV episodes. Each of these are described in more detail below, and representative images and text annotations are shown in Fig. 3.\nDaily life egocentric dataset [11] This dataset consists of 4 egocentric videos of 3-5 hours each. Each video records a subject through natural daily activities such as eating, shopping, and cooking. The videos were recorded using a Looxcie wearable camera at 15 fps and 320\u00d7480 resolution. We provide text annotations and ground-truth summaries for all videos in this dataset.\nDisneyworld egocentric dataset [4] This dataset consists of 8 egocentric videos of 6-8 hours each. Each video records a subject during a day at Disneyworld Park. The videos were recorded using a GoPro wearable camera at 30\nfps and 1280 \u00d7 720 resolution. We provide text annotations and ground-truth summaries for 3 videos in this dataset.\nTV episodes We provide text annotations and ground-truth summaries for 4 TV episodes of 45 minutes each. The episodes consist of 1 from Castle, 1 from The Mentalist, and 2 from Numb3rs.\nIn all, we provide annotations for 40 hours of data split over 11 videos. Our annotations may also be of interest to researchers working in the intersection between images or video and text, similar to [18] and [7]."}, {"heading": "3.3 Obtaining text annotations", "text": "We segmented egocentric videos from the datasets in Sec. 3.2 into 5-second subshots, and TV episodes into 10-second subshots. We then obtained 1-sentence descriptions of each subshot using Amazon\u2019s Mechanical Turk. Workers were\nasked to write a simple and factual sentence about what happened in each subshot. They were instructed to write from a first-person past-tense perspective for the egocentric videos, and from a third-person present-tense perspective for the TV episodes. Workers who annotated the TV episodes were required to be familiar with the episode, and to use the TV character names in their descriptions. The descriptions were edited by additional workers for vocabulary and grammatical consistency.\nChoosing subshot length To choose the subshot length, we first obtained text annotations for an egocentric video at 3, 5, and 10 seconds, and for a TV episode at 5, 10, and 20 seconds. The shortest subshot length for each type of video was chosen to be sufficiently fine to oversegment the video. We then used the ROUGE content similarity metric to compute the similarity between the text annotations at each subshot length. The similarity across the different subshot lengths was high, indicating that content coverage was preserved across the different lengths. Any of the lengths would be appropriate using our framework. We therefore chose to use 5-second subshots for the egocentric videos and 10-second subshots for the TV episodes, to balance the trade-off between having as fine-grained annotations as possible and minimizing the cost of obtaining the annotations.\nWhile VideoSET is designed to evaluate summaries in the form of specificlength subshots, it can easily be adapted and used to evaluate summaries in other formats as well. For example, a summary consisting of keyframes can be represented in text using the annotations for the subshot containing each keyframe. This is appropriate since our subshots are short enough to express a single semantic concept or event. A summary consisting of variable-length subshots can also be evaluated by mapping the subshots to appropriate text annotations."}, {"heading": "3.4 Obtaining ground-truth summaries", "text": "We obtained ground-truth summaries for videos in text form, since humans can most naturally express semantic information through words. It is also easier for humans to write down the information they feel should be in a summary, than it is to comb through a long video and pick out the ideal subshots. For example, it may be clear that a summary should show that the camera-wearer \u201cwalked on the sidewalk.\u201d However, as the examples in Fig. 1 show, many visually diverse and equally good subshots can illustrate this and it is unclear which should be included in a ground-truth.\nWe asked a small group of workers to write a summary in words about what happened in each video. The workers were provided with the text annotations for the video so that similar vocabulary could be used. They were asked to write simple sentences with a similar level of content as the text annotations. They were also asked to rank their sentences in order of importance. Then during the evaluation process, a video summary of |C| subshots is compared with a length-adjusted ground-truth summary consisting of the top |C| most important sentences in temporal order.\nFig. 4 shows an example of length-adjusted, 24-sentence ground-truth summaries written by two different workers. Workers typically wrote and ranked between 40-60 summary sentences per egocentric video, and 20-30 sentences per TV episode."}, {"heading": "3.5 Generating the text representation of a video summary", "text": "Given a video summary C to evaluate, VideoSET first generates a text representation T (C) of the summary. This representation can be acquired by concatenating the pre-existing text annotations (Sec. 3.3) associated with each summary subshot, since the summary is a collection of subshots from the original video. We have released text annotations for the videos in Sec. 3.2 so that no effort is required on the part of the user, and the process is illustrated in Fig. 5."}, {"heading": "3.6 Scoring the video summary", "text": "To score the video summary, a similarity function S(x, y) is used to compare the text representation of the summary with ground-truth text summaries. We use the ROUGE-SU metric from the publicly available ROUGE toolbox [14]. ROUGE-SU measures unigram and skip-bigram co-occurence between a candidate and ground-truth summary, after pre-processing to stem words and remove stopwords. Skip-bigrams are any pair of words in their sentence order, allowing for arbitrary gaps. For example, the skip-bigrams for the sentence \u201cI walked my dog at the park.\u201d are: \u201cwalked dog\u201d, \u201cwalked park\u201d, and \u201cdog park\u201d, where stopwords have not been included. The unigrams and skip-bigrams are treated equally as counting units. We use the F-measure for ROUGE-SU.\nThe ROUGE toolbox is a collection of n-gram comparison metrics that measure text content similarity, and more detail can be found in [14]. We ran experiments using each of the metrics in ROUGE and found ROUGE-SU to have the strongest correlation with human judgment.\nAddressing human subjectivity To address human subjectivity about what is semantically most important, we use the approach of ROUGE to compare a video summary with multiple ground-truth summaries. The score of a video summary C with respect to a set of ground-truth summaries G is computed as f(C,G) = maxgi\u2208C S(T (G), gi), the maximum of pairwise summary-level scores between the video summary and each ground-truth. We have released 3 groundtruth summaries for each video in in Sec. 3.2, and since writing a ground-truth summary is a quick and simple task, this number can be easily scaled in the future."}, {"heading": "4 Experiments", "text": "To assess the effectiveness of VideoSET, we conducted two different experiments. In the first experiment, we generated a number of video summaries using existing video summarization methods, and correlated their VideoSET scores with human judgment. In the second experiment, we analyzed VideoSET\u2019s performance in the full space of possible video summaries. We randomly sampled pairs of video summaries and subshots, and compared VideoSET judgment with human judgment.\nTo confirm our intuition that text distance is more appropriate than visual distance as a measure of semantic similarity, we also compare with a pixel-based distance metric in our experiments."}, {"heading": "4.1 VideoSET evaluation of existing summarization methods", "text": "We generated video summaries using the following existing summarization methods. 2-minute summaries (N = 24 subshots for egocentric video and N = 12 subshots for TV episodes) were generated using each method.\n1. Uniform sampling: N subshots uniformly spaced throughout the original video were selected. 2. Color histogram clustering: Frames extracted at 1fps were clustered into N clusters using \u03c72-distance between color histograms of the frames. Subshots containing the frame closest to the center of each of the N clusters were selected for the video summary. 3. Video-MMR [12]: Frames were extracted at 1fps from the original video. In each of N iterations, a keyframe was chosen that was most visually similar to the frames not yet selected as keyframes, and at the same time different from the frames already selected as keyframes. In other words, each iteratively selected keyframe has Maximal Marginal Relevance (MMR). Given the set of all video frames V and the set of already-selected keyframes Sn\u22121 = {s1, ..., sn\u22121}, the nth keyframe sn is selected as\nsn = arg min f\u2208V \\Sn\u22121 (\u03bb\u03c72(f, V \\Sn\u22121)\u2212 (1\u2212 \u03bb) min s\u2208Sn\u22121 \u03c72(f, s)) (2)\n\u03bb was empirically chosen to be 0.5. Subshots containing the chosen keyframes were selected for the video summary. 4. Object-driven summarization [11]: The method of Lee et al. [11] chooses keyframes containing important people and objects based on a learned metric for importance. Keyframe summaries were provided by the authors for the videos in the Daily life egocentric dataset. The subshots containing the keyframes were selected for the video summary.\nWe also generated summaries using two additional methods that utilize the ground-truth text summaries and text annotations. These methods attempt to maximize our metric score given the ground-truth summaries and thus represent summaries close to what our metric would consider ideal.\n1. Greedy BOW: The words in the ground-truth summary were considered as an unordered \u201cbag of words.\u201d Subshots were greedily selected based on unigram matching of the subshots\u2019 text annotations with the ground-truth bag of words. 2. Sentence-based Ordered Subshot Selection: One subshot was selected for each sentence in the ground-truth summary, using a dynamic programming approach that restricted the selected subshots to be in the same relative order as the corresponding sentences.\nWe computed VideoSET scores for video summaries generated using the above methods, for all the videos in the datasets of Sec. 3.2. For a summary length of 2 minutes, 24 video summaries were generated for the Egocentric daily life dataset (6 methods x 4 original videos), 15 video summaries were generated\nfor the Disney egocentric dataset (5 methods x 3 original videos), and 20 video summaries were generated for the TV episodes (5 methods x 4 episodes). We also computed scores for each of these videos using a pixel-based distance metric for comparison. The pixel-based distance metric was defined as the average visual similarity of the summary subshots to human-defined ground-truth summary subshots, based on minimum \u03c72-color histogram distance of the frames in a subshot to the ground-truth subshot frames.\nWe correlated rankings based on the VideoSET and pixel-based scores with human rankings from a user study. Humans were asked to rank the video summaries generated using the above methods, in terms of how semantically similar they were to the content of ground-truth written summaries. The score was taken to be the highest score with respect to 3 ground-truth summaries. The Spearman\u2019s rank order correlation coefficient between each of the automated metrics and the human-assigned ranks from this study are shown in Table 1.\nThe results in Table 1 show that VideoSET is strongly correlated with human judgment, and has better performance than a pixel-based distance metric. The largest correlation gap between VideoSET and the pixel-based distance is for the Disney dataset, which is most challenging due to the highly varied visual scenes as the subjects tour through the amusement park. The smallest correlation gap is for the TV episodes, where both methods perform strongly due to the fact that TV shows are highly edited with little redundancy."}, {"heading": "4.2 VideoSET Evaluation of Randomly Sampled Summaries and Subshots", "text": "To better understand VideoSET\u2019s performance in the full space of possible summaries, we randomly sampled video summaries as well as subshots, and compared VideoSET judgment with human judgment.\nWe first randomly generated 100 pairs of 2-min. summaries (24 subshots) for a video in the Daily life egocentric dataset [11]. We asked two humans to watch each pair of summaries and judge which was semantically closer to a provided ground-truth text summary. In 40% of the comparisons, the two human judges disagreed, indicating that the difference was too ambiguous even for humans. For the remaining 60% of the comparisons, we computed automated judgments using VideoSET scores as well as a pixel-based distance metric. The results are shown in Table 2, and show that VideoSET scores have higher agreement with human judgment than the pixel-based distance metric.\nAt a finer level, we then assessed the performance of VideoSET on comparing pairs of individual subshots. Since the space is now more constrained, we densely computed VideoSET scores for every pair of subshots in the video with respect to every possible third subshot as a reference. We also computed scores based on the pixel-based distance metric. Based on these, we separated the comparisons into 4 different cases: (1) VideoSET judged both subshots to have no semantic similarity with the reference subshot; (2) VideoSET judged both subshots to have equal, non-zero semantic similarity with the reference subshot; (3) VideoSET judged one subshot to be semantically more similar than the other, and agreed with the pixel-based (PB) judgment; and (4) VideoSET judged one subshot to be semantically more similar than the other, and disagreed with the pixel-based (PB) judgment. We then sampled 300 comparisons from each of these 4 cases (a total of 1200 comparisons). For these samples, we asked humans to judge which subshot in each pair is semantically more similar to the reference subshot, if the pair is equally similar, or if both subshots have no similarity. The agreement of the VideoSET and pixel-based judgments with the human judgments is shown in Table 3.\nBoth VideoSET and humans judged the majority of subshots to have zero similarity (65.1% of comparisons for VideoSET, 77.3% for humans). This is expected since most pairs of subshots should not be semantically related. Because of this, we also show the agreements using only non-zero human judgments. The results indicate that VideoSET has stronger agreement with human judgment than the pixel-based metric. Additionally, when VideoSET and the pixel-based metric both judge that one subshot is semantically closer than the other but disagree, VideoSET agrees with human judgment more than twice as often as the pixel-based metric. Some illustrative examples of comparisons where VideoSET and the pixel-based metric disagree are shown in Fig. 6."}, {"heading": "5 Conclusion", "text": "We have developed an evaluation technique to automatically measure how well a video summary retains the semantic information in the original video. Our approach is based on generating a text representation of the video summary, and measuring the semantic distance of the text to ground-truth text summaries written by humans. Our experiments show that this approach correlates well with human judgment, and outperforms pixel-based distance measures. In addition, our framework can be extended to evaluate any type of video summary, and can accommodate future extensions to our semantic distance metric."}, {"heading": "6 Acknowledgements", "text": "This research is partially supported by an ONR MURI grant and an Intel gift, and a Stanford Graduate Fellowship to S.Y."}], "references": [{"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["G. Doddington"], "venue": "In Proceedings of the Second International Conference on Human Language Technology Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Automatically segmenting lifelog data into events", "author": ["A.R. Doherty", "A.F. Smeaton"], "venue": "In International Workshop on Image Analysis for Multimedia Interactive Services,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Key-frame selection to represent a video", "author": ["F. Dufaux"], "venue": "In IEEE International Conference on Multimedia and Expo,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Social interactions: A first-person perspective", "author": ["A. Fathi", "J.K. Hodgins", "J.M. Rehg"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Summarization system evaluation revisited: N-gram graphs", "author": ["G. Giannakopoulos", "V. Karkaletsis", "G. Vouros", "P. Stamatopoulos"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Schematic storyboarding for video visualization and editing", "author": ["D.B. Goldman", "B. Curless", "D. Salesin", "S.M. Seitz"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Automated summarization evaluation with basic elements", "author": ["E. Hovy", "C. Lin", "L. Zhou", "J. Fukumoto"], "venue": "In Proceedings of the Fifth Conference on Language Resources and Evaluation (LREC", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Large-scale video summarization using web-image priors", "author": ["A. Khosla", "R. Hamid", "C-J Lin", "N. Sundaresan"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "An integrated scheme for object-based video abstraction", "author": ["C. Kim", "J. Hwang"], "venue": "In Proceedings of ACM Multimedia,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Discovering important people and objects for egocentric video summarization", "author": ["Y.J. Lee", "J. Ghosh", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Multi-video summarization based on av-mmr", "author": ["Y. Li", "B. Maerialdo"], "venue": "In ContentBased Multimedia Indexing (CBMI),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Automatic evaluation of video summaries", "author": ["Y. Li", "B. Maerialdo. Vert"], "venue": "In ACM International Conference on Multimedia,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C-Y. Lin"], "venue": "In Workshop on Text Summarization Branches Out (WAS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "A hierarchical visual model for video object summarization", "author": ["D. Liu", "G. Hua", "T. Chen"], "venue": "In PAMI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Story-driven summarization for egocentric video", "author": ["Z. Lu", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Recent advances in content-based video analysis", "author": ["C. Ngo", "H. Zhang", "T. Pong"], "venue": "In International Journal of Image and Graphics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T. Berg"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Key frame selection by motion analysis", "author": ["W. Wolf"], "venue": "In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "An integrated system for content-based video retrieval and browsing", "author": ["H.J. Zhang"], "venue": "In Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Paraeval: Using paraphrases to evaluate summaries automatically", "author": ["L. Zhou", "C. Lin", "D. Munteanu", "E. Hovy"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}], "referenceMentions": [{"referenceID": 15, "context": "Most previous work has performed a diverse range of user comparison studies [17,2,11,16] ar X iv :1 40 6.", "startOffset": 76, "endOffset": 88}, {"referenceID": 1, "context": "Most previous work has performed a diverse range of user comparison studies [17,2,11,16] ar X iv :1 40 6.", "startOffset": 76, "endOffset": 88}, {"referenceID": 9, "context": "Most previous work has performed a diverse range of user comparison studies [17,2,11,16] ar X iv :1 40 6.", "startOffset": 76, "endOffset": 88}, {"referenceID": 14, "context": "Most previous work has performed a diverse range of user comparison studies [17,2,11,16] ar X iv :1 40 6.", "startOffset": 76, "endOffset": 88}, {"referenceID": 11, "context": "that are difficult to replicate, while a few have used pixel-based comparison with a ground truth [13,9].", "startOffset": 98, "endOffset": 104}, {"referenceID": 7, "context": "that are difficult to replicate, while a few have used pixel-based comparison with a ground truth [13,9].", "startOffset": 98, "endOffset": 104}, {"referenceID": 19, "context": "Methods for video summarization: Previous methods for video summarization have used low-level features such as color [21] and motion [20,6], or a combination of both [3].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "Methods for video summarization: Previous methods for video summarization have used low-level features such as color [21] and motion [20,6], or a combination of both [3].", "startOffset": 133, "endOffset": 139}, {"referenceID": 5, "context": "Methods for video summarization: Previous methods for video summarization have used low-level features such as color [21] and motion [20,6], or a combination of both [3].", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "Methods for video summarization: Previous methods for video summarization have used low-level features such as color [21] and motion [20,6], or a combination of both [3].", "startOffset": 166, "endOffset": 169}, {"referenceID": 8, "context": "Some other works have modeled objects [10,15] and their interaction [11,16] to select key subshots.", "startOffset": 38, "endOffset": 45}, {"referenceID": 13, "context": "Some other works have modeled objects [10,15] and their interaction [11,16] to select key subshots.", "startOffset": 38, "endOffset": 45}, {"referenceID": 9, "context": "Some other works have modeled objects [10,15] and their interaction [11,16] to select key subshots.", "startOffset": 68, "endOffset": 75}, {"referenceID": 14, "context": "Some other works have modeled objects [10,15] and their interaction [11,16] to select key subshots.", "startOffset": 68, "endOffset": 75}, {"referenceID": 8, "context": "Kim and Hwang [10] segment the objects in video and use the distance between the objects for video summarization.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "[15] summarize a video by finding the frames that contain the object of interest.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] find the important objects and people in egocentric video and select the events that contain them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Lu and Grauman [16] model video summarization as a story that relates frames to each other based on the objects they contain.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "[9] use web images as a prior to summarize user generated videos.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Techniques for evaluating video summaries: Most previous work evaluate the performance of their video summarization techniques using user studies [17,2,11,16].", "startOffset": 146, "endOffset": 158}, {"referenceID": 1, "context": "Techniques for evaluating video summaries: Most previous work evaluate the performance of their video summarization techniques using user studies [17,2,11,16].", "startOffset": 146, "endOffset": 158}, {"referenceID": 9, "context": "Techniques for evaluating video summaries: Most previous work evaluate the performance of their video summarization techniques using user studies [17,2,11,16].", "startOffset": 146, "endOffset": 158}, {"referenceID": 14, "context": "Techniques for evaluating video summaries: Most previous work evaluate the performance of their video summarization techniques using user studies [17,2,11,16].", "startOffset": 146, "endOffset": 158}, {"referenceID": 13, "context": "[15] measure the performance based on the presence of objects of interest.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Li and Maerialdo [12] and Khosla et al.", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "[9] use pixel-based distance of a summary to the original video for evaluation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Li and Maerialdo [13] introduce VERT, which evaluates video summaries given a ground-truth video summary by counting the number of sub-shots that overlap between the two.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "The first techniques in NLP were created in order to evaluate the quality of text which had been machine translated from one language to another [1,19].", "startOffset": 145, "endOffset": 151}, {"referenceID": 17, "context": "The first techniques in NLP were created in order to evaluate the quality of text which had been machine translated from one language to another [1,19].", "startOffset": 145, "endOffset": 151}, {"referenceID": 12, "context": "Later on, Lin [14] introduced ROUGE for evaluating video summaries.", "startOffset": 14, "endOffset": 18}, {"referenceID": 6, "context": "There have been other more recent techniques for evaluating text summaries [8,22,5], but ROUGE still remains the standard evaluation algorithm.", "startOffset": 75, "endOffset": 83}, {"referenceID": 20, "context": "There have been other more recent techniques for evaluating text summaries [8,22,5], but ROUGE still remains the standard evaluation algorithm.", "startOffset": 75, "endOffset": 83}, {"referenceID": 4, "context": "There have been other more recent techniques for evaluating text summaries [8,22,5], but ROUGE still remains the standard evaluation algorithm.", "startOffset": 75, "endOffset": 83}, {"referenceID": 9, "context": "Daily life egocentric dataset [11] This dataset consists of 4 egocentric videos of 3-5 hours each.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "Disneyworld egocentric dataset [4] This dataset consists of 8 egocentric videos of 6-8 hours each.", "startOffset": 31, "endOffset": 34}, {"referenceID": 16, "context": "Our annotations may also be of interest to researchers working in the intersection between images or video and text, similar to [18] and [7].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "TV#episodes# Disneyworld#egocentric#dataset#[4]# I#walked#around#a#market#and# looked#at#tents.", "startOffset": 44, "endOffset": 47}, {"referenceID": 9, "context": "# Daily#life#egocentric#dataset#[11]#", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "We use the ROUGE-SU metric from the publicly available ROUGE toolbox [14].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "The ROUGE toolbox is a collection of n-gram comparison metrics that measure text content similarity, and more detail can be found in [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "Video-MMR [12]: Frames were extracted at 1fps from the original video.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Object-driven summarization [11]: The method of Lee et al.", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "[11] chooses keyframes containing important people and objects based on a learned metric for importance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Daily life dataset [11] Disney dataset [4] TV episodes", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Daily life dataset [11] Disney dataset [4] TV episodes", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "summaries (24 subshots) for a video in the Daily life egocentric dataset [11].", "startOffset": 73, "endOffset": 77}], "year": 2014, "abstractText": "In this paper we present VideoSET, a method for Video Summary Evaluation through Text that can evaluate how well a video summary is able to retain the semantic information contained in its original video. We observe that semantics is most easily expressed in words, and develop a text-based approach for the evaluation. Given a video summary, a text representation of the video summary is first generated, and an NLP-based metric is then used to measure its semantic distance to ground-truth text summaries written by humans. We show that our technique has higher agreement with human judgment than pixel-based distance metrics. We also release text annotations and ground-truth text summaries for a number of publicly available video datasets, for use by the computer vision community.", "creator": "LaTeX with hyperref package"}}}