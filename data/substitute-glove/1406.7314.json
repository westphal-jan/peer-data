{"id": "1406.7314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2014", "title": "On the Use of Different Feature Extraction Methods for Linear and Non Linear kernels", "abstract": "The ceremony feature extraction today been short push changes in robust speech merit developed; it compared affects early receive performance. In much collected, feel time institute well turn own consist screen extraction experimentation such example parameter intuitive calculus (LPC ), carey spectrum cepstral attenuation (MFCC) though ultrastructural linear predict (PLP) others throughout features verifiable intensive 're rasta firewalls often cepstral mean cross-linking (CMS ). Based however this, an comparative evaluation taken these disc neither performed as the responsible 's text regional speaker voter using for combination as gaussian pudding models (GMM) and formula_4 by measures - linear pineapple firm two join formula_19 machine (SVM ).", "histories": [["v1", "Fri, 27 Jun 2014 20:56:00 GMT  (268kb)", "http://arxiv.org/abs/1406.7314v1", "8 pages, 3 Figures"]], "COMMENTS": "8 pages, 3 Figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["imen trabelsi", "dorra ben ayed"], "accepted": false, "id": "1406.7314"}, "pdf": {"name": "1406.7314.pdf", "metadata": {"source": "CRF", "title": "On the Use of Different Feature Extraction Methods for Linear and Non Linear kernels", "authors": ["Imen TRABELSI", "Dorra BEN AYED"], "emails": ["trabelsi.imen1@gmail.com", "Dorra.mezghani@isi.rnu.tn", "dorrainsat@yahoo.fr"], "sections": [{"heading": null, "text": "- 1 -\nINTRODUCTION\nIn this paper, we highlight some of key related researches, techniques and approaches that have arisen to extract the suitable feature parameters. Currently, there are two major approaches to feature extraction: modeling human voice production and modeling perception system. In the first model, the voice evolved primarily to produce speech for conversation, in the second model, hearing evolved to recognize these sounds. So we try to classify the features extraction under these two models.\nIn order to enhance performance and robustness in automatic speech recognition, pre processing and filtering in speech feature extraction are commonly used.\nIn this paper, we motivate the use of extraction feature techniques for text independent speaker identification system using the GMM supervector in a support vector machine (SVM) classifier."}, {"heading": "1. Different speech feature", "text": "Features extraction in ASR is the computation of a sequence of feature vectors which provides a compact representation of the given speech signal.\nProducing and perceiving speech are basin human activities, a speaker can be presented as an encoder in a speech production process and the listener can be presented as a decoder in a speech perception process.\nFigure 1 shows the complete process of producing and perceiving speech from the formulation of a message of a talker, to the creation of the speech signal, and finally to the understanding of the message by a listener.\nBetween human auditory and speech production systems, some researches believe that the auditory system came first, other researches uses the speech production model as the primary focus [URS 02].\nIt is the acoustic speech signal which mediates between the two systems. Thus, it is only natural to expect that the properties of the acoustic signal can tell us about both the human speech production system and the human auditory system.\n- 2 -\nSpeech Production Process\nSpeech Perception Process\nThis section reviews the production/perception process and the discriminates features extracted from their characteristics."}, {"heading": "1.1. Features based on speech production", "text": "We elucidate in this section the production mechanisms that give rise to different kinds of features.\nSpeech production is produced by the combined motion of articulatory gestures.\nThe mechanism of speech is composed of four processes: language processing, in which the content of an utterance is represented somehow in the brain; generation of motor commands to the vocal organs; articulatory movement for the production of speech by the vocal organs based on these motor commands; and the emission of air sent from the lungs in the form of speech [HON 03].\nThese structures are able to generate and shape a wide variety of waveforms. These waveforms can be broadly categorized into voiced and unvoiced speech.\nThese features describe properties of speech production rather than the properties of the acoustic signal resulting from it.\nBased on the knowledge of the speech production mechanisms, we are able to extract a set of features which can best represent a particular phoneme. The phonemes are classified in terms of manner of articulation, (how is the vocal tract constricted), and place of articulation (where is the vocal tract constricted?) as mentioned in the table below.\nSome features are motivated from a speech point of view like articulatory features and linear predictive analysis.\n1.1.1. Articulatory features Articulatory features (AFs) have attracted interest from the speech recognition community for more than a decade for many reasons [KIN 07] [KIR 00].\nThese features describe the configuration of the human vocal tract and the properties of speech production.\nThe basic idea of this approach is to bears an affinity to the articulatory events underlying the speech signal.\nThis representation is composed of classes describing the most essential articulatory properties of speech sounds such as place, manner, voicing, liprounding, the opening between the lips, and the position of the tongue.\n1.1.2. Linear Predictive Coding - LPC\nLinear predictive analysis of speech were introduced in late 1960s and become the predominant technique for estimating the basic parameters of speech [MAK 75].\nBased on a highly simplified model for speech production, LPC provides both an accurate estimate of the speech parameters such as pitch, formants and spectra. It tries to imitate the human speech production mechanism. In addition, all the vocal tract parameters are represented in a set of LPC coefficients. The number of coefficients is typically 10 to 20 [KIN 07].\nIt is widely used because it is fast, simple and its ability to extract and store time varying formant information."}, {"heading": "1.2. Features based on perception system", "text": "The auditory system is the sensory system for the sense of hearing. Research in speech perception seeks to understand how human listeners recognize speech sounds and use this information to understand spoken language.\nThe acoustic wave is transmitted from the outer ear\n- 3 -\nto the inner ear which performs a transduction from acoustic energy to mechanical vibrations which ultimately are transferred to the basilar membrane inside the cochlea (the main component of the inner ear). The mechanical vibrations are then transuded into activity signals on the auditory nerves corresponding to a feature extraction process [KIM 99].\nThe cochlea performs the filterbank based frequency analysis on the speech signal to extract the pertinent features. Thus, most techniques are pivoting around the filterbank methodology in extracting the features. The difference in the design of the filterbank offers the extraction of different features from the signal.\nIn fact the question of the imitation of the human auditory system characteristics for ASR has been subject of discussion and some researches believe that the analysis based on the effective peripheral auditory processing is the most robust front end in noise [HER 98].\nFrom the point of view of speech perception, we can describe some of these features.\n1.2.1. Mel Frequency Cepstral Coefficients - MFCC The most commonly used acoustic features are Mel-scale frequency cepstral coefficient based on frequency domain using the Mel scale which is based on the human ear scale.\nMFCC takes human perception sensitivity with respect to frequencies into consideration.\nMFCC is based on psychoacoustic research on the pitch and the perception of different frequency bands by the human ear. These parameters are similar to ones that are used by humans for hearing speech.\n1.2.2. Perceptual Linear Prediction -PLP Hermansky [HER 90] introduced a new technique, perceptual linear predictive (PLP) analysis.\nThis technique is based on the short-term spectrum of speech. It combined several engineering approximations to selected characteristics of human hearing and approximates auditory spectra by an autoregressive all-pole model.\nPLP uses engineering approximations for three basic concepts from the psycho-acoustic of hearing: spectrum critical band spectral resolution, the equalloudness curve and intensity power low.\nLike MFCC, PLP employ an auditory based warping of the frequency axis derived from the frequency sensitivity of human hearing.\n1.3. Other speech features\n1.3.1. Dynamic features The set of features described so far capture the average frequency distribution during a frame. Important information in the speech signal is however contained in the temporal evolution of the signal, in its\ndynamics.\nOne way to capture this information is to use the dynamic properties of speech, the first and/or second order differences of static coefficients which are called the delta (speed) and delta-delta (acceleration) coefficients. The time derivative is approximated by differencing between frames after and before the current, for instance:\ndidiyi yy   (1)\nWhere y i is the feature vector at frame i, and d typically is set to 1 or 2.\nIt has become common to combine dynamic features with the basic static features. It usually results in better performance.\n1.3.2. Prosodic features Prosody is defined as any property of speech that is not limited to a specific phoneme.\nProsody is a term that refers to the suprasegmental aspects of speech, including variations in pitch (fundamental frequency), energy, loudness, duration, pause, intonation, rate, stress and rhythm.\nProsody may reflect various features of the speaker, his emotional state or speaking style.\nVery few people have done experiments which directly incorporate prosody as complementary information with ASR.\nKompe [KOM 97] is one of the few people to experiment with prosody. He reports improvements to recognition rates when prosodic information is used for recognition purposes."}, {"heading": "2. Pre-processing", "text": "The speech preprocessing part is the fundamental signal processing applied before extracting features from speech signal, conditions the raw speech signal and prepares it for subsequent manipulations. Commonly used preprocessing techniques are illustrated as follows."}, {"heading": "2.1. Digitalization", "text": "It is the first step in the speech processing speech acquisition, requires a microphone coupled with an analog-to-digital converter to receive the voice speech signal, sample it, and convert it into digital speech.\nThe analog speech signal is digitized with sampling rate of 8 KHZ in digital telephony and 10 KHZ, 12 KHZ or 16 KHZ in non telecommunication application."}, {"heading": "2.2. End Point Detection", "text": "This step is based on signal energy evaluation. A voice signal can be divided into three parts: speech segment, silence segment and background noise. In\n- 4 -\norder to segregate between them we call algorithms for speech end point detection. After which the unnecessary parts have been removed."}, {"heading": "2.3. Pre-emphasis", "text": "The digitized speech signal Y[n] is sent to a Finite Impulse Response (FIR) Filter:\n]1n[x]n[x]n[Y  (2)\n0.19.0  (3)\nWhere x[n] is the input speech signal and Y[n] is the output pre-emphasized signal and \u03b1 is an adjustable parameter.\nThe goal of pre-emphasis is to compensate the high-frequency part that was suppressed during the sound production mechanism of humans. Moreover, it can also amplify the importance of high-frequency formants."}, {"heading": "2.4. Frame blocking", "text": "The continuous Pre-emphasis signal Y is divided into overlapping frames of N samples.\nFrame duration typically rages between 10 and 30 ms short time intervals to guarantee the quasistationary of the signal with optional overlap of [1/3 1/2] of the frame size."}, {"heading": "2.5. Windowing", "text": "After framing, windowing techniques are applied in order to reduce the effect of discontinuity in every frame and at the edges of the frame.\nEach frame has to be multiplied with a windowing technique, there are different types of windowing functions, like rectangular, hamming, barlett, Blackman, Kaiser, bohman, chebyshev, hanning and gaussian windows. The most popular is the hamming window w (n), is defined by:\n)cos(a)a1()a,n(w 1Nn2  (4)\n1Nn0  (5)\nDifferent values of a corresponds to different curves for the Hamming windows\nThen the signal in a frame S[n] after Hamming windowing is:\nw[n]*]n[Y]n[S  (6)"}, {"heading": "3. Post Processing", "text": "This section reviews the various methods which have been proposed for feature normalization."}, {"heading": "3.1. Cepstral Mean Subtraction -CMS", "text": "The algorithm computes a long-term mean cepstral value of the feature vectors and subtracts the mean value from the cepstral vectors of that utterance and then produces a normalized cepstrum vector. CMS\navoids the low frequency noise to be further, amplified but the average vocal tract configuration information pertaining to the speaker may be also lost [FUR 81]."}, {"heading": "3.2. Cepstral variance normalization -CVN", "text": "Cepstral variance normalization is also known as the mean and variance normalization (MVN) [JAI 01] because CVN is often used in conjunction with CMS. The mean and variance of cepstral coefficients are assumed to be invariant in the CVN analysis. Therefore the exclusion of these properties would result in the removal of irrelevant information such as the effects of mismatched environments."}, {"heading": "3.3. RASTA-filtering", "text": "Rasta-filtering was proposed for robust speech recognition by Hermansky and Morgan [HER 94].\nAt the beginning, it was introduced to support Perceptual Linear Prediction (PLP) preprocessing. It uses bandpass filtering in the log spectral domain. It has also been applied to other cepstral feature based preprocessing with both log spectral and the cepstral domain filtering.\nRasta filtering then removes slow channel variations and makes PLP more robust to linear spectral distortions [HER 91]. This technique has proven to be a successful technique for channel normalization in automatic speech recognition."}, {"heading": "3.4. Feature warping", "text": "Also known as cumulative distribution mapping [CHOI 06]. It consists of mapping the observed cepstral feature distribution to a predefined target distribution over a sliding window with zero mean and unit variance [PEL 03].\nThis technique is a real-time equivalent of histogram equalization in image processing that maps a source feature distribution to a target distribution. This feature processing technique has successfully been applied to speaker recognition because it is robust to channel mismatch, additive noise and to some extent, nonlinear effects attributed to handset distortion [PEL 03] [BAR 03]."}, {"heading": "3.5. Short-time Gaussianization", "text": "This is achieved by an iterative scheme in each iteration; a global linear transformation is applied to the features in order to make them more dependant or decorelated in the new feature space before mapping them to an ideal distribution, such as the standard normal distribution [CHEN 00]. This linear transformation can be estimated by Expectation Maximization (EM) algorithm [DEM 77].\n- 5 -"}, {"heading": "4. Experimental evaluation of different features with application in speaker identification", "text": ""}, {"heading": "4.1. System Conditions", "text": "Our baseline system is a text-independent speaker identification task based on hybrid GMM/SVM system [BOU 09a], [BOU 09b].\nThe purpose of this study is to evaluate the performance of different acoustic features, when training data and testing data are in a CLEAN environment in order to show the differences between them.\nExperiments have been conducted under the experimental conditions described (in table 2).\nNumber utterance per speaker 8 sentences for train Number utterance per speaker 2 sentences for test\nIn our experiments, we evaluated several different feature measurements, including Mel-scale Frequency Cepstral (MFCCs), Perceptual Linear Prediction (PLP), Linear Predictive Coding (LPC) both with and without their first and second derivatives and combined with normalization techniques.\nThe performance is measured as the identification rate ( IR)."}, {"heading": "IR (%) = \u00d7", "text": ""}, {"heading": "100 (7)", "text": "Specifications of the input audio stream at the acoustic pre-processor are summarized as follows (in table 3).\nTable3. Pre-Processing Stages Stage Value\nSampling rate End Point Detection Pre Emphasis Frame Duration Frame Shift Windowing\n16 KHZ Energy based VAD 1- 0.95 16 ms 8ms Hamming"}, {"heading": "4.2. GMM-UBM baseline System", "text": "The baseline system was a GMM UBM [REY 95]. Speaker modeling involves 2 step processes: a general\nuniversal background model UBM is trained using acoustic data of different speakers, in order to model the acoustics of speech.\nThe UBM comprised of 128 mixtures is trained using the EM algorithm with a vector quantization pre-estimate (KMEANS). And a target speaker model is adapted by Bayesian adaptation MAP [REY 00] from the UBM by adjusting the UBM means.\nAll Gaussian means vectors are pooled together to get one GMM Supervector [CAM 06]. The GMM supervector can be thought of as a mapping between an utterance and a high-dimensional vector. The Process is shown in Figure2.\nWe produced GMM supervector on a per utterance using MAP adaptation."}, {"heading": "4.3. Support vector machine in the GMM space", "text": "GMM-supervector and SVM combines both generative and discriminative methods and leads to the generative SVM kernels based on the probability distribution estimation.\nIn our case, SVM is applied in the GMMs means supervector space [SCH 96] as shown in figure 3.\nSVMs perform a nonlinear mapping from an input space to an SVM expansion space.\nThe main design component in an SVM is the kernel, which is an inner product in the SVM feature space.\nIn our experiments, we have used two different kernel functions, the first one corresponds to the linear GMM-SVM kernel. The last ones is the non linear GMM-SVM kernels based on the radial basis function\n- 6 -\n(RBF).\nThe Kernel in equation (8), (9) and scoring are implemented using the library LIBSVM [CHA 01].\nk (x, i v ) = x . i v\n(8)\nwhere x is the input data and vi are the support vectors.\nK (x, i v\n) = ])vx( 2\n1 exp[ 2i\n  (9)\nwhere \u03c3 is the standard deviation of the radial basis function.\nThe best RBF parameters are chosen with a cross validation. We first divide the training set into 10 folds of equal size. Sequentially one fold is tested using the classifier trained on the remaining 9 folds.\nVarious parameters values are tried and the one with the best cross-validation accuracy is picked."}, {"heading": "4.4. Experiments results", "text": "Our motivation was to analyze how much the performance rates for speaker identification task are depending on the choice of feature extraction techniques and on the kernel functions training SVMs.\n4.4.1. MFCCs variants Results\nTable 4 shows the performance of our GMM SVM\nsystem based on different combination of MFCC.\nWe observe that the IR is identical for MFCC, MFCC + delta, and MFCC + Energy. This IR is equal to 100% but when MFCC are combined with delta and delta delta, our system achieve an IR between 92, 85% and 96, 42% where the best performance in this case was reported by linear kernel.\nWhen MFCC is used together with its delta, deltadelta and energy, IR obtained is around 96, 42% for both linear and RBF kernels.\nWhereas IR degrades most significantly when MFCC are enhancing with CMS clearly with RBF kernel.\n4.4.2. PLP variants Results Table 5 presents the results obtained for different combination of PLP. These results show that IR keep the same value for PLP and PLP+ first delta+ second delta. It is equal to 100%. But decline significantly when PLP are combined with Rasta filter importantly with RBF kernel.\nsystem using LPC variants.\nWhen LPC is associated with dynamic features, the IR increase from 96, 42% to 100% for linear and RBF kernels.\n- 7 -"}, {"heading": "5. Conclusion", "text": "A major problem in speech recognition system is the decision of the suitable feature set which can faithfully describe in an abstract way the original speech signal.\nThe objective of this paper was to give account of the current knowledge on the area of features extraction, speech pre processing and normalization methods for speaker identification tasks.\nWe have proposed an hybrid GMM-SVM system. We have presented the performance of this combination with different features and two different Kernels.\nThen a comparative study was made to investigate the best choice of kernel function and the best input features.\nFirst, we conclude that MFCC and LPC outperform PLP.\nWe also conclude that including the delta and acceleration coefficients has a negative affect on ASR performance excluding with LPC.\nWe therefore conclude that Rasta Filter and CMS did not improve accuracy.\nThis happens because Data in TIMIT were recorded with high-quality desktop microphones in a clean environment and does not include session variability between train and test. In this case, temporal coefficients and normalization methods remove useful information.\nIn addition, our experiments reveals that linear and RBF kernels give equal performance with a small favor for linear SVM.\nThus, as a future work, we will try to study the performance of SVM for speaker identification task by using all dialects of the TIMIT corpus and eventually extend our study to other different environments with acoustic mismatch. And we will attempt to study the performance of other SVM kernels."}], "references": [{"title": "Feature and score normalization for speaker verification of cellular data", "author": ["C. Barras", "J.L. Gauvain"], "venue": "Proceedings of IEEE ICASSP, vol. 2, p. 49-52", "citeRegEx": "BAR 03", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust Text Independent Speaker Identification Using Hybrid GMM-SVM System", "author": ["S. Zribi Boujelbene", "D. Ben Ayed Mezghani", "N. Ellouze"], "venue": "Journal of Convergence Information Technology \u2013 JDCTA,", "citeRegEx": "Boujelbene et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Boujelbene et al\\.", "year": 1975}, {"title": "Support Vector Machines approaches and its application to speaker identification", "author": ["S. Zribi Boujelbene", "D. Ben Ayed Mezghani", "N. Ellouze"], "venue": "IEEE International Conference on Digital Eco-Systems and Technologies DEST-09,", "citeRegEx": "Boujelbene et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Boujelbene et al\\.", "year": 2009}, {"title": "and A", "author": ["W.M. Campbell", "D.E. Sturim", "D.A. Reynolds"], "venue": "Solomonoff, \"SVM based speaker verification using a GMM-supervector kernel and NAP variability compensation,\" Proc. Int. Conf. Acoustics, Speech, and Signal Processing", "citeRegEx": "CAM 06", "shortCiteRegEx": null, "year": 2006}, {"title": "Lin", "author": ["C.-C. Chang", "C.-J"], "venue": "LIBSVM: a library for support vector machines,", "citeRegEx": "CHA 01", "shortCiteRegEx": null, "year": 2001}, {"title": "A Noise Robust Front-end for Speech Recognition Using Hough Transform and Cumulative Distribution Mapping", "author": ["E.H.C. Choi"], "venue": "18th International Conference on Pattern Recognition, vol. 4, p 286-289", "citeRegEx": "CHOI 06", "shortCiteRegEx": null, "year": 2006}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Cepstral analysis technique for automatic speaker verification", "author": ["S. Furui"], "venue": "IEEE Trans. Acoust. Speech Signal Processing,", "citeRegEx": "Furui,? \\Q1981\\E", "shortCiteRegEx": "Furui", "year": 1981}, {"title": "Perceptual linear predictive (PLP) analysis of speech", "author": ["H. Hermansky"], "venue": "J. Acoustic Soc. Amer., vol. 87, no. 4, pp. 1738-1752, Apr.", "citeRegEx": "HER 90", "shortCiteRegEx": null, "year": 1990}, {"title": "Rasta plp speech analysis", "author": ["H. Hermansky", "N. Morgan", "A. Bayya", "P. Kohn"], "venue": "International Computer Science Institute,", "citeRegEx": "Hermansky et al\\.,? \\Q1947\\E", "shortCiteRegEx": "Hermansky et al\\.", "year": 1947}, {"title": "RASTA processing of speech", "author": ["H. Hermansky", "N. Morgan"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions, vol. 2, no. 4", "citeRegEx": "HER 94", "shortCiteRegEx": null, "year": 1994}, {"title": "Should recognizers have ears", "author": ["H. Hermansky"], "venue": "Speech Communication, vol. 25, no. 1-3, pp. 327", "citeRegEx": "HER 98", "shortCiteRegEx": null, "year": 1998}, {"title": "Improved mean and variance normalization for robust speech recognition", "author": ["P. Jain", "H. Hermansky"], "venue": "Proc. ICASSP", "citeRegEx": "JAI 01", "shortCiteRegEx": null, "year": 2001}, {"title": "S", "author": ["D. Kim"], "venue": "Lee \"Auditory processing of speech signals for robust speech recognition on realworld noisy environment\", IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESING, vol. 7, no. 1", "citeRegEx": "KIM 99", "shortCiteRegEx": null, "year": 1999}, {"title": "Speech production knowledge in automatic speech recognition", "author": ["S. King", "al"], "venue": "JASA,", "citeRegEx": "King and al.,? \\Q2007\\E", "shortCiteRegEx": "King and al.", "year": 2007}, {"title": "Combining acoustic and articulatory feature information for robust speech recognition", "author": ["K. Kirchhoff", "G.A. Fink", "G. Sagerer"], "venue": "Speech Communication, vol. 37, pp. 303\u2013319", "citeRegEx": "KIR 00", "shortCiteRegEx": null, "year": 2000}, {"title": "Linear Prediction: A Tutorial Review", "author": ["J.J. Makhoul"], "venue": "Proc. IEEE, Vol. 63, pp. 561-580,", "citeRegEx": "Mak 75", "shortCiteRegEx": null, "year": 1975}, {"title": "Robust Automatic Speaker Recognition", "author": ["J.W. Pelecanos"], "venue": "PhD thesis, Queensland University of Technology", "citeRegEx": "PEL 03", "shortCiteRegEx": null, "year": 2003}, {"title": "Digital Processing of Speech Signals", "author": ["L.R. Rabiner", "R.W. Schafer"], "venue": "Prentice-Hall, Signal Processing Series, Englewood Cliffs, NJ", "citeRegEx": "RAB 78", "shortCiteRegEx": null, "year": 1978}, {"title": "Robust textindependent speaker identification using Gaussian mixture speaker models", "author": ["D. Reynolds", "R. Rose"], "venue": "\" IEEE Trans. Speech Audio Proc., vol. 3, no. 1, pp. 72\u201383", "citeRegEx": "REY 95", "shortCiteRegEx": null, "year": 1995}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D. Reynolds", "T. Quatieri", "R. Dunn"], "venue": "DSP, vo. 10, no. 1-3, pp. 19\u201341", "citeRegEx": "REY 00", "shortCiteRegEx": null, "year": 2000}, {"title": "Speaker Identification via Support Vector Machies", "author": ["M. Schmidt", "H. Gish"], "venue": "ICASSP, 105-108", "citeRegEx": "SCH 96", "shortCiteRegEx": null, "year": 1996}, {"title": "Triphone Clustering in Finnish Continuous Speech Recognition", "author": ["M. Ursin"], "venue": "Master Thesis, Department of Computer Science, Helsinki University of Technology, Finland", "citeRegEx": "URS 02", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 22, "context": "Between human auditory and speech production systems, some researches believe that the auditory system came first, other researches uses the speech production model as the primary focus [URS 02].", "startOffset": 186, "endOffset": 194}, {"referenceID": 15, "context": "Articulatory features Articulatory features (AFs) have attracted interest from the speech recognition community for more than a decade for many reasons [KIN 07] [KIR 00].", "startOffset": 161, "endOffset": 169}, {"referenceID": 16, "context": "Linear Predictive Coding - LPC Linear predictive analysis of speech were introduced in late 1960s and become the predominant technique for estimating the basic parameters of speech [MAK 75].", "startOffset": 181, "endOffset": 189}, {"referenceID": 13, "context": "The mechanical vibrations are then transuded into activity signals on the auditory nerves corresponding to a feature extraction process [KIM 99].", "startOffset": 136, "endOffset": 144}, {"referenceID": 11, "context": "In fact the question of the imitation of the human auditory system characteristics for ASR has been subject of discussion and some researches believe that the analysis based on the effective peripheral auditory processing is the most robust front end in noise [HER 98].", "startOffset": 260, "endOffset": 268}, {"referenceID": 8, "context": "Perceptual Linear Prediction -PLP Hermansky [HER 90] introduced a new technique, perceptual linear predictive (PLP) analysis.", "startOffset": 44, "endOffset": 52}, {"referenceID": 12, "context": "Cepstral variance normalization -CVN Cepstral variance normalization is also known as the mean and variance normalization (MVN) [JAI 01] because CVN is often used in conjunction with CMS.", "startOffset": 128, "endOffset": 136}, {"referenceID": 10, "context": "RASTA-filtering Rasta-filtering was proposed for robust speech recognition by Hermansky and Morgan [HER 94].", "startOffset": 99, "endOffset": 107}, {"referenceID": 5, "context": "Feature warping Also known as cumulative distribution mapping [CHOI 06].", "startOffset": 62, "endOffset": 71}, {"referenceID": 17, "context": "It consists of mapping the observed cepstral feature distribution to a predefined target distribution over a sliding window with zero mean and unit variance [PEL 03].", "startOffset": 157, "endOffset": 165}, {"referenceID": 17, "context": "This feature processing technique has successfully been applied to speaker recognition because it is robust to channel mismatch, additive noise and to some extent, nonlinear effects attributed to handset distortion [PEL 03] [BAR 03].", "startOffset": 215, "endOffset": 223}, {"referenceID": 0, "context": "This feature processing technique has successfully been applied to speaker recognition because it is robust to channel mismatch, additive noise and to some extent, nonlinear effects attributed to handset distortion [PEL 03] [BAR 03].", "startOffset": 224, "endOffset": 232}, {"referenceID": 19, "context": "GMM-UBM baseline System The baseline system was a GMM UBM [REY 95].", "startOffset": 58, "endOffset": 66}, {"referenceID": 20, "context": "And a target speaker model is adapted by Bayesian adaptation MAP [REY 00] from the UBM by adjusting the UBM means.", "startOffset": 65, "endOffset": 73}, {"referenceID": 3, "context": "All Gaussian means vectors are pooled together to get one GMM Supervector [CAM 06].", "startOffset": 74, "endOffset": 82}, {"referenceID": 21, "context": "In our case, SVM is applied in the GMMs means supervector space [SCH 96] as shown in figure 3.", "startOffset": 64, "endOffset": 72}, {"referenceID": 4, "context": "The Kernel in equation (8), (9) and scoring are implemented using the library LIBSVM [CHA 01].", "startOffset": 85, "endOffset": 93}], "year": 2012, "abstractText": "The speech feature extraction has been a key focus in robust speech recognition research; it significantly affects the recognition performance. In this paper, we first study a set of different features extraction methods such as linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC) and perceptual linear prediction (PLP) with several features normalization techniques like rasta filtering and cepstral mean subtraction (CMS). Based on this, a comparative evaluation of these features is performed on the task of text independent speaker identification using a combination between gaussian mixture models (GMM) and linear and non-linear kernels based on support vector machine (SVM).", "creator": "Microsoft Word - TrabelsiArticle.doc"}}}