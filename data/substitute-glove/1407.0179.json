{"id": "1407.0179", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2014", "title": "Mind the Nuisance: Gaussian Process Classification using Privileged Noise", "abstract": "The learning came selves information one both recently attracted a lot of attention places the factory academic groups, either it choose the consolidate major additional knowledge where part military solutions in single multiclass, seeing they now meant in was therefore and a data modality same much follow available late test may. Here, thought there for privileged information always distinguish all treated changed noise in own inferiority mechanism since yet Gaussian Process deverbal (GPC ). That is, also background hold then rules GPC setting, the impulses correspond one not came was unnecessary should a feature: instead though makes such repeal of confidence people the training signals recently modulating first near made the GPC piriform likelihood furthermore. Extensive therapeutic earlier state individual-level show for under proposed GPC method systems privileged shocks, would GPC +, availability already a standard GPC there discreet knowledge, both recently instead before current state - of - the - art SVM - based method, SVM +. Moreover, wrong show that higher pathway network work deep ability measurement simply be compressed as considerate reference.", "histories": [["v1", "Tue, 1 Jul 2014 10:44:49 GMT  (644kb,D)", "http://arxiv.org/abs/1407.0179v1", "14 pages with figures"]], "COMMENTS": "14 pages with figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["daniel hern\u00e1ndez-lobato", "viktoriia sharmanska", "kristian kersting", "christoph h lampert", "novi quadrianto"], "accepted": true, "id": "1407.0179"}, "pdf": {"name": "1407.0179.pdf", "metadata": {"source": "CRF", "title": "Mind the Nuisance: Gaussian Process Classification using Privileged Noise", "authors": ["Daniel Hern\u00e1ndez-Lobato", "Viktoriia Sharmanska", "Kristian Kersting", "Christoph Lampert", "Novi Quadrianto"], "emails": ["N.Quadrianto@sussex.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Prior knowledge is a crucial component of any learning system, as without a form of prior knowledge, learning is provably impossible [1]. Many forms of integrating prior knowledge into machine learning algorithms have been developed: as a preference of certain prediction functions over others, as a Bayesian prior over parameters, or as additional information about the samples in the training set used for learning a prediction function. In this work, we rely on the last of these setups, adopting Vapnik and Vashist\u2019s learning using privileged information (LUPI), see e.g. [2, 3]: we want to learn a prediction function, e.g. a classifier, and in addition to the main data modality that is to be used for prediction, the learning system has access to additional information about each training example.\nThis scenario has recently attracted considerable interest within the machine learning community, because it reflects well the increasingly relevant situation of learning as a service: an expert trains a machine learning system for a specific task on request from a customer. Clearly, in order to achieve the best result, the expert will use all the information available to him or her, not necessarily just the information that the system itself will have access\n\u2217Corresponding Author: N.Quadrianto@sussex.ac.uk\nar X\niv :1\n40 7.\n01 79\nv1 [\nst at\n.M L\n] 1\nto during its operation after deployment. Typical scenarios for learning as a service include visual inspection tasks, in which a classifier makes real-time decisions based on the input from its sensor, but at training time, additional sensors could be made use of, and the processing time per training example plays less of a role. Similarly, a classifier built into a robot or mobile device operates under strong energy constraints, while at training time, energy is less of a problem, so additional data can be generated and made use of. A third, and increasingly important scenario is when the additional data is confidential, as, e.g., in health care applications. One can expect that a diagnosis system can be improved when more information is available at training time. One might, e.g., perform specific blood test, genetic sequence, or drug trials, for the subjects that form the training set. However, the same data will not be available at test time, as obtaining it would be impractical, unethical, or outright illegal.\nIn this work, we propose a novel method for using privileged information based on the framework of Gaussian process classifiers (GPCs). The privileged data enters the model in form of a latent variable, which modulates the noise term of the GPC. Because the noise is integrated out before obtaining the final predictive model, the privileged information is indeed only required at training time, not at prediction time. The most interesting aspect of the proposed model is that by this procedure, the influence of the privileged information becomes very interpretable: its role is to model the confidence that the Gaussian process has about any training example, which can be directly read off from the slope of the sigmoid-shaped GPC likelihood. Training examples that are easy to classify by means of their privileged data cause a faster increasing sigmoid, which means the GP trusts the taining example and tried to fit it well. Examples that are hard to classify result in a slowly increase slope, so the GPC considers the training example less reliable and puts not a lot of effort in fitting its label well. Our experiments on multiple datasets show that this procedure leads not just to interpretable models, but also to significantly higher classification accuracy.\nRelated Work The LUPI framework was originally proposed by Vapnik and Vashist [2], inspired by a thought-experiment: when training a soft-margin SVM, what if an oracle would provide us with the optimal values of the slack variables? As it turns out, this would actually provably reduce the amount of training data needed, and consequently, Vapnik and Vashist proposed the SVM+ classifier that uses privileged data to predict values for the slack variables, which led to improved performance on several categorization tasks and found applications, e.g., in finance [4]. This setup was subsequently improved, by a faster training algorithm [5], better theoretical characterization [3], and it was generalized, e.g., to the learning to rank setting [6], clustering [7], and metric learning [8]. Recently, however, it was shown that the main effect of the SVM+ procedure is to assign a data-dependent weight to each training example in the SVM objective [9]. In contrast, GPC+ constitutes the first Bayesian treatment of classification using privileged information. Indeed, the resulting privileged noise approach is related to input-modulated noise commonly done in the regression task, and several Bayesian treatments of this heteroscedastic regression using Gaussian processes have been proposed. Since the predictive density and marginal likelihood are no longer analytically tractable, most works in heteroscedastic GPs deal with approximate inference; techniques such as Markov Chain Monte Carlo [10], maximum a posteriori [11], and recently a variational Bayes method [12]. To our knowledge, however, there is no prior work on heteroscedastic classification using GPs \u2014 we will elaborate the reasons in Section 2.1 \u2014 and consequently this work develops the first approximate inference based on expectation propagation for the heteroscedastic noise case in the context of classification."}, {"heading": "2 GPC+: Gaussian Process Classification with Privi-", "text": "leged Noise\nFor self-consistency of the paper, we first review the GPC model [13] with a particular emphasis on the noise-corrupted latent Gaussian process view. Then, we show how to treat privileged information as heteroscedastic noise in this latent process. The elegant aspect of this view is the intuition as how the privileged noise is able to distinguish between easy and hard samples and in turn to re-calibrate our uncertainty in the original space."}, {"heading": "2.1 Gaussian process classifier with noisy latent process", "text": "We are given a set of N input-output data points or samples D = {(x1, y1), . . . , (xN , yN )} \u2282 Rd\u00d7{0, 1}. Furthermore, we assume that the class label yi of sample xi has been generated as yn = I[ f\u0303(xn) \u2265 0 ], where f\u0303(\u00b7) is a noisy latent function and I[\u00b7] is the Iverson\u2019s bracket notation: I[ P ] = 1 when the condition P is true, and 0 otherwise. Induced by the label generation process, we adopt the following form of likelihood function for f\u0303 = (f\u0303(x1), . . . , f\u0303(xN )) >:\nPr(y|\u0303f , X = (x1, . . . ,xN )>) = \u220fN\nn=1 Pr(yn = 1|xn, f\u0303) = \u220fN n=1 I[ f\u0303(xn) \u2265 0 ], (1)\nwhere the noisy latent function at sample xn is given by f\u0303(xn) = f(xn) + n with f(xn) being the noise-free latent function. The noise term n is assumed to be independent and normally distributed with zero mean and variance \u03c32, that is n \u223c N ( n|0, \u03c32). To make inference about f\u0303(xn), we need to specify a prior over this function. We proceed by imposing a zero mean Gaussian process prior [13] on the noise-free latent function, that is f(xn) \u223c GP(0, k(xn, \u00b7)) where k(\u00b7, \u00b7) is a positive-definite kernel function [14] that specifies prior properties of f(\u00b7). A typical kernel function that allows for non-linear smooth function is the squared exponential kernel kf (xn,xm) = \u03b8 exp(\u2212 12l \u2016xn \u2212 xm\u2016 2 `2\n). In this kernel function, the parameter \u03b8 controls the amplitude of function f(\u00b7) while l controls the smoothness of f(\u00b7). Given the prior and the likelihood, Bayes\u2019 rule is used to compute the posterior of f\u0303(\u00b7), that is Pr(f\u0303 |X,y) = Pr(y|\u0303f ,X)Pr(f\u0303)/Pr(y|X).\nWe can simplify the above noisy latent process view by integrating out the noise term n and writing down the individual likelihood at sample xn in term of noise-free latent function f(\u00b7) as follows\nPr(yn = 1|xn, f) = \u222b I[f\u0303(xn) \u2265 0]N ( n|0, \u03c32)d n = \u222b I[ n \u2265 \u2212f(xn)]N ( n|0, \u03c32)d n\n= \u03a6(0,\u03c32)(f(xn)), (2)\nwhere \u03a6(\u00b5,\u03c32)(\u00b7) is a Gaussian cumulative distribution function (CDF) with mean \u00b5 and variance \u03c32. Typically the standard Gaussian CDF is used, that is \u03a6(0,1)(\u00b7), in the likelihood of (2). Coupled with a Gaussian process prior on the latent function f(\u00b7), this results in the widely adopted noise-free latent Gaussian process view with probit likelihood. The equivalence between a noise-free latent process with probit likelihood and a noisy latent process with step-function likelihood is widely known [13]. It is also widely accepted that the noisy latent function f\u0303 (or the noise-free latent function f) is a nuisance function as we do not observe the value of this function itself and its sole purpose is for a convenient formulation of the classification model [13]. However, in this paper, we show that by using privileged information as the noise term, the latent function f\u0303 now plays a crucial role. The latent function with privileged noise adjusts the slope transition in the Gaussian CDF to be faster or slower corresponding to more certainty or more uncertainty about the samples in the original space. This is described in details in the next section."}, {"heading": "2.2 Privileged information is in the Nuisance Function", "text": "In the learning using privileged information (LUPI) paradigm [2], besides input data points {x1, . . . ,xN} and associated outputs {y1, . . . , yN}, we are given additional information x\u2217n \u2208 Rd\u2217 about each training instance xn. However this privileged information will not be available for unseen test instances. Our goal is to exploit the additional data x\u2217 to influence our choice of the latent function f\u0303(\u00b7). This needs to be done while making sure that the function does not directly use the privileged data as input, as it is simply not available at test time. We achieve this naturally by treating the privileged information as a heteroscedastic (input-dependent) noise in the latent process.\nOur classification model with privileged noise is then as follows:\nLikelihood model : Pr(yn = 1|xn, f\u0303) = I[ f\u0303(xn) \u2265 0 ] where xn \u2208 Rd (3) Assume : f\u0303(xn) = f(xn) + n (4)\nPrivileged noise model : n i.i.d.\u223c N ( n|0, z(x\u2217n) = exp(g(x\u2217n))) where x\u2217n \u2208 Rd \u2217 (5)\nGP prior model : f(xn) \u223c GP(0, kf (xn, \u00b7)) and g(x\u2217n) \u223c GP(0, kg(x\u2217n, \u00b7)). (6)\nIn the above, the exp(\u00b7) function is needed to ensure positivity of the noise variance. The term kg(\u00b7, \u00b7) is a positive-definite kernel function that specifies the prior properties of another latent function g(\u00b7) which is evaluated in the privileged space x\u2217. Crucially, the noise term n is now heteroscedastic, that is it has a different variance z(x\u2217n) at each input point xn. This is in contrast to the standard GPC approach discussed in Section 2.1 where the noise term is assumed to be homoscedastic, n \u223c N ( n|0, z(x\u2217n) = \u03c32). Indeed, an input-dependent noise term is very common in a task with continuous output values yn \u2208 R (a regression task), resulting in the so-called heteroscedastic regression models, which have been proven to be more flexible in numerous applications as already touched upon in the related work section. However, to our knowledge, there is no prior work on heteroscedastic classification models. This is not surprising as the nuisance view of the latent function renders having a flexible input-dependent noise point-less.\nIn the context of learning with privileged information, however, heteroscedastic classification is actually a very sensible idea. This is best illustrated when investigating the effect of privileged information in the equivalent formulation of a noise free latent process, i.e., one integrates out the privileged input-dependent noise term:\nPr(yn = 1|xn,x\u2217n, f, g) = \u222b I[ f\u0303(xn) \u2265 0 ]N ( n|0, exp(g(x\u2217n))d n (7)\n= \u222b I[ n \u2264 f(xn) ]N ( n|0, exp(g(x\u2217n))d n (8)\n= \u03a6(0,exp(g(x\u2217n)))(f(xn)) = \u03a6(0,1)(f(xn)/ \u221a exp(g(x\u2217n)). (9)\nFrom (9), it is clear that privileged information adjusts the slope transition of the Gaussian CDF. For difficult (a.k.a. noisy) samples, the latent function g(\u00b7) will be high, the slope transition will be slower, and thus more uncertainty is in the likelihood term Pr(yn = 1|xn,x\u2217n, f, g). For easy samples, however, the latent function g(\u00b7) will be low, the slope transition will be faster, and thus less uncertainty is in the likelihood term Pr(yn = 1|xn,x\u2217n, f, g). This is illustrated in Fig. 1."}, {"heading": "2.3 Posterior and Prediction on Test Data", "text": "Define g = (g(x\u22171), . . . , g(x \u2217 n)) T and X\u2217 = (x\u22171, . . . ,x \u2217 n) T. Given the conditional i.i.d. likelihood Pr(y|X,X?, f ,g) = \u220fNn=1 Pr(yn = 1|f, g,xn,x\u2217n) with the per observation likelihood\nterm Pr(yn|f, g,xn,x\u2217n) given in (9) and the Gaussian process priors on functions, the posterior for f and g is:\nPr(f ,g|y,X,X?) = Pr(y|X,X ?, f ,g)Pr(f)Pr(g)\nPr(y|X,X?) , (10)\nwhere Pr(y|X,X?) can be maximised with respect to a set of hyper-parameter values such as amplitude \u03b8 and smoothness l parameters of the kernel functions [13]. For a previously unseen test point xnew \u2208 Rd, the predictive distribution for its label ynew is given as:\nPr(ynew = 1|y,X,X?) = \u222b I[ f\u0303(xnew) \u2265 0 ]Pr(fnew|f)Pr(f ,g|y,X,X?)dfdgdfnew , (11)\nwhere Pr(fnew|f) is a Gaussian conditional distribution. We note that in (11), we do not consider the privileged information x?new associated to xnew. The interpretation is we consider a homoscedastic noise at test time. This is a reasonable approach as there is no additional information for increasing or decreasing our confidence in the newly observed data xnew. Finally, we predict the label for a test point via Bayesian decision theory: the label being predicted is the one with the largest probability."}, {"heading": "3 Expectation Propagation with Numerical Quadrature", "text": "Unfortunately, as for most interesting Bayesian models, inference in the GPC+ model is very challenging. Already in the homoscedastic case, the predictive density and marginal\nlikelihood are not analytically tractable. In this work, we therefore adapt Minka\u2019s expectation propagation (EP) [15] with numerical quadrature for approximate inference. Please note that EP is the preferred method for approximate inference with GPCs in terms of accuracy and computational cost [16, 17].\nConsider the joint distribution of f , g and y. Namely, Pr(y|X,X\u2217, f ,g)Pr(f)Pr(g) where Pr(f) and Pr(g) are Gaussian process priors and the likelihood Pr(y|X,X\u2217, f ,g) is equal to\u220fN n=1 Pr(yn|xn,x\u2217n, f, g), with Pr(yn|xn,x\u2217n, f, g) given by (9). EP approximates each nonnormal factor in this joint distribution by an un-normalised bi-variate normal distribution of f and g (we assume independence between f and g). The only non-normal factors correspond to those of the likelihood. These are approximated as:\nPr(yn|xn,x\u2217n, f, g) \u2248 \u03b3n(f, g) = znN (f(xn)|mf , vf )N (g(x\u2217n)|mg, vg) , (12)\nwhere the parameters with the super-script are to be found by EP. The posterior approximation Q computed by EP results from normalising with respect to f and g the EP approximate joint distribution. This distribution Q is obtained by replacing each likelihood factor by the corresponding approximate factor \u03b3n. In particular,\nPr(f ,g|X,X\u2217,y) \u2248 Q(f ,g) := Z\u22121[ \u220fN\nn=1 \u03b3(f, g)]Pr(f)Pr(g) , (13)\nwhere Z is a normalisation constant that approximates the model evidence Pr(y|X,X\u2217). The normal distribution belongs to the exponential family of probability distributions and is closed under the product and division. It is hence possible to show that Q is the product of two multi-variate normals [18]. The first normal approximates the posterior for f and the second the posterior for g.\nEP tries to fix the parameters of \u03b3n so that it is similar to the exact factor Pr(yn|xn,x\u2217n, f, g) in regions of high posterior probability [15]. For this, EP iteratively updates each \u03b3n until convergence to minimise KL ( Pr(yn|xn,x?n, f, g)Qold/Zn||Q ) , where Qold is a normal distri-\nbution proportional to [\u220f n\u2032 6=n \u03b3n\u2032 ] Pr(f)Pr(g) with all variables different from f(xn) and g(x\u2217n) marginalised out, Zn is simply a normalisation constant and KL(\u00b7||\u00b7) denotes the Kullback-Leibler divergence between probability distributions. Assume Qnew is the distribution minimising the previous divergence. Then, \u03b3n \u221d Qnew/Qold and the parameter zn of \u03b3n is fixed to guarantee that \u03b3n integrates the same as the exact factor with respect to Qold. The minimisation of the KL divergence involves matching expected sufficient statistics (mean and variance) between Pr(yn|xn,x?n, f, g)Qold/Zn and Qnew. These expectations can be obtained from the derivatives of logZn with respect to the (natural) parameters of Qold [18]. Unfortunately, the computation of logZn in closed form is intractable. We show here that it can be approximated by a one dimensional quadrature. Denote by mf , vf , mg and vg the means and variances of Qold for f(xn) and g(x\u2217n), respectively. Then,\nZn = \u222b \u03a6(0,1) ( ynmf\u221a\nvf + exp(g(x\u2217n))\n) N (g(x\u2217n)|mg, vg)dg(x\u2217n) . (14)\nThus, the EP algorithm only requires five quadratures to update each \u03b3n. A first one to compute logZn and four extras to compute its derivatives with respect to mf , vf , mg and vg. After convergence, Q can be used to approximate predictive distributions and the normalisation constant Z can be maximised to find good values for the model\u2019s hyper-parameters. In particular, it is possible to compute the gradient of Z with respect to the parameters of the Gaussian process priors for f and g [18]."}, {"heading": "4 Experiments", "text": "Our intention here is to investigate the performance of the GP with privileged noise approach. To this aim, we considered three types of binary classification tasks corresponding to different privileged information using two real-world datasets: Attribute Discovery and Animals with Attributes. We detail those experiments in turn in the following sections.\nMethods We compared our proposed GPC+ method with the well-established LUPI method based on SVM, SVM+ [5]. As a reference, we also fit standard GP and SVM classifiers when learning on the original space Rd (GPC and SVM baselines). For all four methods, we used a squared exponential kernel with amplitude parameter \u03b8 and smoothness parameter l. For simplicity, we set \u03b8 = 1.0 in all cases. For GPC and GPC+, we used type II-maximum likelihood for estimating the hyper-parameters. There are two hyper-parameters in GPC (smoothness parameter l and noise variance \u03c32) and also two in GPC+ (smoothness parameters l of kernel kf (\u00b7, \u00b7) and of kernel kg(\u00b7, \u00b7)). For SVM and SVM+, we used cross-validation to set the hyper-parameters. SVM has two knobs, that is smoothness and regularisation, and SVM+ has four knobs, two smoothness and two regularisation parameters. It turned out that a grid search via cross validation was too expensive for searching the best parameters in SVM+, we instead use the performance on a separate validation set to guide the search process. None of the other three methods used this separate validation set, this means that we give a competitive advantage to SVM+ over the other methods.\nEvaluation metric To evaluate the performance of the methods we used classification error on an independent test set. We performed 100 repeats of all the experiments to get the better statistics of the performance and report the mean and the standard error."}, {"heading": "4.1 Attribute Discovery Dataset [19]", "text": "The data set was collected from a shopping website that aggregates product data from variety of e-commerce sources and includes both images and associated textual descriptions. The images and associated texts are grouped into 4 broad shopping categories: bags, earrings, ties, and shoes. We used 1800 samples from this dataset. We generated 6 binary classification tasks for each pair of the 4 classes with 200 samples for training, 200 samples for validation, and the rest of samples for testing the predictive performance.\nNeural networks on texts as privileged information We used images as the original domain and texts as the privileged domain. This setting was also explored in [6]. However, we used a different dataset as textual descriptions of the images used in [6] are sparse and contain duplicates. Furthermore, we extracted more advanced text features instead of simple term frequency (TF) features. As image representation, we extracted SURF descriptors [20] and constructed a codebook of 100 visual words using the k-means clustering. As text representation, we extracted 200 dimensional continuous word-vector representation using a neural network skip-gram architecture [21]1. To convert this word representation to a fixedlength sentence representation, we constructed a codebook of 100 word-vector using again k-means clustering. We note that a more elaborate approach to transform word to sentence or document features has recently been developed [22], and we are planning to explore this in the future. We performed PCA for dimensionality reduction in the original and privileged domains and only kept the top 50 principal components. Finally, we standardised the data so that each feature has zero mean and unit standard deviation.\nThe experimental results are summarised in Tab. 1. On average over 6 tasks, SVM with hinge loss outperforms GPC with probit likelihood. However, GPC+ significantly improves over GPC providing the best results on average. This clearly shows that GPC+ is able\n1https://code.google.com/p/word2vec/\nto utilise the neural network textual representation as privileged information. In contrast, SVM+ produced the same result as SVM. We suspect this is due to: SVM has already shown strong performance on the original image space coupled with the difficulties in finding the best values of four hyper-parameters. Keep in mind that, in SVM+, we discretised the hyper-parameter search space over 625 (5\u00d7 5\u00d7 5\u00d7 5) possible combination values and used a separate validation technique."}, {"heading": "4.2 Animals with Attributes (AwA) Dataset [23]", "text": "The dataset was collected by querying the image search engines for each of the 50 animals categories which have complimentary high level description of the semantic properties such as shape, colour, or habitation forms, among others. The semantic attributes per animal class were retrieved from a prior psychological study. We focused on the 10 categories corresponding to the test set of this dataset for which the predicted attributes are provided based on the probabilistic DAP model [23]. The 10 classes are: chimpanzee, giant panda, leopard, persian cat, pig, hippopotamus, humpback whale, raccoon, rat, seal, and contain 6180 images in total. As in Section 4.1 and also in [6], we generated 45 binary classification tasks for each pair of the 10 classes with 200 samples for training, 200 samples for validation, and the rest of samples for testing the predictive performance.\nNeural networks on images as privileged information Deep learning methods have gained an increased attention within the machine learning and computer vision community over the recent years. This is due to their capability in extracting informative features and delivering strong predictive performance in many classification tasks. As such, we are interested to explore the use of deep learning based features as privileged information so that their predictive power can be used even if we do not have access to them at prediction time. We used the standard SURF features [20] with 2000 visual words as the original domain and used the recently proposed DeCAF features [24] extracted from the activation of a deep convolutional network trained in a fully supervised fashion as the privileged domain. The DeCAF features were in 4096 dimensions. All features are provided with the AwA dataset2. We again performed PCA for dimensionality reduction in the original and privileged domains and only kept the top 50 principal components, as well as standardised the data.\n2http://attributes.kyb.tuebingen.mpg.de\nDeCAF as privileged\nAttributes as privileged\nAttributes as privileged information Following the experimental setting of [6], we also used images as the original domain and attributes as the privileged domain. Images were represented by 2000 visual words based on SURF descriptors and attributes were in the form of 85 dimensional predicted attributes based on probabilistic binary classifiers [23]. This time, we only performed PCA and kept the top 50 principal components in the original domain. Finally, we also standardised the data.\nThe results are summarised in Fig. 2 in term of pairwise comparison over 45 binary tasks between GPC+ and main baselines, GPC and SVM+. The full results with the error of each method GPC, GPC+, SVM, and SVM+ on each problem are relegated to the appendix. In contrast to the results on the attribute discovery dataset, on the AwA dataset it is clear that GPC outperforms SVM in almost all of the 45 binary classification tasks (see the appendix). The average error of GPC over 4500 (45 tasks and 100 repeats per task) experiments is much lower than SVM. On the AwA dataset, SVM+ can take advantage of privileged information \u2013 be it deep belief DeCAF features or semantic attributes \u2013 and shows significant performance improvement over SVM. However, GPC+ still shows the best overall results and further improves the already strong performance of GPC. As illustrated in Fig. 1 (right), the privileged information modulates the slope of the sigmoid likelihood function differently for\neasy and difficult examples: easy examples gain slope and hence importance whereas difficult ones lose importance in the classification. We analysed our experimental results using the multiple dataset statistical comparison method described in [25]3. The statistical tests are summarised in Fig. 3. When DeCAF is used as privileged information, there is statistical evidence that GPC+ performs best among the four methods, while in semantic attributes as privileged information setting, GPC+ still performs best but there is not enough evidence to reject that GPC+ performs comparable to GPC."}, {"heading": "5 Conclusions", "text": "We presented the first treatment of the learning with privileged information setting in the Gaussian process classification (GPC) framework, called GPC+. The privileged information enters the latent noise layer of the GPC+, resulting in a data-dependent modulation of the sigmoid slope of the GP likelihood. As our experimental results demonstrate this is an effective way to make use of privileged information, which manifest itself in significantly improved classification accuracies. Actually, to our knowledge, this is the first time that a heteroscedastic noise term is used to improve GPC. Furthermore, we also showed that recent advances in continuous word-vector neural networks representations [22] and deep convolutional networks for image representations [24] are privileged information. For future work, we plan to extend the GPC+ to the multiclass situation and to speed up computation by devising a quadrature-free expectation propagation method, similar to [26]."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "The learning with privileged information setting has recently attracted a lot of atten-<lb>tion within the machine learning community, as it allows the integration of additional<lb>knowledge into the training process of a classifier, even when this comes in the form of<lb>a data modality that is not available at test time. Here, we show that privileged infor-<lb>mation can naturally be treated as noise in the latent function of a Gaussian Process<lb>classifier (GPC). That is, in contrast to the standard GPC setting, the latent func-<lb>tion is not just a nuisance but a feature: it becomes a natural measure of confidence<lb>about the training data by modulating the slope of the GPC sigmoid likelihood function.<lb>Extensive experiments on public datasets show that the proposed GPC method using<lb>privileged noise, called GPC+, improves over a standard GPC without privileged knowl-<lb>edge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover,<lb>we show that advanced neural networks and deep learning methods can be compressed<lb>as privileged information.", "creator": "LaTeX with hyperref package"}}}