{"id": "1408.6617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2014", "title": "Task-group Relatedness and Generalization Bounds for Regularized Multi-task Learning", "abstract": "In important paper, thinking study seen generalization surprisingly thus self-funding integration - task course (RMTL) place rather topology - valued facilitate, place MTL is existence actually a creative essentially years formula_17 - fortune functions. We not increased concerned to two thesis questions: 1) now actually affect ask RMTL perform better close making smaller operation produce fixed 80 STL? =) under done conditions what RMTL were/are taken can required present consistency as than task during simultaneous learning?", "histories": [["v1", "Thu, 28 Aug 2014 03:27:27 GMT  (26kb)", "http://arxiv.org/abs/1408.6617v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chao zhang", "dacheng tao", "tao hu", "xiang li"], "accepted": false, "id": "1408.6617"}, "pdf": {"name": "1408.6617.pdf", "metadata": {"source": "CRF", "title": "Task-group Relatedness and Generalization Bounds for Regularized Multi-task Learning", "authors": ["Chao Zhang", "Dacheng Tao", "Tao Hu", "Xiang Li"], "emails": ["chao.zhang@dlut.edu.cn).", "dacheng.tao@gmail.com).", "hutaomath@foxmail.com).", "lixiangalixiang@gmail.com)."], "sections": [{"heading": null, "text": "ar X\niv :1\n40 8.\n66 17\nv1 [\ncs .L\nG ]\n2 8\nA ug\nKeywords: multi-task learning, generalization bound, task relatedness, consistency, vectorvalued function"}, {"heading": "1 Introduction", "text": "There is plenty of empirical evidence to suggest that task-relatedness information improves multitask learning (MTL) over single-task learning (STL) in multiple related task (MRT) scenarios.\n\u2217C. Zhang is with the School of Mathematical Sciences, Dalian University of Technology, Dalian, Liaoning, 116024, P.R. China. (e-mail: chao.zhang@dlut.edu.cn).\n\u2020D. Tao is with the Centre for Quantum Computation & Intelligent Systems, FEIT, University of Technology, Sydney, NSW 2007, Australia. (e-mail: dacheng.tao@gmail.com).\n\u2021T. Hu is with the School of Mathematical Sciences, Capital Normal University, Beijing, 100048 , P.R. China. (e-mail: hutaomath@foxmail.com).\n\u00a7X. Li is with the School of Mathematical Sciences, Dalian University of Technology, Dalian, Liaoning, 116024, P.R. China. (e-mail: lixiangalixiang@gmail.com).\nTherefore, capturing relatedness information is important for both theoretical and practical investigations of MTL.\nSeveral learning methods have been proposed to address this problem. Evgeniou et al. [10] introduced regularized MTL to link the simultaneous learning process of MRT scenarios to STL problems, in which the regularization terms encode the relatedness between MRTs. However, regularization term design relies on a priori knowledge of tasks. Other methods that model task relatedness let the different tasks share common structures, e.g., backpropagation networks [7] and the structure learning formulation [2]. Argyriou et al. [3] presented a method to learn a lowdimensional representation shared across MRTs, while Zhang and Yeung [23] applied covariance to model three types of relatedness between two tasks: the positive correlation, the negative correlation, and unrelatedness. From the theoretical standpoint, the notion \u201cF -related\u201d has been proposed to study the generalizability of multi-task classification, where if two tasks Z [1],Z [2] are F -related for a given function class F , there exists a function f \u2208 F such that P [1] = f(P [2]) or P [2] = f(P [1]) [4, 5]. The interested reader is also referred to other theoretical investigations of MTL [16, 17] and learning theory [19, 8, 1, 24, 13, 12]."}, {"heading": "1.1 Overview of Main Results", "text": "As discussed by Micchelli and Pontil [20, 21], MTL can be studied from the viewpoint of vectorvalued function learning. Inspired by [20, 21], we explore the vector-valued framework to study the generalization and consistency properties of regularized MTL (RMTL) and analyze the relationship between the properties of RMTL and task-group relatedness. In particular, we address the following theoretical questions:\n\u2022 Under what conditions does RMTL perform better with a smaller task sample size than STL?\n\u2022 Under what conditions is RMTL generalizable and can guarantee the consistency of each task during simultaneous learning?\nIn order to answer these questions, we also need to consider: 1) measures of task-group relatedness; 2) the joint probability of MRTs; 3) measures of vector-valued function classes; and 4) the specific deviation and symmetrization inequalities for the vector-valued framework.\nHere, we introduce two types of task-group relatedness: the observed discrepancy-dependence measure (ODDM) and the empirical discrepancy-dependence measure (EDDM) (see Section 3).1 ODDM measures the statistical dependence between events that some tasks have large observed discrepancies and the others have small observed discrepancies. EDDM measures the statistical dependence between events that some tasks have large empirical discrepancies and the others have small empirical discrepancies. In contrast to ODDM, EDDM reflects the asymptotic behavior of the relatedness between two task groups when the sample size goes to infinity.2 We show that ODDM (or EDDM) can exist in three states: negative, positive, and zero, which respectively model three types of relatedness between two task groups: the synergy effect, the negative synergy effect, and unrelatedness.\n1In this paper, the observed discrepancy is defined as the discrepancy between an observation and its expectation, and the empirical discrepancy is defined as the discrepancy between the expectation (i.e., expected risk) and its empirical estimate (i.e., empirical risk).\n2For convenience, we assume that all tasks have the same sample size in this paper.\nSince MTL refers to a process in which MRTs are simultaneously processed, we consider the task joint probability, defined in (4), instead of the task summation probability as in [16, 17, 2, 11]. In task joint probability, the generalization bound for MTL is deemed to be the upper bound of the joint probability that there is at least one task with a large empirical discrepancy in MTL. This bound can also be used to describe the consistency of each task in the MTL learning process. In order to obtain the bound, we present the specific deviation inequalities and the symmetrization inequalities for the vector-valued framework and, meanwhile, introduce the Cartesian product-based uniform entropy number (CPUEN), which is induced from the uniform entropy numbers (UENs) of MRTs.\nBased on the resulting generalization bounds, the theoretical properties of RMTL are analyzed and we show that:\n\u2022 the validity of RMTL will theoretically be guaranteed if most of the relatedness between two task groups show a synergy effect. If almost any pair of task groups are predominantly mutual, RMTL performs well with less samples than STL, and the required sample size of each task in RMTL will not increase dramatically, regardless of the (large) number of MRTs (see Remarks 5.1&5.2).\n\u2022 there will be a tighter generalization bound for RMTL if the values of EDDMs are negative, i.e., if most of the relatedness between two task groups show a synergy effect. Moreover, we present a sufficient condition to guarantee the consistency of each task in RMTL.\nFurthermore, we obtain the following theoretical findings:\n\u2022 The aforementioned sufficient condition can be used to examine whether the given tasks, function classes, and regularization terms are suitable for MTL.\n\u2022 The existence of a negative correlation between two tasks is necessary for MTL, which is in accordance with the argument by Zhang and Yeung [23].\n\u2022 The generalization bound of RMTL.\n\u2022 The relationship between the task relatedness and the generalization performance of RMTL.\n\u2022 The sufficient condition to guarantee the consistency of each task in RMTL.\n\u2022 The proposed vector-valued framework can be used to study the theoretical properties of vector-valued function learning [21]"}, {"heading": "1.2 Organization of the Paper", "text": "The rest of this paper is organized as follows. In Section 2, the main research addressed in this paper, including the task-joint probability and generalization bounds for RMTL, is formalized. In Section 3, two quantities for measuring task-group relatedness are presented and CPUEN is introduced in Section 4 to measure the complexity of the vector-valued function classes. The main results are presented in Section 5, along with a method to examine the validity of MTL. In Section 6, we address the generalization performance results using the covariance information of MRTs and the last section concludes the paper. In Appendix, we first present the deviation inequalities and the symmetrization inequalities for the vector-valued framework (Parts A & B). Finally, the proofs of the main results are given in Part C."}, {"heading": "2 Problem Setup", "text": "We first formalize the main research addressed in this paper, including the task-joint probability and generalization bounds for RMTL."}, {"heading": "2.1 Regularized Multi-task Learning", "text": "Given a space X \u2282 RI , let X [m] be the input space of the m-th task with the probability distribution D[m] on X and Y [m] \u2208 RJ be the corresponding output space (1 \u2264 m \u2264 M). Let g [m] \u2217 : X [m] \u2192 Y [m] be the corresponding labeling function. Also, denote the m-th task as Z [m] := X [m] \u00d7Y [m] \u2282 RK with K = I + J . In MTL, let G[1], \u00b7 \u00b7 \u00b7 ,G[M ] \u2282 YX be M function classes corresponding to the learning tasks Z [1], \u00b7 \u00b7 \u00b7 ,Z [M ], respectively. MTL is expected to simultaneously find M functions g\u0303[1], \u00b7 \u00b7 \u00b7 , g\u0303[M ] from G[1], \u00b7 \u00b7 \u00b7 ,G[M ] such that each g\u0303[m] can minimize the expected risk of the corresponding task Z [m] over G[m]:\nE[m](\u2113[m] \u25e6 g[m]) = \u222b \u2113[m](g[m](x[m]),y[m])dP[m](z[m]), 1 \u2264 m \u2264 M, (1)\nwhere \u2113[m] and P [m](z[m]) are the loss function and the probability distribution of the task Z [m], respectively, with z[m] := (x[m],y[m])T .\nSince the task distributions P [1], \u00b7 \u00b7 \u00b7 , P [M ] are usually unknown, the target functions g\u0303[1], \u00b7 \u00b7 \u00b7 , g\u0303[M ]\ncannot be directly obtained by minimizing the expected risks (1) of MRTs. Instead, the empirical risk minimization (ERM) principle can be used to handle this issue. For each task Z [m], let Z [m] N := {z [m] n }Nn=1 be a set of N i.i.d. samples drawn from Z [m] with z [m] n := (x [m] n ,y [m] n )T . The following is the objective function of RMTL:\nM\u2211\nm=1\nE [m] N (\u2113 [m] \u25e6 g[m]) + rR(g[1], \u00b7 \u00b7 \u00b7 , g[M ]),\nwhere\nE [m] N (\u2113\n[m] \u25e6 g[m]) := 1\nN\nN\u2211\nn=1\n\u2113[m](g(x[m]n ),y [m] n ), (2)\nis the empirical risk of the task Z [m], R(g[1], \u00b7 \u00b7 \u00b7 , g[M ]) is the regularization term that is designed to encode the relatedness information between MRTs and r > 0 is the regularization parameter.\nAlternatively, and as mentioned by Kakade et al. [14], the above regularized optimization can be equivalently rewritten as\nmin R(g[1],\u00b7\u00b7\u00b7 ,g[M])\u2264c\nM\u2211\nm=1\nE [m] N (\u2113 [m] \u25e6 g[m]),\nwhere, instead of exploiting the regularization, a hard restriction R(g[1], \u00b7 \u00b7 \u00b7 , g[M ]) \u2264 c is set to combine the function classes G[1], \u00b7 \u00b7 \u00b7 ,G[M ], which shrinks the original search space G to GRc . 3\n3For example, if g[m](x[m]) = x[m] for any 1 \u2264 m \u2264 M , the original search space G is the M -dimensional real\nspace RM . Then, by setting the restriction \u2211M\nm=1(x [m])2 \u2264 c2, the original space G will become anM -dimensional\nsphere GRc with radius c.\nTherefore, a proper regularization term R(g) can correctly encode the relatedness between MRTs, reduce the computational cost, and improve the generalization performance. However, this design relies on a prior knowledge of the MRTs.\nFrom the vector-valued function learning perspective [20, 21], RMTL aims to find a vector-\nvalued function gN = (g [1] N , \u00b7 \u00b7 \u00b7 , g [M ] N ) T by simultaneously solving the M optimization problems:\nmin g\u2208GRc\n{ E [m] N (\u2113 [m] \u25e6 g[m]), 1 \u2264 m \u2264 M } , (3)\nwhere min g\u2208GRc stands for a component-wise minimum operator defined in Section 2.2."}, {"heading": "2.2 Notations of Vector Operations", "text": "For the discussion that follows, it is first necessary to describe some notations of vector operations. Given two vectors, x = (x[1], \u00b7 \u00b7 \u00b7 , x[M ])T and y = (y[1], \u00b7 \u00b7 \u00b7 , y[M ])T , let |x| := (|x[1]|, \u00b7 \u00b7 \u00b7 , |x[M ]|)T and denote the expression x > y (resp. x \u2265 y) as x[m] > y[m] (resp. x[m] \u2265 y[m]) for any 1 \u2264 m \u2264 M . Similarly, we denote x < y (resp. x \u2264 y) as x[m] < y[m] (resp. x[m] \u2264 y[m]) for any 1 \u2264 m \u2264 M .\nFurthermore, given (a[1], \u00b7 \u00b7 \u00b7 , a[M ])T \u2208 RM , we define the component-wise supremum operator\nsup g\u2208G\n{ (g[1](a[1]), \u00b7 \u00b7 \u00b7 , g[M ](a[M ]))T }\nwith g = (g[1], \u00b7 \u00b7 \u00b7 , g[M ])T as follows: if the vector-valued function g\u2020 = (g [1] \u2020 , \u00b7 \u00b7 \u00b7 , g [M ] \u2020 ) T achieves the supremum over G, each component g [m] \u2020 of the vector g\u2020 achieves the supremum sup\ng[m]\u2208G[m] {g[m](a[m])}\nover G[m]. Similarly, we define the component-wise minimum operator as\nmin g\u2208G\n{ (g[1](a[1]), \u00b7 \u00b7 \u00b7 , g[M ](a[M ]))T } ."}, {"heading": "2.3 Task-joint Probability and Generalization Bounds", "text": "In general, the generalization bounds for STL refer to the upper bounds of the supremum\nsup g\u2208G |E(\u2113 \u25e6 g)\u2212 EN (\u2113 \u25e6 g)|\nwith an alternative probability expression\nPr { sup g\u2208G |E(\u2113 \u25e6 g)\u2212 EN(\u2113 \u25e6 g)| > \u03be } ,\nwhose upper bound describes the rarity of the event that the empirical discrepancy between the expected risk E(\u2113 \u25e6 g) and the empirical risk EN(\u2113 \u25e6 g) is larger than a given positive constant \u03be.\nSince MRTs are processed simultaneously in MTL, the following task-joint probability is straightforward: for any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0,\nPr    sup g\u2208GRc      |E[1](\u2113[1] \u25e6 g[1])\u2212 E[1]N (\u2113 [1] \u25e6 g[1])| ...\n|E[M ](\u2113[M ] \u25e6 g[M ])\u2212 E[M ]N (\u2113 [M ] \u25e6 g[M ])|\n     6\u2264   \u03be[1] ... \u03be[M ]      , (4)\nwhich describes the rarity of the event in RMTL that there is at least one task Z [m] with empirical discrepancy larger than the constant \u03be[m]. The upper bound of (4) is the so-called \u201cgeneralization bound\u201d for RMTL. Compared to the STL bound, the RMTL bound (4) not only reflects the generalization performance of each task, but also the dependence between simultaneously learned tasks, i.e., how the success (or failure) of some tasks affects the performance of the others.\nFor convenience, we further define the loss function class:\nF [m] := {z[m] 7\u2192 \u2113[m](g[m](x[m]),y[m]) : g[m] \u2208 G[m]}, 1 \u2264 m \u2264 M ; (5)\nthe Cartesian product F := F [1] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 F [M ] is called the \u201cvector-valued function class\u201d in the rest of this paper. Similarly, based on the regularized vector-valued function class GRc , we define the regularized loss vector-valued function class by\nFRc := { (\u2113[1] \u25e6 g[1], \u00b7 \u00b7 \u00b7 , \u2113[M ] \u25e6 g[M ])T : (g[1], \u00b7 \u00b7 \u00b7 , g[M ])T \u2208 GRc } , (6)\nwhich is also termed the regularized vector-valued function class in the remainder of this paper. Briefly, we denote for any f := (f [1], \u00b7 \u00b7 \u00b7 , f [M ])T \u2208 F ,\nE[m]f [m] := \u222b f [m](z[m])dP[m](z[m]) ; E\n[m] N f\n[m] := 1\nN\nN\u2211\nn=1\nf [m](z[m]n ), (7)\nand the generalization bound (4) is equivalently rewritten as Pr { sup f\u2208FRc { |Ef \u2212 ENf |} 6\u2264 \u03be } with\nEf := (E[1]f [1], \u00b7 \u00b7 \u00b7 ,E[M ]f [M ])T\nand EN f := (E [1] N f [1], \u00b7 \u00b7 \u00b7 ,E[M ]N f [M ])T ."}, {"heading": "3 Measures of Task-group Relatedness", "text": "Some existing works on task relatedness already describe the relationship between two individual tasks, for instance the F -related [5, 4] notion and covariances [23]. In MTL, it is also necessary to consider the relationship between two task groups. Here, we present two measures of taskgroup relatedness: the observed discrepancy-dependence measure (ODDM) and the empirical discrepancy-dependence measure (EDDM)."}, {"heading": "3.1 ODDM", "text": "In probability theory, the dependence between two events A and B can be detected using the quantity Pr{A|B}\u2212Pr{A}, where A and B are positively dependent if the conditional probability Pr{A|B} of A given B is greater than the probability Pr{A} (i.e., Pr{A|B} \u2212 Pr{A} > 0), and they are negatively dependent if the inequality is reversed [6, 22]. We introduce ODDM and EDDM to measure the relatedness between two task groups in MTL, based on the quantity.\nDefinition 3.1 Given M tasks Z [1], \u00b7 \u00b7 \u00b7 ,Z [M ] and a regularized vector-valued function class FRc , let \u039b := {1, \u00b7 \u00b7 \u00b7 ,M} be an index set and \u039b[m] be a subset of \u039b with the cardinality of m. For any \u039b[m] \u2282 \u039b and any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0, ODDM is defined as\n\u03c6F(\u039b [m], \u03be) := sup\nf\u2208FRc\n{ Pr { A\u039b[m] \u2223\u2223B\u039b[m] } \u2212 Pr { A\u039b[m] }} ,\nwhere f = (f [1], \u00b7 \u00b7 \u00b7 , f [M ])T , \u039b[m] stands for the complementary set of \u039b[m] with \u039b[m] \u222a \u039b[m] = \u039b, and the events A\u039b[m] := {s [i] > \u03be[i]}i\u2208\u039b[m] and B\u039b[m] := {s [i] \u2264 \u03be[i]} i\u2208\u039b[m] w.r.t. the observed discrepancy s[i] := |E[i]f [i] \u2212 f [i](z[i])|\nof the task Z [m].\nAs defined above, ODDMmeasures the dependence between the events that the tasks in group \u039b[m] have large observed discrepancies and the tasks in \u039b[m] have small observed discrepancies. In fact, ODDM is determined by the inherent characteristics of MRTs, the selection of function classes and the regularization term. It can exist in one of three states:\n\u2022 a positive ODDM implies that some functions in the search space FRc will result in a negative synergy effect between the tasks {Z [i]}\ni\u2208\u039b[m] and the others {Z [i]}i\u2208\u039b[m], i.e., the\nsuccess of tasks {Z [i]} i\u2208\u039b[m] will benefit from a performance loss in the others {Z [i]}i\u2208\u039b[m];\n\u2022 a negative ODDM means that all functions in FRc will effect the synergy effect on the simultaneous learning process for MRTs, i.e., the success of the tasks {Z [i]}\ni\u2208\u039b[m] contributes\nto improved performance of the others {Z [i]}i\u2208\u039b[m] ;\n\u2022 a zero ODDM reflects that some functions inFRc eliminate the relatedness between {Z [i]}i\u2208\u039b[m]\nand {Z [i]} i\u2208\u039b[m] , and the others will effect synergy effect between the two groups."}, {"heading": "3.2 EDDM", "text": "Since this paper focuses on ERM-based RMTL, we also need to consider the asymptotic behavior of the dependence between two task groups when the sample size N goes to infinity.\nDefinition 3.2 Following the notations in Definition 3.1 and letting Z [m] N := {z [m] n }Nn=1 be N i.i.d. samples drawn from each task Z [m] (1 \u2264 m \u2264 M), EDDM is defined as\n\u03d5N FRc (\u039b[m], \u03be) := Pr { AN\u039b[m] \u2223\u2223BN\u039b[m] } \u2212 Pr { AN\u039b[m] } ,\nwhere the events AN \u039b[m] := {t[i]N > \u03be [i]}i\u2208\u039b[m] and B N \u039b[m] := {t[i]N \u2264 \u03be [i]} i\u2208\u039b[m] with the empirical discrepancy t [i] N := sup\nf\u2208Prj[i](FRc )\n|E[i]f \u2212 E[i]Nf |, (8)\nw.r.t. the sample set Z [m] N drawn from Z [m], and Prj[i](FRc ) stands for the projection of the regularized vector-valued function class FRc onto the function class F [i].\nNote that EDDM measures the dependence between the generalization performances of the two task groups and also has three states:\n\u2022 a positive EDDM implies that the successfully learned tasks {Z [i]} i\u2208\u039b[m] benefit from a loss\nin generalization performance of the others {Z [i]}i\u2208\u039b[m];\n\u2022 a negative EDDM means that the task groups {Z [i]}i\u2208\u039b[m] and {Z [i]} i\u2208\u039b[m] are mutually\nbeneficial;\n\u2022 a zero EDDM with N < \u221e signifies that the two groups are unrelated."}, {"heading": "3.3 Empirically Computing ODDM and EDDM", "text": "By the facts that Pr{A|B} = Pr{A,B}/Pr{B} and Pr{A} = E1{A}, ODDM \u03c6F(\u039b [m], \u03be) can be empirically computed in the following way. Letting {z[m]n }Nn=1 be i.i.d. samples drawn from the task Z [m] (1 \u2264 m \u2264 M), we denote \u03b6j (1 \u2264 j \u2264 J), \u03b7k (1 \u2264 k \u2264 K) and \u03b8p (1 \u2264 p \u2264 P ) as the observations of the events A\u039b[m] \u2227 B\u039b[m] , A\u039b[m] and B\u039b[m] , respectively. Then, an empirical version of ODDM \u03c6F(\u039b [m], \u03be) is given by:\n\u03c6\u0302F(\u039b [m], \u03be) := sup\nf\u2208FRc\n{ J\u22121 \u2211J j=1 1{\u03b6j}\nP\u22121 \u2211P p=1 1{\u03b8p} \u2212K\u22121\nK\u2211\nk=1\n1{\u03b7k}\n} , (9)\nwhere the expected risk E[i]f [i] in s[i] is approximated by its empirical version E [i] Nf [i].\nRecalling the term t [i] N defined in (8), EDDM \u03d5 N FRc (\u039b[m], \u03be) can be approximately computed in the following way. First, fix the sample set {z[i]n }Nn=1 of each task Z [i] (1 \u2264 i \u2264 M) and replace the expected risk E[i]f with the fixed empirical quantity E [i] Nf w.r.t. {z [i] n }Nn=1. Next, we randomly select L samples from of each task Z [i] to form another empirical risk E[i]L f and denote t\u0302 [i] L := sup\nf\u2208Prj[i](FRc )\n|E[i]L f \u2212 E [i] Nf | as an estimate of t [i] N . Denote the events AL := {t\u0302 [i] L > \u03be [i]}i\u2208\u039b[m]\nand BL := {t\u0302 [i] L \u2264 \u03be [i]} i\u2208\u039b[m]\n. Let \u03b6j (1 \u2264 j \u2264 J), \u03b7k (1 \u2264 k \u2264 K) and \u03b8p (1 \u2264 p \u2264 P ) be the observations of the events AL \u2227 BL, AL and BL respectively. We then can empirically compute EDDM \u03d5N\nFRc (\u039b[m], \u03be) as\n\u03d5\u0302N FRc\n(\u039b[m], \u03be) := J\u22121\n\u2211J j=1 1{\u03b6j}\nP\u22121 \u2211P p=1 1{\u03b8p} \u2212K\u22121\nK\u2211\nk=1\n1{\u03b7k}. (10)\nRemark 3.1 There are two difficulties to implement this method to empirically compute ODDM and EDDM:\n\u2022 In general, it is hard to capture the observations of the task-joint events.\n\u2022 If the task number M is large, it is highly time-consuming to compute the empirical estimates of ODDM and EDDM for any \u039b[m]. To reduce the complexity, one feasible way is to cluster the tasks according to the similarity and select a representative task from each cluster to compute ODDM and EDDM."}, {"heading": "4 Cartesian Product-based Uniform Entropy Numbers", "text": "Complexity measures of function classes play an important role in learning theory. Since this paper studies MTL in the vector-valued framework, the classical measures such as the VapnikChervonenkis (VC) dimension and the covering number, are not applicable (or at least cannot be directly applied) to the vector-valued scenario. For example, Ben-David and Borbely [4] applied an extended version of the VC dimension to study the generalization properties of multi-task classification.\nHere, we introduce the Cartesian product-based uniform entropy number (CPUEN) to measure the complexity of the vector-valued function classes. First, we briefly outline the definitions of the covering number and uniform entropy number (UEN) of the scalar-valued function classes. Regarding further details, please refer to Mendelson [18].\nDefinition 4.1 Let F be a function class and d be a metric on F . For any \u03be > 0, the covering number of F at radius \u03be w.r.t. the metric d, denoted by N (F , \u03be, d) is the minimum size of a cover of radius \u03be. Furthermore, given a sample set ZN := {zn}Nn=1 drawn from Z, we denote Z\u2032N := {z \u2032 n} N n=1 as the ghost sample set drawn from Z, such that the ghost sample z \u2032 n has the same distribution as zn for any 1 \u2264 n \u2264 N . Denote Z2N := {ZN ,Z\u2032N}. Setting the metric d as the \u2113p(Z2N) (p > 0) norm, UEN is defined by\nlnNp(F , \u03be, N) := sup ZN lnN (F , \u03be, \u2113p(ZN )) . (11)\nRecall that the vector-valued function class F is a Cartesian product of the function classes F [1], \u00b7 \u00b7 \u00b7 ,F [M ], i.e., F := F [1] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 F [M ]. For each F [m] (1 \u2264 m \u2264 M), let Z\u0303[m]N be the sample set achieving the supremum\nsup Z\n[m] N \u2208(Z [m])N\nlnN ( F [m], \u03be[m], \u2113p(Z [m] N ) ) (12)\nand \u2126 [m] p,N be one of the covers of F [m] related to the supremum w.r.t. the norm \u2113p(Z\u0303 [m] N ). Therefore, the Cartesian product \u2126 [1] p,N \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u2126 [M ] p,N is also a cover of F with the radius vector \u03be := (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T . Following the above notations, we define the CPUEN of the vector-valued function class F as follows:\nDefinition 4.2 Given a vector-valued function class F , consider a Cartesian product-based cover of the vector-valued function F :\n\u2126p,N(F , \u03be) := { AMp \u2208 \u2126 [1] p,N \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u2126 [M ] p,N : A M p \u2229F 6= \u2205 } .\nThen, CPUEN of F is defined as lnN p(F , \u03be, N) := ln |\u2126p,N(F , \u03be)|.\nIn contrast to the classical UEN [see (11)], CPUEN is induced from the cover of the function class F [m] of each task Z [m] (1 \u2264 m \u2264 M) with different norms and radiuses instead of introducing a uniform norm in the vector-valued function space F . Although CPUEN is usually larger than the uniform-norm UEN of the vector-valued function class F , the induction setting of CPUEN has a stronger relationship with the prior information-based design of the regularization term and offers convenience to the theoretical analysis of RMTL."}, {"heading": "5 Generalization Bounds of RegularizedMulti-task Learn-", "text": "ing\nIn this section, we present the generalization bounds of RMTL and discuss how the task-group relatedness affects the generalization properties of RMTL. Moreover, we give a sufficient condition for the consistency of each task in MRTs."}, {"heading": "5.1 Two Special Cases", "text": "Before the formal discussion, we first bound the probabilities of two special events: first, that all tasks have large empirical discrepancies and second, that all tasks have small empirical discrepancies.\nTheorem 5.1 Assume that FRc is a regularized vector-valued function class w.r.t. the constant c, and Z [m] N = {z [m] n }Nn=1 is the set of N i.i.d. samples drawn from the task Z\n[m] (1 \u2264 m \u2264 M). Let \u039b := {1, \u00b7 \u00b7 \u00b7 ,M} be an index set and denote \u039b[m] as a subset of \u039b with the cardinality of m. Denote Z [m] 2N := {Z [m] N ,Z \u2032[m] N }. Given \u03be = (\u03be [1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0 and for any N \u2208 N such that N \u2265 8\u0393(\u039b) 1\u22122\u03a5(\u039b) , it then holds that\nPr { sup f\u2208FRc \u2223\u2223Ef \u2212 ENf \u2223\u2223 > \u03be } \u2264 2M+2N 1 ( FRc , \u03be/8, 2N ) exp { \u2212N \u2211M m=1(\u03be [m])2 32M2(b\u2212 a)2 } , (13)\nwhere\n\u0393(\u039b) :=\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b m(b\u2212 a)2\u2211 i\u2208\u039b[m] (\u03be[i])2 , (14)\nand\n\u03a5(\u039b) :=\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n\u03c6F(\u039b [m], \u03be). (15)\nThis theorem shows that if it holds that N \u2265 8\u0393(\u039b) 1\u22122\u03a5(\u039b)\n, the probability of sup f\u2208FRc\n\u2223\u2223Ef\u2212EN f \u2223\u2223 > \u03be\ncan be bounded by the RHS of (13). Note that if M = 1, since \u03c6F(\u039b [m], \u03be) equals zero, the quantity \u03a5(\u039b) is zero and the bound (13) coincides with the classical result of STL (see Theorem 2.3 of [18]).\nRemark 5.1 In the case of M > 1, the condition N \u2265 8\u0393(\u039b) 1\u22122\u03a5(\u039b) should be satisfied when the quantity \u03a5(\u039b) < 1/2: namely, it is necessary for RMTL to satisfy the condition that the taskgroup relatedness between MRTs should mostly be synergistic. Furthermore, RMTL will perform well with less samples than STL size N \u2265 8(b\u2212a) 2\n\u03be2 if the condition \u03a5(\u039b) \u2264 (1\u22122M\u22121) holds, which\nimplies that almost any pair of task groups \u039b[m] and \u039b[m] predominantly promote mutually.4\n4Actually, letting \u03be0 := min{\u03be [1], \u00b7 \u00b7 \u00b7 , \u03be[M ]} and N0 :=\n8(b\u2212a)2\n\u03be2 0\n, we have 8\u0393(\u039b) < (2M \u2212 1)N0. Thus, the\ncondition N \u2265 8\u0393(\u039b)1\u22122\u03a5(\u039b) holds if N is larger than (2M\u22121)N0 1\u22122\u03a5(\u039b) . We can then infer that each task in RMTL will need less samples than the task in STL if the condition 2 M\u22121\n1\u22122\u03a5(\u039b) < 1 holds.\nRemark 5.2 If \u03be = \u03be[1] = \u00b7 \u00b7 \u00b7 = \u03be[M ] and each ODDM \u03c6F(\u039b[m], \u03be) reaches the minimum value \u22121, the sample size N of each task should be larger than the value 8(b\u2212a) 2\n((2M\u22121)\u22121+2)\u03be2 (M > 1) to\nsupport the inequality (13). This implies that the required sample size of each task in RMTL will approach half of the STL value 8(b\u2212 a)2/\u03be2 at the rate of 2\u2212M as M \u2192 \u221e. This finding shows that if the relationship between any pair of task groups \u039b[m] and \u039b[m] is predominantly synergistic, each task in RMTL needs less samples than STL and the required sample size N in RMTL will not increase dramatically, regardless of a large number of MRTs.\nWe next consider the second special case and present an upper bound of the probability that all tasks have small empirical discrepancies in the simultaneous learning process for MRTs. The following theorem is proved by using the small-deviation techniques [15].\nTheorem 5.2 Following the notations in Theorem 5.1, it then holds that for any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0,\nPr { sup f\u2208FRc \u2223\u2223Ef \u2212EN f \u2223\u2223 \u2264 \u03be } \u2264 2M sup f\u2208FRc Pr { s \u2264 2\u03be } , (16)\nwhere s = ( s[1], \u00b7 \u00b7 \u00b7 , s[M ] )T with s[m] := \u2223\u2223E[m]f [m] \u2212 f [m](z[m]) \u2223\u2223 for any 1 \u2264 m \u2264 M .\nThis theorem converts the case of small empirical discrepancies into a simple case, where the LHS of (16) can be bounded by using the probability that the observed discrepancy of each task Z [m] is smaller than 2\u03be[m] (1 \u2264 m \u2264 M). Compared to the case of empirical discrepancies, the RHS of (16) is only determined by the inherent characteristics of MRTs, e.g., the distributions of tasks, the selection of function classes, and the regularization term."}, {"heading": "5.2 Main Results", "text": "Based on these two special cases, we obtain the generalization bounds of RMTL and a sufficient condition for the consistency of each task in the simultaneous learning process for MRTs.\nTheorem 5.3 Following the notations of Theorem 5.1, given \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0 and for\nany N \u2208 N such that N \u2265 max 1\u2264m\u2264M max \u039b[m]\u2282\u039b\n8\u0393(\u039b[m])\n1\u22122\u03a5(\u039b[m]) , it then holds that\nPr { sup f\u2208FRc \u2223\u2223Ef \u2212EN f \u2223\u2223 6\u2264 \u03be } \u2264 M\u2211 m=1 \u2211\n\u039b[m]\u2282\u039b\n2mPr {{ s[\u03bb] \u2264 2\u03be[\u03bb] } \u03bb\u2208\u039b[m] }\n\u00d7  \u03d5NFRc (\u039b [m], \u03be) + 2m+2N 1 ( Prj FRc \u039b[m] , \u03be\u039b[m] 8 , 2N ) exp    \u2212N \u2211 \u03bb\u2208\u039b[m] (\u03be[\u03bb])2 32M2(b\u2212 a)2      , (17)\nwhere Prj FRc\n\u039b[m] stands for the projection of FRc on the subspace \u220f \u03bb\u2208\u039b[m] F [\u03bb], \u03be\u039b[m] := ( \u03be[\u03bb] ) \u03bb\u2208\u039b[m] ,\n\u0393(\u039b[m]) and \u03a5(\u039b[m]) are defined in (14). Furthermore, if it is satisfied that for any 1 \u2264 m \u2264 M and \u03bb[m] \u2282 \u039b,\nlim N\u2192+\u221e \u03d5N FRc\n( \u039b[m], \u03be ) = lim\nN\u2192+\u221e lnN 1\n( Prj FRc\n\u039b[m] , \u03be\u039b[m]\n8 , 2N\n) = 0, (18)\nit then holds that lim\nN\u2192+\u221e Pr { sup f\u2208FRc \u2223\u2223Ef \u2212 EN f \u2223\u2223 6\u2264 \u03be } = 0. (19)\nIn this theorem, we obtain an upper bound of the joint probability of the event that supf\u2208FRc \u2223\u2223Ef\u2212 EN f\n\u2223\u2223 6\u2264 \u03be and show that the consistency of each task in MTL can be guaranteed if condition (18) is valid. We are concerned with two aspects of the theorem:\n\u2022 the RHS of (17) implies that given N < \u221e, a smaller value of EDDM \u03d5N FRc (\u039b[m], \u03be) will lead\nto a sharper bound, which is in accordance with the argument that the negative EDDM means that the task groups benefit from each other (see Section 3).\n\u2022 The asymptotic convergence of the generalization bound is determined by two factors: 1)\nEDDM \u03d5N FRc\n( \u039b[m], \u03be ) ; and 2) CPUEN lnN 1 ( Prj FRc\n\u039b[m] , \u03be\u039b[m]/8, 2N\n) . In particular, according\nto the classical results of STL (see Theorem 2.3 & Definition 2.5 of [18]), if UEN for each task Z [m] satisfies that lnN1(F [m],\u03be[m]/8,2N)\nN converges to zero when N goes to infinity, the\nsecond equality of (18) holds. Note that the convergence of \u03d5N FRc\n( \u039b[m], \u03be ) is determined\nby the inherent characteristics of MRTs, e.g., distributions of tasks, selection of function classes, and regularization terms.\nRemark 5.3 Moreover, these theoretical findings cause us to preliminarily examine whether the combination of tasks, function classes, and regularization terms is suitable for the ERM-based RMTL according to the rules that\n\u03a5(\u039b) =\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n\u03c6F(\u039b [m], \u03be) <\n1 2 ,\nand lim\nN\u2192+\u221e \u03d5N FRc\n( \u039b[m], \u03be ) = 0\nwith \u03d5N FRc (\u039b[m], \u03be) \u2264 0 for any \u039b[m] \u2282 \u039b (1 \u2264 m \u2264 M)."}, {"heading": "6 Generalization Bounds with Covariance Information", "text": "As discussed in Section 3, since ODDM detects the dependence between two task groups, the bound (13) cannot reflect how the individual relatedness between two tasks affects the generalization performance of RMTL for more than two tasks. Here, we consider the generalization results based on the covariance information between every two tasks.\nTheorem 6.1 Follow the notations of Theorem 5.1. Given \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0 and for any N \u2208 N such that\nN \u2265 8\u03932\n1\u2212 2(\u03a5(\u039b) + \u03a52) , (20)\nthen there holds that\nPr { sup f\u2208FRc \u2223\u2223Ef \u2212 ENf \u2223\u2223 > \u03be } \u2264 2M+2N 1 ( FRc , \u03be/8, 2N ) exp { \u2212N \u2211M m=1(\u03be [m])2 32M2(b\u2212 a)2 } , (21)\nwhere\n\u03932 :=\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\nm(b\u2212 a)2 ( \u2211\ni\u2208\u039b[m] \u03be[i]\n)2 , (22)\nand\n\u03a52 := M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n8 \u2211\ni1<i2 i1,i2\u2208\u039b[m]\nCovF (i1, i2)\n( \u2211 i\u2208\u039b[m] \u03be[i] )2 . (23)\nCompared to Theorem 5.1, the condition (20) incorporates the quantity \u03a52 which is related to the covariance information. Actually, the quantity \u03a52 is derived by replacing \u2211 i\u2208\u039b[m] (\u03be[i])2 with ( \u2211 i\u2208\u039b[m] \u03be[i] )2 as shown in the proof of Lemma B.2. From the condition (20), we can find that the bound (21) is valid when \u03a5(\u039b) + \u03a52 < 1/2, which means that if the synergetic effect is the main group relatedness in the learning process and some of the correlations between tasks are negative, the learning process will perform well with a small sample size N . Zhang and Yeung [23] have highlighted the necessity of the negative correlation and pointed out that the negative correlation is helpful to reduce the search space in MTL, which is in accordance with our theoretical findings.\nHowever, when M = 1, the bound (21) coincides with the canonical results in STL if and only if the quantity CovF (i, i) equals to zero, i.e., the random variable z of the task Z [i] takes a constant with the probability of one. Since this setting is far away from the practical scenario, unlike the result (17), the bound (21) that encodes covariance information cannot reflect the transition from STL to MTL."}, {"heading": "7 Conclusion", "text": "In this paper, we apply the vector-valued framework to study the generalization performance of RMTL and analyze the relationship between the task-group relatedness and the properties of RMTL. In particular, we introduce two types of task-group relatedness: ODDM and EDDM, and we present CPUEN to measure the complexity of the regularized vector-valued function class FRc . By applying the specific deviation and symmetrization inequalities to the vector-valued framework, we obtain the generalization bound for RMTL and provide a sufficient condition to guarantee the consistency of each task in the simultaneous learning process of MRTs. Finally, we show that the theoretical findings of this paper can examine whether the task settings are suitable for the RMTL mechanism\nBased on the theoretical findings, we summarize the relationship between the generalization properties of RMTL and the task-group relatedness as follows:\n\u2022 ODDM is related to the sample size and validity of RMTL (see Theorem 5.1). We first prove that the condition of \u03a5(\u039b) < 1\n2 is necessary for the validity of RMTL and then show\nthat if almost any pair of task groups \u039b[m] and \u039b[m] predominantly mutually promote, the required sample size N of each task in RMTL will be smaller than that of STL for each\ntask. The sample size will also not increase dramatically, regardless of a large number of MRTs (see Remarks 5.1 & 5.2).\n\u2022 EDDM affects the generalization performance of RMTL as follows: 1) a negative EDDM provides a sharper generalization bound; and 2) the asymptotic behavior of EDDM also affects the consistency of the task (see Theorem 5.3).\n\u2022 The existence of a negative correlation between two tasks is necessary for MTL, which is in accordance with the relevant argument of [23].\nIn summary, synergistic task-group relatedness is beneficial to the generalization performance of RMTL. In future works, we will focus on the practical applications of the theoretical findings, for instance by improving the empirical computations of ODDM and EDDM (see Remark 5.3) and designing the regularization term for RMTL based on the task-group relatedness."}, {"heading": "A Deviation Inequalities for Random Vectors", "text": "To obtain the generalization bounds for RMTL, we need to consider the deviation inequalities for random vectors. The following lemma is derived from [9].\nLet sn = (s [1] n , \u00b7 \u00b7 \u00b7 , s [M ] n )T \u2208 RM (1 \u2264 n \u2264 N) be N i.i.d. random vectors such that\nM\u2211\nm=1\ns[m]n \u2264 1, for n = 1, \u00b7 \u00b7 \u00b7 , N , (24)\nand s[m]n \u2265 0, for 1 \u2264 n \u2264 N and 1 \u2264 m \u2264 M . (25) Note that the components s [1] n , \u00b7 \u00b7 \u00b7 , s [M ] n of sn are not necessarily independent. The mean \u00b5 = (\u00b5[1], \u00b7 \u00b7 \u00b7 , \u00b5[M ])T of random vectors sn is expressed as\n\u00b5[m] = E[m]s[m]n , for 1 \u2264 m \u2264 M . (26)\nLemma A.1 For any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0 such that \u2211M\nm=1(\u00b5 [m] + \u03be[m]) < 1, then there\nholds that\nPr {\u2223\u2223\u2223 1 N N\u2211\nn=1\nsn \u2212 \u00b5 \u2223\u2223\u2223 > \u03be } \u2264 2M exp { \u22122N M\u2211\nm=1\n(\u03be[m])2 } . (27)\nMoreover, since the vector-valued function f has the range [a, b], let\ns[m]n := f [m](z [m] n )\u2212 a\nM(b \u2212 a) , 1 \u2264 n \u2264 N, 1 \u2264 m \u2264 M, (28)\nand then\nPr {\u2223\u2223EN f \u2212 Ef \u2223\u2223 > \u03be } = Pr {\u2223\u2223\u2223 1 N N\u2211\nn=1\nsn \u2212 Ef \u2212 a\nM(b\u2212 a) \u2223\u2223\u2223 > \u03be M(b\u2212 a)\n} , (29)\nwhere a = (a, \u00b7 \u00b7 \u00b7 , a)T \u2208 RM . Thus, the combination of Lemma A.1 and (29) leads to a Hoeffdingtype deviation inequality for random vectors.\nTheorem A.1 Given a bounded vector-valued function f = (f [1], \u00b7 \u00b7 \u00b7 , f [M ])T with the range [a, b], there holds that for any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ]) > 0,\nPr {\u2223\u2223EN f \u2212 Ef \u2223\u2223 > \u03be } \u2264 2M exp { \u22122N M\u2211\nm=1\n(\u03be[m])2\nM2(b\u2212 a)2\n} . (30)"}, {"heading": "B Symmetrization Inequalities for Random Vectors", "text": ""}, {"heading": "B.1 Chebyshev Inequalities for Random Vectors", "text": "Definition B.1 Assume that Z [1], \u00b7 \u00b7 \u00b7 ,Z [M ] are M distributions on R. Let \u039b := {1, \u00b7 \u00b7 \u00b7 ,M} be an index set and \u039b[m] be a subset of \u039b with the cardinality of m. For any \u039b[m] \u2282 \u039b and any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0, define\n\u03c8(\u039b[m], \u03be) :=Pr { {s[i] > \u03be[i]}i\u2208\u039b[m] \u2223\u2223{s[i] \u2264 \u03be[i]} i\u2208\u039b[m] } \u2212 Pr { {s[i] > \u03be[i]}i\u2208\u039b[m] } . (31)\nwhere s[i] is the non-negative random variable of the task Z [i], and \u039b[m] stands for the complementary set of \u039b[m] with \u039b[m] \u222a \u039b[m] = \u039b.\nLemma B.1 Let s = (s[1], \u00b7 \u00b7 \u00b7 , s[M ])T be a random vector with nonnegative elements and \u039b = {1, \u00b7 \u00b7 \u00b7 ,M} be an index set. For any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0, then there holds that\nPr {s 6\u2264 \u03be} \u2264 M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n \u03c8(\u039b[m], \u03be) + \u2211 i\u2208\u039b[m] E[i] { (s[i])2 }\n\u2211 i\u2208\u039b[m] (\u03be[i])2\n  , (32)\nwhere s 6\u2264 \u03be means that there is at least one index m \u2208 \u039b such that s[m] > \u03be[m], and \u039b[m] stands for an index set with the cardinality of m.\nLemma B.2 Let s = (s[1], \u00b7 \u00b7 \u00b7 , s[M ])T be a random vector with nonnegative elements and \u039b = {1, \u00b7 \u00b7 \u00b7 ,M} be an index set. For any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0, then there holds that\nPr {s 6\u2264 \u03be} \u2264 M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n  \u03c8(\u039b[m], \u03be) + \u2211 i\u2208\u039b[m] E[i] { (s[i])2 } + 2 \u2211 i<j i,j\u2208\u039b[m] E { s[i]s[j] }\n( \u2211 i\u2208\u039b[m] \u03be[i] )2\n  , (33)\nwhere s 6\u2264 \u03be means that there is at least one index m \u2208 \u039b such that s[m] > \u03be[m], and \u039b[m] stands for an index set with the cardinality of m."}, {"heading": "B.2 Symmetrization Inequalities", "text": "By applying ODDM, we can develop the symmetrization inequality for MTL as follows:\nTheorem B.1 Assume that F is a vector-valued function class with the range [a, b]. For any \u03be \u2265 0 such that\nN \u2265 8\u0393(\u039b)\n1\u2212 2\u03a5(\u039b) , (34)\nthen there holds that\nPr { sup f\u2208F \u2223\u2223Ef \u2212 EN f \u2223\u2223 > \u03be } \u2264 2Pr { sup f\u2208F \u2223\u2223E\u2032Nf \u2212 EN f \u2223\u2223 > \u03be 2 } , (35)\nwhere\n\u0393(\u039b) :=\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b m(b\u2212 a)2\u2211 i\u2208\u039b[m] (\u03be[i])2 , \u03a5(\u039b) :=\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n\u03c6F(\u039b [m], \u03be),\n\u039b = {1, \u00b7 \u00b7 \u00b7 ,M} is an index set and \u039b[m] is a subset of \u039b with the cardinality of m.\nThe following is the symmetrization result incorporating the covariance information between every two tasks.\nTheorem B.2 Assume that F is a vector-valued function class with the range [a, b]. For any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0 such that\nN \u2265 8\u03932\n1\u2212 2(\u03a5(\u039b) + \u03a52) , (36)\nthen there holds that\nPr { sup f\u2208F { |Ef \u2212EN f } | > \u03be } \u2264 2Pr { sup f\u2208F { |E\u2032Nf \u2212EN f | } > \u03be 2 } , (37)\nwhere\n\u03932 :=\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\nm(b\u2212 a)2 ( \u2211\ni\u2208\u039b[m] \u03be[i]\n)2 , \u03a52 := M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n8 \u2211 i1<i2\ni1,i2\u2208\u039b[m]\nCovF (i1, i2)\n( \u2211 i\u2208\u039b[m] \u03be[i] )2\nand CovF (i1, i2) is defined as\nCovF(i, j) := max (f [1],\u00b7\u00b7\u00b7 ,f [M])T\u2208F\nCov ( f [i](z[i]), f [j](z[j]) ) (38)\nwith z[i] and z[j] (1 \u2264 i, j \u2264 M) being the random variables of the tasks Z [i] and Z [j], respectively."}, {"heading": "C Proofs of Main Results", "text": ""}, {"heading": "C.1 Proof of Lemma A.1", "text": "Proof of Lemma A.1. Let t = \u2223\u2223 1 N \u2211N n=1 sn \u2212 \u00b5\n\u2223\u2223. The event |t| > \u03be contains 2M possibilities: for any 1 \u2264 m \u2264 M , there are m components of the vector t such that t[ik] > \u03be[ik] (1 \u2264 k \u2264 m) and the rest are of the case that t[ik] < \u2212\u03be[ik] (1 \u2264 k \u2264 M \u2212 m). For convenience, we also denote {Pi}2 M i=1 as the collection of all 2 M possibilities.\nAccording to Theorem 1 in [9], the following result is valid for any possibility Pi (1 \u2264 i \u2264 2M):\nPr {Pi} \u2264 M\u220f\nm=0\n( \u00b5[m]\np[m]\n)p[m]N , (39)\nwhere p[m] = \u00b5[m] + \u03be[m] (m = 1, \u00b7 \u00b7 \u00b7 ,M), \u00b50 = 1 \u2212 \u2211M m=1 \u00b5 [m] and p0 = 1 \u2212 \u2211M m=1 p\n[m]. Then, we have\nPr {\u2223\u2223\u2223 1 N N\u2211\nn=1\nsn \u2212 \u00b5 \u2223\u2223\u2223 > \u03be } \u2264 2M M\u220f\nm=0\n( \u00b5[m]\np[m]\n)p[m]N . (40)\nThen, consider\nM\u220f\nm=0\n( \u00b5[m]\np[m]\n)p[m]N =exp { N M\u2211\nm=0\np[m] log (\u00b5[m] p[m]\n)}\n=exp { N (( 1\u2212 M\u2211\nm=1\np[m] ) log\n(1\u2212 \u2211M\nm=1 \u00b5 [m]\n1\u2212 \u2211M\nm=1 p [m]\n) + M\u2211\nm=1\np[m] log (\u00b5[m] p[m]\n))}\n\u2264 exp { N ( M\u2211\nm=1\n( 1\u2212 p[m] ) log (1\u2212 \u00b5[m] 1\u2212 p[m] ) + M\u2211\nm=1\np[m] log (\u00b5[m] p[m]\n))} (\u2217)\n= exp { \u2212N M\u2211\nm=1\n\u222b p[m]\n\u00b5[m]\n( p[m]\nx \u2212\n1\u2212 p[m]\n1\u2212 x\n) dx\n}\n=exp { \u2212N M\u2211\nm=1\n\u222b p[m]\n\u00b5[m]\np[m] \u2212 x x(1 \u2212 x) dx\n}\n\u2264 exp { \u2212N M\u2211\nm=1\n4\n\u222b p[m]\n\u00b5[m] (p[m] \u2212 x)dx\n}\n=exp { \u22122N M\u2211\nm=1\n(p[m] \u2212 \u00b5[m])2 } = exp { \u22122N M\u2211\nm=1\n(\u03be[m])2 } , (41)\nbecause x(1\u2212x) \u2264 1/4 for any x \u2208 R, and the step (\u2217) is followed from the fact that the function f is subadditive if f is concave and f(0) \u2265 0."}, {"heading": "C.2 Proof of Lemma B.1", "text": "Proof of Lemma B.1. Given M tasks Z(1), \u00b7 \u00b7 \u00b7 ,Z [M ] and a vector-valued function class F , let \u039b := {1, \u00b7 \u00b7 \u00b7 ,M} be an index set and \u039b[m] be a subset of \u039b with the cardinality of m. For any \u039b[m] \u2282 \u039b and any \u03be = (\u03be(1), \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0, define\n\u03c8(\u039b[m], \u03be) :=Pr { {s[i] > \u03be[i]}i\u2208\u039b[m] \u2223\u2223{s[i] \u2264 \u03be[i]} i\u2208\u039b[m] } \u2212 Pr { {s[i] > \u03be[i]}i\u2208\u039b[m] } . (42)\nThen, the event s 6\u2264 \u03be contains the following possibilities:\n\u2022 P [1]: there is only one index {i} = \u039b[1] \u2282 \u039b satisfying that s[i] > \u03be[i];\n\u2022 P [m]: there are only m (1 < m < M) indices {i[1], \u00b7 \u00b7 \u00b7 , i[m]} = \u039b[m] \u2282 \u039b satisfying that s[ik] > \u03be[ik] (1 \u2264 k \u2264 m);\n\u2022 P [M ]: s[m] > \u03be[m] holds for any 1 \u2264 m \u2264 M .\nThus, we have Pr {s 6\u2264 \u03be} = Pr{P [1]}+ \u00b7 \u00b7 \u00b7+ Pr{P [M ]}. (43)\nAccording to Chebyshev\u2019s inequality and (42), we have\nPr{P [1]} = M\u2211\nm=1\n( \u03c8({m}, \u03be) + Pr{s[m] > \u03be[m]} ) \u2264 M\u2211\nm=1\n( \u03c8({m}, \u03be) + E{(s[m])2}\n(\u03be[m])2\n) , (44)\nand for any 2 \u2264 m \u2264 M ,\nPr{P [m]} = \u2211\n\u039b[m]\u2282\u039b\n( \u03c8(\u039b[m], \u03be) + Pr{s[i] > \u03be[i] : i \u2208 \u039b[m]} )\n= \u2211\n\u039b[m]\u2282\u039b\n( \u03c8(\u039b[m], \u03be) + Pr{(s[i])2 > (\u03be[i])2 : i \u2208 \u039b[m]} )\n\u2264 \u2211\n\u039b[m]\u2282\u039b\n \u03c8(\u039b[m], \u03be) + Pr    \u221a \u2211\ni\u2208\u039b[m]\n(s[i])2 >\n\u221a \u2211\ni\u2208\u039b[m]\n(\u03be[i])2     \n\u2264 \u2211\n\u039b[m]\u2282\u039b\n  \u03c8(\u039b[m], \u03be) + E { \u2211 i\u2208\u039b[m] (s[i])2 }\n\u2211 i\u2208\u039b[m] (\u03be[i])2\n \n= \u2211\n\u039b[m]\u2282\u039b\n \u03c8(\u039b[m], \u03be) + \u2211 i\u2208\u039b[m] E{s[i]}2\n\u2211 i\u2208\u039b[m] (\u03be[i])2\n  . (45)\nThe combination of (43), (44) and (45) leads to the result (32). This completes the proof."}, {"heading": "C.3 Proof of Lemma B.2", "text": "Proof of Lemma B.2. The event s 6\u2264 \u03be contains the following possibilities:\n\u2022 P [1]: there is only one index {i} = \u039b[1] \u2282 \u039b satisfying that s[i] > \u03be[i];\n\u2022 P [m]: there are only m (1 < m < M) indices {i[1], \u00b7 \u00b7 \u00b7 , i[m]} = \u039b[m] \u2282 \u039b satisfying that s[ik] > \u03be[ik] (1 \u2264 k \u2264 m);\n\u2022 P [M ]: s[m] > \u03be[m] holds for any 1 \u2264 m \u2264 M .\nThus, we have Pr {s 6\u2264 \u03be} = Pr{P [1]}+ \u00b7 \u00b7 \u00b7+ Pr{P [M ]}. (46)\nAccording to Chebyshev\u2019s inequality and (42), we have\nPr{P [1]} = M\u2211\nm=1\n( \u03c8({m}, \u03be) + Pr{s[m] > \u03be[m]} ) \u2264 M\u2211\nm=1\n( \u03c8({m}, \u03be) + E{(s[m])2}\n(\u03be[m])2\n) , (47)\nand for any 2 \u2264 m \u2264 M ,\nPr{P [m]} = \u2211\n\u039b[m]\u2282\u039b\n( \u03c8(\u039b[m], \u03be) + Pr{s[i] > \u03be[i] : i \u2208 \u039b[m]} )\n\u2264 \u2211\n\u039b[m]\u2282\u039b\n \u03c8(\u039b[m], \u03be) + Pr    \u2211\ni\u2208\u039b[m]\n(s[i]) > \u2211\ni\u2208\u039b[m]\n(\u03be[i])     \n\u2264 \u2211\n\u039b[m]\u2282\u039b\n  \u03c8(\u039b[m], \u03be) + E {( \u2211 i\u2208\u039b[m] s[i] )2 }\n( \u2211 i\u2208\u039b[m] \u03be[i] )2\n \n= \u2211\n\u039b[m]\u2282\u039b\n  \u03c8(\u039b[m], \u03be) + \u2211 i\u2208\u039b[m] E{(s[i])2}+ 2 \u2211 i,j\u2208\u039b[m] i<j E{s[i]s[j]}\n( \u2211 i\u2208\u039b[m] \u03be[i] )2\n  . (48)\nThe combination of (46), (47) and (48) leads to the result (33). This completes the proof."}, {"heading": "C.4 Proof of Theorem B.1", "text": "Proof of Theorem B.1. Let fN = (f\u0302 [1], \u00b7 \u00b7 \u00b7 , f\u0302 [M ])T be the vector-valued function achieving the supremum sup f\u2208F \u2223\u2223Ef \u2212EN f \u2223\u2223.\nAccording to the triangle inequality, we have\n|EfN \u2212EN fN | \u2212 |E \u2032 N fN \u2212 EfN | \u2264 |E \u2032 N fN \u2212 EN fN |, (49)\nand thus\n1{|EfN\u2212EN fN |>\u03be}1{|EfN\u2212E\u2032N fN |\u2264\u03be/2} =1{|EfN\u2212EN fN |>\u03be}\u2227{|E\u2032N fN\u2212EfN |\u2264\u03be/2}\n\u22641{|E\u2032N fN\u2212EN fN |>\u03be/2} . (50)\nTaking expectations with respect to the ghost samples gives\n1{|EfN\u2212EN fN |>\u03be}Pr \u2032 {\u2223\u2223EfN \u2212 E\u2032NfN \u2223\u2223 \u2264 \u03be\n2\n} \u2264Pr\u2032 {\u2223\u2223E\u2032N fN \u2212EN fN \u2223\u2223 > \u03be\n2\n} . (51)\nAccording to Lemma B.1, since the samples z [m] n (1 \u2264 m \u2264 M, 1 \u2264 n \u2264 M) are independent of\neach other, we have\nPr\u2032 {\u2223\u2223EfN \u2212 E\u2032N fN \u2223\u2223 6\u2264 \u03be 2 }\n=Pr      \u2223\u2223\u2223 \u2211N n=1 ( E[1]f\u0302 [1] \u2212 f\u0302 [1](z[1]n ) )\u2223\u2223\u2223 ...\u2223\u2223\u2223 \u2211N n=1 ( E[M ]f\u0302 [M ] \u2212 f\u0302 [M ](z[M ]n ) )\u2223\u2223\u2223   6\u2264   N\u03be[1] 2 ... N\u03be[M] 2     \n\u2264Pr      \u2211N n=1 \u2223\u2223E[1]f\u0302 [1] \u2212 f\u0302 [1](z[1]n ) \u2223\u2223\n...\u2211N n=1 \u2223\u2223E[M ]f\u0302 [M ] \u2212 f\u0302 [M ](z[M ]n ) \u2223\u2223\n  6\u2264   N\u03be[1] 2 ...\nN\u03be[M]\n2\n    \n\u2264 M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n \u03c6F(\u039b [m], \u03be) + N \u2211 i\u2208\u039b[m] E[i] {( E[i]f\u0302 [i] \u2212 f\u0302 [i](z[i]) )2} N2\n4 \u2211 i\u2208\u039b[m] (\u03be[i])2\n \n= M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n \u03c6F(\u039b[m], \u03be) + \u2211 i\u2208\u039b[m] 4Var[i] ( f\u0302 [i](z[i]) )\nN \u2211\ni\u2208\u039b[m] (\u03be[i])2\n  (\u2217)\n\u2264 M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n \u03c6F(\u039b[m], \u03be) + 4m(b\u2212 a)2\nN \u2211\ni\u2208\u039b[m] (\u03be[i])2\n  , (52)\nwhere the step (\u2217) is followed from the fact that for each task Z [m] (1 \u2264 m \u2264 M), the samples {z[m]n }Nn=1 are independent. Hence, we get\n1{|EfN\u2212EN fN |>\u03be}\n 1\u2212   M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n\u03c6F(\u039b [m], \u03be) +\n4m(b\u2212 a)2\nN \u2211\ni\u2208\u039b[m] (\u03be[i])2\n   \n\u2264Pr\u2032 {\u2223\u2223E\u2032N fN \u2212 EN fN \u2223\u2223 > \u03be 2 } . (53)\nTaking the expectation with respect to the sample collection {Z[m]N } M m=1 of the tasks Z [1], \u00b7 \u00b7 \u00b7 ,Z [M ] and letting\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n( \u03c6F(\u039b [m], \u03be) + 4m(b\u2212 a)2\nN \u2211\ni\u2208\u039b[m] (\u03be[i])2\n) \u2264 1\n2 , (54)\nwe then have for any \u03be > 0,\nPr { sup f\u2208F \u2223\u2223Ef \u2212 EN f \u2223\u2223 > \u03be } \u2264 2Pr { sup f\u2208F \u2223\u2223E\u2032Nf \u2212 EN f \u2223\u2223 > \u03be 2 } .\nThis completes the proof."}, {"heading": "C.5 Proof of Theorem B.2", "text": "Proof of Theorem B.2. Let fN = (f\u03021, \u00b7 \u00b7 \u00b7 , f\u0302M) T be the vector-valued function achieving the supremum sup f\u2208F {\u2223\u2223Ef \u2212EN f \u2223\u2223}.\nSimilar to the proof of Theorem B.1, we have\n1{|EfN\u2212EN fN |>\u03be}Pr \u2032 {\u2223\u2223EfN \u2212 E\u2032NfN \u2223\u2223 \u2264 \u03be\n2\n} \u2264Pr\u2032 {\u2223\u2223E\u2032N fN \u2212EN fN \u2223\u2223 > \u03be\n2\n} . (55)\nAccording to Lemma B.2, we have\nPr\u2032 {\u2223\u2223EfN \u2212E\u2032N fN \u2223\u2223 6\u2264 \u03be 2 }\n\u2264Pr      \u2211N n=1 \u2223\u2223Ef\u03021 \u2212 f\u03021(z[1]n ) \u2223\u2223\n...\u2211N n=1 \u2223\u2223Ef\u0302M \u2212 f\u0302M (z[M ]n ) \u2223\u2223\n  6\u2264   N\u03be[1]/2 ...\nN\u03be[M ]/2\n    \n\u2264 M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b   \u03c6F(\u039b [m], \u03be) + N \u2211 i\u2208\u039b[m] Var ( f\u0302 [i](z[i]) ) + 2N2 \u2211 i1<i2 i1,i2\u2208\u039b[m] Cov ( f\u0302 (i1)(z(i1)), f\u0302 (i2)(z(i2)) ) N2\n4 ( \u2211 i\u2208\u039b[m] \u03be[i] )2\n \n= M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b   \u03c6F(\u039b [m], \u03be) + 4 \u2211 i\u2208\u039b[m] Var ( f\u0302 [i](z[i]) ) N ( \u2211\ni\u2208\u039b[m] \u03be[i]\n)2 +\n8 \u2211 i1<i2\ni1,i2\u2208\u039b[m]\nCov ( f\u0302 (i1)(z(i1)), f\u0302 (i2)(z(i2)) )\n( \u2211 i\u2208\u039b[m] \u03be[i] )2\n \n\u2264 M\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n \u03c6F(\u039b [m], \u03be) + 4m(b\u2212 a)2\nN ( \u2211\ni\u2208\u039b[m] \u03be[i]\n)2 +\n8 \u2211\ni1<i2\ni1,i2\u2208\u039b[m]\nCovF (i1, i2)\n( \u2211 i\u2208\u039b[m] \u03be[i] )2\n  (56)\nMoreover, define\n\u03932 :=\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\nm(b\u2212 a)2 ( \u2211\ni\u2208\u039b[m] \u03be[i]\n)2 ,\nand\n\u03a52 :=\nM\u2211\nm=1\n\u2211\n\u039b[m]\u2282\u039b\n8 \u2211\ni1<i2\ni1,i2\u2208\u039b[m]\nCovF (i1, i2)\n( \u2211 i\u2208\u039b[m] \u03be[i] )2 .\nHence, we get\n1{|EfN\u2212EN fN |>\u03be}\n( 1\u2212 ( 4\u03932 N +\u03a5(\u039b) + \u03a52 )) \u2264 Pr\u2032 {\u2223\u2223E\u2032N fN \u2212EN fN \u2223\u2223 > \u03be 2 } . (57)\nTaking the expectation with respect to {Z[m]N } M m=1 and letting\n4\u03932 N +\u03a5(\u039b) + \u03a52 \u2264 1 2 , (58)\nwe then have for any \u03be > 0\nPr { sup f\u2208F \u2223\u2223Ef \u2212 EN f \u2223\u2223 > \u03be } \u2264 2Pr { sup f\u2208F \u2223\u2223E\u2032Nf \u2212 EN f \u2223\u2223 > \u03be 2 } .\nThis completes the proof."}, {"heading": "C.6 Proof of Theorem 5.1", "text": "Proof of Theorem 5.1. For any 1 \u2264 m \u2264 M , consider {\u01eb[m]n }Nn=1 as independent Rademacher random variables, i.e., independent {\u00b11}-valued random variables with equal probability of taking either value. Given an {\u01eb[m]n }Nn=1 and a Z [m] 2N , denote\n\u2212\u2192\u01eb [m] :=(\u01eb[m]1 , \u00b7 \u00b7 \u00b7 , \u01eb [m] N ,\u2212\u01eb [m] 1 , \u00b7 \u00b7 \u00b7 ,\u2212\u01eb [m] N ) T \u2208 {\u00b11}2N , 1 \u2264 m \u2264 M, (59)\nand for any f = (f1, \u00b7 \u00b7 \u00b7 , fM)T \u2208 F R c ,\n\u2212\u2192 f [m](Z\n[m] 2N ) :=\n( f [m](z\u2032\n[m] 1 ), \u00b7 \u00b7 \u00b7 , f [m](z\u2032 [m] N ), f [m](z [m] 1 ), \u00b7 \u00b7 \u00b7 , f [m](z [m] N )\n)T \u2208 [a, b]2N . (60)\nAccording to Theorem B.1, given any \u03be > 0 and for any N \u2208 N satisfying Condition (34), we have\nPr { sup f\u2208FRc \u2223\u2223Ef \u2212 EN f \u2223\u2223 > \u03be }\n\u22642Pr { sup f\u2208FRc \u2223\u2223E\u2032N f \u2212 EN f \u2223\u2223 > \u03be 2 } (by Theorem B.2)\n=2Pr    sup f\u2208FRc   \u2223\u2223\u2223 1N \u2211N n=1 ( f(z\u2032[1]n )\u2212 f(z [1] n ) )\u2223\u2223\u2223 ...\u2223\u2223\u2223 1N \u2211N n=1 ( f(z\u2032[M ]n )\u2212 f(z [M ] n ) )\u2223\u2223\u2223   >   \u03be[1] 2 ... \u03be[M] 2     \n=2Pr    sup f\u2208FRc   \u2223\u2223\u2223 1N \u2211N n=1 \u01eb [1] n ( f(z\u2032[1]n )\u2212 f(z [1] n ) )\u2223\u2223\u2223 ...\u2223\u2223\u2223 1N \u2211N n=1 \u01eb [M ] n ( f(z\u2032[M ]n )\u2212 f(z [M ] n ) )\u2223\u2223\u2223   >   \u03be[1] 2 ... \u03be[M] 2     \n=2Pr    sup f\u2208FRc   \u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [1],\u2212\u2192f 1(Z[1]2N ) \u232a\u2223\u2223\u2223 ...\u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [M ],\u2212\u2192f M(Z[M ]2N ) \u232a\u2223\u2223\u2223   >   \u03be[1] 4 ... \u03be[M] 4      . (61)\nFor any given sample collection {Z[m]2N } M m=1 of the tasks Z [1], \u00b7 \u00b7 \u00b7 ,Z [M ], let \u2126p,N(F R c , \u03be/8) be the cover of FRc w.r.t. the radius-vectors \u03be/8. Since F R c is composed of the functions with the range [a, b], we assume that the same holds for any h \u2208 \u2126p,N(F R c , \u03be/8). If f\u2020 = (f [1] \u2020 , \u00b7 \u00b7 \u00b7 , f [M ] \u2020 ) T is a vector-valued function that achieves\nsup f\u2208FRc\n  \u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [1],\u2212\u2192f [1](Z[1]2N ) \u232a\u2223\u2223\u2223 ...\u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [M ],\u2212\u2192f [M ](Z[M ]2N ) \u232a\u2223\u2223\u2223   >   \u03be[1] 4 ... \u03be[M] 4   ,\nthere must be an h\u2020 = (h [1] \u2020 , \u00b7 \u00b7 \u00b7 , h [M ] \u2020 ) T \u2208 \u2126p,N(F R c , \u03be/8) such that, for any 1 \u2264 m \u2264 M ,\n1\n2N\nN\u2211\nn=1\n( |f [m]\u2020 (z \u2032[m] n )\u2212 h [m] \u2020 (z \u2032[m] n )|+ |f [m] \u2020 (z [m] n )\u2212 h [m] \u2020 (z [m] n )| ) < \u03be[m]\n8 ,\nand meanwhile, \u2223\u2223\u2223 1 2N \u2329\u2212\u2192\u01eb [m],\u2212\u2192h [M ]\u2020 (Z [m] 2N ) \u232a\u2223\u2223\u2223 > \u03be [m] 8 .\nTherefore, we arrive at\nPr    sup f\u2208FRc   \u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [1],\u2212\u2192f [1](Z[1]2N ) \u232a\u2223\u2223\u2223 ...\u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [M ],\u2212\u2192f [M ](Z[M ]2N ) \u232a\u2223\u2223\u2223   >   \u03be[1] 4 ... \u03be[M] 4     \n\u2264Pr    sup h\u2208\u2126p,N (F R c ,\u03be/8)   \u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [1],\u2212\u2192h [1](Z[1]2N ) \u232a\u2223\u2223\u2223 ...\u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [M ],\u2212\u2192h [M ](Z[M ]2N ) \u232a\u2223\u2223\u2223   >   \u03be[1] 8 ... \u03be[M] 8      . (62)\nOn the other hand, given a \u03be > 0 and for any N \u2208 N satisfying Condition (34),\nPr    sup h\u2208\u2126p,N (F R c ,\u03be/8)   \u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [1],\u2212\u2192h [1](Z[1]2N ) \u232a\u2223\u2223\u2223 ...\u2223\u2223\u2223 12N \u2329\u2212\u2192\u01eb [M ],\u2212\u2192h [M ](Z[M ]2N ) \u232a\u2223\u2223\u2223   >   \u03be[1] 8 ... \u03be[M] 8     \n=Pr    sup h\u2208\u2126p,N (F R c ,\u03be/8)   \u2223\u2223\u2223 1N \u2329\u2212\u2192\u01eb [1],\u2212\u2192h [1](Z[1]2N ) \u232a\u2223\u2223\u2223 ...\u2223\u2223\u2223 1N \u2329\u2212\u2192\u01eb [M ],\u2212\u2192h [M ](Z[M ]2N ) \u232a\u2223\u2223\u2223   >   \u03be[1] 4 ... \u03be[M] 4     \n=Pr { sup\nh\u2208\u2126p,N (F R c ,\u03be/8)\n\u2223\u2223E\u2032Nh\u2212ENh \u2223\u2223 > \u03be\n4\n} (similer to (61))\n\u2264Pr   \n\u2211\nh\u2208\u2126p,N (F R c ,\u03be/8)\n\u2223\u2223E\u2032Nh\u2212ENh \u2223\u2223 > \u03be\n4   \n\u2264Pr   \n\u2211\nh\u2208\u2126p,N (F R c ,\u03be/8)\n\u2223\u2223Eh\u2212ENh \u2223\u2223 + \u2223\u2223Eh\u2212E\u2032Nh \u2223\u2223 > \u03be\n4   \n\u22642Pr   \n\u2211\nh\u2208\u2126p,N (F R c ,\u03be/8)\n\u2223\u2223Eh\u2212 ENh \u2223\u2223 > \u03be\n8   \n\u22642M+1N 1 ( FRc , \u03be/8, 2N ) exp\n{ \u2212N \u2211M m=1(\u03be [m])2\n32M2(b\u2212 a)2\n} . (63)\nThe last inequality of (63) is derived from Definition (4.2) and Theorem A.1. The combination of (61), (62) and (63) leads to the result: given any \u03be > 0, there holds that for any N \u2208 N satisfying Condition (34),\nPr { sup f\u2208FRc \u2223\u2223Ef \u2212EN f \u2223\u2223 > \u03be } \u2264 2M+2N 1 ( FRc , \u03be/8, 2N ) exp { \u2212N \u2211M m=1(\u03be [m])2 32M2(b\u2212 a)2 } .\nThis completes the proof."}, {"heading": "C.7 Proof of Theorem 5.2", "text": "Before the formal proof, we present a necessary lemma.\nLemma C.1 Let sn = (s [1] n , \u00b7 \u00b7 \u00b7 , s [M ] n ) \u2208 RM (1 \u2264 n \u2264 N) be N i.i.d. random vectors. Then, there holds that for any \u03be = (\u03be[1], \u00b7 \u00b7 \u00b7 , \u03be[M ])T > 0,\nPr\n{ N\u2211\nn=1\nsn \u2264 N\u03be\n} \u2264 2MPr { s1 \u2264 2\u03be } . (64)\nProof. For any 1 \u2264 m \u2264 M , we have\nN\u2211\nn=1\ns[m]n \u2265 N\u2211\nn=1\ns[m]n 1 {\ns [m] n >2\u03be[m]\n} \u2265 2\u03be[m] N\u2211\nn=1\n1{ s [m] n >2\u03be[m] }.\nHence, it is followed from the conditional Markov inequality that\nPr\n{ N\u2211\nn=1\nsn \u2264 N\u03be\n} \u2264Pr      2\u03be[1] \u2211N n=1 1 { s [1] n >2\u03be[1] } ... 2\u03be[M ] \u2211N\nn=1 1 {\ns [M] n >2\u03be[M]\n}\n  \u2264 N   \u03be[1]\n... \u03be[M ]\n    \n=Pr      \u2211N n=1 1 { s [1] n \u22642\u03be[1] } ...\u2211N n=1 1 {\ns [M] n \u22642\u03be[M]\n}\n  \u2265 (1\u2212 2 \u22121)N   1 ... 1     \n=Pr\n{ N\u2211\nn=1\n1{ s [1] n \u22642\u03be[1]\n} \u2265 2\u22121N \u2223\u2223 AM2 } Pr { AM2 }\n\u2264\nE {\u2211N n=1 1 {\ns [1] n \u22642\u03be[1]\n} \u2223\u2223 AM2 }\n2\u22121N Pr\n{ AM2 }\n\u2264 NPr\n{ s [1] 1 \u2264 2\u03be [1] \u2223\u2223 AM2 }\n2\u22121N Pr\n{ AM2 } = 2Pr { s [1] 1 \u2264 2\u03be [1],AM2 } ,\nwhere AM2 stands for the event that {\u2211N\nn=1 1{s[m]n \u22642\u03be[m]} }M m=2 . Then, following this way, we have\nPr\n{ N\u2211\nn=1\nsn \u2264 N\u03be\n} \u2264 2Pr { s [1] 1 \u2264 2\u03be [1],AM2 } \u2264 22Pr { s [1] 1 \u2264 2\u03be [1], s [2] 1 \u2264 2\u03be [2],AM3 }\n\u2264 \u00b7 \u00b7 \u00b7 \u2264 2MPr { s [1] 1 \u2264 2\u03be [1], s [2] 1 \u2264 2\u03be [2], \u00b7 \u00b7 \u00b7 , s[M ]1 \u2264 2\u03be [M ] } = 2MPr {s1 \u2264 2\u03be} .\nThis completes the proof. Next, we come up with the proof of Theorem 5.2. Proof of Theorem 5.2. Let f\u0302\u2217 = (f [1] \u2217 , \u00b7 \u00b7 \u00b7 , f [M ] \u2217 )T be the vector-valued function achieving the supremum supf\u2208FRc \u2223\u2223Ef \u2212EN f \u2223\u2223. Then, it is followed from Lemma C.1 that\nPr { sup f\u2208FRc \u2223\u2223Ef \u2212EN f \u2223\u2223 \u2264 \u03be } =Pr {\u2223\u2223Ef\u2217 \u2212EN f\u2217 \u2223\u2223 \u2264 \u03be } \u2264 2MPr {s\u2217 \u2264 2\u03be}\n\u22642M sup f\u2208FRc\nPr { s \u2264 2\u03be } ,\nwhere s\u2217 = ( s [1] \u2217 , \u00b7 \u00b7 \u00b7 , s [M ] \u2217 )T with s [m] \u2217 := \u2223\u2223E[m]f [m]\u2217 \u2212 f [m]\u2217 (z[m]) \u2223\u2223 for any 1 \u2264 m \u2264 M ."}, {"heading": "C.8 Proof of Theorem 5.3", "text": "Proof of Theorem 5.3. Denote tN = (t [1], \u00b7 \u00b7 \u00b7 , t[M ])T with t[i] := \u2223\u2223E[i]f [i] \u2212 E[i]Nf [i] \u2223\u2223 > \u03be[i]. The event tN 6\u2264 \u03be contains the following possibilities:\n\u2022 P [1]: there is only one index {i} = \u039b[1] \u2282 \u039b satisfying that t[i] > \u03be[i];\n\u2022 P [m]: there arem (1 < m < M) indices {i[1], \u00b7 \u00b7 \u00b7 , i[m]} = \u039b[m] \u2282 \u039b satisfying that t[ik] > \u03be[ik]\n(1 \u2264 k \u2264 m);\n\u2022 P [M ]: t[m] > \u03be[m] holds for any 1 \u2264 m \u2264 M .\nThus, we have Pr {tN 6\u2264 \u03be} = Pr{P [1]}+ \u00b7 \u00b7 \u00b7+ Pr{P [M ]}. (65)\nThen, the combination of Definition 3.2, Theorems 5.1&5.2 and (65) leads to the result (17).\nMoreover, since Pr {{ s[\u03bb] \u2264 2\u03be[\u03bb] } \u03bb\u2208\u039b[m] } \u2264 1 holds for any \u039b[m] \u2282 \u039b, the result (19) can be directly obtained. This completes the proof."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "In this paper, we study the generalization performance of regularized multi-task learning<lb>(RMTL) in a vector-valued framework, where MTL is considered as a learning process for<lb>vector-valued functions. We are mainly concerned with two theoretical questions: 1) under<lb>what conditions does RMTL perform better with a smaller task sample size than STL? 2)<lb>under what conditions is RMTL generalizable and can guarantee the consistency of each<lb>task during simultaneous learning? In particular, we investigate two types of task-group<lb>relatedness: the observed discrepancy-dependence measure (ODDM) and the empirical<lb>discrepancy-dependence measure (EDDM), both of which detect the dependence between<lb>two groups of multiple related tasks (MRTs). We then introduce the Cartesian product-<lb>based uniform entropy number (CPUEN) to measure the complexities of vector-valued<lb>function classes. By applying the specific deviation and the symmetrization inequalities<lb>to the vector-valued framework, we obtain the generalization bound for RMTL, which<lb>is the upper bound of the joint probability of the event that there is at least one task<lb>with a large empirical discrepancy between the expected and empirical risks. Finally, we<lb>present a sufficient condition to guarantee the consistency of each task in the simultaneous<lb>learning process, and we discuss how task relatedness affects the generalization performance<lb>of RMTL. Our theoretical findings answer the aforementioned two questions.", "creator": "LaTeX with hyperref package"}}}