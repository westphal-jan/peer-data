{"id": "1409.3358", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2014", "title": "Building Program Vector Representations for Deep Learning", "abstract": "Deep learning has out likely breakthroughs year various such one reproductive critical. Advantages created deep well include the enabled able capture important indeed uses, recession abuses bringing critical education, incl. However, into any keeping perhaps things to use way learning actually analyze support since deep architectures them be cadre effectively with naturally just propagation. In this researches paper, be propose the \" fingerprint criterion \" giving enabling service formula_12 dimensional, over actually 's premise of through educational same program analysis. Our represents learning kind longer makes deep provides to definitely in this making solid. We supervise the learned vector two-dimensional been pragmatically or quantitatively. We indication, based when has theory, the coding estimation itself successful 2001 housing program shapes. To evaluation still keeps approach is development full program analysis, we breeding the representations must deep utilizes networks, others strengthen weaker analyses, the system classification task than \" layers \" methods, reasons as co-ordinates regression for since offered formula_24 conventional. This result h1n1 the feasibility of deep writing to analyze working. It also choice measure evidence where as better in yet new out. We believe putting focus will most instance directorial accomplished for program theory while the near plans.", "histories": [["v1", "Thu, 11 Sep 2014 08:44:28 GMT  (189kb,D)", "http://arxiv.org/abs/1409.3358v1", "This paper was submitted to ICSE'14"]], "COMMENTS": "This paper was submitted to ICSE'14", "reviews": [], "SUBJECTS": "cs.SE cs.LG cs.NE", "authors": ["lili mou", "ge li", "yuxuan liu", "hao peng", "zhi jin", "yan xu", "lu zhang"], "accepted": false, "id": "1409.3358"}, "pdf": {"name": "1409.3358.pdf", "metadata": {"source": "CRF", "title": "Building Program Vector Representations for Deep Learning", "authors": ["Lili Mou", "Ge Li", "Yuxuan Liu", "Hao Peng", "Zhi Jin", "Yan Xu", "Lu Zhang"], "emails": ["zhanglu}@sei.pku.edu.cn", "alandroxu}@gmail.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMachine learning-based program analysis has been studied long in the literature [1], [2], [3]. Hindle et al. compare programming languages to natural languages and conclude that programs also have rich statistical properties [4]. These properties are difficult for human to capture, but they justify using learning-based approaches to analyze programs.\nThe deep neural network, also known as deep learning, has become one of the prevailing machine learning approaches since 2006 [5]. It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc. Compared with traditional machine learning approaches, deep learning has the following major advantages:\n\u2022 The deep architecture can capture highly complicated (non-linear) features efficiently. They are crucial to most real-world applications. \u2022 Very little human engineering and prior knowledge is required. Interestingly, with even a unified model, deep learning achieves better performance than state-of-the-art approaches in many heterogeneous tasks [12].\n\u2217Corresponding author.\nSuch striking results raise the interest of its applications in the field of program analysis. Using deep learning to automatically capture program features is an interesting and prospective research area.\nUnfortunately, it has been practically infeasible for deep learning to analyze programs up till now. Since no proper \u201cpretraining\u201d method is proposed for programs, deep neural networks cannot be trained effectively with pure back propagation [13], [14], [15] because gradients would either vanish or blow up through the deep architecture [16]. No useful features can be extracted, and it results in very poor performance.\nIn this paper, we propose a novel \u201ccoding criterion\u201d to build program vector representations based on abstract syntax trees (ASTs). The vector representations are the premise of deep architectures, and our method directly makes deep learning a reality in the new field\u2014program analysis. In such vector representations, each node in ASTs (e.g. ID, Constant) is mapped to a real-valued vector, with each element indicating a certain feature of the node. The vector representations serve as a \u201cpretraining\u201d method. They can emerge, through a deep architecture, high-level abstract features, and thus benefit ultimate tasks. We analyze the learned representations both qualitatively and quantitatively. We conclude from the experiments that the coding criterion is successful in building program vector representations.\nTo evaluate whether deep learning can be used to analyze programs, we feed the learned representations to a deep neural network in the program classification task. We achieve higher accuracy than \u201cshallow\u201d methods. The result confirms the feasibility of neural program analysis. It also sheds some light on the future of this new area.\nWe publish all of our source codes, datasets, and learned representations on our project website1 to promote future studies. The AST node representations can be used for further researches in various applications of program analysis. The source codes contain a versatile infrastructure of the feedforward neural network, based on which one can build one\u2019s own deep neural architectures.\nTo our best knowledge, this paper is the first to propose representation learning algorithms for programs. It is also the first to analyze programs by deep learning. This study is a\n1 https://sites.google.com/site/learnrepresent/\nar X\niv :1\n40 9.\n33 58\nv1 [\ncs .S\nE ]\n1 1\nSe p\n20 14\npioneering research in the new field. To sum up, the main contributions of this paper include:\n1) Introducing the techniques of deep learning and representation learning to the field of program analysis; 2) Proposing the novel \u201ccoding criterion\u201d to build program representations, which are the premise of deep learning; 3) Exploring the feasibility to analyze programs by deep neural networks, shedding some light on the future; 4) Publishing all of our source codes, datasets, and learned representations online to promote further researches.\nIn the rest of this paper, we first motivate our research in Section II. The background of deep learning and representation learning is introduced in Section III. Then we explain our approach in detail in Section IV. Experimental results are shown in Section V. In Section VI, we look forward to the future of deep learning in the field of program analysis. Last, we draw our conclusion in Section VII."}, {"heading": "II. MOTIVATION", "text": ""}, {"heading": "A. From Machine Learning to Deep Learning", "text": "Traditional machine learning approaches largely depend on human feature engineering, e.g., [17] for bug detection, [18] for clone detection. Such feature engineering is labelconsuming and ad hoc to a specific task. Further, evidence in the literature suggests that human-engineered features may be even worse than automatically learned ones. Mnih et al. report\u2014for example, in the field of natural language processing (NLP)\u2014the automatically learned taxonomy of words [19] is better in their application than the famous WordNet constructed by experts [20] used in [21].\nSuch results pose increasing demands on highly automated learning approaches, such as deep learning with very little human engineering.\nWith deep neural networks, program analysis may be easier with statistical methods. For example, in the task of program classification, deep neural networks automatically extract program features of interest. Features can be organized hierarchically, from local to abstract. Based on these abstract features, we may determine the category of a program. Such deep learning architectures require less human engineering than existing methods like [22]. Moreover, the feature representation nature also makes it easy for multi-task learning. As pointed out in [23], \u201cmany decision problems can be reduced to classification.\u201d Such deep learning architecture is also applicable to other program analysis tasks, including\n\u2022 bug detection as [17], to which the deep learning approach is to automatically extract features of bugs; \u2022 clone detection as [24], to automatically match the features of two programs; \u2022 code retrieval as [25], to automatically match program features with that of the queries; and \u2022 code recommendation as [26], to automatically predict the probability of the next possible codes, e.g. APIs, according to previous ones based on the features (like [27] in NLP).\nIn short, deep neural networks are capable of capturing highly complicated features with little human involvement. Analyzing programs with deep learning is an interesting and promising research topic."}, {"heading": "B. Barriers of Deep Learning for Program Analysis", "text": "Although deep neural networks are powerful enough to capture complicated features, there are still barriers to overcome before they can be practically used to analyze programs.\nSince all program symbols (e.g. nodes in ASTs) are \u201cdiscrete,\u201d no order is defined on these symbols. Such discrete symbols cannot be fed directly to a neural network. A possible solution is to map each symbol to a real-valued vector in some dimension. Each element in the vector characterizes a certain feature of the symbol spontaneously. Hence, it is also referred to as distributed representation. By \u201cdistributed,\u201d it is contrary to one-of-all coding, such as k-means clustering results.\nA direct mapping approach is to randomly initialize these vector representations and train with pure back propagation (like shallow networks). Chances are that we end up with both poor optimization and poor generalization if the network is deep [13], [14], [15]. An alternative is to first learn the representations unsupervisedly regardless of the specific task like clone detection, bug detection, etc. Then they are fed to a neural network for supervised training. The vector representations specify meaningful features of the symbols, and benefit the ultimate tasks. Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.\nHowever, due to the structural differences between natural languages and programming languages [30], existing representation learning algorithms in NLP are improper for programs. As we know, natural languages are always written/spoken in one dimension as time flows. By contrast, programmers always organize their source codes with proper indentation, indicating branches, loops or even nested structures. It will be extremely difficult to read source codes if they are written in one line (like natural languages). The formal grammar rules of the programming language alias the notion of neighborhood. To be concrete, physically neighboring stuffs in a program source code are not necessarily near to each other, but those in one grammar rule are.\nFurther, if we want to build the representations for abstract components of a program (e.g., function declaration/call nodes in ASTs), existing NLP representation learning algorithms are inapplicable since all of them are \u201cflat.\u201d\nTherefore, new approaches are needed to build program vector representations, which are unfortunately not studied.\nThe above facts motivate our research of representation learning for programs. This eventually makes deep learning a reality to analyze programs. Considering current evidence in the literature, we believe deep learning will make great progress in heterogenous tasks of program analysis."}, {"heading": "III. BACKGROUND OF DEEP LEARNING AND REPRESENTATION LEARNING", "text": ""}, {"heading": "A. Deep Neural Networks", "text": "Nowadays, the deep neural network is a widely-used technique in artificial intelligence. Comprehensive reviews include [31], [32].\nA single layer of neurons, the building block for deep neural networks, takes a vector x \u2208 Rn as input and outputs a vector y \u2208 Rm (Part A in Figure 1). Typically y is computed as\ny = f(W \u00b7x + b) (1)\nwhere W \u2208 Rm\u00d7n, b \u2208 Rm are model parameters, which are first randomly initialized and then trained by gradient descent, i.e., W \u2190W\u2212\u03b1 \u2202J\u2202W and b\u2190 b\u2212\u03b1 \u2202J \u2202b . (\u03b1 is the learning rate; J is the cost function.) f is the activation function, usually non-linear, such as sigmoid, tanh, etc. The power of a singlelayer neural network is limited: the decision boundary is linear, and it is insufficient for most real-world applications.\nMulti-layer neural networks (Part B in Figure 1) stack multiple layers of neurons. The parameters can also be trained by gradient descent with back propagation algorithm [33]. Due to the stacking of multiple non-linear layers, multi-layer neural networks gain much more power. It can be proved that a 2-layer2 neural network with sufficient hidden units can approximate arbitrary Boolean or continuous functions, and that a 3-layer network can approximate any function [34]. However, the shallow architecture is inefficient because the number of hidden units may grow exponentially in order to learn complicated (highly non-linear) features of data [35]. Such exponentiation of hidden layer units, and hence parameters, raises the VC-dimension of the model, leading to very poor generalization [36].\nThe theory of circuits suggests deep architectures would be more efficient to capture complicated features [31]. In such deep architectures, features can be organized hierarchically, with local features at lower layers and abstract features at higher layers (Figure 1). However, while deep architectures capture abstract features efficiently, they also make the networks very difficult to train. Few successful researches were reported in early years using deep architectures (except convolutional neural networks [37]).\nIn 2006, Hinton et al. proposed stacked restricted Boltzmann machine (RBM) as a greedy layer-wise pretraining method for deep neural networks [5]. During the pretraining phase, the stacked RBM is learning underlying data features unsupervisedly by minimizing the energy function defined on the unlabeled data (i.e., maximizing the likelihood of the data). Shortly after that, stacked autoencoders are used for pretraining [13], the criterion of which is to minimize the reconstruction error. During the pretraining phase with either stacked RBMs or autoencoders, the weights for neuron connections are learned, which extract underlying features of\n2 The input layer is not counted to the number of layers in our terminology as there is no weight associated with it.\nthe data. Then, for supervised learning, the neural weights are initialized as that have been learned in the pretraining phase instead of random initialization. Standard back propagation algorithm can be used for fine-tuning the weights so as to be specific to the task.\nThe pretraining phase plays a vital role in deep learning. It learns the features of data unsupervisedly, and as a result, the weights are much more meaningful than just chosen randomly. According to the experiments reported in [13], [14], [15], pretraining helps optimization (minimizing the training error) as well as generalization (minimizing the test error).\nHowever, the advantages of deep neural networks are not exploited in the field of program analysis. We believe deep learning will also exhibit its power in this new field."}, {"heading": "B. Existing Representation Learning Approaches in NLP", "text": "Neural networks and the pretraining approaches like RBMs, autoencoders work well with image data, speech data, etc. But they cannot be applied directly to the field like NLP and program analysis because words and program symbols are \u201cdiscrete.\u201d\nIn data like images, a total order is defined on the value. For example, a gray-scale pixel with value 0 is black. If the value increases, it becomes brighter accordingly. If the value is 255, the pixel becomes white. However, in fields like NLP, a word with index 20 is by no means twice as large as the word with index 10 for any meaningful feature. Therefore, unlike traditional approaches in NLP, where each words is treated as an atomic unit, it is meaningless to feed the indexes of words to neural networks. (Note the multiplication W \u00b7x in Equation 1.)\nReal-valued vector representations come to our rescue. With such representations, each word is mapped to a k-dimensional\nvector (k = 30, 100, 300, etc), representing certain (anonymous) features of the word. The value reflects the degree that a feature is satisfied. These word representations can be fed forward to standard neural networks, and every routine of deep learning works.\nEarly word representation learning is related to language modeling, the objective of which is to maximize the joint probability of a linguistic corpus. In [27], they predict the probability of each word given n\u22121 previous words. By maximizing the conditional probability of the n-th word, useful word features are learned. Hinton et al. introduce 3 energybased models in [28], where they learn word representations by minimizing the energy (maximizing the likelihood) defined on neighboring words. In [21], [19], hierarchical architectures are proposed to reduce the computational cost in calculating the probabilities. Later, researchers realized the normalization of probability is not essential if we merely want to learn feature vectors. Fast algorithms are then proposed in [38], [39].\nAll the above approaches adopt the Markovian assumption, where each word is related to finite many previous words. Such approaches take into consideration only local patterns of physically nearing words. Recurrent neural network (RNN) is introduced in order to capture long-term dependencies [40]. However, RNN may be very difficult to train since the gradient would either vanish or blow up during back propagation [16]. Besides, the time-delaying nature of RNN treats data as a onedimensional signal, where structural information is also lost.\nAs we have discussed in Section I, programming languages are different from natural languages in that the former contain richer and more explicit structural information. Therefore, new representation learning algorithms are needed. To solve this problem, we propose the \u201ccoding criterion\u201d based on ASTs. The detail of our approach is explained in the following section."}, {"heading": "IV. THE CODING CRITERION FOR PROGRAM REPRESENTATION LEARNING", "text": "In this section, we first discuss the granularities of program representation. We settle for the granularity of nodes in abstract syntax trees (ASTs).\nIn Subsection IV-B, we formalize our approach and give the learning objective. In Subsection IV-C, we present the stochastic gradient descent algorithm for training."}, {"heading": "A. The Granularity", "text": "We should first answer a fundamental question of program representation learning\u2014the granularity of representation. As we have introduced in previous sections, vector representations map a symbol to a real-valued, distributed vector. Possible granularities of the symbol include character-level, token-level, etc. We analyze each case as follows.\n\u2022 Character-level. Characters are the most atomic units of programming languages. At this level, we treat each character in source codes as a symbol. This means that we should learn the representations for characters like\ndouble doubles(double doublee){ return 2 * doublee; }\n(A) A C code snippet\nFuncDef\nDecl Compound\nFuncDecl Return\nParameterList TypeDecl BinaryOp\nDecl IdentifierType Constant ID\nTypeDecl\nIdentifierType\n(B) The corresponding AST\nsnippet and its corresponding AST, parsed by pycparser3. At this level, we learn the representations for nodes in ASTs, e.g., FuncDef, ID, Constant. As is stated, the AST is more compressed compared with token-level representation. Furthermore, there are only finite many types of nodes in ASTs, which makes it feasible to learn. The tree structural nature of ASTs also provides opportunities to capture structural information of programs. Despite the above facts, the AST representation also has its drawback since we regard all identifiers as a same symbol. Such codes as a*b and c*d cannot be distinguished between each other. We hypothesize that structural information captures the semantics of programs to a large extent. For example, if we see two nested for-loops, inside of which is a branch of comparison followed by three assignments, the code snippet is likely to be an implementation of bubble sort. This level is also used in traditional program analysis like code clone detection [42], [43], vulnerability extrapolation [44], etc. The experimental results in Section V-B also suggest high accuracy in the program classification task at this level. \u2022 Statement-level, function-level or higher. Theoretically, a statement, a function or even a program can also be mapped to a real-valued vector. However, such representations cannot be trained directly. A possible approach of modeling such complex stuff is by composition, i.e., the representation of a complex stuff is composited by that of atomic ones. Such researches in NLP is often referred to as compositional semantics [45]. The state-ofthe-art approaches in NLP compositional semantics can only model sentences, paragraphs roughly. It is very hard to capture the precise semantics; the \u201csemantic barrier\u201d is still not overcome.\nTo sum up, we have analyzed in this part different granularities of program representations. We think the representation for nodes in ASTs has theoretical foundations, and is feasible to learn and useful in applications. In the following subsection, we formalize our coding criterion to build program vector representations."}, {"heading": "B. Formalization", "text": "The basic criterion of representation learning is that similar symbols should have similar representations. Further, symbols that are similar in some aspects should have similar values in corresponding feature dimensions. This is referred to as \u201cdisentangling the underlying factors of variation\u201d in [32].\nIn our scenario of representation learning for AST nodes, similarity is defined based on the following intuition. We think such symbols as ID, Constant are similar because both of them are related to data reference; For, While are similar because both are related to loops. The observation is that similar symbols have similar usages in the programming language: both ID and Constant can be an operand of a\n3 https://pypi.python.org/pypi/pycparser/\nunay/binary operator; both For and While are a block of codes, etc.\nTo capture such similarity using AST structural information, we propose the \u201ccoding criterion\u201d. The idea is that the representation of a node in ASTs should be \u201ccoded\u201d by its children\u2019s representations via a single neural layer.\nWe denote the vector of node x as vec(x). vec(\u00b7) \u2208 RNf , where Nf is the dimension of features. (Nf is set to 30 empirically in our experimental setting.) For each non-leaf node p in ASTs and its direct children c1, \u00b7 \u00b7 \u00b7 , cn, their representations are vec(p), vec(c1), \u00b7 \u00b7 \u00b7 , vec(cn). The primary objective is that\nvec(p) \u2248 tanh\n( n\u2211\ni=1\nliWi \u00b7 vec(ci) + b ) (2)\nwhere Wi \u2208 RNf\u00d7Nf is the weight matrix for node ci; b \u2208 RNf is the bias term. The weights (Wi\u2019s) are weighted by the number of leaves under ci and the coefficients are\nli = #leaves under ci #leaves under p\n(3)\nAs we have noticed in Figure 2, different nodes in ASTs may have different numbers of children, and thus the number of Wi\u2019s is hard to determine. To solve this problem, one extreme is to apply dynamic pooling [46], [47]. In this method, we take the summed or maximal value over vec(ci) for each feature dimension, and thus only one weight matrix is needed. This is also mathematically equivalent to the continuous bagof-words model [38]. However, when pooling is applied, position information of c\u2019s will be totally lost and therefore it is not satisfactory. Another extreme is to assign a different matrix parameter for each different position [45]. This method works in the original application with dependency trees, but may fail in our scenario because there will be too many weights.\nWhat we propose is a model called continuous binary tree, where there are two weight matrices as parameters, namely Wl and Wr. Any weight Wi is a linear combination of the two matrices. That is, regardless the number of children, we treat it as a \u201cbinary\u201d tree. Formally, if p has n (n \u2265 2) children, then for child ci,\nWi = n\u2212 i n\u2212 1 Wl + i\u2212 1 n\u2212 1 Wr (4)\nIf n = 1, Wi = 12Wl + 1 2Wr.\nThis process is illustrated in Figure 3, where the gray-scale values in the two bars represent the weight coefficients for the node at the corresponding position. With this model, the relative position information of a node can be coded into the network.\nNow that we are able to calculate the weight Wi for each node and thus the right-hand side of Equation 2, we measure closeness by the square of Euclidean distance, as below:\nd = \u2225\u2225\u2225\u2225\u2225vec(p)\u2212 tanh ( n\u2211 i=1 liWi \u00b7 vec(ci) + b )\u2225\u2225\u2225\u2225\u2225 2\n2\n(5)\nAccording to our \u201ccoding criterion,\u201d d needs to be as small as possible. However, we cannot directly minimize Equation 5. Otherwise, the network is likely to output trivial representations like vec(\u00b7) = 0,W = 0, b = 0. Such result gives zero distance but is meaningless.\nTo solve the problem, negative sampling can be applied [12], [45], [48]. The idea is that for each data sample x, a new negative sample xc is generated. Since xc violates the patterns of valid data, it needs to have a larger distance (denoted as dc) than d. Hence, negative sampling method is also sometimes referred to as the pairwise ranking criterion [49]. In our program representation learning, we randomly select a symbol (one of p, c1, c2, \u00b7 \u00b7 \u00b7 , cn) in each training sample and substitute it with a different random symbol. The objective is that dc should be at least as large as d+\u2206, where \u2206 is the margin and often set to 1. The error function of training sample x(i) and its negative sample x(i)c is then\nJ(d(i), d(i)c ) = max { 0,\u2206 + d(i) \u2212 d(i)c }\n(6)\nTo prevent our model from over-fitting, we can add `2 regularization to weights (Wl and Wr). The overall training objective is then\nminimize Wl,Wr,b\n1\n2N N\u2211 i=1 J(d(i), d(i)c )\n+ \u03bb\n2M\n( \u2016Wl \u20162F + \u2016Wr \u20162F ) (7)\nwhere N is the number of training samples; M = 2N2f is the number of weights (number of elements in Wl and Wr); \u2016 \u00b7 \u2016F refers to Frobenius norm; \u03bb is the hyperparameter that strikes the balance between coding error and `2 penalty."}, {"heading": "C. Training", "text": "The numerical optimization algorithm we use is stochastic gradient descent with momentum. The model parameters \u0398 = ( vec(\u00b7),Wl,Wr, b ) are first initialized randomly. Then,\nfor each data sample x(i) and its negative sample x(i)c , we compute the cost function according to Formula 7. Back propagation algorithm is then applied to compute the partial derivatives and the parameters are updated accordingly. This process is looped until convergence. The coding criterion of vector representation learning\u2014as a pretraining phase for neural program analysis\u2014is \u201cshallow,\u201d through which error\nAlgorithm 1: StochasticGradientDescentWithMomentum Input: Data samples x(i), i = 1..N ;\nMomentum ; Learning rate \u03b1 Output: Model parameters \u0398 = ( vec(\u00b7),Wl,Wr, b )\nRandomly initialize \u0398; while not converged do\nfor i = 1..N do Generate a negative sample x(i)c for x(i); Propagate forward and backward to compute J (i)\nand the partial derivative \u2202J (i)\n\u2202\u0398 ;\n\u2202J (i)\n\u2202\u0398 \u2190 \u2202J\n(i\u22121)\n\u2202\u0398 + \u2202J (i) \u2202\u0398 ; // momentum\n\u0398\u2190 \u0398\u2212 \u03b1\u2202J (i)\n\u2202\u0398 ; // gradient descent\nend end\ncan back propagate. Thus, useful features are learned for AST nodes.\nTo speed up training, we adopt the momentum method, where the partial derivatives of the last iteration is added to the current ones with decay . Algorithm 1 shows the pseudo-code of the training process."}, {"heading": "V. EXPERIMENTS", "text": "We first evaluate our learned representations by nearest neighbor querying and k-means clustering. These qualitative evaluations give an intuitive idea about our vector representations. We then perform supervised learning in the program classification task. We feed the learned representations forward to deep neural networks. The experimental results show that meaningful representations, as a means of pretraining, make the network much easier to train in deep architectures. We also achieve higher accuracy with the deep, tree-based convolutional neural network compared with baseline methods. We consider this as primary evidence of the success of deep learning for program analysis.\nThe dataset, source codes and learned representations can be downloaded at our project website."}, {"heading": "A. Qualitative Evaluation: Nearest Neighbor Queries and kmeans Clustering", "text": "As we have stated in Section IV-B, similar nodes in ASTs (like ID, Constant) should have similar representations. To evaluate whether our coding criterion for representation learning has accomplished this goal, we perform nearest neighbor queries.\nFor each query of a symbol, we sort all other symbols by distance (measured in Euclidean space). Examples are presented in Table I. As we can see, ID and Constant are the\nnearest neighbor of each other. This seems meaningful since both of them are related to data reference. Similar symbols also include ArrayRef, BinaryOp, which are related to data manipulating. Symbols like If, For, While, Break are similar as they are related to control flow. FuncDecl, ArrayDecl, PtrDecl are similar as they are declarations. Moreover, these three groups are dissimilar with each other. (See most dissimilar part in Table I.)\nTo further confirm the above potential clusters with vector representations, we perform k-means clustering, where k is set to 3. The result is shown in Table II. As we see, almost all the symbols in Cluster 1 are related to data reference/manipulating. Cluster 2 is mainly about declarations. Cluster 3 contains more symbols, the majority of which are related to control flow. This result confirms our conjecture that similar symbols can be clustered into groups with the distributed vector representations that are learned by our approach.\nAs the qualitative evaluations show, the learned representations are meaningful as they can characterize the relationships between different symbols effectively. The results are consistent with human understanding of programs.\nIt should also be reminded that similarity is not the only goal of representation learning. Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics. Thus, they are not suitable for highly-automated learning algorithms, e.g., deep neural networks. On the contrary, real-valued representations are distributed. As each dimension captures a feature in a certain aspect spontaneously, the distributed vector representations can emerge high-level abstract features and benefit various tasks. Therefore, representation learning is crucial to program analysis with deep learning approaches."}, {"heading": "B. Quantitative Evaluation: Improvement for Supervised Learning", "text": "We now evaluate whether building program vector representations is beneficial for real-world tasks, i.e., whether they will improve optimization and/or generalization for supervised learning of interest. We feed the representations to the Treebased Convolutional Neural Network (TCNN) for program classification.\nThe dataset comes from an online Open Judge (OJ) system4, which contains a large number of programming problems for students. Students solve the problems and submit their source codes to the system. The OJ system automatically compiles, runs and judges the validity of the source codes. We select four problems for our program classification task. Source codes (in C programming language) of the four problems are downloaded along with their labels (problem IDs). We split the dataset by 3 : 1 : 1 for training, cross-validating (CV) and testing.\nFigure 4 plots the learning curves for training and CV in first 40 epochs. (One epoch is an iteration over all training\n4 http://programming.grids.cn/\nsamples.) The X axis is the number of epochs during training. The Y axis is the cross-entropy error, computed as\nJ = \u2212 1 N N\u2211 i=1 M\u2211 j=1 t (i) j log y (i) j (8)\nwhere N is the number of data samples (training or CV respectively); M = 4 is the number of labels (different programming problems); yj is the probability for label j estimated by the TCNN model; t is the actual label (one-of-all coding), with tj indicating whether data sample i belongs to label j.\nSince no effective program representation existed before, the deep TCNN model could not be trained at all, as the blue curve demonstrates at the top of Part A in Figure 4. (Here, all model parameters are initialized randomly, which is a prevalent setting in \u201cshallow\u201d architectures.) The reason is that gradients will vanish or blow up during back propagation through a deep network. No useful features are learned, and as a result, TCNN also performs poorly on the CV set, indicated by the cyan curve at the top of Part B in Figure 4.\nOn the contrary, the program representation serves as a pretraining method. If the vector representations and the cod-\ning parameters, namely vec(\u00b7), Wl, Wr and b, are initialized as are learned by our coding criterion, the training and CV errors decrease drastically (the red and magenta curves) after a plateaux of about 15 epochs, which leads to the high performance of TCNN.\nThe fact that unsupervised pretraining improves supervised learning is also reported in [13], [14], [15], where RBMs and autoencoders are used as pretraining methods for generic data (mainly the MNIST handwritten digit dataset in these papers). As pretraining explores underlying data features unsupervised, it gives a much more meaningful initialization of parameters. Therefore, the deep neural networks can be trained much faster and more effectively. Our experimental results in program analysis are consistent with these reports in the literature in other fields.\nTo evaluate whether deep learning may be helpful for program analysis, we compare TCNN to baseline methods in the program classification task. In these baseline methods, we adopt the bag-of-words model, which is a widely-used approach in text classification [52]. As shown in Table 4, logistic regression, as a linear classifier, achieves 81.16% accuracy. The support vector machine (SVM) with radial basis function (RBF) kernel explores non-linearity, and improves the result by 10%. By automatically exploring the underlying features and patterns of programs, TCNN further improves the accuracy by more than 4%. This experiment suggests the promising future of deep leaning approaches in the field of program analysis.\nTo sum up, we evaluate the learned representations empirically by nearest neighbor querying and k-means clustering. Program classification experiment shows the learned representations are greatly beneficial for supervised learning.\nBased on the above experiments, we conclude that the proposed \u201ccoding criterion\u201d based on ASTs is a successful representation learning algorithm for programs.\nOur experimental result in program classification confirms the feasibility of using deep learning to analyze programs. It also shows primary evidence of its success in the new field."}, {"heading": "VI. LOOKING FORWARD TO THE FUTURE", "text": "As evidence in the literature show, deep learning is making breakthroughs in many fields of artificial intelligence. We believe it will also become an important method in various tasks in the field of program analysis. As a pioneering study,\nwe address the following promising research topics in this new area."}, {"heading": "A. Different Perspectives for Program Modeling", "text": "In this paper, we treat a program as a tree, where each node corresponds to an \u201cabstract\u201d component of the program. We hypothesize in this paper that structural information is important to programs to a large extent, and our experiments confirm our conjecture. However, the AST is not the only perspective of program modeling.\nAnother possible perspective is treating a program as a sequence of statements. Such perspective is also adopted in traditional program analysis, e.g., API usage pattern mining [53], [54]. As the representations can be composited by atomic symbols (e.g., AST nodes), we can also apply deep learning approaches to sequences of statements. Although some structural information may be lost, the neighboring information are captured and local patterns can be extracted.\nTreating a program as a 2-dimensional signal is an interesting, novel and also meaningful perspective, which is bionicsinspired. As we, human beings, always read source codes on a 2-D screen, it is also possible for neural networks to model programs in this perspective. Indents and linefeeds on the 2-D screen are useful features because they suggest strong semantics of programs. Existing techniques in computer vision can be applied, e.g. the convolutional neural network (CNN). CNN is analogous to visual cortex of human brains, and thus it has the solid biological foundation in cognitive science. Interestingly, as a bionics-inspired model, deep CNN achieved unexpected high performance [37] before pretraining methods were invented.\nB. Integrating Prior about Programs to Network Architectures\nDespite the fact that a unified architecture of deep neural networks is applicable to various tasks with high performance, we can also integrate human priors to the networks.\nCNN is an example that specifies explicitly the physically neighborhood information of an image. Physically neighboring pixels form local patterns (e.g. a circle, a line), which can be detected by convolution kernels. Being fed forward to higher layers in the network, the local patterns emerge high-level abstract features. Such abstract features are beneficial for the ultimate task (e.g., object recognition). Another widely-used domain specific prior in deep learning is slowness [55], [56]. As it is not desired that features of image/acoustic data are changing too fast, penalties of variation are added to the cost function, so that the learned features are \u201cslow.\u201d\nFor program analysis, priors can also be integrated to the neural networks. In one of our undergoing research, we would like to capture the local features of ASTs. A tree-based convolutional neural network (TCNN) is proposed and studied. Primary results have been reported in Section V-B.\nAnother prior is that we can integrate formal methods of program analysis to neural networks (or vise versa). [57] is an example of neural reasoning for knowledge base. For programs, even though all non-trivial properties are undecidable,\nformal methods can be viewed as an approximation with pure mathematical deduction, often giving the guarantee of either no false-positive, or no false-negative, which may be important to program analysis [58]. For now, it seems hard to combine these two techniques, but once they were combined, it would be beneficial for both.\nC. Various Applications\nThe application of deep learning in the field of program analysis is another promising research topic, which is at least as important as, if not more important than, the theory of deep learning. Some are pointed out in Section I, including code clone detection, bug detection, and code retrieval.\nIn short, as deep learning is brand new to program analysis, the questions addressed in this part still remain unknown to the literature. It is not clear which perspective is most proper to model programs, or which is most suitable for what application. It is also not very clear how to integrate human priors about programs to the neural network architecture. These are among the fundamental questions of deep learning when it is applied to the new field. Besides, real-world applications of deep learning are also important for program analysis."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we study deep learning and representation learning in the field of program analysis. We propose a novel \u201ccoding criterion\u201d to build vector representations of nodes in ASTs, which make deep learning a reality for program analysis. We also feed the learned representations to a deep neural network to classify programs.\nThe experimental results show that our representations successfully capture the similarity and relationships among different nodes in ASTs. The learned representations significantly improve supervised training for deep neural networks in terms of both optimization and generalization. We conclude that the coding criterion is successful in building program vector representations. The experiments also confirm the feasibility of deep learning to analyze programs, and show primary evidence of its success in the new field.\nAs a pioneering study, we address several fundamental problems, including the perspectives of program modeling, the integration of human priors and the applications of deep learning.\nTo promote further researches in the new field, we publish all of our datasets, source codes, and learned representations online.\nWe believe, considering the fact that deep learning has made breakthroughs in many fields of artificial intelligence, along with the primary evidence reported in this paper, deep learning will become an outstanding approach of program analysis in the near future. We call for more studies in this new, prospective field."}], "references": [{"title": "Software defect prediction using semi-supervised learning with dimension reduction", "author": ["H. Lu", "B. Cukic", "M. Culp"], "venue": "Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting memory leaks through introspective dynamic behavior modelling using machine learning", "author": ["S. Lee", "C. Jung", "S. Pande"], "venue": "Proceedings of 36th International Conference on Software Engineering, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining the execution history of a software system to infer the best time for its adaptation", "author": ["K. Canavera", "N. Esfahani", "S. Malek"], "venue": "Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "On the naturalness of software", "author": ["A. Hindle", "E. Barr", "Z. Su", "M. Gabel", "P. Devanbu"], "venue": "Proceedings of 34th International Conference on Software Engineering, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Phone recognition with the mean-covariance restricted Boltzmann machine", "author": ["G. Dahl", "A. Mohamed", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 14\u201322, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine learning, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Greedy layerwise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["D. Erhan", "P. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "Proceedings of International Conference on Artificial Intelligence and Statistics, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1\u201340, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Feature-based detection of bugs in clones", "author": ["D. Steidl", "N. Gode"], "venue": "7th International Workshop on Software Clones, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Syntax tree fingerprinting for source code similarity detection", "author": ["M. Chilowicz", "E. Duris", "G. Roussel"], "venue": "Proceedings of IEEE 17th International Conference on Program Comprehension, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "WordNet: a lexical database for English", "author": ["G. Miller"], "venue": "Communications of the ACM, vol. 38, no. 11, pp. 39\u201341, 1995.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proceedings of International Conference on Artificial Intelligence and Statistics, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "What\u2019s the code?: Automatic classification of source code archives", "author": ["S. Ugurel", "R. Krovetz", "L. Giles"], "venue": "Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K. Murphy"], "venue": "MIT press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Clone detection in software source code using operational similarity of statements", "author": ["R. Kaur", "S. Singh"], "venue": "ACM SIGSOFT Software Engineering Notes, vol. 39, no. 3, pp. 1\u20135, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Source code retrieval using sequence based similarity", "author": ["Y. Udagawa"], "venue": "International Journal of Data Mining & Knowledge Management Process, no. 4, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimizing a search-based code recommendation system", "author": ["N. Murakami", "H. Masuhara"], "venue": "3rd International Workshop on Recommendation Systems for Software Engineering, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th International Conference on Machine learning, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E. Huang", "R. Socher", "C. Manning", "A. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Studying the language and structure in non-programmers\u2019 solutions to programming problems", "author": ["J. Pane", "C. Ratanamahatana", "B. Myers"], "venue": "International Journal of Human-Computer Studies, vol. 54, no. 2, pp. 237\u2013264, 2001.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1828}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals and Systems, vol. 2, no. 4, pp. 303\u2013314, 1989.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1989}, {"title": "On the power of small-depth threshold circuits", "author": ["J. Hastad", "M. Goldmann"], "venue": "Computational Complexity, vol. 1, no. 2, pp. 113\u2013129, 1991.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1991}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E. Sackinger"], "venue": "Proceedings of International Conference on Artificial Neural Networks, 1995.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1995}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshop, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "INTERSPEECH, 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Clone detection using abstract syntax trees", "author": ["I. Baxter", "A. Yahin", "L. Moura", "M. Sant\u2019Anna", "L. Bier"], "venue": "Proceedings of the International Conference on Software Maintenance, 1998.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "Clone detection algorithm based on the Abstract Syntax Tree approach", "author": ["F. Lazar", "O. Banias"], "venue": "Proceedings of 9th IEEE International Symposium on Applied Computational Intelligence and Informatic, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Generalized vulnerability extrapolation using abstract syntax trees", "author": ["F. Yamaguchi", "M. Lottmann", "K. Rieck"], "venue": "Proceedings of 28th Annual Computer Security Applications Conference, 2012.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "Q. Le", "C. Manning", "A. Ng"], "venue": "NIPS Deep Learning Workshop, 2013.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E. Huang", "J. Pennin", "C. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K. Hermann", "P. Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to order things", "author": ["W. Cohen", "R. Schapire", "Y. Singer"], "venue": "Advances in Neural Information Processing Systems, 1998.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1998}, {"title": "Computing semantic relatedness using Wikipedia-based explicit semantic analysis.", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2007}, {"title": "Comparison and evaluation of code clone detection techniques and tools: A qualitative approach", "author": ["C. Roy", "J. Cordy", "R. Koschke"], "venue": "Science of Computer Programming, vol. 74, no. 7, pp. 470\u2013495, 2009.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "The Text Mining Handbook: Advanced Approaches in Analyzing Unstructured Data", "author": ["R. Feldman", "J. Sanger"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "Mining succinct and high-coverage API usage patterns from source code", "author": ["J. Wang", "Y. Dang", "H. Zhang", "K. Chen", "T. Xie", "D. Zhang"], "venue": "Proceedings of IEEE Working Conference on Mining Software Repositories, 2013.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Mining API patterns as partial orders from source code: From usage scenarios to specifications", "author": ["M. Acharya", "T. Xie", "J. Pei", "J. Xu"], "venue": "Proc. of ESEC/SIGSOFT FSE, 2007.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning transformational invariants from natural movies", "author": ["C. Cadieu", "B. Olshausen"], "venue": "Advances in Neural Information Processing Systems, 2008.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2008}, {"title": "Slow, decorrelated features for pretraining complex cell-like networks", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2009.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "A.N.C. Manning"], "venue": "Advances in Neural Information Processing Systems, 2013.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "Leveraging test generation and specification mining for automated bug detection without false positives", "author": ["M. Pradel", "T. Gross"], "venue": "Proceeings of 24th International Conference on Software Engineering, 2012.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Machine learning-based program analysis has been studied long in the literature [1], [2], [3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "Machine learning-based program analysis has been studied long in the literature [1], [2], [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "Machine learning-based program analysis has been studied long in the literature [1], [2], [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "compare programming languages to natural languages and conclude that programs also have rich statistical properties [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "The deep neural network, also known as deep learning, has become one of the prevailing machine learning approaches since 2006 [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 125, "endOffset": 128}, {"referenceID": 8, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 130, "endOffset": 133}, {"referenceID": 9, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 154, "endOffset": 158}, {"referenceID": 10, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "Interestingly, with even a unified model, deep learning achieves better performance than state-of-the-art approaches in many heterogeneous tasks [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "Since no proper \u201cpretraining\u201d method is proposed for programs, deep neural networks cannot be trained effectively with pure back propagation [13], [14], [15] because gradients would either vanish or blow up through the deep architecture [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "Since no proper \u201cpretraining\u201d method is proposed for programs, deep neural networks cannot be trained effectively with pure back propagation [13], [14], [15] because gradients would either vanish or blow up through the deep architecture [16].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "Since no proper \u201cpretraining\u201d method is proposed for programs, deep neural networks cannot be trained effectively with pure back propagation [13], [14], [15] because gradients would either vanish or blow up through the deep architecture [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 15, "context": "Since no proper \u201cpretraining\u201d method is proposed for programs, deep neural networks cannot be trained effectively with pure back propagation [13], [14], [15] because gradients would either vanish or blow up through the deep architecture [16].", "startOffset": 237, "endOffset": 241}, {"referenceID": 16, "context": ", [17] for bug detection, [18] for clone detection.", "startOffset": 2, "endOffset": 6}, {"referenceID": 17, "context": ", [17] for bug detection, [18] for clone detection.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "report\u2014for example, in the field of natural language processing (NLP)\u2014the automatically learned taxonomy of words [19] is better in their application than the famous WordNet constructed by experts [20] used in [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 19, "context": "report\u2014for example, in the field of natural language processing (NLP)\u2014the automatically learned taxonomy of words [19] is better in their application than the famous WordNet constructed by experts [20] used in [21].", "startOffset": 197, "endOffset": 201}, {"referenceID": 20, "context": "report\u2014for example, in the field of natural language processing (NLP)\u2014the automatically learned taxonomy of words [19] is better in their application than the famous WordNet constructed by experts [20] used in [21].", "startOffset": 210, "endOffset": 214}, {"referenceID": 21, "context": "Such deep learning architectures require less human engineering than existing methods like [22].", "startOffset": 91, "endOffset": 95}, {"referenceID": 22, "context": "As pointed out in [23], \u201cmany decision problems can be reduced to classification.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "\u2022 bug detection as [17], to which the deep learning approach is to automatically extract features of bugs; \u2022 clone detection as [24], to automatically match the features of two programs; \u2022 code retrieval as [25], to automatically match program features with that of the queries; and \u2022 code recommendation as [26], to automatically predict the probability of the next possible codes, e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "\u2022 bug detection as [17], to which the deep learning approach is to automatically extract features of bugs; \u2022 clone detection as [24], to automatically match the features of two programs; \u2022 code retrieval as [25], to automatically match program features with that of the queries; and \u2022 code recommendation as [26], to automatically predict the probability of the next possible codes, e.", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "\u2022 bug detection as [17], to which the deep learning approach is to automatically extract features of bugs; \u2022 clone detection as [24], to automatically match the features of two programs; \u2022 code retrieval as [25], to automatically match program features with that of the queries; and \u2022 code recommendation as [26], to automatically predict the probability of the next possible codes, e.", "startOffset": 207, "endOffset": 211}, {"referenceID": 25, "context": "\u2022 bug detection as [17], to which the deep learning approach is to automatically extract features of bugs; \u2022 clone detection as [24], to automatically match the features of two programs; \u2022 code retrieval as [25], to automatically match program features with that of the queries; and \u2022 code recommendation as [26], to automatically predict the probability of the next possible codes, e.", "startOffset": 308, "endOffset": 312}, {"referenceID": 26, "context": "[27] in NLP).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Chances are that we end up with both poor optimization and poor generalization if the network is deep [13], [14], [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Chances are that we end up with both poor optimization and poor generalization if the network is deep [13], [14], [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "Chances are that we end up with both poor optimization and poor generalization if the network is deep [13], [14], [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 87, "endOffset": 91}, {"referenceID": 27, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 104, "endOffset": 107}, {"referenceID": 28, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "However, due to the structural differences between natural languages and programming languages [30], existing representation learning algorithms in NLP are improper for programs.", "startOffset": 95, "endOffset": 99}, {"referenceID": 30, "context": "Comprehensive reviews include [31], [32].", "startOffset": 30, "endOffset": 34}, {"referenceID": 31, "context": "Comprehensive reviews include [31], [32].", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "It can be proved that a 2-layer2 neural network with sufficient hidden units can approximate arbitrary Boolean or continuous functions, and that a 3-layer network can approximate any function [34].", "startOffset": 192, "endOffset": 196}, {"referenceID": 33, "context": "However, the shallow architecture is inefficient because the number of hidden units may grow exponentially in order to learn complicated (highly non-linear) features of data [35].", "startOffset": 174, "endOffset": 178}, {"referenceID": 30, "context": "The theory of circuits suggests deep architectures would be more efficient to capture complicated features [31].", "startOffset": 107, "endOffset": 111}, {"referenceID": 34, "context": "Few successful researches were reported in early years using deep architectures (except convolutional neural networks [37]).", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "proposed stacked restricted Boltzmann machine (RBM) as a greedy layer-wise pretraining method for deep neural networks [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 12, "context": "Shortly after that, stacked autoencoders are used for pretraining [13], the criterion of which is to minimize the reconstruction error.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "According to the experiments reported in [13], [14], [15], pretraining helps optimization (minimizing the training error) as well as generalization (minimizing the test error).", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "According to the experiments reported in [13], [14], [15], pretraining helps optimization (minimizing the training error) as well as generalization (minimizing the test error).", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "According to the experiments reported in [13], [14], [15], pretraining helps optimization (minimizing the training error) as well as generalization (minimizing the test error).", "startOffset": 53, "endOffset": 57}, {"referenceID": 26, "context": "In [27], they predict the probability of each word given n\u22121 previous words.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "introduce 3 energybased models in [28], where they learn word representations by minimizing the energy (maximizing the likelihood) defined on neighboring words.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "In [21], [19], hierarchical architectures are proposed to reduce the computational cost in calculating the probabilities.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [21], [19], hierarchical architectures are proposed to reduce the computational cost in calculating the probabilities.", "startOffset": 9, "endOffset": 13}, {"referenceID": 35, "context": "Fast algorithms are then proposed in [38], [39].", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "Fast algorithms are then proposed in [38], [39].", "startOffset": 43, "endOffset": 47}, {"referenceID": 37, "context": "Recurrent neural network (RNN) is introduced in order to capture long-term dependencies [40].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "However, RNN may be very difficult to train since the gradient would either vanish or blow up during back propagation [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 38, "context": "Although some researches explore character-level modeling for NLP [41], it is improper for programming languages.", "startOffset": 66, "endOffset": 70}, {"referenceID": 39, "context": "This level is also used in traditional program analysis like code clone detection [42], [43], vulnerability extrapolation [44], etc.", "startOffset": 82, "endOffset": 86}, {"referenceID": 40, "context": "This level is also used in traditional program analysis like code clone detection [42], [43], vulnerability extrapolation [44], etc.", "startOffset": 88, "endOffset": 92}, {"referenceID": 41, "context": "This level is also used in traditional program analysis like code clone detection [42], [43], vulnerability extrapolation [44], etc.", "startOffset": 122, "endOffset": 126}, {"referenceID": 42, "context": "Such researches in NLP is often referred to as compositional semantics [45].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "This is referred to as \u201cdisentangling the underlying factors of variation\u201d in [32].", "startOffset": 78, "endOffset": 82}, {"referenceID": 43, "context": "To solve this problem, one extreme is to apply dynamic pooling [46], [47].", "startOffset": 63, "endOffset": 67}, {"referenceID": 44, "context": "To solve this problem, one extreme is to apply dynamic pooling [46], [47].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "This is also mathematically equivalent to the continuous bagof-words model [38].", "startOffset": 75, "endOffset": 79}, {"referenceID": 42, "context": "Another extreme is to assign a different matrix parameter for each different position [45].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "To solve the problem, negative sampling can be applied [12], [45], [48].", "startOffset": 55, "endOffset": 59}, {"referenceID": 42, "context": "To solve the problem, negative sampling can be applied [12], [45], [48].", "startOffset": 61, "endOffset": 65}, {"referenceID": 45, "context": "To solve the problem, negative sampling can be applied [12], [45], [48].", "startOffset": 67, "endOffset": 71}, {"referenceID": 46, "context": "Hence, negative sampling method is also sometimes referred to as the pairwise ranking criterion [49].", "startOffset": 96, "endOffset": 100}, {"referenceID": 47, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 96, "endOffset": 100}, {"referenceID": 48, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 167, "endOffset": 171}, {"referenceID": 24, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 188, "endOffset": 192}, {"referenceID": 12, "context": "The fact that unsupervised pretraining improves supervised learning is also reported in [13], [14], [15], where RBMs and autoencoders are used as pretraining methods for generic data (mainly the MNIST handwritten digit dataset in these papers).", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "The fact that unsupervised pretraining improves supervised learning is also reported in [13], [14], [15], where RBMs and autoencoders are used as pretraining methods for generic data (mainly the MNIST handwritten digit dataset in these papers).", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "The fact that unsupervised pretraining improves supervised learning is also reported in [13], [14], [15], where RBMs and autoencoders are used as pretraining methods for generic data (mainly the MNIST handwritten digit dataset in these papers).", "startOffset": 100, "endOffset": 104}, {"referenceID": 49, "context": "In these baseline methods, we adopt the bag-of-words model, which is a widely-used approach in text classification [52].", "startOffset": 115, "endOffset": 119}, {"referenceID": 50, "context": ", API usage pattern mining [53], [54].", "startOffset": 27, "endOffset": 31}, {"referenceID": 51, "context": ", API usage pattern mining [53], [54].", "startOffset": 33, "endOffset": 37}, {"referenceID": 34, "context": "Interestingly, as a bionics-inspired model, deep CNN achieved unexpected high performance [37] before pretraining methods were invented.", "startOffset": 90, "endOffset": 94}, {"referenceID": 52, "context": "Another widely-used domain specific prior in deep learning is slowness [55], [56].", "startOffset": 71, "endOffset": 75}, {"referenceID": 53, "context": "Another widely-used domain specific prior in deep learning is slowness [55], [56].", "startOffset": 77, "endOffset": 81}, {"referenceID": 54, "context": "[57] is an example of neural reasoning for knowledge base.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "For programs, even though all non-trivial properties are undecidable, formal methods can be viewed as an approximation with pure mathematical deduction, often giving the guarantee of either no false-positive, or no false-negative, which may be important to program analysis [58].", "startOffset": 274, "endOffset": 278}], "year": 2014, "abstractText": "Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation. In this pioneering paper, we propose the \u201ccoding criterion\u201d to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations. To evaluate whether deep learning is beneficial for program analysis, we feed the representations to deep neural networks, and achieve higher accuracy in the program classification task than \u201cshallow\u201d methods, such as logistic regression and the support vector machine. This result confirms the feasibility of deep learning to analyze programs. It also gives primary evidence of its success in this new field. We believe deep learning will become an outstanding technique for program analysis in the near future.", "creator": "LaTeX with hyperref package"}}}