{"id": "1409.3854", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2014", "title": "Linear, Deterministic, and Order-Invariant Initialization Methods for the K-Means Clustering Algorithm", "abstract": "Over part some both decades, formula_2 - means most become the schemas decoding \u2014 giving 2003 than procedure domains primarily due one addition emphasis, time / space efficiency, and three-fold came the authorities of the data points. Unfortunately, the algorithm ' 1 qualities did the initial prior brought the resemble health highly must be its highly failure advantages. Numerous initialization application much because proposed to address this timing. Many especially not methods, however, without time analysis superlinear brought the number of data goals, which really ones impracticable giving large product allowing. On form particularly pulled, geometry qualitative are most entries and / or sensitive to was instructions of the data seconds. These methods themselves generally reliable rest one that quality of through test common frightening. Therefore, rather is exists history not taken additional 52 country called methodology part allowed called output of the run that produces the outstanding predict. Such when practice, however, greatly significant the computational implementation but taken otherwise highly adequate {- change differential. In this document, wrong investigate through observations impressive on months linear, inference (organizations - example ), having needed - orthogonal k - makes codice_9 precise well took large of creating collection a data advance close place UCI Machine Learning Repository. The pre demonstrate which two relatively unknown corporatist quantum-mechanical methods initial to Su been Dy underperformed place three previous methods having nor decided which objective assessing assessment. In well, a sparked algorithms further to Erisoglu cr\u00f3nica al. stand-up uncharacteristically normally.", "histories": [["v1", "Fri, 12 Sep 2014 20:11:36 GMT  (102kb)", "http://arxiv.org/abs/1409.3854v1", "21 pages, 2 figures, 5 tables, Partitional Clustering Algorithms (Springer, 2014). arXiv admin note: substantial text overlap witharXiv:1304.7465,arXiv:1209.1960"]], "COMMENTS": "21 pages, 2 figures, 5 tables, Partitional Clustering Algorithms (Springer, 2014). arXiv admin note: substantial text overlap witharXiv:1304.7465,arXiv:1209.1960", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["m emre celebi", "hassan a kingravi"], "accepted": false, "id": "1409.3854"}, "pdf": {"name": "1409.3854.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["ecelebi@lsus.edu", "kingravi@gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n38 54\nv1 [\ncs .L\nG ]\n1 2\nM. Emre Celebi Department of Computer Science Louisiana State University, Shreveport, LA, USA e-mail: ecelebi@lsus.edu\nHassan A. Kingravi School of Electrical and Computer Engineering Georgia Institute of Technology, Atlanta, GA, USA e-mail: kingravi@gatech.edu\n1"}, {"heading": "1 Introduction", "text": "Clustering, the unsupervised classification of patterns into groups, is one of the most important tasks in exploratory data analysis [58]. Primary goals of clustering include gaining insight into, classifying, and compressing data. Clustering has a long and rich history in a variety of scientific disciplines including anthropology, biology, medicine, psychology, statistics, mathematics, engineering, and computer science. As a result, numerous clustering algorithms have been proposed since the early 1950s [57].\nClustering algorithms can be broadly classified into two groups: hierarchical and partitional [58]. Hierarchical algorithms recursively find nested clusters either in a top-down (divisive) or bottom-up (agglomerative) fashion. In contrast, partitional algorithms find all the clusters simultaneously as a partition of the data and do not impose a hierarchical structure. Most hierarchical algorithms have time complexity quadratic or higher in the number of data points [110] and therefore are not suitable for large data sets, whereas partitional algorithms often have lower complexity.\nGiven a data set X = {x1,x2, . . . ,xN} \u2282 R D, i.e., N points (vectors) each with D attributes (components), hard partitional algorithms divide X into K exhaustive and mutually exclusive clusters C = {C1, C2, . . . , CK}, \u22c3K\ni=1 Ci = X , Ci \u2229 Cj = \u2205 for 1 \u2264 i 6= j \u2264 K. These algorithms usually generate clusters by optimizing a criterion function [47]. The most intuitive and frequently used criterion function is the Sum of Squared Error (SSE) given by\nSSE =\nK \u2211\ni=1\n\u2211\nxj\u2208Ci\n\u2016xj \u2212 ci\u2016 2\n2 , (1)\nwhere\nci = 1\n|Ci|\n\u2211\nxj\u2208Ci\nxj (2)\nand\n\u2016xj\u20162 =\n(\nD \u2211\nd=1\nx2jd\n)1/2\n(3)\ndenote the centroid of cluster Ci (with cardinality |Ci|) and the Euclidean (\u21132) norm of vector xj = (xj1, xj2, . . . , xjD), respectively.\nThe number of ways in which a set of N objects can be partitioned into K non-empty groups is given by Stirling numbers of the second kind\nS(N,K) = 1\nK!\nK \u2211\ni=0\n(\u22121)K\u2212i ( K\ni\n)\niN , (4)\nwhich can be approximated by KN/K! It can be seen that a complete enumeration of all possible clusterings to determine the global minimum of (1) is clearly computationally prohibitive except for very small data sets. In fact, this non-convex optimization problem is proven to be NP-hard even forK = 2 [30, 4] or D = 2 [105, 78]. Consequently, various heuristics have been developed to provide approximate solutions to this problem [101]. Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28]. In contrast, recent approaches are predominantly based on various metaheuristics [93, 29] that are capable of avoiding bad local minima at the expense of significantly increased computational requirements. These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90]. Among all these heuristics, Lloyd\u2019s algorithm [76], often referred to as the (batch) k-means algorithm, is the simplest and most commonly used one. This algorithm starts with K arbitrary centers, typically chosen uniformly at random from the data points. Each point is assigned to the nearest center and then each center is recalculated as the mean of all points assigned to it. These two steps are repeated until a predefined termination criterion is met. K-means can be expressed in algorithmic notation as follows:\n1. Choose the initial set of centers c1, c2, . . . , cK arbitrarily. 2. Assign point xj (j \u2208 {1, 2, . . . , N}) to the nearest center with respect to\n\u21132 distance, that is\nxj \u2208 C\u0131\u0302 \u21d0\u21d2 \u0131\u0302 = argmin i\u2208{1,2,...,K}\n\u2016xj \u2212 ci\u2016 2\n2 .\n3. Recalculate center ci (i \u2208 {1, 2, . . . ,K}) as the centroid of Ci, that is\nci = 1\n|Ci|\n\u2211\nxj\u2208Ci\nxj .\n4. Repeat steps 2 and 3 until convergence.\nK-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47]. Its popularity can be attributed to several reasons. First, it is conceptually simple and easy to implement. Virtually every data mining software includes an implementation of it. Second, it is versatile, i.e., almost every aspect of the algorithm (initialization, distance function, termination criterion, etc.) can be modified. This is evidenced by hundreds of publications over the last fifty years that extend k-means in a variety of ways. Third, it has a time complexity that is linear in N , D, and K (in general, D \u226a N and K \u226a N). For this reason, it can be used to initialize more expensive clustering algorithms such as expectation\nmaximization [81], fuzzy c-means [16, p. 35], DBSCAN [31], spectral clustering [107, 27], ant colony clustering [83], and particle swarm clustering [104]. Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature. Fourth, it has a storage complexity that is linear in N , D, and K. In addition, there exist disk-based variants that do not require all points to be stored in memory [20, 37, 87, 61]. Fifth, it is guaranteed to converge [96] at a quadratic rate [18]. Finally, it is invariant to data ordering, i.e., random shufflings of the data points.\nOn the other hand, k-means has several significant disadvantages. First, it requires the number of clusters, K, to be specified in advance. The value of this parameter can be determined automatically by means of various internal/relative cluster validity measures [106, 9, 6]. Second, it can only detect compact, hyperspherical clusters that are well separated. This can be alleviated by using a more general distance function such as the Mahalanobis distance, which permits the detection of hyperellipsoidal clusters [79, 80]. Third, due its utilization of the squared Euclidean distance, it is sensitive to noise and outlier points since even a few such points can significantly influence the means of their respective clusters. This can be addressed by outlier pruning [111] or by using a more robust distance function such as the city-block (\u21131) distance [98, 59, 36]. Fourth, due to its gradient descent nature, it often converges to a local minimum of the criterion function [96]. For the same reason, it is highly sensitive to the selection of the initial centers [25]. Adverse effects of improper initialization include empty clusters, slower convergence, and a higher chance of getting stuck in bad local minima [23]. Fortunately, except for the first two, these drawbacks can be remedied by using an adaptive initialization method (IM).\nA large number of IMs have been proposed in the literature [92, 53, 23, 32, 25]. Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity). In contrast, linear IMs are often random and/or order-sensitive [38, 60, 77, 11, 103, 99, 19, 7], which renders their results unreliable. In this study, we investigate the empirical performance of six linear, deterministic (non-random), and orderinvariant k-means IMs on a large and diverse collection of data sets from the UCI Machine Learning Repository.\nThe rest of the chapter is organized as follows. Section 2 presents an overview of linear, deterministic, and order-invariant k-means IMs. Section 3 describes the experimental setup. Section 4 presents and discusses the experimental results. Finally, Section 5 gives the conclusions."}, {"heading": "2 Linear, Deterministic, and Order-Invariant K-Means Initialization Methods", "text": "In this study, we focus on IMs that have time complexity linear in N . This is because k-means itself has linear complexity, which is perhaps the most important reason for its popularity. Therefore, an IM for k-means should not diminish this advantage of the algorithm. Accordingly, the following six linear, deterministic, and order-invariant IMs are investigated.\nThe maximin (MM) method [46] chooses the first center c1 arbitrarily from the data points and the remaining (K \u2212 1) centers are chosen successively as follows. In iteration i (i \u2208 {2, 3, . . . ,K}), the ith center ci is chosen to be the point with the greatest minimum \u21132 distance to the previously selected (i\u22121) centers, i.e., c1, c2, . . . , ci\u22121. This method can be expressed in algorithmic notation as follows:\n1. Choose the first center c1 arbitrarily from the data points. 2. Choose the next center ci (i \u2208 {2, 3, . . . ,K}) as the point x\u0302 that satisfies\n\u0302 = argmax j\u2208{1,2,...,N}\n(\nmin k\u2208{1,2,...,i\u22121}\n\u2016xj \u2212 ck\u2016 2\n2\n)\n.\n3. Repeat step 2 (K \u2212 1) times.\nDespite the fact that it was originally developed as a 2-approximation to the K-center clustering problem1, MM is commonly used as a k-means initializer2. In this study, the first center is chosen to be the centroid of X given by\nx\u0304 = 1\nN\nN \u2211\nj=1\nxj . (5)\nNote that c1 = x\u0304 gives the optimal SSE when K = 1. Katsavounidis et al.\u2019s method (KK) [66] is identical to MM with the exception that the first center is chosen to be the point with the greatest \u21132 norm 3, that is, the point x\u0302 that satisfies\n1 Given a set of N points in a metric space, the goal of K-center clustering is to find K representative points (centers) such that the maximum distance of a point to a center is minimized [51, p. 63]. A polynomial-time algorithm is said to be a \u03b4-approximation algorithm for a minimization problem if for every instance of the problem it delivers a solution whose cost is at most \u03b4 times the cost of the optimal solution (\u03b4 is often referred to as the \u201capproximation ratio\u201d or \u201capproximation factor\u201d) [54, p. xv]. 2 Interestingly, several authors including Thorndike [102], Casey and Nagy [22], Batchelor and Wilkins [13], Kennard and Stone [68], and Tou and Gonzalez [103, pp. 92\u201394] had proposed similar (or even identical) methods decades earlier. Gonzalez [46], however, was the one to prove the theoretical properties of the method. 3 This choice was motivated by a vector quantization application.\n\u0302 = argmax j\u2208{1,2,...,N}\n\u2016xj\u2016 2\n2 . (6)\nThe PCA-Part (PP) method [100] uses a divisive hierarchical approach based on Principal Component Analysis (PCA) [63]. Starting from an initial cluster that contains the entire data set X , the method successively selects the cluster with the greatest SSE and divides it into two subclusters using a hyperplane that passes through the cluster centroid and is orthogonal to the principal eigenvector of the cluster covariance matrix. This iterative cluster selection and splitting procedure is repeated (K \u2212 1) times. The final centers are then given by the centroids of the resulting K subclusters. This method can be expressed in algorithmic notation as follows:\n1. Let Ci be the cluster with the greatest SSE and ci be the centroid of this cluster. In the first iteration, C1 = X and c1 = x\u0304. 2. Let p be the projection of ci on the principal eigenvector vi of Ci, i.e., p = ci \u00b7 vi, where \u2018\u00b7\u2019 denotes the dot product. 3. Divide Ci into two subclusters Ci1 and Ci2 according to the following rule: For any xj \u2208 Ci, if xj \u00b7 vi \u2264 p, then assign xj to Ci1 ; otherwise, assign it to Ci2 . 4. Repeat steps 1\u20133 (K \u2212 1) times.\nThe Var-Part (VP) method [100] is an approximation to PP, where, in each iteration, the covariance matrix of the cluster to be split is assumed to be diagonal. In this case, the splitting hyperplane is orthogonal to the coordinate axis with the greatest variance. In other words, the only difference between VP and PP is the choice of the projection axis.\nFigure 1 [24] illustrates VP on a toy data set with four natural clusters [95][67, p. 100]. In iteration 1, the initial cluster that contains the entire data set is split into two subclusters along the Y axis using a line (i.e., a one-dimensional hyperplane) passing through the mean point (92.026667). Between the resulting two clusters, the one above the line has a greater SSE. In iteration 2, this cluster is thus split along the X axis at the mean point (66.975000). In the final iteration, the cluster with the greatest SSE, i.e., the bottom cluster, is split along the X axis at the mean point (41.057143). In Figure 1(d), the centroids of the final four clusters are denoted by stars.\nThe maxisum (MS) method4 is a recent modification of MM. It can be expressed in algorithmic notation as follows:\n1. Determine the attribute with the greatest absolute coefficient of variation (ratio of the standard deviation to the mean), that is, the attribute x.d1 that satisfies\nd1 = argmax d\u2208{1,2,...,D}\n\u2223 \u2223 \u2223 \u2223 sd md \u2223 \u2223 \u2223 \u2223 ,\nwhere\n4 Eris\u0327og\u0306lu et al., Pattern Recognition Letters 32(14), 1701\u20131705 (2011)\nmd = 1\nN\nN \u2211\nj=1\nxjd\nand\ns2d = 1\nN \u2212 1\nN \u2211\nj=1\n(xjd \u2212md) 2\ndenote the mean and variance of the dth attribute x.d, respectively. 2. Determine the attribute with the least Pearson product-moment correla-\ntion with x.d1 , that is, the attribute x.d2 that satisfies\nd2 = argmin d\u2208{1,2,...,D}\nN \u2211\nj=1\n(xjd1 \u2212md1)(xjd \u2212md)\n\u221a\nN \u2211\nj=1\n(xjd1 \u2212md1) 2\n\u221a\nN \u2211\nj=1\n(xjd \u2212md)2\n. (7)\nNote that since we calculated the mean and standard deviation of each attribute in step 1, the following expression can be used in place of (7) to save computational time:\nd2 = argmin d\u2208{1,2,...,D}\nN \u2211\nj=1\n(\nxjd1 \u2212md1 sd1\n)(\nxjd \u2212md sd\n)\n. (8)\n3. Let Y = {y1,y2, . . . ,yN} \u2282 R 2 be the projection ofX = {x1,x2, . . . ,xN} \u2282\nR D onto the two-dimensional subspace determined in steps 1 and 2. In\nother words, yj = (xjd1 , xjd2 ) for j \u2208 {1, 2, . . . , N}. 4. Choose the first center c1 as the point farthest from the centroid y\u0304 of Y\nwith respect to \u21132 distance, that is, the point y\u0302 that satisfies\n\u0302 = argmax j\u2208{1,2,...,N}\n\u2016yj \u2212 y\u0304\u2016 2\n2 .\n5. Choose the next center ci (i \u2208 {2, 3, . . . ,K}) as the point with the greatest cumulative \u21132 distance from the previously selected (i \u2212 1) centers, that is, the point y\u0302 that satisfies\n\u0302 = argmax j\u2208{1,2,...,N}\ni\u22121 \u2211\nk=1\n\u2016yj \u2212 ck\u20162 .\n6. Repeat step 5 (K \u2212 1) times.\nNote that steps 1 and 2 above provide rough approximations to the first two PCs and that steps 4 and 5 are performed in the two-dimensional subspace spanned by the attributes determined in steps 1 and 2.\nClearly, MS is a derivative of MM. Differences between the two methods are as follows:\n\u22b2 MM chooses the first center arbitrarily from the data points, whereas MS chooses it to be the point farthest from the mean of the projected data set. \u22b2 MM chooses the remaining (K \u2212 1) centers iteratively based on their minimum distance from the previously selected centers, whereas MS uses a cumulative distance criterion. Note that while the selection criterion used in MM provides an approximation guarantee of factor 2 for the K-center clustering problem (see footnote 1 on page 5), it is unclear whether or not MS offers any approximation guarantees. \u22b2 MM performs the distance calculations in the original D-dimensional space, whereas MS works in a two-dimensional subspace. A serious drawback of the projection operation employed in MS is that the method disregards all attributes but two and therefore is likely to be effective only for data sets in which the variability is mostly on two dimensions. Unfor-\ntunately, the motivation behind this particular projection scheme is not given by Eris\u0327og\u0306lu et al.\nInterestingly, MS also bears a striking resemblance to a method proposed by DeSarbo et al. [33] almost three decades earlier. The latter method differs from the former in two ways. First, it works in the original D-dimensional space. Second, it chooses the first two centers as the pair of points with the greatest \u21132 distance. Unfortunately, the determination of the first two centers in this method leads to a time complexity quadratic in N . Therefore, this method was not included in the experiments. More recently, Glasbey et al. [42] mentioned a very similar method within the context of color palette design.\nWe also experimented with a modified version of the MS method (MS+), which is identical to MS with the exception that there is no projection involved. In other words, MS+ operates in the original D-dimensional space.\nFor a comprehensive overview of these methods and others, the reader is referred to a recent article by Celebi et al. [25]. It should be noted that, in this study, we do not attempt to compare a mix of deterministic and random IMs. Instead, we focus on deterministic methods for two main reasons. First, these methods are generally computationally more efficient as they need to be executed only once. In contrast, random methods are inherently unreliable in that the quality of their results is unpredictable and thus it is common practice to perform multiple runs of such methods and take the output of the run5 that produces the best objective function value. Second, several studies [100, 24, 25] demonstrated that despite the fact that they are executed only once, some deterministic methods are highly competitive with well-known and effective random methods such as Bradley and Fayyad\u2019s method [19] and k-means++ [7]."}, {"heading": "3 Experimental Setup", "text": ""}, {"heading": "3.1 Data Set Descriptions", "text": "The experiments were performed on 24 commonly used data sets from the UCI Machine Learning Repository [39]. Table 1 gives the data set descriptions. For each data set, the number of clusters (K) was set equal to the number of classes (K \u2032), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].\n5 Each \u2018run\u2019 of a random IM involves the execution of the IM itself followed by that of the clustering algorithm, e.g., k-means."}, {"heading": "3.2 Attribute Normalization", "text": "Normalization is a common preprocessing step in clustering that is necessary to prevent attributes with large variability from dominating the distance calculations and also to avoid numerical instabilities in the computations. Two commonly used normalization schemes are linear scaling to unit range (min-max normalization) and linear scaling to unit variance (z-score normalization). Several studies revealed that the former scheme is preferable to the latter since the latter is likely to eliminate valuable between-cluster variation [82, 43, 44]. As a result, we used the min-max normalization scheme to map the attributes of each data set to the [0, 1] interval."}, {"heading": "3.3 Performance Criteria", "text": "The performance of the IMs was quantified using two effectiveness (quality) and one efficiency (speed) criteria:\n\u22b2 Initial SSE (IS): This is the SSE value calculated after the initialization phase, before the clustering phase. It gives us a measure of the effectiveness of an IM by itself. \u22b2 Final SSE (FS): This is the SSE value calculated after the clustering phase. It gives us a measure of the effectiveness of an IM when its output is refined by k-means. Note that this is the objective function of the k-means algorithm, i.e., (1). \u22b2 Number of Iterations (NI): This is the number of iterations that kmeans requires until reaching convergence when initialized by a particular IM. It is an efficiency measure independent of programming language, implementation style, compiler, and CPU architecture. Note that we do not report CPU time measurements since on most data sets that we tested each of the six IMs completed within a few milliseconds (gcc v4.4.5, Intel Core i7-3960X 3.30GHz).\nThe convergence of k-means was controlled by the disjunction of two criteria: the number of iterations reaches a maximum of 100 or the relative improvement in SSE between two consecutive iterations drops below a threshold [75], i.e., (SSEi\u22121 \u2212 SSEi) /SSEi \u2264 \u01eb, where SSEi denotes the SSE value at the end of the ith (i \u2208 {2, . . . , 100}) iteration. The convergence threshold was set to \u01eb = 10\u22126."}, {"heading": "4 Experimental Results and Discussion", "text": "Tables 2\u20134 give the performance measurements for each method (the best values are underlined). Since the number of iterations fall within [0, 100], we can directly obtain descriptive statistics such as minimum, maximum, mean, and median for this criterion over the 24 data sets. In contrast, initial/final SSE values are unnormalized and therefore incomparable across different data sets. In order to circumvent this problem, for each data set, we calculated the percent SSE of each method relative to the worst (greatest) SSE. For example, it can be seen from Table 2 that on the Breast Cancer Wisconsin data set the initial SSE of MM is 498, whereas the worst initial SSE on the same data set is 596 and thus the ratio of the former to the latter is 0.836. This simply means that on this data set MM obtains 100(1\u2212 0.836) \u2248 16% better initial SSE than the worst method, KK. Table 5 gives the summary statistics for the normalized initial/final SSE\u2019s obtained in this manner and those for the number of iterations. As usual, min (minimum) and max (maximum) represent the best and worst case performance, respectively. Mean represents the aver-\nage case performance, whereas median quantifies the typical performance of a method without regard to outliers. For example, with respect to the initial SSE criterion, PP performs, on the average, about 100\u2212 21.46 \u2248 79% better than the worst method.\nFor convenient visualization, Figure 2 shows the box plots that depict the five-number summaries (minimum, 25th percentile, median, 75th percentile, and maximum) for the normalized initial/final SSE\u2019s calculated in the aforementioned manner and the five-number summary for the number of iterations. Here, the bottom and top end of the whiskers of a box represent the minimum and maximum, respectively, whereas the bottom and top of the box itself are the 25th percentile (Q1 ) and 75th percentile (Q3 ), respectively. The line that passes through the box is the 50th percentile (Q2 ), i.e., the median, while the small square inside the box denotes the mean.\nWith respect to effectiveness, the following observations can be made:\nWith respect to computational efficiency, the following observations can be made:\n\u22b2 An average (or typical) run of KK lead to the fastest k-means convergence. \u22b2 An average (or typical) run of PP lead to the second fastest k-means convergence. \u22b2 An average run of MS lead to the slowest k-means convergence. \u22b2 A typical run of MM lead to the slowest k-means convergence.\nIn summary, our experiments showed that VP and PP performed very similarly with respective to both effectiveness criteria and they outperformed the\nremaining four methods by a large margin. The former method has a time complexity of O(ND), whereas the latter one has a complexity of O(ND2) when implemented using the power method [55]. Therefore, on high dimensional data sets, the former method might be preferable. On the other hand, on low dimensional data sets, the latter method might be preferable as it often leads to faster k-means convergence. The main disadvantage of these two methods is that they are more complicated to implement due to their hierarchical formulation. As for the remaining four methods, when compared to MM, KK was significantly worse in terms of initial SSE, slightly better in terms of final SSE, and significantly better in terms of number of iterations. Interestingly, despite its similarities with MM, the most recent method that we examined, i.e., MS, often gave the worst results. It was also demonstrated that by eliminating the 2-dimensional projection step, the performance of MS can be substantially improved with respect to final SSE. This, however, comes at the expense of a performance degradation with respect to initial SSE. Consequently, in either of its forms, the MS method rediscovered recently by Eris\u0327og\u0306lu et al. does not appear to outperform the classical MM method or the more recent hierarchical methods VP and PP. This is not surprising given that MS can easily choose two nearby points as centers provided that they each have a large cumulative distance to the remaining centers [42]."}, {"heading": "5 Conclusions", "text": "In this chapter we examined six linear, deterministic, and order-invariant methods used for the initialization of the k-means clustering algorithm. These included the popular maximin method and three of its variants and two relatively unknown divisive hierarchical methods. Experiments on a large and diverse collection of real-world data sets from the UCI Machine Learning Repository demonstrated that the hierarchical methods outperform the remaining four methods with respect to two objective effectiveness criteria. These hierarchical methods can be used to initialize k-means effectively, particularly in time-critical applications that involve large data sets. Alternatively, they can be used as approximate clustering algorithms without additional k-means refinement. Our experiments also revealed that the most recent variant of the maximin method performs surprisingly poorly."}, {"heading": "6 Acknowledgments", "text": "This work was supported by a grant from the US National Science Foundation (1117457)."}], "references": [{"title": "A New Algorithm for Cluster Initialization", "author": ["M. Al-Daoud"], "venue": "Proceedings of the 2nd World Enformatika Conference, pp. 74\u201376", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust Partitional Clustering by Outlier and Density Insensitive Seeding", "author": ["M. Al Hasan", "V. Chaoji", "S. Salem", "M. Zaki"], "venue": "Pattern Recognition Letters 30(11), 994\u2013 1002", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "A Tabu Search Approach for the Minimum Sum-of-Squares Clustering Problem", "author": ["K.S. Al-Sultan"], "venue": "Pattern Recognition 28(9), 1443\u20131451", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "NP-Hardness of Euclidean Sum-ofSquares Clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Machine Learning 75(2), 245\u2013248", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "K-Means Clustering Algorithm for Multimedia Applications with Flexible HW/SW Co-Design", "author": ["F. An", "H.J. Mattausch"], "venue": "Journal of Systems Architecture 59(3), 155\u2013164", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "An Extensive Comparative Study of Cluster Validity Indices", "author": ["O. Arbelaitz", "I. Gurrutxaga", "J. Muguerza", "J.M. P\u00e9rez", "I. Perona"], "venue": "Pattern Recognition 46(1), 243\u2013256", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "K-Means++: The Advantages of Careful Seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1027\u20131035", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Speech Analysis by Clustering, or the Hyperphoneme Method", "author": ["M.M. Astrahan"], "venue": "Tech. Rep. AIM-124, Stanford University", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1970}, {"title": "Investigation of Internal Validity Measures for K-Means Clustering", "author": ["J. Baarsch", "M.E. Celebi"], "venue": "Proceedings of the 2012 IAENG International Conference on Data Mining and Applications, pp. 471\u2013476", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Clustering with Evolution Strategies", "author": ["G.P. Babu", "M.N. Murty"], "venue": "Pattern Recognition 27(2), 321\u2013329", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "A Clustering Technique for Summarizing Multivariate Data", "author": ["G.H. Ball", "D.J. Hall"], "venue": "Behavioral Science 12(2), 153\u2013155", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1967}, {"title": "A Transfer Algorithm for Non-Hierarchical Classification", "author": ["C.F. Banfield", "L.C. Bassill"], "venue": "Applied Statistics 26(2), 206\u2013210", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1977}, {"title": "Method for Location of Clusters of Patterns to Initialise a Learning Machine", "author": ["B.G. Batchelor", "B.R. Wilkins"], "venue": "Electronics Letters 5(20), 481\u2013483", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1969}, {"title": "Scaling Up Machine Learning: Parallel and Distributed Approaches", "author": ["R. Bekkerman", "M. Bilenko", "Langford", "J. (eds."], "venue": "Cambridge University Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "A Survey of Clustering Data Mining Techniques", "author": ["P. Berkhin"], "venue": "J. Kogan, C. Nicholas, M. Teboulle (eds.) Grouping Multidimensional Data: Recent Advances in Clustering, pp. 25\u201371. Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Fuzzy Models and Algorithms for Pattern Recognition and Image Processing", "author": ["J.C. Bezdek", "J. Keller", "R. Krisnapuram", "N.R. Pal"], "venue": "Kluwer Academic Publishers", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Clustering Methods: A History of K-Means Algorithms", "author": ["H.H. Bock"], "venue": "P. Brito, G. Cucumel, P. Bertrand, F. de Carvalho (eds.) Selected Contributions in Data Analysis and Classification, pp. 161\u2013172. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Convergence Properties of the K-Means Algorithms", "author": ["L. Bottou", "Y. Bengio"], "venue": "G. Tesauro, D.S. Touretzky, T.K. Leen (eds.) Advances in Neural Information Processing Systems 7, pp. 585\u2013592. MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Refining Initial Points for K-Means Clustering", "author": ["P.S. Bradley", "U. Fayyad"], "venue": "Proceedings of the 15th International Conference on Machine Learning, pp. 91\u201399", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Scaling Clustering Algorithms to Large Databases", "author": ["P.S. Bradley", "U. Fayyad", "C. Reina"], "venue": "Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining, pp. 9\u201315", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "An Initialization Method for the K-Means Algorithm Using Neighborhood Model", "author": ["F. Cao", "J. Liang", "G. Jiang"], "venue": "Computers and Mathematics with Applications 58(3), 474\u2013483", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "An Autonomous Reading Machine", "author": ["R.G. Casey", "G. Nagy"], "venue": "IEEE Transactions on Computers 17(5), 492\u2013503", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1968}, {"title": "Improving the Performance of K-Means for Color Quantization", "author": ["M.E. Celebi"], "venue": "Image and Vision Computing 29(4), 260\u2013271", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Deterministic Initialization of the K-Means Algorithm Using Hierarchical Clustering", "author": ["M.E. Celebi", "H.A. Kingravi"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence 26(7), 1250,018", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "A Comparative Study of Efficient Initialization Methods for the K-Means Clustering Algorithm", "author": ["M.E. Celebi", "H.A. Kingravi", "P.A. Vela"], "venue": "Expert Systems with Applications 40(1), 200\u2013210", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Bandwidth Adaptive Hardware Architecture of K-Means Clustering for Video Analysis", "author": ["T.W. Chen", "S.Y. Chien"], "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems 18(6), 957\u2013966", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallel Spectral Clustering in Distributed Systems", "author": ["W.Y. Chen", "Y. Song", "H. Bai", "C.J. Lin", "E.Y. Chang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 33(3), 568\u2013586", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Information Geometry and Alternating Minimization Procedures", "author": ["I. Csiszar", "G. Tusnady"], "venue": "Statistics and Decisions, Supplemental Issue (1), 205\u2013237", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1984}, {"title": "Metaheuristic Clustering", "author": ["S. Das", "A. Abraham", "Konar", "A. (eds."], "venue": "Springer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "The Hardness of K-Means Clustering", "author": ["S. Dasgupta"], "venue": "Tech. Rep. CS2008-0916, University of California, San Diego", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "1 + 1 > 2\u2019: Merging Distance and Density Based Clustering", "author": ["M. Dash", "H. Liu", "X. Xu"], "venue": "Proceedings of the 7th International Conference on Database Systems for Advanced Applications, pp. 32\u201339", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "On Initializations for the Minkowski Weighted K-Means", "author": ["R.C. de Amorim", "P. Komisarczuk"], "venue": "Proceedings of the 11th International Symposium on Intelligent Data Analysis, pp. 45\u201355", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Synthesized Clustering: A Method for Amalgamating Alternative Clustering Bases with Differential Weighting of Variables", "author": ["W.S. DeSarbo", "J.D. Carroll", "L.A. Clark", "P.E. Green"], "venue": "Psychometrika 49(1), 57\u201378", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1984}, {"title": "Accelerated K-Means with Adaptive Distance Bounds", "author": ["J. Drake", "G. Hamerly"], "venue": "Proceedings of the 5th NIPS Workshop on Optimization for Machine Learning", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Using the Triangle Inequality to Accelerate K-Means", "author": ["C. Elkan"], "venue": "Proceedings of the 20th International Conference on Machine Learning, pp. 147\u2013153", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "Fast and Robust General Purpose Clustering Algorithms", "author": ["V. Estivill-Castro", "J. Yang"], "venue": "Data Mining and Knowledge Discovery 8(2), 127\u2013150", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Scalability for Clustering Algorithms Revisited", "author": ["F. Farnstrom", "J. Lewis", "C. Elkan"], "venue": "SIGKDD Explorations 2(1), 51\u201357", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Cluster Analysis of Multivariate Data: Efficiency vs", "author": ["E. Forgy"], "venue": "Interpretability of Classification. Biometrics 21, 768", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1965}, {"title": "UCI Machine Learning Repository", "author": ["A. Frank", "A. Asuncion"], "venue": "http://archive. ics.uci.edu/ml", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "K-Means", "author": ["J. Ghosh", "A. Liu"], "venue": "X. Wu, V. Kumar (eds.) The Top Ten Algorithms in Data Mining, pp. 21\u201335. Chapman and Hall/CRC", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Histogram-Based Method for Effective Initialization of the K-Means Clustering Algorithm", "author": ["C. Gingles", "M.E. Celebi"], "venue": "Proceedings of the 27th International Florida Artificial Intelligence Research Society Conference, pp. 333\u2013338", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Colour Displays for Categorical Images", "author": ["C. Glasbey", "G. van der Heijden", "V.F.K. Toh", "A. Gray"], "venue": "Color Research and Application 32(4), 304\u2013309", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Weighting and Selection of Variables for Cluster Analysis", "author": ["R. Gnanadesikan", "J.R. Kettenring"], "venue": "Journal of Classification 12(1), 113\u2013136", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1995}, {"title": "Better Alternatives to Current Methods of Scaling and Weighting Data for Cluster Analysis", "author": ["R. Gnanadesikan", "J.R. Kettenring", "S. Maloor"], "venue": "Journal of Statistical Planning and Inference 137(11), 3483\u20133496", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "Experience with a Hybrid Processor: K-Means Clustering", "author": ["M. Gokhale", "J. Frigo", "K. McCabe", "J. Theiler", "C. Wolinski", "D. Lavenier"], "venue": "The Journal of Supercomputing 26(2), 131\u2013148", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2003}, {"title": "Clustering to Minimize the Maximum Intercluster Distance", "author": ["T. Gonzalez"], "venue": "Theoretical Computer Science 38(2\u20133), 293\u2013306", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1985}, {"title": "Objective Function-Based Clustering", "author": ["L.O. Hall"], "venue": "WIREs Data Mining and Knowledge Discovery 2(4), 326\u2013339", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Making K-Means Even Faster", "author": ["G. Hamerly"], "venue": "Proceedings of the 2010 SIAM International Conference on Data Mining, pp. 130\u2013140", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Ant-Based Clustering and Topographic Mapping", "author": ["J. Handl", "J. Knowles", "M. Dorigo"], "venue": "Artificial Life 12(1), 35\u201361", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}, {"title": "J-Means: A New Local Search Heuristic for Minimum Sum of Squares Clustering", "author": ["P. Hansen", "N. Mladenovic"], "venue": "Pattern Recognition 34(2), 405\u2013413", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2001}, {"title": "Geometric Approximation Algorithms", "author": ["S. Har-Peled"], "venue": "American Mathematical Society", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Algorithm AS 136: A K-Means Clustering Algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Journal of the Royal Statistical Society C 28(1), 100\u2013108", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1979}, {"title": "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study", "author": ["J. He", "M. Lan", "C.L. Tan", "S.Y. Sung", "H.B. Low"], "venue": "Proceedings of the 2004 IEEE International Joint Conference on Neural Networks, pp. 297\u2013302", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2004}, {"title": "Approximation Algorithms for NP-Hard Problems", "author": ["Hochbaum", "D.S. (ed."], "venue": "PWS Publishing Company", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1997}, {"title": "Simplified Calculation of Principal Components", "author": ["H. Hotelling"], "venue": "Psychometrika 1(1), 27\u201335", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1936}, {"title": "High Speed C-Means Clustering in Reconfigurable Hardware", "author": ["W.J. Hwang", "C.C. Hsu", "H.Y. Li", "S.K. Weng", "T.Y. Yu"], "venue": "Microprocessors and Microsystems 34(6), 237\u2013 246", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2010}, {"title": "Data Clustering: 50 Years Beyond K-Means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters 31(8), 651\u2013666", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Data Clustering: A Review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys 31(3), 264\u2013323", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1999}, {"title": "A Clustering Method Based on the L1-norm", "author": ["K. Jajuga"], "venue": "Computational Statistics & Data Analysis 5(4), 357\u2013371", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1987}, {"title": "Multidimensional Group Analysis", "author": ["R.C. Jancey"], "venue": "Australian Journal of Botany 14(1), 127\u2013130", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1966}, {"title": "Fast and Exact Out-Of-Core and Distributed K-Means Clustering", "author": ["R. Jin", "A. Goswami", "G. Agrawal"], "venue": "Knowledge and Information Systems 10(1), 17\u201340", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2006}, {"title": "A General Framework for Efficient Clustering of Large Datasets Based on Activity Detection", "author": ["X. Jin", "S. Kim", "J. Han", "L. Cao", "Z. Yin"], "venue": "Statistical Analysis and Data Mining 4(1), 11\u201329", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2011}, {"title": "Principal Component Analysis, Second edn", "author": ["I.T. Jolliffe"], "venue": "Springer", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2002}, {"title": "K-Means Clustering Seeds Initialization Based on Centrality, Sparsity, and Isotropy", "author": ["P. Kang", "S. Cho"], "venue": "Proceedings of the 10th International Conference on Intelligent Data Engineering and Automated Learning, pp. 109\u2013117", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2009}, {"title": "An Efficient K-Means Clustering Algorithm: Analysis and Implementation", "author": ["T. Kanungo", "D. Mount", "N. Netanyahu", "C. Piatko", "R. Silverman", "A. Wu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 24(7), 881\u2013892", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2002}, {"title": "A New Initialization Technique for Generalized Lloyd Iteration", "author": ["I. Katsavounidis", "C.C.J. Kuo", "Z. Zhang"], "venue": "IEEE Signal Processing Letters 1(10), 144\u2013146", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1994}, {"title": "Finding Groups in Data: An Introduction to Cluster Analysis", "author": ["L. Kaufman", "P. Rousseeuw"], "venue": "Wiley-Interscience", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1990}, {"title": "Computer Aided Design of Experiments", "author": ["R.W. Kennard", "L.A. Stone"], "venue": "Technometrics 11(1), 137\u2013148", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1969}, {"title": "Experiments in Projection and Clustering by Simulated Annealing", "author": ["R.W. Klein", "R.C. Dubes"], "venue": "Pattern Recognition 22(2), 213\u2013220", "citeRegEx": "69", "shortCiteRegEx": null, "year": 1989}, {"title": "K-Means for Parallel Architectures Using All-Prefix-Sum Sorting and Updating Steps", "author": ["K.J. Kohlhoff", "V.S. Pande", "R.B. Altman"], "venue": "IEEE Transactions on Parallel and Distributed Systems 24(8), 1602\u20131612", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2013}, {"title": "A Fast K-Means Clustering Algorithm Using Cluster Center Displacement", "author": ["J.Z.C. Lai", "T.J. Huang", "Y.C. Liaw"], "venue": "Pattern Recognition 42(11), 2551\u20132556", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2009}, {"title": "A General Theory of Classificatory Sorting Strategies - II", "author": ["G.N. Lance", "W.T. Williams"], "venue": "Clustering Systems. The Computer Journal 10(3), 271\u2013277", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1967}, {"title": "Speeding up K-Means Algorithm by GPUs", "author": ["Y. Li", "K. Zhao", "X. Chu", "J. Liu"], "venue": "Journal of Computer and System Sciences 79(2), 216\u2013229", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2013}, {"title": "The Global K-Means Clustering Algorithm", "author": ["A. Likas", "N. Vlassis", "J. Verbeek"], "venue": "Pattern Recognition 36(2), 451\u2013461", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2003}, {"title": "An Algorithm for Vector Quantizer Design", "author": ["Y. Linde", "A. Buzo", "R. Gray"], "venue": "IEEE Transactions on Communications 28(1), 84\u201395", "citeRegEx": "75", "shortCiteRegEx": null, "year": 1980}, {"title": "Least Squares Quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory 28(2), 129\u2013136", "citeRegEx": "76", "shortCiteRegEx": null, "year": 1982}, {"title": "Some Methods for Classification and Analysis of Multivariate Observations", "author": ["J. MacQueen"], "venue": "Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability, pp. 281\u2013297", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1967}, {"title": "The Planar k-Means Problem is NP-hard", "author": ["M. Mahajan", "P. Nimbhorkar", "K. Varadarajan"], "venue": "Theoretical Computer Science 442, 13\u201321", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2012}, {"title": "A Self-Organizing Network for Hyperellipsoidal Clustering (HEC)", "author": ["J. Mao", "A.K. Jain"], "venue": "IEEE Transactions on Neural Networks 7(1), 16\u201329", "citeRegEx": "79", "shortCiteRegEx": null, "year": 1996}, {"title": "On K-Means Algorithm with the Use of Mahalanobis Distances", "author": ["I. Melnykov", "V. Melnykov"], "venue": "Statistics & Probability Letters 84, 88\u201395", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2014}, {"title": "Challenges in Model-Based Clustering", "author": ["V. Melnykov"], "venue": "Wiley Interdisciplinary Reviews: Computational Statistics 5(2), 135\u2013148", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2013}, {"title": "A Study of Standardization of Variables in Cluster Analysis", "author": ["G. Milligan", "M.C. Cooper"], "venue": "Journal of Classification 5(2), 181\u2013204", "citeRegEx": "82", "shortCiteRegEx": null, "year": 1988}, {"title": "On Improving Clustering in Numerical Databases with Artificial Ants", "author": ["N. Monmarch\u00e9", "M. Slimane", "G. Venturini"], "venue": "Proceedings of the 5th European Conference on Advances in Artificial Life, pp. 626\u2013635", "citeRegEx": "83", "shortCiteRegEx": null, "year": 1999}, {"title": "In Search of Optimal Clusters Using Genetic Algorithms", "author": ["C.A. Murthy", "N. Chowdhury"], "venue": "Pattern Recognition Letters 17(8), 825\u2013832", "citeRegEx": "84", "shortCiteRegEx": null, "year": 1996}, {"title": "An Overview of Clustering Methods", "author": ["M.G.H. Omran", "A.P. Engelbrecht", "A. Salman"], "venue": "Intelligent Data Analysis 11(6), 583\u2013605", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2007}, {"title": "Careful Seeding Method based on Independent Components Analysis for K-Means Clustering", "author": ["T. Onoda", "M. Sakai", "S. Yamada"], "venue": "Journal of Emerging Technologies in Web Intelligence 4(1), 51\u201359", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient Disk-Based K-Means Clustering for Relational Databases", "author": ["C. Ordonez", "E. Omiecinski"], "venue": "IEEE Transactions on Knowledge and Data Engineering 16(8), 909\u2013921", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2004}, {"title": "A Scatter Search Approach for the Minimum Sum-of-Squares Clustering Problem", "author": ["J.A. Pacheco"], "venue": "Computers and Operations Research 32(5), 1325\u20131335", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2005}, {"title": "Design of Hybrids for the Minimum Sum-of-Squares Clustering Problem", "author": ["J.A. Pacheco", "O. Valencia"], "venue": "Computational Statistics & Data Analysis 43(2), 235\u2013248", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2003}, {"title": "Differential Evolution and Particle Swarm Optimization in Partitional Clustering", "author": ["S. Paterlini", "T. Krink"], "venue": "Computational Statistics & Data Analysis 50(5), 1220\u20131247", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2006}, {"title": "Accelerating Exact K-Means Algorithms with Geometric Reasoning", "author": ["D. Pelleg", "A. Moore"], "venue": "Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 277\u2013281", "citeRegEx": "91", "shortCiteRegEx": null, "year": 1999}, {"title": "An Empirical Comparison of Four Initialization Methods for the K-Means Algorithm", "author": ["J.M. Pena", "J.A. Lozano", "P. Larranaga"], "venue": "Pattern Recognition Letters 20(10), 1027\u20131040", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1999}, {"title": "Metaheuristics for Clustering in KDD", "author": ["V.J. Rayward-Smith"], "venue": "Proceedings of the IEEE Congress on Evolutionary Computation, vol. 3, pp. 2380\u20132387", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2005}, {"title": "A Method for Initialising the K-Means Clustering Algorithm Using kd-trees", "author": ["S.J. Redmond", "C. Heneghan"], "venue": "Pattern Recognition Letters 28(8), 965\u2013973", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2007}, {"title": "Numerical Methods for Fuzzy Clustering", "author": ["E.H. Ruspini"], "venue": "Information Sciences 2(3), 319\u2013350", "citeRegEx": "95", "shortCiteRegEx": null, "year": 1970}, {"title": "K-Means-Type Algorithms: A Generalized Convergence Theorem and Characterization of Local Optimality", "author": ["S.Z. Selim", "M.A. Ismail"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 6(1), 81\u201387", "citeRegEx": "96", "shortCiteRegEx": null, "year": 1984}, {"title": "Euclidean Cluster Analysis", "author": ["D.N. Sparks"], "venue": "Applied Statistics 22(1), 126\u2013130", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1973}, {"title": "L1 Cluster Analysis", "author": ["H. Sp\u00e4th"], "venue": "Computing 16(4), 379\u2013387", "citeRegEx": "98", "shortCiteRegEx": null, "year": 1976}, {"title": "Computational Experiences with the Exchange Method: Applied to Four Commonly Used Partitioning Cluster Analysis Criteria", "author": ["H. Sp\u00e4th"], "venue": "European Journal of Operational Research 1(1), 23\u201331", "citeRegEx": "99", "shortCiteRegEx": null, "year": 1977}, {"title": "In Search of Deterministic Methods for Initializing K-Means and Gaussian Mixture Clustering", "author": ["T. Su", "J.G. Dy"], "venue": "Intelligent Data Analysis 11(4), 319\u2013338", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2007}, {"title": "A Computational Study of Several Relocation Methods for K-Means Algorithms", "author": ["A. Tarsitano"], "venue": "Pattern Recognition 36(12), 2955\u20132966", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2003}, {"title": "Who Belongs in the Family", "author": ["R.L. Thorndike"], "venue": "Psychometrika 18(4),", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 1953}, {"title": "Pattern Recognition Principles", "author": ["J.T. Tou", "R.C. Gonzales"], "venue": "Addison-Wesley", "citeRegEx": "103", "shortCiteRegEx": null, "year": 1974}, {"title": "Data Clustering Using Particle Swarm Optimization", "author": ["D.W. van der Merwe", "A.P. Engelbrecht"], "venue": "Proceedings of the 2003 IEEE Congress on Evolutionary Computation, pp. 215\u2013220", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2003}, {"title": "K-Means Requires Exponentially Many Iterations Even in the Plane", "author": ["A. Vattani"], "venue": "Discrete & Computational Geometry 45(4), 596\u2013616", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2011}, {"title": "Relative Clustering Validity Criteria: A Comparative Overview", "author": ["L. Vendramin", "Campello", "R.J.G.B.", "E.R. Hruschka"], "venue": "Statistical Analysis and Data Mining 3(4), 209\u2013 235", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2010}, {"title": "A Tutorial on Spectral Clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing 17(4), 395\u2013416", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2007}, {"title": "Clustering Billions of Data Points Using GPUs", "author": ["R. Wu", "B. Zhang", "M. Hsu"], "venue": "Proceedings of the 6th ACM Conference on Computing Frontiers, pp. 1\u20136", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2009}, {"title": "Partitive Clustering (K-Means Family)", "author": ["Y. Xiao", "J. Yu"], "venue": "WIREs Data Mining and Knowledge Discovery 2(3), 209\u2013225", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2012}, {"title": "Survey of Clustering Algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "IEEE Transactions on Neural Networks 16(3), 645\u2013678", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust Clustering by Pruning Outliers", "author": ["J.S. Zhang", "Y.W. Leung"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics \u2013 Part B 33(6), 983\u2013999", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 57, "context": "Clustering, the unsupervised classification of patterns into groups, is one of the most important tasks in exploratory data analysis [58].", "startOffset": 133, "endOffset": 137}, {"referenceID": 56, "context": "As a result, numerous clustering algorithms have been proposed since the early 1950s [57].", "startOffset": 85, "endOffset": 89}, {"referenceID": 57, "context": "Clustering algorithms can be broadly classified into two groups: hierarchical and partitional [58].", "startOffset": 94, "endOffset": 98}, {"referenceID": 109, "context": "Most hierarchical algorithms have time complexity quadratic or higher in the number of data points [110] and therefore are not suitable for large data sets, whereas partitional algorithms often have lower complexity.", "startOffset": 99, "endOffset": 104}, {"referenceID": 46, "context": "These algorithms usually generate clusters by optimizing a criterion function [47].", "startOffset": 78, "endOffset": 82}, {"referenceID": 29, "context": "In fact, this non-convex optimization problem is proven to be NP-hard even forK = 2 [30, 4] or D = 2 [105, 78].", "startOffset": 84, "endOffset": 91}, {"referenceID": 3, "context": "In fact, this non-convex optimization problem is proven to be NP-hard even forK = 2 [30, 4] or D = 2 [105, 78].", "startOffset": 84, "endOffset": 91}, {"referenceID": 104, "context": "In fact, this non-convex optimization problem is proven to be NP-hard even forK = 2 [30, 4] or D = 2 [105, 78].", "startOffset": 101, "endOffset": 110}, {"referenceID": 77, "context": "In fact, this non-convex optimization problem is proven to be NP-hard even forK = 2 [30, 4] or D = 2 [105, 78].", "startOffset": 101, "endOffset": 110}, {"referenceID": 100, "context": "Consequently, various heuristics have been developed to provide approximate solutions to this problem [101].", "startOffset": 102, "endOffset": 107}, {"referenceID": 37, "context": "Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28].", "startOffset": 29, "endOffset": 61}, {"referenceID": 59, "context": "Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28].", "startOffset": 29, "endOffset": 61}, {"referenceID": 76, "context": "Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28].", "startOffset": 29, "endOffset": 61}, {"referenceID": 96, "context": "Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28].", "startOffset": 29, "endOffset": 61}, {"referenceID": 98, "context": "Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28].", "startOffset": 29, "endOffset": 61}, {"referenceID": 11, "context": "Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28].", "startOffset": 29, "endOffset": 61}, {"referenceID": 51, "context": "Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28].", "startOffset": 29, "endOffset": 61}, {"referenceID": 75, "context": "Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28].", "startOffset": 29, "endOffset": 61}, {"referenceID": 27, "context": "Most of the early approaches [38, 60, 77, 97, 99, 12, 52, 76] were simple procedures based on the alternating minimization algorithm [28].", "startOffset": 133, "endOffset": 137}, {"referenceID": 92, "context": "In contrast, recent approaches are predominantly based on various metaheuristics [93, 29] that are capable of avoiding bad local minima at the expense of significantly increased computational requirements.", "startOffset": 81, "endOffset": 89}, {"referenceID": 28, "context": "In contrast, recent approaches are predominantly based on various metaheuristics [93, 29] that are capable of avoiding bad local minima at the expense of significantly increased computational requirements.", "startOffset": 81, "endOffset": 89}, {"referenceID": 68, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 99, "endOffset": 102}, {"referenceID": 83, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 123, "endOffset": 127}, {"referenceID": 49, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 158, "endOffset": 162}, {"referenceID": 88, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 183, "endOffset": 187}, {"referenceID": 87, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 204, "endOffset": 208}, {"referenceID": 48, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 234, "endOffset": 238}, {"referenceID": 89, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 263, "endOffset": 267}, {"referenceID": 89, "context": "These include heuristics based on simulated annealing [69], evolution strategies [10], tabu search [3], genetic algorithms [84], variable neighborhood search [50], memetic algorithms [89], scatter search [88], ant colony optimization [49], differential evolution [90], and particle swarm optimization [90].", "startOffset": 301, "endOffset": 305}, {"referenceID": 75, "context": "Among all these heuristics, Lloyd\u2019s algorithm [76], often referred to as the (batch) k-means algorithm, is the simplest and most commonly used one.", "startOffset": 46, "endOffset": 50}, {"referenceID": 57, "context": "K-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47].", "startOffset": 77, "endOffset": 115}, {"referenceID": 109, "context": "K-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47].", "startOffset": 77, "endOffset": 115}, {"referenceID": 14, "context": "K-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47].", "startOffset": 77, "endOffset": 115}, {"referenceID": 16, "context": "K-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47].", "startOffset": 77, "endOffset": 115}, {"referenceID": 84, "context": "K-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47].", "startOffset": 77, "endOffset": 115}, {"referenceID": 39, "context": "K-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47].", "startOffset": 77, "endOffset": 115}, {"referenceID": 56, "context": "K-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47].", "startOffset": 77, "endOffset": 115}, {"referenceID": 108, "context": "K-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47].", "startOffset": 77, "endOffset": 115}, {"referenceID": 46, "context": "K-means is undoubtedly the most widely used partitional clustering algorithm [58, 110, 15, 17, 85, 40, 57, 109, 47].", "startOffset": 77, "endOffset": 115}, {"referenceID": 80, "context": "maximization [81], fuzzy c-means [16, p.", "startOffset": 13, "endOffset": 17}, {"referenceID": 30, "context": "35], DBSCAN [31], spectral clustering [107, 27], ant colony clustering [83], and particle swarm clustering [104].", "startOffset": 12, "endOffset": 16}, {"referenceID": 106, "context": "35], DBSCAN [31], spectral clustering [107, 27], ant colony clustering [83], and particle swarm clustering [104].", "startOffset": 38, "endOffset": 47}, {"referenceID": 26, "context": "35], DBSCAN [31], spectral clustering [107, 27], ant colony clustering [83], and particle swarm clustering [104].", "startOffset": 38, "endOffset": 47}, {"referenceID": 82, "context": "35], DBSCAN [31], spectral clustering [107, 27], ant colony clustering [83], and particle swarm clustering [104].", "startOffset": 71, "endOffset": 75}, {"referenceID": 103, "context": "35], DBSCAN [31], spectral clustering [107, 27], ant colony clustering [83], and particle swarm clustering [104].", "startOffset": 107, "endOffset": 112}, {"referenceID": 90, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 33, "endOffset": 61}, {"referenceID": 64, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 33, "endOffset": 61}, {"referenceID": 34, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 33, "endOffset": 61}, {"referenceID": 70, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 33, "endOffset": 61}, {"referenceID": 47, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 33, "endOffset": 61}, {"referenceID": 61, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 33, "endOffset": 61}, {"referenceID": 33, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 33, "endOffset": 61}, {"referenceID": 44, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 75, "endOffset": 107}, {"referenceID": 107, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 75, "endOffset": 107}, {"referenceID": 55, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 75, "endOffset": 107}, {"referenceID": 25, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 75, "endOffset": 107}, {"referenceID": 13, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 75, "endOffset": 107}, {"referenceID": 4, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 75, "endOffset": 107}, {"referenceID": 72, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 75, "endOffset": 107}, {"referenceID": 69, "context": "Furthermore, numerous sequential [91, 65, 35, 71, 48, 62, 34] and parallel [45, 108, 56, 26, 14, 5, 73, 70] acceleration techniques are available in the literature.", "startOffset": 75, "endOffset": 107}, {"referenceID": 19, "context": "In addition, there exist disk-based variants that do not require all points to be stored in memory [20, 37, 87, 61].", "startOffset": 99, "endOffset": 115}, {"referenceID": 36, "context": "In addition, there exist disk-based variants that do not require all points to be stored in memory [20, 37, 87, 61].", "startOffset": 99, "endOffset": 115}, {"referenceID": 86, "context": "In addition, there exist disk-based variants that do not require all points to be stored in memory [20, 37, 87, 61].", "startOffset": 99, "endOffset": 115}, {"referenceID": 60, "context": "In addition, there exist disk-based variants that do not require all points to be stored in memory [20, 37, 87, 61].", "startOffset": 99, "endOffset": 115}, {"referenceID": 95, "context": "Fifth, it is guaranteed to converge [96] at a quadratic rate [18].", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "Fifth, it is guaranteed to converge [96] at a quadratic rate [18].", "startOffset": 61, "endOffset": 65}, {"referenceID": 105, "context": "The value of this parameter can be determined automatically by means of various internal/relative cluster validity measures [106, 9, 6].", "startOffset": 124, "endOffset": 135}, {"referenceID": 8, "context": "The value of this parameter can be determined automatically by means of various internal/relative cluster validity measures [106, 9, 6].", "startOffset": 124, "endOffset": 135}, {"referenceID": 5, "context": "The value of this parameter can be determined automatically by means of various internal/relative cluster validity measures [106, 9, 6].", "startOffset": 124, "endOffset": 135}, {"referenceID": 78, "context": "This can be alleviated by using a more general distance function such as the Mahalanobis distance, which permits the detection of hyperellipsoidal clusters [79, 80].", "startOffset": 156, "endOffset": 164}, {"referenceID": 79, "context": "This can be alleviated by using a more general distance function such as the Mahalanobis distance, which permits the detection of hyperellipsoidal clusters [79, 80].", "startOffset": 156, "endOffset": 164}, {"referenceID": 110, "context": "This can be addressed by outlier pruning [111] or by using a more robust distance function such as the city-block (l1) distance [98, 59, 36].", "startOffset": 41, "endOffset": 46}, {"referenceID": 97, "context": "This can be addressed by outlier pruning [111] or by using a more robust distance function such as the city-block (l1) distance [98, 59, 36].", "startOffset": 128, "endOffset": 140}, {"referenceID": 58, "context": "This can be addressed by outlier pruning [111] or by using a more robust distance function such as the city-block (l1) distance [98, 59, 36].", "startOffset": 128, "endOffset": 140}, {"referenceID": 35, "context": "This can be addressed by outlier pruning [111] or by using a more robust distance function such as the city-block (l1) distance [98, 59, 36].", "startOffset": 128, "endOffset": 140}, {"referenceID": 95, "context": "Fourth, due to its gradient descent nature, it often converges to a local minimum of the criterion function [96].", "startOffset": 108, "endOffset": 112}, {"referenceID": 24, "context": "For the same reason, it is highly sensitive to the selection of the initial centers [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "Adverse effects of improper initialization include empty clusters, slower convergence, and a higher chance of getting stuck in bad local minima [23].", "startOffset": 144, "endOffset": 148}, {"referenceID": 91, "context": "A large number of IMs have been proposed in the literature [92, 53, 23, 32, 25].", "startOffset": 59, "endOffset": 79}, {"referenceID": 52, "context": "A large number of IMs have been proposed in the literature [92, 53, 23, 32, 25].", "startOffset": 59, "endOffset": 79}, {"referenceID": 22, "context": "A large number of IMs have been proposed in the literature [92, 53, 23, 32, 25].", "startOffset": 59, "endOffset": 79}, {"referenceID": 31, "context": "A large number of IMs have been proposed in the literature [92, 53, 23, 32, 25].", "startOffset": 59, "endOffset": 79}, {"referenceID": 24, "context": "A large number of IMs have been proposed in the literature [92, 53, 23, 32, 25].", "startOffset": 59, "endOffset": 79}, {"referenceID": 71, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 7, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 51, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 66, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 73, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 0, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 93, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 1, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 20, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 63, "context": "Unfortunately, many of these have time complexity superlinear in N [72, 8, 52, 67, 74, 1, 94, 2, 21, 64], which makes them impractical for large data sets (note that k-means itself has linear time complexity).", "startOffset": 67, "endOffset": 104}, {"referenceID": 37, "context": "In contrast, linear IMs are often random and/or order-sensitive [38, 60, 77, 11, 103, 99, 19, 7], which renders their results unreliable.", "startOffset": 64, "endOffset": 96}, {"referenceID": 59, "context": "In contrast, linear IMs are often random and/or order-sensitive [38, 60, 77, 11, 103, 99, 19, 7], which renders their results unreliable.", "startOffset": 64, "endOffset": 96}, {"referenceID": 76, "context": "In contrast, linear IMs are often random and/or order-sensitive [38, 60, 77, 11, 103, 99, 19, 7], which renders their results unreliable.", "startOffset": 64, "endOffset": 96}, {"referenceID": 10, "context": "In contrast, linear IMs are often random and/or order-sensitive [38, 60, 77, 11, 103, 99, 19, 7], which renders their results unreliable.", "startOffset": 64, "endOffset": 96}, {"referenceID": 102, "context": "In contrast, linear IMs are often random and/or order-sensitive [38, 60, 77, 11, 103, 99, 19, 7], which renders their results unreliable.", "startOffset": 64, "endOffset": 96}, {"referenceID": 98, "context": "In contrast, linear IMs are often random and/or order-sensitive [38, 60, 77, 11, 103, 99, 19, 7], which renders their results unreliable.", "startOffset": 64, "endOffset": 96}, {"referenceID": 18, "context": "In contrast, linear IMs are often random and/or order-sensitive [38, 60, 77, 11, 103, 99, 19, 7], which renders their results unreliable.", "startOffset": 64, "endOffset": 96}, {"referenceID": 6, "context": "In contrast, linear IMs are often random and/or order-sensitive [38, 60, 77, 11, 103, 99, 19, 7], which renders their results unreliable.", "startOffset": 64, "endOffset": 96}, {"referenceID": 45, "context": "The maximin (MM) method [46] chooses the first center c1 arbitrarily from the data points and the remaining (K \u2212 1) centers are chosen successively as follows.", "startOffset": 24, "endOffset": 28}, {"referenceID": 65, "context": "\u2019s method (KK) [66] is identical to MM with the exception that the first center is chosen to be the point with the greatest l2 norm , that is, the point x\uf6be\u0302 that satisfies", "startOffset": 15, "endOffset": 19}, {"referenceID": 101, "context": "2 Interestingly, several authors including Thorndike [102], Casey and Nagy [22], Batchelor and Wilkins [13], Kennard and Stone [68], and Tou and Gonzalez [103, pp.", "startOffset": 53, "endOffset": 58}, {"referenceID": 21, "context": "2 Interestingly, several authors including Thorndike [102], Casey and Nagy [22], Batchelor and Wilkins [13], Kennard and Stone [68], and Tou and Gonzalez [103, pp.", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "2 Interestingly, several authors including Thorndike [102], Casey and Nagy [22], Batchelor and Wilkins [13], Kennard and Stone [68], and Tou and Gonzalez [103, pp.", "startOffset": 103, "endOffset": 107}, {"referenceID": 67, "context": "2 Interestingly, several authors including Thorndike [102], Casey and Nagy [22], Batchelor and Wilkins [13], Kennard and Stone [68], and Tou and Gonzalez [103, pp.", "startOffset": 127, "endOffset": 131}, {"referenceID": 45, "context": "Gonzalez [46], however, was the one to prove the theoretical properties of the method.", "startOffset": 9, "endOffset": 13}, {"referenceID": 99, "context": "The PCA-Part (PP) method [100] uses a divisive hierarchical approach based on Principal Component Analysis (PCA) [63].", "startOffset": 25, "endOffset": 30}, {"referenceID": 62, "context": "The PCA-Part (PP) method [100] uses a divisive hierarchical approach based on Principal Component Analysis (PCA) [63].", "startOffset": 113, "endOffset": 117}, {"referenceID": 99, "context": "The Var-Part (VP) method [100] is an approximation to PP, where, in each iteration, the covariance matrix of the cluster to be split is assumed to be diagonal.", "startOffset": 25, "endOffset": 30}, {"referenceID": 23, "context": "Figure 1 [24] illustrates VP on a toy data set with four natural clusters [95][67, p.", "startOffset": 9, "endOffset": 13}, {"referenceID": 94, "context": "Figure 1 [24] illustrates VP on a toy data set with four natural clusters [95][67, p.", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "[33] almost three decades earlier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] mentioned a very similar method within the context of color palette design.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 99, "context": "Second, several studies [100, 24, 25] demonstrated that despite the fact that they are executed only once, some deterministic methods are highly competitive with well-known and effective random methods such as Bradley and Fayyad\u2019s method [19] and k-means++ [7].", "startOffset": 24, "endOffset": 37}, {"referenceID": 23, "context": "Second, several studies [100, 24, 25] demonstrated that despite the fact that they are executed only once, some deterministic methods are highly competitive with well-known and effective random methods such as Bradley and Fayyad\u2019s method [19] and k-means++ [7].", "startOffset": 24, "endOffset": 37}, {"referenceID": 24, "context": "Second, several studies [100, 24, 25] demonstrated that despite the fact that they are executed only once, some deterministic methods are highly competitive with well-known and effective random methods such as Bradley and Fayyad\u2019s method [19] and k-means++ [7].", "startOffset": 24, "endOffset": 37}, {"referenceID": 18, "context": "Second, several studies [100, 24, 25] demonstrated that despite the fact that they are executed only once, some deterministic methods are highly competitive with well-known and effective random methods such as Bradley and Fayyad\u2019s method [19] and k-means++ [7].", "startOffset": 238, "endOffset": 242}, {"referenceID": 6, "context": "Second, several studies [100, 24, 25] demonstrated that despite the fact that they are executed only once, some deterministic methods are highly competitive with well-known and effective random methods such as Bradley and Fayyad\u2019s method [19] and k-means++ [7].", "startOffset": 257, "endOffset": 260}, {"referenceID": 38, "context": "The experiments were performed on 24 commonly used data sets from the UCI Machine Learning Repository [39].", "startOffset": 102, "endOffset": 106}, {"referenceID": 52, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 6, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 99, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 93, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 1, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 20, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 63, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 85, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 23, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 24, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 40, "context": "For each data set, the number of clusters (K) was set equal to the number of classes (K ), as commonly seen in the related literature [53, 7, 100, 94, 2, 21, 64, 86, 24, 25, 41].", "startOffset": 134, "endOffset": 177}, {"referenceID": 81, "context": "Several studies revealed that the former scheme is preferable to the latter since the latter is likely to eliminate valuable between-cluster variation [82, 43, 44].", "startOffset": 151, "endOffset": 163}, {"referenceID": 42, "context": "Several studies revealed that the former scheme is preferable to the latter since the latter is likely to eliminate valuable between-cluster variation [82, 43, 44].", "startOffset": 151, "endOffset": 163}, {"referenceID": 43, "context": "Several studies revealed that the former scheme is preferable to the latter since the latter is likely to eliminate valuable between-cluster variation [82, 43, 44].", "startOffset": 151, "endOffset": 163}, {"referenceID": 0, "context": "As a result, we used the min-max normalization scheme to map the attributes of each data set to the [0, 1] interval.", "startOffset": 100, "endOffset": 106}, {"referenceID": 74, "context": "The convergence of k-means was controlled by the disjunction of two criteria: the number of iterations reaches a maximum of 100 or the relative improvement in SSE between two consecutive iterations drops below a threshold [75], i.", "startOffset": 222, "endOffset": 226}, {"referenceID": 99, "context": "Since the number of iterations fall within [0, 100], we can directly obtain descriptive statistics such as minimum, maximum, mean, and median for this criterion over the 24 data sets.", "startOffset": 43, "endOffset": 51}, {"referenceID": 54, "context": "The former method has a time complexity of O(ND), whereas the latter one has a complexity of O(ND) when implemented using the power method [55].", "startOffset": 139, "endOffset": 143}, {"referenceID": 41, "context": "This is not surprising given that MS can easily choose two nearby points as centers provided that they each have a large cumulative distance to the remaining centers [42].", "startOffset": 166, "endOffset": 170}], "year": 2014, "abstractText": "Over the past five decades, k-means has become the clustering algorithm of choice in many application domains primarily due to its simplicity, time/space efficiency, and invariance to the ordering of the data points. Unfortunately, the algorithm\u2019s sensitivity to the initial selection of the cluster centers remains to be its most serious drawback. Numerous initialization methods have been proposed to address this drawback. Many of these methods, however, have time complexity superlinear in the number of data points, which makes them impractical for large data sets. On the other hand, linear methods are often random and/or sensitive to the ordering of the data points. These methods are generally unreliable in that the quality of their results is unpredictable. Therefore, it is common practice to perform multiple runs of such methods and take the output of the run that produces the best results. Such a practice, however, greatly increases the computational requirements of the otherwise highly efficient k-means algorithm. In this chapter, we investigate the empirical performance of six linear, deterministic (non-random), and order-invariant k-means initialization methods on a large and diverse collection of data sets from the UCI Machine Learning Repository. The results demonstrate that two relatively unknown hierarchical initialization methods due to Su and Dy outperform the remaining four methods with respect to two objective effectiveness criteria. In addition, a recent method due to Eri\u015fo\u011flu et al. performs surprisingly poorly. M. Emre Celebi Department of Computer Science Louisiana State University, Shreveport, LA, USA e-mail: ecelebi@lsus.edu Hassan A. Kingravi School of Electrical and Computer Engineering Georgia Institute of Technology, Atlanta, GA, USA e-mail: kingravi@gatech.edu", "creator": "LaTeX with hyperref package"}}}