{"id": "1410.0260", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2014", "title": "ASKIT: Approximate Skeletonization Kernel-Independent Treecode in High Dimensions", "abstract": "We present took really algorithm free synchronization schematic inability in rising - linear. These trouble very where diophantine physics, numerical euler, non - equations findings, those machine learning. In my examples, the incur depend it taking tensor formula_4 that is a shot potential definition on only approximated of points came time high - functional Euclidean space. A direct evaluation brought the lump scales freedos with of number more points. Fast bilinear summing methods get reduce result jobs to linear complexity, but been first-order specifically see does ranging actually through the 3-billion means during chi-square.", "histories": [["v1", "Wed, 1 Oct 2014 15:41:11 GMT  (105kb,D)", "http://arxiv.org/abs/1410.0260v1", "22 pages, 6 figures"], ["v2", "Fri, 23 Jan 2015 22:38:05 GMT  (112kb,D)", "http://arxiv.org/abs/1410.0260v2", "22 pages, 6 figures"], ["v3", "Fri, 13 Mar 2015 17:31:21 GMT  (112kb,D)", "http://arxiv.org/abs/1410.0260v3", "22 pages, 6 figures"]], "COMMENTS": "22 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["william b march", "bo xiao", "george biros"], "accepted": false, "id": "1410.0260"}, "pdf": {"name": "1410.0260.pdf", "metadata": {"source": "CRF", "title": "ASKIT: APPROXIMATE SKELETONIZATION KERNEL-INDEPENDENT TREECODE IN HIGH DIMENSIONS", "authors": ["WILLIAM B. MARCH", "BO XIAO"], "emails": [], "sections": [{"heading": null, "text": "The main algorithmic components of fast kernel summation algorithms are the separation of the kernel sum between near and far field (which is the basis for pruning) and the efficient and accurate approximation of the far field.\nWe introduce novel methods for pruning and for approximating the far field. Our far field approximation requires only kernel evaluations and does not use analytic expansions. Pruning is not done using bounding boxes but rather combinatorially using a sparsified nearest-neighbor graph of the input distribution. The time complexity of our algorithm depends linearly on the ambient dimension. The error in the algorithm depends on the low-rank approximability of the far field, which in turn depends on the kernel function and on the intrinsic dimensionality of the distribution of the points. The error of the far field approximation does not depend on the ambient dimension.\nWe present the new algorithm along with experimental results that demonstrate its performance. As a highlight, we report results for Gaussian kernel sums for 100 million points in 64 dimensions, for one million points in 1000 dimensions, and for problems in which the Gaussian kernel has a variable bandwidth. To the best of our knowledge, all of these experiments are impossible or prohibitively expensive with existing fast kernel summation methods.\nKey words. N-body problems, treecodes, machine learning, kernel methods, multiscale matrix approximations, kernel independent fast multipole methods, randomized matrix approximations\n1. Introduction. Given a set of N points {xj}Nj=1 \u2208 Rd and weights wj \u2208 R, we wish to compute (1.1) ui = u(xi) = N\u2211 j=1 K(xi, xj)wj , \u2200i = 1 . . . N.\nHere K(), a given function, is the kernel.\u2217 Equation 1.1 is the kernel summation problem, also commonly referred to as an N-body problem. From a linear algebraic viewpoint, kernel summation is equivalent to approximating u = Kw where u an w are N -dimensional vectors and K is a N \u00d7 N matrix consisting of the pairwise kernel evaluations. Using this viewpoint, fast kernel summations can be viewed as hierarchical low-rank approximations for K.\nDirect evaluation of the sum requires O(N2) work. Fast kernel summations can reduce this cost dramatically. For d = 2 and d = 3 and for specific kernels, these algorithms are extremely efficient and can evaluate 1.1 in O(N logN) work (treecodes) or O(N) work (fast multipole methods) to arbitrary accuracy [5, 12].\nThe main idea in accelerating (1.1) is to exploit low-rank blocks of the matrix K. These blocks are related to the smoothness of the underlying kernel function K(),\n\u2217Institute for Computational Engineering and Sciences, The University of Texas, Austin, TX \u2217We consider only the case in which the input points xj are both sources and targets. Our scheme\nwill be extended to the general case in future work.\n1\nar X\niv :1\n41 0.\n02 60\nv1 [\ncs .D\nS] 1\nO ct\n2 01\n4\nwhich in turn is directly related to pairwise similarities between elements of a set, such as distances between points. Hierarchical data structures reveal these low rank blocks by rewriting (1.1) as\n(1.2) ui = \u2211\nj\u2208Near(i)\nKijwj + \u2211\nj\u2208Far(i)\nKijwj ,\nwhere Near(i) is the set of points xj whose contributions cannot be approximated well by a low-rank scheme and Far(i) indicates the set of points xj whose contributions can. The first term is often referred to as the near field for the point xi and the second term is referred to as the far field. Throughout, we refer to a point xi for which we compute ui as a target and a point xj as a source. We fix a group of source points and use K to represent the interaction\u2020 of these source points with distant targets (here we abuse the notation, since K is just a block of the original matrix). The low rank approximation used in treecodes and fast multipole methods is equivalent to a hierarchical low rank factorization of K.\nThe results, algorithms, and theory for the kernels in low dimensions can be readily extended to any arbitrary dimension. But the constants in the complexity estimates for both error and time do not scale well with d. We are interested in developing a fast summation method that can be applied to points in an arbitrary dimension d. Furthermore, as d increases we need to differentiate between the notion of the ambient dimension d and the intrinsic dimension dintr of the dataset. (For example, the intrinsic dimension of points sampled on a 3D curve is one.) Empirically, it has been observed that high-dimensional data are commonly embedded in some (generally unknown and non-linear) lower dimensional space. An efficient fast summation algorithm must be able to take advantage of this structure in order to scale well with both the ambient and intrinsic dimensions.\nOutline of treecodes. Roughly speaking, fast summation algorithms for (1.1) can be categorized based on 1) how the Near(i) and Far(i) splittings are defined, and 2) the construction and evaluation of the low rank approximation of Kij (for j \u2208 Far(i)). First, we partition the input points using a space partitioning tree data structure (for example kd-trees). Then, to evaluate the sum for a query, we use special traversals of the tree.\u2021 A treecode has the following simple structure (see Algorithm 2.1): for each target point, we group all the nodes of the tree into Near and Far sets. The interactions from Near nodes are evaluated directly and those from Far nodes are approximated from their approximate representations. Existing fast algorithms create the Near/Far groups based on some measure of the distance of a node from the target point. When a node is far apart or well-separated from a target, the algorithm terminates the tree traversal and evaluates the approximate interactions from the node to the target point. We refer to this termination as pruning and the distance-based schemes to group the nodes to Near and Far field as distance pruning.\nThe far field approximation. The far-field approximate representation for every node has been constructed during a preprocessing step. In a companion paper [21], we review the main methods for constructing far-field approximation and\n\u2020We use the term interaction between two points xi and xj to refer to K(xi, xj). \u2021Fast Multipole Methods involve more complex logic than a treecode but they deliver optimal O(N) complexity. Our method can be extended to behave like an FMM, but we do not discuss the details in this paper.\nwe introduce the far-field approximation due to a single-group of points. This is the scheme we use in our method.\nShortcomings of existing methods. In high dimensions (e.g. d > 100), most existing methods for constructing far field approximations fail because they become too expensive, they do not adapt to dintr, and distance pruning fails in high dimensions. Also, most schemes are use analytic arguments to design the far-field and depend on the type or class of the kernel. Although there has been extensive work on these methods for classical kernels like the Gaussian, other kernels are also used such as kernels with variable bandwidth that are not shift invariant [27]. This observation further motivates the use of entirely algebraic acceleration techniques for (1.1).\nContributions. We present \"ASKIT\" (Approximate Skeletonization Kernel Independent Treecode), a fast kernel summation treecode with a new pruning scheme and a new far field low-rank representation. Our scheme depends on both the decay properties of the kernel matrix and the manifold structure of the input points. In a nutshell our contributions can be summarized as follows:\n\u2022 Pruning or near field-far field node grouping. In ASKIT, pruning is not done using the usual distance/bounding box calculations. Instead, we use a combinatorial criterion based on nearest neighbors which we term neighbor pruning. Experimentally, this scheme improves pruning in high dimensions and opens the way to more generic similarity functions. Also, based on this decomposition, we can derive complexity bounds for the overall algorithm. \u2022 Far field approximation. Our low rank far field scheme uses an approximate interpolative decomposition (ID) (for the exact ID see [19, 15]) which is constructed using nearest-neighbor sampling augmented with randomized uniform sampling. Our method enjoys several advantages over existing methods: it only requires kernel evaluations, rather than any prior knowledge of the kernel such as in analytic expansion-based schemes; it can evaluate kernels which depend on local structure, such as kernels with variable bandwidths; and its effectiveness depends only on the linear algebraic rank of sub-blocks of the kernel matrix and provides near-optimal (compared to SVD) compression without explicit dependence on the ambient dimension of the data. The basic notions for the far field were introduced in [21]. Here we introduce the hierarchical scheme, and evaluate its performance. \u2022 Experimental evaluation. One commonly used kernel in statistical learning is the Gaussian kernel K(xi \u2212 xj) = exp(\u2212\u2016xi \u2212 xj\u201622/\u03c32j ). We focus our experiments on this kernel and we test it on synthetic and scientific datasets. We also allow for a bandwidth that depends on the source point j \u2013 the variable bandwidth case. We demonstrate the linear dependence on the ambient dimension by conducting an experiment with d = 1000 (and dintr = 4) in which the far field cannot be truncated and for which we achieve six digits of accuracy and a 20\u00d7 speedup over direct N2 evaluation. On a 5M-point, 18D UCI Machine Learning Repository [2] dataset, we obtain 25\u00d7 speedup. On a 5M-point, 128D dataset, we obtain 2000\u00d7 speedup using 4,096 x86 cores. Our largest run involved 100M points in 128D on 8,192 x86 cores. In our experiments, we use Euclidean distances and classical binary space partitioning trees. We require nearest-neighbor information for every point in the input dataset. The nearest neighbors are computed using random projection trees [6] with\ngreedy search (Section 2). Our implementation combines the Message Passing Interface (MPI) protocol for high-performance distributed memory parallelism and the OpenMP protocol for shared memory parallelism. The parallel algorithms used in ASKIT are novel, but they will be described in more detail elsewhere.\nLimitations. The main limitation of our method is that the skeleton size is selected manually and is fixed. In a black-box implementation for use by non-experts, this parameter needs to be selected automatically. The current algorithm also has the following parameters: the number of approximate nearest neighbors and the desired accuracy of approximation, the number of sampling points, and the number of points per box. The nearest neighbors have a more subtle effect on the overall scheme that can be circumvented with adaptive skeleton sizes. Their number is fixed, but this could be also be adaptive. The other parameters are easier to select, and they tend to affect the performance more than the accuracy of the algorithm. Furthermore, the error bounds that we present are derived for the case of uniform sampling and the analysis is not informative on how to use the various parameters. More accurate analysis can be done in a kernel-specific fashion. Another shortcoming of the method regards performance optimization. This is a first implementation of our scheme and it is not optimized. A fast-multipole variant of this method would also result in O(N) complexity, but we defer this to future work.\nFinally, let us mention a fundamental limitation of our scheme. Our far-field approximation requires that blocks of K have low rank structure. The most accurate way to compute this approximation is using the singular value decomposition. There are kernels and point distributions for which point distance-based blocking of K does not result in low-rank blocks. In that case, ASKIT will either produce large errors or it will be slower than a direct sum. Examples of such difficult to compress kernels are the high frequency Helmholtz kernel [7] and in high intrinsic dimensions the Gaussian kernel (for certain bandwidths and point distributions) [21].\nRelated work. Originally, fast kernel summation methods were developed for problems in computational physics. The underlying kernels are related to fundamental solutions of partial differential equations (Green\u2019s functions). Examples include the 3D Laplace potential (reciprocal distance kernel) and the heat potential (Gaussian kernel). Beyond computational physics, kernel summation can be used for radial basis function approximation methods. They also find application to non-parametric statistics and machine learning tasks such as density estimation, regression, and classification. Linear inference methods such as support vector machines [28] and dimension reduction methods such as principal components analysis [23] can be generalized to kernel methods [3], which in turn require fast kernel summation.\nSeminal work for kernel summations in d \u2264 3 includes [11, 5] and [12]. In higher dimensions related work includes [13, 10, 16, 24, 29]. One of the fastest schemes in high dimensions is the improved fast Gauss transform [29, 24]. In all these methods, the low rank approximation of the far field is based on analytic and kernel-specific expansions. The cost of constructing and evaluating these expansions scales either as O(cd) (resulting in SVD-quality errors) or as O(dc), where c > 0 is related to the accuracy of the expansion. Except for very inaccurate approximations, c > 2 and thus all of these schemes become extremely expensive with increasing d. In addition to the expensive scaling with ambient dimension, the approximations in these methods\nmust be derived and implemented individually for each new kernel function.\nAn alternative class of methods is based on a hybrid of analytic arguments and algebraic approximations. Examples include [30, 8, 9]. However, these methods also scale as O(cd) or worse [21]. We also mention methods which rely only on kernel evaluations and use spatial decompositions to scale with dintr in which the far-field approximation is computed on the fly with exact error guarantees [10] or approximately using Monte Carlo methods [17]. However, these methods rely on distance-based pruning, which can fail in extremely high dimensions, and the randomized method is extremely slow to converge. Another scheme that only requires kernel evaluations (in the frequency domain) is [25]. But its performance depends on the ambient dimension and on the kernel being diagonalizable in Fourier space. For a more extensive discussion on the far field approximation that we use here and a more detailed review of the related literature, we refer the reader to our work [21].\nTo the best of our knowledge, all existing treecodes use distance-based pruning, sometimes augmented with kernel evaluations to better control the error. Our scheme is the first one to introduce an alternative approach not based on kernel evaluations or bounding box-based distance calculations.\nSince we present experimental results performed in parallel, we also mention another existing body of work on parallel treecodes [18]. However, our parallel algorithms quite different (and our efficiency relies on neighbor pruning). The specifics will be reported elsewhere since the parallelization of ASKIT is our main point.\nWe use randomized tree searches to compute approximate nearest neighbors. There is a significant body of literature on such methods, but we do not discuss them further since our scheme does not depend on the details of the neighbor search (although it does depend on the approximation error, if the nearest neighbors have been computed approximately.) Representative works include [1] and [6].\n2. Algorithms. We now turn to the description of the new algorithm. We begin by summarizing the basic structure of a treecode. In a treecode, we first construct a tree that is used for space-partitioning of the input points. We use this term to broadly cover any hierarchical partitioning of the data set such that nearby (or similar) points are grouped together. For our purposes, a tree consists of internal nodes with two or more children and leaf nodes with no children.\nGiven such a tree, a treecode performs a two-stage computation to approximate the kernel summations. In the first stage, a bottom-up tree traversal takes place (also known as the upward pass) in which at each node we create a low-rank approximation of the far field generated by all the source points in it. We form these representations at the leaf nodes, then pass them up to parents and combine them in a recursive manner. In the second stage, a concurrent top-down traversal (also known as the downward pass) takes place in which we use these representations to compute approximate potentials. That is, for each target point x, we traverse the tree from the top down. At a node \u03b1, we apply a pruning criterion to determine whether or not we can approximate the far-field generated from sources in a and evaluated at x. If we can approximate it, then we use the low-rank approximation to evaluate the far field at x and then we prune the tree traversal at the node a. If we cannot approximate it, we recurse and visit the children of a. If we still cannot prune at a leaf, we evaluate the contribution of the leaf\u2019s points directly (no approximation takes place). If by K\u0303 we denote the approximate kernel (meaning that we use a low rank approximation), a template for a generic treecode is given by Algorithm 2.1. As described, the algorithm\nAlgorithm 2.1 Treecode(Target point x, Source tree node a)\nif we can approximate u(x) = K(x,X\u03b1)w(X\u03b1) then {using pruning criterion} return u(x) = K\u0303a(x) else if a is a leaf then return \u2211 xj\u2208aK(x, xj)wj else return u(x) = \u2211 a\u2032 Treecode(x, a\n\u2032) for all children a\u2032 of a end if\nresults in O(N logN) complexity. The Fast Multipole Method [11] extends this idea by also constructing an incoming representation which approximates the potentials due to a group of distant sources at a target point; it results in O(n) complexity. As described the algorithms has two main technical components: how do we decide to prune and how do we construct K\u0303?\nThe vast majority of existing codes use distance-based pruning \u2013 i.e. they use the minimum distance between x and the bounding box of a relative to the size of the bounding box. If this distance is greater than a threshold, pruning takes place. This distance can be directly related to error estimates using the smoothness properties of the kernel.\nAs we mentioned, the far-field approximation is much more complicated, and there is a great variety of options which we discuss in [21]. However, for completeness we summarize that discussion in the Figure 2.1(a). In the left subfigure, we depict analytic expansions based on series truncation (e.g.,[11]). These methods compute a series expansion using only the source points. In general, the number of terms needed\nfor a given accuracy scales unfavorably with d. The middle subfigure illustrates the kernel independent fast multipole method which is a hybrid of algebraic and analytic methods [30]. In this approach, the far-field is approximated via interactions between carefully chosen fictitious source and target points. These points are chosen to cover a bounding sphere or box, so they also scale poorly with the ambient dimension. The last figure shows a purely algebraic approach similar to [22]. Here, a subset of the source points is used in place of fictitious sources. However, a number of fictitious targets which scales exponentially with d is still necessary.\nThe basic conclusion is that all of these existing methods for constructing the far-field low-rank approximation do not scale with increasing ambient dimension d. Furthermore distance-based pruning also doesn\u2019t scale with increasing dimensionality because even if the dataset has low intrinsic dimension, the bounding box can be huge so that no pruning takes place.\nASKIT introduces a new pruning a new method to approximate the far field. The effectiveness of both of these methods depends only on the intrinsic dimension and not the ambient dimension. In the remainder of this section, we describe in detail how we carry out each of these steps in ASKIT.\n2.1. Interpolative Decompositions and Sampling. The first main component of our method is the representation of the far field generated by source in a node using an approximate ID scheme, which is summarized in Figure 2.2.\nLet K \u2208 Rn\u00d7m. Let S be an index set with |S| = s and 1 \u2264 Sj \u2264 m. Let KS = K(:,S) be the columns of K indexed by S and KR be the remaining unskeletonized columns of K. Assuming s < m < n and that KS is full rank, we can approximate the columns of KR by KSP where P = K \u2020 SKR, P \u2208 Rs\u00d7m. The ID consists of the index set S, referred to as the skeleton, and matrix P . Following [19, 4], we refer to the construction of this approximation for a matrix as skeletonization.\nIn order to compute an ID such that \u2016KR \u2212KSP\u2016 is small, we employ a pivoted QR factorization to obtainK\u03a0 = QR for some permutation \u03a0, an orthonormal matrix Q, and upper triangular matrix R. The skeleton S corresponds to the first s columns of K\u03a0, and the matrix P can be computed from R in O(s3 + s2(m\u2212 s)) time. It can be shown [15] that (2.1) \u2016KR \u2212KSP\u2016 \u2264 \u221a 1 +m(m\u2212 s)\u03c3s+1(K),\nwhere \u03c3s+1 is the ith singular value of K. The overall cost for s < m < n is O(nm2). Far field using skeletonization: We will use ID to compactly represent the far field of a leaf node \u03b1. Let X\u03b1 be the set of points assigned to \u03b1 (assume |X\u03b1| = m). Let K\u03b1 := K(X \\X\u03b1,X\u03b1) \u2208 R(N\u2212m)\u00d7m. Also let w\u03b1 = w(X\u03b1) \u2208 Rm. Our task is to construct an approximation to K\u03b1w\u03b1.\nWe choose s, compute the skeleton S\u03b1 of K\u03b1 and set\n(2.2) K\u03b1w\u03b1 \u2248 K\u0303\u03b1w\u0303\u03b1, where K\u0303\u03b1 := K\u03b1(:,S\u03b1), w\u0303\u03b1 := w\u03b1(S\u03b1) + P\u03b1w\u03b1(R),\nand R is the index set of the unskeletonized columns of K\u03b1. We term w\u0303\u03b1 \u2208 Rs the skeleton weights.\nGiven the skeleton S\u03b1 and skeleton weights w\u0303\u03b1, we can efficiently approximate the contribution to some target point ui. We first compute the 1\u00d7s matrix of interactions\nK(u,S\u03b1), then apply it to the vector w\u0303\u03b1, to get an approximation with error bounded by (2.1).\nThis approach leaves two issues unanswered: first, how do we choose s? We discuss this in Section 3. Second, computing the ID as described is more expensive than directly evaluating K\u03b1w\u03b1. We address this point next.\nApproximate skeletonization. We have O(N/m) leaves and the skeletonization of each leaf described above costs O(Nm2). When performed for all leaf nodes,\nthis will require O(N2m) work. Instead, we will compute the skeleton of a smaller matrix, which has only a random subset of the rows of K\u03b1. That is, we select ` rows with m < ` N and whose index set we denote by T\u03b1. We form K\u03b1(T\u03b1, :) \u2208 R`\u00d7m, and compute the skeleton S\u03b1 of size s and the corresponding projection matrix P\u03b1. This is equivalent to choosing ` target points. The complexity of the construction for one leaf becomes O(`m2); thus the overall complexity for all nodes in the tree becomes O(N`m). This approach is illustrated in Figure 2.2.\nSampling rows of K: We need to choose a small number of rows such that the ID ofK\u03b1(T\u03b1, :) will be close to the ID ofK. Randomized linear algebra algorithms can achieve this by either random projections [15] or the construction of an importance sampling distribution [20]. Either approach requires O(N) work per node. However, for smoother kernels that decay with distance (or, more generally, dissimilarity) the nearest (more similar) points will tend to dominate the sum in (1.2). Following this intuition, if we can include the nearest neighbors of each point in X\u03b1, then we expect this to be a reasonable approximation to an importance sampling distribution. If we do not have enough neighbors, we add additional uniformly chosen points to reach a sufficient sample size. This process is discussed below.\n2.2. ASKIT. Using the ID as a compact representation, we can now describe the main steps of our treecode:\n\u2022 Approximate the \u03ba-nearest neighbors for all xi. \u2022 Compute a top-down binary tree decomposition for X . \u2022 Perform a bottom-up traversal to build neighbor lists for interior (non-leaf)\nnodes. \u2022 Perform a bottom-up traversal to compute skeletons and equivalent weights. \u2022 Perform a top-down traversal to evaluate ui at each point i.\nWe describe the individual steps below in detail. The basic steps of the algorithm are illustrated in Figure 2.3 (leaves of a tree), Figure 2.4 (skeletonization of a leaf), 2.5 (skeletonization of an internal node) and 2.6 (evaluation).\nNearest neighbors and binary tree decomposition: To find nearest neighbors we use a greedy search using random projection trees [6]. We build a tree and for each xi we collect \u03ba-nearest neighbors found by exhaustive search among the other\npoints in the leaf node that contains xi. Then we discard the tree (we do not perform top-down searches) and iterate, keeping the best candidate neighbors found at each step. The binary tree used in the treecode is built using the following rule: to split a node \u03b1, we compute its center (xc), then the furthest point to xc (xl), then the furthest point to xl (xr). We project all the points on the line (xl, xr), compute the median, and split them into two groups. We recurse until every leaf gets no more than m points.\nNode neighbor lists: During the skeletonization, we need to sample the far field. To do this we need to construct node neighbor lists. These lists are defined in Algorithm 2.2 and are constructed in a bottom-up fashion using a standard preorder traversal of the tree. The set-difference operations can be done in O(log(N/m)) time \u00a7 per point using the binary-tree Morton ID of every node and every point. The\n\u00a7If log(N/m) bits is less than the size of an instruction, this can be done in constant time.\nAlgorithm 2.2 BuildNeighbors(\u03b1)\n1: if IsLeaf(\u03b1), N\u03b1 := (\u222a i\u2208X\u03b1Ni) \\X\u03b1 2: else N\u03b1 := ( Nr(\u03b1) \u222aNl(\u03b1) ) \\ ( Xr(\u03b1) \u222aXl(\u03b1) )\nMorton ID is a bit array that codes the path from the root to the node or point. The Morton ID of a point is the Morton ID of the leaf node it belongs to.\nSkeletonization of leaves: Let X\u03b1 be the set of points. Let the contribution of this node to all X \\X\u03b1 be denoted as K\u03b1. As noted above, we will approximate K\u03b1 by computing a low-rank approximation using an inexact ID that is based on sampling K\u03b1 to create a matrix K(T\u03b1,X\u03b1) for some small set of rows T\u03b1. Sampling the right points makes a difference and can be expensive. In [21], we developed a sampling scheme that is a hybrid between uniform sampling combined with nearest neighbor sampling (for distance decaying kernels). That is we choose the nearest neighbors of the points in X\u03b1 which are not themselves in X\u03b1 and then add uniformly sampled\n(without replacement) distant points as needed to capture the far field.\n(2.3) T\u03b1 = N\u03b1 \u222a Sample( X \\ (X\u03b1 \u222aN\u03b1), `\u2212 |N\u03b1| ),\nThat is, we randomly sample ` \u2212 |N\u03b1| points excluding the points in \u03b1 and we use these points along with the neighbors N\u03b1 as target points. We then compute the ID of K(T\u03b1,X\u03b1) to obtain the skeleton S\u03b1 and skeleton weights w\u0303\u03b1 for K\u03b1.\nOn the other hand, if |N\u03b1| > `, we truncate N\u03b1 to only include ` neighbors. We sort the points in N\u03b1 by the distance from their nearest neighbor in \u03b1, and keep the ` closest points.\nAlgorithm 2.3 Skeletonize(\u03b1)\n1: if \u00ac IsLeaf(\u03b1) 2: Skeletonize(r(\u03b1)), Skeletonize(l(\u03b1)) 3: X\u03b1 = Sr(\u03b1) \u222aSl(\u03b1) 4: Create sampling targets using (2.3) 5: Skeletonize X\u03b1 using QR factorization and store S\u03b1 and w\u0303\u03b1\nSkeletonization of internal nodes. To build the far field approximation for an interior node \u03b1 we use the same algorithm. Instead of using all the points in the leaf descendants of \u03b1, we use the combined skeleton points Sr(\u03b1) \u222aSl(\u03b1) and the neighbors list N\u03b1 constructed with BuildNeighbors(\u03b1). Let w\u0303l(\u03b1) and w\u0303r(\u03b1) be the vectors of skeleton weights of the children.\nWe compute the ID of the matrix K(T\u03b1,Sr(\u03b1) \u222aSl(\u03b1)) to obtain a skeleton for \u03b1 and P\u03b1. We then apply P\u03b1 to the unskeletonized part of the concatenation of w\u0303l(\u03b1) and w\u0303r(\u03b1) to obtain the skeleton weights. These ideas are summarized in SkeletonizeNode(\u03b1).\nPruning: During evaluation phase we use a standard top down traversal. For every xi, we start at the root and traverse the tree. The pruning is based on the neighbors of xi and has nothing to do with distance or kernel evaluations. Node \u03b1 is not pruned if it is either an ancestor of xi or it is an ancestor of any of the nearest neighbors of xi:\n(2.4) Prune(\u03b1, i) = IsTrue(@j \u2208 {i\u222aNi} : \u03b1 \u2208 Aj)\nNote that the ancestor check can be done efficiently using Morton IDs. If Prune(\u03b1, i) is true, we evaluate the kernel at xi using the skeleton points and the equivalent weights and do not traverse the children of \u03b1.\nAlgorithm 2.4 ui = Evaluate(xi, \u03b1)\n1: if prune(\u03b1, i), return K(xi,S\u03b1)w\u0303\u03b1 {Approximate ((2.4))} 2: if IsLeaf(\u03b1), return K(xi,X\u03b1)w\u03b1 {Direct evaluation} 3: return Evaluate(xi, r(\u03b1)) + Evaluate(xi, l(\u03b1)) {Recursion}\nThe evaluation algorithm and the overall scheme are summarized in Algorithm 2.4 and Algorithm 2.5 respectively.\nTo illustrate the difference of the proposed pruning compared to standard distance pruning we conducted a numerical experiment in which we compare the two\nAlgorithm 2.5 u=ASKIT(X , w, s, `,m,N (X )) 1: \u03b1 =BinaryTree(X ,m) {Build binary tree, \u03b1 is the root} 2: BuildNeighbors(\u03b1) {Bottom-up traversal} 3: SkeletonizeNode(\u03b1) {Bottom-up traversal} 4: ui = Evaluate(xi, \u03b1) \u2200i \u2208 X {Top-down traversal}\napproaches. The distance pruning criterion is implemented ass follows. Given a node \u03b1 with points X\u03b1 we compute its centroid c\u03b1 and a radius R\u03b1 = maxx\u2208X\u03b1 \u2016x\u2212 c\u03b1\u20162. Then given a target point x we prune if \u2016x\u2212 c\u03b1\u20162 > R. Notice that in practice R has to be scaled to create some separation between the target and source points, which makes pruning even harder. In Table 2.2, we reported the average number of nodes visited during the tree traversal for evaluating the potential at several target points for a dataset of N = 65, 536 points for different point distributions in 2D, 4D, 32D, and 256D. We used a Gaussian distribution (intrinsic dimension is the same as the ambient dimension), points distributed on a curve (intrinsic dimension is one)\u00b6 and points uniformly distributed on a hypersphere (intrinsic dimension is four). These empirical results show that distance based pruning is not possible in high dimensions even if the underlying intrinsic dimension is small.\n3. Complexity and error. ASKIT has the following parameters that control its computational cost and its accuracy.\n\u2022 m: the number of points per leaf node; it controls the error and the runtime since it governs the trade-off between near and far interactions. \u2022 \u03ba: the number of nearest neighbors for sampling and pruning. The larger \u03ba the less we prune. The larger \u03ba the better our sample is when we compute the interpolative decomposition. If \u03ba is too large the computation becomes quite expensive. In our tests, we set \u03ba = 2m and we have found that increasing \u03ba further does not improve the accuracy. \u2022 s: the skeleton size. In essence this is the rank we use for the far-field approximation .The higher s, the more accurate and expensive the skeletonization and evaluation phases are. Here we fixed s to be the same in all nodes. A\n\u00b6The equation of the curve is xi = f(i\u03c0t), t \u2208 [0, 1], i = 1, . . . , d, with f = cos for odd i and f = sin for even i.\nmore efficient implementation will be to estimate \u2016KR\u2212KSP\u2016 choose s adaptively to be different for each node. This is something that we are currently investigating. \u2022 `: the row sampling size for K\u03b1. Larger values allow a more accurate ID but slower skeletonization. We require ` > m and ` > s so that ID problem is overdetermined. In our experiments we take ` = s + 20. In our experiments taking larger values does not increase the accuracy (if we keep everything else fixed).\nSo given the choices we describe above, the are two main parameters, the number of points per box m and the skeleton size s.\nComputational complexity. We assume that the nearest-neighbor list Ni for each point is given. Note that exact nearest-neighbors can be computed in O(N) time for low-intrinsic dimensional sets [26] and approximation schemes, such as the one we use, are even faster. Thus, we only consider the cost of ASKIT.\nThe number of leaves is M = N/m and the total number of nodes is 2M \u2212 1 = O(M). We first consider the upward pass cost or skeletonization cost TS . Then we consider the downward evaluation cost TE .\nIn the upward pass, the first calculation is the construction of N\u03b1 the per node neighbor lists that are used for sampling (given by Algorithm 2.2).\nFor leaf nodes, the cost of building N\u03b1 (the per-node neighbor lists) involves first merging the per-point neighbor lists Ni for all i \u2208 X\u03b1, sorting them and removing duplicates, and then using the Morton IDs to remove points that belong to a node. The complexity of this operation per node is O(\u03bam log(\u03bam)) and thus the total cost is O(N\u03ba log(\u03bam)). Once we have N\u03b1, we keep only the ` nearest points to the X\u03b1 and use them to construct the node ID. For internal nodes, the operation is simpler. We simply merge the lists of the children, remove duplicates and points belonging to the node, then sort and truncate to ` points (if necessary). The total complexity is O((N/m)` log `).\nThe cost of skeletonization involves QR factorizations of O(M) matrices of size ` \u00d7 m (at the leaves), or of size ` \u00d7 (2s) (for internal nodes). In general, the time required for the evaluation of the kernel function depends linearly on d. The QR factorization requires O(`m2) for each of theM leaves and O(`s2) time for each of the O(M) internal nodes. Thus the total time to construct these matrices and compute their factorizations is O(dM`(m2 + s2)). Therefore the total time TS upward pass (or skeletonization) is given by\n(3.1) TS = O(N\u03ba log(\u03bam)) +O((N/m) ` log `) +O(dN`(m2 + s2)/m).\nThe cost of the downward pass depends on our ability to prune. Given a target point xi, let \u03be be the number of nodes we visit to compute ui. We decompose \u03be into two parts: \u03ben for nodes evaluated directly and \u03bef for nodes approximated via the skeleton (\u03be = \u03bef + \u03ben). Given these parameters, the cost of the downward pass for point xi is at most \u03benm for direct evaluation plus at most \u03befs for approximations. Taking the worse case values of \u03ben and \u03bef for all evaluation points, the overall downward pass cost is bounded by N(\u03benm+ \u03befs). We now bound \u03ben and \u03bef .\nFor \u03ben, the worst case is that for every point we visit \u03ba different leaves, (since we use the point\u2019s nearest neighbors for pruning). Therefore \u03ben \u2264 \u03ba. Notice that this bound is quite pessimistic since as m increases the will be significant overlap of direct\nnodes for target points belonging to the same leaf node. Additionally, the number of leaves visited will be smaller in the presence of a low intrinsic dimensional structure. So this estimate is valid only for m N .\nTo bound \u03bef we proceed as follows. Given a target point i, ASKIT will visit all nodes A(Ni) and will prune all the remaining nodes. In the worst case, all the elements of A(Ni) are unique. Since |Ni| = \u03ba, |A(Ni)| \u2264 \u03ba logM . But if these nodes are unique, that means we can prune their siblings by using their skeletonization. Thus, \u03bef = O(\u03ba logM). That is, we visit \u03ba nodes assuming each neighbor of the evaluation belongs to a different leaf node.\nSince the cost of evaluating the far field of a node to a point is O(ds) (and assuming m N) the overall complexity for TE is given by\n(3.2) TE = O ( dNm\u03ba+ dNs\u03ba log N\nm\n) .\nThe total cost of ASKIT it T = TE + TS . Using the fact that in our implementation ` = O(s) and \u03ba = O(m), the overall cost of ASKIT (for m N)\n(3.3) T = O ( dNm ( sm+ s3\nm + s log\nN\nm\n)) .\nThe ambient dimension d enters only in the cost of kernel evaluations. Error analysis: There are two sources of error related to K\u03b1: the far-field low rank approximation and the error in computing this low rank approximation. This error appears in computing the interpolative decomposition factorization due the selection of a subsample of the rows (the sampling to construct the skeleton). It can be shown that a particular importance sampling distribution (based on what is known as statistical leverage scores) can provide a O(\u03c3s+1) reconstruction error if the sample size ` is proportional to s log s [20]. However, computing the leverage scores is more expensive than the computation of the kernel summation exactly and thus, cannot be applied in our context.\nIt is much cheaper to sample using a uniform distribution. But the error bound is not as sharp. In [21], we show that the combined ID approximation and uniform sampling error in the far-field from a single source node to the rest of the points can be bounded as follows. Let K be the n\u03b1 \u00d7m\u03b1 matrix of interactions between all the points X\u03b1 in the source node \u03b1 and the remaining n\u03b1 = N\u2212m\u03b1 points.\u2016 We sample ` rows (target points) uniformly and independently and construct a rank s interpolative decomposition of the sampled matrix K\u0303 to select the skeleton size. Then, with high probability, the total error incurred is bounded by\n(3.4) \u2016K \u2212 K\u0303\u2016 \u2264 ( 1 + \u221a 6 n\u03b1 ` + \u221a 1 +m\u03b1s(m\u03b1 \u2212 s) ) \u03c3s+1(K)\u03c3s+1(K).\nThis result is Theorem 3.7 in [21]. In the following we use \u03b6(n\u03b1,m\u03b1, s, `) to denote the prefactor of \u03c3s+1(K).\nFor a given evaluation point, we incur this error each time we prune. We know that the number of prunes is bounded by \u03ba log Nm . Denoting by Vi the nodes whose\n\u2016In addition we should exclude all the points not in \u03b1 for which we use direct interactions.\nskeletonization was used to evaluate the potential at xi, and using the fact that m\u03b1 = 2 level(\u03b1)m, the overall absolute error is bounded by\n(3.5) |ui \u2212 uexact(xi)| \u2264 \u2016w\u2016 \u03ba log N\nm max \u03b1\u2208Vi\n\u03b6 (n\u03b1,m\u03b1, s, `)\u03c3 \u03b1 s+1.\nThis is a preliminary result. The remaining of the discussion is qualitative. We remark that the ambient dimension does not appear in the error estimate; \u03ba and errors in finding the exact nearest neighbors affect the maximum of \u03c3\u03b1s+1; and the constant \u03b6 in 3.4 depends on N , s, and ` as well as m\u03b1. This result is derived assuming the skeletonization was computed using uniform random sampling. It is rather pessimistic compared to a result derived for sampling using leverage scores. As we mentioned in our implementation, we employ a heuristic in which we combine nearest neighbors and uniformly chosen samples. We also note that \u03c3\u03b1s+1 is bounded by the (s+ 1)st singular value of the entire kernel matrix. Therefore, in the case that the entire matrix is low rank, our error bound will be small.\nThe parameter \u03b6 can grow significantly if we keep the skeleton size fixed because \u03c3\u03b1s+1 can grow and because m\u03b1 grows. To fix this, we can either increase s adaptively or we can restrict the level(\u03b1) by starting the evaluation phase at a level which is a fixed distance from the leaves. Also, in lower dimensions, a hybrid pruning rule that combines distances and nearest neighbors could be use to derive sharper error.\nHow does (3.5) compare to classical results for treecodes? Using kernel specific analysis and distance pruning one can derive analytic bounds for \u03c3s+1 based on the decay of coefficients in analytic expansions. In that context s corresponds to the number of terms in the expansion. There is no explicit dependence of \u03b6 on n\u03b1,m\u03b1 since the far field is not computed algebraically and no by target sampling. Given that distance pruning is critical in deriving those bounds, the question is what happens in ASKIT since we do not use distance pruning. A critical distance is the distance between the target point xi and its (\u03ba + 1)th nearest neighbor. (Recall that the interactions of xi with all its first \u03ba neighbors are computed directly.) The (\u03ba+ 1)th nearest neighbor may end up being in a node that is pruned. Its distance to xi resembles the one used in the error estimates using distance pruning and can be used to derive more quantitative bounds. Furthermore it can be used to derive an adaptive (per point) selection of \u03bai to further reduce the overall error of ASKIT.\nIn summary, equation (3.5) is suggestive but not particularly useful for quantitatively estimating the error and choosing s and \u03ba. Information regarding the kernel and the point distribution is required to derive a more precise estimate. Next, we present an empirical evaluation of our scheme for the Gaussian kernel with constant and variable bandwidth.\n4. Experiments. We present results for the Gaussian kernel\nK(xi, xj) = exp(\u2212\u2016xi \u2212 xj\u201622/h2j )\nwith constant and variable h; we select h based on kernel density estimation theory [27] (Eq. 4.14 in that book), so that h \u221d N\u22121/(d+4). The bandwidth h plays a critical role in assessing the accuracy of a treecode. For small and large h the kernel compresses well. But for certain h, which depends on the underlying point distribution, the Gaussian kernel does not compress well. The base value we use here is optimal for"}, {"heading": "4 64 4 1.00 97 7E-01 1E+00 5 4 <1 4 15% 3 %", "text": ""}, {"heading": "5 64 32 1.00 97 6E-02 1E+00 5 7 <1 6 15% 3 %", "text": ""}, {"heading": "6 256 128 1.00 88 3E-03 9E-01 7 8 1 7 5 % 1 %", "text": ""}, {"heading": "7 64 4 5.00 97 8E-01 1E+00 5 5 <1 4 15% 3 %", "text": ""}, {"heading": "8 64 32 5.00 97 1E-04 1E+00 5 7 <1 6 15% 3 %", "text": ""}, {"heading": "9 256 128 5.00 88 4E-08 1E+00 7 8 1 7 5 % 1 %", "text": ""}, {"heading": "19 64 32 1.76 35 7E-02 1E+00 57 564 6 559 83% 23%", "text": ""}, {"heading": "20 64 128 1.76 35 5E-02 1E+00 57 729 7 723 83% 23%", "text": ""}, {"heading": "21 256 128 1.76 25 2E-02 1E+00 63 1011 11 1000 55% 8 %", "text": "constant-width Gaussian kernel and normally distributed points. Also let us remark that doing a simple sweep for h may miss the values of h for whichK fails to compress. (More discussion and results can be found in [21].) In our experiments, we choose h\nlarge enough so that far-field is necessary for accurate summation and small enough so that the kernel to be difficult to compress.\nASKIT has been implemented in C++. The direct evaluation is highly optimized using BLAS but the other parts of the code are proof of principle implementations. We use the Intel MKL for linear algebra, and use OpenMP and MPI parallelism in all phases of the algorithm.\nThe hardware employed for the runtime experiments carried out is the Stampede system at the Texas Advanced Computing Center. Stampede entered production in January 2013 and is a high-performance Linux cluster consisting of 6,400 compute nodes, each with dual, eight-core processors for a total of 102,400 CPU-cores. The dual-CPUs in each host are Intel Xeon E5 (Sandy Bridge) processors running at 2.7GHz with 2GB/core of memory and a three-level cache. The nodes also feature the new Intel Xeon Phi coprocessors. Stampede has a 56Gb/s FDR Mellanox InfiniBand network connected in a fat tree configuration which carries all high-speed traffic (including both MPI and parallel file-system data).\nTo test ASKIT, we use normally distributed points in 4, 16, and 64 dimensions for which the intrinsic and ambient dimension coincide. We present results for 100K and 1M points. We also consider the embedding in 1000D of a set of points normally distributed in a 4D hypersphere. We also used a UCI ML repository dataset (SUSY [2]) with 5M points in 18 dimensions. In all experiments, the sources and targets coincide, and we report timings for all pairwise interactions. We present wall-clock times for finding the neighbors, constructing the skeletonization, and the evaluation. Let us remark, that for problems that require multiple all-to-all kernel evaluations (e.g., regression), the cost of construction, skeletonization, and neighbor finding is amortized over the iterations. The only per-iteration cost is TE .\nIn Table 4.1 and Table 4.2, we report results that show the feasibility of ASKIT. The performance of our nearest neighbor search affects the overall runtimes and the performance of ASKIT. Although it is an independent component, we report the numbers since nearest neighbors must be computed somehow. For this reason, we report the nearest neighbor hit rate accuracy (percentage of correct neighbors) and the timings. We test the accuracy of our nearest-neighbors and ASKIT, using exhaustive searches and direct evaluations on 10K randomly sampled points.\nDiscussion: For low-accuracy approximations our scheme outperforms the direct evaluation at about 1M points. Depending on the kernel and the accuracy the speedup can be less dramatic or the cutoff may be at much higher N . Now we make some additional remarks on our runs.\nIn runs 1\u20139 we show how the method converges for different bandwidths and m, s values for 100K points that are normally distributed in 4D. Note that the error converges with increasing m and s. The convergence can be quite rapid (runs 7\u20139). In most of the runs, the far-field is critical in getting accuracy. To show this, we report \u03ba, the error in u constructed using only the \u03ba = 2m nearest neighbors for each point. We see that the far field is essential, and truncation does not get a single digit correct. On the other hand for run 22, with h = 0.75 the far field is wasted effort. In runs 23\u201324 the far field is essential.\nIn runs 27-28, we consider a problem in 1000D. The scheme converges quickly and it is 20\u00d7 faster than the direct evaluation. For the UCI dataset (29-32) ASKIT is 25\u00d7 faster than the direct evaluation. To demonstrate the effects of using the nearest\nneighbors compare runs 10\u201312 to runs 13\u201315. The only difference is that that we use a very approximate search so the hit-rate \"hr\" (correct neighbors/\u03ba) small. As a result the errors are higher and the pruning is not as effective (we visit more leaves and more internal nodes). TE is almost 3\u00d7 larger. Finally, notice that we increase the dimension the neighbors in general become less accurate. This is because we use a fixed number of iterations in our greedy neighbor search. The skeletonization costs are negligible compared to the evaluation costs.\nRuns (1-32) took place on a single node. In runs (33-34) we show a distributed memory run for 5M points in 128D on 16 and 256 nodes resulting a 2000\u00d7 speedup over one-socket direct evaluation. Our largest run on 512 nodes (8,192 cores) we evaluated the sum for 100 million points in 64 dimensions. The details of the parallelization will be reported elsewhere.\n5. Conclusions. We presented a new scheme for high dimensionalN -body problems and conducted a proof-of-concept experimental study. Our scheme is based only on kernel evaluations, uses neighbor-based pruning, and uses neighbor-sampled interpolative decomposition to approximate the far field. Since this method is new, there are many open problems and several opportunities for optimization. The most pressing one is deriving a rigorous error bound that incorporates our sampling scheme; this is ongoing work. There is also further work to be done in optimizing the performance of the scheme, in adaptive determination of the skeleton size, and in improving the sampling. Finally, notice that if we are given similarities and a hierarchical clustering, our scheme does not involve any distance calculations so it should be possible to apply\nto points (objects) in non-metric spaces."}], "references": [{"title": "Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "COMMUNICATIONS OF THE ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "On the compression of low rank matrices", "author": ["Hongwei Cheng", "Zydrunas Gimbutas", "Per-Gunnar Martinsson", "Vladimir Rokhlin"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "A fast adaptive multipole algorithm in three dimensions", "author": ["H. Cheng", "Leslie Greengard", "Vladimir Rokhlin"], "venue": "Journal of Computational Physics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Random projection trees and low dimensional manifolds", "author": ["S. Dasgupta", "Y. Freund"], "venue": "Proceedings of the 40th annual ACM symposium on Theory of computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Fast directional multilevel algorithms for oscillatory kernels", "author": ["B. Engquist", "L. Ying"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "The black-box fast multipole method", "author": ["William Fong", "Eric Darve"], "venue": "Journal of Computational Physics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "A generalized fast mulipole method for nonoscillatory kernels", "author": ["Zydrunas Gimbutas", "Vladimir Rokhlin"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "N-body problems in statistical learning, Advances in neural information processing", "author": ["A.G. Gray", "A.W. Moore"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "A fast algorithm for particle simulations", "author": ["Leslie Greengard", "Vladimir Rokhlin"], "venue": "Journal of Computational Physics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1987}, {"title": "Fast approximation of the discrete Gauss transform in higher dimensions", "author": ["Michael Griebel", "Daniel Wissel"], "venue": "Journal of Scientific Computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "A high-performance, portable implementation of the MPI message passing interface standard", "author": ["W. Gropp", "E. Lusk", "N. Doss", "A. Skjellum"], "venue": "Parallel Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J. Tropp"], "venue": "SIAM Review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Dual-tree fast gauss transforms", "author": ["Dongryeol Lee", "Alexander Gray", "Andrew Moore"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Fast high-dimensional kernel summations using the monte carlo multipole method", "author": ["Dongryeol Lee", "Alexander G Gray"], "venue": "in NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Cur matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Far-field compression for fast kernel summation methods in high dimensions", "author": ["William B. March", "George Biros"], "venue": "arXiv preprint,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "An accelerated kernel-independent fast multipole method in one dimension", "author": ["Per-Gunnar Martinsson", "Vladimir Rokhlin"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "in NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Density Estimation for Statistics and Data Analysis", "author": ["Bernard W. Silverman"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1986}, {"title": "Least squares support vector machine classifiers", "author": ["Johan AK Suykens", "Joos Vandewalle"], "venue": "Neural processing letters,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Improved fast gauss transform and efficient kernel density estimation, in Computer Vision, 2003. Proceedings", "author": ["Changjiang Yang", "Ramani Duraiswami", "Nail A Gumerov", "Larry Davis"], "venue": "Ninth IEEE International Conference on,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "A kernel-independent adaptive fast multipole method in two and three dimensions", "author": ["Lexing Ying", "George Biros", "Denis Zorin"], "venue": "Journal of Computational Physics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "1 in O(N logN) work (treecodes) or O(N) work (fast multipole methods) to arbitrary accuracy [5, 12].", "startOffset": 92, "endOffset": 99}, {"referenceID": 16, "context": "In a companion paper [21], we review the main methods for constructing far-field approximation and", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "Although there has been extensive work on these methods for classical kernels like the Gaussian, other kernels are also used such as kernels with variable bandwidth that are not shift invariant [27].", "startOffset": 194, "endOffset": 198}, {"referenceID": 12, "context": "Our low rank far field scheme uses an approximate interpolative decomposition (ID) (for the exact ID see [19, 15]) which is constructed using nearest-neighbor sampling augmented with randomized uniform sampling.", "startOffset": 105, "endOffset": 113}, {"referenceID": 16, "context": "The basic notions for the far field were introduced in [21].", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "The nearest neighbors are computed using random projection trees [6] with", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "Examples of such difficult to compress kernels are the high frequency Helmholtz kernel [7] and in high intrinsic dimensions the Gaussian kernel (for certain bandwidths and point distributions) [21].", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "Examples of such difficult to compress kernels are the high frequency Helmholtz kernel [7] and in high intrinsic dimensions the Gaussian kernel (for certain bandwidths and point distributions) [21].", "startOffset": 193, "endOffset": 197}, {"referenceID": 20, "context": "Linear inference methods such as support vector machines [28] and dimension reduction methods such as principal components analysis [23] can be generalized to kernel methods [3], which in turn require fast kernel summation.", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "Linear inference methods such as support vector machines [28] and dimension reduction methods such as principal components analysis [23] can be generalized to kernel methods [3], which in turn require fast kernel summation.", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Seminal work for kernel summations in d \u2264 3 includes [11, 5] and [12].", "startOffset": 53, "endOffset": 60}, {"referenceID": 3, "context": "Seminal work for kernel summations in d \u2264 3 includes [11, 5] and [12].", "startOffset": 53, "endOffset": 60}, {"referenceID": 10, "context": "In higher dimensions related work includes [13, 10, 16, 24, 29].", "startOffset": 43, "endOffset": 63}, {"referenceID": 8, "context": "In higher dimensions related work includes [13, 10, 16, 24, 29].", "startOffset": 43, "endOffset": 63}, {"referenceID": 13, "context": "In higher dimensions related work includes [13, 10, 16, 24, 29].", "startOffset": 43, "endOffset": 63}, {"referenceID": 21, "context": "In higher dimensions related work includes [13, 10, 16, 24, 29].", "startOffset": 43, "endOffset": 63}, {"referenceID": 21, "context": "One of the fastest schemes in high dimensions is the improved fast Gauss transform [29, 24].", "startOffset": 83, "endOffset": 91}, {"referenceID": 22, "context": "Examples include [30, 8, 9].", "startOffset": 17, "endOffset": 27}, {"referenceID": 6, "context": "Examples include [30, 8, 9].", "startOffset": 17, "endOffset": 27}, {"referenceID": 7, "context": "Examples include [30, 8, 9].", "startOffset": 17, "endOffset": 27}, {"referenceID": 16, "context": "However, these methods also scale as O(c) or worse [21].", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "We also mention methods which rely only on kernel evaluations and use spatial decompositions to scale with dintr in which the far-field approximation is computed on the fly with exact error guarantees [10] or approximately using Monte Carlo methods [17].", "startOffset": 201, "endOffset": 205}, {"referenceID": 14, "context": "We also mention methods which rely only on kernel evaluations and use spatial decompositions to scale with dintr in which the far-field approximation is computed on the fly with exact error guarantees [10] or approximately using Monte Carlo methods [17].", "startOffset": 249, "endOffset": 253}, {"referenceID": 18, "context": "Another scheme that only requires kernel evaluations (in the frequency domain) is [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "For a more extensive discussion on the far field approximation that we use here and a more detailed review of the related literature, we refer the reader to our work [21].", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": ") Representative works include [1] and [6].", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": ") Representative works include [1] and [6].", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "The Fast Multipole Method [11] extends this idea by also constructing an incoming representation which approximates the potentials due to a group of distant sources at a target point; it results in O(n) complexity.", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "As we mentioned, the far-field approximation is much more complicated, and there is a great variety of options which we discuss in [21].", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": ",[11]).", "startOffset": 1, "endOffset": 5}, {"referenceID": 22, "context": "The middle subfigure illustrates the kernel independent fast multipole method which is a hybrid of algebraic and analytic methods [30].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "The last figure shows a purely algebraic approach similar to [22].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "Following [19, 4], we refer to the construction of this approximation for a matrix as skeletonization.", "startOffset": 10, "endOffset": 17}, {"referenceID": 12, "context": "It can be shown [15] that", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "1(b), we show a method based on placing equivalent sources and finding equivalent densities that can approximate the far field [30].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "One method [22] (Figure 2.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "Randomized linear algebra algorithms can achieve this by either random projections [15] or the construction of an importance sampling distribution [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "Randomized linear algebra algorithms can achieve this by either random projections [15] or the construction of an importance sampling distribution [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "Nearest neighbors and binary tree decomposition: To find nearest neighbors we use a greedy search using random projection trees [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 16, "context": "In [21], we developed a sampling scheme that is a hybrid between uniform sampling combined with nearest neighbor sampling (for distance decaying kernels).", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "\u00b6The equation of the curve is xi = f(i\u03c0t), t \u2208 [0, 1], i = 1, .", "startOffset": 47, "endOffset": 53}, {"referenceID": 15, "context": "It can be shown that a particular importance sampling distribution (based on what is known as statistical leverage scores) can provide a O(\u03c3s+1) reconstruction error if the sample size ` is proportional to s log s [20].", "startOffset": 214, "endOffset": 218}, {"referenceID": 16, "context": "In [21], we show that the combined ID approximation and uniform sampling error in the far-field from a single source node to the rest of the points can be bounded as follows.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "7 in [21].", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "with constant and variable h; we select h based on kernel density estimation theory [27] (Eq.", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "(More discussion and results can be found in [21].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Runs (33-34) are done using 16 and 256 nodes respectively and the MPI library [14].", "startOffset": 78, "endOffset": 82}], "year": 2017, "abstractText": "We present a fast algorithm for kernel summation problems in high-dimensions. These problems appear in computational physics, numerical approximation, non-parametric statistics, and machine learning. In our context, the sums depend on a kernel function that is a pair potential defined on a dataset of points in a high-dimensional Euclidean space. A direct evaluation of the sum scales quadratically with the number of points. Fast kernel summation methods can reduce this cost to linear complexity, but the constants involved do not scale well with the dimensionality of the dataset. The main algorithmic components of fast kernel summation algorithms are the separation of the kernel sum between near and far field (which is the basis for pruning) and the efficient and accurate approximation of the far field. We introduce novel methods for pruning and for approximating the far field. Our far field approximation requires only kernel evaluations and does not use analytic expansions. Pruning is not done using bounding boxes but rather combinatorially using a sparsified nearest-neighbor graph of the input distribution. The time complexity of our algorithm depends linearly on the ambient dimension. The error in the algorithm depends on the low-rank approximability of the far field, which in turn depends on the kernel function and on the intrinsic dimensionality of the distribution of the points. The error of the far field approximation does not depend on the ambient dimension. We present the new algorithm along with experimental results that demonstrate its performance. As a highlight, we report results for Gaussian kernel sums for 100 million points in 64 dimensions, for one million points in 1000 dimensions, and for problems in which the Gaussian kernel has a variable bandwidth. To the best of our knowledge, all of these experiments are impossible or prohibitively expensive with existing fast kernel summation methods.", "creator": "LaTeX with hyperref package"}}}