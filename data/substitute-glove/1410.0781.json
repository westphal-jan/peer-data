{"id": "1410.0781", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2014", "title": "SimNets: A Generalization of Convolutional Networks", "abstract": "We on turn snow layered historical that formula_8 classical convolutional neural businesses (ConvNets ). The architecture, called SimNets, is demand by two operators, turned immediately a anatomical formula_13 whose oldest elements the formula_1 operator used in ConvNets, and although use known a end ' fashioned otto - min - good ' operator once MMECS should dad mathematics provide like ReLU they max - formulas, both recently requires programs the yet SimNets given capable generalization particular ConvNets. Two possibilities entities should emerge from the architecture fewer: (forget) that establishing specified either trace - components set output - nodes factory description as permanent an only cpu processor, and (army) sytem electronic using bdsm communication present environmental. Experiments aims under precision in achieving law of the library calculate each networks more any 1 / 8 the or common decrease ConvNets.", "histories": [["v1", "Fri, 3 Oct 2014 08:47:03 GMT  (628kb,D)", "https://arxiv.org/abs/1410.0781v1", null], ["v2", "Sat, 25 Oct 2014 09:47:07 GMT  (644kb,D)", "http://arxiv.org/abs/1410.0781v2", null], ["v3", "Sun, 7 Dec 2014 15:51:28 GMT  (648kb,D)", "http://arxiv.org/abs/1410.0781v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["nadav cohen", "amnon shashua"], "accepted": false, "id": "1410.0781"}, "pdf": {"name": "1410.0781.pdf", "metadata": {"source": "CRF", "title": "SimNets: A Generalization of Convolutional Networks", "authors": ["Nadav Cohen", "Amnon Shashua"], "emails": ["cohennadav@cs.huji.ac.il", "shashua@cs.huji.ac.il"], "sections": [{"heading": "1. Introduction", "text": "Convolutional neural networks (ConvNets) are attracting much attention largely due to their impressive empirical performance on large scale visual recognition tasks (c.f. [14, 29, 21, 24, 23]). The ConvNet architecture has the capacity to model large learning problems that include thousands of categories, while employing prior knowledge embedded into the architecture. The ConvNet capacity is controlled by varying the number of layers (depth), the size of each layer (breadth), and the size of the convolutional windows (which in turn are based on assumptions on local image statistics). The learning capacity is controlled using over-specified networks (networks that are larger than necessary in order to model the problem), followed by various forms of regularization techniques such as Dropout ([11]).\nDespite their success in recent years, ConvNets still fall short of reaching the holy grail of human-level visual recognition performance. Scaling up to such performance levels\ncould take more than merely dialing up network sizes while relying on prior knowledge to compensate for what we cannot learn. It may be worthwhile to challenge the basic ConvNet architecture, in order to obtain more compact networks for the same level of accuracy, or in other words, in order to increase the abstraction level of the basic network operations.\nA few observations have motivated our work. The first is that the ConvNet architecture has not changed much since its early introduction in the 1980s ([15]) \u2013 there were some attempts to create other types of deep-layered architectures (cf. [19, 3, 18]), but these are not commonly used compared to ConvNets. Arguably, the empirical success that ConvNets have witnessed in recent years is mainly fueled by the ever-growing scale of available computing power and training data, with the contribution of algorithmic advances having secondary importance. Our second observation is that although there were attempts to use unsupervised learning to initialize ConvNets (c.f. [10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]). We nevertheless believe that unsupervised initialization has an important role in scaling up the capacity of deep learning, and therefore find interest in deep architectures that give rise to natural initializations using unsupervised data. The third observation that motivated our work is that the ConvNet learning paradigm completely took over classification engines developed in the 1990s like Support Vector Machines (SVM) and kernel machines in general. These machine learning methods were well suited for \u201cflat\u201d architectures, and while attempts to apply them to deep layered architectures have been made ([4]), they did not keep up with the performance levels of the layered ConvNet architecture. It may be beneficial to develop a deep architecture that includes the body of work on kernel machines, but which still has the capacity to model large learning problems like the ConvNet architecture.\nIn this paper we introduce a new family of layered networks we call SimNets (similarity networks). The general idea is to \u201clift\u201d the classical ConvNet architecture into\nar X\niv :1\n41 0.\n07 81\nv3 [\ncs .N\nE ]\n7 D\nec 2\n01 4\nsomething more general, a multilayer kernel network architecture, which carries several attractive features. First, the architecture bridges the decades-old ConvNets with the statistical learning machinery of the last decade or so. Second, it provides a higher level of abstraction than the convolutional and pooling layers of ConvNets, thus potentially providing more compact networks for the same level of accuracy. Third, the architecture is endowed with a natural initialization based on unlabeled data, which also has the potential for determining the number of channels in each layer based on variance analysis of patterns generated from the previous layer. In other words, the structure of a SimNet can potentially be determined automatically from (unlabeled) training data.\nThe SimNet architecture is based on two operators. The first is analogous to, and generalizes, the convolutional operator in ConvNets. The second, as special cases, plays the role of ReLU activation ([17]) and max pooling in ConvNets, but in addition, has capabilities that make SimNets much more than ConvNets. In a set of limited experiments on CIFAR-10 dataset ([13]) using a small number of layers, we achieved better or comparable performance to state of the art ConvNets with the same number of layers, and the specialized network studied in [5], using 1/9 and 1/5, respectively, of the number of learned parameters.\nIn the following sections, we introduce the two operators that the SimNet architecture comprises, and describe its special cases and properties. The experiments section is still preliminary but demonstrates the power of SimNets and their potential for high capacity learning. Additional experiments with deeper SimNets are underway, but those require extensive optimization and coding infrastructure in order to apply to large scale settings."}, {"heading": "2. The SimNet architecture", "text": "The SimNet architecture consists of two operators \u2013 a \u201csimilarity\u201d operator that generalizes the inner-product operator found in ConvNets, and a soft max-average-min operator called MEX that replaces the ConvNet ReLU activation ([17]) and max/average pooling layers, and allows additional capabilities as will be described below.\nThe similarity operator matches an input x \u2208 Rd with a template z \u2208 Rd and a weight vector u \u2208 Rd+ (Rd+ stands for the non-negative orthant of Rd) through u>\u03c6(x, z), where \u03c6 : Rd \u00d7 Rd \u2192 Rd is a similarity mapping. We will consider two forms of similarity mappings: the \u201clinear\u201d form \u03c6(x, z)i = xizi, such that when setting u = 1 we obtain an inner-product operator, and the \u201clp\u201d form \u03c6(x, z)i = \u2212|xi \u2212 zi|p defined for p > 0.\nIn a layered architecture, a similarity layer is illustrated in fig. 1(a), where the similarity operator is applied to patches xij \u2208 RhwD of widthw, height h and depthD, with the indexes (i, j) describing the location of the patch within\nthe layer\u2019s input. Given n templates z1, ..., zn \u2208 RhwD and weights u1, ...,un \u2208 RhwD+ , the layer\u2019s output at coordinates (i, j, l) becomes u>l \u03c6l(xij , zl), where we use index l in \u03c6l to indicate that the similarity mapping may differ across channels. As customary with ConvNets, the width and height of the layer\u2019s output depends on the \u201cstride\u201d setting, which determines the step-size between input patches, e.g. with horizontal and vertical strides of s, the spatial dimensions of the output become b(H \u2212 h)/sc + 1 and b(W \u2212 w)/sc + 1. Note that using the linear-similarity mapping with unit weights(ul = 1) reduces the similarity layer to a standard convolutional layer where zl are the convolution kernels, whereas for lp-similarity with fixed p = 2, the output at coordinates (i, j, l) measures the weighted Euclidean (Mahalanobis) distance between the input patch xij and the template zl with every pixel weighted through the entries of the weight vector ul. When using lp-similarity in general, fixing the order p is not obligatory \u2013 the order can be learned based on training data, either globally or independently for each output channel.\nWe will see later on that, when setting unit weights, the (unweighted) linear and lp similarity mappings correlate with kernel-SVM methods of statistical machine learning (through special cases of the SimNet architecture), and that the view of zl as templates allows natural unsupervised initialization of networks using conventional statistical estimation methods.\nThe MEX operator, whose name stands for Maximumminimum-Expectation Collapsing Smooth (with \u201cCS\u201d pronounced as \u201cX\u201d), is responsible for the role of activation functions, max or average pooling (both spatially and across channels), and weights necessary for classification. The operator is defined as follows:\nMEX\u03be i=1,...,n\n{ci} := 1\n\u03be log\n( 1\nn n\u2211 i=1 exp{\u03be\u00b7ci}\n) (1)\nwith the alternative notation MEX\u03be{ci}ni=1 used interchangeably. The parameter \u03be \u2208 R spans a continuum between maximum, expectation (mean), and minimum:\nMEX\u03be{ci}ni=1 \u2212\u2192 \u03be\u2192+\u221e max{ci}ni=1 MEX\u03be{ci}ni=1 \u2212\u2192 \u03be\u21920 mean{ci}ni=1 MEX\u03be{ci}ni=1 \u2212\u2192 \u03be\u2192\u2212\u221e min{ci}ni=1\nMoreover, for a given value of \u03be, the operator is smooth and exhibits the \u201ccollapsing\u201d property defined below:\nMEX\u03be{MEX\u03be{cij}1\u2264j\u2264m}1\u2264i\u2264n = MEX\u03be{cij}1\u2264j\u2264m,1\u2264i\u2264n (2)\nIn a layered architecture, the MEX operator is used to define the MEX layer \u2013 see illustration in fig. 1(b). In the\nMEX layer, the input is divided into (possibly overlapping) blocks, each mapped to a single output element. The output value associated with the t\u2019th input block is given by:\nout(t) = MEX\u03bet { {inp(s) + bts}s\u2208block(t) , ct } where the index s runs though the input block, the offsets bts \u2208 R serve various roles as will be described later, and ct \u2208 R are optional (may or may not be used). The MEX layer can realize two standard ConvNet layers \u2013 the ReLU activation and the max-pooling layer. To realize ReLU activation, one should set the input blocks to be single entries, have the output dimensions equal to the input dimensions, set bts = 0, ct = 0, and let \u03bet \u2192 +\u221e, and as a result out(t) = max{inp(t), 0} as required (see fig. 1(c)). To realize a max-pooling layer, set the input blocks to cover a 2D area, set the depth of the output equal to that of the input, set bts = 0, omit ct, and set \u03bet \u2192 +\u221e. As a result out(i, j, l) = max{inp(i\u2032, j\u2032, l)}(i\u2032,j\u2032)\u2208pool(i,j) (see fig. 1(d)). Note that by setting \u03bet \u2192 0 one obtains an average-pooling layer, and moreover, the parameters \u03bet can be learned (optimized) as part of the training process, allowing additional flexibility.\nTo recap, the layers corresponding to the two operators of the SimNet architecture \u2013 similarity and MEX, can realize conventional ConvNets as follows:\n\u2022 Convolutional layer: use similarity layer with linear form \u03c6l(x, z)i = xizi and unit weights ul = 1.\n\u2022 ReLU activation: use MEX layer with bts = 0, ct = 0, \u03bet \u2192 +\u221e and single-entry input blocks.\n\u2022 Max pooling layer: use MEX layer with bts = 0, \u03bet \u2192 +\u221e, ct omitted, and 2D input blocks.\n\u2022 Dense layer: use similarity layer with the entire input as the only patch, linear form \u03c6l(x, z)i = xizi and unit weights ul = 1.\nNext, we make wider use of the two SimNet layers, taking us beyond classical ConvNets, exploring connections to classical statistical learning models with kernel machines."}, {"heading": "3. SimNets and kernel machines", "text": "So far, we set the architectural choices of SimNets to realize classical ConvNets, which form a rudimentary special case of the available possibilities. In particular, we did not make use of the lp similarity, and of the offsets {bts} in the MEX layer. In the following subsection, we consider a \u201cmulti-layer perceptron\u201d (MLP) construction consisting of a single hidden layer in addition to input and output layers. In the subsection that follows, we will study the case where the input layer is processed by patches, the hidden layer involves locality and sharing, and a pooling operation\nfollows the hidden layer \u2013 a structure prevalent in classical ConvNets."}, {"heading": "3.1. MLP analogy: input\u2192 hidden layer\u2192 output", "text": "The Similarity and MEX operators give straightforward generalizations of the convolution and max/average pooling layers in ConvNets. As we now show, they create something of greater consequence when applied one after the other in succession. To make the point as succinctly as possible, consider a MLP construction consisting of d input nodes (making up the input vector x \u2208 Rd), n hidden units, and a single output unit. The value h(x) of the output unit is a result of a mapping Rd \u2192 R, defined by the two SimNet operators applied in succession (n similarity operators with different templates and shared mapping \u03c6, followed by MEX with offsets):\nh(x) = MEX\u03be { u>l \u03c6(x, zl) + bl } l=1,...,n\n(3)\nA straightforward analogy to existing work is obtained by setting unit weights u = 1, linear similarity \u03c6(x, z)i = xizi and \u03be \u2192 \u221e, resulting in h(x) = max { z>l x + bl }n l=1\n\u2013 a maxout operation [8]. There were other attempts to generalize maxout, notably the recently proposed Lp unit [9],\nwhich is defined by (\n1 n \u2211n l=1|z>l x + bl|p )1/p . When p \u2192\n\u221e, this reduces to maxl { |z>l x + bl| } . The differences between this and the SimNet generalization of maxout are: (i) the Lp unit generalizes maximum of absolute values (rather than the values themselves), and (ii) the Lp unit tries to create a maxout in a single operation whereas the SimNet creates h(x) over a succession of two operators \u2013 similarity followed by MEX.\nNext, consider the case of fixed \u03be > 0 and unweighted (ul = 1) linear similarity (u>l \u03c6(x, zl) = x\n>zl) or unweighted lp similarity (u>\u03c6(x, z) = \u2212\u2016x\u2212 z\u2016pp) with fixed 0 < p \u2264 2. We will show below that in this case, the output h(x) is the result of a non-linear monotone activation function applied to the inner-product between a mapping of the input x and a vector w in some high-dimensional feature space RF . More formally, we will show that h(x) = \u03c3(\u3008w, \u03c8\u03c6(x)\u3009), where the mapping \u03c8\u03c6 : Rd \u2192 RF depends on the choice of similarity mapping \u03c6, \u03c3 is a non-linear monotone activation function, and w = \u2211n l=1 \u03b1l\u03c8\u03c6(zl) for some \u03b11, ..., \u03b1n \u2208 R. We thus conclude that the output unit is a \u201cneuron\u201d in the classical sense, but in a high-dimensional feature space. To prove this assertion, we notice that h(x) can be expressed as follows:\nh(x) = MEX\u03be { u>l \u03c6(x, zl) + bl } l=1,...,n\n= 1\n\u03be ln\n( 1\nn n\u2211 l=1 \u03b1l \u00b7 exp\n{ \u03be\nd\u2211 i=1 \u03c6(x, zl)i\n})\n= \u03c3 ( n\u2211 l=1 \u03b1l \u00b7K\u03c6(x, zl) ) (4)\nwhere \u03b1l := e\u03be\u00b7bl and \u03c3 is a non-linear monotone activation function given by \u03c3(t) = 1\u03be ln ( t n ) . We use the notation\nK\u03c6(x, z) := exp { \u03be \u2211d i=1 \u03c6(x, z)i } to indicate that, under the similarity mappings considered, the function is a kernel on Rd. In particular, for the linear and lp similarities we have:\nKlin(x, z) = exp { \u03be \u00b7 x>z } Klp(x, z) = exp { \u2212\u03be \u2016x\u2212 zl\u2016pp\n} As shown in [20], Klin and Klp are kernels on Rd (note that for p > 2, the expression above for Klp is not a kernel). We refer to them as the \u201cExponential\u201d kernel and the \u201cGeneralized Gaussian\u201d kernel1 respectively. Since \u03c3 is monotonically increasing, h(x) realizes a 2-class \u201creduced\u201d kernel-SVM decision rule, with zl being the (reduced) support-vectors2 and \u03b1l \u2265 0 being the coefficients associated with the support-vectors. Let \u03c8\u03c6 : Rd \u2192 RF be a feature mapping corresponding to K\u03c6, i.e. K\u03c6(x, z) = \u3008\u03c8\u03c6(x), \u03c8\u03c6(z)\u3009. Eqn. 4 can now be expressed as h(x) = \u03c3(\u3008w, \u03c8\u03c6(x)\u3009), where w := \u2211n l=1 \u03b1l\u03c8\u03c6(zl). This shows that the output unit h(x) is a \u201cneuron is feature space\u201d, as stated above.\nIn the case of weighted (ul are learned) lp-similarity, the hypothesis space realized by the output unit h(x) is no longer representable by a kernel-SVM. Moreover, the view of h(x) as a neuron in feature space with learned vector w no longer applies. This is stated formally below (proof in app. A):\nTheorem 1. For any dimension d \u2208 N, and constants c > 0 and p > 0, there are no mappings Z : Rd \u2192 Rd and U : Rd \u2192 Rd+ and a kernelK : (Rd\u00d7Rd+)\u00d7(Rd\u00d7Rd+)\u2192 Rd \u00d7 Rd+, such that for all z,x \u2208 Rd and u \u2208 Rd+:\nK ([Z(x), U(x)], [z,u]) = exp { \u2212c\nd\u2211 i=1 ui|xi \u2212 zi|p } (5)\nWe now turn to consider a straightforward extension to the setup above, which includes k output units. The MLP will now consist of an input signal x \u2208 Rd, a set of n hidden units defined by similarity functions over x (all based on the same similarity mapping \u03c6), and a set of k output units defined by MEX operators (all having the same parameter \u03be) with offsets brl where l \u2208 {1, ..., n} runs over the hidden units and r \u2208 {1, ..., k} is the index of the output unit.\n1When p = 2, this reduces to the well-known Gaussian (radial basis function) kernel. When p = 1, it reduces to the Laplacian kernel.\n2We use the term \u201creduced\u201d to refer to the case where the number of support-vectors is predetermined and they are not constrained to lie in the training set. This setting was studied in [27] in the context of binary (2- class) classification. The extension to multiclass ([6]) is straightforward.\nFig. 1(e) illustrates this basic operation. If we consider the output nodes as predicting a label associated with the input x, the chosen label being the index of the node with maximal activation, then running the two operators, similarity and MEX, one following the other, produces the classification rule below:\ny\u0302(x) = argmax r=1,...,k\nMEX\u03be{u>l \u03c6(x, zl) + brl}nl=1 (6)\nThis classification measures weighted similarities to n templates, with class-dependent offsets. The role of the MEX operators is to combine the weighted similarities (with offsets) of the input x to the templates. For example, when \u03be \u2192 +\u221e, the classification rule is attracted to the most similar template where offsets assign relevancy of templates to classes. Let hr(x), r = 1, ..., k, be the value of output unit r when the MLP is fed with input x. Following the lines of the derivation carried out for the single-output MLP, when working with unweighted linear similarity or unweighted lp similarity with 0 < p \u2264 2, it holds that:\nhr(x) = \u03c3 (\u3008wr, \u03c8\u03c6(x)\u3009)\nwhere wr := \u2211n l=1 \u03b1rl\u03c8\u03c6(zl) and \u03b1rl := e\n\u03be\u00b7brl . Moreover, the decision rule in eqn. 6 can be expressed as:\ny\u0302(x) = argmax r\u2208{1,...,k} hr(x)\n= argmax r\u2208{1,...,k}\n\u3008wr, \u03c8\u03c6(x)\u3009\n= argmax r\u2208{1,...,k} n\u2211 l=1 \u03b1rlK\u03c6(x, zl)\nwhere K\u03c6 is a kernel (Exponential or Generalized Gaussian) on Rd. This classification realizes the hypothesis space of a reduced multiclass kernel-SVM3.\nTo summarize so far, we have shown that with linear similarity, the \u201cMLP\u201d construction consisting of input \u2192 hidden layer \u2192 output, gives rise to the hypothesis space of a (reduced) SVM with the Exponential kernel. Replacing the linear similarity with unweighted lp similarity having fixed order p, gives rise to a kernel-SVM if and only if p \u2264 2, in which case the underlying kernel is the Generalized Gaussian kernel (the special cases of Gaussian and Laplacian kernels are obtained for p = 2 and p = 1 respectively). With these similarities that give rise to kernel machines, a unit generated by similarity operators followed by MEX with offsets is a \u201cneuron in feature space\u201d. Finally, with weighted (ul are learned) lp similarity, the framework is no longer representable by a kernel-SVM.\n3Note that the coefficients \u03b1rl are positive, whereas in classical multiclass SVM they may be any real numbers. This however does not limit generality, as we can always add a common offset to all coefficients after SVM training is complete.\nTo obtain a sense of the network\u2019s abstraction level, i.e. its ability to capture concept (category) distributions in input space, consider the classification rule in eqn. 6 in the case \u03be \u2192 +\u221e:\ny\u0302(x) = argmax r=1,...,k max l=1,...,n\n{u>l \u03c6(x, zl) + brl}\nFor any r \u2208 {1, ..., k}, denote by Ar the decision region in input space that corresponds to class r, i.e. Ar := {x \u2208 Rd : y\u0302(x) = r}. To understand the shape of Ar, we make the following definitions:\nAr \u2032,l\u2032\nr,l := (7)\n{x \u2208 Rd : u>l \u03c6(x, zl) + brl \u2265 u>l\u2032 \u03c6(x, zl\u2032) + br\u2032l\u2032}\nAr,l := \u22c2\n(r\u2032,l\u2032)6=(r,l)\nAr \u2032,l\u2032\nr,l\nwhere the class index r\u2032 ranges over {1, ..., k}, and the template indexes l, l\u2032 range over {1, ..., n}. One can readily see that up to boundary conditions:\nAr = \u22c3\nl\u2208{1,...,n}\nAr,l\nConsider first the setting of linear similarity (\u03c6(x, z)i = xizi). In this case A r\u2032,l\u2032\nr,l are half-spaces and Ar,l are intersections of half-spaces (polytopes). The decision region Ar is thus a union of n polytopes. As we now show, this is the same type of decision regions as obtained with unweighted l2 similarity (ul = 1, \u03c6(x, z)i = \u2212|xi \u2212 zi|2). Indeed, in this case the term \u2016x\u201622 in both sides of the inequality defining Ar \u2032,l\u2032\nr,l cancels-out, and we obtain again a half-space. This in turn implies that as before, Ar,l are polytopes and Ar is a union of polytopes. We conclude that with the MLP structure of: input\u2192 hidden layer\u2192 output units, the setting that realizes a Gaussian kernel machine (unweighted l2 similarity), is qualitatively equivalent to the \u201cConvNet\u201d (linear similarity) setting that realizes an Exponential kernel machine. The difference in kernels does not account for any material difference in the network\u2019s hypothesis space, i.e. its abstraction level.\nRemaining with l2 similarity, we now consider the weighted setting, i.e. the setting in which ul are not fixed. Thm. 1 tells us that in this case the hypothesis space is no longer governed by kernel-SVM. From the decision region point-of-view, it is not difficult to see that in this case Ar \u2032,l\u2032\nr,l\nis no longer a half-space, but a region defined by a secondorder hyper-surface. This implies that the set Ar,l is no longer a polytope, and in particular is not necessarily convex. The possible shapes that the decision region Ar can take are thus enriched. We conclude that unlike in the case\nof unweighted l2 similarity, the setting of weighted l2 similarity is characterized by an abstraction level higher than that induced by linear similarity (convolutional operator).\nIn the general setting of lp similarity, the sets A r\u2032,l\u2032\nr,l are more complex, and may be governed by non-convex nonsmooth separating hyper-surfaces. The full analysis is outside the scope of this paper, but an informal illustration of how the space is divided for p = 1 and d = 2 (Rd is the 2D plane) is given in fig. 2. Under this specific setup, the 2D plane is divided into two by a piece-wise linear separating boundary. The unweighted case (uniform weights) is shown in fig. 2(a). In this case the space is divided equally (up to a shift caused by the offsets brl, br\u2032l\u2032 ) based on the l1 (Manhattan) distance metric. Adding weights deforms the boundary line, where the higher the weights associated with a template (zl or zl\u2032 ) are, the less space is allocated to that template. For example, in fig. 2(d) the weights associated with the template zl\u2032 are uniformly high, thereby creating a small aperture in the 2D plane around that template. Given that Ar \u2032,l\u2032\nr,l is highly non-convex in the weighted setting, we expect weighted l1 similarity to provide a higher abstraction level than that of linear similarity (convolutional operator)."}, {"heading": "3.2. A basic 3-layer SimNet with locality, sharing", "text": "and pooling\nNext, we analyze a 3-layer SimNet with locality, sharing and pooling. The network\u2019s input is processed by patches (locality), with the same templates and weights applied to all patches (sharing), thereby creating a stack of feature maps (channels) \u2013 one for each template. Spatial regions of each feature map are then pooled together to reduce dimensionality, and finally, a classification output layer predicts the label of the input. We will show that such a network, consisting of input \u2192 feature maps \u2192 pooling \u2192 output, also corresponds to a kernel-SVM, with the kernels designed for a \u201cpatch-based\u201d representation of the input signal.\nLocality, sharing and pooling are realized in the conventional manner. Namely, the input is divided into (possibly overlapping) patches xij \u2208 Rd where d = h \u00b7 w \u00b7 D, with h,w being the height and width of the patches. A similarity layer as illustrated in fig. 1(a), but with the same similarity mapping \u03c6 for all channels, matches the patch xij with the template zl \u2208 Rd (which is now a template representing a local patch in the layer\u2019s input) using the weights ul \u2208 Rd+, and the resulting value u>l \u03c6(xij , zl) is stored in coordinates (i, j, l) of the layer\u2019s output.\nThe mapping from the similarity layer to the k-node classification output is realized through two MEX layers. The first MEX layer implements a pooling layer as follows. Let q(i, j) = (qh(i), qw(j)) be a (contraction) mapping of the 2D coordinate system in the similarity layer to the 2D coordinate system in the pooling layer. Normally, a 2D co-\nordinate in the pooling layer corresponds to a 2D window in the similarity layer. The value assigned to an element in the pooling layer is simply a MEX operation taken over the corresponding 2D window (in the respective channel l \u2208 {1, ..., n}) in the similarity layer:\npool(ph, pw, l) = MEX\u03be1{sim(i, j, l)}i,j:q(i,j)=(ph,pw)\nAll MEX operators in the layer have the same parameter \u2013 \u03be1. When \u03be1 \u2192 +\u221e for instance, we obtain max-pooling as implemented in conventional ConvNets.\nThe second MEX layer implements a dense mapping from the pooling layer to the k output nodes, which includes offsets. The value of the r\u2019th output node is given by:\nout(r) = MEX\u03be2{pool(ph, pw, l) + brlphpw}ph,pw,l\nwhere (ph, pw) runs over the 2D coordinates of the pooling layer and l runs over the pooling layer\u2019s channels (which correspond to templates). Note that here too all MEX operators have the same parameter \u2013 \u03be2. The offsets brlphpw depend on the output node r and on the 3D coordinates of the pooling layer (ph, pw, l), i.e. for every output node there is an offset for each coordinate in the pooling layer.\nThe SimNet we obtain is illustrated in fig. 1(f). It is a basic similarity-pooling-output network that employs locality and sharing, as conventional ConvNets do. The point to be made next, is that in the special case where ul = 1, \u03be1, \u03be2 are fixed to a constant \u03be > 0, and \u03c6 is set to linear form or lp form with fixed p \u2264 2, the classification resulting from this network is a kernel-SVM, where the kernel is designed for a \u201cpatch-based\u201d representation of the input signal. First, by concatenating the three steps \u2013 similarity, pooling and output, and assuming that \u03be1 = \u03be2 = \u03be > 0, the decision rule associated with the network becomes:\ny\u0302(inp) = argmax r=1,...,k MEX\u03be i,j,l\n{ u>l \u03c6(xij , zl) + br,l,q(i,j) } (8)\nThis follows from the identities below:\nMEX\u03be ph,pw,l\n{ MEX\u03be\ni,j:q(i,j)=(ph,pw)\n{ u>l \u03c6(xij , zl) } + brlphpw } = MEX\u03be ph, pw, l, i,j:q(i,j)=(ph,pw) { u>l \u03c6(xij , zl) + brlphpw\n} = MEX\u03be\ni,j,l\n{ u>l \u03c6(xij , zl) + br,l,q(i,j) } where for the first equality, we used the collapsing property of the MEX operator described in eqn. 2. The classification described in eqn. 8 is similar to that described in eqn. 6, but has two important distinctions: (i) the templates zl are local and similarity is applied locally (hence the \u201clocality\u201d and \u201csharing\u201d), and (ii) the offsets are region-based (hence the \u201cpooling\u201d), i.e. each collection of input patches xij ascribed to the same pool is associated with a single set of offsets (per-class and per-template). To see the kernel structure associated with this classification, we perform the following manipulations to the rule given in eqn. 8:\ny\u0302(inp) =\nargmax r=1,...,k MEX\u03be i,j,l\n{ u>l \u03c6(xij , zl) + br,l,q(i,j) } =\nargmax r=1,...,k \u2211 i,j,l e\u03be\u00b7(u > l \u03c6(xij ,zl)+br,l,q(i,j)) =\nargmax r=1,...,k \u2211 i,j,l e\u03be\u00b7br,l,q(i,j)\ufe38 \ufe37\ufe37 \ufe38 :=\u03b1r,l,q(i,j) \u00b7e\u03be\u00b7u>l \u03c6(xij ,zl) =\nargmax r=1,...,k \u2211 ph,pw,l \u03b1rlphpw \u2211 i,j:q(i,j)=(ph,pw) e\u03be\u00b7u > l \u03c6(xij ,zl)(9)\nSetting ul = 1, and referring to subsec. 3.1, we denote K\u03c6(xij , zl) := exp{\u03be\u00b71>\u03c6(xij , zl)}, emphasizing that this function is a kernel for the similarity mappings we consider (Exponential kernel for linear similarity, Generalized Gaussian kernel for lp similarity with fixed p \u2264 2). Eqn. 9 then becomes:\ny\u0302(inp) = (10) argmax r=1,...,k \u2211 ph,pw,l \u03b1rlphpw \u2211 i,j:q(i,j)=(ph,pw) K\u03c6(xij , zl)\n= argmax r=1,...,k\n\u2211 ph,pw,l \u03b1rlphpwK\u03c6(X,Zlphpw)\nwhere X contains the concatenation of all the input patches xij , and Zlphpw is a structure containing copies of zl in locations corresponding to the pool index (ph, pw) \u2013 the details, including definition of K\u03c6 and proof that it is indeed a kernel, are given in app. B."}, {"heading": "4. Other SimNet settings \u2013 global average pooling", "text": "In subsec. 3.2 we introduced the SimNet basic building chain of the form: input\u2192 similarity\u2192 pooling\u2192 output, whose structure follows the line of classical ConvNets. We noted that the basic building chain realizes a kernel-SVM hypothesis space, where the templates in the similarity layer correspond to the (reduced) support-vectors, and the offsets in the last MEX layer (from pooling to output) are related to the SVM coefficients. The SVM hypothesis space is realized when the similarity operator is set to linear form or lp form with fixed p \u2264 2, and is unweighted (ul = 1). Using weighted lp similarity (weights are not applicable to linear similarity) has the potential of providing a richer hypothesis space than kernel-SVM (at the expense of doubling the number of parameters in the similarity layer). Indeed, experiments we conducted (reported in sec. 6) validate the power of lp similarity weighting, showing that it matters more than merely the added number of parameters to the model.\nIn this section, we introduce another SimNet building chain with two MEX layers, designed in such a way that when the MEX parameters are equal, the chain collapses into the one presented above (decision rule in eqn. 8), but when the MEX parameters are determined separately \u2013 either learned using training data or set manually, the SimNet chain allows for new possibilities (without additional parameters). For example, setting the MEX parameter of the first layer to 1 and that of the second layer to 0 gave rise to the best experimental performance we encountered.\nThe idea is to switch the roles of the two MEX layers \u2013 rather than having the first play the role of pooling and the second the role of classification (using the offsets brl), we start with a MEX layer with offsets and finish with a MEX for pooling. The interpretation of such a structure is that each input patch xij undergoes classification in the first MEX layer. The second MEX layer performs a majority voting over all the patch-based classification results to form a final classification decision. This approach follows the line of the \u201cglobal average pooling\u201d structure recently suggested in the context of ConvNets, which has been shown to outperform the traditional \u201cdense classification\u201d paradigm ([16, 23]). To enforce spatial consistency in the labeling characteristics of patches, we constrain the first MEX layer\u2019s offsets to be uniform inside predetermined spatial regions. The resulting SimNet, which we refer to as a \u201cpatch labeling\u201d network, is illustrated in fig. 1(g) (note\nthat all channels in the similarity layer share the same similarity mapping, and that both MEX layers have global parameters \u03be1, \u03be2). Its classification rule takes the following form:\ny\u0302(inp) = argmax r=1,...,k out(r)\nwith:\nout(r) =\nMEX\u03be2 i,j {MEX\u03be1 l {u>l \u03c6(xij , zl) + br,l,q(i,j)}}\nThe variables which can be learned here are the offsets brlphpw \u2208 R (with r ranging over the classes, l over the templates and (ph, pw) over the regions in which offsets are shared), the templates zl \u2208 RhwD, the similarity weights ul \u2208 RhwD+ , the order p > 0 in case lp similarity is chosen, and the MEX parameters \u03be1, \u03be2 \u2208 R. Assume we constrain the MEX parameters to be equal: \u03be1 = \u03be2 = \u03be. The MEX collapsing property (eqn. 2) then applies, and the classification rule becomes:\ny\u0302(inp) = argmax r=1,...,k\nMEX\u03be{u>l \u03c6(xij , zl) + br,l,q(i,j)}i,j,l\nwhich is identical to the decision rule in eqn. 8. However, there is no reason to have the MEX parameters equal to each other. We can estimate their value during training, or set them manually. For example, during our experimentation we found that the case of equal MEX parameters \u2013 \u03be1 = \u03be2 = \u03be, is significantly outperformed by the setting \u03be2 \u2192 0, which corresponds to the following classification rule:\ny\u0302(inp) =\nargmax r=1,...,k\n\u2211 i,j MEX\u03be1\nl\u2208{1,...,n}\n{ u>l \u03c6(xij , zl) + br,l,q(i,j) }"}, {"heading": "5. Initializing parameters using unsupervised", "text": "learning For classical ConvNets, various schemes of initializing a network based on unlabeled data (unsupervised initialization) have been proposed (c.f. [10, 2, 25]). Over time, however, these were taken over by carefully selected random initializations that do not use data at all (see for example [14, 29, 21]). No initialization scheme to-date is sufficient on its own for overcoming the hardness of training. Indeed, successful training of ConvNets typically requires designing an over-specified network (i.e. a network that is much larger than necessary in order to represent the true hypothesis space). While the latter has been shown to produce good training results, it bares a computational price, and also aggravates the problem of overfitting. The enhanced susceptibility to overfitting has led to various regularization techniques and heuristics (Dropout ([11]) being the most\nprominent), which nowadays form an art that one must master in order to properly train ConvNets. In this section, we discuss a natural unsupervised initialization scheme for SimNets, which is based on statistical estimation. Such a scheme may provide a more effective local minima in the process of training a SimNet, thereby reducing the need for over-specification, supporting smaller networks that are more efficient computationally, and less prone to overfit. Experiments we conducted (reported in sec. 6) validate this conjecture, showing that the SimNet unsupervised initialization scheme indeed improves performance over random initializations, especially in the case of small networks.\nRecall from sec. 2 that measuring weighted similarities to templates forms the similarity layer \u2013 a basic building block of the SimNet architecture (see fig. 1(a)). Focusing on the case of lp similarity mappings (\u03c6(x, z)i = \u2212|xi\u2212 zi|p), we show how the application of statistical estimation methods to unlabeled training data can produce initialization values for the layer\u2019s templates z1, ..., zn, weights u1, ...,un and orders p1, ..., pn. Consider a probability distribution over Rd defined by a mixture of n Generalized Gaussian distributions, each having independent coordinates with a shared shape parameter and separate scales and means:\nP (x) = n\u2211 l=1 \u03bbl d\u220f i=1 \u03b2l 2\u03b1l,i\u0393(1/\u03b2l) exp\n{ \u2212 ( |xi \u2212 \u00b5l,i|\n\u03b1l,i )\u03b2l} In the above, \u03bbl stands for the prior probability of component l (\u03bbl \u2265 0, \u2211 l \u03bbl = 1), \u03b2l > 0 stands for the shape parameter of all coordinates in component l, \u03b1l,i > 0 stands for the scale of coordinate i in component l, \u00b5l,i \u2208 R stands for the mean of coordinate i in component l, and \u0393 is the Gamma function, defined by \u0393(s) = \u222b\u221e 0 e\u2212tts\u22121dt. The log-probability that a vector drawn from this distribution is equal to x and originated from component l is:\nlogP (x \u2227 component l) = \u2212 d\u2211 i=1 \u03b1\u2212\u03b2ll,i |xi \u2212 \u00b5l,i| \u03b2l + cl\nwhere cl := log { \u03bbl \u220fd i=1 \u03b2l 2\u03b1l,i\u0393(1/\u03b2l) } is a constant that depends on the component only (not on x). Setting the layer\u2019s templates by zl.i = \u00b5l,i, its weights by ul,i = \u03b1 \u2212\u03b2l l,i and its lp orders by pl = \u03b2l, would give:\nu>l \u03c6l(xij , zl) = logP (x \u2227 component l)\u2212 cl\nThis implies that if we assume input patches follow a Generalized Gaussian mixture as described, initializing the similarity layer\u2019s templates, weights and orders as above would result in channel l of the layer\u2019s output holding, up to a constant, the probabilistic heat map of component l and the patches. This observation suggests estimating the parameters (shapes \u03b2l, scales \u03b1l,i and means \u00b5l,i) of the Generalized Gaussian mixture using unlabeled input patches (via\nstandard statistical estimation methods, such as that presented in [1]), and initializing the similarity layer accordingly.\nConsider now the case where the initialized lp similarity layer is followed by a MEX layer with learned offsets (see fig. 1(h), where for convenience, the linear index t is used to refer to elements of the MEX layer\u2019s 3D output array). We now assume that not only do input patches come from a mixture of Generalized Gaussian components as above, but also that each input patch location corresponds to a different mixture (priors) of these components. This makes sense, as certain templates that are likely to appear in the center of an image for example, may be less likely to appear on the top-left corner of the image, for example. Using our estimates of the global components obtained during the initialization of the similarity layer, we can estimate a mixture for a certain input patch location, by applying an estimation method to patches only from that location, with the component shapes, means and scales held fixed. We may then calculate offsets for the n elements of the similarity layer\u2019s output that correspond to that location, such that the probabilistic heat maps will take into account the location-dependent statistics, and will be precise (not up to a constant). For example, if there is a region in the input for which a certain template is very unlikely to appear, that template\u2019s heat map in the aforementioned region will be suppressed. The offsets we compute may serve for initialization of the MEX layer\u2019s offsets."}, {"heading": "6. Experiments", "text": "We implemented the \u201cpatch labeling\u201d SimNet discussed in sec. 4, and experimented with the specific architectural settings illustrated in fig. 3(a). The network consists of a lp-similarity layer with p fixed at 1 or 2, followed by two MEX layers. Implementation and evaluation of deeper SimNets is currently under work, and will be reported at a later time. For the experiments reported here, we used the CIFAR-10 dataset ([13]), which consists of 60, 000 color images (50, 000 for training and 10, 000 for testing) of size 32 \u00d7 32 partitioned into 10 classes, with 6, 000 images per class (5, 000 for training, 1, 000 for testing). The network\u2019s input is an RGB image (32 \u00d7 32 \u00d7 3 array), processed by patches of size 6 \u00d7 6 \u00d7 3 with a single-pixel stride between them. For a given number of templates in the similarity layer (denoted by n), the SimNet\u2019s learned parameters are the templates z1, ..., zn \u2208 R108, the similarity weights u1, ...,un \u2208 R108+ , and the MEX offsets brlphpw \u2208 R with r = 1, ..., 10, l = 1, ..., n and pw, ph = 1, 2. We used statistical estimation as described in sec. 5 to initialize the templates zl and the similarity weights ul (initialization was based on training images, without making use of their labels). The network was then trained by minimizing a softmax loss with stochastic gradient descent (SGD) that in-\ncludes momentum and acceleration ([22]). For SGD, we used a batch size of 64, a momentum of 0.9, and a learning rate of 0.01 decreased by a factor of 10 after 50 epochs, running 100 epochs in total. The weight decay for the templates was set to zero, and those for the similarity weights and offsets were set equal to each other, their value chosen via cross-validation.\nWe compared the SimNet to instances of two learning architectures. The first is a ConvNet with a single convolutional layer followed by a pooling layer followed by an output layer (see illustration in fig. 3(b)). The purpose of this comparison is to evaluate the SimNet against an analogous ConvNet, measuring the network sizes (number of learned parameters) required to reach given accuracies. A successful outcome here would be if the SimNet reached the same (or higher) level of performance as the ConvNet, with\nconsiderably smaller network size. The second comparison we held was against the \u201csingle-layer\u201d network studied by Coates et al. ([5]), which has the same depth as the evaluated SimNet, and whose performance on CIFAR-10 is one of the best reported for networks of such depth (absolute state of the art in 2011). In [5], a number of unsupervised learning methods were devised for \u201ccoding\u201d the input image. The coding methods were based on \u201ctemplates\u201d, such that each template corresponded to a single feature map. The feature maps were passed on to a sum-pooling operator, and from there a linear SVM was learned using supervised data. Many coding methods were experimented on, and the one that produced the best results, referred to as \u201ctriangle\u201d coding, was a \u201csoft\u201d Euclidean measure applied to templates learned via k-means. This coding method, along with the other architectural settings that produced the best results,\nare illustrated in fig. 3(c). Finally, we question the importance of the unsupervised initialization scheme described in sec. 5, by training the evaluated SimNet with random initialization (as customary with ConvNets), and examining the effect on the cross-validation accuracies.\nThe results reported below show that the evaluated SimNet achieves performance comparable to that of the ConvNet and the network of Coates et al., with only a fraction of the number of learned parameters. The unsupervised initialization scheme indeed boosts performance, and can be viewed as one of the drivers behind the SimNet\u2019s superiority. We are currently working on the optimization of\nour code (including GPU acceleration), to enable evaluation of larger and deeper SimNets on more meaningful benchmarks, comparing against deep state of the art ConvNets."}, {"heading": "6.1. Benchmarking against the ConvNet", "text": "The ConvNet was implemented using Caffe toolbox ([12]), with random initialization and SGD training. We used a batch size of 100, momentum of 0.95, and learning rate of 10\u22124 decreased by a factor of 10 every 45 epochs, running 150 epochs in total. The global weight decay and the dense layer\u2019s DropOut rate were chosen via cross-validation. The ConvNet\u2019s input was an RGB image\n(32\u00d7 32\u00d7 3 array) normalized for brightness and contrast. For the SimNet we also added patch \u201cwhitening\u201d, in accordance with the suggestion of [5]. The positive effect of whitening for the SimNet (which has l1/l2 similarities) was verified experimentally, whereas for the ConvNet, we observed that whitening does not have a positive effect (complying with the observations of [5]).\nFig. 4 shows the cross-validation accuracies of the evaluated networks as a function of the number of templates (n). Fig. 5 plots the same results against the number of learned parameters in the networks. For the SimNet, we experimented with up to 400 templates (we believe that more templates would only give marginal improvements in accuracy, and thus did not continue further), and reached accuracies of 76.8% and 77.1% with weighted l1 and l2 similarities respectively. We ran the ConvNet with up to 6, 400 templates (beyond that Caffe had GPU memory management issues), with the highest accuracy standing at 76.2%. In comparison, taking into account that with weighted similarities each template carries with it a weight vector, the size (number of learned parameters) of the 400-template SimNet with weighted similarities was less than 1/9 the size of the 6, 400-template ConvNet, while achieving slightly superior accuracy. The performance of the SimNet with unweighted similarities on the other hand, is very similar to that of the ConvNet, thus highlighting the importance of the weights in the similarity layer. The weights double the number of parameters in the layer, but the increase in performance scales up super-linearly with the number of added parameters. In other words, weights provide a gain in accuracy which is much higher than what would be obtained by simply adding more templates until reaching the same network size. For example, the accuracies with weights at 100 templates are considerably higher than the accuracies without weights at 200 templates, despite the fact that in the latter case, the overall network size is higher.\nIt is worth noting that the performance of the SimNet with unweighted l1 and l2 similarities is comparable to that of the ConvNet. This confirms what we observed formally in subsec. 3.1 \u2013 the hypothesis space (analyzed through the shapes of decision regions) corresponding to unweighted l2similarity is essentially the same as that which corresponds to linear similarity (convolutional operator). The hypothesis space corresponding to unweighted l1-similarity is different, but apparently does not provide a higher degree of abstraction (further study of this is deferred to future work).\nAlthough the SimNet accuracies achieved here are not state of the art for this dataset, the results demonstrate the potential of SimNets for modeling learning problems with significant reduction in network sizes compared to ConvNets."}, {"heading": "6.2. Benchmarking against the \u201csingle-layer\u201d network of Coates et al.", "text": "The \u201csingle-layer\u201d network studied by Coates et al. ([5]) is of interest on several accounts. First, with GMM coding, the network is equivalent to the SimNet variant presented in eqn. 9. Second, their best result with \u201ctriangle\u201d coding is one of the highest accuracies on CIFAR-10 reported for networks of this depth (absolute state of the art in 2011). Third, their observations with respect to the effect of whitening are relevant to the SimNet architecture, and indeed, we found that for the evaluated SimNet with l1 and l2 similarities, whitening makes a difference.\nIn [5], the network \u201ctemplates\u201d (i.e. the parameters of the selected coding method) were set using unlabeled data, and were not modified in the supervised training phase. To facilitate a fair comparison against our SimNet (where templates are modified in supervised training), we added an additional supervised training phase, which applied to both the templates and the SVM coefficients. More specifically, we used SGD to jointly modify the network templates and SVM coefficients produced by [5], in an attempt to reach higher accuracy levels than those reported by the authors. As it turned out, with the triangle coding they proposed, the supervised update of the templates did not improve accuracy any further than the original k-means clustering. Deep inspection of this phenomena revealed that the k-means clustering (along with the SVM that follows) provides a strong local minima for the learning problem, so even the training accuracy was not improved. This leads us to believe that the triangle coding is so successful precisely because it creates a representation for which k-means finds optimal templates, that cannot be improved even in the presence of labeled data. In [5] results are reported for up to 1, 600 templates. We used their code to reproduce these results, while running up to 6, 400 templates. The accuracy curve we obtained is displayed in fig. 4, with 77.3% for 1, 600 templates, 78.3% for 3, 200 templates, and 77.8% for 6, 400 templates. The peak accuracy was achieved for 3, 200 templates, and was slightly higher than the SimNet peak accuracy, which stood at 77.1% for weighted l2-similarity and 400 templates. The SimNet on the other hand was almost 1/5 in size (see fig. 5)."}, {"heading": "6.3. The importance of unsupervised initialization", "text": "To assess the importance of the SimNets\u2019 unsupervised initialization scheme presented in sec. 5, we trained the evaluated SimNet (fig. 3(a)) with weighted l1 similarity, using no data for initialization. In particular, we initialized the templates z1, ..., zn randomly with a zero-mean unit-variance Gaussian distribution (in accordance with the fact that the input patches are whitened to have zero-mean and unit variance), and the weights u1, ...,un with constant ones. Besides the difference in initialization, the SimNet was trained exactly as described above. Running the\nexperiment with 200 templates, cross-validation accuracy dropped from 76% to 74.1%. With 400 templates, accuracy declined from 76.8% to 74.4%. With 50 and 100 templates, the learning algorithm did not converge. We conclude that the SimNet unsupervised initialization scheme indeed has significant impact on performance. The impact is especially acute for networks of small size. This complies with conventional wisdom, according to which training small networks poses more difficult optimization problems. The SimNet initialization scheme may provide an alternative to the common practice of over-specifying networks (constructing networks larger than necessary in order to ease the optimization task)."}, {"heading": "7. Discussion", "text": "We presented a deep layered architecture called SimNets, with similar ingredients as classical ConvNets. The architecture is driven by two operators: (i) the similarity operator, which is a generalization of the convolutional operator in ConvNets, and (ii) the MEX operator, which can realize classical operators found in ConvNets like ReLU and max pooling, but has additional capabilities that make SimNets a powerful generalization of ConvNets. One of the interesting properties of the SimNet architecture is that applying its two operators in succession \u2013 similarity followed by MEX, results in what can be viewed as an artificial neuron in a high-dimensional feature space. Moreover, the multilayer perceptron construction of input to hidden layer to output, as well as the fundamental building block incorporating locality, sharing and pooling, are both generalizations of kernel machines.\nWe described two possible similarity measures: the lp similarity, which in its unweighted version gives rise to the Generalized Gaussian kernel, and the linear similarity, which is the operator found in ConvNets, and gives rise to the Exponential kernel. We also showed that the full specification of the lp similarity operator, which includes weights, goes beyond a kernel machine and carries with it a higher abstraction level than what a convolutional layer can express. Another interesting property of the SimNet architecture is that statistical estimation methods for Generalized Gaussian mixture distributions can be used for unsupervised initialization of network parameters. These initializations arise naturally from standard statistical assumptions, having the potential of employing unsupervised learning in an effective manner as part of deep learning.\nImplementing deep SimNets with state of the art optimization techniques (including GPU acceleration) is an ongoing effort, but we were able to implement a basic SimNet and conduct benchmarks comparing it against two networks of the same depth \u2013 an analogous ConvNet and the \u201csinglelayer\u201d network of [5]. The results demonstrate that a SimNet can achieve comparable and/or better accuracy, while\nrequiring a significantly smaller network (in terms of the number of learned parameters) \u2013 around 1/9 the size of the ConvNet and 1/5 the size of the network in [5].\nThe SimNet architecture departs from classical ConvNets in three main respects. First, the similarity layer can incorporate entry-wise weights when the lp similarity is used. With linear similarity (which is essentially an inner-product between an input patch and a convolutional kernel) incorporating weights is meaningless, as they blend into the convolutional kernels. We saw that the unweighted lp and linear similarities give rise to a kernel-SVM building block with the Generalized Gaussian and Exponential kernels, respectively. The weighted lp similarity on the other hand, cannot be realized in the kernel-SVM framework (thm. 1), thereby offering a potentially stronger building block (whose effect is described in more detail in subsec. 3.1). The experiments we carried out highlight the differences between weighted and unweighted lp similarities:\n\u2022 Without weights, lp similarity and the linear similarity (convolutional operator) give rise to comparable performance. This suggests that without weights, SimNets do not exhibit superiority over ConvNets.\n\u2022 When weights are included, lp similarity displays a significant increase in performance, which scales up super-linearly with the number of parameters. That is to say, the increase in accuracy cannot be explained merely by the fact that the number of parameters in the similarity layer has been doubled (weights on top of templates).\nThese findings suggest that the strength of having the basic building block go beyond the hypothesis space of a kernelSVM, has significant appeal in practice.\nThe second respect in which SimNets depart from ConvNets has to do with the ability of the MEX layer to incorporate offsets. When the MEX layer serves as the final layer of the network, these offsets play the role of classification coefficients. However, when the MEX layer is inserted as a pooling layer, the offsets can be interpreted as providing locality-based biases to the templates generated in a previous similarity layer. This is something that classical ConvNets cannot express. Evaluating the practical significance of the MEX offsets requires experimentation with deep layered SimNets, which is an ongoing effort.\nThe third departure (or distinction) from ConvNets, is that the SimNet architecture is endowed with a natural initialization based on unlabeled data. In the case of ConvNets, existing unsupervised initialization schemes have little to no advantage over random initializations. For the SimNets, we reported experimental results that demonstrate the superiority of the unsupervised initialization scheme over random initializations, showing that the effect is more acute when\nthe networks are small. Besides its aid in training, the unsupervised scheme proposed also has the potential of determining the number of channels for a similarity layer based on variance analysis of patterns generated from previous layers. This implies that the structure of SimNets can potentially be determined automatically from (unlabeled) training data.\nFuture work is focused on further implementation, with the purpose of creating an open programming environment for the research community, that will enable wider scale experimentation of SimNets. Further theoretical studies are ongoing as well, with the intent to capture the sample complexity of SimNets, and to gain a better understanding of the typical network structure and size required under different conditions."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Nitzan Guberman and Or Sharir for their dedicated contribution to the experiments carried out in this work. The work is partly funded by Intel grant ICRI-CI no. 9-2012-6133 and by ISF Center grant 1790/12."}, {"heading": "A. Proof of theorem 1", "text": "To prove the theorem, we will need the following definition and lemma:\nDefinition 1. Let H be a Hilbert space and S \u2282 H be a collection of vectors. Given a constant > 0, S is said to be an -orthonormal set if the following two conditions hold:\n\u2200v \u2208 S : \u2016v\u2016H = 1 \u2200v,v\u2032 \u2208 S , v 6= v\u2032 : |\u3008v,v\u2032\u3009H| \u2264\nwhere \u3008\u00b7, \u00b7\u3009H stands for the inner-product in H and \u2016\u00b7\u2016H denotes the induced norm.\nLemma 1. Let H be a Hilbert space over F (F = R or F = C) and V \u2282 H be a set that contains an - orthonormal subset (see def. 1) of size n for any constants > 0 and n \u2208 N. Then, for every vector u \u2208 H it holds that inf {|\u3008u,v\u3009H| : v \u2208 V } = 0. Proof. Let u \u2208 H be an arbitrary vector, and denote c := inf {|\u3008u,v\u3009H| : v \u2208 V } and M := \u2016u\u2016H. We would like to show that c = 0. Let > 0 and n \u2208 N be arbitrary constants. Given our assumption on V , we may choose an -orthonormal subset v1, ...,vn \u2208 V . Denote by G the Gram of v1, ...,vn, i.e. G \u2208 Fn,n is the positive semidefinite (PSD) matrix with entries Gij = \u3008vj ,vi\u3009H. Let \u03b11, ..., \u03b1n \u2208 F be the scalars such that \u2211n i=1 \u03b1ivi is the projection of u onto span{vi}ni=1. It then holds that:\n\u2016 \u2211n i=1 \u03b1ivi\u2016 2 H \u2264 \u2016u\u2016 2 H = M 2\n\u2200j \u2208 {1, ..., n} : |\u3008 \u2211n i=1 \u03b1ivi,vj\u3009H| = |\u3008u,vj\u3009H| \u2265 c\nIf we denote \u03b1 := [\u03b11, ..., \u03b1n]>, and let 1 be the ndimensional vector holding 1 in all entries, the above yields the following matrix inequalities:\n\u03b1\u2217G\u03b1 \u2264M2 (11) |G\u03b1| \u2265 c \u00b7 1 =\u21d2 \u03b1\u2217G\u2217G\u03b1 = \u2016G\u03b1\u201622 \u2265 c2 \u00b7 n (12)\nwhere \u2217 stands for the conjugate transpose operator. The matrix G is PSD, thus having n non-negative eigenvalues. We denote these by \u03bb1 \u2265 ... \u2265 \u03bbn \u2265 0. G is Hermitian and thus G\u2217G = G2, from which we readily conclude that \u03bb21 \u2265 ... \u2265 \u03bb2n \u2265 0 are the eigenvalues of G\u2217G. We thus have the following inequalities:\n\u03b1\u2217G\u03b1 \u2265 \u03bbn \u00b7 \u2016\u03b1\u201622 (13) \u03b1\u2217G\u2217G\u03b1 \u2264 \u03bb21 \u00b7 \u2016\u03b1\u2016 2 2 (14)\nCombining the inequality 11 with 13, and the inequality 12 with 14, we get the following:\n\u03bbn \u00b7 \u2016\u03b1\u201622 \u2264 M 2 (15) \u03bb21 \u00b7 \u2016\u03b1\u2016 2 2 \u2265 c 2 \u00b7 n (16)\nWe now apply Gershgorin\u2019s circle theorem (see [7]) to G. The theorem states that for each eigenvalue \u03bbi, there exists some j \u2208 {1, ..., n} such that:\n|\u03bbi \u2212Gjj | \u2264 \u2211\nj\u2032\u2208{1,...n},j\u2032 6=j\n|Gjj\u2032 |\nPlugging in the definition of G and the -orthonormality of {v1, ...,vn}, we get:\n|\u03bbi \u2212 \u3008vj ,vj\u3009\ufe38 \ufe37\ufe37 \ufe38 =1\n| \u2264 \u2211\nj\u2032\u2208{1,...n},j\u2032 6=j |\u3008vj\u2032 ,vj\u3009|\ufe38 \ufe37\ufe37 \ufe38 \u2264 \u2264 (n\u2212 1) \u00b7\n=\u21d2 1\u2212 (n\u2212 1) \u00b7 \u2264 \u03bbi \u2264 1 + (n\u2212 1) \u00b7\nRecall that > 0 and n \u2208 N were chosen arbitrarily. If we now limit to be smaller than 1n\u22121 , we ensure that \u03bbi > 0 for all i = 1, ..., n. We can thus divide by \u03bbn and \u03bb21 the inequalities 15 and 16 respectively, and reach:\n\u2016\u03b1\u201622 \u2264 M2 \u03bbn \u2264 M 2 1\u2212(n\u22121)\u00b7\n\u2016\u03b1\u201622 \u2265 c2\u00b7n \u03bb21 \u2265 c 2\u00b7n (1+(n\u22121)\u00b7 )2\nCombining these two inequalities, we get:\nM2 \u2265 1\u2212 (n\u2212 1) \u00b7 (1 + (n\u2212 1) \u00b7 )2 \u00b7 c2 \u00b7 n\nNow this holds for arbitrarily small, so in particular: M2 \u2265 (\nlim \u21920+ 1\u2212 (n\u2212 1) \u00b7 (1 + (n\u2212 1) \u00b7 )2 ) \ufe38 \ufe37\ufe37 \ufe38\n=1\n\u00b7c2 \u00b7 n = c2 \u00b7 n (17)\nn is an arbitrary natural number, and M was defined as the norm of u \u2208 H so in particular it is non-negative and finite. In addition, c was defined as inf {|\u3008u,v\u3009H| : v \u2208 V } so it is too non-negative. Thus, the only way that eqn. 17 can hold is if c = 0, which is what we set out to prove.\nEquipped with def. 1 and lemma 1, we head on to prove our main theorem:\nProof of theorem 1. Assume by contradiction that there are mappings Z and U and kernel K as described in the theorem. Let \u03c8 be a feature mapping corresponding to K, i.e. a mapping from Rd \u00d7 Rd+ to some real Hilbert space H such that K([z,u], [z\u2032,u\u2032]) = \u3008\u03c8([z,u]), \u03c8([z\u2032,u\u2032])\u3009H for all z, z\u2032 \u2208 Rd and u,u\u2032 \u2208 Rd+. Fix some x \u2208 Rd, and observe that:\n\u2016\u03c8([Z(x), U(x)])\u2016H \u2264 1 (18)\nThis follows from:\n\u2016\u03c8([Z(x), U(x)])\u20162H = K([Z(x), U(x)], [Z(x), U(x)]) =\nexp \u2212 c\ufe38\ufe37\ufe37\ufe38 >0 \u2211d i=1 U(x)i\ufe38 \ufe37\ufe37 \ufe38\n\u22650 |xi \u2212 Z(x)i|p\ufe38 \ufe37\ufe37 \ufe38 \u22650\n \u2264 1\nSince Z(x) is also an element in Rd, we can replace x by Z(x) in eqn. 18 to obtain:\n\u2016\u03c8([Z(Z(x)), U(Z(x))])\u2016H \u2264 1 (19)\nNext we show that:\n\u3008\u03c8([Z(Z(x)), U(Z(x))]), \u03c8([Z(x), U(x)])\u3009H = 1 (20)\nIndeed:\n\u3008\u03c8([Z(Z(x)), U(Z(x))]), \u03c8([Z(x), U(x)])\u3009H = K([Z(Z(x)), U(Z(x))], [Z(x), U(x)]) =\nexp \u2212c\u2211di=1 U(x)i |Z(x)i \u2212 Z(x)i|p\ufe38 \ufe37\ufe37 \ufe38 =0  = 1 The Cauchy-Schwartz inequality (see [28]) tells us that for any two vectors w,w\u2032 in the real Hilbert space H, it holds that \u3008w,w\u2032\u3009H \u2264 \u2016w\u2016H \u00b7 \u2016w\u2032\u2016H. Moreover, if equality holds then w and w\u2032 are linearly dependent, or more specifically, at least one of the vectors can be obtained by multiplying the other by a non-negative scalar. Applying this to the vectors \u03c8([Z(x), U(x)]) and \u03c8([Z(Z(x)), U(Z(x))]), we conclude from equations 18, 19 and 20 that:\n\u03c8([Z(Z(x)), U(Z(x))]) = \u03c8([Z(x), U(x)]) (21) \u2016\u03c8([Z(x), U(x)])\u2016H = 1 (22)\nUsing eqn. 21 and our assumption about the kernel K (eqn. 5), we conclude that for every z \u2208 Rd and u \u2208 Rd+ (recall that x \u2208 Rd was fixed arbitrarily):\nexp { \u2212c \u2211d i=1 ui|xi \u2212 zi|p } =\nK ([Z(x), U(x)], [z,u]) =\n\u3008\u03c8([Z(x), U(x)]), \u03c8([z,u])\u3009H = \u3008\u03c8([Z(Z(x)), U(Z(x))]), \u03c8([z,u])\u3009H =\nexp { \u2212c \u2211d i=1 ui|Z(x)i \u2212 zi|p } Taking the logarithm of the two outer expressions, we get:\n\u2212c d\u2211 i=1 ui|xi \u2212 zi|p = \u2212c d\u2211 i=1 ui|Z(x)i \u2212 zi|p\n=\u21d2 d\u2211 i=1 ui (|xi \u2212 zi|p \u2212 |Z(x)i \u2212 zi|p) = 0\nFixing some coordinate i0 \u2208 {1, ..., d}, we can choose u to hold 1 at i0 and 0 in the other coordinates. The latter equality would then reduce to |xi0\u2212zi0 |p = |Z(x)i0\u2212zi0 |p, which must hold for any zi0 \u2208 R. The only way for this to be met is if Z(x)i0 = xi0 . Since both the vector x \u2208 Rd and the coordinate i0 \u2208 {1, ..., d} are arbitrary, the mapping\nZ : Rd \u2192 Rd is no other than the identity mapping. The assumption in eqn. 5 thus becomes:\n\u2200 z,x \u2208 Rd,u \u2208 Rd+ : (23) K ([x, U(x)], [z,u]) = exp { \u2212c \u2211d i=1 ui|xi \u2212 zi|p } We again fix x \u2208 Rd, and turn to show that U(x) 6= 0 (0 here stands for the d-dimensional zero vector). Assume by contradiction that this is not the case, i.e. that U(x) = 0. Then, according to eqn. 23, for all x\u2032 \u2208 Rd we have:\n\u3008\u03c8([x\u2032, U(x\u2032)]), \u03c8([x, U(x)])\u3009H = K([x\u2032, U(x\u2032)], [x, U(x)]) =\nexp \u2212c\u2211di=1 U(x)i\ufe38 \ufe37\ufe37 \ufe38 =0 |xi \u2212 x\u2032i|p  = 1\nUsing the fact that \u03c8([x, U(x)]) and \u03c8([x\u2032, U(x\u2032)]) are unit vectors (eqn. 22), and again the Cauchy-Schwartz inequality, we conclude that \u03c8([x, U(x)]) = \u03c8([x\u2032, U(x\u2032)]). This implies that for all z \u2208 Rd and u \u2208 Rd+:\nexp { \u2212c \u2211d i=1 ui|xi \u2212 zi|p } =\nK ([x, U(x)], [z,u]) = \u3008\u03c8([x, U(x)]), \u03c8([z,u])\u3009H = \u3008\u03c8([x\u2032, U(x\u2032)]), \u03c8([z,u])\u3009H = K ([x\u2032, U(x\u2032)], [z,u]) =\nexp { \u2212c \u2211d i=1 ui|x\u2032i \u2212 zi|p } As before, we can isolate the coordinates in {1, ..., d} one at a time, and conclude that x = x\u2032. Since x\u2032 is arbitrary, this is of course a contradiction, showing that our assumption U(x) = 0 was incorrect. There is thus at least one coordinate of U(x) which is positive. Accordingly, the expression \u2212c \u2211d i=1 U(x)i|x\u2032i \u2212 xi|p will tend to \u2212\u221e when all coordinates of x\u2032 tend to\u221e (we denote this condition by x\u2032 \u2192\u221e). We may thus write:\n\u3008\u03c8([x\u2032, U(x\u2032)]), \u03c8([x, U(x)])\u3009H = exp { \u2212c \u2211d i=1 U(x)i|x\u2032i \u2212 xi|p } \u2212\u2192\nx\u2032\u2192\u221e 0\nRecall that x \u2208 Rd is an arbitrary vector. The above convergence thus implies that for any > 0 and n \u2208 N, we can incrementally create a set of n vectors - x1, ...,xn \u2208 Rd, such that:\n\u22001 \u2264 j < i \u2264 n : |\u3008\u03c8([xi, U(xi)]), \u03c8([xj , U(xj)])\u3009H| \u2264\nIndeed, given a set of vectors x1, ...,xj , the next vector xj+1 is obtained by approaching\u221e until all inner-products are small enough.\nTo summarize, we have the following findings:\n\u2022 For any > 0 and n \u2208 N there exist x1, ...,xn \u2208 Rd such that for all 1 \u2264 j < i \u2264 n, |\u3008\u03c8([xi, U(xi)]), \u03c8([xj , U(xj)])\u3009H| \u2264 .\n\u2022 \u2016\u03c8([x, U(x)])\u2016H = 1 for all x \u2208 Rd (eqn. 22).\n\u2022 \u3008\u03c8([x, U(x)]), \u03c8([0,0])\u3009H = K ([x, U(x)], [0,0]) = exp { \u2212c \u2211d i=1 0 \u00b7 |xi \u2212 0|p } = 1 (simply plug-in\nz = 0 and u = 0 in eqn. 23).\nMore succinctly, the set V := {\u03c8([x, U(x)]) : x \u2208 Rd} contains an -orthonormal subset (def. 1) of size n for any constants > 0 and n \u2208 N, and in addition the vector \u03c8([0,0]) has inner-product 1 with every element of V . According to lemma 1 this is impossible! We have thus reached a contradiction, showing the incorrectness of our initial assumption that mappings Z and U and kernel K as stated in the theorem exist."}, {"heading": "B. Patch-based kernel-SVM", "text": "In this appendix we show how the classification described in eqn. 10, which corresponds to the basic \u201clocalitysharing-pooling\u201d SimNet illustrated in fig. 1(f), can be formulated as a multiclass kernel-SVM ([6]) with reduced support-vectors ([27]). In this formulation, the classified instances will not be represented by holistic vectors, but rather by blocks of multiple vectors. Moreover, the supportvectors will be subject to constraints which can be interpreted as enforcing \u201clocality\u201d and \u201csharing\u201d. In the context of the SimNet, the vectors which constitute an instance are simply the input patches, the locality constraint on the support-vectors corresponds to the fact that the input is processed by local patches in a spatially aware manner, and the sharing constraint corresponds to the fact that the same n templates in the similarity layer apply to all input patches. As will be shown below, the SimNet\u2019s pooling operation will also come into play in the locality and sharing constraints.\nLet d \u2208 N be some dimension, and letK : Rd\u00d7Rd \u2192 R be a kernel on Rd. For some D \u2208 N, consider the instance space X := {X = (x1, ...,xD) : xi \u2208 Rd , i = 1, ..., D}. For compatibility, we refer to the vectors that constitute an instance as \u201cpatches\u201d. Assume we have a partitioning of patches into \u201cpools\u201d, namely that there is a constant P \u2208 N and a function q : {1, ..., D} \u2192 {1, ..., P} that assigns to each patch index i \u2208 {1, ..., D} a pool q(i) \u2208 {1, ..., P}. Consider the following rule for classifying an instance X into one of k \u2208 N possible classes:\ny\u0302(X) = argmax r=1,...,k \u2211 1\u2264p\u2264P,1\u2264l\u2264n \u03b1rlp \u2211 1\u2264i\u2264D:q(i)=p K(xi, zl) (24) where n \u2208 N is some predetermined constant, {z1, ..., zn} \u2282 Rd are learned templates, and\n{\u03b1rlp}1\u2264r\u2264k,1\u2264l\u2264n,1\u2264p\u2264P \u2282 R are learned coefficients. This is essentially equivalent to the SimNet classification described in eqn. 10. In fact, the only true difference is that in the latter, the learned coefficients were constrained to be positive, but this does not limit generality, as we can always add a common offset to all coefficients after training is complete.\nWe now add the special character \u2217 (\u201cnull\u201d character) to Rd, extending the latter to V := Rd\u222a{\u2217}. Accordingly, we extend K to the function KV : V \u00d7 V \u2192 R defined by:\nKV (v,v \u2032) = { K(v,v\u2032) if v,v\u2032 6= \u2217 0 otherwise (25)\nLemma 2. KV is a kernel on V .\nProof. Let\u03c8 be a feature mapping corresponding to the kernelK, i.e. \u03c8 is a mapping from Rd to some Hilbert spaceH such that \u2200x,x\u2032 \u2208 Rd : K(x,x\u2032) = \u3008\u03c8(x), \u03c8(x\u2032)\u3009. Extend \u03c8 to the mapping \u03c8V : V \u2192 H as follows:\n\u03c8V (v) = { \u03c8(v) if v 6= \u2217 0H if v = \u2217\nwhere 0H stands for the zero element of H. Obviously, the function from V \u00d7 V to R defined by (v,v\u2032) 7\u2192 \u3008\u03c8V (v), \u03c8V (v\u2032)\u3009 is a kernel on V . Direct computation shows that this function is no other than KV :\n\u3008\u03c8V (v), \u03c8V (v\u2032)\u3009\n=  \u3008\u03c8(v), \u03c8(v\u2032)\u3009 if v 6= \u2217,v\u2032 6= \u2217 \u3008\u03c8(v), 0H\u3009 if v 6= \u2217,v\u2032 = \u2217 \u30080H, \u03c8(v\u2032)\u3009 if v = \u2217,v\u2032 6= \u2217 \u30080H, 0H\u3009 if v = \u2217,v\u2032 = \u2217\n= { \u3008\u03c8(v), \u03c8(v\u2032)\u3009 if v 6= \u2217,v\u2032 6= \u2217 0 otherwise\n= { K(v,v\u2032) if v 6= \u2217,v\u2032 6= \u2217 0 otherwise\n= KV (v,v \u2032)\nNext, we use the kernel KV to define a function K : V D \u00d7 V D \u2192 R, where V D is the D\u2019th Cartesian power of V , i.e. V D := {(v1, ...,vD) : vi \u2208 V , i = 1, ..., D}. K is defined by:\nK((v1, ...,vD), (v \u2032 1, ...,v \u2032 D)) = \u2211 1\u2264i\u2264DKV (vi,v \u2032 i)\n= \u2211\n1\u2264i\u2264D:vi,v\u2032i 6=\u2217 K(vi,v\n\u2032 i) (26)\nLemma 3. K is a kernel on V D.\nProof. The proof is general in the sense that it does not rely on any specific property of the kernel KV on which K is based. In accordance with the above\nnotations, let \u03c8V be a feature mapping corresponding to KV , i.e. a mapping from V to some Hilbert space H such that \u2200v,v\u2032 \u2208 V : KV (v,v\u2032) = \u3008\u03c8V (v), \u03c8V (v\u2032)\u3009. We use \u03c8V to define a mapping \u03a8 from V D to the Hilbert space HD, which is the D\u2019th Cartesian power of H (the elements of HD are D-length sequences of H-elements, and the inner product between (h1, ...,hD) and (h\u20321, ...,h \u2032 D) is defined as \u2211D i=1 \u3008hi,h\u2032i\u3009). The mapping \u03a8 is defined by \u03a8((v1, ...,vD)) = (\u03c8V (v1), ..., \u03c8V (vD)). The function from V D \u00d7 V D to R defined by ((v1, ...,vD), (v\u20321, ...,v\u2032D)) 7\u2192 \u3008\u03a8((v1, ...,vD)),\u03a8((v\u20321, ...,v\u2032D))\u3009 is obviously a kernel on V D. Direct computation shows that this function is no other than K:\n\u3008\u03a8((v1, ...,vD)),\u03a8((v\u20321, ...,v\u2032D))\u3009 = \u3008(\u03c8V (v1), ..., \u03c8V (vD)), (\u03c8V (v\u20321), ..., \u03c8V (v\u2032D))\u3009 = \u2211D i=1 \u3008\u03c8V (vi), \u03c8V (v\u2032i)\u3009 = \u2211D i=1KV (vi,v \u2032 i)\n= K((v1, ...,vD), (v \u2032 1, ...,v \u2032 D))\nUsing K, we may express the classification rule given in eqn. 24 in kernel-form. Simply notice that:\u2211\n1\u2264i\u2264D:q(i)=pK(xi, zl) =\nK((x1, ...,xD), :=Zlp\ufe37 \ufe38\ufe38 \ufe37 (\u2217, ..., \u2217\ufe38 \ufe37\ufe37 \ufe38 q(i)6=p , zl, ..., zl\ufe38 \ufe37\ufe37 \ufe38 q(i)=p , \u2217, ..., \u2217\ufe38 \ufe37\ufe37 \ufe38 q(i)6=p ))\nThus, eqn. 24 can be written as follows:\ny\u0302(X) = argmax r=1,...,k \u2211 1\u2264p\u2264P,1\u2264l\u2264n \u03b1rlp \u00b7K(X,Zlp)\nwhere Zlp = ((Zlp)1, ..., (Zlp)D), for l = 1, ..., n and p = 1, ..., P , are elements in V D meeting the following constraints:\n\u2200i \u2208 {1, ..., D} s.t. q(i) 6= p : (Zlp)i = \u2217 (27) \u2200i \u2208 {1, ..., D} s.t. q(i) = p : (Zlp)i = zl (28)\nfor some global vectors z1, ..., zn \u2208 Rd\nWe interpret the constraint in eqn. 27 as enforcing locality \u2013 entries of Zlp that lie outside the pool p (\u201cout-pool\u201d entries) must hold the null character. The constraint in eqn. 28 is interpreted as enforcing sharing \u2013 entries of Zlp that lie inside the pool p (\u201cin-pool\u201d entries) are identical to each other, and also to the in-pool entries of Zl\u2032p\u2032 in the case where the \u201ctemplate indexes\u201d l and l\u2032 are the same.\nTo conclude, the classifier described in eqn. 24 can also be expressed as:\ny\u0302(X) = argmax r=1,...,k \u2211 1\u2264p\u2264P,1\u2264l\u2264n \u03b1rlp \u00b7K(X,Zlp) (29)\nwhere:\n\u2022 The set V D is simply the instance spaceX with the option of placing null characters in the different entries.\n\u2022 K is a kernel on V D.\n\u2022 Zlp, with l = 1, ..., n and p = 1, ...P , are learned elements of V D meeting the locality and sharing constraints in eqn. 27 and eqn. 28 respectively.\n\u2022 \u03b1rlp, with r = 1, ..., k, l = 1, ..., n and p = 1, ...P , are learned real coefficients.\nThat is to say, the classifier is a reduced kernel-SVM on the space V D with the kernel K, where the train and test instances are known to lie in the subset X \u2282 V D (i.e. they do not contain any null characters), and there are n \u00b7P supportvectors indexed by (l, p) \u2208 {1, ..., n} \u00d7 {1, ..., P}, that are subject to the locality and sharing constraints in eqn. 27 and eqn. 28 respectively. This construction, which we refer to as \u201cpatch-based kernel-SVM\u201d, underlines the strong connection between SimNets and kernel machines. In particular, it demonstrates the effect of locality, sharing and pooling in SimNets on the kernel-SVM equivalent. Namely, while the basic SimNet (illustrated in fig. 1(e)) was associated with standard reduced kernel-SVM, adding locality, sharing and pooling to obtain the SimNet considered here (illustrated in fig. 1(f)), translates the associated kernel machine to patch-based kernel-SVM, in which the concepts of locality, sharing and pooling come into play as constraints on the support-vectors."}], "references": [{"title": "Image thresholding based on the em algorithm and the generalized gaussian distribution", "author": ["Yakoub Bazi", "Lorenzo Bruzzone", "Farid Melgani"], "venue": "Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence", "author": ["Joan Bruna", "St\u00e9phane Mallat"], "venue": "IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Kernel methods for deep learning", "author": ["Youngmin Cho", "Lawrence K Saul"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Andrew Y Ng", "Honglak Lee"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Koby Crammer", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Learned-norm pooling for deep feedforward and recurrent neural networks", "author": ["Caglar Gulcehre", "Kyunghyun Cho", "Razvan Pascanu", "Yoshua Bengio"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Yangqing Jia"], "venue": "http:// caffe.berkeleyvision.org/,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Sum-product networks: A new deep architecture", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Hierarchical models of object recognition in cortex", "author": ["Maximilian Riesenhuber", "Tomaso Poggio"], "venue": "Nature neuroscience,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "MIT press,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A direct method for building sparse kernel learning algorithms", "author": ["Mingrui Wu", "Bernhard Sch\u00f6lkopf", "G\u00f6khan Bak\u0131r"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "An introduction to Hilbert space", "author": ["Nicholas Young"], "venue": "Cambridge university press,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}], "referenceMentions": [{"referenceID": 11, "context": "[14, 29, 21, 24, 23]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "[14, 29, 21, 24, 23]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "[14, 29, 21, 24, 23]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "[14, 29, 21, 24, 23]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "The learning capacity is controlled using over-specified networks (networks that are larger than necessary in order to model the problem), followed by various forms of regularization techniques such as Dropout ([11]).", "startOffset": 211, "endOffset": 215}, {"referenceID": 12, "context": "The first is that the ConvNet architecture has not changed much since its early introduction in the 1980s ([15]) \u2013 there were some attempts to create other types of deep-layered architectures (cf.", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "[19, 3, 18]), but these are not commonly used compared to ConvNets.", "startOffset": 0, "endOffset": 11}, {"referenceID": 2, "context": "[19, 3, 18]), but these are not commonly used compared to ConvNets.", "startOffset": 0, "endOffset": 11}, {"referenceID": 14, "context": "[19, 3, 18]), but these are not commonly used compared to ConvNets.", "startOffset": 0, "endOffset": 11}, {"referenceID": 7, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 1, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 21, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 11, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 17, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 3, "context": "These machine learning methods were well suited for \u201cflat\u201d architectures, and while attempts to apply them to deep layered architectures have been made ([4]), they did not keep up with the performance levels of the layered ConvNet architecture.", "startOffset": 153, "endOffset": 156}, {"referenceID": 13, "context": "The second, as special cases, plays the role of ReLU activation ([17]) and max pooling in ConvNets, but in addition, has capabilities that make SimNets much more than ConvNets.", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "In a set of limited experiments on CIFAR-10 dataset ([13]) using a small number of layers, we achieved better or comparable performance to state of the art ConvNets with the same number of layers, and the specialized network studied in [5], using 1/9 and 1/5, respectively, of the number of learned parameters.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "In a set of limited experiments on CIFAR-10 dataset ([13]) using a small number of layers, we achieved better or comparable performance to state of the art ConvNets with the same number of layers, and the specialized network studied in [5], using 1/9 and 1/5, respectively, of the number of learned parameters.", "startOffset": 236, "endOffset": 239}, {"referenceID": 13, "context": "The SimNet architecture consists of two operators \u2013 a \u201csimilarity\u201d operator that generalizes the inner-product operator found in ConvNets, and a soft max-average-min operator called MEX that replaces the ConvNet ReLU activation ([17]) and max/average pooling layers, and allows additional capabilities as will be described below.", "startOffset": 229, "endOffset": 233}, {"referenceID": 6, "context": "There were other attempts to generalize maxout, notably the recently proposed Lp unit [9], which is defined by ( 1 n \u2211n l=1|zl x + bl| )1/p .", "startOffset": 86, "endOffset": 89}, {"referenceID": 16, "context": "As shown in [20], Klin and Klp are kernels on R (note that for p > 2, the expression above for Klp is not a kernel).", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "This setting was studied in [27] in the context of binary (2class) classification.", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "The extension to multiclass ([6]) is straightforward.", "startOffset": 29, "endOffset": 32}, {"referenceID": 19, "context": "This approach follows the line of the \u201cglobal average pooling\u201d structure recently suggested in the context of ConvNets, which has been shown to outperform the traditional \u201cdense classification\u201d paradigm ([16, 23]).", "startOffset": 204, "endOffset": 212}, {"referenceID": 7, "context": "[10, 2, 25]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 1, "context": "[10, 2, 25]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 21, "context": "[10, 2, 25]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 11, "context": "Over time, however, these were taken over by carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 132, "endOffset": 144}, {"referenceID": 17, "context": "Over time, however, these were taken over by carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 132, "endOffset": 144}, {"referenceID": 8, "context": "The enhanced susceptibility to overfitting has led to various regularization techniques and heuristics (Dropout ([11]) being the most", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "This observation suggests estimating the parameters (shapes \u03b2l, scales \u03b1l,i and means \u03bcl,i) of the Generalized Gaussian mixture using unlabeled input patches (via standard statistical estimation methods, such as that presented in [1]), and initializing the similarity layer accordingly.", "startOffset": 230, "endOffset": 233}, {"referenceID": 10, "context": "For the experiments reported here, we used the CIFAR-10 dataset ([13]), which consists of 60, 000 color images (50, 000 for training and 10, 000 for testing) of size 32 \u00d7 32 partitioned into 10 classes, with 6, 000 images per class (5, 000 for training, 1, 000 for testing).", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "(a) Patch labeling SimNet (b) Comparable ConvNet (c) Comparable \u201csingle-layer\u201d network studied in [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 18, "context": "cludes momentum and acceleration ([22]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 4, "context": "([5]), which has the same depth as the evaluated SimNet, and whose performance on CIFAR-10 is one of the best reported for networks of such depth (absolute state of the art in 2011).", "startOffset": 1, "endOffset": 4}, {"referenceID": 4, "context": "In [5], a number of unsupervised learning methods were devised for \u201ccoding\u201d the input image.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "The ConvNet was implemented using Caffe toolbox ([12]), with random initialization and SGD training.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "For the SimNet we also added patch \u201cwhitening\u201d, in accordance with the suggestion of [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "The positive effect of whitening for the SimNet (which has l1/l2 similarities) was verified experimentally, whereas for the ConvNet, we observed that whitening does not have a positive effect (complying with the observations of [5]).", "startOffset": 228, "endOffset": 231}, {"referenceID": 4, "context": "([5]) is of interest on several accounts.", "startOffset": 1, "endOffset": 4}, {"referenceID": 4, "context": "In [5], the network \u201ctemplates\u201d (i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "More specifically, we used SGD to jointly modify the network templates and SVM coefficients produced by [5], in an attempt to reach higher accuracy levels than those reported by the authors.", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "In [5] results are reported for up to 1, 600 templates.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Implementing deep SimNets with state of the art optimization techniques (including GPU acceleration) is an ongoing effort, but we were able to implement a basic SimNet and conduct benchmarks comparing it against two networks of the same depth \u2013 an analogous ConvNet and the \u201csinglelayer\u201d network of [5].", "startOffset": 299, "endOffset": 302}, {"referenceID": 4, "context": "The results demonstrate that a SimNet can achieve comparable and/or better accuracy, while requiring a significantly smaller network (in terms of the number of learned parameters) \u2013 around 1/9 the size of the ConvNet and 1/5 the size of the network in [5].", "startOffset": 252, "endOffset": 255}], "year": 2014, "abstractText": "We present a deep layered architecture that generalizes classical convolutional neural networks (ConvNets). The architecture, called SimNets, is driven by two operators, one being a similarity function whose family contains the convolution operator used in ConvNets, and the other is a new soft max-min-mean operator called MEX that realizes classical operators like ReLU and max pooling, but has additional capabilities that make SimNets a powerful generalization of ConvNets. Three interesting properties emerge from the architecture: (i) the basic input to hidden layer to output machinery contains as special cases kernel machines with the Exponential and Generalized Gaussian kernels, the output units being \u201dneurons in feature space\u201d (ii) in its general form, the basic machinery has a higher abstraction level than kernel machines, and (iii) initializing networks using unsupervised learning is natural. Experiments demonstrate the capability of achieving state of the art accuracy with networks that are an order of magnitude smaller than comparable ConvNets.", "creator": "LaTeX with hyperref package"}}}