{"id": "1410.8750", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2014", "title": "Learning Mixtures of Ranking Models", "abstract": "This work concerned teacher equations larger three ranking reference followed a heterogeneous population. The certain problem nobody study is creative the parameters of a Mallows Mixture Model. Despite being unlike studied, current neurophysiological more because why come not only context requirements like can let 'd since me as f\u00e9. We present the latter polynomial however theorem which provably loved three processes entire a flavors include leaving Mallows comparison. A the utilizing especially means algorithm given its novel use of spacetime interpolation analyzing to imagine full top - zero prefix in made the qualify. Before this focus, even the clearly between identifiability in entered case of a desired also along Mallows models now stemming.", "histories": [["v1", "Fri, 31 Oct 2014 14:31:54 GMT  (50kb)", "http://arxiv.org/abs/1410.8750v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pranjal awasthi", "avrim blum", "or sheffet", "aravindan vijayaraghavan"], "accepted": true, "id": "1410.8750"}, "pdf": {"name": "1410.8750.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Aravindan Vijayaraghavan"], "emails": ["pawashti@cs.princeton.edu", "avrim@cs.cmu.edu", "osheffet@seas.harvard.edu", "vijayara@cims.nyu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n87 50\nv1 [\ncs .L\nG ]\n3 1"}, {"heading": "1 Introduction", "text": "Probabilistic modeling of ranking data is an extensively studied problem with a rich body of past work [1, 2, 3, 4, 5, 6, 7, 8, 9]. Ranking using such models has applications in a variety of areas ranging from understanding user preferences in electoral systems and social choice theory, to more modern learning tasks in online web search, crowd-sourcing and recommendation systems. Traditionally, models for generating ranking data consider a homogeneous group of users with a central ranking (permutation) \u03c0\u2217 over a set of n elements or alternatives. (For instance, \u03c0\u2217 might correspond to a \u201cground-truth ranking\u201d over a set of movies.) Each individual user generates her own ranking as a noisy version of this one central ranking and independently from other users. The most popular ranking model of choice is the Mallows model [1], where in addition to \u03c0\u2217 there is also a scaling parameter \u03c6 \u2208 (0, 1). Each user picks her ranking \u03c0 w.p. proportional to \u03c6dkt(\u03c0,\u03c0\u2217) where dkt(\u00b7) denotes the Kendall-Tau distance between permutations (see Section 2).1 We denote such a model as Mn(\u03c6, \u03c0\u2217). The Mallows model and its generalizations have received much attention from the statistics, political science and machine learning communities, relating this probabilistic model to the long-studied work about voting and social choice [10, 11]. From a machine learning perspective, the problem is to find the parameters of the model \u2014 the central permutation \u03c0\u2217 and the scaling parameter \u03c6, using independent samples from the distribution. There is a large body of work [4, 6, 5, 7, 12] providing efficient algorithms for learning the parameters of a Mallows model.\n\u2217This work was supported in part by NSF grants CCF-1101215, CCF-1116892, the Simons Institute, and a Simons Foundation Postdoctoral fellowhsip. Part of this work was performed while the 3rd author was at the Simons Institute for the Theory of Computing at the University of California, Berkeley and the 4th author was at CMU.\n1In fact, it was shown [1] that this model is the result of the following simple (inefficient) algorithm: rank every pair of elements randomly and independently s.t. with probability 1\n1+\u03c6 they agree with \u03c0\u2217 and with\nprobability \u03c6 1+\u03c6 they don\u2019t; if all ( n 2 ) pairs agree on a single ranking \u2013 output this ranking, otherwise resample.\nIn many scenarios, however, the population is heterogeneous with multiple groups of people, each with their own central ranking [2]. For instance, when ranking movies, the population may be divided into two groups corresponding to men and women; with men ranking movies with one underlying central permutation, and women ranking movies with another underlying central permutation. This naturally motivates the problem of learning a mixture of multiple Mallows models for rankings, a problem that has received significant attention [8, 13, 3, 4]. Heuristics like the EM algorithm have been applied to learn the model parameters of a mixture of Mallows models [8]. The problem has also been studied under distributional assumptions over the parameters, e.g. weights derived from a Dirichlet distribution [13]. However, unlike the case of a single Mallows model, algorithms with provable guarantees have remained elusive for this problem.\nIn this work we give the first polynomial time algorithm that provably learns a mixture of two Mallows models. The input to our algorithm consists of i.i.d random rankings (samples), with each ranking drawn with probability w1 from a Mallows model Mn(\u03c61, \u03c01), and with probability w2(= 1\u2212 w1) from a different model Mn(\u03c62, \u03c02). Informal Theorem. Given sufficiently many i.i.d samples drawn from a mixture of two Mallows models, we can learn the central permutations \u03c01, \u03c02 exactly and parameters \u03c61, \u03c62, w1, w2 up to \u01eb-accuracy in time poly(n, (min{w1, w2})\u22121, 1\u03c61(1\u2212\u03c61) , 1 \u03c62(1\u2212\u03c62) , \u01eb \u22121).\nIt is worth mentioning that, to the best of our knowledge, prior to this work even the question of identifiability was unresolved for a mixture of two Mallows models; given infinitely many i.i.d. samples generated from a mixture of two distinct Mallow models with parameters {w1, \u03c61, \u03c01, w2, \u03c62, \u03c02} (with \u03c01 6= \u03c02 or \u03c61 6= \u03c62), could there be a different set of parameters {w\u20321, \u03c6\u20321, \u03c0\u20321, w\u20322, \u03c6\u20322, \u03c0\u20322} which explains the data just as well. Our result shows that this is not the case and the mixture is uniquely identifiable given polynomially many samples.\nIntuition and a Na\u0131\u0308ve First Attempt. It is evident that having access to sufficiently many random samples allows one to learn a single Mallows model. Let the elements in the permutations be denoted as {e1, e2, . . . , en}. In a single Mallows model, the probability of element ei going to position j (for j \u2208 [n]) drops off exponentially as one goes farther from the true position of ei [12]. So by assigning each ei the most frequent position in our sample, we can find the central ranking \u03c0\u2217.\nThe above mentioned intuition suggests the following clustering based approach to learn a mixture of two Mallows models \u2014 look at the distribution of the positions where element ei appears. If the distribution has 2 clearly separated \u201cpeaks\u201d then they will correspond to the positions of ei in the central permutations. Now, dividing the samples according to ei being ranked in a high or a low position is likely to give us two pure (or almost pure) subsamples, each one coming from a single Mallows model. We can then learn the individual models separately. More generally, this strategy works when the two underlying permutations \u03c01 and \u03c02 are far apart which can be formulated as a separation condition.2 Indeed, the above-mentioned intuition works only under strong separator conditions: otherwise, the observation regarding the distribution of positions of element ei is no longer true 3. For example, if \u03c01 ranks ei in position k and \u03c02 ranks ei in position k + 2, it is likely that the most frequent position of ei is k+1, which differs from ei\u2019s position in either permutations!\nHandling arbitrary permutations. Learning mixture models under no separation requirements is a challenging task. To the best of our knowledge, the only polynomial time algorithm known is for the case of a mixture of a constant number of Gaussians [17, 18]. Other works, like the recent developments that use tensor based methods for learning mixture models without distance-based separation condition [19, 20, 21] still require non-degeneracy conditions and/or work for specific sub cases (e.g. spherical Gaussians).\nThese sophisticated tensor methods form a key component in our algorithm for learning a mixture of two Mallows models. This is non-trivial as learning over rankings poses challenges which are not present in other widely studied problems such as mixture of Gaussians. For the case of Gaussians, spectral techniques have been extremely successful [22, 16, 19, 21]. Such techniques rely on estimating the covariances and higher order moments in terms of the model parameters to detect structure and dependencies. On the other hand, in the mixture of Mallows models problem there is\n2Identifying a permutation \u03c0 over n elements with a n-dimensional vector (\u03c0(i))i, this separation condition can be roughly stated as \u2016\u03c01 \u2212 \u03c02\u2016\u221e = \u2126\u0303 ( (min{w1, w2})\u22121 \u00b7 (min{log(1/\u03c61), log(1/\u03c62)}))\u22121 ) .\n3Much like how other mixture models are solvable under separation conditions, see [14, 15, 16].\nno \u201cnatural\u201d notion of a second/third moment. A key contribution of our work is defining analogous notions of moments which can be represented succinctly in terms of the model parameters. As we later show, this allows us to use tensor based techniques to get a good starting solution.\nOverview of Techniques. One key difficulty in arguing about the Mallows model is the lack of closed form expressions for basic propositions like \u201cthe probability that the i-th element of \u03c0\u2217 is ranked in position j.\u201d Our first observation is that the distribution of a given element appearing at the top, i.e. the first position, behaves nicely. Given an element e whose rank in the central ranking \u03c0\u2217 is i, the probability that a ranking sampled from a Mallows model ranks e as the first element is \u221d \u03c6i\u22121. A length n vector consisting of these probabilities is what we define as the first moment vector of the Mallows model. Clearly by sorting the coordinate of the first moment vector, one can recover the underlying central permutation and estimate \u03c6. Going a step further, consider any two elements which are in positions i, j respectively in \u03c0\u2217. We show that the probability that a ranking sampled from a Mallows model ranks {i, j} in (any of the 2! possible ordering of) the first two positions is \u221d f(\u03c6)\u03c6i+j\u22122. We call the n \u00d7 n matrix of these probabilities as the second moment matrix of the model (analogous to the covariance matrix). Similarly, we define the 3rd moment tensor as the probability that any 3 elements appear in positions {1, 2, 3}. We show in the next section that in the case of a mixture of two Mallows models, the 3rd moment tensor defined this way has a rank-2 decomposition, with each rank-1 term corresponds to the first moment vector of each of two Mallows models. This motivates us to use tensor-based techniques to estimate the first moment vectors of the two Mallows models, thus learning the models\u2019 parameters.\nThe above mentioned strategy would work if one had access to infinitely many samples from the mixture model. But notice that the probabilities in the first-moment vectors decay exponentially, so by using polynomially many samples we can only recover a prefix of length \u223c log1/\u03c6 n from both rankings. This forms the first part of our algorithm which outputs good estimates of the mixture weights, scaling parameters \u03c61, \u03c62 and prefixes of a certain size from both the rankings. Armed with w1, w2 and these two prefixes we next proceed to recover the full permutations \u03c01 and \u03c02. In order to do this, we take two new fresh batches of samples. On the first batch, we estimate the probability that element e appears in position j for all e and j. On the second batch, which is noticeably larger than the first, we estimate the probability that e appears in position j conditioned on a carefully chosen element e\u2217 appearing as the first element. We show that this conditioning is almost equivalent to sampling from the same mixture model but with rescaled weights w\u20321 and w \u2032 2. The two estimations allow us to set a system of two linear equations in two variables: f (1) (e \u2192 j) \u2013 the probability of element e appearing in position j in \u03c01, and f (2) (e \u2192 j) \u2014 the same probability for \u03c02. Solving this linear system we find the position of e in each permutation.\nThe above description contains most of the core ideas involved in the algorithm. We need two additional components. First, notice that the 3rd moment tensor is not well defined for triplets (i, j, k), when i, j, k are not all distinct and hence cannot be estimated from sampled data. To get around this barrier we consider a random partition of our element-set into 3 disjoint subsets. The actual tensor we work with consists only of triplets (i, j, k) where the indices belong to different partitions. Secondly, we have to handle the case where tensor based-technique fails, i.e. when the 3rd moment tensor isn\u2019t full-rank. This is a degenerate case. Typically, tensor based approaches for other problems cannot handle such degenerate cases. However, in the case of the Mallows mixture model, we show that such a degenerate case provides a lot of useful information about the problem. In particular, it must hold that \u03c61 \u2243 \u03c62, and \u03c01 and \u03c02 are fairly close \u2014 one is almost a cyclic shift of the other. To show this we use a characterization of the when the tensor decomposition is unique (for tensors of rank 2), and we handle such degenerate cases separately. Altogether, we find the mixture model\u2019s parameters with no non-degeneracy conditions.\nLower bound under the pairwise access model. Given that a single Mallows model can be learned using only pairwise comparisons, a very restricted access to each sample, it is natural to ask, \u201cIs it possible to learn a mixture of Mallows models from pairwise queries?\u201d. This next example shows that we cannot hope to do this even for a mixture of two Mallows models. Fix some \u03c6 and \u03c0 and assume our sample is taken using mixing weights of w1 = w2 = 12 from the two Mallows modelsMn(\u03c6, \u03c0) and Mn(\u03c6, rev(\u03c0)), where rev(\u03c0) indicates the reverse permutation (the first element of \u03c0 is the last of rev(\u03c0), the second is the next-to-last, etc.) . Consider two elements, e and e\u2032. Using only pairwise comparisons, we have that it is just as likely to rank e > e\u2032 as it is to rank e\u2032 > e and so this case cannot be learned regardless of the sample size.\n3-wise queries. We would also like to stress that our algorithm does not need full access to the sampled rankings and instead will work with access to certain 3-wise queries. Observe that the first part of our algorithm, where we recover the top elements in each of the two central permutations, only uses access to the top 3 elements in each sample. In that sense, we replace the pairwise query \u201cdo you prefer e to e\u2032?\u201d with a 3-wise query: \u201cwhat are your top 3 choices?\u201d Furthermore, the second part of the algorithm (where we solve a set of 2 linear equations) can be altered to support 3-wise queries of the (admittedly, somewhat unnatural) form \u201cif e\u2217 is your top choice, do you prefer e to e\u2032?\u201d For ease of exposition, we will assume full-access to the sampled rankings.\nFuture Directions. Several interesting directions come out of this work. A natural next step is to generalize our results to learn a mixture of k Mallows models for k > 2. We believe that most of these techniques can be extended to design algorithms that take poly(n, 1/\u01eb)k time. It would also be interesting to get algorithms for learning a mixture of k Mallows models which run in time poly(k, n), perhaps in an appropriate smoothed analysis setting [23] or under other non-degeneracy assumptions. Perhaps, more importantly, our result indicates that tensor based methods which have been very popular for learning problems, might also be a powerful tool for tackling ranking-related problems in the fields of machine learning, voting and social choice.\nOrganization. In Section 2 we give the formal definition of the Mallow model and of the problem statement, as well as some useful facts about the Mallow model. Our algorithm and its numerous subroutines are detailed in Section 3. In Section 4 we experimentally compare our algorithm with a popular EM based approach for the problem. The complete details of our algorithms and proofs are included in the supplementary material."}, {"heading": "2 Notations and Properties of the Mallows Model", "text": "Let Un = {e1, e2, . . . , en} be a set of n distinct elements. We represent permutations over the elements in Un through their indices [n]. (E.g., \u03c0 = (n, n \u2212 1, . . . , 1) represents the permutation (en, en\u22121, . . . , e1).) Let pos\u03c0(ei) = \u03c0\n\u22121(i) refer to the position of ei in the permutation \u03c0. We omit the subscript \u03c0 when the permutation \u03c0 is clear from context. For any two permutations \u03c0, \u03c0\u2032 we denote dkt(\u03c0, \u03c0\u2032) as the Kendall-Tau distance [24] between them (number of pairwise inversions between \u03c0, \u03c0\u2032). Given some \u03c6 \u2208 (0, 1) we denote Zi(\u03c6) = 1\u2212\u03c6 i\n1\u2212\u03c6 , and partition function Z[n](\u03c6) =\u2211 \u03c0 \u03c6 dkt(\u03c0,\u03c00) = \u220fn i=1 Zi(\u03c6) (see Section 6 in the supplementary material). Definition 2.1. [Mallows model (Mn(\u03c6, \u03c00)).] Given a permutation \u03c00 on [n] and a parameter \u03c6 \u2208 (0, 1),4, a Mallows model is a permutation generation process that returns permutation \u03c0 w.p.\nPr (\u03c0) = \u03c6dkt(\u03c0,\u03c00)/Z[n](\u03c6)\nIn Section 6 we show many useful properties of the Mallows model which we use repeatedly throughout this work. We believe that they provide an insight to Mallows model, and we advise the reader to go through them. We proceed with the main definition.\nDefinition 2.2. [Mallows Mixture model w1Mn(\u03c61, \u03c01) \u2295 w2Mn(\u03c62, \u03c02).] Given parameters w1, w2 \u2208 (0, 1) s.t. w1 + w2 = 1, parameters \u03c61, \u03c62 \u2208 (0, 1) and two permutations \u03c01, \u03c02, we call a mixture of two Mallows models to be the process that with probability w1 generates a permutation from M (\u03c61, \u03c01) and with probability w2 generates a permutation from M (\u03c62, \u03c02).\nOur next definition is crucial for our application of tensor decomposition techniques.\nDefinition 2.3. [Representative vectors.] The representative vector of a Mallows model is a vector where for every i \u2208 [n], the ith-coordinate is \u03c6pos\u03c0(ei)\u22121/Zn. The expression \u03c6pos\u03c0(ei)\u22121/Zn is precisely the probability that a permutation generated by a model Mn(\u03c6, \u03c0) ranks element ei at the first position (proof deferred to the supplementary material). Given that our focus is on learning a mixture of two Mallows models Mn(\u03c61, \u03c01) and Mn(\u03c62, \u03c02), we denote x as the representative vector of the first model, and y as the representative vector of the latter. Note that retrieving the vectors x and y exactly implies that we can learn the permutations \u03c01 and \u03c02 and the values of \u03c61, \u03c62.\n4It is also common to parameterize using \u03b2 \u2208 R+ where \u03c6 = e\u2212\u03b2 . For small \u03b2 we have (1\u2212 \u03c6) \u2248 \u03b2.\nFinally, let f (i \u2192 j) be the probability that element ei goes to position j according to mixture model. Similarly f (1) (i \u2192 j) be the corresponding probabilities according to Mallows model M1 and M2 respectively. Hence, f (i \u2192 j) = w1f (1) (i \u2192 j) + w2f (2) (i \u2192 j). Tensors: Given two vectors u \u2208 Rn1 , v \u2208 Rn2 , we define u\u2297v \u2208 Rn1\u00d7n2 as the matrix uvT . Given also z \u2208 Rn3 then u\u2297v\u2297z denotes the 3-tensor (of rank- 1) whose (i, j, k)-th coordinate is uivjzk. A tensor T \u2208 Rn1\u00d7n2\u00d7n3 has a rank-r decomposition if T can be expressed as \u2211 i\u2208[r] ui \u2297 vi \u2297 zi where ui \u2208 Rn1 , vi \u2208 Rn2 , zi \u2208 Rn3 . Given two vectors u, v \u2208 Rn, we use (u; v) to denote the n\u00d7 2 matrix that is obtained with u and v as columns. We now define first, second and third order statistics (frequencies) that serve as our proxies for the first, second and third order moments. Definition 2.4. [Moments] Given a Mallows mixture model, we denote for every i, j, k \u2208 [n]\n\u2022 Pi = Pr (pos (ei) = 1) is the probability that element ei is ranked at the first position \u2022 Pij = Pr (pos ({ei, ej}) = {1, 2}), is the probability that ei, ej are ranked at the first two\npositions (in any order)\n\u2022 Pijk = Pr (pos ({ei, ej , ek}) = {1, 2, 3}) is the probability that ei, ej , ek are ranked at the first three positions (in any order).\nFor convenience, let P represent the set of quantities (Pi, Pij , Pijk)1\u2264i<j<k\u2264n. These can be estimated up to any inverse polynomial accuracy using only polynomial samples. The following simple, yet crucial lemma relates P to the vectors x and y, and demonstrates why these statistics and representative vectors are ideal for tensor decomposition. Lemma 2.5. Given a mixture w1M (\u03c61, \u03c01)\u2295 w2M (\u03c62, \u03c02) let x, y and P be as defined above.\n1. For any i it holds that Pi = w1xi + w2yi.\n2. Denote c2(\u03c6) = Zn(\u03c6) Zn\u22121(\u03c6) 1+\u03c6 \u03c6 . Then for any i 6= j it holds that Pij = w1c2(\u03c61)xixj +\nw2c2(\u03c62)yiyj .\n3. Denote c3(\u03c6) = Z2n(\u03c6) Zn\u22121(\u03c6)Zn\u22122(\u03c6) 1+2\u03c6+2\u03c62+\u03c63\n\u03c63 . Then for any distinct i, j, k it holds that Pijk = w1c3(\u03c61)xixjxk + w2c3(\u03c62)yiyjyk.\nClearly, if i = j then Pij = 0, and if i, j, k are not all distinct then Pijk = 0.\nIn addition, in Lemma 13.2 in the supplementary material we prove the bounds c2(\u03c6) = O(1/\u03c6) and c3(\u03c6) = O(\u03c6\u22123).\nPartitioning Indices: Given a partition of [n] into Sa, Sb, Sc, let x(a), y(a) be the representative vectors x, y restricted to the indices (rows) in Sa (similarly for Sb, Sc). Then the 3-tensor\nT (abc) \u2261 (Pijk)i\u2208Sa,j\u2208Sb,k\u2208Sc = w1c3(\u03c61)x(a) \u2297 x(b) \u2297 x(c) + w2c3(\u03c62)y(a) \u2297 y(b) \u2297 y(c). This tensor has a rank-2 decomposition, with one rank-1 term for each Mallows model. Finally for convenience we define the matrix M = (x; y), and similarly define the matrices Ma = (x(a); y(a)), Mb = (x (b); y(b)), Mc = (x(c); y(c)).\nError Dependency and Error Polynomials. Our algorithm gives an estimate of the parameters w, \u03c6 that we learn in the first stage, and we use these estimates to figure out the entire central rankings in the second stage. The following lemma essentially allows us to assume instead of estimations, we have access to the true values of w and \u03c6.\nLemma 2.6. For every \u03b4 > 0 there exists a function f(n, \u03c6, \u03b4) s.t. for every n, \u03c6 and \u03c6\u0302 satisfying |\u03c6\u2212\u03c6\u0302| < \u03b4f(n,\u03c6,\u03b4) we have that the total-variation distance satisfies \u2016M (\u03c6, \u03c0)\u2212M ( \u03c6\u0302, \u03c0 ) \u2016TV \u2264 \u03b4.\nFor the ease of presentation, we do not optimize constants or polynomial factors in all parameters. In our analysis, we show how our algorithm is robust (in a polynomial sense) to errors in various statistics, to prove that we can learn with polynomial samples. However, the simplification when there are no errors (infinite samples) still carries many of the main ideas in the algorithm \u2014 this in fact shows the identifiability of the model, which was not known previously."}, {"heading": "3 Algorithm Overview", "text": "Algorithm 1 LEARN MIXTURES OF TWO MALLOWS MODELS, Input: a set S of N samples from w1M (\u03c61, \u03c01)\u2295 w2M (\u03c62, \u03c02), Accuracy parameters \u01eb, \u01eb2.\n1. Let P\u0302 be the empirical estimate of P on samples in S . 2. Repeat O(log n) times:\n(a) Partition [n] randomly into Sa, Sb and Sc. Let T (abc) = ( P\u0302ijk ) i\u2208Sa,j\u2208Sb,k\u2208Sc . (b) Run TENSOR-DECOMP from [25, 26, 23] to get a decomposition of T (abc) = u(a) \u2297 u(b) \u2297 u(c) + v(a) \u2297 v(b) \u2297 v(c). (c) If min{\u03c32(u(a); v(a)), \u03c32(u(b); v(b)), \u03c32(u(c); v(c))} > \u01eb2 (In the non-degenerate case these matrices are far from being rank-1 matrices in the sense that their least singular value is bounded away from 0.)\ni. Obtain parameter estimates (w\u03021, w\u03022, \u03c6\u03021, \u03c6\u03022 and prefixes of the central rankings \u03c01\u2032, \u03c02\u2032) from INFER-TOP-K(P\u0302 , M \u2032a , M \u2032 b , M \u2032 c), with M \u2032 i = (u\n(i); v(i)) for i \u2208 {a, b, c}. ii. Use RECOVER-REST to find the full central rankings \u03c0\u03021, \u03c0\u03022.\nReturn SUCCESS and output (w\u03021, w\u03022, \u03c6\u03021, \u03c6\u03022, \u03c0\u03021, \u03c0\u03022).\n3. Run HANDLE DEGENERATE CASES (P\u0302 ).\nOur algorithm (Algorithm 1) has two main components. First we invoke a decomposition algorithm [25, 26, 23] over the tensor T (abc), and retrieve approximations of the two Mallows models\u2019 representative vectors which in turn allow us to approximate the weight parameters w1, w2, scale parameters \u03c61, \u03c62, and the top few elements in each central ranking. We then use the inferred parameters to recover the entire rankings \u03c01 and \u03c02. Should the tensor-decomposition fail, we invoke a special procedure to handle such degenerate cases. Our algorithm has the following guarantee.\nTheorem 3.1. Let w1M (\u03c61, \u03c01) \u2295 w2M (\u03c62, \u03c02) be a mixture of two Mallows models and let wmin = min{w1, w2} and \u03c6max = max{\u03c61, \u03c62} and similarly \u03c6min = min{\u03c61, \u03c62}. Denote \u01eb0 =\nw2min(1\u2212\u03c6max)10 16n22\u03c62max . Then, given any 0 < \u01eb < \u01eb0, suitably small \u01eb2 = poly( 1n , \u01eb, \u03c6min, wmin)\nand N = poly ( n, 1min{\u01eb,\u01eb0} , 1 \u03c61(1\u2212\u03c61) , 1 \u03c62(1\u2212\u03c62) , 1 w1 , 1w2 ) i.i.d samples from the mixture model, Algorithm 1 recovers, in poly-time and with probability \u2265 1 \u2212 n\u22123, the model\u2019s parameters with w1, w2, \u03c61, \u03c62 recovered up to \u01eb-accuracy. Next we detail the various subroutines of the algorithm, and give an overview of the analysis for each subroutine. The full analysis is given in the supplementary material.\nThe TENSOR-DECOMP Procedure. This procedure is a straight-forward invocation of the algorithm detailed in [25, 26, 23]. This algorithm uses spectral methods to retrieve the two vectors generating the rank-2 tensor T (abc). This technique works when all factor matrices Ma = (x(a); y(a)),Mb = (x (b); y(b)),Mc = (x (c); y(c)) are well-conditioned. We note that any algorithm that decomposes non-symmetric tensors which have well-conditioned factor matrices, can be used as a black box.\nLemma 3.2 (Full rank case). In the conditions of Theorem 3.1, suppose our algorithm picks some partition Sa, Sb, Sc such that the matrices Ma,Mb,Mc are all well-conditioned \u2014 i.e. have \u03c32(Ma), \u03c32(Mb), \u03c32(Mc) \u2265 \u01eb\u20322 \u2265 poly( 1n , \u01eb, \u01eb2, w1, w2) then with high probability, Algorithm TENSORDECOMP of [25] finds M \u2032a = (u (a); v(a)),M \u2032b = (u (b); v(b)),M \u2032c = (u (c); v(c)) such that for any \u03c4 \u2208 {a, b, c}, we have u(\u03c4) = \u03b1\u03c4x(\u03c4) + z(\u03c4)1 and v(\u03c4) = \u03b2\u03c4y(\u03c4) + z (\u03c4) 2 ; with \u2016z(\u03c4)1 \u2016, \u2016z (\u03c4) 2 \u2016 \u2264 poly( 1n , \u01eb, \u01eb2, wmin) and, \u03c32(M \u2032\u03c4 ) > \u01eb2 for \u03c4 \u2208 {a, b, c}.\nThe INFER-TOP-K procedure. This procedure uses the output of the tensor-decomposition to retrieve the weights, \u03c6\u2019s and the representative vectors. In order to convert u(a), u(b), u(c) into an approximation of x(a), x(b), x(c) (and similarly with v(a), v(b), v(c) and y(a), y(b), y(c)), we need to find a good approximation of the scalars \u03b1a, \u03b1b, \u03b1c. This is done by solving a certain linear system. This also allows us to estimate w\u03021, w\u03022. Given our approximation of x, it is easy to find \u03c61 and the top first elements of \u03c01 \u2014 we sort the coordinates of x, setting \u03c0\u20321 to be the first elements in the sorted\nvector, and \u03c61 as the ratio between any two adjacent entries in the sorted vector. We refer the reader to Section 8 in the supplementary material for full details. The RECOVER-REST procedure. The algorithm for recovering the remaining entries of the central permutations (Algorithm 2) is more involved.\nAlgorithm 2 RECOVER-REST, Input: a set S of N samples from w1M (\u03c61, \u03c01)\u2295w2M (\u03c62, \u03c02), parameters w\u03021, w\u03022, \u03c6\u03021, \u03c6\u03022 and initial permutations \u03c0\u03021, \u03c0\u03022, and accuracy parameter \u01eb.\n1. For elements in \u03c0\u03021 and \u03c0\u03022, compute representative vectors x\u0302 and y\u0302 using estimates \u03c6\u03021 and \u03c6\u03022. 2. Let |\u03c0\u03021| = r1, |\u03c0\u03022| = r2 and wlog r1 \u2265 r2.\nIf there exists an element ei such that pos\u03c0\u03021(ei) > r1 and pos\u03c0\u03022(ei) < r2/2 (or in the symmetric case), then: Let S1 be the subsample with ei ranked in the first position.\n(a) Learn a single Mallows model on S1 to find \u03c0\u03021. Given \u03c0\u03021 use dynamic programming to find \u03c0\u03022 3. Let ei\u2217 be the first element in \u03c0\u03021 having its probabilities of appearing in first place in \u03c01 and \u03c02 differ\nby at least \u01eb. Define w\u0302\u20321 = ( 1 + w\u03022\nw\u03021\ny\u0302(ei\u2217 ) x\u0302(ei\u2217 )\n)\u22121 and w\u0302\u20322 = 1\u2212 w\u0302\u20321. Let S1 be the subsample with ei\u2217\nranked at the first position. 4. For each ei that doesn\u2019t appear in either \u03c0\u03021 or \u03c0\u03022 and any possible position j it might belong to\n(a) Use S to estimate f\u0302i,j = Pr (ei goes to position j), and S1 to estimate f\u0302 (i \u2192 j|ei\u2217 \u2192 1) = Pr (ei goes to position j|ei\u2217 7\u2192 1).\n(b) Solve the system\nf\u0302 (i \u2192 j) = w\u03021f (1) (i \u2192 j) + w\u03022f (2) (i \u2192 j) (1) f\u0302 (i \u2192 j|ei\u2217 \u2192 1) = w\u0302\u20321f (1) (i \u2192 j) + w\u0302\u20322f (2) (i \u2192 j) (2)\n5. To complete \u03c0\u03021 assign each ei to position argmaxj{f (1) (i \u2192 j)}. Similarly complete \u03c0\u03022 using f (2) (i \u2192 j). Return the two permutations.\nAlgorithm 2 first attempts to find a pivot \u2014 an element ei which appears at a fairly high rank in one permutation, yet does not appear in the other prefix \u03c0\u03022. Let Eei be the event that a permutation ranks ei at the first position. As ei is a pivot, then PrM1 (Eei) is noticeable whereas PrM2 (Eei ) is negligible. Hence, conditioning on ei appearing at the first position leaves us with a subsample in which all sampled rankings are generated from the first model. This subsample allows us to easily retrieve the rest of \u03c01. Given \u03c01, the rest of \u03c02 can be recovered using a dynamic programming procedure. Refer to the supplementary material for details.\nThe more interesting case is when no such pivot exists, i.e., when the two prefixes of \u03c01 and \u03c02 contain almost the same elements. Yet, since we invoke RECOVER-REST after successfully calling TENSOR-DECOMP , it must hold that the distance between the obtained representative vectors x\u0302 and y\u0302 is noticeably large. Hence some element ei\u2217 satisfies |x\u0302(ei\u2217) \u2212 y\u0302(ei\u2217)| > \u01eb, and we proceed by setting up a linear system. To find the complete rankings, we measure appropriate statistics to set up a system of linear equations to calculate f (1) (i \u2192 j) and f (2) (i \u2192 j) up to inverse polynomial accuracy. The largest of these values { f (1) (i \u2192 j) } corresponds to the position of ei in the central ranking of M1. To compute the values { f (r) (i \u2192 j)\n} r=1,2\nwe consider f (1) (i \u2192 j|ei\u2217 \u2192 1) \u2013 the probability that ei is ranked at the jth position conditioned on the element ei\u2217 ranking first according to M1 (and resp. for M2). Using w\u20321 and w\u20322 as in Algorithm 2, it holds that\nPr (ei \u2192 j|ei\u2217 \u2192 1) = w\u20321f (1) (i \u2192 j|ei\u2217 \u2192 1) + w\u20322f (2) (i \u2192 j|ei\u2217 \u2192 1) .\nWe need to relate f (r) (i \u2192 j|ei\u2217 \u2192 1) to f (r) (i \u2192 j). Indeed Lemma 10.1 shows that Pr (ei \u2192 j|ei\u2217 \u2192 1) is an almost linear equations in the two unknowns. We show that if ei\u2217 is ranked above ei in the central permutation, then for some small \u03b4 it holds that\nPr (ei \u2192 j|ei\u2217 \u2192 1) = w\u20321f (1) (i \u2192 j) + w\u20322f (2) (i \u2192 j)\u00b1 \u03b4 We refer the reader to Section 10 in the supplementary material for full details.\nThe HANDLE-DEGENERATE-CASES procedure. We call a mixture model w1M (\u03c61, \u03c01) \u2295 w2M (\u03c62, \u03c02) degenerate if the parameters of the two Mallows models are equal, and the edit distance between the prefixes of the two central rankings is at most two i.e., by changing the positions of at most two elements in \u03c01 we retrieve \u03c02. We show that unless w1M (\u03c61, \u03c01)\u2295w2M (\u03c62, \u03c02) is degenerate, a random partition (Sa, Sb, Sc) is likely to satisfy the requirements of Lemma 3.2 (and TENSOR-DECOMP will be successful). Hence, if TENSOR-DECOMP repeatedly fail, we deduce our model is indeed degenerate. To show this, we characterize the uniqueness of decompositions of rank 2, along with some very useful properties of random partitions. In such degenerate cases, we find the two prefixes and then remove the elements in the prefixes from U , and recurse on the remaining elements. We refer the reader to Section 9 in the supplementary material for full details."}, {"heading": "4 Experiments", "text": "Goal. The main contribution of our paper is devising an algorithm that provably learns any mixture of two Mallows models. But could it be the case that the previously existing heuristics, even though they are unproven, still perform well in practice? We compare our algorithm to existing techniques, to see if, and under what settings our algorithm outperforms them.\nBaseline. We compare our algorithm to the popular EM based algorithm of [5], seeing as EM based heuristics are the most popular way to learn a mixture of Mallows models. The EM algorithm starts with a random guess for the two central permutations. At iteration t, EM maintains a guess as to the two Mallows models that generated the sample. First (expectation step) the algorithm assigns a weight to each ranking in our sample, where the weight of a ranking reflects the probability that it was generated from the first or the second of the current Mallows models. Then (the maximization step) the algorithm updates its guess of the models\u2019 parameters based on a local search \u2013 minimizing the average distance to the weighted rankings in our sample. We comment that we implemented only the version of our algorithm that handles non-degenerate cases (more interesting case). In our experiment the two Mallows models had parameters \u03c61 6= \u03c62, so our setting was never degenerate. Setting. We ran both the algorithms on synthetic data comprising of rankings of size n = 10. The weights were sampled u.a.r from [0, 1], and the \u03c6-parameters were sampled by sampling ln(1/\u03c6) u.a.r from [0, 5]. For d ranging from 0 to ( n 2 ) we generated the two central rankings \u03c01 and \u03c02 to be within distance d in the following manner. \u03c01 was always fixed as (1, 2, 3, . . . , 10). To describe \u03c02, observe that it suffices to note the number of inversion between 1 and elements 2, 3, ..., 10; the number of inversions between 2 and 3, 4, ..., 10 and so on. So we picked u.a.r a non-negative integral solution to x1+ . . .+xn = d which yields a feasible permutation and let \u03c02 be the permutation that it details. Using these models\u2019 parameters, we generated N = 5 \u00b7 106 random samples. Evaluation Metric and Results. For each value of d, we ran both algorithms 20 times and counted the fraction of times on which they returned the true rankings that generated the sample. The results of the experiment for rankings of size n = 10 are in Table 1. Clearly, the closer the two centrals rankings are to one another, the worst EM performs. On the other hand, our algorithm is able to recover the true rankings even at very close distances. As the rankings get slightly farther, our algorithm recovers the true rankings all the time. We comment that similar performance was observed for other values of n as well. We also comment that our algorithm\u2019s runtime was reasonable (less than 10 minutes on a 8-cores Intel x86 64 computer). Surprisingly, our implementation of the EM algorithm typically took much longer to run \u2014 due to the fact that it simply did not converge."}, {"heading": "5 Acknowledgements", "text": "We would like to thank Ariel Procaccia for bringing to our attention various references to Mallows model in social choice theory."}, {"heading": "6 Properties of the Mallows Model", "text": "In this section, we outline some of the properties of the Mallows model. Some of these properties were already shown before (see [27]), but we add them in this appendix for completion. Our algorithm and its analysis rely heavily on these properties.\nNotation. Given a Mallows model Mn (\u03c6, \u03c00) we denote Zn = 1\u2212\u03c6 n\n1\u2212\u03c6 , and we denote Z[n] as the\nsum all weights of all permutations: Z[n] = \u2211 \u03c0 \u03c6 dkt(\u03c0,\u03c00). Given an element e, we abuse notation and denote by \u03c0 \\e the permutation we get by omitting the element e (projecting \u03c0 over all elements but e). The notation \u03c0 = (e, \u03c3) denotes a permutation whose first element is e and elements 2 through n are as given by the permutation over n\u2212 1 elements \u03c3. The first property shows that for any element e, conditioning on e being ranked at the first position results in a reduced Mallows model.\nLemma 6.1. Let M (\u03c6, \u03c0) be a Mallows model over [n]. For any i, the conditional distribution (given that i is ranked at position 1) of rankings over [n] \\ {i}, i.e. Pr (\u03c0|\u03c0(i) = 1) is the same as that of M (\u03c6, \u03c0 \\ i).\nThe above lemma can be extended to conditioning on prefixes as follows.\nLemma 6.2. Let M (\u03c6, \u03c0) be a Mallows model over [n]. For any prefix I of \u03c0, the marginal distribution of rankings over [n] \\ I is the same as that of M (\u03c6, \u03c0 \\ I).\nThe following lemma describe a useful trick that allows us to simulate the addition of another element that is added to the start of the central ranking \u03c0, using the knowledge of \u03c6. This will be particularly useful to simplify certain degenerate cases.\nLemma 6.3. Let M (\u03c6, \u03c0) be a Mallows model over [n]. Given oracle access to M (\u03c6, \u03c0) and a new element e0 /\u2208 [n] we can efficiently simulate an oracle access to M (\u03c6, (e0, \u03c0))."}, {"heading": "6.1 Proofs of Lemmas 6.1, 6.2, 6.3", "text": "Observation. All of the properties we state and prove in this appendix are based on the following important observation. Given two permutations \u03c0 and \u03c0\u2032, denote the first element in \u03c0 as e1. Then we have that\n#pairs (e1, ei)i6=1 that \u03c0, \u03c0 \u2032 disagree on = (position of e1 in \u03c0 \u2032)\u2212 1 = pos\u03c0\u2032(e1)\u2212 1 The same holds for the last element, denoted en, only using the distance between pos\u03c0\u2032(en) and the nth-position (i.e., n\u2212 pos\u03c0\u2032(en)). We begin by characterizing Z[n]. Property 6.4. For every n and any \u03c00 \u2208 Sn we have that Z[n] = \u2211 \u03c0 \u03c6 dkt(\u03c0,\u03c00) =\n\u220fn i=1 Zi =\u220f\ni (\u2211i=1 j=0 \u03c6 j ) .\nProof. By induction on n. For n = 1 there\u2019s a single permutation over the set {1} and Z1 = 1. For any n > 1, given a permutation over n elements \u03c0 \u2208 Sn, denote its first element as e\u03c0. Based on our observation, we have that\ndkt(\u03c0, \u03c00) = #swaps involving e\u03c0 + dkt(\u03c0 \\ e\u03c0, \u03c00 \\ e\u03c0) = (pos\u03c00(e\u03c0)\u2212 1)+ dkt(\u03c0 \\ e\u03c0, \u03c00 \\ e\u03c0) And so we have\nZ[n] = \u2211 \u03c0 \u03c6 dkt(\u03c0,\u03c00) =\nn\u2211\nj=1\n\u2211\n{\u03c0:e\u03c0 is the jth elements in \u03c00} \u03c6dkt(\u03c0,\u03c00)\n= n\u2211\nj=1\n\u2211\n{\u03c0:e\u03c0 is the jth elements in \u03c00} \u03c6j\u22121\u03c6dkt(\u03c0\\e\u03c0,\u03c00\\e\u03c0)\n=\nn\u22121\u2211\nj=0\n\u03c6j \u2211\n\u03c0\u2208Sn\u22121 \u03c6dkt(\u03c0,\u03c0\n\u2212j 0 )\ninduction =\nn\u22121\u2211\nj=0\n\u03c6j\n( n\u22121\u220f\ni=1\nZi\n) = ( n\u22121\u220f\ni=1\nZi ) Zn = n\u220f\ni=1\nZi\nwhere \u03c0\u2212j0 denotes the permutation we get by omitting the jth element from \u03c00.\nObserve that the proof essentially shows how to generate a random ranking from a Mallows model. What we in fact showed is that the given a permutation \u03c0 = (e, \u03c0 \\ e) we have that\nPr[\u03c0] = 1Z[n]\u03c6 (pos\u03c00 (e)\u22121)+dkt(\u03c0\\e,\u03c00\\e) =\n\u03c6(pos\u03c00(e)\u22121)\nZn \u00b7 \u03c6\ndkt(\u03c0\\e,\u03c00\\e)\nZ1:(n\u22121)\nAnd so, to generate a random permutation using \u03c00: place the jth elements of \u03c00 at the first position w.p. \u221d \u03c6j\u22121, and recourse over the truncated permutation. \u03c00 \\e1 to find the rest of the permutation (positions 1, 2, . . . , j \u2212 1, j + 1, . . . , n). This proves Lemma 6.1. Note the symmetry between \u03c0 and \u03c00 in defining the weight of \u03c0. Therefore, denoting e1 as the element \u03c00 ranks at the first position, we have that\ndkt(\u03c0, \u03c00) = #swaps involving e1 + dkt(\u03c0 \\ e1, \u03c00 \\ e1) = (i\u2212 1) + dkt(\u03c0 \\ e1, \u03c00 \\ e1)\nand so, the probability of permutation \u03c0 in which e1 is ranked at position j and the rest of the permutation is as a given permutation \u03c3 over n\u2212 1 elements is:\nPr[\u03c0] = 1Z[n]\u03c6 (j\u22121)+dkt(\u03c0\\e1,\u03c00\\e1) =\n\u03c6(j\u22121)\nZn \u00b7 \u03c6\ndkt(\u03c0\\e1,\u03c00\\e1)\nZ1:(n\u22121)\nSo, an alternative way to generate a random permutation using \u03c00 is to rank element e1 at position j w.p. \u221d \u03c6j\u22121 and then to recourse over the truncated permutation \u03c00 \\ e1. Repeating this argument for each element in a given prefix I of \u03c00 proves Lemma 6.2.\nObserve that the algorithms the generate a permutation for a given Mallows model also allow us to simulate a random sample from a Mallows model over n + 1 elements. That is, given \u03c00, we can introduce a new element e0 and denote \u03c0\u20320 = (e0, \u03c00). Now, to sample from a Mallows model centered at \u03c0\u20320 all we need is to pick the position of e0 (moving it to position j w.p. \u03c6\nj\u22121/Zn+1), then sampling from original Mallows model. This proves Lemma 6.3."}, {"heading": "6.2 Total Variation Distance", "text": "In this subsection, our goal is to prove Lemma 2.6. Namely, we aim to show that given \u03c6, for every \u03b4 > 0 we can pick any \u03c6\u0302 sufficiently close to \u03c6, and have that the total variation distance between\nthe two models M (\u03c6, \u03c00) and M ( \u03c6\u0302, \u03c00 ) is at most \u03b4.\nProof of Lemma 2.6. First, denote \u03c6 = e\u2212\u03b2 and \u03c6\u0302 = e\u2212\u03b2\u0302 . And so it holds that\n|\u03b2 \u2212 \u03b2\u0302| = | ln(1/\u03c6)\u2212 ln(1/\u03c6\u0302)| = | ln(\u03c6\u0302/\u03c6)| \u2264 | ln(1 + |\u03c6\u2212\u03c6\u0302|\u03c6min )| \u2264 |\u03c6\u2212\u03c6\u0302| \u03c6min\nassuming some global lower bound \u03c6min on \u03c6, \u03c6\u0302.\nObserve that for every \u03c0 we have that\n\u03c6dkt(\u03c0,\u03c00) = exp(\u2212\u03b2dkt(\u03c0, \u03c00)) = exp(\u2212\u03b2\u0302dkt(\u03c0, \u03c00)) exp(\u2212(\u03b2\u2212\u03b2\u0302)dkt(\u03c0, \u03c00)) \u2264 e 1 2n 2|\u03b2\u2212\u03b2\u0302|\u03c6\u0302dkt(\u03c0,\u03c00)\nAlgorithm 3 LEARN MIXTURES OF TWO MALLOWS MODELS, Input: a set S of N samples from w1M (\u03c61, \u03c01)\u2295 w2M (\u03c62, \u03c02), Accuracy parameters \u01eb, \u01eb2.\n1. Set threshold \u01eb2 = f2(\u01eb).\n2. Let P\u0302 be the empirical estimate of P on samples in S . 3. Run O(log n) times\n(a) Partition [n] randomly into Sa, Sb and Sc. (b) Set T (abc) = ( P\u0302ijk\n) i\u2208Sa,j\u2208Sb,k\u2208Sc .\n(c) Run TENSOR-DECOMP as in Theorem 4.2 of([25]) to get a decomposition of Tabc = u(a) \u2297 u(b) \u2297 u(c) + v(a) \u2297 v(b) \u2297 v(c). (d) Let M \u2032a = (u (a); v(a)), M \u2032b = (u (b); v(b)), M \u2032c = (u (c); v(c)). (e) If min(\u03c32(M \u2032a), \u03c32(M \u2032 b), \u03c32(M \u2032 c)) \u2265 \u01eb2,\ni. (w\u03021, w\u03022, \u03c6\u03021, \u03c6\u03022, \u03c01\u2032, \u03c02\u2032) \u2190 INFER-TOP-K(P\u0302 , M \u2032a , M \u2032b , M \u2032c). ii. (\u03c0\u03021, \u03c0\u03022) \u2190 RECOVER-REST(S,w\u03021, w\u03022, \u03c6\u03021, \u03c6\u03022, \u03c01\u2032, \u03c02\u2032 , \u01eb2/ \u221a 2n).\nReturn SUCCESS and output (w\u03021, w\u03022, \u03c6\u03021, \u03c6\u03022, \u03c0\u03021, \u03c0\u03022). (f) Else if \u03c32(M \u2032a) < \u01eb2 and \u03c32(M \u2032 b) \u2265 \u01eb2, and \u03c32(M \u2032c) \u2265 \u01eb2 (or other symmetric cases),\nlet p(a) = ( P\u0302i )\ni\u2208Sa .\n\u03c6\u0302 \u2190 ESTIMATE-PHI(p(a)). (g) Else \u03c6\u0302 = median ( ESTIMATE-PHI(p(a)), ESTIMATE-PHI(p(b)), ESTIMATE-PHI(p(c)) ) .\n(h) Else, (at least two of the three matrices M \u2032a,M \u2032 b,M \u2032 c are essentially rank-1)\nlet \u03c4 \u2208 {a, b, c} denote a matrix M \u2032\u03c4 s.t. \u03c32(M \u2032\u03c4 ) < \u01eb2, and let p(\u03c4) = (P\u0302i)i\u2208S\u03c4 . \u03c6\u0302 \u2190 ESTIMATE-PHI(p(\u03c4)).\n4. Run HANDLE-DEGENERATE-CASE(P\u0302 , \u03c6\u0302, \u01eb).\nSumming over all permutation (and replacing the role of \u03c6 and \u03c6\u0302) we have also that \u2211\n\u03c0 \u03c6 dkt(\u03c0,\u03c00) \u2265\ne\u2212 1 2n 2|\u03b2\u2212\u03b2\u0302| \u2211 \u03c0 \u03c6\u0302 dkt(\u03c0,\u03c00). Let p\u03c0 (resp. p\u0302\u03c0) denote the probability of sampling the permutation \u03c0\nfrom a Mallows model M (\u03c6, \u03c00) (resp. M ( \u03c6\u0302, \u03c00 ) ). It follows that for every \u03c0 we have\np\u03c0 = \u03c6dkt(\u03c0,\u03c00)\u2211 \u03c0\u2032 \u03c6 dkt(\u03c0\u2032,\u03c00) \u2264 en2|\u03b2\u2212\u03b2\u0302| \u03c6\u0302 dkt(\u03c0,\u03c00) \u2211 \u03c0\u2032 \u03c6\u0302 dkt(\u03c0\u2032,\u03c00) = en 2|\u03b2\u2212\u03b2\u0302|p\u0302\u03c0\nand similarly, p\u0302\u03c0 \u2264 en 2|\u03b2\u2212\u03b2\u0302|p\u03c0.\nTherefore, assuming that |\u03b2 \u2212 \u03b2\u0302| is sufficiently small, and using the fact that |1 \u2212 ex| \u2264 2|x| for x \u2208 (\u2212 12 , 12 ), then we have\n\u2016M (\u03c6, \u03c0)\u2212M ( \u03c6\u0302, \u03c0 ) \u2016TV = 1\n2\n\u2211\n\u03c0\n|p\u03c0 \u2212 p\u0302\u03c0|\n= 1\n2\n\u2211\n\u03c0\np\u03c0 \u2223\u2223\u2223\u22231\u2212 p\u0302\u03c0 p\u03c0 \u2223\u2223\u2223\u2223 \u2264 1 2 \u2211\n\u03c0\n2p\u03c0n 2|\u03b2 \u2212 \u03b2\u0302| = n\n2\n\u03c6min |\u03c6\u2212 \u03c6\u0302|\nIt follows that in order to bound the total variation distance by \u03b4 we need to have \u03c6 and \u03c6\u0302 close up to a factor of \u03b4 \u00b7 \u03c6min/n2."}, {"heading": "7 Algorithm and Subroutines", "text": "We now describe the algorithm and its subroutines in full detail. These will be followed by the analysis of the algorithms and proof of correctness in the following sections. Broadly speaking, our algorithm (Algorithm 1) has two main components.\nRetrieving the Top Elements and Parameters. In the first part we use spectral methods to recover elements which have a good chance of appearing in the first position. The algorithm tries\nAlgorithm 4 INFER-TOP-K, Input: P\u0302 ,M \u2032a = (u (a); v(a)),M \u2032b = (u (b); v(b)),M \u2032c = (u (c); v(c)).\n1. Let P\u0302a = P\u0302 (i \u2208 a) 2. Set (\u03b1a, \u03b2a)T = (M \u2032a) \u2020 P\u0302a\n(\u03b1b, \u03b2b) T = (M \u2032b) \u2020 P\u0302b (\u03b1c, \u03b2c) T = (M \u2032c) \u2020 P\u0302c.\n3. Set w\u03021 = \u2016\u03b1au(a)\u20161 + \u2016\u03b1bu(b)\u20161 + \u2016\u03b1cu(c)\u20161, w\u03022 = 1\u2212 w\u03021. 4. Let u =\n( \u03b1a w1 u(a), \u03b1b w1 u(b), \u03b1c w1 u(c) ) .\nv = (\n\u03b2a w2 v(a), \u03b2b w2 v(b), \u03b2c w2\nv(c) ) .\n5. Sort the vectors u and v in decreasing order, i.e., U \u2190SORT(u), V \u2190SORT(v). 6. \u03c6\u03021 = U2U1 and \u03c6\u03022 = V2 V1 . 7. Define \u03b3 = (1\u2212 \u02c6\u03c6max) 2\n4n \u02c6\u03c6max . Let r1 = log1/\u03c6\u03021\n( n10\nw2min\u03b3 2\n) and r2 = log1/\u03c6\u03022 ( n10\nw2min\u03b3 2\n) .\n8. Output \u03c0\u20321 to be the first r1 ordered elements according to U and \u03c0 \u2032 2 to be the first r2 ordered elements\naccording to V .\nO(log n) different random partitions Sa, Sb, Sc, and constructs the tensor T (abc) from the samples as described in step 3(b). We then try to find a rank-2 decomposition of the tensor using a black-box algorithm for decomposing non-symmetric tensors. While we use the algorithm of [25] here, we can use the more practically efficient algorithm of Jennrich [23], or other power-iteration methods that are suitably modified to handle non-symmetric tensors.\nThese algorithms work when the factor matrices Ma,Mb,Mc have polynomially bounded condition number (in other words their second largest singular values \u03c32(\u00b7) is lower bounded by a polynomial in the input parameters) \u2014 in such cases the tensor T (abc) has a unique rank-2 decomposition. If this condition holds for any of the random partitions, then one can recover the top few elements of both \u03c01 and \u03c02 correctly. In addition, we can also infer the parameters w\u2019s and \u03c6\u2019s to good accuracy \u01eb (corresponding to INFER-TOP-K (Algorithm 4). This is detailed in section 8.\nIf any random partition Sa, Sb, Sc fails to produce a tensor T (abc) with well-conditioned factor matrices, then we are already in a special case. We show that in this case, the scaling parameters \u03c61 \u2248 \u03c62 with high probability. We exploit the random choice of the partition to make this argument (see Lemma 9.1). However, we still need to find the top few elements of the permutations and the weights. If all these O(log n) random partitions fail, then we show that we are in the Degenerate case that we handle separately; we describe a little later. Otherwise, if at least one of the random partitions succeeds, then we have estimated the scaling parameters, the mixing weights and the top few elements of both permutations.\nRecovering Rest of the Elements. The second part of the algorithm (corresponding to RECOVERREST) takes the inferred parameters and the initial prefixes as input and uses this information to recover the entire rankings \u03c01 and \u03c02. This is done by observing that the probability of an element ei going to position j can be written as a weighted combination of the corresponding probabilities under \u03c01 and \u03c02. In addition, as mentioned in Section 2, the reduced distribution obtained by conditioning on a particular element ej going to position 1 is again a mixture of two Mallows models with the same parameters. Hence, by conditioning on a particular element which appears in the initial learned prefix, we get a system of linear equations which can be used to infer the probability of every other element ei going to position j in both \u03c01 and \u03c02. This will allow us to infer the entire rankings.\nDegenerate Cases. In the case when none of the random partition produces a tensor which has well-conditioned factor matrices (or alternately, a unique rank-2 decomposition), the instance is a very special instance, that we term degenerate. The additional subroutine (HANDLE-DEGENERATECASE) takes care of such degenerate instances. Before we do so, we introduce some notation to describe these degenerate cases.\nNotation. Define L\u01eb = {ei : Pi \u2265 \u01eb}. If \u01eb not stated explicitly L refers to L\u221a\u01eb where \u01eb is the accuracy required in Theorem 3.1.\nNow we have the following definition that helps us formally define the degenerate case.\nDefinition 7.1 (Bucketing by relative positions). For every \u2113 \u2208 Z, let B\u2113 = {ei \u2208 L : pos\u03c01(ei)\u2212 pos\u03c02(ei) = \u2113}. Further let \u2113\u2217 be the majority bucket for the elements in L.\nWe call a mixture model w1M (\u03c61, \u03c01) \u2295 w2M (\u03c62, \u03c02) as degenerate if except for at most 2 elements, all the elements in L fall into the majority bucket. In other words, |\u2113\u2217| \u2265 |L| \u2212 2. Intuitively, in this case one of the partitions Sa, Sb, Sc constructed by the algorithm will have their corresponding u and v vectors as parallel to each other and hence the tensor method will fail. We show that when this happens, it can be detected and in fact this case provides useful information about the model parameters. More specifically, we show that in a degenerate case, \u03c61 will be almost equal to \u03c62 and the two rankings will be aligned in a couple of very special configurations (see Section 9). Procedure HANDLE-DEGENERATE-CASE is designed to recover the rankings in such scenarios."}, {"heading": "8 Retrieving the Top elements", "text": "Here we show how the first stage of the algorithm i.e. steps (a)-(e.i) manages to recover the top few elements of both rankings \u03c01 and \u03c02 and also estimate the parameters \u03c61, \u03c62, w1, w2 up to accuracy \u01eb. We first show that if Ma,Mb,Mc have non-negligible minimum singular values (at least \u01eb\u20322 as in Lemma 8.1), then the decomposition is unique, and hence we can recover the top few elements and parameters from INFER TOP-K. Otherwise, we show that if this procedure did not work for all O(log n) iterations, we are in the degenerate case (Lemma 9.1 and Lemma 9.6), and handle this separately.\nFor the sake of analysis, we denote by \u03b3min the smallest length of the vectors in the partition i.e. \u03b3min = min\u03c4\u2208{a,b,c}min { \u2016x(\u03c4)\u2016, \u2016y(\u03c4)\u2016 } . Lemma 9.10 shows that with high probability \u03b3min \u2265 \u03c6C log nmin (1\u2212 \u03c6) for some large constant C. The following lemma shows that when Ma,Mb,Mc are well-conditioned, Algorithm TENSORDECOMP finds a decomposition close to the true decomposition up to scaling. This Lemma essentially follows from the guarantees of the Tensor Decomposition algorithm in [25]. It also lets us conclude that \u03c32(M \u2032a), \u03c32(M \u2032 b), \u03c32(M \u2032 c) are all also large enough. Hence, these singular values of the matrices M \u2032a,M \u2032 b,M \u2032 c that we obtain from TENSOR-DECOMP algorithm can be tested to check if this step worked.\nLemma 8.1 (Decomposition guarantees). In the conditions of Theorem 3.1, suppose there exists a partition Sa, Sb, Sc such that the matrices Ma = (x(a); y(a)),Mb = (x(b); y(b)) and Mc = (x(c); y(c)) are well-conditioned i.e. \u03c32(Ma), \u03c32(Mb), \u03c32(Mc) \u2265 \u01eb\u20322, then with high probability, Algorithm TENSORDECOMP finds M \u2032a = (u (a); v(a)),M \u2032b = (u (b); v(b)),M \u2032c = (u\n(c); v(c)) such that\n1. For \u03c4 \u2208 {a, b, c}, we have u(\u03c4) = \u03b1ax(\u03c4) + z(\u03c4)1 and v(\u03c4) = \u03b2ay(\u03c4) + z (\u03c4) 2 where\n\u2016z(\u03c4)1 \u2016, \u2016z (\u03c4) 2 \u2016 \u2264 \u03d18.1(n, \u01eb, \u01eb2, wmin)\n2. \u03c32(M \u2032a) \u2265 \u03b3min(\u01eb\u20322 \u2212 \u03d18.1) (similarly for M \u2032b,M \u2032c).\nwhere \u03d18.1 is a polynomial function \u03d18.1 = min {\u221a \u03d1tensors(n, 1, \u03ba = 1 \u01eb2 , \u01ebsn3/2), \u03b34minwmin 4 } and \u03d1tensors is the error bound attained in Theorem 2.6 of [25].\nProof. Let \u01eb\u2032 = \u03d18.1. The entry-wise sampling error is \u01ebs \u2264 3 logn/ \u221a N . Hence, the rank-2 decomposition for T (abc) is n3/2\u01ebs close in Frobenius norm. We use the algorithm given in [25] to find a rank-2 decomposition of T (abc) that is O(\u01ebs) close in Frobenius norm. Further, the rank1 term u(a) \u2297 u(b) \u2297 u(c) is \u01eb\u20322-close to w1c3(\u03c61)x(a) \u2297 x(b) \u2297 x(c). Let us renormalize so that \u2016u(a)\u2016 = \u2016u(b)\u2016 = \u2016u(c)\u2016 \u2265 w1/3min \u03b3min.\nApplying Lemma 13.1, we see that u(a) = \u03b1ax(a) + z (a) 1 where \u2016z (a) 1 \u2016 \u2264 \u01eb\u2032, and similarly v(a) = \u03b2ay (a) + z (a) 2 where \u2016z2\u2016 \u2264 \u01eb\u2032. Further w 1/3 min \u03b3min\u03c61/4 \u2264 \u03b1a \u2264 1/\u03b3min. Further\n\u03c32 ( \u03b1ax (a);\u03b2ay (a) ) \u2265 min {\u03b1a, \u03b2a} \u03c32(Ma) \u2265 w 1/3 min \u03b3min\u03c61\n4 \u03c32(Ma).\nHence, \u03c32(M \u2032a) \u2265 w 1/3 min \u03b3min\u03c61\u03c32(Ma)/2\u22122\u01eb\u2032, as required. The same proof also works for M \u2032b,M \u2032c.\nInstead of using the enumeration algorithm of [25], the simultaneous eigen-decomposition algorithms in [23] and [26] can also be used. The only difference is that the \u201cfull-rank conditions\u201d involving the Ma,Mb,Mc are checked in advance, using the empirical second moment. Note that TENSOR-DECOMP only relies on elements that have a non-negligible chance of appearing in the first position L: this can lead to large speedup for constant \u03c61, \u03c62 < 1 by restricting to a much smaller tensor.\nLemma 3.2 captures how Algorithm 1 (steps 3 (a - e.i)) performs the first stage using Algorithm 4 and recovers the weights w1, w2 and x, y when the factor matrices Ma,Mb,Mc are well-conditioned.\nIn the proof we show that in this case, for one of the O(log n) random partitions, Lemma 8.1 succeeds and recovers vectors u(a), v(a) which are essentially parallel to x(a) and y(a) respectively (similarly for u(b), u(c), v(b), v(c)). Sorting the entries of u(a) would give the relative ordering among those in Sa of the top few elements of \u03c01. However, to figure out all the top-k elements, we need to figure out the correct scaling of u(a), u(b), u(c) to obtain x(a). This is done by setting up a linear system.\nNow we present the complete proof of the lemmas."}, {"heading": "8.1 Proof of Lemma 3.2: the Full Rank Case", "text": "If such a partition S\u2217a , S \u2217 b , S \u2217 c exists such that \u03c32(Ma) \u2265 \u01eb\u20322, then there exists a 2-by-2 submatrix of Ma corresponding to elements ei1 , ej1 which has \u03c32(\u00b7) \u2265 \u01eb\u20322. Similarly there exists such pairs of elements ei2 , ej2 and ei3 , ej3 in Sb andSc respectively. But with constant probability the random partition Sa, Sb, Sc has ei1 , ej1 \u2208 Sa, ei2 , ej2 \u2208 Sb, ei3 , ej3 \u2208 Sc respectively. Hence in the O(log n) iterations, at least one iteration will produce sets Sa, Sb, Sc such that \u03c32(Ma), \u03c32(Mb), \u03c32(Mc) \u2265 \u01eb\u20322 with high probability. Further, Lemma 8.1 also ensures that \u03c32(M \u2032a), \u03c32(M \u2032 b), \u03c32(M \u2032 c) \u2265 \u01eb2.\nLemma 8.1 recovers vectors u(a), v(a) which are essentially parallel to x(a) and y(a) respectively (similarly for u(b), u(c), v(b), v(c)). While sorting the entries of u(a) would give the relative ordering among those in Sa of the top few elements of \u03c01, we need to figure out the correct scaling of u(a), u(b), u(c) to recover the top few elements of \u03c01.\nFrom Lemma 8.1, we can express\nw1x (a) = \u03b1\u2032au (a) + z (a) 1 where z (a) 1 \u22a5 u(a)where \u2016z (a) 1 \u2016 \u2264 \u03d18.1(n, \u01ebs, \u01eb\u20322).\nSimilarly w2y(a) = \u03b2\u2032av (a) + z (a) 2 , where \u2016z (a) 2 \u2016 \u2264 \u03d18.1. If \u01ebs is the sampling error for each entry in p(a), we have\n\u2016w1x(a) + w2x(b) \u2212 p(a)\u2016 < \u221a n\u01ebs (3)\n\u2016\u03b1\u2032au(a) + \u03b2v(a) \u2212 p(a)\u2016 < \u221a n\u01ebs + 1\n2 w\n1/3 min \u03c61\u03b3min\u03d18.1 (4)\nEq (4) allows us to define a set of linear equations with unknowns \u03b1\u2032a, \u03b2 \u2032 a, constraint matrix given by M \u2032a = (u (a); v(a)). Hence, the error in the values of \u03b1\u2032a, \u03b2 \u2032 a is bounded by the condition number of the system and the error in the values i.e.\n\u01eb\u03b1 \u2264 \u03ba(M \u2032a).w 1/3 min \u03b3min\u03d18.1 \u2264\n( 1\n4 w\n1/3 min \u03c6min\u03b3min\u01eb \u2032 2 \u2212 \u03d18.1\n)\u22121 \u00b7 \u03c6min\n2 w\n1/3 min \u03b3min\u03d18.1.\nThe same holds for \u03b1b, \u03b1c, \u03b2b, \u03b2c.\nAlgorithm 5 REMOVE-COMMON-PREFIX, Input: a set S of N samples from w1M (\u03c6, \u03c01) \u2295 w2M (\u03c6, \u03c02), \u01eb.\n1. Initialize I \u2190 \u2205, S = [n]. 2. for t = 1 to n,\n(a) For each element x \u2208 [n] \\ I , estimate p\u0302x,1 = Pr(x goes to position t). (b) Let xt = argmaxx\u2208[n]\\I p\u0302x,1. (c) If |p\u0302x,1 \u2212 1Zn\u2212t+1 | > \u03d1(\u01eb), return I and QUIT. (d) Else I \u2190 I \u222a xt\n3. Output I .\nHowever, we also know that \u2016x(a)\u20161 + \u2016x(b)\u20161 + \u2016x(c)\u20161 = 1. Hence,\n|\u2016\u03b1au(a)\u20161 + \u2016\u03b1bu(b)\u20161 + \u2016\u03b1cu(c)\u20161 \u2212 w1| \u2264 \u01eb \u2264 3 \u221a n(\u01eb\u03b1 + \u03d18.1).\nThus, w\u03021, w\u03022 are within \u01eb of w1, w2. Hence, we can recover vectors x by concatenating \u03b1a w1 u(a), \u03b1bw1u (b), \u03b1cw1u\n(c) (similarly y). Since we have \u03d18.1 < \u03c61(1 \u2212 \u03c6)/wmin, it is easy to verify that by sorting the entries and taking the ratio of the top two entries, \u03c6\u03021 estimates \u03c61 up to error 2\u03d18.1\u03c61(1\u2212\u03c61)\nwmin (similarly \u03c62). Finally, since we recovered x up to error \u01eb\u2032\u2032 = 2\u03d18.1 wmin\n, we recovered the top m elements of \u03c01 where m \u2264 log\u03c61 (2\u03d18.1(1\u2212 \u03c61)/wmin)."}, {"heading": "9 Degenerate Case", "text": "While we know that we succeed when Ma,Mb,Mc have non-negligible minimum singular value for one of the the O(log n) random partitions, we will now understand when this does not happen. Recollect that L = L\u221a\u01eb = {ei : Pi \u2265 \u221a \u01eb}. For every \u2113 \u2208 Z, let B\u2113 ={\nei \u2208 L : \u03c0\u221211 (i)\u2212 \u03c0\u221212 (i) = \u2113 }\n. Further let \u2113\u2217 be the majority bucket for the elements in L. We call a mixture model w1M (\u03c61, \u03c01)\u2295w2M (\u03c62, \u03c02) as degenerate if the parameters of the two Mallows models are equal, and except for at most 2 elements, all the elements in L fall into the majority bucket. In other words, |\u2113\u2217| \u2265 |L| \u2212 2. We first show that if the tensor method fails, then the parameters of the two models \u03c61 and \u03c62 are essentially the same. Further, we show how the algorithm finds this parameter as well.\nLemma 9.1 (Equal parameters). In the notation of the Algorithm 1, for any \u01eb\u2032 > 0, suppose \u03c32(M \u2032 a) < \u01eb2 \u2264 \u03d19.1(n, \u01eb\u2032, wmin, \u03c61, \u03c62) (or M \u2032b,M \u2032c), then with high probability (1 \u2212 1/n3), we have that |\u03c61 \u2212 \u03c62| \u2264 \u01eb\u2032 and further Algorithm 9 (ESTIMATE-PHI) finds |\u03c6\u0302 \u2212 \u03c61| \u2264 \u01eb\u2032/2. The number of samples needed N > poly(n, 1\u01eb\u2032 ).\nThis lemma is proven algorithmically. We first show that Algorithm 9 finds a good estimate \u03c6\u0302 of \u03c61. However, by the same argument \u03c6\u0302 will also be a good estimate of \u03c62! Since \u03c6\u0302 will be \u01eb\u2032/2-close to both \u03c61 and \u03c62, this will imply that |\u03c61 \u2212 \u03c62| \u2264 \u01eb\u2032 ! We prove this formally in the next section. But first, we first characterize when the tensor T (abc) does not have a unique decomposition \u2014 this characterization of uniqueness of rank-2 tensors will be crucial in establishing that \u03c61 \u2248 \u03c62.\n9.1 Characterizing the Rank and Uniqueness of tensor T (abc) based on Ma,Mb,Mc\nTo establish Lemma 9.1, we need the following simple lemma, which establishes that the conditioning of the matrices output by the Algorithm TensorDecomp is related to the conditioning of the parameter matrices Ma,Mb,Mc. Lemma 9.2 (Rank-2 components). Suppose we have sets of vectors (gi, hi, g\u2032i, h \u2032 i)i=1,2,3 with length at most one (\u2016 \u00b7 \u20162 \u2264 1) such that T = g1 \u2297 g2 \u2297 g3 + h1 \u2297 h2 \u2297 h3 and \u2016T \u2212 g\u20321 \u2297 g\u20322 \u2297 g\u20323 + h\u20321 \u2297 h\u20322 \u2297 h\u20323\u2016 \u2264 \u01ebs\nsuch that matrices have minimum singular value \u03c32 ( g1;h1 ) , \u03c32 ( g2;h2 ) \u2265 \u03bb and \u2016g3\u2016, \u2016h3\u2016 \u2265\n\u03b3min, then we have that for matrices M \u20321 = ( g\u20321;h \u2032 1 ) ,M \u20322 = ( g\u20322;h \u2032 2 )\n\u03c32(M \u2032 1) \u2265 \u03bb2\u03b3min 4n \u2212 \u01ebs and \u03c32(M \u20321) \u2265 \u03bb2\u03b3min 4n \u2212 \u01ebs.\nProof. Let matrices M1 = ( g1;h1 ) ,M2 = ( g2;h2 ) . For a unit vector w (of appropriate dimension) let\nMw = T (\u00b7, \u00b7, w) = \u3008w, g3\u3009g1 \u2297 g2 + \u3008w, h3\u3009h1 \u2297 h2\n= M1DwM T 2 where Dw =\n( \u3008w, g3\u3009 0\n0 \u3008w, h3\u3009\n) .\nBesides, since w is a random gaussian unit vector,Pr|\u3008w, g3\u3009| \u2265 \u2016g3\u2016/4 \u221a n with probability> 1/2.\nHence, using there exists a unit vector w such that min{|\u3008w, g3\u3009|, |\u3008w, h3\u3009|} \u2265 \u03b3min/(4 \u221a n). Hence,\n\u03c32(Mw) \u2265 \u03bb2\u03b3min 4 \u221a n .\nHowever, \u2016Mw \u2212M \u20321D\u2032w(M \u20322)T \u2016F \u2264 \u01ebs where D\u2032w = ( \u3008w, g\u20323\u3009 0\n0 \u3008w, h\u20323\u3009\n) .\nHence, \u03c32 ( M \u20321D \u2032 w(M \u2032 2) T ) \u2265 \u03c32(Mw)\u2212 \u01ebs.\nCombining this with the fact that \u03c32 ( M \u20321D \u2032 w(M \u2032 2) T )\n\u2264 \u03c32(M \u20321)\u03c31(D\u2032w)\u03c31(M \u20322) gives us the claimed bound.\nThis immediately implies the following lemma in the contrapositive.\nLemma 9.3 (Rank-1 components). Suppose \u03c32(M \u2032a) < \u01eb and \u03c32(M \u2032 b) < \u01eb, then two of the matrices\nMa,Mb,Mc have \u03c32(\u00b7) < \u221a\n8\u01ebn \u03b3min , when the number of samples N > poly(n, 1/\u01eb)."}, {"heading": "9.2 Equal Scaling Parameters", "text": "The following simple properties of our random partition will be crucial for our algorithm.\nLemma 9.4. The random partition of [m] into A,B,C satisfies with high probability (at least 1\u2212 exp ( \u2212 1C9.4 \u00b7m ) ):\n1. |A|, |B|, |C| \u2265 m/6\n2. There are many consecutive numbers in each of the three sets A,B,C i.e.\n|{i \u2208 A and i+ 1 \u2208 A}| \u2265 m/100.\nProof. The claimed bounds follow by a simple application of Chernoff Bounds, since each element is chosen in A with probability 1/3 independently at random. The second part follows by considering the m/2 disjoint consecutive pairs of elements, and observing that each pair fall entirely into A with probability 1/9.\nLemma 9.5. Consider a set of indices S \u2286 [n] and let pS be the true probability vector p of a single Mallows model M(\u03c0, \u03c6) restricted to subset S. Suppose the empirical vector \u2016p\u0302S\u2212pS\u2016\u221e < \u01eb1, and there exists consecutive elements of \u03c0 in S i.e. \u2203i such that \u03c0(i), \u03c0(i + 1) \u2208 S, with p(\u03c0(i + 1)) \u2265\u221a \u01eb1. Then, if we arrange the entries of pS in decreasing order as r1, r2, . . . , r|S| we have that\n\u03c6\u0302 = max i:ri+1\u2265 \u221a \u01eb1 ri+1 ri satisfies |\u03c6\u0302\u2212 \u03c6| < 2\u221a\u01eb1.\nProof. By the properties of the Mallows model, the ratio of any two probabilities is a power of \u03c6 i.e.\np\u21132 p\u21131\n= \u03c6\u03c0 \u22121(\u21132)\u2212\u03c0\u22121(\u21131). If p(\u03c0(i + 1)) \u2265 \u221a\u01eb1, we have that\np\u0302(\u03c0(i + 1)) p\u0302(\u03c0(i)) \u2264 \u03c6 \u00b7 p(\u03c0(i)) + \u01eb1 p(\u03c0(i))\u2212 \u01eb1\n\u2264 \u03c6+ \u03c6 (p(\u03c0(i))\u2212 p\u0302(\u03c0(i)) + \u01eb1) p\u0302(\u03c6(i)) \u2264 \u03c6+ \u01eb1 (1 + \u03c6) p\u0302(\u03c6(i)) \u2264 \u03c6+ 2\u221a\u01eb1\nThe same proof holds for the lower bound.\nWe now proceed to showing that the scaling parameters are equal algorithmically.\nProof of Lemma 9.1. We now proceed to prove that \u03c61 \u2248 \u03c62. We note that \u2016T (abc)\u2016F \u2264 1 since the entries of T (abc) correspond to probabilities, and for any vector z, \u2016z\u20162 \u2264 \u2016z\u20161. This implies that all the vectors in the decomposition can be assumed to have \u21132 norm at most 1, without loss of generality. We can first conclude that at least one of the three matrices Ma,Mb,Mc has \u03c32(\u00b7) <\u221a\n8n\u01eb2 \u03b3minwmin . Otherwise, we get a contradiction by applying Lemma 9.2 (contrapositive) to M \u2032a,M \u2032 b and M \u2032a,M \u2032 c. Now, we will show how the algorithm gives an accurate estimate \u03c6\u0302 of \u03c61. However the exact argument applied to \u03c62 will show that \u03c6\u0302 is also a good estimate for \u03c62, implying that \u03c61 \u2248 \u03c62. We have two cases depending on whether one of \u03c32(M \u2032b) and \u03c32(M \u2032 c) are non-negligible or not.\nCase 1: \u03c32(M \u2032b) \u2265 (\u01eb 1/4 2 8n \u03b3minwmin )3/4) and \u03c32(M \u2032c) \u2265 (\u01eb 1/4 2 8n \u03b3minwmin )3/4): Applying Lemma 9.2, we conclude that \u03c32(Mb) \u2265 \u01eb1/22 (8n/\u03b3minwmin)1/2 and \u03c32(Mc) \u2265 \u01eb 1/2 2 ( 8n \u03b3minwmin )1/2. However one of the matrices Ma,Mb,Mc has small \u03c32 value. Hence\n\u03c32(Ma) < \u01eb 1/2 2\n( 8n\n\u03b3minwmin\n)1/2 = \u01eb\u20322 (say).\nLet y(a) = \u03b1x(a) + y\u22a5 where y\u22a5 \u22a5 x(a). Then \u2016y\u22a5\u2016 \u2264 \u01eb\u20322 and \u03b1 \u2265 (\u2016y(a)\u2016\u2212\u01eb\u20322)\n\u2016x(a)\u2016 \u2265 \u03b3min/2. Further, p(a) = (w1 + w2\u03b1)x(a) + w2y\u22a5. Hence,\nx(a) = \u03b2p(a) \u2212 w2\u03b2y\u22a5, where 0 \u2264 \u03b2 < 2\n\u03b3min .\nSince the sampling error is \u01ebs, we have\nx(a) = \u03b2p\u0302(a) + \u03b2(p(a) \u2212 p\u0302(a))\u2212 w2\u03b2y\u22a5\n= \u03b2p\u0302(a) + z where \u2016z\u2016\u221e \u2264 \u03b2(\u01ebs + \u01eb\u20322) \u2264 4\u01eb2 \u03b3min = \u01eb3\nConsider the first m = C9.4 logn elements F of \u03c01.\n\u2200i \u2208 F, xi \u2265 \u03c6 C9.4 logn 1\n1\u2212 \u03c61 \u2265 n\nC9.4 log(1/\u03c61)\n1\u2212 \u03c61 \u2265 \u221a\u01eb3 due to our choice of error parameters\nApplying Lemma 9.4, \u2126(logn) consecutive elements of \u03c01 occur in Sa. Hence applying Lemma 9.5, we see that the estimate \u03c6\u0302 output by the algorithm satisfies |\u03c6\u0302 \u2212 \u03c61| \u2264 2 \u221a \u01eb3 = 16\u01eb 1/4 2 n 1/4\n\u03b3 3/4 min w 1/4 min\n, as\nrequired.\nCase 2: \u03c32(M \u2032b) < (\u01eb 1/4 2 8n \u03b3minwmin )3/4): We also know that \u03c32(M \u2032a) < \u01eb2. Applying Lemma 9.3, we see that two of the three matrices Ma,Mb,Mc have \u03c32(\u00b7) being negligible i.e.\n\u03c32(\u00b7) < \u01eb1/42 ( 8n\n\u03b3minwmin\n)7/8 .\nAlgorithm 6 HANDLE-DEGENERATE, Input: a set S of N samples from w1M (\u03c61, \u03c01) \u2295 w2M (\u03c62, \u03c02), \u03c6\u0302.\n1. \u03c0pfx \u2190 REMOVE-COMMON-PREFIX(S ). Let \u03c0rem1 = \u03c01 \\ \u03c0pfx, \u03c0rem2 = \u03c02 \\ \u03c0pfx. 2. If |\u03c0pfx| = n, then output IDENTICAL MALLOWS MODELS and parameters \u03c6\u0302 and \u03c0pfx. 3. Let M\u2032 be the Mallows mixture obtained by adding three artificial elements e\u22171, e\u22172, e\u22173 to the front. 4. Run steps (1-3) of Algorithm 1 on M\u2032. If SUCCESS, output w\u03021, w\u03022, \u03c01, \u03c02, \u03c6\u0302. 5. If FAIL, let P\u0302 (i), P\u0302 (i, j) be the estimates of P (i), P (i, j) when samples according to M\u2032. 6. Divide elements in L\u221a\u01eb into R \u2264 log(\u01ebZn(\u03c6\u0302))2 log(\u03c6\u0302) disjoint sets\nIr =\n{ i : P\u0302 (i) \u2208 [ \u03c6\u0302r\nZn(\u03c6\u0302) \u2212 \u01eb, \u03c6\u0302\nr\nZn(\u03c6\u0302) + \u01eb\n]}\n.\n7. If |Ir| = 1 set \u03c0rem1 (i) to be the only element in Ir. 8. Let Ibad be the remaining elements in the sets I1 \u222a I2 . . . IR along with L\u221a\u01eb \\ \u22c3 r Ir . If |Ibad| > 4\nor |Ibad| < 2, output FAIL. 9. Let Sa, Sb is any partition of I1 \u222a I2 \u222a IR \\ Ibad.\nFind i1, j1 \u2208 Ibad such that M = ( P\u0302ij )\ni\u2208Sa\u222a{i1},Sb\u222a{j1} has \u03c32(M) \u2265\n\u221a \u01ebn.\n10. For i \u2208 Ibad \\ {i1, j1} and i \u2208 Ir, set \u03c0rem1 (r) = \u03c0rem2 (r) = i. Set \u03c0rem1 (1) = i1, \u03c0 rem 2 (1) = j1, and \u03c0 rem 1 (k) = j1, \u03c0 rem 2 (k) = i1 where k \u2264 R is unfilled\nposition.\n11. Output \u03c01 = \u03c0pfx \u25e6 \u03c0rem1 , \u03c02 = \u03c0pfx \u25e6 \u03c0pfx2 , \u03c6\u0302. Output w\u03021, w\u03022 = 1\u2212 w\u03021, by solving for w\u03021 from P\u0302 (i) = w\u03021\u03c0\u221211 (i) + (1\u2212 w\u03021)\u03c0\u221212 (i).\nUsing the same argument as in the previous case, we see that the estimates given by two of the three partitions Sa, Sb, Sc is 2 \u221a \u01eb3 close to the \u03c61. Hence the median value \u03c6\u0302 of these estimates is also as close. As stated before, applying the same argument for \u03c62 (and \u03c02) , we see that \u03c6\u0302 is 2 \u221a \u01eb3 close to \u03c62 as\nwell. Hence, \u03c61 is 4 \u221a \u01eb3 close to \u03c61."}, {"heading": "9.3 Establishing Degeneracy", "text": "Next, we establish that if none of the O(log n) rounds were successful, then the two central permutations (restricted to the top O(log1/\u03c6min n) positions) are essentially the same shifted by at most a couple of elements.\nLemma 9.6. Consider the large elements L\u221a\u01eb. Suppose |B\u2113\u2217| \u2265 |L\u221a\u01eb| \u2212 3, then the one of the O(log n) rounds of the Tensor Algorithm succeeds with high probability.\nProof. We have two cases depending on whether \u2113\u2217 \u2264 log(\u01eb)/ log(\u03c6) or not. Suppose |\u2113\u2217| \u2264 log(\u01eb(1 \u2212 \u03c6))/ log(\u03c6). Let i, j, k be the indices of elements in L\u221a\u01eb that are not in B\u2113\u2217 . With constant probability the random partition Sa, Sb, Sc puts these three elements in different partitions. In that case, by applying Lemma 9.11 we see that \u03c32(Ma), \u03c32(Mb), \u03c32(Mc) \u2265 \u01eb2(1 \u2212 \u03c6)2. Hence, Lemma 3.2 would have succeeded with high probability.\nSuppose |\u2113\u2217| > log(\u01eb(1 \u2212 \u03c6))/ log(\u03c6). Assume without loss of generality that \u2113\u2217 \u2265 0. Consider the first three elements of \u03c02. They can not belong to B\u2113\u2217 since \u2113\u2217 > 3. Hence, by pairing each of these elements with some three elements of B\u2113\u2217, and repeating the previous argument we get that \u03c32(Ma), \u03c32(Mb), \u03c32(Mc) \u2265 \u01eb2(1 \u2212 \u03c6)2 in one of the iterations w.h.p. Hence, Lemma 3.2 would have succeeded with high probability.\nHence we now have two kinds of degenerate cases to deal with. The next two lemmas show how such cases are handled.\nLemma 9.7 (Staggered degenerate case). Suppose \u03c6 = \u03c61 = \u03c62, and at most two of the top elements L\u221a\u01eb are not in bucket B\u2113\u2217 i.e. B\u2113\u2217 \u2265 L\u221a\u01eb \u2212 2 with \u2113\u2217 6= 0. Then, for any \u01eb > 0, given N > poly(n, \u03c6, \u01eb, wmin) samples, step (3-4) of Algorithm HANDLE-DEGENERATE finds finds w\u03021, w\u03022 of w1, w2 up to \u01eb accuracy and the top m elements of \u03c01, \u03c02 respectively where m = logZn(\u01eb) 2 log \u03c6 .\nProof. Since \u03c61 = \u03c62, we can use Lemma 6.3, we can sample from a Mallows mixture where we add one new element e\u22173 to the front of both permutations \u03c01, \u03c02. Doing this two more times we can sample from a Mallows mixture where we add e\u22171, e \u2217 2, e \u2217 3 to the front of both permutations. Let these new concatenated permutations be \u03c0\u22171 , \u03c0 \u2217 2 . Since the majority bucket corresponds to \u2113\n\u2217 6= 0, we have at least three pairs of elements which satisfy Lemma 9.11, we see that w.h.p. in one of the O(log n) iterations, the partitions Sa, Sb, Sc have \u03c32(\u00b7) \u2265 (\u03b3min\u03c66)2(1 \u2212 \u03c6)3. Hence, by using the Tensor algorithm with guarantees from Lemma 3.2, and using Algorithm RECOVER-REST, we get the full rankings as required (using Lemma 10.2).\nLemma 9.8 (Aligned Degenerate case). Suppose \u03c6 = \u03c61 = \u03c62, and at most two of the top elements L\u221a\u01eb are not in bucket B0 i.e. |B0| \u2265 |L\u221a\u01eb|\u22122. For any \u01eb > 0, given N = O( n\n2 logn \u01eb8w2min(1\u2212\u03c6)4 ) samples,\nsteps (5-10) of Algorithm 6 (HANDLE-DEGENERATE) finds estimates w\u03021, w\u03022 up to \u01eb accuracy and prefixes \u03c0\u20321, \u03c0 \u2032 2 of \u03c01, \u03c02 respectively that contain at least the top m elements where m = logZn(\u01eb) 2 log\u03c6 .\nProof. The first position differs because of step(1-2) of Algorithm 6. Without loss of generality \u03c01 = \u03c0 pfx 1 and \u03c02 = \u03c0 pfx 2 . B0 \u2265 m \u2212 2, hence |B0| = m \u2212 2. Let ei1 , ej1 be the other two elements in L\u221a\u01eb.\nFor elements ei \u2208 B0, \u03c0\u221211 (i) = \u03c0\u221212 (i). The sampling error in the entries of P\u0302 is at most \u01ebs = \u01eb4wmin(1 \u2212 \u03c6)2/n. Hence, they fall into the set I\u03c0\u221211 (i). Therefore, there can be at most four sets with at most four elements between them that constitute Ibad.\nConsider M = (P\u0302ij)i\u2208Sa\u222a{i1},j\u2208Sb\u222a{j1}. Also let Ma = (x (a); y(a)) and Mb = (x(a); y(a)) applied to M\u2032. By Lemma 9.11, we see that \u03c32(Ma), \u03c32(Mb) \u2265 \u01eb(1\u2212 \u03c6). Further,\n\u2016M \u2212Ma ( w1 0 0 w2 ) MTb \u2016F \u2264 \u01ebsn.\nHence, \u03c32(M) \u2265 \u01eb2(1\u2212 \u03c6)2wmin. If i1, j1 do not belong to the two different partitions Sa, Sb, it is easy to see that \u03c32(M) \u2264 \u221a \u01ebs > \u01eb\n2wmin(1 \u2212 \u03c6)2. Hence, we identify the two irregular elements that are not in bucket B0, and use this to figure out the rest of the permutations.\nFinally, the following lemma shows how the degenerate cases are handled.\nLemma 9.9. For 0 < \u01eb, given \u03c61, \u03c62 with |\u03c61 \u2212 \u03c62| \u2264 \u01eb1 = \u03d19.9(n, \u03c6, \u01eb, wmin), such that at most two elements of L\u221a\u01eb are not in the bucket B\u2113\u2217 , then Algorithm HANDLE-DEGENERATE finds w.h.p. estimates w\u03021, w\u03022 of w1, w2 up to \u01eb accuracy, and recovers \u03c01, \u03c02.\nProof. We can just consider the case \u03c6\u0302 = \u03c6\u03021 = \u03c62 using Lemma 2.6 as long as \u01eb1 < \u03c6\nn2N(n,\u03c6,\u01eb)2 , where N is the number of samples used by Lemma 9.8 and Lemma 9.7 to recover the rest of the permutations and parameters up to error \u01eb. This is because the simulation oracle does not fail on any of the samples w.h.p, by a simple union bound.\nIf the two permutations do not differ at all, then by Lemma 10.6, Algorithm 5 returns the whole permutation \u03c01 = \u03c02. Further, any set of weights can be used since both are identical models (\u03c61 = \u03c62 = \u03c6).\nLet m = L\u221a\u01eb. In the remaining mixture M\u2032, the first position of the two permutations differ: hence, B\u2113\u2217 < m. Further, we know that B\u2113\u2217 \u2265 m\u2212 2. We have two cases, depending on whether the majority bucket B\u2113\u2217 corresponds to \u2113\u2217 = 0 or \u2113\u2217 6= 0. In the first case, Lemma 9.7 shows that we find the permutations \u03c01, \u03c02 and parameters up to\naccuracy \u01eb. If this FAILS, we are in the case \u2113\u2217 = 0, and hence Lemma 9.8 shows that we find the permutations \u03c01, \u03c02 and parameters up to accuracy \u01eb."}, {"heading": "9.4 Auxiliary Lemmas for Degenerate Case", "text": "Lemma 9.10. For any Mallows model with parameters \u03c61, \u03c62 has\n\u03b3min = min \u03c4\u2208{a,b,c}\nmin { \u2016x(\u03c4)\u2016, \u2016y(\u03c4)\u2016 } \u2265 min { \u03c62C logn1 (1\u2212 \u03c61), \u03c62C logn2 (1 \u2212 \u03c62) } with probability 1\u2212nC\nProof. Consider a partition A, and the top m \u2265 2C logn elements according to \u03c0. The probability that none of the them belong to A is at most 1/nC . This easily gives the required conclusion.\nLemma 9.11. When \u03c61 = \u03c62 = \u03c6, if two large elements ei, ej \u2208 L\u221a\u01eb belonging to different buckets B\u21131 and B\u21132 respectively with max {|\u21131|, |\u21132|} \u2264 log(\u01eb)log(\u03c6) . Suppose further that these elements are in the partition Sa. Then the corresponding matrix Ma has \u03c32(Ma) \u2265 \u01eb2(1\u2212 \u03c6) when \u03c61 = \u03c62 = \u03c6.\nProof. Consider the submatrix\nM = ( xi yi xj yj ) = xi ( 1 \u03c6\u21131 \u03c6\u03c0 \u22121 1 (i)\u2212\u03c0 \u22121 1 (j) \u03c6\u03c0 \u22121 1 (i)\u2212\u03c0 \u22121 1 (j) \u00b7 \u03c6\u21132 ) .\nUsing a simple determinant bound, it is easy to see that\n\u03c31(M)\u03c32(M) \u2265 max {xi, yi}max {xj , yj}\u00b7(\u03c6|\u21131|\u2212\u03c6|\u21132|) \u2265 max {xi, yi}\u00b7\u01eb\u03c6min{|\u21131|,|\u21132|}(1\u2212\u03c6).\nSince \u03c31(M) \u2264 4maxxi, yi, we see that \u03c32(M) \u2265 \u01eb 2(1\u2212\u03c6)\n4 ."}, {"heading": "10 Recovering the complete rankings", "text": "Let f (1) (i \u2192 j) be the probability that element ei goes to position j according to Mallows Model M1 (and similarly f (2) (i \u2192 j) for model M2). To find the complete rankings, we measure appropriate statistics to set up a system of linear equations to calculate f (1) (i \u2192 j) and f (2) (i \u2192 j) up to inverse polynomial accuracy. The largest of these values { f (1) (i \u2192 j) } corresponds to the position of ei in the central ranking of M (,1). To compute these values { f (r) (i \u2192 j) } r=1,2\nwe consider statistics of the form \u201cwhat is the probability that ei goes to position j conditioned on ei\u2217 going to the first position?\u201d. This statistic is related to f (1) (i \u2192 j) , f (2) (i \u2192 j) for element ei\u2217 that is much closer than ei to the front of one of the permutations.\nNotation: Let fM (i \u2192 j) be the probability that element ei goes to position j according to Mallows Model M, and let f (r) (i \u2192 j) be the same probability for the Mallows model Mr (r \u2208 {1, 2}). Let f (1) (i \u2192 j|ei\u2217 \u2192 1) be the probability that ei goes to the jth position conditioned on the element ei\u2217 going to the first position according to M1 (similarly M (,2)). Finally for any Mallows model M (\u03c6, \u03c0), and any element ei\u2217 \u2208 pi, let M\u2212i\u2217 represent the Mallows model on n \u2212 1 elements M (\u03c6, \u03c0 \u2212 i\u2217). In the notation defined above, we have that for any elements ei\u2217 , ei and position j, we have\nPr (ei \u2192 j|ei\u2217 \u2192 1) = w\u20321f (1) (i \u2192 j|ei\u2217 \u2192 1) + w\u20322f (2) (i \u2192 j|ei\u2217 \u2192 1) where w\u20321 =\nw1xi\u2217 w1xi\u2217 + w2yi\u2217 , w\u20322 = 1\u2212 w\u20321\nHowever, these statistics are not in terms of the unknown variables f (1) (i \u2192 j) , f (2) (i \u2192 j). The following lemma shows that these statistics are almost linear equations in the unknowns f (1) (i \u2192 j) , f (2) (i \u2192 j) for the i, j pairs that we care about. For threshold \u03b4, let r1 be the smallest number r such that \u03b4 > \u03c6r\u221211 /Zn(\u03c61). Similarly let r2 be the corresponding number for second Mallows models M2.\nAlgorithm 7 RECOVER-REST, Input: a set S of N samples from w1M (\u03c61, \u03c01)\u2295w2M (\u03c62, \u03c02), w\u03021, w\u03022, \u03c6\u03021, \u03c6\u03022, \u03c0\u03021, \u03c0\u03022, \u01eb.\n1. Let |\u03c0\u03021| = r1, |\u03c0\u03022| = r2 and let r1 \u2265 r2 w.l.o.g. (the other case is the symmetric analog). 2. For any element ei, define f\u0302 (1) (i \u2192 1) = \u03c6\u03021 (\u03c0\u03021\u22121(ei)\u22121)\nZn(\u03c6\u03021) , and f\u0302 (2) (i \u2192 1) = \u03c6\u03022( \u03c0\u03022 \u22121(ei)\u22121) Zn(\u03c6\u03022) .\nIf ei does not appear in \u03c0\u03021 set f\u0302 (1) (i \u2192 1) = 0. Similarly, if ei does not appear in \u03c0\u03022 set f\u0302 (2) (i \u2192 1) = 0. Define g(n, \u03c6) = C. n2\u03c62\n(1\u2212\u03c6)2 log n, where C is an absolute constant.\n3. For each ei \u2208 \u03c0\u03021(1 : r1/2)\n(a) If f\u0302 (2) (i \u2192 1) < min{w\u03021,w\u03022} 16 f\u0302(1)(i\u21921) n2g(n,\u03c6\u03021)\ni. \u03c0\u03021 \u2190 LEARN-SINGLE-MALLOW(Sei7\u21921). Here Sei 7\u21921 refers to the samples in S where ei goes to position 1.\nii. \u03c0\u03022 \u2190 FIND-PI(S , \u03c0\u03021 , w\u03021, w\u03022 , \u03c6\u03021 , \u03c6\u03022). Output SUCCESS and return \u03c0\u03021 and \u03c0\u03022, w\u03021, w\u03022, \u03c6\u03021 and \u03c6\u03022.\n4. Do similar check for each ei \u2208 \u03c0\u03022(1 : r2/2). 5. Let ei\u2217 be the first element in \u03c0\u03021 such that |f\u0302 (1) (i\u2217 \u2192 1) \u2212 f\u0302 (2) (i\u2217 \u2192 1) | > \u01eb. Define w\u0302\u20321 =\n1\n1+ w\u03022 w\u03021\nf\u0302(2)(i\u2217\u21921) f\u0302(1)(i\u2217\u21921)\nand w\u0302\u20322 = 1\u2212 w\u0302\u20321.\n6. For each ei /\u2208 \u03c0\u03021 and j > r1 (a) Estimate f\u0302 (i \u2192 j) = Pr[ei goes to position j] and f\u0302 (i \u2192 j|ei\u2217 \u2192 1) =\nPr[ei goes to position j|ei\u2217 7\u2192 1]. (b) Solve the system\nf\u0302 (i \u2192 j) = w\u03021f\u0302 (1) (i \u2192 j) + w\u03022f\u0302 (2) (i \u2192 j) (5) f\u0302 (i \u2192 j|ei\u2217 \u2192 1) = w\u0302\u20321f\u0302 (1) (i \u2192 j) + w\u0302\u20322f\u0302 (2) (i \u2192 j) (6)\n7. Form the ranking \u03c0\u03021 = \u03c0\u03021 \u25e6 \u03c0\u20321 s.t. for each ei /\u2208 \u03c0\u03021, pos(ei) = argmaxj>r1 f\u0302 (1) (i \u2192 j). 8. \u03c0\u03022 \u2190FIND-PI(S , \u03c0\u03021, w\u03021 , w\u03022 , \u03c6\u03021, \u03c6\u03022, \u01eb). Output SUCCESS and return \u03c0\u03021 and \u03c0\u03022, w\u03021, w\u03022, \u03c6\u03021 and\n\u03c6\u03022.\nAlgorithm 8 LEARN-SINGLE-MALLOW, Input: a set S of N samples from M(\u03c6, \u03c0).\n1. For each element ei, estimate f\u0302 (1) (i \u2192 j) = Pr[ei goes to position j]. 2. Output a ranking \u03c0\u0302 such that for all ei, pos(ei) = argmaxj f\u0302 (1) (i \u2192 j).\nLemma 10.1. For any j > r1, any elements ei\u2217 , ei with pos\u03c01(i \u2217) > r1, pos\u03c01(i) > pos\u03c01(i \u2217), we have in the notation defined above that\nf (1) (i \u2192 j|ei\u2217 \u2192 1) = f (1) (i \u2192 j) + \u03b4\u2032 where |\u03b4\u2032| \u2264 \u03b4n. The corresponding statement also holds for Mallows model M2.\nProof. When samples are generated according to Mallows model M1, we have for these sets of i, i\u2217, j that the conditional probability f (1) (i \u2192 j|ei\u2217 \u2192 1) = fM(\u03c61,\u03c01\u2212i\u2217) (i \u2192 j \u2212 1), where the term on the right is a Mallows model over n\u2212 1 elements.\nf (1) (i \u2192 j) = n\u2211\ni\u2032=1\nPr (ei\u2032 \u2192 1) f (1) (i \u2192 j|ei\u2217 \u2192 1) \u2264 r1\u2211\ni\u2032=1\nPr (ei\u2032 \u2192 1) fM(\u03c61,\u03c01\u2212i\u2032) (i \u2192 j \u2212 1) + \u03b4\n= fM(\u03c61,\u03c01\u2212i\u2217) (i \u2192 j \u2212 1) r1\u2211\ni\u2032=1\nPr (ei\u2032 \u2192 1) .\nThe last equality is because the probability is independent of i\u2032 (since pos\u03c01(ei) > pos\u03c01(ei\u2217)). Hence, it follows easily that\nf (1) (i \u2192 j|ei\u2217 \u2192 1) (1\u2212 \u03b4) \u2264 f (1) (i \u2192 j) \u2264 f (1) (i \u2192 j|ei\u2217 \u2192 1) + \u03b4.\nAlgorithm 9 ESTIMATE-PHI, Input: P\u0302 .\n1. Sort P in decreasing order. Return mini{Pi+1Pi }.\nAlgorithm 10 FIND-PI, Input: a set S of N elements from w\u03021M ( \u03c6\u03021, \u03c01 ) \u2295 w\u03022M\u0302 ( \u03c6\u03022, \u03c02 ) , \u03c0\u03021,\nw\u03021, w\u03022, \u03c6\u03021, \u03c6\u03022.\n1. Compute f\u0302 (1) (i \u2192 j) = Pr (ei goes to position j|\u03c0\u03021) (see Lemma 10.8). 2. For each element ei, estimate f\u0302ei,j = Pr (ei goes to position j). 3. Solve for f\u0302 (1) (i \u2192 j) using the equation f\u0302ei,j = w\u03021f\u0302 (1) (i \u2192 j) + w\u03022f\u0302 (2) (i \u2192 j). 4. Output \u03c0\u03022 such that for each ei, pos(ei) = argmaxj f\u0302 (2) (i \u2192 j).\nHence, by picking an appropriate element ei\u2217 , we can set up a system of linear equations and solves for the quantities { f (1) (i \u2192 j) , f (2) (i \u2192 j) } . Suppose there exists an element ei\u2217 that occurs in the top few positions in both the permutations, then that element would suffice for our purpose. On the other hand, if we condition on an element i\u2217 which occurs near the top in one permutation but far away in the other permutation, gives us a single Mallows model. The sub-routine RECOVERREST of the main algorithm figures out which of the cases we are in, and succeeds in recovering the entire permutations \u03c01 and \u03c02 in the case that w1M (\u03c61, \u03c01)\u2295w2M (\u03c62, \u03c02) is non-degenerate (the degenerate cases have been handled separately in the previous section). In such a scenario, from the guarantee of Lemma 3.2 we can assume that we have parameters {w\u03021, w\u03022, \u03c6\u03021, \u03c6\u03022} which are within \u01eb \u2264 \u01eb0 of the true parameters. For the rest of this section we will assume that RECOVER-REST and every sub-routine it uses has access to samples from w\u03021M ( \u03c6\u03021, \u03c01 ) \u2295 w\u03022M\u0302 ( \u03c6\u03022, \u03c02 ) . This is\nw.l.o.g. due to Lemma 2.6.\nThe rankings \u03c0\u03021 and \u03c0\u03022 are obtained from INFER-TOP-K. Define \u03b3 = (1\u2212\u03c6max)2 4n\u03c6max . By our choice\nof \u01eb0, rankings |\u03c0\u03021| = r1 \u2265 log1/\u03c61 ( n10Zn(\u03c61) w2min\u03b3 2 ) and |\u03c0\u03022| = r2 \u2265 log1/\u03c62 ( n10Zn(\u03c62) w2min\u03b3 2 ) . We note that the values f (1) (i \u2192 j) , f (2) (i \u2192 j) in the following Lemma are defined with respect to w\u03021M ( \u03c6\u03021, \u03c01 ) \u2295 w\u03022M\u0302 ( \u03c6\u03022, \u03c02 ) .\nLemma 10.2. Given access to an oracle for M\u0302 and rankings \u03c0\u03021 and \u03c0\u03022 which agree with \u03c01 and \u03c02 in the first r1 and r2 elements respectively, where r1 \u2265 log1/\u03c61 ( n10Zn(\u03c61)\nw2min\u03b3 2\n) and r2 \u2265\nlog1/\u03c62\n( n10Zn(\u03c62)\nw2min\u03b3 2\n) , then procedure RECOVER-REST with \u01eb = 110\u03b3, outputs the rankings \u03c01 and\n\u03c02 with high probability.\nProof. First suppose that the condition in Step 2 of Recover-Rest is true for some ei\u2217 . This would imply that f (2) (i\u2217 \u2192 1) < w\u03021w\u03022 f(1)(i\u2217\u21921) n2g(n,\u03c6\u03021) . Hence, conditioned on ei\u2217 going to the first position, the new weight w\u20321 would be 1\n1+ w\u03022 w\u03021\nf(2)(i\u2217\u21921) f(1)(i\u2217\u21921)\n\u2265 1 \u2212 1 ng(n,\u03c6\u03021) . Since, g(n, \u03c6\u03021) is an upper bound on the\nsample complexity of learning a single Mallows model with parameter \u03c6\u03021, with high probability we will only see samples from \u03c01 and from the guarantees of Lemma 10.7 and Lemma 10.8, we will recover both the permutations. A similar analysis is also true for step 4 of RECOVER-REST. If none of the above conditions happen, then step 5 will succeed because of the guarantee from Lemma 3.2.\nNext we will argue about the correctness of the linear equations in step 6. We have set a threshold \u03b4 = wmin\u03b3 2\nn4 , from Lemma 10.1, we know that the linear equations are correct up to error \u03b4. Once we have obtained good estimates for f (1) (i \u2192 j) for all ei and j > r, Lemma 10.3 implies that\nstep 7 of RECOVER-REST will give us the correct ranking \u03c01. This combined with Lemma 10.8 will recover both the rankings with high probability.\nWe now present the Lemmas needed in the proof of the previous Lemma 10.2.\nLemma 10.3. Consider a length n Mallows model with parameter \u03c6. Consider an element ei and let pos(ei) = j. Let f (i \u2192 k) = Pr[ei 7\u2192 k]. Then we have\n1. f (i \u2192 k) is maximum at k = j.\n2. For all k > j, f (i \u2192 k \u2212 1) \u2265 f (i \u2192 k) (1 + gain(\u03c6)).\n3. For all k < j, f (i \u2192 k) \u2265 f (i \u2192 k \u2212 1) (1 + gain(\u03c6)).\nHere gain(\u03c6) = (1\u2212\u03c6)4\u03c6 min( 1 n , 1\u2212 \u03c62).\nProof. The case j = 1 is easy. Let j > 1 and consider the case k > j. Let Sk = {\u03c0 : pos\u03c0(ei) = k}. Similarly let Sk\u22121 = {\u03c0 : pos\u03c0(ei) = k \u2212 1}. For a set U of rankings, let p(U) = Pr[\u03c0 \u2208 U ]. Notice that f (i \u2192 k \u2212 1) = p(Sk\u22121) and f (i \u2192 k) = p(Sk). Let X = {ej : pos\u03c0\u2217(ej) > pos\u03c0\u2217(ei)} and Y = {ej : pos\u03c0\u2217(ej) < pos\u03c0\u2217(ei)}. We will divide Sk into 4 subsets depending on the elements \u03c41 and \u03c42 which appear in positions (k \u2212 1) and (k \u2212 2) respectively. In each case we will also present a bijection to the rankings in Sk\u22121.\n\u2022 Sk,1 = {\u03c0 \u2208 Sk : \u03c41, \u03c42 \u2208 X}. For each such ranking in Sk we form a ranking in Sk\u22121 by swapping ei and \u03c41. Call the corresponding subset of Sk\u22121 as Sk\u22121,1.\n\u2022 Sk,2 = {\u03c0 \u2208 Sk : \u03c41 \u2208 X, \u03c42 \u2208 Y }. For each such ranking in Sk we form a ranking in Sk\u22121 by swapping ei and \u03c41. Call the corresponding subset of Sk\u22121 as Sk\u22121,2.\n\u2022 Sk,3 = {\u03c0 \u2208 Sk : \u03c41 \u2208 Y, \u03c42 \u2208 X}. For each such ranking in Sk we form a ranking in Sk\u22121 by swapping ei and \u03c41. Call the corresponding subset of Sk\u22121 as Sk\u22121,3.\n\u2022 Sk,4 = {\u03c0 \u2208 Sk : \u03c41, \u03c42 \u2208 Y }. Consider a particular ranking \u03c0 in Sk,4. Notice that since ei is not in it\u2019s intended position there must exist at least one element x \u2208 X such that pos\u03c0(x) < pos\u03c0(ei) in Sk. Let x\u2217 be such an element with the largest value of pos\u03c0(x). Let y \u2208 Y be the element in the position pos\u03c0(x\u2217) + 1. For each such ranking in Sk we form a ranking in Sk\u22121 by swapping ei and \u03c41 and x\u2217 and y. Call the corresponding subset of Sk\u22121 as Sk\u22121,4.\nIt is easy to see that the above construction gives a bijection from Sk to Sk\u22121. We also have the following\n\u2022 p(Sk\u22121,1) = 1\u03c6p(Sk,1). This is because the swap is decreasing the number of inversions by exactly 1.\n\u2022 p(Sk\u22121,2) = 1\u03c6p(Sk,2). This is because the swap is decreasing the number of inversions by exactly 1. p(Sk\u22121,3) = \u03c6p(Sk,3). This is because the swap is increasing the number of inversions by exactly 1. p(Sk\u22121,4) = p(Sk,4). This is because the two swaps maintain the number of inversions.\nAlso note that there is a bijection between Sk,2 and Sk,3 such that every ranking in Sk,3 has one more inversion than the corresponding ranking in Sk,2. Hence we have p(Sk,3) = \u03c6p(Sk,2).\nNow we have\nf (i \u2192 k \u2212 1) = \u2211\ni\np(Sk\u22121,i) (7)\n= 1\n\u03c6 p(Sk,1) +\n1 \u03c6 p(Sk,2) + \u03c6p(Sk,3) + p(Sk,4) (8)\n= f (i \u2192 k) + p(Sk,1)( 1\n\u03c6 \u2212 1) + p(Sk,2)(\n1 \u03c6 \u2212 1)\u2212 p(Sk,3)(1 \u2212 \u03c6) (9)\n(10)\nIf p(Sk,1) \u2265 14p(Sk) or p(Sk,2) \u2265 14p(Sk), then gain(\u03c6) \u2265 (1\u2212\u03c6) 4\u03c6 (1 \u2212 \u03c62). If not, then we have p(Sk,4) \u2265 1/4. Divide Sk,4 as \u222ajSk,4,j where Sk,4,j = {\u03c0 \u2208 Sk,4 : pos\u03c0(x\u2217) = j}. It is easy to see that p(Sk,4,j) = \u03c6(Sk,4,j\u22121). Hence we have p(Sk,2) > p(Sk,3) > 1np(Sk,4) \u2265 14n . In this case we will have gain(\u03c6) \u2265 (1\u2212\u03c6)4n\u03c6 (1 \u2212 \u03c62).\nThe case k < j is symmetric.\nLemma 10.4. Consider a length n Mallows model with parameter \u03c6. Let the target ranking be \u03c0\u2217 = (e1, e2, . . . , en). Let f (1) (i \u2192 j) be the probability that the element at position i goes to position j. We have for all i, j\nf (1) (i \u2192 j) = f (1) (j \u2192 i)\nProof. We will prove the statement by induction on n. For n = 1, 2, the statement is true for all \u03c6. Now assume it is true for all n \u2264 l \u2212 1. Consider a length l Mallows model. We have f (1) (i \u2192 j) = \u2211\nk\u2264i f (1) (i\u2212 1 \u2192 j \u2212 1|ek \u2192 1)Pr(ek 7\u2192 1) +\n\u2211\nj\u2265k>i f (1) (i \u2192 j \u2212 1|ek \u2192 1)Pr(ek 7\u2192 1)\n+ \u2211\nk>j\nf (1) (i \u2192 j|ek \u2192 1)Pr(ek 7\u2192 1)\n= \u2211\nk\u2264i f (1) (j \u2212 1 \u2192 i\u2212 1|ek \u2192 1)Pr(ek 7\u2192 1) +\n\u2211\nj\u2265k>i f (1) (j \u2212 1 \u2192 i|ek \u2192 1)Pr(ek 7\u2192 1)\n+ \u2211\nk>j\nf (1) (j \u2192 i|ek \u2192 1)Pr(ek 7\u2192 1)\n=f (1) (j \u2192 i)\nLemma 10.5. Consider a length n Mallows model with parameter \u03c6. Let the target ranking be \u03c0\u2217 = (e1, e2, . . . , en). Consider a position i which has element ei.\n1. f (j \u2192 i) is maximum at j = i.\n2. For all k > i, f (k \u2212 1 \u2192 i) \u2265 f (k \u2192 i) (1 + gain(\u03c6)).\n3. For all k < i, f (k \u2192 i) \u2265 f (k \u2212 1 \u2192 i) (1 + gain(\u03c6)).\nHere gain(\u03c6) = (1\u2212\u03c6)4\u03c6 min( 1 n , 1\u2212 \u03c62).\nProof. Follows from Lemmas 10.3 and 10.4.\nLemma 10.6. Given access to m = O( 1gain(\u03c6)2 log( n \u03b4 )) samples w1M (\u03c61, \u03c01) \u2295 w2M (\u03c62, \u03c02), with \u03c61 = \u03c62, procedure REMOVE-COMMON-PREFIX with \u01eb = 110gain(\u03c6), succeeds with probability 1\u2212 \u03b4.\nProof. If the two permutations have the same first element e1, then we have x1 = 1/Zn(\u03c6). Since m is large enough, all our estimates will be correct up to multiplicative error of \u221a 1 + gain(\u03c6). By induction, assume that the two permutations have the same prefix till t \u2212 1. By the property of\nthe Mallows model, we know that the remaining permutations are also a mixture of two Mallows models with the same weight. Hence, at step t, if we estimate each probability within multiplicative factor of \u221a 1 + gain(\u03c6), we will succeed with high probability.\nLemma 10.7. Given access to m = O( 1gain(\u03c6)2 log( n \u03b4 )) samples from a Mallows model M(\u03c6, \u03c0), procedure LEARN-SINGLE-MALLOW with \u01eb = 110gain(\u03c6), succeeds with probability 1\u2212 \u03b4.\nProof. In order to learn, it is enough to estimate f (i \u2192 j) = Pr[ei goes to position j] for every element ei and position j. Having done that we can simply assign pos(ei) = argmaxj f (i \u2192 j). From Lemma 10.3 we know that this probability is maximum at the true location of ei and hence is at least 1/n. Hence, it is enough to estimate all f (i \u2192 j) which are larger than 1/n up to multiplicative error of \u221a 1 + gain(\u03c6). By standard Chernoff bounds, it is enough to sample O( 1gain(\u03c6)2 log( n \u03b4 )) from the oracle for M(\u03c6, \u03c0). Lemma 10.8. Given the parameters of a mixture model w1M (\u03c61, \u03c01) \u2295 w2M (\u03c62, \u03c02) and one of the permutations \u03c01, procedure Find-Pi with \u01eb = wmin\u03b3 10 , succeeds with probability 1 \u2212 \u03b4. Here \u03b3 = min(gain(\u03c61), gain(\u03c62)).\nProof. For any element ei and position j, we have that\nf (i \u2192 j) = w1f (1) (i \u2192 j) + w2f (2) (i \u2192 j) . (11) Here f (1) (i \u2192 j) is the probability that element ei goes to position j in M(\u03c61, \u03c01). Similarly, f (2) (i \u2192 j) is the probability that element ei goes to position j in M(\u03c62, \u03c02). We can compute f (1) (i \u2192 j) = f (1)(n,j,i) using dynamic programming via the following relation\nf (1) (n,1,i) = \u03c6 i\u22121 1 /Zn(\u03c61)\nf (1) (n,l,i) = 1 Zn(\u03c61)\n    i\u22121\u2211\nj=1\n\u03c6j\u221211   f (1)(n\u22121,l\u22121,i\u22121) +   n\u2211\nj=i+1\n\u03c6j\u221211   f (1)(n\u22121,l\u22121,1)  \nHere f (1)(n,l,i) is the probability that the element at the ith position goes to position l in a length n Mallows model. Notice that this probability is independent of the underlying permutation \u03c0. Having computed f (1) (i \u2192 j) using the above formula, we can solve Equation 11 to get f (2) (i \u2192 j) to accuracy \u221a1 + wmin\u03b3 and figure out \u03c02. The total number of samples required will be O( 1\n\u03b32w2min log(n\u03b4 ))."}, {"heading": "11 Wrapping up the Proof", "text": "Proof of Theorem 3.1. Let \u01ebs be the entry-wise error in P from the estimates. From Lemma 13.3, \u01ebs < 3 logn/ \u221a N . We aim to estimate each of the parameters \u03c61, \u03c62w1, w2 up to error at most \u01eb. Let for convenience, \u03b3 = (1\u2212\u03c6max) 2\n4n\u03c6max .\nLet \u01eb = min {\u01eb, \u01eb0}. Let \u01eb3 = \u03d19.9(n, \u03c6min, \u01eb). Let us also set \u01eb2 = \u03d19.1(n, \u01eb3, \u03c6min, wmin). Let \u01eb\u20322 be a parameter chosen large enough such that \u01eb\u20322 \u2265 \u01eb2\u03b3min + \u03d18.1, and \u01eb \u2264 \u03d13.2(n, \u01eb \u2032 2, \u01ebs, wmin, \u03c6min).\nIn the non-degenerate case, suppose there is a partition such that \u03c32(Ma), \u03c32(Mb), \u03c32(Mc) \u2265 \u01eb\u20322, Lemma 8.1 guarantees that \u03c32(M \u2032a), \u03c32(M \u2032 b), \u03c32(M \u2032 c) \u2265 \u01eb2. In this case, Lemma 3.2 ensures that one of the O(log n) rounds of the algorithm succeeds and we get the parameters w1, w2, \u03c61, \u03c62 within an error \u01eb using Lemma 3.2. Further, Lemma 3.2 will also find the top r, s elements of \u03c01 and \u03c02 respectively where r = log1/\u03c61 ( n10 \u03b32wmin ) and s = log1/\u03c62 ( n10 \u03b32wmin ) . We will then appeal to Lemma 10.2 (along with Lemma 2.6) to recover the entire rankings \u03c01, \u03c02.\nLemma 2.6 implies that the total variation distance between distributions of w1M (\u03c61, \u03c01) \u2295 w2M (\u03c62, \u03c02) and w\u03021M ( \u03c6\u03021, \u03c01 ) \u2295 w\u03022M\u0302 ( \u03c6\u03022, \u03c02 ) is at most \u01ebn 2\n\u03c6min . Since \u01eb \u2264 \u01eb0, this vari-\nation distance is at most \u03c6min 10n3S(wmin/2, \u221a \u03c6max)\n. Here S(wmin/2, \u221a \u03c6max) is an upper bound on the\nnumber of samples needed by RECOVER-REST to work given true parameters (and not estimations). This allows to analyze the performance of RECOVER-REST assuming that we get perfect estimates of the parameters (w1, w2, \u03c61, \u03c62) since samples used by RECOVER-REST which are drawn from w1M (\u03c61, \u03c01) \u2295 w2M (\u03c62, \u03c02) will be indistinguishable from samples from w\u03021M ( \u03c6\u03021, \u03c01 ) \u2295 w\u03022M\u0302 ( \u03c6\u03022, \u03c02 ) except with probability 110n3 . This followed by the guarantee of Lemma 10.2 will recover the complete rankings \u03c01 and \u03c02.\nIn the degenerate case, due to our choice of \u01eb2, Lemma 9.1 shows that \u03c6\u0302 is \u01eb3 close to both \u03c61 and \u03c62. Using Lemma 9.9 we then conclude that step 4 of Algorithm 1 recovers \u03c01, \u03c02 and the parameters w1, w2 within error \u01eb."}, {"heading": "12 Conclusions and Future Directions", "text": "In this paper we gave the first polynomial time algorithm for learning the parameters of a mixture of two Mallows models. Our algorithm works for an arbitrary mixture and does not need separation among the underlying base rankings. We would like to point out that we can obtain substantial speed-up in the first stage (tensor decompositions) of our algorithm by reducing to an instance with just k \u223c log1/\u03c6 n elements. Several interesting directions come out of this work. A natural next step is to generalize our results to learn a mixture of k Mallows models for k > 2. We believe that most of these techniques can be extended to design algorithms that take poly(n, 1/\u01eb)k time. It would also be interesting to get algorithms for learning a mixture of k Mallows models which run in time poly(k, n), perhaps in an appropriate smoothed analysis setting [23] or under other non-degeneracy assumptions. Perhaps, more importantly, our result indicates that tensor based methods which have been very popular for problems such as mixture of Gaussians, might be a powerful tool for solving learning problems over rankings as well. We would like to understand the effectiveness of such tools by applying them to other popular ranking models as well."}, {"heading": "13 Some Useful Lemmas for Error Analysis", "text": "Lemma 13.1. Let u, u\u2032, v, v\u2032 denote vectors and fix parameters \u03b4, \u03b3 > 0. Suppose \u2016u \u2297 v \u2212 u\u2032 \u2297 v\u2032\u2016F < \u03b4, and \u03b3 \u2264 \u2016u\u2016, \u2016v\u2016, \u2016u\u2032\u2016, \u2016v\u2032\u2016 \u2264 1, with \u03b4 < \u03b3 2\n2 . Given a decomposition u = \u03b11u \u2032 + u\u22a5 and v = \u03b12v\u2032 + v\u22a5, where u\u22a5 and v\u22a5 are\northogonal to u\u2032, v\u2032 respectively, then we have\n\u2016u\u22a5\u2016 < \u221a \u03b4 and \u2016v\u22a5\u2016 < \u221a \u03b4.\nProof. We are given that u = \u03b11u\u2032 + u\u22a5 and v = \u03b12v\u2032 + v\u22a5. Now, since the tensored vectors are close\n\u2016u\u2297 v \u2212 u\u2032 \u2297 v\u2032\u20162F < \u03b42\n\u2016(1\u2212 \u03b11\u03b12)u\u2032 \u2297 v\u2032 + \u03b12u\u22a5 \u2297 v\u2032 + \u03b11u\u2032 \u2297 v\u22a5 + u\u22a5 \u2297 v\u22a5\u20162F < \u03b42\n\u03b34(1\u2212 \u03b11\u03b12)2 + \u2016u\u22a5\u20162\u03b122\u03b32min + \u2016v\u22a5\u20162\u03b121\u03b32 + \u2016u\u22a5\u20162\u2016v\u22a5\u20162 < \u03b42 (12) This implies that |1\u2212 \u03b11\u03b12| < \u03b4/\u03b32. Now, let us assume \u03b21 = \u2016u\u22a5\u2016 > \u221a \u03b4. This at once implies that \u03b22 = \u2016v\u22a5\u2016 < \u221a \u03b4. Hence one of\nthe two (say \u03b22) is smaller than \u221a \u03b4. Also\n\u03b32 \u2264 \u2016v\u20162 = \u03b122\u2016v\u2032\u20162 + \u03b222 \u03b32 \u2212 \u03b4 \u2264 \u03b122\nHence, \u03b12 \u2265 \u03b3\n2\nNow, using (12), we see that \u03b21 < \u221a \u03b4.\nLemma 13.2. Let \u03c6 \u2208 (0, 1) be a parameter and denote c2(\u03c6) = Zn(\u03c6)Zn\u22121(\u03c6) 1+\u03c6 \u03c6 and c3(\u03c6) = Z2n(\u03c6) Zn\u22121(\u03c6)Zn\u22122(\u03c6) 1+2\u03c6+2\u03c62+\u03c63 \u03c63 . Then we have that 1 \u2264 c2(\u03c6) \u2264 3/\u03c6 and 1 \u2264 c3(\u03c6) \u2264 50/\u03c63.\nProof. Since 0 < \u03c6 < 1, we have that Zn\u22121(\u03c6) \u2264 11\u2212\u03c6 . Observe that 1 \u2264 Zn(\u03c6) Zn\u22121(\u03c6) \u2264 1+ 1Zn\u22121(\u03c6) \u2264 2. The bounds now follow immediately.\nLemma 13.3. In the notation of section 2, given N independent samples, the empirical average P\u0302 satisfied \u2016P \u2212 P\u0302\u2016\u221e < \u221a C log nN with probability 1\u2212 n\u2212C/8.\nProof. This follows from a standard application of Bernstein inequality followed by a union bound over the O(n3) events."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "This work concerns learning probabilistic models for ranking data in a heteroge-<lb>neous population. The specific problem we study is learning the parameters of a<lb>Mallows Mixture Model. Despite being widely studied, current heuristics for this<lb>problem do not have theoretical guarantees and can get stuck in bad local optima.<lb>We present the first polynomial time algorithm which provably learns the param-<lb>eters of a mixture of two Mallows models. A key component of our algorithm is<lb>a novel use of tensor decomposition techniques to learn the top-k prefix in both<lb>the rankings. Before this work, even the question of identifiability in the case of a<lb>mixture of two Mallows models was unresolved.", "creator": "LaTeX with hyperref package"}}}