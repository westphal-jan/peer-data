{"id": "1411.2539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2014", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models", "abstract": "Inspired before due advances well prioritisation aspects, machine quotation, we introducing the transducer - backhaul pipeline did convinces (way ): no aspect-oriented joint multidimensional space with messages and explaining all (b ): a monograph language definition for decoding cover formulae from our it. Our trans effectively unifies focus comes - interpreted multidimensional using both multimodal neural language models. We introduce similar structure - form molecular english wheel however disentangles when exterior created takes court whether key content, conditioned on representations producing created the smp. The jvm provides still but army messages and sentences instead the sensor probably amounts novel descriptions from bite. Using LSTM this isoforms sentences, feel match during municipal - another - along - taught terrific out Flickr8K by Flickr30K while rather therefore unmeasurable. We be out new best match never material rest 53 - layer Oxford davinci network. Furthermore lot come that with linear encoders, this learned equivalently space remarkable multimodal regularities in terms of vector field trigonometry read. u. * fit beyond a stripe car * - \" blue \" + \" red \" although base videos include leaves motor. Sample par101 attracted for 50 touch rest made. other comparison.", "histories": [["v1", "Mon, 10 Nov 2014 19:09:41 GMT  (2905kb,D)", "http://arxiv.org/abs/1411.2539v1", "13 pages. NIPS 2014 deep learning workshop"]], "COMMENTS": "13 pages. NIPS 2014 deep learning workshop", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["ryan kiros", "ruslan salakhutdinov", "richard s zemel"], "accepted": false, "id": "1411.2539"}, "pdf": {"name": "1411.2539.pdf", "metadata": {"source": "CRF", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models", "authors": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel"], "emails": ["zemel}@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "Generating descriptions for images has long been regarded as a challenging perception task integrating vision, learning and language understanding. One not only needs to correctly recognize what appears in images but also incorporate knowledge of spatial relationships and interactions between objects. Even with this information, one then needs to generate a description that is relevant and grammatically correct. With the recent advances made in deep neural networks, tasks such as object recognition and detection have made significant breakthroughs in only a short time. The task of describing images is one that now appears tractable and ripe for advancement. Being able to append large image databases with accurate descriptions for each image would significantly improve the capabilities of content-based image retrieval systems. Moreover, systems that can describe images well, could in principle, be fine-tuned to answer questions about images also.\nThis paper describes a new approach to the problem of image caption generation, casted into the framework of encoder-decoder models. For the encoder, we learn a joint image-sentence embedding where sentences are encoded using long short-term memory (LSTM) recurrent neural networks [1]. Image features from a deep convolutional network are projected into the embedding space of the LSTM hidden states. A pairwise ranking loss is minimized in order to learn to rank images and their descriptions. For decoding, we introduce a new neural language model called the structure-content neural language model (SC-NLM). The SC-NLM differs from existing models in that it disentangles the structure of a sentence to its content, conditioned on distributed representations produced by the encoder. We show that sampling from an SC-NLM allows us to generate realistic image captions, significantly improving over the generated captions produced by [2]. Furthermore, we argue that this combination of approaches naturally fits into the experimentation framework of [3], that is, a good encoder can be used to rank images and captions while a good decoder can be used to generate new captions from scratch. Our approach effectively unifies image-text embedding models (encoder\nar X\niv :1\n41 1.\n25 39\nv1 [\ncs .L\nG ]\n1 0\nN ov\nphase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7]. Furthermore, our method builds on analogous approaches being used in machine translation [8, 9, 10, 11].\nWhile the application focus of our work is on image description generation and ranking, we also qualitatively analyse properties of multimodal vector spaces learned using images and sentences. We show that using a linear sentence encoder, linguistic regularities [12] also carry over to multimodal vector spaces. For example, *image of a blue car* - \"blue\" + \"red\" results in a vector that is near images of red cars. We qualitatively examine several types of analogies and structures with PCA projections. Consequently, even with a global image-sentence training objective the encoder can still be used to retrieve locally (e.g. individual words). This is analogous to pairwise ranking methods used in machine translation [13, 14]."}, {"heading": "1.1 Multimodal representation learning", "text": "A large body of work has been done on learning multimodal representations of images and text. Popular approaches include learning joint image-word embeddings [4, 5] as well as embedding images and sentences into a common space [6, 15]. Our proposed pipeline makes direct use of these ideas. Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18]. Several bi-directional approaches to ranking images and captions have also been proposed, based off of kernel CCA [3], normalized CCA [19] and dependency tree recursive networks [6]. From an architectural standpoint, our encoder-decoder model is most similar to [20], who proposed a two-step embedding and generation procedure for semantic parsing."}, {"heading": "1.2 Generating descriptions of images", "text": "We group together approaches to generation into three types of methods, each described here in more detail:\nTemplate-based methods. Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25]. While\nthese approaches can produce accurate descriptions, they are often more \u2018robotic\u2019 in nature and do not generalize to the fluidity and naturalness of captions written by humans.\nComposition-based methods. These approaches aim to harness existing image-caption databases by extracting components of related captions and composing them together to generate novel descriptions [26, 27]. The advantage of these approaches are that they allow for a much broader and more expressive class of captions that are more fluent and human-like then template-based approaches.\nNeural network methods. These approaches aim to generate descriptions by sampling from conditional neural language models. The initial work in this area, based off of multimodal neural language models [2], generated captions by conditioning on feature vectors from the output of a deep convolutional network. These ideas were recently extended to multimodal recurrent networks with significant improvements [7]. The methods described in this paper produce descriptions that at least qualitatively on par with current state-of-the-art composition-based methods [27].\nDescription generation systems have been plagued with issues of evaluation. While Bleu and Rouge have been used in the past, [3] has argued that such automated evaluation methods are unreliable and do not match human judgements. These authors instead proposed that the problem of ranking images and captions can be used as a proxy for generation. Since any generation system requires a scoring function to access how well a caption and image match, optimizing this task should naturally carry over to an improvement in generation. Many recent methods have since used this approach for evaluation. None the less, the question on how to transfer improvements on ranking to generating new descriptions remained. We argue that encoder-decoder methods naturally fit into this experimentation framework. That is, the encoder gives us a way to rank images and captions and develop good scoring functions, while the decoder can use the representations learned to optimize the scoring functions as a way of generating and scoring new descriptions."}, {"heading": "1.3 Encoder-decoder methods for machine translation", "text": "Our proposed pipeline, while new to caption generation, has already experienced several successes in Neural Machine Translation (NMT). The goal of NMT is to develop an end-to-end translation system with a large neural network, as opposed to using a neural network as an additional feature function to an existing phrase-based system. NMT methods are based on the encoder-decoder principle. That is, an encoder is used to map an English sentence to a distributed vector. A decoder is then conditioned on this vector to generate a French translation from the source text. Current methods include using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11]. While still a young research area, these methods have already achieved performance on par with strong phrase-based systems and have improved on the start-of-the-art when used for rescoring.\nWe argue that it is natural to think of image caption generation as a translation problem. That is, our goal is to translate an image into a description. This point of view has also been used by [28] and allows us to make use of existing ideas in the machine translation literature. Furthermore, there is a natural correspondence between the concept of scoring functions (how well does a caption and image match) and alignments (which parts of a description correspond to which parts of an image) that can naturally be exploited for generating descriptions."}, {"heading": "2 An encoder-decoder model for ranking and generation", "text": "In this section we describe our image caption generation pipeline. We first review LSTM RNNs which are used for encoding sentences, followed by how to learn multimodal distributed representations. We then review log-bilinear neural language models [29], multiplicative neural language models [30] and then introduce our structure-content neural language model."}, {"heading": "2.1 Long short-term memory RNNs", "text": "Long short-term memory [1] is a recurrent neural network that incorporates a built in memory cell to store information and exploit long range context. LSTM memory cells are surrounded by gating units for the purpose of reading, writing and reseting information. LSTMs have been used to achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence generation [32] speech recognition [33] and machine translation [11] among others. Dropout [34] strategies have also been proposed to prevent overfitting in deep LSTMs. [35]\nLet Xt denote a matrix of training instances at time t. In our case, Xt is used to denote a matrix of word representations for the t-th word of each sentence in the training batch. Let (It,Ft,Ct,Ot,Mt) denote the input, forget, cell, output and hidden states of the LSTM at time step t. The LSTM architecture in this work is implemented using the following equations:\nIt = \u03c3(Xt \u00b7Wxi +Mt\u22121 \u00b7Whi +Ct\u22121 \u00b7Wci + bi) (1) Ft = \u03c3(Xt \u00b7Wxf +Mt\u22121 \u00b7Whf +Ct\u22121 \u00b7Wcf + bf ) (2) Ct = Ft \u2022Ct\u22121 + It \u2022 tanh(Xt \u00b7Wxc +Mt\u22121 \u00b7Whc + bc) (3) Ot = \u03c3(Xt \u00b7Wxo +Mt\u22121 \u00b7Who +Ct \u00b7Wco + bo) (4) Mt = Ot \u2022 tanh(Ct) (5)\nwhere (\u03c3) denotes the sigmoid activation function, (\u00b7) indicates matrix multiplication and (\u2022) indicates component-wise multiplication. 1"}, {"heading": "2.2 Multimodal distributed representations", "text": "Suppose for training we are given image-description pairs each corresponding to an image and a description that correctly describes the image. Images are represented as the top layer (before the softmax) of a convolutional network trained on the ImageNet classification task [36].\nLet D be the dimensionality of an image feature vector (e.g. 4096 for AlexNet [36]), K the dimensionality of the embedding space and let V be the number of words in the vocabulary. Let WI \u2208 RK\u00d7D and WT \u2208 RK\u00d7V be the image embedding matrix and word embedding matrices, respectively. Given an image description S = {w1, . . . , wN} with words w1, . . . , wN , 2 let {w1, . . . ,wN},wi \u2208 RK , i = 1, . . . , n denote the corresponding word representations to words w1, . . . , wN (entries in the matrix WT ). The representation of a sentence v is the hidden state of the LSTM at time stepN (i.e. the vector mt). We note that other approaches for computing sentence representations for image-text embeddings have been proposed, including dependency tree RNNs [6] and bags of dependency parses [15]. Let q \u2208 RD denote an image feature vector (for the image corresponding to description S) and let x = WI \u00b7 q \u2208 RK be the image embedding. We define a scoring function s(x,v) = x \u00b7 v, where x and v are first scaled to have unit norm (making s equivalent to cosine similarity). Let \u03b8 denote all the parameters to be learned (WI and all the LSTM weights) 3. We optimize the following pairwise ranking loss:\nmin \u03b8 \u2211 x \u2211 k max{0, \u03b1\u2212 s(x,v) + s(x,vk)}+ \u2211 v \u2211 k max{0, \u03b1\u2212 s(v,x) + s(v,xk)} (6)\nwhere vk is a contrastive (non-descriptive) sentence for image embedding x, and vice-versa with xk. For all of our experiments, we initialize the word embeddings WT to be pre-computed K = 300 dimensional vectors learned using a continuous bag-of-words model [37]. The contrastive terms are chosen randomly from the training set and resampled every epoch.\n1For additional details on LSTM: http://people.idsia.ch/~juergen/rnn.html. 2As a slight abuse of notation, we refer to wi as both a word and an index into the word embedding matrix. 3We keep the word embedding matrix WT fixed."}, {"heading": "2.3 Log-bilinear neural language models", "text": "The log-bilinear language model (LBL) [29] is a deterministic model that may be viewed as a feedforward neural network with a single linear hidden layer. Each word w in the vocabulary is represented as a K-dimensional real-valued vector w \u2208 RK , as in the case of the encoder. Let R denote a V \u00d7K matrix of word representation vectors 4 where V is the vocabulary size. Let (w1, . . . wn\u22121) be a tuple of n\u2212 1 words where n\u2212 1 is the context size. The LBL model makes a linear prediction of the next word representation as\nr\u0302 = n\u22121\u2211 i=1 C(i)wi, (7)\nwhere C(i), i = 1, . . . , n \u2212 1 are K \u00d7 K context parameter matrices. Thus, r\u0302 is the predicted representation of wn. The conditional probability P (wn = i|w1:n\u22121) of wn given w1, . . . , wn\u22121 is\nP (wn = i|w1:n\u22121) = exp(r\u0302T ri + bi)\u2211V\nj=1 exp(r\u0302T rj + bj) , (8)\nwhere b \u2208 RV is a bias vector. Learning is done with stochastic gradient descent."}, {"heading": "2.4 Multiplicative neural language models", "text": "Suppose now we are given a vector u \u2208 RK from the multimodal vector space, which has an association with a word sequence S = {w1, . . . , wN}. For example, u may be the embedded representation of an image whose description is given by S. A multiplicative neural language model [30] models the distributionP (wn = i|w1:n\u22121,u) of a new wordwn given context from the previous words and the vector u. A multiplicative model has the additional property that the word embedding matrix is instead replaced with a tensor T \u2208 RV\u00d7K\u00d7G where G is the number of slices. Given u, we can compute a word representation matrix as a function of u as T u = \u2211G i=1 uiT\n(i) i.e. word representations with respect to u are computed as a linear combination of slices weighted by each component ui of u. Here, the number of slices G is equal to K, the dimensionality of u.\nIt is often unnecessary to use a fully unfactored tensor. As in e.g. [38, 39], we re-represent T in terms of three matrices Wfk \u2208 RF\u00d7K , Wfd \u2208 RF\u00d7G and Wfv \u2208 RF\u00d7V , such that\nT u = (Wfv)> \u00b7 diag(Wfdu) \u00b7Wfk (9) where diag(\u00b7) denotes the matrix with its argument on the diagonal. These matrices are parametrized by a pre-chosen number of factors F . In [30], the conditioning vector u is referred to as an attribute and using a third-order model of words allows one to model conditional similarity: how meanings of words change as a function of the attributes they\u2019re conditioned on.\nLet E = (Wfk)>Wfv denote a \u2018folded\u2019 K \u00d7 V matrix of word embeddings. Given the context w1, . . . , wn\u22121, the predicted next word representation r\u0302 is given by:\nr\u0302 = n\u22121\u2211 i=1 C(i)E(:, wi), (10)\nwhere E(:, wi) denotes the column of E for the word representation of wi and C(i), i = 1, . . . , n\u22121 are K \u00d7 K context matrices. Given a predicted next word representation r\u0302, the factor outputs are f = (Wfkr\u0302) \u2022 (Wfdu), where \u2022 is a component-wise product. The conditional probability P (wn = i|w1:n\u22121,u) of wn given w1, . . . , wn\u22121 and u can be written as\nP (wn = i|w1:n\u22121,u) = exp ( (Wfv(:, i))>f + bi )\u2211V j=1 exp ( (Wfv(:, j))>f + bj\n) , where Wfv(:, i) denotes the column of Wfv corresponding to word i. In contrast to the log-bilinear model, the matrix of word representations R from before is replaced with the factored tensor T that we have derived. We compared the multiplicative model against an additive variant [2] and found on large datasets, such as the SBU Captioned Photo dataset [40], the multiplicative variant significantly outperforms its additive counterpart. Thus, the SC-NLM is derived from the multiplicative variant.\n4Note that this is a different matrix then that used by the encoder. We use the same vocabulary throughout both models."}, {"heading": "2.5 Structure-content neural language models", "text": "We now describe the structure-content neural language model. Suppose that, along with a description S = {w1, . . . , wN}, we are also given a sequence of word-specific structure variables T = {t1, . . . , tN}. Throughout our experiments, each ti corresponds to the part-of-speech for word wi, although other possibilities can be used instead. Given an embedding u (the content vector), our goal is to model the distribution P (wn = i|w1:n\u22121, tn:n+k,u) from previous word context w1:n\u22121 and forward structure context tn:n+k, where k is the forward context size. Figure 3 gives an illustration of the model and prediction problem. Intuitively, the structure variables help guide the model during the generation phrase and can be thought of as a soft template to help avoid the model from generating grammatical nonsense. Note that this model shares a resemblance with the NNJM of [41] for machine translation, where the previous word context are predicted words in the target language, and the forward context are words in the source language.\nOur model can be interpreted as a multiplicative neural language model but where the attribute vector is no longer u but instead an additive function of u and the structure variables T . Let {tn, . . . , tn+k}, ti \u2208 RK , i = n, . . . , n + k be embedding vectors for the structure variables T . These are obtained from a learned lookup table in the same way as words are. We introduce a sequence of G\u00d7G structure context matrices T(i), i = n, . . . , n+ k which play the same role as the word context matrices C(i). Let Tu denote a G \u00d7 K context matrix for the multimodal vector u. The attribute vector u\u0302 of combined structure and content information is computed as\nu\u0302 = [( n+k\u2211 i=n T(i)ti ) +T(u)u+ b ] +\n(11)\nwhere [\u00b7]+ = max{\u00b7, 0} is a ReLU non-linearity and b is a bias vector. The vector u\u0302 now plays the same role as the vector u for the multiplicative model previously described and the remainder of the model remains unchanged. Our experiments use G = K = 300 and factors F = 100.\nThe SC-NLM is trained on a large collection of image descriptions (e.g. Flickr30K). There are several choices available for representing the conditioning vectors u. One choice would be to use the embedding of the corresponding image. An alternative choice, which is the approach we take, is to condition on the embedding vector for the description S computed with the LSTM. The advantage of this approach is that the SC-NLM can be trained purely on text alone. This allows us to make use of large amounts of monolingual text (e.g. non image captions) to improve the quality of the language model. Since the embedding vectors of S share a joint space with the image embeddings, we can also condition the SC-NLM on image embeddings (e.g. at test time, when no description is available) after the model has been trained. This is a significant advantage over a conditional language model that explicitly requires image-caption pairs for training and highlights the strength of a multimodal encoding space.\nDue to space limitations, we leave the full details of our caption generation procedure to the supplementary material."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Image-sentence ranking", "text": "Our main quantitative results is to establish the effectiveness of using an LSTM sentence encoder for ranking image and descriptions. We perform the same experimental procedure as done by [15] on the Flickr8K [3] and Flickr30K [42] datasets. These datasets come with 8,000 and 30,000 images respectively with each image annotated using 5 sentences by independent annotators. As with [15], we did not do any explicit text preprocessing. We used two convolutional network architectures for extracting 4096 dimensional image features: the Toronto ConvNet 5 as well as the 19-layer OxfordNet [43] which finished 2nd place in the ILSVRC 2014 classification competition. Following the protocol of [15], 1000 images are used for validation, 1000 for testing and the rest are used for training. Evaluation is performed using Recall@K, namely the mean number of images for which the correct caption is ranked within the top-K retrieved results (and vice-versa for sentences). We also report the median rank of the closest ground truth result from the ranked list. We compare our results to each of the following methods:\nDeViSE. The deep visual semantic embedding model [5] was proposed as a way of performing zeroshot object recognition and was used as a baseline by [15]. In this model, sentences are represented as the mean of their word embeddings and the objective function optimized matches ours.\nSDT-RNN. The semantic dependency tree recursive neural network [6] is used to learn sentence representations for embedding into a joint image-sentence space. The same objective is used.\nDeFrag. Deep fragment embeddings [15] were proposed as an alternative to embedding full-frame image features and take advantage of object detections from the R-CNN [44] detector. Descriptions are represented as a bag of dependency parses. Their objective incorporates both a global and fragment objectives, for which their global objective matches ours.\nm-RNN. The multimodal recurrent neural network [7] is a recently proposed method that uses perplexity as a bridge between modalities, as first introduced by [2]. Unlike all other methods, the m-RNN does not use a ranking loss and instead optimizes the log-likelihood of predicting the next word in a sequence conditioned on an image.\nOur LSTMs use 1 layer with 300 units and weights initialized uniformly from [-0.08, 0.08]. The margin \u03b1 was set to \u03b1 = 0.2, which we found performed well on both datasets. Training is done using stochastic gradient descent with an initial learning rate of 1 and was exponentially decreased. We used minibatch sizes of 40 on Flickr8K and 100 on Flickr30K. No momentum was used. The same hyperparameters are used for the OxfordNet experiments."}, {"heading": "3.1.1 Results", "text": "Tables 1 and 2 illustrate our results on Flickr8K and Flickr30K respectively. The performance of our model is comparable to that of the m-RNN. For some metrics we outperform or match existing results while on others m-RNN outperforms our model. The m-RNN does not learn an explicit embedding between images and sentences and relies on perplexity as a means of retrieval. Methods that\n5https://github.com/TorontoDeepLearning/convnet\nlearn explicit embedding spaces have a significant speed advantage over perplexity-based retrieval methods, since retrieval is easily done with a single matrix multiply of stored embedding vectors from the dataset with the query vector. Thus explicit embedding methods are much better suited for scaling to large datasets.\nPerhaps more interestingly is the fact that both our method and the m-RNN outperform existing models that integrate object detections. This is contradictory to [6], where recurrent networks are the worst performing models. This highlights the effectiveness of LSTM cells for encoding dependencies across descriptions and learning meaningful distributed sentence representations. Integrating object detections into our framework should almost surely improve performance as well as allow for interpretable retrievals, as in the case of DeFrag.\nUsing image features from the OxfordNet model results in a significant performance boost across all metrics, giving new state-of-the-art numbers on these evaluation tasks."}, {"heading": "3.2 Multimodal linguistic regularities", "text": "Word embeddings learned with skip-gram [37] or neural language models [45] were shown by [12] to exhibit linguistic regularities that allow these models to perform analogical reasoning. For instance, \"man\" is to \"woman\" as \"king\" is to ? can be answered by finding the closest vector to \"king\" - \"man\" + \"woman\". A natural question we ask is whether multimodal vector spaces exhibit the same phenomenon. Would *image of a blue car* - \"blue\" + \"red\" be near images of red cars?\nSuppose that we train an embedding model with a linear encoder, namely v = \u2211N\ni=1 wi for word vectors wi and sentence vector v (where both v and the image embedding are normalized to unit length). Using our example above, let vblue, vred and vcar denote the word embeddings for blue, red and car respectively. Let Ibcar and Ircar denote embeddings of images with blue and red cars. After training a linear encoder, the model has the property that vblue + vcar \u2248 Ibcar and vred + vcar \u2248 Ircar. It follows that\nvcar \u2248 Ibcar \u2212 vblue (12) vred + vcar \u2248 Ibcar \u2212 vblue + vred (13)\nIrcar \u2248 Ibcar \u2212 vblue + vred (14) Thus given a query image q, a negative word wn and a positive word wp (all with unit norm), we seek an image x\u2217 such that:\nx\u2217 = argmax x (q\u2212wn +wp)>x \u2016q\u2212wn +wp\u2016\n(15)\nThe supplementary material contains qualitative evidence that the above holds for several types of regularities and images. 6 In our examples, we consider retrieving the top-4 nearest images. Occasionally we observed that a poor result would be obtained within the top-4 among good results. We found a simple strategy for removing these cases is to first retrieve the top N nearest images, then re-sort these based on their distance to the mean of the N images.\nIt is worth noting that these kinds of regularities are not well observed with an LSTM encoder, since sentences are no longer just a sum of their words. The linear encoder is roughly equivalent to the\n6For this model we finetune the word representations.\nDeViSE baselines in tables 1 and 2, which perform significantly worse for retrieval than an LSTM encoder. So while these regularities are interesting the learned multimodal vector space is not well apt for ranking sentences and images."}, {"heading": "3.3 Image caption generation", "text": "We generated image descriptions for roughly 800 images from the SBU captioned photo dataset [40]. These are the same images used to display results by the current state-of-the-art composition based approach, TreeTalk [27]. 7 Our LSTM encoder and SC-NLM decoder were trained by concatenating the Flickr30K dataset with the recently released Microsoft COCO dataset [46], which combined give us over 100,000 images and over 500,000 descriptions for training. The SBU dataset contains 1 million images each with a single description and was used by [27] for training their model. While the SBU dataset is larger, the annotated descriptions are noisier and more personalized.\nThe generated results can be found at http://www.cs.toronto.edu/~rkiros/lstm_ scnlm.html 8. For each image we show the original caption, the nearest neighbour sentence from the training set, the top-5 generated samples from our model and the best generated result from TreeTalk. The nearest neighbour sentence is displayed to demonstrate that our model has not simply learned to copy the training data. Our generated descriptions are arguably the nicest ones to date."}, {"heading": "4 Discussion", "text": "When generating a description, it is often the case that only a small region is relevant at any given time. We are developing an attention-based model that jointly learns to align parts of captions to images and use these alignments to determine where to attend next, thus dynamically modifying the vectors used for conditioning the decoder. We also plan on experimenting with LSTM decoders as well as deep and bidirectional LSTM encoders."}, {"heading": "Acknowledgments", "text": "We would like to thank Nitish Srivastava for assistance with his ConvNet package as well as preparing the Oxford convolutional network. We also thank the anonymous reviewers from the NIPS 2014 deep learning workshop for their comments and suggestions."}, {"heading": "5 Supplementary material: Additional experimentation and details", "text": ""}, {"heading": "5.1 Multimodal linguistic regularities", "text": "Figure 4 illustrates sample results using a model trained on the SBU dataset. All queries were downloaded online and retrieved images are from the SBU images used for training. What is of interest to note is that the resulting images depend highly on the image used for the query. For example, searching for the word \u2018night\u2019 retrieves arbitrary images taken at night. On the other hand, an image with a building predominantly as its focus will return night images when \u2018day\u2019 is\nsubtracted and \u2018night\u2019 is added. A similar phenomenon occurs with the example of cats, bowls and boxes. As additional visualizations, we computed PCA projections of cars and their corresponding colors as well as images and the weather occurrences in Figure 5. These results give us strong evidence for the regularities apparent in multimodal vector spaces trained with linear encoders. Of course, sensible results are only likely to be obtained if (a) the content of the image is correctly recognized, (b) the subtraction word is relevant to the image and (c) an image exists that is sensible for the corresponding query."}, {"heading": "5.2 Image description generation", "text": "The SC-NLM was trained on the concatenation of training sentences from both Flickr30K and Microsoft COCO. Given an image, we first map it into the multimodal space. From this embedding, we define 2 sets of candidate conditioning vectors to the SC-NLM:\nImage embedding. The embedded image itself. Note that the SC-NLM was not trained with images but can be conditioned on images since the embedding space is multimodal.\ntop-N nearest words and sentences. After first computing the image embedding, we obtain the top-N nearest neighbour words and training sentences using cosine similarity. These retrievals are treated as a \u2018bag of concepts\u2019 for which we compute an embedding vector as the mean of each concept. All of our results use N = 5.\nAlong with the candidate conditioning vectors, we also compute candidate POS sequences used by the SC-NLM. For this, we obtain a set of all POS sequences from the training set whose lengths were between 4 and 12, inclusive. Captions are generated by first sampling a conditioning vector, next sampling a POS sequence, then computing a MAP estimate from the SC-NLM. We generate a large list of candidate descriptions (1000 for each image in our results) and rank these candidates using a scoring function. Our scoring function consists of two feature functions:\nTranslation model. The candidate description is embedded into the multimodal space using the LSTM. We then compute a translation score as the cosine similarity between the image embedding and the embedding of the candidate description. This scores how relevant the content of the candidate is to the image. We also augment to this score a multiplicative penalty to non-stopwords that appear too frequently in the description. 9\nLanguage model. We trained a Kneser-Ney trigram model on a large corpus and compute the logprobability of the candidate under the model. This scores how reasonable of an English sentence is the candidate.\nThe total score of a caption is then the weighted sum of the translation and language models. Due to the challenge of quantitatively evaluating generated descriptions, we tuned the weights by hand on qualitative results alone. All of the candidate descriptions are ranked by their scores, and the top-5 captions are returned.\n9For instance, given an image of a car, we would want a candidate to be ranked low if each noun in the description was \u2018car\u2019."}], "references": [{"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Richard S Zemel", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "metrics. JAIR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeffrey Dean"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Q Le", "C Manning", "A Ng"], "venue": "TACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille"], "venue": "arXiv preprint arXiv:1410.1090,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "EMNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In NAACL-HLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "ICLR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Multilingual models for compositional distributional semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Li Fei-Fei"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Ng"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Yangqing Jia", "Mathieu Salzmann", "Trevor Darrell"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": "In ECCV", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "A deep architecture for semantic parsing", "author": ["Phil Blunsom", "Nando de Freitas", "Edward Grefenstette", "Karl Moritz Hermann"], "venue": "In ACL 2014 Workshop on Semantic Parsing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth"], "venue": "In ECCV", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["Siming Li", "Girish Kulkarni", "Tamara L Berg", "Alexander C Berg", "Yejin Choi"], "venue": "CONLL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Yezhou Yang", "Ching Lik Teo", "Hal Daum\u00e9 III", "Yiannis Aloimonos"], "venue": "In EMNLP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["Margaret Mitchell", "Xufeng Han", "Jesse Dodge", "Alyssa Mensch", "Amit Goyal", "Alex Berg", "Kota Yamaguchi", "Tamara Berg", "Karl Stratos", "Hal Daum\u00e9 III"], "venue": "In EACL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Collective generation of natural image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Alexander C Berg", "Tamara L Berg", "Yejin Choi"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Treetalk : Composition and compression of trees for image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Tamara L. Berg", "Yejin Choi"], "venue": "TACL,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan Thater", "Manfred Pinkal", "Bernt Schiele"], "venue": "In ICCV,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "A multiplicative model for learning distributed text-based attribute representations", "author": ["Ryan Kiros", "Richard S Zemel", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "In IEEE Workshop on ASRU,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Unsupervised learning of image transformations", "author": ["Roland Memisevic", "Geoffrey Hinton"], "venue": "In CVPR, pages", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["Alex Krizhevsky", "Geoffrey E Hinton"], "venue": "In AISTATS,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg"], "venue": "In NIPS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Fast and robust neural network joint models for statistical machine", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Julia Hockenmaier"], "venue": "TACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "For the encoder, we learn a joint image-sentence embedding where sentences are encoded using long short-term memory (LSTM) recurrent neural networks [1].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "We show that sampling from an SC-NLM allows us to generate realistic image captions, significantly improving over the generated captions produced by [2].", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Furthermore, we argue that this combination of approaches naturally fits into the experimentation framework of [3], that is, a good encoder can be used to rank images and captions while a good decoder can be used to generate new captions from scratch.", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 7, "endOffset": 16}, {"referenceID": 4, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 7, "endOffset": 16}, {"referenceID": 5, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 7, "endOffset": 16}, {"referenceID": 1, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "Furthermore, our method builds on analogous approaches being used in machine translation [8, 9, 10, 11].", "startOffset": 89, "endOffset": 103}, {"referenceID": 8, "context": "Furthermore, our method builds on analogous approaches being used in machine translation [8, 9, 10, 11].", "startOffset": 89, "endOffset": 103}, {"referenceID": 9, "context": "Furthermore, our method builds on analogous approaches being used in machine translation [8, 9, 10, 11].", "startOffset": 89, "endOffset": 103}, {"referenceID": 10, "context": "Furthermore, our method builds on analogous approaches being used in machine translation [8, 9, 10, 11].", "startOffset": 89, "endOffset": 103}, {"referenceID": 11, "context": "We show that using a linear sentence encoder, linguistic regularities [12] also carry over to multimodal vector spaces.", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "This is analogous to pairwise ranking methods used in machine translation [13, 14].", "startOffset": 74, "endOffset": 82}, {"referenceID": 13, "context": "This is analogous to pairwise ranking methods used in machine translation [13, 14].", "startOffset": 74, "endOffset": 82}, {"referenceID": 3, "context": "Popular approaches include learning joint image-word embeddings [4, 5] as well as embedding images and sentences into a common space [6, 15].", "startOffset": 64, "endOffset": 70}, {"referenceID": 4, "context": "Popular approaches include learning joint image-word embeddings [4, 5] as well as embedding images and sentences into a common space [6, 15].", "startOffset": 64, "endOffset": 70}, {"referenceID": 5, "context": "Popular approaches include learning joint image-word embeddings [4, 5] as well as embedding images and sentences into a common space [6, 15].", "startOffset": 133, "endOffset": 140}, {"referenceID": 14, "context": "Popular approaches include learning joint image-word embeddings [4, 5] as well as embedding images and sentences into a common space [6, 15].", "startOffset": 133, "endOffset": 140}, {"referenceID": 15, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 125, "endOffset": 128}, {"referenceID": 16, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 6, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 175, "endOffset": 178}, {"referenceID": 17, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 2, "context": "Several bi-directional approaches to ranking images and captions have also been proposed, based off of kernel CCA [3], normalized CCA [19] and dependency tree recursive networks [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 18, "context": "Several bi-directional approaches to ranking images and captions have also been proposed, based off of kernel CCA [3], normalized CCA [19] and dependency tree recursive networks [6].", "startOffset": 134, "endOffset": 138}, {"referenceID": 5, "context": "Several bi-directional approaches to ranking images and captions have also been proposed, based off of kernel CCA [3], normalized CCA [19] and dependency tree recursive networks [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 19, "context": "From an architectural standpoint, our encoder-decoder model is most similar to [20], who proposed a two-step embedding and generation procedure for semantic parsing.", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 21, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 22, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 23, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 24, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 25, "context": "These approaches aim to harness existing image-caption databases by extracting components of related captions and composing them together to generate novel descriptions [26, 27].", "startOffset": 169, "endOffset": 177}, {"referenceID": 26, "context": "These approaches aim to harness existing image-caption databases by extracting components of related captions and composing them together to generate novel descriptions [26, 27].", "startOffset": 169, "endOffset": 177}, {"referenceID": 1, "context": "The initial work in this area, based off of multimodal neural language models [2], generated captions by conditioning on feature vectors from the output of a deep convolutional network.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "These ideas were recently extended to multimodal recurrent networks with significant improvements [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 26, "context": "The methods described in this paper produce descriptions that at least qualitatively on par with current state-of-the-art composition-based methods [27].", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "While Bleu and Rouge have been used in the past, [3] has argued that such automated evaluation methods are unreliable and do not match human judgements.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "Current methods include using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Current methods include using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 9, "context": "Current methods include using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 10, "context": "Current methods include using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 27, "context": "This point of view has also been used by [28] and allows us to make use of existing ideas in the machine translation literature.", "startOffset": 41, "endOffset": 45}, {"referenceID": 28, "context": "We then review log-bilinear neural language models [29], multiplicative neural language models [30] and then introduce our structure-content neural language model.", "startOffset": 51, "endOffset": 55}, {"referenceID": 29, "context": "We then review log-bilinear neural language models [29], multiplicative neural language models [30] and then introduce our structure-content neural language model.", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Long short-term memory [1] is a recurrent neural network that incorporates a built in memory cell to store information and exploit long range context.", "startOffset": 23, "endOffset": 26}, {"referenceID": 30, "context": "LSTMs have been used to achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence generation [32] speech recognition [33] and machine translation [11] among others.", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "LSTMs have been used to achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence generation [32] speech recognition [33] and machine translation [11] among others.", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "LSTMs have been used to achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence generation [32] speech recognition [33] and machine translation [11] among others.", "startOffset": 160, "endOffset": 164}, {"referenceID": 10, "context": "LSTMs have been used to achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence generation [32] speech recognition [33] and machine translation [11] among others.", "startOffset": 189, "endOffset": 193}, {"referenceID": 33, "context": "Dropout [34] strategies have also been proposed to prevent overfitting in deep LSTMs.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "[35]", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Images are represented as the top layer (before the softmax) of a convolutional network trained on the ImageNet classification task [36].", "startOffset": 132, "endOffset": 136}, {"referenceID": 35, "context": "4096 for AlexNet [36]), K the dimensionality of the embedding space and let V be the number of words in the vocabulary.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "We note that other approaches for computing sentence representations for image-text embeddings have been proposed, including dependency tree RNNs [6] and bags of dependency parses [15].", "startOffset": 146, "endOffset": 149}, {"referenceID": 14, "context": "We note that other approaches for computing sentence representations for image-text embeddings have been proposed, including dependency tree RNNs [6] and bags of dependency parses [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 36, "context": "For all of our experiments, we initialize the word embeddings WT to be pre-computed K = 300 dimensional vectors learned using a continuous bag-of-words model [37].", "startOffset": 158, "endOffset": 162}, {"referenceID": 28, "context": "The log-bilinear language model (LBL) [29] is a deterministic model that may be viewed as a feedforward neural network with a single linear hidden layer.", "startOffset": 38, "endOffset": 42}, {"referenceID": 29, "context": "A multiplicative neural language model [30] models the distributionP (wn = i|w1:n\u22121,u) of a new wordwn given context from the previous words and the vector u.", "startOffset": 39, "endOffset": 43}, {"referenceID": 37, "context": "[38, 39], we re-represent T in terms of three matrices W \u2208 RF\u00d7K , W \u2208 RF\u00d7G and W \u2208 RF\u00d7V , such that T u = (Wfv)> \u00b7 diag(Wu) \u00b7W (9) where diag(\u00b7) denotes the matrix with its argument on the diagonal.", "startOffset": 0, "endOffset": 8}, {"referenceID": 38, "context": "[38, 39], we re-represent T in terms of three matrices W \u2208 RF\u00d7K , W \u2208 RF\u00d7G and W \u2208 RF\u00d7V , such that T u = (Wfv)> \u00b7 diag(Wu) \u00b7W (9) where diag(\u00b7) denotes the matrix with its argument on the diagonal.", "startOffset": 0, "endOffset": 8}, {"referenceID": 29, "context": "In [30], the conditioning vector u is referred to as an attribute and using a third-order model of words allows one to model conditional similarity: how meanings of words change as a function of the attributes they\u2019re conditioned on.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "We compared the multiplicative model against an additive variant [2] and found on large datasets, such as the SBU Captioned Photo dataset [40], the multiplicative variant significantly outperforms its additive counterpart.", "startOffset": 65, "endOffset": 68}, {"referenceID": 39, "context": "We compared the multiplicative model against an additive variant [2] and found on large datasets, such as the SBU Captioned Photo dataset [40], the multiplicative variant significantly outperforms its additive counterpart.", "startOffset": 138, "endOffset": 142}, {"referenceID": 40, "context": "Note that this model shares a resemblance with the NNJM of [41] for machine translation, where the previous word context are predicted words in the target language, and the forward context are words in the source language.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "0 500 SDT-RNN [6] 4.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "0 29 \u2020 DeViSE [5] 4.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "6 29 \u2020 SDT-RNN [6] 6.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "7 25 DeFrag [15] 5.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "5 32 \u2020 DeFrag [15] 12.", "startOffset": 14, "endOffset": 18}, {"referenceID": 6, "context": "5 15 m-RNN [7] 14.", "startOffset": 11, "endOffset": 14}, {"referenceID": 14, "context": "We perform the same experimental procedure as done by [15] on the Flickr8K [3] and Flickr30K [42] datasets.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "We perform the same experimental procedure as done by [15] on the Flickr8K [3] and Flickr30K [42] datasets.", "startOffset": 75, "endOffset": 78}, {"referenceID": 41, "context": "We perform the same experimental procedure as done by [15] on the Flickr8K [3] and Flickr30K [42] datasets.", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "As with [15], we did not do any explicit text preprocessing.", "startOffset": 8, "endOffset": 12}, {"referenceID": 42, "context": "We used two convolutional network architectures for extracting 4096 dimensional image features: the Toronto ConvNet 5 as well as the 19-layer OxfordNet [43] which finished 2nd place in the ILSVRC 2014 classification competition.", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "Following the protocol of [15], 1000 images are used for validation, 1000 for testing and the rest are used for training.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "The deep visual semantic embedding model [5] was proposed as a way of performing zeroshot object recognition and was used as a baseline by [15].", "startOffset": 41, "endOffset": 44}, {"referenceID": 14, "context": "The deep visual semantic embedding model [5] was proposed as a way of performing zeroshot object recognition and was used as a baseline by [15].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "The semantic dependency tree recursive neural network [6] is used to learn sentence representations for embedding into a joint image-sentence space.", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "Deep fragment embeddings [15] were proposed as an alternative to embedding full-frame image features and take advantage of object detections from the R-CNN [44] detector.", "startOffset": 25, "endOffset": 29}, {"referenceID": 43, "context": "Deep fragment embeddings [15] were proposed as an alternative to embedding full-frame image features and take advantage of object detections from the R-CNN [44] detector.", "startOffset": 156, "endOffset": 160}, {"referenceID": 6, "context": "The multimodal recurrent neural network [7] is a recently proposed method that uses perplexity as a bridge between modalities, as first introduced by [2].", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "The multimodal recurrent neural network [7] is a recently proposed method that uses perplexity as a bridge between modalities, as first introduced by [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "0 500 \u2020 DeViSE [5] 4.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "7 25 \u2020 SDT-RNN [6] 9.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "1 16 \u2020 DeFrag [15] 14.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "2 14 \u2020 DeFrag + Finetune CNN [15] 16.", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "5 13 m-RNN [7] 18.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "This is contradictory to [6], where recurrent networks are the worst performing models.", "startOffset": 25, "endOffset": 28}, {"referenceID": 36, "context": "Word embeddings learned with skip-gram [37] or neural language models [45] were shown by [12] to exhibit linguistic regularities that allow these models to perform analogical reasoning.", "startOffset": 39, "endOffset": 43}, {"referenceID": 44, "context": "Word embeddings learned with skip-gram [37] or neural language models [45] were shown by [12] to exhibit linguistic regularities that allow these models to perform analogical reasoning.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Word embeddings learned with skip-gram [37] or neural language models [45] were shown by [12] to exhibit linguistic regularities that allow these models to perform analogical reasoning.", "startOffset": 89, "endOffset": 93}, {"referenceID": 39, "context": "We generated image descriptions for roughly 800 images from the SBU captioned photo dataset [40].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "These are the same images used to display results by the current state-of-the-art composition based approach, TreeTalk [27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "The SBU dataset contains 1 million images each with a single description and was used by [27] for training their model.", "startOffset": 89, "endOffset": 93}], "year": 2014, "abstractText": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.", "creator": "LaTeX with hyperref package"}}}