{"id": "1411.2738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2014", "title": "word2vec Parameter Learning Explained", "abstract": "The word2vec configuration and specific according Mikolov math\u00e9matiques mullah. have hundreds a still per of without 2004 recent held in. The velocity representations of words work later word2vec models have has believes to be try coming could semantic meanings there besides necessary another separate NLP designing. As has increasing multiple own how would like but brain with word2vec, I notice that there sufficient called material nothing rigorously? the formula_4 learning change of word2vec same explanation, thus affect all we with always neural spectrum experience from understanding how very word2vec contemporary.", "histories": [["v1", "Tue, 11 Nov 2014 09:24:00 GMT  (242kb,D)", "http://arxiv.org/abs/1411.2738v1", null], ["v2", "Fri, 13 Nov 2015 19:33:04 GMT  (569kb,D)", "http://arxiv.org/abs/1411.2738v2", null], ["v3", "Sat, 30 Jan 2016 21:35:51 GMT  (569kb,D)", "http://arxiv.org/abs/1411.2738v3", null], ["v4", "Sun, 5 Jun 2016 07:17:40 GMT  (569kb,D)", "http://arxiv.org/abs/1411.2738v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xin rong"], "accepted": false, "id": "1411.2738"}, "pdf": {"name": "1411.2738.pdf", "metadata": {"source": "CRF", "title": "word2vec Parameter Learning Explained", "authors": ["Xin Rong"], "emails": ["ronxin@umich.edu"], "sections": [{"heading": null, "text": "This note provides detailed derivations and explanations of the parameter update equations for the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram models, as well as advanced tricks, hierarchical soft-max and negative sampling. In the appendix a review is given on the basics of neuron network models and backpropagation."}, {"heading": "1 Continuous Bag-of-Word Model", "text": ""}, {"heading": "1.1 One-word context", "text": "We start from the simplest version of the continuous bag-of-word model (CBOW) introduced in Mikolov et al. (2013a). We assume that there is only one word considered per context, which means the model will predict one target word given one context word, which is like a bigram model.\nFigure 1 shows the network model under the simplified context definition1. In our setting, the vocabulary size is V , and the hidden layer size is N . The nodes on adjacent layers are fully connected. The input vector is a one-hot encoded vector, which means for a given input context word, only one node of {x1, \u00b7 \u00b7 \u00b7 , xV } will be 1, and all other nodes are 0.\nThe weights between the input layer and the output layer can be represented by a V \u00d7 N matrix W. Each row of W is the N -dimension vector representation vw of the\n1In Figures 1, 2, 3, and the rest of this note, W\u2032 is not the transpose of W, but a different matrix instead.\nar X\niv :1\n41 1.\n27 38\nv1 [\ncs .C\nL ]\n1 1\nN ov\n2 01\nInput layer Hidden layer Output layer\nassociated word of the input layer. Given a context (a word), assuming xk = 1 and xk\u2032 = 0 for k\u2032 6= k, then\nh = xTW = W(k,\u00b7) := vwI , (1)\nwhich is essentially copying the k-th row of W to h. vwI is the vector representation of the input word wI .\nFrom the hidden layer to the output layer, there is a different weight matrix W\u2032 = {w\u2032ij}, which is a N \u00d7 V matrix. Using these weights, we can compute a score uj for each word in the vocabulary,\nuj = v \u2032 wj T \u00b7 h, (2)\nwhere v\u2032wj is the j-th column of the matrix W \u2032. Then we can use soft-max, a log-linear classification model, to obtain the posterior distribution of words, which is a multinomial distribution.\np(wj |wI) = yj = exp(uj)\u2211V\nj\u2032=1 exp(uj\u2032) , (3)\nwhere yj is the output of the j-the node in the output layer. Substituting (1) and (2) into (3), we obtain\np(wj |wI) = exp\n( v\u2032wO TvwI ) \u2211V\nj\u2032=1 exp ( v\u2032 w\u2032j TvwI ) (4) Note that vw and v \u2032 w are two representations of the word w. vw comes from rows of W, which is the input\u2192hidden weight matrix, and v\u2032w comes from columns of W\u2032, which is the hidden\u2192output matrix. In subsequent analysis, we call vw as the \u201cinput vector\u201d, and v\u2032w as the \u201coutput vector\u201d of the word w.\nUpdate equation for hidden\u2192output weights\nLet us now derive the weight update equation for this model. Although the actual computation is impractical (explained below), we are doing the derivation to gain insights on this original model with no tricks applied. For a review of basics of backpropagation, see Appendix A.\nThe training objective (for one training sample) is to maximize (4), the conditional probability of observing the actual output word wO (denote its index in the output layer as j\u2217) given the input context word wI with regard to the weights.\nmax p(wO|wI) = max yj\u2217 (5) = max log yj\u2217 (6)\n= uj\u2217 \u2212 log V\u2211\nj\u2032=1\nexp(uj\u2032) := \u2212E, (7)\nwhere E = \u2212 log p(wO|wI) is our loss function (we want to minimize E), and j\u2217 is the index of the actual output word in the output layer. Note that this loss function can be understood as a special case of the cross-entropy measurement between two probabilistic distributions.\nLet us now derive the update equation of the weights between hidden and output layers. Take the derivative of E with regard to j-th node\u2019s net input uj , we obtain\n\u2202E \u2202uj = yj \u2212 tj := ej (8)\nwhere tj = 1(j = j \u2217), i.e., tj will only be 1 when the j-th node is the actual output word, otherwise tj = 0. Note that this derivative is simply the prediction error ej of the output layer.\nNext we take the derivative on w\u2032ij to obtain the gradient on the hidden\u2192output weights.\n\u2202E \u2202w\u2032ij = \u2202E \u2202uj \u00b7 uj \u2202w\u2032ij = ej \u00b7 hi (9)\nTherefore, using stochastic gradient descent, we obtain the weight updating equation for hidden\u2192output weights:\nw\u2032ij (new) = w\u2032ij (old) \u2212 \u03b7 \u00b7 ej \u00b7 hi. (10)\nor v\u2032wj (new) = v\u2032wj (old) \u2212 \u03b7 \u00b7 ej \u00b7 h for j = 1, 2, \u00b7 \u00b7 \u00b7 , V. (11)\nwhere \u03b7 > 0 is the learning rate, ej = yj \u2212 tj , and hi is the i-th node in the hidden layer; v\u2032wj is the output vector of wj . Note that this update equation implies that we have to go through every possible word in the vocabulary, check its output probability yj , and\ncompare yj with its expected output tj (either 0 or 1). If yj > tj (\u201coverestimating\u201d), then we subtract a proportion of the hidden vector h (i.e., vwI ) from v \u2032 wO\n, thus making v\u2032wO farther away from vwI ; if yj < tj (\u201cunderestimating\u201d), we add some h to v \u2032 wO\n, thus making v\u2032wO closer\n2 to vwI . If yj is very close to tj , then according to the update equation, very little change will be made to the weights. Note, again, that vw (input vector) and v \u2032 w (output vector) are two different vector representations of the word w.\nUpdate equation for input\u2192hidden weights\nHaving obtained the update equations for W\u2032, we can now move on to W. We take the derivative of E on the output of the hidden layer, obtaining\n\u2202E \u2202hi = V\u2211 j=1 \u2202E \u2202uj \u00b7 \u2202uj \u2202hi = V\u2211 j=1 ej \u00b7 w\u2032ij := EHi (12)\nwhere hi is the output of the i-th node of the hidden layer; uj is defined in (2), the net input of the j-th node in the output layer; and ej = yj \u2212 tj is the prediction error of the j-th word in the output layer. EH, a N -dim vector, is the sum of the output vectors of all words in the vocabulary, weighted by their prediction error.\nNext we should take the derivative of E on W. First,remember that the hidden layer performs an linear computation on the values from the input layer. Expanding the vector notation in (1) we get\nhi = V\u2211\nk=1\nxk \u00b7 wki (13)\nNow we can take the derivative of E with regard to W, obtaining\n\u2202E \u2202wki = \u2202E \u2202hi \u00b7 \u2202hi \u2202wki = EHi \u00b7 xk (14)\n\u2202E\n\u2202W = x \u00b7 EH (15)\nfrom which we obtain a V \u00d7 N matrix. Since only one component of x is non-zero, only one row of \u2202E\u2202W is non-zero, and the value of that row is EH, a N -dim vector. We obtain the update equation of W as\nv(new)wI = v (old) wI \u2212 \u03b7 \u00b7 EH (16)\nwhere vwI is a row of W, the \u201cinput vector\u201d of the only context word, and is the only row of W whose derivative is non-zero. We do not need to care about other rows of W because their derivatives are zero.\n2Here when I say \u201ccloser\u201d or \u201cfarther\u201d, I meant using the inner product instead of Euclidean as the distance measurement.\nIntuitively, since vector EH is the sum of output vectors of all words in vocabulary weighted by their prediction error ej = yj = tj , we can understand (16) as adding a portion of every output vector in vocabulary to the input vector of the context word. If, in the output layer, the probability of a word wj being the output word is overestimated (yj > tj), then the input vector of the context word wI will tend to move farther away from output vector of wj ; on the contrary, if the probability of wj being the output word is underestimated (yj < tj), then the input vector wI will tend to move closer to the output vector of wj ; if the probability of wj is fairly accurately predicted, then it will have little effect on the movement of the input vector of wI . The movement of the input vector of wI is determined by the prediction error of all vectors in the vocabulary; the larger the prediction error, the more significant effects a word will exert on the movement. This may be the explanation of why this model may obtain \u201cking\u201d - \u201cqueen\u201d = \u201cman\u201d - \u201cwoman\u201d. Imagine the word \u201cking\u201d being dragged around by different forces from the words it intimately co-occurs with; it may end up at a stabilized position determined by its most frequently co-occurring words. Because we see these word pairs so many times, these top-frequent neighbors will dominate the movements of the target word."}, {"heading": "1.2 Multi-word context", "text": "Figure 2 shows the CBOW model with a multi-word context setting. When computing the hidden layer output, instead of directly copying the input vector of the input context word, the CBOW model takes the average of the vectors of the input context words, and use the product of the input\u2192hidden weight matrix and the average vector as the output.\nh = 1\nC W \u00b7 (x1 + x2 + \u00b7 \u00b7 \u00b7+ xC) (17)\n= 1\nC \u00b7 (vw1 + vw2 + \u00b7 \u00b7 \u00b7+ vwC ) (18)\nwhere C is the number of words in the context, w1, \u00b7 \u00b7 \u00b7 , wC are the words the in the context, and vw is the input vector of a word w. The loss function is\nE = = \u2212 log p(wO|wI,1, \u00b7 \u00b7 \u00b7 , wI,C) (19)\n= \u2212uj\u2217 + log V\u2211\nj\u2032=1\nexp(uj\u2032) (20)\n= \u2212v\u2032wO T \u00b7 h + log V\u2211 j\u2032=1 exp(v\u2032wj T \u00b7 h) (21)\nwhich is the same as (7), the objective of the one-word-context model, except that h is different, as defined in (18) instead of (1).\nThe update equation for the hidden\u2192output weights stay the same as that for the one-word-context model (11). We copy it here:\nv\u2032wj (new) = v\u2032wj (old) \u2212 \u03b7 \u00b7 ej \u00b7 h for j = 1, 2, \u00b7 \u00b7 \u00b7 , V. (22)\nNote that we need to apply this to every element of of the hidden\u2192output weight matrix for each training instance.\nThe update equation for input\u2192hidden weights is similar to (16), except that now we need to apply the following equation for every word wI,c in the context:\nv(new)wI,c = v (old) wI,c \u2212 1 C \u00b7 \u03b7 \u00b7 EH for c = 1, 2, \u00b7 \u00b7 \u00b7 , C. (23)\nwhere vwI,c is the input vector of the c-th word in the input context; \u03b7 is a positive learning rate; and EH = \u2202E\u2202hi is given by (12). The intuitive understanding of this update equation is the same as that for (16)."}, {"heading": "2 Skip-Gram Model", "text": "The skip-gram model is introduced in Mikolov et al. (2013a,b). Figure 3 shows the skipgram model. It is the opposite of the CBOW model. The target word is now at the input layer, and the context words are on the output layer.\nWe still use vwI to denote the input vector of the only word on the input layer, and thus we have the same definition of the hidden-layer outputs h as in (1), which means h is simply copying a row of the input\u2192hidden weight matrix, W, associated with the input word wI . We copy the definition of h below:\nh = W(k,\u00b7) := vwI , (24)\nOn the output layer, instead of outputing one multinomial distribution, we are outputing C multinomial distributions. Each output is computed using the same hidden\u2192output matrix:\np(wc,j = wO,c|wI) = yc,j = exp(uc,j)\u2211V j\u2032=1 exp(uj\u2032)\n(25)\nwhere wc,j is the j-th word on the c-th panel of the output layer; wO,c is the actual c-th word in the output context words; wI is the only input word; yc,j is the output of the j-th\nnode on the c-th panel of the output layer; uc,j is the net input of the j-th node on the c-th panel of the output layer. Because the output layer panels share the same weights, thus\nuc,j = uj = v \u2032 wj T \u00b7 h, for c = 1, 2, \u00b7 \u00b7 \u00b7 , C (26)\nwhere v\u2032wj is the output vector of the j-th word in the vocabulary, wj , and also v \u2032 wj is taken from a column of the hidden\u2192output weight matrix, W\u2032. The derivation of parameter update equations is not so different from the one-wordcontext model. The loss function is changed to\nE = \u2212 log p(wO,1, wO,2, \u00b7 \u00b7 \u00b7 , wO,C |wI) (27)\n= \u2212 log C\u220f c=1 exp(uc,j\u2217c )\u2211V j\u2032=1 exp(uj\u2032)\n(28)\n= \u2212 C\u2211 c=1 uj\u2217c + C \u00b7 log V\u2211 j\u2032=1 exp(uj\u2032) (29)\nwhere j\u2217c is the index of the actual c-th output context word in the vocabulary. We take the derivative of E with regard to the net input of every node on every panel of the output layer, uc,j and obtain\n\u2202E\n\u2202uc,j = yc,j \u2212 tc,j := ec,j (30)\nwhich is the prediction error on the node, the same as in (8). For notation simplicity, we define a V -dimensional vector EI = {EI1, \u00b7 \u00b7 \u00b7 ,EIV } as the sum of prediction errors over all context words:\nEIj = C\u2211 c=1 ec,j (31)\nNext, we take the derivative of E with regard to the hidden\u2192output matrix W\u2032, and obtain\n\u2202E\n\u2202w\u2032ij = C\u2211 c=1 \u2202E \u2202uc,j \u00b7 \u2202uc,j \u2202w\u2032ij = EIj \u00b7 hi (32)\nThus we obtain the update equation for the hidden\u2192output matrix W\u2032,\nw\u2032ij (new) = w\u2032ij (old) \u2212 \u03b7 \u00b7 EIj \u00b7 hi (33)\nor v\u2032wj (new) = v\u2032wj (old) \u2212 \u03b7 \u00b7 EIj \u00b7 h for j = 1, 2, \u00b7 \u00b7 \u00b7 , V. (34)\nThe intuitive understanding of this update equation is the same as that for (11), except that the prediction error is summed across all context words in the output layer. Note that\nwe need to apply this update equation for every element of the hidden\u2192output matrix for each training instance.\nThe derivation of the update equation for the input\u2192hidden matrix is identical to (12) to (16), except taking into account that the prediction error ej is replaced with EIj . We directly give the update equation:\nv(new)wI = v (old) wI \u2212 \u03b7 \u00b7 EH (35)\nwhere EH is an N -dim vector, each component of which is defined as\nEHi = V\u2211 j=1 EIj \u00b7 w\u2032ij . (36)\nThe intuitive understanding of (35) is the same as that for (16)."}, {"heading": "3 Optimizing Computational Efficiency", "text": "So far the models we have discussed (\u201cbigram\u201d model, CBOW and skip-gram) are both in their original forms, without any efficiency optimization tricks being applied.\nFor all these models, there exist two vector presentations for each word in the vocabulary: the input vector vw, and the output vector v \u2032 w. Learning the input vectors is cheap; but learning the output vectors is very expensive. From the update equations (22) and (33), we can find that, in order to update v\u2032w, for each training instance, we have to iterate through every word wj in the vocabulary, compute their net input uj , probability prediction yj (or yc,j for skip-gram), their prediction error ej (or EIj for skip-gram), and finally use their prediction error to update their output vector v\u2032j .\nDoing such computations for all words for each training instance is very expensive, making it impractical to scale up to large training corpora. To solve this problem, an intuition is to limit the number of output vectors we have to update per training instance. One elegant approach to achieving this is hierarchical soft-max; another approach is through sampling, which will be discussed in the next section.\nBoth tricks optimize only the computation of the updates for output vectors. In our derivations, we care about three values: (1) E, the new objective function; (2) \u2202E\u2202v\u2032w , the new update equation for the output vectors; and (3) \u2202E\u2202h , the weighted sum of predictions errors to be backpropagated for updating input vectors."}, {"heading": "3.1 Hierarchical Softmax", "text": "Hierarchical soft-max is an efficient way of computing soft-max (Morin and Bengio, 2005; Mnih and Hinton, 2009). The model uses a binary tree to represent all words in the vocabulary. The V words must be leaf nodes of the tree. It can be proved that there are\nV \u2212 1 inner nodes. For each leaf node, there exists an unique path from the root to the node; and this path is used to estimate the probability of the word represented by the leaf node. See Figure 4 for an example tree.\nIn the hierarchical soft-max model, there is no output vector representation for words. Instead, each of the V \u2212 1 inner nodes has an output vector v\u2032n(w,j). And the probability of a word being the output word is defined as\np(w = wO) = L(w)\u22121\u220f j=1 \u03c3 ( Jn(w, j + 1) = ch(n(w, j))K \u00b7 v\u2032n(w,j) T h )\n(37)\nwhere ch(n) is the left child of node n; v\u2032n(w,j) is the vector representation (\u201coutput vector\u201d) of the inner node n(w, j); h is the output value of the hidden layer (in the skip-gram model h = vwI ; and in CBOW, h = 1 C \u2211C c=1 vwc); JxK is a special function defined as\nJxK = { 1 if x is true;\n\u22121 otherwise. (38)\nLet us intuitively understand the equation by going through an example. Looking at Figure 4, suppose we want to compute the probability that w2 being the output word. We define this probability as the probability of a random walk starting from the root ending at the leaf node in question. At each inner node (including the root node), we need to assign the probabilities of going left and going right 3. We define the probability of going left at\n3While an inner node of a binary tree may not always have both children, a binary Huffman tree\u2019s inner nodes always do. Although theoretically one can use many different types of trees for hierarchical softmax, word2vec uses a binary Huffman tree for fast training.\nan inner node n to be p(n, left) = \u03c3 ( v\u2032n T \u00b7 h )\n(39)\nwhich is determined by both the vector representation of the inner node, and the hidden layer\u2019s output value (which is then determined by the vector representation of the input word(s)). Apparently the probability of going right at node n is\np(n, right) = 1\u2212 \u03c3 ( v\u2032n T \u00b7 h ) = \u03c3 ( \u2212v\u2032n T \u00b7 h )\n(40)\nFollowing the path from the root to w2 in Figure 4, we can compute the probability of w2 being the output word as\np(w2 = wO) = p (n(w2, 1), left) \u00b7 p (n(w2, 2), left) \u00b7 p (n(w2, 3), right) (41) = \u03c3 ( v\u2032n(w2,1) T h ) \u00b7 \u03c3 ( v\u2032n(w2,2) T h ) \u00b7 \u03c3 ( \u2212v\u2032n(w2,3) T h ) (42)\nwhich is exactly what is given by (37). It should not be hard to verify that\nV\u2211 i=1 p(wi = wO) = 1 (43)\nmaking the hierarchical softmax a well defined multinomial distribution among all words. Now let us derive the parameter update equation for the vector representations of the inner nodes. For simplicity, we look at the one-word context model first. Extending the update equations to CBOW and skip-gram models is easy.\nFor the simplicity of notation, we define the following shortenizations without introducing ambiguity:\nJ\u00b7K := Jn(w, j + 1) = ch(n(w, j))K (44) v\u2032j := v \u2032 nw,j (45)\nFor a training instance, the error function is defined as\nE = \u2212 log p(w = wO|wI) = \u2212 L(w)\u22121\u2211 j=1 log \u03c3(J\u00b7Kv\u2032j T h) (46)\nWe take the derivative of E with regard to v\u2032jh, obtaining\n\u2202E\n\u2202v\u2032jh =\n( \u03c3(J\u00b7Kv\u2032j T h)\u2212 1 ) J\u00b7K (47)\n=\n{ \u03c3(v\u2032j\nTh)\u2212 1 (J\u00b7K = 1) \u03c3(v\u2032j Th) (J\u00b7K = \u22121) (48)\n= \u03c3(v\u2032j T h)\u2212 tj (49)\nwhere tj = 1 if J\u00b7K = 1 and tj = 0 otherwise. Next we take the derivative of E with regard to the vector representation of the inner node n(w, j) and obtain\n\u2202E \u2202v\u2032j = \u2202E \u2202v\u2032jh \u00b7 \u2202v\u2032jh \u2202v\u2032j = ( \u03c3(v\u2032j T h)\u2212 tj ) \u00b7 h (50)\nwhich results in the following update equation:\nv\u2032j (new) = v\u2032j (old) \u2212 \u03b7 ( \u03c3(v\u2032j T h)\u2212 tj ) \u00b7 h (51)\nwhich should be applied to j = 1, 2, \u00b7 \u00b7 \u00b7 , L(w)\u2212 1. We can understand \u03c3(v\u2032j Th)\u2212 tj as the prediction error for the inner node n(w, j). The \u201ctask\u201d for each inner node is to predict whether it should follow the left child or the right child in the random walk. tj = 1 means the ground truth is to follow the left child; tj = 0 means it should follow the right child. \u03c3(v\u2032j Th) is the prediction result. For a training instance, if the prediction of the inner node is very close to the ground truth, then its vector representation v\u2032j will move very little; otherwise v\u2032j will move in an appropriate direction by moving (either closer or farther away 4 from h) so as to reduce the prediction error for this instance. This update equation can be used for both CBOW and the skip-gram model. When used for the skip-gram model, we need to repeat this update procedure for each of the C words in the output context.\nIn order to backpropagate the error to learn input\u2192hidden weights, we take the derivative of E with regard to the output of the hidden layer and obtain\n\u2202E \u2202h = L(w)\u22121\u2211 j=1 \u2202E \u2202v\u2032jh \u00b7 \u2202v\u2032jh \u2202h (52)\n= L(w)\u22121\u2211 j=1 ( \u03c3(v\u2032j T h)\u2212 tj ) \u00b7 v\u2032j (53)\n:= EH (54)\nwhich can be directly substituted into (23) to obtain the update equation of input vectors for CBOW; or substituted into (35) for the skip-gram model. When applied to the skipgram model, we need to take the sum of EH for every word in the context as the derivative to be applied into (35).\nFrom the update equations, we can see that the computational complexity per training instance per context word is reduced from O(V ) to O(log(V )), which is a big improvement in speed. We still have roughly the same number of parameters (V \u2212 1 inner-node vectors compared to originally V word-output vectors).\n4Again, the distance measurement is inner product."}, {"heading": "3.2 Negative Sampling", "text": "The idea of negative sampling is more straightforward than hierarchical softmax: in order to deal with there being too many nodes to compute for one iteration, we can just sample some of the nodes which we compute scores for.\nApparently we need to keep the output word in the ground truth as the positive sample. We need to sample several negative examples (thus the name \u201cnegative sampling\u201d). The distribution of words for sampling (aka the noise distribution Pn(w) can be arbitrary. One can use experiments to empirically determine a good distribution5.\nIn word2vec (Mikolov et al., 2013b), instead of using a form of negative sampling that produces a well-defined posterior multinomial distribution, the authors use the following simplified training objective function per output word instance6,\nE = \u2212 log \u03c3(v\u2032wO T h)\u2212 K\u2211 i=1 log \u03c3(\u2212v\u2032wi T h) (55)\nwhere {wi|i = 1, \u00b7 \u00b7 \u00b7 ,K} are sampled from the \u201cnoise distribution\u201d Pn(w). h is the output value of the hidden layer. In the skip-gram model h = vwI ; and in the CBOW model, h = 1C \u2211C c=1 vwc . Vector vw is the input vector of word w, taken from a row from the input\u2192hidden weight matrix; and v\u2032w is the output vector, taken from a column from the hidden\u2192output weight matrix. Each word has two vector representations.\nWe take the derivative of E with regard to a scalar, v\u2032wj Th, and obtain\n\u2202E\n\u2202v\u2032wj Th\n=\n{ \u03c3(v\u2032wj\nTh)\u2212 1 for wj = wO \u03c3(v\u2032wj Th) for wj \u2208 {wi|i = 1, \u00b7 \u00b7 \u00b7 ,K} (56)\n= \u03c3(v\u2032wj T h)\u2212 tj (57)\nwhere tj is the \u201clabel\u201d of word wj . t = 1 when wj is a positive sample; t = 0 otherwise. Next we take the derivative of E with regard to the output vector of a word v\u2032wj and obtain\n\u2202E\n\u2202v\u2032wj =\n\u2202E \u2202v\u2032wj Th \u00b7 \u2202v\u2032wj Th \u2202v\u2032wj = ( \u03c3(v\u2032wj T h)\u2212 tj ) h (58)\nwhich results in the following update equation for output vectors\nv\u2032wj (new) = v\u2032wj (old) \u2212 \u03b7 ( \u03c3(v\u2032wj T h)\u2212 tj ) h (59)\nfor wj \u2208 {wO} \u222a {wi \u223c Pn(w) | i = 1, \u00b7 \u00b7 \u00b7 ,K} 5As discussed in (Mikolov et al., 2013b), word2vec uses unigram distribution raised to 3/4th power for best quality of results 6Goldberg and Levy (2014) provide a theoretical analysis on the reason of using this objective function.\nwhich is significantly less amount of computation compared to (11), in which we had to go through every word in the vocabulary. The intuition to understand this update equation is the same as for (11). This equation can be used for both CBOW and the skip-gram model. For the skip-gram model, we apply this equation for one context word at a time.\nTo backpropagate the error to the hidden layer and thus update the input vectors of words, we need to take the derivative of E with regard to the hidden layer\u2019s output, obtaining\n\u2202E \u2202h =\n\u2202E \u2202v\u2032wj Th \u00b7 \u2202v\u2032wj Th \u2202h = ( \u03c3(v\u2032wj T h)\u2212 tj ) v\u2032wj := EH (60)\nBy plugging EH to (23) we obtain the update equation of input vectors for the CBOW model; or (35) for the skip-gram model. When applied to the skip-gram model, we need to take the sum of EH for every context and plug the summed EH into (35)."}, {"heading": "4 Discussions", "text": ""}, {"heading": "4.1 Why softmax?", "text": "Using softmax can help us obtain a well-defined probabilistic (multinomial) distribution among words. The loss function for this model is cross-entropy. Theoretically we can also use squared sum as the error function, but then the efficiency performance optimization tricks developed for softmax will not apply."}, {"heading": "4.2 Relationship between input vectors and output vectors?", "text": "Input vectors are taken from rows of the input\u2192hidden weight matrix, and output vectors are taken from columns of the hidden\u2192output weight matrix. By definition they are two different vector representations for words.\nBased on the parameter update equations (16) and (11), intuitively the vectors for the same word should be close to each other. For hierarchical softmax, the output vectors have different meanings; but in negative sampling, one can experiment with forcing the two vectors to be the same and see what will happen."}, {"heading": "A Back Propagation Basics", "text": "A.1 Learning Algorithms for a Single Neuron\nFigure 5 shows an artificial neuron. {x1, \u00b7 \u00b7 \u00b7 , xK} are input values; {w1, \u00b7 \u00b7 \u00b7 , wK} are weights; y is a scalar output; and f is the link function (also called activation/decision/transfer function).\nThe neuron works in the following way:\ny = f(u), (61)\nwhere u is a scalar number, which is the net input (or \u201cnew input\u201d) of the neuron. u is defined as\nu = K\u2211 i=0 wixi. (62)\nUsing vector notation, we can write u = wTx (63)\nNote that here we ignore the bias term in u. To include a bias term, one can simply add an input dimension (e.g., x0) that is constant 1.\nApparently, different link functions result in distinct behaviors of the neuron. We discuss two example choices of link functions here.\nThe first example choice of f(u) is the unit step function (aka Heaviside step function):\nf(u) =\n{ 1 if u > 0\n0 otherwise (64)\nA neuron with this link function is called a perceptron. The learning algorithm for a perceptron is the perceptron algorithm. Its update equation is defined as:\nw(new) = w(old) \u2212 \u03b7 \u00b7 (y \u2212 t) \u00b7 x (65)\nwhere t is the label (gold standard) and \u03b7 is the learning rate (\u03b7 > 0). Note that a perceptron is a linear classifier, which means its description capacity can be very limited. If we want to fit more complex functions, we need to use a non-linear model.\nThe second example choice of f(u) is the logistic function (a most common kind of sigmoid function), defined as\n\u03c3(u) = 1\n1 + e\u2212u (66)\nThe logistic function has two primary good properties: (1) the output y is always between 0 and 1, and (2) unlike a unit step function, \u03c3(u) is smooth and differentiable, making the derivation of update equation very easy.\nNote that \u03c3(u) also has the following two properties that can be very convenient and will be used in our subsequent derivations:\n\u03c3(\u2212u) = 1\u2212 \u03c3(u) (67)\nd\u03c3(u)\ndu = \u03c3(u)\u03c3(\u2212u) (68)\nWe use stochastic gradient descent as the learning algorithm of this model. In order to derive the update equation, we need to define the error function, i.e., the training objective. The following objective function seems to be convenient:\nE = 1\n2 (t\u2212 y)2 (69)\nWe take the derivative of E with regard to wi,\n\u2202E \u2202wi = \u2202E \u2202y \u00b7 \u2202y \u2202u \u00b7 \u2202u \u2202wi\n(70)\n= (y \u2212 t) \u00b7 y(1\u2212 y) \u00b7 xi (71)\nwhere \u2202y\u2202u = y(1 \u2212 y) because y = f(u) = \u03c3(u), and (67) and (68). Once we have the derivative, we can apply stochastic gradient descent:\nw(new) = w(old) \u2212 \u03b7 \u00b7 (y \u2212 t) \u00b7 y(1\u2212 y) \u00b7 x. (72)\nA.2 Back-propagation with Multi-Layer Network\nFigure 6 shows a multi-layer neural network with an input layer {xk} = {x1, \u00b7 \u00b7 \u00b7 , xK}, a hidden layer {hi} = {h1, \u00b7 \u00b7 \u00b7 , hN}, and an output layer {yj} = {y1, \u00b7 \u00b7 \u00b7 , yM}. For clarity we use k, i, j as the subscript for input, hidden, and output layer nodes respectively. We use ui and u\u2032j to denote the net input of hidden layer nodes and output layer nodes respectively.\nInput layer Hidden layer Output layer\nWe want to derive the update equation for learning the weights wki between the input and hidden layers, and w\u2032ij between the hidden and output layers. We assume that all the computation units (i.e., nodes in the hidden layer and the output layer) use the logistic function \u03c3(u) as the link function. Therefore, for a node hi in the hidden layer, its output is defined as\nhi = \u03c3(ui) = \u03c3 ( K\u2211 k=1 wkixk ) . (73)\nSimilarly, for a node yj in the output layer, its output is defined as\nyj = \u03c3(u \u2032 j) = \u03c3 ( N\u2211 i=1 w\u2032ijhi ) . (74)\nWe use the squared sum error function given by\nE(x, t,W,W\u2032) = 1\n2 M\u2211 j=1 (yj \u2212 tj)2, (75)\nwhere W = {wki}, a K \u00d7 N weight matrix (input-hidden), and W\u2032 = {w\u2032ij}, a N \u00d7M weight matrix (hidden-output). t = {t1, \u00b7 \u00b7 \u00b7 , tM}, a M -dimension vector, which is the gold-standard labels of output.\nTo obtain the update equations for wki and w \u2032 ij , we simply need to take the derivative of the error function E with regard to the weights respectively. To make the derivation straightforward, we do start computing the derivative for the right-most layer (i.e., the output layer), and then move left. For each layer, we split the computation into three steps, computing the derivative of the error with regard to the output, net input, and weight respectively. This process is shown below.\nWe start with the output layer. The first step is to compute the derivative of the error w.r.t. the output:\n\u2202E \u2202yj = yj \u2212 tj . (76)\nThe second step is to compute the derivative of the error with regard to the net input of the output layer. Note that when taking derivatives with regard to something, we need to keep everything else fixed. Also note that this value is very important because it will be reused multiple times in subsequent computations. We denote it as EI\u2032j for simplicity.\n\u2202E \u2202u\u2032j = \u2202E \u2202yj \u00b7 \u2202yj \u2202u\u2032j = (yj \u2212 tj) \u00b7 yj(1\u2212 yj) := EI\u2032j (77)\nThe third step is to compute the derivative of the error with regard to the weight between the hidden layer and the output layer.\n\u2202E \u2202w\u2032ij = \u2202E \u2202u\u2032j \u00b7 \u2202u\u2032j \u2202w\u2032ij = EI\u2032j \u00b7 hi (78)\nSo far, we have obtained the update equation for weights between the hidden layer and the output layer.\nw\u2032ij (new) = w\u2032ij (old) \u2212 \u03b7 \u00b7 \u2202E\n\u2202w\u2032ij (79)\n= w\u2032ij (old) \u2212 \u03b7 \u00b7 EI\u2032j \u00b7 hi. (80)\nwhere \u03b7 > 0 is the learning rate. We can repeat the same three steps to obtain the update equation for weights of the previous layer, which is essentially the idea of back propagation. We repeat the first step and compute the derivative of the error with regard to the output of the hidden layer. Note that the output of the hidden layer is related to all nodes in the output layer.\n\u2202E \u2202hi = M\u2211 j=1 \u2202E \u2202n\u2032j \u2202n\u2032j \u2202hi = M\u2211 j=1 EI\u2032j \u00b7 w\u2032ij . (81)\nThen we repeat the second step above to compute the derivative of the error with regard to the net input of the hidden layer. This value is again very important, and we denote it\nas EIi.\n\u2202E \u2202ui = \u2202E \u2202hi \u00b7 \u2202hi \u2202ui = M\u2211 j=1 EI\u2032j \u00b7 w\u2032ij \u00b7 hi(1\u2212 hi) := EIi (82)\nNext we repeat the third step above to compute the derivative of the error with regard to the weights between the input layer and the hidden layer.\n\u2202E \u2202wki = \u2202E \u2202ui \u00b7 \u2202ui \u2202wki = EIi \u00b7 xk, (83)\nFinally, we can obtain the update equation for weights between the input layer and the hidden layer.\nwki (new) = wki (old) \u2212 \u03b7 \u00b7 EIi \u00b7 xk. (84)\nFrom the above example, we can see that the intermediate results (EI\u2032j) when computing the derivatives for one layer can be reused for the previous layer. Imagine there were another layer prior to the input layer, then EIi can also be reused to continue computing the chain of derivatives efficiently. Compare Equations (77) and (82), we may find that in (82), the factor \u2211M j=1 EI \u2032 jw \u2032 ij is just like the \u201cerror\u201d of the hidden layer node hi. We may interpret this term as the error \u201cback-propagated\u201d from the next layer, and this propagation may go back further if the network has more hidden layers."}], "references": [{"title": "word2vec explained: deriving mikolov et al.\u2019s negativesampling word-embedding method. arXiv:1402.3722 [cs, stat", "author": ["Y. Goldberg", "O. Levy"], "venue": null, "citeRegEx": "Goldberg and Levy,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Information Processing Systems,", "citeRegEx": "Mnih and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Curran Associates", "F. Inc. Morin", "Y. Bengio"], "venue": "Information Processing Systems", "citeRegEx": "Associates et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Associates et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 1, "context": "1 One-word context We start from the simplest version of the continuous bag-of-word model (CBOW) introduced in Mikolov et al. (2013a). We assume that there is only one word considered per context, which means the model will predict one target word given one context word, which is like a bigram model.", "startOffset": 111, "endOffset": 134}, {"referenceID": 2, "context": "1 Hierarchical Softmax Hierarchical soft-max is an efficient way of computing soft-max (Morin and Bengio, 2005; Mnih and Hinton, 2009).", "startOffset": 87, "endOffset": 134}, {"referenceID": 0, "context": ", 2013b), word2vec uses unigram distribution raised to 3/4th power for best quality of results Goldberg and Levy (2014) provide a theoretical analysis on the reason of using this objective function.", "startOffset": 95, "endOffset": 120}], "year": 2014, "abstractText": "The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been proven to be able to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec, I notice that there lacks a material that comprehensively explains the parameter learning process of word2vec in details, thus preventing many people with less neural network experience from understanding how exactly word2vec works. This note provides detailed derivations and explanations of the parameter update equations for the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram models, as well as advanced tricks, hierarchical soft-max and negative sampling. In the appendix a review is given on the basics of neuron network models and backpropagation. 1 Continuous Bag-of-Word Model 1.1 One-word context We start from the simplest version of the continuous bag-of-word model (CBOW) introduced in Mikolov et al. (2013a). We assume that there is only one word considered per context, which means the model will predict one target word given one context word, which is like a bigram model. Figure 1 shows the network model under the simplified context definition1. In our setting, the vocabulary size is V , and the hidden layer size is N . The nodes on adjacent layers are fully connected. The input vector is a one-hot encoded vector, which means for a given input context word, only one node of {x1, \u00b7 \u00b7 \u00b7 , xV } will be 1, and all other nodes are 0. The weights between the input layer and the output layer can be represented by a V \u00d7 N matrix W. Each row of W is the N -dimension vector representation vw of the In Figures 1, 2, 3, and the rest of this note, W\u2032 is not the transpose of W, but a different matrix instead. 1 ar X iv :1 41 1. 27 38 v1 [ cs .C L ] 1 1 N ov 2 01 4 Input layer Hidden layer Output layer x1 x2 x3 xk", "creator": "LaTeX with hyperref package"}}}