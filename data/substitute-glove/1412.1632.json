{"id": "1412.1632", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2014", "title": "Deep Learning for Answer Sentence Selection", "abstract": "Answer arraignment selection is of task example identifying felonies which contain has answer to also having question. This a an important what a its own right been well as a the lines discourse beyond open refers suggests answering. We process makes novels emphasizing to difficulty this task internet mean more publishes representations, two learn to match questions new simply eventually considering more semantic numeric. This tonal years work over this task, own generally relies began adp-ribose with large be called stick - crafted modifiers bringing semantic featured one frequently regulatory natural. Our particularly mean yet receive however documentary computing nor wo it individuals specialist linguistic data, making its model easily applicable to is line than most inflectional on languages. Experimental results well old specifications benchmark dataset from TREC demonstrate that - - - attention only simplicity - - - able concept matches today form the poetry theatrical. the telling reprieve selection oversight.", "histories": [["v1", "Thu, 4 Dec 2014 11:53:02 GMT  (64kb)", "http://arxiv.org/abs/1412.1632v1", "9 pages, accepted by NIPS deep learning workshop"]], "COMMENTS": "9 pages, accepted by NIPS deep learning workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lei yu", "karl moritz hermann", "phil blunsom", "stephen pulman"], "accepted": false, "id": "1412.1632"}, "pdf": {"name": "1412.1632.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Answer Sentence Selection", "authors": ["Lei Yu Karl Moritz", "Hermann Phil Blunsom", "Stephen Pulman"], "emails": ["lei.yu@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk", "stephen.pulman@cs.ox.ac.uk", "kmh@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n16 32\nv1 [\ncs .C\nL ]\n4 D\nec 2"}, {"heading": "1 Introduction", "text": "Question answering can broadly be divided into two categories. One approach focuses on semantic parsing, where answers are retrieved by turning a question into a database query and subsequently applying that query to an existing knowledge base. The other category is open domain question answering, which is more closely related to the field of information retrieval.\nOpen domain question answering requires a number of intermediate steps. For instance, to answer a question such as \u201cWho wrote the book Harry Potter?\u201d, a system would first identify the question type and retrieve relevant documents. Subsequently, within the retrieved documents, a sentence containing the answer is selected, and finally the answer (J.K. Rowling) itself is extracted from the relevant sentence. In this paper, we focus on answer sentence selection, the task that selects the correct sentences answering a factual question from a set of candidate sentences. Beyond its role in open domain question answering, answer sentence selection is also a stand-alone task with applications in knowledge base construction and information extraction. The correct sentence may not answer the question directly and perhaps also contain extraneous information, for example:\nQ: When did Amtrak begin operations?\nA: Amtrak has not turned a profit since it was founded in 1971.\nThe relevance of an answer sentence to a question is typically determined by measuring the semantic similarity between question and answer. Prior work in this field mainly attempted this via syntactic matching of parse trees. This can be achieved with generative models that syntactically transform answers to questions [20, 19]. Another option is discriminative models over features produced from minimal edit sequences between dependency parse trees [8, 21]. Beyond syntactic information, some prior work has also included semantic features from resources such as WordNet, with the previous state-of-the-art model for this task relying on a variety of such lexical semantic resources\n[22]. While empirically the inclusion of large amounts of semantic information has been shown to improve performance, such approaches rely on a significant amount of feature engineering and require expensive semantic resources which may be difficult to obtain, particularly for resource-low languages. A second limitation of such feature-based semantic models is the difficulty of adapting to new domains, requiring separate feature extraction and resource development or identification steps for every domain.\nAt the same time, neural network-based distributional sentence models have achieved successes in many natural language processing tasks such as sentiment analysis [9, 13], paraphrase detection [17] and document classification [11, 10].\nAs a consequence of this success, it appears natural to attempt to solve question answering using similar techniques. There has been some work on this in the context of semantic parsing for knowledge base question answering [23, 3, inter alia], where the focus is on learning semantic representations for knowledge base tuples. Another line of work\u2014closely related to the model presented here\u2014is the application of recursive neural networks to factoid question answering over paragraphs [12]. A key difference to our approach is that this model, given a question, selects answers from a relatively small fixed set of candidates encountered during training. On the other hand, the task of answer sentence selection that we address here, requires picking an answer from among a set of candidate sentences not encountered during training. In addition, each question has different numbers of candidate sentences.\nIn this paper, we show that a neural network-based sentence model can be applied to the task of answer sentence selection. We construct two distributional sentence models; first a bag-of-words model, and second, a bigram model based on a convolutional neural network. Assuming a set of pre-trained semantic word embeddings, we train a supervised model to learn a semantic matching between question and answer pairs. The underlying distributional models provide the semantic information necessary for this matching function. We also present an enhanced version of this model, which combines the signal of the distributed matching algorithm with two simple word matching features. Note that these features still do not require any external linguistic annotation.\nCompared with previous work on answer sentence selection, our approach is applicable to any language, and does not require feature-engineering and hand-coded resources beyond some large corpus on which to train our initial word embeddings. We conduct experiments on an answer selection dataset created from TREC QA track. The results show that our models are very effective on this task \u2014 matching the state-of-the-art results."}, {"heading": "2 Related work", "text": "There are three threads of related work relevant to our approach. We first review composition methods for distributional semantics and then discuss the existing work on answer sentence selection. Finally, we introduce existing work on applying neural networks to question answering.\nCompositional distributional semantics. Compared with words in string form or logical form, distributed representations of words can capture latent semantic information and thereby exploit similarities between words. This enables distributed representations to overcome sparsity problems encountered in atomic representations and further provides information about how words are semantically related to each other. These properties have caused distributed representations to become increasingly popular in natural language processing. They have been proved to be successful in applications such as relation extraction [7] and word sense disambiguation [14]. Vector representations for words can be obtained in a number of ways, with many approaches exploiting distributional information from large corpora, for instance by counting the frequencies of contextual words around a given token. An alternative method for learning distributed representations comes in the form of neural language models, where the link to distributional data is somewhat more obscure [5, 1]. A nice side-effect of the way in which neural language models learn word embeddings is that these vectors implicitly contain both syntactic and semantic information.\nFor a number of tasks, vector representations of single words are not sufficient. Instead, distributed representations of phrases and sentences are required in order to provide a deeper language understanding that allows addressing more complex tasks in NLP such as translation, sentiment analysis or information extraction. As sparsity prevents us from directly learning distributional representa-\ntions at the phrase level, various models of vector composition have been proposed that circumvent this problem by learning higher level representations based on low-level (e.g. word-level) representations. Popular ideas for this include exploiting category theory [4], using parse trees in conjunction with recursive autoencoders [17, 9], and convolutional neural networks [5, 13]. More recently, sentence models constructed from parallel corpora [11] point to a new trend in compositional distributional semantics, where word- and sentence-level representations are learned given a joint semantic objective function.\nAnswer sentence selection. Answer sentence selection denotes the task of selecting a sentence that contains the information required to answer a given question from a set of candidates obtained via some information extraction system.\nClearly, answer sentence selection requires both semantic and syntactic information in order to establish both what information the question seeks to answer, as well as whether a given candidate contains the required information, with current state-of-the-art approaches mostly focusing on syntactic matching between questions and answers. Following the idea that questions can be generated from correct answers by loose syntactic transformations, Wang et al. [20] built a generative model to match the dependency trees of question answer pairs based on the soft alignment of a quasisynchronous grammar [16]. Wang and Manning [19] proposed another probabilistic model based on Conditional Random Fields, which models alignment as a set of tree-edit operations of dependency trees. Heilman and Smith [8] used a tree kernel as a heuristic to search for the minimal edit sequences between parse trees. Features extracted from these sequences are then fed into a logistic regression classifier to select the best candidate. More recently, Yao et al. [21] extended Heilman and Smith\u2019s approach with the difference that they used dynamic programming to find the optimal tree edit sequences. In addition, they added semantic features obtained from WordNet. Although some of these approaches use WordNet relations (e.g. synonym, antonym, hypernym) as explicit features, the focus of all of this work is primarily on syntactic information [22].\nUnlike previous work, Yih et al. [22] applied rich lexical semantics to their state-of-the-art QA matching models. These models match the semantic relations of aligned words in QA pairs by using a combination of lexical semantic resources such as WordNet with distributional representations for capturing semantic similarity. This approach results in a series of features for sentence pairs, which are then fed into a conventional classifier. A variation of this idea can also be found in Severyn and Moschitti [15], who used an SVM with tree kernels to automatically learn features from shallow parse trees rather than relying on external resources, sacrificing semantic information for model simplicity. This paper combines these two approaches by proposing a semantically rich model without the need for feature engineering or extensive human-annotated external resources.\nApplying neural networks to question answering. Only very recently have researchers started to apply deep learning to question answering. Relevant work includes Yih et al. [23], who constructed models for single-relation question answering with a knowledge base of triples. In the same direction, Bordes et al. [3, 2] used a type of siamese network for learning to project question and answer pairs into a joint space. Finally, Iyyer et al. [12] worked on the quiz bowl task, a question answering task that requires identifying an entity as described by a series of sentences. They modelled semantic composition with a recursive neural network. Both of these tasks differ from the work presented here in that answer selection can require mapping questions containing multiple relations to answer sentences also containing several concepts and relations."}, {"heading": "3 Model description", "text": "Answer sentence selection can be viewed as a binary classification problem. Assume a set of questions Q, where each question qi \u2208 Q is associated with a list of answer sentences {ai1, ai2, \u00b7 \u00b7 \u00b7 , aim}, together with their judgements {yi1, yi2, \u00b7 \u00b7 \u00b7 , yim}, where yij = 1 if the answer is correct and yij = 0 otherwise. While this could be approached as a multi-labelling task, we simply treat each data point as a triple (qi, aij , yij). Thus, our task is to learn a classifier over these triples so that it can predict the judgements of any additional QA pairs.\nOur solution to this problem assumes that correct answers have high semantic similarity to questions. Unlike previous work, which measured the similarity mainly using syntactic information and handcrafted semantic resources, we model questions and answers as vectors, and evaluate the relatedness\nof each QA pair in a shared vector space. Formally, following Bordes et al. [3], given the vector representations of a question q and an answer a (both in Rd), the probability of the answer being correct is\np(y = 1|q, a) = \u03c3(qT Ma+ b), (1)\nwhere the bias term b and the transformation matrix M \u2208 Rd\u00d7d are model parameters. This formulation can intuitively be understood as an expression of the generative approach to open domain question answering: given a candidate answer sentence, we \u2018generate\u2019 a question through the transformation q\u2032 = M a, and then measure the similarity of the generated question q\u2032 and the given question q by their dot product. The sigmoid function squashes the similarity scores to a probability between 0 and 1. The model is trained by minimising the cross entropy of all labelled data QA pairs:\nL = \u2212 log \u220f\nn\np(yn|qn, an) + \u03bb\n2 \u2016\u03b8\u20162F\n= \u2212 \u2211\nn\nyn log \u03c3(q T n M an + b) + (1\u2212 yn) log(1 \u2212 \u03c3(q T n Man + b)) +\n\u03bb 2 \u2016\u03b8\u20162F ,\n(2)\nwhere \u2016\u03b8\u20162F is the Frobenius norm of \u03b8, and \u03b8 includes {M, b} as well as any parameters introduced in the sentence composition model. Next we describe two methods employed in this work for projecting sentences into vector space representations."}, {"heading": "3.1 Bag-of-words model", "text": "Given word embeddings, the bag-of-words model generates the vector representation of a sentence by summing over the embeddings of all words in the sentence\u2014having previously removed stop words from the input. The vector is then normalised by the length of the sentence.\ns = 1\n|s|\n|s|\u2211\ni=1\nsi. (3)"}, {"heading": "3.2 Bigram model", "text": "Due to its inability to account for word ordering and other structural information, the simple bag-ofwords model proposed above is unable to capture more complex semantics of a sentence. To address this issue, we also evaluate a sentence model based on a convolutional neural network (CNN).\nThe advantage of this composition model is that it is sensitive to word ordering and is able to capture features of n-grams independent of their positions in the sentences. Further, the convolutional network can learn to correspond to the internal syntactic structure of sentences, removing reliance on external resources such as parse trees [13]. Finally, the convolution and pooling layers help us to capture long-range dependencies, which are common in questions. CNN-based models have been proved to be effective in applications such as semantic role labelling [5], twitter sentiment prediction [13] and semantic parsing [23]. Figure 1 illustrates the architecture of the CNN-based sentence model in one dimension. We use bigrams here with one convolutional layer and one pooling layer. The convolutional vector t \u2208 R2, which is shared by all bigrams, projects every bigram into a feature value ci, computed as follows: ci = tanh(t \u00b7 si:i+1 + b). (4) Since we would like to capture the meaning of the full sentence, we then use average pooling to combine all bigram features. This produces a full-sentence representation of the same dimensionality as the initial word embeddings. In practice, of course, these representations are not a single value, but d-dimensional vectors, and hence t and each bigram are matrices. Thus, Equation 4 is calculated for each row of t and the corresponding row of s. Similarly, average pooling is performed across each row of the convolved matrix. Formally, our bigram model is\ns =\n|s|\u22121\u2211\ni=1\ntanh(TL si +TR si+1 + b), (5)\nwhere si is the vector of the i-th word in the sentence, and s is the vector representation of the sentence. Both vectors are in Rd. TL and TR are model parameters in Rd\u00d7d and b is bias. From\na linguistic perspective, these parameters can be considered as informing the ordering of individual word pairs."}, {"heading": "4 Experiments", "text": "We evaluated both models presented in this paper on a standard answer selection dataset. We briefly introduce the dataset before describing our experimental setup. Finally, we report our results and compare them with previous work."}, {"heading": "4.1 TREC Answer Selection Dataset", "text": "The answer sentence selection dataset contains a set of factoid questions, with a list of answer sentences corresponding to each question. Wang et al. [20] created this dataset from Text REtrieval Conference (TREC) QA track (8-13) data, with candidate answers automatically selected from each question\u2019s document pool. Answer candidates were chosen using a combination of overlapping non-stop word counts and pattern matching. Subsequently, the correctness of the candidate answers were judged manually for parts of the dataset. Table 1 summarises the answer selection dataset, and describes the train/dev/test split of the data. The TRAIN-ALL training set\u2014significantly larger than any of the other parts of the data\u2014is noisy as answers were labelled automatically using pattern matching.\nThe task is to rank the candidate answers based on their relatedness to the question, and is thus measured in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), which are standard metrics in Information Retrieval and Question Answering. Whereas MRR measures the rank of any correct answer, MAP examines the ranks of all the correct answers. In general, MRR is slightly higher than MAP on the same list of ranked outputs, except that they are the same in the case where each question has exactly one correct answer. The scores are calculated using the official trec eval evaluation scripts."}, {"heading": "4.2 Experimental setup", "text": "We used word embeddings (d = 50) that were computed using Collobert and Weston\u2019s neural language model [5] and provided by Turian et al. [18]. Even though our objective function would allow us to learn word embeddings directly, we fixed those representations in light of the small size of the QA answer selection dataset. The other model weights were randomly intitialised using a Gaussian distribution (\u00b5 = 0, \u03c3 = 0.01). All hyperparameters were optimised via grid search on the MAP score on the development data. We use the AdaGrad algorithm [6] for training.\nOne weakness of the distributed approach is that\u2014unlike symbolic approaches\u2014distributed representations are not very well equipped for dealing with cardinal numbers and proper nouns, especially considering the small dataset. As these are important artefacts in any question answering task, we mitigate this issue by using a second feature that counts co-occurring words in question-answer pairs. We integrate this feature with our distributional models by training a logistic regression classifier with three features: word co-occurrence count, word co-occurrence count weighted by IDF value and QA matching probability as provided by the distributional model. Notice that here we did not add more n-gram features, since we would like to see how much the distributional models contributed in this task, rather than n-gram overlapping. L-BFGS was used to train the logistic regression classifier, with L2 regulariser of 0.01."}, {"heading": "4.3 Results", "text": "Table 2 summarises the results of our models. As can be seen, the bigram model performs better than the unigram model and the addition of the IDF-weighted word count features significantly improve performance for both models by 10% \u2013 15%. Wang et al. [20] reported that training with the noisy dataset TRAIN-ALL negatively impacted their models. This does not apply to our models, where performance of the models increases.\nTable 3 surveys published results on this task, and places our best models in the context of the current state-of-the-art results. The table also includes three baseline models provided in [22]. The first model randomly assigns scores to each answer. The second model counts the number of words co-occurring in each QA pair, with another version of that baseline weighting these word counts by IDF values. As can be seen in Table 3, our best models (bigram + count) outperform all baselines and prior work on MAP and are very close to the best model proposed by Yih et al. on MRR. Considering the lack of complexity of our models compared to those of Yih et al., these results are very promising and indicate the soundness of our approach to the QA answer selection task."}, {"heading": "5 Discussion", "text": "As already stated in the background section of this paper, most prior work focuses on syntactic analysis, with semantic aspects mainly being incorporated through a number of manually engineered features and external resources such as WordNet. Interestingly, however, the best performing pub-\nlished model is also the only piece of prior work that is primarily focused on semantics [22]. In their model, Yih et al. match aligned words between questions and answers and extract features from word pairs. The word-level features are then aggregated to represent sentences, which are used for classification. They combine a group of word matching features with semantic features obtained from a wide range of linguistic resources including WordNet, polarity-inducing latent semantic analysis (PILSA) model [24] and different vector space models.\nWhen considering the heavy reliance of resources of those models in comparison to the simplicity of our approach, the relative performance of our models is highly encouraging. As we only use two non-distributional features\u2014question-answer pair word matching and word matching weighted by IDF values\u2014it is plausible to regard the distributional aspect of our models as a replacement for the numerous lexical semantic resources and features utilised in Yih et al.\u2019s work. In the context of these results, it is also worth noting that\u2014unlike the model of Yih et al.\u2014our models can directly be applied across languages, as we are not relying on any external resources beyond some large corpus on which to train our initial word embeddings.\nEarlier in this paper we argued that methods based purely on vector representations may not be sufficient for solving complex problems such as paraphrase detection and question answering because of their weakness in dealing with certain aspects of language such as numbers and\u2014to a lesser extent\u2014proper nouns. For example, the mismatching of numbers are crucial for rejecting a pair of \u2018paraphrases\u2019 or an answer. Surface-form matching is particularly important in our experiment because we did not learn word embeddings from the answer selection dataset and the given word dictionaries may not cover all the words in the dataset1. Most of the non-covered words are proper nouns, which are then assigned the UNKNOWN token. While these are likely to be crucial for judging the relevance of an answer candidate, they cannot be incorporated into the distributional aspect of our model.\nHowever, it is also important to then establish the opposite fact, namely that distributional semantics improve over purely word counting model. When reviewing the baseline results (Table 3) relative to the performance of our models, it is also evident, that adding distributional semantics as a feature improves over models based purely on co-occurrence counts and word matching. In fact, this addition boosts both MAP and MRR scores by approximately 10%. We analysed this effect by considering sentences where our combined model (bigram + count) performs better than the counting\n1Approximately 5% of words in the answer selection dataset are not covered in Collobert and Weston\u2019s embeddings.\nbaseline. Here are two examples, where the model of co-occurrence word count failed to identify the correct answer, but the combined model prevailed:\n1. Q: When did James Dean die? A1: In \u3008num\u3009, actor James Dean was killed in a two-car collision near Cholame, Calif.\n(correct) A2: In \u3008num\u3009, the studio asked him to become a technical adviser on Elia Kazan\u2019s \u201cEast of\nEden,\u201d starring James Dean. (incorrect)\n2. Q: How many members are there in the singing group the Wiggles? A1: The Wiggles are four effervescent performers from the Sydney area: Anthony Field,\nMurry Cook, Jeff Fatt and Greg Page. (correct) A2: Let\u2019s now give a welcome to the Wiggles, a goofy new import from Australia. (incor-\nrect)\nIn both cases, the baseline model cannot tell the difference between the two candidate answers since they have the same number of matched words to the question. However, for the first example the combined model assigns higher score to the first answer since the word die is semantically close to killed. Similarly, for the second example, the word performers in the first sentence is related to singing group in the question, and hence the first one gets higher score.\nAlthough the use of distributional semantics model could bring benefits to pure syntactic matching, there are still problems that the current method cannot tackle. For example,\n3. Q: What is the name of Durst\u2019s group? A: Limp Bizkit lead singer Fred Durst did a lot before he hit the big time.\nHere, the judgement requires a good understanding of the sentence and some background knowledge about the relation between lead singer and group."}, {"heading": "6 Conclusion", "text": "This paper demonstrated the effectiveness of applying distributional sentence models to answer sentence selection. We projected questions and answers into vectors and learned a semantic matching function between QA pairs. Subsequently we combined this function with a simple, weighted QA co-occurrence counter.\nWe demonstrated that this approach with a bag-of-words sentence model significantly improves performance over the original count-based baseline model. By using a more complex sentence model based on a convolutional neural network over bigrams, we improved performance further still, and attained the state of the art on the answer selection task. Compared with previous work based on feature engineering and external hand-coded semantic resources, our approach is much simpler and more flexible.\nIn the future, we would like to investigate more complex sentence models for this task, for example a convolutional network based sentence model with higher order n-grams and multiple feature maps, as well as recursive neural network-based models. Moreover, since answer sentence selection is similar to textual entailment and paraphrase detection, we are hoping to extend this line of work to these tasks."}, {"heading": "Acknowledgment", "text": "We thank Wen-tau Yih and Xuchen Yao for sharing the answer sentence selection dataset. This work was supported by a Xerox Foundation Award and EPSRC grant number EP/K036580/1."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent"], "venue": "In Proceedings of NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Question answering with subgraph embeddings", "author": ["Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Open question answering with weakly supervised embedding models", "author": ["Antoine Bordes", "Jason Weston", "Nicolas Usunier"], "venue": "In Proceedings of ECML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A compositional distributional model of meaning", "author": ["Stephen Clark", "Bob Coecke", "Mehrnoosh Sadrzadeh"], "venue": "Proceedings of the Second Symposium on Quantum Interaction,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Explorations in Automatic Thesaurus Discovery", "author": ["Gregory Grefenstette"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Tree edit models for recognizing textual entailments, paraphrases, and answers to questions", "author": ["Michael Heilman", "Noah A. Smith"], "venue": "In Proceedings of NAACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of ACL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of ICLR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Multilingual Models for Compositional Distributional Semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Finding predominant word senses in untagged text", "author": ["Diana McCarthy", "Rob Koeling", "Julie Weeds", "John A. Carroll"], "venue": "In Proceedings of ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Automatic feature engineering for answer selection and extraction", "author": ["Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies", "author": ["David A Smith", "Jason Eisner"], "venue": "In Proceedings of the Workshop on Statistical Machine Translation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP-CoNLL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Probabilistic tree-edit models with structured latent variables for textual entailment and question answering", "author": ["Mengqiu Wang", "Christopher D Manning"], "venue": "In Proceedings of COLING,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "What is the jeopardy model? a quasisynchronous grammar for qa", "author": ["Mengqiu Wang", "Noah A Smith", "Teruko Mitamura"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Answer extraction as sequence tagging with tree edit distance", "author": ["Xuchen Yao", "Benjamin Van Durme", "Peter Clark"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak"], "venue": "In Proceedings of ACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Semantic parsing for single-relation question answering", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek"], "venue": "In Proceedings of ACL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Polarity inducing latent semantic analysis", "author": ["Wen-tau Yih", "Geoffrey Zweig", "John C Platt"], "venue": "In Proceedings of EMNLP-CoNLL,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "This can be achieved with generative models that syntactically transform answers to questions [20, 19].", "startOffset": 94, "endOffset": 102}, {"referenceID": 18, "context": "This can be achieved with generative models that syntactically transform answers to questions [20, 19].", "startOffset": 94, "endOffset": 102}, {"referenceID": 7, "context": "Another option is discriminative models over features produced from minimal edit sequences between dependency parse trees [8, 21].", "startOffset": 122, "endOffset": 129}, {"referenceID": 20, "context": "Another option is discriminative models over features produced from minimal edit sequences between dependency parse trees [8, 21].", "startOffset": 122, "endOffset": 129}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "At the same time, neural network-based distributional sentence models have achieved successes in many natural language processing tasks such as sentiment analysis [9, 13], paraphrase detection [17] and document classification [11, 10].", "startOffset": 163, "endOffset": 170}, {"referenceID": 12, "context": "At the same time, neural network-based distributional sentence models have achieved successes in many natural language processing tasks such as sentiment analysis [9, 13], paraphrase detection [17] and document classification [11, 10].", "startOffset": 163, "endOffset": 170}, {"referenceID": 16, "context": "At the same time, neural network-based distributional sentence models have achieved successes in many natural language processing tasks such as sentiment analysis [9, 13], paraphrase detection [17] and document classification [11, 10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "At the same time, neural network-based distributional sentence models have achieved successes in many natural language processing tasks such as sentiment analysis [9, 13], paraphrase detection [17] and document classification [11, 10].", "startOffset": 226, "endOffset": 234}, {"referenceID": 9, "context": "At the same time, neural network-based distributional sentence models have achieved successes in many natural language processing tasks such as sentiment analysis [9, 13], paraphrase detection [17] and document classification [11, 10].", "startOffset": 226, "endOffset": 234}, {"referenceID": 11, "context": "Another line of work\u2014closely related to the model presented here\u2014is the application of recursive neural networks to factoid question answering over paragraphs [12].", "startOffset": 159, "endOffset": 163}, {"referenceID": 6, "context": "They have been proved to be successful in applications such as relation extraction [7] and word sense disambiguation [14].", "startOffset": 83, "endOffset": 86}, {"referenceID": 13, "context": "They have been proved to be successful in applications such as relation extraction [7] and word sense disambiguation [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 4, "context": "An alternative method for learning distributed representations comes in the form of neural language models, where the link to distributional data is somewhat more obscure [5, 1].", "startOffset": 171, "endOffset": 177}, {"referenceID": 0, "context": "An alternative method for learning distributed representations comes in the form of neural language models, where the link to distributional data is somewhat more obscure [5, 1].", "startOffset": 171, "endOffset": 177}, {"referenceID": 3, "context": "Popular ideas for this include exploiting category theory [4], using parse trees in conjunction with recursive autoencoders [17, 9], and convolutional neural networks [5, 13].", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "Popular ideas for this include exploiting category theory [4], using parse trees in conjunction with recursive autoencoders [17, 9], and convolutional neural networks [5, 13].", "startOffset": 124, "endOffset": 131}, {"referenceID": 8, "context": "Popular ideas for this include exploiting category theory [4], using parse trees in conjunction with recursive autoencoders [17, 9], and convolutional neural networks [5, 13].", "startOffset": 124, "endOffset": 131}, {"referenceID": 4, "context": "Popular ideas for this include exploiting category theory [4], using parse trees in conjunction with recursive autoencoders [17, 9], and convolutional neural networks [5, 13].", "startOffset": 167, "endOffset": 174}, {"referenceID": 12, "context": "Popular ideas for this include exploiting category theory [4], using parse trees in conjunction with recursive autoencoders [17, 9], and convolutional neural networks [5, 13].", "startOffset": 167, "endOffset": 174}, {"referenceID": 10, "context": "More recently, sentence models constructed from parallel corpora [11] point to a new trend in compositional distributional semantics, where word- and sentence-level representations are learned given a joint semantic objective function.", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "[20] built a generative model to match the dependency trees of question answer pairs based on the soft alignment of a quasisynchronous grammar [16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[20] built a generative model to match the dependency trees of question answer pairs based on the soft alignment of a quasisynchronous grammar [16].", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "Wang and Manning [19] proposed another probabilistic model based on Conditional Random Fields, which models alignment as a set of tree-edit operations of dependency trees.", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "Heilman and Smith [8] used a tree kernel as a heuristic to search for the minimal edit sequences between parse trees.", "startOffset": 18, "endOffset": 21}, {"referenceID": 20, "context": "[21] extended Heilman and Smith\u2019s approach with the difference that they used dynamic programming to find the optimal tree edit sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "synonym, antonym, hypernym) as explicit features, the focus of all of this work is primarily on syntactic information [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "[22] applied rich lexical semantics to their state-of-the-art QA matching models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "A variation of this idea can also be found in Severyn and Moschitti [15], who used an SVM with tree kernels to automatically learn features from shallow parse trees rather than relying on external resources, sacrificing semantic information for model simplicity.", "startOffset": 68, "endOffset": 72}, {"referenceID": 22, "context": "[23], who constructed models for single-relation question answering with a knowledge base of triples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3, 2] used a type of siamese network for learning to project question and answer pairs into a joint space.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[3, 2] used a type of siamese network for learning to project question and answer pairs into a joint space.", "startOffset": 0, "endOffset": 6}, {"referenceID": 11, "context": "[12] worked on the quiz bowl task, a question answering task that requires identifying an entity as described by a series of sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3], given the vector representations of a question q and an answer a (both in R), the probability of the answer being correct is p(y = 1|q, a) = \u03c3(q Ma+ b), (1)", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Further, the convolutional network can learn to correspond to the internal syntactic structure of sentences, removing reliance on external resources such as parse trees [13].", "startOffset": 169, "endOffset": 173}, {"referenceID": 4, "context": "CNN-based models have been proved to be effective in applications such as semantic role labelling [5], twitter sentiment prediction [13] and semantic parsing [23].", "startOffset": 98, "endOffset": 101}, {"referenceID": 12, "context": "CNN-based models have been proved to be effective in applications such as semantic role labelling [5], twitter sentiment prediction [13] and semantic parsing [23].", "startOffset": 132, "endOffset": 136}, {"referenceID": 22, "context": "CNN-based models have been proved to be effective in applications such as semantic role labelling [5], twitter sentiment prediction [13] and semantic parsing [23].", "startOffset": 158, "endOffset": 162}, {"referenceID": 19, "context": "[20] created this dataset from Text REtrieval Conference (TREC) QA track (8-13) data, with candidate answers automatically selected from each question\u2019s document pool.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "We used word embeddings (d = 50) that were computed using Collobert and Weston\u2019s neural language model [5] and provided by Turian et al.", "startOffset": 103, "endOffset": 106}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We use the AdaGrad algorithm [6] for training.", "startOffset": 29, "endOffset": 32}, {"referenceID": 19, "context": "[20] reported that training with the noisy dataset TRAIN-ALL negatively impacted their models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The table also includes three baseline models provided in [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "lished model is also the only piece of prior work that is primarily focused on semantics [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "They combine a group of word matching features with semantic features obtained from a wide range of linguistic resources including WordNet, polarity-inducing latent semantic analysis (PILSA) model [24] and different vector space models.", "startOffset": 197, "endOffset": 201}], "year": 2014, "abstractText": "Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that\u2014despite its simplicity\u2014our model matches state of the art performance on the answer sentence selection task.", "creator": "LaTeX with hyperref package"}}}