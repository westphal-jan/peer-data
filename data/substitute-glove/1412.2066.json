{"id": "1412.2066", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2014", "title": "Learning Multi-target Tracking with Quadratic Object Interactions", "abstract": "We describe similar sport to multi - clear searching based on associating collections of candidate 5,113 where pins included a film. In order to specifications eulerian interactions continuing different records, there although suppression by overlapping tracks between contextual cues about joint - occurence whose except characteristics, we augment a equivalent po - income direction aspects each integers terms until detection variables. We learn this parameters of this comparable sometimes utilizing prediction and a disappointment linear several pathwidth once biggest - attack intelligence validation. We deciding some though transformation put finding an ph set of compilations the model objective based same an LP instituting and in narrated ornery transfer leave integrated introducing that administrative pairwise potentials. We find been greedy generalization achieves equivalent performance while rest LP induced and being 35 - plantronics change every gives domestic perturbation. The resulting model with learned parameters outperforms extend utilizing across taken standard on the KITTI indicators traders.", "histories": [["v1", "Fri, 5 Dec 2014 17:04:35 GMT  (1493kb,D)", "https://arxiv.org/abs/1412.2066v1", null], ["v2", "Tue, 9 Dec 2014 05:25:44 GMT  (1489kb,D)", "http://arxiv.org/abs/1412.2066v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["shaofei wang", "charless c fowlkes"], "accepted": false, "id": "1412.2066"}, "pdf": {"name": "1412.2066.pdf", "metadata": {"source": "CRF", "title": "Learning Multi-target Tracking with Quadratic Object Interactions", "authors": ["Shaofei Wang", "Charless Fowlkes"], "emails": ["shaofeiw@uci.edu,", "fowlkes@ics.uci.edu"], "sections": [{"heading": "1. Introduction", "text": "Multi-target tracking is a classic topic of research in computer vision. Thanks to advances of object detector performance in single, still images, \u201dtracking-by-detection\u201d approaches that build tracks on top of a collection of candidate object detections have shown great promise. Trackingby-detection avoids some problems such as drift and is often able to recover from extended periods of occlusion since it is \u201cself-initializing\u201d. Finding an optimal set of detections corresponding to each track is often formulated as a discrete optimization problem of finding low-cost paths through a graph of candidate detections for which there are often efficient combinatorial algorithms (such as min-cost matching or min-cost network-flow) that yield globally optimal solutions (e.g., [27, 20]).\nTracking by detection is somewhat different than traditional generative formulations of multi-target tracking, which draw a distinction between the problem of estimat-\ning a latent continuous trajectory for each object from the discrete per-frame data-association problem of assigning observations (e.g., detections) to underlying tracks. Such methods (e.g., [2, 19, 24]) allow for explicitly specifying an intuitive model of trajectory smoothness but face a difficult joint inference problem over both continuous and discrete variables with little guarantee of optimality.\nIn tracking by detection, trajectories are implicitly defined by the selected group of detections. For example, the path may skip over some frames entirely due to occlusions or missing detections. The transition cost of utilizing a given edge between detections in successive frames thus could be interpreted as some approximation of the marginal likelihood associated with integrating over a set of underlying continuous trajectories associated with the corresponding pair of detections. This immediately raises difficulties, both in (1) encoding strong trajectory models with only pairwise potentials and (2) identifying the parameters of these potentials from training data.\nOne line of attack is to first group detections in to can-\nar X\niv :1\n41 2.\n20 66\nv2 [\ncs .C\nV ]\n9 D\nec 2\n01 4\ndidate tracklets and then perform scoring and association of these tracklets [25, 4, 23]. Tracklets allow for scoring much richer trajectory and appearance models while maintaining some benefits of purely combinatorial grouping. Another approach is to attempt to include higher-order constraints directly in a combinatorial framework [5, 6]. In either case, there are a large number of parameters associated with these richer models which necessitates application of machine learning techniques. This is particularly true for (undirected) combinatorial models based on, e.g. networkflow, where parameters are often set empirically by hand.\nIn this work, we introduce an extension to the standard min-cost flow tracking objective that allows us to model pairwise interactions between tracks. This allows us to incorporate useful knowledge such as typical spatial relationships between detections of different objects and suppression of multiple overlapping tracks of the same object. This quadratic interaction necessitates the development of approximate inference methods which we describe in Section 3. In Section 5 we describe an approach to joint learning of model parameters in order to maximize tracking performance on a training data set using techniques for structured prediction [22]. Structured prediction has been applied in tracking to learning inter-frame affinity metrics [14] and association [18] as well as a variety of other learning tasks such as fitting CRF parameters for segmentation [21] and word alignment for machine translation [15]. To our best knowledge, the work presented here is unique in utilizing discriminative structured prediction to jointly learn the complete set of parameters of a tracking model from labeled data, including track birth/death bias, transition affinities, and multi-object contextual relations. We conclude with experimental results (Section 6) which demonstrate that the learned quadratic model and inference routines yield state of the art performance on multi-target, multi-category object tracking in urban scenes."}, {"heading": "2. Model", "text": "We begin by formulating multi-target tracking and data association as a min-cost flow network problem equivalent to that of [27], where individual tracks are described by a first-order Markov Model whose state space is spatialtemporal locations in videos. This framework incorporates a state transition likelihood that generates transition features in successive frames, and an observation likelihood that generates appearance features for objects and background."}, {"heading": "2.1. Tracking by Min-cost Flow", "text": "For a given video sequence, we consider a discrete set of candidate object detection sites V where each candidate site x = (l, \u03c3, t) is described by its location, scale and frame number. We write \u03a6 = {\u03c6a(x)|x \u2208 V } for the image evidence (appearance features) extracted at each corre-\nsponding spatial-temporal location in a video. A single object track consists of an ordered set of these detection sites: T = {x1, ..., xn}, with strictly increasing frame numbers.\nWe model the whole video by a collection of tracks T = {T1, ..., Tk}, each of which independently generates foreground object appearances at the corresponding sites according to distribution pfg(\u03c6a) while the remaining site appearances are generated by a background distribution pbg(\u03c6a). Each site can only belong to a single track. Our task is to infer a collection of tracks that maximize the posterior probability P (T |\u03a6) under the model. Assuming that tracks behave independently of each other and follow a firstorder Markov model, we can write an expression for MAP inference:\nT \u2217 = argmax T \u220f T\u2208T P (\u03a6|T )P (T )\n= argmax T ( \u220f T\u2208T \u220f x\u2208T l(\u03c6a(x)) ) \u00d7\n\u220f T\u2208T ( ps(x1)pe(xN ) N\u22121\u220f i=1 pt(xi+1|xi) ) (1)\nwhere\nl(\u03c6a(x)) = pfg(\u03c6a(x))\npbg(\u03c6a(x))\nis the appearance likelihood ratio that a specific location x corresponds to the object tracked and ps, pe and pt represent the likelihoods for tracks starting, ending and transitioning between given sites.\nThe set of optimal tracks can be found by taking the log of 1 to yield an integer linear program (ILP) over flow variables f .\nmin f \u2211 i csif s i + \u2211 ij\u2208E cijfij + \u2211 i cifi + \u2211 i ctif t i (2)\ns.t. fsi + \u2211 j fji = fi = f t i + \u2211 j fij\nfsi , f t i , fi, fij \u2208 {0, 1}\nwhere E is the set of valid transitions between sites in successive frames and the costs are given by\nci = \u2212 log l(\u03c6a(x)), cij = \u2212 log p(xj |xi) csi = \u2212 log ps(xi), cti = \u2212 log pt(xi)\n(3)\nThis ILP is a well studied problem known as minimum-cost network flow [1]. The constraints satisfy the total unimodularity property and thus can be solved exactly using any LP solver or via various efficient specialized solvers, including network simplex, successive shortest path and push-relabel with bisectional search [27].\nWhile these approaches yield globally optimal solutions, the authors of [20] consider even faster approximations based on multiple rounds of dynamic programming (DP). In particular, the successive shortest paths algorithm (SSP) finds optimal flows by applying Dijkstra\u2019s algorithm on a residual graph constructed from the original network in which some edges corresponding to instanced tracks have been reversed. This can be implemented by performing multiple forward and backward passes of dynamic programming (see Appendix for details). [20] found that two or even one pass of DP often performs nearly as well as SSP in practical tracking scenarios. In our experiments we evaluate several of these variants."}, {"heading": "2.1.1 Track interdependence", "text": "The aforementioned model assumes tracks are independent of each other, which is not always true in practice. A key contribution of our work is showing that pairwise relations between tracks can be integrated into the model to improve tracking performance. In order to allow interactions between multiple objects, we add a pairwise cost term denoted qij and qji for jointly activating a pair of flows fi and fj corresponding to detections at sites xi = (li, \u03c3i, ti) and xj = (lj , \u03c3j , tj). An intuitive example of qij and qji would be penalty for overlap locations or a boost for cooccurring objects. We only consider pairwise interactions between pairs of sites in the same video frame which we denote by EC = {ij : ti = tj}. Adding this term to 2 yields an Integer Quadratic Program (IQP):\nmin f \u2211 i csif s i + \u2211 ij\u2208E cijfij + \u2211 i cifi\n+ \u2211 ij\u2208EC qijfifj + \u2211 i ctif t i\n(4)\ns.t. fsi + \u2211 j fji = fi = f t i + \u2211 j fij\nfsi , f t i , fi, fij \u2208 {0, 1}\nThe addition of quadratic terms makes this objective hard to solve in general. In the next section we discuss two different approximations for finding high quality solutions f . In Section 5 we describe how the costs c can be learned from data."}, {"heading": "3. Inference", "text": "Now we describe different methods to conduct tracking inference (finding the optimal flows f ). These inference routines are used both for predicting a set of tracks at test time as well as optimizing parameters during learning (see Section 5).\nAs mentioned in previous section, for traditional mincost network flow problem defined in Equation 2 there ex-\nists various efficient solvers that explores its total unimodularity property to find the global optimum. We employ MOSEK\u2019s built-in network simplex solver in our experiments, as other alternative algorithms yield exactly the same solution.\nIn contrast, finding the global minimum of the IQP problem 4 is NP-hard [26] due to the quadratic terms. We evaluate two different schemes for finding high-quality approximate solutions. The first is a standard approach of introducing auxiliary variables and relaxing the integral constraints to yield a linear program (LP) that lower-bounds the original objective. We also consider a greedy approximation based on successive rounds of dynamic programming that also yields good solutions while avoiding the expense of solving a large scale LP."}, {"heading": "3.1. LP Relaxation and Rounding", "text": "If we relax the integer constraints and deform the costs as necessary to make the objective convex, then the global optimum of 4 can be found in polynomial time. For example, one could apply Frank-Wolfe algorithm to optimize the relaxed, convexified QP while simultaneously keeping track of good integer solutions [13]. However, for real-world tracking over long videos, the relaxed QP is still quite expensive. Instead we follow the approach proposed by Chari et al. [6], reformulating the IQP as an equivalent ILP problem by replacing the quadratic terms fifj with a set of auxiliary variables uij :\nmin f \u2211 i csif s i + \u2211 ij\u2208E cijfij + \u2211 i cifi\n+ \u2211 ij\u2208EC qijuij + \u2211 i ctif t i\n(5)\ns.t. fsi , f t i , fi, fj , fij , uij \u2208 {0, 1} fsi + \u2211 j fji = fi = f t i + \u2211 j fij\nuij \u2264 fi, uij \u2264 fj fi + fj \u2264 uij + 1\nThe new constraint sets enforce uij to be 1 only when fi and fj are both 1. By relaxing the integer constraints, program 5 can be solved efficiently via large scale LP solvers such as CPLEX or MOSEK.\nDuring test time we would like to predict a discrete set of tracks. This requires rounding the solution of the relaxed LP to some solution that satisfies not only integer constraints but also flow constraints. [6] proposed two rounding heuristics: a Euclidean rounding scheme that minimizes \u2016f\u2212f\u0302\u20162 where f\u0302 is the non-integral solution given by the LP relaxation. When f is constrained to be binary, this objective simplifies to a linear function (1\u22122f\u0302)T f +\u2016f\u0302\u20162, which can be optimized using a standard linear min-cost flow\nsolver. Alternately, one can use a linear under-estimator of 4 similar to the Frank-Wolfe algorithm:\u2211\ni\ncsif s i + \u2211 ij\u2208E\ncijfij+\u2211 i (ci + \u2211 ij\u2208EC qij u\u0302ij + \u2211 ji\u2208EC qjiu\u0302ji)fi + \u2211 i ctif t i (6)\nBoth of these rounding heuristics are linear functions subject to the original integer and flow constraints and thus can be solved as an ordinary min-cost network flow problem. In our experiments we execute both rounding heuristics and choose the solution with lower cost."}, {"heading": "3.2. Greedy Sequential Search", "text": "We now describe a simple greedy algorithm inspired by the combination of dynamic programming and nonmaximal suppression proposed in [20]. We carry out a series of rounds of dynamic programming to find the shortest path between source and sink nodes. In each round, once we have identified a track, we update the (unary) costs associated with all detections to include the effect of the pairwise quadratic interaction term of the newly activated track (e.g. suppressing overlapping detections, boosting the scores of commonly co-occurring objects). This is analogous to greedy algorithms for maximum-weight independent set where the elements are paths through the network.\nAlgorithm 1 DP with pairwise Cost Update 1: Input: A Directed-Acyclic-Graph G with edge\nweights ci, cij 2: initialize T \u2190 \u2205 3: repeat 4: Find shortest start-to-end path p on G 5: track cost = cost(p) 6: if track cost < 0 then 7: for all locations xi in p do 8: cj = cj + qij + qji for all ij, ji \u2208 EC 9: ci = +\u221e\n10: end for 11: T \u2190 T \u222a p 12: end if 13: until track cost \u2265 0 14: Output: track collection T\nIn the absence of quadratic terms, this algorithm corresponds to the 1-pass DP approximation of the successiveshortest paths (SSP) algorithm. Hence it does not guarantee an optimal solution, but, as we show in the experiments, it performs well in practice. A practical implementation difference (from the linear objective) is that updating the costs with the quadratic terms when a track is instanced has the unfortunate effect of invalidating cost-to-go estimates\nwhich could otherwise be cached and re-used between successive rounds to accelerate the DP computation.\nInterestingly, the greedy approach to updating the pairwise terms can also be used with a 2-pass DP approximation to SSP where backward passes subtract quadratic penalties. We describe the details of our implementation of the 2-pass algorithm in the Appendix. We found the 1-pass approach superior as the complexity and runtime grows substantially for multi-pass DP with pairwise updates."}, {"heading": "4. Tracking Features and Potentials", "text": "In order to learn the tracking potentials (c and q) we parameterize the flow cost objective by a vector of weights w and a set of features \u03a8(X, f) that depend on features extracted from the video, the spatio-temporal relations between candidate detections, and which tracks are instanced. With this linear parameterization we write the cost of a given flow asC(f) = \u2212wT\u03a8(X, f) where the negative sign is a useful convention to convert the minimization problem into a maximization. The vector components of the weight and feature vector are given by:\nw =  wS wt ws wa wE  \u03a8(X, f) = \n\u2211 i \u03c6S(x s i )f s i\u2211\nij\u2208E \u03c8t(xi, xj)fij\u2211 ij\u2208EC \u03c8s(xi, xj)fifj\u2211\ni \u03c6a(xi)fi\u2211 i \u03c6E(x t i)f t i\n (7)\nHere wa represents local appearance template for the tracked objects of interest, wt represents weights for transition features, ws represents weights for pairwise interactions, wS and wE represents weights associated with track births and deaths. \u03c6a(xi) is the image feature at spatialtemporal location xi, \u03c8t(xi, xj) represents the feature of transition from location xi to location xj , \u03c8s(xi, xj) represents the feature of pairwise interaction between location xi and xj that are in the same frame, \u03c6S(xsi ) represents feature of birth node to location xi and \u03c6E(xti) represents feature of location xi to sink node.\nLocal appearance model: We make use of an off-theshelf detector to capture local appearance. Our local appearance feature thus consists of the detector score along with a constant 1 to allow for a variable bias.\nTransition model: We use a simple motion model (described in Section 6) to predict candidate windows\u2019 locations in future frames; we connect a candidate xi at time ti with another candidate xj at a later time ti + n, only if the overlap ratio between xi\u2019s predicted window at ti + n and xj\u2019s window at ti + n exceeds 0.3. The overlap ratio is defined as two windows\u2019 intersection over their union. We use this overlap ratio as a feature associated with each transition link. The transition link\u2019s feature will be 1 if this\nratio is lower than 0.5, and 0 otherwise. In our experiments we allow up to 7 frames occlusion for all the network-flow methods. We append a constant 1 to this feature and bin these features according to the length of transition. This yields a 16 dimensional feature for each transition link.\nBirth/death model: In applications with static cameras it can be useful to learn a spatially varying bias to model where tracks are likely to appear or disappear. However, videos in our experiments are all captured from a moving vehicle, we thus use a single constant value 1 for the birth and death features.\nPairwise interactions: ws is a weight vector that encodes valid geometric configurations of two objects. \u03c8(xi, xj) is a discretized spatial-context feature that bins relative location of detection window at location xi and window at location xj into one of the D relations including on top of, above, below, next-to, near, far and overlap (similar to the spatial context of [7]). To mimic the temporal NMS described in [20] we add one additional relation, strictly overlap, which is defined as the intersection of two boxes over the area of the first box; we set the corresponding feature to 1 if this ratio is greater than 0.9 and 0 otherwise. Now assume that we have K classes of objects in the video, then ws is a DK2 vector, i.e. ws = [w T s11, w T s12, ..., w T sij , ..., w T sKK ]\nT , in which wsij is a length of D column vector that encodes valid geometric configurations of object of class i w.r.t. object of class j. In such way we can capture intra- and inter-class contextual relationships between tracks."}, {"heading": "5. Learning", "text": "We formulate parameter learning of tracking models as a structured prediction problem. With some abuse of notation, assume we have N training videos (Xn, fn) \u2208 X \u00d7 F , n = 1, ..., N . Given ground-truth tracks in training videos specified by flow variables fn, we discriminatively learn tracking model parameters w using a structured SVM with margin rescaling:\nw\u2217 = argmin w,\u03ben\u22650\n1 2 \u2016w\u20162 + C \u2211 n \u03ben (8)\ns.t. \u2200n, f\u0302 , \u3008w,4\u03a8(Xn, fn, f\u0302)\u3009 \u2265 L(fn, f\u0302)\u2212 \u03ben\nwhere\n4\u03a8(Xn, fn, f\u0302) = \u03a8(Xn, fn)\u2212\u03a8(Xn, f\u0302)\nwhere \u03a8(Xn, fn) are the features extracted from nth training video. L(fn, f\u0302) is a loss function that penalize any difference between the inferred label f\u0302 and the ground truth label fn. The constraint on the slack variables \u03ben ensure that we pay a cost for any training videos in which the flow cost of the ground-truth tracks under modelw is higher than some other incorrect labeling."}, {"heading": "5.1. Cutting plane optimization", "text": "We optimize the structured SVM objective in 8 using a standard cutting-plane method [12] in which the exponential number of constraints (one for each possible flow f\u0302 ) are approximated by a much smaller number of terms. Given a current estimate of w we find a \u201cmost violated constraint\u201d for each training video:\nf\u0302\u2217n = argmax f\u0302 L(fn, f\u0302)\u2212 \u3008w,4\u03a8(Xn, fn, f\u0302)\u3009\nWe can then add these constraints to the optimization problem and solve for an updated w. This procedure is iterated until no additional constraints are added to the problem. In our implementation, at each iteration we add a single linear constraint which is a sum of violating constraints derived from individual videos in the dataset which is also a valid cutting plane constraint [7].\nThe key subroutine is finding the most-violated constraint for a given video which requires solving the lossaugmented inference problem (we drop the n subscript notation from here on)\nf\u0302\u2217 = argmin f\u0302 \u3008w,\u03a8(X, f\u0302)\u3009 \u2212 L(f , f\u0302) (9)\nAs long as the loss function L(f , f\u0302) decomposes as a sum over flow variables then this problem has the same form as our test time tracking inference problem, the only difference being that the cost of variables in f is augmented by their corresponding negative loss.\nWe note that our two inference algorithms behave somewhat differently when producing constraints. The greedy algorithm has no guarantee of finding the optimal flow for a given tracking problem and hence may not generate all the necessary constraints for learning w. In contrast, for the LP relaxation, we have the option of adding constraints corresponding to fractional solutions (rather than rounding them to discrete tracks). If we use a loss function that penalizes incorrect non-integral solutions, this may push the structured SVM to learn parameters that tend to result in tight relaxations. These scenarios are termed \u201cundergenerating\u201d and \u201covergenerating\u201d respectively by [9] since approximate inference is performed over a subset or superset of the exact space of flows."}, {"heading": "5.2. Loss function", "text": "Now we describe loss functions for multi-target tracking problem. We use a weighted hamming loss to measure loss between ground truth labels f and inferred labels f\u0302 :\nL(f\u0302 , f) = \u2211 fi\u2208f lossi \u2223\u2223\u2223fi \u2212 f\u0302i\u2223\u2223\u2223 (10)\nwhere {loss1, ..., lossi, ..., loss|f |} is a vector indicating the penalty for differences between the estimated flow f\u0302 and the ground-truth f . For example, when loss = 1 it becomes the hamming loss.\nTransition Loss: A critical aspect for successful learning is to define a good loss vector that closely reassembles major tracking performance criteria, such as Multiple Object Tracking Accuracy (MOTA [3]). Metrics such as false positive, false negative, true positive, true negative and true/false birth/death can be easily incorporated by setting their corresponding values in loss to 1.\nBy definition, id switches and fragmentations [16] are determined by looking at labels of two consecutive transition links simultaneously, under such definition the loss cannot be optimized by our inference routine which only considers pairwise relations between detections within a frame. Instead, we propose a decomposable loss for transition links that attempts to capture important aspects of MOTA by taking into account the length and localization of transition links rather than just using a constant (Hamming) loss on mislabeled links. We found empirically that careful specification of the loss function is crucial for learning a good tracking model.\nIn order to describe our transition loss, let us first denote four types of transition links: NN is the link from a false detection to another false detection, PN is the link from a true detection to a false detection, NP is the link from a false detection to a true detection, PP+ is the link from a true detection to another true detection with the same identity, and PP\u2212 is the link from a true detection to another true detection with a different identity. For all the transition links, we interpolate detections between its start detection and end detection (if their frame numbers differ more than 1); the interpolated virtual detections are considered either true virtual detection or false virtual detection, depending on whether they overlap with a ground truth label or not. Loss for different types of transition is defined as:\n1. For NN links, the loss will be (number of true virtual detections + number of false virtual detections) 2. For PN andNP links, the loss will be (number of true virtual detections + number of false virtual detections + 1) 3. For PP+ links, the loss will be (number of true virtual detections) 4. For PP\u2212 links, the loss will be (number of true virtual detections + number of false virtual detections + 2) Ground-truth flows: In practice, available training datasets specify ground-truth bounding boxes that need to be mapped onto ground-truth flow variables fn for each video. To do this mapping, we first consider each frame separately, taking the highest scoring detection window that overlaps a ground truth label as true detection, each true detection will be assigned a track identity label same as\nthe ground truth label it overlaps. Next, for each track identity, we run a simplified version of the dynamic programming algorithm to find the path that claims the largest number of true detections. After we iterate through all id labels, any instanced graph edge will be a true detection/transition/birth/death while the remainder will be false."}, {"heading": "6. Experimental results", "text": "Dataset: We have focused our experiments on training sequences of KITTI tracking benchmark [11]. KITTI tracking benchmark consists of to 21 training sequences with a total of 8008 frames and 8 classes of labeled objects; of all the labeled objects we evaluated three categories which had\nsufficient number of instances for comparative evaluation: cars, pedestrians and cyclists. We use publicly available LSVM [8] reference detections and evaluation script1. The evaluation script only evaluates objects that are not too far away and not truncated by more than 15 percent, it also does not consider vans as false positive for cars or sitting persons as false positive for pedestrians. The final dataset contains 636 labeled car trajectories, 201 labeled pedestrian trajectories and 37 labeled cyclists trajectories.\nTraining with ambiguous labels: One difficulty of training on the KITTI tracking benchmark is that it has special evaluation rules for ground truth labels such as small/truncated objects and vans for cars, sitting persons for pedestrians. This is resolved by removing all detection candidates that correspond to any of these \u201cambiguous\u201d ground truth labels during training; in this way we avoid mining hard negatives from those labels. Also, to speed up training, we partition full-sized training sequences in to 10-framelong subsequences with a 5-frame overlap, and define losses on each subsequence separately.\nData-dependent transition model: In order to keep the size of tracking graphs tractable for our inference methods, we need a heuristic to select a sparse set of links between detection candidates across frames. We found that simply predicting candidate\u2019s locations in future frames via optical flow gives very good performance. Specifically, we first compute frame-wise optical flow using software of [17], then for a candidate detection xi at frame ti, we compute the mean of vertical flows and the mean of horizontal flows within the candidate box, and use them to predict candi-\n1http://www.cvlibs.net/datasets/kitti/eval_ tracking.php\ndate\u2019s location in the next frame ti + 1; for xi\u2019s predicted locations in frame ti + 2 we use its newly predicted location at ti + 1 and candidate\u2019s original box size to repeat the process described above, and same for ti + n.\nTrajectory smoothing: During evaluation we observe that many track fragmentation errors (FRAG) reported by the benchmark are due to the raw trajectory oscillating away from the ground-truth due to poorly localized detection candidates. Inspired by the trajectory model of [2], we postprocess each output raw trajectory by fitting a cubic Bspline. This smoothing of the trajectory eliminates many FRAGs from the raw track, making the fragmentation number more meaningful when compared across different models.\nBaselines: We use the publicly available code from [10] as a first baseline. It relies on a three-stages tracklet linking scheme with occlusion sensitive appearance learning; it is by far the best tracker for cars on KITTI tracking benchmark among all published methods. Also we consider dynamic programming (DP) and successive shortest path (SSP) with default parameters in [20] as another two baselines, denoted as DP+Flow and SSP+Flow in our table.\nParameter settings: We tuned the structural parameters of the various baselines to give good performance. For all baselines we only use detections that have a positive score. For DP+Flow and SSP+Flow we also remove all transition links that have overlap ratios lower than 0.5. For learned tracking models (+Struct) we use detections that have scores greater than -0.5, and transition links that have overlap ratios greater than 0.3.\nBenchmark Results: We evaluate performance using a standard battery of performance measures. The evaluation result for each object category, as well as for all categories are shown in Table 1. For our learned tracking models (+Struct) we use either network simplex solver (for SSP+Flow+Struct) or LP relaxation (for LP/DP+Flow+Struct) for training and conduct leave-onesequence-out cross-validation with C = 2\u22129, 2\u22128, ..., 23. We report cross-validation result under best C, which is C = 2\u22128 for SSP+Flow+Struct and C = 2\u22127 for LP/DP+Flow+Struct. Our simple motion model helps DP+Flow outperform state-of-the-art baseline by a significant margin. One exception is IDSW which we attribute to the fact that the network-flow methods do not explicitly model target appearance. While SSP+Flow seems to perform poorly with default parameters, it turns out that with properly learned parameters (SSP+Flow+Struct), it produces results that are comparable to (and often better than) DP+Flow, this indicates that there is much more potential of SSP than suggested in previous work. In addition, SSP\u2019s guarantee of optimality makes it very attractive if more complicated features and network structure are to be used in learning. As shown in Table 1, in our evaluation over all objects our model learned with pairwise costs (LP/DP+Flow+Struct) achieves the best MOTA, Recall, Mostly Tracked(MT) and Mostly Lost(ML) performance while keeping other metrics competitive.\nApproximate Inference: To evaluate quality of the LP+rounding and DP approximation, we run both LP+rounding and DP inference on models trained via LP relaxation and DP respectively. We then average the running time and minimum cost found on each sequence for LP+rounding and DP, respectively. Fig 5 shows the accumulative running time and cost for each algorithm. During our experiments, LP+rounding often finds the exact relaxed global optimal, and when it doesn\u2019t it still gives very close approximation. While greedy forward search using DP rarely reach relaxed global optimum, it still produced good approximate solutions that were often within 1% of relaxed global optimum while running significantly faster (2-7x) than LP+rounding.\nOvergenerating versus Undergenerating: Previous works have shown that in general, models trained with relaxed inference are preferable than models trained with greedy inference. To investigate this idea in our particular problem, we also conduct leave-one-sequence-out crossvalidation using either DP or the LP relaxation as the inference method for training. The evaluation results under different training/testing inference combinations are shown in Table 2. Notice that model trained with the LP relaxation does slightly better in most metrics, whereas DP stands out as a good inference algorithm at test time. Moreover, though slightly falling behind, model trained with greedy\nDP is very close to the performance of that trained with LP and thus suggests the greedy algorithm proposed here is a very competitive inference method."}, {"heading": "7. Summary", "text": "We augmented the well-studied network-flow tracking model with pairwise cost, and proposed an endto-end framework that jointly optimizes parameters for such model. We extensively evaluated a traditional LP relaxation-based method and a novel greedy dynamic programming method for inference in the augmented network, both of which achieves state-of-the-art performance, while our greedy DP algorithm being 2-7x faster than a commercial LP solver."}, {"heading": "8. Acknowledgements", "text": "This work was supported by NSF DBI-1053036, IIS1253538 and a Google Research Award."}, {"heading": "9. Appendix: Multi-Pass Dynamic Programming to Approximate Successive Shortest", "text": "Path\nNow we describe two dynamic programming (DP) algorithms proposed by [20] which approximate successive shortest path (SSP) algorithm. Recall the network-flow problem described in Equation 2:\nmin f \u2211 i csif s i + \u2211 ij\u2208E cijfij + \u2211 i cifi + \u2211 i ctif t i\ns.t. fsi + \u2211 j fji = fi = f t i + \u2211 j fij\nfsi , f t i , fi, fij \u2208 {0, 1}\nThe corresponding graphical model is shown in Fig 6. SSP finds the global optimum of Objective 2 by repeating:\n1. Find the minimum cost source to sink path on residual graph Gr(f) 2. If the cost of the path is negative, push a flow through the path to update f Until no negative cost path can be found. A residual graph Gr(f) is the same as the original graphG except all edges in f are reversed and their cost negated. We focus on describing the DP algorithms and refer readers to [1] for detailed description of SSP algorithm."}, {"heading": "9.1. One-pass DP", "text": "Assume the detection nodes are sorted in time. We denote cost(i) as the cost of the shortest path from source node to node i, link(i) as i\u2019s predecessor in this shortest path, and birth node(i) as the first detection node in this shortest path. We initialize cost(i) = ci + csi , link(i) = \u2205, and birth node(i) = i for all i \u2208 V .\nTo find the shortest path on the initial DAG G, we can sweep from first frame to last frame, computing cost(i) as:\ncost(i) = ci + min(\u03c0, csi ), \u03c0 = min ji\u2208E cji + cost(j) (11)\nAnd update birth node(i), link(i) accordingly. After we sweeping through all frames, we find a node i such that cost(i) + cti is minimum, and reconstruct the shortest path by backtracking cached link variables. The cost of this path would be cost(i) + cti. After the shortest path is found, we remove all nodes and edges in this shortest path from G, the resulting graph G\u2032 will still be a DAG, thus we can repeat this procedure until we cannot find any path that has a negative cost. Even more speed up can be achieved by only recomputing cost(i), birth node(i) and link(i) for those i whose birth node is the same as the birth node of the track found in previous iteration.\nIt is also straightforward to integrate NMS into this algorithm: when we pick up a shortest path, we also prune all\nnodes that overlap the shortest path. In practice this \u201dtemporal NMS\u201d can be much more aggressive than pre-processing NMS, since the confidence of a track being composed of true positives is much higher than single detections."}, {"heading": "9.2. Two-pass DP", "text": "2-pass DP works very similarly to successive shortest path, the only difference is that instead of using Dijkstra\u2019s algorithm, we use two passes of dynamic programming to approximate shortest path on the residual graph Gr(f). We denote Vforward as the set of forward nodes in current residual graph, and Vbackward as the set of backward nodes in current residual graph, we describe one iteration of 2-pass DP as below:\n1. Ignore all backward edges (including reversed detection edges) and perform one pass of forward DP (from first frame to last frame) on all nodes. For each node i, there will be a path(i) array that stores mininum-cost source to i path, with cost(i) being the total cost of this path. 2. Use cost(i) from step 1 as initial values and perform one pass of backward DP (from last frame to first frame) on Vbackward. After this, cost(i) for i \u2208 Vbackward would be the cost(j) \u2212 cij , where j is i\u2019s best (backward) predecessor and cij is from the original graph. Set cost(i) = +\u221e for backward node i that has no backward edge coming to it. 3. Perform one pass of forward DP on i \u2208 Vforward. To avoid running into cyclic path, we need to backtrack\nshortest paths for all j \u2208 N(i), where N(i) is all neighboring nodes that are connected to i via a forward edge. 4. Find node i with minimum cost(i) + cti, the (approximate) shortest path is then path(i). 5. Update solution f by setting all forward variables along path(i) to 1 and all backward variables along path(i) to 0.\nIt is straightforward to show that during the first iteration, 1-pass DP and 2-pass DP behave identically. Also, the path found by 2-pass DP will never go into a source node or go out of a sink node, thus in each iteration we generate exactly one more track, either by splitting a previously found track, or by choosing a entirely new track. Therefore the algorithm will terminate after at most |V | iterations."}, {"heading": "10. Appendix: Incorporating Quadratic Interactions in Multi-pass DP", "text": "Recall the augmented network-flow problem with quadratic cost (Eqn. 4):\nmin f \u2211 i csif s i + \u2211 ij\u2208E cijfij + \u2211 i cifi\n+ \u2211 ij\u2208EC qijfifj + \u2211 i ctif t i\ns.t. fsi + \u2211 j fji = fi = f t i + \u2211 j fij\nfsi , f t i , fi, fij \u2208 {0, 1}\nWhere EC = {ij : ti = tj}. We propose two new variants of DP algorithm that can approximately minimize the Objective 4. They are also divided into 1-pass DP and 2-pass DP. Since we already described 1-pass DP with pairwise interactions in the paper, we will focus on 2-pass DP with pairwise interactions here."}, {"heading": "10.1. Two-pass DP with quadratic interactions", "text": "A feasible solution f on the network corresponds to a residual graphGr(f). We could apply the steps described in section 9.2 to find an approximate shortest path. This path may consist of both forward nodes and backward nodes, which correspond to uninstanced detections (but will be instanced after this iteration) and already instanced detections (but will be uninstanced after this iteration) respectively. We then update the (unary) cost of other nodes by adding or subtracting the pairwise cost imposed by turning on or off selected nodes on the path. Additionally, at step 3 of 2-pass DP, one could also consider the pairwise cost to current node imposed by previously selected nodes in the same path. The entire procedure is described as Algorithm 2.\nNotice that, to simplify our notation, we construct temporary residual graph at the beginning of each iteration and\nAlgorithm 2 Two-pass DP with pairwise Cost Update 1: Input: A Directed-Acyclic-Graph G with node and\nedge weights ci, cij 2: initialize f = 0 3: repeat 4: Find start-to-end min-cost unit flow f\u2217 onGr(f) 5: track cost = cost(f\u2217) 6: if track cost < 0 then 7: for all fi \u2208 f\u2217 do 8: if fi = 0 then 9: cj = cj + qij + qji,\u2200ij, ji \u2208 EC\n10: else 11: cj = cj \u2212 qij \u2212 qji,\u2200ij, ji \u2208 EC 12: end if 13: end for 14: f\u2217 = \u00acf\u2217 15: end if 16: until track cost \u2265 0 17: Output: Solution f\ndo not negate edge weights in the original graph. In practice, we can instead update edge costs and directions on the original graph at the end of each iteration, in such a case we should add pairwise costs to forward nodes or subtract pairwise costs from backward nodes if we turn on some node, similarly we subtract pairwise costs from forward nodes or add pairwise costs to backward nodes if we turn off some node."}, {"heading": "10.2. Approximation Quality of Two-pass DP", "text": "We found that 2-pass DP often finds lower cost than 1- pass DP but still not as good as LP+rounding. It also runs significantly slower, even slower than LP+rounding on long sequences. On a 1059 frame-long video with 3 categories of objects, 2-pass DP uses about 6 minutes to finish, whereas 1-pass DP finishes within 1 minute and LP+rounding finishes within 4 minutes. The leave-one-sequence-out crossvalidation result using 2-pass DP gets a MOTA of 60.4%, which is equivalent to that of 1-pass DP and LP relaxation.\nWe observe that most of the running time for 2-pass DP is on the second forward pass, which involves backtracking for each forward node to avoid cyclic path. It should be noted that with proper data structure such as a hash linkedlist to cache path arrays, checking cyclic path can be done in O(1). Also, in the second forward pass, one could set all backward nodes as active and propagate active labels to other forward nodes, so eventually we might not need to look at every forward node. Overall, though showing some incompetence in running time in our current implementation, 2-pass DP should still be a promising inference method with better choice of data-structures and moderate optimization."}], "references": [{"title": "Network Flows: Theory, Algorithms, and Applications", "author": ["R.K. Ahuja", "T.L. Magnanti", "J.B. Orlin"], "venue": "Prentice-Hall, Inc., Upper Saddle River, NJ, USA,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Discretecontinuous optimization for multi-target tracking", "author": ["A. Andriyenko", "K. Schindler", "S. Roth"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluating multiple object tracking performance: The clear mot metrics", "author": ["K. Bernardin", "R. Stiefelhagen"], "venue": "J. Image Video Process., 2008:1:1\u20131:10, Jan.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiobject tracking as maximum weight independent set", "author": ["W. Brendel", "M. Amer", "S. Todorovic"], "venue": "In Proc. IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-target tracking by lagrangian relaxation to min-cost network flow", "author": ["A.A. Butt", "R.T. Collins"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "On pairwise cost for multi-object network flow tracking", "author": ["V. Chari", "S. Lacoste-Julien", "I. Laptev", "J. Sivic"], "venue": "CoRR, abs/1408.3304,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative models for multi-class object layout", "author": ["C. Desai", "D. Ramanan", "C. Fowlkes"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Training structural SVMs when exact inference is intractable", "author": ["T. Finley", "T. Joachims"], "venue": "International Conference on Machine Learning (ICML), pages 304\u2013311,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "3d traffic scene understanding from movable platforms", "author": ["A. Geiger", "M. Lauer", "C. Wojek", "C. Stiller", "R. Urtasun"], "venue": "Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.-N. Yu"], "venue": "Machine Learning, 77(1):27\u201359,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient image and video co-localization with frank-wolfe algorithm", "author": ["A. Joulin", "K. Tang", "L. Fei-Fei"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Online multitarget tracking by large margin structured learning", "author": ["S. Kim", "S. Kwak", "J. Feyereisl", "B. Han"], "venue": "Proceedings of the 11th Asian Conference on Computer Vision - Volume Part III, ACCV\u201912, pages 98\u2013111, Berlin, Heidelberg,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Word alignment via quadratic assignment", "author": ["S. Lacoste-Julien", "B. Taskar", "D. Klein", "M.I. Jordan"], "venue": "Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL \u201906, pages 112\u2013 119, Stroudsburg, PA, USA,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning to associate: Hybridboosted multi-target tracker for crowded scene", "author": ["Y. Li", "C. Huang", "R. Nevatia"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Beyond Pixels: Exploring New Representations and Applications for Motion Analysis", "author": ["C. Liu"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Structured Learning for Cell Tracking", "author": ["X. Lou", "F.A. Hamprecht"], "venue": "Twenty-Fifth Annual Conference on Neural Information Processing Systems (NIPS 2011),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Detection- and trajectory-level exclusion in multiple object tracking", "author": ["A. Milan", "K. Schindler", "S. Roth"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Globallyoptimal greedy algorithms for tracking a variable number of objects", "author": ["H. Pirsiavash", "D. Ramanan", "C.C. Fowlkes"], "venue": "IEEE conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning crfs using graph cuts", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "European Conference on Computer Vision, October", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "MIT Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Tracklet association with online target-specific metric learning", "author": ["B. Wang", "G. Wang", "K. Luk Chan", "L. Wang"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Coupling detection and data association for multiple object tracking", "author": ["Z. Wu", "A. Thangali", "S. Sclaroff", "M. Betke"], "venue": "In Proceeding of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "An online learned crf model for multi-target tracking", "author": ["B. Yang", "R. Nevatia"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Article: A survey of quadratic assignment problems", "author": ["A.N.H. Zaied", "L.A.E. fatah Shawky"], "venue": "International Journal of Computer Applications,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Global data association for multi-object tracking using network flows", "author": ["L. Zhang", "Y. Li", "R. Nevatia"], "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition, 0:1\u20138,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 25, "context": ", [27, 20]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 18, "context": ", [27, 20]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 1, "context": ", [2, 19, 24]) allow for explicitly specifying an intuitive model of trajectory smoothness but face a difficult joint inference problem over both continuous and discrete variables with little guarantee of optimality.", "startOffset": 2, "endOffset": 13}, {"referenceID": 17, "context": ", [2, 19, 24]) allow for explicitly specifying an intuitive model of trajectory smoothness but face a difficult joint inference problem over both continuous and discrete variables with little guarantee of optimality.", "startOffset": 2, "endOffset": 13}, {"referenceID": 22, "context": ", [2, 19, 24]) allow for explicitly specifying an intuitive model of trajectory smoothness but face a difficult joint inference problem over both continuous and discrete variables with little guarantee of optimality.", "startOffset": 2, "endOffset": 13}, {"referenceID": 23, "context": "didate tracklets and then perform scoring and association of these tracklets [25, 4, 23].", "startOffset": 77, "endOffset": 88}, {"referenceID": 3, "context": "didate tracklets and then perform scoring and association of these tracklets [25, 4, 23].", "startOffset": 77, "endOffset": 88}, {"referenceID": 21, "context": "didate tracklets and then perform scoring and association of these tracklets [25, 4, 23].", "startOffset": 77, "endOffset": 88}, {"referenceID": 4, "context": "Another approach is to attempt to include higher-order constraints directly in a combinatorial framework [5, 6].", "startOffset": 105, "endOffset": 111}, {"referenceID": 5, "context": "Another approach is to attempt to include higher-order constraints directly in a combinatorial framework [5, 6].", "startOffset": 105, "endOffset": 111}, {"referenceID": 20, "context": "In Section 5 we describe an approach to joint learning of model parameters in order to maximize tracking performance on a training data set using techniques for structured prediction [22].", "startOffset": 183, "endOffset": 187}, {"referenceID": 12, "context": "Structured prediction has been applied in tracking to learning inter-frame affinity metrics [14] and association [18] as well as a variety of other learning tasks such as fitting CRF parameters for segmentation [21] and word alignment for machine translation [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "Structured prediction has been applied in tracking to learning inter-frame affinity metrics [14] and association [18] as well as a variety of other learning tasks such as fitting CRF parameters for segmentation [21] and word alignment for machine translation [15].", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "Structured prediction has been applied in tracking to learning inter-frame affinity metrics [14] and association [18] as well as a variety of other learning tasks such as fitting CRF parameters for segmentation [21] and word alignment for machine translation [15].", "startOffset": 211, "endOffset": 215}, {"referenceID": 13, "context": "Structured prediction has been applied in tracking to learning inter-frame affinity metrics [14] and association [18] as well as a variety of other learning tasks such as fitting CRF parameters for segmentation [21] and word alignment for machine translation [15].", "startOffset": 259, "endOffset": 263}, {"referenceID": 25, "context": "We begin by formulating multi-target tracking and data association as a min-cost flow network problem equivalent to that of [27], where individual tracks are described by a first-order Markov Model whose state space is spatialtemporal locations in videos.", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "This ILP is a well studied problem known as minimum-cost network flow [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 25, "context": "The constraints satisfy the total unimodularity property and thus can be solved exactly using any LP solver or via various efficient specialized solvers, including network simplex, successive shortest path and push-relabel with bisectional search [27].", "startOffset": 247, "endOffset": 251}, {"referenceID": 18, "context": "While these approaches yield globally optimal solutions, the authors of [20] consider even faster approximations based on multiple rounds of dynamic programming (DP).", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "[20] found that two or even one pass of DP often performs nearly as well as SSP in practical tracking scenarios.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "In contrast, finding the global minimum of the IQP problem 4 is NP-hard [26] due to the quadratic terms.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "For example, one could apply Frank-Wolfe algorithm to optimize the relaxed, convexified QP while simultaneously keeping track of good integer solutions [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 5, "context": "[6], reformulating the IQP as an equivalent ILP problem by replacing the quadratic terms fifj with a set of auxiliary variables uij :", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] proposed two rounding heuristics: a Euclidean rounding scheme that minimizes \u2016f\u2212f\u0302\u2016 where f\u0302 is the non-integral solution given by the LP relaxation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "We now describe a simple greedy algorithm inspired by the combination of dynamic programming and nonmaximal suppression proposed in [20].", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "\u03c8(xi, xj) is a discretized spatial-context feature that bins relative location of detection window at location xi and window at location xj into one of the D relations including on top of, above, below, next-to, near, far and overlap (similar to the spatial context of [7]).", "startOffset": 269, "endOffset": 272}, {"referenceID": 18, "context": "To mimic the temporal NMS described in [20] we add one additional relation, strictly overlap, which is defined as the intersection of two boxes over the area of the first box; we set the corresponding feature to 1 if this ratio is greater than 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "We optimize the structured SVM objective in 8 using a standard cutting-plane method [12] in which the exponential number of constraints (one for each possible flow f\u0302 ) are approximated by a much smaller number of terms.", "startOffset": 84, "endOffset": 88}, {"referenceID": 6, "context": "In our implementation, at each iteration we add a single linear constraint which is a sum of violating constraints derived from individual videos in the dataset which is also a valid cutting plane constraint [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 7, "context": "These scenarios are termed \u201cundergenerating\u201d and \u201covergenerating\u201d respectively by [9] since approximate inference is performed over a subset or superset of the exact space of flows.", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "Transition Loss: A critical aspect for successful learning is to define a good loss vector that closely reassembles major tracking performance criteria, such as Multiple Object Tracking Accuracy (MOTA [3]).", "startOffset": 201, "endOffset": 204}, {"referenceID": 14, "context": "By definition, id switches and fragmentations [16] are determined by looking at labels of two consecutive transition links simultaneously, under such definition the loss cannot be optimized by our inference routine which only considers pairwise relations between detections within a frame.", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "Dataset: We have focused our experiments on training sequences of KITTI tracking benchmark [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "Specifically, we first compute frame-wise optical flow using software of [17], then for a candidate detection xi at frame ti, we compute the mean of vertical flows and the mean of horizontal flows within the candidate box, and use them to predict candi-", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "Inspired by the trajectory model of [2], we postprocess each output raw trajectory by fitting a cubic Bspline.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "Baselines: We use the publicly available code from [10] as a first baseline.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "Also we consider dynamic programming (DP) and successive shortest path (SSP) with default parameters in [20] as another two baselines, denoted as DP+Flow and SSP+Flow in our table.", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "Baseline [10] 57.", "startOffset": 9, "endOffset": 13}], "year": 2014, "abstractText": "We describe a model for multi-target tracking based on associating collections of candidate detections across frames of a video. In order to model pairwise interactions between different tracks, such as suppression of overlapping tracks and contextual cues about co-occurence of different objects, we augment a standard min-cost flow objective with quadratic terms between detection variables. We learn the parameters of this model using structured prediction and a loss function which approximates the multi-target tracking accuracy. We evaluate two different approaches to finding an optimal set of tracks under model objective based on an LP relaxation and a novel greedy extension to dynamic programming that handles pairwise interactions. We find the greedy algorithm achieves equivalent performance to the LP relaxation while being 2-7x faster than a commercial solver. The resulting model with learned parameters outperforms existing methods across several categories on the KITTI tracking benchmark.", "creator": "LaTeX with hyperref package"}}}