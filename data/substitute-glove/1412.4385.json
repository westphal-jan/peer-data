{"id": "1412.4385", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2014", "title": "Unsupervised Domain Adaptation with Feature Embeddings", "abstract": "Representation learning most along roots processes year self-paced entity award-winning, but applications requires often require when specification of \" thrust studio \" that generalize across domains, which often pick by our - multiple problem-solving. We show made yet autobiography but simple comedy semantic particularly example better usual, previously exploiting from clips template structure common three NLP risks.", "histories": [["v1", "Sun, 14 Dec 2014 17:44:58 GMT  (145kb,D)", "http://arxiv.org/abs/1412.4385v1", null], ["v2", "Mon, 30 Mar 2015 19:35:12 GMT  (139kb,D)", "http://arxiv.org/abs/1412.4385v2", "For more details, please refer to the long version of this paper:this http URL"], ["v3", "Thu, 16 Apr 2015 01:44:48 GMT  (139kb,D)", "http://arxiv.org/abs/1412.4385v3", "For more details, please refer to the long version of this paper:this http URL"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yi yang", "jacob eisenstein"], "accepted": true, "id": "1412.4385"}, "pdf": {"name": "1412.4385.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yi Yang"], "emails": ["jacobe}@gatech.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Domain adaptation is crucial if natural language processing is to be successfully employed in highimpact application areas such as social media, patient medical records, and historical texts. Unsupervised domain adaptation is particularly appealing, since it requires no labeled data in the target domain. Some of the most successful approaches to unsupervised domain adaptation are based on representation learning: transforming sparse high-dimensional surface features into dense vector representations, which are often more robust to domain shift (Blitzer et al., 2006; Glorot et al., 2011). However, these methods are computationally expensive (sometimes employing thousands of dense features), and often require special task-specific heuristics to select good \u201cpivot features\u201d.\nWe present FEMA (Feature EMbeddings for domain Adaptation), a novel representation learning approach for domain adaptation in structured feature spaces. Like prior work in representation learning, FEMA learns dense features that are more robust to domain shift. However, FEMA diverges from previous approaches based on reconstructing pivot features; instead, it uses techniques from neural language models to directly obtain low-dimensional embeddings. FEMA outperforms prior work on adapting POS tagging from the Penn Treebank to web text."}, {"heading": "2 LEARNING FEATURE EMBEDDINGS", "text": "Feature co-occurrence statistics are the primary source of information driving many unsupervised methods for domain adaptation. For example, both Structural Correspondence Learning (SCL; Blitzer et al., 2006) and Denoising Autoencoders (Chen et al., 2012) learn to reconstruct a subset of \u201cpivot features\u201d, as shown in Figure 1(a). The reconstruction function is then employed to project each instance into a dense representation, which will hopefully be better suited to crossdomain generalization. The pivot features are chosen to be both predictive of the label and general across domains. Meeting these two criteria requires task-specific heuristics. Furthermore, the pivot features correspond to a small subspace of the feature co-occurrence matrix. We face a tradeoff between the amount of feature co-occurrence information that we can use, and the computational complexity for representation learning and downstream training.\nWe avoid this tradeoff by inducing low dimensional feature embeddings directly. We exploit the tendency of many NLP tasks to divide features into templates, with exactly one active feature per template (Smith, 2011); this is shown in the center of Figure 1. Rather than treating each instance as an undifferentiated bag-of-features, we exploit this template structure to induce feature embeddings: dense representations of individual features. Each embedding is selected to help predict the features that fill out the other templates; see Figure 1(b). The embeddings for each active feature are then concatenated together across templates, giving a dense representation for the entire instance.\nar X\niv :1\n41 2.\n43 85\nv1 [\ncs .C\nL ]\n1 4\nD ec\n2 01\n4\nOur feature embeddings are based on the skip-gram model, trained with negative sampling (SGNS; Mikolov et al., 2013), which is a simple yet efficient method for learning word embeddings. The training objective is to find feature embeddings that are useful for predicting other active features in the instance. For the instance n \u2208 {1 . . . N} and feature template t \u2208 {1 . . . T}, we denote fn(t) as the index of the active feature; for example, in the instance shown in Figure 1, fn(t) = \u2018new\u2019 when t indicates the previous-word template. The skip-gram approach induces distinct \u201cinput\u201d and \u201coutput\u201d embeddings for each feature, written ufn(t) and vfn(t), respectively. The role of these embeddings can be seen in the negative sampling objective,\n`n = 1\nT T\u2211 t=1 T\u2211 t\u2032 6=t [ log \u03c3(u>fn(t)vfn(t\u2032)) + kEi\u223cP (n) t\u2032 log \u03c3(\u2212u>fn(t)vi) ] (1)\nwhere t and t\u2032 are feature templates, k is the number of negative samples, P (n)t\u2032 is a noise distribution for template t\u2032, and \u03c3 is the sigmoid function.\nFeature embeddings can be applied to domain adaptation by learning embeddings of all features on the union of the source and target data sets. The dense feature vector for each instance is obtained by concatenating the feature embeddings for each template. Finally, since it has been shown that nonlinearity is important for generating robust representations (Bengio et al., 2013), we follow (Chen et al., 2012) and apply the hyperbolic tangent function to the embeddings. The augmented representation x(aug)n of instance n is the concatenation of the original feature vector and the feature embeddings: x(aug)n = xn \u2295 tanh[ufn(1) \u2295 \u00b7 \u00b7 \u00b7 \u2295 ufn(T )], where \u2295 is vector concatenation."}, {"heading": "3 EXPERIMENTS", "text": "We evaluate FEMA on part-of-speech (POS) tagging: adaptation of English POS tagging from news text to web text, as in the SANCL shared task (Petrov & McDonald, 2012)."}, {"heading": "3.1 EXPERIMENT SETUP", "text": "Datasets We use data from the SANCL shared task (Petrov & McDonald, 2012), which contains several web-related corpora (newsgroups, reviews, weblogs, answers, emails) as well as the WSJ portion of OntoNotes corpus (Hovy et al., 2006). Following Schnabel & Schu\u0308tze (2014), we use sections 02-21 of WSJ for training and section 22 for development, and use 100,000 unlabeled WSJ sentences from 1988 for learning representations. On the web text side, each of the five target domains has an unlabeled training set of 100,000 sentences, along with development and test sets of about 1000 labeled sentences each.\nSVM tagger While POS tagging is classically treated as a structured prediction problem, we follow Schnabel & Schu\u0308tze (2014) by taking a classification-based approach. Specifically, we apply a support vector machine (SVM) classifier, adding dense features from FEMA (and the alternative\nrepresentation learning techniques) to the set of basic features. We apply sixteen basic feature templates introduced by Ratnaparkhi et al. (1996). Feature embeddings are learned for all lexical and affix features, yielding a total of thirteen embeddings per instance.\nCompetitive systems We consider two competitive unsupervised domain adaptation methods that require pivot features: Structural Correspondence Learning (SCL; Blitzer et al., 2006) and marginalized Denoising Autoencoders (mDA; Chen et al, 2012). We use structured dropout noise for mDA (Yang & Eisenstein, 2014). We also directly compare with WORD2VEC1 word embeddings, and with a baseline approach in which we simply train on the source domain data using the surface features, and then test on the target domain. Aside from our own implemented methods, we compare against published results of FLORS (Schnabel & Schu\u0308tze, 2014), which uses distributional features for domain adaptation. We also republish the results of Schnabel and Schutze using the Stanford POS Tagger, a maximum entropy Markov model (MEMM) tagger.\nParameter tuning All the hyper-parameters are tuned on development data. Following Blitzer et al. (2006), we consider 6918 pivot features that appear more than 50 times in all the domains for SCL and mDA. The best parameters for SCL are dimensionality K = 50 and rescale factor \u03b1 = 5. For both FEMA and word2vec, the best embedding size is 100 and the best number of negative samples is 5. The noise distribution P (n)t is simply the unigram probability of each feature in the template t. Mikolov et al. (2013b) argue for exponentiating the unigram distribution, but we find it makes little difference here. The window size of word embeddings is set as 5."}, {"heading": "3.2 RESULTS", "text": "As shown in Table 1 and 2, FEMA outperforms competitive systems on all target domains except REVIEW, where FLORS performs slightly better. FLORS uses more basic features than FEMA; these features could in principle be combined with feature embeddings for better performance. Compared with the other representation learning approaches, FEMA is roughly 1% better on average, corresponding to an error reduction of 10%. Its training time is approximately 70 minutes on a 24-core machine, using an implementation based on gensim.2 This is slightly faster than SCL, although slower than mDA with structured dropout noise."}, {"heading": "4 RELATED WORK", "text": "Representation learning approaches to domain adaptation seek for cross-domain representations, which were first induced via auxiliary prediction problems (Ando & Zhang, 2005), such as the\n1https://code.google.com/p/word2vec/ 2http://radimrehurek.com/gensim/\nprediction of pivot features (Blitzer et al., 2006). In these approaches, as well as in later work on denoising autoencoders (Chen et al., 2012), the key mechanism is to learn a function to predict a subset of features for each instance, based on other features of the instance.\nWord embeddings can be viewed as special case of representation learning, where the goal is to learn representations for each word, and then to supply these representations in place of lexical features (Turian et al., 2010)."}, {"heading": "5 CONCLUSION", "text": "Feature embeddings can be used for domain adaptation in any problem involving feature templates. They offer strong performance, avoid practical drawbacks of alternative representation learning approaches, and are easy to learn using existing word embedding methods."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Rie Kubota", "Zhang", "Tong"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer", "John", "McDonald", "Ryan", "Pereira", "Fernando"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Chen", "Minmin", "Z. Xu", "Weinberger", "Killian", "Sha", "Fei"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Ontonotes: the 90% solution", "author": ["Hovy", "Eduard", "Marcus", "Mitchell", "Palmer", "Martha", "Ramshaw", "Lance", "Weischedel", "Ralph"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In Proceedings of International Conference on Learning Representations,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "Slav", "McDonald", "Ryan"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["Ratnaparkhi", "Adwait"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Ratnaparkhi and Adwait,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi and Adwait", "year": 1996}, {"title": "Flors: Fast and simple domain adaptation for part-of-speech tagging", "author": ["Schnabel", "Tobias", "Sch\u00fctze", "Hinrich"], "venue": "Transactions of the Association of Computational Linguistics,", "citeRegEx": "Schnabel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2014}, {"title": "Linguistic structure prediction", "author": ["Smith", "Noah A"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "Smith and A.,? \\Q2011\\E", "shortCiteRegEx": "Smith and A.", "year": 2011}, {"title": "Word Representation: A Simple and General Method for Semi-Supervised Learning", "author": ["Turian", "Joseph", "Ratinov", "Lev", "Bengio", "Yoshua"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Under review as a workshop contribution at ICLR", "author": ["Yi", "Eisenstein", "Jacob"], "venue": null, "citeRegEx": "Yi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Some of the most successful approaches to unsupervised domain adaptation are based on representation learning: transforming sparse high-dimensional surface features into dense vector representations, which are often more robust to domain shift (Blitzer et al., 2006; Glorot et al., 2011).", "startOffset": 244, "endOffset": 287}, {"referenceID": 4, "context": "Some of the most successful approaches to unsupervised domain adaptation are based on representation learning: transforming sparse high-dimensional surface features into dense vector representations, which are often more robust to domain shift (Blitzer et al., 2006; Glorot et al., 2011).", "startOffset": 244, "endOffset": 287}, {"referenceID": 2, "context": "For example, both Structural Correspondence Learning (SCL; Blitzer et al., 2006) and Denoising Autoencoders (Chen et al.", "startOffset": 53, "endOffset": 80}, {"referenceID": 3, "context": ", 2006) and Denoising Autoencoders (Chen et al., 2012) learn to reconstruct a subset of \u201cpivot features\u201d, as shown in Figure 1(a).", "startOffset": 35, "endOffset": 54}, {"referenceID": 6, "context": "Our feature embeddings are based on the skip-gram model, trained with negative sampling (SGNS; Mikolov et al., 2013), which is a simple yet efficient method for learning word embeddings.", "startOffset": 88, "endOffset": 116}, {"referenceID": 1, "context": "Finally, since it has been shown that nonlinearity is important for generating robust representations (Bengio et al., 2013), we follow (Chen et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 3, "context": ", 2013), we follow (Chen et al., 2012) and apply the hyperbolic tangent function to the embeddings.", "startOffset": 19, "endOffset": 38}, {"referenceID": 5, "context": "Datasets We use data from the SANCL shared task (Petrov & McDonald, 2012), which contains several web-related corpora (newsgroups, reviews, weblogs, answers, emails) as well as the WSJ portion of OntoNotes corpus (Hovy et al., 2006).", "startOffset": 213, "endOffset": 232}, {"referenceID": 5, "context": "Datasets We use data from the SANCL shared task (Petrov & McDonald, 2012), which contains several web-related corpora (newsgroups, reviews, weblogs, answers, emails) as well as the WSJ portion of OntoNotes corpus (Hovy et al., 2006). Following Schnabel & Sch\u00fctze (2014), we use sections 02-21 of WSJ for training and section 22 for development, and use 100,000 unlabeled WSJ sentences from 1988 for learning representations.", "startOffset": 214, "endOffset": 270}, {"referenceID": 2, "context": "Competitive systems We consider two competitive unsupervised domain adaptation methods that require pivot features: Structural Correspondence Learning (SCL; Blitzer et al., 2006) and marginalized Denoising Autoencoders (mDA; Chen et al, 2012).", "startOffset": 151, "endOffset": 178}, {"referenceID": 2, "context": "Following Blitzer et al. (2006), we consider 6918 pivot features that appear more than 50 times in all the domains for SCL and mDA.", "startOffset": 10, "endOffset": 32}, {"referenceID": 2, "context": "Following Blitzer et al. (2006), we consider 6918 pivot features that appear more than 50 times in all the domains for SCL and mDA. The best parameters for SCL are dimensionality K = 50 and rescale factor \u03b1 = 5. For both FEMA and word2vec, the best embedding size is 100 and the best number of negative samples is 5. The noise distribution P (n) t is simply the unigram probability of each feature in the template t. Mikolov et al. (2013b) argue for exponentiating the unigram distribution, but we find it makes little difference here.", "startOffset": 10, "endOffset": 440}, {"referenceID": 2, "context": "prediction of pivot features (Blitzer et al., 2006).", "startOffset": 29, "endOffset": 51}, {"referenceID": 3, "context": "In these approaches, as well as in later work on denoising autoencoders (Chen et al., 2012), the key mechanism is to learn a function to predict a subset of features for each instance, based on other features of the instance.", "startOffset": 72, "endOffset": 91}, {"referenceID": 12, "context": "Word embeddings can be viewed as special case of representation learning, where the goal is to learn representations for each word, and then to supply these representations in place of lexical features (Turian et al., 2010).", "startOffset": 202, "endOffset": 223}], "year": 2017, "abstractText": "Representation learning is the dominant technique for unsupervised domain adaptation, but existing approaches often require the specification of \u201cpivot features\u201d that generalize across domains, which are selected by task-specific heuristics. We show that a novel but simple feature embedding approach provides better performance, by exploiting the feature template structure common in NLP problems.", "creator": "LaTeX with hyperref package"}}}