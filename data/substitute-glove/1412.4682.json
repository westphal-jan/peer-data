{"id": "1412.4682", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2014", "title": "Rule-based Emotion Detection on Social Media: Putting Tweets on Plutchik's Wheel", "abstract": "We health buoyant techniques because the typical granularity on activation with up use Plutchik ', spinning bringing touch which. We required RBEM - Emo. instance implementation because is Rule - Based Emission Model algorithm for deduces such contradictions from dealing - essay messages. We assign our approach full two different read-out and those also achieved apart the recent current - of - instead - art method bringing cries monitoring, including a recursive auto - griess. The 2004 in with integrated furthermore widely that RBEM - Emo whole second providing approach stretched the year official - addition - the - abstract under shock detection.", "histories": [["v1", "Mon, 15 Dec 2014 17:20:47 GMT  (122kb,D)", "http://arxiv.org/abs/1412.4682v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["erik tromp", "mykola pechenizkiy"], "accepted": false, "id": "1412.4682"}, "pdf": {"name": "1412.4682.pdf", "metadata": {"source": "CRF", "title": "Rule-based Emotion Detection on Social Media: Putting Tweets on Plutchik\u2019s Wheel", "authors": ["Erik Tromp", "Mykola Pechenizkiy"], "emails": ["erik.tromp@adversitement.com", "m.pechenizkiy@tue.nl"], "sections": [{"heading": "1 Introduction", "text": "Current sentiment analysis methods - ranging from baseline bag-of-words methods to state-ofthe-art neural methods - typically focus on deducing information on subjectivity or polarity only (Section 2). Human emotions move far beyond these simple metrics and are much more diverse. This implies that such subjectivity- or polarityanalysis only gives limited information on the actual intent of an author of a message.\nDefining axes of polarity is not a hard task, typically one has negativity, positivity and a notion of neutrality or objectivity in between. For emotions however, defining a complete and clear set of emotions is much more difficult. Though several researchers attempted at defining standards in this field (Parrott, 2001; Plutchik, 1980; Schroder et al., 2011), AAAC1, there is still no consensus on a basic set of emotions that is generally accepted and could be objectively verified.\nThe goal of this paper is to present a sentiment analysis approach accompanied by a model\n1The Association for the Advancement of Affective Computing - http://emotion-research.net/\nof emotions that fit well together in order to set a standard in emotion analysis to expand upon.\nWe present a new RBEM-Emo approach for emotion detection from human-written texts.2 This algorithm is based on work by (Tromp and Pechenizkiy, 2013) where the authors introduced the Rule-Based Emission Model (RBEM) algorithm for polarity detection only. RBEM generates positive and negative emissions based on several groups of patterns that capture various ways how sentiment can be expressed in natural language. We show how this approach can be developed further to go beyond polarity and measure emotions as given by Plutchik\u2019s wheel of emotions.\nWe conducted an experimental evaluation of RBEM-Emo on a publicly available benchmark and on a new benchmark that we constructed. The results of our evaluation suggest that RBEMEmo outperforms the current state-of-the-art approaches for emotion detection. To facilitate reproducibility of the results and further progress in emotion classification we made our benchmark publicly available."}, {"heading": "2 Related Work", "text": "Moving beyond polarity in sentiment analysis in currently upcoming and not well studied yet. Few examples can be found where novel methods are introduced to capture more information than just polarity such as the work of (Socher et al., 2011) where a recursive auto-encoder is used to predict sentiment distributions in five dimensions. (Cambria and Hussain, 2012) and (Cambria et al., 2012) promote affective computing using a framework they call SenticNet. The sentiment dimensions of this framework are modeled in an hourglassmodel which is a derivative of Plutchik\u2019s wheel of emotions (Plutchik, 1980). In (Mohammad,\n2We expect a revised and extended version of this manuscript describing RBEM-Emo to appear in (Tromp and Pechenizkiy, 2015)\nar X\niv :1\n41 2.\n46 82\nv1 [\ncs .C\nL ]\n1 5\nD ec\n2 01\n4\n2012) the author collected and experimented with a large collection of tweets with self-labeled emotion hashtags.\nThe closest work to our approach is (Andreevskaia and Bergler, 2007), in which the authors considered a rule-based approach based on a set of positive and negative patterns and valence shifters for handling negations and other linguistic constructs defining the sentiment of a sentence.\nStandards on emotion frameworks are difficult to define as emotions are usually subjective and cannot be crisply defined. Works of (Parrott, 2001; Plutchik, 1980; Schroder et al., 2011) do aim to define standards in this area by defining a minimal set of basic emotions from which more complex ones can be derived or constructed by combining basic emotions. In (Cambria et al., 2012) the authors develop methods to reason about emotions. In (Ekman, 1989), facial expressions are linked to emotions and a final six universal basic emotions are presented."}, {"heading": "3 Approach to Emotion Detection", "text": ""}, {"heading": "3.1 Plutchik\u2019s Wheel of Emotions", "text": "To tackle the problem of emotion detection, one needs to have a notion of emotion. As e.g. in text mining the problem can be formulated differently depending on whether we have just two classes like in spam filtering, or several categories like topic classification or a large number of categories like in automated tagging. We choose the wheel of emotions defined by Robert Plutchik (Plutchik, 1980) (see Figure 1) because it defines only eight basic emotions, which makes the problem manageable for envisioned applications and RBEMEmo a good match to perform classification according to this model of emotions.\nThese eight emotions are assumed to be complete in the sense that any expressed emotion is related or subsumed by one of the eight. In his work, Plutchik states that these emotions are culturally independent. Given this assumption, we can apply this model to any given language, which we consider to be a strong point.\nAnother reason for using this model is that each of these eight basic emotions are opposites of one of the other basic emotions. This means that we can in fact measure four axes where opposite emotions exist on the two extremes of a single axis. Additionally, Plutchik defines eight human feelings that are derivatives of combinations of two\nbasic emotions. This in fact means that with modeling only four axes, we can get a total of sixteen dimensions of emotions and feelings."}, {"heading": "3.2 RBEM for Emotion Detection", "text": "In our previous work we conducted several case studies with RBEM illustrating that it is rather generic and easily extendable allowing to develop different solutions that are scalable, transparent, and easy to maintain and adapt for the needs of a particular domain. We considered integrating RBEM into a larger data analytics project (Tromp and Pechenizkiy, 2011) and into mobile settings with computing resources be a bottleneck (Chambers et al., 2012). Hence, we had a good incentive to extend it to emotion classification on social media.\nThe original Rule-Based Emission Model (RBEM) algorithm (Tromp and Pechenizkiy, 2013) can be used for polarity detection assigning new messages a label that is one of positive, neutral, negative. The algorithm\u2019s internals work in such a way that either positive or negative emissions can be generated upon which subsequently different rules are executed to modify these emissions.\nThe rules work on patterns that belong to one of the following groups: positive and negative patterns, e.g. good, well done and bad, terrible; amplifier and attenuator patterns to strengthen or weaken polarity of entities very much, a lot and a little, a tiny bit; right- and left-flip patterns to\nhandle negations, e.g. not, no and sentences containing constructs with e.g. but, however; continuator patterns to handle constructs with e.g. and, and also; stop patterns to interrupt the emission of polarity when punctuation signs such as a dot or an exclamation mark, expressing the general case that polarity does not cross sentence boundaries, appear in a message.\nCrucial to the algorithm is that positivity and negativity are opposites of each other and hence allow for example negations to simply invert the emission. This specific characteristic of the algorithm makes it work well with Plutchik\u2019s model since the emotions defined in that model are also opposites of each other. We in fact extend the RBEM algorithm to perform the same type of rules but now \u2013 instead of having one axis to measure; positive on one end of the extreme and negative on the other extreme \u2013 we have four different axes, together yielding eight different emotions being measured.\nThe RBEM algorithm requires pattern groups to be defined. It uses a pattern matching on wildcards to identify patterns in a message. When classifying previously unseen messages, two steps are performed. First all patterns in the model that match a message are collected. Then, rule(s) associated with each pattern group for each pattern present in the message are applied.\nThis actual internal algorithms for constructing and applying RBEM remain unchanged. We refer to the original paper on RBEM for their formal description (Tromp and Pechenizkiy, 2013).\nRBEM-Emo extends RBEM for emotion detection by introducing new pattern groups. The RBEM algorithm uses two base pattern groups to define emission of polarity, positive and negative patterns. For our RBEM-Emo algorithm, we replace these two pattern groups with eight new pattern groups, one for each basic emotion of Plutchik\u2019s model: joy, sadness, trust, disgust, fear, anger, surprise, anticipation. Similarly, we replace the two rules that are defined on positive and negative patterns with eight new rules. Note that conceptually, we perform the exact same process we do for positive polarity on one hand and negative polarity on the other hand, but now four times, once for each axis.\nSince we no longer operate on a single emission score but instead on four, we define a mapping from emotions to an index by escemo\nand we define a sign counterparts signemo for each emotion on a single axis. Here escJoy = escSadness = 1 and signJoy = 1, signSadness = \u22121, escTrust = escDisgust = 2 and signTrust = 1, signDisgust = \u22121, escFear = escAnger = 3 and signFear = 1, signAnger = \u22121, escSurprise = escAnticipation = 4 and signSurprise = 1, signAnticipation = \u22121. We also define a subscripted emission score emj(ei) where j \u2208 [1, 4] and the value of j corresponds with the emotion axis for the emotions that map to j using escemo (i.e. em1 is the axis function used by Joy and Sadness).\nThe new rules that replace the original rules defining positive sentiment emission and negative sentiment emission are defined as shown at the top of the page3.\nAll the the other original RBEM rules are executed four times, once for every emj , j \u2208 [1, 4]. When the algorithm terminates, this yields us four emission scores, i.e. one score per dimension.\nOnce the algorithm has terminated, we can obtain a total score for each pair or opposite emotions, e.g. for Joy and Sadness by summing of all emissions of emj . JoySadness = \u2211n i=1 em1(ei). Whenever JoySadness > 0 we say that Joy was expressed in the original message. Similarly, when JoySadness < 0, we say that Sadness was expressed. If JoySadness = 0, neither Joy nor Sadness was expressed. The other three emission axes can be interpreted similarly.\nAs an illustrative example, consider the sentence I thought I would like the new XYZ phone, but now that I have it, it is a huge disappointment, it makes me angry. Suppose also that we have the following patterns (Part-of-Speech tags left out for simplicity): (I \u2217 like, Anticipation), (but, Leftflip), (huge,Amplifier), (disappointment, Sadness), (angry,Anger). The algorithm would first assign the emotion scores to all parts of the sentence where patterns are found. This would yield the first part emitting negatively on em4, the third phrase emitting negatively on em1 and the last phrase emitting negatively on em3. Next, the scores on pattern indicated by the word huge will amplify the emissions on all axes, with the biggest effect on em1. Finally, the leftflip indicated by but will convert all negative emissions on its left \u2013 influencing\n3Note that the RBEM algorithm requires rules to be executed in-order.\nEmission of emotions rules:\n\u2200emo\u2208{Joy,Sadness,Trust,Disgust,Fear,Anger,Surprise,Anticipation} :\n\u2200(s,f,emo)\u2208maxPatterns : c = b s+ f\n2 c \u2227 (\u2200ei\u2208m : \u00ac(\u2203t\u2208stops : c \u2265 i\u21d2 i \u2264 t \u2264 c \u2228 i \u2265 c\u21d2 c \u2264 t \u2264 i)\n\u21d4 emescemo (ei) = emescemo (ei) + signemo \u00b7 e\u2212i)\nem4 mainly \u2013 to its opposite direction, yielding positive emissions on em4. The final outcome will hence be that \u2013 ordered by decreasing strength \u2013 Sadness, Anger and Surprise are present."}, {"heading": "4 Experimental Evaluation", "text": "With the experimental study we aim to evaluate the proposed RBEM-Emo algorithm, which is tailored towards Plutchik\u2019s model of emotions."}, {"heading": "4.1 Experiment Setup", "text": "We compare our method against a majority class baseline, Support Vector Machines (SVMs), regression and the recursive auto-encoder of (Socher et al., 2011) and evaluate on accuracy. In (Socher et al., 2011) five-dimensional sentiment model originating from the Experience Project4 is introduced. It would be reasonable to evaluate on this dataset, but the five labels used to express emotions in that dataset are quite arbitrary and ambiguous5, as the authors already indicate. In addition, these labels are produced by users that read an actual confession by a different person and instead of capturing the emotion of the actual message hence capture the emotion triggered with an external reader.\nDue to the impracticalities of the Experience Project dataset for our experiments, we instead benchmark on a different, well-accepted dataset introduced in (Alm, 2008). This dataset is annotated using Ekman\u2019s emotions (Ekman, 1989) instead of Plutchik\u2019s, but since the six basic emotions of Ekman are subsumed by the eight emotions of Plutchik\u2019s model, we can use the labels in a straightforward manner, ignoring labels produced by RBEM-Emo that do not exist in Ekman\u2019s model and producing the majority class as label in case we find a non-existing emotion. We refer to this dataset as the Affect Dataset.\nIn addition to benchmarking on a well-accepted\n4See http://www.experienceproject.com 5The labels are Sorry, Hugs, You Rock, Teehee, I Under-\nstand and Wow, Just Wow\npublic dataset, we also introduce our own Twitter Dataset that is annotated on Plutchik\u2019s emotions.\nFor the SVM and regression classification we use LibShortText (Yu et al., 2013). We experiment using both word counts and TF-IDF scores as features. For the recursive auto-encoder, we use the Java version referenced to by the authors of (Socher et al., 2011)6. To ensure we have the right setup of the auto-encoder, we reproduced the polarity detection experiments on the rotten tomatoes dataset as done in (Socher et al., 2011) and obtained an accuracy of 77.0%. This is in line with the results presented in (Socher et al., 2011), illustrating our setup is valid. When we apply our RBEM-Emo classifier, we get four scores for each axis in Plutchik\u2019s model, summing up to eight emotions. Finally, we assign a single label corresponding to the highest of all eight emotion scores."}, {"heading": "4.2 Datasets Description", "text": "The Affect Dataset we use is presented in (Alm, 2008) and is publicly available7. This dataset consists of snippets of text obtained from books written by three different authors.\nFor each snippet, every sentence is annotated by two annotators. These annotators provide two different labels each, one for the prevailing emotion found in the sentence and one for the mood found. The available labels are the six basic emotions of Ekman\u2019s universal emotions, being angry, disgusted, fearful, happy, sad, surprised. In addition, the authors could also indicate neutrality.\nWe use only those messages for which both annotators agree upon emotion and we discard the mood label produced by the annotators. Moreover, since 85% of all sentences in the dataset are neutral, and many general purpose classification techniques suffer from class imbalance, we produce two different datasets, one where neutral sentences are removed and only emotion-bearing sen-\n6Can be found at https://github.com/sancha/jrae 7http://lrc.cornell.edu/swedish/\ndataset/affectdata/\ntences are maintained and one where neutral messages are included. For evaluation purposes, we use roughly 23 of the data for training and 1 3 for testing. The resulting sizes of the training sets are 7527 and 1084 instances depending on the in- or exclusion of the neutral class, and for test sets \u2013 3590 and 488 instances correspondingly.\nTwitter Dataset. Since the proposed RBEMEmo method is tightly integrated with Plutchik\u2019s wheel of emotions, we evaluate on data annotated on these emotions. We collected a large amount of tweets in three different languages: English, Dutch and German. We had at least two independent annotators to annotate each of these messages using a dedicated Web-based annotation tool. In case of disagreement, we use the prevailing emotion label given by the annotators as actual label for a message. If there is no agreement on the prevailing emotion label, the message was discarded.\nIn addition, the annotators were asked to identify patterns in these messages such that we can later on construct the RBEM-Emo model from them.\nThe data was collected from Twitter where a language detection algorithm was used to filter out those messages that are written in English, Dutch or German as a first step. All messages wrongly identified by language are later on filtered out by the annotators.\nIn line with the setup of the experiments presented in (Socher et al., 2011) and adhered to here, we randomly split the data into roughly 23 training and 13 test data. The resulting training/test set sizes are Dutch 289/113 for Dutch, 235/113 for English and 225/109 for German.\nThe Twitter dataset is made publicly available8."}, {"heading": "4.3 Results", "text": "The accuracies of the best performing general purpose classification techniques on the Affect Dataset are compared to those of RBEM-Emo in Table 1. The majority class classification accuracy is given as a baseline. We report accuracies both for the case when neutral messages are kept in our dataset and when they are filtered out. We do this since the neutral messages compose 85% of the entire original dataset and it is expected that generic classification techniques will suffer from class imbalance and learn biases towards this data\n8http://www.win.tue.nl/\u02dcmpechen/ projects/smm/\nrather than find actual emotions. This is reflected in the accuracies of the SVM and regression classifiers which are marginally higher than the majority class baseline. Surprisingly, the recursive auto-encoder (RAE) that is currently claimed to be the state-of-the-art technique for emotion classification performs worse than several simpler classifiers and in fact is as good as a majority class classifier. One possible reason for this might be that the size of our dataset is relatively small. RBEMEmo classifier being a tailor approach to deduce emotional patterns outperforms the other classifiers.\nIn the second column of Table 1, we report the accuracies when all messages belonging to the neutral class are removed, yielding a more classbalanced dataset. Here we see much better improvements over the majority class baseline for SVM and regression and now also for the recursive auto-encoder. Using TF-IDF scores for features is favored over using just word counts. The RBEMEmo method however, still outperforms the other classifiers.\nTable 2 lists the accuracies obtained per language on our own Twitter corpus. For each classifier, we report the accuracy on each language (being Dutch, English and German) and report a total accuracy which is the average accuracy over all messages in all three languages. A generic result over all classifiers is that the accuracies on English data seem to be the lowest, implying most ambiguity within this language. Remarkable is that the recursive auto-encoder performs worse than SVM and regression models and yields no benefit over the majority class guess. Again, this could be due to the small size of the corpus or difficulty in finding the most suitable model parameters. There is no clear evidence on whether TF-IDF scores or word counts work better for this dataset. The RBEM-Emo classifiers yields the highest accuracy for each of three languages."}, {"heading": "5 Conclusions", "text": "In this work we have introduced a new rulebased classification technique called RBEM-Emo for emotion classification on social media. This emotion classification approach is tightly coupled with the Plutchik\u2019s model of emotions. We proposed to use this model because it relatively compact yet complete and models emotions as opposites of each other, a feature that works well with RBEM-Emo.\nThe results of our experimental study show that RBEM-Emo is competitive to the current state-ofthe-art approaches to sentiment and emotion classification.\nNew approaches for emotion classification appear every year. It is important to facilitate an easy way to benchmark and compare their performance. For studying emotion classification with Plutchik\u2019s model, we developed a new benchmark with carefully annotated Twitter messages in three different languages. To increase the reproducibility of our work and facilitate further development in this area, we released this benchmark to the public access. We also released the RBEM-Emo patterns extracted from the training dataset."}], "references": [{"title": "Affect in text and speech, PhD thesis", "author": ["E.C.O. Alm"], "venue": null, "citeRegEx": "Alm.,? \\Q2008\\E", "shortCiteRegEx": "Alm.", "year": 2008}, {"title": "Clac and clacnb: Knowledge-based and corpus-based approaches to sentiment tagging", "author": ["A. Andreevskaia", "S. Bergler."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval \u201907, pages 117\u2013120, Stroudsburg, PA, USA.", "citeRegEx": "Andreevskaia and Bergler.,? 2007", "shortCiteRegEx": "Andreevskaia and Bergler.", "year": 2007}, {"title": "Sentic Computing: Techniques, Tools, and Applications", "author": ["E. Cambria", "A. Hussain."], "venue": "Berlin Heidelberg: Springer.", "citeRegEx": "Cambria and Hussain.,? 2012", "shortCiteRegEx": "Cambria and Hussain.", "year": 2012}, {"title": "Senticnet 2: A semantic and affective resource", "author": ["E. Cambria", "C. Havasi", "A. Hussain"], "venue": null, "citeRegEx": "Cambria et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cambria et al\\.", "year": 2012}, {"title": "Mobile sentiment analysis", "author": ["L. Chambers", "E. Tromp", "M. Pechenizkiy", "M. Gaber."], "venue": "Advances in Knowledge-Based and Intelligent Information and Engineering Systems - 16th Annual KES Conference, pages 470\u2013479.", "citeRegEx": "Chambers et al\\.,? 2012", "shortCiteRegEx": "Chambers et al\\.", "year": 2012}, {"title": "Handbook of Social Psychophysiology, chapter The Argument and Evidence about Universals in Facial Expressions of Emotion, pages 143\u2013164", "author": ["P. Ekman"], "venue": null, "citeRegEx": "Ekman,? \\Q1989\\E", "shortCiteRegEx": "Ekman", "year": 1989}, {"title": "emotional tweets", "author": ["S. Mohammad."], "venue": "SEM 2012: The 1st Joint Conference on Lexical and Computational Semantics, pages 246\u2013255. Association for Computational Linguistics.", "citeRegEx": "Mohammad.,? 2012", "shortCiteRegEx": "Mohammad.", "year": 2012}, {"title": "Emotions in Social Psychology", "author": ["W.G. Parrott."], "venue": "Psychology Press, Philadelphia.", "citeRegEx": "Parrott.,? 2001", "shortCiteRegEx": "Parrott.", "year": 2001}, {"title": "A general psychoevolutionary theory of emotion, pages 3\u201333", "author": ["R. Plutchik"], "venue": null, "citeRegEx": "Plutchik,? \\Q1980\\E", "shortCiteRegEx": "Plutchik", "year": 1980}, {"title": "Emotionml - an upcoming standard for representing emotions and related states", "author": ["M. Schroder", "P. Baggia", "F. Burkhardt", "C. Pelachaud", "C. Peter", "E. Zovato."], "venue": "Proceedings of the 4th International Conference on Affective Computing and Intelligent", "citeRegEx": "Schroder et al\\.,? 2011", "shortCiteRegEx": "Schroder et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Senticorr: Multilingual sentiment analysis of personal correspondence", "author": ["E. Tromp", "M. Pechenizkiy."], "venue": "11th IEEE International Conference on Data Mining (demo paper), pages 1247\u20131250.", "citeRegEx": "Tromp and Pechenizkiy.,? 2011", "shortCiteRegEx": "Tromp and Pechenizkiy.", "year": 2011}, {"title": "RBEM: a rule based approach to polarity detection", "author": ["E. Tromp", "M. Pechenizkiy."], "venue": "Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, WISDOM 2013.", "citeRegEx": "Tromp and Pechenizkiy.,? 2013", "shortCiteRegEx": "Tromp and Pechenizkiy.", "year": 2013}, {"title": "Pattern-based emotion classification on social media", "author": ["E. Tromp", "M. Pechenizkiy."], "venue": "(to appear) book chapter in Advances in Social Media Analysis. Springer.", "citeRegEx": "Tromp and Pechenizkiy.,? 2015", "shortCiteRegEx": "Tromp and Pechenizkiy.", "year": 2015}, {"title": "Libshorttext: A library for short-text classification and analysis", "author": ["H. Yu", "C. Ho", "Y. Juan", "C. Lin"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "Though several researchers attempted at defining standards in this field (Parrott, 2001; Plutchik, 1980; Schroder et al., 2011), AAAC1, there is still no consensus on a basic set of emotions that is generally accepted and could be objectively verified.", "startOffset": 73, "endOffset": 127}, {"referenceID": 8, "context": "Though several researchers attempted at defining standards in this field (Parrott, 2001; Plutchik, 1980; Schroder et al., 2011), AAAC1, there is still no consensus on a basic set of emotions that is generally accepted and could be objectively verified.", "startOffset": 73, "endOffset": 127}, {"referenceID": 9, "context": "Though several researchers attempted at defining standards in this field (Parrott, 2001; Plutchik, 1980; Schroder et al., 2011), AAAC1, there is still no consensus on a basic set of emotions that is generally accepted and could be objectively verified.", "startOffset": 73, "endOffset": 127}, {"referenceID": 12, "context": "2 This algorithm is based on work by (Tromp and Pechenizkiy, 2013) where the authors introduced the Rule-Based Emission Model (RBEM) algorithm for polarity detection only.", "startOffset": 37, "endOffset": 66}, {"referenceID": 10, "context": "Few examples can be found where novel methods are introduced to capture more information than just polarity such as the work of (Socher et al., 2011) where a recursive auto-encoder is used to predict sentiment distributions in five dimensions.", "startOffset": 128, "endOffset": 149}, {"referenceID": 2, "context": "(Cambria and Hussain, 2012) and (Cambria et al.", "startOffset": 0, "endOffset": 27}, {"referenceID": 3, "context": "(Cambria and Hussain, 2012) and (Cambria et al., 2012) promote affective computing using a framework they call SenticNet.", "startOffset": 32, "endOffset": 54}, {"referenceID": 8, "context": "The sentiment dimensions of this framework are modeled in an hourglassmodel which is a derivative of Plutchik\u2019s wheel of emotions (Plutchik, 1980).", "startOffset": 130, "endOffset": 146}, {"referenceID": 13, "context": "We expect a revised and extended version of this manuscript describing RBEM-Emo to appear in (Tromp and Pechenizkiy, 2015) ar X iv :1 41 2.", "startOffset": 93, "endOffset": 122}, {"referenceID": 1, "context": "The closest work to our approach is (Andreevskaia and Bergler, 2007), in which the authors considered a rule-based approach based on a set of positive and negative patterns and valence shifters for handling negations and other linguistic constructs defining the sentiment of a sentence.", "startOffset": 36, "endOffset": 68}, {"referenceID": 7, "context": "Works of (Parrott, 2001; Plutchik, 1980; Schroder et al., 2011) do aim to define standards in this area by defining a minimal set of basic emotions from which more complex ones can be derived or constructed by combining basic emotions.", "startOffset": 9, "endOffset": 63}, {"referenceID": 8, "context": "Works of (Parrott, 2001; Plutchik, 1980; Schroder et al., 2011) do aim to define standards in this area by defining a minimal set of basic emotions from which more complex ones can be derived or constructed by combining basic emotions.", "startOffset": 9, "endOffset": 63}, {"referenceID": 9, "context": "Works of (Parrott, 2001; Plutchik, 1980; Schroder et al., 2011) do aim to define standards in this area by defining a minimal set of basic emotions from which more complex ones can be derived or constructed by combining basic emotions.", "startOffset": 9, "endOffset": 63}, {"referenceID": 3, "context": "In (Cambria et al., 2012) the authors develop methods to reason about emotions.", "startOffset": 3, "endOffset": 25}, {"referenceID": 5, "context": "In (Ekman, 1989), facial expressions are linked to emotions and a final six universal basic emotions are presented.", "startOffset": 3, "endOffset": 16}, {"referenceID": 8, "context": "We choose the wheel of emotions defined by Robert Plutchik (Plutchik, 1980) (see Figure 1) because it defines only eight basic emotions, which makes the problem manageable for envisioned applications and RBEMEmo a good match to perform classification according to this model of emotions.", "startOffset": 59, "endOffset": 75}, {"referenceID": 8, "context": "Additionally, Plutchik defines eight human feelings that are derivatives of combinations of two Figure 1: Plutchik\u2019s wheel of emotions (Plutchik, 1980)", "startOffset": 135, "endOffset": 151}, {"referenceID": 11, "context": "We considered integrating RBEM into a larger data analytics project (Tromp and Pechenizkiy, 2011) and into mobile settings with computing resources be a bottleneck (Chambers et al.", "startOffset": 68, "endOffset": 97}, {"referenceID": 4, "context": "We considered integrating RBEM into a larger data analytics project (Tromp and Pechenizkiy, 2011) and into mobile settings with computing resources be a bottleneck (Chambers et al., 2012).", "startOffset": 164, "endOffset": 187}, {"referenceID": 12, "context": "The original Rule-Based Emission Model (RBEM) algorithm (Tromp and Pechenizkiy, 2013) can be used for polarity detection assigning new messages a label that is one of positive, neutral, negative.", "startOffset": 56, "endOffset": 85}, {"referenceID": 12, "context": "We refer to the original paper on RBEM for their formal description (Tromp and Pechenizkiy, 2013).", "startOffset": 68, "endOffset": 97}, {"referenceID": 10, "context": "We compare our method against a majority class baseline, Support Vector Machines (SVMs), regression and the recursive auto-encoder of (Socher et al., 2011) and evaluate on accuracy.", "startOffset": 134, "endOffset": 155}, {"referenceID": 10, "context": "In (Socher et al., 2011) five-dimensional sentiment model originating from the Experience Project4 is introduced.", "startOffset": 3, "endOffset": 24}, {"referenceID": 0, "context": "Due to the impracticalities of the Experience Project dataset for our experiments, we instead benchmark on a different, well-accepted dataset introduced in (Alm, 2008).", "startOffset": 156, "endOffset": 167}, {"referenceID": 5, "context": "This dataset is annotated using Ekman\u2019s emotions (Ekman, 1989) instead of Plutchik\u2019s, but since the six basic emotions of Ekman are subsumed by the eight emotions of Plutchik\u2019s model, we can use the labels in a straightforward manner, ignoring labels produced by RBEM-Emo that do not exist in Ekman\u2019s model and producing the majority class as label in case we find a non-existing emotion.", "startOffset": 49, "endOffset": 62}, {"referenceID": 14, "context": "For the SVM and regression classification we use LibShortText (Yu et al., 2013).", "startOffset": 62, "endOffset": 79}, {"referenceID": 10, "context": "For the recursive auto-encoder, we use the Java version referenced to by the authors of (Socher et al., 2011)6.", "startOffset": 88, "endOffset": 109}, {"referenceID": 10, "context": "To ensure we have the right setup of the auto-encoder, we reproduced the polarity detection experiments on the rotten tomatoes dataset as done in (Socher et al., 2011) and obtained an accuracy of 77.", "startOffset": 146, "endOffset": 167}, {"referenceID": 10, "context": "This is in line with the results presented in (Socher et al., 2011), illustrating our setup is valid.", "startOffset": 46, "endOffset": 67}, {"referenceID": 0, "context": "The Affect Dataset we use is presented in (Alm, 2008) and is publicly available7.", "startOffset": 42, "endOffset": 53}, {"referenceID": 10, "context": "In line with the setup of the experiments presented in (Socher et al., 2011) and adhered to here, we randomly split the data into roughly 2 3 training and 1 3 test data.", "startOffset": 55, "endOffset": 76}], "year": 2014, "abstractText": "We study sentiment analysis beyond the typical granularity of polarity and instead use Plutchik\u2019s wheel of emotions model. We introduce RBEM-Emo as an extension to the Rule-Based Emission Model algorithm to deduce such emotions from human-written messages. We evaluate our approach on two different datasets and compare its performance with the current state-of-the-art techniques for emotion detection, including a recursive autoencoder. The results of the experimental study suggest that RBEM-Emo is a promising approach advancing the current state-of-the-art in emotion detection.", "creator": "TeX"}}}