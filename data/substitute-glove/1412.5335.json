{"id": "1412.5335", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews", "abstract": "Sentiment review does also referred working in development english manufacturing given achieving keep distinguish skew of a text document (kinds a consumer recommendations ). In both axiomatic settings, how discriminate than split impact it negative markets, everything the decisions again first different alternatively classification problem. We compare mainly ma - chine learn define to change lack, making combine them before strive came best possible event. We seeing how out that give that task before its generative wan - downshift models, there besides slightly complementary to when state both under library techniques. We achieve steady results on instead except - known dataset of IMDB theater publication. Our predict are never exogenously, as we printed also the refers needed allow repeat all effects. This done simplify expected advance is the governors of well art, as other analyzing therefore combine through techniques however cares though really effort.", "histories": [["v1", "Wed, 17 Dec 2014 11:02:04 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v1", null], ["v2", "Thu, 18 Dec 2014 14:17:16 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v2", null], ["v3", "Fri, 19 Dec 2014 11:36:14 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v3", null], ["v4", "Tue, 3 Feb 2015 20:03:35 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v4", null], ["v5", "Wed, 4 Feb 2015 05:17:55 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v5", null], ["v6", "Thu, 16 Apr 2015 14:26:14 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v6", null], ["v7", "Wed, 27 May 2015 06:40:09 GMT  (34kb)", "http://arxiv.org/abs/1412.5335v7", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG cs.NE", "authors": ["gr\\'egoire mesnil", "tomas mikolov", "marc'aurelio ranzato", "yoshua bengio"], "accepted": true, "id": "1412.5335"}, "pdf": {"name": "1412.5335.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n53 35\nv1 [\ncs .C\nL ]\n1 7"}, {"heading": "1 INTRODUCTION", "text": "Sentiment analysis is among the most popular, simple and useful tasks in natural language processing. It aims at predicting the attitude of text, typically a sentence or a review. For instance, movies or restaurant are often rated with a certain number of stars, which indicate the degree to which the reviewer was satisfied.\nThis task is often considered as one of the simplest in NLP because basic machine learning techniques can yield strong baselines (Wang & Manning, 2012), often beating much more intricate approaches (Socher et al., 2011). In the simplest settings, this task can be seen as a binary classification between positive and negative sentiment.\nHowever, there are several challenges towards achieving the best possible accuracy. It is not obvious how to represent variable length documents beyond simple bag of words approaches that lose word order information. One can use advanced machine learning techniques such as recurrent neural networks and their variations (Mikolov et al., 2010; Socher et al., 2011), however it is not clear if these provide any significant gain over simple bag-of-words and bag-of-ngram techniques (Pang & Lee, 2008; Wang & Manning, 2012).\nIn this work, we compared several different approaches and realized, without much surprise, that model combination performs better than any individual technique. The ensemble best benefits from models that are complementary, thus having diverse set of techniques is desirable. The vast majority of models proposed in the literature are discriminative in nature, as their parameters are tuned for the classification task directly. In this work, we boost the performance of the ensemble by considering a generative language model. To this end, we train two language models, one on the positive reviews and one on the negative ones, and use the likelihood ratio of these two models\nevaluated on the test data as an additional feature. For example, we assume that a positive review will have higher likelihood to be generated by a model that was trained on a large set of positive reviews, and lower likelihood given the negative model. In this paper, we constrained our work to binary classification where we trained two generative models, positive and negative. One could consider a higher number of classes since this approach scales linearily with the number of models to be train, i.e. one for each class. The large pool of diverse models is a) simple to implement (in line with previous work by Wang and Manning (Wang & Manning, 2012)) and b) it yields state of the art performance on one of the largest publicly available benchmarks of movie reviews, the Stanford IMDB dataset of reviews. Code to reproduce our experiments is available at https://github.com/mesnilgr/iclr15."}, {"heading": "2 DESCRIPTION OF THE MODELS", "text": "In this section we describe in detail the approaches we considered in our study. The novelty of this paper consists in combining both generative and discriminative models together for sentiment prediciton."}, {"heading": "2.1 GENERATIVE MODEL", "text": "A generative model defines a distribution over the input. By training a generative model for each class, we can then use Bayes rule to predict which class a test sample belongs to. More formally, given a dataset of pairs {x(i), y(i)}i=1,...,N where x(i) is the i-th document in the training set, y(i) \u2208 {\u22121,+1} is the corresponding label and N is the number of training samples, we train two models: p+(x|y = +1) for {x(i) subject to y(i) = +1} and p\u2212(x|y = \u22121) for {x subject to y = \u22121}. Then, given an input x at test time we compute the ratio (derived from Bayes rule): r = p+(x|y = +1)/p\u2212(x|y = \u22121) \u00d7 p(y = +1)/p(y = \u22121). If r > 1, then x is assigned to the positive class, otherwise to the negative class.\nWe have a few different choices of distribution we can choose from. The most common one is the n-gram, a count-based non-parametric method to compute p(x(i)k |x (i) k\u22121, x (i) k\u22122, . . . , x (i) k\u2212N+1), where x (i) k is the k-th word in the i-th document. In order to compute the likelihood of a document, we use the Markov assumption and simply multiply the n-gram probabilities over all words in the document: p(x(i)) = \u220fK k=1 p(x (i) k |x (i) k\u22121, x (i) k\u22122, . . . , x (i) k\u2212N+1). As mentioned before, we train one n-gram language model using the positive documents and one model using the negative ones.\nIn our experiments, we used SRILM toolkit (Stolcke et al., 2002) to train the n-gram language models using modified Kneser-Ney smoothing (Kneser & Ney, 1995). Furthermore, as both language models are trained on different datasets, there is a mismatch between vocabularies: some words can appear only in one of the training sets. This can be a problem during scoring, as the test data contain novel words that were not seen in at least one of the training datasets. To avoid this problem, it is needed to add penalty during scoring for each out of vocabulary word.\nN-grams are a very simple data-driven way to build language models. However, they suffer from both data sparsity and large memory requirement. Since the number of word combinations grows exponentially with the length of the context, there is always little data to accurately estimate probabilities for higher order n-grams.\nIn contrast with N-grams languages models, Recurrent neural networks (RNNs) (Mikolov et al., 2010) are parametric models that can address these issues. The inner architecture of the RNNs gives them potentially infinite context window, allowing them to perform smoother predictions. We know that in practice, the context window is limited due to exploding and vanishing gradients (Pascanu et al., 2012). Still, RNNs outperform significantly n-grams and are the state of the art for statistical language modeling. A review of these techniques is beyond the scope of this short paper and we point the reader to (Mikolov, 2012) for a more in depth discussion on this topic.\nBoth when using n-grams and RNNs, we compute the probability of the test document belonging to the positive and negative class via Bayes\u2019 rule. These scores are then averaged in the ensemble with other models, as explained in Section 2.4.\nInput features Accuracy Unigrams 88.74% Unigrams+Bigrams 91.32% Unigrams+Bigrams+Trigrams 91.59%"}, {"heading": "2.2 LINEAR CLASSIFICATION OF WEIGHTED N-GRAM FEATURES", "text": "Among purely discriminative methods, the most popular choice is a linear classifier on top of a bagof-word representation of the document. The input representation is usually a tf-idf weighted word counts of the document. In order to preserve local ordering of the words, a better representation would consider also the position-independent n-gram counts of the document (bag-of-n-grams).\nIn our ensemble, we used a supervised reweighing of the counts as in the Naive Bayes Support Vector Machine (NB-SVM) approach (Wang & Manning, 2012). This approach computes a log-ratio vector between the average word counts extracted from positive documents and the average word counts extracted from negative documents. The input to the logistic regression classifier corresponds to the log-ratio vector multiplied by the binary pattern for each word in the document vector. Note that the logictic regression can be replaced by a linear SVM. Our implementation1 slightly improved the performance reported in (Wang & Manning, 2012) by adding tri-grams (improvement of +0.3%), as shown in Table 1."}, {"heading": "2.3 SENTENCE VECTORS", "text": "Recently, (Le & Mikolov, 2014) proposed an unsupervised method to learn distributed representations of words and paragraphs. The key idea is to learn a compact representation of a word or paragraph by predicting nearby words in a fixed context window. This captures co-occurence statistics and it learns embeddings of words and paragraphs that capture rich semantics. Synonym words and similar paragraphs often are surrounded by similar context, and therefore, they will be mapped into nearby feature vectors (and vice versa).\nSuch embeddings can then be used to represent a new document (for instance, by averaging the representations of the paragraphs that constitute the document) via a fixed size feature vector. The authors then use such a document descriptor as input to a one hidden layer neural network for sentiment discrimination."}, {"heading": "2.4 MODEL ENSEMBLE", "text": "In this work, we combine the log probability scores of the above mentioned models via linear interpolation. More formally, we define the overall probability score as the weighted geometric mean of baseline models: p(y = +1|x) = \u220f pk(y = +1|x)\u03b1k , with \u03b1k > 0.\nWe find the best setting of weights via brute force grid search, quantizing the coefficient values in the interval [0, 1] at increments of 0.1. The search is evaluated on a validation set to avoid overfitting. We do not focus on a smarter way to find the \u03b1 since we consider only 3 models in our approach and we consider it out of the scope of this paper. Using more models would make the use of such method prohibitive. For a larger number of models, one might want to consider random search of the \u03b1 coefficients or even Bayesian approaches as these techniques will give better running time performance."}, {"heading": "3 RESULTS", "text": "In this section we report results on one of the largest publicly available sentiment analysis datasets, the IMDB dataset of movie reviews. The dataset consists of 50, 000 movie reviews which are categorized as being either positive or negative. We use 25, 000 reviews for training and the rest for\n1https://github.com/mesnilgr/nbsvm\nEnsemble Accuracy RNN-LM + NB SVM Trigram 92.11% RNN-LM + Sentence Vectors 91.68% Sentence Vectors + NB-SVM Trigrams 92.46% All 92.77% State of the art 92.58%\ntesting, using the same protocol proposed by (Maas et al., 2011). All experiments can be reproduced using the code available at https://github.com/mesnilgr/iclr15.\nTable 2 reports the results of each individual model. We have found that generative models performed the worst, with RNNs slightly better than n-grams. The most competitive methods are the method based on sentence vectors (Le & Mikolov, 2014) and the method based on reweighed bagof-words (Wang & Manning, 2012). In our experiments, we found both methods producing similar accuracy. In particular, we obtained only a marginal improvement (0.3% absolute) when using a one-hidden layer neural network as opposed to logistic regression for the sentence vectors2. Favoring simplicity and reproducibility of our performance, all results reported in this paper were produced by a linear classifier.\nFinally, Table 3 reports the results of combining the previous models into an ensemble. When we interpolate the scores of RNN, sentence vectors and NB-SVM, we achieve a new state-of-the-art performance of 92.77%, to be compared to 92.58% reported by (Le & Mikolov, 2014). Notice that our implementation of their method alone yielded only 90.6% (a difference of \u2243 2%). In order to measure the contribution of each model to the final ensemble classifier, we remove one model at a time from the ensemble. We observe that the removal of the generative model affects the least the ensemble performance. Overall, all three models contribute to the success of the overall ensemble, suggesting that these three models pick up complimentary features useful for discrimination. In Table 4, we show test reviews misclassified by single models but classified accurately by the ensemble."}, {"heading": "4 CONCLUSION", "text": "We have proposed a very simple yet powerful ensemble system for sentiment analysis. We combine three rather complementary and conceptually different baseline models: one based on a generative approach (language models), one based on continuous representations of sentences and one based on a clever reweighing of tf-idf bag-of-word representation of the document. Each such model contributes to the success of the overall system, achieving the new state of the art performance on the challenging IMDB movie review dataset. Code to reproduce our experiments is available at: https://github.com/mesnilgr/iclr15. We hope researchers will take advantage of our code to include their new results into our ensemble and focus on improving the state of the art for Sentiment Analysis.\n2We were not able to reproduce the accuracy reported by Le et al.\nModel Sentences (positive) a really realistic , sensible movie by ramgopal verma . no stupidity like NB-SVM songs as in other hindi movies . class acting by nana patekar . much similarities to real \u2019encounters\u2019 . (negative) leslie nielson is a very talented actor , who made a huge mistake by doing this film . it doesn\u2019t even come close to being funny . the best word to describe it is stupid ! (positive) this is a good film . this is very funny . yet after this film there RNN-LM were no good ernest films ! (negative) a real hoot , unintentionally . sidney portier\u2019s character is so sweet and lovable you want to smack him . nothing about this movie rings true . and it\u2019s boring to boot . (positive) this movie is based on the novel island of dr . moreau by Sentence Vector version by john frankenheimer . (negative) if it wasn\u2019t for the terrific music , i would not hesitate to give this cinematic underachievement 2/10 . but the music actually makes me like certain passages , and so i give it 5/10 ."}], "references": [{"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Reinhard", "Ney", "Hermann"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas", "Andrew L", "Daly", "Raymond E", "Pham", "Peter T", "Huang", "Dan", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Statistical language models based on neural networks", "author": ["Mikolov", "Tom\u00e1\u0161"], "venue": "PhD thesis,", "citeRegEx": "Mikolov and Tom\u00e1\u0161.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tom\u00e1\u0161.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "Foundations and trends in information retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher", "Richard", "Pennington", "Jeffrey", "Huang", "Eric", "Ng", "Andrew", "Manning", "Christopher D"], "venue": "Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Stolcke", "Andreas"], "venue": "In INTERSPEECH,", "citeRegEx": "Stolcke and Andreas,? \\Q2002\\E", "shortCiteRegEx": "Stolcke and Andreas", "year": 2002}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Wang", "Sida", "Manning", "Christopher D"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "This task is often considered as one of the simplest in NLP because basic machine learning techniques can yield strong baselines (Wang & Manning, 2012), often beating much more intricate approaches (Socher et al., 2011).", "startOffset": 198, "endOffset": 219}, {"referenceID": 4, "context": "One can use advanced machine learning techniques such as recurrent neural networks and their variations (Mikolov et al., 2010; Socher et al., 2011), however it is not clear if these provide any significant gain over simple bag-of-words and bag-of-ngram techniques (Pang & Lee, 2008; Wang & Manning, 2012).", "startOffset": 104, "endOffset": 147}, {"referenceID": 7, "context": "One can use advanced machine learning techniques such as recurrent neural networks and their variations (Mikolov et al., 2010; Socher et al., 2011), however it is not clear if these provide any significant gain over simple bag-of-words and bag-of-ngram techniques (Pang & Lee, 2008; Wang & Manning, 2012).", "startOffset": 104, "endOffset": 147}, {"referenceID": 4, "context": "In contrast with N-grams languages models, Recurrent neural networks (RNNs) (Mikolov et al., 2010) are parametric models that can address these issues.", "startOffset": 76, "endOffset": 98}, {"referenceID": 6, "context": "We know that in practice, the context window is limited due to exploding and vanishing gradients (Pascanu et al., 2012).", "startOffset": 97, "endOffset": 119}, {"referenceID": 2, "context": "testing, using the same protocol proposed by (Maas et al., 2011).", "startOffset": 45, "endOffset": 64}], "year": 2016, "abstractText": "Sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document (typically a consumer review). In the simplest settings, we discriminate only between positive and negative sentiment, turning the task into a standard binary classification problem. We compare several machine learning approaches to this problem, and combine them to achieve the best possible results. We show how to use for this task the standard generative language models, which are slightly complementary to the state of the art techniques. We achieve strong results on a well-known dataset of IMDB movie reviews. Our results are easily reproducible, as we publish also the code needed to repeat the experiments. This should simplify further advance of the state of the art, as other researchers can combine their techniques with ours with little effort.", "creator": "LaTeX with hyperref package"}}}