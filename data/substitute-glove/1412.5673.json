{"id": "1412.5673", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Entity-Augmented Distributional Semantics for Discourse Relations", "abstract": "Discourse role cell seen perspectives e.g. one coherent arabic. However, manually multiple discourse discussion it how, make yet requires differences the semantics of the reportedly arbitrary. A actually subtle leads is whether they latter not enough if numbers now merely given each incarceration of a sociological relation, longer the equivalence due depend days based middle lower - level reference, such as entity written. Our complicated formula_4 correlations object representations by distinguished up although clastic template bamboo. A promising sense soon made work on textural spatiotemporal semantics \u201d that need also \u0192 formulae taking constituted attested, using a novel revising intonation pass. Discourse relations are predicted not any from although distributional symbolic following later guilty, but also of. coreferent entity word. The resulting system obtains immediate cost days the marked state - country - in - art held outcome clarity rational relations in under Penn Discourse Treebank.", "histories": [["v1", "Wed, 17 Dec 2014 23:26:48 GMT  (42kb)", "https://arxiv.org/abs/1412.5673v1", "Three pages plus one page reference. Submit to the ICLR2015 workshop track for review"], ["v2", "Wed, 15 Apr 2015 23:17:48 GMT  (46kb)", "http://arxiv.org/abs/1412.5673v2", "Accepted as a workshop contribution at ICLR 2015"], ["v3", "Tue, 28 Apr 2015 14:14:44 GMT  (42kb)", "http://arxiv.org/abs/1412.5673v3", "Accepted as a workshop contribution at ICLR 2015"]], "COMMENTS": "Three pages plus one page reference. Submit to the ICLR2015 workshop track for review", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yangfeng ji", "jacob eisenstein"], "accepted": true, "id": "1412.5673"}, "pdf": {"name": "1412.5673.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jiyfeng@gatech.edu", "jacobe@gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n56 73\nv3 [\ncs .C\nL ]\n2 8\nA pr\n2 01"}, {"heading": "1 INTRODUCTION", "text": "The high-level organization of text can be characterized in terms of discourse relations between adjacent spans of text (Knott, 1996; Mann, 1984; Webber et al., 1999). Identifying these relations has been shown to be relevant to tasks such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al., 2011). While the Penn Discourse Treebank (PDTB) now provides a large dataset annotated for discourse relations (Prasad et al., 2008), the automatic identification of implicit discourse relations is a difficult task, with state-of-the-art performance at roughly 40% (Lin et al., 2009).\nOne reason for this poor performance is that predicting implicit discourse relations is a fundamentally semantic task, and the relevant semantics may be difficult to recover from surface level features. For example, consider the discourse relation between the following two sentences in Example (1), where a discourse connector like \u201cbecause\u201d seems appropriate to indicate the relationship. However, without discourse connector, there is little surface information to signal the relationship. We address this issue by applying a discriminatively-trained model of compositional distributional semantics to discourse relation classification (Socher et al., 2013; Baroni et al., 2014). The meaning of each sentence is represented as a vector (Turney et al., 2010), which is computed through a series of compositional operations over the parse tree.\nExample (1) : Bob gave Tina the burger. Example (2) : Bob gave Tina the burger. She was hungry. He was hungry.\nWe further argue that purely vector-based representations on sentences are insufficiently expressive to capture discourse relations. To see why, consider what happens in Example (2), where a tiny change is made based on Example (1). After changing the subject of the second sentence to Bob, the original discourse relation seems no longer holding in Example (2). But despite the radical difference in meaning, the distributional representation of the second sentence will be almost unchanged: the syntactic structure remains identical, and the words \u201che\u201d and \u201cshe\u201d have very similar word representations. We address this issue by computing vector representations not only for each sentence, but also for each coreferent entity mention within the sentences. These representations are\nmeant to capture the role played by the entity in the text. We compute entity-role representations using a novel feed-forward compositional model, which combines upward and downward passes through the syntactic structure. Representations for these coreferent mentions are then combined into a classification model, and help to predict the implicit discourse relation. In combination, our approach achieves a 3% improvement in accuracy over the best previous work (Lin et al., 2009) on the second-level discourse relation identification in the PDTB.1.\nOur model requires a syntactic parse tree, which is produced automatically from the Stanford CoreNLP parser Klein & Manning (2003). A reviewer asked whether it might be better to employ a left-to-right recurrent neural network, which would obviate the need for this language-specific resource. While it would clearly be preferable to avoid the use of language-specific resources whenever possible, we think this approach is unlikely to succeed in this case. A key difference between language and other types of data is that language has inherent recursive structure. A rich literature in both linguistics and natural language processing elaborates on the close relationship between (recursively-structured) syntax and semantics. Therefore, we see strong theoretical evidence \u2014 as well as practical evidence from the history of natural language processing \u2014 that syntactic parse structures are central to capturing the meaning in text.\nRegarding the multilingual question, there are now accurate parsers and annotated treebanks for dozens of languages,2 and training accurate parsers for \u201clow resource\u201d languages is a hot research topic, with substantial interest from both industry and academia. Languages differ substantially in the importance of word ordering, with English emphasizing word order more than most other languages (Bender, 2013). To our knowledge, it is an open question as to whether left-to-right recurrent neural networks will successfully extract meaning in languages where word order is more free."}, {"heading": "2 ENTITY AUGMENTED DISTRIBUTIONAL SEMANTICS FOR RELATION IDENTIFICATION", "text": "We briefly describe our approach to entity-augmented distributional semantics and to discourse relation identification. Our relation identification model is named as DISCO2, since it is a distributional compositional approach to discourse relations."}, {"heading": "2.1 ENTITY AUGMENTED DISTRIBUTIONAL SEMANTICS", "text": "The entity-augmented distributional semantics includes two passes in composition procedure: the upward pass for distributional representation of sentence, while the downward pass for distributional representation of entities shared between sentences.\nUpward pass Distributional representations for sentences are computed in a feed-forward upward pass: each non-terminal in the binarized syntactic parse tree has a K-dimensional distributional representation that is computed from the distributional representations of its children, bottoming out in representations of individual words. We follow the Recursive Neural Network (RNN) model proposed by Socher et al. (2011). Specifically, for a given parent node i, we denote the left child as \u2113(i), and the right child as r(i). We compose their representations to obtain, ui = tanh ( U[u\u2113(i);ur(i)] )\n, where tanh (\u00b7) is the element-wise hyperbolic tangent function, and U \u2208 RK\u00d72K is the upward composition matrix. We apply this compositional procedure from the bottom up, ultimately obtaining the sentence-level representation u0.\nDownward pass As seen in the contrast between Examples (1) and (2), a model that uses a single vector representation for each sentence would find little to distinguish between \u201cshe was hungry\u201d and \u201che was hungry\u201d. It would therefore almost certainly fail to identify the correct discourse relation for at least one of these cases, which requires tracking the roles played by the entities that are coreferent in each pair of sentences. To address this issue, we augment the representation of each sentence with additional vectors, representing the semantics of the role played by each coreferent entity in each\n1For more details, please refer to the long version of this paper (Ji & Eisenstein, 2015) 2http://universaldependencies.github.io/docs/#language-other\nsentence. Rather than represent this information in a logical form \u2014 which would require robust parsing to a logical representation \u2014 we represent it through additional distributional vectors. The role of a constituent i can be viewed as a combination of information from two neighboring nodes in the parse tree: its parent \u03c1(i), and its sibling s(i). We can make a downward pass, computing the downward vector di from the downward vector of the parent d\u03c1(i), and the upward vector of the sibling us(i): di = tanh ( V[d\u03c1(i);us(i)] )\n, where V \u2208 RK\u00d72K is the downward composition matrix. The base case of this recursive procedure occurs at the root of the parse tree, which is set equal to the upward representation, d0 , u0."}, {"heading": "2.2 RELATION IDENTIFICATION MODEL", "text": "To predict the discourse relation between an sentence pair (m,n), the decision function is a sum of bilinear products,\n\u03c8(y) = (u (m) 0 ) \u22a4 Ayu (n) 0 +\n\u2211\ni,j\u2208A(m,n)\n(d (m) i ) \u22a4 Byd (n) j + \u03b2 \u22a4\ny \u03c6(m,n) + by, (1)\nwhere the predicted relation is given by y\u0302 = argmaxy\u2208Y \u03c8(y), and Ay,By \u2208 RK\u00d7K are the classification parameters for relation y. A scalar by is used as the bias term for relation y, and A(m,n) is the set of coreferent entity mentions shared among the sentence pair (m,n). For the cases where there are no coreferent entity mentions between two sentences, A(m,n) = \u2205, the classification model considers only the upward vectors at the root. We also use the surface features vector \u03c6(m,n) in the decision function, as we find that, this approach outperforms prior work on the classification of implicit discourse relations in the PDTB, when combined with a small number of surface features."}, {"heading": "3 EXPERIMENTS", "text": "We evaluate our approach on the implicit discourse relation identification in the Penn Discourse Treebank (PDTB). PDTB relations may be explicit, meaning that they are signaled by discourse connectives (e.g., because); alternatively, they may be implicit, meaning that the connective is absent. We focus on the more challenging problem of classifying implicit discourse relations. Aiming to build a discourse parser in future, we follow the same experimental setting proposed by Lin et al. (2009), and evaluate our relation identification model on the second-level relation types.\nWe run the Stanford parser (Klein & Manning, 2003) and the Berkeley coreference system (Durrett & Klein, 2013) to obtain syntactic trees and coreference results respectively. In the PDTB, each discourse relation is annotated between two argument spans. For non-sentence argument span, we identify the syntactic subtrees with the span, and construct a right-branching superstructure to unify them into a tree.\nTable 1 presents results for multiclass identification of second-level PDTB relations. As shown in lines 5 and 6, DISCO2 outperforms the prior state-of-the-art (line 1). The strongest performance is\nobtained by including the entity distributional semantics, with a 3.4% improvement over the accuracy reported by Lin et al. (2009) (p < .05). The improvement over our reimplementation of this work (line 2) is even greater, which shows how the distributional representation provides additional value over the surface features. The contribution of entity semantics is shown in Table 1 by the accuracy differences between lines 3 and 4, and between lines 5 and 6."}, {"heading": "4 CONCLUSION", "text": "Discourse relations are determined by the meaning of their arguments, and progress on discourse parsing therefore requires computing representations of the argument semantics. We present a compositional method for inducing distributed representations not only of discourse arguments, but also of the entities that thread through the discourse. By jointly learning the relation classification weights and the compositional operators, this approach outperforms prior work based on hand-engineered surface features. More discussion and experimental results can be found in a forthcoming journal paper (Ji & Eisenstein, 2015)."}], "references": [{"title": "Frege in space: A program for compositional distributional semantics", "author": ["Baroni", "Marco", "Bernardi", "Raffaella", "Zamparelli", "Roberto"], "venue": "Linguistic Issues in Language Technologies,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax, volume 6 of Synthesis Lectures on Human Language Technologies", "author": ["Bender", "Emily M"], "venue": "doi: 10.2200/s00493ed1v01y201303hlt020. URL http://dx.doi.org/10.2200/s00493ed1v01y201303hlt020", "citeRegEx": "Bender and M.,? \\Q2013\\E", "shortCiteRegEx": "Bender and M.", "year": 2013}, {"title": "Easy Victories and Uphill Battles in Coreference Resolution", "author": ["Durrett", "Greg", "Klein", "Dan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "One vector is not enough: Entity-augmented distributional semantics for discourse relations. Conditionally accepted to Transactions of the Association for Computational Linguistics (TACL)", "author": ["Ji", "Yangfeng", "Eisenstein", "Jacob"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Dan", "Manning", "Christopher D"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "A data-driven methodology for motivating a set of coherence relations", "author": ["Knott", "Alistair"], "venue": "PhD thesis, The University of Edinburgh,", "citeRegEx": "Knott and Alistair.,? \\Q1996\\E", "shortCiteRegEx": "Knott and Alistair.", "year": 1996}, {"title": "Recognizing implicit discourse relations in the Penn Discourse Treebank", "author": ["Lin", "Ziheng", "Kan", "Min-Yen", "Ng", "Hwee Tou"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Automatically Evaluating Text Coherence Using Discourse Relations", "author": ["Lin", "Ziheng", "Ng", "Hwee Tou", "Kan", "Min-Yen"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Discourse indicators for content selection in summarization", "author": ["Louis", "Annie", "Joshi", "Aravind", "Nenkova", "Ani"], "venue": "In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Louis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Louis et al\\.", "year": 2010}, {"title": "Discourse structures for text generation", "author": ["Mann", "William"], "venue": "Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Mann and William.,? \\Q1984\\E", "shortCiteRegEx": "Mann and William.", "year": 1984}, {"title": "The Penn Discourse Treebank 2.0", "author": ["Prasad", "Rashmi", "Dinesh", "Nikhil", "Lee", "Alan", "Miltsakaki", "Eleni", "Robaldo", "Livio", "Joshi", "Aravind", "Webber", "Bonnie"], "venue": "In LREC,", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Manning", "Chris", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification", "author": ["Somasundaran", "Swapna", "Namata", "Galileo", "Wiebe", "Janyce", "Getoor", "Lise"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Somasundaran et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2009}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Peter D", "Pantel", "Patrick"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Discourse relations: A structural and presuppositional account using lexicalised tag", "author": ["Webber", "Bonnie", "Knott", "Alistair", "Stone", "Matthew", "Joshi", "Aravind"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL), pp", "citeRegEx": "Webber et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Webber et al\\.", "year": 1999}, {"title": "Dependency-based Discourse Parser for Single-Document Summarization", "author": ["Yoshida", "Yasuhisa", "Suzuki", "Jun", "Hirao", "Tsutomu", "Nagata", "Masaaki"], "venue": "In EMNLP,", "citeRegEx": "Yoshida et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yoshida et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "The high-level organization of text can be characterized in terms of discourse relations between adjacent spans of text (Knott, 1996; Mann, 1984; Webber et al., 1999).", "startOffset": 120, "endOffset": 166}, {"referenceID": 8, "context": "Identifying these relations has been shown to be relevant to tasks such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al.", "startOffset": 89, "endOffset": 131}, {"referenceID": 16, "context": "Identifying these relations has been shown to be relevant to tasks such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al.", "startOffset": 89, "endOffset": 131}, {"referenceID": 13, "context": ", 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al.", "startOffset": 28, "endOffset": 55}, {"referenceID": 7, "context": ", 2009), and coherence evaluation (Lin et al., 2011).", "startOffset": 34, "endOffset": 52}, {"referenceID": 10, "context": "While the Penn Discourse Treebank (PDTB) now provides a large dataset annotated for discourse relations (Prasad et al., 2008), the automatic identification of implicit discourse relations is a difficult task, with state-of-the-art performance at roughly 40% (Lin et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 6, "context": ", 2008), the automatic identification of implicit discourse relations is a difficult task, with state-of-the-art performance at roughly 40% (Lin et al., 2009).", "startOffset": 140, "endOffset": 158}, {"referenceID": 12, "context": "We address this issue by applying a discriminatively-trained model of compositional distributional semantics to discourse relation classification (Socher et al., 2013; Baroni et al., 2014).", "startOffset": 146, "endOffset": 188}, {"referenceID": 0, "context": "We address this issue by applying a discriminatively-trained model of compositional distributional semantics to discourse relation classification (Socher et al., 2013; Baroni et al., 2014).", "startOffset": 146, "endOffset": 188}, {"referenceID": 14, "context": "The meaning of each sentence is represented as a vector (Turney et al., 2010), which is computed through a series of compositional operations over the parse tree.", "startOffset": 56, "endOffset": 77}, {"referenceID": 6, "context": "In combination, our approach achieves a 3% improvement in accuracy over the best previous work (Lin et al., 2009) on the second-level discourse relation identification in the PDTB.", "startOffset": 95, "endOffset": 113}, {"referenceID": 6, "context": "In combination, our approach achieves a 3% improvement in accuracy over the best previous work (Lin et al., 2009) on the second-level discourse relation identification in the PDTB.1. Our model requires a syntactic parse tree, which is produced automatically from the Stanford CoreNLP parser Klein & Manning (2003). A reviewer asked whether it might be better to employ a left-to-right recurrent neural network, which would obviate the need for this language-specific resource.", "startOffset": 96, "endOffset": 314}, {"referenceID": 11, "context": "We follow the Recursive Neural Network (RNN) model proposed by Socher et al. (2011). Specifically, for a given parent node i, we denote the left child as l(i), and the right child as r(i).", "startOffset": 63, "endOffset": 84}, {"referenceID": 6, "context": "Lin et al. (2009) Yes 40.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Lin et al. (2009) Yes 40.2 Our work 2. Surface feature model Yes 39.69 3. DISCO2 No No 50 36.98 4. DISCO2 Yes No 50 37.63 5. DISCO2 No Yes 50 42.53 6. DISCO2 Yes Yes 50 43.56 \u2217 signficantly better than Lin et al. (2009) with p < 0.", "startOffset": 0, "endOffset": 220}, {"referenceID": 6, "context": "The results of Lin et al. (2009) are shown in line 1; the results for our reimplementation of this system are shown in line 2.", "startOffset": 15, "endOffset": 33}, {"referenceID": 6, "context": "Aiming to build a discourse parser in future, we follow the same experimental setting proposed by Lin et al. (2009), and evaluate our relation identification model on the second-level relation types.", "startOffset": 98, "endOffset": 116}, {"referenceID": 6, "context": "4% improvement over the accuracy reported by Lin et al. (2009) (p < .", "startOffset": 45, "endOffset": 63}], "year": 2015, "abstractText": "Discourse relations bind smaller linguistic elements into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank.", "creator": "LaTeX with hyperref package"}}}