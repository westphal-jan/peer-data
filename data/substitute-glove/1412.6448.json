{"id": "1412.6448", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Embedding Word Similarity with Neural Machine Translation", "abstract": "Neural language vehicles able sometimes representations, one embeddings, done capture rich linguistic and methodological checking. Here something investigate taken embeddings 've immediately atrophy tractor translation designs, each recently - work class of neural english components. We day that embeddings 15 translation changing uob because them by monolingual models at tasks because applications knowledge of has underpinnings particular. lexical - definitions powers. We already presenting that often generated could that translating only both English only French such English to German, without think that another sensible comparable of \u201d sortes should looking otherwise independently beyond the source much taken various. Finally, we apply a but method addition entered dendritic refers versions has though normally lexicons, and reality neither this idioms effect probability results in minimal scarcity example embedding quality. Our topological dimensional mean be erroneously years an amazon.com demo and downloaded from our web page. Overall, they analyses indicate that translates - based embeddings bring already can under applications once changes concepts not fact organised indicated to relation and / each typology exact, while plautdietsch role-play be things varied once bookkeeping (swellings) inter - example relatedness.", "histories": [["v1", "Fri, 19 Dec 2014 17:22:03 GMT  (184kb,D)", "https://arxiv.org/abs/1412.6448v1", "arXiv admin note: text overlap witharXiv:1410.0718"], ["v2", "Fri, 27 Feb 2015 16:55:46 GMT  (56kb,D)", "http://arxiv.org/abs/1412.6448v2", "arXiv admin note: text overlap witharXiv:1410.0718"], ["v3", "Tue, 31 Mar 2015 18:30:44 GMT  (51kb,D)", "http://arxiv.org/abs/1412.6448v3", "arXiv admin note: text overlap witharXiv:1410.0718"], ["v4", "Fri, 3 Apr 2015 18:11:54 GMT  (210kb,D)", "http://arxiv.org/abs/1412.6448v4", "arXiv admin note: text overlap witharXiv:1410.0718"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1410.0718", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix hill", "kyunghyun cho", "sebastien jean", "coline devin", "yoshua bengio"], "accepted": true, "id": "1412.6448"}, "pdf": {"name": "1412.6448.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Felix Hill", "KyungHyun Cho"], "emails": ["felix.hill@cl.cam.ac.uk"], "sections": [{"heading": null, "text": "Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness."}, {"heading": "1 INTRODUCTION", "text": "It is well known that word representations can be learned from the distributional patterns in corpora. Originally, such representations were constructed by counting word co-occurrences, so that the features in one word\u2019s representation corresponded to other words (Landauer & Dumais, 1997; Turney & Pantel, 2010). Neural language models, an alternative method for learning word representations, use language data to optimise (latent) features with respect to a language modelling objective. The objective can be to predict either the next word given the initial words of a sentence (Bengio et al., 2003; Mnih & Hinton, 2009; Collobert & Weston, 2008), or simply a nearby word given a single cue word (Mikolov et al., 2013b; Pennington et al., 2014). The representations learned by neural models (sometimes called embeddings) perform very effectively when applied as pre-trained features in a range of NLP applications and tasks (Baroni et al., 2014).\nar X\niv :1\n41 2.\n64 48\nv4 [\ncs .C\nDespite these clear results, it is not well understood how the architecture of neural models affects the information encoded in their embeddings. Here we contribute to this understanding by considering the embeddings learned by architectures with a very different objective function: neural machine translation (NMT) models. NMT models have recently emerged as an alternative to statistical, phrase-based translation models, and are beginning to achieve impressive translation performance (Kalchbrenner & Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014).\nWe show that NMT models are not only a potential new direction for machine translation, but are also an effective means of learning word embeddings. Specifically, translation-based embeddings encode information relating to conceptual similarity (rather than non-specific relatedness or association) and lexical syntactic role more effectively than embeddings from monolingual neural language models. We demonstrate that these properties persist when translating between different language pairs (English-French and English-German). Further, based on the observation of subtle languagespecific effects in the embedding spaces, we conjecture as to why similarity dominates over other semantic relations in translation embedding spaces. Finally, we discuss a potential limitation of the application of NMT models for embedding learning - the computational cost of training large vocabularies of embeddings - and show that a novel method for overcoming this issue preserves the aforementioned properties of translation-based embeddings."}, {"heading": "2 LEARNING EMBEDDINGS WITH NEURAL LANGUAGE MODELS", "text": "All neural language models, including NMT models, learn real-valued embeddings (of specified dimension) for words in some pre-specified vocabulary, V , covering many or all words in their training corpus. At each training step, a \u2018score\u2019 for the current training example (or batch) is computed based on the embeddings in their current state. This score is compared to the model\u2019s objective function, and the error is backpropagated to update both the model weights (affecting how the score is computed from the embeddings) and the embedding features themselves. At the end of this process, the embeddings should encode information that enables the model to optimally satisfy its objective."}, {"heading": "2.1 MONOLINGUAL MODELS", "text": "In the original neural language model (Bengio et al., 2003) and subsequent variants (Collobert & Weston, 2008), training examples consist of an ordered sequence of n words, with the model trained to predict the n-th word given the first n\u22121 words. The model first represents the input as an ordered sequence of embeddings, which it transforms into a single fixed length \u2018hidden\u2019 representation, generally by concatenation and non-linear projection. Based on this representation, a probability distribution is computed over the vocabulary, from which the model can sample a guess for the next word. The model weights and embeddings are updated to maximise the probability of correct guesses for all sentences in the training corpus.\nMore recent work has shown that high quality word embeddings can be learned via simpler models with no nonlinear hidden layer (Mikolov et al., 2013b; Pennington et al., 2014). Given a single word or unordered window of words in the corpus, these models predict which words will occur nearby. For each word w in V , a list of training cases (w, c) : c \u2208 V is extracted from the training corpus according to some algorithm. For instance, in the skipgram approach (Mikolov et al., 2013b), for each \u2018cue word\u2019 w the \u2018context words\u2019 c are sampled from windows either side of tokens of w in the corpus (with c more likely to be sampled if it occurs closer to w).1 For each w in V , the model initialises both a cue-embedding, representing the w when it occurs as a cue-word, and a context-embedding, used when w occurs as a context-word. For a cue word w, the model uses the corresponding cue-embedding and all context-embeddings to compute a probability distribution over V that reflects the probability of a word occurring in the context of w. When a training example (w, c) is observed, the model updates both the cue-word embedding of w and the context-word embeddings in order to increase the conditional probability of c.\n1 Subsequent variants use different algorithms for selecting the (w, c) from the training corpus (Hill & Korhonen, 2014; Levy & Goldberg, 2014)"}, {"heading": "2.2 BILINGUAL REPRESENTATION-LEARNING MODELS", "text": "Various studies have demonstrated that word representations can also be effectively learned from bilingual corpora, aligned at the document, paragraph or word level (Haghighi et al., 2008; Vulic\u0301 et al., 2011; Mikolov et al., 2013a; Hermann & Blunsom, 2014; Chandar et al., 2014). These approaches aim to represent the words from two (or more) languages in a common vector space so that words in one language are close to words with similar or related meanings in the other. The resulting multilingual embedding spaces have been effectively applied to bilingual lexicon extraction (Haghighi et al., 2008; Vulic\u0301 et al., 2011; Mikolov et al., 2013a) and document classification (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Koc\u030cisky\u0301 et al., 2014).\nWe focus our analysis on two representatives of this class of (non-NMT) bilingual model. The first is that of Hermann & Blunsom (2014), whose embeddings improve on the performance of A. Klementiev & Bhattarai (2012) in document classification applications. As with the NMT models introduced in the next section, this model can be trained directly on bitexts aligned only at the sentence rather than word level. When training, for aligned sentences SE and SF in different languages, the model computes representations RE and RF by summing the embeddings of the words in SE and SF respectively. The embeddings are then updated to minimise the divergence betweenRE andRF (since they convey a common meaning). A noise-contrastive loss function ensures that the model does not arrive at trivial (e.g. all zero) solutions to this objective. Hermann & Blunsom (2014) show that, despite the lack of prespecified word alignments, words in the two languages with similar meanings converge in the bilingual embedding space.2\nThe second model we examine is that of Faruqui & Dyer (2014). Unlike the models described above, Faruqui & Dyer (2014) showed explicitly that projecting word embeddings from two languages (learned independently) into a common vector space can favourably influence the orientation of word embeddings when considered in their monolingual subspace; i.e relative to other words in their own language. In contrast to the other models considered in this paper, the approach of Faruqui & Dyer (2014) requires bilingual data to be aligned at the word level."}, {"heading": "2.3 NEURAL MACHINE TRANSLATION MODELS", "text": "The objective of NMT is to generate an appropriate sentence in a target language St given a sentence Ss in the source language (see, e.g., Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014). As a by-product of learning to meet this objective, NMT models learn distinct sets of embeddings for the vocabularies Vs and Vt in the source and target languages respectively.\nObserving a training case (Ss, St), these models represent Ss as an ordered sequence of embeddings of words from Vs. The sequence for Ss is then encoded into a single representation RS .3 Finally, by referencing the embeddings in Vt, RS and a representation of what has been generated thus far, the model decodes a sentence in the target language word by word. If at any stage the decoded word does not match the corresponding word in the training target St, the error is recorded. The weights and embeddings in the model, which together parameterise the encoding and decoding process, are updated based on the accumulated error once the sentence decoding is complete.\nAlthough NMT models can differ in their low-level architecture (Kalchbrenner & Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), the translation objective exerts similar pressure on the embeddings in all cases. The source language embeddings must be such that the model can combine them to form single representations for ordered sequences of multiple words (which in turn must enable the decoding process). The target language embeddings must facilitate the process of decoding these representations into correct target-language sentences.\n2The models of Chandar et al. (2014) and Hermann & Blunsom (2014) both aim to minimise the divergence between source and target language sentences represented as sums of word embeddings. Because of these similarities, we do not compare with both in this paper.\n3Alternatively, subsequences (phrases) of Ss may be encoded at this stage in place of the whole sentence (Bahdanau et al., 2014)."}, {"heading": "3 EXPERIMENTS", "text": "To learn translation-based embeddings, we trained two different NMT models. The first is the RNN encoder-decoder (RNNenc, Cho et al., 2014), which uses a recurrent-neural-network to encode all of the source sentence into a single vector on which the decoding process is conditioned. The second is the RNN Search architecture (Bahdanau et al., 2014), which was designed to overcome limitations exhibited by the RNN encoder-decoder when translating very long sentences. RNN Search includes a attention mechanism, an additional feed-forward network that learns to attend to different parts of the source sentence when decoding each word in the target sentence.4 Both models were trained on a 348m word corpus of English-French sentence pairs or a 91m word corpus of English-German sentence pairs.5\nTo explore the properties of bilingual embeddings learned via objectives other than direct translation, we trained the BiCVM model of Hermann & Blunsom (2014) on the same data, and also downloaded the projected embeddings of Faruqui & Dyer (2014), FD, trained on a bilingual corpus of comparable size (\u2248 300 million words per language).6 Finally, for an initial comparison with monolingual models, we trained a conventional skipgram model (Mikolov et al., 2013b) and its Glove variant (Pennington et al., 2014) for the same number of epochs on the English half of the bilingual corpus.\nTo analyse the effect on embedding quality of increasing the quantity of training data, we then trained the monolingual models on increasingly large random subsamples of Wikipedia text (up to a total of 1.1bn words). Lastly, we extracted embeddings from a full-sentence language model (CW, Collobert & Weston, 2008), which was trained for several months on the same Wikipedia 1bn word corpus. Note that increasing the volume of training data for the bilingual (and NMT) models was not possible because of the limited size of available sentence-aligned bitexts."}, {"heading": "3.1 SIMILARITY AND RELATEDNESS MODELLING", "text": "As in previous studies (Agirre et al., 2009; Bruni et al., 2014; Baroni et al., 2014), our initial evaluations involved calculating pairwise (cosine) distances between embeddings and correlating these distances with (gold-standard) human judgements of the strength of relationships between concepts. For this we used three different gold standards: WordSim-353 (Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2014). Importantly, there is a clear distinction between WordSim-353 and MEN, on the one hand, and SimLex-999, on the other, in terms of the semantic relationship that they quantify. For both WordSim-353 and MEN, annotators were asked to rate how related or associated two concepts are. Consequently, pairs such as [clothes-closet], which are clearly related but ontologically dissimilar, have high ratings in WordSim-353 and MEN. In contrast, such pairs receive a low rating in SimLex-999, where only genuinely similar concepts, such as [coast- shore], receive high ratings.\nTo reproduce the scores in SimLex-999, models must thus distinguish pairs that are similar from those that are merely related. In particular, this requires models to develop sensitivity to the distinction between synonyms (similar) and antonyms (often strongly related, but highly dissimilar).7\nTable 1 shows the correlations of NMT (English-French) embeddings, other bilingually-trained embeddings and monolingual embeddings with these three lexical gold-standards. NMT outperform monolingual embeddings, and, to a lesser extent, the other bilingually trained embeddings, on SimLex-999. However, this clear advantage is not observed on MEN and WordSim-353, where the projected embeddings of Faruqui & Dyer (2014), which were tuned for high performance on WordSim-353, perform best. Given the aforementioned differences between the evaluations, this\n4Access to source code and limited GPU time prevent us from training and evaluating the embeddings from other NMT models such as that of (Kalchbrenner & Blunsom, 2013), (Devlin et al., 2014) and Sutskever et al. (2014). The underlying principles of encoding-decoding also apply to these models, and we expect the embeddings would exhibit similar properties to those analysed here.\n5These corpora were produced from the WMT \u201914 parallel data after conducting the data-selection procedure described by Cho et al. (2014).\n6Available from http://www.cs.cmu.edu/\u02dcmfaruqui/soft.html. The available embeddings were trained on English-German aligned data, but the authors report similar to for English-French.\n7For a more detailed discussion of the similarity/relatedness distinction, see (Hill et al., 2014).\nsuggests that bilingually-trained embeddings, and NMT based embeddings in particular, better capture similarity, whereas monolingual embedding spaces are orientated more towards relatedness.\nTo test this hypothesis further, we ran three more evaluations designed to probe the sensitivity of models to similarity as distinct from relatedness or association. In the first, we measured performance on SimLex-Assoc-333 (Hill et al., 2014). This evaluation comprises the 333 most related pairs in SimLex-999, according to an independent empirical measure of relatedness (free associate generation (Nelson et al., 2004)). Importantly, the pairs in SimLex-Assoc-333, while all strongly related, still span the full range of similarity scores.8 Therefore, the extent to which embeddings can model this data reflects their sensitivity to the similarity (or dissimilarity) of two concepts, even in the face of a strong signal in the training data that those concepts are related.\nThe TOEFL synonym test is another similarity-focused evaluation of embedding spaces. This test contains 80 cue words, each with four possible answers, of which one is a correct synonym (Landauer & Dumais, 1997). We computed the proportion of questions answered correctly by each model, where a model\u2019s answer was the nearest (cosine) neighbour to the cue word in its vocabulary.9 Note that, since TOEFL is a test of synonym recognition, it necessarily requires models to recognise similarity as opposed to relatedness.\nFinally, we tested how well different embeddings enabled a supervised classifier to distinguish between synonyms and antonyms, since synonyms are necessarily similar and people often find antonyms, which are necessarily dissimilar, to be strongly associated. For 744 word pairs handselected as either synonyms or antonyms,10 we presented a Gaussian SVM with the concatenation of the two word embeddings. We evaluated accuracy using 10-fold cross-validation.\n8The most dissimilar pair in SimLex-Assoc-333 is [shrink,grow] with a score of 0.23. The highest is [vanish,disappear] with 9.80.\n9To control for different vocabularies, we restricted the effective vocabulary of each model to the intersection of all model vocabularies, and excluded all questions that contained an answer outside of this intersection.\n10Available online at http://www.cl.cam.ac.uk/\u02dcfh295/.\nAs shown in Table 1, with these three additional similarity-focused tasks we again see the same pattern of results. NMT embeddings outperform other bilingually-trained embeddings which in turn outperform monolingual models. The difference is particularly striking on SimLex-Assoc-333, which suggests that the ability to discern similarity from relatedness (when relatedness is high) is perhaps the most clear distinction between the bilingual spaces and those of monolingual models.\nThese conclusions are also supported by qualitative analysis of the various embedding spaces. As shown in Table 2, in the NMT embedding spaces the nearest neighbours (by cosine distance) to concepts such as teacher are genuine synonyms such as professor or instructor. The bilingual objective also seems to orientate the non-NMT embeddings towards semantic similarity, although some purely related neighbours are also oberved. In contrast, in the monolingual embedding spaces the neighbours of teacher include highly related but dissimilar concepts such as student or college."}, {"heading": "3.2 IMPORTANCE OF TRAINING DATA QUANTITY", "text": "In previous work, monolingual models were trained on corpora many times larger than the English half of our parallel translation corpus. Indeed, the ability to scale to large quantities of training data was one of the principal motivations behind the skipgram architecture (Mikolov et al., 2013b). To check if monolingual models simply need more training data to capture similarity as effectively as bilingual models, we therefore trained them on increasingly large subsets of Wikipedia.11 As shown in Figure 1, this is not in fact the case. The performance of monolingual embeddings on similarity tasks remains well below the level of the NMT embeddings and somewhat lower than the non-MT bilingual embeddings as the amount of training data increases."}, {"heading": "3.3 ANALOGY RESOLUTION", "text": "Lexical analogy questions have been used as an alternative way of evaluating word representations. In this task, models must identify the correct answer (girl) when presented with analogy questions such as \u2018man is to boy as woman is to ?\u2019. It has been shown that Skipgram-style models are surprisingly effective at answering such questions (Mikolov et al., 2013b). This is because, if m,b and w are skigram-style embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary (by cosine distance) to the vector v = w + b\u2212m. We evaluated embeddings on analogy questions using the same vector-algebra method as Mikolov et al. (2013b). As in the previous section, for fair comparison we excluded questions containing a\n11We did not do the same for our translation models because sentence-aligned bilingual corpora of comparable size do not exist.\nword outside the intersection of all model vocabularies, and restricted all answer searches to this reduced vocabulary. This left 11,166 analogies. Of these, 7219 are classed as \u2018syntactic\u2019, in that they exemplify mappings between parts-of-speech or syntactic roles (e.g. fast is to fastest as heavy is to heaviest), and 3947 are classed as \u2018semantic\u2018 (Ottawa is to Canada as Paris is to France), since successful answering seems to rely on some (world) knowledge of the concepts themselves.\nAs shown in Fig. 2, NMT embeddings yield relatively poor answers to semantic analogy questions compared with monolingual embeddings and the bilingual embeddings FD (which are projections of similar monolingual embeddings).12 It appears that the translation objective prevents the embedding space from developing the same linear, geometric regularities as skipgram-style models with respect to semantic organisation. This also seems to be true of the embeddings from the full-sentence language model CW. Further, in the case of the Glove and FD models this advantage seems to be independent of both the domain and size of the training data, since embeddings from these models trained on only the English half of the translation corpus still outperform the translation embeddings.\nOn the other hand, NMT embeddings are effective for answering syntactic analogies using the vector algebra method. They perform comparably to or even better than monolingual embeddings when trained on less data (albeit bilingual data). It is perhaps unsurprising that the translation objective incentivises the encoding of a high degree of lexical syntactic information, since coherent targetlanguage sentences could not be generated without knowledge of the parts-of-speech, tense or case of its vocabulary items. The connection between the translation objective and the embedding of lexical syntactic information is further supported by the fact that embeddings learned by the bilingual model BiCVM do not perform comparably on the syntactic analogy task. In this model, sentential semantics is transferred via a bag-of-words representation, presumably rendering the precise syntactic information less important.\nWhen considering the two properties of NMT embeddings highlighted by these experiments, namely the encoding of semantic similarity and lexical syntax, it is worth noting that items in the similarityfocused evaluations of the previous section (SimLex-999 and TOEFL) consist of word groups or pairs that have identical syntactic role. Thus, even though lexical semantic information is in general pertinent to conceptual similarity (Levy & Goldberg, 2014), the lexical syntactic and conceptual properties of translation embeddings are in some sense independent of one another."}, {"heading": "4 EFFECT OF TARGET LANGUAGE", "text": "To better understand why a translation objective yields embedding spaces with particular properties, we trained the RNN Search architecture to translate from English to German.\n12The performance of the FD embeddings on this task is higher than that reported by Faruqui & Dyer (2014) because we search for answers over a smaller total candidate vocabulary.\nAs shown in Table 3 (left side), the performance of the source (English) embeddings learned by this model was comparable to that of those learned by the English-to-French model on all evaluations, even though the English-German training corpus (91 million words) was notably smaller than the English-French corpus (348m words). This evidence shows that the desirable properties of translation embeddings highlighted thus far are not particular to English-French translation, and can also emerge when translating to a different language family, with different word ordering conventions."}, {"heading": "5 OVERCOMING THE VOCABULARY SIZE PROBLEM", "text": "A potential drawback to using NMT models for learning word embeddings is the computational cost of training such a model on large vocabularies. To generate a target language sentence, NMT models repeatedly compute a softmax distribution over the target vocabulary. This computation scales with vocabulary size and must be repeated for each word in the output sentence, so that training models with large output vocabularies is challenging. Moreover, while the same computational bottleneck does not apply to the encoding process or source vocabulary, there is no way in which a translation model could learn a high quality source embedding for a word if the plausible translations were outside its vocabulary. Thus, limitations on the size of the target vocabulary effectively limit the scope of NMT models as representation-learning tools. This contrasts with the shallower monolingual and bilingual representation-learning models considered in this paper, which efficiently compute a distribution over a large target vocabulary using either a hierarchical softmax (Morin & Bengio, 2005) or approximate methods such as negative sampling (Mikolov et al., 2013b; Hermann & Blunsom, 2014), and thus can learn large vocabularies of both source and target embeddings.\nA recently proposed solution to this problem enables NMT models to be trained with larger target vocabularies (and hence larger meaningful source vocabularies) at comparable computational cost to training with a small target vocabulary (Jean et al., 2014). The algorithm uses (biased) importance sampling (Bengio & Se\u0301ne\u0301cal, 2003) to approximate the probability distribution of words over a large target vocabulary with a finite set of distributions over subsets of that vocabulary. Despite this element of approximation in the decoder, extending the effective target vocabulary in this way significantly improves translation performance, since the model can make sense of more sentences in the training data and encounters fewer unknown words at test time. In terms of representation learning, the method provides a means to scale up the NMT approach to vocabularies as large as those learned by monolingual models. However, given that the method replaces an exact calculation with an approximate one, we tested how the quality of source embeddings is affected by scaling up the target language vocabulary in this way.\nAs shown in Table 4, there is no significant degradation of embedding quality when scaling to large vocabularies with using the approximate decoder. Note that for a fair comparison we filtered these evaluations to only include items that are present in the smaller vocabulary. Thus, the numbers do not directly reflect the quality of the additional 470k embeddings learned by the extended vocabulary models, which one would expect to be lower since they are words of lower frequency. All embeddings can be downloaded from http://www.cl.cam.ac.uk/\u02dcfh295/, and the embeddings\nfrom the smaller vocabulary models can be interrogated at http://lisa.iro.umontreal. ca/mt-demo/embs/.13"}, {"heading": "6 HOW SIMILARITY EMERGES", "text": "Although NMT models appear to encode both conceptual similarity and syntactic information for any source and target languages, it is not the case that embedding spaces will always be identical. Interrogating the nearest neighbours of the source embedding spaces of the English-French and English-German models reveals occasional language-specific effects. As shown in Table 3 (right side), the neighbours for the word earned in the English-German model are as one might expect, whereas the neighbours from the English-French model contain the somewhat unlikely candidate won. In a similar vein, while the neighbours of the word castle from the English-French model are unarguably similar, the neighbours from the English-German model contain the word padlock.\nThese infrequent but striking differences between the English-German and English-French source embedding spaces indicate how similarity might emerge effectively in NMT models. Tokens of the French verb gagner have (at least) two possible English translations (win and earn). Since the translation model, which has limited encoding capacity, is trained to map tokens of win and earn to the same place in the target embedding space, it is efficient to move these concepts closer in the source space. Since win and earn map directly to two different verbs in German, this effect is not observed. On the other hand, the English nouns castle and padlock translate to a single noun (Schloss) in German, but different nouns in French. Thus, padlock and castle are only close in the source embeddings from the English-German model.\nBased on these considerations, we can conjecture that the following condition on the semantic configuration between two language is crucial to the effective induction of lexical similarity.\n(1) For s1 and s2 in the source language, there is some t in the target language such that there are sentences in the training data in which s1 translates to t and sentences in which s2 translates to t.\nif and only if\n(2) s1 and s2 are semantically similar.\nOf course, this condition is not true in general. However, we propose that the extent to which it holds over all possible word pairs corresponds to the quality of similarity induction in the translation embedding space. Note that strong polysemy in the target language, such as gagner = win, earn,\n13A different solution to the rare-word problem was proposed by (Luong et al., 2014). We do not evaluate the effects on the resulting embeddings of this method because we lack access to the source code.\ncan lead to cases in which 1 is satisfied but 2 is not. The conjecture claims that these cases are detrimental to the quality of the embedding space (at least with regards to similarity). In practice, qualitative analyses of the embedding spaces and native speaker intuitions suggest that such cases are comparatively rare. Moreover, when such cases are observed, s1 and s2, while perhaps not similar, are not strongly dissimilar. This could explain why related but strongly dissimilar concepts such as antonym pairs do not converge in the translation embedding space. This is also consistent with qualitative evidence presented by (Faruqui & Dyer, 2014) that projecting monolingual embeddings into a bilingual space orientates them to better reflect the synonymy/antonymy distinction."}, {"heading": "7 CONCLUSION", "text": "In this work, we have shown that the embedding spaces from neural machine translation models are orientated more towards conceptual similarity than those of monolingual models, and that translation embedding spaces also reflect richer lexical syntactic information. To perform well on similarity evaluations such as SimLex-999, embeddings must distinguish information pertinent to what concepts are (their function or ontology) from information reflecting other non-specific inter-concept relationships. Concepts that are strongly related but dissimilar, such as antonyms, are particularly challenging in this regard (Hill et al., 2014). Consistent with the qualitative observation made by Faruqui & Dyer (2014), we suggested how the nature of the semantic correspondence between the words in languages enables NMT embeddings to distinguish synonyms and antonyms and, more generally, to encode the information needed to reflect human intuitions of similarity.\nThe language-specific effects we observed in Section 4 suggest a potential avenue for improving translation and multi-lingual embeddings in future work. First, as the availability of fast GPUs for training grows, we would like to explore the embeddings learned by NMT models that translate between much more distant language pairs such as English-Chinese or English-Arabic. For these language pairs, the word alignment will less monotonic and may result in even more important semantic and syntactic information being encoded in the lexical representation. Further, as observed by both Hermann & Blunsom (2014) and Faruqui & Dyer (2014), the bilingual representation learning paradigm can be naturally extended to update representations based on correspondences between multiple languages (for instance by interleaving English-French and English-German training examples). Such an approach should smooth out language-specific effects, leaving embeddings that encode only language-agnostic conceptual semantics and are thus more generally applicable. Another related challenge is to develop smaller or less complex representation-learning tools that encode similarity with as much fidelity as NMT models but without the computational overhead. One promising approach for this is to learn word alignments and word embeddings jointly (Koc\u030cisky\u0301 et al., 2014). This approach is effective for cross-lingual document classification, although the authors do evaluate the monolingual subspace induced by the model.14\nNot all word embeddings learned from text are born equal. Depending on the application, those learned by NMT models may have particularly desirable properties. For decades, distributional semantic models have aimed to exploit Firth\u2019s famous distributional hypothesis to induce word meanings from (monolingual) text. However, the hypothesis also betrays the weakness of the monolingual distributional approach when it comes to learning humah-quality concept representations. For while it is undeniable that \u201cwords which are similar in meaning appear in similar distributional contexts\u201d (Firth, 1957), the converse assertion, which is what really matters, is only sometimes true."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank the developers of Theano Bergstra et al. (2010); Bastien et al. (2012). We acknowledge the support of the following agencies for research funding and computing support: St John\u2019s College Cambridge, NSERC, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR.\n14These embeddings are not publicly available and we were unable to re-train them using the source code."}], "references": [{"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai"], "venue": "In COLING,", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Agirre", "Eneko", "Alfonseca", "Enrique", "Hall", "Keith", "Kravalova", "Jana", "Pasca", "Marius", "Soroa", "Aitor"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "[cs.CL],", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni", "Marco", "Dinu", "Georgiana", "Kruszewski", "Germ\u00e1n"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Bengio", "Yoshua", "S\u00e9n\u00e9cal", "Jean-S\u00e9bastien"], "venue": "In Proceedings of AISTATS 2003,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Janvin", "Christian"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Multimodal distributional semantics", "author": ["Bruni", "Elia", "Tran", "Nam-Khanh", "Baroni", "Marco"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "An Autoencoder Approach to Learning Bilingual Word Representations", "author": ["Chandar", "Sarath", "Lauly", "Stanislas", "Larochelle", "Hugo", "Khapra", "Mitesh M", "Ravindran", "Balaraman", "Raykar", "Vikas", "Saha", "Amrita"], "venue": "In NIPS,", "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Ronan", "Weston", "Jason"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin", "Jacob", "Zbib", "Rabih", "Huang", "Zhongqiang", "Lamar", "Thomas", "Schwartz", "Richard", "Makhoul", "John"], "venue": "In 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Manaal", "Dyer", "Chris"], "venue": "In Proceedings of EACL,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "A synopsis of linguistic theory 1930-1955", "author": ["J Firth"], "venue": "pp. 1\u201332. Oxford: Philological Society,", "citeRegEx": "Firth and R.,? \\Q1957\\E", "shortCiteRegEx": "Firth and R.", "year": 1957}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Haghighi", "Aria", "Liang", "Percy", "Berg-Kirkpatrick", "Taylor", "Klein", "Dan"], "venue": "In ACL,", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["Hermann", "Karl Moritz", "Blunsom", "Phil"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Learning abstract concepts from multi-modal data: Since you probably can\u2019t see what i mean", "author": ["Hill", "Felix", "Korhonen", "Anna"], "venue": "In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill", "Felix", "Reichart", "Roi", "Korhonen", "Anna"], "venue": "arXiv preprint arXiv:1408.3456,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean", "S\u00e9bastien", "Cho", "Kyunghyun", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.2007,", "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "Seattle, October", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Learning Bilingual Word Representations by Marginalizing Alignments", "author": ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Hermann", "Karl Moritz", "Blunsom", "Phil"], "venue": "In Proceedings of ACL,", "citeRegEx": "Ko\u010disk\u00fd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ko\u010disk\u00fd et al\\.", "year": 2014}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Thomas K", "Dumais", "Susan T"], "venue": "Psychological review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Omer", "Goldberg", "Yoav"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong", "Thang", "Sutskever", "Ilya", "Le", "Quoc V", "Vinyals", "Oriol", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "The university of south florida free association, rhyme, and word fragment norms", "author": ["Nelson", "Douglas L", "McEvoy", "Cathy L", "Schreiber", "Thomas A"], "venue": "Behavior Research Methods, Instruments, & Computers,", "citeRegEx": "Nelson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nelson et al\\.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher"], "venue": "In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "arXiv preprint arXiv:1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Peter D", "Pantel", "Patrick"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Identifying word translations from comparable corpora using latent topic models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume", "author": ["Vuli\u0107", "Ivan", "De Smet", "Wim", "Moens", "Marie-Francine"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "The objective can be to predict either the next word given the initial words of a sentence (Bengio et al., 2003; Mnih & Hinton, 2009; Collobert & Weston, 2008), or simply a nearby word given a single cue word (Mikolov et al.", "startOffset": 91, "endOffset": 159}, {"referenceID": 30, "context": ", 2003; Mnih & Hinton, 2009; Collobert & Weston, 2008), or simply a nearby word given a single cue word (Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 104, "endOffset": 152}, {"referenceID": 3, "context": "The representations learned by neural models (sometimes called embeddings) perform very effectively when applied as pre-trained features in a range of NLP applications and tasks (Baroni et al., 2014).", "startOffset": 178, "endOffset": 199}, {"referenceID": 12, "context": "NMT models have recently emerged as an alternative to statistical, phrase-based translation models, and are beginning to achieve impressive translation performance (Kalchbrenner & Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014).", "startOffset": 164, "endOffset": 239}, {"referenceID": 31, "context": "NMT models have recently emerged as an alternative to statistical, phrase-based translation models, and are beginning to achieve impressive translation performance (Kalchbrenner & Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014).", "startOffset": 164, "endOffset": 239}, {"referenceID": 5, "context": "In the original neural language model (Bengio et al., 2003) and subsequent variants (Collobert & Weston, 2008), training examples consist of an ordered sequence of n words, with the model trained to predict the n-th word given the first n\u22121 words.", "startOffset": 38, "endOffset": 59}, {"referenceID": 30, "context": "More recent work has shown that high quality word embeddings can be learned via simpler models with no nonlinear hidden layer (Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 126, "endOffset": 174}, {"referenceID": 15, "context": "Various studies have demonstrated that word representations can also be effectively learned from bilingual corpora, aligned at the document, paragraph or word level (Haghighi et al., 2008; Vuli\u0107 et al., 2011; Mikolov et al., 2013a; Hermann & Blunsom, 2014; Chandar et al., 2014).", "startOffset": 165, "endOffset": 278}, {"referenceID": 33, "context": "Various studies have demonstrated that word representations can also be effectively learned from bilingual corpora, aligned at the document, paragraph or word level (Haghighi et al., 2008; Vuli\u0107 et al., 2011; Mikolov et al., 2013a; Hermann & Blunsom, 2014; Chandar et al., 2014).", "startOffset": 165, "endOffset": 278}, {"referenceID": 9, "context": "Various studies have demonstrated that word representations can also be effectively learned from bilingual corpora, aligned at the document, paragraph or word level (Haghighi et al., 2008; Vuli\u0107 et al., 2011; Mikolov et al., 2013a; Hermann & Blunsom, 2014; Chandar et al., 2014).", "startOffset": 165, "endOffset": 278}, {"referenceID": 15, "context": "The resulting multilingual embedding spaces have been effectively applied to bilingual lexicon extraction (Haghighi et al., 2008; Vuli\u0107 et al., 2011; Mikolov et al., 2013a) and document classification (Klementiev et al.", "startOffset": 106, "endOffset": 172}, {"referenceID": 33, "context": "The resulting multilingual embedding spaces have been effectively applied to bilingual lexicon extraction (Haghighi et al., 2008; Vuli\u0107 et al., 2011; Mikolov et al., 2013a) and document classification (Klementiev et al.", "startOffset": 106, "endOffset": 172}, {"referenceID": 9, "context": ", 2013a) and document classification (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Ko\u010disk\u00fd et al., 2014).", "startOffset": 37, "endOffset": 125}, {"referenceID": 21, "context": ", 2013a) and document classification (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Ko\u010disk\u00fd et al., 2014).", "startOffset": 37, "endOffset": 125}, {"referenceID": 0, "context": ", 2013a) and document classification (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Ko\u010disk\u00fd et al., 2014). We focus our analysis on two representatives of this class of (non-NMT) bilingual model. The first is that of Hermann & Blunsom (2014), whose embeddings improve on the performance of A.", "startOffset": 38, "endOffset": 262}, {"referenceID": 0, "context": ", 2013a) and document classification (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Ko\u010disk\u00fd et al., 2014). We focus our analysis on two representatives of this class of (non-NMT) bilingual model. The first is that of Hermann & Blunsom (2014), whose embeddings improve on the performance of A. Klementiev & Bhattarai (2012) in document classification applications.", "startOffset": 38, "endOffset": 343}, {"referenceID": 0, "context": ", 2013a) and document classification (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Ko\u010disk\u00fd et al., 2014). We focus our analysis on two representatives of this class of (non-NMT) bilingual model. The first is that of Hermann & Blunsom (2014), whose embeddings improve on the performance of A. Klementiev & Bhattarai (2012) in document classification applications. As with the NMT models introduced in the next section, this model can be trained directly on bitexts aligned only at the sentence rather than word level. When training, for aligned sentences SE and SF in different languages, the model computes representations RE and RF by summing the embeddings of the words in SE and SF respectively. The embeddings are then updated to minimise the divergence betweenRE andRF (since they convey a common meaning). A noise-contrastive loss function ensures that the model does not arrive at trivial (e.g. all zero) solutions to this objective. Hermann & Blunsom (2014) show that, despite the lack of prespecified word alignments, words in the two languages with similar meanings converge in the bilingual embedding space.", "startOffset": 38, "endOffset": 987}, {"referenceID": 0, "context": ", 2013a) and document classification (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Ko\u010disk\u00fd et al., 2014). We focus our analysis on two representatives of this class of (non-NMT) bilingual model. The first is that of Hermann & Blunsom (2014), whose embeddings improve on the performance of A. Klementiev & Bhattarai (2012) in document classification applications. As with the NMT models introduced in the next section, this model can be trained directly on bitexts aligned only at the sentence rather than word level. When training, for aligned sentences SE and SF in different languages, the model computes representations RE and RF by summing the embeddings of the words in SE and SF respectively. The embeddings are then updated to minimise the divergence betweenRE andRF (since they convey a common meaning). A noise-contrastive loss function ensures that the model does not arrive at trivial (e.g. all zero) solutions to this objective. Hermann & Blunsom (2014) show that, despite the lack of prespecified word alignments, words in the two languages with similar meanings converge in the bilingual embedding space.2 The second model we examine is that of Faruqui & Dyer (2014). Unlike the models described above, Faruqui & Dyer (2014) showed explicitly that projecting word embeddings from two languages (learned independently) into a common vector space can favourably influence the orientation of word embeddings when considered in their monolingual subspace; i.", "startOffset": 38, "endOffset": 1202}, {"referenceID": 0, "context": ", 2013a) and document classification (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Ko\u010disk\u00fd et al., 2014). We focus our analysis on two representatives of this class of (non-NMT) bilingual model. The first is that of Hermann & Blunsom (2014), whose embeddings improve on the performance of A. Klementiev & Bhattarai (2012) in document classification applications. As with the NMT models introduced in the next section, this model can be trained directly on bitexts aligned only at the sentence rather than word level. When training, for aligned sentences SE and SF in different languages, the model computes representations RE and RF by summing the embeddings of the words in SE and SF respectively. The embeddings are then updated to minimise the divergence betweenRE andRF (since they convey a common meaning). A noise-contrastive loss function ensures that the model does not arrive at trivial (e.g. all zero) solutions to this objective. Hermann & Blunsom (2014) show that, despite the lack of prespecified word alignments, words in the two languages with similar meanings converge in the bilingual embedding space.2 The second model we examine is that of Faruqui & Dyer (2014). Unlike the models described above, Faruqui & Dyer (2014) showed explicitly that projecting word embeddings from two languages (learned independently) into a common vector space can favourably influence the orientation of word embeddings when considered in their monolingual subspace; i.", "startOffset": 38, "endOffset": 1260}, {"referenceID": 0, "context": ", 2013a) and document classification (Klementiev et al.; Hermann & Blunsom, 2014; Chandar et al., 2014; Ko\u010disk\u00fd et al., 2014). We focus our analysis on two representatives of this class of (non-NMT) bilingual model. The first is that of Hermann & Blunsom (2014), whose embeddings improve on the performance of A. Klementiev & Bhattarai (2012) in document classification applications. As with the NMT models introduced in the next section, this model can be trained directly on bitexts aligned only at the sentence rather than word level. When training, for aligned sentences SE and SF in different languages, the model computes representations RE and RF by summing the embeddings of the words in SE and SF respectively. The embeddings are then updated to minimise the divergence betweenRE andRF (since they convey a common meaning). A noise-contrastive loss function ensures that the model does not arrive at trivial (e.g. all zero) solutions to this objective. Hermann & Blunsom (2014) show that, despite the lack of prespecified word alignments, words in the two languages with similar meanings converge in the bilingual embedding space.2 The second model we examine is that of Faruqui & Dyer (2014). Unlike the models described above, Faruqui & Dyer (2014) showed explicitly that projecting word embeddings from two languages (learned independently) into a common vector space can favourably influence the orientation of word embeddings when considered in their monolingual subspace; i.e relative to other words in their own language. In contrast to the other models considered in this paper, the approach of Faruqui & Dyer (2014) requires bilingual data to be aligned at the word level.", "startOffset": 38, "endOffset": 1634}, {"referenceID": 31, "context": "The objective of NMT is to generate an appropriate sentence in a target language St given a sentence Ss in the source language (see, e.g., Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014).", "startOffset": 127, "endOffset": 192}, {"referenceID": 10, "context": "Although NMT models can differ in their low-level architecture (Kalchbrenner & Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), the translation objective exerts similar pressure on the embeddings in all cases.", "startOffset": 63, "endOffset": 134}, {"referenceID": 2, "context": "Although NMT models can differ in their low-level architecture (Kalchbrenner & Blunsom, 2013; Cho et al., 2014; Bahdanau et al., 2014), the translation objective exerts similar pressure on the embeddings in all cases.", "startOffset": 63, "endOffset": 134}, {"referenceID": 2, "context": "Alternatively, subsequences (phrases) of Ss may be encoded at this stage in place of the whole sentence (Bahdanau et al., 2014).", "startOffset": 104, "endOffset": 127}, {"referenceID": 8, "context": "The models of Chandar et al. (2014) and Hermann & Blunsom (2014) both aim to minimise the divergence between source and target language sentences represented as sums of word embeddings.", "startOffset": 14, "endOffset": 36}, {"referenceID": 8, "context": "The models of Chandar et al. (2014) and Hermann & Blunsom (2014) both aim to minimise the divergence between source and target language sentences represented as sums of word embeddings.", "startOffset": 14, "endOffset": 65}, {"referenceID": 2, "context": "The second is the RNN Search architecture (Bahdanau et al., 2014), which was designed to overcome limitations exhibited by the RNN encoder-decoder when translating very long sentences.", "startOffset": 42, "endOffset": 65}, {"referenceID": 30, "context": ", 2013b) and its Glove variant (Pennington et al., 2014) for the same number of epochs on the English half of the bilingual corpus.", "startOffset": 31, "endOffset": 56}, {"referenceID": 2, "context": "The second is the RNN Search architecture (Bahdanau et al., 2014), which was designed to overcome limitations exhibited by the RNN encoder-decoder when translating very long sentences. RNN Search includes a attention mechanism, an additional feed-forward network that learns to attend to different parts of the source sentence when decoding each word in the target sentence.4 Both models were trained on a 348m word corpus of English-French sentence pairs or a 91m word corpus of English-German sentence pairs.5 To explore the properties of bilingual embeddings learned via objectives other than direct translation, we trained the BiCVM model of Hermann & Blunsom (2014) on the same data, and also downloaded the projected embeddings of Faruqui & Dyer (2014), FD, trained on a bilingual corpus of comparable size (\u2248 300 million words per language).", "startOffset": 43, "endOffset": 671}, {"referenceID": 2, "context": "The second is the RNN Search architecture (Bahdanau et al., 2014), which was designed to overcome limitations exhibited by the RNN encoder-decoder when translating very long sentences. RNN Search includes a attention mechanism, an additional feed-forward network that learns to attend to different parts of the source sentence when decoding each word in the target sentence.4 Both models were trained on a 348m word corpus of English-French sentence pairs or a 91m word corpus of English-German sentence pairs.5 To explore the properties of bilingual embeddings learned via objectives other than direct translation, we trained the BiCVM model of Hermann & Blunsom (2014) on the same data, and also downloaded the projected embeddings of Faruqui & Dyer (2014), FD, trained on a bilingual corpus of comparable size (\u2248 300 million words per language).", "startOffset": 43, "endOffset": 759}, {"referenceID": 1, "context": "As in previous studies (Agirre et al., 2009; Bruni et al., 2014; Baroni et al., 2014), our initial evaluations involved calculating pairwise (cosine) distances between embeddings and correlating these distances with (gold-standard) human judgements of the strength of relationships between concepts.", "startOffset": 23, "endOffset": 85}, {"referenceID": 8, "context": "As in previous studies (Agirre et al., 2009; Bruni et al., 2014; Baroni et al., 2014), our initial evaluations involved calculating pairwise (cosine) distances between embeddings and correlating these distances with (gold-standard) human judgements of the strength of relationships between concepts.", "startOffset": 23, "endOffset": 85}, {"referenceID": 3, "context": "As in previous studies (Agirre et al., 2009; Bruni et al., 2014; Baroni et al., 2014), our initial evaluations involved calculating pairwise (cosine) distances between embeddings and correlating these distances with (gold-standard) human judgements of the strength of relationships between concepts.", "startOffset": 23, "endOffset": 85}, {"referenceID": 1, "context": "For this we used three different gold standards: WordSim-353 (Agirre et al., 2009), MEN (Bruni et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 8, "context": ", 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 17, "context": ", 2014) and SimLex-999 (Hill et al., 2014).", "startOffset": 23, "endOffset": 42}, {"referenceID": 1, "context": "As in previous studies (Agirre et al., 2009; Bruni et al., 2014; Baroni et al., 2014), our initial evaluations involved calculating pairwise (cosine) distances between embeddings and correlating these distances with (gold-standard) human judgements of the strength of relationships between concepts. For this we used three different gold standards: WordSim-353 (Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2014). Importantly, there is a clear distinction between WordSim-353 and MEN, on the one hand, and SimLex-999, on the other, in terms of the semantic relationship that they quantify. For both WordSim-353 and MEN, annotators were asked to rate how related or associated two concepts are. Consequently, pairs such as [clothes-closet], which are clearly related but ontologically dissimilar, have high ratings in WordSim-353 and MEN. In contrast, such pairs receive a low rating in SimLex-999, where only genuinely similar concepts, such as [coast- shore], receive high ratings. To reproduce the scores in SimLex-999, models must thus distinguish pairs that are similar from those that are merely related. In particular, this requires models to develop sensitivity to the distinction between synonyms (similar) and antonyms (often strongly related, but highly dissimilar).7 Table 1 shows the correlations of NMT (English-French) embeddings, other bilingually-trained embeddings and monolingual embeddings with these three lexical gold-standards. NMT outperform monolingual embeddings, and, to a lesser extent, the other bilingually trained embeddings, on SimLex-999. However, this clear advantage is not observed on MEN and WordSim-353, where the projected embeddings of Faruqui & Dyer (2014), which were tuned for high performance on WordSim-353, perform best.", "startOffset": 24, "endOffset": 1728}, {"referenceID": 12, "context": "Access to source code and limited GPU time prevent us from training and evaluating the embeddings from other NMT models such as that of (Kalchbrenner & Blunsom, 2013), (Devlin et al., 2014) and Sutskever et al.", "startOffset": 168, "endOffset": 189}, {"referenceID": 17, "context": "For a more detailed discussion of the similarity/relatedness distinction, see (Hill et al., 2014).", "startOffset": 78, "endOffset": 97}, {"referenceID": 11, "context": "Access to source code and limited GPU time prevent us from training and evaluating the embeddings from other NMT models such as that of (Kalchbrenner & Blunsom, 2013), (Devlin et al., 2014) and Sutskever et al. (2014). The underlying principles of encoding-decoding also apply to these models, and we expect the embeddings would exhibit similar properties to those analysed here.", "startOffset": 169, "endOffset": 218}, {"referenceID": 10, "context": "These corpora were produced from the WMT \u201914 parallel data after conducting the data-selection procedure described by Cho et al. (2014). Available from http://www.", "startOffset": 118, "endOffset": 136}, {"referenceID": 17, "context": "In the first, we measured performance on SimLex-Assoc-333 (Hill et al., 2014).", "startOffset": 58, "endOffset": 77}, {"referenceID": 29, "context": "This evaluation comprises the 333 most related pairs in SimLex-999, according to an independent empirical measure of relatedness (free associate generation (Nelson et al., 2004)).", "startOffset": 156, "endOffset": 177}, {"referenceID": 25, "context": "It has been shown that Skipgram-style models are surprisingly effective at answering such questions (Mikolov et al., 2013b). This is because, if m,b and w are skigram-style embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary (by cosine distance) to the vector v = w + b\u2212m. We evaluated embeddings on analogy questions using the same vector-algebra method as Mikolov et al. (2013b). As in the previous section, for fair comparison we excluded questions containing a We did not do the same for our translation models because sentence-aligned bilingual corpora of comparable size do not exist.", "startOffset": 101, "endOffset": 445}, {"referenceID": 19, "context": "A recently proposed solution to this problem enables NMT models to be trained with larger target vocabularies (and hence larger meaningful source vocabularies) at comparable computational cost to training with a small target vocabulary (Jean et al., 2014).", "startOffset": 236, "endOffset": 255}, {"referenceID": 24, "context": "A different solution to the rare-word problem was proposed by (Luong et al., 2014).", "startOffset": 62, "endOffset": 82}, {"referenceID": 17, "context": "Concepts that are strongly related but dissimilar, such as antonyms, are particularly challenging in this regard (Hill et al., 2014).", "startOffset": 113, "endOffset": 132}, {"referenceID": 21, "context": "One promising approach for this is to learn word alignments and word embeddings jointly (Ko\u010disk\u00fd et al., 2014).", "startOffset": 88, "endOffset": 110}, {"referenceID": 17, "context": "Concepts that are strongly related but dissimilar, such as antonyms, are particularly challenging in this regard (Hill et al., 2014). Consistent with the qualitative observation made by Faruqui & Dyer (2014), we suggested how the nature of the semantic correspondence between the words in languages enables NMT embeddings to distinguish synonyms and antonyms and, more generally, to encode the information needed to reflect human intuitions of similarity.", "startOffset": 114, "endOffset": 208}, {"referenceID": 17, "context": "Concepts that are strongly related but dissimilar, such as antonyms, are particularly challenging in this regard (Hill et al., 2014). Consistent with the qualitative observation made by Faruqui & Dyer (2014), we suggested how the nature of the semantic correspondence between the words in languages enables NMT embeddings to distinguish synonyms and antonyms and, more generally, to encode the information needed to reflect human intuitions of similarity. The language-specific effects we observed in Section 4 suggest a potential avenue for improving translation and multi-lingual embeddings in future work. First, as the availability of fast GPUs for training grows, we would like to explore the embeddings learned by NMT models that translate between much more distant language pairs such as English-Chinese or English-Arabic. For these language pairs, the word alignment will less monotonic and may result in even more important semantic and syntactic information being encoded in the lexical representation. Further, as observed by both Hermann & Blunsom (2014) and Faruqui & Dyer (2014), the bilingual representation learning paradigm can be naturally extended to update representations based on correspondences between multiple languages (for instance by interleaving English-French and English-German training examples).", "startOffset": 114, "endOffset": 1067}, {"referenceID": 17, "context": "Concepts that are strongly related but dissimilar, such as antonyms, are particularly challenging in this regard (Hill et al., 2014). Consistent with the qualitative observation made by Faruqui & Dyer (2014), we suggested how the nature of the semantic correspondence between the words in languages enables NMT embeddings to distinguish synonyms and antonyms and, more generally, to encode the information needed to reflect human intuitions of similarity. The language-specific effects we observed in Section 4 suggest a potential avenue for improving translation and multi-lingual embeddings in future work. First, as the availability of fast GPUs for training grows, we would like to explore the embeddings learned by NMT models that translate between much more distant language pairs such as English-Chinese or English-Arabic. For these language pairs, the word alignment will less monotonic and may result in even more important semantic and syntactic information being encoded in the lexical representation. Further, as observed by both Hermann & Blunsom (2014) and Faruqui & Dyer (2014), the bilingual representation learning paradigm can be naturally extended to update representations based on correspondences between multiple languages (for instance by interleaving English-French and English-German training examples).", "startOffset": 114, "endOffset": 1093}, {"referenceID": 6, "context": "The authors would like to thank the developers of Theano Bergstra et al. (2010); Bastien et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 4, "context": "(2010); Bastien et al. (2012). We acknowledge the support of the following agencies for research funding and computing support: St John\u2019s College Cambridge, NSERC, Calcul Qu\u00e9bec, Compute Canada, the Canada Research Chairs and CIFAR.", "startOffset": 8, "endOffset": 30}], "year": 2015, "abstractText": "Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness.", "creator": "LaTeX with hyperref package"}}}