{"id": "1412.6597", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "An Analysis of Unsupervised Pre-training in Light of Recent Advances", "abstract": "Convolutional neural focused conducted even opening relation merit far of a number of strong seeking: remiss linear squadrons (ReLUs ), data cypher, dropout, , form blacklists analysing. Unsupervised accurately in been approval both takes way soon strengthening performance. Unfortunately, traineeship taking - assignment is not same by state - major - new - masterpieces methods leading have the years it: Is unsupervised pre - missions much suggests so has overcome? If so, went? We answers this of each addition: must 1) develop next unsupervised processes that incorporates ReLUs and late unsupervised regularization aspects, =) tools the benefits of unsupervised after - specialist 1.9 may calculations passivation under dropout on CIFAR - 6 while varying present variable of babysitting put internship samples, 3) withheld our document on STL - 90. We learn overstaying expected - skills, now ahead, helps out place versus significant apnoea without supervised processed particular up, on surprisingly, hurts when over 0.5 same modest. We part provided unsupervised drop - instructors and providing visual gynecological from achieve near oregon - also - put - sculpture musical that STL - 90.", "histories": [["v1", "Sat, 20 Dec 2014 04:20:55 GMT  (1040kb,D)", "https://arxiv.org/abs/1412.6597v1", "9 pages, 3 figures"], ["v2", "Tue, 27 Jan 2015 22:03:40 GMT  (1039kb,D)", "http://arxiv.org/abs/1412.6597v2", "9 pages, 3 figures We made two changes: + 2 of our 32 experiments in the CIFAR10 analysis section were misreported. Those are corrected in Table 2 and Figure 2. + Reported full results of STL10 experiments with color augmentation in Table 3"], ["v3", "Mon, 2 Mar 2015 21:05:34 GMT  (1046kb,D)", "http://arxiv.org/abs/1412.6597v3", "9 pages, 3 figures We made two changes: + 2 of our 32 experiments in the CIFAR10 analysis section were misreported. Those are corrected in Table 2 and Figure 2. + Reported full results of STL10 experiments with color augmentation in Table 3. In Version 3, we made minor changes to address reviewer questions, and added links to the code and experiments"], ["v4", "Fri, 10 Apr 2015 21:26:31 GMT  (1046kb,D)", "http://arxiv.org/abs/1412.6597v4", "Accepted as a workshop contribution to ICLR 2015"]], "COMMENTS": "9 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["tom le paine", "pooya khorrami", "wei han", "thomas s huang"], "accepted": true, "id": "1412.6597"}, "pdf": {"name": "1412.6597.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RECENT ADVANCES", "Tom Le Paine", "Pooya Khorrami", "Wei Han", "Thomas S. Huang"], "emails": ["paine1@illinois.edu", "pkhorra2@illinois.edu", "weihan3@illinois.edu", "t-huang1@illinois.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "We analyze the benefits of unsupervised pre-training in the context of recent deep learning innovations including: rectified linear units, data augmentation, and dropout. Recent work shows that convolutional neural networks (CNNs) can achieve state-of-the-art performance for object classification (Krizhevsky et al. (2012)) and object detection (Girshick et al. (2013)), when there is enough training data. However, in many cases there is a dearth of labeled data. In these cases regularization is necessary for good results. The most common types of regularization are data augmentations (Krizhevsky et al. (2012); Dosovitskiy et al. (2014)) and dropout (Hinton et al. (2012)). Another form of regularization, unsupervised pre-training (Hinton et al. (2006); Bengio et al. (2007); Erhan et al. (2010)), has recently fallen out of favor.\nWhile there has been significant work in unsupervised learning, most of these works came before rectified linear units, which significantly help training deep supervised neural networks, and before simpler regularization schemes for unsupervised learning, such as zero-bias with linear encoding for auto-encoders (Memisevic et al. (2014)).\nWe train an unsupervised method that takes advantage of these improvements we call Zero-bias Convolutional Auto-encoders (CAEs). Previous work showed that pre-trained tanh CAEs achieved an increase in performance over randomly initialized tanh CNNs. We conduct this experiment with our zero-bias CAE and observe a larger boost in performance.\nWe analyze the effectiveness of our technique when combined with the popular regularization techniques used during supervised training on CIFAR-10 while varying the ratio of unsupervised to supervised samples. We do this comparing against randomly initialized CNNs without any additional regularization. We find that, when ratio is large, unsupervised pre-training provides useful regularization, increasing test set performance. When the ratio is small, we find that unsupervised pre-training hurts performance.\n\u2217- Authors contributed equally to this work.\nar X\niv :1\n41 2.\n65 97\nv4 [\ncs .C\nV ]\n1 0\nA pr\n2 01\n5\nWe verify our finding that unsupervised pre-training can boost performance when the ratio of unsupervised to supervised samples is high by running our algorithm on the STL-10 dataset, which has a ratio of 100:1. As expected, we observe an improvement (3.87%). When combined with additional color augmentation, we achieve near state-of-the-art results. Our unsupervised regularization still yields an improvement of (1.69%).\nWe will begin by reviewing related work on fully-connected and convolutional auto-encoders. In Section 3, we will present our method and how it is trained both during unsupervised pre-training and supervised fine-tuning. We present our results on the CIFAR-10 and STL-10 datasets in Section 4, and in Section 5 we conclude the paper."}, {"heading": "2 RELATED WORK", "text": "Many methods have used unsupervised learning to learn parameters, which are subsequently used to initialize a neural network to be trained on supervised data. These are called unsupervised pretraining, and supervised fine-tuning respectively. We will highlight some of the unsupervised learning methods related to our work."}, {"heading": "2.1 AUTO-ENCODERS", "text": "One of the most widely-used models for unsupervised learning, an auto-encoder is a model that learns a function that minimizes the squared error between the input x \u2208 Rn and its reconstruction r(x):\nL = \u2016x\u2212 r(x)\u201622 (1) r(x) = WTd f(Wex+ b) + c (2)\nIn the above equation, We represents the weight matrix that transforms the input, x into some hidden representation, b is vector of biases for each hidden unit and f(\u00b7) is some nonlinear function. Commonly chosen examples for f(\u00b7) include the sigmoid and hyperbolic tangent functions. Meanwhile, Wd is the weight matrix that maps back from the hidden representation to the input space and c is a vector of biases for each input (visible) unit. These parameters are commonly learned by minimizing the loss function over the training data via stochastic gradient descent.\nWhen no other constraints are imposed on the loss function, the auto-encoder weights tend to learn the identity function. To combat this, some form of regularization must imposed upon the model so that the model can uncover the underlying structure in the data. Some forms of regularization include adding noise to the input units (Vincent et al. (2010)) and requiring the hidden unit activations be sparse (Coates et al. (2011)) or have small derivatives (Rifai et al. (2011)). These models are known as de-noising, sparse, and contractive auto-encoders respectively. A more recent work by Memisevic et al. (2014) showed that training an auto-encoder with rectified linear units (ReLU) caused the activations to form tight clusters due to having negative bias values. They showed that using thresholded linear (TLin) or thresholded rectifier (TRec) activations with no bias can allow one to train an auto-encoder without the need for additional regularization."}, {"heading": "2.2 CONVOLUTIONAL AUTO-ENCODERS", "text": "While the aforementioned fully-connected techniques have shown impressive results, they do not directly address the structure of images. Convolutional neural networks (CNNs) (LeCun et al. (1998); Lee et al. (2009)) present a way to reduce the number of connections by having each hidden unit only be responsible for a small local neighborhood of visible units. Such schemes allow for dense feature extraction followed by pooling layers which when stacked could allow the network to learn over larger and larger receptive fields. Convolutional auto-encoders (CAEs) combined aspects from both auto-encoders and convolutional neural nets making it possible to extract highly localized patchbased information in an unsupervised fashion. There have been several works in this area including Jarrett et al. (2009) and Zeiler et al. (2010). Both rely on sparse coding to force their unsupervised learning to learn non-trival solutions. Zeiler et al. (2011) extended this work by introducing pooling/unpooling and visualizing how individual feature maps at different layers influenced specific portions of the reconstruction. These sparse coding approaches had limitations because they used\nan iterative procedure for inference. A later work by Masci et al. (2011) trained deep feed forward convolutional auto-encoders, using only max-pooling and saturating tanh non-linearities as a form of regularization, while still showing a modest improvement over randomly initialized CNNs. While tanh was a natural choice at the time, Krizhevsky et al. (2012) showed that ReLUs are more suitable for learning given their non-saturating behavior."}, {"heading": "3 OUR APPROACH", "text": "Our method\u2019s training framework can be broken up into two phases: (i) unsupervised pre-training and (ii) supervised fine-tuning. We describe those in more detail below."}, {"heading": "3.1 UNSUPERVISED PRE-TRAINING", "text": "Our method incorporates aspects of previous unsupervised learning methods in order to learn salient features, yet be efficient to train. Our model is similar to the deconvolutional network in Zeiler et al. (2011) where the cost we minimize at each layer is the mean square error on the original image. However, unlike the network in Zeiler et al. (2011), our method does not use any form of sparse coding. Our model also is similar to that of Masci et al. (2011), however we improve upon it by introducing regularization in the convolutional layers through the use of zero-biases and ReLUs as discussed in Memisevic et al. (2014).\nWe now describe the model architecture in detail. Like the previous work described above, our model involves several encoding modules followed by several decoding modules. A single encoding module El(\u00b7) consists of a convolution layer Fl, a nonlinearity f(\u00b7), followed by a pooling layer Psl with switches sl.\nEl(x) = Pslf(Flx) (3)\nEach encoding module has an associated decoding module Dl, which unpools using El pooling switches sl and deconvolves with El\u2019s filters, (i.e. FTl ).\nDl(x) = F T l Uslx (4)\nA two layer network can be written as:\nr(x) = D1(D2(E2(E1(x)))) (5)\nWe train each encoder/decoder pair in a greedy fashion (i.e. first a 1 layer CAE, then a 2 layer CAE, etc.) while keeping the parameters of previous layers fixed. Like Zeiler et al. (2011), we compute the cost by taking the mean squared error between the original image and the network\u2019s reconstruction of the input. Thus, the costs for a one layer network (C1(x)) and two layer network (C2(x)) would be expressed in the following manner:\nC1(x) = \u2016x\u2212D1(E1(x))\u201622 (6) C2(x) = \u2016x\u2212D1(D2(E2(E1(x))))\u201622 (7)\nWe regularize our learned representation by fixing the biases of our convolutional and deconvolutional layers at zero and using ReLUs as our activation function during encoding. We use linear activations for our decoders. Unlike the work by Memisevic et al. (2014) which analyzes fullyconnected auto-encoders, our work is the first, to our knowledge, that trains zero-bias CAEs for unsupervised learning."}, {"heading": "3.1.1 UNSUPERVISED WEIGHT INITIALIZATION", "text": "Weight initialization is often a key component of successful neural network training. For ReLU\u2019s it is important to ensure the input to the ReLU is greater than 0. This can be achieved by setting the bias appropriately. This cannot be done for zero-bias auto-encoders. Instead we use two methods\nfor initializing the weights to achieve this 1) in the first layer, we initialize each of the filters to be a randomly drawn patch from the dataset, 2) on the later layers, we sample weights from a Gaussian distribution and find the nearest orthogonal matrix by taking the singular value decomposition (SVD) of the weight matrix and setting all of the singular values to one. For CNNs we must take into account the additive effect of overlapping patches thus we weight each filter by a 2D hamming window to prevent intensity build-up."}, {"heading": "3.2 SUPERVISED FINE-TUNING", "text": "After the weights of the CAE have been trained, we remove all of the decoder modules and leave just the encoding modules. We add an additional fully-connected layer and a softmax layer to the pretrained encoding modules. The weights of these layers are drawn from a Gaussian distribution with zero mean and standard deviation of k/ \u221a NFAN IN , where k is drawn uniformly from [0.2, 1.2]."}, {"heading": "3.3 TRAINING", "text": "For both unsupervised and supervised training we use stochastic gradient descent with a constant momentum of 0.9, and a weight decay parameter of 1e-5. We select the highest learning rate that doesn\u2019t explode for the duration of training. For these experiments we do not anneal the learning rate. The only pre-processing we do to each patch is centering (i.e. mean subtraction) and scaling to unit variance."}, {"heading": "4 EXPERIMENTS AND ANALYSIS", "text": ""}, {"heading": "4.1 DATASETS", "text": "We run experiments on two natural image datasets, CIFAR-10 (Krizhevsky and Hinton (2009)) and STL-10 (Coates et al. (2011)). CIFAR-10 is a common benchmark for object recognition. Many unsupervised and supervised neural network approaches have been tested on it. It consists of 32x32 pixel color images drawn from 10 object categories. It has 50,000 training images, and 10,000 testing images. STL-10 is also an object recognition benchmark, but was designed to test unsupervised learning algorithms, so it has a relatively small labeled training set of 500 images per class, and an additional unsupervised set which contains 100,000 unlabeled images. The test set contains 800 labeled images per class. All examples are 96x96 pixel color images."}, {"heading": "4.2 CIFAR-10", "text": "On CIFAR-10, we train a network with structure similar to Masci et al. (2011), so that we can directly show the benefits of our modifications. The network consists of three convolutional layers with 96, 144, and 192 filters respectively. The filters in the first two layers are of size 5x5 while the filters in the third layer are of size 3x3. We also add 2x2 max pooling layers after the first two convolutional layers. There is also a full-connected layer with 300 hidden units followed by a softmax layer with 10 output units. All of our nets were trained using our own open source neural network library 1.\nAs stated in the methods section, we first train our unsupervised model on 100% of the training images, do supervised fine-tuning, and report overall accuracy on the test set. We 1) present qualitative results of unsupervised learning, 2) show our zero-bias convolutional auto-encoder performs well compared to previous convolutional auto-encoder work by Masci et al. (2011) developed before the popularization of rectified linear units, and zero-bias auto-encoders, 3) we show our analysis of various regularization techniques, and vary the ratio of unsupervised to supervised data, 4) for completeness we report our best results when training on the full CIFAR-10 dataset, however this is not the main point of this work."}, {"heading": "4.2.1 QUALITATIVE RESULTS", "text": "One way in which we ensure the quality of our learned representation is by inspecting the first layer filters. We visualize the filters learned by our model in Figure 1. So that we can directly compare\n1https://github.com/ifp-uiuc/anna\nwith the filters presented in Masci et al. (2011), we trained an additional zero-bias convolutional auto-encoder with filters of size 7x7x3 (instead of 5x5x3) in the first layer. From Figure 1, we can see that, indeed, our model is able to capture interpretable patterns such as Gabor-like oriented edges (both color and intensity) and center-surrounds."}, {"heading": "4.2.2 UNSUPERVISED PRE-TRAINING FOR TANH CAES AND ZERO-BIAS CAES", "text": "For our quantitative experiments, we first compare the performance of the tanh CAE proposed by Masci et al. (2011) with our zero-bias CAE. In their paper, Masci et al. (2011) trained a tanh CNN from a random initialization and compared it with one pre-trained using a tanh CAE. They also added 5% translations as a form of data augmentation. We re-conduct this experiment using a zero-bias CNN trained from a random initialization, and compare it to one pretrained using our zero-bias CAE.\nIn Table 1 we compare the improvements of our model with that of Masci et al. (2011)\u2019s, on various subsets of CIFAR-10. As expected, the\nzero-bias CNN (a ReLU CNN without bias parameters) performs significantly better than the tanh CNN (2.53%, 8.53%, 5.23%). More interestingly, notice that on each subset, compared to Masci et al. (2011) our pre-trained model shows similar or better performance over the randomly initialized CNN. When the ratio of unsupervised to supervised data is high, we experience an 8.44% increase in accuracy as opposed to Masci et al. (2011)\u2019s 3.22% increase."}, {"heading": "4.2.3 ANALYSIS OF REGULARIZATION METHODS", "text": "Next, we analyze how different supervised regularization techniques affect our model\u2019s performance. Specifically, we consider the effects of dropout, data augmentation (via translations and horizontal flips), unsupervised pre-training (with our zero-bias CAE) and their combinations. We compare each regularization technique to a zero-bias CNN trained from random initialization without any regularization (labeled CNN in Figure 2). Figure 2 shows the classification accuracy improvement over CNN for each type of regularization both individually and together.\nWe perform this analysis for subsets of CIFAR-10 with different unsupervised to supervised sample ratios ranging from 50:1 to 1:1, by fixing the unsupervised data size, and varying the number of supervised examples. It is important to note that as this ratio approaches 1:1, the experimental setup favors data augmentation and dropout because the number of virtual supervised samples is larger than number of unsupervised samples.\nIn Figure 2a, where the ratio of unsupervised to supervised samples is 50:1, there are three notable effects: (i) unsupervised pre-training alone yields a larger improvement (4.09%) than data augmentation (2.67%) or dropout (0.59%), (ii) when unsupervised pre-training is combined with either data augmentation or dropout, the improvement is greater than the sum of the individual contributions, (iii) we experience the largest gains (15.86%) when we combine all three forms of regularization.\nWe see that effect (ii) is also observed in the case where the ratio of unsupervised to supervised samples is 10:1 (Figure 2b), and to a lesser extent when the ratio is 5:1 (Figure 2c). Unfortunately, effects (i) and (iii) are not observed when the ratio of unsupervised to supervised samples decreases. We will elaborate on effect (i) below.\nIn Figure 3, we observe that the improvement in performance from unsupervised learning decreases rapidly as the ratio of unsupervised to supervised samples decreases. Surprisingly, when the ratio is 1:1, we see that unsupervised learning actually hurts performance (-0.67%)."}, {"heading": "4.2.4 COMPARISON WITH EXISTING METHODS", "text": "We also compare the performance of our algorithm on the full CIFAR-10 dataset with other techniques in Table 2, though we show above our method performs worse when the ratio of unsupervised to supervised samples is 1:1. We outperform all methods that use unsupervised pre-training (Masci et al. (2011), Coates et al. (2011), Dosovitskiy et al. (2014), Lin and Kung (2014)), however we are not competitive with supervised state-of-the-art. We include some representative supervised methods in Table 2."}, {"heading": "4.3 STL-10", "text": "Next, we assess the effects of unsupervised pre-training on STL-10. From the CIFAR-10 experiments, it is clear unsupervised pre-training can be beneficial if the unsupervised dataset is much larger than the supervised dataset. STL-10 was designed with this in mind, and has a ratio of unsupervised to supervised data of 100:1. So we experimentally show this benefit.\nWe design our network to have structure similar to Dosovitskiy et al. (2014), to ease comparison. The network used consists 3 convolutional layers with 64, 128, and 256 filters in each layer, a fully-connected layer with 512 units, and a softmax layer with 10 output units. We also apply maxpooling layers of size 2x2 after the first two convolutional layers and quadrant pooling after the third convolutional layer.\nWe train the zero-bias CAE on 100,000 unlabeled images. We then fine-tune the network on each of the 10 provided splits of training set, each consisting of 1000 samples (100 samples per class), and evaluate all of them on the test set. The accuracies are subsequently averaged to obtain the final recognition accuracy. Similar to our CIFAR-10 experiments, we also train a zero-bias CNN with the same structure as our zero-bias CAE on each of the splits to further highlight the benefits of unsupervised learning.\nTable 3 presents our results on the STL10 dataset and compares them with other methods. As expected, unsupervised pretraining gives a 3.87% increase over the randomly initialized CNN."}, {"heading": "4.3.1 ADDITIONAL DATA AUGMENTATION: COLOR AND CONTRAST", "text": "The current best result on STL-10 (Dosovitskiy et al. (2014)) makes extensive use of additional data augmentation including: scaling, rotation, color and two forms of contrast. They do not perform these augmentations during supervised training, but during a discriminative unsupervised feature learning period. We test the regularizing effects of these additional augmentations when applied directly to supervised training, and test how these regularization effects hold up when combined with with unsupervised pre-training. To do this, we use some of these additional data-augmentations during our supervised training: color augmentation and contrast augmentation.\nColor augmentation: The images are represented in HSV color space (h, s, v). Here we generate a single random number for each image and add it to the hue value for each pixel like so:\na \u223c Uniform(\u22120.1, 0.1) (8) h = h+ a (9)\nContrast augmentation: Here we generate six random numbers for each image, with the following distributions:\na, d \u223c Uniform(0.7, 1.4) (10) b, e \u223c Uniform(0.25, 4) (11) c, f \u223c Uniform(\u22120.1, 0.1) (12)\nAnd use them to modify the saturation and value for every pixel in the image, like so:\ns = asb + c (13) v = dse + f (14)\nWe find that a) additional data-augmentation is incredibly helpful, increasing accuracy by 6.5%, b) unsupervised pre-training still maintains an advantage (1.69%)."}, {"heading": "5 CONCLUSIONS", "text": "We present a new type of convolutional auto-encoder that has zero-bias and ReLU activations and achieves superior performance to previous methods. We conduct thorough experiments on CIFAR10 to analyze the effects of unsupervised pre-training as a form of regularization when used in isolation and in combination with supervised forms of regularization such as data augmentation and dropout. We observe that, indeed, unsupervised pre-training can provide a large gain in performance when the ratio of unsupervised to supervised samples is large. Finally, we verify our findings by applying our model to STL-10, a dataset with far more unlabeled samples than labeled samples (100:1). We find that with additional regularization, via color augmentation, our method is able to achieve nearly state-of-the-art results.\nCODE\nAll experiments were run using our own open source library Anna, which can be found at: https: //github.com/ifp-uiuc/anna\nCode to reproduce the experiments can be found at: https://github.com/ifp-uiuc/ an-analysis-of-unsupervised-pre-training-iclr-2015"}, {"heading": "ACKNOWLEDGMENTS", "text": "This material is based upon work supported by the National Science Foundation under Grant No. 392 NSF IIS13-18971. The two Tesla K40 GPUs used for this research were donated by the NVIDIA Corporation. We would like to acknowledge Theano (Bergstra et al. (2010)) and Pylearn2 (Goodfellow et al. (2013a)), on which our code is based. Also, we would like to thank Shiyu Chang for many helpful discussions and suggestions."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Unsupervised feature learning for rgb-d based object recognition", "author": ["Liefeng Bo", "Xiaofeng Ren", "Dieter Fox"], "venue": "In Experimental Robotics,", "citeRegEx": "Bo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bo et al\\.", "year": 2013}, {"title": "Selecting receptive fields in deep networks", "author": ["Adam Coates", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Coates and Ng.,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Andrew Y Ng", "Honglak Lee"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2014}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "arXiv preprint arXiv:1311.2524,", "citeRegEx": "Girshick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2013}, {"title": "Pylearn2: a machine learning research", "author": ["Ian J Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "M Ranzato", "Yann LeCun"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Stable and efficient representation learning with nonnegativity constraints", "author": ["Tsung-Han Lin", "H.T. Kung"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Lin and Kung.,? \\Q2014\\E", "shortCiteRegEx": "Lin and Kung.", "year": 2014}, {"title": "Convolutional kernel networks", "author": ["Julien Mairal", "Piotr Koniusz", "Zaid Harchaoui", "Cordelia Schmid"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mairal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2014}, {"title": "Stacked convolutional autoencoders for hierarchical feature extraction", "author": ["Jonathan Masci", "Ueli Meier", "Dan Cire\u015fan", "J\u00fcrgen Schmidhuber"], "venue": "In Artificial Neural Networks and Machine Learning\u2013", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Zero-bias autoencoders and the benefits of co-adapting features", "author": ["Roland Memisevic", "Kishore Konda", "David Krueger"], "venue": "arXiv preprint arXiv:1402.3337,", "citeRegEx": "Memisevic et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2014}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Salah Rifai", "Pascal Vincent", "Xavier Muller", "Xavier Glorot", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Multi-task bayesian optimization", "author": ["Kevin Swersky", "Jasper Snoek", "Ryan P Adams"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Swersky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2013}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Deconvolutional networks", "author": ["Matthew D Zeiler", "Dilip Krishnan", "Graham W Taylor", "Robert Fergus"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zeiler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Matthew D Zeiler", "Graham W Taylor", "Rob Fergus"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "Recent work shows that convolutional neural networks (CNNs) can achieve state-of-the-art performance for object classification (Krizhevsky et al. (2012)) and object detection (Girshick et al.", "startOffset": 128, "endOffset": 153}, {"referenceID": 4, "context": "(2012)) and object detection (Girshick et al. (2013)), when there is enough training data.", "startOffset": 30, "endOffset": 53}, {"referenceID": 4, "context": "(2012)) and object detection (Girshick et al. (2013)), when there is enough training data. However, in many cases there is a dearth of labeled data. In these cases regularization is necessary for good results. The most common types of regularization are data augmentations (Krizhevsky et al. (2012); Dosovitskiy et al.", "startOffset": 30, "endOffset": 299}, {"referenceID": 4, "context": "(2012); Dosovitskiy et al. (2014)) and dropout (Hinton et al.", "startOffset": 8, "endOffset": 34}, {"referenceID": 4, "context": "(2012); Dosovitskiy et al. (2014)) and dropout (Hinton et al. (2012)).", "startOffset": 8, "endOffset": 69}, {"referenceID": 4, "context": "(2012); Dosovitskiy et al. (2014)) and dropout (Hinton et al. (2012)). Another form of regularization, unsupervised pre-training (Hinton et al. (2006); Bengio et al.", "startOffset": 8, "endOffset": 151}, {"referenceID": 0, "context": "(2006); Bengio et al. (2007); Erhan et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2006); Bengio et al. (2007); Erhan et al. (2010)), has recently fallen out of favor.", "startOffset": 8, "endOffset": 50}, {"referenceID": 0, "context": "(2006); Bengio et al. (2007); Erhan et al. (2010)), has recently fallen out of favor. While there has been significant work in unsupervised learning, most of these works came before rectified linear units, which significantly help training deep supervised neural networks, and before simpler regularization schemes for unsupervised learning, such as zero-bias with linear encoding for auto-encoders (Memisevic et al. (2014)).", "startOffset": 8, "endOffset": 424}, {"referenceID": 19, "context": "Some forms of regularization include adding noise to the input units (Vincent et al. (2010)) and requiring the hidden unit activations be sparse (Coates et al.", "startOffset": 70, "endOffset": 92}, {"referenceID": 4, "context": "(2010)) and requiring the hidden unit activations be sparse (Coates et al. (2011)) or have small derivatives (Rifai et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 4, "context": "(2010)) and requiring the hidden unit activations be sparse (Coates et al. (2011)) or have small derivatives (Rifai et al. (2011)).", "startOffset": 61, "endOffset": 130}, {"referenceID": 4, "context": "(2010)) and requiring the hidden unit activations be sparse (Coates et al. (2011)) or have small derivatives (Rifai et al. (2011)). These models are known as de-noising, sparse, and contractive auto-encoders respectively. A more recent work by Memisevic et al. (2014) showed that training an auto-encoder with rectified linear units (ReLU) caused the activations to form tight clusters due to having negative bias values.", "startOffset": 61, "endOffset": 268}, {"referenceID": 13, "context": "Convolutional neural networks (CNNs) (LeCun et al. (1998); Lee et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 13, "context": "Convolutional neural networks (CNNs) (LeCun et al. (1998); Lee et al. (2009)) present a way to reduce the number of connections by having each hidden unit only be responsible for a small local neighborhood of visible units.", "startOffset": 38, "endOffset": 77}, {"referenceID": 11, "context": "There have been several works in this area including Jarrett et al. (2009) and Zeiler et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 11, "context": "There have been several works in this area including Jarrett et al. (2009) and Zeiler et al. (2010). Both rely on sparse coding to force their unsupervised learning to learn non-trival solutions.", "startOffset": 53, "endOffset": 100}, {"referenceID": 11, "context": "There have been several works in this area including Jarrett et al. (2009) and Zeiler et al. (2010). Both rely on sparse coding to force their unsupervised learning to learn non-trival solutions. Zeiler et al. (2011) extended this work by introducing pooling/unpooling and visualizing how individual feature maps at different layers influenced specific portions of the reconstruction.", "startOffset": 53, "endOffset": 217}, {"referenceID": 17, "context": "A later work by Masci et al. (2011) trained deep feed forward convolutional auto-encoders, using only max-pooling and saturating tanh non-linearities as a form of regularization, while still showing a modest improvement over randomly initialized CNNs.", "startOffset": 16, "endOffset": 36}, {"referenceID": 13, "context": "While tanh was a natural choice at the time, Krizhevsky et al. (2012) showed that ReLUs are more suitable for learning given their non-saturating behavior.", "startOffset": 45, "endOffset": 70}, {"referenceID": 21, "context": "Our model is similar to the deconvolutional network in Zeiler et al. (2011) where the cost we minimize at each layer is the mean square error on the original image.", "startOffset": 55, "endOffset": 76}, {"referenceID": 21, "context": "Our model is similar to the deconvolutional network in Zeiler et al. (2011) where the cost we minimize at each layer is the mean square error on the original image. However, unlike the network in Zeiler et al. (2011), our method does not use any form of sparse coding.", "startOffset": 55, "endOffset": 217}, {"referenceID": 18, "context": "Our model also is similar to that of Masci et al. (2011), however we improve upon it by introducing regularization in the convolutional layers through the use of zero-biases and ReLUs as discussed in Memisevic et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 18, "context": "Our model also is similar to that of Masci et al. (2011), however we improve upon it by introducing regularization in the convolutional layers through the use of zero-biases and ReLUs as discussed in Memisevic et al. (2014). We now describe the model architecture in detail.", "startOffset": 37, "endOffset": 224}, {"referenceID": 23, "context": "Like Zeiler et al. (2011), we compute the cost by taking the mean squared error between the original image and the network\u2019s reconstruction of the input.", "startOffset": 5, "endOffset": 26}, {"referenceID": 19, "context": "Unlike the work by Memisevic et al. (2014) which analyzes fullyconnected auto-encoders, our work is the first, to our knowledge, that trains zero-bias CAEs for unsupervised learning.", "startOffset": 19, "endOffset": 43}, {"referenceID": 11, "context": "We run experiments on two natural image datasets, CIFAR-10 (Krizhevsky and Hinton (2009)) and STL-10 (Coates et al.", "startOffset": 60, "endOffset": 89}, {"referenceID": 4, "context": "We run experiments on two natural image datasets, CIFAR-10 (Krizhevsky and Hinton (2009)) and STL-10 (Coates et al. (2011)).", "startOffset": 102, "endOffset": 123}, {"referenceID": 18, "context": "On CIFAR-10, we train a network with structure similar to Masci et al. (2011), so that we can directly show the benefits of our modifications.", "startOffset": 58, "endOffset": 78}, {"referenceID": 18, "context": "On CIFAR-10, we train a network with structure similar to Masci et al. (2011), so that we can directly show the benefits of our modifications. The network consists of three convolutional layers with 96, 144, and 192 filters respectively. The filters in the first two layers are of size 5x5 while the filters in the third layer are of size 3x3. We also add 2x2 max pooling layers after the first two convolutional layers. There is also a full-connected layer with 300 hidden units followed by a softmax layer with 10 output units. All of our nets were trained using our own open source neural network library 1. As stated in the methods section, we first train our unsupervised model on 100% of the training images, do supervised fine-tuning, and report overall accuracy on the test set. We 1) present qualitative results of unsupervised learning, 2) show our zero-bias convolutional auto-encoder performs well compared to previous convolutional auto-encoder work by Masci et al. (2011) developed before the popularization of rectified linear units, and zero-bias auto-encoders, 3) we show our analysis of various regularization techniques, and vary the ratio of unsupervised to supervised data, 4) for completeness we report our best results when training on the full CIFAR-10 dataset, however this is not the main point of this work.", "startOffset": 58, "endOffset": 986}, {"referenceID": 18, "context": "with the filters presented in Masci et al. (2011), we trained an additional zero-bias convolutional auto-encoder with filters of size 7x7x3 (instead of 5x5x3) in the first layer.", "startOffset": 30, "endOffset": 50}, {"referenceID": 18, "context": ") For direct comparison with tanh CAE please see Masci et al. (2011) Figure 2c.", "startOffset": 49, "endOffset": 69}, {"referenceID": 18, "context": ") For direct comparison with tanh CAE please see Masci et al. (2011) Figure 2c. For our quantitative experiments, we first compare the performance of the tanh CAE proposed by Masci et al. (2011) with our zero-bias CAE.", "startOffset": 49, "endOffset": 195}, {"referenceID": 18, "context": ") For direct comparison with tanh CAE please see Masci et al. (2011) Figure 2c. For our quantitative experiments, we first compare the performance of the tanh CAE proposed by Masci et al. (2011) with our zero-bias CAE. In their paper, Masci et al. (2011) trained a tanh CNN from a random initialization and compared it with one pre-trained using a tanh CAE.", "startOffset": 49, "endOffset": 255}, {"referenceID": 18, "context": ") For direct comparison with tanh CAE please see Masci et al. (2011) Figure 2c. For our quantitative experiments, we first compare the performance of the tanh CAE proposed by Masci et al. (2011) with our zero-bias CAE. In their paper, Masci et al. (2011) trained a tanh CNN from a random initialization and compared it with one pre-trained using a tanh CAE. They also added 5% translations as a form of data augmentation. We re-conduct this experiment using a zero-bias CNN trained from a random initialization, and compare it to one pretrained using our zero-bias CAE. In Table 1 we compare the improvements of our model with that of Masci et al. (2011)\u2019s, on various subsets of CIFAR-10.", "startOffset": 49, "endOffset": 655}, {"referenceID": 18, "context": ") For direct comparison with tanh CAE please see Masci et al. (2011) Figure 2c. For our quantitative experiments, we first compare the performance of the tanh CAE proposed by Masci et al. (2011) with our zero-bias CAE. In their paper, Masci et al. (2011) trained a tanh CNN from a random initialization and compared it with one pre-trained using a tanh CAE. They also added 5% translations as a form of data augmentation. We re-conduct this experiment using a zero-bias CNN trained from a random initialization, and compare it to one pretrained using our zero-bias CAE. In Table 1 we compare the improvements of our model with that of Masci et al. (2011)\u2019s, on various subsets of CIFAR-10. As expected, the zero-bias CNN (a ReLU CNN without bias parameters) performs significantly better than the tanh CNN (2.53%, 8.53%, 5.23%). More interestingly, notice that on each subset, compared to Masci et al. (2011) our pre-trained model shows similar or better performance over the randomly initialized CNN.", "startOffset": 49, "endOffset": 909}, {"referenceID": 18, "context": ") For direct comparison with tanh CAE please see Masci et al. (2011) Figure 2c. For our quantitative experiments, we first compare the performance of the tanh CAE proposed by Masci et al. (2011) with our zero-bias CAE. In their paper, Masci et al. (2011) trained a tanh CNN from a random initialization and compared it with one pre-trained using a tanh CAE. They also added 5% translations as a form of data augmentation. We re-conduct this experiment using a zero-bias CNN trained from a random initialization, and compare it to one pretrained using our zero-bias CAE. In Table 1 we compare the improvements of our model with that of Masci et al. (2011)\u2019s, on various subsets of CIFAR-10. As expected, the zero-bias CNN (a ReLU CNN without bias parameters) performs significantly better than the tanh CNN (2.53%, 8.53%, 5.23%). More interestingly, notice that on each subset, compared to Masci et al. (2011) our pre-trained model shows similar or better performance over the randomly initialized CNN. When the ratio of unsupervised to supervised data is high, we experience an 8.44% increase in accuracy as opposed to Masci et al. (2011)\u2019s 3.", "startOffset": 49, "endOffset": 1139}, {"referenceID": 18, "context": "Table 1: Comparison between Tanh CAE (Masci et al. (2011)) and our model on various subsets of CIFAR-10.", "startOffset": 38, "endOffset": 58}, {"referenceID": 18, "context": "Unsupervised to supervised ratio (Samples per Class) 50:1 (100) 10:1 (500) 5:1 (1000) 1:1 (5000) Tanh CNN - Masci et al. (2011) 44.", "startOffset": 108, "endOffset": 128}, {"referenceID": 18, "context": "Unsupervised to supervised ratio (Samples per Class) 50:1 (100) 10:1 (500) 5:1 (1000) 1:1 (5000) Tanh CNN - Masci et al. (2011) 44.48 % \u2014 64.77 % 77.50 % Tanh CAE - Masci et al. (2011) 47.", "startOffset": 108, "endOffset": 185}, {"referenceID": 15, "context": "We outperform all methods that use unsupervised pre-training (Masci et al. (2011), Coates et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 4, "context": "(2011), Coates et al. (2011), Dosovitskiy et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 4, "context": "(2011), Coates et al. (2011), Dosovitskiy et al. (2014), Lin and Kung (2014)), however we are not competitive with supervised state-of-the-art.", "startOffset": 8, "endOffset": 56}, {"referenceID": 4, "context": "(2011), Coates et al. (2011), Dosovitskiy et al. (2014), Lin and Kung (2014)), however we are not competitive with supervised state-of-the-art.", "startOffset": 8, "endOffset": 77}, {"referenceID": 5, "context": "We design our network to have structure similar to Dosovitskiy et al. (2014), to ease comparison.", "startOffset": 51, "endOffset": 77}, {"referenceID": 5, "context": "The current best result on STL-10 (Dosovitskiy et al. (2014)) makes extensive use of additional data augmentation including: scaling, rotation, color and two forms of contrast.", "startOffset": 35, "endOffset": 61}, {"referenceID": 11, "context": "Algorithm Accuracy Convolutional Auto-encoders - Masci et al. (2011) 79.", "startOffset": 49, "endOffset": 69}, {"referenceID": 3, "context": "20 % Single layer K-means - Coates et al. (2011) 79.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "60 % Convolutional K-means Networks - Coates and Ng (2011) 82.", "startOffset": 38, "endOffset": 59}, {"referenceID": 3, "context": "60 % Convolutional K-means Networks - Coates and Ng (2011) 82.00 % Exemplar CNN - Dosovitskiy et al. (2014) 82.", "startOffset": 38, "endOffset": 108}, {"referenceID": 3, "context": "60 % Convolutional K-means Networks - Coates and Ng (2011) 82.00 % Exemplar CNN - Dosovitskiy et al. (2014) 82.00 % Convolutional Kernel Networks - Mairal et al. (2014) 82.", "startOffset": 38, "endOffset": 169}, {"referenceID": 3, "context": "60 % Convolutional K-means Networks - Coates and Ng (2011) 82.00 % Exemplar CNN - Dosovitskiy et al. (2014) 82.00 % Convolutional Kernel Networks - Mairal et al. (2014) 82.18 % NOMP - Lin and Kung (2014) 82.", "startOffset": 38, "endOffset": 204}, {"referenceID": 3, "context": "60 % Convolutional K-means Networks - Coates and Ng (2011) 82.00 % Exemplar CNN - Dosovitskiy et al. (2014) 82.00 % Convolutional Kernel Networks - Mairal et al. (2014) 82.18 % NOMP - Lin and Kung (2014) 82.90 % Max-Out Networks - Goodfellow et al. (2013b) 90.", "startOffset": 38, "endOffset": 257}, {"referenceID": 3, "context": "60 % Convolutional K-means Networks - Coates and Ng (2011) 82.00 % Exemplar CNN - Dosovitskiy et al. (2014) 82.00 % Convolutional Kernel Networks - Mairal et al. (2014) 82.18 % NOMP - Lin and Kung (2014) 82.90 % Max-Out Networks - Goodfellow et al. (2013b) 90.65 % Network In Network - Lin et al. (2013) 91.", "startOffset": 38, "endOffset": 304}, {"referenceID": 3, "context": "60 % Convolutional K-means Networks - Coates and Ng (2011) 82.00 % Exemplar CNN - Dosovitskiy et al. (2014) 82.00 % Convolutional Kernel Networks - Mairal et al. (2014) 82.18 % NOMP - Lin and Kung (2014) 82.90 % Max-Out Networks - Goodfellow et al. (2013b) 90.65 % Network In Network - Lin et al. (2013) 91.20 % Deeply-Supervised Nets - Lee et al. (2014) 91.", "startOffset": 38, "endOffset": 355}, {"referenceID": 2, "context": "Algorithm Accuracy Convolutional K-means Networks - Coates and Ng (2011) 60.", "startOffset": 52, "endOffset": 73}, {"referenceID": 2, "context": "Algorithm Accuracy Convolutional K-means Networks - Coates and Ng (2011) 60.1 % \u00b1 1.0 % Convolutional Kernel Networks - Mairal et al. (2014) 62.", "startOffset": 52, "endOffset": 141}, {"referenceID": 2, "context": "32 % Hierarchical Matching Pursuit (HMP) - Bo et al. (2013) 64.", "startOffset": 43, "endOffset": 60}, {"referenceID": 2, "context": "32 % Hierarchical Matching Pursuit (HMP) - Bo et al. (2013) 64.5 % \u00b1 1.0 % NOMP - Lin and Kung (2014) 67.", "startOffset": 43, "endOffset": 102}, {"referenceID": 2, "context": "32 % Hierarchical Matching Pursuit (HMP) - Bo et al. (2013) 64.5 % \u00b1 1.0 % NOMP - Lin and Kung (2014) 67.9 % \u00b1 0.6 % Multi-task Bayesian Optimization - Swersky et al. (2013) 70.", "startOffset": 43, "endOffset": 174}, {"referenceID": 2, "context": "32 % Hierarchical Matching Pursuit (HMP) - Bo et al. (2013) 64.5 % \u00b1 1.0 % NOMP - Lin and Kung (2014) 67.9 % \u00b1 0.6 % Multi-task Bayesian Optimization - Swersky et al. (2013) 70.1 % \u00b1 0.6 % Exemplar CNN - Dosovitskiy et al. (2014) 72.", "startOffset": 43, "endOffset": 230}, {"referenceID": 1, "context": "We would like to acknowledge Theano (Bergstra et al. (2010)) and Pylearn2 (Goodfellow et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 1, "context": "We would like to acknowledge Theano (Bergstra et al. (2010)) and Pylearn2 (Goodfellow et al. (2013a)), on which our code is based.", "startOffset": 37, "endOffset": 101}], "year": 2015, "abstractText": "Convolutional neural networks perform well on object recognition because of a number of recent advances: rectified linear units (ReLUs), data augmentation, dropout, and large labelled datasets. Unsupervised data has been proposed as another way to improve performance. Unfortunately, unsupervised pre-training is not used by state-of-the-art methods leading to the following question: Is unsupervised pre-training still useful given recent advances? If so, when? We answer this in three parts: we 1) develop an unsupervised method that incorporates ReLUs and recent unsupervised regularization techniques, 2) analyze the benefits of unsupervised pre-training compared to data augmentation and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised samples, 3) verify our findings on STL-10. We discover unsupervised pre-training, as expected, helps when the ratio of unsupervised to supervised samples is high, and surprisingly, hurts when the ratio is low. We also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on STL-10.", "creator": "LaTeX with hyperref package"}}}