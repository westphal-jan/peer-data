{"id": "1412.7009", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Generative Class-conditional Autoencoders", "abstract": "Recent work included Bengio belles-lettres --. (1960) proposes next methods procedure years vicissitude autoencoders which involves learning the accelerate mci mainly little Markov toy. The begin monopoly, example customisable, which favor despite capacity for newest specific electronic. In order when compose utilized accurately later conditional increment, find sign this works, rest infinitely two algorithmically, but gated autoencoders (Memisevic, 1978 ), The expansion model is either to value overwhelming serve - conditional batches during trained on both the MNIST such TFD non-clinical.", "histories": [["v1", "Mon, 22 Dec 2014 14:57:05 GMT  (2522kb,D)", "http://arxiv.org/abs/1412.7009v1", null], ["v2", "Sat, 28 Feb 2015 00:16:55 GMT  (2523kb,D)", "http://arxiv.org/abs/1412.7009v2", null], ["v3", "Thu, 9 Apr 2015 01:54:33 GMT  (2523kb,D)", "http://arxiv.org/abs/1412.7009v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["jan rudy", "graham taylor"], "accepted": true, "id": "1412.7009"}, "pdf": {"name": "1412.7009.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Graham Taylor"], "emails": ["jrudy@uoguelph.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "In the field of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014). However, recent work has rekindled interest in generative models of data (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013; Kingma & Welling, 2014; Goodfellow et al., 2014a). The recently proposed sampling procedure for denoising autoencoders (Bengio et al., 2013) and their generalization to Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) presents a novel training procedure which, instead of attempting to maximize the likelihood of the data under the model, amounts to learning the transition operator of a Markov chain (Bengio et al., 2013).\nAlthough these models have shown both theoretically and empirically to have the capacity to model the underlying data generating distribution, the unimodal transition operator learned in (Bengio et al., 2013) and (Bengio & Thibodeau-Laufer, 2013) limits the types of distributions that can be modeled successfully. One way to address this issue is by adopting an alternative generative model such as the Neural Autoregressive Density Estimator (NADE) as the output distribution of the transition operator (Ozair et al., 2013). In this paper, we propose a simpler approach. When labeled data is available, we can use the label information in order to carve up the landscape of the data distribution.\nThis work begins by presenting an overview of related work, including a treatment of autoencoders, denoising autoencoders, autoencoders as generative models and gated autoencoders. Next, we propose a class-conditional gated autoencoder along with training and sampling procedures based on the work of Bengio et al. (2013). Finally, we present experimental results of our model on two image-based datasets."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 AUTOENCODERS", "text": "An autoencoder is a feed-forward neural network which aims to minimize the reconstruction error of an input data vector via a latent representation. This can be interpreted as the composition of two learned functions, the encoder function f and the decoder function g. The encoder function f is a mapping from input space onto the representation space. Formally, given input vector x \u2208 RnX ,\nar X\niv :1\n41 2.\n70 09\nv1 [\ncs .N\nE ]\n2 2\nD ec\n2 01\n4\ninput weights W \u2208 RnH\u00d7nX and hidden biases bh \u2208 RnH , f(x) = sH(Wx+ b) (1)\nwhere nH is the dimension of the hidden representation and sH is an activation function. The activation is a non-linear function, often from the sigmoidal family (e.g. the logistic or hyperbolic tangent functions) or a piecewise linear function (e.g. rectified linear). The decoder then projects this representation h = f(x) back onto the input space,\nx\u0302 = g(h) = sO(W \u2032h+ b\u2032) (2)\nwhere x\u0302 is the reconstruction of the input vector, sO is the output activation function, W\u2032 \u2208 RnX\u00d7nH are the output weights and b\u2032 \u2208 RnX are the output biases. In order to restrict the number of free parameters of the model, the input and output weight matrices are often \u2018tied\u2019 such that W\u2032 = WT .\nThe model parameters (i.e. weights and biases) are updated via a gradient-based optimization in order to minimize a loss function L(x) based on reconstruction error. The choice of loss function depends on the data domain. When dimensions of x are real-valued, a typical choice of L(x) is the squared error, i.e. L(x) = \u2211nX i=1(xi \u2212 x\u0302i)2. When x is binary, a more appropriate loss function is\nthe cross-entropy loss, i.e. L(x) = \u2211nX i=1 xi log x\u0302i + (1\u2212 xi) log(1\u2212 x\u0302i)."}, {"heading": "2.2 DENOISING AUTOENCODERS", "text": "When the dimension of the hidden representation nH is smaller than the dimension of the data space nX , the learning procedure encourages the model to learn the underlying structure of the data. The data representation can exploit structure in order to compress the data to fewer dimensions than the original space. As such, each dimension of the representation space is interpretable as a useful feature of the data. For example, when trained on images of handwritten digits these features are interpretable as strokes.\nHowever, when nH \u2265 nX , the autoencoder can achieve perfect reconstruction without learning useful features in the data by simply learning the identity function. In this so-called \u201covercomplete\u201d setup, regularization is essential. Among the various kinds of regularized autoencoders, the denoising autoencoder (DAE) (Vincent et al., 2008) is among the most popular and well-understood. Instead of reconstructing the data from the actual input vector x, the DAE attempts to reconstruct the original input from an encoding of a corrupted version, f(x\u0303). This effectively prohibits the model from learning a trivial solution while learning robust features of the data.\nThe corruption procedure is defined as a sample from the conditional distribution x\u0303 \u223c C(x\u0303|x). Here, x\u0303 is a noisy version of x where the type of noise is defined by the distribution C(x\u0303|x). Typical choices of noise are\n1. Gaussian noise - adding i \u223c N(0, \u03c3) to each dimension. 2. Masking noise - setting xi = 0 with probability \u03c1, 3. Salt-and-pepper noise - similar to masking noise, but corrupting xi with probability \u03c1 and\neach corrupted dimension is set to xi = 0 or xi = 1 with probability 0.5.\nApart from preventing the model to learn a trivial representation of the input by acting as a form or regularization, the DAE can be interpreted as a means of learning the manifold of the underlying data generating distribution (Vincent et al., 2008). Under the assumption that the data lies along a low dimensional manifold in the input space, the corruption procedure is a means of moving the training data away from this manifold. In order to successfully denoise the corrupted input, the model must learn to project the corrupted input back onto the manifold. Under this interpretation the hidden representation can be interpreted as a coordinate system of manifolds (Vincent et al., 2008)."}, {"heading": "2.3 DENOISING AUTOENCODERS AS GENERATIVE MODELS", "text": "Although DAEs are useful as a means of pre-training discriminative models, especially when stacked to form deep models (Vincent et al., 2010), recent work by Bengio et al. (2013) has shown that DAEs\nand their variants locally characterize the data generating density. This provides an important link between DAEs and probabilistic generative models.\nWe define observed data x such that x \u223c P(x) where P(x) is the true data distribution and we define C(x\u0303|x) as the conditional distribution of the corruption process. When such models are trained using a loss function that can be interpreted as log-likelihood, by predicting x given x\u0303, the model is learning the conditional distribution P\u03b8(x|x\u0303) (where \u03b8 represents the parameters of the model).\nIn order to generate samples from the model, one simply forms a Markov chain which alternately samples from learned distribution and the corruption distribution (Bengio et al., 2013). Where xt is the state of the Markov chain at time t, then xt \u223c P\u03b8(x|x\u0303t\u22121) and x\u0303t \u223c C(x\u0303|xt). In other words, samples can be generated by alternating between the corruption process and the denoising function learned by the autoencoder.\nNotice that this is not a true Gibbs sampling procedure, as (xt, x\u0303t\u22121) may not share the same asymptotic distribution as (xt, x\u0303t). Regardless, theoretical results indicate that under some conditions elaborated on in Sec. 3, the asymptotic distribution of the generated samples converges to the datagenerating distribution (Bengio et al., 2013).\nAlthough the above procedure does produce convincing samples, it will also generate spurious examples that lie far from any of the training data. This is because the training procedure does not sufficiently explore the input space. Under the manifold interpretation described above, the corruption procedure defines a region around each example that is explored during training, the size of which is determined by the amount of the corruption (e.g. \u03c3 in the case of Gaussian noise and \u03c1 for masking or salt and pepper noise). This can leave much of the input space unexplored, allowing the model to place non-insignificant amounts of probability mass (i.e. spurious modes) in regions of the space that lie far from any training example.\nOne solution involves using large or increasing amounts of noise during training, however this results in a naive search of the space. A more efficient procedure called \u201cwalkback training\u201d is described in (Bengio et al., 2013) and bears resemblance to contrastive divergence training of restricted Boltzman machines. Instead of defining the loss as the reconstruction cost of a single step of corruption and reconstruction chain, walkback training defines a series of k reconstructions via a random walk originating at the training example. Each reconstruction is corrupted and subsequently reconstructed, where the final cost is defined as the sum of the reconstruction costs of each intermediate reconstruction. Since the training procedure mimics that of the sampling procedure, walkback training is a means of seeking out these spurious modes and redistributing their probability mass back towards the manifold."}, {"heading": "2.4 GATED AUTOENCODERS", "text": "Gated autoencoders (GAE), also called Relational autoencoders, are an extension the autoencoder framework which learn relations on input-output pairs x \u2208 RnX given y \u2208 RnY (Memisevic, 2013). Instead of defining a fixed weight matrix W \u2208 RnH\u00d7nX , the GAE learns a function w(y) where the model weights are modulated by the value of the conditioning variable y.\nThe naive implementation involves constructing a weight tensor W \u2208 RnH\u00d7nX\u00d7nY and defining\nwij(y) = nY\u2211 k=1 Wijkyk (3)\nwhere the subscripts indicate indexing. Under this model, the encoder in Equation 1 becomes\nh = f(x,y) = sH(w(y)x+ b) (4)\nHowever, this requires storing and learning (nX \u00d7 nY \u00d7 nH) model parameters. In practice, this is infeasible for all but the smallest problems as the number of weights is cubic in the number of units (assuming nX , nY and nH are roughly equal). However, we can restrict the interactions between x, y, and h by first projecting each onto factors FX , FY , FH \u2208 RnF and allowing only element-wise interaction between the factors. Instead of a quadratic number of weights needed in the naive method above, the factored model is parameterized via three weight matrices: WX \u2208\nRnX\u00d7nF ,WY \u2208 RnY \u00d7nF and WH \u2208 RnF\u00d7nH . The hidden representation under the factored model is defined as\nh = f(x,y) = sH\n(( WH )T ( WXx\u2297WY y ) + bH ) (5)\nwhere \u2297 denotes elementwise multiplication and bH \u2208 RnH is the hidden bias. Notice that the encoder is a function over both x and y. Similarly, the decoder function will also be over two input variables, the choice of which being dependant on which of x and y are to be reconstructed. When learning a conditional model, one of the input variables will be held fixed. For example, the reconstruction of x given y is defined as\nx\u0302 = g(h,y) = sO\n(( WX )T ( WHh\u2297WY y ) + bX ) (6)\nwhere bH \u2208 RnX is the output bias and sO is the output activation function. Equations 5 and 6 describe a symmetric model where the encoder and decoder share the same set of weight matrices. However, this is not a hard requirement and various regimes of tied vs untied weights for gated models have been explored by Alain & Olivier (2013).\nLike the traditional autoencoder, the GAE is typically trained with a denoising criterion where the noise is applied to both x and y inputs. Yet where a traditional autoencoder learns features of the input, a GAE can learn relationships between its two inputs. For example, when trained on pairs of images where one is translated version of the other, the filters learned by the input factors resemble phase-shifted Fourier components (Memisevic, 2013).\nThe loss function is defined in much the same way as the classical autoencoder described above, using squared error on real-valued inputs and cross entropy loss where the input values are binary. As all operations are differentiable, the model can be trained via stochastic gradient descent while making use of any optimization techniques (e.g. momentum, adaptive learning rates, RMSprop, etc.) that have been developed for neural network training."}, {"heading": "3 GATED AUTOENCODERS AS CLASS-CONDITIONAL GENERATIVE MODELS", "text": "The sampling procedure proposed by Bengio et al. (2013) for classical denoising autoencoders can also be applied to a GAE. Here, we define the true data distribution as the conditional distribution\nP(x|y). Here x \u2208 RnX is an input data point and y \u2208 RnY is the associated class label in a \u2018onehot\u2019 encoding. Although GAE training typically applies noise to both x and y, we will examine the case where the corruption procedure is applied to x only. Thus, the corruption distribution is the same as in the DAE framework, namely the noise procedure draws samples from C(x\u0303|x). By choosing a loss function which is interpretable as log-likelihood, the GAE learns the conditional distribution P\u03b8(x|x\u0303,y) Like the sampling procedure for DAEs, the Markov chain formed by alternating samples from xt \u223c P\u03b8(x|x\u0303t\u22121,y) and x\u0303t \u223c C(x\u0303|x) will generate samples from the true distribution P(x|y). During training, we can also apply a class-conditional version of the walkback training algorithm to seek out and squash any spurious modes of our model.\nBengio et al. (2013) provide a proof of the following theorem:\nTheorem 1. If P\u03b8(x|x\u0303) is a consistent estimator of the true conditional distribution P(x|x\u0303) and Tn defines an ergodic Markov chain, then as the number of examples n \u2192 \u221e, the asymptotic distribution \u03c0n(x) of the generated samples converges to the data-generating distribution P(x).\nFor the conditional case of GAE the same arguments hold, where each of the following substitutions are made: P\u03b8(x|x\u0303) with P\u03b8(x|x\u0303,y), P(x|x\u0303) with P(x|x\u0303,y) and P(x) with P(x|y). The arguments for consistency and ergoticity can also be made in the same manner as those in (Bengio et al., 2013).\nGeometrically, this model can be seen as learning a conditional manifold of the data. Like the DAE, the model learns to correct the noisy input by pushing it back towards the data manifold. However, the model makes use of the class labels in order to learn a separate manifold for each class label. The FY factors, via their multiplicative interactions, scale the features learned by the FX factors depending on the class. It is akin to learning a DAE for each class, yet the gated model can make use of cross-class structure by sharing weights between each of the class-specific models.\nWhen training on images of hand written digits, for example, the \u2018tail\u2019 of a 4, 7, or 9 may all make use of the same feature learned by one of the FX factors. As such, the FY factor for these classes would have a relatively large value for this shared feature. However, this tail feature may not be useful for reconstruction of a 3, 8 or 6. For these classes, the FY values that correspond to the tail feature can learn to down-weight the importance of this tail feature. Under this interpretation, the FY factors are a means of weighting each of the FX factors dependent on the conditioning class label."}, {"heading": "4 EXPERIMENTS", "text": "We demonstrate the generative properties of the class-conditional GAE model on two datasets: binarized MNIST (LeCun & Cortes, 1998) and the Toronto Face Database (TFD) (Susskind et al., 2010). The MNIST database of handwritten digits consists of 60,000 training examples and 10,000 test examples. Pixel intensities of each 28 \u00d7 28 image have been scaled to [0, 1] and thresholded at 0.5, such that each pixel takes on a binary value from {0, 1}. For the MNIST dataset, a GAE was trained with 1024 factors and 512 hidden units. The hidden units used a rectified linear (ReLU) activation function, while the visible activation was the logistic (i.e. sigmoid) function. Using a cross entropy loss, the model was trained for 200 epochs via minibatch gradient descent with a batch size of 100. The initial learning rate of 0.25 was decreased multiplicatively at each epoch by a factor of 0.995. For optimization, the model was trained using Nesterov\u2019s accelerated gradient (Sutskever et al., 2013) with a parameter of 0.9. Salt and pepper noise was applied during training where each pixel has a 0.5 probability of corruption. Training followed the walkback training procedure, with the training loss averaged over 5 reconstructions from the Markov chain starting at the training example.\nFigure 2 shows 250 consecutive samples generated while conditioning on each class label. Note that the \u2018samples\u2019 depicted show the expected value of each pixel for each step of the Markov chain defined by the sampling procedure. Each chain was initialized to a vector of zeros and at each step, the image was corrupted with 0.5 salt-and-pepper noise. The model begins to generate convincing samples after only a few steps of the chain. Also notice that the chain exhibits mixing between modes in the data distribution. For example, the samples from the \u20182\u2019 class contains both twos with\na loop and a cusp. Finally, notice that the samples contain very few spurious examples \u2013 i.e. most samples resemble recognizable digits.\nThe TFD consists of 4,178 expression-labeled images of human faces. Each image is 48\u00d7 48 pixels with 256 integer value levels of gray intensity. The intensity values were linearly scaled to the range [0, 1]. Unlike the MNIST experiment, the intensity values were not binarized.\nA GAE with 512 factors and 1024 ReLU hidden units and sigmoid outputs was trained on squarederror loss for 500 epochs. Again, we used mini-batch gradient descent, however, a mini-batch size of 50 was used with a learning rate of 1.0 annealed by a factor of 0.995 at each epoch. Training followed 5 walkback steps with a Nesterov momentum of 0.9. The model was trained and sampled using salt-and-pepper noise with probability of 0.5.\nWhen training on MNIST, each walkback reconstruction was then sampled using the sigmoid activation as the probabilities of a factorized Bernoulli distribution. Since the TFD faces were not binarized, the sigmoid activations were used instead. Figure 3 shows 150 consecutive samples for each class label, each starting at the zero vector.\nNotice that there is less variation in the TFD samples than those from the MNIST model. This is likely due to the relative small size of the TFD training set (4,178 TFD training cases vs. 60,000 MNIST training cases). The TFD dataset also provides 112,234 unlabeled examples and it may be possible to pretrain the FX factor weights on this unlabeled data. However, these experiments are beyond the scope of the current analysis."}, {"heading": "5 CONCLUSION", "text": "The class-conditional GAE can be interpreted as a generative model in much the same ways as the DAE. In fact, the GAE is akin to learning a separate DAE model for each class, but with significant weight sharing between the models. In this light, the gating acts as a means of modulating the model\u2019s weights depending on the class label. As such, the theoretical and practical consideration that have been applied to DAE\u2019s as generative models can also be applied to gated models. Future work will apply these techniques to richer conditional distributions, such as the task of image tagging as explored by Mirza & Osindero (2014)."}], "references": [{"title": "Gated autoencoders with tied input weights", "author": ["Alain", "Droniou", "Olivier", "Sigaud"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Alain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "\u00c9ric"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Phone recognition with the mean-covariance restricted boltzmann machine", "author": ["Dahl", "George E", "Marc\u2019Aurelio Ranzato", "Abdel-rahman Mohamed", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Dahl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2010}, {"title": "Binary coding of speech spectrograms using a deep auto-encoder", "author": ["Deng", "Li", "Seltzer", "Michael L", "Yu", "Dong", "Acero", "Alex", "Mohamed", "Abdel-Rahman", "Hinton", "Geoffrey E"], "venue": "In Interspeech,", "citeRegEx": "Deng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2010}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["Goodfellow", "Ian J", "Bulatov", "Yaroslav", "Ibarz", "Julian", "Arnoud", "Sacha", "Shet", "Vinay"], "venue": "In ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning to relate images. Pattern Analysis and Machine Intelligence", "author": ["Memisevic", "Roland"], "venue": "IEEE Transactions on,", "citeRegEx": "Memisevic and Roland.,? \\Q2013\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2013}, {"title": "Conditional generative adversarial nets", "author": ["Mirza", "Mehdi", "Osindero", "Simon"], "venue": "In NIPS 2014 Workshop on Deep Learning,", "citeRegEx": "Mirza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mirza et al\\.", "year": 2014}, {"title": "Multimodal transitions for generative stochastic networks", "author": ["Ozair", "Sherjil", "Yao", "Li", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1312.5578,", "citeRegEx": "Ozair et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ozair et al\\.", "year": 2013}, {"title": "The toronto face database", "author": ["Susskind", "Josh M", "Anderson", "Adam K", "Hinton", "Geoffrey E"], "venue": "Department of Computer Science,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain.", "startOffset": 15, "endOffset": 36}, {"referenceID": 3, "context": "In the field of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014).", "startOffset": 152, "endOffset": 263}, {"referenceID": 4, "context": "In the field of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014).", "startOffset": 152, "endOffset": 263}, {"referenceID": 8, "context": "In the field of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014).", "startOffset": 152, "endOffset": 263}, {"referenceID": 15, "context": "In the field of deep neural networks, purely supervised models trained on massive labeled datasets have garnered much attention over the last few years (Dahl et al., 2010; Deng et al., 2010; Krizhevsky et al., 2012; Goodfellow et al., 2014b; Szegedy et al., 2014).", "startOffset": 152, "endOffset": 263}, {"referenceID": 1, "context": "However, recent work has rekindled interest in generative models of data (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013; Kingma & Welling, 2014; Goodfellow et al., 2014a).", "startOffset": 73, "endOffset": 177}, {"referenceID": 1, "context": "The recently proposed sampling procedure for denoising autoencoders (Bengio et al., 2013) and their generalization to Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) presents a novel training procedure which, instead of attempting to maximize the likelihood of the data under the model, amounts to learning the transition operator of a Markov chain (Bengio et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 1, "context": ", 2013) and their generalization to Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) presents a novel training procedure which, instead of attempting to maximize the likelihood of the data under the model, amounts to learning the transition operator of a Markov chain (Bengio et al., 2013).", "startOffset": 284, "endOffset": 305}, {"referenceID": 1, "context": "Although these models have shown both theoretically and empirically to have the capacity to model the underlying data generating distribution, the unimodal transition operator learned in (Bengio et al., 2013) and (Bengio & Thibodeau-Laufer, 2013) limits the types of distributions that can be modeled successfully.", "startOffset": 187, "endOffset": 208}, {"referenceID": 12, "context": "One way to address this issue is by adopting an alternative generative model such as the Neural Autoregressive Density Estimator (NADE) as the output distribution of the transition operator (Ozair et al., 2013).", "startOffset": 190, "endOffset": 210}, {"referenceID": 1, "context": "However, recent work has rekindled interest in generative models of data (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013; Kingma & Welling, 2014; Goodfellow et al., 2014a). The recently proposed sampling procedure for denoising autoencoders (Bengio et al., 2013) and their generalization to Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) presents a novel training procedure which, instead of attempting to maximize the likelihood of the data under the model, amounts to learning the transition operator of a Markov chain (Bengio et al., 2013). Although these models have shown both theoretically and empirically to have the capacity to model the underlying data generating distribution, the unimodal transition operator learned in (Bengio et al., 2013) and (Bengio & Thibodeau-Laufer, 2013) limits the types of distributions that can be modeled successfully. One way to address this issue is by adopting an alternative generative model such as the Neural Autoregressive Density Estimator (NADE) as the output distribution of the transition operator (Ozair et al., 2013). In this paper, we propose a simpler approach. When labeled data is available, we can use the label information in order to carve up the landscape of the data distribution. This work begins by presenting an overview of related work, including a treatment of autoencoders, denoising autoencoders, autoencoders as generative models and gated autoencoders. Next, we propose a class-conditional gated autoencoder along with training and sampling procedures based on the work of Bengio et al. (2013). Finally, we present experimental results of our model on two image-based datasets.", "startOffset": 74, "endOffset": 1589}, {"referenceID": 16, "context": "Among the various kinds of regularized autoencoders, the denoising autoencoder (DAE) (Vincent et al., 2008) is among the most popular and well-understood.", "startOffset": 85, "endOffset": 107}, {"referenceID": 16, "context": "Apart from preventing the model to learn a trivial representation of the input by acting as a form or regularization, the DAE can be interpreted as a means of learning the manifold of the underlying data generating distribution (Vincent et al., 2008).", "startOffset": 228, "endOffset": 250}, {"referenceID": 16, "context": "Under this interpretation the hidden representation can be interpreted as a coordinate system of manifolds (Vincent et al., 2008).", "startOffset": 107, "endOffset": 129}, {"referenceID": 17, "context": "Although DAEs are useful as a means of pre-training discriminative models, especially when stacked to form deep models (Vincent et al., 2010), recent work by Bengio et al.", "startOffset": 119, "endOffset": 141}, {"referenceID": 1, "context": ", 2010), recent work by Bengio et al. (2013) has shown that DAEs", "startOffset": 24, "endOffset": 45}, {"referenceID": 1, "context": "In order to generate samples from the model, one simply forms a Markov chain which alternately samples from learned distribution and the corruption distribution (Bengio et al., 2013).", "startOffset": 161, "endOffset": 182}, {"referenceID": 1, "context": "3, the asymptotic distribution of the generated samples converges to the datagenerating distribution (Bengio et al., 2013).", "startOffset": 101, "endOffset": 122}, {"referenceID": 1, "context": "A more efficient procedure called \u201cwalkback training\u201d is described in (Bengio et al., 2013) and bears resemblance to contrastive divergence training of restricted Boltzman machines.", "startOffset": 70, "endOffset": 91}, {"referenceID": 1, "context": "The sampling procedure proposed by Bengio et al. (2013) for classical denoising autoencoders can also be applied to a GAE.", "startOffset": 35, "endOffset": 56}, {"referenceID": 1, "context": "Bengio et al. (2013) provide a proof of the following theorem: Theorem 1.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "The arguments for consistency and ergoticity can also be made in the same manner as those in (Bengio et al., 2013).", "startOffset": 93, "endOffset": 114}, {"referenceID": 13, "context": "We demonstrate the generative properties of the class-conditional GAE model on two datasets: binarized MNIST (LeCun & Cortes, 1998) and the Toronto Face Database (TFD) (Susskind et al., 2010).", "startOffset": 168, "endOffset": 191}, {"referenceID": 14, "context": "For optimization, the model was trained using Nesterov\u2019s accelerated gradient (Sutskever et al., 2013) with a parameter of 0.", "startOffset": 78, "endOffset": 102}], "year": 2017, "abstractText": "Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain. The transition operator is typically unimodal, which limits its capacity to model complex data. In order to perform efficient sampling from conditional distributions, we extend this work, both theoretically and algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is able to generate convincing class-conditional samples when trained on both the MNIST and TFD datasets.", "creator": "LaTeX with hyperref package"}}}