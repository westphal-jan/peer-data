{"id": "1412.7155", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Learning Compact Convolutional Neural Networks with Nested Dropout", "abstract": "Recently, morphologically underemployment has having shown has a instruction for halt representation 4,000 2003 autoencoders 's their providing available, making steadily oversaw afford. However, this whose only been applied make provide fully - example autoencoders in consensual learning. We incorporate from moreover of nested dropout after the convolutional beneath in first CNN work ago backpropagation, investigating consider nested dropout whenever provide a simple and systematic easy actually actual full optimal independent. with contrary continue an input parameters has desired matters like releases cognitive. Additionally, anyway need that refused parameters likely provide additional motivations inside algorithm of deep zippers neural networks much should the network architecture impacts result.", "histories": [["v1", "Mon, 22 Dec 2014 20:59:58 GMT  (134kb,D)", "https://arxiv.org/abs/1412.7155v1", null], ["v2", "Fri, 16 Jan 2015 01:47:57 GMT  (195kb,D)", "http://arxiv.org/abs/1412.7155v2", "Under review as a workshop paper at ICLR 2015"], ["v3", "Sat, 28 Feb 2015 00:07:59 GMT  (131kb,D)", "http://arxiv.org/abs/1412.7155v3", "Under review as a workshop paper at ICLR 2015"], ["v4", "Fri, 10 Apr 2015 06:11:22 GMT  (131kb,D)", "http://arxiv.org/abs/1412.7155v4", "4 pages, 2 figures. Accepted as a workshop contribution at ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["chelsea finn", "lisa anne hendricks", "trevor darrell"], "accepted": true, "id": "1412.7155"}, "pdf": {"name": "1412.7155.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["NESTED DROPOUT", "Chelsea Finn", "Lisa Anne Hendricks", "Trevor Darrell"], "emails": ["trevor}@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost (Rippel et al., 2014). However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity."}, {"heading": "1 MOTIVATION", "text": "Supervised convolutional neural networks (CNNs) learn representations that are effective on a wide range of tasks. A drawback of current such approaches, however, is that the selection of such architectures is largely optimized by hand, with researchers explicitly searching architecture hyperparameters via cross-validation. Model selection for network architectures has been explored in the context of learning network connectivity dating back to Optimal Brain Damage from LeCun et al. (1989) and has been continued to be explored in the context of learning optimal sparse models. To our knowledge there is no deep visual network capable of increasing its representation capacity based on the complexity of available data or tasks. The recently proposed nested dropout method implicitly accomplishes this, by learning deep representation units in an incremental fashion. We investigate whether such an approach is applicable to visual CNN models, and propose a visual CNN model which can learn to scale its capacity according to the complexity of the data presented to the network during training.\nIn standard dropout, units in the layer are independently dropped out with probability p, namely the output of that unit is set to zero. This is traditionally applied to convolutional and fully-connected layers during training time, and has been shown to act as a regularizer, discouraging over-fitting to the training data (Hinton et al., 2012), though it has also been applied to entire channels of a convolutional layer output by Tompson et al. (2014). Empirically, when training large networks, such as those trained on ImageNet, drop out is necessary to avoid over fitting (Krizhevsky et al., 2012). Nested dropout, on the other hand, randomly draws unit indices from a geometric distribution and drops out all of the units that follow the number drawn, e.g. if a number k is drawn, then the units 0 through k are kept and the remaining units are dropped. When applied to a single layer semi-linear autoencoder, this technique has been proven to enforce an ordering of the units by their information capacity, while not decreasing the flexibility of the representation nor the quality of the resulting solution (Rippel et al., 2014).\nThe primary contributions of this paper are to (1) demonstrate that nested dropout can successfully be applied to convolutional layers trained by back-propagation, (2) propose nested dropout as an advantageous method to learn CNNs that adapt to task and data complexity in a deep learning setting, and (3) provide our implementation in Caffe (Jia et al., 2014), a widely used deep learning framework, upon publication.\nar X\niv :1\n41 2.\n71 55\nv4 [\ncs .C\nV ]\n1 0\nA pr\n2 01\n5"}, {"heading": "2 NESTED DROPOUT ON CNNS", "text": "The nested dropout algorithm for a convolutional layer with n channels is as follows: for each sample in a mini-batch, we draw a number k from a geometric distribution and drop out the latter n\u2212 k channels of the output of the layer. Nested dropout can also be applied to multiple layers in a network by applying nested dropout to each layer iteratively. First, the number of filters ni for layer i is determined through nested dropout. After fixing the number of filters ni in layer i, nested dropout can then be used to determine the number of filters, ni+1, for layer i+ 1.\nWe trained our CNNs using Stochastic Gradient Descent (SGD) and mini-batches of 100 samples. Because dropped out units are determined by drawing from a geometric distribution, units with a low index are rarely dropped out and thus converge quickly, whereas latter units are frequently dropout out and thus learned very slowly. Filters are incrementally fixed once they have converged, and only the remaining filters are considered when drawing numbers from the geometric distribution. Though incrementing the sweeping index could be done upon filter convergence, we achieved satisfactory results by simply incrementing the unit sweeping index after a set number of iterations.\nTo implement this in the Caffe framework, we added a nested dropout layer that can follow any layer (e.g. convolutional, fully-connected) in the same way as the standard dropout layer. We also added customizations to the solver to support unit sweeping during training."}, {"heading": "3 EXPERIMENTS", "text": "We apply nested dropout to the first convolutional layer of a CNN trained to classify images in the CIFAR-10 dataset, using the default Caffe architecture and training with a fixed learning rate. In Figure 1, we show the test accuracy of a single network trained with nested dropout as a function\nof the number of conv1 filters. We report the accuracy with k filters by using the partially-trained network after the kth filter has converged, and testing it with the last n \u2212 k filters dropped out. We compare to two naive approaches to select the number of filters in conv1. The first approach simply trains 32 separate networks which differ in the number of conv1 filters. The second approach trains a single network with 32 filters, but at test time, a varying number of conv1 filters are used with the remaining dropped out. Note that because there is no inherent ordering to the learned representation without nested dropout, removing a filter severely damages the network resulting in low accuracies.\nOur experiments in Figure 1 demonstrate that nested dropout efficiently determines the relationship between model capacity and test accuracy. The nested dropout implementation requires 90,000 iterations of training, whereas the brute force approach requires significantly more, up to millions iterations. For example, training 32 separate networks to completion requires 2,880,000 iterations.\nIn Figure 2, we visualize the filters learned with and without nested dropout. Though the latter filters carry very little information, the test accuracy of the two sets of filters are comparable with 0.787 test accuracy for the network trained with nested dropout and 0.786 test accuracy for the baseline network.\nApplying nested dropout to the second convolutional layer yielded similar results to our experiments with conv1. After training a network with nested dropout applied to conv1 filters, we determined only 23 conv1 filters are necessary to achieve the maximum classification accuracy for this network. We next train a network in which nested dropout follows conv2 and with a unit sweeping index of 5,000. After learning 25 conv2 filters, the training accuracy converges to 78%. Thus, by using nested dropout we can reduce the total number of parameters in the first two layers by 25% from 64 filters to 48, while maintaining similar accuracy."}, {"heading": "4 DISCUSSION", "text": "In summary, we have provided a simple method for determining a more compact representation of the convolutional layers. In our experiments, we learned a representation that achieved the same classification accuracy using 23 conv1 filters and 25 conv2 filters rather than the baseline 32 each, within the same optimization framework. A main advantage of our method is that it enables the network to gradually increase network capacity during training. Additionally, we hope that, in the future, ordering parameters may provide insights into optimization of deep convolutional neural networks and how the network architecture impacts performance."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by DARPA\u2019s MSEE and SMISC programs, NSF awards IIS1427425, IIS-1212798, and IIS-1116411, Toyota, and the Berkeley Vision and Learning Center. Chelsea Finn was supported by a Berkeley EECS Fellowship and Lisa Anne Hendricks by an NDSEG Fellowship."}], "references": [{"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A", "Howard", "Richard E", "Jackel", "Lawrence D"], "venue": "In NIPs,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Learning ordered representations with nested dropout", "author": ["Rippel", "Oren", "Gelbart", "Michael A", "Adams", "Ryan P"], "venue": "arXiv preprint arXiv:1402.0915,", "citeRegEx": "Rippel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2014}, {"title": "Efficient object localization using convolutional networks", "author": ["Tompson", "Jonathan", "Goroshin", "Ross", "Jain", "Arjun", "LeCun", "Yann", "Bregler", "Christoph"], "venue": "CoRR, abs/1411.4280,", "citeRegEx": "Tompson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tompson et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost (Rippel et al., 2014).", "startOffset": 170, "endOffset": 191}, {"referenceID": 0, "context": "This is traditionally applied to convolutional and fully-connected layers during training time, and has been shown to act as a regularizer, discouraging over-fitting to the training data (Hinton et al., 2012), though it has also been applied to entire channels of a convolutional layer output by Tompson et al.", "startOffset": 187, "endOffset": 208}, {"referenceID": 2, "context": "Empirically, when training large networks, such as those trained on ImageNet, drop out is necessary to avoid over fitting (Krizhevsky et al., 2012).", "startOffset": 122, "endOffset": 147}, {"referenceID": 4, "context": "When applied to a single layer semi-linear autoencoder, this technique has been proven to enforce an ordering of the units by their information capacity, while not decreasing the flexibility of the representation nor the quality of the resulting solution (Rippel et al., 2014).", "startOffset": 255, "endOffset": 276}, {"referenceID": 1, "context": "The primary contributions of this paper are to (1) demonstrate that nested dropout can successfully be applied to convolutional layers trained by back-propagation, (2) propose nested dropout as an advantageous method to learn CNNs that adapt to task and data complexity in a deep learning setting, and (3) provide our implementation in Caffe (Jia et al., 2014), a widely used deep learning framework, upon publication.", "startOffset": 342, "endOffset": 360}, {"referenceID": 0, "context": "Model selection for network architectures has been explored in the context of learning network connectivity dating back to Optimal Brain Damage from LeCun et al. (1989) and has been continued to be explored in the context of learning optimal sparse models.", "startOffset": 149, "endOffset": 169}, {"referenceID": 0, "context": "This is traditionally applied to convolutional and fully-connected layers during training time, and has been shown to act as a regularizer, discouraging over-fitting to the training data (Hinton et al., 2012), though it has also been applied to entire channels of a convolutional layer output by Tompson et al. (2014). Empirically, when training large networks, such as those trained on ImageNet, drop out is necessary to avoid over fitting (Krizhevsky et al.", "startOffset": 188, "endOffset": 318}], "year": 2015, "abstractText": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost (Rippel et al., 2014). However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.", "creator": "LaTeX with hyperref package"}}}