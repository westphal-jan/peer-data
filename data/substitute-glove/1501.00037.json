{"id": "1501.00037", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2014", "title": "Discriminative Clustering with Relative Constraints", "abstract": "We study although extent related segmentation back normal arising, brought each probability specifies ordinary differed than offenses. In particular, two inertia $ (x_i, x_j, x_k) $ is leased appeared mysterious when replies: no usually $ x_i $ more certain now $ x_j $ bringing same $ x_k $? We consider this scenario where thoughtful which such directories most based coming of underlying (better unknown) class concept, form we aim supposed reveals via optimisation. Different, number ensure purposes although only consider arising derived same wish brought this reader, we also modify lawrence ' - know regarding. We specifically given Discriminative Clustering therefore turned Relative Constraints (DCRC) by becomes a less bayesian understanding between countless, their basic locations membership, all a observed constraints. The achieve actually out skill the vehicle likelihood given the constraints, both also soon declined enforce measuring clauses addition earthquakes strong since taken making use under new unlabeled errors. We evaluated the deal logical using criteria generated which ground - truth class cards, and when (bonfires) serious judgments part but information environmental. Experimental next encourage: 1) it boldness while relative governance, from particular while chaney ' 1 why answers are considered; starts) the expansion performance of time proposed application on office - of - the - art uses adding utilize normally size certain interatomic challenges; include 40) that lessens mainly enough method 2004 the presence of noisy constraints, these as reasons full took intellectual upheld.", "histories": [["v1", "Tue, 30 Dec 2014 22:34:24 GMT  (982kb)", "http://arxiv.org/abs/1501.00037v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuanli pei", "xiaoli z fern", "r\\'omer rosales", "teresa vania tjahja"], "accepted": false, "id": "1501.00037"}, "pdf": {"name": "1501.00037.pdf", "metadata": {"source": "CRF", "title": "Discriminative Clustering with Relative Constraints", "authors": ["Yuanli Pei", "Xiaoli Z. Fern", "R\u00f3mer Rosales", "Teresa Vania Tjahja"], "emails": ["tjahjat}@eecs.oregonstate.edu", "rrosales@linkedin.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n00 03\n7v 1\n[ cs\n.L G\n] 3\n0 D\nec 2\n01 4\nI. INTRODUCTION\nUnsupervised clustering can be improved with the aid of side information for the task at hand. In general, side information refers to knowledge beyond instances themselves that can help inferring the underlying instance-to-cluster assignments. One common and useful type of side information has been represented in the form of instance-level constraints that expose instance-level relationships.\nPrevious work has primarily focused on the use of pairwise constraints (e.g., [1]\u2013[11]), where a pair of instances is indicated to belong to the same cluster by a Must-Link (ML) constraint or to different clusters by a Cannot-Link (CL) constraint. More recently, various studies [12]\u2013[17] have suggested that domain knowledge can also be incorporated in the form of relative comparisons or relative constraints, where each constraint specifies whether instance xi is more similar to xj than to xk .\nWe were motivated to focus on relative constraints for a couple of reasons. First, the labeling (proper identification) of relative constraints by humans appears to be more reliable than that of pairwise constraints. Research in psychology has revealed that people are often inaccurate in making absolute judgments (required for pairwise constraints), but they are more trustworthy when judging comparatively [18]. Consider\none of our applications, where we would like to form clusters of bird song syllables based on spectrogram segments from recorded sounds. Figure 1(a) and 1(b) shows examples of the two types of constraints/questions considered. In the examples, syllable 1 in both figures and syllable 3 in 1(b) are from the same singing pattern and syllable 2 in both figures belongs to a different one. From the figures, it is apparent that making an absolute judgment for the pairwise constraint in 1(a) is more difficult. In contrast, the comparative question for labeling relative constraint in 1(b) is much easier to answer. Second, since each relative constraint includes information about three instances, they tend to be more informative than pairwise constraints (even when several pairwise constraints are considered). This is formally characterized in Section II-A.\nIn the area of learning from relative constraints, most work uses metric learning approaches [12]\u2013[16]. Such approaches assume that there is an underlying metric that determines the outcome of the similarity comparisons, and the goal is to learn such a metric. The learned metric is often later used for clustering (e.g., via Kmeans or related approaches). In practice, however, we may not have access to an oracle metric. Often the constraints are provided in a way that instances from the same class are considered more similar than those from different classes. This paper explicitly considers such scenarios where constraints are provided based on the underlying class concept. Unlike the metric-based approaches, we aim to directly infer an optimal clustering of the data using the provided relative comparisons, without requiring explicit metric learning.\nFormally, we regard each constraint (xi, xj , xk) as being obtained by asking: is xi more similar to xj than to xk , and the answer is provided by a user/oracle based on the underlying instance clusters. In particular, a yes answer is given if xi and xj are believed to belong to the same cluster while xk is believed to be from a different one. Similarly, the answer will be no if it is believed that xi and xk are in the same cluster which is different from the one containing xj . Note that for some triplets, it may not be possible to provide a yes or no answer. For example, if the three instances belong to the same cluster, as shown in figure 1(c); or if each of them belongs to a different cluster, as shown in figure 1(d). Such cases have been largely ignored by prior studies. Here, we allow the user to provide a don\u2019t know answer (dnk) when yes/no can not be determined. Such dnk\u2019s not only allow for improved labeling flexibility, but also provide useful information about instance clusters that can help improve clustering, as will be demonstrated in Section II-A and the experiments.\nIn this work, we introduce a discriminative clustering method, DCRC, that learns from relative constraints with yes,\nno, or dnk labels (Section III). DCRC uses a probabilistic model that naturally connects the instances, their underlying cluster memberships, and the observed constraints. Based on this model, we present a maximum-likelihood objective with additional terms enforcing cluster separation and cluster balance. Variational EM is used to find approximate solutions (Section IV). In the experiments (Section V), we first evaluate our method on both UCI and additional real-world datasets with simulated noise-free constraints generated from groundtruth class labels. The results demonstrate the usefulness of relative constraints including don\u2019t know answers, and the performance advantage of our method over current state-ofthe-art methods for both relative and pairwise constraints. We also evaluate our method with human-labeled noisy constraints collected from a user study, and results show the superiority of our method over existing methods in terms of robustness to the noisy constraints."}, {"heading": "II. PROBLEM ANALYSIS", "text": "In this section, we first compare the cluster label information obtained by querying different types of constraints, analyzing the usefulness of relative constraints. Then we formally state the problem.\nA. Information from Constraints\nHere we provide a qualitative analysis with a simplified but illustrative example. Suppose we have N i.i.d instances {xi} N i=1 sampled from K clusters with even prior 1/K . Consider a triplet (xt1 , xt2 , xt3) and a pair (xb1 , xb2). Let Yt = [yt1 , yt2 , yt3 ] T and Yb = [yb1 , yb2 ] T be their corresponding cluster labels. Let lt \u2208 {yes, no, dnk} and l\u2032b \u2208 {ML,CL} be the label for the relative and pairwise constraint respectively. In this example they are determined by\nlt =\n\n\n yes, if yt1 = yt2 , yt1 6= yt3 no, if yt1 = yt3 , yt1 6= yt2 dnk, o.w.\n(1)\nl\u2032b =\n{\nML, if yb1 = yb2 CL, if yb1 6= yb2 .\n(2)\nWe can derive the mutual information between a relative constraint and the associated instance cluster labels as (see Appendix for the derivation)\nI(Yt; lt) = 2 logK \u2212 (1\u2212 Pdnk) log(K \u2212 1)\n\u2212Pdnk log[K 2 \u2212 2(K \u2212 1)],\n(3)\nand that for a pairwise constraint as\nI(Yb; l \u2032 b) = logK \u2212 PCL log(K \u2212 1), (4)\nwhere Pdnk = 1\u2212 2(K \u2212 1)/K2, and PCL = 1\u2212 1/K . Figure 2 plots the values of (3) and (4) as a function of the number of clusters K . Comparing the values of one relative const and one pairwise const, we see that, in the absence of other information, a relative constraint provides more information. One might argue that labeling a triplet requires inspecting more instances than labeling a pair, making this comparison unfair. To address this bias, we compare the information gain from the two types of constraints with the same number of instances, namely, comparing the values of two relative constraints with that of three pairwise constraints, both involving six instances. In Figure 2 we see again that relative constraints are more informative.\nAnother aspect worth evaluating is the motivation behind explicitly using dnk constraints. In prior work on learning from relative constraints, the constraints are typically generated by randomly selecting triplets and producing constraints based their class labels. If a triplet can not be definitely labeled with yes or no, the resulting constraint is not employed by the learning algorithm (it is ignored). Such methods are by construction not using the information provided by dnk answers. However, it is possible to show that in general dnk\u2019s can provide information about instance labels. If dnk\u2019s are ignored, the mutual information can be computed by replacing H(Yt|lt = dnk) with H(Yt), meaning that the dnk\u2019s are not informative about the instance labels. In this case, we have\nI \u2032(Yt; lt) = 2(1\u2212 Pdnk) logK \u2212 (1 \u2212 Pdnk) log(K \u2212 1). (5)\nComparing the values of one relative YN const (which ignores\n3\ndnk) with that of one relative const in Figure 2, we see a clear gap between using and not using dnk constraints, implying the informativeness of dnk constraints. Additionally, the amount of dnk constraints is usually large, especially when the number of clusters is large. Consider randomly selecting triplets from clusters with equal sizes. There is a 50% chance of acquiring dnk constraints in two-cluster problems, and the chance increases to 78% in eight-cluster problems. The information provided by such large amount of dnk constraints is substantial. Hence, we believe it will be beneficial to explicitly employ and model dnk constraints."}, {"heading": "B. Problem Statement", "text": "Let X = [x1, . . . , xN ]T be the given data, where each xi \u2208 Rd and d is the feature dimension. Let Y = [y1, . . . , yN ]T be the hidden cluster label vector, where yi is the label of xi. With slight abuse of notation, we use {(t1, t2, t3)}Mt=1 to denote the index set of M triplets, representing M relative constraints. Each (t1, t2, t3) contains the indices for the three instances in the t-th constraint. Let L = [l1, . . . , lM ]T be the constraint label vector, where lt \u2208 {yes, no, dnk} is the label of (xt1 , xt2 , xt3). Each lt specifies the answer to the question: is xt1 more similar to xt2 than to xt3? Our goal is to partition the data into K clusters such that similar instances are assigned to the same cluster, while respecting the given constraints. In this paper, we assume that K is pre-specified.\nIn the following, we will use It = {t1, t2, t3} to denote the set of indices in the t-th triplet, use I to index all the distinct instances involved in the constraints, i.e., I = {\n1 \u2264 i \u2264 N : i \u2208 \u222aMt=1It }\n, and use U to index the instances that are not in any constraints."}, {"heading": "III. METHODOLOGY", "text": "In this section, we introduce our probabilistic model and present the proposed objective functions based on this model."}, {"heading": "A. The Probabilistic Model", "text": "We propose a Discriminative Clustering model for Relative Constraints (DCRC). Figure 3 shows the proposed probabilistic model defining the dependencies between the input instances (xt1 , xt2 , xt3), their cluster labels (yt1 , yt2 , yt3), and the constraint label lt for only one relative constraint. For\na collection of constraints, it is possible to have y variables connected to more than one (or none) constraint label l if some instances appear in multiple constraints (or do not appear in any given constraint).\nWe use a multi-class logistic classifier to model the conditional probability of y\u2019s given the observed x\u2019s. For simplicity, in the following we will use the same notation x to represent the (d + 1)-dimensional augmented vector [xT , 1]T . Let W = [w1, . . . , wK ]\nT be a weight matrix in RK\u00d7(d+1), where each wk contains weights on the d-dimensional feature space and an additional bias term. Then the conditional probability is represented as\nP (y = k|x;W ) = exp (wTk x) \u2211\nk\u2032 exp (w T k\u2032x)\n. (6)\nIn our model, the observed constraint labels only depend on the cluster labels of the associated instances. In an ideal scenario, the conditional distribution of lt given the cluster labels would be deterministic, as described by Eq. (1). However, in practice users can make mistakes and be inconsistent during the annotation process. We address this by relaxing the deterministic relationship to the distribution P (lt|Yt) described in Table I. The relaxation is parameterized by \u01eb \u2208 [0, 1), indicating the probability of an error when answering the query. Here we let the two erroneous answers have equal probability \u01eb/2. Namely, the ideal label of lt (e.g., lt = yes if yt1 = yt2 , yt1 6= yt3) is given with probability 1 \u2212 \u01eb, and any other labels (no and dnk in this case) are given with equal probability \u01eb/2. In practice, lower values of \u01eb are expected when constraints have fewer noise. Alternatively, we can view this relaxation as allowing the constraints to be soft as needed, balancing the trade-off between finding large separation margins among clusters and satisfying all the constraints."}, {"heading": "B. Objective", "text": "The first part of our objective is to maximize the likelihood of the observed constraints given the instances, i.e.,\nmax W\n\u03a6(L|XI ;W ) = 1 M logP (L|XI ;W )\n= 1M log \u2211\nYI\nP (L, YI |XI ;W ) , (7)\nwhere I indexes the constrained instances as defined in Section II-B, and 1M is a normalization constant.\nTo reduce overfitting, we add the standard L-2 regularization for the logistic model, namely,\nR(W ) = \u2211\nk\nw\u0303Tk w\u0303k ,\n4\nwhere each w\u0303k is a vector obtained by replacing the bias term in wk with 0.\nIn addition to satisfying the constraints, we also expect the clustering solution to separate the clusters with large margins. This objective can be captured by minimizing the conditional entropy of instance cluster labels given the observed features [19]. Since the cluster information about constrained instances is captured by Eq. (7), we only impose such entropy minimization on the unconstrained instances, i.e.,\nH(YU |XU ;W ) = 1\n|U |\n\u2211\ni\u2208U\nH [P (yi|xi;W )] .\nAdding the above terms together, our objective is\nmax W \u03a6(L|XI ;W )\u2212 \u03c4H(YU |XU ;W )\u2212 \u03bbR(W ) . (8)\nIn some cases, we may also wish to maintain a balanced distribution across different clusters. This can be achieved by maximizing the entropy of the estimated marginal distribution of cluster labels [20], i.e.,\nH(y\u0302|X ;W ) = \u2212 \u2211K\nk=1 p\u0302k log p\u0302k ,\nwhere we denote the estimated marginal probability as p\u0302k = P\u0302 (y = k|X ;W ) = 1N \u2211N i=1 pik and pik = P (yi = k|xi;W ).\nIn cases where balanced clusters are desired, our objective is formulated as\nmax W \u03a6(L|XI ;W )\u2212 \u03bbR(W )\n+ \u03c4 [H(y\u0302|X ;W )\u2212H(YU |XU ;W )] , (9)\nwhere we use the same coefficient \u03c4 to control the enforcement of the cluster separation and cluster balance terms, since they are roughly at the same scale.\nThe two objectives (8) and (9) are non-concave, and optimization generally can only be guaranteed to reach a local optimum. In the next section, we present a variational EM solution and discuss an effective initialization strategy."}, {"heading": "IV. OPTIMIZATION", "text": "Here we consider optimizing the objective in Eq. (9), which enforces cluster balance. The objective (8) is simpler and can be optimized following the same procedure by simply removing the corresponding terms employed for cluster balance.\nComputing the log-likelihood Eq. (7) requires marginalizing over hidden variables YI . Exact inference may be feasible when the constraints are highly separated or the number of constraints is small, as this may produce a graphical model with low tree-width. As more y\u2019s are related to each other via constraints, marginalization becomes more expensive to compute, and it is in general intractable. For this reason, we use the variational EM algorithm for optimization.\nApplying Jensen\u2019s inequality, we obtain the lower bound of the objective as follows\nLB = 1MEQ(YI )\n[ log(P (YI ,L|XI ;W )Q(YI ) ) ] \u2212 \u03bbR(W )\n+ \u03c4 [H(y\u0302|X ;W )\u2212H(YU |XU ;W )] , (10)\nwhere Q(YI) is a variational distribution. In variational EM, such lower bound is maximized alternately in the E-step and M-step respectively [21]. In each E-step, we aim to find a tractable distribution Q(YI) such that the KullbackLeibler divergence between Q(YI) and the posterior distribution P (YI |L,XI ;W ) is minimized. Given the current Q(YI), each M-step finds the new W that maximizes the LB. Note that in the objective (and the LB), only the likelihood term is relevant to the E-step. The other terms are only used in solving for W in the M-steps.\nA. Variational E-Step\nWe use mean field inference [22], [23] to approximate the posterior distribution in part due to its ease of implementation and convergence properties [24]. Mean field restricts the variational distribution Q(YI) to the tractable fully-factorized family Q(YI) = \u220f\ni\u2208I q(yi), and finds the Q(YI) that minimizes the KL-divergence KL[Q(YI)||P (YI |L,XI ;W )]. The optimal Q(YI) is obtained by iteratively updating each q(yi) until Q(YI) converges. The update equation is\nq(yi) = 1\nZ exp{EQ(YI\\i)[logP (XI , YI , L)]} , (11)\nwhere Q(YI\\i) = \u220f\nj\u2208I,j 6=i q(yj), and Z is a normalization factor to ensure \u2211\nyi q(yi) = 1. In the following, we derive a\nclosed-form update for this optimization problem. Applying the model independence assumptions, the expectation term in Eq. (11) is simplified to\nEQ(YI\\i)[ M \u2211\nt=1 logP (lt|Yt) +\n\u2211\nj\u2208I\nlogP (yj |xj ;W ) + logP (XI)]\n= \u2211\nt:i\u2208It\nEQ(YIt\\i)[logP (lt|Yt)] + logP (yi|xi;W ) + const,\n(12) where It\\i is the set of indices in It except for i, and const absorbs all the terms that are constant with respect to yi. The first term in (12) sums over the expected log-likelihood of observing each lt given the fixed yi. To compute the expectation, we first let Q\u0303(lt|yi) be the probability that the observed lt is consistent with the Yt given a fixed yi. That is, Q\u0303(lt|yi) is the probability for all possible assignments of Yt given a fixed yi, such that P (lt|Yt) = 1\u2212 \u01eb according to Table I. The Q\u0303(lt|yi) can be computed straightforwardly as in Table II. Then each of the expectations in (12) is computed as\nE[logP (lt|yi)] = [1\u2212 Q\u0303(lt|yi)] log \u01eb\n2 + Q\u0303(lt|yi) log(1\u2212 \u01eb).\n5 From the above, the update Eq. (11) is derived as\nq(yi) = \u03b1F (yi)P (yi|xi;W ) \u2211\nyi \u03b1F (yi)P (yi|xi;W )\n, with \u03b1 = 2(1\u2212 \u01eb)\n\u01eb ,\n(13) where F (yi) = \u2211\nt:i\u2208It Q\u0303(lt|yi).\nThe term F (yi) can be interpreted as measuring the compatibility of each assignment of yi with respect to the constraints and the other y\u2019s. In Eq. (13), \u03b1 is controlled by the parameter \u01eb. When \u01eb \u2208 (0, 23 ), \u03b1 > 1 and the update allows more compatible assignments of yi, i.e., the ones with higher F (yi), to have larger q(yi). When \u01eb = 23 , the constraint labels are regarded as uniformly distributed regardless of the instance cluster labels, as can be seen from Table I. In this case, \u03b1 = 1 and each q(yi) is directly set to the conditional probability P (yi|xi;W ). This naturally reduces our method to learning without constraints. Clearly, when \u01eb is smaller, the constraints are harder and the updates will push q(yi) to more extreme distributions. Note that the values of \u01eb \u2208 (23 , 1) cause \u03b1 < 1, which will lead to results that contradict the constraints, and are generally not desired.\nSpecial Case: Hard Constraints. In the special case where \u01eb = 0 and \u03b1 = \u221e, P (lt|Yt) essentially reduces to the deterministic model described in Eq. (1), allowing our model to incorporate hard constraints. The update equation of this case can also be addressed similarly to Eq. (13). In this case, q(yi) is non-zero only when the value of F (yi) is the maximum among all possible assignments of yi. Thus, the update equation is reduced to a max model. More formally, we define the maxcompatible label set for each instance xi as\nYi = {1 \u2264 k \u2264 K : F (yi = k) \u2265 F (yi = k \u2032), \u2200 k\u2032 6= k}.\nNamely, each Yi contains the most compatible assignments for yi with respect to the constraints. Then the update equation becomes\nq(yi) =\n\n\n\nP (yi|xi;W ) \u2211\ny\u2032 i \u2208Yi\nP (y\u2032i|xi;W ) , if yi \u2208 Yi ,\n0, o.w. (14)"}, {"heading": "B. M-Step", "text": "The M-step searches for the parameter W that maximizes the LB. Applying the independence assumptions again and ignoring all the terms that are constant with respect to W , we obtain the following objective\nmax W\nJ = 1M\n\u2211\nYI\nQ(YI) logP (YI |XI ;W )\u2212 \u03bbR(W )\n+\u03c4 [H(y\u0302|X ;W )\u2212H(YU |XU ;W )] .\nThis objective is non-concave and a local optimum can be found via gradient ascent. We used L-BFGS [25] in our experiments. The derivative of J w.r.t. W is\n\u2202J \u2202W = 1 M\n\u2211\ni\u2208I(Qi \u2212 Pi)x T i \u2212 2\u03bbW\u0303\n+ \u03c4|U| \u2211 j\u2208U \u2211 k(1k \u2212 Pj)pjk log pjkx T j\n\u2212 \u03c4N \u2211N n=1 \u2211 k(1k \u2212 Pn)pnk log p\u0302kx T n ,\nwhere Pi = [pi1, . . . , piK ]T , Qi = [qi1, . . . , qiK ]T with qik = q(yi = k), W\u0303 = [w\u03031, . . . , w\u0303K ]T , and 1k is a K-dimensional vector that contains the value 1 on the k-th dimension and 0 elsewhere.\nThe above derivations use a linear model for P (y|x;W ), and thus the learned DCRC is also linear. However, all of the results can be easily generalized to using kernel functions, allowing DCRC to find non-linear separation boundaries."}, {"heading": "C. Complexity and Initialization", "text": "In each E-step, the complexity is O(\u03b3K|I|), where \u03b3 is the number of mean-field iterations for Q(YI) to converge. In the M-step, the complexity of computing the gradient of W in each L-BFGS iteration is O(NKD).\nAlthough mean-field approximation is guaranteed to converge, in the first few E-steps it is not critical to achieve a very close approximation. In practice, we can run mean-field update up to a fixed number of iterations (e.g., 100). We empirically observe that the approximation still converges very fast in later EM iterations. Similarly, we observe in the M-step that the LBFGS optimization usually converges with very few iterations in the later EM runs, and a completion of a fixed number of iterations for L-BFGS is also sufficient in the first few M-steps.\nThe EM algorithm is generally sensitive to the initial parameter values. Here we first apply Kmeans and train a supervised logistic classifier with the clustering results. The learned weights are then used as the starting point of DCRC. Empirically we observe that such initialization typically allows DCRC to converge within 100 iterations."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we experimentally examine the effectiveness of our model in utilizing relative constraints to improve clustering. We first evaluate all methods on both UCI and other real-world datasets with noise-free constraints generated from true class labels. We then present a preliminary user study where we ask users to label constraints and evaluate all the methods on these human-labeled (noisy) constraints."}, {"heading": "A. Baseline Methods and Evaluation Metric", "text": "We compare our algorithm with existing methods that consider relative constraints or pairwise constraints. The methods employing pairwise constraints are Xing\u2019s method [2] (distance metric learning for a diagonal matrix) and ITML [26]. These are the state-of-the-art methods that are usually compared in the literature and have publicly available source code.\nFor methods considering relative constraints, we compare with: 1) LSML [15], a very recent metric learning method studying relative constraints (we use Euclidean distance as the prior); 2) SSSVaD [16], a method that directly finds clustering solutions with relative constraints; and 3) sparseLP [13], an earlier method that hasn\u2019t been extensively compared. We also experimented with a SVM-style method proposed in [12] and observed that its performance is generally worse. Thus, we do not report the results on this method.\n7\nXing\u2019s method, ITML, LSML, and sparseLP are metric learning techniques. Here we apply Kmeans with the learned metric (50 times) to form cluster assignments, and the clustering solution with the minimum mean-squared error is chosen.\nWe evaluated the clustering results based on the ground-truth class labels using pairwise F-measure [3], Adjusted Rand Index and Normalized Mutual Information. The results are highly similar with different measures, thus we only present the FMeasure results."}, {"heading": "B. Controlled Experiments", "text": "In this set of experiments, we use simulated noise-free constraints to evaluate all the methods.\n1) Datasets: We evaluate all methods on five UCI datasets: Ionosphere, Pima, Balance-Scale, Digits-389, and LettersIJLT. We also use three extra real-world datasets: 1) a subset of image segments of the MSRCv2 data1, which contains the six largest classes of the image segments; 2) the HJA Birdsong data [27], which contains automatically extracted segments from spectrograms of birdsong recordings, and the goal is to identify the species for each segment; and 3) the Stonefly9 data [28], which contains insect images and the task is to identify the species of the insect for each image. Table III summarizes the dataset information. In our experiments, all features are standardized to have zero mean and unit standard deviation.\n2) Experimental Setup: For each dataset, we vary the number of constraints from 0.05N to 0.3N with a 0.05N increment, where N is the total number of instances. For each size, triplets are randomly generated and constraint labels are assigned according to Eq. (1). We evaluated our method in two settings, one with all constraints as input (shown as DCRC), and the other with only yes/no constraints (shown as DCRCYN). The baseline methods for relative constraints are designed for yes/no constraints only and cannot be easily extended to incorporate dnk constraints, so we drop the dnk constraints for these methods. To form the corresponding pairwise constraints, we infer one ML and one CL constraints from each relative constraint with yes/no labels (note that no pairwise constraints could be directly inferred from dnk relative constraints). Thus, all the baselines use the same information as DCRC-YN, since no dnk constraints are employed by them.\nWe use five-fold cross-validation to tune parameters for all methods. The same training and validation folds are used across all the methods (removing dnk constraints, or converting to pairwise constraints when necessary). For each method,\n1http://research.microsoft.com/en-us/projects/ObjectClassRecognition/\nwe select the parameters that maximize the averaged constraint prediction accuracy on the validation sets. For our method, we search for the optimal \u03c4 \u2208 {0.5, 1, 1.5} and \u03bb \u2208 {2\u221210, 2\u22128, 2\u22126, 2\u22124, 2\u22122}. We empirically observed that our method is very robust to the choice of \u01eb when it is within the range [0.05, 0.15]. Here we set \u01eb = 0.05 for this set experiments with the simulated noise-free constraints. Experiments are repeated using 20 randomized runs, each with independently sampled constraints.\n3) Overall Performance: Figure 4 shows the performance of all methods with different number of constraints. The sparseLP does not scale to the high-dimensional Stonefly9 dataset and hence is not reported on this particular data.\nFrom the results we see that DCRC consistently outperforms all baselines on all datasets as the constraints increase, demonstrating the effectiveness of our method.\nComparing DCRC with DCRC-YN, we observe that the additional dnk constraints provide substantial benefits, especially for datasets with large number of clusters (e.g., MSRCv2, Birdsong). This is consistent with our expectation because the portion of dnk constraints increases significantly when K is large, leading to more information to be utilized by DCRC.\nComparing DCRC-YN with the baselines, we observe that DCRC-YN achieves comparable or better performance even compared with the best baseline ITML. This suggests that, with noise-free constraints, our model is competitive with the stateof-the-art methods even without considering the additional information provided by dnk constraints.\n4) Soft Constraints vs. Hard Constraints: In this set of experiments, we explore the impact on our model when soft constraints (\u01eb = 0.05) and hard constraints (\u01eb = 0) are used respectively. We first use two synthetic datasets to examine and illustrate their different behaviors. These two datasets each contain three clusters, 50 instances per cluster. The clusters are close to each other in one dataset, and far apart\n0 100 200 300 0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nNo. of Constraints\nF \u2212\nM ea\nsu re\nDCRC DCRC\u2212Hard\n(c) MSRCv2\n0 500 1000 1500 0.3\n0.35\n0.4\n0.45\n0.5\n0.55\nNo. of Constraints\nF \u2212\nM ea\nsu re\nDCRC DCRC\u2212Hard\n(d) Birdsong\nFig. 6. Performance of DCRC using soft constraints vs. hard constraints.\n(and thus easily separable) in the other. For each dataset, we randomly generated 500 relative constraints using points near the decision boundaries. Figure 5 shows the prediction entropy and prediction accuracy on instances cluster labels for both datasets achieved by our model, using soft and hard constraints respectively. We can see that when clusters are easily separable, both soft and hard constraints produce reasonable decision boundaries and perfect prediction accuracy. However, when cluster boundaries are fuzzy, the results of using soft constraints appear preferable. This indicates that by softening the constraints, our method could search for more reasonable decision to avoid overfitting to the constrained instances.\nWe then compare the performances of using soft (\u01eb = 0.05) versus hard (\u01eb = 0) constraints on real datasets with the same setting utilized in Section V-B3. Due to space limit, here we only show results on four representative datasets in Figure 6. The behavior of other datasets are similar. We can see that using soft constraints generally leads to better performance than using hard constraints. In particular, on the MSRCv2 dataset, using hard constraints produces a large \u201cdip\u201d at the beginning of the curve while this issue is not severe for soft constraints. This suggests that using soft constraints makes our model less susceptible to overfitting to small sets of constraints.\n5) Effect of Cluster Balance Enforcement: This set of experiments test the effect of the cluster balance enforcement on the performance of DCRC for the unbalanced Birdsong and the balanced Letters-IJLT datasets. Figure 7 reports the performance of DCRC (soft constraints, \u01eb = 0.05) with and without such enforcement with varied number of constraints. We see that when there is no constraint, it is generally beneficial to enforce the cluster balance. The reason is, when cluster balance is not enforced, the entropy that enforces cluster separation can be trivially reduced by removing cluster boundaries, causing degenerate solutions. However, as the constraint increases, enforcing cluster balance on the unbalanced Birdsong hurts\nthe performance. Conceivably, such enforcement would cause DCRC to prefer solutions with balanced cluster distributions, which is undesirable for datasets with uneven classes. On the other hand, appropriate enforcement on the balanced LettersIJLT dataset provides further improvement. In practice, one could determine whether to enforce cluster balance based on prior knowledge of the application domain.\n6) Computational Time: We record the runtime of learning with 1500 constraints on the Birdsong dataset, on a standard desktop computer with 3.4 GHz CPU and 11.6 GB of memory. On average it takes less than 2 minutes to train the model using an un-optimized Matlab implementation. This is reasonable for most applications with similar scale."}, {"heading": "C. Case Study: Human-labeled Constraints", "text": "We now present a case study where we investigate the impact of human-labeled constraints on the proposed method and its competitors.\n1) Dataset and Setup: This case study is situated in one of our applications where the goal is to find bird singing patterns by clustering. The birdsong dataset used in Section V-B contains spectrogram segments labeled with bird species. In reality, birds of the same species may vocalize in different patterns, which we hope to identify as different clusters. Toward this goal, we created another birdsong dataset consisting of clusters that contains relatively pure singing patterns. We briefly describe the data generation process as follows.\nWe first manually selected a collection of representative examples of the singing patterns, and then use them as templates to extract segments from birdsong spectrograms by applying template matching. Each of the extracted segments is assigned to the cluster represented by the corresponding template. We then manually inspected and edited the clusters to ensure the quality of the clusters. As a result, each cluster contains relatively pure segments that are actually from the same bird species and represent the same vocalization pattern. See Figure 1 for examples of several different vocalization patterns, which we refer to as syllables. We extract features for each segment using the same method as described in [27]. This process results in a new Birdsong dataset containing 2601 instances and 14 ground-truth clusters.\nAfter obtaining informed consents according to the protocol approved by the Institutional Review Board of our institution, we tested six human subjects\u2019 behaviors on labeling constraints. None of the users has any prior experience/knowledge\n9\non the data. They were first given a short tutorial on the data and the concepts of clustering and constraints. Then each user is asked to label randomly selected 150 triplets, and 225 pairs, using a graphical interface that displays the spectrogram segments. To neutralize the potential bias introduced by the task ordering (triplets vs. pairs), we randomly split the users into two groups with each group using a different ordering.\n2) Results and Discussion: Table IV lists the average confusion matrix of the human-labeled constraints versus the labels produced based on the ground-truth cluster labels. From Table IV(a), we see that the dnk constraints make up more than half of the relative constraints, which is consistent with our analysis in Section II-A that the number of dnk constraints can be dominantly large. The users rarely confuse between the yes and no labels but they do tend to provide more erroneous dnk labels. This phenomenon is not surprising because when in doubt, we are often more comfortable to abstain from giving an definite yes/no answer and resort to the dnk option.\nFor pairwise constraints, the CL constraints are the majority, and the confusions for both CL and ML are similar. We note that the confusion between the yes/no constraints is much smaller than that of ML/CL constraints. This shows that the increased flexibility introduced by dnk label allows the users to more accurately differentiate yes/no labels. The overall labeling accuracy of pairwise constraints is slightly higher than that of relative constraints. We suspect that this is due to the presence of the large amount of dnk constraints.\nWe evaluated all the methods using these human-labeled constraints. To account for the labeling noise in the constraints, we set \u01eb = 0.15 for DCRC and DCRC-YN2. The averaged results for all methods are listed in Table V. We observe that while most of the competing methods\u2019 performance degrade\n2For these noisy constraints, our method remains robust to the choice of \u01eb. Using different values of \u01eb ranging from 0.05 to 0.2 only introduces minor fluctuations (within 0.01 difference) to the F-measure.\nwith added constraints compared with unsupervised Kmeans, our method still shows significant performance improvement even with the noisy constraints. We want to point out that the performance difference we observe is not due to the use of the multi-class logistic classifier. In particular, as shown in Table V(a), without considering any constraints, the logistic model achieves significantly lower performance than Kmeans. This further demonstrates the effectiveness of our method in utilizing the side information provided by noisy constraints to improve clustering.\nRecall that ITML is competitive with DCRC-YN previously considering noise-free constraints. Here with noisy constraints, DCRC-YN achieves far better accuracy than ITML, suggesting that our method is much more robust to labeling noise. It is also worth noting that although the dnk constraints tend to be quite noisy, they do not seem to degrade the performance of DCRC compared with DCRC-YN.\nOur case study also points to possible ways to further improve our model. As revealed by Table IV, the noise on the labels for relative constraints is not uniform as assumed by our model. An interesting future direction is to introduce a non-uniform noise process to more realistically model the users\u2019 labeling behaviors."}, {"heading": "VI. RELATED WORK", "text": "Clustering with Constraints: Various techniques have been proposed for clustering with pairwise constraints [4]\u2013[8], [10]. Our work is aligned with most of these methods in the sense that we assume the guidance for labeling constraints is the underlying instance clusters.\nFewer work has been done on clustering with relative constraints. The work in [12]\u2013[16] propose metric learning approaches that use d(xi, xj) < d(xi, xk) to encode that xi is more similar to xj than to xk , where d(\u00b7) is the distance function. The work [15] studies learning from relative comparisons between two pairs of instances, which can be viewed as the same type of constraints when only three distinct examples are involved. By construction, these methods only consider constraints with yes/no labels. Practically, such answers might not always be provided, causing limitation of their applications. In contrast, our method is more flexible by allowing users to provide dnk constraints,\nThere also exist studies that encode the instance relative similarities in the form of hieratical ordering and attempt hierarchical algorithms that directly find clustering solutions satisfying the constraints [17], [29]. Different with those studies, our work builds on a natural probabilistic model that has not been considered for learning with relative constraints.\nSemi-supervised Learning: Related work also exists in a much broader area of semi-supervised learning, involving studies on both clustering and classification problems. The work [19] proposes that to enforce the formed clusters with large separation margins, we could minimize the entropy on the unlabeled data, in addition to learning from the labeled ones. The study [20] suggests to also maximize the entropy of the cluster label distribution in order to find balanced clustering solution. Our final formulation draws inspiration from the above work.\n10"}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, we studied clustering with relative constraints, where each constraint is generated by posing a query: is xi more similar to xj than to xk . Unlike existing methods that only consider yes/no responses to such queries, we studied the case where the answer could also be dnk (don\u2019t know). We developed a probabilistic method DCRC that learns to cluster the instances based on the responses acquired by such queries. We empirically evaluated the proposed method using both simulated (noise-free) constraints and human-labeled (noisy) constraints. The results demonstrated the usefulness of dnk constraints, the significantly improved performance of DCRC over existing methods, and the superiority of our method in terms of the robustness to noisy constraints."}], "references": [{"title": "Constrained Kmeans Clustering with Background Knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "ICML, 2001, pp. 577\u2013584.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Distance Metric Learning with Application to Clustering with Side-information", "author": ["E. Xing", "A. Ng", "M. Jordan", "S. Russell"], "venue": "NIPS, 2003, pp. 521\u2013528.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Integrating Constraints and Metric Learning in Semi-supervised Clustering", "author": ["M. Bilenko", "S. Basu", "R.J. Mooney"], "venue": "ICML, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Computing Gaussian Mixture Models with EM using Equivalence Constraints", "author": ["N. Shental", "A. Bar-hillel", "T. Hertz", "D. Weinshall"], "venue": "NIPS, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised Learning with Penalized Probabilistic Clustering", "author": ["Z. Lu", "T.K. Leen"], "venue": "NIPS, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "A Probabilistic Framework for Semi-supervised Clustering", "author": ["S. Basu", "M. Bilenko", "R.J. Mooney"], "venue": "KDD, 2004, pp. 59\u201368.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning with Constrained and Unlabelled Data", "author": ["T. Lange", "M.H.C. Law", "A.K. Jain", "J.M. Buhmann"], "venue": "CVPR, 2005, pp. 731\u2013738.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Revisiting Probabilistic Models for Clustering with Pair-wise Constraints", "author": ["B. Nelson", "I. Cohen"], "venue": "ICML, 2007, pp. 673\u2013680.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Constraint-Driven Clustering", "author": ["R. Ge", "M. Ester", "W. Jin", "I. Davidson"], "venue": "KDD, 2007, pp. 320\u2013329.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Semi-supervised Clustering with Pairwise Constraints: A Discriminative Approach", "author": ["Z. Lu"], "venue": "Journal of Machine Learning Research, vol. 2, pp. 299\u2013306, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Constrained Clustering: Advances in Algorithms, Theory, and Applications, 1st ed", "author": ["S. Basu", "I. Davidson", "K. Wagstaff"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Learning a Distance Metric from Relative Comparisons", "author": ["M. Schultz", "T. Joachims"], "venue": "NIPS, 2003, p. 41.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning Sparse Metrics via Linear Programming", "author": ["R. Rosales", "G. Fung"], "venue": "KDD, 2006, pp. 367\u2013373.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalized Sparse Metric Learning with Relative Comparisons", "author": ["K. Huang", "Y. Ying", "C. Campbell"], "venue": "Knowl. Inf. Syst., vol. 28, no. 1, pp. 25\u201345, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Metric Learning from Relative Comparisons by Minimizing Squared Residual", "author": ["E.Y. Liu", "Z. Guo", "X. Zhang", "V. Jojic", "W. Wang"], "venue": "ICDM, 2012, pp. 978\u2013983.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Semisupervised Clustering with Metric Learning using Relative Comparisons", "author": ["N. Kumar", "K. Kummamuru"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 20, no. 4, pp. 496\u2013503, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering with Relative Constraints", "author": ["E. Liu", "Z. Zhang", "W. Wang"], "venue": "KDD, 2011, pp. 947\u2013955.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised Learning by Entropy Minimization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "NIPS, 2005, pp. 33\u201340.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Discriminative Clustering by Regularized Information Maximization", "author": ["R. Gomes", "A. Krause", "P. Perona"], "venue": "NIPS, 2010, pp. 775\u2013783.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Mean Field Theory for Sigmoid Belief Networks", "author": ["L. Saul", "T. Jaakkola", "M. Jordan"], "venue": "Journal of Artificial Intelligence Research, vol. 4, pp. 61\u201376, 1996.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "A tutorial on Variational Bayesian Inference", "author": ["C.W. Fox", "S.J. Roberts"], "venue": "Artificial Intelligence Review, vol. 38, no. 2, pp. 85\u201395, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "On Mean Field Convergence and Stationary Regime", "author": ["M. Benaim", "J.-Y.L. Boudec"], "venue": "arXiv preprint arXiv:1111.5710, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "L-bfgs software", "author": ["M. Schmidt"], "venue": "Website, 2012, http://www.di.ens.fr/\u223cmschmidt/Software/minFunc.html.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Information- Theoretic Metric Learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "ICML, 2007, pp. 209\u2013216.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Rank-loss Support Instance Machines for MIML Instance Annotation", "author": ["F. Briggs", "X.Z. Fern", "R. Raich"], "venue": "KDD, 2012, pp. 534\u2013 542.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Dictionary-Free Categorization of Very Similar Objects via Stacked Evidence Trees", "author": ["G. Martinez-Munoz", "N. Larios", "E. Mortensen", "W. Zhang", "A. Yamamuro", "R. Paasch", "N. Payet", "D. Lytle", "L. Shapiro", "S. Todorovic", "A. Moldenke", "T. Dietterich"], "venue": "CVPR, 2009, pp. 549 \u2013556.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchical Constraints", "author": ["K. Bade", "A. N\u00fcrnberger"], "venue": "Machine Learning, pp. 1\u201329, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": ", [1]\u2013[11]), where a pair of instances is indicated to belong to the same cluster by a Must-Link (ML) constraint or to different clusters by a Cannot-Link (CL) constraint.", "startOffset": 2, "endOffset": 5}, {"referenceID": 10, "context": ", [1]\u2013[11]), where a pair of instances is indicated to belong to the same cluster by a Must-Link (ML) constraint or to different clusters by a Cannot-Link (CL) constraint.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "More recently, various studies [12]\u2013[17] have suggested that domain knowledge can also be incorporated in the form of relative comparisons or relative constraints, where each constraint specifies whether instance xi is more similar to xj than to xk .", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "More recently, various studies [12]\u2013[17] have suggested that domain knowledge can also be incorporated in the form of relative comparisons or relative constraints, where each constraint specifies whether instance xi is more similar to xj than to xk .", "startOffset": 36, "endOffset": 40}, {"referenceID": 11, "context": "In the area of learning from relative constraints, most work uses metric learning approaches [12]\u2013[16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "In the area of learning from relative constraints, most work uses metric learning approaches [12]\u2013[16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 17, "context": "This objective can be captured by minimizing the conditional entropy of instance cluster labels given the observed features [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "This can be achieved by maximizing the entropy of the estimated marginal distribution of cluster labels [20], i.", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "We use mean field inference [22], [23] to approximate the posterior distribution in part due to its ease of implementation and convergence properties [24].", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "We use mean field inference [22], [23] to approximate the posterior distribution in part due to its ease of implementation and convergence properties [24].", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "We use mean field inference [22], [23] to approximate the posterior distribution in part due to its ease of implementation and convergence properties [24].", "startOffset": 150, "endOffset": 154}, {"referenceID": 22, "context": "We used L-BFGS [25] in our experiments.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "The methods employing pairwise constraints are Xing\u2019s method [2] (distance metric learning for a diagonal matrix) and ITML [26].", "startOffset": 61, "endOffset": 64}, {"referenceID": 23, "context": "The methods employing pairwise constraints are Xing\u2019s method [2] (distance metric learning for a diagonal matrix) and ITML [26].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "For methods considering relative constraints, we compare with: 1) LSML [15], a very recent metric learning method studying relative constraints (we use Euclidean distance as the prior); 2) SSSVaD [16], a method that directly finds clustering solutions with relative constraints; and 3) sparseLP [13], an earlier method that hasn\u2019t been extensively compared.", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "For methods considering relative constraints, we compare with: 1) LSML [15], a very recent metric learning method studying relative constraints (we use Euclidean distance as the prior); 2) SSSVaD [16], a method that directly finds clustering solutions with relative constraints; and 3) sparseLP [13], an earlier method that hasn\u2019t been extensively compared.", "startOffset": 196, "endOffset": 200}, {"referenceID": 12, "context": "For methods considering relative constraints, we compare with: 1) LSML [15], a very recent metric learning method studying relative constraints (we use Euclidean distance as the prior); 2) SSSVaD [16], a method that directly finds clustering solutions with relative constraints; and 3) sparseLP [13], an earlier method that hasn\u2019t been extensively compared.", "startOffset": 295, "endOffset": 299}, {"referenceID": 11, "context": "We also experimented with a SVM-style method proposed in [12] and observed that its performance is generally worse.", "startOffset": 57, "endOffset": 61}, {"referenceID": 2, "context": "We evaluated the clustering results based on the ground-truth class labels using pairwise F-measure [3], Adjusted Rand Index and Normalized Mutual Information.", "startOffset": 100, "endOffset": 103}, {"referenceID": 24, "context": "We also use three extra real-world datasets: 1) a subset of image segments of the MSRCv2 data1, which contains the six largest classes of the image segments; 2) the HJA Birdsong data [27], which contains automatically extracted segments from spectrograms of birdsong recordings, and the goal is to identify the species for each segment; and 3) the Stonefly9 data [28], which contains insect images and the task is to identify the species of the insect for each image.", "startOffset": 183, "endOffset": 187}, {"referenceID": 25, "context": "We also use three extra real-world datasets: 1) a subset of image segments of the MSRCv2 data1, which contains the six largest classes of the image segments; 2) the HJA Birdsong data [27], which contains automatically extracted segments from spectrograms of birdsong recordings, and the goal is to identify the species for each segment; and 3) the Stonefly9 data [28], which contains insect images and the task is to identify the species of the insect for each image.", "startOffset": 363, "endOffset": 367}, {"referenceID": 24, "context": "We extract features for each segment using the same method as described in [27].", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "RELATED WORK Clustering with Constraints: Various techniques have been proposed for clustering with pairwise constraints [4]\u2013[8], [10].", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "RELATED WORK Clustering with Constraints: Various techniques have been proposed for clustering with pairwise constraints [4]\u2013[8], [10].", "startOffset": 125, "endOffset": 128}, {"referenceID": 9, "context": "RELATED WORK Clustering with Constraints: Various techniques have been proposed for clustering with pairwise constraints [4]\u2013[8], [10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "The work in [12]\u2013[16] propose metric learning approaches that use d(xi, xj) < d(xi, xk) to encode that xi is more similar to xj than to xk , where d(\u00b7) is the distance function.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "The work in [12]\u2013[16] propose metric learning approaches that use d(xi, xj) < d(xi, xk) to encode that xi is more similar to xj than to xk , where d(\u00b7) is the distance function.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "The work [15] studies learning from relative comparisons between two pairs of instances, which can be viewed as the same type of constraints when only three distinct examples are involved.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "In contrast, our method is more flexible by allowing users to provide dnk constraints, There also exist studies that encode the instance relative similarities in the form of hieratical ordering and attempt hierarchical algorithms that directly find clustering solutions satisfying the constraints [17], [29].", "startOffset": 297, "endOffset": 301}, {"referenceID": 26, "context": "In contrast, our method is more flexible by allowing users to provide dnk constraints, There also exist studies that encode the instance relative similarities in the form of hieratical ordering and attempt hierarchical algorithms that directly find clustering solutions satisfying the constraints [17], [29].", "startOffset": 303, "endOffset": 307}, {"referenceID": 17, "context": "The work [19] proposes that to enforce the formed clusters with large separation margins, we could minimize the entropy on the unlabeled data, in addition to learning from the labeled ones.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "The study [20] suggests to also maximize the entropy of the cluster label distribution in order to find balanced clustering solution.", "startOffset": 10, "endOffset": 14}], "year": 2015, "abstractText": "We study the problem of clustering with relative constraints, where each constraint specifies relative similarities among instances. In particular, each constraint (xi, xj , xk) is acquired by posing a query: is instance xi more similar to xj than to xk? We consider the scenario where answers to such queries are based on an underlying (but unknown) class concept, which we aim to discover via clustering. Different from most existing methods that only consider constraints derived from yes and no answers, we also incorporate don\u2019t know responses. We introduce a Discriminative Clustering method with Relative Constraints (DCRC) which assumes a natural probabilistic relationship between instances, their underlying cluster memberships, and the observed constraints. The objective is to maximize the model likelihood given the constraints, and in the meantime enforce cluster separation and cluster balance by also making use of the unlabeled instances. We evaluated the proposed method using constraints generated from ground-truth class labels, and from (noisy) human judgments from a user study. Experimental results demonstrate: 1) the usefulness of relative constraints, in particular when don\u2019t know answers are considered; 2) the improved performance of the proposed method over state-of-theart methods that utilize either relative or pairwise constraints; and 3) the robustness of our method in the presence of noisy constraints, such as those provided by human judgement.", "creator": "LaTeX with hyperref package"}}}