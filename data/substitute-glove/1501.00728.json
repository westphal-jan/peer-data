{"id": "1501.00728", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jan-2015", "title": "Differential Search Algorithm-based Parametric Optimization of Fuzzy Generalized Eigenvalue Proximal Support Vector Machine", "abstract": "Support Vector Machine (SVM) become which combination model while least phylogenetic affecting. However, SVM manage entered possible of now constants strategy still requirements training allows. In separate, SVM has one defined, example affects the performance of SVM sultanhisar. Recently, although Generalized Eigenvalue Proximal SVM (GEPSVM) because been drawn to imperative it SVM variations. In way become applications computer fall suffering by error so braking, working from yet computers even a challenging problem. In this or, being policies its been proposed one embarrassment nothing problem. This method yet called DSA - GEPSVM. The city substantially especially taken out primarily for the 2007: plus) single narrative shades means years when linear case. 13) A has Kernel factor in now measurement referring. lost) Differential Search Algorithm (DSA) is guzzling without find across i.e. values of only GEPSVM furthermore, given x86 algorithms. The experimental results show make seen cuts challenging goes able out sure the depending correlation importantly, took to lowered synonyms accuracy 2.5 now how represent algorithms.", "histories": [["v1", "Sun, 4 Jan 2015 22:12:36 GMT  (964kb)", "http://arxiv.org/abs/1501.00728v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["m h marghny", "rasha m abd elaziz", "ahmed i taloba"], "accepted": false, "id": "1501.00728"}, "pdf": {"name": "1501.00728.pdf", "metadata": {"source": "META", "title": "Differential Search Algorithm-based Parametric Optimization of Fuzzy Generalized Eigenvalue Proximal Support Vector Machine", "authors": ["M. H. Marghny", "Rasha M. Abd El-Aziz", "Ahmed I. Taloba"], "emails": [], "sections": [{"heading": null, "text": "many classification problems. However, SVM needs the solution of a quadratic program which require specialized code. In addition, SVM has many parameters, which affects the performance of SVM classifier. Recently, the Generalized Eigenvalue Proximal SVM (GEPSVM) has been presented to solve the SVM complexity. In real world applications data may affected by error or noise, working with this data is a challenging problem. In this paper, an approach has been proposed to overcome this problem. This method is called DSA-GEPSVM. The main improvements are carried out based on the following: 1) a novel fuzzy values in the linear case. 2) A new Kernel function in the nonlinear case. 3) Differential Search Algorithm (DSA) is reformulated to find near optimal values of the GEPSVM parameters and its kernel parameters. The experimental results show that the proposed approach is able to find the suitable parameter values, and has higher classification accuracy compared with some other algorithms.\nKeywords Support Vector Machines, Generalized Eigenvalues, Proximal Classifier, Fuzzy Data Classification, Differential Search Algorithm, Kernel Function."}, {"heading": "1. INTRODUCTION", "text": "Recently, information, growing in huge volumes creates the need to process large amounts of data. In order to find hidden patterns of data and convert them into useful knowledge, this is known Data Mining. This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9]. This area of research has recently become more and more important.\nClassification is the process of arranging data into homogenous group or classes according to some common characteristics present in the data. Support vector machine (SVM) has an excellent performance in many real life classification problems such as image processing, text classification and bioinformatics.\nSVM which is an emerging data classification technique proposed by Vapnik in 1995 [10], and has been widely adopted in various fields of classification, nevertheless it suffers from complexity and parameters selection. A new method has been introduced in [11] by Olvi L. Mangasarian which called Proximal Support Vector Machine (PSVM). This method has solved the problem of complexity of standard SVM, but it suffers from poor performance in the case of noisy and unbalanced data. Recently an efficient approach to\nPSVM has been proposed also by Olvi L. Mangasarian which is called the Generalized Eigenvalue Proximal Support Vector Machine (GEPSVM) [12]. The complexity of standard support vector machine has been solved by GEPSVM. A fundamental difference between GEPSVM and SVM is that, GEPSVM solves two generalized eigenvalue problems to obtain two non-parallel hyper-planes, whereas, SVM solves one quadratic programming problem (QPP) to obtain one hyper-plane. Therefore, GEPSVM works faster than SVM. Experimental results in [12] showed the effectiveness of GEPSVM on some public datasets.\nIn real world applications data may affected by noise or error which significantly influences on the performance of GEPSVM. There are many approaches have been proposed by researchers for this problem [13-19]. More efforts are needed in order to improve the performance of the classification task in this type of data.\nIn addition, the major problems that are encounter in SVM and all its inferred methods are how to find near optimal values for the SVM parameters and select a SVM kernel as well as tuning its parameters. Unsuitable parameters setting lead to poor classification outcomes. Authors in [20-25] tried to find solution for SVM parameters. There are no particular method to find the optimal values for the SVM or GEPSVM parameters and kernel parameters. This problem is still an interesting topic for more research to find more appropriate values for GEPSVM parameters and kernel parameters.\nFor these reasons, an improved version of GEPSVM, called DSA-GEPSVM for short is proposed. A new method for computing the fuzzy membership function is used in the linear case. Furthermore, a new kernel is used in the nonlinear case. The new kernel is a combination between the polynomial and the radial base function kernel. For solving the problem of parameters selection, a new and powerful method which is called Differential Search Algorithm (DSA) [26-31] has been used. This makes the optimal separating hyper-planes obtainable in both linear and non-linear classification problems.\nThe remainder of this paper is organized as follows. In section 2 we briefly give description of the GEPSVM. Section 3 give description of the DSA. In section 4 the proposed method is described. Section 5 reports experimental results. Finally, the conclusions make up Section 6."}, {"heading": "2. GENERALIZED EIGENVALUE PSVM", "text": "In 2006 Olvi L. Mangasarian and Edward W. Wild proposed the Generalized Eigenvalue Proximal Support Vector Machine GEPSVM [12] as a generalization of the SVM method. The new formulation does not need the planes to be parallel, but for each class, the algorithm finds a plane that is\nas close as possible to the points of one class and as far as possible to those in the other class. Due to the simplicity of GEPSVM, many researchers have refined it to improve the general performance of the classifier [32-35]. But GEPSVM still needs more improvements, and is a very good topic for researchers.\nFirst we consider the classification problem of m points in the n dimensional real space \ud835\udc45\ud835\udc5b , represented by the \ud835\udc5a1 \u00d7 \ud835\udc5b matrix A belonging to class 1 and \ud835\udc5a2 \u00d7 \ud835\udc5b matrix B belonging to class 2, with \ud835\udc5a1 + \ud835\udc5a2 = \ud835\udc5a. For this problem, a standard linear SVM is given by a plane halfway between the two parallel bounding planes that bound two disjoint half spaces each containing points mostly of class 1 or 2 [12].\nIn MSPSVM the parallelism condition has been dropped, but requires that each plane be as close as possible to one of the data sets and as far as possible from the other one. Thus, we are seeking for the two planes in \ud835\udc45\ud835\udc5b :\n\ud835\udc431:\ud835\udc65\u2032\ud835\udc641 \u2212 \ud835\udefe1 = 0, \ud835\udc432: \ud835\udc65\u2032\ud835\udc642 \u2212 \ud835\udefe2 = 0, (1)\nwhere the plane P1 is closest to the points of class 1 and furthest from the points in class 2, while the plane P2 is closest to the points in class 2 and furthest from the points in class 1. Then the first plane of (1) is obtained by solving the following optimization problem:\nmin(\ud835\udc64 ,\ud835\udefe)\u22600 \ud835\udc34\ud835\udc64\u2212\ud835\udc52\ud835\udefe 2 /\n\ud835\udc64 \ud835\udefe\n2\n\ud835\udc35\ud835\udc64\u2212\ud835\udc52\ud835\udefe 2 / \ud835\udc64 \ud835\udefe\n2 , (2)\nwhere . is the two-norm, and it has been assumed in [12] that \ud835\udc64 , \ud835\udefe \u2260 0 which implies to \ud835\udc35\ud835\udc64 \u2212 \ud835\udc52 \ud835\udefe \u2260 0. As introduced in [12] the numerator of the minimization problem (2) is the sum of squares of two-norm distances in the (\ud835\udc64, \ud835\udefe)-space of points in the first class to the plane \ud835\udc65\u2032\ud835\udc641 \u2212 \ud835\udefe1 = 0, while the denominator of (2) is the sum of squares of two norm distances in the (\ud835\udc64, \ud835\udefe) space of points in the second class to the same plane. By simplifying (2) we can write,\nmin(\ud835\udc64 ,\ud835\udefe)\u22600 \ud835\udc34\ud835\udc64\u2212\ud835\udc52\ud835\udefe 2\n\ud835\udc35\ud835\udc64\u2212\ud835\udc52\ud835\udefe 2 . (3)\nThen Tikhonov regularization term is added [36] to reduce the norm of the problem variables \ud835\udc64, \ud835\udefe that determine the proximal planes (1). Thus, for a parameter \ud835\udeff , problem (3) has been rewritten as follows:\nmin(\ud835\udc64 ,\ud835\udefe)\u22600 \ud835\udc34\ud835\udc64\u2212\ud835\udc52\ud835\udefe 2+ \ud835\udeff\n\ud835\udc64 \ud835\udefe\n2\n\ud835\udc35\ud835\udc64\u2212\ud835\udc52\ud835\udefe 2 , (4)\nWe can rewrite (4) as follow:\nmin \ud835\udc67\u22600 \ud835\udc5f \ud835\udc67 \u2254 \ud835\udc67 \u2032\ud835\udc3a \ud835\udc67\n\ud835\udc67 \u2032\ud835\udc3b \ud835\udc67 ,\n(5)\nwhere,\n\ud835\udc3a \u2236= \ud835\udc34 \u2212\ud835\udc52 \u2032 \ud835\udc34 \u2212\ud835\udc52 + \ud835\udeff \ud835\udc3c, (6)\n\ud835\udc3b \u2236= \ud835\udc35 \u2212\ud835\udc52 \u2032 \ud835\udc35 \u2212\ud835\udc52 , \ud835\udc67 \u2254 \ud835\udc64 \ud835\udefe . (7)\nG and H are symmetric matrices in \ud835\udc45(\ud835\udc5b+1)\u00d7(\ud835\udc5b+1) and I is an identity matrix.\nAs pointed out in [37], the objective function of (5) is known as the Rayleigh quotient, hence its solution can be obtained by solving a generalized eigenvalues problem. That is, the eigenvector corresponding to the smallest eigenvalue can determine a plane effectively.\nSimilarly we can directly get the second plane by solving the following optimization problem.\nmin \ud835\udc67\u22600 \ud835\udc60 \ud835\udc67 \u2254 \ud835\udc67 \u2032 \ud835\udc3f \ud835\udc67\n\ud835\udc67 \u2032\ud835\udc40 \ud835\udc67 ,\n(8)\nwhere,\n\ud835\udc3f \u2236= \ud835\udc35 \u2212\ud835\udc52 \u2032 \ud835\udc35 \u2212\ud835\udc52 + \ud835\udeff \ud835\udc3c, (9)\n\ud835\udc40 \u2236= \ud835\udc34 \u2212\ud835\udc52 \u2032 \ud835\udc34 \u2212\ud835\udc52 . (10)\nL and M are again symmetric matrices in \ud835\udc45(\ud835\udc5b+1)\u00d7(\ud835\udc5b+1). As analyzed above, the two non-parallel planes can be obtained directly by solving the classical generalized eigenvalue problem.\nThe Nonlinear GEPSVM can be obtained easily by considering the problem of finding two non-parallel planes\n\ud835\udc431:\ud835\udc3e \ud835\udc65\u2032 ,\ud835\udc36 \u2032 \ud835\udc621 \u2212 \ud835\udefe1 = 0, \ud835\udc432:\ud835\udc3e \ud835\udc65\u2032 ,\ud835\udc36 \u2032 \ud835\udc622 \u2212 \ud835\udefe2 = 0, (11)\nwhere \ud835\udc36 \u2236= \ud835\udc34 \ud835\udc35 .\nK is the kernel function, which will be presented in the next section. By employing the same regularization strategy as in equations (4-10), we can also obtain the two non-parallel planes by solving the optimization problems of the following equations:\nmin \ud835\udc67\u22600 \ud835\udc5f \ud835\udc67 \u2254 \ud835\udc67 \u2032\ud835\udc3a \ud835\udc67\n\ud835\udc67 \u2032\ud835\udc3b \ud835\udc67 , \ud835\udc64\ud835\udc52\ud835\udc5f\ud835\udc52 \ud835\udc67 \u2236= \ud835\udc62 \ud835\udefe ,\n(12)\nmin \ud835\udc67\u22600 \ud835\udc60 \ud835\udc67 \u2254 \ud835\udc67 \u2032 \ud835\udc3f \ud835\udc67\n\ud835\udc67 \u2032\ud835\udc40 \ud835\udc67 , \ud835\udc64\ud835\udc52\ud835\udc5f\ud835\udc52 \ud835\udc67 \u2236= \ud835\udc62 \ud835\udefe ,\n(13)\nwhere,\n\ud835\udc3a \u2236= \ud835\udc3e(\ud835\udc34,\ud835\udc36 \u2032) \u2212\ud835\udc52 \u2032 \ud835\udc3e(\ud835\udc34,\ud835\udc36 \u2032) \u2212\ud835\udc52 + \ud835\udeff \ud835\udc3c, (14)\n\ud835\udc3b \u2236= \ud835\udc3e(\ud835\udc35,\ud835\udc36 \u2032) \u2212\ud835\udc52 \u2032 \ud835\udc3e(\ud835\udc35,\ud835\udc36 \u2032) \u2212\ud835\udc52 , (15)\n\ud835\udc3f \u2236= \ud835\udc3e(\ud835\udc35,\ud835\udc36 \u2032) \u2212\ud835\udc52 \u2032 \ud835\udc3e(\ud835\udc35,\ud835\udc36 \u2032) \u2212\ud835\udc52 + \ud835\udeff \ud835\udc3c, (16)\n\ud835\udc40 \u2236= \ud835\udc3e(\ud835\udc34,\ud835\udc36 \u2032) \u2212\ud835\udc52 \u2032 \ud835\udc3e(\ud835\udc34,\ud835\udc36 \u2032) \u2212\ud835\udc52 . (17)\nG, H, L and M are symmetric matrices in \ud835\udc45(\ud835\udc5a+1)\u00d7(\ud835\udc5a+1).\nSince 2006 GEPSVM has achieved great performance in many real live applications, but in some cases data may affected by noise and errors. Most classification methods give low classification accuracy with this kind of data, and need some modifications in order to increase the classification accuracy. One of the most effective ways to overcome this problem is by adding a fuzzy value to each training sample. The works of many researches carried out by adding fuzzy values to the standard SVM [13, 14, 17, 18]. Many attempts for adding fuzzy to GEPSVM have been illustrated as in [15, 16, 19]. A first attempt to obtain a fuzzy version of the GEPSVM classification is presented in [15, 16]. In [15] the authors attempt to solve the following problem:\nmin(\ud835\udc64 ,\ud835\udefe)\u22600 \ud835\udc46\ud835\udc34 \ud835\udc34\ud835\udc64\u2212\ud835\udc52\ud835\udefe\n2\n\ud835\udc46\ud835\udc35 \ud835\udc35\ud835\udc64\u2212\ud835\udc52\ud835\udefe 2 . (18)\nWith SA is the fuzzy membership weights for each point Ai and SB is the fuzzy membership weights for each point Bi. S A and SB are diagonal matrices.\n\ud835\udc46\ud835\udc56\ud835\udc56 \ud835\udc34 = 0.5 +\n\ud835\udc52\ud835\udc53(\ud835\udc51 \ud835\udc34\ud835\udc56 ,\ud835\udc36\ud835\udc35 \u2212 \ud835\udc51 \ud835\udc34\ud835\udc56 ,\ud835\udc36\ud835\udc34 )/\ud835\udc51\ud835\udc34\ud835\udc35\u2212 \ud835\udc52\u2212\ud835\udc53\n2(\ud835\udc52\ud835\udc53 \u2212 \ud835\udc52\u2212\ud835\udc53) , \ud835\udc56 = 1,\u2026 , \ud835\udc5d, (19)\n\ud835\udc46\ud835\udc56\ud835\udc56 \ud835\udc35 = 0.5 +\n\ud835\udc52\ud835\udc53(\ud835\udc51 \ud835\udc35\ud835\udc56 ,\ud835\udc36\ud835\udc34 \u2212 \ud835\udc51 \ud835\udc35\ud835\udc56 ,\ud835\udc36\ud835\udc35 )/\ud835\udc51\ud835\udc34\ud835\udc35\u2212 \ud835\udc52\u2212\ud835\udc53\n2(\ud835\udc52\ud835\udc53 \u2212 \ud835\udc52\u2212\ud835\udc53) , \ud835\udc56 = 1,\u2026 ,\ud835\udc5a. (20)\nWhere CA and CB are the center of mass of the two classes, dAB is the distance between the two means, the function \ud835\udc51(. , . ) is the Euclidean distance between two points, and f is a constant that determines the rate at which the fuzzy membership decreases towards 0.5. Another recently attempt of fuzzy GEPSVM can be found in [19] where the author proposed the following fuzzy function\n\ud835\udc46\ud835\udc56\ud835\udc56 \ud835\udc34 = \ud835\udc60 + 1 \u2212 \ud835\udc60 . \ud835\udc52\n\u2212 min (\ud835\udc51(\ud835\udc34\ud835\udc56 ,\ud835\udc36\ud835\udc34 max (\ud835\udc51(\ud835\udc34\ud835\udc56 ,\ud835\udc36\ud835\udc35\n2\n. (21)\nWhere min (d(Ai,CA)) is the minimum distance of the point Ai from the centers in CA, max (d(Ai,CB)) is the maximum distance of the point Ai from the centers CB of the other class, and s is a parameter weighting the contribution of the exponential term to SA ,for more detail see [19] ."}, {"heading": "3. DIFFERENTIAL SEARCH ALGORITHM (DSA)", "text": "Differential Search Algorithm (DSA) is a recently and efficient evolutionary algorithm. DSA is effectively used to solve numerical optimization problems. The main idea of the DSA algorithm was inspired form the migration of superorganisms making use of brownian like motion [28].\nAlgorithms that make use of the principle of evolutionary computation are known as Evolutionary Algorithms (EA). These algorithms are suitable to search for the optimal (best) solution of many optimization problems. In real world problems the optimization process may have more than one solution, for searching for the optimal solution among all these solutions in a short time is a challenging task. If the search space is small then searching for the optimal solution will take short time. Working with data whose search space is very large is a challenge for most researchers. When the problem is very large with a great number of possible solutions, then finding the optimal solution is difficult. Evolutionary computation techniques are powerful and effective with this kind of data. EA includes the following techniques [28]:\n Ant colony algorithm  Artificial Bee Colony (ABC) algorithm  Cultural algorithms  Differential evolution  Evolutionary algorithms  Evolutionary programming  Evolution strategy  Gene expression programming  Genetic algorithm  Genetic programming  Harmony search  Learnable Evolution Model  Particle swarm optimization  Self-organization such as self-organizing maps\n Swarm intelligence\nThe Differential Search Algorithm (DSA) is the most recent addition. There are a number of computational-intelligence algorithms that model the behaviors of the superorganisms [28-30]. In the present work DSA is used to get the best values of parameter values in the proposed algorithm, due to it has the ability to manage such problem. The pseudo-code indicating the function of DS algorithm is given in Appendix."}, {"heading": "4. PROPOSED APPROACH", "text": ""}, {"heading": "4.1 Linear Fuzzy DSA-GEPSVM", "text": "The proposed approach introduce a technique for computing the fuzzy membership values. If the data affected with noise or outliers then the classification process will influence, so the data needs some preprocessing steps. We propose a method by adding a fuzzy value for those examples that away from the center of the class and the remaining examples don\u2019t have any change. Now the new formulation of the problems become as follow:\nmin(\ud835\udc64 ,\ud835\udefe)\u22600 \ud835\udc46\ud835\udc34 \ud835\udc34\ud835\udc64\u2212\ud835\udc52\ud835\udefe\n2 + \ud835\udeff \ud835\udc64 \ud835\udefe\n2\n\ud835\udc46\ud835\udc35 \ud835\udc35\ud835\udc64\u2212\ud835\udc52\ud835\udefe 2 ,\n(22)\nwhere,\n\ud835\udc3a \u2236= \ud835\udc46\ud835\udc34 \ud835\udc34 \u2212\ud835\udc52 \u2032 \ud835\udc46\ud835\udc34 \ud835\udc34 \u2212\ud835\udc52 + \ud835\udeff \ud835\udc3c, (23)\n\ud835\udc3b \u2236= \ud835\udc46\ud835\udc35\ud835\udc35 \u2212\ud835\udc52 \u2032 \ud835\udc46\ud835\udc35 \ud835\udc35 \u2212\ud835\udc52 , \ud835\udc67 \u2254 \ud835\udc64 \ud835\udefe . (24)\nThe optimization problem (22) becomes:\nmin \ud835\udc67\u22600 \ud835\udc5f \ud835\udc67 \u2254 \ud835\udc67 \u2032\ud835\udc3a \ud835\udc67\n\ud835\udc67 \u2032\ud835\udc3b \ud835\udc67 .\n(25)\nSimilarly we can directly get the second fuzzy plane by solving the following optimization problem.\nmin \ud835\udc67\u22600 \ud835\udc60 \ud835\udc67 \u2254 \ud835\udc67 \u2032 \ud835\udc3f \ud835\udc67\n\ud835\udc67 \u2032\ud835\udc40 \ud835\udc67 ,\n(26)\nwhere,\n\ud835\udc3f \u2236= \ud835\udc46\ud835\udc35 \ud835\udc35 \u2212\ud835\udc52 \u2032 \ud835\udc46\ud835\udc35 \ud835\udc35 \u2212\ud835\udc52 + \ud835\udeff \ud835\udc3c, (27)\n\ud835\udc40 \u2236= \ud835\udc46\ud835\udc34\ud835\udc34 \u2212\ud835\udc52 \u2032 \ud835\udc46\ud835\udc34 \ud835\udc34 \u2212\ud835\udc52 . (28)\nFigure 1 explains the process of computing fuzzy matrix SA.\nWhere s is a parameter weighting the contribution of the exponential term to SA. The same way for SB. The next step and the most important step in the proposed method is how we can get the optimal parameters. In the linear MSPSVM there is only one parameter \ud835\udeff. DSA algorithm has been used to find the optimal value of \ud835\udeff."}, {"heading": "4.2 Nonlinear DSA-GEPSVM", "text": "Appropriate choice of the kernel function increases the accuracy of the classification. In real life applications the choice of kernel function depends on the dataset used.\nHere are some of the most popular kernels.\nPolynomial function:\nA polynomial kernel is a common method for nonlinear modeling.\n\ud835\udc3e \ud835\udc65, \ud835\udc65\u2032 = (< \ud835\udc65, \ud835\udc65\u2032 > +1)\ud835\udc51 . (29)\nGaussian radial basis function:\nThis function has received significant attention, most commonly with a Gaussian of the form,\n\ud835\udc3e \ud835\udc65, \ud835\udc65\u2032 = \ud835\udc52\ud835\udc65\ud835\udc5d \u2212 \ud835\udc65\u2212\ud835\udc65 \u2032 2\n2 \ud835\udf0e2 . (30)\nExponential radial basis function:\n\ud835\udc3e \ud835\udc65, \ud835\udc65\u2032 = \ud835\udc52\ud835\udc65\ud835\udc5d((\u2212 \ud835\udc65 \u2212 \ud835\udc65\u2032 )/(\ud835\udc37 \ud835\udf0e )) . (31)\nIn [38] a new kernel has been introduced the author used the new kernel with the standard SVM, in the presented work we use the new kernel which is called PolyRBF witch is a hybrid between a polynomial kernel and a Gaussian RBF kernel.\n\ud835\udc3e \ud835\udc65, \ud835\udc65\u2032 = (1 + \ud835\udc52\ud835\udc65\ud835\udc5d \u2212 \ud835\udc65\u2212\ud835\udc65 \u2032 2\n\ud835\udc37 \ud835\udf0e )\ud835\udc51 . (32)\nWhere D is the dimension of the data, now we have three parameters in the nonlinear case, the first parameter is the regularization of GEPSVM and second and third parameters is for the kernel function if we use the polynomial kernel then we have the parameter d, and if we use the RBF then we have the parameter \ud835\udf0e, last if we use the PolyRBF the we have two parameters d and \ud835\udf0e.\nIn order to get the nonlinear planes in equation (11), we proposed to use the PolyRBF kernel. In the next subsection we explain how to get the best parameters in the linear and nonlinear classifier."}, {"heading": "4.3 Parameter Optimization using DSA", "text": "A population in DSA assumed to be made up of random solutions of the problem corresponds to an artificialsuperorganism migrating. In DSA, artificial- superorganism migrates to the global minimum value of the optimization problem. In the migration time the artificial-superorganism tests whether some positions which was selected randomly are suitable temporary during the migration. Then the process stops over on the suitable tested position for a temporary time during the migration, the members of the artificial that made such discovery immediately settle at the discovered position and continue their migration from this position [28].\nIn the implementation of DSA, artificial-organisms (i.e., Xi, i={1,2,3,\u2026,N}) making up an artificial-superorganism (i.e., Superorganismg, g={1,2,3,\u2026,G}) contain members as much as the size of the problem (i.e., xij, j={1,2,3,\u2026,D}). Where, N signifies number of elements in the superorganism (Size of the population), G represents number of maximum generation, and D indicates size of the problem [28-30].\n\ud835\udc65\ud835\udc56\ud835\udc57 = \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc51. \ud835\udc62\ud835\udc5d\ud835\udc57 \u2212 \ud835\udc59\ud835\udc5c\ud835\udc64\ud835\udc57 + \ud835\udc59\ud835\udc5c\ud835\udc64\ud835\udc57 (33)\nThe stopover site is an important step in migration. The method to find a stopover site at the remaining between the artificial- organisms may be described by a Brownian-like random walk model [28]. By a random selection of individuals of the artificial- organisms move toward the targets of \ud835\udc51\ud835\udc5c\ud835\udc5b\ud835\udc5c\ud835\udc5f = \ud835\udc4b\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc5c\ud835\udc5a _\ud835\udc60\ud835\udc62\ud835\udc53\ud835\udc53\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc54 (\ud835\udc56) to discover stopover sites. The scale value (R) is used to control the size of the change occurred in the positions of members of the artificialorganisms. The way of calculation R makes the respective artificial-superorganism to radically change direction in the habitat [28-30].\nThe stopover site position in DSA is produced by using equation (34):\n\ud835\udc46\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc46\ud835\udc56\ud835\udc61\ud835\udc52 = \ud835\udc46\ud835\udc62\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc5f\ud835\udc54\ud835\udc4e\ud835\udc5b\ud835\udc56\ud835\udc60\ud835\udc5a+ \ud835\udc46\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc52 \u2217 (\ud835\udc51\ud835\udc5c\ud835\udc5b\ud835\udc5c\ud835\udc5f \u2212 \ud835\udc46\ud835\udc62\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc5f\ud835\udc54\ud835\udc4e\ud835\udc5b\ud835\udc56\ud835\udc60\ud835\udc5a) (34)\nA random process is used to determine the members of the artificial organisms of the superorganism of stopover site. If the one of the stopover site elements goes outside the limits of the search space for any reason, it randomly deferred to another position in the search space. If the stopover site is better than the sources owned by the artificial-organism, the artificial-organism moves to that stopover site. While the artificial-organisms change site, the superorganism containing the artificial organisms continues its migration to the global minimum.\nThere are two control parameters in DSA, which are p1 and p2. The tested and the most appropriate values for these parameters were conducted by [28]. Figure 2 describes the main steps of the proposed approach.\nIn the proposed approach we are seeking for optimal values for the linear and nonlinear parameters. The stopping criteria for the procedure is either max number of cycles are reached or the 100% accuracy is obtained."}, {"heading": "5. EXPERIMENTAL RESULTS", "text": "The proposed approach is implemented on personal computer with a core i3 processor 2.13GHz, 3GB of RAM, and windows 8.1 operating system. Matlab 2010b framework is used in development. To verify the proposed approach quality the following datasets are used from UCI repository [39], the datasets characteristics are shown in Table 1.\nTables 2 and 6 summarize all parameters setting in the linear and nonlinear DSA-MSPSVM respectively with their assigned values. Where the values are chosen based on our numerical experiments.\nFor implementation, the data was divided into ten parts or folds, nine of which comprised the training data, with the tenth being used for testing the generalization ability of the classifier. This process was repeated ten times, using a different fold for testing on each occasion. This process is known as tenfold cross validation and is a standard methodology for reporting the performance of a classifier. The classification accuracy was computed by computing the average across all the ten trials.\nTable 2 Linear GEPSVM parameter\nParameter Symbol Interval\nP1 \ud835\udeff [0.001,10000]\nThe DSA parameters setting are shown in table 3.\nTable 4 illustrates the results obtained after implementing Linear DSA-GEPSVM on several public benchmark datasets.\nTable 5 illustrates the comparison between Linear DSAGEPSVM and four recently methods GEPSVM, FSVM, FTSVM, and IGEPSVM. The proposed method given promising results for all dataset from other methods, and the mean accuracy of proposed method is the best.\nTable 5 Training accuracy of linear DSA-GEPSVM and compared methods on UCI datasets\nDataset Proposed GEPSVM\n[12]\nFSVM\n[13]\nFTSVM\n[18]\nIGEPSVM\n[36]\nAustralian 70.6924 - 85.56 86.08 -\nBreast Cancer 97.3984 - 65. 01 65.60 -\nLiver Disorders 70.7395 68.86 76.67 77.80 73.83\nDiabetes 74.2775 67.93 - - 74.61\nGerman 74.7778 75.49 71.68 78.20 77.15\nHeart Disease 87.2428 - 83.33 84.44 -\nSonar 91.9786 83.66 - - 88.47\nwpbc 89.3258 83.98 - - 87.74\nmean 82.0541 75.984 79.31 78.424 80.36\nThe deferent kernels were applied to nonlinear DSAGEPSVM. The first kernel is the polynomial kernel, the second kernel is the radial base function kernel and the last one the hybrid kernel between the previously mentioned kernels. In order to prove how the hybrid kernel is effective, we applied the nonlinear DSA-GEPSVM three times on each kernel. Table 7 shows the detailed results that obtained. Figure 3 shows how the hybrid kernel is effective in most cases.\nTable 8 and Table 9 illustrate the comparison between training and testing accuracies of nonlinear DSA-GEPSVM on four datasets with other recently methods GA+SVM, SA+SVM, PSO+SVM, CV-ACC, and S.C. Chen. Results proved how the proposed approach give comparable and promising results.\nGA+SVM[20] 94.23 94.58 96.61 95.22\nSA+SVM[21] 97.95 87.97 97.5 91.85\nPSO+SVM[22] 97.95 88.17 97.5 88.32\nCV-ACC[23] 96.69 84.753 97.714 100\nS.C. Chen [24] 96.04 86.32 96.60 96.07"}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we presented a DSA-MSPSVM method for data classification based on MSPSVM and DSA approaches. It is well known that the MSPSVM regularization parameter \ud835\udeff and kernel parameters are important to the performance of the classifier. But it is difficult to choose a kernel function and its parameters because they are dependent on datasets. The DSA has been applied to optimize these parameters. We conducted experiments to evaluate the performance of the proposed approach with three different kernel functions Poly, RBF, and PolyRBF in the nonlinear classifier. The results obtained were compared with those obtained with other algorithms. The results show enough evidence that the proposed approach has less error rates across most of the datasets with other algorithms. We can also conclude that PolyRBF kernel gives better results as compared with other kernel functions. Further, we plan to extend the DSA-MSPSVM approach to deal with multiclass problems in the linear and nonlinear cases, and study the kernel function effects in the datasets."}, {"heading": "7. REFERENCES", "text": "[1] M.H. Marghny, Rasha M. Abd El-Aziz, and Ahmed I.\nTaloba. An Effective Evolutionary Clustering Algorithm: Hepatitis C case study. International Journal of Computer Applications, 2011, 34(6): 1-6.\n[2] M.H. Marghny, and Ahmed I. Taloba. Outlier Detection using Improved Genetic K-means. International Journal\nof Computer Applications, 2011, 28(11): 33-36.\n[3] Adel A. Sewisy, M.H. Marghny, Rasha M. Abd El-Aziz, and Ahmed I. Taloba. Fast Efficient Clustering\nAlgorithm for Balanced Data. International Journal of Advanced Computer Science and Applications (IJACSA), 2014, 5(6): 123-129.\n[4] C.C. Aggarwal, and C.K Reddy. Data clustering: algorithms and applications. Chapman and Hall/CRC\nPress, 2013.\n[5] M.H. Marghny, and I.E. El-Semman. Extracting logical classification rules with gene expression programming:\nmicroarray case study. Proceedings of the International Conference on Artificial Intelligence and Machine Learning (AIML 05), Cairo, Egypt, 2005, 11\u201316.\n[6] M.H. Marghny, and I.E. El-Semman. Extracting fuzzy classification rules with gene expression programming. .\nProceedings of the International Conference on Artificial Intelligence and Machine Learning (AIML 05), Cairo, Egypt, 2005.\n[7] M.H. Marghny, and H.E. Refaat. A new parallel association rule mining algorithm on distributed shared\nmemory system. International Journal of Business Intelligence and Data Mining, 2012, 7(4): 233-252.\n[8] M.H. Marghny, and A.A. Shakour. Fast, Simple and Memory Efficient Algorithm for Mining Association\nRules. International Review on Computers & Software, 2007, 2(1).\n[9] M.H. Marghny, and A.A. Shakour. Scalable Algorithm for Mining Association Rules. ICCST, 2006, 6(3): 55-60.\n[10] C. Cortes, and V. N. Vapnik. Support vector networks. Machine Learning, 1995, 20(3): 273 \u2013 297.\n[11] Glenn Fung, and Olvi L. Mangasarian. Proximal support vector machine classifiers. In Proceedings of the Seventh\nACM SIGKDD International conference on knowledge discovery and data mining, KDD 01, ACM, New York, NY, USA, 2001, 77\u201386.\n[12] Olvi L. Mangasarian, and Edward W. Wild. Multisurface proximal support vector machine classification via\ngeneralized eigenvalues. IEEE transaction on pattern analysis and machine intelligence, 2006, 28(1): 69\u201374.\n[13] Abe S, and Inoue T. Fuzzy Support Vector Machines for Pattern Classification. IEEE, 2001, 2: 1449-1454.\n[14] Chun-Fu Lin, and Sheng-De Wang. Fuzzy support vector machines. IEEE transactions on neural networks, 2002,\n13(2): 464-471.\n[15] Jayadeva, R. Khemchandani, and S. Chandra. Fuzzy proximal support vector classification via generalized\neigenvalues. Springer-Verlag Berlin Heidelberg, 2005, 360 \u2013363.\n[16] Jayadeva, R. Khemchandani, and S. Chandra. Fuzzy multi-category proximal support vector classification via\ngeneralized eigenvalues. Soft Computing \u2013 A Fusion of Foundations, Methodologies and Applications, 2007, 11: 679\u2013685.\n[17] Ding Shifei, and Gu Yaxiang. A fuzzy support vector machine algorithm with dual membership based on\nhypersphere, Journal of computational information systems, 2011, 7(6): 2028-2034.\n[18] Li. Kai, and Hongyan Ma. A fuzzy twin support vector machine algorithm. International journal of application\nor innovation in engineering & management, 2013, 2(3): 459- 465.\n[19] M.R. Guarracino, A. Irpino, R. Jasinevicius, and R. Verde. Fuzzy regularized generalized eigenvalue\nclassifier with a novel membership function. Information Sciences, 2013, 245: 53\u201362.\n[20] Huang. Cheng-Lung, and Chieh-Jen Wang. A ga-based feature selection and parameters optimization for support\nvector machines. Expert systems with applications, 2006, 31: 231\u2013240.\n[21] S. Lin, Z. Lee, S. Chen, and T. Tseng. Parameter determination of support vector machine and feature\nselection using simulated annealing approach. Applied soft computing, 2008, 8(4): 1505\u20131512.\n[22] S. Lin, K. Ying, S. Chen, and Z. Lee. Particle swarm optimization for parameter determination and feature\nselection of support vector machines. Expert systems with applications, 2008, 35(4): 1817\u20131824.\n[23] L. Luo, D. Huang, H. Peng, Q. Zhou, G. Shao, and F. Yang. A new parameter selection method for support\nvector machine based on the decision value. Convergence information technology, 2010, 5(8): 36\u201341.\n[24] S.C. Chen, S.W. Linb, and S.Y. Chou. Enhancing the classification accuracy by scatter-based ensemble\napproach. Applied soft computing, 2011, 11(1): 1021\u2013 1028.\n[25] X. Zhang, D. Qiu, and F. Chen. Support vector machine with parameter optimization by a novel hybrid method\nand its application to fault diagnosis. Neurocomputing, 2014, 149(Part B): 641\u2013651.\n[26] P. Civicioglu. Understanding the nature of evolutionary search algorithms. Additional technical report for the\nproject of 110Y309-Tubitak, 2013.\n[27] D. Goswami, and S. Chakraborty. Differential search algorithm-based parametric optimization of\nelectrochemical micromachining processes. International journal of industrial engineering computations, 2014, 5(1): 41\u201354.\n[28] P. Civicioglu. Transforming geocentric cartesian coordinates to geodetic coordinates by using differential\nsearch algorithm. Computers & Geosciences, 2012, 46: 229-247.\n[29] X. Song, L. Li, X. Zhang, X. Shi, J. Huang, J. Cai, S. Jin, J. Ding. An implementation of differential search\nalgorithm (DSA) for inversion of surface wave data. Journal of Applied Geophysics, 2014, 111: 334-345.\n[30] R. Devi, E. Barlaskar, O. Devi, S. Medhi, and R. Shimray. Survey on evolutionary computation tech\ntechniques and its application in different fields. International Journal on Information Theory (IJIT), 2014, 3(3): 73-82.\n[31] Y. Amrane, M. Boudour, and M. Belazzoug. A new optimal reactive power planning based on Differential\nSearch Algorithm. International Journal of Electrical Power & Energy Systems, 2015, 64: 551-561.\n[32] M.R. Guarracino, C. Cifarelli, O. Seref, and PM. Pardalos. A classification algorithm based on generalized\neigenvalue problems. Optimization methods and software, 2007, 22(1): 73\u201381.\n[33] M.R. Guarracino, A. Irpino, and R. Verde. Multiclass generalized eigenvalue proximal support vector\nmachines. Complex, Intelligent and Software Intensive Systems , IEEE Computer Society, 2010, 25\u201332.\n[34] Y. Shao, N. Deng, W. Chen, and W. Zhen. Improved generalized eigenvalue proximal support vector machine.\nIEEE signal processing letters, 2013, 20(3): 213-216.\n[35] P. Xanthopoulos, M. R. Guarracino, and P. M. Pardalos. Robust generalized eigenvalue classifier with ellipsoidal\nuncertainty. Ann Oper Res, 2014, 216(1): 327\u2013342.\n[36] A.N. Tikhonov, and V.Y. Arsenin. Solutions of Ill posed problems. New York: john wiley and sons, 1977.\n[37] B.N. Parlett. The symmetric eigenvalue problem. Philadelphia: SIAM, 1998.\n[38] A. Afifi. Improving the classification accuracy using support vector machines (SVMS) with new kernel.\nJournal of global research in computer science, 2013, 4(2): 1-7.\n[39] K. Bache, and M. Lichman. UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:\nUniversity of California, School of Information and Computer Science, 2013."}, {"heading": "8. APPENDIX", "text": "Pseudo code: Differential search algorithm [28]\nRequire:"}, {"heading": "N: Size of the population, where i = {1, 2, 3, \u2026, N}", "text": "D: Dimension of the problem\nG: Number of maximum generation\n1: Superorganism = initialize(), where Superorganism = [ArtificialOrganismi]\n2: yi = Evaluate(ArtificialOrganismi )\n3: for cycle = 1: G do\n4: donor = SuperorganismRandom_Shuffling(i)\n5: Scale = randg[2.rand1] . (rand2 - rand3)\n6: StopoverSite =Superorganism +Scale . (donor - Superorganism)\n7: p1=0.3 . rand4 and p2=0.3 . rand5\n8: if rand6 < rand7 then\n9: if rand8 < p1 then\n10: r = rand(N,D)\n11: for Counter1=1 : N do\n12: r(Counter1,:) = r(Counter1,:) < rand9\n13: end for\n14: else\n15: r = ones(N,D)\n16: for Counter2=1 : N do\n17: r(Counter2, randi(D)) = r(Counter2, randi(D)) <rand10\n18: end for\n19: end if\n20: else\n21: r = ones(N,D)\n22: for Counter3=1 : N do\n23: d = randi(D,1, p2 . rand . D )\n24: for Counter4 = 1 : size(d) do\n25: r(Counter3,d(Counter4)) = 0\n26: end for\n27: end for\n46\n28: end if\n29: individualsI,J  rI,J > 0 | I \u2208 i,J \u2208 [1 D]\n30: StopoverSite(individualsI,J) := Superorganism(individualsI,J)\n31: if StopoverSitei,j < lowi,j or StopoverSitei,j > upi,j then\n32: StopoverSitei, j = rand . (upj \u2013 lowj) + lowj\n33: end if\n34: y StopoverSite;i = evaluate(StopoverSitei)\n35: \ud835\udc66Superorganism ;\ud835\udc56 = yStopoverSite ;i \ud835\udc56\ud835\udc53 yStopoverSite ;i < \ud835\udc66Superorganism ;\ud835\udc56\n\ud835\udc66Superorganism ;\ud835\udc56 \ud835\udc52\ud835\udc59\ud835\udc60\ud835\udc52\n36: ArtificialOrganism\ud835\udc56 = \ud835\udc46\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc46\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc56 if yStopoverSite ;i < \ud835\udc66Superorganism ;\ud835\udc56\nArtificialOrganism\ud835\udc56 \ud835\udc52\ud835\udc59\ud835\udc60\ud835\udc52\n37: end for\nIJCATM : www.ijcaonline.org"}], "references": [{"title": "Rasha M", "author": ["M.H. Marghny"], "venue": "Abd El-Aziz, and Ahmed I. Taloba. An Effective Evolutionary Clustering Algorithm: Hepatitis C case study. International Journal of Computer Applications", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "and Ahmed I", "author": ["M.H. Marghny"], "venue": "Taloba. Outlier Detection using Improved Genetic K-means. International Journal of Computer Applications", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast Efficient Clustering Algorithm for Balanced Data", "author": ["Adel A. Sewisy", "M.H. Marghny", "Rasha M. Abd El-Aziz", "Ahmed I. Taloba"], "venue": "International Journal of Advanced Computer Science and Applications (IJACSA),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Data clustering: algorithms and applications", "author": ["C.C. Aggarwal", "C.K Reddy"], "venue": "Chapman and Hall/CRC Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Extracting logical classification rules with gene expression programming: International Journal of Computer Applications (0975 \u2013 8887) Volume 108 \u2013 No 19", "author": ["M.H. Marghny", "I.E. El-Semman"], "venue": "December 2014 44  microarray case study. Proceedings of the International Conference on Artificial Intelligence and Machine Learning (AIML 05), Cairo, Egypt", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Extracting fuzzy classification rules with gene expression programming", "author": ["M.H. Marghny", "I.E. El-Semman"], "venue": ". Proceedings of the International Conference on Artificial Intelligence and Machine Learning (AIML 05), Cairo, Egypt", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "A new parallel association rule mining algorithm on distributed shared memory system", "author": ["M.H. Marghny", "H.E. Refaat"], "venue": "International Journal of Business Intelligence and Data Mining", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast", "author": ["M.H. Marghny", "A.A. Shakour"], "venue": "Simple and Memory Efficient Algorithm for Mining Association Rules. International Review on Computers & Software", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Scalable Algorithm for Mining Association Rules", "author": ["M.H. Marghny", "A.A. Shakour"], "venue": "ICCST", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Support vector networks", "author": ["C. Cortes", "V.N. Vapnik"], "venue": "Machine Learning", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Proximal support vector machine classifiers", "author": ["Glenn Fung", "Olvi L. Mangasarian"], "venue": "In Proceedings of the Seventh ACM SIGKDD International conference on knowledge discovery and data mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Multisurface proximal support vector machine classification via generalized eigenvalues", "author": ["Olvi L. Mangasarian", "Edward W. Wild"], "venue": "IEEE transaction on pattern analysis and machine intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Fuzzy Support Vector Machines for Pattern Classification", "author": ["S Abe", "T. Inoue"], "venue": "IEEE, 2001,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Fuzzy support vector machines", "author": ["Chun-Fu Lin", "Sheng-De Wang"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Fuzzy proximal support vector classification via generalized eigenvalues", "author": ["Jayadeva", "R. Khemchandani", "S. Chandra"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Fuzzy multi-category proximal support vector classification via generalized eigenvalues. Soft Computing \u2013 A Fusion of Foundations", "author": ["Jayadeva", "R. Khemchandani", "S. Chandra"], "venue": "Methodologies and Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "A fuzzy support vector machine algorithm with dual membership based on hypersphere", "author": ["Ding Shifei", "Gu Yaxiang"], "venue": "Journal of computational information systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "A fuzzy twin support vector machine algorithm. International journal of application or innovation in engineering", "author": ["Li. Kai", "Hongyan Ma"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Fuzzy regularized generalized eigenvalue classifier with a novel membership function", "author": ["M.R. Guarracino", "A. Irpino", "R. Jasinevicius", "R. Verde"], "venue": "Information Sciences", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A ga-based feature selection and parameters optimization for support  vector machines", "author": ["Huang. Cheng-Lung", "Chieh-Jen Wang"], "venue": "Expert systems with applications,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Parameter determination of support vector machine and feature selection using simulated annealing approach", "author": ["S. Lin", "Z. Lee", "S. Chen", "T. Tseng"], "venue": "Applied soft computing", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Particle swarm optimization for parameter determination and feature selection of support vector machines", "author": ["S. Lin", "K. Ying", "S. Chen", "Z. Lee"], "venue": "Expert systems with applications", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "A new parameter selection method for support vector machine based on the decision value", "author": ["L. Luo", "D. Huang", "H. Peng", "Q. Zhou", "G. Shao", "F. Yang"], "venue": "Convergence information technology", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Enhancing the classification accuracy by scatter-based ensemble approach", "author": ["S.C. Chen", "S.W. Linb", "S.Y. Chou"], "venue": "Applied soft computing", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Support vector machine with parameter optimization by a novel hybrid method and its application to fault diagnosis", "author": ["X. Zhang", "D. Qiu", "F. Chen"], "venue": "Neurocomputing", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding the nature of evolutionary search algorithms", "author": ["P. Civicioglu"], "venue": "Additional technical report for the project of 110Y309-Tubitak", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Differential search algorithm-based parametric optimization of electrochemical micromachining processes", "author": ["D. Goswami", "S. Chakraborty"], "venue": "International journal of industrial engineering computations", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Transforming geocentric cartesian coordinates to geodetic coordinates by using differential search algorithm", "author": ["P. Civicioglu"], "venue": "Computers & Geosciences", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "An implementation of differential search algorithm (DSA) for inversion of surface wave data", "author": ["X. Song", "L. Li", "X. Zhang", "X. Shi", "J. Huang", "J. Cai", "S. Jin", "J. Ding"], "venue": "Journal of Applied Geophysics", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Survey on evolutionary computation tech techniques and its application in different fields", "author": ["R. Devi", "E. Barlaskar", "O. Devi", "S. Medhi", "R. Shimray"], "venue": "International Journal on Information Theory ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "A new optimal reactive power planning based on Differential Search Algorithm", "author": ["Y. Amrane", "M. Boudour", "M. Belazzoug"], "venue": "International Journal of Electrical Power & Energy Systems", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "A classification algorithm based on generalized eigenvalue problems", "author": ["M.R. Guarracino", "C. Cifarelli", "O. Seref", "PM. Pardalos"], "venue": "Optimization methods and software", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiclass generalized eigenvalue proximal support vector machines", "author": ["M.R. Guarracino", "A. Irpino", "R. Verde"], "venue": "Complex, Intelligent and Software Intensive Systems , IEEE Computer Society", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Improved generalized eigenvalue proximal support vector machine", "author": ["Y. Shao", "N. Deng", "W. Chen", "W. Zhen"], "venue": "IEEE signal processing letters", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust generalized eigenvalue classifier with ellipsoidal uncertainty", "author": ["P. Xanthopoulos", "M.R. Guarracino", "P.M. Pardalos"], "venue": "Ann Oper Res", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Solutions of Ill posed problems", "author": ["A.N. Tikhonov", "V.Y. Arsenin"], "venue": "New York: john wiley and sons", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1977}, {"title": "The symmetric eigenvalue problem", "author": ["B.N. Parlett"], "venue": "Philadelphia: SIAM", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving the classification accuracy using support vector machines (SVMS) with new kernel", "author": ["A. Afifi"], "venue": "Journal of global research in computer science", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9].", "startOffset": 92, "endOffset": 97}, {"referenceID": 1, "context": "This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9].", "startOffset": 92, "endOffset": 97}, {"referenceID": 2, "context": "This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9].", "startOffset": 92, "endOffset": 97}, {"referenceID": 3, "context": "This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9].", "startOffset": 92, "endOffset": 97}, {"referenceID": 4, "context": "This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9].", "startOffset": 123, "endOffset": 129}, {"referenceID": 5, "context": "This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9].", "startOffset": 123, "endOffset": 129}, {"referenceID": 6, "context": "This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9].", "startOffset": 204, "endOffset": 209}, {"referenceID": 7, "context": "This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9].", "startOffset": 204, "endOffset": 209}, {"referenceID": 8, "context": "This direction includes methods other than classical analysis, based on clustering analysis [1-4], classification analysis [5, 6], and solving problems of generalization, association and finding patterns [7-9].", "startOffset": 204, "endOffset": 209}, {"referenceID": 9, "context": "SVM which is an emerging data classification technique proposed by Vapnik in 1995 [10], and has been widely adopted in various fields of classification, nevertheless it suffers from complexity and parameters selection.", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "A new method has been introduced in [11] by Olvi L.", "startOffset": 36, "endOffset": 40}, {"referenceID": 11, "context": "Mangasarian which is called the Generalized Eigenvalue Proximal Support Vector Machine (GEPSVM) [12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Experimental results in [12] showed the effectiveness of GEPSVM on some public datasets.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "There are many approaches have been proposed by researchers for this problem [13-19].", "startOffset": 77, "endOffset": 84}, {"referenceID": 13, "context": "There are many approaches have been proposed by researchers for this problem [13-19].", "startOffset": 77, "endOffset": 84}, {"referenceID": 14, "context": "There are many approaches have been proposed by researchers for this problem [13-19].", "startOffset": 77, "endOffset": 84}, {"referenceID": 15, "context": "There are many approaches have been proposed by researchers for this problem [13-19].", "startOffset": 77, "endOffset": 84}, {"referenceID": 16, "context": "There are many approaches have been proposed by researchers for this problem [13-19].", "startOffset": 77, "endOffset": 84}, {"referenceID": 17, "context": "There are many approaches have been proposed by researchers for this problem [13-19].", "startOffset": 77, "endOffset": 84}, {"referenceID": 18, "context": "There are many approaches have been proposed by researchers for this problem [13-19].", "startOffset": 77, "endOffset": 84}, {"referenceID": 19, "context": "Authors in [20-25] tried to find solution for SVM parameters.", "startOffset": 11, "endOffset": 18}, {"referenceID": 20, "context": "Authors in [20-25] tried to find solution for SVM parameters.", "startOffset": 11, "endOffset": 18}, {"referenceID": 21, "context": "Authors in [20-25] tried to find solution for SVM parameters.", "startOffset": 11, "endOffset": 18}, {"referenceID": 22, "context": "Authors in [20-25] tried to find solution for SVM parameters.", "startOffset": 11, "endOffset": 18}, {"referenceID": 23, "context": "Authors in [20-25] tried to find solution for SVM parameters.", "startOffset": 11, "endOffset": 18}, {"referenceID": 24, "context": "Authors in [20-25] tried to find solution for SVM parameters.", "startOffset": 11, "endOffset": 18}, {"referenceID": 25, "context": "For solving the problem of parameters selection, a new and powerful method which is called Differential Search Algorithm (DSA) [26-31] has been used.", "startOffset": 127, "endOffset": 134}, {"referenceID": 26, "context": "For solving the problem of parameters selection, a new and powerful method which is called Differential Search Algorithm (DSA) [26-31] has been used.", "startOffset": 127, "endOffset": 134}, {"referenceID": 27, "context": "For solving the problem of parameters selection, a new and powerful method which is called Differential Search Algorithm (DSA) [26-31] has been used.", "startOffset": 127, "endOffset": 134}, {"referenceID": 28, "context": "For solving the problem of parameters selection, a new and powerful method which is called Differential Search Algorithm (DSA) [26-31] has been used.", "startOffset": 127, "endOffset": 134}, {"referenceID": 29, "context": "For solving the problem of parameters selection, a new and powerful method which is called Differential Search Algorithm (DSA) [26-31] has been used.", "startOffset": 127, "endOffset": 134}, {"referenceID": 30, "context": "For solving the problem of parameters selection, a new and powerful method which is called Differential Search Algorithm (DSA) [26-31] has been used.", "startOffset": 127, "endOffset": 134}, {"referenceID": 11, "context": "Wild proposed the Generalized Eigenvalue Proximal Support Vector Machine GEPSVM [12] as a generalization of the SVM method.", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "Due to the simplicity of GEPSVM, many researchers have refined it to improve the general performance of the classifier [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 32, "context": "Due to the simplicity of GEPSVM, many researchers have refined it to improve the general performance of the classifier [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 33, "context": "Due to the simplicity of GEPSVM, many researchers have refined it to improve the general performance of the classifier [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 34, "context": "Due to the simplicity of GEPSVM, many researchers have refined it to improve the general performance of the classifier [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 11, "context": "For this problem, a standard linear SVM is given by a plane halfway between the two parallel bounding planes that bound two disjoint half spaces each containing points mostly of class 1 or 2 [12].", "startOffset": 191, "endOffset": 195}, {"referenceID": 11, "context": "is the two-norm, and it has been assumed in [12] that w , \u03b3 \u2260 0 which implies to Bw \u2212 e \u03b3 \u2260 0.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "As introduced in [12] the numerator of the minimization problem (2) is the sum of squares of two-norm distances in the (w, \u03b3)-space of points in the first class to the plane xw \u2212 \u03b3 = 0, while the denominator of (2) is the sum of squares of two norm distances in the (w, \u03b3) space of points in the second class to the same plane.", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "Then Tikhonov regularization term is added [36] to reduce the norm of the problem variables w, \u03b3 that determine the proximal planes (1).", "startOffset": 43, "endOffset": 47}, {"referenceID": 36, "context": "As pointed out in [37], the objective function of (5) is known as the Rayleigh quotient, hence its solution can be obtained by solving a generalized eigenvalues problem.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "The works of many researches carried out by adding fuzzy values to the standard SVM [13, 14, 17, 18].", "startOffset": 84, "endOffset": 100}, {"referenceID": 13, "context": "The works of many researches carried out by adding fuzzy values to the standard SVM [13, 14, 17, 18].", "startOffset": 84, "endOffset": 100}, {"referenceID": 16, "context": "The works of many researches carried out by adding fuzzy values to the standard SVM [13, 14, 17, 18].", "startOffset": 84, "endOffset": 100}, {"referenceID": 17, "context": "The works of many researches carried out by adding fuzzy values to the standard SVM [13, 14, 17, 18].", "startOffset": 84, "endOffset": 100}, {"referenceID": 14, "context": "Many attempts for adding fuzzy to GEPSVM have been illustrated as in [15, 16, 19].", "startOffset": 69, "endOffset": 81}, {"referenceID": 15, "context": "Many attempts for adding fuzzy to GEPSVM have been illustrated as in [15, 16, 19].", "startOffset": 69, "endOffset": 81}, {"referenceID": 18, "context": "Many attempts for adding fuzzy to GEPSVM have been illustrated as in [15, 16, 19].", "startOffset": 69, "endOffset": 81}, {"referenceID": 14, "context": "A first attempt to obtain a fuzzy version of the GEPSVM classification is presented in [15, 16].", "startOffset": 87, "endOffset": 95}, {"referenceID": 15, "context": "A first attempt to obtain a fuzzy version of the GEPSVM classification is presented in [15, 16].", "startOffset": 87, "endOffset": 95}, {"referenceID": 14, "context": "In [15] the authors attempt to solve the following problem:", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Another recently attempt of fuzzy GEPSVM can be found in [19] where the author proposed the following fuzzy function", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "Where min (d(Ai,CA)) is the minimum distance of the point Ai from the centers in CA, max (d(Ai,CB)) is the maximum distance of the point Ai from the centers CB of the other class, and s is a parameter weighting the contribution of the exponential term to S ,for more detail see [19] .", "startOffset": 278, "endOffset": 282}, {"referenceID": 27, "context": "The main idea of the DSA algorithm was inspired form the migration of superorganisms making use of brownian like motion [28].", "startOffset": 120, "endOffset": 124}, {"referenceID": 27, "context": "EA includes the following techniques [28]: \uf0a7 Ant colony algorithm \uf0a7 Artificial Bee Colony (ABC) algorithm \uf0a7 Cultural algorithms \uf0a7 Differential evolution \uf0a7 Evolutionary algorithms \uf0a7 Evolutionary programming \uf0a7 Evolution strategy \uf0a7 Gene expression programming \uf0a7 Genetic algorithm \uf0a7 Genetic programming \uf0a7 Harmony search \uf0a7 Learnable Evolution Model \uf0a7 Particle swarm optimization \uf0a7 Self-organization such as self-organizing maps \uf0a7 Swarm intelligence The Differential Search Algorithm (DSA) is the most recent addition.", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "There are a number of computational-intelligence algorithms that model the behaviors of the superorganisms [28-30].", "startOffset": 107, "endOffset": 114}, {"referenceID": 28, "context": "There are a number of computational-intelligence algorithms that model the behaviors of the superorganisms [28-30].", "startOffset": 107, "endOffset": 114}, {"referenceID": 29, "context": "There are a number of computational-intelligence algorithms that model the behaviors of the superorganisms [28-30].", "startOffset": 107, "endOffset": 114}, {"referenceID": 37, "context": "In [38] a new kernel has been introduced the author used the new kernel with the standard SVM, in the presented work we use the new kernel which is called PolyRBF witch is a hybrid between a polynomial kernel and a Gaussian RBF kernel.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "Then the process stops over on the suitable tested position for a temporary time during the migration, the members of the artificial that made such discovery immediately settle at the discovered position and continue their migration from this position [28].", "startOffset": 252, "endOffset": 256}, {"referenceID": 27, "context": "Where, N signifies number of elements in the superorganism (Size of the population), G represents number of maximum generation, and D indicates size of the problem [28-30].", "startOffset": 164, "endOffset": 171}, {"referenceID": 28, "context": "Where, N signifies number of elements in the superorganism (Size of the population), G represents number of maximum generation, and D indicates size of the problem [28-30].", "startOffset": 164, "endOffset": 171}, {"referenceID": 29, "context": "Where, N signifies number of elements in the superorganism (Size of the population), G represents number of maximum generation, and D indicates size of the problem [28-30].", "startOffset": 164, "endOffset": 171}, {"referenceID": 27, "context": "The method to find a stopover site at the remaining between the artificial- organisms may be described by a Brownian-like random walk model [28].", "startOffset": 140, "endOffset": 144}, {"referenceID": 27, "context": "The way of calculation R makes the respective artificial-superorganism to radically change direction in the habitat [28-30].", "startOffset": 116, "endOffset": 123}, {"referenceID": 28, "context": "The way of calculation R makes the respective artificial-superorganism to radically change direction in the habitat [28-30].", "startOffset": 116, "endOffset": 123}, {"referenceID": 29, "context": "The way of calculation R makes the respective artificial-superorganism to radically change direction in the habitat [28-30].", "startOffset": 116, "endOffset": 123}, {"referenceID": 27, "context": "The tested and the most appropriate values for these parameters were conducted by [28].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "[12] FSVM", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] FTSVM [18] IGEPSVM", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[13] FTSVM [18] IGEPSVM", "startOffset": 11, "endOffset": 15}, {"referenceID": 35, "context": "[36]", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "92 100 100 100 GA+SVM[20] 94.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "SA+SVM[21] 97.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "PSO+SVM[22] 97.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "CV-ACC[23] 96.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Chen [24] 96.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "GA+SVM[20] 94.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "SA+SVM[21] 97.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "PSO+SVM[22] 97.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "CV-ACC[23] 95.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Chen [24] 96.", "startOffset": 5, "endOffset": 9}], "year": 2014, "abstractText": "M. H. Marghny Computer Science Department, Faculty of Computers and Information, Assiut University, Egypt. Rasha M. Abd El-Aziz Computer Science Department, Faculty of Science, Assiut University, Egypt. Ahmed I. Taloba Computer Science Department, Faculty of Computers and Information, Assiut University, Egypt . ABSTRACT Support Vector Machine (SVM) is an effective model for many classification problems. However, SVM needs the solution of a quadratic program which require specialized code. In addition, SVM has many parameters, which affects the performance of SVM classifier. Recently, the Generalized Eigenvalue Proximal SVM (GEPSVM) has been presented to solve the SVM complexity. In real world applications data may affected by error or noise, working with this data is a challenging problem. In this paper, an approach has been proposed to overcome this problem. This method is called DSA-GEPSVM. The main improvements are carried out based on the following: 1) a novel fuzzy values in the linear case. 2) A new Kernel function in the nonlinear case. 3) Differential Search Algorithm (DSA) is reformulated to find near optimal values of the GEPSVM parameters and its kernel parameters. The experimental results show that the proposed approach is able to find the suitable parameter values, and has higher classification accuracy compared with some other algorithms.", "creator": "Microsoft\u00ae Office Word 2007"}}}