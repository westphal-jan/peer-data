{"id": "1501.04244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2015", "title": "Generalised Random Forest Space Overview", "abstract": "Assuming but view of took Random Forest still a personnel has by each nested ensemble of fitted subsystems, nobody used the generalisation telescope allowing one from easily produce monograph practices based on both optimized. We discuss similar role and any related other implementations coming then level, especially in context though some will administration RF generalisations.", "histories": [["v1", "Sat, 17 Jan 2015 23:42:08 GMT  (20kb,D)", "http://arxiv.org/abs/1501.04244v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["miron b kursa"], "accepted": false, "id": "1501.04244"}, "pdf": {"name": "1501.04244.pdf", "metadata": {"source": "CRF", "title": "Generalised Random Forest Space Overview", "authors": ["Miron B. Kursa", "M.B. Kursa"], "emails": ["M.Kursa@icm.edu.pl"], "sections": [{"heading": "1 Introduction", "text": "Random Forest (RF) is a popular, powerful ensemble machine learning method proposed by Breiman [2001]. Although the canonical version of this algorithm is known to be very versatile and perform well in numerous applications, many variants of this method have been proposed, for many different purposes: to extend the RF capabilities, to generalise over specific, non-standard data, to increase accuracy in a certain conditions, to improve the attribute importance measure produced by this method, to speed-up training or prediction, just to name a few.\nThis work aims to provide a conceptual framework of generalised Random Forest (GRF) methods, useful both in classification of existing RF variants and defining research opportunities in this field."}, {"heading": "2 Generalised Random Forest", "text": "In the presented model of the generalised Random Forest, we assume a following three-layer nested ensemble structure, as shown on Figure 1.\nThe data is ingested into the model via numerous pivot models, which have to fit the form of the data and are expected to be very simple and easy to train, although are not necessary have to be neither accurate nor robust; their intended role is to become an interface between the input and internal logic of the GRF model. In other words, we assume that they handle the feature construction step of the modelling process.\nPivot models are grouped are converted into a meaningful ensemble models with the sharpening ensembles; this modules of this class orchestrate the generation of pivots by using the information from the decision attribute. The sharpening ensemble is expected to produce accurate models regardless of their\nar X\niv :1\n50 1.\n04 24\n4v 1\n[ cs\n.L G\n] 1\n7 Ja\nn 20\n15\nrobustness; however, they must not require external optimisation of any kind and should be computationally efficient.\nThe outermost layer of the GRF is the conditioning ensemble which builds and groups sharpening ensembles. Its role is to finally build a robust model by joining a lot of accurate models for which one does not know which are overfitted and which are not. It is trivial to show that this can be achieved even with a very small fraction of meaningful models, provided that sharpening ensembles will not be correlated; to this end the conditioning ensemble usually involves some permutation strategy.\nMoreover, conditioning ensemble can be expanded to provide a generalisation of some of the additional features of Random Forest, like internal error approximation, object-object dissimilarity measure or feature importance. It is also a good place to implement parallel computing capabilities."}, {"heading": "3 Pivot models", "text": "Pivot models are the only part of the GRF structure that has a direct contact with predictors of an information system, thus it is a place for modifying the interface with the data structure. The form of an output of a pivot classifier and the algorithm which is used to train it is naturally enforced by a form of the sharpening ensemble; most often pivot forms an embranchment in some kind of a decision tree, thus shall return a direction in which certain object should descent within the tree. As the trees are most often binary, this gives us two options, which we will later in the paper call R (for right) and L (for left).\nIn a standard Random Forest [Breiman, 2001], it is assumed that an information system is composed of either categorical or continuous1 predictors. This way, we have only two possible pivot classifiers, respectively for categorical and continuous feature, in a following forms:\nf(x \u2208 Cx;\u039e \u2208 2Cx)\u2192 {R,L} := { R : x \u2208 \u039e, L : x /\u2208 \u039e, , (1)\nwhere Cx is a set of categories of x, and f(x \u2208 R;\u039e \u2208 R)\u2192 {R,L} := { R : x \u2265 \u039e, L : x < \u039e, . (2)\nIn either case, x is a feature on which the pivot is executed, and \u039e is the threshold value. Both are selected when pivot is generated within the sharpening ensemble; most often they are optimised to maximise the decision homogeneity within the subsets of objects sent left and right, usually measured by information gain or Gini index.\nThe other popular idea, started by Geurts et al. [2006] with their Extra-Trees method, is to generate pivots at random, obviously with constraint that a resulting criterion should not sent all objects in one direction. Such a method removes a problem of finding an appropriate homogeneity measure and performing the optimisation, which in return increases the computational efficiency and, in a number of problems, the robustness of a final model as it leads to a greater divergence among sharpening ensemble models. Naturally, this way one also removes an impact of a possible problems with the homogeneity measure, which may surface for instance in case of unbalanced sets. Still, there are problems in the probability of finding even a slightly meaningful pivot is very small, and they usually case random pivot-based algorithm to perform poorly.\nAnd in-between solution is to use some kind of heuristic instead of a full optimisation; a simple example of this technique is to generate some number of random pivots and select the best one. Other approaches include generic soft optimisation methods like genetic algorithms, randomly reducing the search space (often the set of used predictors, ad done by the standard Random Forest) or disturbing the homogeneity measure. Rodr\u0301\u0131guez et al. [2006] proposed to employ PCA in pivot generation.\nThe other way of generalising the pivot models is to modify their structure to accommodate information systems going beyond simple sets of categorical and continuous predictors. Bosch et al. [2007] perform image classification with GRF with pivots calculating two vector image descriptors, summing their valus with random weights and comparing with a random threshold. Fan [2009] proposed to employ some kernel function used for SVM and build pivot by partial application of this kernel to a randomly selected object and compare the result with a threshold optimised in terms of information gain. This approach was later used for sound and temporal gene expression patterns [Cao and Fan, 2010, Fan et al., 2010].\n1 One should note that ordered categorical data, like for instance cold < warm < hot, can be treated as continuous without any loss of generality.\n4 Sharpening ensemble\nThe selection of a sharpening module is more complex, since it gives a lot of space for different solutions and approaches. Some of a popular options, illustrated in Figure 2, are:\n\u2013 decision tree is a sharpening module used by the Random Forest method; it uses simple embranchment pivots stacked in an iterative manner, i.e. after applying a pivot a sub-tree is build separately for each of the branches, until the sets of objects in leaves will become homogeneous or some pre-set maximum tree depth will be achieved. Training of pivots within a decision tree may be performed both randomly or via optimisation; the leaves of a\ntree will likely end up homogeneous either way. To this end, the output of a tree sharpening module is a direct prediction of class. \u2013 decision fern [O\u0308zuysal et al., 2008, Kursa, 2014] is basically a form of a full decision tree in which each pivot module at a given depth is identical. To this end, evaluation of a given pivot is not dependent on the others and can happen in any order, which makes a fern more computationally effective than a decision tree. On the other hand, this makes optimisation of a fern is highly non-obvious, thus individual pivots are usually generated randomly. This way the leaves of a fern are often non-homogeneous, and thus the prediction of a fern is often encoded as a vector of class probabilities, which naturally requires appropriate adaptation of the voting scheme present in the conditioning ensemble. \u2013 decision trunk, proposed by Ulfenborg et al. [2013], is composed of a flat series of pivot models similar to a fern, though requires the decision to be binary (say A or B), and employs pivot modules (segments) which classify into three groups, A, B meaning that it is certain that an object belongs to a respective class, and ? meaning that the decision is relayed to a next pivot classifier within the trunk. Obviously, different than in case of decision ferns, the order of trunk segments is significant because the consideration at level i comes in a context that the object was claimed undecided by segments 1, . . . , i\u221212. Trunks practically cannot be implemented without optimisation of pivot models and thus they provide sharp predictions similar to decision trees. One should note that ternary pivot models can be realised by combining a pair of regular entrancement pivots, only modified to optimise homogeneity in one branch, respectively for class A and B; ? is then given to objects directed to second branch in both pivots and for those for which prediction of both pivots are in conflict. \u2013 Although boosting [Schapire, 1990] is almost always considered as a standalone ensemble, wrapping it in another layer may lead to a better resilience to noise and mislabelled objects. Moreover, boosting has a good support for regression problems, making it a promising alternative to regression trees in context of some of their known problems in this set-up. \u2013 there is also a degenerate option which we will call here null ensemble, i.e. just using a single pivot classifier. This solution can be effective in case when pivot classifiers are very good on their own and applying a more complex sharpening will only result in an increased computational load. A litmus test for such a situation is when a more complex sharpening ensemble of a dynamic complexity, such as a decision tree, is creating shallow models composed of a small number of pivots.\nMoreover, sometimes the procedure of building the sharpening ensemble is modified to support the outer conditioning ensemble in de-correlation of individual sharpening ensembles. For instance, in the canonical Random Forest each\n2 This way trunks can be perceived as a discrete version of boosting.\npivot is build on a randomly generated subset of attributes; this approach is very generic and can be easily ported over other sharpeners, yet obviously a number of alternative methods can be applied as well."}, {"heading": "5 Conditioning ensemble", "text": "As mentioned earlier, the role of the conditioning ensemble is to justify a robust prediction from a set of sharpening ensembles of an unknown reliability. To this end, this module has to either somehow assess the member models or ensure independence of members so that the noise generated by the overfitted ones will average-out during voting. The first approach is yet rarely used, mostly because having a reliable enough method of assessing robustness would in practice mean that employing the whole GRF structure is redundant.\nThe second approach is mostly, as in the canonical Random Forest algorithm, realised through bootstrap aggregation (bagging), also proposed by Breiman [1996], or some variation of this method. Precisely, the procedure generates a number of object sub-samples for each of the sharpening ensemble; this way they are presented with differently stressed incarnations of the training data and thus exploring a wider range of aspects and becoming less correlated. Similar approach can be applied to features or implemented as weighting instead of strict selection. The other substantial option is to simply use a very variable sharpener that will generate diverse models on its own."}, {"heading": "6 Conclusions", "text": "By re-imagining Random Forest as a three-level nested ensemble, we propose a generic, modular framework for extending and modifying this method.\nAcknowledgments. This work has been financed by the National Science Centre, grant 2011/01/N/ST6/07035."}], "references": [{"title": "Image classification using random forests and ferns", "author": ["Anna Bosch", "Andrew Zisserman", "Xavier Munoz"], "venue": "IEEE 11th International Conference on Computer Vision, pages 1\u20138", "citeRegEx": "Bosch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bosch et al\\.", "year": 2007}, {"title": "Signal classification using random forest with kernels", "author": ["Jiguo Cao", "Guangzhe Fan"], "venue": "Sixth Advanced International Conference on Telecommunications,", "citeRegEx": "Cao and Fan.,? \\Q2010\\E", "shortCiteRegEx": "Cao and Fan.", "year": 2010}, {"title": "Kernel-induced classification trees and random forests", "author": ["Guangzhe Fan"], "venue": null, "citeRegEx": "Fan.,? \\Q2009\\E", "shortCiteRegEx": "Fan.", "year": 2009}, {"title": "Functional data classification for temporal gene expression data with kernel-induced random forests", "author": ["Guangzhe Fan", "Jiguo Cao", "Jiheng Wang"], "venue": "IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology,", "citeRegEx": "Fan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2010}, {"title": "Extremely randomized trees", "author": ["Pierre Geurts", "Damien Ernst", "Louis Wehenkel"], "venue": "Machine Learning,", "citeRegEx": "Geurts et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geurts et al\\.", "year": 2006}, {"title": "rFerns: An implementation of the random ferns method for general-purpose machine learning", "author": ["Miron Bartosz Kursa"], "venue": "Journal of Statistical Software,", "citeRegEx": "Kursa.,? \\Q2014\\E", "shortCiteRegEx": "Kursa.", "year": 2014}, {"title": "Rotation forest: A new classifier ensemble method", "author": ["Juan J Rod\u0155\u0131guez", "Ludmila I Kuncheva", "Carlos J Alonso"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Rod\u0155\u0131guez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rod\u0155\u0131guez et al\\.", "year": 2006}, {"title": "The strength of weak learnability", "author": ["Robert E. Schapire"], "venue": "Machine learning,", "citeRegEx": "Schapire.,? \\Q1990\\E", "shortCiteRegEx": "Schapire.", "year": 1990}, {"title": "Classification of tumor samples from expression data using decision trunks", "author": ["Benjamin Ulfenborg", "Karin Klinga-Levan", "Bj\u00f6rn Olsson"], "venue": "Cancer Informatics,", "citeRegEx": "Ulfenborg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ulfenborg et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The other popular idea, started by Geurts et al. [2006] with their Extra-Trees method, is to generate pivots at random, obviously with constraint that a resulting criterion should not sent all objects in one direction.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "The other popular idea, started by Geurts et al. [2006] with their Extra-Trees method, is to generate pivots at random, obviously with constraint that a resulting criterion should not sent all objects in one direction. Such a method removes a problem of finding an appropriate homogeneity measure and performing the optimisation, which in return increases the computational efficiency and, in a number of problems, the robustness of a final model as it leads to a greater divergence among sharpening ensemble models. Naturally, this way one also removes an impact of a possible problems with the homogeneity measure, which may surface for instance in case of unbalanced sets. Still, there are problems in the probability of finding even a slightly meaningful pivot is very small, and they usually case random pivot-based algorithm to perform poorly. And in-between solution is to use some kind of heuristic instead of a full optimisation; a simple example of this technique is to generate some number of random pivots and select the best one. Other approaches include generic soft optimisation methods like genetic algorithms, randomly reducing the search space (often the set of used predictors, ad done by the standard Random Forest) or disturbing the homogeneity measure. Rod\u0155\u0131guez et al. [2006] proposed to employ PCA in pivot generation.", "startOffset": 35, "endOffset": 1299}, {"referenceID": 0, "context": "Bosch et al. [2007] perform image classification with GRF with pivots calculating two vector image descriptors, summing their valus with random weights and comparing with a random threshold.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Bosch et al. [2007] perform image classification with GRF with pivots calculating two vector image descriptors, summing their valus with random weights and comparing with a random threshold. Fan [2009] proposed to employ some kernel function used for SVM and build pivot by partial application of this kernel to a randomly selected object and compare the result with a threshold optimised in terms of information gain.", "startOffset": 0, "endOffset": 202}, {"referenceID": 7, "context": "\u2013 Although boosting [Schapire, 1990] is almost always considered as a standalone ensemble, wrapping it in another layer may lead to a better resilience to noise and mislabelled objects.", "startOffset": 20, "endOffset": 36}, {"referenceID": 5, "context": ", 2008, Kursa, 2014] is basically a form of a full decision tree in which each pivot module at a given depth is identical. To this end, evaluation of a given pivot is not dependent on the others and can happen in any order, which makes a fern more computationally effective than a decision tree. On the other hand, this makes optimisation of a fern is highly non-obvious, thus individual pivots are usually generated randomly. This way the leaves of a fern are often non-homogeneous, and thus the prediction of a fern is often encoded as a vector of class probabilities, which naturally requires appropriate adaptation of the voting scheme present in the conditioning ensemble. \u2013 decision trunk, proposed by Ulfenborg et al. [2013], is composed of a flat series of pivot models similar to a fern, though requires the decision to be binary (say A or B), and employs pivot modules (segments) which classify into three groups, A, B meaning that it is certain that an object belongs to a respective class, and ? meaning that the decision is relayed to a next pivot classifier within the trunk.", "startOffset": 8, "endOffset": 732}], "year": 2015, "abstractText": "Assuming a view of the Random Forest as a special case of a nested ensemble of interchangeable modules, we construct a generalisation space allowing one to easily develop novel methods based on this algorithm. We discuss the role and required properties of modules at each level, especially in context of some already proposed RF generalisations.", "creator": "LaTeX with hyperref package"}}}