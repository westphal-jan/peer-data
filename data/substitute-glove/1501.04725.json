{"id": "1501.04725", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2015", "title": "Learning Invariants using Decision Trees", "abstract": "The stress of inferring an syllogistic covariant took implicates successful board use be blueprint in terms of binary classification. This is goes standard extent in installing creative: given into obtained of we when hardly points, one is asked next lot to isotype that generalizes from also evidence and periphery seen four sets. Here, following better throws they place defensible further present was sponsored, and set bad points are take referring reach a maintenance mortgage violation. Thus, a talking classifier gives perfect candidate invariants. In this paper, 're lawmakers a focus kernel it typical asking bricks without learn candidate transformations between all called. arbitrary Boolean vary present corresponding organisational. We we specific they algorithm to verify C centers taken last the literature. The non-linear way able however infer be invariants for single wide of consistent benchmarks already compares polled might though ML - based formula_18 inference modelling. In generally, whole scales even to large sample sets.", "histories": [["v1", "Tue, 20 Jan 2015 07:20:30 GMT  (338kb,D)", "http://arxiv.org/abs/1501.04725v1", "15 pages, 2 figures"]], "COMMENTS": "15 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.PL cs.LG", "authors": ["siddharth krishna", "christian puhrsch", "thomas wies"], "accepted": false, "id": "1501.04725"}, "pdf": {"name": "1501.04725.pdf", "metadata": {"source": "CRF", "title": "Learning Invariants using Decision Trees", "authors": ["Siddharth Krishna", "Christian Puhrsch", "Thomas Wies"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Finding inductive invariants is a fundamental problem in program verification. Many static analysis techniques have been proposed to infer invariants automatically. However, it is often difficult to scale those techniques to large programs without compromising on precision at the risk of introducing false alarms. Some techniques, such as abstract interpretation [9], are effective at striking a good balance between scalability and precision by allowing the analysis to be fine-tuned for a specific class of programs and properties. This fine-tuning requires careful engineering of the analysis [11]. Instead of manually adapting the analysis to work well across many similar programs, refinement-based techniques adapt the analysis automatically to the given program and property at hand [8]. A promising approach to achieve this automatic adaptation is to exploit synergies between static analysis and testing [15,17,34]. Particularly interesting is the use of Machine Learning (ML) to infer likely invariants from test data [14,31,32]. In this paper, we present a new algorithm of this type that learns arbitrary Boolean combinations of numerical inequalities.\nIn most ML problems, one is given a small number of sample points labeled by an unknown function. The task is then to learn a classifier that performs well on unseen points, and is thus a good approximation to the underlying function. Binary classification is a specific instance of this problem. Here, the sample data is partitioned into good and bad points and the goal is to learn a predicate that separates the two sets. Invariant inference can be viewed as a binary classification problem [31]. If the purpose of the invariant is to prove a safety property, then the good points are the forward-reachable safe states of the program and the bad points are the backward-reachable unsafe states. These two sets are sampled using program testing. The learned classifier then represents\nar X\niv :1\n50 1.\n04 72\n5v 1\n[ cs\n.P L\n] 2\n0 Ja\nn 20\n15\na candidate invariant, which is proved safe using a static analysis or theorem prover. If the classifier is not a safe invariant, the failed proof yields a spurious counterexample trace and, in turn, new test data to improve the classifier in a refinement loop.\nOur new algorithm is an instance of this ML-based refinement scheme, where candidate invariants are inferred using a decision tree learner. In this context, a decision tree (DT) is a binary tree in which each inner node is labeled by a function f from points to reals, called a feature, and a real-valued threshold t. Each leaf of the tree is labeled with either \u201cgood\u201d or \u201cbad\u201d. Such a tree encodes a predicate on points that takes the form of a Boolean combination of inequalities, fpxq \u010f t, between features and thresholds. Given sets of features and sample points, a DT learner computes a DT that is consistent with the samples. In our algorithm, we project the program states onto the numerical program variables yielding points in a d-dimensional space. The features describe distances from hyperplanes in this space. The DT learner thus infers candidate invariants in the form of arbitrary finite unions of polyhedra. However, the approach also easily generalizes to features that describe nonlinear functions. Our theoretical contribution is a probabilistic completeness guarantee. More precisely, using the Probably Approximately Correct model for learning [33], we provide a bound on the sample size that ensures that our algorithm successfully learns a safe inductive invariant with a given probability.\nWe have implemented our algorithm for specific classes of features that we automatically derive from the input program. In particular, inspired by the octagon abstract domain [22], we use as features the set of all hyperplane slopes of the form \u02d8xi \u02d8 xj , where 1 \u010f i \u0103 j \u010f d. We compared our implementation to other invariant generation tools on benchmarks taken from the literature. Our evaluation indicates that our approach works well for a range of benchmarks that are challenging for other tools. Moreover, we observed that DT learners often produce simpler invariants and scale better to large sample sets compared to other ML-based invariant inference techniques such as [14, 30\u201332]."}, {"heading": "2 Overview", "text": "In this section, we discuss an illustrative example and walk through the steps taken in our algorithm to compute invariants. To this end, consider the program in Fig. 1. Our goal is to find an inductive invariant for the loop on line 4 that is sufficiently strong to prove that the assertion x \u2030 0 on line 12 is always satisfied. Good and Bad States. We restrict ourselves to programs over integer variables x \u201c px1, . . . , xdq without procedures. Then a state is a point in Zd that corresponds to some assignment to each of the variables. For simplicity, we assume that our example program has a single control location corresponding to the head of the loop. That is, its states are pairs pv1, v2q where v1 is the value of x and v2 the value of y. When our program begins execution, the initial state could be p0, 1q or p0,\u00b43q, but it cannot be p2, 3q or p0, 0q because of the precondition specified by the assume statement in the program. A good state is defined as any state that the program could conceivably reach when it is started from a state consistent with the precondition. If we start execution at\np0,\u00b43q, then the states we reach are tp\u00b41,\u00b42q, p\u00b42,\u00b41q, p\u00b43, 0qu and thus these are all good states.\nSimilarly, bad states are defined to be the states such that if the program execution was to be started at that point (if we ran the program from the loop head, with those values) then the loop will exit after a finite point and the assertion will fail. For example, p0, 0q is a bad state, as the loop will not run, and we directly go to the assertion and fail it. Similarly, p\u00b42,\u00b42q is a bad state, as after one iteration of the loop the state becomes p\u00b41,\u00b41q, and after another iteration we reach p0, 0q, which fails the assertion.\nThe right-hand side of Fig. 1 shows some of the good and bad states of the program. A safe inductive invariant can be expressed in terms of a disjunction of the indicated hyperplanes, which separate the good from the bad states. Our algorithm automatically finds such an invariant.\nOverview. The high-level overview of our approach is as follows: good and bad states are sampled by running the program on different initial states. Next, a numeric abstract domain that is likely to contain the invariant is chosen manually. We use disjunctions of octagons by default. For each hyperplane (up to translation) in the domain we add a new \u201cfeature\u201d to each of the sample points corresponding to the distance to that hyperplane. A Decision Tree (DT) learning algorithm is then used to learn a DT that can separate the good and bad states in the sample, and this tree is converted into a formula that is a candidate invariant. Finally, this candidate invariant is passed to a theorem prover that verifies the correctness of the invariant. We now discuss these steps in detail.\nSampling. The first step in our algorithm is to sample good and bad states of this program. We sample the good states by picking states satisfying the precondition, running the program from these states and collecting all states reached. To sample bad states, we look at all points close to good states, run the program from these. If the loop exits within a bounded number of iterations and fails the assert, we mark all states reached as bad states. The sampled good and bad states are shown in Fig. 1.\nFeatures. The next step is to choose a candidate hyperplane set for the inequalities in the invariant. For most of our benchmarks, we used the octagon abstract domain, which consists of all linear inequalities of the form:\nbi \u00a8 xi ` bj \u00a8 xj \u010f c where 1 \u010f i \u0103 j \u010f d, bi, bj P t\u00b41, 0, 1u, and c P Z.\nWe then let H \u201c tw1,w2, . . . u be the set of hyperplane slopes for this domain. Then we transform our sample points (both good and bad) according to these slopes. For each sample point x, we get a new point z given by zi \u201c x \u00a8wi. In our example, the octagon slopes H and some of our transformed good and bad points are\nH \u201c\n\u00bb\n\u2014 \u2013 1 0 0 1 1 \u00b41 1 1\nfi\nffi fl , and X \u00a8HT \u201c\n\u00bb\n\u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2013\n0 1 1 0 \u00b41 0 . . . 0 0 2 \u00b42 \u00b41 1 . . .\nfi\nffi ffi ffi ffi ffi ffi ffi ffi fl \u00a8 \u201e 1 0 1 1 0 1 \u00b41 1  \u201c\n\u00bb\n\u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2013\n0 1 \u00b41 1 1 0 1 1 \u00b41 0 \u00b41 \u00b41 . . .\n0 0 0 0 2 \u00b42 4 0 \u00b41 1 \u00b42 0 . . .\nfi\nffi ffi ffi ffi ffi ffi ffi ffi fl .\nLearning the DT. After this transformation, we run a Decision Tree learning algorithm on the processed data. A DT (see Fig. 2 for an example) is a concise way to represent a set of rules as a binary tree. Each inner node is labeled by a feature and a threshold. Given a sample point and its features, we evaluate the DT by starting at the root and taking the path given by the rules: if the features is less than or equal to the threshold, we go to the left child, otherwise the right. Leaves specify the output label on that point. Most DT learning algorithms start at an empty tree, and greedily pick the best feature to split on at each node. From the good and bad states listed above, we can easily see that a good feature to split on must be the last one, as all bad states have the last column 0. Indeed, the first split made by the DT is to split on z4 at \u00b40.5. Since w4 \u201c p1, 1q, this split corresponds to the linear inequality x ` y \u010f \u00b40.5. Now, half the good states are represented in the left child of the root (corresponding to z4 \u010f \u00b40.5). The right child contains all the bad states and the other half of the good states. So the algorithm leaves the left child as is and tries to find the best split for the right child. Again, we see the same pattern with z4, and so the algorithm picks the split z4 \u0103 1. Now, all bad states fall into the left child, and all good states fall into the right child, and we are done. The computed DT is shown in Fig. 2.\nFinally, we need to convert the DT back into a formula. To do this, we can follow all paths from the root that lead to good leaves, and take the conjunction of all inequalities on the path, and finally take the disjunction of all such paths. In our example, we get the candidate invariant:\npx` y \u010f \u00b40.5q _ px` y \u0105 \u00b40.5^ x` y \u0105 0.5q .\nThis can be simplified to px` y \u2030 0q. Verifying the Candidate Invariant. Our program is then annotated with this invariant and passed to a theorem prover to verify that the invariant is indeed sufficient to prove the program correct."}, {"heading": "3 Preliminaries", "text": "Problem Statement. We think of programs as transition systemsP \u201c pS,R, init, safeq, where S is a set of states, R \u010e S \u02c6 S is a transition relation on the states, init \u010e S is a set of initial states, and safe \u010e S is the set of safe states that we want our program to remain within.\nFor any set of states X , the set postpXq represents all the successor states with respect to R. More formally,\npostpXq \u201c tx1 | Dx P X.Rpx, x1q u .\nThen, we can define the set of reachable states of the program (the good states) to be the least fixed point,\ngood \u201c lfpp\u03bbX. initY postpXqq .\nSimilarly, we can define\nprepXq \u201c tx | Dx1 P X.Rpx, x1q u and bad \u201c lfpp\u03bbX. error Y prepXqq,\nwhere error is the complement of safe. Then we see that the program is correct, with respect to the safety property given by safe, if good X bad \u201c H. Thus, our task is to separate the good states from the bad states.\nOne method to show that these sets are disjoint is to show the existence of a safe inductive invariant. A safe inductive invariant is a set inv that satisfies the following three properties:\n\u2013 init \u010e inv, \u2013 postpinvq \u010e inv, \u2013 inv \u010e safe.\nIt is easy to see that these conditions imply, in particular, that inv separates the good and bad states: good \u010e inv and inv X bad \u201c H.\nInvariant Generation as Binary Classification. The set up for most machine learning problems is as follows. We have an input space X and an output space Y , and one is given a set of samples X \u010e X that are labeled by some unknown function f : X \u00d1 Y . We fix a hypothesis set H \u010e YX , and the aim is to find the hypothesis h P H that most closely approximates f . The samples are often given in terms of feature vectors, and thus a sample x can be thought of as a point in some d-dimensional space.\nBinary classification is a common instance of this problem, where the labels are restricted to a binary set Y \u201c t0, 1u. Following [31], we view the problem of computing a safe inductive invariant for a program P \u201c pS,R, init, safeq as a binary classification problem by defining the input space as the set of all program states X \u201c S. Sample states x P bad are labeled by 0 and x P good are labeled by 1. Thus, the unknown function f is the characteristic function of the set of good states. The hypothesis to be learned is a safe inductive invariant. Note that if sampling the program shows that some state x is both good and bad, there exists no safe inductive invariant, and the program is shown to be unsafe. Hence, we assume that the set of sample states can be partitioned into good and bad states.\nDecision Trees. Instead of considering a hypothesis space that can represent arbitrary invariants, we restrict it to a specific abstract domain, namely those invariants that can be represented using decision trees. A Decision Tree (DT) [23] is a binary tree that represents a Boolean function. Each inner node v of T is labeled by a decision of the form xi \u010f t, where xi is one of the input features and t is a real valued threshold. We denote this inequality by v.cond. We denote the left and right children of an inner node by v.left and v.right respectively. Each leaf v is labeled by an output v.label. To evaluate an input, we trace a path down from the root node T.root of the tree, going left at each inner node if the decision is true, and right otherwise. The output of the tree on this input is the label of the leaf reached by this process. The hypothesis set corresponding to all DTs is thus arbitrary Boolean combinations of linear inequalities of the form xi \u010f t (axis-aligned hyperplanes).\nAs one can easily see, many DTs can represent the same underlying function. However, the task of finding the smallest (in terms of number of nodes) DT for a particular function can be shown to be NP-complete [19]. Standard algorithms to learn DTs work by greedily selecting at each node the co-ordinate and threshold that separates the remaining training data best [7,27]. This procedure is followed recursively until all leaves have samples labeled by a single class.\nThe criterion for separation is normally a measure such as conditional entropy. Entropy is a commonly used measure of uncertainty in a system. It is a function that is low when the system is homogeneous (in this case, think of when all samples reaching a node have the same label), and high otherwise. Conditional entropy, analogously, measures how homogeneous the samples are after choosing a particular co-ordinate and threshold. More formally, at each node, we look at the samples that reach that node, and define the conditional entropy of splitting feature xi at threshold t as\nHpy|xi : tq \u201c ppxi \u0103 tqHpy|xi \u0103 tq ` ppxi \u011b tqHpy|xi \u011b tq,\nwhere Hpy|xi \u0103 tq \u201c \u00b4 \u00ff\naPY ppy \u201c a|xi \u0103 tq log ppy \u201c a|Xi \u0103 tq,\nAlgorithm 1: DTInv: Invariant generation algorithm using DT learning def DTInv (P : program): safe inductive invariant for P or fail \u201c\nval X,y \u201c Sampler(P) val H \u201c Slopes(P) val Z \u201c X \u00a8HT val T \u201c LearnDT(Z, y) val \u03d5 \u201c DTtoForm(T.root) if IsInvariant(P , \u03d5) then \u03d5 else fail\ndef DTtoForm (v: node of a decision tree): formula represented by subtree rooted at v \u201c if v is a leaf then v.label else pv.cond^ DTtoForm(v.left)_ v.cond^ DTtoForm(v.right)q\nand ppAq is the empirical probability, i.e., the fraction of sample points reaching this node that satisfy the conditionA.Hpy|xi \u011b tq is defined similarly toHpy|xi \u0103 tq. Note that if a particular split perfectly separates good and bad samples, then the conditional entropy is 0. The greedy heuristic is to pick the feature and split that minimize the conditional entropy. There are other measures as well, such as the Gini index, which is used by the DT learner we used in our experiments [25]."}, {"heading": "4 Algorithm", "text": "We now present our DT-learning algorithm. We assume that we have black box procedures for sampling points from the given program, and for getting a set of slopes from a chosen abstract domain. To this end, let Sampler be a procedure that takes a program and returns an n\u02c6d matrix X of n sample points, and an n-dimensional vector y corresponding to the label of each sample point (1 for good states, 0 otherwise). Similarly, let Slopes be a procedure that takes a program and returns an m \u02c6 d matrix H of m hyperplane slopes. We describe the actual procedures used in our experiments in Section 5.1.\nOur final algorithm is surprisingly simple, and is given in Algorithm 1. We get the sample points and the slopes from the helper functions mentioned above, and then transform the sample points according to the slopes given. We run a standard DT learning algorithm on the transformed sample to obtain a tree that classifies all samples correctly. The tree is then transformed into a formula that is a candidate invariant, by a simple procedure DTtoForm. Finally, the program is annotated with the candidate invariant and verified. This final step is realized by another black box procedure IsInvariant, which checks that the invariant satisfies the three conditions necessary to be a safe inductive invariant. For example, this can be done by encoding init, inv, post and safe as SMT formulas and feeding the three conditions into an SMT solver.\nTo convert the DT into a formula, we note that the set of states that reach a particular leaf is given by the conjunction of all predicates on the path from the root to that leaf. Thus, the set of all states classified as good by the DT is the disjunction of the sets of states that reach all the good leaves. A simple conversion is then to take the disjunction\nover all paths to good leaves of the conjunction of all predicates on such paths. The procedure DTtoForm computes this formula recursively by traversing the learned DT. Since the tree was learnt on the transformed sample data, the predicate at each node of the tree will be of the form zi \u010f c where zi is one of the columns of Z and c is a constant. Since Z \u201c X \u00a8 HT , we know that zipxq \u201c x \u00a8 wi for a sample x. Thus, the predicate is equivalent to x \u00a8 wi \u010f c, which is a linear inequality in the program variables. Combining this with the conversion procedure above, we see that our algorithm outputs an invariant which is a Boolean combination of linear inequalities over the numerical program variables.\nSoundness. From the above discussion, we see that the formula \u03d5 returned by the procedure DTtoForm is of the required format. Moreover, we assume that the procedure IsInvariant is correct. Thus, we can say that our invariant generation procedure DTInv is sound: if it terminates successfully, it returns a safe inductive invariant.\nProbabilistic Completeness. It is harder to prove that such invariant generation algorithms are complete. We see that the performance of our algorithm depends heavily on the sample set, if the sample is inadequate, it is impossible for the DT learner to learn the underlying invariant. One could augment our algorithm with a refinement loop, which would make the role of the sampler less pronounced, for if the invariant is incorrect, the theorem prover will return a counterexample that could potentially be added to the sample set and we can re-run learning. However, we find in practice that we do not need a refinement loop if our sample set is large enough.\nWe can justify this observation using Valiant\u2019s PAC (probably approximately correct) model [33]. In this model, one can prove that an algorithm that classifies a large enough sample of data correctly has small error on all data, with high probability. It must be noted that one key assumption of this model is that the sample data is drawn from the same distribution as the underlying data, an assumption that is hard to justify in most of its applications, including this one. In practice however, PAC learning algorithms are empirically successful on a variety of applications where the assumption on distribution is not clearly true. Formally, we can give a generalization guarantee for our algorithm using this result of Blumer et al. [6]:\nTheorem 1. A learning algorithm for a hypothesis class H that outputs a hypothesis h P H will have true error at most with probability at least 1 \u00b4 \u03b4 if h is consistent with a sample of size maxp 4 log\n2 \u03b4 ,\n8V CpHq log 13 q.\nIn the above theorem, a hypothesis is said to be consistent with a sample if it classifies all points in the sample correctly. The quantity V CpHq is a property of the hypothesis class called the Vapnik-Chervonenkis (VC) dimension, and is a measure of the expressiveness of the hypothesis class. As one might imagine, more complex classes lead to a looser bound on the error, as they are more likely to over-fit the sample and less likely to generalize well.\nThus, it suffices for us to bound the VC dimension of our hypothesis class, which is all finite Boolean combinations of hyperplanes in d dimensions. The VC dimension of a class H is defined as the cardinality of the largest set of points that H can shatter. A set of points is said to be shattered by a hypothesis class H if for every possible labeling of the points, there exists a hypothesis in H that is consistent with it. Unfortunately,\nDTs in all their generality can shatter points of arbitrarily high cardinality. Given any set of m points, we can construct a DT with m leaves such that each point ends up at a different leaf, and now we can label the leaf to match the labeling given.\nSince in practice we will not be learning arbitrarily large trees, we can restrict our algorithm a-priori to stop growing the tree when it reaches K nodes, for some fixed K independent of the sample. Now one can use a basic, well-known lemma from [24] combined with Sauer\u2019s Lemma [29] to get that the VC dimension of bounded decision trees is OpKd logKq. Combining this with Theorem 1, we get the following polynomial bound for probabilistic completeness:\nTheorem 2. Under the assumptions of the PAC model, the algorithm DTInv returns an invariant that has true error at most with probability at least 1 \u00b4 \u03b4, given that its sample size is Op 1 Kd logK log\n1 \u03b4 q .\nComplexity. The running time of our algorithm depends on many factors such as the running time of the sampling (which in turn depends on the benchmark being considered), and so is hard to measure precisely. However, the running time of the learning routine for DTs is Opmn logpnqq, where m is the number of hyperplane slopes in H and n is the number of sample points [7, 25]. The learning algorithm therefore scales well to large sets of sample data. Nonlinear invariants. An important property of our algorithm is that it generalizes elegantly to nonlinear invariants as well. For example, if a particular program requires invariants that reason about px mod 2q for some variable x, then we can learn such invariants as follows: given the sampled states X , we add to it a new column that corresponds to a variable x1, such that x1 \u201c x mod 2. We then run the rest of our algorithm as before, but in the final invariant, replace all occurrences of x1 by px mod 2q. As is easy to see, this procedure correctly learns the required nonlinear invariant. We have added this feature to our implementation, and show that it works on benchmarks requiring these nonlinear features (see Section 5.2)."}, {"heading": "5 Implementation and Evaluation", "text": ""}, {"heading": "5.1 Implementation", "text": "We implemented our algorithm in Python, using the scikit-learn library\u2019s decision tree classifier [25] as the DT learner LearnDT. This implementation uses the CART algorithm from [7] which learns in a greedy manner as described in Section 3, and uses the Gini index.\nWe implemented a simple and naive Sampler: we considered all states that satisfied the precondition where the value of every variable was in the interval r\u00b4L,Ls. For these states, we ran the program with a bound I on the number of iterations of loops, and collected all states reached as good states. To find bad states, we looked at all states that were a margin M away from every good state, ran the program from this state (again with at most I iterations), and if the program failed an assertion, we collected all the states on this path as bad states. The bounds L, I,M were initialized to low values and increased until we had sampled enough states to prove our program correct.\nFor the Slopes function, we found that for most of the programs in our benchmarks it sufficed to consider slopes in the octagonal abstract domain. This consists of all vectors in t\u00b41, 0, 1ud with at most two non zero elements. In a few cases, we needed additional slopes (see Table 1). For the programs hola15 and hola34, we used a class of slopes of vectors in Cd where C is a small set of constants that appear in the program, and their negations.\nWe also developed methods to learn the class of slopes needed by a program by looking at the states sampled. In [31], the authors suggest working in the null space of the good states, viewed as a matrix. This is because if the good states lie in some lower dimensional space, this would automatically suggest equality relationships among them that can be used in the invariant. It also reduces the running time of the learning algorithm. Inspired by this, we propose using Principal Component Analysis [20] on the good states to generate slopes. PCA is a method to find the basis of a set of points so as to maximize the variance of the points with respect to the basis vectors. For example, if all the good points lie along the line 2x ` 3y \u201c 4, then the first PCA vector will be p2, 3q, and intuition suggests that the invariant will use inequalities of the form 2x` 3y \u0103 c.\nFinally, for the IsInvariant routine, we used the program verifier Boogie [4], which allowed us to annotate our programs with the invariants we verified. Boogie uses the SMT solver Z3 [12] as a back-end."}, {"heading": "5.2 Evaluation", "text": "We compared our algorithm with a variety of other invariant inference tools and static analyzers. We mainly focused on ML-based algorithms, but also considered tools based on interpolation and abstract interpretation. Specifically, we considered:\n\u2013 ICE [14]: an ML algorithm based on ICE-learning that uses an SMT solver to learn numerical invariants. \u2013 MCMC [30]: an ML algorithm based on Markov Chain Monte Carlo methods. There are two versions of this algorithm, one that uses templates such as octagons for the invariant, and one with all constants in the slopes picked from a fixed bag of constants. The two algorithms have very similar characteristics. We ran both versions 5 times each (as they are randomized) and report the better average result of the two algorithms for each benchmark. \u2013 SC [31]: an ML algorithm based on set cover. We only had access to the learning algorithm proposed in [31] and not the sampling procedure. To obtain a meaningful comparison, we combined it with the same sampler that we used in the implementation of our algorithm DTInv. \u2013 CPAchecker [5]: a configurable software model checker. We chose the default analysis based on predicate abstraction and interpolation. \u2013 UFO [2]: a software model checker that combines abstract interpretation and interpolation (denoted CPA). \u2013 InvGen [18]: an inference tool for linear invariants that combines abstract interpretation, constraint solving, and testing.\nFor our comparison, we chose a combination of 22 challenging benchmarks from various sources. In particular, we considered a subset of the benchmarks from [13, 14, 18, 31]. We chose the benchmarks at random among those that were hard for at least one other tool to solve. Due to this bias in the selection, our experimental results do not reflect the average performance of the tools that we compare against. Instead, the comparison should be considered as an indication that our approach provides a valuable complementary technique to existing algorithms.\nWe ran our experiments on a machine with a quad-core 3.40GHz CPU and 16GB RAM, running Ubuntu GNU/Linux. For the analysis of each benchmark, we used a memory limit of 8GB and a timeout of 5 minutes. The results of the experiment are summarized in Table 1. Here are the observations from our experiments (we provide more in-depth explanations for these observations in the next section, where we discuss related work in more detail):\n\u2013 Our algorithm DTInv seems to learn complex Boolean invariants as easily as simple conjunctions. \u2013 ICE seems to struggle on programs that needed large invariants. For some of these (gopan, popl07), this is because the constraint solver runs out of time/memory, as the constants used in the invariants are also large. For hola19 and prog4, ICE stops because the tool has an inbuilt limit for the complexity of Boolean templates. \u2013 However, ICE solves sum1 and trex3 quickly, even though they need many predicates, because the constant terms are small, so this space is searched first by ICE. \u2013 Similarly, we notice that MCMC has difficulty finding large invariants, again because the search space is huge. \u2013 SC\u2019s learning algorithm is consistently slower than DTInv, due to its higher running time complexity. It also runs out of memory for large sample sizes. \u2013 DTInv is able to easily handle benchmarks that CPA, UFO and InvGen struggle on. This is mainly because they are specialized for reasoning about linear invariants, and have issues dealing with invariants that have complicated Boolean structure.\nWe also learned some of the weak points in our current approach:\n\u2013 DTInv is slow in processing fig1 and prog4, both of which are handled by at least one other tool without much effort. However, we note that most of this time is spent in the sampling routine, which is currently a naive implementation. We therefore believe that DT learning could benefit from a combination with a static analysis that provides approximations of the good and bad states to guide the sampler. \u2013 We also note that the method of \u201cconstant slopes\u201d which we used to handle the non octagonal benchmarks (hola15, hola34) is ad-hoc, and might not work well for larger benchmarks.\nBeyond octagons. As mentioned in Section 4, we implemented a feature to learn certain nonlinear invariants. We were able to verify some benchmarks that needed reasoning about the modulus of certain variables, as shown in Table 1. Finally, we show one example where we were able to infer a nonlinear invariant (specifically s \u201c i2 ^ i \u010f n for square).\nWe believe our experiments show that Decision Trees are a natural representation for invariants, and that the greedy learning heuristics guide the algorithm to discover simple invariants of complex structures without additional overhead."}, {"heading": "6 Related Work and Conclusions", "text": "Our experimental evaluation compared against other algorithms for invariant generation. We discuss these algorithms in more detail. Sharma et. al. [31] used the greedy set cover algorithm SC to learn invariants in the form of arbitrary Boolean combinations of linear inequalities. Our algorithm based on decision trees is simpler than the set cover algorithm, works better on our benchmarks (which includes most of the benchmarks from [31]), and scales much better to large sample sets of test data. The improved scalability is due to the better complexity of DT learners. The running time of our learning algorithm is Opmn logpnqq where m is the number of features/hyperplane slopes that we consider, and n the number of sample points. On the other hand, the set cover algorithm has a running time of Opmn3q. This is because the greedy algorithm for set cover takes time Ophn2q where h is the number of hyperplanes, and [31] considers one hyperplane for every candidate slope and sample point, yielding h \u201c mn.\nWhen invariant generation is viewed as binary classification, then there is a problem in the refinement loop: if the learned invariant is not inductive, it is unclear whether the counterexample model produced by the theorem prover should be considered a \u201cbad\u201d or a \u201cgood\u201d state. The ICE-learning framework [14] solves this problem by formulating invariant generation as a more general classification problem that also accounts for implication constraints between points. We note that our algorithm does not fit within this framework, as we do not have a refinement loop that can handle counterexamples in the form of implications. However, we found in our experiments that we did not need any refinement loop as our algorithm was able to infer correct invariants directly after sampling enough data. Nevertheless, considering an ICE version of DT learning is interesting as sampling without a refinement loop becomes difficult for more complex programs.\nThe paper [14] also proposes a concrete algorithm for inferring linear invariants that fits into the ICE-learning framework (referred to as ICE in our evaluation). If we compare the complexity of learning given a fixed sample, our algorithm performs better than [14] both in terms of running time and expressiveness of the invariant. The ICE algorithm of [14] iterates through templates for the invariant. This iteration is done by dovetailing between more complex Boolean structures and increasing the range of the thresholds used. For a fixed template, it formulates the problem of this template being consistent with all given samples as a constraint in quantifier-free linear integer arithmetic. Satisfiability of this constraint is then checked using an SMT solver. We note that the size of the generated constraint is linear in the sample size, and that solving such constraints is NP-complete. In comparison, our learning is sub-quadratic time in the sample size. Also, we do not need to fix templates for the Boolean structure of the invariant or bound the thresholds a priori. Instead, the DT learner automatically infers those parameters from the sample data.\nAnother ICE-learning algorithm based on randomized search was proposed in [30] (the algorithm MCMC in our evaluation). This algorithm searches over a fixed space of invariants S that is chosen in advance either by bounding the Boolean structure and coefficients of inequalities, or by picking some finite sub-lattice of an abstract domain. Given a sample, it randomly searches using a combination of random walks and hill climbing until it finds a candidate invariant that satisfies all the samples. There is no obvious bound on the time of this search other than the trivial bound of |S|. Again, we have the advantage that we do not have to provide templates of the Boolean structure and the thresholds of the hyperplanes. These parameters have to be fixed for the algorithm in [30]. Furthermore, the greedy nature of DT learning is a heuristic to try simpler invariants before more complex ones, and hence the invariants we find for these benchmarks are often much simpler than those found by MCMC.\nDecision trees have been previously used for inferring likely preconditions of procedures [28]. Although this problem is related to invariant generation, there are considerable technical differences to our algorithm. In particular, the algorithm proposed in [28] only learns formulas that fall into a finite abstract domain (Boolean combinations of a given finite set of predicates), whereas we use decision trees to learn more general formulas in an infinite abstract domain (e.g., unions of octagons).\nWe believe that the main value of our algorithm is its ability to infer invariants with a complex Boolean structure efficiently from test data. Other techniques for inferring such invariants include predicate abstraction [16] as well as abstract interpretation techniques such as disjunctive completion [10]. However, for efficiency reasons, many static analyses are restricted to inferring conjunctive invariants in practice [3,11]. There exist techniques for recovering loss of precision due to imprecise joins using counterexample-guided refinement [1, 21, 26]. In the future, we will explore whether DL learning can be used to complement such refinement techniques for static analyses."}], "references": [{"title": "Craig interpretation. In SAS, volume 7460 of LNCS, pages 300\u2013316", "author": ["A. Albarghouthi", "A. Gurfinkel", "M. Chechik"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "UFO: verification with interpolants and abstract interpretation - (competition contribution)", "author": ["A. Albarghouthi", "A. Gurfinkel", "Y. Li", "S. Chaki", "M. Chechik"], "venue": "In TACAS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Boolean and cartesian abstraction for model checking C programs", "author": ["T. Ball", "A. Podelski", "S.K. Rajamani"], "venue": "In TACAS, volume 2031 of LNCS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Boogie: A modular reusable verifier for object-oriented programs", "author": ["M. Barnett", "B.E. Chang", "R. DeLine", "B. Jacobs", "K.R.M. Leino"], "venue": "FMCO", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Cpachecker: A tool for configurable software verification", "author": ["D. Beyer", "M.E. Keremoglu"], "venue": "In CAV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Learnability and the vapnikchervonenkis dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth"], "venue": "J. ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "Classification and Regression Trees", "author": ["L. Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1984}, {"title": "Counterexample-guided abstraction refinement", "author": ["E.M. Clarke", "O. Grumberg", "S. Jha", "Y. Lu", "H. Veith"], "venue": "In CAV, volume 1855 of LNCS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints", "author": ["P. Cousot", "R. Cousot"], "venue": "In POPL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1977}, {"title": "Systematic design of program analysis frameworks", "author": ["P. Cousot", "R. Cousot"], "venue": "In POPL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1979}, {"title": "The astre\u00e9 analyzer", "author": ["P. Cousot", "R. Cousot", "J. Feret", "L. Mauborgne", "A. Min\u00e9", "D. Monniaux", "X. Rival"], "venue": "In ESOP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "An Efficient SMT Solver", "author": ["L. De Moura", "N. Bj\u00f8rner. Z"], "venue": "In TACAS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Inductive invariant generation via abductive inference", "author": ["I. Dillig", "T. Dillig", "B. Li", "K.L. McMillan"], "venue": "In OOPSLA,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "ICE: A robust framework for learning invariants", "author": ["P. Garg", "C. L\u00f6ding", "P. Madhusudan", "D. Neider"], "venue": "In CAV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Compositional may-must program analysis: unleashing the power of alternation", "author": ["P. Godefroid", "A.V. Nori", "S.K. Rajamani", "S. Tetali"], "venue": "In POPL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Construction of abstract state graphs with PVS", "author": ["S. Graf", "H. Saidi"], "venue": "In CAV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "SYNERGY: a new algorithm for property checking", "author": ["B.S. Gulavani", "T.A. Henzinger", "Y. Kannan", "A.V. Nori", "S.K. Rajamani"], "venue": "In FSE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Invgen: An efficient invariant generator", "author": ["A. Gupta", "A. Rybalchenko"], "venue": "In CAV, volume 5643 of LNCS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Constructing optimal binary decision trees is np-complete", "author": ["L. Hyafil", "R.L. Rivest"], "venue": "Inf. Process. Lett.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1976}, {"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1986}, {"title": "Tree automata-based refinement with application to horn clause verification", "author": ["B. Kafle", "J.P. Gallagher"], "venue": "In VMCAI, LNCS. Springer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "The octagon abstract domain", "author": ["A. Min\u00e9"], "venue": "Higher-Order and Symbolic Computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Machine learning. McGraw Hill series in computer science", "author": ["T.M. Mitchell"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Foundations of machine learning 2014: Homework assignment 1 (problem c)", "author": ["M. Mohri"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Counterexample-guided focus", "author": ["A. Podelski", "T. Wies"], "venue": "In POPL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan. C"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1993}, {"title": "Dynamic inference of likely data preconditions over predicates by tree learning", "author": ["S. Sankaranarayanan", "S. Chaudhuri", "F. Ivancic", "A. Gupta"], "venue": "In ISSTA,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "On the density of families of sets", "author": ["N. Sauer"], "venue": "J. Comb. Theor. Ser. A,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1972}, {"title": "From invariant checking to invariant inference using randomized search", "author": ["R. Sharma", "A. Aiken"], "venue": "In CAV,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Verification as learning geometric concepts", "author": ["R. Sharma", "S. Gupta", "B. Hariharan", "A. Aiken", "A.V. Nori"], "venue": "editors, SAS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Interpolants as classifiers", "author": ["R. Sharma", "A.V. Nori", "A. Aiken"], "venue": "In CAV, volume 7358 of LNCS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Commun. ACM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1984}, {"title": "Testing, abstraction, theorem proving: better together", "author": ["G. Yorsh", "T. Ball", "M. Sagiv"], "venue": "In ISSTA,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}], "referenceMentions": [{"referenceID": 8, "context": "Some techniques, such as abstract interpretation [9], are effective at striking a good balance between scalability and precision by allowing the analysis to be fine-tuned for a specific class of programs and properties.", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "This fine-tuning requires careful engineering of the analysis [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Instead of manually adapting the analysis to work well across many similar programs, refinement-based techniques adapt the analysis automatically to the given program and property at hand [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 14, "context": "A promising approach to achieve this automatic adaptation is to exploit synergies between static analysis and testing [15,17,34].", "startOffset": 118, "endOffset": 128}, {"referenceID": 16, "context": "A promising approach to achieve this automatic adaptation is to exploit synergies between static analysis and testing [15,17,34].", "startOffset": 118, "endOffset": 128}, {"referenceID": 33, "context": "A promising approach to achieve this automatic adaptation is to exploit synergies between static analysis and testing [15,17,34].", "startOffset": 118, "endOffset": 128}, {"referenceID": 13, "context": "Particularly interesting is the use of Machine Learning (ML) to infer likely invariants from test data [14,31,32].", "startOffset": 103, "endOffset": 113}, {"referenceID": 30, "context": "Particularly interesting is the use of Machine Learning (ML) to infer likely invariants from test data [14,31,32].", "startOffset": 103, "endOffset": 113}, {"referenceID": 31, "context": "Particularly interesting is the use of Machine Learning (ML) to infer likely invariants from test data [14,31,32].", "startOffset": 103, "endOffset": 113}, {"referenceID": 30, "context": "Invariant inference can be viewed as a binary classification problem [31].", "startOffset": 69, "endOffset": 73}, {"referenceID": 32, "context": "More precisely, using the Probably Approximately Correct model for learning [33], we provide a bound on the sample size that ensures that our algorithm successfully learns a safe inductive invariant with a given probability.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "In particular, inspired by the octagon abstract domain [22], we use as features the set of all hyperplane slopes of the form \u0306xi  \u0306 xj , where 1 \u010f i \u0103 j \u010f d.", "startOffset": 55, "endOffset": 59}, {"referenceID": 13, "context": "Moreover, we observed that DT learners often produce simpler invariants and scale better to large sample sets compared to other ML-based invariant inference techniques such as [14, 30\u201332].", "startOffset": 176, "endOffset": 187}, {"referenceID": 29, "context": "Moreover, we observed that DT learners often produce simpler invariants and scale better to large sample sets compared to other ML-based invariant inference techniques such as [14, 30\u201332].", "startOffset": 176, "endOffset": 187}, {"referenceID": 30, "context": "Moreover, we observed that DT learners often produce simpler invariants and scale better to large sample sets compared to other ML-based invariant inference techniques such as [14, 30\u201332].", "startOffset": 176, "endOffset": 187}, {"referenceID": 31, "context": "Moreover, we observed that DT learners often produce simpler invariants and scale better to large sample sets compared to other ML-based invariant inference techniques such as [14, 30\u201332].", "startOffset": 176, "endOffset": 187}, {"referenceID": 30, "context": "Following [31], we view the problem of computing a safe inductive invariant for a program P \u201c pS,R, init, safeq as a binary classification problem by defining the input space as the set of all program states X \u201c S.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "A Decision Tree (DT) [23] is a binary tree that represents a Boolean function.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "However, the task of finding the smallest (in terms of number of nodes) DT for a particular function can be shown to be NP-complete [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "Standard algorithms to learn DTs work by greedily selecting at each node the co-ordinate and threshold that separates the remaining training data best [7,27].", "startOffset": 151, "endOffset": 157}, {"referenceID": 26, "context": "Standard algorithms to learn DTs work by greedily selecting at each node the co-ordinate and threshold that separates the remaining training data best [7,27].", "startOffset": 151, "endOffset": 157}, {"referenceID": 24, "context": "There are other measures as well, such as the Gini index, which is used by the DT learner we used in our experiments [25].", "startOffset": 117, "endOffset": 121}, {"referenceID": 32, "context": "We can justify this observation using Valiant\u2019s PAC (probably approximately correct) model [33].", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "[6]:", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Now one can use a basic, well-known lemma from [24] combined with Sauer\u2019s Lemma [29] to get that the VC dimension of bounded decision trees is OpKd logKq.", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "Now one can use a basic, well-known lemma from [24] combined with Sauer\u2019s Lemma [29] to get that the VC dimension of bounded decision trees is OpKd logKq.", "startOffset": 80, "endOffset": 84}, {"referenceID": 6, "context": "However, the running time of the learning routine for DTs is Opmn logpnqq, where m is the number of hyperplane slopes in H and n is the number of sample points [7, 25].", "startOffset": 160, "endOffset": 167}, {"referenceID": 24, "context": "However, the running time of the learning routine for DTs is Opmn logpnqq, where m is the number of hyperplane slopes in H and n is the number of sample points [7, 25].", "startOffset": 160, "endOffset": 167}, {"referenceID": 24, "context": "We implemented our algorithm in Python, using the scikit-learn library\u2019s decision tree classifier [25] as the DT learner LearnDT.", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "This implementation uses the CART algorithm from [7] which learns in a greedy manner as described in Section 3, and uses the Gini index.", "startOffset": 49, "endOffset": 52}, {"referenceID": 30, "context": "In [31], the authors suggest working in the null space of the good states, viewed as a matrix.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "Inspired by this, we propose using Principal Component Analysis [20] on the good states to generate slopes.", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "Finally, for the IsInvariant routine, we used the program verifier Boogie [4], which allowed us to annotate our programs with the invariants we verified.", "startOffset": 74, "endOffset": 77}, {"referenceID": 11, "context": "Boogie uses the SMT solver Z3 [12] as a back-end.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "\u2013 ICE [14]: an ML algorithm based on ICE-learning that uses an SMT solver to learn numerical invariants.", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": "\u2013 MCMC [30]: an ML algorithm based on Markov Chain Monte Carlo methods.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "\u2013 SC [31]: an ML algorithm based on set cover.", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "We only had access to the learning algorithm proposed in [31] and not the sampling procedure.", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "\u2013 CPAchecker [5]: a configurable software model checker.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "\u2013 UFO [2]: a software model checker that combines abstract interpretation and interpolation (denoted CPA).", "startOffset": 6, "endOffset": 9}, {"referenceID": 17, "context": "\u2013 InvGen [18]: an inference tool for linear invariants that combines abstract interpretation, constraint solving, and testing.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "In particular, we considered a subset of the benchmarks from [13, 14, 18, 31].", "startOffset": 61, "endOffset": 77}, {"referenceID": 13, "context": "In particular, we considered a subset of the benchmarks from [13, 14, 18, 31].", "startOffset": 61, "endOffset": 77}, {"referenceID": 17, "context": "In particular, we considered a subset of the benchmarks from [13, 14, 18, 31].", "startOffset": 61, "endOffset": 77}, {"referenceID": 30, "context": "In particular, we considered a subset of the benchmarks from [13, 14, 18, 31].", "startOffset": 61, "endOffset": 77}, {"referenceID": 13, "context": "Octagonal: ex23 [14] 4 conj.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "02 fig6 [14] 2 conj.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "01 fig9 [14] 2 conj.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "01 hola10 [13] 4 conj.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "03 F F nested2 [31] 4 conj.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "03 nested5 [18] 4 conj.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "03 fig1 [14] 2 disj.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "64 F test1 [31] 4 disj.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "04 cegar2 [14] 3 ABC 5 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "18 F gopan [31] 2 ABC 8 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "29 F hola18 [13] 3 ABC 6 1.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "38 F hola19 [13] 4 ABC 7 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 30, "context": "20 F TO F F F popl07 [31] 2 ABC 7 0.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "20 F prog4 [31] 3 ABC 8 2.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "13 F F F sum1 [14] 3 ABC 6 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "17 F trex3 [14] 8 ABC 9 8.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "Non octagonal: hola15 [13] 3 conj.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "Modulus: hola02 [13] 4 conj.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "06 F NA F F F hola06 [13] 4 conj.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "82 F NA F F F hola22 [13] 4 conj.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "Non octagonal modulus: hola34 [13] 4 ABC 6 1.", "startOffset": 30, "endOffset": 34}, {"referenceID": 30, "context": "[31] used the greedy set cover algorithm SC to learn invariants in the form of arbitrary Boolean combinations of linear inequalities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Our algorithm based on decision trees is simpler than the set cover algorithm, works better on our benchmarks (which includes most of the benchmarks from [31]), and scales much better to large sample sets of test data.", "startOffset": 154, "endOffset": 158}, {"referenceID": 30, "context": "This is because the greedy algorithm for set cover takes time Ophnq where h is the number of hyperplanes, and [31] considers one hyperplane for every candidate slope and sample point, yielding h \u201c mn.", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "The ICE-learning framework [14] solves this problem by formulating invariant generation as a more general classification problem that also accounts for implication constraints between points.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "The paper [14] also proposes a concrete algorithm for inferring linear invariants that fits into the ICE-learning framework (referred to as ICE in our evaluation).", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "If we compare the complexity of learning given a fixed sample, our algorithm performs better than [14] both in terms of running time and expressiveness of the invariant.", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "The ICE algorithm of [14] iterates through templates for the invariant.", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "Another ICE-learning algorithm based on randomized search was proposed in [30] (the algorithm MCMC in our evaluation).", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "These parameters have to be fixed for the algorithm in [30].", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Decision trees have been previously used for inferring likely preconditions of procedures [28].", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "In particular, the algorithm proposed in [28] only learns formulas that fall into a finite abstract domain (Boolean combinations of a given finite set of predicates), whereas we use decision trees to learn more general formulas in an infinite abstract domain (e.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "Other techniques for inferring such invariants include predicate abstraction [16] as well as abstract interpretation techniques such as disjunctive completion [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "Other techniques for inferring such invariants include predicate abstraction [16] as well as abstract interpretation techniques such as disjunctive completion [10].", "startOffset": 159, "endOffset": 163}, {"referenceID": 2, "context": "However, for efficiency reasons, many static analyses are restricted to inferring conjunctive invariants in practice [3,11].", "startOffset": 117, "endOffset": 123}, {"referenceID": 10, "context": "However, for efficiency reasons, many static analyses are restricted to inferring conjunctive invariants in practice [3,11].", "startOffset": 117, "endOffset": 123}, {"referenceID": 0, "context": "There exist techniques for recovering loss of precision due to imprecise joins using counterexample-guided refinement [1, 21, 26].", "startOffset": 118, "endOffset": 129}, {"referenceID": 20, "context": "There exist techniques for recovering loss of precision due to imprecise joins using counterexample-guided refinement [1, 21, 26].", "startOffset": 118, "endOffset": 129}, {"referenceID": 25, "context": "There exist techniques for recovering loss of precision due to imprecise joins using counterexample-guided refinement [1, 21, 26].", "startOffset": 118, "endOffset": 129}], "year": 2015, "abstractText": "The problem of inferring an inductive invariant for verifying program safety can be formulated in terms of binary classification. This is a standard problem in machine learning: given a sample of good and bad points, one is asked to find a classifier that generalizes from the sample and separates the two sets. Here, the good points are the reachable states of the program, and the bad points are those that reach a safety property violation. Thus, a learned classifier is a candidate invariant. In this paper, we propose a new algorithm that uses decision trees to learn candidate invariants in the form of arbitrary Boolean combinations of numerical inequalities. We have used our algorithm to verify C programs taken from the literature. The algorithm is able to infer safe invariants for a range of challenging benchmarks and compares favorably to other ML-based invariant inference techniques. In particular, it scales well to large sample sets.", "creator": "TeX"}}}