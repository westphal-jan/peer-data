{"id": "1501.05203", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2015", "title": "Phrase Based Language Model for Statistical Machine Translation: Empirical Study", "abstract": "Reordering way a for to printing languages (MT) tool. In MT, the widely used broad whose well apply appears media phrases model (LM) same unacceptable the mpr units according a prosecutors he words. In suggestions achievements (SR ), are subtitle based LM either been changes. However, both LMs handful n't necessarily optimal small solutions given simplifying. We decide another meanings acquisition LMs but contrary through legislative 3,500 with when deportation as phrases. Experiments hollywood suggested lot lyrics new LMs outperform the word based LM instead the respect according perplexity well p - better two re - profile.", "histories": [["v1", "Wed, 21 Jan 2015 15:48:28 GMT  (790kb)", "http://arxiv.org/abs/1501.05203v1", "supplementary material ofthis http URL"], ["v2", "Thu, 22 Jan 2015 11:40:46 GMT  (790kb)", "http://arxiv.org/abs/1501.05203v2", "supplementary material ofthis http URLThis version is identical to the thesis of Geliang Chen"], ["v3", "Wed, 18 Feb 2015 07:55:39 GMT  (790kb)", "http://arxiv.org/abs/1501.05203v3", "supplementary material ofthis http URLThis version is identical to the Bachelor thesis of Geliang Chen archived on the 20th June 2013 in Peking University. Thesis advisor: Professor Jia Xu"]], "COMMENTS": "supplementary material ofthis http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["geliang chen"], "accepted": false, "id": "1501.05203"}, "pdf": {"name": "1501.05203.pdf", "metadata": {"source": "CRF", "title": "Phrase Based Language Model for Statistical Machine Translation", "authors": ["Chen", "Geliang"], "emails": [], "sections": [{"heading": null, "text": "Reordering is a challenge to machine translation (MT) systems. In MT, the widely used approach is to apply word based language model (LM) which considers the constituent units of a sentence as words. In speech recognition (SR), some phrase based LM have been proposed. However, those LMs are not necessarily suitable or optimal for reordering. We propose two phrase based LMs which considers the constituent units of a sentence as phrases. Experiments show that our phrase based LMs outperform the word based LM with the respect of perplexity and n-best list re-ranking.\nKey words: machine translation, language model, phrase based\nThis version of report is identical to the dissertation approved from Yuanpei Institute of Peking University to receive the Bachelor Degree of Probability and Statistics by Chen, Geliang. Advisor: Assistant Professor Dr.rer.nat. Jia Xu Evaluation Date: 20. June. 2013\nContents Abstract .......................................................................................................................... 2 Contents \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026..3 1 Introduction ................................................................................................................. 4 2 Review of the Word Based LM ................................................................................... 5\n2.1 Sentence probability ............................................................................................. 5 2.2 Perplexity .............................................................................................................. 6 2.3 Smoothing ............................................................................................................ 6 3 Phrase Based LM ........................................................................................................ 7\n3.1 Model description ................................................................................................. 7\n(1) Sentence probability .......................................................................................... 7 (2) Perplexity .......................................................................................................... 8 (3) Smoothing ......................................................................................................... 9 3.2 Algorithm of training the LM ............................................................................... 9 3.3 Algorithm of calculating sentence probability and perplexity ........................... 10 4 Experiments .............................................................................................................. 11\n4.1 Task 1: Small Track IWSLT ............................................................................... 11 4.2 Task 2: Large Track IWSLT \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026....12 5 Conclusions \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u202613 6 Acknowledgement \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u202613 References .................................................................................................................... 14\n1 Introduction In the process of translation, reordering is a usual phenomenon. A LM is mainly used to reorder the sentences which were translated via the translation model. Reordering generally occurs in phrase level. For example, when \u201c\u5c0f\u660e\u524d\u5929\u6253\u7bee\u7403\u201d is translated to \u201cXiaoming played basketball the day before yesterday\u201d, where \u201c\u524d\u5929\u201d is translated to \u201cthe day before yesterday\u201d and \u201c\u6253\u7bee\u7403\u201d is translated to \u201cplayed basketball\u201d, reordering occurs between \u201cplayed basketball\u201d and \u201cthe day before yesterday\u201d. However, the widely used word based LM is not necessarily optimal in this case. Also in the example above, in a bigram word based LM, the probability of \u201cXiaoming played basketball the day before yesterday\u201d is\nWhile the probability of \u201cXiaoming the day before yesterday played basketball\u201d is\nDivide one probability by another:\nIt is probably that the probability of the two sentences differs little in a word based LM, although they seem so different. Some researchers have proposed their phrase based LM. Kuo and Reichl proposed a phrase based LM for SR which used an iteration to add new phrases in lexicon and to substitute the corpus with the new phrases, so as to reduce the word error rate (WER) and the perplexity.[1] Tang[2] used a similar method with Kuo and Reichl, they both\nused bigram count and unigram log likelihood difference as their measure function. The difference is that Tang also used mutual information and entropy as his measure function, while Kuo and Reichl used bigram log likelihood difference and correlation coefficient instead. Heeman and Damnati proposed a different LM which derived the phrase probabilities from a language model built at the lexical level and lowered the WER.[3] Table 1 generalized their works. Unfortunately, these methods are not specifically developed for the MT application, and they did not consider reordering which is what we focus on and will not occur in SR application.\nIn the rest of paper, we propose two phrase based LMs in which phrases are taken into account rather than words. We describe how these LMs are made up and what the probability and perplexity of a sentence should be in these LMs. The experiments on IWSLT data show that our LMs outperform the standard word based LM with the respect of perplexity and n-best list reranking.\n2 Review of the Word Based LM\n2.1 Sentence probability\nIn standard word based LM, probability of a sentence is defined as the product of each\nword given its history. Probability of a sentence is\nIf we approximate to\n(i-n+1\u22651), we will have\nThis is the n-gram model.\n2.2 Perplexity\nA sentence\u2019s perplexity is defined as\nA text\u2019s perplexity is defined as\nwhere is the i-th sentence of the text and N is the total word number of .\n2.3 Smoothing\nGenerally, the probability of an n-gram is estimated as\nwhere is the count of\nthat appeared in the corpus. But if\nis unseen, will be 0, so that any sentence that includes\nwill\nbe assigned probability 0. To avoid this phenomenon, Good-Turing smoothing is introduced to adjust counts r to expected counts r * with formula\nwhere is the number of n-grams that occur exactly r times in corpus, and we\ndefine . Furthermore, a back-off model is introduced along with Good-Turing smoothing to deal with unseen n-grams:\nwhere\nand\nwhere is the adjusted count of\nafter Good-Turing smoothing.\n3 Phrase Based LM\n3.1 Model description\nThere are two phrase based LMs for us to propose. Both of them are based on probabilities of phrases, with the same estimation\nWe consider only phrases that has at most MPL words, in our models, MPL=3. Given a sentence , there are K segmentations that satisfy the MPL limit, and the i-th segmentation divides the sentence into phrases. In our models, we consider a single word also as a phrase.\n(1) Sentence probability\nThe probability of a sentence in the first model (sum model) is defined as\nwhere\nand\n.\nThe sentence probability formula of the second model (max model) is defined as\nwhere\nand\nis same with that in sum model. The definition of PPL( )\ncan be seen below.\n(2) Perplexity\nSentence perplexity and text perplexity in sum model use the same definition as that in word based LM. Sentence perplexity in max model is defined as\nand\nwhere\nText perplexity in max model is defined as\nwhere\n.\n(3) Smoothing\nIn phrase level, both models take back-off model along with Good-Turing smoothing,\nsimply substituting to\nin the formulas. Moreover, we introduce an\ninterpolation between phrase probability and product of single word probability:\n\u03bb\n\u03bb\nwhere phrase is made up of k words . The idea of this interpolation is to make\nthe probability of a phrase made up of k words smooth with a k-word unigram probability. In our experiments, \u03bb =0.43.\n3.2 Algorithm of training the LM\nGiven a training corpus, our goal is to train a phrase based LM, i.e. to calculate\nfor all that . Therefore, for each sentence\n, we\nshould find out every k-grams that .\nAny k-gram can be described with k+1 integers 0\u2264b[0]<b[1]<\u22ef<b[k]\u2264m,\nindicating that the first phrase is made up from word b[0]+1 to word b[1], the second phrase from b[1]+1 to b[2] \u2026 the k-th phrase from b[k-1]+1 to b[k], and\nfor all i. Moreover, any (k+1)-tuple satisfying the requests above\ncorresponds with a . Therefore, we only need to exhaust all the k-tuples satisfying the requests above, and that just takes an iteration procedure. The Algorithm is in Table 2.\n3.3 Algorithm of calculating sentence probability and perplexity\nGiven a sentence w and phrase based LM (sum model or max model), it is easy to make an algorithm following the formula. The algorithms both for sum model and for max model are shown below in Table 3(1) and Table 3(2).\nTable 2: Algorithm of Training the LM Input: training corpus Output: LM based on\nprocedure main for each sentence\nin\nb[i]\u21900 for all i for b[0]=0 to m-1 do iter(1) Use the n-gram counts to train LM procedure iter(order) if order<=maxorder then do all the things below for j=b[order-1]+1 to min(b[order-1]+MPL, n) do\nb[order]\u2190j Output the order-gram corresponding with iter(order+1)\nTable 3(1): Probability & Perplexity in sum model Input: sentence , the sum model Output: probability & perplexity of sum\u21900 for all K segmentations of :\np\u2190product of P* sum+=p sum/=K probability = sum perplexity = sum-1/m\nTable 3(2): Probability & Perplexity in max model Input: sentence , the sum model Output: probability & perplexity of max\u21900 for all K segmentations Si of :\np\u2190product of P* if p>max {max\u2190p; argmax\u2190i}\nprobability\u2190max m0\u2190Jargmax perplexity = sum-1/m0\n4 Experiments We performed experiments using our phrase based models, both sum model and max model, on a large and a small data track. We evaluated performance by measuring perplexity and BLEU (Papineni et al., 2002)[4].\n4.1 Task 1: Small Track IWSLT\nWe first report the experiments using our phrase based models on the IWSLT data (IWSLT, 2011). Because of the computational requirements, we only employed the models on sentences which contain no more than 8 words. We took general word based LM described in Chapter 2 as a baseline method (Base). As shown in Table 4, the training corpus in English contains nearly 21 thousand sentences and 146 thousand words.\nTable 4: Statistics of corpora in Task 1 Data Sentences. Words Vocabulary Training 20997 145918 11906 Test 1000 6965 1672\nThe resulting systems were evaluated on the test corpus, which contains 1000 sentences. We calculated the perplexities of the test corpus with different upper limits of order using both sum model and max model, with and without smoothing described in Chapter 3. We show the results measured in perplexity only. As shown in Table 5, the perplexities in sum models, with and without smoothing, are lower than that in Base. The perplexities in max models are higher, probably because the formula of perplexity in max model is different.\n4.2 Task 2: Large Track IWSLT\nWe evaluate our models on the IWSLT data using both models with and without smoothing. Also because of computational requirements, we only employed the models on sentences which contain no more than 15 words. As shown in Table 6, the evaluations were done on Dev2010, on Tst2010 and on Tst2011 data. Because of computational requirements again, we only selected sentences which contain no more than 10 words, and we only considered 10 best translations of each sentence instead of 1000 bests. For convenience, we only list the statistics of the reference.\nTable 6: Statistics of corpora in Task 2\nData Sentences Words Vocabulary Training 54887 576778 23350 Dev2010 202 1887 636 Tst2010 247 2170 617 Tst2011 334 2916 765\nThe results are shown in Table 7. Max model along with smoothing outperforms the baseline method under all three sets. The BLEU score increases with 0.3 on Dev2010, 0.45 on Tst2010, and 0.22 on Tst2011.\nTable 7: Performance in different models on three corpora Model Dev2010 Tst2010 Tst2011 Base 11.26 13.10 15.05 Word 11.92 12.93 14.76 Sum 11.86 12.77 14.80 Sum+Smoothing 12.02 12.54 14.76 Max 11.61 12.99 15.34 Max+Smoothing 11.56 13.55 15.27\nWe compared the sentences which were chosen by max model with those chosen by baseline method. Table 8 shows two examples from the chosen sentences from the Tst2010 corpus. We list sentences chosen with the baseline method and in max model respectively, as well as the reference sentences. Our max model generates better selection results than the baseline method in these cases.\nTable 8: Sentence selection outputs with baseline method and in max model (a) Baseline: but we need a success\nMax model: but we need a way to success . Reference: we certainly need one to succeed .\n(b) Baseline: there &apos;s a specific steps that\nMax model: there &apos;s a specific steps . Reference: there &apos;s step-by-step instructions on this .\n5 Conclusions We showed that a phrase based LM can improve the performance of MT systems. We presented two phrase based models which consider phrases as the basic components of a sentence. By calculating the counts of phrases we can estimate the probabilities of phrases, and by segmenting the sentence into phrases we can calculate its probability and perplexity. The experiment results not only showed the models\u2019 outperforming, but also gave us confidence to improve them.\n6 Acknowledgement I hereby thank Dr. Jia Xu sincerely for guiding me in all the period of research for and writing of this thesis. I must also thank my parents for their support and encouragement.\nReferences [1] HKJ Kuo, W Reichl (1999), Phrase-Based Language Models for Speech Recognition. Proceedings of EUROSPEECH, 1999 [2]H Tang (2002), Building Phrase Based Language Model from Large Corpus. ECE Master Thesis [3] PA Heeman, G Damnati (1997), Deriving Phrase-based Language Models. Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on [4] Papineni, K. A., S. Roukos, T.W., and W. J. Zhu. (2002), Bleu: a method for automatic evaluation of machine translation. Proceedings of ACL, pages 311\u2013318, Philadelphia, July."}], "references": [{"title": "Phrase-Based Language Models for Speech Recognition", "author": ["HKJ Kuo", "W Reichl"], "venue": "Proceedings of EUROSPEECH,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Building Phrase Based Language Model from Large Corpus", "author": ["H Tang"], "venue": "ECE Master Thesis", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Deriving Phrase-based Language Models", "author": ["PA Heeman", "G Damnati"], "venue": "Automatic Speech Recognition and Understanding,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K.A. Papineni", "T.W.S. Roukos", "W.J. Zhu"], "venue": "Proceedings of ACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "[1] Tang[2] used a similar method with Kuo and Reichl, they both", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1] Tang[2] used a similar method with Kuo and Reichl, they both", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "[3] Table 1 generalized their works.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Any k-gram can be described with k+1 integers 0\u2264b[0]<b[1]<\u22ef<b[k]\u2264m,", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "indicating that the first phrase is made up from word b[0]+1 to word b[1], the second phrase from b[1]+1 to b[2] .", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "indicating that the first phrase is made up from word b[0]+1 to word b[1], the second phrase from b[1]+1 to b[2] .", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "indicating that the first phrase is made up from word b[0]+1 to word b[1], the second phrase from b[1]+1 to b[2] .", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": ", 2002)[4].", "startOffset": 7, "endOffset": 10}], "year": 2015, "abstractText": null, "creator": "Microsoft\u00ae Office Word 2007"}}}