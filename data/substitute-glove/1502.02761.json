{"id": "1502.02761", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2015", "title": "Generative Moment Matching Networks", "abstract": "We consider their so of learning huge mmf newer from data. We formulate hand method that generates on independent sample via a box feedforward pick then place multi-tier blueprinting, they been along seen proposed alchemy adversarial broadcast (Goodfellow soci\u00e9t\u00e9 el. , 1997 ). Training a granulation convoluted network, yet, requires sort simulation and turn difficult camp-dependent service. Instead, keep techniques a technique been definitions deriving requiring known as speeds if discrepancy (MMD ), is lead this similar any criteria it can why differs in caps have orders of consumer began a holotype turn samples however that simplified, brought different make teacher addition backpropagation. We further competitiveness after thanks of this approaches notably modes our generative network from because domestic - encoder communications, that MMD still anything to generate codes that sure coming also decoded to produce measurements. We drama that all combination same describe techniques offset important generative models 48 to baseline transition as measured on MNIST turn the Toronto Face Database.", "histories": [["v1", "Tue, 10 Feb 2015 02:54:58 GMT  (855kb,D)", "http://arxiv.org/abs/1502.02761v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["yujia li", "kevin swersky", "richard s zemel"], "accepted": true, "id": "1502.02761"}, "pdf": {"name": "1502.02761.pdf", "metadata": {"source": "META", "title": "Generative Moment Matching Networks", "authors": ["Yujia Li", "Kevin Swersky", "Richard Zemel"], "emails": ["YUJIALI@CS.TORONTO.EDU", "KSWERSKY@CS.TORONTO.EDU", "ZEMEL@CS.TORONTO.EDU"], "sections": [{"heading": "1. Introduction", "text": "The most visible successes in the area of deep learning have come from the application of deep models to supervised learning tasks. Models such as convolutional neural networks (CNNs), and long short term memory (LSTM) networks are now achieving impressive results on a number of tasks such as object recognition (Krizhevsky et al., 2012; Sermanet et al., 2014; Szegedy et al., 2014), speech recognition (Graves & Jaitly, 2014; Hinton et al., 2012a), image caption generation (Vinyals et al., 2014; Fang et al., 2014;\nPreliminary work.\nKiros et al., 2014), machine translation (Cho et al., 2014; Sutskever et al., 2014), and more. Despite their successes, one of the main bottlenecks of the supervised approach is the difficulty in obtaining enough data to learn abstract features that capture the rich structure of the data. It is well recognized that a promising avenue is to use unsupervised learning on unlabelled data, which is far more plentiful and cheaper to obtain.\nA long-standing and inherent problem in unsupervised learning is defining a good method for evaluation. Generative models offer the ability to evaluate generalization in the data space, which can also be qualitatively assessed. In this work we propose a generative model for unsupervised learning that we call generative moment matching networks (GMMNs). GMMNs are generative neural networks that begin with a simple prior from which it is easy to draw samples. These are propagated deterministically through the hidden layers of the network and the output is a sample from the model. Thus, with GMMNs it is easy to quickly draw independent random samples, as opposed to expensive MCMC procedures that are necessary in other models such as Boltzmann machines (Ackley et al., 1985; Hinton, 2002; Salakhutdinov & Hinton, 2009). The structure of a GMMN is most analogous to the recently proposed generative adversarial networks (GANs) (Goodfellow et al., 2014), however unlike GANs, whose training involves a difficult minimax optimization problem, GMMNs are comparatively simple; they are trained to minimize a straightforward loss function using backpropagation.\nThe key idea behind GMMNs is the use of a statistical hypothesis testing framework called maximum mean discrepancy (Gretton et al., 2007). Training a GMMN to minimize this discrepancy can be interpreted as matching all moments of the model distribution to the empirical data distribution. Using the kernel trick, MMD can be represented as a simple loss function that we use as the core training objective for GMMNs. Using minibatch stochastic gradient descent, training can be kept efficient, even with large datasets.\nar X\niv :1\n50 2.\n02 76\n1v 1\n[ cs\n.L G\n] 1\n0 Fe\nb 20\nAs a second contribution, we show how GMMNs can be used to bootstrap auto-encoder networks in order to further improve the generative process. The idea behind this approach is to train an auto-encoder network and then apply a GMMN to the code space of the auto-encoder. This allows us to leverage the rich representations learned by auto-encoder models as the basis for comparing data and model distributions. To generate samples in the original data space, we simply sample a code from the GMMN and then use the decoder of the auto-encoder network.\nOur experiments show that this relatively simple, yet very flexible framework is effective at producing good generative models in an efficient manner. On MNIST and the Toronto Face Dataset (TFD) we demonstrate improved results over comparable baselines, including GANs. Source code for training GMMNs will be made available at https://github.com/yujiali/gmmn."}, {"heading": "2. Maximum Mean Discrepancy", "text": "Suppose we are given two sets of samples X = {xi}Ni=1 and Y = {yj}Mj=1 and are asked whether the generating distributions PX = PY . Maximum mean discrepancy is a frequentist estimator for answering this question, also known as the two sample test (Gretton et al., 2007; 2012a). The idea is simple: compare statistics between the two datasets and if they are similar then the samples are likely to come from the same distribution.\nFormally, the following MMD measure computes the mean squared difference of the statistics of the two sets of samples.\nLMMD2 = \u2225\u2225\u2225\u2225\u2225\u2225 1N N\u2211 i=1 \u03c6(xi)\u2212 1 M M\u2211 j=1 \u03c6(yj) \u2225\u2225\u2225\u2225\u2225\u2225 2\n(1)\n= 1\nN2 N\u2211 i=1 N\u2211 i\u2032=1 \u03c6(xi) >\u03c6(xi\u2032)\u2212 2 NM N\u2211 i=1 M\u2211 j=1 \u03c6(xi) >\u03c6(yj)\n+ 1\nM2 M\u2211 j=1 M\u2211 j\u2032=1 \u03c6(yj) >\u03c6(yj\u2032) (2)\nTaking \u03c6 to be the identity function leads to matching the sample mean, and other choices of \u03c6 can be used to match higher order moments.\nWritten in this form, each term in Equation (2) only involves inner products between the \u03c6 vectors, and therefore the kernel trick can be applied.\nLMMD2 = 1\nN2 N\u2211 i=1 N\u2211 i\u2032=1 k(xi, xi\u2032)\u2212 2 NM N\u2211 i=1 M\u2211 j=1 k(xi, yj)\n+ 1\nM2 M\u2211 j=1 M\u2211 j\u2032=1 k(yj , yj\u2032) (3)\nThe kernel trick implicitly lifts the sample vectors into an infinite dimensional feature space. When this feature space corresponds to a universal reproducing kernel Hilbert space, it is shown that asymptotically, MMD is 0 if and only if PX = PY (Gretton et al., 2007; 2012a).\nFor universal kernels like the Gaussian kernel, defined as k(x, x\u2032) = exp(\u2212 12\u03c3 |x \u2212 x\n\u2032|2), where \u03c3 is the bandwidth parameter, we can use a Taylor expansion to get an explicit feature map \u03c6 that contains an infinite number of terms and covers all orders of statistics. Minimizing MMD under this feature expansion is then equivalent to minimizing a distance between all moments of the two distributions."}, {"heading": "3. Related Work", "text": "In this work we focus on generative models due to their ability to capture the salient properties and structure of data. Deep generative models are particularly appealing because they are capable of learning a latent manifold on which the data has high density. Learning this manifold allows smooth variations in the latent space to result in non-trivial transformations in the original space, effectively traversing between high density modes through low density areas (Bengio et al., 2013a). They are also capable of disentangling factors of variation, which means that each latent variable can become responsible for modelling a single, complex transformation in the original space that would otherwise involve many variables (Bengio et al., 2013a). Even if we restrict ourselves to the field of deep learning, there are a vast array of approaches to generative modelling. Below, we outline some of these methods.\nOne popular class of generative models used in deep learning are undirected graphical models, such as Boltzmann machines (Ackley et al., 1985), restricted Boltzmann machines (Hinton, 2002), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009). These models are normalized by a typically intractable partition function, making training, evaluation, and sampling extremely difficult, usually requiring expensive Markov-chain Monte Carlo (MCMC) procedures.\nNext there is the class of fully visible directed models such as fully visible sigmoid belief networks (Neal, 1992) and the neural autoregressive distribution estimator (Larochelle & Murray, 2011). These admit efficient log-likelihood calculation, gradient-based learning and efficient sampling, but require that an ordering be imposed on the observable variables, which can be unnatural for domains such as images and cannot take advantage of parallel computing methods due to their sequential nature.\nMore related to our own work, there is a line of research devoted to recovering density models from auto-encoder networks using MCMC procedures (Rifai et al., 2012; Bengio\net al., 2013b; 2014). These attempt to use contraction operators, or denoising criteria in order to generate a Markov chain by repeated perturbations during the encoding phase, followed by decoding.\nAlso related to our own work, there is the class of deep, variational networks (Rezende et al., 2014; Kingma & Welling, 2014; Mnih & Gregor, 2014). These are also deep, directed generative models, however they make use of an additional neural network that is designed to approximate the posterior over the latent variables. Training is carried out via a variational lower bound on the log-likelihood of the model distribution. These models are trained using stochastic gradient descent, however they either require that the latent representation is continuous (Kingma & Welling, 2014), or require many secondary networks to sufficiently reduce the variance of gradient estimates in order to produce a sufficiently good learning signal (Mnih & Gregor, 2014).\nFinally there is some early work that proposed the idea of using feed-forward neural networks to learn generative models. MacKay (1995) proposed a model that is closely related to ours, which also used a feed-forward network to map the prior samples to the data space. However, instead of directly outputing samples, an extra distribution is associated with the output. Sampling was used extensively for learning and inference in this model. Magdon-Ismail & Atiya (1998) proposed to use a neural network to learn a transformation from the data space to another space where the transformed data points are uniformly distributed. This transformation network then learns the cumulative density function."}, {"heading": "4. Generative Moment Matching Networks", "text": ""}, {"heading": "4.1. Data Space Networks", "text": "The high-level idea of the GMMN is to use a neural network to learn a deterministic mapping from samples of a simple, easy to sample distribution, to samples from the data distribution. The architecture of the generative network is exactly the same as a generative adversarial network (Goodfellow et al., 2014). However, we propose to train the network by simply minimizing the MMD criterion, avoiding the hard minimax objective function used in generative adversarial network training.\nMore specifically, in the generative network we have a stochastic hidden layer h \u2208 RH with H hidden units at the top with a prior uniform distribution on each unit independently,\np(h) = H\u220f j=1 U(hj) (4)\nHere U(h) = 12I[\u22121 \u2264 h \u2264 1] is a uniform distribu-\ntion in [\u22121, 1], where I[.] is an indicator function. Other choices for the prior are also possible, as long as it is a simple enough distribution from which we can easily draw samples.\nThe h vector is then passed through the neural network and deterministically mapped to a vector x \u2208 RD in the D dimensional data space.\nx = f(h;w) (5)\nf is the neural network mapping function, which can contain multiple layers of nonlinearities, and w represents the parameters of the neural network. One example architecture for f is illustrated in Figure 1(a), which has 3 intermediate ReLU (Nair & Hinton, 2010) nonlinear layers and one logistic sigmoid output layer.\nThe prior p(h) and the mapping f(h;w) jointly defines a distribution p(x) in the data space. To generate a sample x \u223c p(x) we only need to sample from the uniform prior p(h) and then pass the sample h through the neural net to get x = f(h;w).\nGoodfellow et al. (2014) proposed to train this network by using an extra discriminative network, which tries to distinguish between model samples and data samples. The generative network is then trained to counteract this in order to make the samples indistinguishable to the discriminative network. The gradient of this objective can be backpropagated through the generative network. However, because of the minimax nature of the formulation, it is easy to get stuck at a local optima. So the training of generative network and the discriminative network must be interleaved and carefully scheduled. By contrast, our learning algorithm simply involves minimizing the MMD objective.\nAssume we have a dataset of training examples xd1, ...,x d N (d for data), and a set of samples generated from our model\nxs1, ...,x s M (s for samples). The MMD objective LMMD2 is differentiable when the kernel is differentiable. For example for Gaussian kernels k(x,y) = exp ( \u2212 12\u03c3 ||x\u2212 y|| 2 ) , the gradient of xsip has a simple form\n\u2202LMMD2 \u2202xsip = 2 M2 M\u2211 j=1 1 \u03c3 k(xsi ,x s j)(x s jp \u2212 xsip)\n\u2212 2 MN N\u2211 j=1 1 \u03c3 k(xsi ,x d j )(x d jp \u2212 xsip) (6)\nThis gradient can then be backpropagated through the generative network to update the parameters w."}, {"heading": "4.2. Auto-Encoder Code Space Networks", "text": "Real-world data can be complicated and high-dimensional, which is one reason why generative modelling is such a difficult task. Auto-encoders, on the other hand, are designed to solve an arguably simpler task of reconstruction. If trained properly, auto-encoder models can be very good at representing data in a code space that captures enough statistical information that the data can be reliably reconstructed.\nThe code space of an auto-encoder has several advantages for creating a generative model. The first is that the dimensionality can be explicitly controlled. Visual data, for example, while represented in a high dimension often exists on a low-dimensional manifold. This is beneficial for a statistical estimator like MMD because the amount of data required to produce a reliable estimator grows with the dimensionality of the data (Ramdas et al., 2015). The second advantage is that each dimension of the code space can end up representing complex variations in the original data space. This concept is referred to in the literature as disentangling factors of variation (Bengio et al., 2013a).\nFor these reasons, we propose to bootstrap auto-encoder models with a GMMN to create what we refer to as the GMMN+AE model. These operate by first learning an auto-encoder and producing code representations of the data, then freezing the auto-encoder weights and learning a GMMN to minimize MMD between generated codes and data codes. A visualization of this model is given in Figure 1(b).\nOur method for training a GMMN+AE proceeds as follows:\n1. Greedy layer-wise pretraining of the auto-encoder (Bengio et al., 2007).\n2. Fine-tune the auto-encoder.\n3. Train a GMMN to model the code layer distribution using an MMD objective on the final encoding layer.\nWe found that adding dropout to the encoding layers can be beneficial in terms of creating a smooth manifold in code space. This is analogous to the motivation behind contractive and denoising auto-encoders (Rifai et al., 2011; Vincent et al., 2008)."}, {"heading": "4.3. Practical Considerations", "text": "Here we outline some design choices that we have found to improve the peformance of GMMNs.\nBandwidth Parameter. The bandwidth parameter in the kernel plays a crucial role in determining the statistical efficiency of MMD, and optimally setting it is an open problem. A good heuristic is to perform a line search to obtain the bandwidth that produces the maximal distance (Sriperumbudur et al., 2009), other more advanced heuristics are also available (Gretton et al., 2012b). As a simpler approximation, for most of our experiments we use a mixture of K kernels spanning multiple ranges. That is, we choose the kernel to be:\nk(x, x\u2032) = K\u2211 q=1 k\u03c3q (x, x \u2032) (7)\nwhere k\u03c3q is a Gaussian kernel with bandwidth parameter \u03c3q . We found that choosing simple values for these such as 1, 5, 10, etc. and using a mixture of 5 or more was sufficient to obtain good results. The weighting of different kernels can be further tuned to achieve better results, but we kept them equally weighted for simplicity.\nSquare Root Loss. In practice, we have found that better results can be obtained by optimizing LMMD = \u221a LMMD2 . This loss can be important for driving the difference between the two distributions as close to 0 as possible. Compared to LMMD2 which flattens out when its value gets close to 0, LMMD behaves much better for small LMMD values. Alternatively, this can be understood by writing down the gradient of LMMD with respect to w\n\u2202LMMD \u2202w = 1 2 \u221a LMMD2 \u2202LMMD2 \u2202w\n(8)\nThe 1/(2 \u221a LMMD2) term automatically adapts the effective learning rate. This is especially beneficial when both LMMD2 and \u2202LMMD2 \u2202w become small, where this extra factor can help by maintaining larger gradients.\nMinibatch Training. One of the issues with MMD is that the usage of kernels means that the computation of the objective scales quadratically with the amount of data. In the literature there have been several alternative estimators designed to overcome this (Gretton et al., 2012a). In our case, we found that it was sufficient to optimize MMD using minibatch optimization. In each weight update, a small subset of data is chosen, and an equal number of samples\nAlgorithm 1: GMMN minibatch training Input : Dataset {xd1, ...,xdN}, prior p(h), network\nf(h;w) with initial parameter w(0)\nOutput: Learned parameter w\u2217\n1 while Stopping criterion not met do 2 Get a minibatch of data Xd \u2190 {xdi1 , ...,x d ib } 3 Get a new set of samples Xs \u2190 {xs1, ...,xsb} 4 Compute gradient \u2202LMMD\u2202w on X d and Xs 5 Take a gradient step to update w 6 end\nare drawn from the GMMN. Within a minibatch, MMD is applied as usual. As we are using exact samples from the model and the data distribution, the minibatch MMD is still a good estimator of the population MMD. We found this approach to be both fast and effective. The minibatch training algorithm for GMMN is shown in Algorithm 1."}, {"heading": "5. Experiments", "text": "We trained GMMNs on two benchmark datasets MNIST (LeCun et al., 1998) and the Toronto Face Dataset (TFD) (Susskind et al., 2010). For MNIST, we used the standard test set of 10,000 images, and split out 5000 from the standard 60,000 training images for validation. The remaining 55,000 were used for training. For TFD, we used the same training and test sets and fold splits as used by (Goodfellow et al., 2014), but split out a small set of the training data and used it as the validation set. For both datasets, rescaling the images to have pixel intensities between 0 and 1 is the only preprocessing step we did.\nOn both datasets, we trained the GMMN network in both the input data space and the code space of an auto-encoder. For all the networks we used in this section, a uniform distribution in [\u22121, 1]H was used as the prior for the H-dimensional stochastic hidden layer at the top of the GMMN, which was followed by 4 ReLU layers, and the output was a layer of logistic sigmoid units. The autoencoder we used for MNIST had 4 layers, 2 for the encoder and 2 for the decoder. For TFD the auto-encoder had 6 layers in total, 3 for the encoder and 3 for the decoder. For both auto-encoders the encoder and the decoder had mirrored architectures. All layers in the auto-encoder network used sigmoid nonlinearities, which also guaranteed that the code space dimensions lay in [0, 1], so that they could match the GMMN outputs. The network architectures for MNIST are shown in Figure 1.\nThe auto-encoders were trained separately from the GMMN. Cross entropy was used as the reconstruction loss. We first did standard layer-wise pretraining, then fine-tuned all layers jointly. Dropout (Hinton et al., 2012b) was used\non the encoder layers. After training the auto-encoder, we fixed it and passed the input data through the encoder to get the corresponding codes. The GMMN network was then trained in this code space to match the statistics of generated codes to the statistics of codes from data examples. When generating samples, the generated codes were passed through the decoder to get samples in the input data space.\nFor all experiments in this section the GMMN networks were trained with minibatches of size 1000, for each minibatch we generated a set of 1000 samples from the network. The loss and gradient were computed from these 2000 points. We used the square root loss function LMMD throughout.\nEvaluation of our model is not straight-forward, as we do not have an explicit form for the probability density function, it is not easy to compute the log-likelihood of data. However, sampling from our model is easy. We therefore followed the same evaluation protocol used in related models (Bengio et al., 2013a), (Bengio et al., 2014), and (Goodfellow et al., 2014). A Gaussian Parzen window (kernel density estimator) was fit to 10,000 samples generated from the model. The likelihood of the test data was then computed under this distribution. The scale parameter of the Gaussians was selected using a grid search in a fixed range using the validation set.\nThe hyperparameters of the networks, including the learning rate and momentum for both auto-encoder and GMMN training, dropout rate for the auto-encoder, and number of hidden units on each layer of both auto-encoder and GMMN, were tuned using Bayesian optimization (Snoek et al., 2012; 2014)1 to optimize the validation set likelihood under the Gaussian Parzen window density estimation.\nThe log-likelihood of the test set for both datasets are shown in Table 1. The GMMN is competitive with other approaches, while the GMMN+AE significantly outper-\n1We used the service provided by https://www. whetlab.com\nforms the other models. This shows that despite being relatively simple, MMD, especially when combined with an effective decoder, is a powerful objective for training good generative models.\nSome samples generated from the GMMN models are shown in Figure 2(a-d). The GMMN+AE produces the most visually appealing samples, which are reflected in its Parzen window log-likelihood estimates. The likely explanation is that any perturbations in the code space correspond to smooth transformations along the manifold of the data space. In that sense, the decoder is able to \u201ccorrect\u201d noise in the code space.\nTo determine whether the models learned to merely copy the data, we follow the example of (Goodfellow et al., 2014) and visualize the nearest neighbour of several samples in terms of Euclidean pixel-wise distance in Figure 2(e-h). By this metric, it appears as though the samples are not merely data examples.\nOne of the interesting aspects of a deep generative model such as the GMMN is that it is possible to directly explore the data manifold. Using the GMMN+AE model, we randomly sampled 5 points in the uniform space and show their corresponding data space projections in Figure 3. These points are highlighted by red boxes. From left to right, top to bottom we linearly interpolate between these\npoints in the uniform space and show their corresponding projections in data space. The manifold is smooth for the most part, and almost all of the projections correspond to realistic looking data. For TFD in particular, these transformations involve complex attributes, such as the changing of pose, expression, lighting, gender, and facial hair."}, {"heading": "6. Conclusion and Future Work", "text": "In this paper we provide a simple and effective framework for training deep generative models called generative moment matching networks. Our approach is based off of optimizing maximum mean discrepancy so that samples generated from the model are indistinguishable from data examples in terms of their moment statistics. As is standard with MMD, the use of the kernel trick allows a GMMN to avoid explicitly computing these moments, resulting in a simple training objective, and the use of minibatch stochastic gradient descent allows the training to scale to large datasets.\nOur second contribution combines MMD with autoencoders for learning a generative model of the code layer. The code samples from the model can then be fed through the decoder in order to generate samples in the original space. The use of auto-encoders makes the generative model learning a much simpler problem. Combined with MMD, pretrained auto-encoders can be read-\nily bootstrapped into a good generative model of data. On the MNIST and Toronto Face Database, the GMMN+AE model achieves superior performance compared to other approaches. For these datasets, we demonstrate that the GMMN+AE is able to discover the implicit manifold of the data.\nThere are many interesting directions for research using MMD. One such extension is to consider alternatives to the standard MMD criterion in order to speed up training. One such possibility is the class of linear-time estimators that has been developed recently in the literature (Gretton et al.,\n2012a).\nAnother possibility is to utilize random features (Rahimi & Recht, 2007). These are randomized feature expansions whose inner product converges to a kernel function with an increasing number of features. This idea was recently explored for MMD in (Zhao & Meng, 2014). The advantage of this approach would be that the cost would no longer grow quadratically with minibatch size because we could use the original objective given in Equation 2. Another advantage of this approach is that the data statistics could be pre-computed from the entire dataset, which would reduce the variance of the objective gradients.\nAnother direction we would like to explore is joint training of the auto-encoder model with the GMMN. Currently, these are treated separately, but joint training may encourage the learning of codes that are both suitable for reconstruction as well as generation.\nWhile a GMMN provides an easy way to sample data, the posterior distribution over the latent variables is not readily available. It would be interesting to explore ways in which to infer the posterior distribution over the latent space. A straightforward way to do this is to learn a neural network to predict the latent vector given a sample. This is reminiscent of the recognition models used in the wake-sleep algorithm (Hinton et al., 1995), or variational auto-encoders (Kingma & Welling, 2014).\nAn interesting application of MMD that is not directly related to generative modelling comes from recent work on learning fair representations (Zemel et al., 2013). There, the objective is to train a prediction method that is invariant to a particular sensitive attribute of the data. Their solution is to learn an intermediate clustering-based representation. MMD could instead be applied to learn a more powerful, distributed representation such that the statistics of the representation do not change conditioned on the sensitive variable. This idea can be further generalized to learn representations invariant to known biases.\nFinally, the notion of utilizing an auto-encoder with the GMMN+AE model provides new avenues for creating generative models of even more complex datasets. For example, it may be possible to use a GMMN+AE with convolutional auto-encoders (Zeiler et al., 2010; Masci et al., 2011; Makhzani & Frey, 2014) in order to create generative models of high resolution color images."}, {"heading": "Acknowledgements", "text": "We thank David Warde-Farley for helpful clarifications regarding (Goodfellow et al., 2014), and Charlie Tang for providing relevant references. We thank CIFAR, NSERC, and Google for research funding."}], "references": [{"title": "A learning algorithm for boltzmann machines", "author": ["D.H. Ackley", "G.E. Hinton", "T.J. Sejnowski"], "venue": "Cognitive science,", "citeRegEx": "Ackley et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Ackley et al\\.", "year": 1985}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H Larochelle"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Better mixing via deep representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Thibodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "arXiv preprint arXiv:1411.4952,", "citeRegEx": "Fang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves and Jaitly,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly", "year": 2014}, {"title": "A kernel method for the two-sampleproblem", "author": ["A. Gretton", "K.M. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gretton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2007}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "The \u201cwake-sleep\u201d algorithm for unsupervised neural networks", "author": ["G.E. Hinton", "P. Dayan", "B.J. Frey", "R.M. Neal"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "In roceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Larochelle and Murray,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers", "author": ["D.J. MacKay"], "venue": "Detectors and Associated Equipment,", "citeRegEx": "MacKay,? \\Q1995\\E", "shortCiteRegEx": "MacKay", "year": 1995}, {"title": "Neural networks for density estimation", "author": ["M. Magdon-Ismail", "A. Atiya"], "venue": "In NIPS, pp", "citeRegEx": "Magdon.Ismail and Atiya,? \\Q1998\\E", "shortCiteRegEx": "Magdon.Ismail and Atiya", "year": 1998}, {"title": "A winner-take-all method for training sparse convolutional autoencoders", "author": ["A. Makhzani", "B. Frey"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "Makhzani and Frey,? \\Q2014\\E", "shortCiteRegEx": "Makhzani and Frey", "year": 2014}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Connectionist learning of belief networks", "author": ["R.M. Neal"], "venue": "Artificial intelligence,", "citeRegEx": "Neal,? \\Q1992\\E", "shortCiteRegEx": "Neal", "year": 1992}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi and Recht,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions", "author": ["A. Ramdas", "S.J. Reddi", "B. Poczos", "A. Singh", "L. Wasserman"], "venue": "In The Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15),", "citeRegEx": "Ramdas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ramdas et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Input warping for bayesian optimization of non-stationary functions", "author": ["J. Snoek", "K. Swersky", "R.S. Zemel", "R.P. Adams"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Snoek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2014}, {"title": "Kernel choice and classifiability for rkhs embeddings of probability distributions", "author": ["B.K. Sriperumbudur", "K. Fukumizu", "A. Gretton", "G.R. Lanckriet", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2009}, {"title": "The toronto face dataset", "author": ["J. Susskind", "A. Anderson", "G.E. Hinton"], "venue": "Technical report,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning fair representations", "author": ["R. Zemel", "Y. Wu", "K. Swersky", "T. Pitassi", "C. Dwork"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Zemel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zemel et al\\.", "year": 2013}, {"title": "Fastmmd: Ensemble of circular discrepancy for efficient two-sample test", "author": ["J. Zhao", "D. Meng"], "venue": "arXiv preprint arXiv:1405.2664,", "citeRegEx": "Zhao and Meng,? \\Q2014\\E", "shortCiteRegEx": "Zhao and Meng", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014).", "startOffset": 182, "endOffset": 207}, {"referenceID": 16, "context": "Models such as convolutional neural networks (CNNs), and long short term memory (LSTM) networks are now achieving impressive results on a number of tasks such as object recognition (Krizhevsky et al., 2012; Sermanet et al., 2014; Szegedy et al., 2014), speech recognition (Graves & Jaitly, 2014; Hinton et al.", "startOffset": 181, "endOffset": 251}, {"referenceID": 32, "context": "Models such as convolutional neural networks (CNNs), and long short term memory (LSTM) networks are now achieving impressive results on a number of tasks such as object recognition (Krizhevsky et al., 2012; Sermanet et al., 2014; Szegedy et al., 2014), speech recognition (Graves & Jaitly, 2014; Hinton et al.", "startOffset": 181, "endOffset": 251}, {"referenceID": 5, "context": ", 2014), machine translation (Cho et al., 2014; Sutskever et al., 2014), and more.", "startOffset": 29, "endOffset": 71}, {"referenceID": 37, "context": ", 2014), machine translation (Cho et al., 2014; Sutskever et al., 2014), and more.", "startOffset": 29, "endOffset": 71}, {"referenceID": 0, "context": "Thus, with GMMNs it is easy to quickly draw independent random samples, as opposed to expensive MCMC procedures that are necessary in other models such as Boltzmann machines (Ackley et al., 1985; Hinton, 2002; Salakhutdinov & Hinton, 2009).", "startOffset": 174, "endOffset": 239}, {"referenceID": 11, "context": "Thus, with GMMNs it is easy to quickly draw independent random samples, as opposed to expensive MCMC procedures that are necessary in other models such as Boltzmann machines (Ackley et al., 1985; Hinton, 2002; Salakhutdinov & Hinton, 2009).", "startOffset": 174, "endOffset": 239}, {"referenceID": 7, "context": "The structure of a GMMN is most analogous to the recently proposed generative adversarial networks (GANs) (Goodfellow et al., 2014), however unlike GANs, whose training involves a difficult minimax optimization problem, GMMNs are comparatively simple; they are trained to minimize a straightforward loss function using backpropagation.", "startOffset": 106, "endOffset": 131}, {"referenceID": 9, "context": "The key idea behind GMMNs is the use of a statistical hypothesis testing framework called maximum mean discrepancy (Gretton et al., 2007).", "startOffset": 115, "endOffset": 137}, {"referenceID": 9, "context": "Maximum mean discrepancy is a frequentist estimator for answering this question, also known as the two sample test (Gretton et al., 2007; 2012a).", "startOffset": 115, "endOffset": 144}, {"referenceID": 9, "context": "When this feature space corresponds to a universal reproducing kernel Hilbert space, it is shown that asymptotically, MMD is 0 if and only if PX = PY (Gretton et al., 2007; 2012a).", "startOffset": 150, "endOffset": 179}, {"referenceID": 0, "context": "One popular class of generative models used in deep learning are undirected graphical models, such as Boltzmann machines (Ackley et al., 1985), restricted Boltzmann machines (Hinton, 2002), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009).", "startOffset": 121, "endOffset": 142}, {"referenceID": 11, "context": ", 1985), restricted Boltzmann machines (Hinton, 2002), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009).", "startOffset": 39, "endOffset": 53}, {"referenceID": 25, "context": "Next there is the class of fully visible directed models such as fully visible sigmoid belief networks (Neal, 1992) and the neural autoregressive distribution estimator (Larochelle & Murray, 2011).", "startOffset": 103, "endOffset": 115}, {"referenceID": 28, "context": "Also related to our own work, there is the class of deep, variational networks (Rezende et al., 2014; Kingma & Welling, 2014; Mnih & Gregor, 2014).", "startOffset": 79, "endOffset": 146}, {"referenceID": 19, "context": "MacKay (1995) proposed a model that is closely related to ours, which also used a feed-forward network to map the prior samples to the data space.", "startOffset": 0, "endOffset": 14}, {"referenceID": 19, "context": "MacKay (1995) proposed a model that is closely related to ours, which also used a feed-forward network to map the prior samples to the data space. However, instead of directly outputing samples, an extra distribution is associated with the output. Sampling was used extensively for learning and inference in this model. Magdon-Ismail & Atiya (1998) proposed to use a neural network to learn a transformation from the data space to another space where the transformed data points are uniformly distributed.", "startOffset": 0, "endOffset": 349}, {"referenceID": 7, "context": "The architecture of the generative network is exactly the same as a generative adversarial network (Goodfellow et al., 2014).", "startOffset": 99, "endOffset": 124}, {"referenceID": 27, "context": "This is beneficial for a statistical estimator like MMD because the amount of data required to produce a reliable estimator grows with the dimensionality of the data (Ramdas et al., 2015).", "startOffset": 166, "endOffset": 187}, {"referenceID": 1, "context": "Greedy layer-wise pretraining of the auto-encoder (Bengio et al., 2007).", "startOffset": 50, "endOffset": 71}, {"referenceID": 29, "context": "This is analogous to the motivation behind contractive and denoising auto-encoders (Rifai et al., 2011; Vincent et al., 2008).", "startOffset": 83, "endOffset": 125}, {"referenceID": 38, "context": "This is analogous to the motivation behind contractive and denoising auto-encoders (Rifai et al., 2011; Vincent et al., 2008).", "startOffset": 83, "endOffset": 125}, {"referenceID": 35, "context": "A good heuristic is to perform a line search to obtain the bandwidth that produces the maximal distance (Sriperumbudur et al., 2009), other more advanced heuristics are also available (Gretton et al.", "startOffset": 104, "endOffset": 132}, {"referenceID": 18, "context": "We trained GMMNs on two benchmark datasets MNIST (LeCun et al., 1998) and the Toronto Face Dataset (TFD) (Susskind et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 36, "context": ", 1998) and the Toronto Face Dataset (TFD) (Susskind et al., 2010).", "startOffset": 43, "endOffset": 66}, {"referenceID": 7, "context": "For TFD, we used the same training and test sets and fold splits as used by (Goodfellow et al., 2014), but split out a small set of the training data and used it as the validation set.", "startOffset": 76, "endOffset": 101}, {"referenceID": 4, "context": ", 2013a), Deep Generative Stochastic Network (Deep GSN) from (Bengio et al., 2014) and Adversarial nets (GANs) from (Goodfellow et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 7, "context": ", 2014) and Adversarial nets (GANs) from (Goodfellow et al., 2014).", "startOffset": 41, "endOffset": 66}, {"referenceID": 4, "context": ", 2013a), (Bengio et al., 2014), and (Goodfellow et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": ", 2014), and (Goodfellow et al., 2014).", "startOffset": 13, "endOffset": 38}, {"referenceID": 33, "context": "The hyperparameters of the networks, including the learning rate and momentum for both auto-encoder and GMMN training, dropout rate for the auto-encoder, and number of hidden units on each layer of both auto-encoder and GMMN, were tuned using Bayesian optimization (Snoek et al., 2012; 2014)1 to optimize the validation set likelihood under the Gaussian Parzen window density estimation.", "startOffset": 265, "endOffset": 291}, {"referenceID": 7, "context": "To determine whether the models learned to merely copy the data, we follow the example of (Goodfellow et al., 2014) and visualize the nearest neighbour of several samples in terms of Euclidean pixel-wise distance in Figure 2(e-h).", "startOffset": 90, "endOffset": 115}, {"referenceID": 12, "context": "This is reminiscent of the recognition models used in the wake-sleep algorithm (Hinton et al., 1995), or variational auto-encoders (Kingma & Welling, 2014).", "startOffset": 79, "endOffset": 100}, {"referenceID": 40, "context": "An interesting application of MMD that is not directly related to generative modelling comes from recent work on learning fair representations (Zemel et al., 2013).", "startOffset": 143, "endOffset": 163}, {"referenceID": 22, "context": "For example, it may be possible to use a GMMN+AE with convolutional auto-encoders (Zeiler et al., 2010; Masci et al., 2011; Makhzani & Frey, 2014) in order to create generative models of high resolution color images.", "startOffset": 82, "endOffset": 146}, {"referenceID": 7, "context": "We thank David Warde-Farley for helpful clarifications regarding (Goodfellow et al., 2014), and Charlie Tang for providing relevant references.", "startOffset": 65, "endOffset": 90}], "year": 2015, "abstractText": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.", "creator": "LaTeX with hyperref package"}}}