{"id": "1502.05375", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2015", "title": "On learning k-parities with and without noise", "abstract": "We 2000 consider while example particular learning $ subset $ - devaluate in. brought - route 'm - being introducing: result taking onto invariant $ 98 \\ in \\ {2-0, 1 \\} ^ n $ took $ | 2 | = formula_2 $ and man encoded following \" comments \" $ 9,946, a_2, .. . \\ entered \\ {1-0, 2002 \\} ^ n $, central into algorithm them statements intended separate mean with $ & comm; a_i, (& gt; \\ pmod 26 $, perhaps entire an make misjudgment although since ten similar how did saw the algorithm their based same structural? We establishing long 2002 world already both Buhrman m\u00e9moires salah. began any $ \\ trv (\u03b1) $ term also full as parameters.", "histories": [["v1", "Wed, 18 Feb 2015 20:36:19 GMT  (16kb)", "http://arxiv.org/abs/1502.05375v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.DM cs.LG", "authors": ["arnab bhattacharyya", "ameet gadekar", "ninad rajgopal"], "accepted": false, "id": "1502.05375"}, "pdf": {"name": "1502.05375.pdf", "metadata": {"source": "CRF", "title": "On learning k-parities with and without noise", "authors": ["Arnab Bhattacharyya", "Ameet Gadekar", "Ninad Rajgopal"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n05 37\n5v 1\n[ cs\n.D S]\n1 8\nFe b\nWe first consider the problem of learning k-parities in the on-line mistake-bound model: given a hidden vector x \u2208 {0, 1}n with |x| = k and a sequence of \u201cquestions\u201d a1, a2, \u00b7 \u00b7 \u00b7 \u2208 {0, 1}\nn, where the algorithm must reply to each question with \u3008ai, x\u3009 (mod 2), what is the best tradeoff between the number of mistakes made by the algorithm and its time complexity? We improve the previous best result of Buhrman et. al. [BGM10] by an exp(k) factor in the time complexity.\nSecond, we consider the problem of learning k-parities in the presence of classification noise of rate \u03b7 \u2208 (0, 1/2). A polynomial time algorithm for this problem (when \u03b7 > 0 and k = \u03c9(1)) is a longstanding challenge in learning theory. Grigorescu et al. [GRV11] showed an algorithm running in time (\nn k/2\n)1+4\u03b72+o(1) . Note that this algorithm inherently requires time (\nn k/2\n)\neven when the noise rate \u03b7 is poly-\nnomially small. We observe that for sufficiently small noise rate, it is possible to break the ( n k/2 ) barrier. In particular, if for some function f(n) = \u03c9(1) and \u03b1 \u2208 [1/2, 1), k = n/f(n) and \u03b7 = o(f(n)\u2212\u03b1/ log n), then there is an algorithm for the problem with running time poly(n) \u00b7 (\nn k )1\u2212\u03b1 \u00b7 e\u2212k/4.01."}, {"heading": "1 Introduction", "text": "By now, the \u201cParity Problem\u201d of Blum, Kalai and Wasserman [BKW03] has acquired widespread notoriety. The question is simple enough to be in our second sentence: in order to learn a hidden vector x \u2208 {0, 1}n, what is the least number of random examples (a, \u2113) that need to be seen, where a is uniformly chosen from {0, 1}n and \u2113 = \u2211\ni aixi (mod 2) with probability at least 1 \u2212 \u03b7? Information-theoretically, x can be recovered only after O(n) examples, even if the noise rate \u03b7 is close to 1/2. But if we add the additional constraint that the running time of the learning algorithm be minimized, the barely subexponential running time of [BKW03]\u2019s algorithm, 2O(n/ log n), still holds the record of being the fastest known for this problem!\nLearning parities with noise is a central problem in theoretical computer science. It has incarnations in several different areas of computer science, including coding theory as the problem of learning random binary linear codes and cryptography as the \u201clearning with errors\u201d problem that underlies lattice-based cryptosystems [Reg09, BV11]. In learning theory, the special case of the problem where the hidden vector x is known to be supported on a set of size k much smaller than n has great relevance. We refer to this problem as learning k-parity with noise or k-LPN. Feldman et al. [FGKP09] showed that learning k-juntas, as well as learning 2k-term DNFs from uniformly random examples and variants of these problems in which the noise is adversarial instead of random, all reduce to the k-LPN problem. For the k-LPN problem, the current record is that of Grigorescu, Reyzin and Vempala [GRV11] who showed a learning algorithm that succeeds with constant probability, takes (\nn k/2 )1+(2\u03b7)2+o(1) time and uses k logn(1\u22122\u03b7)2 \u00b7 \u03c9(1) samples. When the\n\u2217Supported in part by DST Ramanujan Fellowship. Email: arnabb@csa.iisc.ernet.in \u2020Email: ameet.gadekar@csa.iisc.ernet.in \u2021Email: ninad.rajgopal@csa.iisc.ernet.in\nnoise rate \u03b7 is close to 1/2, this running time is improved by an algorithm due to G. Valiant [Val12] that runs in time n0.8k \u00b7 poly( 11\u22122\u03b7 ). It is a wide open challenge to find a polynomial time algorithm for k-LPN for growing k or to prove a negative result.\nAnother outstanding challenge in machine learning is the problem of learning parities without noise in the \u201cattribute-efficient\u201d setting [Blu96]. The algorithm is given access to a source of examples (a, \u2113) where a is chosen uniformly from {0, 1}n and \u2113 = \u2211\ni aixi (mod 2) with no noise, and the question is to learn x while simultaneously reducing the time complexity and the number of examples drawn by the algorithm. Again, we focus on the case where x has sparsity k \u226a n. Information-theoretically, of course, O(k logn) examples should be sufficient, as each linearly independent example reduces the number of consistent k-parities by a factor of 2. But the fastest known algorithm making O(k logn) samples runs in time O\u0303( (\nn k/2\n)\n) [KS06], and it\nis open whether there exists a polynomial time algorithm for learning parities that is attribute-efficient, i.e. it makes poly(k logn) samples. Buhrman, Garc\u0301\u0131a-Soriano and Matsliah [BGM10] give the current best tradeoffs between the sample complexity and running time for learning parities in this noiseless setting. Notice that with O(n) samples, it is easy to learn the k-parity in polynomial time using Gaussian elimination."}, {"heading": "1.1 Our Results", "text": "We first study the noiseless setting. Our main technical result is an improved tradeoff between the sample complexity and runtime for learning parities.\nTheorem 1.1. Let k, t : N \u2192 N be two functions1 satisfying log logn \u226a k(n) \u226a t(n) \u226a n. For any \u03b4 > 0, there is an algorithm that learns the concept class of k-parities on n variables with confidence parameter \u03b4, using O(kn/t + log (\nt k\n) + log(1/\u03b4)) uniformly random examples and e\u2212k/4.01 ( t k )\n\u00b7 poly(n) \u00b7 log(1/\u03b4) running time2.\nActually, we prove our result in the mistake-bound model [Lit89] that is stronger than the PAC model discussed above (in fact, strictly stronger assuming the existence of one-way functions [Blu94]). As a consequence, a theorem of the above form also holds when the examples come from an arbitrary distribution. We defer the statement of the result for the mistake-bound model to Section 3.\nFor comparison, let us quote the closely related result of Buhrman et. al.:\nTheorem 1.2 (Theorem 2.1 of [BGM10]). Let k, t : N \u2192 N be two functions satisfying k(n) 6 t(n) 6 n. For any \u03b4 > 0, there is an algorithm that learns the concept class of k-parities on n variables with confidence parameter \u03b4, using O(kn/t+log (\nt k\n) +log(1/\u03b4)) uniformly random examples and ( t k )\n\u00b7poly(n)\u00b7log(1/\u03b4) running time.\nThus, in the comparable regime, our Theorem 1.1 improves the runtime complexity of Theorem 3.2 by an exp(k) factor while its sample complexity remains the same upto constant factors. Note that as t approaches k, our algorithm makes O(n) samples and takes poly(n) time which is the complexity of the Gaussian elimination approach. On the other hand, if t = n/ log(n/k), our algorithm makes O(k log(n/k)) samples and takes3 exp(\u2212k) \u00b7 (\nn/k k\n)\ntime (ignoring polynomial factors), compared to the trivial approach which explicitly keeps track of the subset of all the k-weight parities consistent with examples given so far and which makes O(k log(n/k)) samples and takes O( (\nn k\n)\n) time. We next examine the noisy setting. Here, our contribution is a simple, general observation that does not\nseem to have been explicitly made before.\nTheorem 1.3. Given an algorithm A that learns PAR(k) over the uniform distribution with confidence parameter \u03b4 using s(\u03b4) samples and running time t(\u03b4),there is an algorithm A\u2032 that solves the k-LPN problem with noise rate \u03b7 \u2208 (0, 1/3), using O(s(\u03b4/2) log(1/\u03b4)) examples and running time exp(O(H(3\u03b7/2) \u00b7 s(\u03b4/2) \u00b7 log(1/\u03b4)))) \u00b7 (t(\u03b4/2) + s(\u03b4/2) log(1/\u03b4)) and with confidence parameter \u03b4.\n1We assume throughout, as in [BGM10], that k and t are constructible in quadratic time. 2The \u201c4.01\u201d can be replaced by any constant more than 4. 3By exp(\u00b7), we mean 2O(\u00b7).\nIn the above, H : [0, 1] \u2192 [0, 1] denotes the binary entropy function H(p) = p log2 1 p + (1 \u2212 p) log2 1 1\u2212p . The main conceptual message carried by Theorem 1.3 is that improving the sample complexity for efficient learning of noiseless parity improves the running time for learning of noisy parity. For instance, if we use Spielman\u2019s algorithm as A, reported in [KS06], that learns k-parity using O(k logn) samples and O( (\nn k/2\n)\n)\nrunning time, we immediately get the following:\nCorollary 1.4. For any \u03b7 \u2208 (0, 1/3) and constant confidence parameter, there is an algorithm for k-LPN with sample complexity O(k log n) and running time (\nn k/2 )1+O(H(1.5\u03b7)) .\nFor comparison, the current best result of [GRV11] has runtime ( n k/2\n)1+4\u03b72+o(1) and sample complex-\nity \u03c9(k logn). In the regime under consideration, our algorithm\u2019s runtime has a worse exponent but an asymptotically better sample complexity.\nThe result of [GRV11] requires ( n k/2 ) time regardless of how small \u03b7 is. We show via Theorem 1.3 and\nTheorem 1.1 that it is possible to break the ( n k/2 ) barrier when \u03b7 is a small enough function of n.\nCorollary 1.5. Suppose k(n) = n/f(n) for some function f : N \u2192 N for which f(n) \u226a n/ log logn, and suppose \u03b7(n) = o( 1((f(n))\u03b1 logn) ) for some \u03b1 \u2208 [1/2, 1). Then, for constant confidence parameter, there exists an algorithm for k-LPN with noise rate \u03b7 with running time e\u2212k/4.01+o(k) \u00b7 (\nn k )1\u2212\u03b1 \u00b7 poly(n) and sample\ncomplexity O(k(f(n))\u03b1).\nWe note that because of the results of Feldman et. al. [FGKP09], the above results for k-LPN also extend to the setting where the example source adversarially mislabels examples instead of randomly but with the same rate \u03b7."}, {"heading": "1.2 Our Techniques", "text": "We first give an algorithm to learn parities in the noiseless setting in the mistake bound model. We use the same approach as that of [BGM10] (which was itself inspired by [APY09]). The idea is to consider a family S of subsets of {0, 1}n such that the hidden k-sparse vector is contained inside one of the elements of S. We maintain this invariant throughout the algorithm. Now, each time an example comes, it specifies a halfspace H of {0, 1}n inside which the hidden vector is lying. So, we can update S by taking the intersection of each of its elements with H . If we can ensure that the set of points covered by the elements of S is decreasing by a constant factor at every round, then after O(log \u2211\nS\u2208S |S|) examples, the hidden vector is learned. The runtime is determined by the number of sets in S times the cost of taking the intersection of each set with a halfspace.\nOne can think of the argument of Buhrman et al. [BGM10] as essentially initializing S to be the set of all (\nn k\n)\nsubspaces spanned by k standard basis vectors. The intersections of these subspaces with a halfspace can be computed efficiently by Gaussian elimination. Our idea is to reduce the number of sets in S. Note that we can afford to make the size of each set in S larger by some factor C because this only increases the sample complexity by an additive logC. Our approach is (essentially) to take S to be a random collection of subspaces spanned by \u03b1k standard basis vectors, where \u03b1 > 1 is a sufficiently large constant. We show that it is sufficient for the size of S to be smaller than (\nn/\u03b1 k\n)\nby a factor that is exponential in k, so that the running time is also improved by the same factor. Moreover, the sample complexity increases by only a lower-order additive term.\nOur second main contribution is a reduction from noiseless parity learning to noisy parity learning. The algorithm is a simple exhaustive search which guesses the location of the mislabelings, corrects those labels, applies the learner for noiseless parity and then verifies whether the output hypothesis matches the examples by drawing a few more samples. Surprisingly, this seemingly immediate algorithm allows us to devise the first algorithm which has a better running time than (\nn k/2\n)\nin the presence of a non-trivial amount of noise.\nThe lesson seems to be that if we hope to beat ( n k/2 ) for constant noise rates, we should first address the open question of Blum [Blu96] of devising an attribute-efficient algorithm to learn parity without noise."}, {"heading": "2 Preliminaries", "text": "Let PAR(k) be the class of all f \u2208 {0, 1}n of Hamming weight k. So, |PAR(k)| = ( n k )\n. With each vector f \u2208 PAR(k), we associate a parity function f : {0, 1}n \u2192 {0, 1} defined by f(a) =\n\u2211n i=1 xiai (mod 2).\nLet C be a concept class of Boolean functions on n variables, such as PAR(k). We discuss two models of learning in this work. One is Littlestone\u2019s online mistake bound model [Lit89]. Here, learning proceeds in a series of rounds, where in each round, the learner is given an unlabeled boolean example a \u2208 {0, 1}n and must predict the value f(a) of an unknown target function f \u2208 C. Once the learner predicts the value of f(a), the true value of f(a) is revealed to the learner by the teacher. The mistake bound of a learning algorithm is the worst-case number of mistakes that the algorithm makes over all sequences of examples and all possible target functions f \u2208 C.\nThe second model of learning we consider is Valiant\u2019s famous PAC model [Val84] of learning from random examples. Here, for an unknown target function f \u2208 C, the learner has access to a source of examples (a, f(a)) where a is chosen independently from a distribution D on {0, 1}n. A learning algorithm is said to PAC-learn C with sample complexity s, running time t, approximation parameter \u03b5 and confidence parameter \u03b4 if for all distributions D and all target functions f \u2208 C, the algorithm draws at most s samples from the example source, runs for time at most t and outputs a function f\u2217 such that, with a probability at least 1\u2212 \u03b4:\nPr a\u2190D\n[f(a) 6= f\u2217(a)] < \u03b5\nOften in this paper (e.g., all of the Introduction), we consider PAC-learning over the uniform distribution, in which case D is fixed to be uniform on {0, 1}n. Notice that for learning PAR(k) over the uniform distribution, we can take \u03b5 = 12 because any two distinct parities differ on half of {0, 1}\nn. There are standard conversion techniques which can be used to transform any mistake-bound algorithm\ninto a PAC learning algorithm (over arbitrary distributions):\nTheorem 2.1 ([Ang88, Hau88, Lit89]). Any algorithm A that learns a concept class C in the mistake-bound model with mistake bound m and running time t per round can be converted into an algorithm A\u2032 that PAClearns C with sample complexity O(1\u03b5m+ 1 \u03b5 log 1 \u03b4 ), running time O( 1 \u03b5mt+ t \u03b5 log 1 \u03b4 ), approximation parameter \u03b5, and confidence parameter \u03b4.\nThe k-LPN problem with noise rate \u03b7, introduced in Section 1, corresponds to the problem of PAClearning PAR(k) under the uniform distribution, when the example source can mislabel examples with a rate \u03b7 \u2208 (0, 1/2). More generally, one can study the k-LPN problem over D, an arbitrary distribution. [GRV11] show the following for this problem:\nTheorem 2.2 (Theorem 5 of [GRV11]). For any \u03b5, \u03b4, \u03b7 \u2208 (0, 1/2), and distribution D over {0, 1}n, the k-LPN problem over D with noise rate \u03b7 can be solved using k log(n/\u03b4)\u03c9(1)\u03b52(1\u22122\u03b7)2 samples in time 1\n\u03b52(1\u22122\u03b7)2 \u00b7 (\nn k/2\n)1+( \u03b7 \u03b5+\u03b7\u22122\u03b5\u03b7 )2+o(1) , where \u03b5 and \u03b4 are the approximation and confidence parameters respectively."}, {"heading": "3 In the absence of noise", "text": "We state the main result of this section.\nTheorem 3.1. Let k, t : N \u2192 N be two functions such that log logn \u226a k(n) \u226a t(n) \u226a n. Then for every n \u2208 N, there is an algorithm that learns PAR(k) in the mistake-bound model, with mistake bound at most (1 + o(1))knt + log ( t k ) and running time per round e\u2212k/4.01 \u00b7 ( t k ) \u00b7 O\u0303 ( (kn/t) 2 ) .\nUsing Theorem 1.3, we directly obtain Theorem 1.1. In fact, since Theorem 1.3 produces a PAClearner over any distribution, a statement of the form of Theorem 1.1 holds for examples obtained from any distribution.\nFor comparison, we quote the relevant result of [BGM10] in the mistake-bound model.\nTheorem 3.2 (Theorem 2.1 of [BGM10]). Let k, t : N \u2192 N be two functions such that k(n) 6 t(n) 6 n. Then for every n \u2208 N, there is a deterministic algorithm that learns PAR(k) in the mistake-bound model, with mistake bound at most k\u2308nt \u2309+ log ( t k ) and running time per round ( t k ) \u00b7 O((kn/t) 2 ).\nNote that their mistake bound is better by a lower-order term which we do not see how to avoid in our setup. This slack is not enough though to recover Theorem 3.1 from Theorem 3.2: dividing t by C roughly multiplies the sample complexity by C and divides the running time by Ck in [BGM10]\u2019s algorithm, whereas in our algorithm, dividing t by C roughly multiplies the sample complexity by C and divides the running time by (1.28C)k."}, {"heading": "3.1 The Algorithm", "text": "Let f \u2208 {0, 1}n be the hidden vector of sparsity k that the learning algorithm is trying to learn. Let e = {e1, e2, \u00b7 \u00b7 \u00b7 , en} be the set of standard basis of the vector space {0, 1}\nn. Let \u03b1 be a large constant we set later, and let T = \u03b1t. Note that T \u226a n. We define an arbitrary partition \u03c0 = C1, C2, \u00b7 \u00b7 \u00b7 , CT on the set e into T parts, each of size at most \u2308n/T \u2309. Next, let S1, . . . , Sm \u2282 [T ] be m random subsets of [T ], each of size \u03b1k. We choose m to ensure the following:\nClaim 3.3. If m = O\u0303\n(\n( T\u03b1k) ( T\u2212k\u03b1k\u2212k)\n)\n, then with nonzero probability, for every set A \u2282 [T ] of size k, A \u2282 Si for\nsome i \u2208 [m].\nProof. This follows from the simple observation that for any fixed i \u2208 [m], Pr[A \u2282 Si] = ( T\u2212k \u03b1k\u2212k )/( T \u03b1k )\n, and so,\nPr[\u2203i \u2208 [m], A 6\u2282 Si] =\n(\n1\u2212\n(\nT \u2212 k\n\u03b1k \u2212 k\n)/(\nT\n\u03b1k\n))m\n6 e\u2212m( T\u2212k \u03b1k\u2212k)/( T \u03b1k)\nChoosing m = 2 ( T\u03b1k) ( T\u2212k\u03b1k\u2212k) log ( T k ) and applying the union bound finishes the proof.\nWe fix some choice of S1, . . . , Sm \u2282 [T ] that satisfies the conclusion of Claim 3.3 for what follows. In fact, the rest is exactly [BGM10]\u2019s algorithm, which we reproduce for completeness.\nFor every i \u2208 [m], let Mi \u2282 {0, 1} n be the span of\n\u22c3\nj\u2208Si Cj . Note that\n\u2223 \u2223 \u2223 \u22c3\nj\u2208Si Cj\n\u2223 \u2223 \u2223 6 \u03b1k\u2308n/T \u2309 6\n\u03b1k \u00b7 ( n T + 1 ) 6 knt + \u03b1k = (1 + o(1))kn/t, as t \u226a n and \u03b1 is a constant. So, Mi is a linear subspace containing at most 2(1+o(1))kn/t points. Note that every f \u2208 {0, 1}n with |f | = k is contained in some Mi. This is simply because every set of k standard basis vectors is contained in at most k of the T parts in the partition \u03c0, and by Claim 3.3, every subset of [T ] of size k is contained in some Si.\nInitially, the unknown target vector f can be in any of the Mi\u2019s. Consider what happens when the learner sees an example a \u2208 {0, 1}n and a label y \u2208 {0, 1}. For i \u2208 [m], let Mi(a, y) = {f \u2208 Mi : f(a) = y}. Mi(a, y) may be of size 0, |Mi| or |Mi|/2. Note that the size of Mi(a, y) can be efficiently found using Gaussian elimination.\nWe are now ready to describe the algorithm:\n\u2013 Initialization: The learning algorithm begins with a set of affine spaces Ni, i \u2208 [m] represented by a system of linear equations. Initialize the affine spaces Ni = Mi for all i \u2208 [m].\n\u2013 On receiving an example a \u2208 {0, 1}n: Predict its label y\u0302 \u2208 {0, 1} such that \u2211\ni\u2208[m] |Ni(a, y\u0302)| > \u2211\ni\u2208[m] |Ni(a, 1\u2212 y\u0302)|.\n\u2013 On receiving the answer from the teacher y = f(a): Update Ni to Ni(a, y) for each i \u2208 [m]."}, {"heading": "3.2 Analysis", "text": "Before we analyze the algorithm, we first establish a combinatorial claim that is the crux of our improvement:\nLemma 3.4. If \u03b1 is a large enough constant, (\nT \u03b1k\n)\n(\nT\u2212k \u03b1k\u2212k\n) 6 e\u2212k/4.01 \u00b7\n(\nt\nk\n)\nProof.\n1 (\nt k\n) \u00b7\n(\nT \u03b1k\n)\n(\nT\u2212k \u03b1k\u2212k\n) =\nk\u22121 \u220f\ni=0\nk \u2212 i t\u2212 i \u00b7 T \u2212 i \u03b1k \u2212 i\n=\nk\u22121 \u220f\ni=0\n\u03b1t\u2212 i \u03b1k \u2212 i \u00b7 k \u2212 i t\u2212 i\n=\nk\u22121 \u220f\ni=1\n 1\u2212 i ( 1\u2212 1\u03b1 )\n(\n1 k\u2212i \u2212 1 t\u2212i\n)\n1 + ik\u2212i ( 1\u2212 1\u03b1 )\n\n\n6\nk\u22121 \u220f\ni=1\n(\n1\u2212 0.999\n1 + k\u2212ii \u03b1 \u03b1\u22121\n)\nwhere the equalities are routine calculation and the inequality is using that k(n) \u226a t(n). Each individual term in the product is strictly less than 1. So, the above is bounded by:\n6\nk\u22121 \u220f\ni=k/(2\u2212\u03b5)\n(\n1\u2212 0.999\n1 + k\u2212ii \u03b1 \u03b1\u22121\n)\n6\n(\n1\u2212 0.999\n1 + (1\u2212 \u03b5) \u03b1\u03b1\u22121\n) 1\u2212\u03b5 2\u2212\u03b5 k\n6 exp\n(\n\u2212 lg e \u00b7 0.999(1\u2212 \u03b5)\n(2 \u2212 \u03b5)(1 + (1\u2212 \u03b5) \u03b1\u03b1\u22121 ) k\n)\n6 e\u2212k/4.01\nfor a small enough constant \u03b5 > 0 and large enough constant \u03b1 > 1.\nProof of Theorem 3.1. Fix \u03b1 to be a constant that makes the conclusion of Lemma 3.4 true. We first check that the invariant is maintained throughout the algorithm that f \u2208 \u222ai\u2208[m]Ni. This holds at initiation by the argument given earlier. After that, obviously, if f \u2208 Ni, then f \u2208 Ni(a, f(a)) for any a \u2208 {0, 1}n, and so the invariant holds. Therefore, if the algorithm terminates, it will find the hidden vector f and return it as the solution. The rate of convergence is precisely captured by the number of mistakes learning algorithm makes, which we describe next.\nMistake Bound. Notice that when the algorithm begins, the sum of the sizes of all the affine spaces, \u2211\ni |Ni| 6 O\u0303\n(\n( T\u03b1k) ( T\u2212k\u03b1k\u2212k)\n)\n2(1+o(1))kn/t. Now whenever the learner makes a mistake by predicting y\u0302 6= y, the size\nof all affine spaces \u2211\ni |Ni| reduces by a factor of at least 2. This is due to the definition of y\u0302 and the fact that |Ni(a, y\u0302)|+ |Ni(a, 1\u2212 y\u0302)| = |Ni|.\nHence, using Lemma 3.4, after at most\nlog\n(\n\u2211\ni\n|Ni|\n)\n6 log\n[\nO\u0303\n(\n(\nT \u03b1k\n)\n(\nT\u2212k \u03b1k\u2212k\n)\n)\n2(1+o(1))kn/t\n]\n6 (1 + o(1))kn/t+ log\n(\nt\nk\n)\n\u2212 \u2126(k) + logO\n(\nlog\n(\nt\nk\n))\nmistakes, the size of \u222as\u2208SNs will decrease to 1, which by the invariant above will imply that \u222as\u2208SNs = {f}, and hence the learner makes no more mistakes. . Since we assume k \u226b log logn and t \u226a n, we can bound the number of mistakes by: (1 + o(1))kn/t+ log (\nt k\n)\nRunning Time. We analyze the running time of the learner for each round. At each round, for a question a \u2208 {0, 1}n, we need to compute |Ni(a, 0)| and |Ni(a, 1)| as well as store a representation of the updated Ni. Now, since for each Ni is spanned by at most \u2113 = (1 + o(1))kn/t basis vectors, we can treat each Ni as a linear subspace in {0, 1}\u2113. Ni(a, 0) and Ni(a, 1) can be computed by performing Gaussian elimination on a system of linear equations involving \u2113 variables, which takes O(\u21132) time. Thus, the total running time is O(m\u21132), which using Lemma 3.4 is exactly the bound claimed in Theorem 3.1."}, {"heading": "4 In the presence of noise", "text": "Recall the k-LPN problem. In this section, we show a reduction from k-LPN to noiseless learning of PAR(k) and its applications."}, {"heading": "4.1 The Reduction", "text": "We focus on the case when the noise rate \u03b7 is bounded by a constant less than half.\nTheorem 1.3 (recalled) Given an algorithm A that learns PAR(k) over the uniform distribution with confidence parameter \u03b4 using s(\u03b4) samples and running time t(\u03b4),there is an algorithm A\u2032 that solves the k-LPN problem with noise rate \u03b7 \u2208 (0, 1/3), using O(s(\u03b4/2) log(1/\u03b4)) examples and running time exp(O(H(3\u03b7/2) \u00b7 s(\u03b4/2) \u00b7 log(1/\u03b4)))) \u00b7 (t(\u03b4/2) + s(\u03b4/2) log(1/\u03b4)) and with confidence parameter \u03b4.\nLet A(\u03b4) be a PAC-learning algorithm over the uniform distribution for PAR(k) of length n with confidence parameter \u03b4 that draws s(\u03b4) examples and runs in time t(\u03b4). Below is our algorithm for k-LPN. Here, H denotes the binary entropy function p 7\u2192 p log2(1/p) + (1\u2212 p) log2(1/(1\u2212 p)).\nNoisy(\u03b4, \u03b7)\n1: Draw s\u2032 = 20s(\u03b4/2) log(1/\u03b4) random examples (a1, \u21131), . . . , (as\u2032 , \u2113s\u2032) \u2208 {0, 1} n \u00d7 {0, 1}. 2: for all S \u2286 [s\u2032], |S| 6 32\u03b7s \u2032 do 3: for i \u2208 [s\u2032] do 4: if i \u2208 S then \u2113\u0303i \u2190 1\u2212 \u2113i 5: else \u2113\u0303i \u2190 \u2113i 6: end if 7: end for 8: xS \u2190 A(\u03b4/2) applied to examples (a1, \u2113\u03031), . . . , (as\u2032 , \u2113\u0303s\u2032). 9: end for\n10: Draw s\u2032\u2032 = 600(s\u2032 \u00b7H(3\u03b7/2) + log(8/\u03b4)) random examples (b1,m1), . . . , (bs\u2032\u2032 ,ms\u2032\u2032) \u2208 {0, 1} n \u00d7 {0, 1} 11: S\u2217 \u2190 argmaxS\u2282[s\u2032],|S|63\u03b5s\u2032/2 |{i \u2208 [s \u2032\u2032] : \u3008bi, xS\u3009 = mi}| 12: return xS\u2217\nProof of Theorem 1.3.\nLemma 4.1. The sample complexity of Noisy is s\u2032 + s\u2032\u2032 = O(s(\u03b4/2) log(1/\u03b4)).\nProof. Immediate.\nLemma 4.2. The running time of Noisy is 2O(H(3\u03b7/2)s(\u03b4/2) log(1/\u03b4)) \u00b7 (t(\u03b4/2) + s(\u03b4/2) log(1/\u03b4))).\nProof. We use the standard estimate \u2211\u03b1x\ni=0\n(\nx i\n)\n6 2H(\u03b1)x for \u03b1 6 12 . The bound is then immediate.\nLemma 4.3. If x is the hidden vector and x\u2217 is output by Noisy(\u03b4), then with probability at least 1 \u2212 \u03b4, x\u2217 = x.\nProof. We make the assumption throughout that \u03b7 = \u2126(1/s(\u03b4/2)), as otherwise, with high probability, the example source won\u2019t mislabel any of s(\u03b4/2) samples and so the reduction is trivial.\nLet T = {i \u2208 [s\u2032] : \u3008ai, x\u3009 6= \u2113i} be the subset of the s \u2032 samples drawn in line 1 that are mislabeled by the example source. By the Chernoff bound: Pr[|T | > 3\u03b7s\u2032/2] 6 e\u2212\u03b7s \u2032/12 6 \u03b4/4. If |T | 6 3\u03b7s\u2032/2, we have with probability at least 1\u2212 \u03b4/2, xT = x. Thus, for any i \u2208 [s \u2032\u2032],Prbi [\u3008xT , bi\u3009 6= mi] 6 \u03b7. On the other hand, for all xS 6= xT , Prbi [\u3008xS , bi\u3009 6= \u3008x, bi\u3009] = 1/2, and so Prbi [\u3008xS , bi\u3009 6= mi] = 1/2 as the noise is random. Again, using Chernoff bounds,\nPr[\u2203S 6= T s.t.|{i \u2208 [s\u2032\u2032] : \u3008bi, xS\u3009 6= mi}| 6 5s \u2032\u2032/12] 6 2H(3\u03b7/2)s\n\u2032 \u00b7 e\u2212s \u2032\u2032/450 <\n\u03b4\n8\nOn the other hand, for xT itself, Pr[|{i \u2208 [s \u2032\u2032] : \u3008bi, xT \u3009 6= mi}| > 5s \u2032\u2032/12] < \u03b48 by a similar use of Chernoff bounds. So, in all, with probability at least 1\u2212 \u03b4, xT will be returned in step 12.\nWhen the noise rate \u03b7 is more than 1/3, a similar reduction can be given by adjusting the parameters accordingly. Also, when the distribution is arbitrary but the noise rate is less than 1/4, a similar reduction can be made to work. In the latter case, A is invoked with a smaller approximation parameter than the one given to Noisy so that the filtering step in line 10 works."}, {"heading": "4.2 Applications", "text": "An immediate application of Theorem 1.3 is obtained by letting A be the current fastest known attributeefficient algorithm for learning PAR(k), the algorithm due to Spielman4 [KS06] that makesO(k logn) samples and takes O( (\nn k/2\n)\n) time (for constant confidence parameter \u03b4). (We ignore the confidence parameter in this\nsection for simplicity.)\nCorollary 1.4 (recalled) For any \u03b7 \u2208 (0, 1/3) and constant confidence parameter, there is an algorithm for k-LPN with sample complexity O(k logn) and running time (\nn k/2 )1+O(H(1.5\u03b7)) .\nProof. Immediate from Theorem 1.3.\nOur next application of Theorem 1.1 uses our improved PAR(k) learning algorithm from Section 3.\nCorollary 1.5 (recalled) Suppose k(n) = n/f(n) for some function f : N \u2192 N for which 1 \u226a f(n) \u226a n/ log logn, and suppose \u03b7(n) = o(1/((f(n))\u03b1 logn)) for some \u03b1 \u2208 [1/2, 1). Then, for constant confidence parameter, there exists an algorithm for k-LPN with noise rate \u03b7 with running time e\u2212k/4.01+o(k) \u00b7 (\nn k )1\u2212\u03b1 \u00b7\npoly(n) and sample complexity O(k(f(n))\u03b1).\nProof. Let A be the algorithm of Theorem 1.1 with t(n) = \u2308n/(f(n))\u03b1\u2309. The running time of A is e\u2212k/4.01 \u00b7 (\nn k )1\u2212\u03b1 \u00b7 poly(n) and its sample complexity is O(k \u00b7 f(n))\u03b1). Now, applying Theorem 1.3, we see that since H(1.5\u03b7) = o((f(n))\u2212\u03b1), the running time for Noisy is only a 2o(k) factor times the running time of A. This yields our desired result.\n4Though a similar algorithm was also proposed by Hopper and Blum [HB01]"}], "references": [{"title": "Queries and concept learning", "author": ["Dana Angluin"], "venue": "Mach. Learn.,", "citeRegEx": "Angluin.,? \\Q1988\\E", "shortCiteRegEx": "Angluin.", "year": 1988}, {"title": "Deterministic approximation algorithms for the nearest codeword problem", "author": ["Noga Alon", "Rina Panigrahy", "Sergey Yekhanin"], "venue": "Algorithms and Techniques,", "citeRegEx": "Alon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2009}, {"title": "Learning parities in the mistakebound model", "author": ["Harry Buhrman", "David Gar\u0107\u0131a-Soriano", "Arie Matsliah"], "venue": "Inform. Process. Lett.,", "citeRegEx": "Buhrman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Buhrman et al\\.", "year": 2010}, {"title": "Noise tolerant learning, the parity problem, and the statistical query model", "author": ["Avrim Blum", "Adam Kalai", "Hal Wasserman"], "venue": "J. ACM,", "citeRegEx": "Blum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2003}, {"title": "Separating distribution-free and mistake-bound learning models over the boolean domain", "author": ["Avrim Blum"], "venue": "SIAM J. on Comput.,", "citeRegEx": "Blum.,? \\Q1994\\E", "shortCiteRegEx": "Blum.", "year": 1994}, {"title": "On-line algorithms in machine learning", "author": ["Avrim Blum"], "venue": "In Workshop on on-line algorithms,", "citeRegEx": "Blum.,? \\Q1996\\E", "shortCiteRegEx": "Blum.", "year": 1996}, {"title": "Efficient fully homomorphic encryption from (standard) LWE", "author": ["Zvika Brakerski", "Vinod Vaikuntanathan"], "venue": "In Proc. 52nd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Brakerski and Vaikuntanathan.,? \\Q2011\\E", "shortCiteRegEx": "Brakerski and Vaikuntanathan.", "year": 2011}, {"title": "On agnostic learning of parities, monomials, and halfspaces", "author": ["Vitaly Feldman", "Parikshit Gopalan", "Subhash Khot", "Ashok Kumar Ponnuswami"], "venue": "SIAM J. on Comput.,", "citeRegEx": "Feldman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2009}, {"title": "On noise-tolerant learning of sparse parities and related problems. In Algorithmic Learning Theory, pages 413\u2013424", "author": ["Elena Grigorescu", "Lev Reyzin", "Santosh Vempala"], "venue": null, "citeRegEx": "Grigorescu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grigorescu et al\\.", "year": 2011}, {"title": "Space efficient learning algorithms", "author": ["David Haussler"], "venue": "Technical Report UCSC-CRL-88-2, University of California at Santa Cruz,", "citeRegEx": "Haussler.,? \\Q1988\\E", "shortCiteRegEx": "Haussler.", "year": 1988}, {"title": "Secure human identification protocols", "author": ["Nicholas J Hopper", "Manuel Blum"], "venue": "In Advances in cryptology\u2013ASIACRYPT", "citeRegEx": "Hopper and Blum.,? \\Q2001\\E", "shortCiteRegEx": "Hopper and Blum.", "year": 2001}, {"title": "Toward attribute efficient learning of decision lists and parities", "author": ["Adam R Klivans", "Rocco A Servedio"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Klivans and Servedio.,? \\Q2006\\E", "shortCiteRegEx": "Klivans and Servedio.", "year": 2006}, {"title": "From on-line to batch learning", "author": ["Nick Littlestone"], "venue": "In Proc. 2nd Annual ACM Workshop on Computational Learning Theory,", "citeRegEx": "Littlestone.,? \\Q1989\\E", "shortCiteRegEx": "Littlestone.", "year": 1989}, {"title": "On lattices, learning with errors, random linear codes, and cryptography", "author": ["Oded Regev"], "venue": "J. ACM,", "citeRegEx": "Regev.,? \\Q2009\\E", "shortCiteRegEx": "Regev.", "year": 2009}, {"title": "A theory of the learnable", "author": ["Leslie G Valiant"], "venue": "Comm. Assn. Comp. Mach.,", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "Finding correlations in subquadratic time, with applications to learning parities and juntas", "author": ["Gregory Valiant"], "venue": "In Proc. 53rd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Valiant.,? \\Q2012\\E", "shortCiteRegEx": "Valiant.", "year": 2012}], "referenceMentions": [], "year": 2015, "abstractText": "We first consider the problem of learning k-parities in the on-line mistake-bound model: given a hidden vector x \u2208 {0, 1} with |x| = k and a sequence of \u201cquestions\u201d a1, a2, \u00b7 \u00b7 \u00b7 \u2208 {0, 1} , where the algorithm must reply to each question with \u3008ai, x\u3009 (mod 2), what is the best tradeoff between the number of mistakes made by the algorithm and its time complexity? We improve the previous best result of Buhrman et. al. [BGM10] by an exp(k) factor in the time complexity. Second, we consider the problem of learning k-parities in the presence of classification noise of rate \u03b7 \u2208 (0, 1/2). A polynomial time algorithm for this problem (when \u03b7 > 0 and k = \u03c9(1)) is a longstanding challenge in learning theory. Grigorescu et al. [GRV11] showed an algorithm running in time ( n k/2 )1+4\u03b7+o(1) . Note that this algorithm inherently requires time ( n k/2 ) even when the noise rate \u03b7 is polynomially small. We observe that for sufficiently small noise rate, it is possible to break the ( n k/2 ) barrier. In particular, if for some function f(n) = \u03c9(1) and \u03b1 \u2208 [1/2, 1), k = n/f(n) and \u03b7 = o(f(n)/ log n), then there is an algorithm for the problem with running time poly(n) \u00b7 ( n k )1\u2212\u03b1 \u00b7 e.", "creator": "LaTeX with hyperref package"}}}