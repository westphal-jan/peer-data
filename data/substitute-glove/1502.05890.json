{"id": "1502.05890", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Contextual semibandits via supervised learning oracles", "abstract": "We subjects takes genetic own seen contextual bandit difference, small followed make cup, next platform-independent plays which sample its intended, assistance a prominently besides each personal take, of reward instead considered linearly account to except notable. This course has applications means new dns, audience - nutraceutical, networking database, many how other genes. If took formula_3 transformation is latter, do ways the graphical that is reused particular to the matrix of Agarwal fran\u00e7ais single. [1960] however performances though indeed enjoys up mistake placed between $ \\ njc {O} (\\ sqrt {KLT \\ nte N} ) $ which $ \\ index.htm {O} (L \\ sqrt {KT \\ scr N} ) $, where $ K $ part the number taken dealt, $ L $ is the covers common hold an precise, $ T $ it instead handful similar pair, without $ N $ comes first 100 of restrictions. If after first-order scale become exact, indeed show same referring clustering that both explores might learn full trace adjust simultaneously corresponding contour through moved uses later 35,000 weights can essential $ \\ aol.com {O} (\\ |) \\ | _1 (KT) ^ {2005 / 60} \\ sqrt {\\ 50-company N} ) $ indignation, of $ w $ is put find (finding) strength vector. Both parameters able also constructs yahoo also forced suggestions 1-900 another time propose few fully none computationally cheaper anything example fuel encoding for is essentially program merely makes products.", "histories": [["v1", "Fri, 20 Feb 2015 14:55:41 GMT  (39kb,D)", "http://arxiv.org/abs/1502.05890v1", null], ["v2", "Thu, 5 Mar 2015 01:38:23 GMT  (39kb,D)", "http://arxiv.org/abs/1502.05890v2", null], ["v3", "Tue, 14 Jun 2016 00:43:13 GMT  (234kb,D)", "http://arxiv.org/abs/1502.05890v3", null], ["v4", "Fri, 4 Nov 2016 19:28:07 GMT  (236kb,D)", "http://arxiv.org/abs/1502.05890v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay krishnamurthy", "alekh agarwal", "miroslav dud\u00edk"], "accepted": true, "id": "1502.05890"}, "pdf": {"name": "1502.05890.pdf", "metadata": {"source": "CRF", "title": "Efficient Contextual Semi-Bandit Learning", "authors": ["Akshay Krishnamurthy", "Alekh Agarwal"], "emails": ["akshaykr@cs.cmu.edu", "alekha@microsoft.com", "mdudik@microsoft.com"], "sections": [{"heading": null, "text": "\u221a KLT lnN)1\nand O\u0303(L \u221a KT lnN), where K is the number of actions, L is the length of each action sequence, T is the number of rounds, and N is the number of policies. If the linear transformation is unknown, we show that an algorithm that first explores to learn the unknown weights via linear regression and thereafter uses the estimated weights can achieve O\u0303(\u2016w\u20161(KT )3/4 \u221a lnN) regret, where w is the true (unknown) weight vector. Both algorithms use an optimization oracle to avoid explicit enumeration of the policies and consequently are computationally efficient whenever an efficient algorithm for the fully supervised setting is available."}, {"heading": "1 Introduction", "text": "Learning from partial feedback (\u201cbandit\u201d feedback) is of great practical importance and has seen a recent surge of research interest [Bubeck and Cesa-Bianchi, 2012]. Motivating examples include healthcare [Robins, 1989] \u2013 where we only observe the result of the treatment prescribed to the patient, but obtain no information about how other treatments would have worked \u2013 or Internet applications [Li et al., 2010, Bottou et al., 2013] \u2013 where we only observe the user\u2019s reaction (e.g., click or purchase) to the content (e.g., advertisement, news article, merchandise description) that we show, but not to other possible content. These problems fall under the mathematical framework known as contextual bandits, where the learner repeatedly observes a context, then takes an action, and finally observes a reward for the chosen action. This learning paradigm captures the partial feedback aspect of the above examples as the learner does not observe rewards for unselected action. The aim of the learner is to maximize the reward over the course of many rounds of this interactions. \u2217akshaykr@cs.cmu.edu \u2020alekha@microsoft.com \u2021mdudik@microsoft.com 1Throughout the paper, we use the O\u0303 notation to suppress logarithmic dependence on K,T, L and lnN/\u03b4.\nar X\niv :1\n50 2.\n05 89\n0v 1\n[ cs\n.L G\n] 2\n0 Fe\nThe above applications, and others, often involve making a complex decision and receiving more detailed feedback about this decision. For example, in internet applications, we often recommend a set of items, but record information about the users interaction with each individual item (e.g., click, conversion, or hover time). To model this richer interaction, we assume that on each round, the learner makes a composite action, which is an ordered tuple of simple actions, and receives reward for the composite action, but also feedback about each simple actions played. This additional feedback is unhelpful unless it somehow relates to the total reward, and we assume that this relationship is linear. When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al., 2010] in the literature. Our goal is to design learning algorithms whose running time and statistical performance (measured by regret) scale with the number of simple actions rather than the number of composite actions.\nIn the first part of the paper, we assume that the linear relationship between the reward and the feedback on the simple actions is known, and we derive a new algorithm for contextual semi-bandits that meets our goal. Our approach builds on the recent contextual bandit algorithms of Dud\u0131\u0301k et al. [2011] and Agarwal et al. [2014] and enjoys a regret guarantee between O\u0303( \u221a KLT lnN) and O\u0303(L \u221a KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2. The policy class \u03a0 is a set of functions mapping contexts into composite actions (e.g., linear learners, decision trees, or neural nets), which we access via an optimization oracle. We show that the algorithm makes O\u0303(T 3/2) calls to the optimization oracle3, meaning that, given an efficient supervised learning algorithm, the algorithm has running time that is only logarithmic in |\u03a0|. This contrasts with the work of Kale et al. [2010] on contextual semi-bandits, which explicitly enumerates the policy class, and therefore has running time that is linear in |\u03a0|. Moreover, our algorithm only executes composite actions that policies choose, which allows implicit encoding of constraints defining valid composite actions.\nIn the second part of the paper, we move to a more general setting. We still assume that the reward for the composite action is linearly related to the feedback for the simple actions (which we call \u201cfeatures\u201d), but we no longer assume that the weights in this relationship are known; the composite reward is, however, still observed. Our solution for this setting is a two-stage algorithm. In the first stage, the algorithm chooses actions randomly to learn the weights in the linear mapping and to estimate the performance of each policy. Based on these estimates, we pick an empirically optimal policy and use it to exploit in the second stage. We show that the regret of this algorithm scales as O\u0303(\u2016w\u20161(KT )3/4 \u221a lnN), where w is the true (unknown) weight vector. The number of calls to the optimization oracle is independent of |\u03a0| in this case. While we expect the statistical performance of this algorithm to be sub-optimal, we view this as the first step in generalizing contextual semi-bandits to richer settings that assume less about the form of the reward.\nRelated Work. There is a growing body of work on combinatorial bandits, which are also referred to as semi-bandits, or slate bandits [Gyo\u0308rgy et al., 2007, Uchiya et al., 2010, Kale et al., 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that O\u0303( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a O\u0303( \u221a KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al. [2011] to semi-bandits, imposing the assumption that the feedback on the simple actions is linearly related\n2Extension to VC classes is straightforward using standard arguments. 3The dependence can be improved to O\u0303(T 1/2) using warm-start and epoching ideas identical to Agarwal et al. [2014].\nAlgorithm 1: SEMIBANDIT-VCEE (Variance Constrained Explore Exploit) Algorithm\ninput Allowed failure probability \u03b4 \u2208 (0, 1). 1: Q0 = 0, the all zeros-vector. Define: \u00b5t = min { 1 2K , \u221a ln(16t2|\u03a0|/\u03b4) Kpmint } .\n2: for each round t = 1, . . . , T do 3: Observe context xt \u2208 X 4: Sample At \u223c Q\u00b5t\u22121t\u22121 (\u00b7|xt) with remaining mass on \u03c0t\u22121, the previous best policy. 5: Play At and observe rewards {yt(at,l)}Ll=1. 6: Obtain Qt by solving OP with Ht = Ht\u22121 \u222a (xt, {yt(at,l)}Ll=1, {qt(at,l)}Ll=1) and \u00b5t. 7: end for\nSemi-bandit Optimization Problem (OP)\nGiven history H and a smoothing parameter \u00b5, define b\u03c0 = \u2016w\u20161 \u2016w\u201622 R\u0302egt(\u03c0) \u03c8\u00b5pmin and \u03c8 = 100. Find Q \u2208 \u2206|\u03a0| such that: \u2211 \u03c0\u2208\u03a0 Q(\u03c0)b\u03c0 \u2264 2KL/pmin (1)\n\u2200\u03c0 \u2208 \u03a0. E\u0302x\u223cH [ L\u2211 l=1\n1\nQ\u00b5(\u03c0(x)l|x) ] \u2264 2KL pmin + b\u03c0 (2)\nto the context. This contrasts with our setting: we make no assumptions about the feedback on the simple actions, but impose that the overall reward is linearly related to this feedback.\nExcept for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions. Qin et al. [2014] generalize this slightly by assuming that the reward is a known function of the context and features. We are not aware of any work that attempts to learn a relationship between the reward for the composite action and the simple action features as we do in the second part of the paper.\nAnother popular strategy used to cope with large action spaces is referred to as linear or parametric bandits [Filippi et al., 2010, Rusmevichientong and Tsitsiklis, 2010, Chu et al., 2011]. Here, the context revealed at each round includes a feature vector for each action, and the reward for each action is an unknown linear (or parametric) function of its feature vector. This can be a good fit for some applications, but it does not model those features that are only observed after playing actions. While our set up also assumes a linear relationship between the reward for the composite action and the features for the simple actions, the crucial difference is that the features are observed only after playing the composite action, rather than before."}, {"heading": "2 SEMIBANDIT-VCEE for Contextual Semi-Bandits with Known Weights", "text": "When the weights in the linear transformation are known, we propose an algorithm that has a similar structure to a recent algorithm for the classical contextual bandit problem [Agarwal et al., 2014]. The algorithm maintains a distribution over policies and uses the smoothed version to play actions at each round.\nThe core of the algorithm involves finding this distribution by solving a convex optimization problem, which we call OP, using the past record of interaction H . OP is a feasibility problem that looks for a distribution with both low regret (measured by the empirical regret) and low variance. The constraint in Equation 1 enforces that the distribution has low empirical regret, thereby placing mass on policies that are performing well. On the other hand, the constraint in Equation 2, ensures that the variance of the importance weighted reward estimates remains small for each policy \u03c0, and therefore requires that the distribution is not too peaked. The constraint enforces a bound on the variance in reward estimates for the policy \u03c0, at a level regulated by the empirical regret of \u03c0, thereby ensuring sufficient exploration amongst all good policies. These two constraints juggle the exploration-exploitation tradeoff, as Equation 1 encourages placing mass on good policies, while Equation 2 encourages playing more uniformly.\nThe main differences between SEMIBANDIT-VCEE and the algorithm of Agarwal et al. [2014] are in the OP and the definitions. One crucial modification is in the variance constraint in Equation 2. This constraint involves the marginal probabilities of the simple actions rather than the composite actions as would be the most obvious adaptation of the original algorithm to our setting. By working with the simple actions rather than the composite actions, this constraint leads to much tighter control of the importance-weighted reward estimates and consequently an improved regret bound.\nA related modification is in the definition of the reward estimates, which leverages the additional feedback on the simple actions. Without using this additional feedback in the reward estimate, the constraint in Equation 2 would not relate to the variance of this quantity. In addition to these definitional changes, which are really the crux of the modifications, we also modify the OP to account for the influence of the weight vector on the scaling of the rewards and the influence of pmin on the smoothing distribution."}, {"heading": "2.1 Regret Guarantee for SEMIBANDIT-VCEE", "text": "Our analysis of this algorithm leads to the following regret guarantee:\nTheorem 1. For any T \u2208 N, with probability at least 1 \u2212 \u03b4, there is a universal constant c0 > 0 such that the regret of Algorithm 1 is at most:\n(4\u03c8 + c0) \u2016w\u201622 \u2016w\u20161 L\n( K ln(16T 2|\u03a0|/\u03b4)\npmin + 12\n\u221a KT ln(16T 2|\u03a0|/\u03b4)\npmin\n) (3)\nAsymptotically, the regret of SEMIBANDIT-VCEE is O\u0303(\u2016w\u2016 2 2 \u2016w\u20161L \u221a KT ln |\u03a0|/pmin) which scales sublinearly with the number of simple actions K and only logarithmically with the number of composite actions, which is \u0398(KL). Note that any classical contextual bandit algorithm that ignores the additional feedback and structure in this setting would suffer regret \u2126\u0303( \u221a KLT ln |\u03a0|). Therefore, our result shows how a dramatic reduction in regret can be obtained through additional feedback. In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting. Specifically, they assume that weights w = 1 and that uniform exploration is possible, and they obtain an O\u0303( \u221a KLT ln |\u03a0|) regret bound. Theorem 1 matches this bound, as \u2016w\u201622 = \u2016w\u20161 = L and pmin = L in this case. Our result improves on theirs in two directions: statistically we show how a non-uniform weight vector and restricted exploration distribution affects the regret and, computationally, our algorithm can be efficiently implemented with an optimization oracle while theirs cannot.\nWhen uniform exploration is not allowed, as considered by Kveton et al. [2015] in the non-contextual setting, we can set pmin = 1 and our bound is worse than theirs by a factor of \u221a L. This discrepancy may\nAlgorithm 2: Coordinate Descent Algorithm for Semi-Bandit OP\ninput History H and smoothing parameter \u00b5. 1: Initialize weights Q = 0 \u2208 \u2206\u03a0. 2: while true do 3: For all \u03c0, define:\nV\u03c0(Q) = E\u0302x\u223cH [ L\u2211 l=1\n1\nQ\u00b5(\u03c0(x)l|x)\n] , S\u03c0(Q) = E\u0302x\u223cH [ L\u2211 l=1\n1\nQ\u00b5(\u03c0(x)l|x)2\n] , D\u03c0(Q) = V\u03c0(Q)\u2212 2KL\npmin \u2212 b\u03c0\n4: If \u2211 \u03c0 Q(\u03c0)( 2KL pmin + b\u03c0) > 2KL pmin , replace Q by cQ where c = 2KL/pmin\u2211 \u03c0 Q(\u03c0)(2KL/pmin+b\u03c0) < 1. 5: Else if \u2203\u03c0 s.t. D\u03c0(Q) > 0, update Q(\u03c0) = Q(\u03c0) + \u03b1\u03c0(Q) where \u03b1\u03c0(Q) = V\u03c0(Q)+D\u03c0(Q)2(1\u2212K\u00b5)S\u03c0(Q) . 6: Otherwise halt and output Q. 7: end while\nbe a by-product of moving to the more challenging contextual setting, as a UCB-style algorithm, which they use, is no longer suitable. In particular, all contextual bandit algorithms we are aware involve some degree of uniform exploration, and it seems that O\u0303(L \u221a KT ln |\u03a0|) is unavoidable if the best exploration distribution has pmin = O(1)."}, {"heading": "2.2 Computational Guarantee for SEMIBANDIT-VCEE", "text": "We now turn to analyzing the computational aspects of Algorithm 1. The main bottleneck is in solving the optimization problem (OP), and our analysis focuses on this subroutine. This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm:\nTheorem 2. For any history H and parameter \u00b5, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u00b5))\u00b5pmin iterations and each iteration can be implemented efficiently, with at most one call to AMO.\nSince the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as O\u0303 ( T 3/2 \u221a K\npmin log(|\u03a0|/\u03b4)\n) by the setting of \u00b5t. Moreover, due to the nature of the coordinate descent algo-\nrithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |\u03a0|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over \u2206|\u03a0|.\nWe mention in passing that Agarwal et al. [2014] also develop two improvements that lead to a more efficient algorithm. They partition the game into epochs and only solve OP once every epoch, rather than in every round as we do here. They also show how to use the weight vector from the previous round to warm-start the next coordinate descent execution. Both of these optimizations can also be implemented\nhere, and they will lead to a better computational guarantee for the algorithm, although we omit these details to simplify the presentation."}, {"heading": "2.3 Proof Sketch of Theorem 1", "text": "The proof of the regret bound is quite technical and we sketch the arguments here, deferring all details to Appendix A. To start off, we establish two uniform deviation bounds, one on the variance estimates used in Equation 2, and the other on the reward estimate \u03b7t(\u03c0) used in b\u03c0 . For the former, we show that if \u00b5t and t are large enough, then simultaneously for all P, \u03c0, with high probability\nV (P, \u03c0, \u00b5t) \u2264 6.4V\u0302t(P, \u03c0, \u00b5t) + 81.3KL/pmin,\nwhere V (P, \u03c0, \u00b5t) = Ex\u223cD \u2211L l=1 1 P\u00b5(\u03c0(x)l|x) and V\u0302t is the empirical version. The other main deviation bound is on the reward estimates. We use Freedman\u2019s inequality to show that with high probability, for each time t and each policy \u03c0:\n|\u03b7t(\u03c0,w)\u2212R(\u03c0)| \u2264 \u2016w\u201622 \u2016w\u20161 \u00b5t\u22121 (Vt(\u03c0)pmin +KL) ,\nwhere Vt(\u03c0) = max0\u2264\u03c4\u2264t\u22121 V (Q\u0303\u03c4 , \u03c0, \u00b5\u03c4 ), and Q\u0303\u03c4 is the distribution used in the \u03c4 th round by our algorithm. The important thing with these bounds is that the variance deviation does not depend on the size of the composite action space, and that V\u0302 (P, \u03c0, \u00b5t), which we control in Equation 2, does indeed control the variance of the reward estimates. This tighter control is the main statistical gain.\nEquipped with these deviation inequalities, we proceed to bound the deviation between the empirical and the true regret. This is made possible by leveraging the variance control in Equation 2, leading to a bound on the reward estimates, which make up the empirical regret, R\u0302egt(\u00b7). A careful inductive argument leads to the bounds (with high probability):\nReg(\u03c0) \u2264 2R\u0302egt(\u03c0) + c0 \u2016w\u201622 \u2016w\u20161 KL\u00b5t and R\u0302egt(\u03c0) \u2264 2Reg(\u03c0) + c0 \u2016w\u201622 \u2016w\u20161 KL\u00b5t\nNow the constraint in Equation 1 ensures that the empirical regret is small, which, by the above inequalities, also ensures that the actual regret when playing according to Q\u0303t is small. In particular at round t, we play with distribution Q\u0303\u00b5t\u22121t\u22121 and we show that:\u2211\n\u03c0\nQ\u0303t\u22121(\u03c0)Reg(\u03c0) \u2264 C \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121\nThis bound applies to the unsmoothed distribution, so we also must control the regret associated with smoothing. However, we know that the per-round regret is bounded by \u2016w\u20161 and the smoothing probability is K\u00b5t, so both terms are bounded by the sum of the \u00b5ts, which grows at rate \u221a T ."}, {"heading": "2.4 Proof Sketch of Theorem 2", "text": "First, if the algorithm halts, then both of the conditions must be satisfied. The regret condition must be satisfied since we know that \u2211 \u03c0 Q(\u03c0)(2KL/pmin + b\u03c0) \u2264 2KL/pmin which in particular implies that\u2211\n\u03c0 Q(\u03c0)b\u03c0 \u2264 2KL/pmin as required. Note that this also ensures that \u2211 \u03c0 Q(\u03c0) \u2264 1 so Q \u2208 \u2206|\u03a0|. Finally,\nif we halted, then for each \u03c0, we must have D\u03c0(Q) \u2264 0 which implies V\u03c0(Q) \u2264 2KLpmin + b\u03c0 so the variance constraint is also satisfied.\nThe algorithm can be implemented by first accessing the oracle on the importance weighted history H\u0302 to obtain \u03c0t (so that we can compute b\u03c0). The low regret check in Step 4 of Algorithm 2 can be done efficiently, since each policy in the support of the current distribution Q was added at a previous iteration of Algorithm 2, and we can store the regret of the policy at that time for no extra computational burden. This allows us to always maintain the expected regret of the current distribution Q for no added cost. Finding a policy violating the variance check can be done by one call to the oracle AMO. At round t of the contextual bandit problem, we create a dataset of the form (xi, zi, vi) of size 2t. The first t terms come from the variance V\u03c0(Q) and the second t terms come from the rescaled empirical regret b\u03c0 . For \u03c4 \u2264 t, we define x\u03c4 to be the \u03c4 th context,\nz\u03c4 (a) = 1\ntQ\u00b5(a|x\u03c4 ) , and v\u03c4 = 1.\nWith this definition, it is easily seen that V\u03c0(Q) = \u2211t \u03c4=1 v T \u03c4 z\u03c4 (\u03c0(x\u03c4 )). For \u03c4 > t, we define x\u03c4 to be the context from round \u03c4 \u2212 t and\nz\u03c4 (a) = \u2212\u2016w\u20161\n\u2016w\u201622t\u03c8\u00b5pmin y\u0302\u03c4 (a), and v\u03c4 = w.\nIt can now be verified that \u22112t \u03c4=t+1 v T \u03c4 z\u03c4 recovers the b\u03c0 term up to additive constants independent of the policy \u03c0 (essentially up to the \u03b7t(\u03c0t) term). Combining everything, it can be checked that:\nD\u03c0(Q) = 2t\u2211 \u03c4=1 z\u03c4 (\u03c0(x\u03c4 )) T v\u03c4 \u2212 2KL pmin \u2212 \u2016w\u20161 \u2016w\u201622 \u03b7t(\u03c0t) \u03c8\u00b5pmin\nThe two terms at the end are independent of \u03c0 so by calling the argmax oracle with this 2t sized dataset, we can find the policy \u03c0 with the largest value of D\u03c0 . If the largest value is non-positive, then no constraint violation exists. If it is strictly positive, then we have found a constraint violator to update the probability distribution on.\nAs for the iteration complexity bound, the analysis is based on the potential function:\n\u03a6(Q) =\n( E\u0302[RE(UAS(x)||Q\u00b5(\u00b7|x))]\n1\u2212K\u00b5 +\n\u2211 \u03c0 Q(\u03c0)b\u03c0\n2K/pmin\n)\nwhere RE(p||q) = \u2211 a\u2208A pa log(pa/qa) + qa \u2212 pa is the unnormalized relative entropy. In Appendix B, we show two main facts about Algorithm 2 and this potential function:\n1. When the regret constraint is violated, the shrinking update does not increase the potential. More formally, for any c < 1, we have \u03a6(cQ) \u2264 \u03a6(Q) whenever \u2211 \u03c0 Q\u03c0( 2KL pmin + b\u03c0) > 2KL pmin . 2. The additive update when D\u03c0 > 0 for some \u03c0 lowers the potential by at least L\u00b5pmin4(1\u2212K\u00b5) .\nThese are the analogs of Lemmas 6 and 7 in Agarwal et al. [2014]. The proof of the first is based on showing that the derivative of the function g(c) = \u03a6(cQ) is positive so that by convexity of \u03a6, shrinking the weights Q can only decrease the potential. The proof of the second involves directly calculating the difference in potential before and after the update, and we use a second order taylor expansion of log(x) to obtain the quadratic term S\u03c0(Q).\nAlgorithm 3: SEMIBANDIT-EELS (Explore-Exploit Least Squares)\ninput Time Horizon T , failure probability \u03b4 \u2208 (0, 1)\n1: Set H = \u2205, \u03a3 = 0 \u2208 RL\u00d7L, \u03bb? = \u221a KLT ln(8/\u03b4) \u2016w\u20162 , nT = ( TK\u2016w\u20162 \u2016w\u20161 )2/3 ( ln(8|\u03a0|/\u03b4)L )\n1/3. 2: while \u03bbmin(\u03a3) \u2264 \u03bb?, for at least nT rounds do 3: Observe context xt, play action At \u223c P , where P is the uniform distribution on A(xt). 4: Observe reward rt(At) and feature vector yt(At). 5: Update \u03a3 = \u03a3 + yt(At)yt(At)T , H = H \u222a (xt, {yt(at,l)}Ll=1, {p(at,l)}Ll=1). 6: end while 7: Estimate weights: w\u0302 = \u03a3\u22121 ( \u2211 i yi(Ai)ri(Ai)) (Least Squares). 8: Optimize policy \u03c0\u0302 = AMO(w\u0302,H) with importance weighted feature vectors. 9: For every remaining round, observe context xt and play At = \u03c0\u0302(xt).\nIt is also easy to see that \u03a6(0) \u2264 L log(1/(K\u00b5))/(1 \u2212 K\u00b5), that \u03a6(Q) is convex in Q and \u03a6 is nonnegative. All of these facts together means that with at most 4 log(1/(K\u00b5)\u00b5pmin executions of the variance update, we will have decreased the potential to zero. By the fact that the potential is non-negative, this bounds the number of executions of the additive update. As the shrinking update will never be executed twice in a row by construction, we can at worst alternate between the two updates, so that the total number of iterations is 8 log(1/(K\u00b5)\n\u00b5pmin . This proves the theorem."}, {"heading": "3 Contextual Semi-bandits with Unknown Weights", "text": "When the weights are unknown, we propose an algorithm that first explores and then exploits. The aim of this algorithm is to explore so that two things happen: we can accurately estimate the weights on the simple action features, and we can use these to accurately estimate the expected reward for each policy. In this section we assume that the time horizon T is known to the algorithm. We also assume that for each round, one can play uniformly over a subset of simple actions A(x) \u2282 A, meaning that all combinations and orderings of legal simple actions are allowed.\nPseudocode for our algorithm, SEMIBANDIT-EELS, is displayed in Algorithm 3. Structurally, the algorithm devotes the first several rounds to uniform exploration, which leads to reliable estimates for the weight vector as well as the expected feature vector for each policy. Taking their inner product, we naturally obtain good estimates of the reward for each policy, which can then be used in the AMO to find the policy with the best empirical performance. For the remaining rounds, the algorithm plays according to this policy. The analog of this algorithm in a normal contextual bandit setting would be to explore uniformly over the actions for first few rounds, find the best policy by using the estimated rewards and then exploiting with that policy.\nThe tradeoff between exploration and exploitation is negotiated by two things: the smallest eigenvalue of the feature covariance and a minimum number of rounds. Since we perform least squares in line 7 to obtain a vector w\u0302 that we use for reward estimation, by exploring until the eigenvalues of \u03a3 are large, we obtain a bound on \u2016w\u0302 \u2212 w\u20162. The other stopping condition ensures that the important weighted reward feature vectors are well behaved, and combined, these two conditions ensure that we are competitive with the optimal policy in the exploitation rounds.\nThe more challenging part of the analysis is ensuring that we do not accumulate too much regret in the exploration phase. The difficulty is that the condition involving the feature covariance does not bound the number of exploration rounds. Our analysis proceeds by showing that the eigenvalues of the feature\ncovariance can be lower bounded by a quantity that also upper bounds the exploration regret. Consequently, during exploration rounds we either have good estimates of the weights, or we have not accumulated too much regret and can afford to explore more. Formalizing this intuition reveals the following regret bound for Algorithm 3:\nTheorem 3. For any T \u2208 N with probability at least 1\u2212 \u03b4, the regret of Algorithm 3 is at most:\nO ( \u2016w\u20161(KT )3/4 \u221a ln(LT 2|\u03a0|/\u03b4) ) (4)\nThis theorem guarantees sublinear regret for Algorithm 3, and, to our knowledge, it is the first result on learning a relationship between simple action features and rewards under semi-bandit feedback. We do however make some undesirable assumptions. The algorithm requires knowledge of the time horizon T , which can be relaxed by variants of the Epoch-Greedy [Langford and Zhang, 2008] or -greedy approaches, although the analysis here is significantly simpler. We also make a strong assumption on the structure of the action space A(x) at each round, which is much harder to relax. We require that the exploration regret is related to the minimum eigenvalue of the feature covariance, and while there are slightly weaker conditions on the exploration distribution that enable this, the two quantities are not related in general, and therefore we may not be able to learn the weights while guaranteeing low exploration regret as we do here. We remind the reader that the exploration problem is much harder here, than say linear bandits, where the feature vector is revealed ahead of time rather than features being observed after taking the actions.\nNote that one can always run a classical contextual bandit algorithm here, ignoring the task of weight estimation and the simple action features. Such an algorithm can at best achieve O\u0303( \u221a KLT ) regret, as the number of composite actions is \u0398(KL). Thus our algorithm is favorable for shorter time horizons. We leave as future work the challenge of tempering the dependence on T to T 1/2.\nWe now sketch the proof of Theorem 3, with details in Appendix C. Proof of Theorem 3 (Sketch only) The proof of the theorem is based on trading off the regret associated with exploration and exploitation rounds. We state the intermediate results in terms of \u03bb? and nT , and will optimize over them at the end of the proof. Based on the stopping conditions for the exploration phase of the algorithm, the exploitation regret is bounded by:\nExploitation Regret \u2264 2TK 2\u2016w\u20162 \u221a\nln(8|\u03a0|/\u03b4) LnT +\n\u221a c ln(8/\u03b4)\n\u03bb?  The two terms in this decomposition are based on using \u03b7t(\u03c0, w\u0302) as an estimate ofR(\u03c0), which involves the true weight vector. The first term stems from the deviation between the importance weighted feature vectors y\u0302 and the true features vectors y, and is small provided that nT is large. The second term bounds the least squares error \u2016w\u0302 \u2212 w\u20162, which is small provided that \u03bb? is large.\nWe show that the exploration regret is bounded by:\nExploration Regret \u2264 \u2016w\u20161 ( 2nT + \u221a 2T ln(8/\u03b4) ) + \u2016w\u20162 (\u221a KT\u03bb?\n2L + T 3/4\n\u221a 2K ln1/4(8LT 2/\u03b4)\n)\nThe first term here is straightforward; if the eigenvalues of \u03a3 grow quickly, we perform at most nT rounds of exploration and suffer at most 2\u2016w\u20161 regret per round. If the eigenvalues grow slowly, then we may explore for longer, but we know that these eigenvalues are small for the entire exploration phase. The three other terms stem from relating these eigenvalues to the exploration regret.\nWe first argue that the exploration regret for round t is bounded by \u2016w\u20162 \u221a K(x) L Var(yt) where Var(yt) =\n1 K(x) \u2211 a\u2208A(x)(yt(a)\u2212 y\u0304t)2. This follows from the Cauchy-Schwarz inequality and, by a standard deviation bound, implies that the cumulative regret up to round t is bounded as:\nt\u2211 \u03c4=1 r\u03c4 (\u03c0?(x\u03c4 ))\u2212 r\u03c4 (A\u03c4 ) \u2264 \u2016w\u20162 t\u2211 \u03c4=1\n\u221a K(x\u03c4 ) Var(y\u03c4 )\nL + \u2016w\u20161\n\u221a 2t ln(8/\u03b4)\nThe second term in Equation 5 is exactly the deviation term here. We also show, by direct calculation, that if we explore uniformly (over a subset), then in expectation over our random choice of action, the smallest eigenvalue of the covariance matrix at round t is lower bounded by 2 \u2211t \u03c4=1 Var(y\u03c4 ). By the Matrix-Hoeffding inequality, the eigenvalues of the sample feature covariance concentrate around this population version, leading to the bound:\n\u03a3t\n( 2\nt\u2211 \u03c4=1 Var(y\u03c4 )\u2212 4L \u221a t ln(2Lt2/\u03b4)\n) IL,\nwhere \u03a3t is the feature covariance at round t. These two bounds imply that if the covariance matrix has small eigenvalues, then the total exploration regret must also be small. The fact that we only explore when \u03bbmin(\u03a3) is small immediately translates into a bound on the accumulated exploration regret. The third term in Equation 5 comes precisely from this argument, while the last comes from the deviation of the eigenvalues of \u03a3. We arrive at the theorem by optimizing over \u03bb? and nT ."}, {"heading": "4 Conclusion and Discussion", "text": "In this paper we studied the contextual semi-bandit problem where the learner plays a composite action and observes features for each simple action in this tuple in addition to the total reward. We assumed that the reward was linearly related to the features of the simple actions. If this linear relationship is known, we showed that an adaptation of the algorithm of Agarwal et al. [2014] achieves regret between O\u0303( \u221a KLT lnN) and O\u0303(L \u221a KT lnN), and can be implemented efficiently with access to an optimization oracle. If the weights are unknown, we provided a simple algorithm that achieves O\u0303(\u2016w\u20161(KT )3/4 lnN) regret. These algorithms show how to leverage additional feedback to avoid regret that scales with KL, the size of the composite action space.\nSeveral interesting questions arise from our work:\n1. When the weights are known, can we obtain O\u0303( \u221a KLT ) regret even when the set of feasible actions\nare constrained, rather than O\u0303(L \u221a KT ) regret as in Theorem 1? The work on non-contextual combinatorial bandits suggests that the answer is yes [Kveton et al., 2015], but all algorithms for contextual bandit learning involve some degree of uniform exploration, which would prohibit such a regret bound. 2. When the weights are unknown, can we obtain O( \u221a T ) regret while still avoiding dependence on the\nsize of the composite action space? 3. Can we learn other transformations in this partial feedback setting? Many applications call for mod-\neling interaction between simple actions, so moving beyond linear transformations is not only of theoretical interest.\nWe hope to address these questions in future work."}, {"heading": "A Full Proof of Theorem 1", "text": "The proof hinges on two uniform deviation bounds, and then a careful inductive analysis of the regret using the OP. The first deviation bound shows that the variance estimates used in Equation 2 are suitable estimators for the true variance of the distribution. To state this deviation bound, we need some definitions:\nV (P, \u03c0, \u00b5) = Ex\u223cDx [ L\u2211 l=1\n1\nP\u00b5(\u03c0(x)l|x)\n] V\u0302t(P, \u03c0, \u00b5) = E\u0302x\u223cHt [ L\u2211 l=1\n1\nP\u00b5(\u03c0(x)l|x)\n] (5)\nThe deviation bound is in the following Theorem:\nTheorem 4. For any \u03b4 \u2208 (0, 1), if:\n\u00b5t \u2265 \u221a ln(2|\u03a0|t2/\u03b4) Ktpmin\nt \u2265 4K ln(2|\u03a0|t 2/\u03b4)\npmin\nthen with probability at least 1\u2212 \u03b4, for all distribution P over \u03a0, all \u03c0 \u2208 \u03a0, and all t \u2208 N, we have:\nV (P, \u03c0, \u00b5t) \u2264 6.4V\u0302t(P, \u03c0, \u00b5t) + 81.3 KL\npmin (6)\nProof. The proof of this theorem is similar in spirit to a related theorem in Agarwal et al. [2014]. We first use Freedman\u2019s inequality (Lemma 18) to argue that for a fixed P, \u03c0, \u00b5, and t, the empirical version of the variance is close to the true variance. We use a discretization of the set of all distributions and then take a union bound to extend this deviation inequality to all P, \u03c0, \u00b5, t. In particular, we have:\nLemma 5. For fixed P, \u03c0, \u00b5, t and for any \u03bb \u2208 [ 0, \u00b5pminL ] , with probability at least 1\u2212 \u03b4:\nV (P, \u03c0, \u00b5)\u2212 V\u0302t(P, \u03c0, \u00b5) \u2264 (e\u2212 2)\u03bbL \u00b5pmin V (P, \u03c0, \u00b5) + ln(1/\u03b4) t\u03bb\nProof. Let:\nZi = L\u2211 l=1\n1\nP\u00b5(\u03c0(xi)l|xi) \u2212 Ex\u223cDx L\u2211 l=1\n1\nP\u00b5(\u03c0(x)l|x) ,\nand notice that 1t \u2211t i=1 Zt = V\u0302t(P, \u03c0, \u00b5) \u2212 V (P, \u03c0, \u00b5). Clearly, EZi = 0 for all i and maxi |Zi| \u2264 L \u00b5pmin since when we smooth by \u00b5, each action that \u03c0 could play must appear with probabiity at least \u00b5pmin. By the Cauchy-Schwarz and Holder\u2019s Inequalities, the conditional variance is:\nEx\u223cDxZ2i \u2264 Ex\u223cDx ( L\u2211 l=1\n1\nP\u00b5(\u03c0(x)l|x)\n)2 \u2264 LEx\u223cDx L\u2211 l=1\n1\nP\u00b5(\u03c0(x)l|x)2\n\u2264 L \u00b5pmin Ex\u223cDx L\u2211 l=1\n1\nP\u00b5(\u03c0(x)l|x) =\nL\n\u00b5pmin V (P, \u03c0, \u00b5).\nThe lemma now follows by Freedman\u2019s inequality.\nTo prove the variance deviation bound, we next use a discretization lemma from Dud\u0131\u0301k et al. [2011], which immediately implies that for any P , there exists a distribution P \u2032 supported on at most Nt policies such that for ct > 0, if Nt \u2265 6\u03b32t \u00b5tpmin :\nV (P, \u03c0, \u00b5)\u2212 V (P \u2032, \u03c0, \u00b5t) + ct ( V\u0302t(P \u2032, \u03c0, \u00b5t)\u2212 V\u0302t(P, \u03c0, \u00b5t) ) \u2264 \u03b3t(V (P, \u03c0, \u00b5t) + ctV\u0302t(P, \u03c0, \u00b5t))\nWe set \u03b3t = \u221a\n1\u2212K\u00b5t Nt\u00b5tpmin + 3 1\u2212K\u00b5tNt\u00b5tpmin , ct = 1 1\u2212 (e\u22122)L\u03bbt\u00b5tpmin , Nt = d 12(1\u2212K\u00b5t)\u00b5tpmin e and \u03bbt = 0.66\u00b5tpmin/L and\ntake a union bound over all t \u2208 N, Nt-point distributions P over \u03a0, and all \u03c0 \u2208 \u03a0 to arrive at:\nV (P, \u03c0, \u00b5t) \u2264 6.4V\u0302t(P, \u03c0, \u00b5t) + 6.3L ln(2|\u03a0|2m2/\u03b4)\n\u00b5ttpmin + 75L(1\u2212K\u00b5t) ln |\u03a0| \u00b52t tp 2 min .\nThe theorem now follows from the stated bounds on \u00b5t and t.\nThe other main deviation bound is a straightforward application of Freedman\u2019s inequality and a union bound. To state the lemma, we must introduct one more definition. Let Vt(\u03c0) = max0\u2264\u03c4\u2264t\u22121 V (Q\u0303\u03c4 , \u03c0, \u00b5\u03c4 ) where Q\u0303\u03c4 is Q\u03c4 (the distribution computed at the \u03c4 th round of the game) with any additional mass placed on \u03c0\u03c4 , the empirical regret minimizer at round \u03c4 .\nLemma 6. For any \u03bbt\u22121 \u2208 [0, \u00b5t\u22121pmin/\u2016w\u20161] for all t \u2208 N, \u03c0 \u2208 \u03a0 and for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4:\n|\u03b7t(\u03c0)\u2212R(\u03c0)| \u2264 \u2016w\u201622Vt(\u03c0)\u03bbt\u22121 + ln(4t2|\u03a0|/\u03b4)\nt\u03bbt\u22121 (7)\nProof. The rewards form a martingale and it is easy to see that the \u03c4 th term has range bounded by \u2016w\u20161pmin\u00b5\u03c4\u22121 \u2264 \u2016w\u20161 pmin\u00b5t\u22121 since the \u00b5s are non-increasing. Moreover the conditional variance can be bounded by using the Cauchy-Schwarz Inequality:\nE[Z2\u03c4 |H\u03c4\u22121] \u2264 \u2016w\u201622 L\u2211 l=1 Ex\u223cDxEy|x y(\u03c0(x)l) 2 Q\u0303 \u00b5\u03c4\u22121 \u03c4\u22121 (\u03c0(x)l|x) \u2264 \u2016w\u201622V (Q\u0303\u03c4\u22121, \u03c0, \u00b5\u03c4\u22121 \u2264 \u2016w\u201622Vt(\u03c0)\nAnd the claim now follows by Freedman\u2019s inequality.\nEquipped with these two deviation bounds we will proceed to prove the main theorem. Define dt = ln(16t2|\u03a0|/\u03b4) and let:\nt0 = min t { dt t \u2264 pmin 4K } .\nNote that t0 \u2265 4 since dt \u2265 1 and K \u2265 pmin. Set \u03c1 = mint>t0 \u221a t/(t\u2212 1) and note that \u03c1 \u2264 \u221a 2. With this\ndefinition of dt, we see that with \u00b5t \u2265 \u221a\ndt Ktpmin and t \u2265 4Kdt/pmin we have that with probability 1\u2212 \u03b4/8 Equation 6 holds for all distributions P , policies \u03c0 and t \u2208 N (provided t \u2265 4Kdt/pmin, i.e. t \u2265 t0). We also have that, for all t \u2208 N, \u03c0 \u2208 \u03a0, with probability \u2265 1\u2212 \u03b4/4:\n|\u03b7t(\u03c0)\u2212R(\u03c0)| \u2264 \u2016w\u201622Vt(\u03c0)\u03bbt\u22121 + dt\nt\u03bbt\u22121 ,\nand we will set:\n\u03bbt = 1\n\u2016w\u20161 \u221a pmindt 2Kt 1[t \u2264 t0] + \u00b5t\u22121pmin \u2016w\u20161 1[t > t0]\nNotice that to apply Lemma 6 we require \u03bbt \u2208 [0, \u00b5tpmin\u2016w\u20161 ] so our setting of \u03bbt is only valid for t \u2265 t0. Let E denote the event that both the variance and reward deviation bounds hold and observe that P(E) \u2265 1\u2212 \u03b4/2.\nUsing the variance constraint, it is straightforward to prove the following Lemma:\nLemma 7. Assume event E holds, then for any round t \u2208 N and any policy \u03c0 \u2208 \u03a0, let t? be the epoch achieving the max in the definition of Vt(\u03c0). Then there are universal constants \u03b81 > 2 and \u03b82 such that:\nVt(\u03c0) \u2264  2KL pmin if \u00b5t? = 1 2K \u03b81KL\npmin + \u2016w\u20161 \u2016w\u201622 R\u0302egt?(\u03c0) \u03b82\u00b5t?pmin if \u00b5t? \u2264 1 2K\n(8)\nProof. The first claim follows trivially by the definition of Vt(\u03c0) and the choice of \u00b5t? . For the second claim, we use the variance deviation bound and the optimization constraint. In particular, since 12K > \u00b5t? =\u221a dt?/(Kt?pmin) we have that t? \u2265 4Kdt?/pmin so we can apply the variance deviation bound:\nV (Q\u0303t? , \u03c0, \u00b5t?) \u2264 6.4V\u0302t?(Q\u0303t? , \u03c0, \u00b5t?) + 81.3 KL\npmin ,\nand we can use the optimization constraint which gives an upper bound on V\u0302t?(Q\u0303t? , \u03c0, \u00b5t?):\nV\u0302t?(Q\u0303t? , \u03c0, \u00b5t?) \u2264 V\u0302t?(Qt? , \u03c0, \u00b5t?) \u2264 2KL pmin + \u2016w\u20161 \u2016w\u201622 R\u0302egt?(\u03c0) \u03c8\u00b5t?pmin\nThe bound follows by the choice \u03b81 = 94.1 and \u03b82 = \u03c8/6.4.\nWe next compare Reg(\u03c0) and R\u0302eg(\u03c0) using the variance bounds above.\nLemma 8. Assume event E holds and define c0 = 4\u03c1(1 + \u03b81). For all t \u2265 t0 and all policies \u03c0 \u2208 \u03a0:\nReg(\u03c0) \u2264 2R\u0302egt(\u03c0) + c0 \u2016w\u201622 \u2016w\u20161 KL\u00b5t and R\u0302egt(\u03c0) \u2264 2Reg(\u03c0) + c0 \u2016w\u201622 \u2016w\u20161 KL\u00b5t (9)\nProof. The proof is by induction on t. As the base case, consider t = t0 where, by definition we have \u00b5t = 1/(2K) for all t < t0 so that Vt(\u03c0) \u2264 2KL/pmin for all \u03c0 \u2208 \u03a0 by Lemma 7. Using the reward deviation bounds, which hold under E we have:\n|\u03b7t(\u03c0)\u2212R(\u03c0)| \u2264 \u2016w\u201622Vt(\u03c0)\u03bbt + dt t\u03bbt \u2264 2KL\u2016w\u201622\u03bbt/pmin + dt t\u03bbt ,\nfor all \u03c0 \u2208 \u03a0. Since we are in round t0, we know that dt0/t0 \u2264 pmin/(4K) so we can set \u03bbt as specified above. This gives:\n|\u03b7t(\u03c0)\u2212R(\u03c0)| \u2264 2 \u221a 2 \u2016w\u201622 \u2016w\u20161 KL\u00b5t0 .\nHere we use the fact that \u2016w\u20161 \u2264 \u221a L\u2016w\u20162 and the definition of \u00b5t0 = \u221a dt0/(Kt0pmin). Now both directions of the bound follow from the triangle inequality and the optimality of \u03c0t for \u03b7t(\u00b7) and \u03c0? forR(\u00b7). We also use the fact that c0 \u2265 4 \u221a 2 by definition of \u03b81.\nFor the inductive step, fix some round t and assume that the claim holds for all for all t0 \u2264 t\u2032 < t and all \u03c0 \u2208 \u03a0. By the optimality of \u03c0t for \u03b7t and Lemma 6 (with our choice of \u03bbt = \u00b5t\u22121pmin/\u2016w\u20161), we have:\nReg(\u03c0)\u2212 R\u0302egt(\u03c0) = (R(\u03c0?)\u2212R(\u03c0))\u2212 (\u03b7t(\u03c0t)\u2212 \u03b7t(\u03c0)) \u2264 (R(\u03c0?)\u2212R(\u03c0))\u2212 (\u03b7t(\u03c0?)\u2212 \u03b7t(\u03c0))\n\u2264 (Vt(\u03c0?) + Vt(\u03c0)) \u2016w\u201622 \u2016w\u20161 \u00b5t\u22121pmin + 2\u2016w\u20161dt t\u00b5t\u22121pmin\nNow by Lemma 7, there exists rounds i, j < t such that:\nVt(\u03c0) \u2264 \u03b81KL pmin + \u2016w\u20161 \u2016w\u201622 R\u0302egi(\u03c0) \u03b82\u00b5ipmin 1[\u00b5i < 1/(2K)]\nVt(\u03c0?) \u2264 \u03b81KL pmin + \u2016w\u20161 \u2016w\u201622 R\u0302egj(\u03c0?) \u03b82\u00b5jpmin 1[\u00b5j < 1/(2K)]\nFor the term involving Vt(\u03c0) if \u00b5i \u2265 1/(2K) then trivially we have the bound:\nVt(\u03c0) \u2016w\u201622 \u2016w\u20161 \u00b5t\u22121pmin \u2264 \u03b81 \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121\nOn the other hand, if \u00b5i < 1/(2K) then by the applying the inductive hypothesis to R\u0302egi(\u03c0) we have:\n\u2016w\u20161 \u2016w\u201622 R\u0302egi(\u03c0) \u03b82\u00b5ipmin \u2264 \u2016w\u20161 \u2016w\u201622 2Reg(\u03c0) \u03b82\u00b5ipmin + c0KL \u03b82pmin\nVt(\u03c0) \u2016w\u201622 \u2016w\u20161 \u00b5t\u22121pmin \u2264 (\u03b81 + c0 \u03b82 )KL\u00b5t\u22121 + 2Reg(\u03c0) \u03b82\nSimilarly for the Vt(\u03c0?) term, we have the bound:\nVt(\u03c0?) \u2016w\u201622 \u2016w\u20161 \u00b5t\u22121pmin \u2264 (\u03b81 + c0 \u03b82 ) \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121 + 2 Reg(\u03c0?) \u03b82 \u2264 (\u03b81 + c0 \u03b82 ) \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121,\nsince \u03c0? has no regret. Combining these bounds gives:\nReg(\u03c0) \u2264 1 1\u2212 2\u03b82\n( R\u0302egt(\u03c0) + 2 ( \u03b81 +\nc0 \u03b82 ) \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121 + 2\u2016w\u20161dt t\u00b5t\u22121pmin ) Recall that \u03b81 = 94.1, \u03b82 = \u03c8/6.4, \u03c8 = 100, c0 = 4\u03c1(1 + \u03b81) and \u03c1 \u2264 \u221a 2. This means that \u03b82/2 \u2264 1/2 so the pre-multiplier on the R\u0302egt(\u03c0) term is at most 2. For the third term, since dt/t is non-increasing, we have theb ound dtt\u00b5t\u22121pmin \u2264 K\u00b5t\u22121 by the definition of \u00b5t\u22121. We also use the bound \u2016w\u2016 2 1 \u2264 L\u2016w\u201622. Since \u00b5t\u22121 \u2264 \u03c1\u00b5t we replace all \u00b5t\u22121 terms with \u03c1\u00b5t above. Lastly, one can verify that if \u03b82 \u2265 4\u03c1, which it is given our choice of \u03c8 = 100 and \u03c1 \u2264 \u221a 2, the pre-multiplier to the \u2016w\u2016 2 2\n\u2016w\u20161KL\u00b5t term is bounded by c0. This gives one direction of the inequality.\nThe other direction proceeds similarly to before. Under event E we have:\nR\u0302egt(\u03c0)\u2212 Reg(\u03c0) = \u03b7t(\u03c0t)\u2212 \u03b7t(\u03c0)\u2212R(\u03c0?) +R(\u03c0) \u2264 \u03b7t(\u03c0t)\u2212 \u03b7t(\u03c0)\u2212R(\u03c0t) +R(\u03c0)\n\u2264 (Vt(\u03c0) + Vt(\u03c0t)) \u2016w\u201622 \u2016w\u20161 \u00b5t\u22121pmin + 2\u2016w\u20161dt t\u00b5t\u22121pmin\nAs before, we have the bound:\nVt(\u03c0) \u2016w\u201622 \u2016w\u20161 \u00b5t\u22121pmin \u2264 (\u03b81 + c0 \u03b82 )KL\u00b5t\u22121 + 2Reg(\u03c0) \u03b82\nbut for the Vt(\u03c0t) term we must use the inductive hypothesis twice. We know there exists a round j < t for which\nVt(\u03c0t) \u2264 \u03b81 KL pmin + \u2016w\u20161 \u2016w\u201622 R\u0302egj(\u03c0) \u03b82\u00b5jpmin 1[\u00b5j < 1/(2K)].\nApplying the inductive hypothesis twice gives:\n\u2016w\u20161 \u2016w\u201622 R\u0302egj(\u03c0t) \u03b82\u00b5jpmin \u2264 \u2016w\u20161 \u2016w\u201622\n( 2Reg(\u03c0t) + c0 \u2016w\u201622 \u2016w\u20161KL\u00b5j ) \u03b82\u00b5jpmin\n\u2264 \u2016w\u20161 \u2016w\u201622\n2 (\n2R\u0302egt(\u03c0t) + c0 \u2016w\u201622 \u2016w\u20161KL\u00b5t ) + c0 \u2016w\u201622 \u2016w\u20161KL\u00b5j\n\u03b82\u00b5jpmin\n\u2264 3c0 \u03b82 KL pmin\nHere we use the inductive hypothesis twice, once at round j and once at round t and then use the fact that \u03c0t has no regret at round t, i.e. R\u0302egt(\u03c0t) = 0. We also use the fact that the \u00b5s are non-increasing so that \u00b5t/\u00b5j \u2264 1. This gives the bound:\nVt(\u03c0t) \u2016w\u201622 \u2016w\u20161 \u00b5t\u22121pmin \u2264 (\u03b81 + 3c0 \u03b82 ) \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121\nCombining the bounds for Vt(\u03c0) and Vt(\u03c0t) gives:\nR\u0302egt(\u03c0) \u2264 ( 1 + 2\n\u03b82\n) Reg(\u03c0) + ( 2\u03b81 +\n4c0 \u03b82 ) \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121 + 2\u2016w\u20161dt t\u00b5t\u22121pmin\nSince \u03b82 \u2265 2 the pre-multiplier on the first term is at most 2. As before, the third term is bounded by 2 \u2016w\u201622 \u2016w\u20161KL\u00b5t\u22121 and \u00b5t\u22121 \u2264 \u03c1\u00b5t. Then, by definition of c0, \u03b81, \u03b82, we have that \u03c1(2\u03b81 + 4c0/\u03b82 + 2) \u2264 c0 which proves the claim.\nThe last key ingredient of the proof is the following Lemma, which shows that the low-regret constraint in Equation 1, which is based on the regret estimates, actually ensures low regret.\nLemma 9. Assume event E holds. Then for every round t \u2208 N:\u2211 \u03c0\u2208\u03a0 Q\u0303t\u22121(\u03c0)Reg(\u03c0) \u2264 (4\u03c8 + c0) \u2016w\u201622 \u2016w\u20161 K\u00b5t\u22121 (10)\nProof. If t \u2264 t0 then \u00b5t\u22121 = 1/(2K) in which case (since Reg(\u03c0) \u2264 \u2016w\u20161):\u2211 \u03c0\u2208\u03a0 Q\u0303t\u22121(\u03c0)Reg(\u03c0) \u2264 \u2016w\u20161 \u2264 \u2016w\u201622 \u2016w\u20161 L \u2264 2\u03c8 \u2016w\u2016 2 2 \u2016w\u20161 KL\u00b5t\u22121,\nso the claim holds. Now for t > t0 we have:\u2211 \u03c0\u2208\u03a0 Q\u0303t\u22121Reg(\u03c0) \u2264 \u2211 \u03c0\u2208\u03a0 Q\u0303t\u22121(\u03c0) ( 2R\u0302egt\u22121(\u03c0) + c0 \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121 )\n\u2264 ( 2 \u2211 \u03c0\u2208\u03a0 Qt\u22121(\u03c0)R\u0302egt\u22121(\u03c0) ) + c0 \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121\n\u2264 (4\u03c8 + c0) \u2016w\u201622 \u2016w\u20161 KL\u00b5t\u22121\nThe first inequality follows by Lemma 8 and the second follows from the fact that Q\u0303t\u22121 places its remaining mass on \u03c0t\u22121 which suffers no empirical regret at round t \u2212 1. The last inequality is due to the low regret optimization constraint.\nTo control the regret, we must first add up the \u00b5ts, which relate to the probability of exploring. Our definition of \u00b5t differs from Agarwal et al. [2014] only in the introduction of pmin, so by a straightforward adaptation we have:\nLemma 10. For any T \u2208 N:\nT\u2211 t=1 \u00b5t \u2264 2 \u221a TdT Kpmin\nand T\u2211 t=1 \u00b5t\u22121 \u2264 t0 2K + \u221a 8TdT Kpmin\nWe are finally ready to prove the theorem by adding up the total regret for the algorithm.\nLemma 11. For any T \u2208 N, with probability at least 1\u2212 \u03b4, the regret after T rounds is at most:\n\u2016w\u201622 \u2016w\u20161 L\n[ 2 \u221a\n2T ln(2/\u03b4) + (4\u03c8 + c0 + 1) ( 2Kdt0 pmin + \u221a 8KTdT pmin )]\nProof. For each round t \u2208 N let Zt = rt(\u03c0?(xt)) \u2212 rt(At) \u2212 \u2211 \u03c0\u2208\u03a0 Q\u0303 \u00b5t\u22121 t\u22121 Reg(\u03c0). Since at round t, we play action At with probability Q\u0303 \u00b5t\u22121 t\u22121 , this sequence of random variables is clearly centered. Moreover we have |Zi| \u2264 2\u2016w\u20161 and it follows by Azuma\u2019s inequality (Lemma 19) that with probability at least 1\u2212 \u03b4/2:\nT\u2211 t=1 |Zt| \u2264 2\u2016w\u20161 \u221a 2T ln(2/\u03b4)\nTo control the mean, we use event E , which, by Theorem 4 and Lemma 6 holds with probability at least\n1\u2212 \u03b4/2. By another union bound, with probability at least 1\u2212 \u03b4, the regret of the algorithm is bounded by:\nRegret \u2264 2\u2016w\u20161 \u221a 2T ln(2/\u03b4) + T\u2211 t=1 \u2211 \u03c0\u2208\u03a0 Q\u0303 \u00b5t\u22121 t\u22121 (\u03c0)Reg(\u03c0)\n\u2264 2\u2016w\u20161 \u221a 2T ln(2/\u03b4) + T\u2211 t=1 \u2211 \u03c0\u2208\u03a0 (1\u2212K\u00b5t\u22121)Q\u0303t\u22121(\u03c0)Reg(\u03c0) + \u2016w\u20161K\u00b5t\u22121 \u2264 2\u2016w\u20161 \u221a 2T ln(2/\u03b4) +\nT\u2211 t=1 (4\u03c8 + c0 + 1) \u2016w\u201622 \u2016w\u20161 LK\u00b5t\u22121\n\u2264 \u2016w\u2016 2 2\n\u2016w\u20161 L\n[ 2 \u221a\n2T ln(2/\u03b4) + (4\u03c8 + c0 + 1) ( t0 2 + \u221a 8KTdT pmin )]\n\u2264 \u2016w\u2016 2 2\n\u2016w\u20161 L\n[ 2 \u221a\n2T ln(2/\u03b4) + (4\u03c8 + c0 + 1) ( 2Kdt0 pmin + \u221a 8KTdT pmin )] Here the first inequality is from the application of Azuma\u2019s inequality above. The second one uses the definition of Q\u0303\u00b5t\u22121t\u22121 to split into rounds where we play as Q\u0303t\u22121 and rounds where we explore. The exploration rounds occur with probability K\u00b5t\u22121, and on those rounds we suffer regret at most \u2016w\u20161. For the other rounds, we use Lemma 9 and then we use Lemma 10. We also use the identity \u2016w\u20161 \u2264 L\u2016w\u201622/\u2016w\u20161 in order to collect terms. Finally we use the fact that t0 \u2265 4Kdt0/pmin."}, {"heading": "B Full Proof for Theorem 2", "text": "In this section we prove Theorem 2, characterizing the coordinate descent optimization Algorithm 2. Recall that the potential function we use in this analysis is:\n\u03a6(Q) =\n( E\u0302[RE(UAS(x)||Q\u00b5(\u00b7|x))]\n1\u2212K\u00b5 +\n\u2211 \u03c0 Q(\u03c0)b\u03c0\n2K/pmin ) with:\nRE(p||q) = \u2211 a\u2208A pa ln(pa/qa) + qa \u2212 pa\nFor intuition, note that the partial derivative of the potential function with respect to a coordinateQ(\u03c0) relate exactly the variance V\u03c0(Q):\n\u2202\u03a6(Q) \u2202Q(\u03c0) =  1t \u2211t\u03c4=1\u2211a\u2208\u03c0(xt) \u2212pa(1\u2212K\u00b5)Q\u00b5(a|xt) + (1\u2212K\u00b5) 1\u2212K\u00b5 + b\u03c0\n2K/pmin  \u2264 ( \u2212pmin K V\u03c0(Q) + L+ pminb\u03c0 2K\n) = pmin 2K ( \u22122V\u03c0(Q) + 2KL pmin + b\u03c0\n) = pmin 2K (\u2212D\u03c0(Q)\u2212 V\u03c0(Q))\nThis means that if D\u03c0(Q) > 0, then the partial derivative is very negative, and by increasing the weight on Q, we can decrease the potential function \u03a6.\nWe establish the five facts:\n1. \u03a6(0) \u2264 L ln(1/(K\u00b5))/(1 \u2212K\u00b5). This follows by the fact that the exploration distribution in Q\u00b5 is exactly UAS(x).\n2. \u03a6(Q) is convex in Q.\n3. \u03a6(Q) \u2265 0 for all Q.\n4. The shrinking update when the regret constraint is violated does not increase the potential. More formally, for any c < 1, we have \u03a6(cQ) \u2264 \u03a6(Q) whenever \u2211 \u03c0 Q\u03c0(2KL/pmin + b\u03c0) > 2KL/pmin.\n5. The additive update when D\u03c0 > 0 for some \u03c0 lowers the potential by at least L\u00b5pmin4(1\u2212K\u00b5) .\nThe first three are fairly straightforward and the proof of the later two are based on the arguments of Agarwal et al. [2014]. For the first claim we have:\n\u03a6(0) = \u2211\na\u2208A(x)\npa ln( pa K\u00b5pa )\u2212 (1\u2212K\u00b5)pa\n1\u2212K\u00b5 \u2264 L ln(1/(K\u00b5)) 1\u2212K\u00b5\nSince the marginals pa some to at most L. Convexity of this function follows from the fact that the unnormalized relative entropy is convex in the second argument, and the fact that the marginal distribution is linear in the vector Q. The third fact follows by the non-negative of both the empirical regret b\u03c0 and of the unnormalized relative entropy RE(\u00b7||\u00b7).\nFor the fourth fact, we prove the following lemma. Lemma 12. Let Q be a weight vector for which \u2211 \u03c0 Q(\u03c0)(2KL/pmin \u2212 b\u03c0) > 2KL/pmin and define c = 2KL/pmin\u2211 \u03c0 Q(\u03c0)(2KL/pmin\u2212b\u03c0) < 1. Then \u03a6(cQ) \u2264 \u03a6(Q).\nProof. Define g(c) = B0\u03a6(cQ) and let Q\u00b5c (a|x) = (1\u2212K\u00b5)cQ(a|x) +K\u00b5Punif (a|x). By the chain rule, using the calculation of the derivative above, we have:\ng\u2032(c) = B0 \u2211 \u03c0 Q(\u03c0) \u2202\u03a6(cQ) \u2202Q(\u03c0)\n\u2265 pminB0 2K \u2211 \u03c0 Q(\u03c0) 2KL pmin + b\u03c0 \u2212 2E\u0302 \u2211 a\u2208\u03c0(x)\n1\nQ\u00b5c (a|x)  For the last term, we have:\u2211\n\u03c0\nQ(\u03c0)E\u0302 \u2211\na\u2208\u03c0(x)\n1\nQ\u00b5c (a|x) = E\u0302 \u2211 a\u2208A \u2211 \u03c0\u2208\u03a0 Q(\u03c0)1[a \u2208 \u03c0(x)] Q\u00b5c (a|x)\n= E\u0302 \u2211 a\u2208A Q(a|x) Q\u00b5c (a|x) = 1 c E\u0302 \u2211 a\u2208A cQ(a|x) Q\u00b5c (a|x)\nNow define qa = cQ(a|x) and the inner sum can be upper bounded by: \u2264 \u2211 a\u2208A qa (1\u2212K\u00b5)qa + \u00b5pmin = K 1 K \u2211 a\u2208A 1 (1\u2212K\u00b5) + \u00b5pmin/qa\n\u2264 K 1 (1\u2212K\u00b5) + K\u00b5pmin\u2211\na qa\n\u2264 K 1 (1\u2212K\u00b5) + K\u00b5pminL\n= KL\npmin 1 L pmin (1\u2212K\u00b5) +K\u00b5 \u2264 KL pmin\nThe first inequality uses the lower bound pmin/K for exploration distribution, then we use Jensen\u2019s inequality and the fact that \u2211 a qa \u2264 L since c < 1. Finally, we use the fact that L/pmin \u2265 1 and K\u00b5 \u2264 1 so that the first term in the denominator is \u2265 1\u2212K\u00b5. Plugging this in above we have:\n\u03a6(c) \u2265 pminB0 2K (\u2211 \u03c0 Q\u03c0 ( 2KL pmin + b\u03c0 ) \u2212 2KL pmin ) > 0\nBy the condition in the algorithm. Since g is convex, this means that g(c) is nondecreasing for all values exceeding c. Since c < 1, we have:\nB0\u03a6(Q) = g(1) \u2265 g(c) = B0\u03a6(cQ) So any positive B0 is fine.\nAnd for the fifth fact, we have:\nLemma 13. Let Q denote a set of weights and suppose, for some policy \u03c0, that D\u03c0(Q) > 0. Let Q\u2032 be the new set of weights which is identical except that Q\u2032(\u03c0) = Q(\u03c0) + \u03b1 with \u03b1 = \u03b1\u03c0(Q) > 0. Then\n\u03a6(Q)\u2212 \u03a6(Q\u2032) \u2265 \u00b5Lpmin 4(1\u2212K\u00b5)\nProof. Let Q\u2032(\u00b7) = Q(\u00b7) + \u03b1\u00b7 = \u03c0, i.e. the update we perform when \u03c0 is found to violate the inequality in the algorithm. Since Q\u2032\u00b5(a|x) = Q\u00b5(a|x) + (1\u2212K\u00b5)\u03b11[a \u2208 \u03c0(x)] only updates a few coordinates of the marginal probabilities, we have by a direct calculation:\n2K(\u03a6(Q)\u2212 \u03a6(Q\u2032)) = 2K\n( E\u0302 \u2211 a pa ln(pa/q \u00b5 a )\u2212 pa ln(pa/q\u2032\u00b5a ) + q\u00b5a \u2212 q\u2032\u00b5a\n(1\u2212K\u00b5) \u2212 \u03b1b\u03c0pmin 2K\n)\n= 2K\n1\u2212K\u00b5 E\u0302 \u2211 a\u2208\u03c0(x) pa ln ( q\u2032\u00b5a q\u00b5a )\u2212 \u03b1(2KL+ b\u03c0pmin) \u2265 2pmin\n1\u2212K\u00b5 E\u0302 \u2211 a\u2208\u03c0(x) ln ( 1 + \u03b1(1\u2212K\u00b5) Q\u00b5(a|x) )\u2212 pmin\u03b1(2KL pmin + b\u03c0 ) The term inside the expectation can be bounded using the fact that ln(1 + x) \u2265 x\u2212 x2/2:\nE\u0302 \u2211\na\u2208\u03c0(x)\nln ( 1 +\n\u03b1(1\u2212K\u00b5) Q\u00b5(a|x)\n) \u2265 E\u0302 \u2211 a\u2208\u03c0(x) \u03b1(1\u2212K\u00b5) Q\u00b5(a|x) \u2212 1 2 ( \u03b1(1\u2212K\u00b5) Q\u00b5(a|x) )2 = \u03b1(1\u2212K\u00b5)V\u03c0(Q)\u2212 \u03b12(1\u2212K\u00b5)2\n2 S\u03c0(Q)\nPlugging this in above gives a lower bound:\n2K(\u03a6(Q)\u2212 \u03a6(Q\u2032)) \u2265 2pmin\u03b1V\u03c0(Q)\u2212 (1\u2212K\u00b5)pmin\u03b12S\u03c0(Q)\u2212 pmin\u03b1( 2KL\npmin + b\u03c0)\n= pmin\u03b1(V\u03c0(Q) +D\u03c0(Q))\u2212 (1\u2212K\u00b5)pmin\u03b12S\u03c0(Q)\nUsing the definition D\u03c0(Q) = V\u03c0(Q)\u2212 2KLpmin \u2212 b\u03c0 . Now we set \u03b1 = (V\u03c0(Q)+D\u03c0(Q)) 2(1\u2212K\u00b5)S\u03c0(Q) as in the algorithm and obtain:\n2K(\u03a6(Q)\u2212 \u03a6(Q\u2032)) \u2265 pmin(V\u03c0(Q) +D\u03c0(Q)) 2\n4(1\u2212K\u00b5)S\u03c0(Q)\nNote that S\u03c0(Q) \u2265 1\u00b5pminV\u03c0(Q) (by bounding one of the terms in the square by the range which is \u00b5pmin) and that V\u03c0(Q) > 2KLpmin since D\u03c0(Q) > 0. this gives:\n2K(\u03a6(Q)\u2212 \u03a6(Q\u2032)) \u2265 \u00b5p 2 min(V\u03c0(Q) +D\u03c0(Q)) 2\n4(1\u2212K\u00b5)V\u03c0(Q)\n\u2265 \u00b5p 2 minV\u03c0(Q) 4(1\u2212K\u00b5) \u2265 KL\u00b5pmin 2(1\u2212K\u00b5)\nDividing both sides of this inequality by 2K proves the lemma."}, {"heading": "C Full Proof of Theorem 3", "text": "The proof of the theorem is based on trading off the regret associated with exploration and exploitation rounds. The first ingredient of the proof is a decomposition of the exploitation regret. For any policy \u03c0 we first bound the deviation from our estimate of the reward to the true reward.\nLemma 14. For all policies \u03c0 \u2208 \u03a0, for any \u03b4 \u2208 (0, 1), if we have explored for n \u2265 ln(2|\u03a0|/\u03b4) rounds, then with probability \u2265 1\u2212 \u03b4:\n|\u03b7n(\u03c0, w\u0302n)\u2212R(\u03c0)| \u2264 K\u221a L \u2016w\u0302t \u2212 w\u20162 + 2K\u2016w\u20162\n\u221a ln(2|\u03a0|/\u03b4)\nLn\nProof. Start with the decomposition:\n|\u03b7n(\u03c0, w\u0302n)\u2212R(\u03c0,w)| = \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 y\u0302n(\u03c0(xi)) T w\u0302 \u2212 Ex,yy(\u03c0(x))Tw \u2223\u2223\u2223\u2223\u2223 \u2264\n \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 y\u0302i(\u03c0(xi)) T w\u0302 \u2212 1 n n\u2211 i=1 y\u0302i(\u03c0(xi)) Tw \u2223\u2223\u2223\u2223\u2223+\u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 y\u0302i(\u03c0(xi)) Tw \u2212 Ex,yy(\u03c0(x))Tw \u2223\u2223\u2223\u2223\u2223\nFor the first term, since the features are bounded between [0, 1] and the minimum probability for any action is \u2265 L/K, by the Cauchy-Schwarz inequality, this term can be bounded by:\u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 y\u0302i(\u03c0(xi)) T w\u0302 \u2212 1 n n\u2211 i=1 y\u0302i(\u03c0(xi)) Tw \u2223\u2223\u2223\u2223\u2223 \u2264 K\u221aL\u2016w\u0302 \u2212 w\u20162 The second term can be controlled by a deviation bound similar to Lemma 6 above. For all policies \u03c0 \u2208 \u03a0, and for any \u03b4 \u2208 (0, 1), if \u03bb \u2208 [0, LK\u2016w\u20161 ] then with probability \u2265 1\u2212 \u03b4, we have:\u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 y\u0302i(\u03c0(xi)) Tw \u2212 Ex,yy(\u03c0(x))Tw\n\u2223\u2223\u2223\u2223\u2223 \u2264 K2\u2016w\u201622\u03bbL + ln(2|\u03a0|/\u03b4)n\u03bbt Since the minimum probability is at least L/K, the range term is bounded by \u2016w\u20161K/L and the variance is bounded by \u2016w\u201622K2/L. The result follows now by choosing \u03bb = 1K\u2016w\u20162 \u221a L ln(2|\u03a0|/\u03b4)\nn which is a valid setting when n \u2265 ln(2|\u03a0|/\u03b4).\nTo control the regret associated with the exploitation rounds, we also need to bound \u2016w\u0302 \u2212 w\u20162 which follows from a standard analysis of linear regression.\nLemma 15. Let \u03a3 denote the feature covariance matrix after the exploration phase. There is a universal constant c > 0 such that for any \u03b4 \u2208 (0, 2/e), with probability \u2265 1\u2212 \u03b4:\n\u2016w\u0302 \u2212 w\u20162\u03a3 \u2264 cL ln(2/\u03b4)\nProof. This lemma is just the standard analysis of fixed-design linear regression with bounded noise. By definition of the ordinary least squares estimator, we have w\u0302 = \u03a3\u22121Y T r where Y \u2208 Rn\u00d7L is the matrix of features, r \u2208 Rn is the responses and \u03a3 = Y TY is the feature covariance. The true weight vector can be written as w = \u03a3\u22121Y T (r \u2212 \u03be) where \u03be \u2208 Rn is the noise vector. Thus:\n\u2016w\u0302 \u2212 w\u20162\u03a3 = \u2016\u03a3\u22121Y T \u03be\u20162\u03a3 = \u03beTY \u03a3\u22121Y T \u03be.\nSince \u03a3\u22121 = (Y TY )\u22121 this matrix in the middle is a projection matrix, and it can be written as UUT where U \u2208 Rn\u00d7d. We now have to bound the term \u2016UT \u03be\u201622. Note that the vector \u03be is a subgaussian random vector with independent components, so we can apply subgaussian tail bounds. Specifically, Lemma 20, due to Rudelson and Vershynin [2013], reveals that with probablity \u2265 1\u2212 \u03b4:\n\u2016UT \u03be\u20162 \u2264 \u221a L+ \u221a c ln(2/\u03b4)\nfor some universal constant c > 0. Squaring this inequality and using the upper bound on \u03b4 leads to the claim.\nSince we only exploit after the exploration phase, we know that for any exploitation round \u03bbmin(\u03a3) \u2265 \u03bb?. This immediately gives a `2 bound on the weight vector estimate and consequently a bound on the exploitation regret. Setting both failure probabilities to be \u03b4/4, as we will take a union bound over four total events in this proof, we have:\n\u2016w\u0302 \u2212 w\u201622 \u2264 \u2016\u03a3\u22121\u20162\u2016w\u0302 \u2212 w\u20162\u03a3 \u2264 cL ln(8/\u03b4)\n\u03bb?\nR(\u03c0\u0302)\u2212R(\u03c0?) \u2264 4K\u2016w\u20162\n\u221a ln(8|\u03a0|/\u03b4)\nLn + 2K\n\u221a c ln(8/\u03b4)\n\u03bb?\nBy a union bound over the two events here, the last bound holds with probability at least 1\u2212 \u03b4/2. Since we explore for at least nT rounds, this means that the total exploitation regret is at most:\nExploitation Regret \u2264 4TK\u2016w\u20162 \u221a ln(8|\u03a0|/\u03b4) LnT + 2TK \u221a c ln(8/\u03b4) \u03bb?\nThe more challenging part of the analysis is to bound the exploration regret. Since the length of the exploration phase is governed by \u03bbmin(\u03a3), we need to understand the spectral properties of this matrix in order to control the exploration regret. In the following lemma, we establish a lower bound on the smallest eigenvalue of precisely this matrix.\nLemma 16. Fix \u03b4 \u2208 (0, 1). Let t? denote the last round for which \u03bbmin(\u03a3) \u2264 \u03bb?. For any t \u2264 t?, let \u03a3t denote the feature covariance matrix at time t, with probability at least 1\u2212 \u03b4:\n\u03a3t\n( 2\nt\u2211 \u03c4=1 Var(y\u03c4 )\u2212 4L \u221a t ln(2Lt2/\u03b4)\n) IL\nProof. The lemma follows from an application of the Matrix Bernstein inequality (Lemma 21) and an analysis of the mean of \u03a3t, where the randomness is over the actions taken by the algorithm. To characterize the mean, consider a fixed exploration round \u03c4 with context x\u03c4 and let K\u03c4 = |A(x\u03c4 )| denote the size of the feasible set of simple actions. Let S\u03c4 = EA\u223cP [y\u03c4 (A)y\u03c4 (A)T ] \u2208 RL\u00d7L be the mean matrix for that round. We have:\nzTS\u03c4z = L\u2211 l=1 z2l \u2211\na\u2208A(x\u03c4 )\n1\nK\u03c4 y(a)2 + \u2211 l 6=l\u2032 zlzl\u2032 \u2211\na 6=a\u2032\u2208A(x\u03c4 )\ny(a)y(a\u2032)\nK\u03c4 (K\u03c4 \u2212 1)\n= \u2016y\u201622\u2016z\u201622 K\u03c4 + \u2211 l 6=l\u2032 zlz \u2032 l \u2211 a,a\u2032\u2208A(x\u03c4 ) y(a)y(a\u2032) K\u03c4 (K\u03c4 \u2212 1) \u2212 \u2211 l 6=l\u2032 zlz \u2032 l \u2211 a\ny(a)2\nK\u03c4 (K\u03c4 \u2212 1)\nDefine y\u0304 = 1K\u03c4 \u2211 a\u2208A(x\u03c4 ) y(a), E(y 2) = 1K\u03c4 \u2211 a\u2208A(x\u03c4 ) y(a)\n2 and Var(y) = E(y2) \u2212 y\u03042. This expression becomes:\nzTS\u03c4z = E[y2]\u2016z\u201622 + \u2211 l 6=l\u2032 zlzl\u2032 ( K\u03c4 K\u03c4 \u2212 1 y\u03042 \u2212 1 K\u03c4 \u2212 1 Ey2 )\n= K\u03c4\nK\u03c4 \u2212 1 Var(y)\u2016z\u201622 +\n( K\u03c4\nK\u03c4 \u2212 1 y\u03042 \u2212 1 K\u03c4 \u2212 1 Ey2\n) (zT1)2\nThe second term is positive for any z, as long as the y vector is non-negative, as it is in our case. This means that for any unit vector z, we have:\nzTS\u03c4z \u2265 K\u03c4\nK\u03c4 \u2212 1 Var(y) \u2265 2 Var(y)\nSince if K\u03c4 < 2 we only have one simple action available, so there are no feasible composite actions. Now we can apply the Matrix Hoeffding inequality. Let z\u03c4 = y\u03c4 (At) \u2208 RL be the realization of the feature vector on round \u03c4 . Define the random matrices:\nZ\u03c4 = z\u03c4z T \u03c4 \u2212 EA\u223cP [y\u03c4 (A)y\u03c4 (A)T ]\nNote that we are only considering the randomization of the actions A \u223c P and our bound leaves the dependence on nature\u2019s randomness (i.e. the y\u03c4 ). It is easy to see that \u2016Z\u03c4\u20162 \u2264 2L trivially, since y, z \u2208 [0, 1], so that the scaling term in the concentration bound is \u03c32 \u2264 2tL2. The Matrix Hoeffding Inequality reveals that for all rounds t \u2208 N with probability at least 1\u2212 \u03b4 we have:\n\u2016 t\u2211\n\u03c4=1\nZ\u03c4\u20162 \u2264 4L \u221a t ln(2t2L/\u03b4)\nCombining with our lower bound on the mean for these matrices, we obtain the theorem.\nThe regret associated with exploration can be bounded as follows:\nLemma 17. Let t? denote the last round for which \u03bbmin(\u03a3) \u2264 \u03bb? and fix \u03b4 \u2208 (0, 1). Then with probability at least 1\u2212 \u03b4 over the randomness of the actions of the algorithm, the regret up to round t? \u2212 1 is bounded by:\nt?\u22121\u2211 t=1 rt(\u03c0?(xt))\u2212 rt(At) \u2264 \u2016w\u20162 t?\u22121\u2211 t=1\n\u221a K Varp(yt)\nL + \u2016w\u20161\n\u221a 2(t? \u2212 1) ln(2/\u03b4)\nProof. Let us first characterize the expected regret, where expectation is just over the actions of the algorithm. Assume without loss of generality that \u03c0? plays A? = (\u03b11, . . . , \u03b1L). Let y\u0304tl = Ea\u223cplyt(a) for each l and let y\u0304t = (y\u0304t1, . . . , y\u0304tL)T . By the Cauchy-Schwarz inequality:\nrt(\u03c0?(xt))\u2212 EA\u223cP rt(A) = EA\u223cP L\u2211 l=1 wl(yt(\u03b1l)\u2212 yt(al))\n= wT (yt(\u03c0?(xt))\u2212 y\u0304t) \u2264 \u2016w\u2016 \u221a\u221a\u221a\u221a L\u2211 l=1 (yt(\u03b1l)\u2212 y\u0304tl)2\nBy Holder\u2019s inequality:\u221a\u221a\u221a\u221a L\u2211 l=1 (yt(\u03b1l)\u2212 y\u0304tl)2 \u2264 \u221a\u221a\u221a\u221a( L\u2211 l=1 p(\u03b1l)(yt(\u03b1l)\u2212 y\u0304tl)2 )( max l 1 p(\u03b1l) ) \u2264 \u221a K Var(yt) L ,\nsince each action appears with probability at least L/K. This gives:\nt?\u22121\u2211 t=1 rt(\u03c0?(xt))\u2212 EA\u223cP rt(A) \u2264 \u2016w\u20162 t?\u22121\u2211 t=1\n\u221a K Var(yt)\nL\nTo account for the randomness in the algorithm, notice that the random variables rt(At)\u2212 EA\u223cP rt(A) are bounded by 2\u2016w\u20161 and centered, so we can apply Hoeffding\u2019s inequality. Specifically, with probability at least 1\u2212 \u03b4, we have: \u2223\u2223\u2223\u2223\u2223 t?\u22121\u2211 t=1 rt(At)\u2212 EA\u223cP rt(A)\n\u2223\u2223\u2223\u2223\u2223 \u2264 \u2016w\u20161\u221a2(t? \u2212 1) ln(2/\u03b4) Combining this deviation bound with the expected regret bound above proves the theorem.\nThe insight is that the exploration regret is related to the exploration stopping condition. In particular, the exploration regret associated with the rounds up until t? can be bounded as:\nt?\u22121\u2211 t=1 rt(\u03c0?(xt))\u2212 EA\u223cP rt(A)\n\u2264 \u2016w\u20162\n\u221a K\nL t?\u22121\u2211 t=1 \u221a Var(yt) + \u2016w\u20161 ( 1 + \u221a 2(t? \u2212 1) ln(8/\u03b4) )\n\u2264 \u2016w\u20162 \u221a Kt? L \u221a\u221a\u221a\u221a t?\u2211 t=1 Var(yt) + \u2016w\u20161 ( 1 + \u221a 2t? ln(8/\u03b4) )\n\u2264 \u2016w\u20162 \u221a Kt? L \u221a 1 2 \u03bb? + 2L \u221a t? ln(8Lt2?/\u03b4) + \u2016w\u20161 ( 1 + \u221a 2t? ln(8/\u03b4) ) \u2264 \u2016w\u20162 \u221a KT\u03bb?\n2L + \u2016w\u20162T 3/4\n\u221a 2K ln1/4(8LT 2/\u03b4) + \u2016w\u20161 ( 1 + \u221a 2T ln(8/\u03b4) ) By allocating failure probability \u03b4/4 to each of the two events above, this holds with probability \u2265 1\u2212 \u03b4/2. Here we had to account for the fact that our upper bound on \u03bbmin(\u03a3) \u2264 \u03bb? is only valid up to round t? \u2212 1, but the regret in any round, in particular round t?, can be bounded by \u2016w\u20161. The exploration regret associated with the rounds up until nT can be trivially bounded by nT \u2016w\u20161. This means that the total exploration regret is at most:\nExploration Regret \u2264 \u2016w\u20161 ( 2nT + \u221a 2T ln(8/\u03b4) ) + \u2016w\u20162 (\u221a KT\u03bb?\n2L + T 3/4\n\u221a 2K ln1/4(8LT 2/\u03b4) ) We now have to select both the thresholds \u03bb? and nT . The terms involving \u03bb? are:\nmin \u03bb? \u2016w\u20162\u221aKT\u03bb? 2L + 2TK \u221a c ln(8/\u03b4) \u03bb?  \u2264 3(KT )3/4\u221ac\u2016w\u20162 4\u221a ln(8/\u03b4) L\nAnd this bound is achieved when we set \u03bb? = \u221a KLT log(8/\u03b4)\n\u2016w\u20162 . Optimizing over nT reveals that we should select nT = ( TK\u2016w\u20162 \u2016w\u20161 )2/3 (ln(8|\u03a0|/\u03b4)/L)1/3 and gives the bound:\nmin nT\n2nT \u2016w\u20161 + 4TK\u2016w\u20162 \u221a\nln(8|\u03a0|/\u03b4) LnT\n \u2264 6(TK)2/3\u2016w\u20162/32 3 \u221a \u2016w\u20161 ln(4|\u03a0|/\u03b4)\nL\nWith both of these bounds, the total regret is at most:\nRegret \u2264  6(TK)2/3\u2016w\u20162/32 3 \u221a \u2016w\u20161 ln(4|\u03a0|/\u03b4) L + 3(KT )3/4 \u221a c\u2016w\u20162 4 \u221a ln(8/\u03b4) L\n+ \u2016w\u20161 \u221a 2T ln(8/\u03b4) + \u2016w\u20162T 3/4 \u221a 2K ln1/4(8LT 2/\u03b4)\n= O ( \u2016w\u20161(KT )3/4 \u221a ln(LT 2|\u03a0|/\u03b4) ) .\nThis bound holds with probability at least 1\u2212 \u03b4 since both the exploration regret and the exploitation regret bounds each hold with probability at least 1\u2212 \u03b4/2."}, {"heading": "D Deviation Bounds", "text": "We collect here several deviation bounds that we use in our proofs. All of these results are well known and we point to references rather than provide the proofs. The first inequality, which is a Bernstein-type deviation bound for martingales, is Freedman\u2019s inequality, which is from Beygelzimer et al. [2011]\nLemma 18 (Freedman\u2019s Inequality). Let X1, X2, . . . , XT be a sequence of real-valued random variables. Assume for all t \u2208 {1, 2, . . . , T} that Xt \u2264 R and E[Xt|X1, . . . , Xt\u22121] = 0. Define S = \u2211T t=1Xt and\nV = \u2211T t=1 E[X2t |X1, . . . , Xt\u22121]. For any \u03b4 \u2208 (0, 1) and \u03bb \u2208 [0, 1/R], with probability at least 1\u2212 \u03b4:\nS \u2264 (e\u2212 2)\u03bbV + ln(1/\u03b4) \u03bb\nWe also use Azuma\u2019s inequality, a Hoeffding-type inequality for martingales.\nLemma 19 (Azuma\u2019s Inequality). Let X1, X2, . . . , XT be a sequence of real-valued random variables. Assume for all t \u2208 {1, 2, . . . , T} that Xt \u2264 R and E[Xt|X1, . . . , Xt\u22121] = 0. Define S = \u2211T t=1Xt. For any \u03b4 \u2208 (0, 1) and \u03bb \u2208 [0, 1/R], with probability at least 1\u2212 \u03b4:\nS \u2264 R \u221a 2T ln(1/\u03b4)\nWe also make use of a vector-valued version of Hoeffding\u2019s inequality, due to Rudelson and Vershynin [2013].\nLemma 20 (Vector-valued subgaussian concentration). Let A \u2208 Rm\u00d7n be a fixed matrix, and let X = (X1, . . . , Xn) be independent random variables with EXi = 0 and |Xi| \u2264 1 almost surely. Then there is a universal constant c > 0 such that, for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4:\n\u2016AX\u20162 \u2212 \u2016A\u2016F \u2264 \u2016A\u20162 \u221a c ln(1/\u03b4)\nFinally, we use a well known matrix-valued version of Hoeffding\u2019s inequality, for example from Tropp [2011].\nLemma 21 (Matrix-Hoeffding). Consider a finite sequence {Xk} of independent random, self-adjoint, matrices with dimension d, and let {Ak} be a sequence of fixed self-adjoint matrices. Assume that for each random matrix, we have EXk = 0 and X2k A2k almost surely. Then for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4:\n\u03bbmax( \u2211 k Xk) \u2264 \u221a 8\u03c32 ln(d/\u03b4) with \u03c32 = \u2016 \u2211 k A2k\u20162"}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E Schapire"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Minimax policies for combinatorial prediction", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "games. arXiv:1105.4871,", "citeRegEx": "Audibert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2011}, {"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2014}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E Schapire"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2011}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L\u00e9on Bottou", "Jonas Peters", "Joaquin Qui\u00f1onero-Candela", "Denis Xavier Charles", "D. Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Simard", "Ed Snelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Combinatorial pure exploration of multi-armed bandits", "author": ["Shouyuan Chen", "Tian Lin", "Irwin King", "Michael R Lyu", "Wei Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Combinatorial multi-armed bandit: General framework and applications", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Contextual bandits with linear payoff functions", "author": ["Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E Schapire"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Chu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2011}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dud\u0131\u0301k", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "In Uncertainty and Artificial Intelligence,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Parametric bandits: The generalized linear case", "author": ["Sarah Filippi", "Olivier Cappe", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "The on-line shortest path problem under partial monitoring", "author": ["Andr\u00e1s Gy\u00f6rgy", "Tam\u00e1s Linder", "G\u00e1bor Lugosi", "Gy\u00f6rgy Ottucs\u00e1k"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gy\u00f6rgy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gy\u00f6rgy et al\\.", "year": 2007}, {"title": "Non-stochastic bandit slate problems", "author": ["Satyen Kale", "Lev Reyzin", "Robert E Schapire"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kale et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kale et al\\.", "year": 2010}, {"title": "Matroid bandits: Fast combinatorial optimization with learning", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Hoda Eydgahi", "Brian Eriksson"], "venue": "In Uncertainty and Artificial Intelligence,", "citeRegEx": "Kveton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2014}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesv\u00e1ri"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Kveton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2015}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Langford and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E. Schapire"], "venue": "In International Conference on World Wide Web,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Contextual combinatorial bandit and its application on diversified online recommendation", "author": ["Lijing Qin", "Shouyuan Chen", "Xiaoyan Zhu"], "venue": "In SIAM International Conference on Data Mining,", "citeRegEx": "Qin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qin et al\\.", "year": 2014}, {"title": "The analysis of randomized and nonrandomized AIDS treatment trials using a new approach to causal inference in longitudinal studies", "author": ["J.M. Robins"], "venue": "In Health Service Research Methodology: A Focus on AIDS,", "citeRegEx": "Robins.,? \\Q1989\\E", "shortCiteRegEx": "Robins.", "year": 1989}, {"title": "Hanson-wright inequality and sub-gaussian concentration", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": null, "citeRegEx": "Rudelson and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2013}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong and Tsitsiklis.", "year": 2010}, {"title": "User-Friendly Tail Bounds for Sums of Random Matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Tropp.", "year": 2011}, {"title": "Algorithms for adversarial bandit problems with multiple plays", "author": ["Taishi Uchiya", "Atsuyoshi Nakamura", "Mineichi Kudo"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Uchiya et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Uchiya et al\\.", "year": 2010}, {"title": "The proof of this theorem is similar in spirit to a related theorem", "author": ["Agarwal"], "venue": null, "citeRegEx": "Agarwal,? \\Q2014\\E", "shortCiteRegEx": "Agarwal", "year": 2014}, {"title": "The first three are fairly straightforward and the proof of the later two are based on the arguments", "author": ["Agarwal"], "venue": null, "citeRegEx": "Agarwal,? \\Q2014\\E", "shortCiteRegEx": "Agarwal", "year": 2014}, {"title": "Freedman\u2019s Inequality)", "author": ["Beygelzimer"], "venue": "Let X1,", "citeRegEx": "Beygelzimer,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "If the linear transformation is known, we analyze an algorithm that is structurally similar to the algorithm of Agarwal et al. [2014] and show that it enjoys a regret bound between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), where K is the number of actions, L is the length of each action sequence, T is the number of rounds, and N is the number of policies.", "startOffset": 112, "endOffset": 134}, {"referenceID": 6, "context": "Learning from partial feedback (\u201cbandit\u201d feedback) is of great practical importance and has seen a recent surge of research interest [Bubeck and Cesa-Bianchi, 2012].", "startOffset": 133, "endOffset": 164}, {"referenceID": 19, "context": "Motivating examples include healthcare [Robins, 1989] \u2013 where we only observe the result of the treatment prescribed to the patient, but obtain no information about how other treatments would have worked \u2013 or Internet applications [Li et al.", "startOffset": 39, "endOffset": 53}, {"referenceID": 2, "context": "When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 13, "context": ", 2014] or slate bandits [Kale et al., 2010] in the literature.", "startOffset": 25, "endOffset": 44}, {"referenceID": 0, "context": "When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al., 2010] in the literature. Our goal is to design learning algorithms whose running time and statistical performance (measured by regret) scale with the number of simple actions rather than the number of composite actions. In the first part of the paper, we assume that the linear relationship between the reward and the feedback on the simple actions is known, and we derive a new algorithm for contextual semi-bandits that meets our goal. Our approach builds on the recent contextual bandit algorithms of Dud\u0131\u0301k et al. [2011] and Agarwal et al.", "startOffset": 88, "endOffset": 667}, {"referenceID": 0, "context": "[2011] and Agarwal et al. [2014] and enjoys a regret guarantee between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2.", "startOffset": 11, "endOffset": 33}, {"referenceID": 0, "context": "[2011] and Agarwal et al. [2014] and enjoys a regret guarantee between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2. The policy class \u03a0 is a set of functions mapping contexts into composite actions (e.g., linear learners, decision trees, or neural nets), which we access via an optimization oracle. We show that the algorithm makes \u00d5(T ) calls to the optimization oracle3, meaning that, given an efficient supervised learning algorithm, the algorithm has running time that is only logarithmic in |\u03a0|. This contrasts with the work of Kale et al. [2010] on contextual semi-bandits, which explicitly enumerates the policy class, and therefore has running time that is linear in |\u03a0|.", "startOffset": 11, "endOffset": 787}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al.", "startOffset": 8, "endOffset": 353}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting.", "startOffset": 8, "endOffset": 375}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al.", "startOffset": 8, "endOffset": 427}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a \u00d5( \u221a KLT ) regret but require explicit enumeration of the policy class.", "startOffset": 8, "endOffset": 481}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a \u00d5( \u221a KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al.", "startOffset": 8, "endOffset": 617}, {"referenceID": 0, "context": ", 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a \u00d5( \u221a KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al. [2011] to semi-bandits, imposing the assumption that the feedback on the simple actions is linearly related 2Extension to VC classes is straightforward using standard arguments.", "startOffset": 8, "endOffset": 678}, {"referenceID": 0, "context": "3The dependence can be improved to \u00d5(T 1/2) using warm-start and epoching ideas identical to Agarwal et al. [2014].", "startOffset": 93, "endOffset": 115}, {"referenceID": 16, "context": "Except for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions.", "startOffset": 23, "endOffset": 41}, {"referenceID": 16, "context": "Except for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions. Qin et al. [2014] generalize this slightly by assuming that the reward is a known function of the context and features.", "startOffset": 23, "endOffset": 189}, {"referenceID": 0, "context": "When the weights in the linear transformation are known, we propose an algorithm that has a similar structure to a recent algorithm for the classical contextual bandit problem [Agarwal et al., 2014].", "startOffset": 176, "endOffset": 198}, {"referenceID": 0, "context": "The main differences between SEMIBANDIT-VCEE and the algorithm of Agarwal et al. [2014] are in the OP and the definitions.", "startOffset": 66, "endOffset": 88}, {"referenceID": 13, "context": "In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting.", "startOffset": 65, "endOffset": 84}, {"referenceID": 13, "context": "In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting. Specifically, they assume that weights w = 1 and that uniform exploration is possible, and they obtain an \u00d5( \u221a KLT ln |\u03a0|) regret bound. Theorem 1 matches this bound, as \u2016w\u20162 = \u2016w\u20161 = L and pmin = L in this case. Our result improves on theirs in two directions: statistically we show how a non-uniform weight vector and restricted exploration distribution affects the regret and, computationally, our algorithm can be efficiently implemented with an optimization oracle while theirs cannot. When uniform exploration is not allowed, as considered by Kveton et al. [2015] in the non-contextual setting, we can set pmin = 1 and our bound is worse than theirs by a factor of \u221a L.", "startOffset": 65, "endOffset": 720}, {"referenceID": 0, "context": "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2).", "startOffset": 43, "endOffset": 65}, {"referenceID": 0, "context": "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter \u03bc, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u03bc)) \u03bcpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.", "startOffset": 43, "endOffset": 1062}, {"referenceID": 0, "context": "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter \u03bc, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u03bc)) \u03bcpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as \u00d5 ( T 3/2 \u221a K pmin log(|\u03a0|/\u03b4) ) by the setting of \u03bct. Moreover, due to the nature of the coordinate descent algorithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |\u03a0|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over \u2206|\u03a0|.", "startOffset": 43, "endOffset": 1591}, {"referenceID": 0, "context": "This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter \u03bc, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u03bc)) \u03bcpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as \u00d5 ( T 3/2 \u221a K pmin log(|\u03a0|/\u03b4) ) by the setting of \u03bct. Moreover, due to the nature of the coordinate descent algorithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |\u03a0|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over \u2206|\u03a0|. We mention in passing that Agarwal et al. [2014] also develop two improvements that lead to a more efficient algorithm.", "startOffset": 43, "endOffset": 1689}, {"referenceID": 0, "context": "These are the analogs of Lemmas 6 and 7 in Agarwal et al. [2014]. The proof of the first is based on showing that the derivative of the function g(c) = \u03a6(cQ) is positive so that by convexity of \u03a6, shrinking the weights Q can only decrease the potential.", "startOffset": 43, "endOffset": 65}, {"referenceID": 16, "context": "The algorithm requires knowledge of the time horizon T , which can be relaxed by variants of the Epoch-Greedy [Langford and Zhang, 2008] or -greedy approaches, although the analysis here is significantly simpler.", "startOffset": 110, "endOffset": 136}, {"referenceID": 15, "context": "When the weights are known, can we obtain \u00d5( \u221a KLT ) regret even when the set of feasible actions are constrained, rather than \u00d5(L \u221a KT ) regret as in Theorem 1? The work on non-contextual combinatorial bandits suggests that the answer is yes [Kveton et al., 2015], but all algorithms for contextual bandit learning involve some degree of uniform exploration, which would prohibit such a regret bound.", "startOffset": 243, "endOffset": 264}, {"referenceID": 0, "context": "If this linear relationship is known, we showed that an adaptation of the algorithm of Agarwal et al. [2014] achieves regret between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), and can be implemented efficiently with access to an optimization oracle.", "startOffset": 87, "endOffset": 109}, {"referenceID": 0, "context": "The proof of this theorem is similar in spirit to a related theorem in Agarwal et al. [2014]. We first use Freedman\u2019s inequality (Lemma 18) to argue that for a fixed P, \u03c0, \u03bc, and t, the empirical version of the variance is close to the true variance.", "startOffset": 71, "endOffset": 93}, {"referenceID": 10, "context": "To prove the variance deviation bound, we next use a discretization lemma from Dud\u0131\u0301k et al. [2011], which immediately implies that for any P , there exists a distribution P \u2032 supported on at most Nt policies such that for ct > 0, if Nt \u2265 6 \u03b32 t \u03bctpmin :", "startOffset": 79, "endOffset": 100}, {"referenceID": 0, "context": "Our definition of \u03bct differs from Agarwal et al. [2014] only in the introduction of pmin, so by a straightforward adaptation we have: Lemma 10.", "startOffset": 34, "endOffset": 56}, {"referenceID": 0, "context": "The first three are fairly straightforward and the proof of the later two are based on the arguments of Agarwal et al. [2014]. For the first claim we have:", "startOffset": 104, "endOffset": 126}, {"referenceID": 20, "context": "Specifically, Lemma 20, due to Rudelson and Vershynin [2013], reveals that with probablity \u2265 1\u2212 \u03b4: \u2016U \u03be\u20162 \u2264 \u221a L+ \u221a c ln(2/\u03b4)", "startOffset": 31, "endOffset": 61}, {"referenceID": 4, "context": "The first inequality, which is a Bernstein-type deviation bound for martingales, is Freedman\u2019s inequality, which is from Beygelzimer et al. [2011] Lemma 18 (Freedman\u2019s Inequality).", "startOffset": 121, "endOffset": 147}, {"referenceID": 20, "context": "We also make use of a vector-valued version of Hoeffding\u2019s inequality, due to Rudelson and Vershynin [2013]. Lemma 20 (Vector-valued subgaussian concentration).", "startOffset": 78, "endOffset": 108}, {"referenceID": 22, "context": "Finally, we use a well known matrix-valued version of Hoeffding\u2019s inequality, for example from Tropp [2011]. Lemma 21 (Matrix-Hoeffding).", "startOffset": 95, "endOffset": 108}], "year": 2017, "abstractText": "We study a variant of the contextual bandit problem, where on each round, the learner plays a sequence of actions, receives a feature for each individual action, and reward that is linearly related to these features. This setting has applications to network routing, crowd-sourcing, personalized search, and many other domains. If the linear transformation is known, we analyze an algorithm that is structurally similar to the algorithm of Agarwal et al. [2014] and show that it enjoys a regret bound between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), where K is the number of actions, L is the length of each action sequence, T is the number of rounds, and N is the number of policies. If the linear transformation is unknown, we show that an algorithm that first explores to learn the unknown weights via linear regression and thereafter uses the estimated weights can achieve \u00d5(\u2016w\u20161(KT ) \u221a lnN) regret, where w is the true (unknown) weight vector. Both algorithms use an optimization oracle to avoid explicit enumeration of the policies and consequently are computationally efficient whenever an efficient algorithm for the fully supervised setting is available.", "creator": "LaTeX with hyperref package"}}}