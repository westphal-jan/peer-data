{"id": "1502.07314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2015", "title": "Path Finding under Uncertainty through Probabilistic Inference", "abstract": "We support a york oriented failed learning path - finding even under change one held allowed fact probabilistic modeled and stricter domain - largely inference modes should the high-performance. This aspects separates not specific from the estimation inference used resource a framework for cheap learning of path - that adopt. We effectiveness which to example over though Canadian Traveler Problem, which we consult more a probabilistic model, which show how rule-based probabilistic allows levels performance stochastic favors to be recommended brought to means.", "histories": [["v1", "Wed, 25 Feb 2015 19:21:04 GMT  (124kb,D)", "https://arxiv.org/abs/1502.07314v1", null], ["v2", "Sat, 2 May 2015 21:53:39 GMT  (130kb,D)", "http://arxiv.org/abs/1502.07314v2", null], ["v3", "Mon, 8 Jun 2015 05:02:53 GMT  (130kb,D)", "http://arxiv.org/abs/1502.07314v3", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["david tolpin", "brooks paige", "jan willem van de meent", "frank wood"], "accepted": false, "id": "1502.07314"}, "pdf": {"name": "1502.07314.pdf", "metadata": {"source": "CRF", "title": "Path Finding under Uncertainty through Probabilistic Inference", "authors": ["David Tolpin", "Brooks Paige", "Jan Willem van de Meent", "Frank Wood"], "emails": ["dtolpin@robots.ox.ac.uk", "brooks@robots.ox.ac.uk", "jwvdm@robots.ox.ac.uk", "fwood@robots.ox.ac.uk"], "sections": [{"heading": "Introduction", "text": "In planning under uncertainty the objective is to find the optimal policy \u2014 a policy that maximizes the expected reward. In most interesting cases the optimal policy cannot be found exactly, and approximation schemes are used to discover the policy, either represented explicitly or as an implicit property of the planning algorithm, through reinforcement learning. Approximation schemes include value/policy iteration, Q-learning, policy gradient methods (Sutton and Barto 1998), as well as methods based on heuristic search (Bonet and Geffner 2001) and Monte Carlo sampling such as MCTS (Kocsis and Szepesva\u0301ri 2006; Browne et al. 2012).\nDomain-independent planning algorithms (Bonet and Geffner 2001; Haslum, Bonet, and Geffner 2005; Helmert 2006) can be applied to different domains with little modification, however for many applications domain-dependant techniques are still critical in order to obtain a high performance policy, and put the burden of implementation on the domain expert formulating the planning problem.\nThe framework of probabilistic inference (Pearl 1988) proposes solutions to a wide range of Artificial Intelligence problems by representing them as probabilistic models. Efficient domain-independent algorithms are available for several classes of representations, in particular for graphical models (Lauritzen 1996), where inference can be performed either exactly and approximately. However, graphical models typically require that the full graph of the model to be represented explicitly, and are not powerful enough for\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nproblems where the state space is exponential in the problem size, such as the generative models common in planning (Szo\u0308re\u0301nyi, Kedenburg, and Munos 2014).\nProbabilistic programs (Goodman et al. 2008; Mansinghka, Selsam, and Perov 2014; Wood, van de Meent, and Mansinghka 2014) can represent arbitrary probabilistic models, efficient approximate inference algorithms have recently emerged (Wingate, Stuhlmu\u0308ller, and Goodman 2011; Wood, van de Meent, and Mansinghka 2014; Paige et al. 2014). In addition to expressive power, probabilistic programming separates modeling and inference, allowing the problem to be specified in a simple language which does not assume any particular inference technique.\nIn this paper, we show a connection between probabilistic inference and path finding, which allows many pathfinding problems to be cast as inference problems using probabilistic programs. Based on this connection, we provide a generic scheme for expressing a path-finding problem as a probabilistic program that infers the path-finding policy. We illustrate this generic scheme by its application to the Canadian Traveller Problem (Papadimitriou and Yannakakis 1991; Bar-Noy and Schieber 1991; Nikolova and Karger 2008). In the empirical evaluation, we show that high performance stochastic policies can be obtained using domain-independent inference techniques. In the concluding section, we discuss other possible areas of application of probabilistic programming in planning, as well as possible difficulties."}, {"heading": "Preliminaries", "text": ""}, {"heading": "Probabilistic Programming", "text": "Probabilistic programs are regular programs extended by two constructs (Gordon et al. 2014):\n\u2022 The ability to draw random values from probability distributions.\n\u2022 The ability to condition values computed in the programs on probability distributions.\nA probabilistic program implicitly defines a probability distribution over the program\u2019s output. Formally, we define a probabilistic program as a stateful deterministic computation P with the following properties: \u2022 Initially, P expects no arguments.\nar X\niv :1\n50 2.\n07 31\n4v 3\n[ cs\n.A I]\n8 J\nun 2\n01 5\n\u2022 On every call, P returns either a distribution F , a distribution and a value (G, y), a value z, or \u22a5.\n\u2022 Upon returning F , P expects a value x drawn from F as the argument to continue.\n\u2022 Upon returning (G, y) or z, P is invoked again without arguments.\n\u2022 Upon returning \u22a5, P terminates. A program is run by calling P repeatedly until termination. Every run of the program implicitly produces a sequence of pairs (Fi, xi) of distributions and drawn from them values of latent random variables. We call this sequence a trace and denote it by x. A trace induces a sequence of pairs (Gj , yj) of distributions and values of observed random variables. We call this sequence an image and denote it by y . We call a sequence of values zk an output of the program and denote it by z . Program output is deterministic given the trace.\nBy definition, the probability of a trace is proportional to the product of the probability of all random choices x and the likelihood of all observations y:\npP(x|y) \u221d |x|\u220f i=1 pFi(xi) |y|\u220f j=1 pGj (yj) (1)\nThe objective of inference in probabilistic program P is to discover the distribution of z .\nSeveral implementations of general probabilistic programming languages are available (Goodman et al. 2008; Mansinghka, Selsam, and Perov 2014; Wood, van de Meent, and Mansinghka 2014). Inference is usually performed using Monte Carlo sampling algorithms for probabilistic programs (Wingate, Stuhlmu\u0308ller, and Goodman 2011; Wood, van de Meent, and Mansinghka 2014; Paige et al. 2014). While some algorithms are better suited for certain inference types, most can be used with any valid probabilistic program."}, {"heading": "Canadian Traveller Problem", "text": "Canadian Traveller Problem (CTP) was introduced in (Papadimitriou and Yannakakis 1991) as a problem of finding the shortest travel distance in a graph where some edges may be blocked. There are several variants of CTP (Bar-Noy and Schieber 1991; Nikolova and Karger 2008; Bnaya, Felner, and Shimony 2009); here we consider the stochastic version. In the stochastic CTP we are given\n\u2022 Undirected weighted graph G = (V,E). \u2022 The initial and the final location nodes s and t. \u2022 Edge weights w : E \u2192 R. \u2022 Traversability probabilities: po : E \u2192 (0, 1]. The actual state of each edge is fixed for every problem instance but becomes known only upon reaching one of the edge vertices. The goal is to find a policy that minimizes the expected travel distance from s to t. The travel distance is the sum of weights of all traversed edges during the travel, where traversing in each direction is counted separately.\nCTP problem is PSPACE-hard (Fried et al. 2013), however a number of heuristic algorithms were proposed, including high-quality policies based on Monte Carlo methods (Eyerich, Keller, and Helmert 2010). Policies are empirically compared by averaging the distance travel over multiple instantiations of the actual states of the edges (open or blocked) according to the traversal probabilities. Since the travel distance is defined only for instance where a path between s and t exists, instantiations in which t cannot be reached from s are ignored.\nA trivial travel policy is realized by traversing the problem graph in a depth-first order until the final location is reached. The expected travel distance of the policy is bounded from above by the sum of weights of all edges in the graph by noticing that every edge is traversed at most once in each direction, and at most half of the edges are traversed on average."}, {"heading": "Duality between Path Finding and", "text": "Probabilistic Inference\nWe shall now show a connection between path finding and probabilistic inference. This connection was noticed earlier (Shimony and Charniak 1991) and was used to search for the maximum a-posteriori probability (MAP) assignment in graphical models using a best-first search algorithm. Here we further extend the analogy and establish a bilateral correspondence between inferring the distribution defined by a probabilistic model and learning the optimal policy in a path-finding problem.\nWe proceed in two steps. First, following earlier work, we establish a connection between a MAP assignment and the shortest path. Then, based on this analogy, we explain how discovering the optimal policy in a generative model can be translated into inferring the output distribution of a probabilistic program and vice versa.\nInference on probabilistic programs computes a representation of distribution (1). An equivalent form of (1) is obtained by taking logarithm of both sides:\nlog pP(x) = |x|\u2211 i=1 log pFi(xi) + |y|\u2211 j=1 log pGj (yj) + C (2)\nwhere C is a constant that does not depend on x. To find the MAP assignment xMAP , one must maximize log p(x). One can view x as a specification of a path in a graph where each node corresponds to either (Fi, xi) or (Gj , yj), and the costs of edges entering (Fi, xi) or (Gj , yj) is \u2212 log pFi(xi) o \u2212 log pHj (yj), correspondingly (Figure 1). Then, finding\nthe MAP assignment is tantamount to finding the tracex that produces the shortest path. (Shimony and Charniak 1991; Sun, Druzdzel, and Yuan 2007) use this correspondence in their MAP algorithms for graphical models.\nWe shall turn now to a more general case when the MAP assignment of a part of the trace x\u03b8 is inferred. In a probabilistic program, this is expressed by selecting x\u03b8 as the program output, z \u2190 x\u03b8. The distribution of z is marginalized over the rest of the trace x\u00ac\u03b8 = x \\ x\u03b8, and finding the MAP assignment for x\u03b8 corresponds to finding the mode of the output distribution:\nx\u03b8MAP = argmax pP(z)\n\u221d argmax \u222b x\u00ac\u03b8\n|x \u03b8 |\u220f\ni=1\npF \u03b8i x \u03b8 i |y|\u220f j=1 pGj (yj)  pP(x\u00ac\u03b8)dx\u00ac\u03b8, (3)\nwhere the integrand in equation (3) depends on x\u00ac\u03b8. Just like in the case of MAP assignment to all random variables, equation (3) corresponds to a path finding problem: x\u03b8 can be viewed as a policy, and determining x\u03b8MAP corresponds to learning a policy which minimizes the expected path length\nEx\u00ac\u03b8 \u2212 |x \u03b8|\u2211\ni=1\nlog pF \u03b8i (x \u03b8 i )\u2212 |y|\u2211 j=1 log pGj (yj)  (4) While in principle policy learning algorithms could be used for MAP estimation, a greater potential lies, in our opinion, in casting planning problems as probabilistic programs and learning the optimal policies by estimating the modes of the programs\u2019 distributions. We suggest to adopt the Bayesian approach, according to which prior beliefs are imposed on policy parameters, and the optimal policy is learned through inferring posterior beliefs by conditioning the beliefs on observations. We explore this approach in the next section."}, {"heading": "Stochastic Policy Learning through", "text": "Probabilistic Inference\nWe have shown that in order to infer the optimum policy, a probabilistic program for policy learning should run the agent on the distribution of problem instance and policies, and compute probability of each execution such that the logarithm of the probability is equal to the negated travel cost. The generic program shown in Algorithm 1 achieves this by\nAlgorithm 1 Policy learning through probabilistic inference. Require: agent, Instances, Policies\n1: instance\u2190 DRAW(Instances) 2: policy\u2190 DRAW(Policies) 3: cost\u2190 RUN(agent, instance, policy) 4: OBSERVE(1, Bernoulli(e\u2212cost)) 5: PRINT(policy)\nrandomly drawing problem instances and policies from their\ndistributions supplied as program arguments (lines 1 and 2) and updating the log probability of the sample (line 4) by calling OBSERVE. OBSERVE adds the log probability of its first argument, the value, with respect to its second argument, the distribution. Consequently, the log probability of the output policy\nlogpP(policy) =\nlog pPolicies(policy) + log e \u2212cost(policy) + C\n=\u2212cost(policy) + log pPolicies(policy) + C (5)\nWhen policies are drawn from their distribution uniformly, log pPolicies(policy) is the same for any policy, and does not affect the distribution of policies specified by the probabilistic program:\nlog pP(policy) = \u2212cost(policy) + C \u2032 (6)\nIn practice, this is achieved by using a uniform distribution on policy parameters, such as the uniform continuous or discrete distribution for scalars, the categorical distribution with equal choice probabilities for discrete choices, or the symmetric Dirichlet distribution with parameter 1 for real vectors. Alternatively, if different policies have different probabilities with respect to the distribution Policies from which the policies are drawn, their log probabilities (taken with the opposite sign) have the interpretation of the costs of the corresponding policies and provide a means for specifying preferences of the model designer with respect to different policies. In either case, the optimal policy is approximated by estimating the mode of the program output.\nWhen policies are drawn uniformly, the scale of the travel cost does not affect the choice of optimal policy. However, as follows from equation (6), the shape of the probability density (or probability mass for discrete distributions) depends on the cost scale \u2014 the higher the cost, the sharper the shape. Thus, by altering the cost scale we can affect the performance of the inference algorithm: on one hand, the mode estimate of a sharper function can be computed with higher accuracy, on the other hand, when pP(policy) changes too fast with its argument in the high probability region, approximate inference algorithms converge slowly. The right scale depends on the probabilistic program, and finding the most appropriate scale is a parameter optimization problem.\nNote that the probabilistic program for policy learning is independent of the inference algorithm which would be used to obtain the results. The programmer does not need to make any assumptions about the way the mode of the output distribution is estimated, and even whether approximate or exact inference (if appropriate) is performed."}, {"heading": "Case Study: Canadian Traveller Problem", "text": "We evaluated the proposed policy learning scheme on the Canadian Traveller Problem (Algorithm 2). The algorithm draws CTP problem instances from a given graph with traversability of each edge randomly selected according to the probabilities p, and learns a stochastic policy based on depth-first search \u2014 the policy is specified by a vector of probabilities of selecting each of the adjacent edges in every\nnode. When the policy is realized, the selection probabilities are conditioned such that only open unexplored edges are selected, in accordance with the base depth-first search traversal. Dirichlet(1deg(v)) is a uniform distribution, hence the\nAlgorithm 2 Learning stochastic policy for the Canadian traveller problem Require: CTP(G, s, t, w, p)\n1: instance\u2190 DRAW(CTP(G,w, p)) 2: for v \u2208 V do 3: policy(v)\u2190 DRAW(Dirichlet(1deg(v))) 4: end for 5: repeat 6: (reached, distance)\u2190 STDFS(instance, policy) 7: until reached 8: OBSERVE(1, Bernoulli ( e\u2212distance ) ) 9: PRINT(policy)\nlog probability of a trace is equal to the path cost taken with the opposite sign. STDFS (line 6) is a flavour of depth-first search which enumerates node children in a random order according to the policy for the current node. An optimal policy is expected to assign a higher probability to edges leading to shorter paths having lower probability to become blocked.\nTo assess the quality of learned policies we generated several CTP problem specifications by triangulating a randomly drawn set of either 50 or 20 nodes from Poisson-distributed points on a unit square. The average DFS travel cost in fully traversable instances was 7.9 for 50 node instances, and 5.7 for 20 node instances. The same traversal probability in the range [0.35, 1.0] is assigned to every edge of the graph (the bond percolation threshold for Delaunay triangulation is\u22480.33 (Becker and Ziff 2009), hence instances with p < 0.3 are disconnected with high probability). A 50 node instance is shown in Figure 2. The s and t nodes are marked by the red circles, and edge weights are equal to the Euclidean distances between the nodes.\nLightweight Metropolis-Hastings (Wingate, Stuhlmu\u0308ller, and Goodman 2011) was used for inference. We learned a policy for each problem specification by running the inference algorithm for 10,000 iterations. Then, we evaluated policies returned at different numbers of iterations on 1,000 randomly drawn instances to estimate the average travel cost. The average computation time of learning and evaluation per instance was \u224880s on Intel Core i5 CPU.\nThe results are shown in Figure 3, where the solid lines correspond to the average travel cost over the set of problems of the corresponding size, and dashed lines to 95% confidence intervals. For both 50 and 20 node problems, the policy mostly converged after \u22481000 iterations, achieving 50\u201380% improvement compared to the uniform stochastic policy. While a further refinement of the policy is possible, a different type of policy should be learned to obtain significantly better results, for example, a deterministic policy which takes online information into account. This, however, would complicate the probabilistic program which we chose to keep as simple as possible \u2014 the actual implementation\nof the program is just above 100 lines of code, including the implementation of DFS.\nA learned policy for a 50 node problem is visualized in Figure 4. Edge widths correspond to the confidence about the policy for the edge. Edges with higher precision (lower variance) of the policy are broader. Edge color is blue when a traversal through the edge is much more probable in one than in the other direction, and green when traversal in either direction has the same probability, with shades of green and blue reflecting how directed the edge is. As we would expect in a converged policy, edges in the center of the graph are thicker, that is, more explored, than at the periphery, where changes in the policy are less likely to affect the average\ntravel cost. Bright blue (uni-directional) edges are mostly radial relative to the direction from the initial position (node 1) to the goal (node 25), and many well-explored tangential edges are green (bi-directional). This corresponds to an intuition about the policy \u2014 traversals through radial edges are mostly in the direction of the goal, and through the tangential edges in either direction to find an alternative route when the edge leading to the goal is blocked."}, {"heading": "Discussion", "text": "We introduced a new approach to policy learning based on casting a policy learning task as a probabilistic program. The main contributions of the paper are:\n\u2022 Discovery of bilateral correspondence between probabilistic inference and policy learning for path finding.\n\u2022 A new approach to policy learning based on the established correspondence.\n\u2022 A realization of the approach for the Canadian traveller problem, where improved policies were consistently learned by probabilistic program inference.\nThe proposed approach can be extended to many different planning problems, both in well-known path-finding applications and in other domains involving policy learning under uncertainty; Partially observable Markov Decision Processes and generalized Multi-armed bandit settings are just two examples. At the same time, the exposure of probabilistic programming tools to different domains and new applications is challenging. These tools were initially developed with certain applications in mind. Our limited experience shows that the probabilistic programming paradigm scales well to new applications and larger problems. However, as more problems are approached using the probabilistic programming methodology, apparent weaknesses and limita-\ntions are uncovered, and a more powerful and flexible inference algorithm will have to be developed.\nThe policy learning algorithm presented here follows the offline learning scheme \u2014 the policy is selected before acting, and then used unmodified until the goal is reached. Although this is, indeed, the easiest way to cast policy learning as probabilistic inference, online learning can also be implemented so that when additional computation during acting is justified by the time cost, the policy is updated based on the information gathered online, as in some of state-of-theart algorithms for CTP (Eyerich, Keller, and Helmert 2010). Moreover, the time cost of updating the policy incrementally based on the new evidence is lower than of inferring a new policy due to the any-time nature of Bayesian updating. Online inference is a subject of ongoing research in probabilistic programming.\nBy performing inference on a probabilistic program, we obtain a representation of distribution of policies rather than a single policy. We then use this distribution to select a policy. When the inference is performed approximately, which is a common case, the expected quality of the selected policy improves with more computation. In the most basic setting, a fixed threshold on the number of iterations of the inference algorithm can be imposed. In general, however, determining when to stop the inference and commit to a particular policy, whether in offline or online setting, is a rational metareasoning decision (Russell and Wefald 1991; Hay et al. 2012). Making this decision in an informed and systematic way is another topic for research."}], "references": [{"title": "and Schieber", "author": ["A. Bar-Noy"], "venue": "B.", "citeRegEx": "Bar.Noy and Schieber 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "R", "author": ["A.M. Becker", "Ziff"], "venue": "M.", "citeRegEx": "Becker and Ziff 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "S", "author": ["Z. Bnaya", "A. Felner", "Shimony"], "venue": "E.", "citeRegEx": "Bnaya. Felner. and Shimony 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Geffner", "author": ["B. Bonet"], "venue": "H.", "citeRegEx": "Bonet and Geffner 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "A survey of Monte Carlo tree search methods", "author": ["Browne"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on 4(1):1\u201343", "citeRegEx": "Browne,? \\Q2012\\E", "shortCiteRegEx": "Browne", "year": 2012}, {"title": "M", "author": ["P. Eyerich", "T. Keller", "Helmert"], "venue": "2010. High-quality policies for the Canadian traveler\u2019s problem. In Proc. of the Twenty-Fourth AAAI Conference on Artificial Intelligence, Atlanta, Georgia, USA, July 11-15,", "citeRegEx": "Eyerich. Keller. and Helmert 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "S", "author": ["Fried, D.", "Shimony"], "venue": "E.; Benbassat, A.; and Wenner, C.", "citeRegEx": "Fried et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "J", "author": ["N.D. Goodman", "V.K. Mansinghka", "D.M. Roy", "K. Bonawitz", "Tenenbaum"], "venue": "B.", "citeRegEx": "Goodman et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "S", "author": ["A.D. Gordon", "T.A. Henzinger", "A.V. Nori", "Rajamani"], "venue": "K.", "citeRegEx": "Gordon et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "New admissible heuristics for domain-independent planning", "author": ["Bonet Haslum", "P. Geffner 2005] Haslum", "B. Bonet", "H. Geffner"], "venue": "In Proceedings, The Twentieth National Conference on Artificial Intelligence and the Seventeenth Innovative Applications of Artificial Intel-", "citeRegEx": "Haslum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Haslum et al\\.", "year": 2005}, {"title": "S", "author": ["N. Hay", "S.J. Russell", "D. Tolpin", "Shimony"], "venue": "E.", "citeRegEx": "Hay et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Szepesv\u00e1ri", "author": ["L. Kocsis"], "venue": "C.", "citeRegEx": "Kocsis and Szepesv\u00e1ri 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Y", "author": ["V.K. Mansinghka", "D. Selsam", "Perov"], "venue": "N.", "citeRegEx": "Mansinghka. Selsam. and Perov 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["E. Nikolova", "Karger"], "venue": "R.", "citeRegEx": "Nikolova and Karger 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Asynchronous anytime sequential Monte Carlo", "author": ["Paige"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Paige,? \\Q2014\\E", "shortCiteRegEx": "Paige", "year": 2014}, {"title": "and Yannakakis", "author": ["C.H. Papadimitriou"], "venue": "M.", "citeRegEx": "Papadimitriou and Yannakakis 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "and Wefald", "author": ["S. Russell"], "venue": "E.", "citeRegEx": "Russell and Wefald 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "and Charniak", "author": ["S.E. Shimony"], "venue": "E.", "citeRegEx": "Shimony and Charniak 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "M", "author": ["Sun, X.", "Druzdzel"], "venue": "J.; and Yuan, C.", "citeRegEx": "Sun. Druzdzel. and Yuan 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "A", "author": ["R.S. Sutton", "Barto"], "venue": "G.", "citeRegEx": "Sutton and Barto 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Optimistic planning in markov decision processes using a generative model", "author": ["Kedenburg Sz\u00f6r\u00e9nyi", "B. Munos 2014] Sz\u00f6r\u00e9nyi", "G. Kedenburg", "R. Munos"], "venue": null, "citeRegEx": "Sz\u00f6r\u00e9nyi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sz\u00f6r\u00e9nyi et al\\.", "year": 2014}, {"title": "N", "author": ["D. Wingate", "A. Stuhlm\u00fcller", "Goodman"], "venue": "D.", "citeRegEx": "Wingate. Stuhlm\u00fcller. and Goodman 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A new approach to probabilistic programming inference", "author": ["van de Meent Wood", "F. Mansinghka 2014] Wood", "J.W. van de Meent", "V. Mansinghka"], "venue": "In Artificial Intelligence and Statistics", "citeRegEx": "Wood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "We introduce a new approach to solving path-finding problems under uncertainty by representing them as probabilistic models and applying domain-independent inference algorithms to the models. This approach separates problem representation from the inference algorithm and provides a framework for efficient learning of path-finding policies. We evaluate the new approach on the Canadian Traveller Problem, which we formulate as a probabilistic model, and show how probabilistic inference allows high performance stochastic policies to be obtained for this problem.", "creator": "LaTeX with hyperref package"}}}