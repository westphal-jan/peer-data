{"id": "1503.00064", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "Generating Multi-Sentence Lingual Descriptions of Indoor Scenes", "abstract": "This collected accepts a retelling frameworks than generating a-block explaining of indoor musical. Whereas substantial aid them has although to tackle unfortunately problem, time approaches closely primarily on concentrated a single prisoner they each image, which is follow insufficient for descriptions complex themes. We any time how any seen, other creates coherent descriptions with and/or punishable. Our manner one \u2019s from lighter longer however followed aspects: (37) also 3D enhanced parsing requires already jointly eym\u00fcr object, singular, without concerning; (41) turn dogmatics elementary working allowed also taking verse; although (3) kind text generations encoding that full into account the coherence among sentences. Experiments on the augmented NYU - yy dataset well that surely framework why capacity well depictions with substantially higher ROGUE drawing compared to people latter by its precise.", "histories": [["v1", "Sat, 28 Feb 2015 04:26:21 GMT  (6435kb,D)", "http://arxiv.org/abs/1503.00064v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["dahua lin", "chen kong", "sanja fidler", "raquel urtasun"], "accepted": false, "id": "1503.00064"}, "pdf": {"name": "1503.00064.pdf", "metadata": {"source": "CRF", "title": "Generating Multi-Sentence Lingual Descriptions of Indoor Scenes", "authors": ["Dahua Lin", "Sanja Fidler", "Raquel Urtasun"], "emails": ["dhlin@ie.cuhk.edu.hk", "chenk@cs.cmu.edu", "fidler@cs.toronto.edu", "urtasun@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "Image understanding has been the central goal of computer vision. Whereas a majority of work on image understanding focuses on class-based annotation, we believe, however, that describing an image using natural language is still the best way to show one\u2019s understanding. The task of automatically generating textual descriptions for images has received increasing attention from both the computer vision and natural language processing communities. This is an important problem, as an effective solution to this problem can enable many exciting real-world applications, such as human robot interaction, image/video synopsis, and automatic caption generation.\nWhile this task has been explored in previous work, existing methods mostly rely on pre-defined\ntemplates (Barbu et al., 2012; Krishnamoorthy et al., 2013), which often result in tedious descriptions. Another line of work solves the description generation problem via retrieval, where a description for an image is borrowed from semantically most similar image from the training set (Ordonez et al., 2011; Farhadi et al., 2010). This setting is, however, less applicable to complex scenes composed of a large set of objects in diverse configurations, such as for example indoor environments.\nRecently, the field has witnessed a boom in generating image descriptions via deep neural networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Chen and Zitnick, 2014) which are able to both, learn a weak language model as well as generalize description to unseen images. These approaches typically represent the image and words/sentences with vectors and reason in a joint embedding space. The results have been impressive, perhaps partly due to powerful representation on the image side (Krizhevsky et al., 2012). This line of work mainly generates a single sentence for each image, which typically focus on one or two objects and typically contain very few prepositional relations between objects.\nIn this paper, we are interested in generat-\nar X\niv :1\n50 3.\n00 06\n4v 1\n[ cs\n.C V\n] 2\n8 Fe\nb 20\n15\ning multi-sentence descriptions of cluttered indoor scenes, which is particularly relevant for indoor robotics. Complex, multi-sentence output requires us to deal with challenging problems such as consistent co-referrals to visual entities across sentences. Furthermore, the sequence of sentences needs to be as natural as possible, mimicking how humans describe the scene. This is particularly important for example in the context of social robotics to enable realistic communications.\nTowards this goal, we develop a framework with three major components: (1) a holistic visual parser that couples the inference of objects, attributes, and relations to produce a semantic representation of a 3D scene (Fig. 1); (2) a generative grammar automatically learned from training text; and (3) a text generation algorithm that takes into account subtle dependencies across sentences, such as logical order, diversity, saliency of objects, and co-references.\nTo test the effectiveness of our approach, we construct an augmented dataset based on NYURGBD (Silberman et al., 2012), where each scene is associated with up to 5 natural language descriptions from human annotators. This allows us to learn a language model to describe images the way that humans do. Experiments show that our method produces natural descriptions, significantly improving the F-measures of ROUGE scores over the baseline."}, {"heading": "2 Related Work", "text": "A large body of existing work deals with images and text in one form or the other. The dominant subfield exploits text in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al., 2012; Silberer et al., 2013). This type of approaches have also been explored in videos to learn visual action models from textual summaries of videos (Ramanathan et al., 2013), or learning visual concepts from videos described with short sentences (Yu and Siskind, 2013). Another direction is to exploit short sentences associated with images in order to improve visual recognition tasks (Fidler et al., 2013; Kong et al., 2014). Just recently, an interested problem domain was introduced in (Malinowski and Fritz, 2014) with the aim to learn how to answer questions about images from Q&A ex-\namples. In (Lin et al., 2014), the authors address visual search with complex natural lingual queries.\nThere has been substantial work in automatically generating a caption or description for a given image. The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011). This line of work bypasses having to deal with language template specification or template learning. However, typically such approaches adopt a limited image representation such as triplets actionobject-scene (Farhadi et al., 2010). This makes a restrictive setting, as neither the image representation nor the retrieved sentence can faithfully model a truly complex scene. In (Kuznetsova et al., 2014) the authors go further by only learning phrases from related images.\nParallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014). These methods transform the image as well as a sentence into a vector representation and learn a joint embedding between the two modalities. The output of these approaches is typically a short sentence for each image. In contrast, our goal here is to generate multiple dependent sentences that describe the salient objects in the scene, their properties and spatial relations.\nGenerating descriptions has also been explored in the video domain. (Barbu et al., 2012; Krishnamoorthy et al., 2013) output a video description in the form of subject-action-object. In (Das et al., 2013), \u201cconcept detectors\u201d are formed, which are detectors for combined object and action or scene in a particular chunk of a video. Via lingual templates the concept detectors of particular types then produce cohesive video descriptions. Due to a limited set of concepts and templates the final descriptions do not seem very natural. (Rohrbach et al., 2013) predicts semantic representations from low-level video features and uses machine translation techniques to generate a sentence.\nThe closest to our work is (Kulkarni et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014) which, like us, is able to describe objects, their modifiers, and prepositions between objects. However, our paper differs from (Kulkarni et al.,\n2011; Mitchell et al., 2012) in several important ways. In our work, we reason in 3D as opposed to 2D giving us more natural physical interpretations. We aim to describe rich indoor scenes that contain many objects of various classes and appear in various arrangements. In such a setting, describing every detectable object and all relations between them as in (Kulkarni et al., 2011) would generate prohibitively long, complex and unnatural descriptions. Our model tries to mimic what and how people describe such complex 3D scenes, thus taking into account visual saliency at the level of objects, attributes and relations, as well as the ordering and coherence of sentences. Another important aspect that sets us apart from most past work is that instead of using a few hand-crafted templates, we learn the grammar from training text."}, {"heading": "3 Framework Overview", "text": "Our framework for generating descriptions for indoor scenes is based on a key rationale: images and their corresponding descriptions are two different ways to express the underlying common semantics shared by both. As shown in Figure 2, given an image, it first recovers the underlying semantics through holistic visual analysis (Lin et al., 2013), which results in a scene graph that captures detected objects and the spatial relations between them (e.g. on-top-of and near, etc).\nThe semantics embodied by a visual scene usually has multiple aspects. When describing such a complex scene, humans often use a paragraph comprised of multiple sentences, each focusing on a specific aspect. To imitate this behavior, this framework transforms the scene graph into a sequence of semantic trees, and yields multiple sen-\ntences, each from a semantic tree. To make the results as natural as possible, we adopt two strategies: (1) Instead of prescribing templates in advance, we learn the grammar from a training set \u2013 a set of RGB-D scenes with descriptions provided by humans. (2) We take into account dependencies among sentences, including logical order, saliency, coreference and diversity."}, {"heading": "4 From RGB-D Images to Semantics", "text": "Given an RGB-D image, we extract semantics through holistic visual parsing. Particularly, we first parse the image to obtain the objects of interest, their attributes, and their physical relations, and then construct a scene graph, which provides a coherent summary of these aspects."}, {"heading": "4.1 Holistic Visual Parsing", "text": "To parse the visual scene we use a recently proposed approach for 3D object detection in RGBD data (Lin et al., 2013). We briefly summarize this approach here. First, a set of \u201cobjectness\u201d regions are generated following (Carreira and Sminchisescu, 2012), which are encouraged to respect intensity as well as occlusion boundaries in 3D. These regions are projected to 3D via depth and then cuboids are fit tightly around them, under the constraint that they are parallel to the ground floor.\nA holistic CRF model is then constructed to jointly reason about the classes of the cuboids as well as the class of the scene (e.g., kitchen, bathroom). The CRF thus has a random variable for each cuboid representing its class, and a variable for the scene. To have the possibility to remove a bad, non-object cuboid, we have an additional background state for each cuboid. The model ex-\nploits various geometric and semantic relations by incorporating them into the CRF formulation as potentials, which are summarized below:\nScene Appearance. To incorporate global information, a unary potential over the scene label is computed by means of a logistic on top of the scene classification score (Xiao et al., 2010).\nCuboid class potential. Appearance-based classifiers, including CPMC-o2 (Carreira et al., 2012), superpixel scores (Ren et al., 2012) are used to classify cuboids into a pre-defined set of object classes. In this paper, we additionally use CNN (Krizhevsky et al., 2012) features for classification. The classification scores for each cuboid are used as different unary potentials in the CRF.\nObject geometry. Cuboids are also classified based on geometric features (e.g. height, longer width, aspect ratio, etc) with SVM, and the classification scores used as another unary potential.\nSemantic context. Two co-occurrence relationships are used: scene-object and object-object. The potential values are estimated from the training set by counting the co-occurence frequencies.\nGeometric context. Two potentials are used to exploit the spatial relations between cuboids in 3D, encoding close-to and on-top-of relations. The potentials are defined to be the empirical cooccurrence frequencies for each type of relation.\nThe CRF weights to combine the potentials are learned with a primal dual learning framework (Hazan and Urtasun, 2010), and inference of class labels is done with an approximated algorithm (Schwing et al., 2011)."}, {"heading": "4.2 Scene Graphs", "text": "Based on the extracted visual information, we construct a scene graph that captures objects, their attributes, such as color and size, and the relations between them. In particular, a scene graph uses nodes to represent objects and their attributes, and edges to represent relations between nodes. Here, we consider three kinds of edges: attribute edges that link objects to their attributes, position edges that represent the positions of objects relative to the scene, (e.g. corner-of-room), and pairwise edges that characterize the relative positions between objects (e.g. on-top-of and next-to).\nGiven an image, a set of objects (with class labels) and the scene class are obtained through visual parsing as explained in the previous Section. However, to form a scene graph, we still need\nfurther analysis to extract attributes and relations. For each object we also compute saliency, i.e. how likely an object will be described. We next describe how we obtain such information.\nObject attributes: For each object, we use RGB histograms and C-SIFT, and cluster them to obtain a visual word representation. We train classifiers for nine colors that are most mentioned in the training set, as well as two material properties (wooden and bright). We also train classifiers for four different sizes (wide, tall, large, and small) using geometric features. To encode the correlations between size and the object class, we augment the feature with a class indicator vector.\nObject saliency: The dataset of (Kong et al., 2014) contains alignment between the nouns in a sentence and the visual objects in the scene. We make use of this information to train a classifier predicting whether an object in the scene is likely to be mentioned in text. We train an SVM classifier using class-based features (classification scores for each cuboid), geometric relations (volume, distance to camera), and color features.\nObject relations: We consider six types of object positions (corner-of-room, front-of-camera, far-away-from-camera, center-of-room, left-ofroom, and right-of-room), and eight types of pairwise relations (next-to, near, top-of, above, infront-of, behind, to-left-of, and to-right-of ). We manually specify a few rules that help us decide whether a specific relation is present or not1."}, {"heading": "5 Generating Lingual Descriptions", "text": "Given a scene graph, our framework generates a descriptive paragraph in two steps. First, it transforms the scene graph into a sequence of semantic trees, each focusing on a certain semantic aspect. Then, it produces sentences, one from each semantic tree, following a generative grammar."}, {"heading": "5.1 Semantic Trees", "text": "A semantic tree captures information such as what entities are being described and what are the relationships between them. Specifically, a semantic tree contains a set of terminal nodes corresponding to individual entities or their attributes and relational nodes that express relations among them.\n1We tried obtaining ground-truth for relations via MTurk (which would allow us to train classifiers instead), however, the results of all batches were extremely noisy.\nConsider a sentence \u201cA red box is on top of a table\u201d. The corresponding semantic tree can be expressed as\non-top-of(indet(color(box, red)), indet(table))\nThis tree has three terminals: \u201cbox\u201d, \u201ctable\u201d, and \u201cred\u201d. The relation node \u201ccolor(box, red)\u201d describes the relation between \u201cbox\u201d and \u201cred\u201d, namely, \u201cred\u201d specifying the color of the \u201cbox\u201d. The relation \u201cindet\u201d qualifies the cardinality of its child; while \u201con-top-of\u201d characterizes the spatial relation between its children."}, {"heading": "5.2 Dependencies among Sentences", "text": "In human descriptions, sentences are put together in a way that makes the resultant paragraphs coherent. In particular, the dependencies among sentences, as outlined below, play a crucial role in preserving the coherence a descriptive paragraph:\nLogical order. When describing a scene, people present things in certain orders. The leading sentence often mentions the type of the entire scene and one of the most salient object, e.g. \u201cThere is a table in the dining room.\u201d\nDiversity. People generally avoid using the same prepositional relation in multiple sentences. Also, when an object is mentioned in multiple sentences, it usually plays a different role, e.g. \u201cThere is a table near the wall. On top of the table is a microwave oven.\u201d Here, \u201ctable\u201d respectively serves as a source and a target in these two sentences2.\nSaliency. Saliency influences the order of sentences. The statistics in (Kong et al., 2014) shows that bigger objects are often mentioned earlier on in a description and co-referred across sentences, e.g. one would say \u201cThis room has a dining table with a mug on top. Next to the table is a chair.\u201d and not \u201cThere is a mug on a table. Next to the mug is a chair.\u201d Saliency also depends on context, e.g. for bathrooms, toilets are often mentioned.\nCo-reference. When an object is mentioned for the second time following its debut, a pronoun is often used to make the sentence concise.\nRichness vs. Conciseness. When talking about an object for the first time, describing its color/size makes the sentence interesting and informative. However, this is generally unnecessary the next time the object is mentioned.\n2Each relation is considered as an edge. For example, in phrases \u201cA on-top-of B\u201d and \u201cA near B\u201d, \u201cA\u201d is considered as the source, while \u201cB\u201d considered as the target."}, {"heading": "5.3 From Scene Graphs to Semantic Trees", "text": "Motivated by these considerations, we devise a method below that transforms a scene graph into a sequence of semantic trees, each for a sentence.\nFirst of all, we initialize wsi = w t i = si \u00b7 ci. Here, wsi and w t i are the weights that respectively control how likely the i-th object will be chosen as a source or a target in the next sentence; si is a positive value measuring the saliency of the i-th object, while ci is given by the classifier to indicate its confidence as to whether it makes a correct prediction of the object\u2019s class. These weights are updated as the generation proceeds.\nTo generate the leading sentence, we first draw a source i with a probability proportional to wsi , and create a semantic tree by choosing a relation, say \u201cin\u201d, which would lead to a sentence like \u201cThere is a table in the dining room.\u201d Once the i-th object is chosen to be a source, wsi will be set to 0, precluding it from being chosen as a source again. However, wti remains unchanged, as it remains fine for it to serve as a target later.\nFor each subsequent sentence, we draw a source i, a target j, and a relation r between i and j, with probability proportional to wsiw t j\u03c1r, where \u03c1r is the prior weight of the relation r. At each iteration, one may also choose to terminate without generating a new sentence, with a probability proportional to a positive value \u03c4 . These choices together result in a semantic tree in the form of \u201cr(make tree(i), make tree(j))\u201d. Here, \u201cmake tree(i)\u201d creates a sub-tree describing the object i, which may be \u201cindet(color(table, black))\u201d when the color is known.\nAfter the generation of this semantic tree, the weights wsi , w t j , and \u03c1r will be set to zero to prevent the objects i and j from being used again for the same role, and the relation r from being chosen next time. Our algorithm also takes care of co-references \u2013 if an object is selected again in the next sentence, it will be replaced by a pronoun."}, {"heading": "5.4 Grammar and Derivation", "text": "Given a semantic tree, our framework produces a sentence following a generative grammar, namely, a map from each semantic relation to a set of templates (i.e. derivation rules), as illustrated below: indet --> a {1} color --> {2} {1} on-top-of --> {1} is on top of {2}\nOn top of {2} is {1} There is {1} on top of {2}\nEach template has a weight that is set to its fre-\nquency in the training set. The generation of a sentence from a semantic tree proceeds from the root, and downward recursively to the terminals. For each relation node, a template will be chosen, with a probability proportional to the associated weight. Below is an example showing how a sentence is derived following the grammar above.\n{on-top-of(indet(color(box, red)), indet(table))}\n=> {indet(color(box, red))} is on top of {indet(table)} => a {color(box, red)} is on top of a table => a red box is on top of a table\nAs the choices of templates for relational nodes are randomized, different sentences can be derived for the same tree, with different probabilities."}, {"heading": "5.5 Learning the Grammar", "text": "The grammar for generating sentences are often specified manually in previous work (Barbu et al., 2012; Das et al., 2013). This way, however, is time consuming, unreliable, and tends to oversimplify the language. In this work, we explore a new approach, that is, to learn the grammar from data. The basic idea is to construct a semantic tree from each sentence through linguistic parsing, and then derive the templates by matching nodes of the semantic tree to parts of the sentence.\nFirst, we use the Stanford parser (Toutanova et al., 2003) to obtain a parse tree for each sentence, which is then simplified through a series of filtering operations. For example, we merge noun phrases (e.g. \u201cfire distinguisher\u201d) into a single node and compress common prepositional phrases (e.g. \u201cin the left of\u201d) into a single link.\nA semantic tree can then be derived by recursively translating the simplified trees. This is straightforward. For example, a noun \u201cbox\u201d with an adjective \u201cred\u201d will be translated into \u201ccolor(box, red)\u201d; a noun with a definite or indef-\ninite article will be translated into an det and indet relation node; two nouns or noun phrases \u201cA\u201d and \u201cB\u201d linked by a prepositional link \u201cabove\u201d will be translated into \u201cabove(A, B)\u201d.\nWith a sentence and a semantic tree constructed thereon, we can derive the template through recursive matching, where matched children are replaced by a placeholder, while other words are preserved literally in the template. Figure 3 illustrates this procedure. We collect templates respectively for each relation, and set the weight of each template to its frequency. Empirically, we observed a long tailed distribution \u2013 a small number of common templates occur many times, while a dominant portion of templates are used sporadically. To improve the reliability, we discard all the templates that occur less than 5 times and all relations whose total weight is less than 20."}, {"heading": "6 Experimental Evaluation", "text": "We test the proposed framework on the NYU-v2 dataset (Silberman et al., 2012) augmented with an additional set of textual descriptions, one for each image. Particularly, we focus on assessing both the relevance and quality of the generated descriptions."}, {"heading": "6.1 Data Preparation", "text": "The NYU-v2 dataset has 1449 RGB-D images of indoor scenes (e.g. dining rooms, kitchens, offices). These images are divided into a training a testing set, following the partition used in (Lin et al., 2013). The training set contains 795 scenes, while the testing set contains the remaining 654. We use the descriptions from (Kong et al., 2014) which were collected by asking MTurkers to describe the image to someone who does not see it in order to provide him/her with a vidid impression of the scene. The number of sentences per description ranges from 1 to 10 with an average of 3. There are on average 40 words in a description.\nWe learn the generative grammar using the algorithm described in Section 5.5 from the training set of descriptions. We also train the CRF for visual analysis and apply it to detect objects and predict their attributes and relations, following the procedure described in Section 4.1. These models are then used to produce textual descriptions for each test scene."}, {"heading": "6.2 Performance Metrics", "text": "To evaluate our method, we look for metrics typically used in machine translation. These include the BLEU (Papineni et al., 2002) and ROUGE metrics among others. The BLEU score measures precision on n-grams, and is thus less suitable for our goal of lingual image description, as already noted in (Mitchell et al., 2012; Das et al., 2013). On the other hand, ROUGE is an n-gram recall oriented measures which evaluates the information coverage between summaries produced by the human annotators and those automatically produced by systems. ROUGE-1 (unigram) recall is the best option to use for comparing descriptions based only on predicted keywords (Das et al., 2013). ROUGE-2 (bigram) and ROUGE-SU4 (skip-4 bigram) are best to evaluate summaries with respect to coherence and fluency. We use the ROUGE metrics following (Das et al., 2013) who uses it to evaluate lingual video summarization."}, {"heading": "6.3 Comparison of Results", "text": "The proposed text generation method has five optional switches, controlling whether the following features are used during generation: (1) diversity: encourage diversity of the sentences by suppressing the entities and relations that have been mentioned; (2) saliency: draw salient objects with higher probability; (3) scene: leading sentence mentions the class of the scene; (4) attributes: use colors and sizes to describe objects when they are available; (5) coreference: use a pronoun to refer to an object when it is mentioned in the previous sentence. Our experiments test the framework under six feature-levels, level-0 to level-5, where the level-k configuration uses the first k features when\ngenerating the sentences. In particular, level-0 uses none of the features above, and thus each sentence is generated independently using the grammar; while level-5 uses all of these features.\nTo put our performance in perspective, we compare our method to an intelligent baseline which follows a conventional approach in description generation. The baseline describes an image by retrieving visually the most similar image from the training set, and simply using its corresponding description. To compute our baseline, we use a battery of visual features such as spatial pyramids of SIFT, HOG, LBP, geometric context, etc, and kernels with different distances. We use (Xiao et al., 2010) to compute the kernels. Based on a combined kernel, we simply retrieve the training image with the highest matching score.\nTable 1 shows the results. We evaluate two settings: using ground-truth objects (denoted with GT) and using the results obtained via the visual parser (denoted with Real). We can see that the proposed method significantly outperforms the baseline in all three ROGUE measures. Also, configurations above level 3 are clearly better than level 1 and 2, which indicates that a special leading sentence that gives an overview of the scene is important for description generation. In addition, we observe that there are is a noticeable improvement from level 3 to level 4 and 5. This is not surprising: whereas attributes and coreference improve the quality of descriptions by making them richer and less verbose, such improvement on quality does not contribute substantially to the ROGUE score that are based on n-gram comparisons.\nFigure 4 shows descriptions generated using our approach on a diverse set of scenes. It can be seen that linguistic issues such as sentence diver-\nsity, using attributes to describe objects, and using pronouns for coreferences have been properly addressed. However, there remain some problems that need future efforts to address. For example, since the choices of templates for different sentences are independent, sometimes an unfortunate selection of a template sequence may make the paragraph slightly unnatural."}, {"heading": "7 Conclusion", "text": "We presented a new framework for generating natural descriptions of indoor scenes. Our framework\nintegrates a CRF model for visual parsing, a generative grammar automatically learned from training descriptions, as well as a transformation algorithm to derive semantic trees from scene graphs, which takes into account the dependencies across sentences. Our experiments show substantially better descriptions than those produced by a baseline. Such findings indicate that high quality description generation requires not only reliable image understanding, but also delicate attention to linguistic issues, such as diversity, coherence, and logical order of sentences."}], "references": [{"title": "Cpmc: Automatic object segmentation using constrained parametric min-cuts", "author": ["J. Carreira", "C. Sminchisescu."], "venue": "TPAMI.", "citeRegEx": "Carreira and Sminchisescu.,? 2012", "shortCiteRegEx": "Carreira and Sminchisescu.", "year": 2012}, {"title": "Semantic segmentation with secondorder pooling", "author": ["J. Carreira", "R. Caseiroa", "J. Batista", "C. Sminchisescu."], "venue": "ECCV12.", "citeRegEx": "Carreira et al\\.,? 2012", "shortCiteRegEx": "Carreira et al\\.", "year": 2012}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick."], "venue": "arXiv:1411.5654.", "citeRegEx": "Chen and Zitnick.,? 2014", "shortCiteRegEx": "Chen and Zitnick.", "year": 2014}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["P. Das", "C. Xu", "R.F. Doell", "J. J Corso."], "venue": "CVPR.", "citeRegEx": "Das et al\\.,? 2013", "shortCiteRegEx": "Das et al\\.", "year": 2013}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell."], "venue": "arXiv:1411.4389.", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig."], "venue": "arXiv:1411.4952.", "citeRegEx": "Fang et al\\.,? 2014", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Every picture tells a story: Generating sentences for images", "author": ["A. Farhadi", "M. Hejrati", "M. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth."], "venue": "ECCV.", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "A sentence is worth a thousand pixels", "author": ["S. Fidler", "A. Sharma", "R. Urtasun."], "venue": "CVPR.", "citeRegEx": "Fidler et al\\.,? 2013", "shortCiteRegEx": "Fidler et al\\.", "year": 2013}, {"title": "Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers", "author": ["A. Gupta", "L. Davis."], "venue": "ECCV.", "citeRegEx": "Gupta and Davis.,? 2008", "shortCiteRegEx": "Gupta and Davis.", "year": 2008}, {"title": "A primal-dual message-passing algorithm for approximated large scale structured prediction", "author": ["T. Hazan", "R. Urtasun."], "venue": "NIPS.", "citeRegEx": "Hazan and Urtasun.,? 2010", "shortCiteRegEx": "Hazan and Urtasun.", "year": 2010}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei."], "venue": "arXiv:1412.2306.", "citeRegEx": "Karpathy and Fei.Fei.,? 2014", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel."], "venue": "arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "What are you talking about? text-to-image coreference", "author": ["C. Kong", "D. Lin", "M. Bansal", "R. Urtasun", "S. Fidler."], "venue": "CVPR.", "citeRegEx": "Kong et al\\.,? 2014", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Generating natural-language video descriptions using textmined knowledge", "author": ["N. Krishnamoorthy", "G. Malkarnenkar", "R.J. Mooney", "K. Saenko", "S. Guadarrama."], "venue": "AAAI, July.", "citeRegEx": "Krishnamoorthy et al\\.,? 2013", "shortCiteRegEx": "Krishnamoorthy et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton."], "venue": "NIPS, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A. Berg", "T. Berg."], "venue": "CVPR.", "citeRegEx": "Kulkarni et al\\.,? 2011", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A. Berg", "T. Berg", "Y. Choi."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Kuznetsova et al\\.,? 2012", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2012}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "T.L. Berg", "Y. Choi."], "venue": "TACL.", "citeRegEx": "Kuznetsova et al\\.,? 2014", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "Towards total scene understanding:classification, annotation and segmentation in an automatic framework", "author": ["L. Li", "R. Socher", "L. Fei-Fei."], "venue": "CVPR.", "citeRegEx": "Li et al\\.,? 2009", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Holistic scene understanding for 3d object detection with rgbd cameras", "author": ["D. Lin", "S. Fidler", "R. Urtasun."], "venue": "ICCV.", "citeRegEx": "Lin et al\\.,? 2013", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Visual semantic search: Retrieving videos via complex textual queries", "author": ["D. Lin", "S. Fidler", "C. Kong", "R. Urtasun."], "venue": "CVPR.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz."], "venue": "NIPS.", "citeRegEx": "Malinowski and Fritz.,? 2014", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille."], "venue": "arXiv:1410.1090.", "citeRegEx": "Mao et al\\.,? 2014", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["C. Matuszek", "N. FitzGerald", "L. Zettlemoyer", "L. Bo", "D. Fox."], "venue": "International Conference on Machine Learning.", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "J. Dodge", "A. Goyal", "Kota Yamaguchi", "K. Sratos", "X. Han", "A. Mensch", "A.C. Berg", "T.L. Berg", "H. Daume III."], "venue": "European Chapter of the Association for compu-", "citeRegEx": "Mitchell et al\\.,? 2012", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T. Berg."], "venue": "NIPS.", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["S.K. Papineni", "T. Ward Roukos", "W.J. Zhu."], "venue": "ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning visual representations using images with captions", "author": ["A. Quattoni", "M. Collins", "T. Darrell."], "venue": "CVPR07.", "citeRegEx": "Quattoni et al\\.,? 2007", "shortCiteRegEx": "Quattoni et al\\.", "year": 2007}, {"title": "Video event understanding using natural language descriptions", "author": ["V. Ramanathan", "P. Liang", "L. Fei-Fei."], "venue": "ICCV.", "citeRegEx": "Ramanathan et al\\.,? 2013", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2013}, {"title": "Rgb-(d) scene labeling: Features and algorithms", "author": ["X. Ren", "L. Bo", "D. Fox."], "venue": "CVPR.", "citeRegEx": "Ren et al\\.,? 2012", "shortCiteRegEx": "Ren et al\\.", "year": 2012}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele."], "venue": "ICCV.", "citeRegEx": "Rohrbach et al\\.,? 2013", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "Distributed message passing for large scale graphical models", "author": ["A. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun."], "venue": "CVPR.", "citeRegEx": "Schwing et al\\.,? 2011", "shortCiteRegEx": "Schwing et al\\.", "year": 2011}, {"title": "Models of semantic representation with visual attributes", "author": ["C. Silberer", "V. Ferrari", "M. Lapata."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Silberer et al\\.,? 2013", "shortCiteRegEx": "Silberer et al\\.", "year": 2013}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "P. Kohli", "D. Hoiem", "R. Fergus."], "venue": "ECCV.", "citeRegEx": "Silberman et al\\.,? 2012", "shortCiteRegEx": "Silberman et al\\.", "year": 2012}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei."], "venue": "CVPR.", "citeRegEx": "Socher and Fei.Fei.,? 2010", "shortCiteRegEx": "Socher and Fei.Fei.", "year": 2010}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C. Manning."], "venue": "HLT-NAACL.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan."], "venue": "arXiv:1411.4555.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba."], "venue": "CVPR.", "citeRegEx": "Xiao et al\\.,? 2010", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "III H. Daum\u00e9", "Y. Aloimonos."], "venue": "EMNLP, pages 444\u2013454.", "citeRegEx": "Yang et al\\.,? 2011", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Grounded language learning from video described with sentences", "author": ["H. Yu", "J.M. Siskind."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Yu and Siskind.,? 2013", "shortCiteRegEx": "Yu and Siskind.", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "templates (Barbu et al., 2012; Krishnamoorthy et al., 2013), which often result in tedious descriptions.", "startOffset": 10, "endOffset": 59}, {"referenceID": 25, "context": "Another line of work solves the description generation problem via retrieval, where a description for an image is borrowed from semantically most similar image from the training set (Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 182, "endOffset": 226}, {"referenceID": 6, "context": "Another line of work solves the description generation problem via retrieval, where a description for an image is borrowed from semantically most similar image from the training set (Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 182, "endOffset": 226}, {"referenceID": 11, "context": "Recently, the field has witnessed a boom in generating image descriptions via deep neural networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Chen and Zitnick, 2014) which are able to both, learn a weak language model as well as generalize description to unseen images.", "startOffset": 99, "endOffset": 170}, {"referenceID": 2, "context": "Recently, the field has witnessed a boom in generating image descriptions via deep neural networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Chen and Zitnick, 2014) which are able to both, learn a weak language model as well as generalize description to unseen images.", "startOffset": 99, "endOffset": 170}, {"referenceID": 14, "context": "The results have been impressive, perhaps partly due to powerful representation on the image side (Krizhevsky et al., 2012).", "startOffset": 98, "endOffset": 123}, {"referenceID": 33, "context": "To test the effectiveness of our approach, we construct an augmented dataset based on NYURGBD (Silberman et al., 2012), where each scene is associated with up to 5 natural language descriptions from human annotators.", "startOffset": 94, "endOffset": 118}, {"referenceID": 27, "context": "The dominant subfield exploits text in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al.", "startOffset": 113, "endOffset": 202}, {"referenceID": 18, "context": "The dominant subfield exploits text in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al.", "startOffset": 113, "endOffset": 202}, {"referenceID": 34, "context": "The dominant subfield exploits text in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al.", "startOffset": 113, "endOffset": 202}, {"referenceID": 8, "context": "The dominant subfield exploits text in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al.", "startOffset": 113, "endOffset": 202}, {"referenceID": 23, "context": ", 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al., 2012; Silberer et al., 2013).", "startOffset": 80, "endOffset": 126}, {"referenceID": 32, "context": ", 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al., 2012; Silberer et al., 2013).", "startOffset": 80, "endOffset": 126}, {"referenceID": 28, "context": "This type of approaches have also been explored in videos to learn visual action models from textual summaries of videos (Ramanathan et al., 2013), or learning visual concepts from videos described with short sentences (Yu and Siskind, 2013).", "startOffset": 121, "endOffset": 146}, {"referenceID": 39, "context": ", 2013), or learning visual concepts from videos described with short sentences (Yu and Siskind, 2013).", "startOffset": 80, "endOffset": 102}, {"referenceID": 7, "context": "Another direction is to exploit short sentences associated with images in order to improve visual recognition tasks (Fidler et al., 2013; Kong et al., 2014).", "startOffset": 116, "endOffset": 156}, {"referenceID": 12, "context": "Another direction is to exploit short sentences associated with images in order to improve visual recognition tasks (Fidler et al., 2013; Kong et al., 2014).", "startOffset": 116, "endOffset": 156}, {"referenceID": 21, "context": "Just recently, an interested problem domain was introduced in (Malinowski and Fritz, 2014) with the aim to learn how to answer questions about images from Q&A examples.", "startOffset": 62, "endOffset": 90}, {"referenceID": 20, "context": "In (Lin et al., 2014), the authors address visual search with complex natural lingual queries.", "startOffset": 3, "endOffset": 21}, {"referenceID": 25, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 6, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 16, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 30, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 38, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 6, "context": "However, typically such approaches adopt a limited image representation such as triplets actionobject-scene (Farhadi et al., 2010).", "startOffset": 108, "endOffset": 130}, {"referenceID": 17, "context": "In (Kuznetsova et al., 2014) the authors go further by only learning phrases from related images.", "startOffset": 3, "endOffset": 28}, {"referenceID": 11, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 36, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 22, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 4, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 5, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 2, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 13, "context": "(Barbu et al., 2012; Krishnamoorthy et al., 2013) output a video description in the form of subject-action-object.", "startOffset": 0, "endOffset": 49}, {"referenceID": 3, "context": "In (Das et al., 2013), \u201cconcept detectors\u201d are formed, which are detectors for combined object and action or scene in a particular chunk of a video.", "startOffset": 3, "endOffset": 21}, {"referenceID": 30, "context": "(Rohrbach et al., 2013) predicts semantic representations from low-level video features and uses machine translation techniques to generate a sentence.", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "The closest to our work is (Kulkarni et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014) which, like us, is able to describe objects, their modifiers, and prepositions between objects.", "startOffset": 27, "endOffset": 98}, {"referenceID": 24, "context": "The closest to our work is (Kulkarni et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014) which, like us, is able to describe objects, their modifiers, and prepositions between objects.", "startOffset": 27, "endOffset": 98}, {"referenceID": 17, "context": "The closest to our work is (Kulkarni et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014) which, like us, is able to describe objects, their modifiers, and prepositions between objects.", "startOffset": 27, "endOffset": 98}, {"referenceID": 15, "context": "In such a setting, describing every detectable object and all relations between them as in (Kulkarni et al., 2011) would generate prohibitively long, complex and unnatural descriptions.", "startOffset": 91, "endOffset": 114}, {"referenceID": 19, "context": "As shown in Figure 2, given an image, it first recovers the underlying semantics through holistic visual analysis (Lin et al., 2013), which results in a scene graph that captures detected objects and the spatial relations between them (e.", "startOffset": 114, "endOffset": 132}, {"referenceID": 19, "context": "To parse the visual scene we use a recently proposed approach for 3D object detection in RGBD data (Lin et al., 2013).", "startOffset": 99, "endOffset": 117}, {"referenceID": 0, "context": "First, a set of \u201cobjectness\u201d regions are generated following (Carreira and Sminchisescu, 2012), which are encouraged to respect intensity as well as occlusion boundaries in 3D.", "startOffset": 61, "endOffset": 94}, {"referenceID": 37, "context": "To incorporate global information, a unary potential over the scene label is computed by means of a logistic on top of the scene classification score (Xiao et al., 2010).", "startOffset": 150, "endOffset": 169}, {"referenceID": 1, "context": "Appearance-based classifiers, including CPMC-o2 (Carreira et al., 2012), superpixel scores (Ren et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 29, "context": ", 2012), superpixel scores (Ren et al., 2012) are used to classify cuboids into a pre-defined set of object classes.", "startOffset": 27, "endOffset": 45}, {"referenceID": 14, "context": "In this paper, we additionally use CNN (Krizhevsky et al., 2012) features for classification.", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "The CRF weights to combine the potentials are learned with a primal dual learning framework (Hazan and Urtasun, 2010), and inference of class labels is done with an approximated algorithm (Schwing et al.", "startOffset": 92, "endOffset": 117}, {"referenceID": 31, "context": "The CRF weights to combine the potentials are learned with a primal dual learning framework (Hazan and Urtasun, 2010), and inference of class labels is done with an approximated algorithm (Schwing et al., 2011).", "startOffset": 188, "endOffset": 210}, {"referenceID": 12, "context": "Object saliency: The dataset of (Kong et al., 2014) contains alignment between the nouns in a sentence and the visual objects in the scene.", "startOffset": 32, "endOffset": 51}, {"referenceID": 12, "context": "The statistics in (Kong et al., 2014) shows that bigger objects are often mentioned earlier on in a description and co-referred across sentences, e.", "startOffset": 18, "endOffset": 37}, {"referenceID": 3, "context": "The grammar for generating sentences are often specified manually in previous work (Barbu et al., 2012; Das et al., 2013).", "startOffset": 83, "endOffset": 121}, {"referenceID": 35, "context": "First, we use the Stanford parser (Toutanova et al., 2003) to obtain a parse tree for each sentence, which is then simplified through a series of filtering operations.", "startOffset": 34, "endOffset": 58}, {"referenceID": 33, "context": "We test the proposed framework on the NYU-v2 dataset (Silberman et al., 2012) augmented with an additional set of textual descriptions, one for each image.", "startOffset": 53, "endOffset": 77}, {"referenceID": 19, "context": "These images are divided into a training a testing set, following the partition used in (Lin et al., 2013).", "startOffset": 88, "endOffset": 106}, {"referenceID": 12, "context": "We use the descriptions from (Kong et al., 2014) which were collected by asking MTurkers to describe the image to someone who does not see it in order to provide him/her with a vidid impression of the scene.", "startOffset": 29, "endOffset": 48}, {"referenceID": 26, "context": "These include the BLEU (Papineni et al., 2002) and ROUGE metrics among others.", "startOffset": 23, "endOffset": 46}, {"referenceID": 24, "context": "The BLEU score measures precision on n-grams, and is thus less suitable for our goal of lingual image description, as already noted in (Mitchell et al., 2012; Das et al., 2013).", "startOffset": 135, "endOffset": 176}, {"referenceID": 3, "context": "The BLEU score measures precision on n-grams, and is thus less suitable for our goal of lingual image description, as already noted in (Mitchell et al., 2012; Das et al., 2013).", "startOffset": 135, "endOffset": 176}, {"referenceID": 3, "context": "ROUGE-1 (unigram) recall is the best option to use for comparing descriptions based only on predicted keywords (Das et al., 2013).", "startOffset": 111, "endOffset": 129}, {"referenceID": 3, "context": "We use the ROUGE metrics following (Das et al., 2013) who uses it to evaluate lingual video summarization.", "startOffset": 35, "endOffset": 53}, {"referenceID": 37, "context": "We use (Xiao et al., 2010) to compute the kernels.", "startOffset": 7, "endOffset": 26}], "year": 2015, "abstractText": "This paper proposes a novel framework for generating lingual descriptions of indoor scenes. Whereas substantial efforts have been made to tackle this problem, previous approaches focusing primarily on generating a single sentence for each image, which is not sufficient for describing complex scenes. We attempt to go beyond this, by generating coherent descriptions with multiple sentences. Our approach is distinguished from conventional ones in several aspects: (1) a 3D visual parsing system that jointly infers objects, attributes, and relations; (2) a generative grammar learned automatically from training text; and (3) a text generation algorithm that takes into account the coherence among sentences. Experiments on the augmented NYU-v2 dataset show that our framework can generate natural descriptions with substantially higher ROGUE scores compared to those produced by the baseline.", "creator": "TeX"}}}