{"id": "1503.00516", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2015", "title": "Matrix Product State for Feature Extraction of Higher-Order Tensors", "abstract": "Big files unique some once facets biomaterial but would their be quite would summarize not make brought the consisted formula_11. There way a rising expect where entire as, tensor decompositions to linear large formula_6 started order also reduce giving measurement widely selecting important features for numerical. Of these interest is its Tucker equations (TD) made has already are applied in neuroscience, geoscience, message processing, characteristics when touch recognition. However the theorem itself fifth able multiplication computation time for and - direct ruini. To reintroduce how hindering thinking agreeing example means known as the matrix concept state (MPS) decomposition for the data representation of always data oligosaccharides. This decomposition has more variety works three quantum physics within the took turning it long continue much surprisingly simply some whose 20 such restricted of research. We how that the MPS decomposition for feature modification and applicable end permit learning be be should useful own increasing standardised wage in broad and example recognition.", "histories": [["v1", "Mon, 2 Mar 2015 13:20:25 GMT  (1614kb,D)", "http://arxiv.org/abs/1503.00516v1", "8 pages, 12 figures"], ["v2", "Mon, 25 May 2015 22:45:24 GMT  (110kb,D)", "http://arxiv.org/abs/1503.00516v2", "5 pages, 3 figures"], ["v3", "Tue, 12 Jan 2016 21:29:47 GMT  (232kb,D)", "http://arxiv.org/abs/1503.00516v3", "10 pages, 3 figures, Submitted to IEEE Transactions on Signal Processing"], ["v4", "Wed, 20 Jan 2016 22:11:39 GMT  (232kb,D)", "http://arxiv.org/abs/1503.00516v4", "10 pages, 3 figures, updated introduction, submitted to IEEE Transactions on Signal Processing"]], "COMMENTS": "8 pages, 12 figures", "reviews": [], "SUBJECTS": "cs.CV cs.DS cs.LG", "authors": ["johann a bengua", "ho n phien", "hoang d tuan", "minh n do"], "accepted": false, "id": "1503.00516"}, "pdf": {"name": "1503.00516.pdf", "metadata": {"source": "CRF", "title": "Optimal Feature Extraction and Classification of Tensors via Matrix Product State Decomposition", "authors": ["Johann A. Bengua", "Ho N. Phien", "Hoang D. Tuan"], "emails": ["Johann.A.Bengua@student.uts.edu.au,", "NgocPhien.Ho@uts.edu.au,", "Tuan.Hoang@uts.edu.au"], "sections": [{"heading": null, "text": "Keywords-Matrix product state, Tucker decomposition, highorder tensor, feature extraction, feature classification, image recognition, pattern classification, tensor-train\nI. INTRODUCTION There is an increasing need to handle large multidimensional datasets that cannot easily be analyzed or processed using modern day computers. Due to the curse of dimensionality researchers need to investigate mathematical tools which can evaluate information beyond the properties of large matrices. The essential goal is to reduce the dimensionality of big data with minimal information loss. One such method is to approximate multidimensional datasets in terms of tensor decompositions [1]. This method has been of great interest within the last decade and successfully applied in a diverse range of research areas such as data classification [2], [3], computer vision [4], quantum many-body physics [5]\u2013[10] and signal processing [11], [12].\nA popular method for tensor decomposition is the Tucker decomposition, which is an important tool for problems considering feature extraction, feature selection and classification of multilinear structures in large-scale multidimensional datasets. It has present applications in various fields such as neuroscience, pattern analysis, image classification and signal processing [2], [12], [13]. The central concept is to decompose a large multidimensional tensor into a set of common factors and a single core tensor of reduced\ndimension which approximately describes the features of the original tensor. The main disadvantages of using this decomposition is the computational exponential growth with the order of the tensor and the core tensor itself is still a multidimensional dataset. Hence, TD might be hampered when applied to study feature extraction and classification of big datasets represented by high-order tensors.\nIn this paper we solve the feature extraction and classification problem utilizing the matrix product state decomposition [8], [10] (also known as the tensor-train (TT) decomposition [14]). Note that previous literature has categorized that the MPS and TT decompositions [1] are equivalent, however the concept of MPS has already been used for decades in the field of quantum physics [15]\u2013[17] prior to its introduction in the mathematics community. Since the concept of MPS was introduced from the point of view of quantum information theory [18], more specifically the quantum entanglement theory [5]\u2013[8], it has been broadly applied to study problems in one-dimensional quantum many-body physics with great success.\nWhen comparing the MPS decomposition with the TD, the MPS decomposition has some significant advantages over the TD that can be employed to study the feature extraction and classification problem of big datasets. For instance, an MPS can be employed to represent a tensor which computational complexity increases polynomially with the increasing of the number of tensor orders as compared to the exponential growth in the case of using TD. In addition, due to its natural structure, an MPS is commonly represented by a set of local tensors of low orders, i.e. the maximum order of a local tensor is usually three, it allows us to update the local tensors modified by some local transformation without affecting the others. This is again a huge advantage in terms of saving computational complexity.\nTo benchmark our state-of-the-art method we have studied the pattern and image classification for a few well-known big datasets, e.g. the Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22] from the Computer Vision Laboratory located at the University of California San Diego. The results can be comparable with the ones obtained from other well-known methods. To our knowledge this is the\nar X\niv :1\n50 3.\n00 51\n6v 1\n[ cs\n.C V\n] 2\nM ar\n2 01\n5\nfirst paper applying the MPS decomposition in computer vision, pattern recognition and data classification which rely on feature extraction, feature selection and classification. However, without any restriction, the method can be potentially applied to study other data mining problems such as compressive sensing or unsupervised learning.\nThe rest of the paper is structured as follows. Section II reviews mathematical foundation that will be used throughout the paper. In Section III we state the problem of pattern recognition and data classification of tensors and provide a mathematical analysis comparing both TD and MPS decomposition. In Section IV we describe the MPS algorithm in detail. In Section V we showcase the algorithm with several experiments in pattern and image classification. Lastly, Section VI concludes the paper."}, {"heading": "II. MATHEMATICAL FOUNDATION", "text": "A tensor is a multidimensional array and its order (also known as ways or modes) is the number of dimensions it contains. Zero-order tensors are scalars and denoted by lowercase letters, e.g., a. A first-order tensor is a vector, which we denote by boldface lowercase letters, e.g., a. A matrix is a tensor of order two and is defined by boldface capital letters, e.g., A. A higher-order tensor (tensors of order three and above) are denoted by boldface calligraphic letters, e.g., X . Therefore a general Nth-order tensor can be defined as X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN , where each Ii is the dimension of the local subspace i. We denote ai as the ith entry of a vector a and aij as an element of a matrix A. The element of a third-order tensor X is denoted as xijk and thus defined similarly for a general Nth-order tensor. Indices will range from 1 to their captial version, e.g., i = 1, . . . , I or \u03b4 = 1, . . . ,\u2206 for Greek letters. The nth element in a sequence of tensors is denoted with a superscript in parentheses, e.g., A(n) is the nth matrix. The n-mode matrix product of a tensor X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN and a matrix A \u2208 RJ\u00d7In is denoted by X\u00d7nA, and results in an N th-order tensor of size I1\u00d7\u00b7 \u00b7 \u00b7\u00d7In\u22121\u00d7J\u00d7In+1\u00d7\u00b7 \u00b7 \u00b7\u00d7IN . The index notation is given by\n(X \u00d7n A)i1\u00b7\u00b7\u00b7in\u22121jin+1\u00b7\u00b7\u00b7iN = In\u2211 in=1 xi1\u00b7\u00b7\u00b7in\u00b7\u00b7\u00b7iNajin . (1)\nThe multiplication in all possible modes (n = 1, . . . , N) of the tensor X with a set of matrices A(n) is denoted as\nX \u00d7 {A} = X \u00d71 A(1) \u00d72 A(2) \u00b7 \u00b7 \u00b7 \u00d7N A(N). (2)\nThe matricization, also known as unfolding or flattening, is the procedure to transform a tensor into a matrix. The mode-n matricization of a tensor X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN is denoted by X(n) \u2208 RIn\u00d7(I1\u00b7\u00b7\u00b7In\u22121In+1\u00b7\u00b7\u00b7IN ), which arranges the mode-n fibers to be the columns of the resulting matrix. The definition of reshaping a tensor X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN is changing from the N th-order to another order as long as the\nexact number of parameters is kept, e.g. the matricization of X to X(i1\u00b7\u00b7\u00b7in) is said to be reshaping X to a (i1 \u00b7 \u00b7 \u00b7 in) \u00d7 (in+1 \u00b7 \u00b7 \u00b7 iN ) matrix.\nGraphical notation has been used extensively [10] in order to communicate more efficiently the ideas of tensor decompositions. In a tensor network diagram (TND), tensors are represented as shapes and their corresponding indices are lines protruding from these shapes as shown in Fig 1. A tensor contraction or Einstein summation can be performed by connecting common indices in the diagram. For example the n-mode matrix product defined in Eq. (1) can be easily converted to a TND shown in Fig 2. For the remainder of this paper we will use TND\u2019s to support explanations."}, {"heading": "III. TENSOR DECOMPOSITIONS FOR MULTILINEAR CLASSIFICATION", "text": ""}, {"heading": "A. Problem formulation", "text": "In this section we present the general problem of feature extraction and classification [2] for a set of K tensors and derive a new concept based on MPS to address the problem.\nProblem statement: Given a set of K training samples represented by Nth-order tensors X (k) \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN (k = 1, 2, . . . ,K) corresponding to Q categories, and a set of T test data X (t) \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN (t = 1, 2, . . . , T ), classify the test data into the categories Q with high accuracy.\nThis can be achieved by the following steps: \u2022 Step 1: Find the set of common factors and correspond-\ning core feature tensor from the training data X (k). \u2022 Step 2: Perform feature extraction on the test data X (t)\nusing the basis factors found from the training data. \u2022 Step 3: Classify the test data. The classification problem is a supervised problem where the categories Q are defined according to the problem\nand the data provided. Our main result utilizes the MPS decomposition for the training data in Step 1 to obtain the common factors and core tensor. The features from the training and test data can be used with any classification method such as Support Vector Machine (SVM) or K-Nearest Neighbours (KNN). The following subsections describe how to mathematically model the problem in TD and MPS decomposition."}, {"heading": "B. Tucker decomposition for feature extraction and classification", "text": "Given an N th-order tensor X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN , the Tucker decomposition [1] is defined as\nX = G \u00d71 A(1) \u00d72 A(2) \u00b7 \u00b7 \u00b7 \u00d7N A(N) + E = G \u00d7 {A}+ E (3)\nwhere G \u2208 R\u22061\u00d7\u22062\u00d7\u00b7\u00b7\u00b7\u00d7\u2206N is known as the core tensor and each In \u00d7\u2206n matrix A(n) (\u2206i \u2264 Ii) is known as a factor matrix, E denotes the approximation error. For simplicity if we assume I1 = I2 = \u00b7 \u00b7 \u00b7 = IN = I and \u22061 = \u22062 = \u00b7 \u00b7 \u00b7 = \u2206N = \u2206, then the TD representation of X consists of O(NI\u2206 + \u2206N ) parameters, which is exponential in N and thus is only suitable for small N . In feature extraction, for sets of K N th-order tensors X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN , the TD can be considered as\nX (k) = G(k) \u00d71 A(1) \u00d72 A(2) \u00b7 \u00b7 \u00b7 \u00d7N A(N) + E(k), (4)\nwhere {A(i)}Ni=1 is the common set of factor matrices obtained for each core tensor X (k), for k = 1, . . . ,K. However, one can choose a more effective way to represent the TD for the set of all K N th-order tensors in a single equation by concatenating all tensors X (k) such that\nY = [X (1)X (2) \u00b7 \u00b7 \u00b7X (K)] \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN\u00d7K (5)\nthen its TD is\nY = G \u00d71 A(1) \u00d72 A(2) \u00b7 \u00b7 \u00b7 \u00d7N A(N) + E, (6)\nwhere Y ,E \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN\u00d7K and G \u2208 R\u22061\u00d7\u22062\u00d7\u00b7\u00b7\u00b7\u00d7\u2206N\u00d7K is an (N + 1)th-order tensor, see Fig. 3 for a TND of the core tensor and factor matrices. As we can see, the number of parameters in the core tensor increases exponentially with the number of orders N in\nthe tensor Y . Accordingly, this representation can make simulation intractable by increasing N . Alternatively, in what follows we apply a different decomposition to represent the tensor Y in terms of a MPS decomposition such that the number of parameters only depends polynomially in N , and more importantly the number of parameters in the core tensor does not directly depend on N ."}, {"heading": "C. MPS decomposition for feature extraction and classification", "text": "The MPS decomposition of the tensor Y in Eq. (5) is defined as\nyi1\u00b7\u00b7\u00b7k\u00b7\u00b7\u00b7iN = a (1) i1 \u00b7 \u00b7 \u00b7A(n\u22121)in\u22121 A (n) k A (n+1) in+1 \u00b7 \u00b7 \u00b7 a(M)iM (7)\nwhere the kth index is positioned at n and M = N + 1. For each k and ij (j 6= 1, N ), the corresponding matrix A(\u00b7) has the size \u2206(j\u22121) \u00d7 \u2206j . For each i1, iM , a(1) and a(M) correspond to row and column vectors of size \u22061 and \u2206(M\u22121), respectively. See Fig 4 for a TND of Eq. (7).\nGiven the assumptions of index sizes stated previously in Section III-B, the MPS consists of O((M \u2212 2)I\u22062 + 2I\u2206) parameters, which is polynomial in M . Also, if we choose \u2206 = IM/2, the MPS can exactly represent the tensor Y without any approximation. However, this will result into a large computational complexity. In practice, when dealing with high dimensional tensors, the MPS decomposition is employed as an approximation representation of the tensor such that \u2206 can be chosen to be much smaller than IM/2.\nIn the above MPS representation one can easily represent it in such a way that each link between the tensors can be described by an orthonormal base, this is known as a canonical MPS decomposition. For instance, an MPS is in a left-canonical form if it satisfies the following conditions\u2211\nin (A(in))\u2020A(in) = I, (n = 2, . . . ,M \u2212 1) (8)\u2211 in (a(in))\u2020a(in) = I, (n = 1) (9)\nand right-canonical form if\u2211 in\nC(in)(C(in))\u2020 = I, (n = 2, . . . ,M \u2212 1) (10)\u2211 in c(in)(c(in))\u2020 = I, (n = M) (11)\nwhere I is the identity matrix and (A(in))\u2020 represents the comjugate transpose of A(in). These conditions can be visualized as diagrams shown in Fig. 5.\nThere is also a mixed-canonical form which can be used for feature extraction and classification. In this form the tensors to the left and right of A(n) are the factor matrices/vectors A/a and C/c, which satisfy left- and rightcanonical form conditions, respectively. We also relabel A(n) to G to highlight that it is the core tensor consisting of K matrices, see Fig. 6.\nRecall that in feature extraction for K N th-order training data we need to find the MPS decomposition of Y defined in Eq. (5). As it will be shown later, by representing the MPS in the mixed-canonical form, we can extract the core tensor and the common factors of Y directly. Furthermore, the order of the core tensor is independent of N which is contrary to the case of using TD."}, {"heading": "IV. MPS ALGORITHM FOR FEATURE EXTRACTION", "text": "In this section we will present the procedure to solve the problem of multilinear classification stated in Section III-A using the MPS decomposition. In what follows, we firstly introduce the scheme to decompose the training data tensor into the mixed-canonical MPS. We then apply the alternating least squares (ALS) method to find the optimal MPS representation of the training data tensor which contains the core tensor and the basis factors. Using the basis factors from the MPS of training data, we can extract the core tensor for the test data tensor. In Section V we will apply classification methods to train the classification model with the training\ncore tensor and then proceed to classify the test data based on the test core tensor."}, {"heading": "A. Mixed-canonical form of the MPS", "text": "Suppose that the training data is represented by an (N + 1)th-order tensor Y defined in Eq. (5). The algorithm to obtain the mixed-canonical MPS decomposition of Y is divided into two sweeps (left-to-right and right-to-left) of an iterative SVD algorithm. These sweeps are needed to find each of the left-canonical and right-canonical common factors of the MPS decomposition. Each sweep is completed when the next index to be evaluated is k, which will be the core tensor.\nAssume that the k index is positioned at n. The following two steps are performed consecutively to obtain the mixedcanonical form:\n1) Left-to-right sweep: The left-to-right sweep involves acquiring common factors 1, . . . , n \u2212 1 in left-canonical form. First perform a mode-i1 matricization on Y to obtain the matrix Z = Y(1) of size I1 \u00d7 (I2..K..IM ). In terms of elements, yi1..k..iM = zi1,(i2..k..iM ), then the SVD of Z gives\nyi1..k..iM = zi1,(i2..k..iM )\n= \u22061\u2211 \u03b41=1 ui1,\u03b41d\u03b41,\u03b41v \u2217 \u03b41,(i2..k..iM )\n= \u22061\u2211 \u03b41=1 a (1) i1,\u03b41 t\u03b41,(i2..k..iM ). (12)\nIn Eq. (12) T = DV\u2020 and since U is left-orthogonal and contains the index i1, we let a(1) = U be the first common factor. Next reshape T to a (\u22061I2) \u00d7 (I3..K..IM ) matrix, then the SVD of T gives,\nyi1..k..iM = \u22061,\u22062\u2211 \u03b41,\u03b42=1 a (1) i1,\u03b41 u(\u03b41i2),\u03b42d\u03b42,\u03b42v \u2217 \u03b42,(i3..k..iM )\n= \u22061,\u22062\u2211 \u03b41,\u03b42=1 a (1) i1,\u03b41 a (2) \u03b41,i2,\u03b42 t\u03b42,(i3..k..iM ). (13)\nSimilar to Eq. (12), in Eq. (13) we had T = DV\u2020 and since U is left-orthogonal, we obtain common factor A(2) of dimension \u22061 \u00d7 I2 \u00d7 \u22062 from reshaping U to a thirdorder tensor. The process above is repeated until we obtain all common factors up to A(n\u22121) and the last T matrix has dimension \u2206n\u22121 \u00d7 (K..IM ).\n2) Right-to-left sweep: The right-to-left sweep involves acquiring common factors n + 1, . . . ,M in right-canonical form. The last T matrix from the left-to-right sweep is reshaped to (\u2206n\u22121K..IM\u22121)\u00d7 IM and its SVD leads to a right-orthogonal common factor c(M) = V\u2020. Consequently we acquire common factors n+1, . . . ,M\u22121 by performing successive SVD\u2019s and at each instance reshaping the right\northogonal matrix V\u2020 to a third-order tensor. Finally, reshape the last T matrix of dimension (\u2206n\u22121K) \u00d7 \u2206n to a third order tensor to obtain the core tensor G as shown in the following equations,\nyi1..k..iM = \u2206n\u22121,\u2206n\u2211 \u03b4n\u22121,\u03b4n=1 Lt(\u03b4n\u22121k)\u03b4nR (14)\n= \u2206n\u22121,\u2206n\u2211 \u03b4n\u22121,\u03b4n=1 Lg\u03b4n\u22121,k,\u03b4nR (15)\nwith\nL = \u22061,...,\u2206n\u22122\u2211 \u03b41,...,\u03b4n\u22122=1 a (1) i1,\u03b41 \u00b7 \u00b7 \u00b7 a(n\u22121)\u03b4n\u22122,in\u22121,\u03b4n\u22121 (16)\nand\nR = \u2206n+1,...,\u2206M\u22121\u2211 \u03b4n+1,...,\u03b4M\u22121=1 a (n+1) \u03b4n,in+1,\u03b4n+1 \u00b7 \u00b7 \u00b7 a(M)\u03b4M\u22121,iM (17)\nEq. (15) is the mixed-canonical form of Y and an example of the two-step process for a (6 + 1)th-order tensor can be seen in Fig. 7. Hence we have completed Step 1 of the the feature extraction and classification problem.\nNote that the position of the core tensor n can be varied. Intuitively it is better to position the core tensor between two indices where it can achieve the maximum bond dimension so as to increase the number of features available to the core tensor.\nAn ALS method can be used to variationally optimize each of the common factors. After the ALS procedure is completed, to ensure we can extract the training core tensor with minimal error it is important to be certain that the MPS decomposition is in mixed-canonical form. This can be achieved by using an iterative sweep on the MPS as explained in detail by Schollwo\u0308ck [9]."}, {"heading": "B. Feature extraction of test data", "text": "To solve Step 2 we can directly apply the common factors obtained from the mixed-canonical MPS decomposition to extract the core tensor for the test data tensor. However, for more optimized results you can use the common factors obtained after the ALS method.\nConsider the concatenation of test data X (t), then we can also represent this as a tensor\nW = [X (1)X (2) \u00b7 \u00b7 \u00b7X (T )] \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN\u00d7T . (18)\nSubsequently the test core tensor can be extracted from this concatenated tensor by projecting common factors. The resultant test core tensor will also be third-order and consist of the same dimension of features as the training core tensor. An example for the extraction of the test core tensor from a (6 + 1)th-order test data tensor can be seen in the TND in Fig. 8."}, {"heading": "V. EXPERIMENTS", "text": "The algorithm was tested with three datasets: The Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22]. Fig 9 displays the images of all classes in each dataset."}, {"heading": "A. Experimental conditions", "text": "1) COIL-20: The database consists of 1440 grayscale images of 20 objects (72 images per object) with different reflectance and complex geometric characteristics. Each object was rotated 360 degrees as 72 images were taken per object; one every 5 degrees of rotation. All images used were initially 128\u00d7 128 pixels and then downsampled to 32\u00d7 32 grayscale (0-255).\nImages were randomly divided into two partitions according to a hold/out ratio. For our the experiment we tested with hold/out ratios of 10%, 30%, 50%, 70%, 90% and 95%, e.g. a 10% hold/out ratio accounts to 10% test data and 90% training data. The training and test data were structured as third-order tensors of dimensions K\u00d732\u00d732 and T\u00d732\u00d732,\nrespectively. The test core tensor extracted was of size 32 \u00d7 T \u00d7 32. This experiment was conducted to initially test the performance of the MPS algorithm for training data with trivial matrix form. The results were averaged over 10 trials.\n2) COIL-100: This database has 7200 color images of 100 objects (72 images per object) with different reflectance and complex geometric characteristics. Similar to COIL-20 each object was rotated 360 degrees as 72 images were taken per object; one every 5 degrees of rotation. The hold/out ratios for training and test data were also tested with 10%, 30%, 50%, 75%, 90% and 95%. Images were originally 128\u00d7128\u00d73 pixels and downsampled to 32\u00d732\u00d73 pixels. The training and test data were constructed as fourth-order tensors of dimensions K\u00d732\u00d732\u00d73 and T \u00d732\u00d732\u00d73, respectively. The test core tensor extracted was of size 32\u00d7 T \u00d7 32. The results were averaged over 10 trials.\n3) Extended Yale Face database B: The database contains 16128 grayscale images with 28 human subjects under 9 poses, where for each pose there is 64 illumination conditions. Similar to [3], to improve computational time each image was cropped to keep only the center area containing the face, then resized to 73 x 55. Training data and test data was not selected randomly but partitioned according to poses. For training and test data we selected poses 0, 2, 4, 6 and 8 and 1, 3, 5, and 7, respectively. For a single subject the training tensor was size 5 \u00d7 73 \u00d7 55 \u00d7 64 and the test tensor was size 4\u00d7 73\u00d7 55\u00d7 64. Hence for all 28 subjects we had a training fourth-order tensors 140\u00d773\u00d755\u00d764 and 112\u00d7 73\u00d7 55\u00d7 64 for training and test data, respectively. The test core tensor extracted was of size 55\u00d7 T \u00d7 55."}, {"heading": "B. Results", "text": "1) COIL-20: The classification algorithm used was KNN with correlational distance. The classification accuracy was plotted versus the bond dimension and the number of features used is the square of the bond dimension. Six plots are compared for hold/out ratios in Fig. 10 and the highest classification accuracy came from a 10% hold/out ratio as expected, with a 100% classification accuracy in several bond dimensions. With 30% hold/out ratio the highest accuracy was 99.79%. At 50% hold/out ratio the highest accuracy is 99.35%. The remaining hold/out ratios 70%, 90% and 95% had maximum classification accuracies of 97.64%, 89.95% and 81.18%, respectively.\n2) COIL-100: Similar to COIL-20 the classification algorithm used was KNN with correlational distance. Six plots are compared for hold/out ratios in Fig. 11 and the highest classification accuracy came from a 10% hold/out ratio with a 99.85% classification accuracy. With 30% hold/out ratio the highest accuracy was 99.74%. At 50% hold/out ratio the highest accuracy is 99.22%. The remaining hold/out ratios 75%, 90% and 95% had maximum classification accuracies of 96.65%, 89.14% and 80.59%, respectively.\n3) Extended Yale Face database B: Four classification methods were used to compare performance: SVM oneagainst-one (1v1), SVM one-against-all (1vall), KNN 1 and KNN 2 algorithms. Classification accuracy was plotted versus bond dimension as shown in Fig. 12. Specifically, the 1v1 SVM algorithm obtains the highest accuracy with 93.75% compared to the 1vall SVM with a maximum accuracy of 88.39%. Using KNN, we see that KNN 1 and KNN 2 have classification accuracy rates of 91.07% and 89.29%, respectively."}, {"heading": "C. Analysis", "text": "The bond dimension has a direct affect on the classification of the test data. In all experiments a small bond dimension was only needed to achieve high classification accuracies, however the accuracy does not necessarily increase at higher bond dimensions. This is because a larger number of features does not necessarily correlate to increased classification performance.\nIn the COIL-20 and COIL-100 results it was expected that larger hold/out ratios decreases the classification performance and this can be seen by the decreased accuracies at a hold/out ratio of 95%. For COIL-100 our algorithm outperformed several methods [23]\u2013[25] at the 75% hold/out ratio. This is also using the full color image as opposed to the converted grayscale image for these experiments.\nThe classification performance for the Extended Yale\nFace database B (EYFB) is much more stable, has lower computational complexity, and has higher classification accuracies from low to high bond dimensions compared to Direct General Tensor Discriminant Analysis (DGTDA) and Constrained Multilinear Discriminant Analysis (CMDA) proposed recently by Li & Schonfeld [3] as well as other methods mentioned in their paper.\nIt is imperative to highlight that the experiments conducted are not meant to outperform all current techniques in image and pattern analysis but to show that the MPS decomposition can be used to represent a generic multidimensional dataset using low-order tensors with polynomial computational complexity."}, {"heading": "VI. CONCLUSION", "text": "The classification performance has shown that the MPS decomposition can be used as an efficient and simple mechanism in representing multidimensional datasets for feature extraction and classification with supervised learning. Furthermore the core tensor required to sufficiently classify data was of a greatly reduced order and dimension, showing that only a small number of features is necessary to classify multidimensional datasets in pattern and image analysis. From these promising results we hope to pursue multiple directions that can have a major impact in many fields of research. One direction is the investigation of state-of-theart techniques to increase classification accuracies. Other directions can be the reformulation of research problems in signal processing, compressive sensing or remote sensing in terms of the MPS decomposition."}], "references": [{"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Review 51(2009), pp. 455\u2013500.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor decompositions for feature extraction and classification of high dimensional datasets", "author": ["A.H. Phan", "A. Cichocki"], "venue": "Nonlinear Theory and Its Applications, IEICE 1(2010), pp. 37\u201368.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Multilinear discriminant analysis for higher-order tensor data classification", "author": ["Q. Li", "D. Schonfeld"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 36(2014), pp. 2524\u20132537.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilinear analysis of image ensembles: Tensorfaces", "author": ["M.A.O. Vasilescu", "D. Terzopoulos"], "venue": "Proceedings of the European Conference on Computer Vision (ECCV \u201902) 2350(2002), pp. 447\u2013460.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient classical simulation of slightly entangled quantum computation", "author": ["G. Vidal"], "venue": "Phys. Rev. Lett. 91(2003), p. 147902.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient simulation of one-dimensional quantum many-body systems", "author": ["\u2014\u2014"], "venue": "Phys. Rev. Lett. 93(2004), p. 040502.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Density matrix renormalization group and periodic boundary conditions: A quantum information perspective", "author": ["F. Verstraete", "D. Porras", "J.I. Cirac"], "venue": "Phys. Rev. Lett. 93(2004), p. 227205.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Matrix product state representations", "author": ["D. P\u00e9rez-Garc\u0131\u0301a", "F. Verstraete", "M. Wolf", "J. Cirac"], "venue": "Quantum Information and Computation 7(2007), pp. 401\u2013430.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "The density-matrix renormalization group in the age of matrix product states", "author": ["U. Schollw\u00f6ck"], "venue": "Annals of Physics 326(2011), pp. 96\u2013192.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "A practical introduction to tensor networks: matrix product states and projected entangled pair states", "author": ["R. Or\u00fas"], "venue": "Annals of Physics 349(2014), pp. 117\u2013158.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Multidimensional filtering based on a tensor approach", "author": ["D. Muti", "S. Bourennane"], "venue": "Signal Process. 85(2005), pp. 2338\u2013 2353.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Tensor decompositions for signal processing applications: From two-way to multiway component analysis", "author": ["A. Cichocki", "D. Mandic", "L. De Lathauwer", "G. Zhou", "Q. Zhao", "C. Caiafa", "H. Phan"], "venue": "Signal Processing Magazine, IEEE 32(2015), pp. 145\u2013163.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "A tensor-based approach for big data representation and dimensionality reduction", "author": ["L. Kuang", "F. Hao", "L. Yang", "M. Lin", "C. Luo", "G. Min"], "venue": "Emerging Topics in Computing, IEEE Transactions on 2(2014), pp. 280\u2013291.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor-train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM Journal on Scientific Computing 33(2011), pp. 2295\u20132317.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix product ground states for one-dimensional spin-1 quantum antiferromagnets", "author": ["A. Kl\u00fcmper", "A. Schadschneider", "J. Zittartz"], "venue": "EPL (Europhysics Letters) 24(1993), p. 293.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1993}, {"title": "Rigorous results on valence-bond ground states in antiferromagnets", "author": ["I. Affleck", "T. Kennedy", "E.H. Lieb", "H. Tasaki"], "venue": "Phys. Rev. Lett. 59(1987), pp. 799\u2013802.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1987}, {"title": "Thermodynamic limit of density matrix renormalization", "author": ["S. \u00d6stlund", "S. Rommer"], "venue": "Phys. Rev. Lett. 75(1995), pp. 3537\u2013 3540.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "Quantum computation and quantum information", "author": ["M.A. Nielsen", "I.L. Chuang"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "Technical Report CUCS-005-96, Feb 1996.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "Columbia object image library (coil-100)", "author": ["\u2014\u2014"], "venue": "Technical Report CUCS-005-96, Feb 1996.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1996}, {"title": "Support vector machines for 3d object recognition", "author": ["M. Pontil", "A. Verri"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 20(1998), pp. 637\u2013646.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "From few to many: illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 23(2001), no. 6, pp. 643\u2013 660.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning to recognize 3d objects with snow", "author": ["M.-H. Yang", "D. Roth", "N. Ahuja"], "venue": "Computer Vision - ECCV 2000, ser. Lecture Notes in Computer Science. Springer Berlin Heidelberg, 1842(2000), pp. 439\u2013454.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Random subwindows for robust image classification", "author": ["R. Maree", "P. Geurts", "J. Piater", "L. Wehenkel"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on 1(2005), pp. 34\u201340.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Object recognition methods based on transformation covariant features", "author": ["J. Matas", "S. Obdrz\u0306\u00e1lek"], "venue": "Proc. 12th European Signal Processing Conference (EUSIPCO 2004), 2004.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "One such method is to approximate multidimensional datasets in terms of tensor decompositions [1].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "This method has been of great interest within the last decade and successfully applied in a diverse range of research areas such as data classification [2], [3], computer vision [4], quantum many-body physics [5]\u2013[10] and signal processing [11], [12].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "This method has been of great interest within the last decade and successfully applied in a diverse range of research areas such as data classification [2], [3], computer vision [4], quantum many-body physics [5]\u2013[10] and signal processing [11], [12].", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "This method has been of great interest within the last decade and successfully applied in a diverse range of research areas such as data classification [2], [3], computer vision [4], quantum many-body physics [5]\u2013[10] and signal processing [11], [12].", "startOffset": 178, "endOffset": 181}, {"referenceID": 4, "context": "This method has been of great interest within the last decade and successfully applied in a diverse range of research areas such as data classification [2], [3], computer vision [4], quantum many-body physics [5]\u2013[10] and signal processing [11], [12].", "startOffset": 209, "endOffset": 212}, {"referenceID": 9, "context": "This method has been of great interest within the last decade and successfully applied in a diverse range of research areas such as data classification [2], [3], computer vision [4], quantum many-body physics [5]\u2013[10] and signal processing [11], [12].", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "This method has been of great interest within the last decade and successfully applied in a diverse range of research areas such as data classification [2], [3], computer vision [4], quantum many-body physics [5]\u2013[10] and signal processing [11], [12].", "startOffset": 240, "endOffset": 244}, {"referenceID": 11, "context": "This method has been of great interest within the last decade and successfully applied in a diverse range of research areas such as data classification [2], [3], computer vision [4], quantum many-body physics [5]\u2013[10] and signal processing [11], [12].", "startOffset": 246, "endOffset": 250}, {"referenceID": 1, "context": "It has present applications in various fields such as neuroscience, pattern analysis, image classification and signal processing [2], [12], [13].", "startOffset": 129, "endOffset": 132}, {"referenceID": 11, "context": "It has present applications in various fields such as neuroscience, pattern analysis, image classification and signal processing [2], [12], [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "It has present applications in various fields such as neuroscience, pattern analysis, image classification and signal processing [2], [12], [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "In this paper we solve the feature extraction and classification problem utilizing the matrix product state decomposition [8], [10] (also known as the tensor-train (TT) decomposition [14]).", "startOffset": 122, "endOffset": 125}, {"referenceID": 9, "context": "In this paper we solve the feature extraction and classification problem utilizing the matrix product state decomposition [8], [10] (also known as the tensor-train (TT) decomposition [14]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "In this paper we solve the feature extraction and classification problem utilizing the matrix product state decomposition [8], [10] (also known as the tensor-train (TT) decomposition [14]).", "startOffset": 183, "endOffset": 187}, {"referenceID": 0, "context": "Note that previous literature has categorized that the MPS and TT decompositions [1] are equivalent, however the concept of MPS has already been used for decades in the field of quantum physics [15]\u2013[17] prior to its introduction in the mathematics community.", "startOffset": 81, "endOffset": 84}, {"referenceID": 14, "context": "Note that previous literature has categorized that the MPS and TT decompositions [1] are equivalent, however the concept of MPS has already been used for decades in the field of quantum physics [15]\u2013[17] prior to its introduction in the mathematics community.", "startOffset": 194, "endOffset": 198}, {"referenceID": 16, "context": "Note that previous literature has categorized that the MPS and TT decompositions [1] are equivalent, however the concept of MPS has already been used for decades in the field of quantum physics [15]\u2013[17] prior to its introduction in the mathematics community.", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "Since the concept of MPS was introduced from the point of view of quantum information theory [18], more specifically the quantum entanglement theory [5]\u2013[8], it has been broadly applied to study problems in one-dimensional quantum many-body physics with great success.", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "Since the concept of MPS was introduced from the point of view of quantum information theory [18], more specifically the quantum entanglement theory [5]\u2013[8], it has been broadly applied to study problems in one-dimensional quantum many-body physics with great success.", "startOffset": 149, "endOffset": 152}, {"referenceID": 7, "context": "Since the concept of MPS was introduced from the point of view of quantum information theory [18], more specifically the quantum entanglement theory [5]\u2013[8], it has been broadly applied to study problems in one-dimensional quantum many-body physics with great success.", "startOffset": 153, "endOffset": 156}, {"referenceID": 18, "context": "the Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22] from the Computer Vision Laboratory located at the University of California San Diego.", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "the Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22] from the Computer Vision Laboratory located at the University of California San Diego.", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "the Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22] from the Computer Vision Laboratory located at the University of California San Diego.", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "the Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22] from the Computer Vision Laboratory located at the University of California San Diego.", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "Graphical notation has been used extensively [10] in order to communicate more efficiently the ideas of tensor decompositions.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "In this section we present the general problem of feature extraction and classification [2] for a set of K tensors and derive a new concept based on MPS to address the problem.", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "Tucker decomposition for feature extraction and classification Given an N th-order tensor X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN , the Tucker decomposition [1] is defined as", "startOffset": 135, "endOffset": 138}, {"referenceID": 8, "context": "This can be achieved by using an iterative sweep on the MPS as explained in detail by Schollw\u00f6ck [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 18, "context": "The algorithm was tested with three datasets: The Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "The algorithm was tested with three datasets: The Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "The algorithm was tested with three datasets: The Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "The algorithm was tested with three datasets: The Columbia University Image Libraries COIL-20 [19] and COIL-100 [20], [21], and the Extended Yale B dataset [22].", "startOffset": 156, "endOffset": 160}, {"referenceID": 2, "context": "Similar to [3], to improve computational time each image was cropped to keep only the center area containing the face, then resized to 73 x 55.", "startOffset": 11, "endOffset": 14}, {"referenceID": 22, "context": "For COIL-100 our algorithm outperformed several methods [23]\u2013[25] at the 75% hold/out ratio.", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "For COIL-100 our algorithm outperformed several methods [23]\u2013[25] at the 75% hold/out ratio.", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "The classification performance for the Extended Yale Face database B (EYFB) is much more stable, has lower computational complexity, and has higher classification accuracies from low to high bond dimensions compared to Direct General Tensor Discriminant Analysis (DGTDA) and Constrained Multilinear Discriminant Analysis (CMDA) proposed recently by Li & Schonfeld [3] as well as other methods mentioned in their paper.", "startOffset": 364, "endOffset": 367}], "year": 2017, "abstractText": "Big data consists of large multidimensional datasets that would often be difficult to analyze if working with the original tensor. There is a rising interest in the use of tensor decompositions to approximate large tensors in order to reduce their dimensions by selecting important features for classification. Of particular interest is the Tucker decomposition (TD), which has already been applied in neuroscience, geoscience, signal processing, pattern and image recognition. However the decomposition itself leads to exponential computational time for high-order tensors. To circumvent this obstacle we propose an alternative known as the matrix product state (MPS) decomposition for the data representation of big data tensors. This decomposition has been used extensively in quantum physics within the last decade and its benefit has surprisingly not been seen in other areas of research. We prove that the MPS decomposition for feature extraction and classification in supervised learning can be implemented efficiently with high classification rates in pattern and image recognition. Keywords-Matrix product state, Tucker decomposition, highorder tensor, feature extraction, feature classification, image recognition, pattern classification, tensor-train", "creator": "LaTeX with hyperref package"}}}