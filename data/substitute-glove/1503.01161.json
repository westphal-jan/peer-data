{"id": "1503.01161", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification", "abstract": "We current the Bayesian Case Model (BCM ), giving official framework not Bayesian case - including reasoning (CBR) them variant category other coefficients. BCM brings the storytelling power from CBR to one Bayesian generative progress. The BCM finds setups, now \" beatnik \" analyzed could excellent than interacting in rather dataset, continued guest cooperation deterministic month cluster music, designing and especially piece. Simultaneously, BCM pursues under-representation by classes manifolds, all make of features even one thus singers in the characterization country called gearboxes. The prototype even subspace determining integrated modification maintain in interpretability with intellectual subtype timing. Human issue discovery reveal matched significant require to women ' understanding although standard particulars produced previous BCM, contrast to yet given once prior exhibits.", "histories": [["v1", "Tue, 3 Mar 2015 23:25:55 GMT  (1372kb,D)", "http://arxiv.org/abs/1503.01161v1", "Published in Neural Information Processing Systems (NIPS) 2014, Neural Information Processing Systems (NIPS) 2014"]], "COMMENTS": "Published in Neural Information Processing Systems (NIPS) 2014, Neural Information Processing Systems (NIPS) 2014", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["been kim", "cynthia rudin", "julie a shah"], "accepted": true, "id": "1503.01161"}, "pdf": {"name": "1503.01161.pdf", "metadata": {"source": "CRF", "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification", "authors": ["Been Kim", "Cynthia Rudin", "Julie Shah"], "emails": ["shah}@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "People like to look at examples. Through advertising, marketers present examples of people we might want to emulate in order to lure us into making a purchase. We might ignore recommendations made by Amazon.com and look instead at an Amazon customer\u2019s Listmania to find an example of a customer like us. We might ignore medical guidelines computed from a large number of patients in favor of medical blogs where we can get examples of individual patients\u2019 experiences.\nNumerous studies have demonstrated that exemplar-based reasoning, involving various forms of matching and prototyping, is fundamental to our most effective strategies for tactical decisionmaking ([26, 9, 21]). For example, naturalistic studies have shown that skilled decision makers in the fire service use recognition-primed decision making, in which new situations are matched to typical cases where certain actions are appropriate and usually successful [21]. To assist humans in leveraging large data sources to make better decisions, we desire that machine learning algorithms provide output in forms that are easily incorporated into the human decision-making process.\nStudies of human decision-making and cognition provided the key inspiration for artificial intelligence Case-Based Reasoning (CBR) approaches [2, 28]. CBR relies on the idea that a new situation can be well-represented by the summarized experience of previously solved problems [28]. CBR has been used in important real-world applications [24, 4], but is fundamentally limited, in that it does not learn the underlying complex structure of data in an unsupervised fashion and may not scale to datasets with high-dimensional feature spaces (as discussed in [29]).\nIn this work, we introduce a new Bayesian model, called the Bayesian Case Model (BCM), for prototype clustering and subspace learning. In this model, the prototype is the exemplar that is most representative of the cluster. The subspace representation is a powerful output of the model because we neither need nor want the best exemplar to be similar to the current situation in all possible ways:\nar X\niv :1\n50 3.\n01 16\n1v 1\n[ st\nat .M\nL ]\n3 M\nfor instance, a moviegoer who likes the same horror films as we do might be useful for identifying good horror films, regardless of their cartoon preferences. We model the underlying data using a mixture model, and infer sets of features that are important within each cluster (i.e., subspace). This type of model can help to bridge the gap between machine learning methods and humans, who use examples as a fundamental part of their decision-making strategies.\nWe show that BCM produces prediction accuracy comparable to or better than prior art for standard datasets. We also verify through human subject experiments that the prototypes and subspaces present as meaningful feedback for the characterization of important aspects of a dataset. In these experiments, the exemplar-based output of BCM resulted in statistically significant improvements to participants\u2019 performance of a task requiring an understanding of clusters within a dataset, as compared to outputs produced by prior art."}, {"heading": "2 Background and Related Work", "text": "People organize and interpret information through exemplar-based reasoning, particularly when they are solving problems ([26, 7, 9, 21]). AI Cased-Based Reasoning approaches are motivated by this insight, and provide example cases along with the machine-learned solution. Studies show that example cases significantly improve user confidence in the resulting solutions, as compared to providing the solution alone or by also displaying a rule that was used to find the solution [11]. However, CBR requires solutions (i.e. labels) for previous cases, and does not learn the underlying structure of the data in an unsupervised fashion. Maintaining transparency in complex situations also remains a challenge [29]. CBR models designed explicitly to produce explanations [1] rely on the backward chaining of the causal relation from a solution, which does not scale as complexity increases. The cognitive load of the user also increases with the complexity of the similarity measure used for comparing cases [14]. Other CBR models for explanations require the model to be manually crafted in advance by experts [25].\nAlternatively, the mixture model is a powerful tool for discovering cluster distributions in an unsupervised fashion. However, this approach does not provide intuitive explanations for the learned clusters (as pointed out in [8]). Sparse topic models are designed to improve interpretability by reducing the number of words per topic [32, 13]. However, using the number of features as a proxy for interpretability is problematic, as sparsity is often not a good or complete measure of interpretability [14]. Explanations produced by mixture models are typically presented as distributions over features. Even users with technical expertise in machine learning may have a difficult time interpreting such output, especially when the cluster is distributed over a large number of features [14].\nOur approach, the Bayesian Case Model (BCM), simultaneously performs unsupervised clustering and learns both the most representative cases (i.e., prototypes) and important features (i.e., subspaces). BCM preserves the power of CBR in generating interpretable output, where interpretability comes not only from sparsity but from the prototype exemplars.\nIn our view, there are at least three widely known types of interpretable models: sparse linear classifiers ([30, 8, 31]); discretization methods, such as decision trees and decision lists (e.g., [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.g., nearest neighbors [10] or a supervised optimization-based method [5]). (See [14] for a review of interpretable classification.) BCM is intended as the third model type, but uses unsupervised generative mechanisms to explain clusters, rather than supervised approaches [16] or by focusing myopically on neighboring points [3]."}, {"heading": "3 The Bayesian Case Model", "text": "Intuitively, BCM generates each observation using the important pieces of related prototypes. The model might generate a movie profile made of the horror movies from a quintessential horror movie watcher, and action movies from a quintessential action moviegoer.\nBCM begins with a standard discrete mixture model [18, 6] to represent the underlying structure of the observations. It augments the standard mixture model with prototypes and subspace feature indicators that characterize the clusters. We show in Section 4.2 that prototypes and subspace feature indicators improve human interpretability as compared to the standard mixture model output. The graphical model for BCM is depicted in Figure 1.\nWe start with N observations, denoted by x = {x1, x2, . . . , xN}, with each xi represented as a random mixture over clusters. There are S clusters, where S is assumed to be known in advance. (This assumption can easily be relaxed through extension to a non-parametric mixture model.) Vector \u03c0i are the mixture weights over these clusters for the ith observation xi, \u03c0i \u2208 RS+. Each observation has P features, and we denote the jth feature of the ith observation as xij . Each feature j of the observation xi comes from one of the clusters, the index of the cluster for xij is denoted by zij and the full set of cluster assignments for observation-feature pairs is denoted by z. Each zij takes on the value of a cluster index between 1 and S. Hyperparameters q, \u03bb, c, and \u03b1 are assumed to be fixed.\nThe explanatory power of BCM results from how the clusters are characterized. While a standard mixture model assumes that each cluster take the form of a predefined parametric distribution (e.g., normal), BCM characterizes each cluster by a prototype, ps, and a subspace feature indicator, \u03c9s. Intuitively, the subspace feature indicator selects only a few features that play an important role in identifying the cluster and prototype (hence, BCM clusters are subspace clusters). We intuitively define these latent variables below.\nPrototype, ps: The prototype ps for cluster s is defined as one observation in x that maximizes p(ps|\u03c9s, z,x), with the probability density and \u03c9s as defined below. Our notation for element j of ps is psj . Since ps is a prototype, it is equal to one of the observations, so psj = xij for some i. Note that more than one maximum may exist per cluster; in this case, one prototype is arbitrarily chosen. Intuitively, the prototype is the \u201cquintessential\u201d observation that best represents the cluster.\nSubspace feature indicator \u03c9s: Intuitively, \u03c9s \u2018turns on\u2019 the features that are important for characterizing cluster s and selecting the prototype, ps. Here, \u03c9s \u2208 {0, 1}P is an indicator variable that is 1 on the subset of features that maximizes p(\u03c9s|ps, z,x), with the probability for \u03c9s as defined below. Here, \u03c9s is a binary vector of size P , where each element is an indicator of whether or not feature j belongs to subspace s.\nThe generative process for BCM is as follows: First, we generate the subspace clusters. A subspace cluster can be fully described by three components: 1) a prototype, ps, generated by sampling uniformly over all observations, 1 . . . N ; 2) a feature indicator vector, \u03c9s, that indicates important features for that subspace cluster, where each element of the feature indicator (\u03c9sj) is generated according to a Bernoulli distribution with hyperparameter q; and 3) the distribution of feature outcomes for each feature, \u03c6s, for subspace s, which we now describe.\nDistribution of feature outcomes \u03c6s for cluster s: Here, \u03c6s is a data structure wherein each \u201crow\u201d \u03c6sj is a discrete probability distribution of possible outcomes for feature j. Explicitly, \u03c6sj is a vector of length Vj , where Vj is the number of possible outcomes of feature j. Let us define \u0398 as a vector of the possible outcomes of feature j (e.g., for feature \u2018color\u2019, \u0398 = [red, blue, yellow]), where \u0398v represents a particular outcome for that feature (e.g., \u0398v = blue). We will generate \u03c6s so that it mostly takes outcomes from the prototype ps for the important dimensions of the cluster. We do this by considering the vector g, indexed by possible outcomes v, as follows:\ngpsj ,\u03c9sj ,\u03bb(v) = \u03bb(1 + c1[wsj=1 and psj=\u0398v ]),\nwhere c and \u03bb are constant hyperparameters that indicate how much we will copy the prototype in order to generate the observations. The distribution of feature outcomes will be determined by g through \u03c6sj \u223c Dirichlet(gpsj ,\u03c9sj ,\u03bb). To explain at an intuitive level: First, consider the irrelevant dimensions j in subspace s, which have wsj = 0. In that case, \u03c6sj will look like a uniform distribu-\ntion over all possible outcomes for features j; the feature values for the unimportant dimensions are generated arbitrarily according to the prior. Next, consider relevant dimensions where wsj = 1. In this case, \u03c6sj will generally take on a larger value \u03bb+c for the feature value that prototype ps has on feature j, which is called \u0398v . All of the other possible outcomes are taken with lower probability \u03bb. As a result, we will be more likely to select the outcome \u0398v that agrees with the prototype ps. In the extreme case where c is very large, we can copy the cluster\u2019s prototype directly within the cluster\u2019s relevant subspace and assign the rest of the feature values randomly.\nAn observation is then a mix of different prototypes, wherein we take the most important pieces of each prototype. To do this, mixture weights \u03c0i are generated according to a Dirichlet distribution, parameterized by hyperparameter \u03b1. From there, to select a cluster and obtain the cluster index zij for each xij , we sample from a multinomial distribution with parameters \u03c0i. Finally, each feature for an observation, xij , is sampled from the feature distribution of the assigned subspace cluster (\u03c6zij ). (Note that Latent Dirichlet Allocation (LDA) [6] also begins with a standard mixture model, though our feature values exist in a discrete set that is not necessarily binary.) Here is the full model, with hyperparameters c, \u03bb, q, and \u03b1:\n\u03c9sj \u223c Bernoulli(q) \u2200s, j ps \u223c Uniform(1, N) \u2200s \u03c6sj \u223c Dirichlet(gpsj ,\u03c9sj ,\u03bb) \u2200s, j where gpsj ,\u03c9sj ,\u03bb(v) = \u03bb(1 + c1[wsj=1 and psj=\u0398v ]) \u03c0i \u223c Dirichlet(\u03b1) \u2200i zij \u223c Multinomial(\u03c0i) \u2200i, j xij \u223c Multinomial(\u03c6zijj) \u2200i, j.\nOur model can be readily extended to different similarity measures, such as standard kernel methods or domain specific similarity measures, by modifying the function g. For example, we can use the least squares loss i.e., for fixed threshold , gpsj ,\u03c9sj ,\u03bb(v) = \u03bb(1 + c1[wsj=1 and (psj\u2212\u0398v)2\u2264 ]); or, more generally, gpsj ,\u03c9sj ,\u03bb(v) = \u03bb(1 + c1[wsj=1 and `(psj ,\u0398v)\u2264 ]).\nIn terms of setting hyperparameters, there are natural settings for \u03b1 (all entries being 1). This means that there are three real-valued parameters to set, which can be done through cross-validation, another layer of hierarchy with more diffuse hyperparameters, or plain intuition. To use BCM for classification, vector \u03c0i is used as S features for a classifier, such as SVM."}, {"heading": "3.1 Motivating example", "text": "This section provides an illustrative example for prototypes, subspace feature indicators and subspace clusters, using a dataset composed of a mixture of smiley faces. The feature set for a smiley face is composed of types, shapes and colors of eyes and mouths. For the purpose of this example, assume that the ground truth is that there are three clusters, each of which has two features that are important for defining that cluster. In Table 1, we show the first cluster, with a subspace defined by the color (green) and shape (square) of the face; the rest of the features are not important for defining the cluster. For the second cluster, color (orange) and eye shape define the subspace. We generated 240 smiley faces from BCM\u2019s prior with \u03b1 = 0.1 for all entries, and q = 0.5, \u03bb = 1 and c = 50.\nBCM works differently to Latent Dirichlet Allocation (LDA) [6], which presents its output in a very different form. Table 1 depicts the representation of clusters in both LDA (middle column) and BCM (right column). This dataset is particularly simple, and we chose this comparison because the two most important features that both LDA and BCM learn are identical for each cluster. However, LDA does not learn prototypes, and represents information differently. To convey cluster information using LDA (i.e., to define a topic), we must record several probability distributions \u2013 one for each feature. For BCM, we need only to record a prototype (e.g., the green face depicted in the top row, right column of the figure), and state which features were important for that cluster\u2019s subspace (e.g., shape and color). For this reason, BCM is more succinct than LDA with regard to what information must be recorded in order to define the clusters. One could define a \u201cspecial\u201d constrained version of LDA with topics having uniform weights over a subset of features, and with \u201cword\u201d distributions centered around a particular value. This would require a similar amount of memory; however, it loses information, with respect to the fact that BCM carries a full prototype within it for each cluster.\nA major benefit of BCM over LDA is that the \u201cwords\u201d in each topic (the choice of feature values) are coupled and not assumed to be independent \u2013 correlations can be controlled depending on the choice of parameters. The independence assumption of LDA can be very strong, and this may be crippling for its use in many important applications. Given our example of images, one could easily generate an image with eyes and a nose that cannot physically occur on a single person (perhaps overlapping). BCM can also generate this image, but it would be unlikely, as the model would generally prefer to copy the important features from a prototype.\nBCM performs joint inference on prototypes, subspace feature indicators and cluster labels for observations. This encourages the inference step to achieve solutions where clusters are better represented by prototypes. We will show that this is beneficial in terms of predictive accuracy in Section 4.1. We will also show through an experiment involving human subjects that BCM\u2019s succinct representation is very effective for communicating the characteristics of clusters in Section 4.2."}, {"heading": "3.2 Inference: collapsed Gibbs sampling", "text": "We use collapsed Gibbs sampling to perform inference, as this has been observed to converge quickly, particularly in mixture models [17]. We sample \u03c9sj , zij , and ps, where \u03c6 and \u03c0 are integrated out. Note that we can recover \u03c6 by simply counting the number of feature values assigned to each subspace. Integrating out \u03c6 and \u03c0 results in the following expression for sampling zij :\np(zij = s|zi\u00acj ,x, p, \u03c9, \u03b1, \u03bb) \u221d \u03b1/S + n(s,i,\u00acj,\u00b7)\n\u03b1+ n \u00d7 g(psj , \u03c9sj , \u03bb) + n(s,\u00b7,j,xij)\u2211 s g(psj , \u03c9sj , \u03bb) + n(s,\u00b7,j,\u00b7) , (1)\nwhere n(s,i,j,v) = 1(zij = s, xij = v). In other words, if xij takes feature value v for feature j and is assigned to cluster s, then n(s,i,j,v) = 1, or 0 otherwise. Notation n(s,\u00b7,j,v) is the number of times that the jth feature of an observation takes feature value v and that observation is assigned to subspace cluster s (i.e., n(s,\u00b7,j,v) = \u2211 i 1(zij = s, xij = v)). Notation n(s,\u00b7,j,\u00b7) means sum over i and v. We use n(s,i,\u00acj,v) to denote a count that does not include the feature j. The derivation is similar to the standard collapsed Gibbs sampling for LDA mixture models [17].\nSimilarly, integrating out \u03c6 results in the following expression for sampling \u03c9sj :\np(\u03c9sj = b|q, psj , \u03bb, \u03c6,x, z, \u03b1) \u221d  q \u00d7 B(g(psj , 1, \u03bb) + n(s,\u00b7,j,\u00b7)) B(g(psj , 1, \u03bb)) b = 1 1\u2212 q \u00d7 B(g(psj , 0, \u03bb) + n(s,\u00b7,j,\u00b7))\nB(g(psj , 0, \u03bb)) b = 0,\n(2)\nwhere B is the Beta function and comes from integrating out \u03c6 variables, which are sampled from Dirichlet distributions."}, {"heading": "4 Results", "text": "In this section, we show that BCM produces prediction accuracy comparable to or better than LDA for standard datasets. We also verify the interpretability of BCM through human subject experiments involving a task that requires an understanding of clusters within a dataset. We show statistically\nsignificant improvements in objective measures of task performance using prototypes produced by BCM, compared to output of LDA. Finally, we visually illustrate that the learned prototypes and subspaces present as meaningful feedback for the characterization of important aspects of the dataset."}, {"heading": "4.1 BCM maintains prediction accuracy.", "text": "We show that BCM output produces prediction accuracy comparable to or better than LDA, which uses the same mixture model (Section 3) to learn the underlying structure but does not learn explanations (i.e., prototypes and subspaces). We validate this through use of two standard datasets: Handwritten Digit [19] and 20 Newsgroups [22]. We use the implementation of LDA available from [27], which incorporates Gibbs sampling, the same inference technique used for BCM.\nFigure 2a depicts the ratio of correctly assigned cluster labels for BCM and LDA. In order to compare the prediction accuracy with LDA, the learned cluster labels are provided as features to a support vector machine (SVM) with linear kernel, as is often done in the LDA literature on clustering [6]. The improved accuracy of BCM over LDA, as depicted in the figures, is explained in part by the ability of BCM to capture dependencies among features via prototypes, as described in Section 3. We also note that prediction accuracy when using the full 20 Newsgroups dataset acquired by LDA (accuracy: 0.68\u00b1 0.01) matches that reported previously for this dataset when using a combined LDA and SVM approach [33]. Also, LDA accuracy for the full Handwritten Digit dataset (accuracy: 0.76 \u00b1 0.017) is comparable to that produced by BCM using the subsampled dataset (70 samples per digit, accuracy: 0.77 \u00b1 0.03). As indicated by Figure 2b, BCM achieves high unsupervised clustering accuracy as a function of iterations. We can compute this measure for BCM because each cluster is characterized by a prototype \u2013 a particular data point with a label in the given datasets. (Note that this is not possible for LDA.) We set \u03b1 to prefer each \u03c0i to be sparse, so only one prototype generates each observation,\nand we use that prototype\u2019s label for the observation. Sensitivity analysis in Figure 2c indicates that the additional parameters introduced to learn prototypes and subspaces (i.e., q, \u03bb and c) are not too sensitive within the range of reasonable choices."}, {"heading": "4.2 Verifying the interpretability of BCM", "text": "We verified the interpretability of BCM by performing human subject experiments that incorporated a task requiring an understanding of clusters within a dataset. This task required each participant to assign 16 recipes, described only by a set of required ingredients (recipe names and instructions were withheld), to one cluster representation out of a set of four to six. (This approach is similar to those used in prior work to measure comprehensibility [20].) We chose a recipe dataset1 for this task because such a dataset requires clusters to be well-explained in order for subjects to be able to perform classification, but does not require special expertise or training.\nOur experiment incorporated a within-subjects design, which allowed for more powerful statistical testing and mitigated the effects of inter-participant variability. To account for possible learning effects, we blocked the BCM and LDA questions and balanced the assignment of participants into the two ordering groups: Half of the subjects were presented with all eight BCM questions first, while the other half first saw the eight LDA questions. Twenty-four participants (10 females, 14 males, average age 27 years) performed the task, answering a total of 384 questions. Subjects were encouraged to answer the questions as quickly and accurately as possible, but were instructed to take a 5-second break every four questions in order to mitigate the potential effects of fatigue.\nCluster representations (i.e., explanations) from LDA were presented as the set of top ingredients for each recipe topic cluster. For BCM we presented the ingredients of the prototype without the name of the recipe and without subspaces. The number of top ingredients shown for LDA was set as the number of ingredients from the corresponding BCM prototype and ran Gibbs sampling for LDA with different initializations until the ground truth clusters were visually identifiable.\nUsing explanations from BCM, the average classification accuracy was 85.9%, which was statistically significantly higher (c2(1, N = 24) = 12.15, p 0.001) than that of LDA, (71.3%). For both LDA and BCM, each ground truth label was manually coded by two domain experts: the first author and one independent analyst (kappa coefficient: 1). These manually-produced ground truth labels were identical to those that LDA and BCM predicted for each recipe. There was no statistically significant difference between BCM and LDA in the amount of time spent on each question (t(24) = 0.89, p = 0.37); the overall average was 32 seconds per question, with 3% more time spent on BCM than on LDA. Subjective evaluation using Likert-style questionnaires produced no statistically significant differences between reported preferences for LDA versus BCM. Interestingly, this suggests that participants did not have insight into their superior performance using output from BCM versus that from LDA.\n1Computer Cooking Contest: http://liris.cnrs.fr/ccc/ccc2014/\nOverall, the experiment demonstrated substantial improvement to participants\u2019 classification accuracy when using BCM compared with LDA, with no degradation to other objective or subjective measures of task performance."}, {"heading": "4.3 Learning subspaces", "text": "Figure 4a illustrates the learned prototypes and subspaces as a function of sampling iterations for the Handwritten Digit dataset. For the later iterations, shown on the right of the figure, the BCM output effectively characterizes the important aspects of the data. In particular, the subspaces learned by BCM are pixels that define the digit for the cluster\u2019s prototype.\nInterestingly, the subspace highlights the absence of writing in certain areas. This makes sense: For example, one can define a \u20187\u2019 by showing the absence of pixels on the left of the image where the loop of a \u20189\u2019 might otherwise appear. The pixels located where there is variability among digits of the same cluster are not part of the defining subspace for the cluster.\nBecause we initialized randomly, in early iterations, the subspaces tend to identify features common to the observations that were randomly initialized to the cluster. This is because \u03c9s assigns higher likelihood to features with the most similar values across observations within a given cluster. For example, most digits \u2018agree\u2019 (i.e., have the same zero pixel value) near the borders; thus, these are the first areas that are refined, as shown in Figure 4a. Over iterations, the third row of Figure 4a shows how BCM learns to separate the digits \u201c3\u201d and \u201c5,\u201d which tend to share many pixel values in similar locations. Note that the sparsity of the subspaces can be customized by hyperparameter q.\nNext, we show results for BCM using the Computer Cooking Contest dataset in Figure 4b. Each prototype consists of a set of ingredients for a recipe, and the subspace is a set of important ingredients that define that cluster, highlighted in red boxes. For instance, BCM found a \u201cchili\u201d cluster defined by the subspace \u201cbeer,\u201d \u201cchili powder,\u201d and \u201ctomato.\u201d A recipe called \u201cGeneric Chili Recipe\u201d was chosen as the prototype for the cluster. (Note that beer is indeed a typical ingredient in chili recipes.)"}, {"heading": "5 Conclusion", "text": "The Bayesian Case Model provides a generative framework for case-based reasoning and prototypebased modeling. Its clusters come with natural explanations; namely, a prototype (a quintessential exemplar for the cluster) and a set of defining features for that cluster. We showed the quantitative advantages in prediction quality and interpretability resulting from the use of BCM. Exemplar-based modeling (nearest-neighbors, case-based reasoning) has historical roots dating back to the beginning of artificial intelligence; this method offers a fresh perspective on this topic, and a new way of thinking about the balance of accuracy and interpretability in predictive modeling."}], "references": [{"title": "A knowledge-intensive, integrated approach to problem solving and sustained learning", "author": ["A. Aamodt"], "venue": "Knowledge Engineering and Image Processing Group. University of Trondheim, pages 27\u201385,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1991}, {"title": "Case-based reasoning: Foundational issues, methodological variations, and system approaches", "author": ["A. Aamodt", "E. Plaza"], "venue": "AI communications,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "How to explain individual classification decisions", "author": ["D. Baehrens", "T. Schroeter", "S. Harmeling", "M. Kawanabe", "K. Hansen", "K.R. M\u00fcller"], "venue": "JMLR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Case-based reasoning in the health sciences: What\u2019s next", "author": ["I. Bichindaritz", "C. Marling"], "venue": "AI in medicine,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Prototype selection for interpretable classification", "author": ["J. Bien", "R. Tibshirani"], "venue": "AOAS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JMLR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Analyzing decision behavior: The magician\u2019s audience", "author": ["J.S. Carroll"], "venue": "Cognitive processes in choice and decision behavior,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1980}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J. Chang", "J.L. Boyd-Graber", "S. Gerrish", "C. Wang", "D.M. Blei"], "venue": "NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Metarecognition in time-stressed decision making: Recognizing, critiquing, and correcting", "author": ["M.S. Cohen", "J.T. Freeman", "S. Wolf"], "venue": "Human Factors,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "Information Theory,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1967}, {"title": "An evaluation of the usefulness of case-based explanation", "author": ["P. Cunningham", "D. Doyle", "J. Loughrey"], "venue": "CBRRD. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Classification and regression trees: a powerful yet simple technique for ecological data analysis", "author": ["G. De\u2019ath", "K.E. Fabricius"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Sparse additive generative models of text", "author": ["J. Eisenstein", "A. Ahmed", "E. Xing"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Comprehensible classification models: a position paper", "author": ["A. Freitas"], "venue": "ACM SIGKDD Explorations,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Box drawings for learning with imbalanced data", "author": ["S. Goh", "C. Rudin"], "venue": "KDD,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Prototype classification: Insights from machine learning", "author": ["A. Graf", "O. Bousquet", "G. R\u00e4tsch", "B. Sch\u00f6lkopf"], "venue": "Neural computation,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "PNAS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "ACM SIGIR,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "TPAMI,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models", "author": ["J. Huysmans", "K. Dejaeger", "C. Mues", "J. Vanthienen", "B. Baesens"], "venue": "DSS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Do decision biases explain too much", "author": ["G.A. Klein"], "venue": "HFES,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "ICML,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Interpretable classifiers using rules and Bayesian analysis", "author": ["B. Letham", "C. Rudin", "T. McCormick", "D. Madigan"], "venue": "Technical report, University of Washington,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Ranking-order case-based reasoning for financial distress prediction", "author": ["H. Li", "J. Sun"], "venue": "KBSI,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Assessing elaborated hypotheses: An interpretive case-based reasoning approach", "author": ["J.W. Murdock", "D.W. Aha", "L.A. Breslow"], "venue": "ICCBR. Springer,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Human problem solving", "author": ["A. Newell", "H.A. Simon"], "venue": "Prentice-Hall Englewood Cliffs,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1972}, {"title": "GibbsLDA++, AC/C++ implementation of latent dirichlet allocation using gibbs sampling for parameter estimation and inference", "author": ["X. Phan", "C. Nguyen"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Case-based reasoning: A research paradigm", "author": ["S. Slade"], "venue": "AI magazine,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1991}, {"title": "Explanation in case-based reasoning\u2013perspectives and goals", "author": ["F. S\u00f8rmo", "J. Cassens", "A. Aamodt"], "venue": "AI Review,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "JRSS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Methods and models for interpretable linear classification", "author": ["B. Ustun", "C. Rudin"], "venue": "ArXiv,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "The IBP compound dirichlet process and its application to focused topic modeling", "author": ["S. Williamson", "C. Wang", "K. Heller", "D. Blei"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "MedLDA: maximum margin supervised topic models", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "JMLR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "Numerous studies have demonstrated that exemplar-based reasoning, involving various forms of matching and prototyping, is fundamental to our most effective strategies for tactical decisionmaking ([26, 9, 21]).", "startOffset": 196, "endOffset": 207}, {"referenceID": 8, "context": "Numerous studies have demonstrated that exemplar-based reasoning, involving various forms of matching and prototyping, is fundamental to our most effective strategies for tactical decisionmaking ([26, 9, 21]).", "startOffset": 196, "endOffset": 207}, {"referenceID": 20, "context": "Numerous studies have demonstrated that exemplar-based reasoning, involving various forms of matching and prototyping, is fundamental to our most effective strategies for tactical decisionmaking ([26, 9, 21]).", "startOffset": 196, "endOffset": 207}, {"referenceID": 20, "context": "For example, naturalistic studies have shown that skilled decision makers in the fire service use recognition-primed decision making, in which new situations are matched to typical cases where certain actions are appropriate and usually successful [21].", "startOffset": 248, "endOffset": 252}, {"referenceID": 1, "context": "Studies of human decision-making and cognition provided the key inspiration for artificial intelligence Case-Based Reasoning (CBR) approaches [2, 28].", "startOffset": 142, "endOffset": 149}, {"referenceID": 27, "context": "Studies of human decision-making and cognition provided the key inspiration for artificial intelligence Case-Based Reasoning (CBR) approaches [2, 28].", "startOffset": 142, "endOffset": 149}, {"referenceID": 27, "context": "CBR relies on the idea that a new situation can be well-represented by the summarized experience of previously solved problems [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "CBR has been used in important real-world applications [24, 4], but is fundamentally limited, in that it does not learn the underlying complex structure of data in an unsupervised fashion and may not scale to datasets with high-dimensional feature spaces (as discussed in [29]).", "startOffset": 55, "endOffset": 62}, {"referenceID": 3, "context": "CBR has been used in important real-world applications [24, 4], but is fundamentally limited, in that it does not learn the underlying complex structure of data in an unsupervised fashion and may not scale to datasets with high-dimensional feature spaces (as discussed in [29]).", "startOffset": 55, "endOffset": 62}, {"referenceID": 28, "context": "CBR has been used in important real-world applications [24, 4], but is fundamentally limited, in that it does not learn the underlying complex structure of data in an unsupervised fashion and may not scale to datasets with high-dimensional feature spaces (as discussed in [29]).", "startOffset": 272, "endOffset": 276}, {"referenceID": 25, "context": "People organize and interpret information through exemplar-based reasoning, particularly when they are solving problems ([26, 7, 9, 21]).", "startOffset": 121, "endOffset": 135}, {"referenceID": 6, "context": "People organize and interpret information through exemplar-based reasoning, particularly when they are solving problems ([26, 7, 9, 21]).", "startOffset": 121, "endOffset": 135}, {"referenceID": 8, "context": "People organize and interpret information through exemplar-based reasoning, particularly when they are solving problems ([26, 7, 9, 21]).", "startOffset": 121, "endOffset": 135}, {"referenceID": 20, "context": "People organize and interpret information through exemplar-based reasoning, particularly when they are solving problems ([26, 7, 9, 21]).", "startOffset": 121, "endOffset": 135}, {"referenceID": 10, "context": "Studies show that example cases significantly improve user confidence in the resulting solutions, as compared to providing the solution alone or by also displaying a rule that was used to find the solution [11].", "startOffset": 206, "endOffset": 210}, {"referenceID": 28, "context": "Maintaining transparency in complex situations also remains a challenge [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "CBR models designed explicitly to produce explanations [1] rely on the backward chaining of the causal relation from a solution, which does not scale as complexity increases.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "The cognitive load of the user also increases with the complexity of the similarity measure used for comparing cases [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "Other CBR models for explanations require the model to be manually crafted in advance by experts [25].", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "However, this approach does not provide intuitive explanations for the learned clusters (as pointed out in [8]).", "startOffset": 107, "endOffset": 110}, {"referenceID": 31, "context": "Sparse topic models are designed to improve interpretability by reducing the number of words per topic [32, 13].", "startOffset": 103, "endOffset": 111}, {"referenceID": 12, "context": "Sparse topic models are designed to improve interpretability by reducing the number of words per topic [32, 13].", "startOffset": 103, "endOffset": 111}, {"referenceID": 13, "context": "However, using the number of features as a proxy for interpretability is problematic, as sparsity is often not a good or complete measure of interpretability [14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "Even users with technical expertise in machine learning may have a difficult time interpreting such output, especially when the cluster is distributed over a large number of features [14].", "startOffset": 183, "endOffset": 187}, {"referenceID": 29, "context": "In our view, there are at least three widely known types of interpretable models: sparse linear classifiers ([30, 8, 31]); discretization methods, such as decision trees and decision lists (e.", "startOffset": 109, "endOffset": 120}, {"referenceID": 7, "context": "In our view, there are at least three widely known types of interpretable models: sparse linear classifiers ([30, 8, 31]); discretization methods, such as decision trees and decision lists (e.", "startOffset": 109, "endOffset": 120}, {"referenceID": 30, "context": "In our view, there are at least three widely known types of interpretable models: sparse linear classifiers ([30, 8, 31]); discretization methods, such as decision trees and decision lists (e.", "startOffset": 109, "endOffset": 120}, {"referenceID": 11, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 31, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 12, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 22, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 14, "context": ", [12, 32, 13, 23, 15]); and prototype- or case-based classifiers (e.", "startOffset": 2, "endOffset": 22}, {"referenceID": 9, "context": ", nearest neighbors [10] or a supervised optimization-based method [5]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": ", nearest neighbors [10] or a supervised optimization-based method [5]).", "startOffset": 67, "endOffset": 70}, {"referenceID": 13, "context": "(See [14] for a review of interpretable classification.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": ") BCM is intended as the third model type, but uses unsupervised generative mechanisms to explain clusters, rather than supervised approaches [16] or by focusing myopically on neighboring points [3].", "startOffset": 142, "endOffset": 146}, {"referenceID": 2, "context": ") BCM is intended as the third model type, but uses unsupervised generative mechanisms to explain clusters, rather than supervised approaches [16] or by focusing myopically on neighboring points [3].", "startOffset": 195, "endOffset": 198}, {"referenceID": 17, "context": "BCM begins with a standard discrete mixture model [18, 6] to represent the underlying structure of the observations.", "startOffset": 50, "endOffset": 57}, {"referenceID": 5, "context": "BCM begins with a standard discrete mixture model [18, 6] to represent the underlying structure of the observations.", "startOffset": 50, "endOffset": 57}, {"referenceID": 5, "context": "(Note that Latent Dirichlet Allocation (LDA) [6] also begins with a standard mixture model, though our feature values exist in a discrete set that is not necessarily binary.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "BCM works differently to Latent Dirichlet Allocation (LDA) [6], which presents its output in a very different form.", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "We use collapsed Gibbs sampling to perform inference, as this has been observed to converge quickly, particularly in mixture models [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "The derivation is similar to the standard collapsed Gibbs sampling for LDA mixture models [17].", "startOffset": 90, "endOffset": 94}, {"referenceID": 18, "context": "Figure 2: Prediction test accuracy reported for the Handwritten Digit [19] and 20 Newsgroups datasets [22].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "Figure 2: Prediction test accuracy reported for the Handwritten Digit [19] and 20 Newsgroups datasets [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "We validate this through use of two standard datasets: Handwritten Digit [19] and 20 Newsgroups [22].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "We validate this through use of two standard datasets: Handwritten Digit [19] and 20 Newsgroups [22].", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "We use the implementation of LDA available from [27], which incorporates Gibbs sampling, the same inference technique used for BCM.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "In order to compare the prediction accuracy with LDA, the learned cluster labels are provided as features to a support vector machine (SVM) with linear kernel, as is often done in the LDA literature on clustering [6].", "startOffset": 213, "endOffset": 216}, {"referenceID": 32, "context": "01) matches that reported previously for this dataset when using a combined LDA and SVM approach [33].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "(This approach is similar to those used in prior work to measure comprehensibility [20].", "startOffset": 83, "endOffset": 87}], "year": 2015, "abstractText": "We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the \u201cquintessential\u201d observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants\u2019 understanding when using explanations produced by BCM, compared to those given by prior art.", "creator": "LaTeX with hyperref package"}}}