{"id": "1503.02335", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2015", "title": "An Unsupervised Method for Uncovering Morphological Chains", "abstract": "Most state - latter - soon - art limited today creating morphological analysis independent perhaps days definitions changes. In considerably, we propose rather addition five altercations morphology knowledge nor databases ordinal well semantic views part poem. We styling word formation 2004 terms also morphological chains, once low telling up over normally word, after making chain left private - mothers relations. We use door - linear models took sub-sections and \u201c - level alternate coming predict concerned parents, including bring upgrades, over be word. The include set of candidate parents made time word ineffective contrastive estimation amenable. Our model obviously matches or clunkier made today - has - new - author allows made Arabic, English it Turkish.", "histories": [["v1", "Sun, 8 Mar 2015 22:18:30 GMT  (120kb,D)", "http://arxiv.org/abs/1503.02335v1", "11 pages, Appearing in the Transactions of the Association for Computational Linguistics (TACL), 2015"]], "COMMENTS": "11 pages, Appearing in the Transactions of the Association for Computational Linguistics (TACL), 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karthik narasimhan", "regina barzilay", "tommi jaakkola"], "accepted": true, "id": "1503.02335"}, "pdf": {"name": "1503.02335.pdf", "metadata": {"source": "CRF", "title": "An Unsupervised Method for Uncovering Morphological Chains", "authors": ["Karthik Narasimhan", "Regina Barzilay", "Tommi Jaakkola"], "emails": ["tommi}@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Morphologically related words exhibit connections at multiple levels, ranging from orthographical patterns to semantic proximity. For instance, the words playing and played share the same stem, but also carry similar meaning. Ideally, all these complementary sources of information would be taken into account when learning morphological structures.\nMost state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009). In these approaches, words are commonly modeled as concatenations of morphemes.\n1Code is available at https://github.com/ karthikncode/MorphoChain.\nThis morpheme-centric view is well-suited for uncovering distributional properties of stems and affixes. But it is not well-equipped to capture semantic relatedness at the word level.\nIn contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002). Given two candidate words, the proximity is assessed using standard word-distributional measures such as mutual information. However, the fact that these models do not model morphemes directly greatly limits their performance.\nIn this paper, we propose a model to integrate orthographic and semantic views. Our goal is to build a chain of derivations for a current word from its base form. For instance, given a word playfully, the corresponding chain is play \u2192 playful \u2192 playfully. The word play is a base form of this derivation as it cannot be reduced any further. Individual derivations are obtained by adding a morpheme (ex. -ful ) to a parent word (ex. play). This addition may be implemented via a simple concatenation, or it may involve transformations. At every step of the chain, the model aims to find a parent-child pair (ex. playplayful ) such that the parent also constitutes a valid entry in the lexicon. This allows the model to directly compare the semantic similarity of the parentchild pair, while also considering the orthographic properties of the morphemic combination.\nWe model each step of a morphological chain by means of a log-linear model that enables us to incorporate a wide range of features. At the semantic level, we consider the relatedness between two words using the corresponding vector embeddings. At the orthographic level, features capture whether\nar X\niv :1\n50 3.\n02 33\n5v 1\n[ cs\n.C L\n] 8\nM ar\n2 01\nthe words in the chain actually occur in the corpus, how affixes are reused, as well as how the words are altered during the addition of morphemes. We use Contrastive Estimation (Smith and Eisner, 2005) to efficiently learn this model in an unsupervised manner. Specifically, we require that each word has greater support among its bounded set of candidate parents than an artificially constructed neighboring word would.\nWe evaluate our model on datasets in three languages: Arabic, English and Turkish. We compare our performance against five state-of-the-art unsupervised systems: Morfessor Baseline (Virpioja et al., 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al., 2011; Stallard et al., 2012) and the system of Poon et al. (2009). Our model consistently equals or outperforms these systems across the three languages. For instance, on English, we obtain an 8.5% gain in F-measure over Morfessor. Our experiments also demonstrate the value of semantic information. While the contribution varies from 3% on Turkish to 11% on the English dataset, it nevertheless improves performance across all the languages."}, {"heading": "2 Related Work", "text": "Currently, top performing unsupervised morphological analyzers are based on the orthographic properties of sub-word units (Creutz and Lagus, 2005; Creutz and Lagus, 2007; Poon et al., 2009; Sirts and Goldwater, 2013). Adding semantic information to these systems is not an easy task, as they operate at the level of individual morphemes, rather than morphologically related words.\nThe value of semantic information has been demonstrated in earlier work on morphological analysis. Schone and Jurafsky (2000) employ an LSAbased similarity measure to identify morphological variants from a list of orthographically close word pairs. The filtered pairs are then used to identify stems and affixes. Based on similar intuition, Baroni et al. (2002) design a method that integrates these sources of information, captured as two word pair lists, ranked based on edit distance and mutual information. These lists are subsequently combined using a deterministic weighting function.\nIn both of these algorithms, orthographic relatedness is based on simple deterministic rules. Therefore, semantic relatedness plays an essential role in the success of these methods. However, these algorithms do not capture distributional properties of morphemes that are critical to the success of current state-of-the-art algorithms. In contrast, we utilize a single statistical framework that seamlessly combines both sources of information. Moreover, it allows us to incorporate a wide range of additional features.\nOur work also relates to the log-linear model for morphological segmentation developed by Poon et al. (2009). They propose a joint model over all words (observations) and their segmentations (hidden), using morphemes and their contexts (character n-grams) for the features. Since the space of all possible segmentation sets is huge, learning and inference are quite involved. They use techniques like Contrastive Estimation, sampling and simulated annealing. In contrast, our formulation does not result in such a large search space. For each word, the number of parent candidates is bounded by its length multiplied by the number of possible transformations. Therefore, Contrastive Estimation can be implemented via enumeration, and does not require sampling. Moreover, operating at the level of words (rather than morphemes) enables us to incorporate semantic and word-level features.\nMost recently, work by Sirts and Goldwater (2013) uses Adaptor Grammars for minimally supervised segmentation. By defining a morphological grammar consisting of zero or more prefixes, stems and suffixes, they induce segmentations over words in both unsupervised and semi-supervised settings. While their model (AGMorph) builds up a word by combining morphemes in the form of a parse tree, we operate at the word level and build up the final word via intermediate words in the chain.\nIn other related work, Dreyer and Eisner (2011) tackle the problem of recovering morphological paradigms and inflectional principles. They use a Bayesian generative model with a log-linear framework, using expressive features, over pairs of strings. Their work, however, handles a different task from ours and requires a small amount of annotated data to seed the model.\nIn this work, we make use of semantic infor-\nmation to help morphological analysis. Lee et al. (2011) present a model that takes advantage of syntactic context to perform better morphological segmentation. Stallard et al. (2012) improve on this approach using the technique of Maximum Marginal decoding to reduce noise. Their best system considers entire sentences, while our approach (and the morphological analyzers described above) operates at the vocabulary level without regarding sentence context. Hence, though their work is not directly comparable to ours, it presents an interesting orthogonal view to the problem."}, {"heading": "3 Model", "text": ""}, {"heading": "3.1 Definitions and Framework", "text": "We use morphological chains to model words in the language. A morphological chain is a short sequence of words that starts from the base word and ends up in a morphological variant. Each node in the chain is, by assumption, a valid word. We refer to the word that is morphologically changed as the parent word and its morphological variant as the child word. A word that does not have any morphological parent is a base word (e.g., words like play, chat, run).2\nWords in a chain (other than the base word) are created from their parents by adding morphemes (prefixes, suffixes, or other words). For example, a morphological chain that ends up in the word internationally could be nation \u2192 national \u2192 international \u2192 internationally. The base word for this chain is nation. Note that the same word can belong to multiple morphological chains. For example, the word national appears also as part of another chain that ends up in nationalize. These chains are treated separately but with shared statistical support for the common parts. For this reason, our model breaks morphological chains into possible parent-child relations such as (nation, national ).\nWe use a log-linear model for predicting parentchild pairs. A log-linear model allows an easy, efficient way of incorporating several different features pertaining to parent-child relations. In our case, we leverage both orthographic and semantic patterns to encode representative features.\n2We distinguish base words from morphological roots which do not strictly speaking have to be valid words in the language.\nA log-linear model consists of a set of features represented by a feature vector \u03c6 : W \u00d7 Z \u2192 Rd and a corresponding weight vector \u03b8 \u2208 Rd. Here,W is a set of words and Z is the set of candidates for words inW , that includes the parents as well as their types. Specifically, a candidate is a (parent, type) pair, where the type variable keeps track of the type of morphological change (or the lack thereof if there is no parent) as we go from the parent to the child. In our experiments, Z is obtained by collecting together all sub-words created by splitting observed words in W at all different points. For instance, if we take the word cars, the candidates obtained by splitting would include (car, Suffix), (ca, Suffix), (c, Suffix), (ars, Prefix), (rs, Prefix) and (s, Prefix).\nNote that the parent may undergo changes as it is joined with the affix and thus, there are more choices for the parent than just the ones obtained by splitting. Hence, to the set of candidates, we also add modified sub-words where transformations include character repetition (plan \u2192 planning), deletion (decide \u2192 deciding) or replacement (carry \u2192 carried ).3 Following the above example for the word cars, we get candidates like (cat, Modify) and (cart, Delete). Each word also has a stop candidate (-, Stop), which is equivalent to considering it as a base word with no parent.\nLet us define the probability of a particular wordcandidate pair (w \u2208 W, z \u2208 Z) as P (w, z) \u221d e\u03b8\u00b7\u03c6(w,z). The conditional probability of a candidate\n3We found that restricting the set of parents to sub-words that are at least half the length of the original word helped improve the performance of the system.\nz given a word w is then\nP (z|w) = e \u03b8\u00b7\u03c6(w,z)\u2211\nz\u2032\u2208C(w) e \u03b8\u00b7\u03c6(w,z\u2032) , z \u2208 C(w)\nwhere C(w) \u2282 Z refers to the set of possible candidates (parents and their types) for the word w \u2208 W .\nIn order to generate a possible ancestral chain for a word, we recursively predict parents until the word is predicted to be a base word. In our model, these choices are included in the set of candidates for the specific word, and their likelihood is controlled by the type related features. Details of these and other features are given in section 3.2."}, {"heading": "3.2 Features", "text": "This section provides an overview of the features used in our model. The features are defined for a given word w, parent p and type t (recall that a candidate z \u2208 Z is the pair (p, t)). For computing some of these features, we use an unannotated list of words with frequencies (details in section 4). Table 2 provides a summary of the features.\nSemantic Similarity We hypothesize that morphologically related words exhibit semantic similarity. To this end, we introduce a feature that measures cosine similarity between the word vectors of the word (~w) and the parent (~p). These word vectors are learned from co-occurrence patterns from a large corpus4 (see section 4 for details).\nTo validate this measure, we computed the cosine similarity between words and their morphological parents from the CELEX2 database (Baayen et al., 1995). On average, the resulting word-parent similarity score is 0.351, compared to 0.116 for randomly chosen word-parent combinations.5\nAffixes A distinctive feature of affixes is their frequent occurrence in multiple words. To capture this pattern, we automatically generate a list of frequently occurring candidate affixes. These candidates are collected by considering the string difference between a word and its parent candidates which appear in the word list. For example, for the word paints, possible suffixes include -s derived from the\n4For strings which do not have a vector learnt from the corpus, we set the cosine value to be -0.5.\n5The cosine values range from around -0.1 to 0.7 usually.\nparent paint, -ts from the parent pain and -ints from the word pa. Similarly, we compile a list of potential prefixes. These two lists are sorted by their frequency and thresholded. For each affix in the lists, we have a corresponding indicator variable. For unseen affixes, we use an UNK (unknown) indicator.\nThese automatically constructed lists act as a proxy for the gold affixes. In English, choosing the top 100 suffixes in this manner gives us 43 correct suffixes (compared against gold suffixes). Table 3 gives some examples of suffixes generated this way.\nAffix Correlation While the previous feature considers one affix assignment at a time, there is a known correlation between affixes attached to the same stem. For instance, in English, verbs that can be modified by the suffix -ing, can also take the related suffix -ed. Therefore, we introduce a feature that measures, whether for a given affix and parent, we also observe in the wordlist the same parent modified by its related affix. For example, for the pair (walking, walk), the feature instance AffixCorr(ing, ed) is set to 1, because the word walked is in the WordList.\nTo construct pairs of related affixes, we compute the correlation between pairs in auto-generated affix list described previously. This correlation is proportional to the number of stems the two affixes share. For English, examples of such pairs include (inter-, re-), (under-, over-), (-ly, -s), and (-er, -ing).\nPresence in Wordlist We want to bias the model to select parents that constitute valid words.6 Moreover, we would like to take into account the frequency of the parent words. We encode this information as the logarithm of their word counts in the wordlist (WordFreq). For parents not in the wordlist, we set a binary OutOfVocab feature to 1.\n6This is not an absolute requirement in the model.\nTransformations We also support transformations to enable non-concatenative morphology. Even in English, which is mostly concatenative, such transformations are frequent. We consider three kinds of transformations previously considered in the literature (Goldwater and Johnson, 2004):\n\u2022 repetition of the last character in the parent (ex. plan \u2192 planning)\n\u2022 deletion of the last character in the parent (ex. decide \u2192 deciding)\n\u2022 modification of the last character of the parent (ex. carry \u2192 carried )\nWe add features that are the cartesian product of the type of transformation and the character(s) involved. For instance, for the parent-child pair (believe, believing), the feature type=Delete \u00d7 chars=(e,-) will be activated, while the rest of the transformational features will be 0.\nStop Condition Finally, we introduce features that aim to identify base words which do not have a parent. The features include the length of the word, and the starting and ending character unigrams and bigrams. In addition, we include a feature that records the highest cosine similarity between the word and any of its candidate parents. This feature will help, for example, to identify paint as a base word, instead of choosing pain as its parent."}, {"heading": "3.3 Learning", "text": "We learn the log-linear model in an unsupervised manner without explicit feedback about correct morphological segmentations. We assume that we have an unannotated wordlist D for this purpose. A typical approach to learning such a model would be to maximize the likelihood of all the observed words in D over the space of all strings constructible in the alphabet, \u03a3\u2217, by marginalizing over the hidden candidates.7 In other words, we could use the EMalgorithm to maximize\nL(\u03b8;D) = \u220f w\u2217\u2208D P (w\u2217)\n= \u220f w\u2217\u2208D \u2211 z\u2208C(w\u2217) P (w\u2217, z)\n= \u220f w\u2217\u2208D\n[ \u2211 z\u2208C(w\u2217) e\n\u03b8\u00b7\u03c6(w\u2217,z)\u2211 w\u2208\u03a3\u2217 \u2211 z\u2208C(w) e \u03b8\u00b7\u03c6(w,z) ] (1)\nHowever, maximizing L(\u03b8;D) is problematic since approximate methods would be needed to sum over \u03a3\u2217 in order to calculate the normalization term in (1). Moreover, we would like to encourage the model to emphasize relevant parent-child pairs8 out of a smaller set of alternatives rather than those pertaining to all the words.\n7We also tried maximizing instead of marginalizing, but the model gets stuck in one of the numerous local optima.\n8In other words, assign higher probability mass.\nWe employ Contrastive Estimation (Smith and Eisner, 2005) and replace the normalization term by a sum over the neighbors of each word. For each word in the language, we create neighboring strings in two sets. For the first set, we transpose a single pair of adjacent characters of the word. We perform this transposition over the first k or the last k characters of the word.9 For the second set, we transpose two pairs of characters simultaneously \u2013 one from the first k characters and one from the last k.\nThe combined set of artificially constructed words represents the events that we wish to move probability mass away from in favor of the actually observed words. The neighbors facilitate the learning of good weights for the affix features by providing the required contrast (at both ends of the words) to the actual words in the vocabulary. A remaining concern is that the model may not distinguish any arbitrary substring from a good suffix/prefix. For example, -ng appears in all the words that end with -ing, and could be considered a valid suffix. We include other features to help make this distinction. Specifically, we include features such as word vector similarity and the presence of the parent in the observed wordlist. For example, in the word painting, the parent candidate paint is likely to occur and also has a high cosine similarity with painting in terms of their word vectors. In contrast, painti does not.\nGiven the list of words and their neighborhoods, we define the contrastive likelihood as follows:\n(2)\nLCE(\u03b8,D) = \u220f w\u2217\u2208D\n[ \u2211 z\u2208C(w\u2217) e\n\u03b8\u00b7\u03c6(w\u2217,z)\u2211 w\u2208N(w\u2217) \u2211 z\u2208C(w) e \u03b8\u00b7\u03c6(w,z) ] where N(w\u2217) is the neighborhood of w\u2217. This likelihood is much easier to evaluate and optimize.\nAfter adding in a standard regularization term, we maximize the following log likelihood objective:\n(3)\n\u2211 w\u2217 \u2208D log \u2211 z\u2208C(w\u2217) e\u03b8\u00b7\u03c6(w \u2217,z)\n\u2212 log \u2211\nw\u2208N(w\u2217) \u2211 z\u2208C(w) e\u03b8\u00b7\u03c6(w,z) \u2212 \u03bb||\u03b8||2 9The performance increases with increasing k until k = 5,\nafter which no gains were observed.\nThe corresponding gradient can be derived as:\n\u2202LCE(\u03b8;D)\n\u2202\u03b8j\n= \u2211 w\u2217\u2208D\n[\u2211 z\u2208C(w\u2217) \u03c6j(w\n\u2217, z) \u00b7 e\u03b8\u00b7\u03c6(w\u2217,z)\u2211 z\u2208C(w\u2217) e \u03b8\u00b7\u03c6(w\u2217,z)\n\u2212 \u2211 w\u2208N(w\u2217) \u2211\nz\u2208C(w) \u03c6j(w, z) \u00b7 e\u03b8\u00b7\u03c6(w,z)\u2211 w\u2208N(w\u2217) \u2211 z\u2208C(w) e \u03b8\u00b7\u03c6(w,z) ] \u2212 2\u03bb\u03b8j\n(4)\nWe use LBFGS-B (Byrd et al., 1995) to optimize LCE(\u03b8;D) with gradients given above."}, {"heading": "3.4 Prediction", "text": "Given a test word, we predict a morphological chain in a greedy step by step fashion. In each step, we use the learnt weights to predict the best parent for the current word (from the set of candidates), or choose to stop and declare the current word as a base word if the stop case has the highest score. Once we have the chain, we can derive a morphological segmentation by inserting a segmentation point (into the test word) appropriately for each edge in the chain.\nAlgorithms 1 and 2 provide details on the prediction procedure. In both algorithms, type refers to the type of modification (or lack of) that the parent undergoes: Prefix/Suffix addition, types of transformation like repetition, deletion, modification, or the Stop case.\nAlgorithm 1 Procedure to predict a parent for a word\n1: procedure PREDICT(word) 2: candidates\u2190 CANDIDATES(word) 3: bestScore\u2190 0 4: bestCand\u2190 (\u2212, STOP ) 5: for cand \u2208 candidates do 6: features\u2190 FEATURES(word, cand) 7: score\u2190 MODELSCORE(features) 8: if score > bestScore then 9: bestScore\u2190 score\n10: bestCand\u2190 cand 11: return bestCand\nAlgorithm 2 Procedure to predict a morphological chain\n1: procedure GETCHAIN(word) 2: candidate\u2190 PREDICT(word) 3: parent, type\u2190 candidate 4: if type = STOP then return\n[(word, STOP)] 5: return GETCHAIN(parent)+[(parent, type)]"}, {"heading": "4 Experimental Setup", "text": "Data We run experiments on three different languages: English, Turkish and Arabic. For each language, we utilize corpora for training, testing and learning word vectors. The training data consists of an unannotated wordlist with frequency information, while the test data is a set of gold morphological segmentations. For the word vectors, we train the word2vec tool (Mikolov et al., 2013) on large text corpora and obtain 200-dimensional vectors for all three languages. Table 4 provides information about each dataset.\nEvaluation measure We test our model on the task of morphological segmentation. We evaluate performance on individual segmentation points, which is standard for this task (Virpioja et al., 2011). We compare predicted segmentations against the gold test data for each language and report overall Precision, Recall and F-1 scores calculated across\n10http://research.ics.aalto.fi/events/morphochallenge/\nall segmentation points in the data. As is common in unsupervised segmentation (Poon et al., 2009; Sirts and Goldwater, 2013), we included the test words (without their segmentations) with the training words during parameter learning.\nBaselines We compare our model with five other systems: Morfessor Baseline (Morf-Base), Morfessor CatMap (Morf-Cat), AGMorph, the Lee Segmenter and the system of Poon et al. (2009). Morfessor has achieved excellent performance on the MorphoChallenge dataset, and is widely used for performing unsupervised morphological analysis on various languages, even in fairly recent work (Luong et al., 2013). In our experiments, we employ two variants of the system because their relative performance varies across languages. We use publicly available implementations of these variants (Virpioja et al., 2013; Creutz and Lagus, 2005). We perform several runs with various parameters, and choose the run with the best performance on each language.\nWe evaluate AGMorph by directly obtaining the posterior grammars from the authors.11 We show results for the Compounding grammar, which we find has the best average performance over the languages. The Lee Segmenter (Lee et al., 2011), improved upon by using Maximum Marginal decoding in Stallard et al. (2012), has achieved excellent performance on the Arabic (ATB) dataset. We perform comparison experiments with the model 2 (M2) of the segmenter, which employs latent POS tags, and does not require sentence context which is not available for other languages in the dataset. We obtained the code for the system, and run it on our English and Turkish datasets.12 We do not have access to an implementation of Poon et al\u2019s system; hence, we directly report scores from their paper on the ATB dataset and test our model on the same data."}, {"heading": "5 Results", "text": "Table 5 details the performance of the various models on the segmentation task. We can see that our method outperforms both variants of Morfessor,\n11The grammars were trained using data we provided to them.\n12We report numbers on Arabic directly from their paper.\nwith an absolute gain of 8.5%, 5.1% and 5% in Fscore on English, Turkish and Arabic, respectively. On Arabic, we obtain a 2.2% absolute improvement over Poon et al.\u2019s model. AGMorph doesn\u2019t segment better than Morfessor on English and Arabic but does very well on Turkish (60.9% F1 compared to our model\u2019s 61.2%). This could be due to the fact that the Compounding grammar is well suited to the agglutinative morphology in Turkish and hence provides more gains than for English and Arabic. The Lee Segmenter (M2) performs the best on Arabic (82% F1), but lags behind on English and Turkish. This result is consistent with the fact that the system was optimized for Arabic.\nThe table also demonstrates the importance of the added semantic information in our model. For all three languages, having the features that utilize cosine similarity provides a significant boost in performance. We also see that the transformation features and affix correlation features play a role in improving the results, although a less important one.\nNext, we study the effect of data quality on the prediction of the algorithm. A training set often contains misspellings, abbreviations and truncated words. Thresholding based on frequency is commonly used to reduce this noise. Figure 1 shows the performance of the algorithm as a function of the data size obtained at various degrees of thresholding. We note that the performance of the model on all three languages remains quite stable from about\n1000 to 10000 training words, after which the deviations are more apparent. The plot also demonstrates that the model works well even with a small amount of quality data (\u22483000 most frequent words).\nError analysis We look at a random subset of 50 incorrectly segmented words13 in the model\u2019s output for each language. Table 7 gives a breakup of errors in all 3 languages due to over or under-segmentation. Table 6 provides examples of correct and incorrect segmentations predicted by our model.\n13Words with at least one segmentation point incorrect\nIn English, most errors are due to undersegmentation of a word. We find that around 60% of errors are due to roots that undergo transformations while morphing into a variant (see table 6 for examples). Errors in Turkish are also mostly due to undersegmentation. On further investigation, we find that most such errors (58% of the 78%) are due to parent words either not in vocabulary or having a very low word count (\u2264 10). In contrast, we observe a majority of over-segmentation errors in Arabic (60%). This is likely because of Arabic having more single character affixes than the other languages. We find that 56% of errors in Arabic involve a singlecharacter affix, which is much higher than the 24.6% that involve a two-letter affix. In contrast, 25% of errors in English are due to single character affixes \u2013 around the same number as the 24% of errors due to\ntwo-letter affixes. Since our model is an unsupervised one, we make several simplifying assumptions to keep the candidate set size manageable for learning. For instance, we do not explicitly model infixes, since we select parent candidates by only modifying the ends of a word. Also, the root-template morphology of Arabic, a Semitic language, presents a complexity we do not directly handle. For instance, words in Arabic can be formed using specific patterns (known as binyanim) (ex. nZr \u2192 yntZr). However, on going through the errors, we find that only 14% are due to these binyanim patterns not being captured.14 Adding in transformation rules to capture these types of language-specific patterns can help increase both chain and segmentation accuracy.\nAnalysis of learned distributions To investigate how decisive the learnt model is, we examine the final probability distribution P (z|w) of parent candidates for the words in the English wordlist. We observe that the probability of the best candidate (maxzP (z|w)), averaged over all words, is 0.77. We also find that the average entropy of the distri-\n14This might be due to the fact that the gold segmentations also do not capture such patterns. For example, the gold segmentation for yntZrwn is given as y-ntZr-wn, even though ntZr is not a valid root.\nbutions is 0.65, which is quite low considering that the average number of candidates is 10.76 per word, which would result in a max possible entropy of around 2.37 if the distributions were uniform. This demonstrates that the model tends to prefer a single parent for every word,15 which is exactly the behavior we want.\nAffix analysis We also analyze the various affixes produced by the model, and compare with the gold affixes. Particularly, we plot the frequency distributions of the affixes16 obtained from the gold and\n15Note that the candidate probability distribution may have more than a single peak in some cases.\n16To conserve space, we only show the distribution of suffixes here, but we observe a similar trend for prefixes.\npredicted segmentations for the English test data in figure 2.\nFrom the figure, we can see that our model learns to identify good affixes for the given language. Several of the top affixes predicted are also present in the gold list, and we also observe similarities in the frequency distributions."}, {"heading": "6 Conclusion", "text": "In this work, we have proposed a discriminative model for unsupervised morphological segmentation that seamlessly integrates orthographic and semantic properties of words. We use morphological chains to model the word formation process and show how to employ the flexibility of log-linear models to incorporate both morpheme and wordlevel features, while handling transformations of parent words. Our model consistently equals or outperforms five state-of-the-art systems on Arabic, English and Turkish. Future directions of work include using better neighborhood functions for contrastive estimation, exploring other views of the data that could be incorporated, examining better prediction schemes and employing morphological chains in other applications in NLP."}, {"heading": "Acknowledgements", "text": "We thank Kairit Sirts and Yoong Keok Lee for helping run experiments with their unsupervised morphological analyzers, and Yonatan Belinkov for helping with error analysis in Arabic. We also thank the anonymous TACL reviewers and members of MIT\u2019s NLP group for their insightful comments and suggestions. This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense US Army Research Laboratory contract number W911NF-12C-0013. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government."}], "references": [{"title": "CELEX2 LDC96L14", "author": ["R Baayen", "R Piepenbrock", "L Gulikers."], "venue": "Philadelphia: Linguistic Data Consortium.", "citeRegEx": "Baayen et al\\.,? 1995", "shortCiteRegEx": "Baayen et al\\.", "year": 1995}, {"title": "Unsupervised discovery of morphologically related words based on orthographic and semantic similarity", "author": ["Marco Baroni", "Johannes Matiasek", "Harald Trost."], "venue": "CoRR, cs.CL/0205006.", "citeRegEx": "Baroni et al\\.,? 2002", "shortCiteRegEx": "Baroni et al\\.", "year": 2002}, {"title": "A limited memory algorithm for bound constrained optimization", "author": ["Richard H Byrd", "Peihuang Lu", "Jorge Nocedal", "Ciyou Zhu."], "venue": "SIAM Journal on Scientific Computing, 16(5):1190\u20131208.", "citeRegEx": "Byrd et al\\.,? 1995", "shortCiteRegEx": "Byrd et al\\.", "year": 1995}, {"title": "Inducing the morphological lexicon of a natural language from unannotated text", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR), pages", "citeRegEx": "Creutz and Lagus.,? 2005", "shortCiteRegEx": "Creutz and Lagus.", "year": 2005}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "ACM Trans. Speech Lang. Process., 4(1):3:1\u20133:34, February.", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "Discovering morphological paradigms from plain text using a dirichlet process mixture model", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 616\u2013627. Association for", "citeRegEx": "Dreyer and Eisner.,? 2011", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2011}, {"title": "Priors in bayesian learning of phonological rules", "author": ["Sharon Goldwater", "Mark Johnson."], "venue": "Proceedings of the 7th Meeting of the ACL Special Interest Group in Computational Phonology: Current Themes in Computational Phonology and Morphology, SIG-", "citeRegEx": "Goldwater and Johnson.,? 2004", "shortCiteRegEx": "Goldwater and Johnson.", "year": 2004}, {"title": "Modeling syntactic context improves morphological segmentation", "author": ["Yoong Keok Lee", "Aria Haghighi", "Regina Barzilay."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL \u201911, pages 1\u20139, Stroudsburg,", "citeRegEx": "Lee et al\\.,? 2011", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "CoNLL, Sofia, Bulgaria.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Arabic Treebank: Part 1 v 2.0 LDC2003T06. Philadelphia: Linguistic Data Consortium", "author": ["Mohamed Maamouri", "Ann Bies", "Hubert Jin", "Tim Buckwalter"], "venue": null, "citeRegEx": "Maamouri et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Maamouri et al\\.", "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1310.4546.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Arabic Gigaword fifth edition LDC2011T11", "author": ["Robert Parker", "David Graff", "Ke Chen", "Junbo Kong", "Kazuaki Maeda."], "venue": "Philadelphia: Linguistic Data Consortium.", "citeRegEx": "Parker et al\\.,? 2011", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["Hoifung Poon", "Colin Cherry", "Kristina Toutanova."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for", "citeRegEx": "Poon et al\\.,? 2009", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Turkish language resources: Morphological parser, morphological disambiguator and web corpus", "author": ["Ha\u015fim Sak", "Tunga G\u00fcng\u00f6r", "Murat Sara\u00e7lar."], "venue": "Advances in natural language processing, pages 417\u2013 427. Springer.", "citeRegEx": "Sak et al\\.,? 2008", "shortCiteRegEx": "Sak et al\\.", "year": 2008}, {"title": "Knowledgefree induction of morphology using latent semantic analysis", "author": ["Patrick Schone", "Daniel Jurafsky."], "venue": "Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning - Vol-", "citeRegEx": "Schone and Jurafsky.,? 2000", "shortCiteRegEx": "Schone and Jurafsky.", "year": 2000}, {"title": "Minimallysupervised morphological segmentation using adaptor grammars", "author": ["Kairit Sirts", "Sharon Goldwater."], "venue": "TACL, 1:255\u2013266.", "citeRegEx": "Sirts and Goldwater.,? 2013", "shortCiteRegEx": "Sirts and Goldwater.", "year": 2013}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Noah A. Smith", "Jason Eisner."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL \u201905, pages 354\u2013362, Stroudsburg, PA, USA. Association", "citeRegEx": "Smith and Eisner.,? 2005", "shortCiteRegEx": "Smith and Eisner.", "year": 2005}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "The Annual Conference of the Association for Computational Linguistics.", "citeRegEx": "Snyder and Barzilay.,? 2008", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2008}, {"title": "Unsupervised morphology rivals supervised morphology for arabic mt", "author": ["David Stallard", "Jacob Devlin", "Michael Kayser", "Yoong Keok Lee", "Regina Barzilay."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational", "citeRegEx": "Stallard et al\\.,? 2012", "shortCiteRegEx": "Stallard et al\\.", "year": 2012}, {"title": "Empirical comparison of evaluation methods for unsupervised learning of morphology", "author": ["Sami Virpioja", "Ville T. Turunen", "Sebastian Spiegler", "Oskar Kohonen", "Mikko Kurimo."], "venue": "TAL, 52(2):45\u201390.", "citeRegEx": "Virpioja et al\\.,? 2011", "shortCiteRegEx": "Virpioja et al\\.", "year": 2011}, {"title": "Morfessor 2.0: Python implementation and extensions for Morfessor Baseline. Report in Aalto University publication series SCIENCE + TECHNOLOGY, Department of Signal Processing", "author": ["Sami Virpioja", "Peter Smit", "Stig-Arne Gr\u00f6nroos", "Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009).", "startOffset": 154, "endOffset": 253}, {"referenceID": 4, "context": "Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009).", "startOffset": 154, "endOffset": 253}, {"referenceID": 17, "context": "Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009).", "startOffset": 154, "endOffset": 253}, {"referenceID": 12, "context": "Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009).", "startOffset": 154, "endOffset": 253}, {"referenceID": 14, "context": "In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002).", "startOffset": 124, "endOffset": 172}, {"referenceID": 1, "context": "In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002).", "startOffset": 124, "endOffset": 172}, {"referenceID": 16, "context": "We use Contrastive Estimation (Smith and Eisner, 2005) to efficiently learn this model in an unsupervised manner.", "startOffset": 30, "endOffset": 54}, {"referenceID": 20, "context": "We compare our performance against five state-of-the-art unsupervised systems: Morfessor Baseline (Virpioja et al., 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al.", "startOffset": 98, "endOffset": 121}, {"referenceID": 3, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al.", "startOffset": 26, "endOffset": 50}, {"referenceID": 15, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al.", "startOffset": 60, "endOffset": 87}, {"referenceID": 7, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al., 2011; Stallard et al., 2012) and the system of Poon et al.", "startOffset": 107, "endOffset": 148}, {"referenceID": 18, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al., 2011; Stallard et al., 2012) and the system of Poon et al.", "startOffset": 107, "endOffset": 148}, {"referenceID": 3, "context": ", 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al., 2011; Stallard et al., 2012) and the system of Poon et al. (2009). Our model consistently equals or outperforms these sys-", "startOffset": 27, "endOffset": 186}, {"referenceID": 13, "context": "Schone and Jurafsky (2000) employ an LSAbased similarity measure to identify morphological variants from a list of orthographically close word pairs.", "startOffset": 0, "endOffset": 27}, {"referenceID": 1, "context": "Based on similar intuition, Baroni et al. (2002) design a method that integrates these sources of information, captured as two word pair lists, ranked based on edit distance and mutual information.", "startOffset": 28, "endOffset": 49}, {"referenceID": 12, "context": "Our work also relates to the log-linear model for morphological segmentation developed by Poon et al. (2009). They propose a joint model over all", "startOffset": 90, "endOffset": 109}, {"referenceID": 15, "context": "Most recently, work by Sirts and Goldwater (2013) uses Adaptor Grammars for minimally supervised segmentation.", "startOffset": 23, "endOffset": 50}, {"referenceID": 5, "context": "In other related work, Dreyer and Eisner (2011) tackle the problem of recovering morphological paradigms and inflectional principles.", "startOffset": 23, "endOffset": 48}, {"referenceID": 7, "context": "Lee et al. (2011) present a model that takes advantage of syntactic context to perform better morphological segmentation.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Lee et al. (2011) present a model that takes advantage of syntactic context to perform better morphological segmentation. Stallard et al. (2012) improve on this approach using the technique of Maximum Marginal decoding to reduce noise.", "startOffset": 0, "endOffset": 145}, {"referenceID": 0, "context": "To validate this measure, we computed the cosine similarity between words and their morphological parents from the CELEX2 database (Baayen et al., 1995).", "startOffset": 131, "endOffset": 152}, {"referenceID": 6, "context": "the literature (Goldwater and Johnson, 2004):", "startOffset": 15, "endOffset": 44}, {"referenceID": 16, "context": "We employ Contrastive Estimation (Smith and Eisner, 2005) and replace the normalization term by a sum over the neighbors of each word.", "startOffset": 33, "endOffset": 57}, {"referenceID": 2, "context": "We use LBFGS-B (Byrd et al., 1995) to optimize LCE(\u03b8;D) with gradients given above.", "startOffset": 15, "endOffset": 34}, {"referenceID": 13, "context": "MC-10 = MorphoChallenge 201010, MC-05:10 = MorphoChallenges 2005-10 (aggregated), BOUN = BOUN corpus (Sak et al., 2008), Gigaword = Arabic Gigaword", "startOffset": 101, "endOffset": 119}, {"referenceID": 11, "context": "corpus (Parker et al., 2011), ATB = Arabic Treebank (Maamouri et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 9, "context": ", 2011), ATB = Arabic Treebank (Maamouri et al., 2003)", "startOffset": 31, "endOffset": 54}, {"referenceID": 10, "context": "For the word vectors, we train the word2vec tool (Mikolov et al., 2013) on large text corpora and obtain 200-dimensional vectors for all three languages.", "startOffset": 49, "endOffset": 71}, {"referenceID": 19, "context": "We evaluate performance on individual segmentation points, which is standard for this task (Virpioja et al., 2011).", "startOffset": 91, "endOffset": 114}, {"referenceID": 12, "context": "As is common in unsupervised segmentation (Poon et al., 2009; Sirts and Goldwater, 2013), we included the test words (without their segmentations) with the training words during parameter learning.", "startOffset": 42, "endOffset": 88}, {"referenceID": 15, "context": "As is common in unsupervised segmentation (Poon et al., 2009; Sirts and Goldwater, 2013), we included the test words (without their segmentations) with the training words during parameter learning.", "startOffset": 42, "endOffset": 88}, {"referenceID": 8, "context": "Morfessor has achieved excellent performance on the MorphoChallenge dataset, and is widely used for performing unsupervised morphological analysis on various languages, even in fairly recent work (Luong et al., 2013).", "startOffset": 196, "endOffset": 216}, {"referenceID": 11, "context": "Baselines We compare our model with five other systems: Morfessor Baseline (Morf-Base), Morfessor CatMap (Morf-Cat), AGMorph, the Lee Segmenter and the system of Poon et al. (2009). Morfessor has achieved excellent performance on the MorphoChallenge dataset, and is widely used for performing unsupervised morphological analysis on various languages, even in fairly recent work (Luong et al.", "startOffset": 162, "endOffset": 181}, {"referenceID": 20, "context": "available implementations of these variants (Virpioja et al., 2013; Creutz and Lagus, 2005).", "startOffset": 44, "endOffset": 91}, {"referenceID": 3, "context": "available implementations of these variants (Virpioja et al., 2013; Creutz and Lagus, 2005).", "startOffset": 44, "endOffset": 91}, {"referenceID": 7, "context": "The Lee Segmenter (Lee et al., 2011), improved upon by using Maximum Marginal decoding in Stallard et al.", "startOffset": 18, "endOffset": 36}, {"referenceID": 7, "context": "The Lee Segmenter (Lee et al., 2011), improved upon by using Maximum Marginal decoding in Stallard et al. (2012), has achieved excellent performance on the Arabic (ATB) dataset.", "startOffset": 19, "endOffset": 113}], "year": 2015, "abstractText": "Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and wordlevel features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.1", "creator": "LaTeX with hyperref package"}}}