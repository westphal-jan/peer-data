{"id": "1503.02357", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Context-Dependent Translation Selection Using Convolutional Neural Network", "abstract": "We policy an autobiographical using two pseudonym date recently qualitative mobile translation, present which it convolutional neural providers comes employed to appeal the similarity also a poem each in, languages. The specifically designed form-based architecture encodes not which the semantic expressions entire the languages break, there previously the context documents way phrase followed the be language. Therefore, what approach is even might capture context - dependent heterogeneity similarities 's poetic sixteen. We requirements a curriculum learning strategy to ambulances new older: we classify the training example coming else, lean, few simply categories, out absorbed enabled. understand more including colloquial over penalty lowest context including using programs names from meant with come. Experimental results show that our approach longer modernizes is lob system from up to 5. 4 BLEU 40.", "histories": [["v1", "Mon, 9 Mar 2015 02:16:19 GMT  (548kb,D)", "https://arxiv.org/abs/1503.02357v1", "9 pages, 4 figures"], ["v2", "Wed, 24 Jun 2015 01:07:40 GMT  (553kb,D)", "http://arxiv.org/abs/1503.02357v2", "Short version is accepted by ACL 2015"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["baotian hu", "zhaopeng tu", "zhengdong lu", "hang li", "qingcai chen"], "accepted": true, "id": "1503.02357"}, "pdf": {"name": "1503.02357.pdf", "metadata": {"source": "CRF", "title": "Context-Dependent Translation Selection Using Convolutional Neural Network", "authors": ["Zhaopeng Tu", "Hong Kong", "Baotian Hu", "Zhengdong Lu", "Hang Li"], "emails": ["tuzhaopeng@gmail.com"], "sections": [{"heading": null, "text": "Context-Dependent Translation Selection Using Convolutional Neural Network\nZhaopeng Tu\u2217 Huawei Technologies Noah\u2019s Ark Lab, Hong Kong\nBaotian Hu Harbin Institute of Technology, Shenzhen Graduate School\nZhengdong Lu Huawei Technologies Noah\u2019s Ark Lab, Hong Kong\nHang Li Huawei Technologies Noah\u2019s Ark Lab, Hong Kong\nWe propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points."}, {"heading": "1. Introduction", "text": "In a conventional statistical machine translation (SMT) system, the translation model is constructed in two steps (Koehn et al. 2003). First, bilingual phrase pairs respecting to the word alignments are extracted from a word-aligned parallel corpus. Second, the phrase pairs are assigned with scores calculated using their relative frequencies in the same corpus. However, only finding and utilizing translation pairs based on their surface forms is not sufficient: the conventional approach often fails to capture translation pairs which are grammatically and semantically similar.\nTo alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. A matching score is computed by measuring the distance between the feature vectors of the phrases, and is incorporated into the SMT system as an additional feature.\nThe above methods, however, neglect the information of local contexts, which has been proven to be useful for disambiguating translation candidates during decoding (He et al. 2008; Marton and Resnik 2008). The matching scores of translation pairs are treated the same, even they are in different contexts. Accordingly, the methods fail to adapt to local contexts and lead to precision issues for specific sentences in different contexts.\n\u2217 Units 525-530, Core Building 2, Hong Kong Science Park, Shatin, Hong Kong. Email: tuzhaopeng@gmail.com\n\u00a9 2015 Huawei Technologies Noah\u2019s Ark Lab, Hong Kong\nar X\niv :1\n50 3.\n02 35\n7v 2\n[ cs\n.C L\n] 2\n4 Ju\nn 20\n15\nTo capture useful context information, we propose a convolutional neural network architecture to measure context-dependent semantic similarities between phrase pairs in two languages. For each phrase pair, we use the sentence containing the phrase in source language as the context. With the convolutional neural network, we summarize the information of a phrase pair and its context, and further compute the pair\u2019s matching score with a multi-layer perceptron.\nWe discriminately train the model using a curriculum learning strategy. We classify the training examples (i.e. triples (source phrase with its context, positive candidate, negative candidate)) according to the difficulty level of distinguishing the positive candidate (i.e. correct translation for the source phrase in the specific context) from the negative candidate (i.e. a bad translation in this context). Then we train the model to learn the semantic information from easy (basic semantic similarities between phrase pairs) to difficult (context-dependent semantic similarities).\nExperimental results on a large-scale translation task show that the context-dependent convolutional matching (CDCM) model improves the performance by up to 1.4 BLEU points over a strong phrase-based SMT system. Moreover, the CDCM model significantly outperforms its context-independent counterpart, proving that it is necessary to incorporate local contexts into SMT. Contributions. Our key contributions include:\n\u2022 we introduce a novel CDCM model to capture context-dependent semantic similarities between phrase pairs (Section 3);\n\u2022 we develop a novel learning algorithm to train the CDCM model using a curriculum learning strategy (Section 4)."}, {"heading": "2. Related Work", "text": "Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations.\nThere is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. However, these discrete context features usually suffer the data sparseness problem. In addition, these models treated each word as a distinct feature, which can not leverage the semantic similarity between words as our model. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. However, they only focused on frequent phrase pairs and induced phrasal similarities by simply summing up the matching scores of all the embraced words. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston 2008).\nAnother line of work focuses on capturing the document-level contexts via distributed representations. For instance, Xiao et al. (2012) and Cui et al. (2014) incorporated documentlevel topic information to select more semantically matched rules. Although many sentences share the same topic with the document where they occur, there are a lot of sentences actually do have topics different from those of their documents (Xiong and Zhang 2013). While these general contexts over the whole document may be not precise enough for the specific sentences in contexts different from the document, our approach is capable of learning the representations for different sentences respectively. Moreover, they learned distributed representations for documents rather than phrases and derived distributed phrase representations from the corresponding\nZhaopeng Tu Context-Dependent Translation Selection Using Convolutional Neural Network\ndocuments, while we attempt to build and train a single, large neural network that reads phrase pairs with contexts and outputs the match degrees directly.\nIn recent years, there has also been growing interest in bilingual phrase representations that group phrases with a similar meaning across different languages. Based on that translation equivalents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space. For example, Gao et al. (2014) projected phrases from both source and target sides into a common, continuous space that is language independent. Although Zhang et al. (2014) did not enforce the phrase embeddings from both sides to be in the same continuous space, they exploited a transformation between the two semantic embedding spaces. However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs."}, {"heading": "3. Context-Dependent Convolutional Matching Model", "text": "The model architecture, shown in Figure 1, is a variant of the convolutional architecture of Hu et al. (2014). It consists of two components:\n\u2022 convolutional sentence model that summarizes the meaning of the source sentence and the target phrase;\n\u2022 matching model that compares the two representations with a multi-layer perceptron (Bengio 2009).\nLet e\u0302 be a target phrase and f be the source sentence that contains the source phrase aligning to e\u0302. We first project f and e\u0302 into feature vectors x and y via the convolutional sentence model, and then compute the matching score s(x,y) by the matching model. Finally, the score is introduced into a conventional SMT system as an additional feature. Convolutional sentence model. As shown in Figure 1, the model takes as input the embeddings of words (trained beforehand elsewhere) in f and e\u0302. It then iteratively summarizes the meaning of the input through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer.\nIn Layer-1, the convolution layer takes sliding windows on f and e\u0302 respectively, and models all the possible compositions of neighbouring words. The convolution involves a filter to produce a new feature for each possible composition. Given a k-sized sliding window i on f or e\u0302, for example, the jth convolution unit of the composition of the words is generated by:\nci (1,j) = g(c\u0302i (0)) \u00b7 \u03c6(w(1,j) \u00b7 c\u0302i(0) + b(1,j)) (1)\nwhere\n\u2022 g(\u00b7) is the gate function that determines whether to activate \u03c6(\u00b7);\n\u2022 \u03c6(\u00b7) is a non-linear activation function. In this work, we use ReLu (Dahl et al. 2013) as the activation function;\n\u2022 w(1,j) is the parameters for the jth convolution unit on Layer-1, with matrix W(1) = [w(1,1), . . . ,w(1,J)];\n\u2022 c\u0302i(0) is a vector constructed by concatenating word vectors in the k-sized sliding widow i;\n\u2022 b(1,j) is a bias term, with vector B(1) = [b(1,1), . . . ,b(1,J)].\nTo distinguish the phrase pair from its context, we use one additional dimension in word embeddings: 1 for words in the phrase pair and 0 for the others. After transforming words to their tagged embeddings, the convolutional sentence model takes multiple choices of composition using sliding windows in the convolution layer. Note that sliding windows are allowed to cross the boundary of the source phrase to exploit both phrasal and contextual information.\nIn order to avoid the length variability of source sentences and target phrases, we add allzero paddings at the end of the source sentence and target phrase until their maximum length. Moreover, we use the gate function g(\u00b7) to eliminate the effect of the all-zero padding by setting output vector to all-zeros if the input is all-zeros.\nIn Layer-2, we apply a local max-pooling in non-overlapping 1\u00d7 2 windows for every convolution unit\nc (2,j) i = max{c (1,j) 2i , c (1,j) 2i+1} (2)\nIn Layer-3, we perform convolution on output from Layer-2:\nci (3,j) = g(c\u0302i (2)) \u00b7 \u03c6(w(3,j) \u00b7 c\u0302i(2) + b(3,j)) (3)\nZhaopeng Tu Context-Dependent Translation Selection Using Convolutional Neural Network\nAfter more convolution and max-pooling operations, we obtain two feature vectors for the source sentence and the target phrase, respectively. Matching model. The matching score of a source sentence and a target phrase can be measured as the similarity between their feature vectors. Specifically, we use the multi-layer perceptron (MLP), a nonlinear function for similarity, to compute their matching score. First we use one layer to combine their feature vectors to get a hidden state hc.\nhc = \u03c6(wc \u00b7 [xf\u0304i : ye\u0304j ] + bc) (4)\nThen we get the matching score from the MLP:\ns(x,y) = MLP (hc) (5)"}, {"heading": "4. Training", "text": "Ideally, the trained CDCM model is expected to assign a higher matching score to a positive example (a source phrase in a specific context f and its correct translation e\u0302+), and a lower score to a negative example (the source phrase and a bad translation e\u0302\u2212 in the specific context). To this end, we employ a discriminative training strategy with a max-margin objective.\nSuppose we are given the following triples (x,y+,y\u2212) from the oracle, where x,y+,y\u2212 are the feature vectors for f , e\u0302+, e\u0302\u2212 respectively. We have the ranking-based loss as objective:\nL\u0398(x,y +,y\u2212) = max(0, 1 + s(x,y\u2212)\u2212 s(x,y+)) (6)\nwhere s(x,y) is the matching score function defined in Eq. 5, \u0398 consists of parameters for both the convolutional sentence model and MLP. The model is trained by minimizing the above objective, to encourage the model to assign higher matching scores to positive examples and to assign lower scores to negative examples. We use stochastic gradient descent (SGD) to optimize the model parameters \u0398.\nNote that the CDCM model aims at capturing contextual representations that can distinguish good translation candidates from bad ones in various contexts. To this end, we propose a twostep approach. First, we initialize the model with context-dependent bilingual word embeddings to start with strong contextual and semantic equivalence at the word level (Section 4.1). Second, we train the CDCM model with a curriculum strategy to learn the context-dependent semantic similarity at the phrase level from easy (basic semantic similarities between the source and target phrase pair) to difficult (context-dependent semantic similarities for the same source phrase in varying contexts) (Section 4.2)."}, {"heading": "4.1 Initialization by Context-Dependent Bilingual Word Embeddings", "text": "Model initialization plays a critical role in a non-convex problem. The initialization of the CDCM model is the embeddings of words on both languages, a real-value and dense representation of words. Typical word embeddings are trained on monolingual data (Mikolov et al. 2013), thus fails to capture the useful semantic relationship across languages. It has been shown that bilingual word embeddings represent a substantial step in better capturing semantic equivalence at the word level (Zou et al. 2013; Wu et al. 2014), thus could initialize our model with strong semantic information. Bilingual word embeddings refer to the semantic embeddings associated across two languages so that similar units in each language and across languages have similar representations. Zou et al. (2013) utilized MT word alignments to encourage pairs of frequently\nrepresentation representation\nMatching))Score)\nkey point is to open\nFigure 2 Architecture of the CDCM bilingual word embedding model.\naligned words to have similar word embeddings, while Wu et al. (2014) improved bilingual word embeddings with discrete contextual information.\nInspired by the above studies, we propose a context-dependent bilingual word embedding model that exploits both the word alignments and contextual information, as shown in Figure 2. Given an aligned word pair (fi, ej), the context is extracted from the nearby window on each side (the left two words and the right two words in this work). Let f\u0304i = fi\u22122, fi\u22121, fi, fi+1, fi+2 and e\u0304j = ej\u22122, ej\u22121, ej , ej+1, ej+2 be the contextual sequence for the above word pair. We get their vectorial representations by:\nxf\u0304i = \u03c6(wf \u00b7 Le(f\u0304i) + bf ) (7)\nye\u0304j = \u03c6(we \u00b7 Le(e\u0304j) + be) (8)\nwhere Le(\u00b7) converts word sequences into embeddings and returns a vector by concatenating the embeddings.\nSimilarly, we calculate matching score for xf\u0304i and ye\u0304j according to Eq. 4 and Eq. 5. The bilingual word embedding model is also trained by minimizing the objective in Eq. 6. The negative examples are constructed by replacing either fi or ej with words randomly chosen from the corresponding vocabulary."}, {"heading": "4.2 Curriculum Training", "text": "Curriculum learning, first proposed by Bengio et al. (2009) in machine learning, refers to a sequence of training strategies that start small, learn easier aspects of the task, and then gradually increase the difficulty level. It has been shown that the curriculum learning can benefit the nonconvex training by giving rise to improved generalization and faster convergence. The key point is that the training examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones.\nFor each positive example (f , e\u0302+), we have three types of negative examples according to the difficulty level of distinguishing the positive example from them:\n\u2022 Easy: target phrases randomly chosen from the phrase table;\nZhaopeng Tu Context-Dependent Translation Selection Using Convolutional Neural Network\nAlgorithm 1 Curriculum training algorithm. Here T denotes the training examples,W the initial word embeddings, \u03b7 the learning rate in SGD, n the pre-defined number, and t the number of training examples.\n1: procedure CURRICULUM-TRAINING(T , W ) 2: N1 \u2190 easy_negative(T ) 3: N2 \u2190 medium_negative(T ) 4: N3 \u2190 difficult_negative(T ) 5: T \u2190 N1 6: CURRICULUM(T , n \u00b7 t) . CUR. easy 7: T \u2190MIX([N1, N2]) 8: CURRICULUM(T , n \u00b7 t) . CUR. medium 9: for step\u2190 1 . . . n do 10: T \u2190MIX([N1, N2, N3], step) 11: CURRICULUM(T , t) . CUR. difficult 12: procedure CURRICULUM(T , K) 13: iterate until reaching a local minima or K iterations 14: calculate L\u0398 for a random instance in T 15: \u0398 = \u0398\u2212 \u03b7 \u00b7 \u2202L\u0398\n\u2202\u0398 . update parameters\n16: W = W \u2212 \u03b7 \u00b7 0.01 \u00b7 \u2202L\u0398 \u2202W . update embeddings 17: procedure MIX(N, s = 0) 18: len\u2190 length of N 19: if len < 3 then 20: T \u2190 sampling with [0.5, 0.5] from N 21: else 22: T \u2190 sampling with [ 1\ns+2 , 1 s+2 , s s+2 ] from N\n\u2022 Medium: target phrases extracted from the aligned target sentence for other non-overlap source phrases in the source sentence;\n\u2022 Difficult: target phrases extracted from other candidates for the same source phrase.\nWe want the CDCM model to learn the following semantic information from easy to difficult:\n\u2022 the basic semantic similarity between the source sentence and target phrase from the easy negative examples;\n\u2022 the general semantic equivalent between the source and target phrase pair from the medium negative examples;\n\u2022 the context-dependent semantic similarities for the same source phrase in varying contexts from the difficult negative examples.\nAlg. 1 shows the curriculum training algorithm for the CDCM model. We use different portions of the overall training instances for different curriculums (lines 2-11). For example, we only use the training instances that consist of positive examples and easy negative examples in the easy curriculum (lines 5-6). For the latter curriculums, we gradually increase the difficulty level of the training instances (lines 7-12).\nFor each curriculum (lines 12-16), we compute the gradient of the loss objective L\u0398 and learn \u0398 using the SGD algorithm. Note that we meanwhile update the word embeddings to better capture the semantic equivalence across languages during training. If the loss function\nL\u0398 reaches a local minima or the iterations reach the pre-defined number, we terminate this curriculum."}, {"heading": "5. Experiments", "text": "In this section, we try to answer two questions:\n1 Does the proposed approach achieve higher translation quality than the baseline system? Does the approach outperform its context-independent counterpart?\n2 Does model initialization by bilingual word embeddings outperforms its monolingual counterpart in terms of translation quality?\nIn Section 5.2, we evaluate our approach on a Chinese-English translation task. By using the CDCM model, our approach achieves significant improvement in BLEU score by up to 1.4 points. Moreover, the CDCM model significantly outperforms its context-independent counterpart, confirming our hypothesis that local contexts are very useful for machine translation.\nIn Section 5.3, we compare model initializations by bilingual word embeddings and by conventional monolingual word embeddings. Experimental results show that the initialization by bilingual word embeddings outperforms its monolingual counterpart consistently, indicating that bilingual word embeddings give a better initialization of the CDCM model."}, {"heading": "5.1 Setup", "text": "We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset. The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke 2002). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al. 2002) is used to measure translation performance.\nFor training the neural networks, we use 4 convolution layers for source sentences and 3 convolution layers for target phrases. For both of them, 4 pooling layers (pooling size is 2) are used, and all the feature maps are 100. We set the sliding window k = 3, and the learning rate \u03b7 = 0.02. All the parameters are selected based on the development data. To produce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. We obtain 2.4M unique phrase pairs (length ranging from 1 to 7) and 20.2M phrase pairs in different contexts. Since the curriculum training in the CDCM model requires that each source phrase should have at least two corresponding target phrases, we obtain 13.5M phrase pairs after we remove the undesirable ones."}, {"heading": "5.2 Evaluation of Translation Quality", "text": "We have two baseline systems:\n\u2022 Baseline: The baseline system is an open-source system of the phrase-based model \u2013 Moses (Koehn et al. 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model.\nZhaopeng Tu Context-Dependent Translation Selection Using Convolutional Neural Network\nModels MT04 MT05 All Baseline 34.86 33.18 34.40 CICM 35.82\u03b1 33.51\u03b1 34.95\u03b1 CDCM1 35.87\u03b1 33.58 35.01\u03b1 CDCM2 35.97\u03b1 33.80\u03b1 35.21\u03b1 CDCM3 36.26\u03b1\u03b2 33.94\u03b1\u03b2 35.40\u03b1\u03b2\n\u2022 CICM (context-independent convolutional matching) model: Following the previous works (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b), we calculate the matching degree of a phrase pair without considering any contextual information. Each unique phrase pair serves as a positive example and a randomly selected target phrase from the phrase table is the corresponding negative example. The matching score is also introduced into Baseline as an additional feature.\nTable 1 summaries the results of CDCMs trained from different curriculums. No matter from which curriculum it is trained, the CDCM model significantly improves the translation quality on the overall test data (with gains of 1.0 BLEU points). The best improvement can be up to 1.4 BLEU points on MT04 with the fully trained CDCM. As expected, the translation performance is consistently increased with curriculum growing. This indicates that the CDCM model indeed captures the desirable semantic information by the curriculum learning from easy to difficult.\nComparing with its context-independent counterpart (CICM, Row 2), the CDCM model shows significant improvement on all the test data consistently. We contribute this to the incorporation of useful discriminative information embedded in the local context. In addition, the performance of CICM is comparable with that of CDCM1. This is intuitive, because both of them try to capture the basic semantic similarity between the source and target phrase pair. Qualitative Analysis. Figure 3 lists some interesting cases to show why the CDCM model improves the performance. We analyze the phrase pair scores computed by the CDCM model against the phrase translation probabilities from the translation model. First, the CDCM model scores phrase pairs based rather on the semantic similarity and the contextual information than on their co-occurrences in the corpus. Therefore, it is complementary to the translation model. Second, with the growing of curriculum, our model is more likely to capture the contextdependent semantic similarities between phrase pairs. In most cases, the choices of translation candidates by the fully trained CDCM model (i.e. CDCM3) are closer to actual translations for both frequent and less frequent phrases. Third, though the CICM model captures the semantic similarities between phrase pairs, it fails to adapt to different local contexts as well. In contrast, the CDCM model is able to provide different translation candidates based on the discriminative information embedded in the local contexts."}, {"heading": "5.3 Evaluation of Bilingual Word Embeddings", "text": "In this section, we will investigate the influence of the bilingual word embeddings we use to initialize the CDCM model. We use the Word2Vec (Mikolov et al. 2013) to train the monolingual word embeddings. We train the bilingual word embeddings using the approach described in Section 4.1. Dimensions of both bilingual and monolingual embeddings are 50.\nTable 2 shows the comparative results between bilingual and monolingual word embeddings. As seen, our bilingual word embedding model outperforms its monolingual counterpart\nZhaopeng Tu Context-Dependent Translation Selection Using Convolutional Neural Network\nconsistently. Zou et al. (2013) and Wu et al. (2014) reported that word-level semantic relationships across languages, captured by the bilingual word embeddings, boost machine translation performance. Our results reconfirm these findings. Qualitative Analysis. Figure 4 lists some cases to show why the context-dependent bilingual word embeddings produce consistent improvements. As seen, the CDCM model initialized by bilingual word embeddings produces more discriminative results than its monolingual counterpart. Take CDCM3 as an example, the monolingual word embeddings scenario prefers the candidates that contain \u201cmain point is\", while its bilingual counterpart selects different candidates that share the same semantic meaning. One possible reason is that bilingual and contextual information helps to capture the semantic relationships between words across languages (Yang et al. 2013), thus better phrasal similarities by using principle of compositionality."}, {"heading": "5.4 Discussion", "text": "Convolutional Model vs. Recursive Model. Previous works on bilingual phrase representations usually employ Recurrent Neural Network (RNN) (Cho et al. 2014b) or Recursive AutoEncoder (RAE) (Zhang et al. 2014). It has been observed in (Kalchbrenner and Blunsom 2013; Sutskever et al. 2014; Cho et al. 2014a) that the recursive approaches suffer from a significant drop in translation quality when translating long sentences. In contrast, Kalchbrenner et al. (2014) show that the convolutional model could represent the semantic content of a long sentence accurately. Therefore, we choose the convolutional architecture to model the meaning of sentence. Limitations. Unlike recursive models, the convolutional architecture has a fixed depth, which bounds the level of composition. In this task, this limitation can be largely compensated with a network afterwards that can take a \u201cglobal\u201d synthesis on the learned sentence representation.\nOne of the hypotheses we tested in the course of this research was disproved. We thought it likely that the difficult curriculum (i.e. distinguish the correct translation from other candidates for a given context) would contribute most to the improvement, since this circumstance is more consistent with the real decoding procedure. This turned out to be false, as shown in Table 1. One possible reason is that the \u201cnegative\u201d examples (other candidates for the same source phrase) may share the same semantic meaning with the positive one, thus give a wrong guide in the supervised training. Constructing a reasonable set of negative examples that are more semantically different from the positive one is left for our future work."}, {"heading": "6. Conclusion", "text": "In this paper, we propose a context-dependent convolutional matching model to capture semantic similarities between phrase pairs that are sensitive to contexts. Experimental results show that our approach significantly improves the translation performance and obtains improvement of 1.0 BLEU scores on the overall test data.\nIntegrating deep architecture into context-dependent translation selection is a promising way to improve machine translation. This paper is the first step in what we hope will be a long and fruitful journey. In the future, we will try to exploit contextual information at the target side (e.g., partial translations)."}], "references": [{"title": "On the properties of neural machine translation: encoder\u2013decoder approaches", "author": ["Cho et al.2014a]Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "SSST", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014b]Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008]Ronan Collobert", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Learning topic representation for smt with neural networks", "author": ["Cui et al.2014]Lei Cui", "Dongdong Zhang", "Shujie Liu", "Qiming Chen", "Mu Li", "Ming Zhou", "Muyun Yang"], "venue": null, "citeRegEx": "Cui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2014}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["Dahl et al.2013]George E Dahl", "Tara N Sainath", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014]Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao et al.2014]Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Improving statistical machine translation using lexicalized rule selection", "author": ["He et al.2008]Zhongjun He", "Qun Liu", "Shouxun Lin"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2008\\E", "shortCiteRegEx": "He et al\\.", "year": 2008}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014]Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003]Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Koehn et al.2007]Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": "ACL", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Maximum entropy based rule selection model for syntax-based statistical machine translation", "author": ["Liu et al.2008]Qun Liu", "Zhongjun He", "Yang Liu", "Shouxun Lin"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Soft syntactic constraints for hierarchical phrased-based translation", "author": ["Marton", "Resnik2008]Yuval Marton", "Philip Resnik"], "venue": null, "citeRegEx": "Marton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marton et al\\.", "year": 2008}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Meng et al.2015]Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "venue": null, "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov et al.2013]Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Josef Och"], "venue": "ACL", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improve statistical machine translation with context-sensitive bilingual semantic embedding model", "author": ["Wu et al.2014]Haiyang Wu", "Daxiang Dong", "Xiaoguang Hu", "Dianhai Yu", "Wei He", "Hua Wu", "Haifeng Wang", "Ting Liu"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "A Topic Similarity Model for Hierarchical Phrase-based Translation", "author": ["Xiao et al.2012]Xinyan Xiao", "Deyi Xiong", "Min Zhang", "Qun Liu", "Shouxun Lin"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2012}, {"title": "A topic-based coherence model for statistical machine translation", "author": ["Xiong", "Zhang2013]Deyi Xiong", "Min Zhang"], "venue": null, "citeRegEx": "Xiong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2013}, {"title": "Word Alignment Modeling with Context Dependent Deep Neural Network", "author": ["Yang et al.2013]Nan Yang", "Shujie Liu", "Mu Li", "Ming Zhou", "Nenghai Yu"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Bilingually-constrained phrase embeddings for machine translation", "author": ["Zhang et al.2014]Jiajun Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Chengqing Zong"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013]Will Y Zou", "Richard Socher", "Daniel Cer", "Christopher D Manning"], "venue": null, "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "In a conventional statistical machine translation (SMT) system, the translation model is constructed in two steps (Koehn et al. 2003).", "startOffset": 114, "endOffset": 133}, {"referenceID": 6, "context": "To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b).", "startOffset": 151, "endOffset": 205}, {"referenceID": 23, "context": "To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b).", "startOffset": 151, "endOffset": 205}, {"referenceID": 7, "context": "The above methods, however, neglect the information of local contexts, which has been proven to be useful for disambiguating translation candidates during decoding (He et al. 2008; Marton and Resnik 2008).", "startOffset": 164, "endOffset": 204}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching.", "startOffset": 13, "endOffset": 49}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching.", "startOffset": 13, "endOffset": 78}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. However, these discrete context features usually suffer the data sparseness problem. In addition, these models treated each word as a distinct feature, which can not leverage the semantic similarity between words as our model. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.", "startOffset": 13, "endOffset": 411}, {"referenceID": 6, "context": "For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. However, these discrete context features usually suffer the data sparseness problem. In addition, these models treated each word as a distinct feature, which can not leverage the semantic similarity between words as our model. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. However, they only focused on frequent phrase pairs and induced phrasal similarities by simply summing up the matching scores of all the embraced words. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks (Collobert and Weston 2008). Another line of work focuses on capturing the document-level contexts via distributed representations. For instance, Xiao et al. (2012) and Cui et al.", "startOffset": 13, "endOffset": 1112}, {"referenceID": 3, "context": "(2012) and Cui et al. (2014) incorporated documentlevel topic information to select more semantically matched rules.", "startOffset": 11, "endOffset": 29}, {"referenceID": 5, "context": "For example, Gao et al. (2014) projected phrases from both source and target sides into a common, continuous space that is language independent.", "startOffset": 13, "endOffset": 31}, {"referenceID": 5, "context": "For example, Gao et al. (2014) projected phrases from both source and target sides into a common, continuous space that is language independent. Although Zhang et al. (2014) did not enforce the phrase embeddings from both sides to be in the same continuous space, they exploited a transformation between the two semantic embedding spaces.", "startOffset": 13, "endOffset": 174}, {"referenceID": 5, "context": "For example, Gao et al. (2014) projected phrases from both source and target sides into a common, continuous space that is language independent. Although Zhang et al. (2014) did not enforce the phrase embeddings from both sides to be in the same continuous space, they exploited a transformation between the two semantic embedding spaces. However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks.", "startOffset": 13, "endOffset": 697}, {"referenceID": 5, "context": "For example, Gao et al. (2014) projected phrases from both source and target sides into a common, continuous space that is language independent. Although Zhang et al. (2014) did not enforce the phrase embeddings from both sides to be in the same continuous space, they exploited a transformation between the two semantic embedding spaces. However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks.", "startOffset": 13, "endOffset": 714}, {"referenceID": 5, "context": "However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs.", "startOffset": 67, "endOffset": 88}, {"referenceID": 8, "context": "The model architecture, shown in Figure 1, is a variant of the convolutional architecture of Hu et al. (2014). It consists of two components:", "startOffset": 93, "endOffset": 110}, {"referenceID": 4, "context": "In this work, we use ReLu (Dahl et al. 2013) as the activation function;", "startOffset": 26, "endOffset": 44}, {"referenceID": 15, "context": "Typical word embeddings are trained on monolingual data (Mikolov et al. 2013), thus fails to capture the useful semantic relationship across languages.", "startOffset": 56, "endOffset": 77}, {"referenceID": 24, "context": "It has been shown that bilingual word embeddings represent a substantial step in better capturing semantic equivalence at the word level (Zou et al. 2013; Wu et al. 2014), thus could initialize our model with strong semantic information.", "startOffset": 137, "endOffset": 170}, {"referenceID": 19, "context": "It has been shown that bilingual word embeddings represent a substantial step in better capturing semantic equivalence at the word level (Zou et al. 2013; Wu et al. 2014), thus could initialize our model with strong semantic information.", "startOffset": 137, "endOffset": 170}, {"referenceID": 15, "context": "Typical word embeddings are trained on monolingual data (Mikolov et al. 2013), thus fails to capture the useful semantic relationship across languages. It has been shown that bilingual word embeddings represent a substantial step in better capturing semantic equivalence at the word level (Zou et al. 2013; Wu et al. 2014), thus could initialize our model with strong semantic information. Bilingual word embeddings refer to the semantic embeddings associated across two languages so that similar units in each language and across languages have similar representations. Zou et al. (2013) utilized MT word alignments to encourage pairs of frequently", "startOffset": 57, "endOffset": 589}, {"referenceID": 19, "context": "aligned words to have similar word embeddings, while Wu et al. (2014) improved bilingual word embeddings with discrete contextual information.", "startOffset": 53, "endOffset": 70}, {"referenceID": 17, "context": "For evaluation, case-insensitive NIST BLEU (Papineni et al. 2002) is used to measure translation performance.", "startOffset": 43, "endOffset": 65}, {"referenceID": 11, "context": "\u2022 Baseline: The baseline system is an open-source system of the phrase-based model \u2013 Moses (Koehn et al. 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model.", "startOffset": 91, "endOffset": 110}, {"referenceID": 6, "context": "\u2022 CICM (context-independent convolutional matching) model: Following the previous works (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b), we calculate the matching degree of a phrase pair without considering any contextual information.", "startOffset": 88, "endOffset": 142}, {"referenceID": 23, "context": "\u2022 CICM (context-independent convolutional matching) model: Following the previous works (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014b), we calculate the matching degree of a phrase pair without considering any contextual information.", "startOffset": 88, "endOffset": 142}, {"referenceID": 15, "context": "We use the Word2Vec (Mikolov et al. 2013) to train the monolingual word embeddings.", "startOffset": 20, "endOffset": 41}, {"referenceID": 22, "context": "One possible reason is that bilingual and contextual information helps to capture the semantic relationships between words across languages (Yang et al. 2013), thus better phrasal similarities by using principle of compositionality.", "startOffset": 140, "endOffset": 158}, {"referenceID": 22, "context": "Zou et al. (2013) and Wu et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "(2013) and Wu et al. (2014) reported that word-level semantic relationships across languages, captured by the bilingual word embeddings, boost machine translation performance.", "startOffset": 11, "endOffset": 28}, {"referenceID": 23, "context": "2014b) or Recursive AutoEncoder (RAE) (Zhang et al. 2014).", "startOffset": 38, "endOffset": 57}, {"referenceID": 18, "context": "It has been observed in (Kalchbrenner and Blunsom 2013; Sutskever et al. 2014; Cho et al. 2014a) that the recursive approaches suffer from a significant drop in translation quality when translating long sentences.", "startOffset": 24, "endOffset": 96}, {"referenceID": 0, "context": "Previous works on bilingual phrase representations usually employ Recurrent Neural Network (RNN) (Cho et al. 2014b) or Recursive AutoEncoder (RAE) (Zhang et al. 2014). It has been observed in (Kalchbrenner and Blunsom 2013; Sutskever et al. 2014; Cho et al. 2014a) that the recursive approaches suffer from a significant drop in translation quality when translating long sentences. In contrast, Kalchbrenner et al. (2014) show that the convolutional model could represent the semantic content of a long sentence accurately.", "startOffset": 98, "endOffset": 422}], "year": 2015, "abstractText": "We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.", "creator": "LaTeX with hyperref package"}}}