{"id": "1503.03238", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2015", "title": "Scalable Discovery of Time-Series Shapelets", "abstract": "Time - comic classification is particular significant cause for the data machinery civic current hold the instead extending both application domains involving put - series signals. A recent paradigm, used shapelets, moreover patterns still any highly estimation full given target constants. Shapelets are samples by 4.4 's prediction aptitude of way set still huge (shapelet) candidates. The contenders varies consist in all into includes of now cross-validation, because, was discovery thus shapelets example labor-intensive expensive. This paper aims on essays method believe avoids impacts where correction complexity large similar dole in Euclidean mile space, taking an data schemas pruning experiment. In addition, our matrix incorporates a supervised shapelet naming no waveguide out when so favored that improve occupational accuracy. Empirical sources on 37 non-clinical from this UCR artists intend does certainly algorithms this 3 - 48 orders of 7.0 shift all the world upgrading shapelet - search method, while support ca measured airspeed.", "histories": [["v1", "Wed, 11 Mar 2015 09:38:49 GMT  (1110kb,D)", "http://arxiv.org/abs/1503.03238v1", "Under review in the journal \"Knowledge and Information Systems\" (KAIS)"]], "COMMENTS": "Under review in the journal \"Knowledge and Information Systems\" (KAIS)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["josif grabocka", "martin wistuba", "lars schmidt-thieme"], "accepted": false, "id": "1503.03238"}, "pdf": {"name": "1503.03238.pdf", "metadata": {"source": "CRF", "title": "Scalable Discovery of Time-Series Shapelets", "authors": ["Josif Grabocka", "Martin Wistuba", "Lars Schmidt-Thieme"], "emails": ["josif@ismll.uni-hildesheim.de", "wistuba@ismll.uni-hildesheim.de", "schidt-thieme@ismll.uni-hildesheim.de"], "sections": [{"heading": null, "text": "Scalable Discovery of Time-Series Shapelets\nJosif Grabocka, Martin Wistuba, Lars Schmidt-Thieme Information Systems and Machine Learning Lab\nUniversity of Hildesheim, 31141 Hildesheim, Germany Email: {josif,wistuba,schidt-thieme}@ismll.uni-hildesheim.de\nAbstract\u2014Time-series classification is an important problem for the data mining community due to the wide range of application domains involving time-series data. A recent paradigm, called shapelets, represents patterns that are highly predictive for the target variable. Shapelets are discovered by measuring the prediction accuracy of a set of potential (shapelet) candidates. The candidates typically consist of all the segments of a dataset, therefore, the discovery of shapelets is computationally expensive. This paper proposes a novel method that avoids measuring the prediction accuracy of similar candidates in Euclidean distance space, through an online clustering pruning technique. In addition, our algorithm incorporates a supervised shapelet selection that filters out only those candidates that improve classification accuracy. Empirical evidence on 45 datasets from the UCR collection demonstrate that our method is 3-4 orders of magnitudes faster than the fastest existing shapelet-discovery method, while providing better prediction accuracy.\nI. INTRODUCTION\nClassification of time-series data has attracted considerable interest in the recent decades, which is not surprising given the numerous domains where time series are collected. A recent paradigm has emerged into the perspective of classifying time series, the notion of shapelets. Shapelets are supervised segments of series that are highly descriptive of the target variable [1]. In the recent years, shapelets have achieved a high momentum in terms of research focus [1], [2], [3], [4], [5].\nDistances of time series to shapelets can be perceived as new classification predictors, also baptized as \u201dthe shapelettransformed data\u201d [2], [6]. It has been shown by various researchers that shapelet-derived predictors boost the classification accuracy [7], [2], [4]. In particular, shapelets are efficient in datasets where the class discrimination is attributed to local variations of the series content, instead of the global structure [1]. Even though not explicitly mentioned by the related work, the discovery of shapelets can be categorized as a supervised dimensionality reduction technique. In addition, shapelets also provide interpretive features that help domain experts understand the differences between the target classes.\nThe discovery of shapelets, on the other hand, has not been as enthusiastic as their prediction accuracy. The current discovery methods need to search for the most predictive shapelets from all the possible segments of a time series dataset [1], [4], [2]. Since the number of possible candidates is high, the required time for evaluating the prediction quality of each candidate is prohibitive for large datasets. Therefore, the time series research community has proposed different speed-up techniques [1], [4], [5], aiming at making shapelet discovery feasible in terms of time.\nThis paper proposes a novel method that discovers timeseries shapelets considerably faster than the fastest existing method. Our method follows the knowledge that time-series instances contain lots of similar segments. Often inter-class variations of time series depend on differences within small segments, with the remaining parts of the series being similar. Therefore, we hypothesize that the time needed to discover shapelets can be scaled-up by pruning candidate segments that are similar in Euclidean distance space. We introduce a fast distance-based clustering approach to prune future segments that result similar to previously considered ones. In addition, we propose a fast supervised selection of shapelets that filters out the qualitative shapelets using an incremental nearest-neighbor classifier. Extensive experiments conducted on real-life data demonstrate a large reduction (3-4 orders of magnitude) of the discovery time, by even gaining prediction accuracy with respect to baselines. The contributions of this paper can be short-listed as follows:\n1) A fast pruning strategy for similar shapelets in Euclidean space involving a distance-based clustering approach; 2) A fast supervised selection of qualitative shapelets using an incremental nearest-neighbor classifier, conducted jointly with the pruning; 3) Extensive experimental results against the fastest existing shapelet discovery methods on a large set of 45 time-series datasets."}, {"heading": "II. RELATED WORK", "text": "Shapelets were introduced by [1] as a new primitive representation of time-series that is highly predictive of the target. A large pool of candidates from all segments of a dataset were assessed as potential shapelet candidates, while the minimum distance of series to shapelets was used as a predictive feature. The best performing candidates were ranked using the information gain criteria over the target. Successively, other prediction quality metrics were also elaborated such as the Kruskal-Wallis or Mood\u2019s median [6], as well as F-Stats [8]. The minimum distance of the time-series to a set of shapelets can be categorized as a data transformation (dimensionality reduction) and is named as shapelet-transformed data [2]. Standard classifiers have been shown to perform competitively over the shapelet-transformed predictors [6].\nThe excessive amount of potential candidates makes the brute-force (exhaustive) shapelet discovery intractable for large datasets. Therefore, researchers have come up with various approaches for speeding up the search. Early abandoning of the Euclidean distance computation combined with an entropy pruning of the information gain metric is an early pioneer\nar X\niv :1\n50 3.\n03 23\n8v 1\n[ cs\n.L G\n] 1\n1 M\nar 2\n01 5\nin that context [1]. Additional papers emphasize the reuse of computations and the pruning of the search space [4], while the projection of series to the SAX representation was also elaborated [5]. Furthermore, the discovery time of shapelets has been minimized by mining infrequent shapelet candidates [9]. Speed-ups have also been attempted by using hardwarebased implementations, such as the usage of the processing power of GPUs for boosting the search time [10].\nIn terms of applicability, shapelets have been utilized in a battery of real-life domains. Unsupervised shapelets discovery, for instance, has been shown useful in clustering time series [3]. Shapelets have seen action in classifying/identifying humans through their gait patterns [11]. Gesture recognition is another application domain where the discovery of shapelets has played an instrumental role in improving the prediction accuracy [12], [13]. In the realm of medical and health informatics, interpretable shapelets have been shown to help the early classification of time-series [14], [15].\nIn comparison to the state-or-the-art methods, we propose a fast novel method that discovers shapelets by combining a pruning strategy of similar candidates with an incremental classification technique."}, {"heading": "III. SCALABLE SHAPELET DISCOVERY", "text": ""}, {"heading": "A. Distances of Shapelets to Series as Classification Features", "text": "Throughout this paper we denote a time-series dataset having N series of M points each, as T \u2208 RN\u00d7M . While our method can work with series of arbitrary lengths, we define a single length M for ease of mathematical formalism. The distances of shapelets to series can be used as classification features, also known as shapelet-transformed features [2], [6]. The distance of a candidate shapelet to the closest segment of a series can be perceived as a membership degree for that particular shapelet. Equations 1 and 2 formalize the minimum distances between a shapelet s \u2208 Rm and the dataset T as a vector of the Euclidean distances (D) between the shapelet and the closest segment of each series. (The notation Va:b denotes a sub-sequence of vector V from the a-th element to the b-th element.)\nMinDist(s, T ) :=  D(s, T1) D(s, T2)\n... D(s, TN )  (1) D(s, Ti) := min\nj=1,...,M\u2212m+1 \u2016Ti,j:j+m\u22121 \u2212 s\u20162 (2)\nAn illustration of the minimum distances between shapelets and series is shown in Figure 1 for the TwoLeadECG dataset. Two shapelets (purple) are matched to four time-series of two different classes (red and blue). Following the principle that Equation 2 states, the distance of a shapelet is computed to the closest series segment. The distances between training timeseries and the two shapelets can project the dataset to a 2- dimensional shapelet-transformed space, as shown on the right sub-plot. A nearest neighbor classifier and the corresponding classification decision boundary is also illustrated."}, {"heading": "B. Quantification of Similarity Using a Distance Threshold", "text": "A time series dataset contains lots of similar patterns spread over various instances. Since series from the same class follow a similar structure, similar patterns repeat over timeseries of the same class. Similarities can also be observed among time series of different classes, because often classes are discriminated by differences in small sub-sequences rather than the global structure. As a result, we raise the hypothesis that existing state-of-the-art techniques, which exhaustively search all candidates, inefficiently consider lots of very similar patterns.\nFigure 2 illustrates the distribution of distances among arbitrary pairs of candidate segments from various time series of the UCR collection of datasets [16]. As can be seen from subfigure a), the distribution of distances is highly skewed towards\nzero, which indicate that most candidates are very similar to each other. However, a threshold separation on the similarity distance is required to judge segments as being similar or not. We propose to use a threshold over the percentile on the distribution of distances. For instance, Figure 2.b) displays pairs of similar segments whose pairwise distances are within the 25-th percentile of the distance distribution.\nAlgorithm 1: ComputeThreshold: Compute the pruning similarity distance threshold .\nData: Time series data T \u2208 RN\u00d7M , Percentile p \u2208 [1, . . . , 100], Shapelet Lengths \u03a6 \u2208 NL\nResult: Threshold distance \u2208 R Z \u2190 \u2205;1 for 1, . . . , NM do2 Draw random shapelet length \u03a6l \u223c U(\u03a61, . . . ,\u03a6L) ;3 Draw segment indices4 (i, j) \u223c (U(1, . . . , N),U(1, . . . ,M \u2212 \u03a6l + 1)) ; Draw segment indices5 (i\u2032, j\u2032) \u223c (U(1, . . . , N),U(1, . . . ,M \u2212 \u03a6l + 1)) ; Z \u2190 Z \u222a { 1\n\u03a6l ||Ti,j:j+\u03a6l\u22121 \u2212 Ti\u2032,j\u2032:j\u2032+\u03a6l\u22121||2 } ;6\nend7 Z \u2190 sort(Z);8 \u2190 Zd p100N Me;9 return 10\nThe procedure of determining a distance threshold value, denoted and belonging to the p-th percentile of the distance distribution, is described in Algorithm 1. The algorithm selects a pair of random segments starting at indices (i, j), (i\u2032, j\u2032) and having random shapelet lengths \u03a6l. Then a distribution is build by accumulating the distances of random pairs of segments and the distance value that corresponds to the desired percentile p is computed from the sorted list of distance values. For instance, in case all the distance values are sorted from smallest to largest, then the 25-th percentile is the value at the index that belongs to 25% of the total indices.\nTotally there are NML segments in a time-series dataset and the total number of pairs is 12 (NML)(NML \u2212 1). However, in order to guess the distribution of a set of values (here distances), one doesn\u2019t need to have access to the full population of values. On the contrary, a sample of values are sufficient for estimating the distribution. In order to balance between a fast and accurate compromise we choose to select NM -many random segment pairs for estimating the distance distributions. The runtime speed up success of Section IV-C indicates that the distance threshold estimation is accurate."}, {"heading": "C. Main Method: Scalable Discovery of Time-series Shapelets", "text": "The scalable discovery of time series shapelets follows the two primary principles of this paper: i) Pruning of similar candidates, and ii) on-the-fly supervised selection of shapelets. The rationale of these principles is based on the knowledge that the majority of patterns from any specific time series are similar to patterns in other series of the same dataset. Therefore, it is computationally non-optimal to measure the quality of lots of very similar candidates. Instead, we aim at considering only a small nucleus of non-redundant candidates.\nAlgorithm 2: DiscoverShapelets: Scalable discovery of shapelets\nData: Time series data T \u2208 RN\u00d7M , Labels Y \u2208 RN Distance Threshold Percentile p \u2208 [1, . . . , 100], Piecewise Aggregate Approximation ratio: r \u2208 { 12 , 1 4 , . . . }, Shapelet lengths: \u03a6 \u2208 N L Result: Accepted shapelets list A \u2208 R\u2217\u00d7\u2217, Minimum Distances D \u2208 R\u2217\u00d7\u2217\n\u2190 ComputeThreshold(T, p,\u03a6);1 A \u2190 \u2205,R \u2190 \u2205, D \u2190 \u2205, X \u2190 0N\u00d7N , prevAccuracy\u21902 \u2212\u221e; for 1, . . . , NML do3 Draw random series: i \u223c U{1, . . . , N};4 Draw random shapelet length:5 \u03a6l \u223c U{\u03a61, . . . ,\u03a6L}; Draw random segment start:6 j \u223c U{1, . . . ,M \u2212 \u03a6l + 1}; Selected random candidate: s\u2190 Ti,j:j+\u03a6l\u22121;7 if \u00acLookUp(s,A, ) \u2227 \u00acLookUp(s,R, ) then8\nds \u2190 MinDist(s, T ) ;9 for i = 1, . . . , N ; m = i+ 1, . . . , N do10 Xi,m \u2190 Xi,m+ (dsi \u2212 dsm) 2;11 end12 if Accuracy(X,Y ) > prevAccuracy then13 A \u2190 A\u222a {s};14 D \u2190 D \u222a {ds};15 prevAccuracy\u2190 Accuracy(X,Y );16 else17 R \u2190 R\u222a {s};18 for i = 1, . . . , N ; m = i+ 1, . . . , N do19 Xi,m \u2190 Xi,m\u2212 (dsi \u2212 dsm) 2;20 end21 end22 end23 end24 return A, D25\n1) Taxonomy of The Terms: By refused candidates we mean the candidates that are similar to previously considered ones, while by considered candidates we mean those who are not refused. Among the considered candidates, some of them will be accepted and the rest rejected. The decision tree below helps clarifying the terms.\nIs candidate similar to previously considered ones?\nREFUSE candidate! Does candidate improve accuracy?\nACCEPT candidate! REJECT candidate!\nYes. No. Then CONSIDER candidate!\nYes. No.\nThe similarity of a candidate is first evaluated by looking up whether a close candidate has been previously considered, i.e has been previously flagged as either accepted or rejected. The considered non-redundant (non-similar to previous) candidates are subsequently checked on whether they improve the classification accuracy of previously selected candidates, and are either marked as accepted or rejected.\nWe are presenting our method as Algorithm 2 and incre-\nmentally walking the reader through the steps. The algorithm is started by compressing the time-series via the Piecewise Aggregate Approximation technique, to be detailed in Section III-D. In order to prune similar candidates, the threshold distance is computed using Algorithm 1. Our method operates by populating two lists of accepted and rejected shapelets, denoted as A and R, and storing a distance matrix X for distances between series in the shapelet-transformed space.\n2) Pruning Similar Candidates: Random shapelet candidates, denoted s, are drawn from the training time-series and a similarity search is conducted by looking up whether similar candidates have been previously considered (lines 4-8). Equation 3 formalizes the procedure as a similarity search over a list L (e.g., A or R), considering candidates having same length (len()). Please note that in the concrete implementation we use a pruning of the Euclidean distance computations, by stopping comparisons exceeding the threshold .\nLookUp(s,L, ) := \u2203q \u2208 L | ||s\u2212 q||2 < \u2227 length(s) = length(q) (3)\nIn case a candidate is found to be novel (not similar to previously considered), then the distance of the candidate to training series are computed using Equation 1 and stored as ds. Our approach evaluates the joint accuracy of accepted shapelets, so far, using a nearest neighbor classifier over the shapelet-transformed data, i.e. distances of series to accepted shapelets. When checking how does a new (n+1)-st candidate influence the accuracy of n currently accepted candidates, an important speed-up trick can be used. We can pre-compute the distances among shapelet-transformed features in an incremental fashion. The distances among series in the featuretransformed space are stored in a distance matrix, denoted X , and the contribution of a new candidate can be simply added to the distance matrix. Those steps correspond to lines 10-12 and 19-21 in Algorithm 2. It is trivial to verify that this technique can improve the run-time of a nearest neighbor from O ( N2|A| ) to O ( N2 ) , which means that we can avoid recomputing distances among previously accepted |A|-many shapelets.\n3) Supervised Shapelet Selection: In case the contribution of a unique candidate improves the classification accuracy of a nearest neighbor classifier, then the shapelet is added to the accepted list and the distance vector is stored in a shapelettransformed data representation D, in order to be later on used for classifying the test instances. Otherwise, the shapelet is inserted to the rejected list and the contribution of the candidate to the distance matrix X is rolled back. The classification accuracy of the distances between series and a set of shapelets is measured by the nearest neighbor accuracy of the cumulative distance matrix X . The accuracy over the training data is formalized in Equation 4.\nAccuracy(X,Y ) := 1\nN \u2223\u2223\u2223{i | Yi = Yargminm,m6=i Xi,m}\u2223\u2223\u2223N i=1 (4)\n4) Number of Sampled Candidates: Algorithm 2 samples shapelet candidates randomly, however the total number of sampled candidates is NML, that upper bounds the total possible series segments of a dataset. Our method could perform competitively even if we would sample a subset of\nthe total possible candidates, as indicated by Figure 3 plot c). That plot illustrates that the train and test accuracy on the StarLightCurves dataset converges way before trying out all the candidates. However, since the state of the art methods try out all the series segments as candidates, then we also opted for the same approach. In that way, the runtime comparison against the baselines provides an isolated hint on the impact of the pruning strategy.\n5) An Illustration of The Process: We present the main idea of our method with the aid of Figure 3. Sub-figures a), b), c) display the progress of the method on the StarLightCurves dataset, the largest dataset from the UCR collection [16]. The fraction of considered (accepted+rejected) shapelets are shown in a) with respect to the total candidates in the X-axis. As can be seen, the first few candidates are considered until the accepted and rejected lists are populated with patterns from the dataset. Afterwards, the algorithm starts refusing (pruning/not considering) previously considered candidates within the 25-th percentile threshold, while in the end, an impressive 99.97% of candidates are pruned. In fact this behavior is not special to the StarLightCurves dataset. We run the algorithm over all the 45 datasets of the UCR collection and measured the fraction of refused candidates as displayed in the histogram of subfigure c). In average, 99.14% of candidates can be pruned, with cross-validated values p, r on the training data for each dataset.\nAmong the considered candidates, a supervised selection of shapelets is carried on by accepting only those candidates that improve the classification accuracy. Sub-figure b) shows that the number of rejections overcomes the number of acceptances as candidates are evaluated, which validates the current belief that very few shapelets can accurately classify a dataset [1]. As a consequence of the accepted shapelets, the train and test accuracy of the method on the dataset is improved as testified by sub-figure c). With respect to all datasets of the UCR collection, histograms of sub-figures d), e) show that in average only 0.06% of candidates are accepted and 0.81% are rejected.\n6) A further intuition: The similarity based pruning of candidates can be compared to a particular type of clustering where the considered candidates represent centroids. In principle, the mechanism resembles fast online clustering methods [17]. Figure 4 illustrates how the considered shapelets (blue) can be perceived as an threshold clustering of the refused candidates (gray). Each cluster is represented by a hyperball of radius in a m-dimensional space, for m being the shapelet length. For the sake of illustration we selected random points of the shapelets and printed 2-dimensional plots of the 6 considered candidates and 7036 refused candidates from the MALLAT dataset.\nThe threshold distance used for pruning similar candidates has a significant effect on the quantity of refused candidates. Figure 5 analyses that the increase of the percentile parameter both deteriorates the classification accuracy (sub-figure a)) and significantly shortens the running time (sub-figure b)). The higher the distance threshold percentile, the more distant segments will be considered similar and subsequently more candidates will be refused. In order to avoid a severe accuracy deterioration, the percentile parameter p needs to be fixed by cross-validating over the training accuracy.\n0 50 100\n0.4\n0.5\n0.6\n0.7\na) Percentile vs Accuracy\nPercentile\nA cc\nur ac\ny\nOSULeaf Chlorine Adiac\n0 50 100 0\n0.5\n1\nb) Percentile vs Time (sec)\nPercentile\nT im\ne (s\nec )\nOSULeaf Chlorine Adiac\n0 50 100\n96\n98\n100\nc) Percentile vs % Refused\nPercentile\n% R\nef us\ned\nOSULeaf Chlorine Adiac\nFigure 5. Impact of alternating the distance threshold\u2019s percentile (p) value on accuracy, discovery time and the fraction of refused candidates."}, {"heading": "D. Piecewise Aggregate Approximation (PAA)", "text": "The Piecewise Aggregate Approximation (PAA) is a dimensionality reduction technique that shortens time-series by averaging neighbor values [18]. Algorithm 3 illustrates how the time-series of a dataset can be compressed by a ratio r. For instance, if r = 14 then every four consecutive points are replaced by their average values.\nPAA significantly reduces the discovery time of shapelets as shown in Figure 6 (sub-figure b) for selected datasets. On the other hand, subfigure a) shows that the classification accuracy does not deteriorate significantly because time-series data often have a redundancy in length and can be compressed.\nThe exact amount of PAA reduction and the percentile\nAlgorithm 3: PiecewiseAggregateApproximation: Compress every series by a ratio r.\nData: Time series data T \u2208 RN\u00d7M , PAA ratio r \u2208 { 1 2 , 1 3 , 1 4 , . . . } Result: T PAA \u2208 RN\u00d7dM re T \u2190 0N\u00d7dM re;1 for i \u2208 1, . . . , N, j = 1, . . . , dM re do2 for k \u2208 d 1r (j \u2212 1) + 1e, . . . , d j r e do3 T PAAi,j \u2190 T PAAi,j + Ti,k;4 end5 T PAAi,j \u2190 T PAAi,j r;6 end7 return T PAA8\nof the pruning similarity threshold are hyper-parameters that need to be fixed per each dataset using the training data. For instance, Figure 6 (sub-figure c) illustrates the accuracy heatmap on the 50words dataset as a result of alternating both parameters. As shown, optimal accuracy is achieved for moderate values of percentile threshold and compression. As a contrast, (i) excessive compression and (ii) high threshold percentiles can deteriorate accuracy by (i) destroying informative local patterns by compression and (ii) pruning qualitative variations of shapelet candidates."}, {"heading": "E. Algorithmic Analysis of the Runtime Speed-Up", "text": "The running time of shapelet discovery algorithms, which explore candidates among series segments, is upper bounded by the number of candidates in a dataset. Given N -many training series of length M , the total number of shapelet candidates has an order of O ( NM2 ) , while the time needed to find the\nbest shapelet is O ( N2M4 ) . Please note that the discovery time is quadratic in terms of the number of candidates. Applying Piecewise Aggregate Approximation (PAA), in order to reduce the length of time-series by a ratio r \u2208 { 12 , 1 3 , 1 4 , . . . , ...},\ndoes alter the runtime complexity into O ( N2 (rM) 4 )\ntranslated to O ( r4N2M4 ) . In other words, PAA reduces the running time by a factor of r4. Furthermore, similarity pruning of candidates has a determinant role in reducing the runtime complexity. Let us denote the fraction of considered candidates as f := #accepted+#rejectedNM2 . Therefore, if executed after a PAA reduction, our algorithm reduces the number of candidates to O ( fN (rM) 2 ) and impacts the total runtime complex-\nity by O ( fN (rM) 2 \u00d7 ( N (rM) 2 + 2N2 )) , which is upper\nbounded by O ( fr4N2M4 ) , since usually (rM)2 >> 2N . Ultimately, the expected runtime reduction factor achieved by this paper is upper-bounded by fr4.\nThere is an addition term that adds up into the runtime complexity: the time needed to check whether any sampled candidate has been previously considered. Such a complexity is O ( N(rM2)\u00d7 f |r\u03a6\u2217| ) , in other words, all candidates times the time needed to search for similarity on the accepted and rejected lists (f -considered candidates having length |r\u03a6\u2217|). Since |r\u03a6\u2217| \u223c O (rM), then the whole operation has a final complexity of O ( fr3NM3 ) . Such a complexity is smaller\nthat the time needed to evaluate the accuracy of the candidates (O ( fr4N2M4 ) ), therefore does not alter the big-O complex-\nity. Remember, e.g.: O ( 3x3 + 7x2 + 100x ) \u223c O ( x3 ) .\nLet us illustrate the theoretically expected speed-up via an example. Assume we compress time-series into a quarter of the original lengths, i.e. r = 12 . On the other hand, the average fraction of considered shapelets in the UCR datasets is f = 0.0086, as previously displayed in Figure 3. Therefore, a runtime reduction factor of fr4 = (0.0086) (0.065) \u2248 5.3\u00d710\u22124 is expected. As shown, the expected theoretic runtime speedup can be 4 orders of magnitude compared to the exhaustive shapelet discovery. A detailed analysis of the effects of the dimensionality reduction (PAA compression) and pruning on the runtime performance is provided in Section IV-F. Furthermore, in Section IV-C we will empirically demonstrate that our method is faster than existing shapelet discovery methods."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Baselines", "text": "In order to evaluate the efficiency of the proposed method, denoted SD, the fastest state-of-the-art shapelet discovery methods were selected, being:\n1) Logical Shapelet [4] (denoted as LS): advances the original shapelet discovery method [1] by one order of magnitude, via: (i) caching and reusing computations, and (ii) applying an admissible pruning of the search space [4]. 2) Fast Shapelet [5] (denoted as FS): is a recent stateof-the-art method that proposes a random projection technique on the SAX representation by filtering potential candidates [5]. FS has been shown to reduce the shapelet discovery time of LS by two to three orders of magnitude [5]. 3) Improved Fast Shapelet (denoted as FS++): is a variation of FS that we created for the sake of being fair to the FS baseline. The original FS paper iterates through all the shapelet lengths from one to the length of the series. In comparison, our method SD iterates through a subset of the possible lengths (\u03a6) as mentioned in Section IV-B. In order to be fair (with respect to runtime), we created a variant of the FS, named FS++, that also iterates through the same subsets of shapelet lengths that SD does.\nThe comparison against the listed state-of-the-art methods will testify the efficiency of our method in terms of runtime scalability. When proposing a faster solution to a supervised learning task, it is crucial to also demonstrate that the speed-up does not deteriorate the prediction accuracy. For this reason, we payed attention to additionally compare the classification accuracy against the baselines."}, {"heading": "B. Setup and Reproducibility", "text": "In order to demonstrate the speed-up achievements of the proposed shapelet discovery method, we use the popular collection of time-series datasets from the UCR collection [16]. For the sake of completeness, we experimented using all the 45 datasets of the collection. The statistics of the datasets are shown in Table I. For each dataset the number of series instances, the number of classes and the length of the time series is presented.\nOur Scalable Shapelet Discovery method, denoted as SD, requires the tuning of two parameters, the aggregation ratio r and the threshold percentile p. The parameters were searched for each dataset via cross-validation using only the training\ndata. The combination (r, p) that yielded the highest accuracy on train was selected. In case of equal train accuracy scores, then we picked the highest (r, p) values. A grid search was conducted with parameter ranges being r \u2208 { 1 2 , 1 4 , 1 8 } and p \u2208 {15, 25, 35}. Finally, the winning combination of parameters was applied over the test data. We would like to note that we used three shapelet lengths for all our experiments, i.e. L = 3 and \u03a6 = {0.2M, 0.4M, 0.6M}. In order to promote reproducibility we are presenting the p, r values found by our parameter search in Table II.\nWe used the Java programming language to implement our method (SD), while the other baselines (LS, FS, FS++) are implemented in C++. We decided to use the C++ source codes provided and optimized by the respective baseline paper authors [5], [4], in order to avoid typical allegations on inefficient re-implementations. Finally, we are presenting the exact number of accepted shapelets per each dataset and the respective percentages of the accepted, rejected and refused candidates in the columns merged under \u201dSD Performance\u201d. All experiments (both our method and the baselines) were conducted in a Sun Grid Engine distributed cluster with 40 node processors, each being Intel Xeon E5-2670v2 with speed 2.50GHz and 64GB of shared RAM for all nodes. The operating system was Linux CentOS 6.3. All the experiments were launched using the same cluster parameters.\nThe authors are devoted to promote experimental reproducibility. For this reason the source code, all the datasets, the executable file and instructions are provided unconditionally1."}, {"heading": "C. Highly Qualitative Runtime Results", "text": "The empirical results include both the discovery time and the classification accuracy of our method SD against baselines for 45 UCR datasets. Table II contains a list of results per dataset, where the discovery time is measured in seconds. A time-out threshold of 24 hours was set for the discovery of shapelets of a single dataset. As can be seen, the Logical Shapelet (LS) exceeded the time-out threshold in a considerable number of datasets. The reader is invited to notice that 24 hours (86400 seconds) is a very large threshold, given that our method SD often finds the shapelets within a fraction of one second, as for instance in the 50words dataset. Finally, we are presenting the exact number of accepted shapelets per\n1https://www.dropbox.com/sh/btiee2pyn6a989q/ AACDfzkkpdYPmgw7pgTgUoeYa\n0 2 4 6\n0\n5\n10\nSD\nlo g(\nLS )\na) Time (sec): SD vs LS\n10 SD 102 SD 103 SD 104 SD 105 SD\n0.4 0.6 0.8 1 0.4\n0.6\n0.8\n1\nSD is better\nLS is better\nb) Accuracy: SD vs LS\nSD\nLS\n0 2 4 6\n0\n5\n10\nSD\nlo g(\nF S\n)\nc) Time (sec): SD vs FS\n10 SD 102 SD 103 SD 104 SD 105 SD\n0.2 0.4 0.6 0.8 1 0.2\n0.4\n0.6\n0.8\n1\nSD is better\nFS is better\nd) Accuracy: SD vs FS\nSD\nF S\n0 2 4 6\n0\n5\n10\nSD\nlo g(\nF S\n+ +\n)\ne) Time (sec): SD vs FS++\n1 SD 10 SD 102 SD 103 SD\n0.4 0.6 0.8 1\n0.4\n0.6\n0.8\n1\nSD is better\nFS++ is better\nf) Accuracy: SD vs FS++\nSD\nF S\n+ +\nFigure 7. Time and accuracy comparison of our method (denoted SD) against state-of-the-art methods both in terms of discovery time and classification accuracy for all the 45 UCR datasets.\neach dataset and the respective percentages of the accepted, rejected and refused candidates in the columns merged under \u201dSD Performance\u201d.\nIt can be clearly deduced that our method SD is faster than the fastest existing baselines LS [4] and FS [5]. There is no dataset where any of the baselines is faster. Even, our modification of FS, i.e. the FS++, is considerably slower than SD. For instance, it took only 3.19 seconds for our method to find the shapelets of the StarLightCurves dataset, which has 1000 training instances each having 1024 points. The highlevel conclusion from the discovery time results is: \u201dSince the introduction of shapelets in 2009, time-series community believed shapelets are very useful classification patterns, but finding them is slow. This paper demonstrates that shapelets can be discovered very fast.\u201d\nThe discovery time measurements do not include the time needed by a practitioner to tune the parameters of the methods. While our method has two parameters (p and r, totaling 3\u00d73 = 9 combinations, see Section IV-B), the strongest baseline (Fast Shapelet) has more parameters, concretely four: the reduced\ndimensionality and cardinality of SAX, the random projection iterations and the number of SAX candidates (denoted d,c,r,k in the original paper [5])."}, {"heading": "D. Competitive Prediction Accuracy", "text": "Yet, our results are atypical in another positive aspect. Most scalability papers propose speed-ups of the learning time by sacrificing a certain fraction of the prediction accuracy. In contrast, our results show that our method is both faster and more accurate than the baselines. The winning method that achieves the highest accuracy on each dataset (on each row) is distinguished in bold. Our method has more wins than the baselines (21 wins against 13 of the second best method) and also a better rank (1.889 against 2.178 of the second best method). The accuracy improvement arises from the joint interaction of accepted shapelets as predictors (distance matrix X in Algorithm 2), while the baselines measure the quality of each shapelet separately, without considering their interactions [1], [4], [5]. Incorporating the interactions among\nshapelets into the prediction model has been recently shown to achieve high classification accuracy [?]."}, {"heading": "E. Speed-Up Analysis", "text": "In order to show the speed-up factor of our method with respect to the (former) state-of-the-art, we provide another presentation of the results in Figure 7. The three plots on the left side show the discovery time of SD in x-axis and the logarithm of the discovery time of each baseline as the y-axis. As can be easily observed from the illustrative order lines, SD is 4 to 5 orders of magnitude faster than the Logical Shapelet (LS) and 3 to 4 orders of magnitude faster than the Fast Shapelet (FS). The datasets where LS exceeds the 24 hour threshold are depicted in light blue. In addition, FS++ is faster than FS because it iterates over less shapelet length sizes, yet it is still 1 to 2 orders of magnitude slower than SD.\nThe plots on the right represent scatter plots of the classification accuracy of SD against the baselines. While generally better than LS and FS, our method SD is largely superior to\nFS++. Such a finding indicates that the accuracy of the Fast Shapelet (FS) is dependent on trying shapelet candidates from a fine-grained set of lengths, while our method is very accurate even though it iterates over few shapelet lengths."}, {"heading": "F. A Modular Decomposition of the Performance", "text": "We have already seen that our proposed method, SD, outperforms significantly the state-of-the-art in terms of runtime and produces even better prediction accuracy. Nevertheless, there are a couple of questions that can be addressed to our method, such as:\n1) What fraction of SD\u2019s runtime reduction is attributed to the novel candidate pruning and what fraction to the PAA compression? 2) To what extent does pruning deteriorate the prediction accuracy?\nIn order to address those analytic questions we will decompose our method in a modular fashion. Our method, SD,\nconducts both a PAA approximation and a pruning by the parameters r, p provided in Table II. In order to isolate the effect of compression and pruning we are creating four variants of our method, namely all the permutations \u201dWith/Without PAA compression\u201d and \u201dWith/Without Pruning\u201d (w.r.t. to p, r from Table II). All the decomposed results of the SD variants are shown in Table III. Note that \u201dNo pruning\u201d means p = 0, while \u201dno PAA\u201d means r = 1. The variant with both pruning and PAA is the same as SD from Section IV-C, which already was shown to be superior to the state of the art.\nLooking into the results of Table III, it is important to observe that the variant with PAA compression alone is significantly faster than the variant without compression (columns 4 vs column 3). However, using pruning without compression is much faster than the exhaustive approach and also much faster than compression alone (column 5 vs. columns 3,4). When pruning and compression are combined (column 6), then the runtime reduction effect multiplies. More concretely, Figure 8 analyses the runtime reduction of SD variants: that\nuse pruning (X-axis) against variants without pruning (Y-axis) for both scenarios with PAA (plot a)) or without PAA (plot b)) compression. As can be clearly deduced, pruning alone has a significant effect on the runtime reduction by 3 to 4 orders of magnitude, compared to the cases where no pruning is employed. While PAA helps our method to be even faster, it is clear that the lion share of the speedup arises from the proposed pruning mechanism.\nThere is still a concern on how does pruning affect the classification accuracy. The prediction accuracy results are demonstrated in Table III for all the datasets, with the winning variant emphasized in bold. The total wins and the ranks of the variants indicate that the best prediction performance is attributed to the exhaustive methods (no pruning, columns 7,8). Such a finding is natural because exhaustive approaches consider all the candidate variants and can extracts more qualitative minimum distance features. Yet, are the results of the exhaustive variants better with a statistical significance margin? Table IV illustrates the p-values of a Wilcoxon Signed Rank test of statistical significance, for a two-tailed hypothesis with a significance level of 5% (\u03b1 = 0.05).\nThe p-values which compare variants that use pruning against variants that does not use pruning are shown in bold and correspond to p = 0.119, p = 0.112. Therefore, the prediction quality using pruning is not significantly (significance means p < 0.05) worse than the exhaustive approach. The final message of this section is: \u201dPruning of candidates provides 3 to 4 orders of runtime speedup without any statistically significant deterioration in terms of classification accuracy.\u201d."}, {"heading": "V. CONCLUSION", "text": "Shapelets represent discriminative segments of a timeseries dataset and the distances of time-series to shapelets are shown to be successful features for classification. The discovery of shapelets is currently conducted by trying out candidates from the segments (sub-sequences) of the time-series. Since the number of candidate segments is large, the time-series community has spent efforts on speeding up the discovery time\nof shapelets. This paper proposed a novel method that prunes the candidates based on a distance threshold to previously considered other similar candidates. In a parallel fashion, a novel supervised selection filters those shapelets that boost classification accuracy. We empirically showed that our method is 3-4 orders of magnitude faster than the fastest existing shapelet discovery methods, while providing a better prediction accuracy."}], "references": [{"title": "Time series shapelets: a new primitive for data mining", "author": ["L. Ye", "E. Keogh"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "A shapelet transform for time series classification", "author": ["J. Lines", "L. Davis", "J. Hills", "A. Bagnall"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Clustering time series using unsupervised-shapelets", "author": ["J. Zakaria", "A. Mueen", "E. Keogh"], "venue": "Proceedings of the 12th IEEE International Conference on Data Mining, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Logical-shapelets: an expressive primitive for time series classification", "author": ["A. Mueen", "E. Keogh", "N. Young"], "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast shapelets: A scalable algorithm for discovering time series shapelets", "author": ["T. Rakthanmanon", "E. Keogh"], "venue": "Proceedings of the 13th SIAM International Conference on Data Mining, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Classification of time series by shapelet transformation", "author": ["J. Hills", "J. Lines", "E. Baranauskas", "J. Mapp", "A. Bagnall"], "venue": "Data Mining and Knowledge Discovery, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Time series shapelets: a novel technique that allows accurate, interpretable and fast classification", "author": ["L. Ye", "E. Keogh"], "venue": "Data Mining and Knowledge Discovery, vol. 22, no. 1, pp. 149\u2013182, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Alternative quality measures for time series shapelets", "author": ["J. Lines", "A. Bagnall"], "venue": "Intelligent Data Engineering and Automated Learning, ser. Lecture Notes in Computer Science, 2012, vol. 7435, pp. 475\u2013483.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast time series classification based on infrequent shapelets", "author": ["Q. He", "F. Zhuang", "T. Shang", "Z. Shi"], "venue": "11th IEEE International Conference on Machine Learning and Applications, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient patternbased time series classification on gpu", "author": ["K.-W. Chang", "B. Deka", "W.-M.W. Hwu", "D. Roth"], "venue": "Proceedings of the 12th IEEE International Conference on Data Mining, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Human gait recognition and classification using time series shapelets", "author": ["P. Sivakumar", "T. Shajina"], "venue": "IEEE International Conference on Advances in Computing and Communications, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Gesture recognition with inertial sensors and optimized dtw prototypes", "author": ["B. Hartmann", "N. Link"], "venue": "IEEE International Conference on Systems Man and Cybernetics, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Prototype optimization for temporarily and spatially distorted time series", "author": ["B. Hartmann", "I. Schwab", "N. Link"], "venue": "the AAAI Spring Symposia, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Extracting interpretable features for early classification on time series", "author": ["Z. Xing", "J. Pei", "P. Yu", "K. Wang"], "venue": "Proceedings of the 11th SIAM International Conference on Data Mining, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Early classification on time series", "author": ["Z. Xing", "J. Pei", "P. Yu"], "venue": "Knowledge and information systems, vol. 31, no. 1, pp. 105\u2013127, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "The UCR Time Series Classification/Clustering Homepage", "author": ["E. Keogh", "Q. Zhu", "H.Y.B. Hu", "X. Xi", "L. Wei", "C.A. Ratanamahatana"], "venue": "www.cs.ucr.edu/\u223ceamonn/time series data/, 2011, [Online; accessed 02-March-2014].", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "On-line new event detection and tracking", "author": ["J. Allan", "R. Papka", "V. Lavrenko"], "venue": "Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, ser. SIGIR \u201998. New York, NY, USA: ACM, 1998, pp. 37\u201345.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Locally adaptive dimensionality reduction for indexing large time series databases", "author": ["K. Chakrabarti", "E. Keogh", "S. Mehrotra", "M. Pazzani"], "venue": "ACM Trans. Database Syst., vol. 27, no. 2, pp. 188\u2013228, Jun. 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Shapelets are supervised segments of series that are highly descriptive of the target variable [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "In the recent years, shapelets have achieved a high momentum in terms of research focus [1], [2], [3], [4], [5].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "In the recent years, shapelets have achieved a high momentum in terms of research focus [1], [2], [3], [4], [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "In the recent years, shapelets have achieved a high momentum in terms of research focus [1], [2], [3], [4], [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "In the recent years, shapelets have achieved a high momentum in terms of research focus [1], [2], [3], [4], [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "In the recent years, shapelets have achieved a high momentum in terms of research focus [1], [2], [3], [4], [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "Distances of time series to shapelets can be perceived as new classification predictors, also baptized as \u201dthe shapelettransformed data\u201d [2], [6].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "Distances of time series to shapelets can be perceived as new classification predictors, also baptized as \u201dthe shapelettransformed data\u201d [2], [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": "It has been shown by various researchers that shapelet-derived predictors boost the classification accuracy [7], [2], [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "It has been shown by various researchers that shapelet-derived predictors boost the classification accuracy [7], [2], [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "It has been shown by various researchers that shapelet-derived predictors boost the classification accuracy [7], [2], [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "In particular, shapelets are efficient in datasets where the class discrimination is attributed to local variations of the series content, instead of the global structure [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 0, "context": "The current discovery methods need to search for the most predictive shapelets from all the possible segments of a time series dataset [1], [4], [2].", "startOffset": 135, "endOffset": 138}, {"referenceID": 3, "context": "The current discovery methods need to search for the most predictive shapelets from all the possible segments of a time series dataset [1], [4], [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "The current discovery methods need to search for the most predictive shapelets from all the possible segments of a time series dataset [1], [4], [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "Therefore, the time series research community has proposed different speed-up techniques [1], [4], [5], aiming at making shapelet discovery feasible in terms of time.", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "Therefore, the time series research community has proposed different speed-up techniques [1], [4], [5], aiming at making shapelet discovery feasible in terms of time.", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Therefore, the time series research community has proposed different speed-up techniques [1], [4], [5], aiming at making shapelet discovery feasible in terms of time.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "Shapelets were introduced by [1] as a new primitive representation of time-series that is highly predictive of the target.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "Successively, other prediction quality metrics were also elaborated such as the Kruskal-Wallis or Mood\u2019s median [6], as well as F-Stats [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 7, "context": "Successively, other prediction quality metrics were also elaborated such as the Kruskal-Wallis or Mood\u2019s median [6], as well as F-Stats [8].", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "The minimum distance of the time-series to a set of shapelets can be categorized as a data transformation (dimensionality reduction) and is named as shapelet-transformed data [2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "Standard classifiers have been shown to perform competitively over the shapelet-transformed predictors [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 0, "context": "in that context [1].", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "Additional papers emphasize the reuse of computations and the pruning of the search space [4], while the projection of series to the SAX representation was also elaborated [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "Additional papers emphasize the reuse of computations and the pruning of the search space [4], while the projection of series to the SAX representation was also elaborated [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 8, "context": "Furthermore, the discovery time of shapelets has been minimized by mining infrequent shapelet candidates [9].", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "Speed-ups have also been attempted by using hardwarebased implementations, such as the usage of the processing power of GPUs for boosting the search time [10].", "startOffset": 154, "endOffset": 158}, {"referenceID": 2, "context": "Unsupervised shapelets discovery, for instance, has been shown useful in clustering time series [3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 10, "context": "Shapelets have seen action in classifying/identifying humans through their gait patterns [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "Gesture recognition is another application domain where the discovery of shapelets has played an instrumental role in improving the prediction accuracy [12], [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 12, "context": "Gesture recognition is another application domain where the discovery of shapelets has played an instrumental role in improving the prediction accuracy [12], [13].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "In the realm of medical and health informatics, interpretable shapelets have been shown to help the early classification of time-series [14], [15].", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "In the realm of medical and health informatics, interpretable shapelets have been shown to help the early classification of time-series [14], [15].", "startOffset": 142, "endOffset": 146}, {"referenceID": 1, "context": "The distances of shapelets to series can be used as classification features, also known as shapelet-transformed features [2], [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "The distances of shapelets to series can be used as classification features, also known as shapelet-transformed features [2], [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 15, "context": "Figure 2 illustrates the distribution of distances among arbitrary pairs of candidate segments from various time series of the UCR collection of datasets [16].", "startOffset": 154, "endOffset": 158}, {"referenceID": 15, "context": "Sub-figures a), b), c) display the progress of the method on the StarLightCurves dataset, the largest dataset from the UCR collection [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "Sub-figure b) shows that the number of rejections overcomes the number of acceptances as candidates are evaluated, which validates the current belief that very few shapelets can accurately classify a dataset [1].", "startOffset": 208, "endOffset": 211}, {"referenceID": 16, "context": "In principle, the mechanism resembles fast online clustering methods [17].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "The Piecewise Aggregate Approximation (PAA) is a dimensionality reduction technique that shortens time-series by averaging neighbor values [18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "1) Logical Shapelet [4] (denoted as LS): advances the original shapelet discovery method [1] by one order of magnitude, via: (i) caching and reusing computations, and (ii) applying an admissible pruning of the search space [4].", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "1) Logical Shapelet [4] (denoted as LS): advances the original shapelet discovery method [1] by one order of magnitude, via: (i) caching and reusing computations, and (ii) applying an admissible pruning of the search space [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "1) Logical Shapelet [4] (denoted as LS): advances the original shapelet discovery method [1] by one order of magnitude, via: (i) caching and reusing computations, and (ii) applying an admissible pruning of the search space [4].", "startOffset": 223, "endOffset": 226}, {"referenceID": 4, "context": "2) Fast Shapelet [5] (denoted as FS): is a recent stateof-the-art method that proposes a random projection technique on the SAX representation by filtering potential candidates [5].", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "2) Fast Shapelet [5] (denoted as FS): is a recent stateof-the-art method that proposes a random projection technique on the SAX representation by filtering potential candidates [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 4, "context": "FS has been shown to reduce the shapelet discovery time of LS by two to three orders of magnitude [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 15, "context": "In order to demonstrate the speed-up achievements of the proposed shapelet discovery method, we use the popular collection of time-series datasets from the UCR collection [16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": "We decided to use the C++ source codes provided and optimized by the respective baseline paper authors [5], [4], in order to avoid typical allegations on inefficient re-implementations.", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "We decided to use the C++ source codes provided and optimized by the respective baseline paper authors [5], [4], in order to avoid typical allegations on inefficient re-implementations.", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "It can be clearly deduced that our method SD is faster than the fastest existing baselines LS [4] and FS [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "It can be clearly deduced that our method SD is faster than the fastest existing baselines LS [4] and FS [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "dimensionality and cardinality of SAX, the random projection iterations and the number of SAX candidates (denoted d,c,r,k in the original paper [5]).", "startOffset": 144, "endOffset": 147}, {"referenceID": 0, "context": "The accuracy improvement arises from the joint interaction of accepted shapelets as predictors (distance matrix X in Algorithm 2), while the baselines measure the quality of each shapelet separately, without considering their interactions [1], [4], [5].", "startOffset": 239, "endOffset": 242}, {"referenceID": 3, "context": "The accuracy improvement arises from the joint interaction of accepted shapelets as predictors (distance matrix X in Algorithm 2), while the baselines measure the quality of each shapelet separately, without considering their interactions [1], [4], [5].", "startOffset": 244, "endOffset": 247}, {"referenceID": 4, "context": "The accuracy improvement arises from the joint interaction of accepted shapelets as predictors (distance matrix X in Algorithm 2), while the baselines measure the quality of each shapelet separately, without considering their interactions [1], [4], [5].", "startOffset": 249, "endOffset": 252}], "year": 2015, "abstractText": "Time-series classification is an important problem for the data mining community due to the wide range of application domains involving time-series data. A recent paradigm, called shapelets, represents patterns that are highly predictive for the target variable. Shapelets are discovered by measuring the prediction accuracy of a set of potential (shapelet) candidates. The candidates typically consist of all the segments of a dataset, therefore, the discovery of shapelets is computationally expensive. This paper proposes a novel method that avoids measuring the prediction accuracy of similar candidates in Euclidean distance space, through an online clustering pruning technique. In addition, our algorithm incorporates a supervised shapelet selection that filters out only those candidates that improve classification accuracy. Empirical evidence on 45 datasets from the UCR collection demonstrate that our method is 3-4 orders of magnitudes faster than the fastest existing shapelet-discovery method, while providing better prediction accuracy.", "creator": "LaTeX with hyperref package"}}}