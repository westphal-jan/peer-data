{"id": "1503.03506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2015", "title": "Diverse Landmark Sampling from Determinantal Point Processes for Scalable Manifold Learning", "abstract": "High biochemistry low of manifold learning licenses its licensing taken large short take. A related strategy with overcome this problem and to perform dimensionality cuts to titles landmarks then well successively embed well entire scintillator with the Nystr \\ \" bharat numerical. The along an questions far arise any: (but) a urban selected first guarantee - Euclidean geometries must far, first dramatically reconstruction loss, (1937) the formula_6 walls from remote acoustically sites must approximate held dimensions there. We propose first sampling followed temples followed determinantal derivative on government - Euclidean venues. Since current determinantal sampling non-linear especially main both dimension often _ addition singularities learning, if present an desirable approximation running second formula_10 still. Further, we recover the to geometry month then sparsification continued calculate give millennium a local covariance formula_10, nearly also itself instance point all. The sustained neighborhood competition recently on three Bhattacharyya distance improvement a embedding is sparsely sampled manifolds. Our microgravity show turn far performer costs proportion. state - taken - following - art landmark unusual subjects.", "histories": [["v1", "Wed, 11 Mar 2015 21:09:28 GMT  (3498kb,D)", "http://arxiv.org/abs/1503.03506v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["christian wachinger", "polina golland"], "accepted": false, "id": "1503.03506"}, "pdf": {"name": "1503.03506.pdf", "metadata": {"source": "CRF", "title": "Diverse Landmark Sampling from Determinantal Point Processes for Scalable Manifold Learning", "authors": ["Christian Wachinger", "Polina Golland"], "emails": [], "sections": [{"heading": null, "text": "High computational costs of manifold learning prohibit its application for large point sets. A common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the Nystro\u0308m method. The two main challenges that arise are: (i) the landmarks selected in non-Euclidean geometries must result in a low reconstruction error, (ii) the graph constructed from sparsely sampled landmarks must approximate the manifold well. We propose the sampling of landmarks from determinantal distributions on non-Euclidean spaces. Since current determinantal sampling algorithms have the same complexity as those for manifold learning, we present an efficient approximation running in linear time. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Bhattacharyya distance improves the embedding of sparsely sampled manifolds. Our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques."}, {"heading": "1 Introduction", "text": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31]. A limiting factor for the spectral analysis is the computational cost of the eigen decomposition. To overcome this limitation, the Nystro\u0308m method [36] is commonly applied to approximate the spectral decomposition of\nthe Gramian matrix. A subset of rows/columns is selected and based on the eigen decomposition of the resulting small sub-matrix, the spectrum of the original matrix can be approximated. While the Nystro\u0308m extension is the standard method for the matrix reconstruction, the crucial part is the subset selection. In early work [36], uniform sampling without replacement was proposed. This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11]. A recent comparison of several approaches is presented in [20].\nOf particular interest for subset selection is volume sampling [11], equivalent to determinantal sampling [4], because reconstruction error bounds exist. This method is, however, not used in practice because of the high computational complexity of sampling from the underlying distributions [20]. Independently, determinantal point processes (DPPs) have been proposed recently for tracking and pose estimation [18]. They were originally designed to model the repulsive interaction between particles. DPPs are well suited for modeling diversity in a point set. A sampling algorithm for DPPs was presented in [17, 18], which has complexity O(n3) for n points. Since this algorithm has the same complexity as the spectral analysis, it cannot be directly used as a subset selection scheme.\nIn this paper, we focus on nonlinear dimensionality reduction for large datasets via manifold learning. Popular manifold learning techniques include kernel PCA [27], Isomap [30], and Laplacian eigenmaps [6]. All of these methods are based on a kernel matrix of size O(n2) that contains the information about the pairwise relationships between the input points. The spectral decomposition of the kernel matrix leads to the low-dimensional embedding of the points. For large n, one is interested in avoiding its explicit calculation and storage. In contrast to general rank-k matrix approximation, this is possible by taking the nature of the non-linear dimensionality reduction into account and relating the entries of the kernel matrix\nar X\niv :1\n50 3.\n03 50\n6v 1\n[ cs\n.L G\n] 1\n1 M\nar 2\n01 5\ndirectly to the original point set.\nConsequently, we propose to perform DPP sampling on the original point set to extract a diverse set of landmarks. Since the input points lie in a non-Euclidean space, ignoring the underlying geometry leads to poor results. To account for the non-Euclidean geometry of the input space, we replace the Euclidean distance with the geodesic distance along the manifold, which is approximated by the graph shortest path distance. Due to the high complexity of DPP sampling, we derive an efficient approximation that runs in O(ndk), with input dimensionality d and subset cardinality k. The algorithm restricts the updates to be local, which enables sampling on complex geometries. This, together with its low computational complexity, makes the algorithm well suited for the subset selection in large scale manifold learning.\nA consequence of the landmark selection is that the manifold is less densely sampled than before, making its approximation with neighborhood graphs more difficult. It was noted in [3], as a critical response to [30], that the approximation of manifolds with graphs is topologically unstable. In order to improve the graph construction, we retain the local geometry around each landmark by locally estimating the covariance matrix on the original point set. This allows us to compare multivariate Gaussian distributions with the Bhattacharyya distance for neighborhood selection, yielding improved embeddings. A shorter version of this work was published in [32]."}, {"heading": "2 Background", "text": "We assume n points in high dimensional space x1, . . . , xn \u2208 Rd and let X \u2208 Rd\u00d7n be the matrix whose i-th column is the point xi. Non-linear dimensionality reduction techniques are based on a positive semidefinite kernel K, with a typical choice of Gaussian or heat kernel Ki,j = exp(\u2212\u2016xi\u2212xj\u20162/2\u03c32). The resulting kernel matrix is of size O(n2). Necessary for spectral analysis is the eigen decomposition of the kernel matrix, which has complexity O(n3). For most techniques, it is only necessary to compute the leading k eigenvectors. The problem can therefore also be considered as finding the best rank-k approximation of the matrix K, with the optimal solution Kk = \u2211k i=1 \u03bbiuiu > i , where \u03bbi is the i-th largest eigenvalue and ui is the corresponding eigenvector."}, {"heading": "2.1 Nystro\u0308m Method", "text": "Suppose J \u2286 {1, . . . , n} is a subset of the original point set of size k and J\u0304 is its complement. We can reorder\nthe kernel matrix K such that\nK = [ KJ\u00d7J KJ\u00d7J\u0304 K>J\u00d7J\u0304 KJ\u0304\u00d7J\u0304 ] , K\u0303 = [ KJ\u00d7J KJ\u00d7J\u0304 K>J\u00d7J\u0304 K > J\u00d7J\u0304K \u22121 J\u00d7JKJ\u00d7J\u0304 ]\nwith K\u0303 being the matrix estimated via the Nystro\u0308m method [36]. The Nystro\u0308m extension leads to the approximation KJ\u0304\u00d7J\u0304 \u2248 K>J\u00d7J\u0304K \u22121 J\u00d7JKJ\u00d7J\u0304 . The matrix inverse is replaced by the Moore-Penrose generalized inverse in case of rank deficiency. The Nystro\u0308m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36]. The challenge lies in finding landmarks that minimize the reconstruction error\n\u2016K \u2212 K\u0303\u2016tr = tr(KJ\u0304\u00d7J\u0304)\u2212 tr(K>J\u00d7J\u0304K \u22121 J\u00d7JKJ\u00d7J\u0304). (1)\nThe trace norm \u2016.\u2016tr is applied because results only depend on the spectrum due to its unitary invariance."}, {"heading": "2.2 Annealed Determinantal Sampling", "text": "A large variety of methods have been proposed for selecting the subset J . For general matrix approximation, this step is referred to as row/column selection of the matrix K, which is equivalent to selecting a subset of points X. This property is important because it avoids explicit computation of the O(n2) entries in the kernel matrix K. We focus on volume sampling for subset selection because of its theoretical advantages [11]. We employ the factorization KJ\u00d7J = Y > J YJ , which exists because KJ is positive semidefinite. Based on this factorization, the volume Vol({Yi}i\u2208J) of the simplex spanned by the origin and the selected points YJ is calculated, which is equivalent to the volume of the parallelepiped spanned by YJ . The subset J is then sampled proportional to the squared volume. This is directly related to the calculation of the determinant with det(KJ\u00d7J) = det(Y >J YJ) = det(YJ)\n2 = Vol2({Yi}i\u2208J). These ideas were further generalized in [4] based on annealed determinantal distributions\nps(J) \u221d det(KJ\u00d7J)s = det(Y >J YJ)s = det(YJ)2s. (2)\nThis distribution is well defined because the principal submatrices of a positive semidefinite matrix are themselves positive semidefinite. Varying the exponent s \u2265 0 results in a family of distributions, modeling the annealing behavior as used in stochastic computations. For s = 0 this is equivalent to uniform sampling [36]. In the following derivations, we focus on s = 1. It was shown in [11] that for J \u223c p(J), |J | = k\nE [ \u2016K \u2212 K\u0303\u2016tr ] \u2264 (k + 1)\u2016K \u2212Kk\u20162F , (3)\nwhere K\u0303 is the Nystro\u0308m reconstruction of the kernel based on the subset J , Kk the best rank-k approximation achieved by selecting the largest eigenvectors, and \u2016.\u2016F the Frobenius norm. It was further shown that the factor k+1 is the best possible for a k-subset. Related bounds were presented in [5]. A further result is that for any matrix K, there exist k + k(k + 1)/ rows whose span contains the rows of a rank-k matrix K\u0303k such that\n\u2016K \u2212 K\u0303k\u20162F \u2264 (1 + \u03b5)\u2016K \u2212Kk\u20162F . (4)\nThis result establishes a connection between matrix approximation and projective clustering, with the selection of a subsets columns being similar to the construction of a coreset [1, 15]."}, {"heading": "3 Method", "text": "In the following, we will first analyze the sampling from determinantal distributions on non-Euclidean geometries. Subsequently, we introduce an efficient algorithm for approximate DPP sampling on manifolds. Finally, we present our approach for robust graph construction on sparsely sampled manifolds."}, {"heading": "3.1 DPP Sampling on Manifolds", "text": "As described in Section 2.2, sampling from determinantal distributions is used for row/column selection. Independently, determinantal point processes (DPPs) were introduced for modeling probabilistic mutual exclusion [22]. They present an attractive scheme for ensuring diversity in the selected subset. An interesting construction of DPPs is based on L-ensembles [9]. Given a positive semidefinite matrix L \u2208 Rn\u00d7n, the likelihood for selecting the subset J \u2286 {1, . . . , n} is\nPL(J) = det(LJ\u00d7J)\ndet(L+ I) , (5)\nwhere I is the identity matrix and LJ\u00d7J is the submatrix of L containing the rows and columns indexed by J . Identifying the L-ensemble matrix L with the kernel matrix K, we can apply DPPs to sample subsets from the point set X.\nTo date, applications using determinantal point processes have assumed Euclidean geometry [8, 18]. For non-linear dimensionality reduction, we assume that the data points lie on non-Euclidean spaces, such as the Swiss roll in Fig. 1(a). To evaluate the performance of DPPs on manifolds, we sample from the Swiss roll. Since we know the construction rule in this case, we can invert it and display the sampled 3D points in the underlying 2D space. The result in Fig. 1(b) shows\nthat the inner part of the roll is almost entirely neglected, as a consequence of not taking the manifold structure into account. A common solution is to use geodesic distances [30], which can be approximated by the graph shortest path algorithm. Consequently, we replace the Euclidean distance \u2016.\u2016 in the construction of the kernel matrix K with the geodesic distance Ki,j = exp(\u2212\u2016xi \u2212 xj\u20162geo/2\u03c32). The result is shown in Fig. 1(c). We observe a clear improvement in the diversity of the sampling, now also including points in the interior part of the Swiss roll."}, {"heading": "3.2 Efficient Approximation of DPP Sampling on Manifolds", "text": "We have seen in the last sections that it is possible to adapt determinantal sampling to non-Euclidean geometries and that error characterizations for the subset selection exist. However, we are missing an efficient sampling algorithm for dealing with large point sets. In [11], an approximative sampling based on the Markov chain Monte Carlo method is proposed to circumvent the combinatorial problem with( n k ) possible subsets. Further approximations include sampling proportional to the diagonal elements Kii or its squared version K2ii, leading to additive error bounds [5, 13]. In [12], an algorithm is proposed that yields a k! approximation to volume sampling, worsening the approximation from (k + 1) to (k + 1)!.\nAlgorithm 1 DPP sampling equivalent to [18]\nRequire: Eigen decomposition of K: {(vi, \u03bbi)}ni=1 1: for i = 1 to n do 2: Add eigenvector vi with probability\n\u03bbi \u03bbi+1\nto V 3: end for 4: B = V > 5: for 1 to |V | do 6: Select i \u2208 1 . . . n with probability P (i) \u221d \u2016Bi\u20162 7: J \u2190 J \u222a i 8: Bj \u2190 Proj\u22a5BiBj for all j \u2208 {1, . . . , n} 9: end for\n10: return J\nAn exact sampling algorithm for DPPs was presented in [17, 18], which requires the eigen decomposition of K = \u2211n i=1 \u03bbiviv > i . We state an equivalent reformulation of this sampling algorithm in Algorithm 1. First, eigenvectors are selected proportional to the magnitude of their eigenvalues and stored as columns in V . Assuming m vectors are selected then V \u2208 Rn\u00d7m. By setting B = V >, we denote the rows of V as Bi \u2208 Rm. In each iteration, we select one of the n points where point i is selected proportional to the squared norm \u2016Bi\u20162. The point is added to the subset J . After the selection of i, all vectors Bj are projected to the or-\nthogonal space of Bi. Since Proj\u22a5BiBi = 0, the same point is almost surely not selected twice. The update formulation differs from [18], where an orthonormal basis of the eigenvectors in V to the i-th basis vector ei \u2208 Rn is constructed. Both formulations are equivalent but provide a different point of view on the algorithm. The modification is, however, essential for the justification of the proposed efficient sampling procedure. The following proposition characterizes the behavior of the update in the algorithm.\nProposition 1. Let Bi, Bj \u2208 Rm \\ {0} be two nonzero vectors in Rm, and \u03b8 = \u2220(Bi, Bj) be the angle between them. Then\n\u2016Proj\u22a5BiBj\u2016 2 = \u2016Bj\u20162 sin2 \u03b8, (6)\nwhere Proj\u22a5BiBj is the the projection of Bj on the subspace perpendicular to Bi. For Bi 6= 0 and Bj = 0 the projection is \u2016Proj\u22a5BiBj\u2016 2 = 0.\nWe state the proof in the supplementary material.\nSampling from a determinantal distribution is not only advantageous because of the presented error bounds but it also makes intuitive sense that a selection of a diverse set of points yields an accurate matrix reconstruction. The computational complexity of Algorithm 1 is, however, similar or even higher than for manifold learning because the spectral decomposition of a dense graph is required, whereas Laplacian eigenmaps operate on sparse matrices. An approach for efficient sampling proposed in [18] works with the dual representation of K = Y >Y to obtain Q = Y Y >, with Q being hopefully smaller than the matrix K. Considering that we work with a Gaussian kernel matrix, this factorization corresponds to the inner product in feature space \u03c6(xi)\n>\u03c6(xj) of the original points xi, xj . It is noted in the literature that the Gaussian kernel corresponds to an infinite dimensional feature space [27]. Since we work with symmetric, positive definite matri-\nces, we can calculate a Cholesky decomposition. However, the dual representation has the same size as the original matrix and therefore yields no improvement.\nAlgorithm 2 Efficient approximation of DPP sampling\nRequire: Point set X, subset cardinality k, nearest neighbors m, update function f\n1: Initialize D = 1n 2: for 1 to k do 3: Select i \u2208 1 . . . n with probability p(i) \u221d Di 4: J \u2190 J \u222a i 5: Calculate \u2206j = \u2016xi \u2212 xj\u2016, \u2200j \u2208 1 . . . n 6: Set m nearest neighbors as neighborhood Ni\nbased on {\u2206j}j=1...n 7: Dj \u2190 Dj \u00b7 f(\u2206j), \u2200j \u2208 Ni 8: Optional: Calculate covariance Ci in local\nneighborhood Ni around xi 9: end for\n10: return J and optionally {Ci}i\u2208J\nTo cope with the high computational costs of exact DPP sampling, we present an efficient approximation in Algorithm 2. The computational complexity is O(ndk). Vector D \u2208 Rn models the probabilities for the selection of points as does \u2016Bi\u20162 in the original DPP sampling algorithm. The algorithm proceeds by sampling k points. Note that the cardinality of the subset cannot be set in the original DPP sampling algorithm, which led to the introduction of k-DPPs [19]. At each iteration we select one point xi with probability p(i) \u221d Di. Next we calculate distances {\u2206j}j=1...n of the selected point xi to all points in X. Based on these distances we identify a local neighborhood Ni of m nearest neighbors around the selected point xi. The update of the probabilities D is restricted to the neighborhood Ni, which proves advantageous for sampling on manifolds. In contrast, Algorithm 1 updates probabilities for all points. If we are interested in achiev-\ning a similar behavior to Algorithm 1, the local neighborhood should include all points. The update function f takes distances \u2206 as input, where we consider f(\u2206) = sin2(\u2206/\u03c4) and f(\u2206) = (1 \u2212 exp(\u2212\u22062/2\u03c32)), as motivated below. In subsequent iterations of the algorithm, points close xi will be selected with lower probability.\nWe initialize the vector D = 1n, since it was noted in [18] that the squared norm of the vectors \u2016Bi\u20162 gives rise initially to a fairly uniform distribution because no points have yet been selected. For the update step, Proposition 1 implies that the update of the probabilities for selecting a point changes with sin2(\u03b8). The angle \u03b8 = \u2220(Bi, Bj) correlates strongly with the distance \u2016Bi\u2212Bj\u2016, since \u2016Bj\u2016 and \u2016Bi\u2016 are initially the same. The selection of the eigenvectors in Algorithm 1 will most likely choose the ones with the largest eigenvalues. We can therefore draw the analogy to multidimensional scaling (MDS) [35] with a Gaussian kernel, where MDS selects the top eigenvectors. Consequently, vectors Bi correspond to low-dimensional embeddings produced by multidimensional scaling of original points xi. The characteristic property of MDS is to preserve pairwise distances between the original space and the embedding space, permitting the approximation \u2016Bi \u2212 Bj\u2016 \u2248 \u2016xi \u2212 xj\u2016. This allows us to approximate the update dependent on the angle \u03b8 by the distance of points in the original space, sin2(\u03b8) \u2248 sin2(\u2016xi \u2212 xj\u2016/\u03c4). The scaling factor \u03c4 ensures that values are in the range [\u2212\u03c0/2;\u03c0/2]. This update is actually very similar to the Welsch function (1 \u2212 exp(\u2212\u2016xi \u2212 xj\u20162/2\u03c32)), which is directly related to the weights in the kernel matrix and is commonly used in machine learning. We illustrate the similarity of both functions in Fig. 2 and focus on the Welsch function in our experiments. For subsequent iterations of the algorithm, the assumption of a similar norm of all vectors Bi is violated, because the projection\non the orthogonal space changes their lengths. Note, however, that this change is locally restricted around the currently selected point. Since this region is less likely to be sampled in the subsequent iterations, the assumption still holds for parts of the space that contain most probability.\nRemark: The proposed algorithm bears similarities to K-means++ [2], which replaces the initialization through uniform sampling of K-means by a new seeding algorithm. K-means++ seeding is a heuristic that samples points based on the minimal distance to previously selected landmarks. Initially, when only one landmark is selected, our proposed algorithm has a nearly identical update rule for a maximal neighborhood Ni. In later iterations, the algorithms differ because K-means++ bases the selection only on the distance to the nearest landmark, while all landmarks influence the probability space in our algorithm. Consequently, our approach potentially yields subsets with higher diversity, as illustrated in the Fig. 3."}, {"heading": "3.3 Robust Landmark-based Graph Construction", "text": "After selecting the landmarks, the next step in the spectral analysis consists of building a graph that approximates the manifold. Common techniques for the graph construction include selection of nearest neighbors or \u03b5-balls around each node. Both approaches require the setting of a parameter, either the number of neighbors or the size of the ball, which is crucial for the performance. Setting the parameter too low leads to a large number of disconnected components, while for many applications one is interested in having all points connected to obtain a consistent embedding\nof all points. Choosing too high values of the parameters leads to short cuts, yielding a bad approximation of the manifold. The appropriate selection of the parameters is more challenging on sparsely sampled manifolds. This is problematic for the subset selection with consecutive Nystro\u0308m reconstruction because we dramatically reduce the sampling rate to limit computational complexity.\nTo address this issue, we propose a new technique for graph construction that takes the initial distribution of the points into account. For each landmark xi, we estimate the covariance matrix Ci around this point from its nearest neighbors Ni, as indicated as optional step in Algorithm 2. This corresponds to multivariate Gaussian distributions G(xi, Ci) centered at the landmark. A commonly used distance to compare distributions is the Bhattacharyya distance, which in case of Gaussian distributions corresponds to\nB(Gi,Gj) = 1\n8 (xi\u2212xj)>C\u22121(xi\u2212xj)+\n1 2 ln ( |C|\u221a |Ci||Cj | ) ,\n(7)\nwith C = Ci+Cj\n2 . This distance is less likely to produce short cuts across the manifold because points that follow the local geometry appear much closer than points that are off the geometry. Consequently, we replace the Euclidean distance for neighborhood selection in manifold learning with the Bhattacharyya distance, as schematically illustrated in Fig. 4. Space requirements of this step are O(d2k). An alternative for limited space and large d is to only use the diagonal entries of the covariance matrix, requiring O(dk). We summarize all steps of the proposed scalable manifold learning in Algorithm 3.\nAlgorithm 3 Summary of scalable manifold learning\n1: Select landmarks with approximate DPP sampling (Algorithm 2) 2: Construct neighborhood graph on landmarks with Bhattacharyya distance (Equ. 7) 3: Calculate low-dimensional embedding based on neighborhood graph 4: Embed non-landmark points by performing outof-sample extension with Nystro\u0308m method"}, {"heading": "4 Experiments", "text": "In our first experiment, we show that the proposed efficient DPP sampling algorithm is well suited for subset selection on non-Euclidean spaces. The benefit for this scenario is that we can restrict the update of the sampling probability D to a local neighborhood Ni around\nthe current point xi. This is in line with the motivation of many manifold learning algorithms, assuming that the space behaves locally like a Euclidean space. In our experiment, we set the local neighborhoodNi to be the 20 nearest neighbors around the selected point xi. The sampling result is shown in Fig. 1(d). We obtain a point set with high diversity, covering the entire manifold. This illustrates that the proposed algorithm preserves the DPP characteristic on complex geometries and is therefore appropriate for subset selection in the context of non-linear dimensionality reduction.\nIn our second experiment, we quantify the reconstruction error for matrix completion as formulated in Equ. (1). We compare the efficient DPP sampling result with uniform sampling [36] and K-means clustering with uniform seeding [37], which performed best in several studies including a recent one [20]. Moreover, we compare to selecting the subset with the K-means++ seeding and the K-means++ algorithm, which we have not yet seen for landmark selection. We construct a Gaussian kernel matrix from 1,000 points on a Swiss roll (Fig. 1) and on a fish bowl (Fig. 5). The fish bowl dataset is a punctured sphere proposed in [26], which is sparsely sampled at the bottom and densely at the top, as shown in the supplementary material. We select subsets varying in size between 25 and 100 and set the parameters \u03c3 = 1 and m = 30 for the Swiss roll and \u03c3 = 1 and m = 150 for the fish bowl. Note that a further improvement can be achieved by adapting these parameters to the size of\nthe subset. For smaller subsets, larger \u03c3 and m lead to improvements. We show the average reconstruction error for the different methods and datasets in Table 1, calculated over 50 different runs. Generally, the performance of uniform sampling is worst. The K-means++ seeding yields better results. K-means improves the results on both initializations, where K-means++ benefits from the better seeding. The diverse set of landmarks selected with efficient DPP sampling leads to the lowest average reconstruction errors for almost all settings.\nIn our third experiment, we perform manifold learning with Laplacian eigenmaps on a point set consisting of 10 million points lying on a Swiss roll. The dataset is too large to apply manifold learning directly. We select 2,500 landmark points with the efficient DPP sampling algorithm and estimate the local covariance matrices, which we feed into the manifold learning al-\ngorithm. We compare the the graph construction with Euclidean and Bhattacharyya neighborhood selection. The graph construction yields a weight matrix W and the corresponding degree matrix Dj,j = \u2211 iWi,j . The weight matrix W is a sparse version of the kernel matrixKJ\u00d7J , where the sparsity is controlled by the number of nearest neighbors in the graph. The generalized eigenvalue problem solved in Laplacian eigenmaps is\n(D \u2212W )\u03c6j = \u03bbjD\u03c6j , (8)\nwith eigenvalues \u03bbj and eigenvectors \u03c6j . The l eigenvectors corresponding to the l smallest non-zero eigenvalues \u039bj,j = \u03bbj , j = 1, . . . l constitute the embedding in l-dimensional space, \u03a6J = [\u03c61, . . . , \u03c6l]. Fig. 6 shows the low-dimensional embedding in 2D for Euclidean (first row) and Bhattacharyya neighborhood selection (second row). We vary the number of nearest neighbors in the graph from 25 to 500. The results show that the Bhattacharyya based neighborhood selection is much more robust with respect to the number of neighbors. Moreover, we notice that for Euclidean neighborhood selection the points seem to cluster along stripes. While we observe this effect also on the Bhattacharyya based embeddings, it is much less pronounced."}, {"heading": "4.1 Image Data", "text": "After having evaluated each of the steps of the proposed approach separately, we now present results for scalable manifold learning on image data. We work with two datasets, one consisting of handwritten digits and a second one consisting of patches extracted from 3D medical images. Each dataset is too large to apply manifold learning directly. Consequently, we select landmarks with the discussed method, perform manifold learning on the landmarks with the Bhattacharyya distance, and use the Nystro\u0308m method to\nembed the entire point set. We only consider the diagonal entries of the covariance matrices due to space limitations. Given the low-dimensional embedding of the landmarks \u03a6J \u2208 Rk\u00d7l and the diagonal matrix of eigenvalues \u039b \u2208 Rl\u00d7l, we use out-of-sample extension with the Nystro\u0308m method to calculate the embedding of the remaining points\n\u03a6J\u0304 = K\u0303 > J\u00d7J\u0304\u03a6J\u039b \u22121, (9)\nK\u0303i,j = Ki,j k \u00b7 \u221a Ei\u2032 [Ki\u2032,j ] \u00b7 Ej\u2032 [Ki,j\u2032 ] , i \u2208 J, j \u2208 J\u0304 (10)\nwhere K\u0303 is the normalized kernel for Laplacian eigenmaps and the expectation is calculated over landmark points [7].\nSince it is difficult to quantify the quality of the embedding, we use the labels associated to the image data to perform nearest neighbor classification in the low dimensional space. We expect advantages for the DPP landmark selection scheme because a diverse set of landmarks spreads the entire point set in the embedding space and helps the classification. For the same reason, we also expect a good performance for the Kmeans++ seeding algorithm. Note that we abstain from pre-processing the data and from applying more advanced classifiers because we are not interested in the absolute classification performance but only in the relative performances across the different landmark selection methods."}, {"heading": "4.1.1 MNIST", "text": "We work with the MNIST dataset [21], consisting of 60,000 binary images of handwritten digits for training and 10,000 for testing, with a resolution of 28\u00d7 28 pixels. We set the neighborhood size to m = 5000 and \u03c3 = 5. We embed the images into 100 dimensional space with Laplacian eigenmaps. Fig. 7(a) shows the statistical analysis over 20 repetitions for several landmark selection schemes across different numbers of landmarks k, as well as the Bhattacharyya based graph construction. The results show that the K-means++ seeding outperforms the uniform initialization, where K-means++ cannot further improve the initialization. Moreover, we observe a significant improvement in classification performance for approximate DPP sampling compared to K-Means++ seeding. Finally, the Bhattacharyya based graph construction further improves the results."}, {"heading": "4.1.2 Head and Neck", "text": "In a second classification experiment, we use 3D CT scans from the head and neck region, having a resolution of 512 \u00d7 512 \u00d7 145 voxels. These images were acquired for radiation therapy of patients with head and neck tumors. Fig. 8 shows one cross sectional slice with segmentations of three structures of risk: left parotid (green), right parotid (blue), and brainstem (green). The segmentation of these structures during treatment planning is of high clinical importance to ensure that they obtain a low radiation dose. We ex-\ntract image patches from the left and right parotid glands, the brainstem and the surrounding region. We are interested in classifying patches into these four groups, where the outcome can readily serve in segmentation algorithms. We work with patches of size 7\u00d7 7\u00d7 3 to reflect the physical resolution of the data which is 0.98\u00d70.98\u00d72.5 mm3. This results in roughly 150,000 patches extracted from three scans. 80,000 patches are used for training and the remaining ones for testing. We extract landmarks from the training patches (m = 5000, \u03c3 = 5) and embed them into 50 dimensional space with Laplacian eigenmaps. Fig. 7(b) shows the statistical analysis of the classification performance over 20 repetitions for various numbers of selected landmarks and selection schemes. K-\nmeans++ seeding outperforms uniform sampling. Kmeans clearly improves the uniform initialization. Kmeans++ shows a similar performance to the initial seeding. Similar to the experiments for MNIST, the efficient DPP approximation leads to significantly better classification results, with an additional gain for the Bhattacharyya based graph construction. In addition to the significant improvement, our runtime measurements showed that our unoptimized Matlab code for efficient DPP sampling runs approximately 15% faster than an optimized MEX version of K-means."}, {"heading": "5 Conclusion", "text": "We have presented contributions for two crucial issues of scalable manifold learning: (i) efficient sampling of diverse subsets from manifolds and (ii) robust graph construction on sparsely sampled manifolds. Precisely, we analyzed the sampling from determinantal distributions on non-Euclidean spaces and proposed an efficient approximation of DPP sampling. The algorithm is well suited for landmark selection on manifolds because probability updates are locally restricted. Further, we proposed the local covariance estimation around landmarks to capture the local characteristics of the space. This enabled a more robust graph construction with the Bhattacharyya distance and yielded low dimensional embeddings of higher quality. We compared to state-of-the-art subset selection procedures and obtained significantly better results with the proposed algorithm.\nAcknowledgements: This work was supported\nin part by the Humboldt foundation, the National Alliance for Medical Image Computing (U54EB005149), and the NeuroImaging Analysis Center (P41-EB015902)."}], "references": [{"title": "Geometric approximation via coresets", "author": ["P. Agarwal", "S. Har-Peled", "K. Varadarajan"], "venue": "Combinatorial and computational geometry 52,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In: SODA. pp", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "The Isomap algorithm and topological stability", "author": ["M. Balasubramanian", "E.L. Schwartz"], "venue": "Science 295(5552),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "On landmark selection and sampling in high-dimensional data analysis", "author": ["M. Belabbas", "P. Wolfe"], "venue": "Philosophical Transactions of the Royal Society", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["M. Belabbas", "P. Wolfe"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering", "author": ["Y. Bengio", "J. Paiement", "P. Vincent", "O. Delalleau", "N. Le Roux", "M. Ouimet"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "On adding a list of numbers (and other one-dependent determinantal processes)", "author": ["A. Borodin", "P. Diaconis", "J. Fulman"], "venue": "Bull. Amer. Math. Soc 47,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Determinantal point processes", "author": ["A. Borodin"], "venue": "The Oxford Handbook of Random Matrix Theory", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W. Chen", "Y. Song", "H. Bai", "C. Lin", "E. Chang"], "venue": "TPAMI pp", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Adaptive sampling and fast low-rank matrix approximation. Approximation, Randomization, and Combinatorial Optimization", "author": ["A. Deshpande", "S. Vempala"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "On the nystr\u00f6m method for approximating a gram matrix for improved kernelbased learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "A novel greedy algorithm for nystr\u00f6m approximation", "author": ["A. Farahat", "A. Ghodsi", "M. Kamel"], "venue": "IASTATS", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Scalable training of mixture models via coresets", "author": ["D. Feldman", "M. Faulkner", "A. Krause"], "venue": "In: NIPS. pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Spectral grouping using the nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "TPAMI pp", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Determinantal processes and independence", "author": ["J. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability Surveys", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Structured determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "NIPS", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "k-dpps: Fixed-size determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML). pp", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Sampling techniques for the nystrom method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "Journal of Machine Learning Research 13,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "The coincidence approach to stochastic point processes", "author": ["O. Macchi"], "venue": "Advances in Applied Probability pp", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1975}, {"title": "Greedy spectral embedding", "author": ["M. Ouimet", "Y. Bengio"], "venue": "Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics. pp", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Fastmap, metricmap, and landmark mds are all nystrom algorithms", "author": ["J.C. Platt"], "venue": "Proceedings of 10th International Workshop on Artificial Intelligence and Statistics. pp", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Laplacebeltrami spectra as \u201dshape-dna\u201d of surfaces and solids", "author": ["M. Reuter", "F.E. Wolter", "N. Peinecke"], "venue": "Computer-Aided Design 38(4),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Think globally, fit locally: unsupervised learning of low dimensional manifolds", "author": ["L. Saul", "S. Roweis"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K. M\u00fcller"], "venue": "Neural computation", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "TPAMI 22(8),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Large-scale manifold learning", "author": ["A. Talwalkar", "S. Kumar", "H. Rowley"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J. Tenenbaum", "V. Silva", "J. Langford"], "venue": "Science 290(5500),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Spectral label fusion", "author": ["C. Wachinger", "P. Golland"], "venue": "In: MICCAI. pp", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Sampling from determinantal point processes for scalable manifold learning. In: Information Processing in Medical Imaging", "author": ["C. Wachinger", "P. Golland"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Initiative, A.D.N., et al.: Brainprint: A discriminative characterization of brain morphology", "author": ["C. Wachinger", "P. Golland", "W. Kremen", "B. Fischl", "M. Reuter"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Manifold learning for image-based breathing gating in ultrasound and mri. Medical image analysis", "author": ["C. Wachinger", "M. Yigitsoy", "E.J. Rijkhorst", "N. Navab"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "On a connection between kernel pca and metric multidimensional scaling", "author": ["C. Williams"], "venue": "In: NIPS. pp", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In: NIPS. pp", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Improved nystr\u00f6m low-rank approximation and error analysis", "author": ["K. Zhang", "I. Tsang", "J. Kwok"], "venue": "In: ICML. pp. 1232\u20131239", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}], "referenceMentions": [{"referenceID": 29, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 148, "endOffset": 156}, {"referenceID": 33, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 148, "endOffset": 156}, {"referenceID": 24, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 173, "endOffset": 181}, {"referenceID": 32, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 173, "endOffset": 181}, {"referenceID": 27, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 200, "endOffset": 208}, {"referenceID": 30, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 200, "endOffset": 208}, {"referenceID": 35, "context": "To overcome this limitation, the Nystr\u00f6m method [36] is commonly applied to approximate the spectral decomposition of the Gramian matrix.", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "In early work [36], uniform sampling without replacement was proposed.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 96, "endOffset": 104}, {"referenceID": 13, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 96, "endOffset": 104}, {"referenceID": 3, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 127, "endOffset": 134}, {"referenceID": 10, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 127, "endOffset": 134}, {"referenceID": 19, "context": "A recent comparison of several approaches is presented in [20].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "Of particular interest for subset selection is volume sampling [11], equivalent to determinantal sampling [4], because reconstruction error bounds exist.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "Of particular interest for subset selection is volume sampling [11], equivalent to determinantal sampling [4], because reconstruction error bounds exist.", "startOffset": 106, "endOffset": 109}, {"referenceID": 19, "context": "This method is, however, not used in practice because of the high computational complexity of sampling from the underlying distributions [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "Independently, determinantal point processes (DPPs) have been proposed recently for tracking and pose estimation [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "A sampling algorithm for DPPs was presented in [17, 18], which has complexity O(n) for n points.", "startOffset": 47, "endOffset": 55}, {"referenceID": 17, "context": "A sampling algorithm for DPPs was presented in [17, 18], which has complexity O(n) for n points.", "startOffset": 47, "endOffset": 55}, {"referenceID": 26, "context": "Popular manifold learning techniques include kernel PCA [27], Isomap [30], and Laplacian eigenmaps [6].", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "Popular manifold learning techniques include kernel PCA [27], Isomap [30], and Laplacian eigenmaps [6].", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "Popular manifold learning techniques include kernel PCA [27], Isomap [30], and Laplacian eigenmaps [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "It was noted in [3], as a critical response to [30], that the approximation of manifolds with graphs is topologically unstable.", "startOffset": 16, "endOffset": 19}, {"referenceID": 29, "context": "It was noted in [3], as a critical response to [30], that the approximation of manifolds with graphs is topologically unstable.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "A shorter version of this work was published in [32].", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "with K\u0303 being the matrix estimated via the Nystr\u00f6m method [36].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 15, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 23, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 28, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 35, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 10, "context": "We focus on volume sampling for subset selection because of its theoretical advantages [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "These ideas were further generalized in [4] based on annealed determinantal distributions", "startOffset": 40, "endOffset": 43}, {"referenceID": 35, "context": "For s = 0 this is equivalent to uniform sampling [36].", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "It was shown in [11] that for J \u223c p(J), |J | = k", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "Related bounds were presented in [5].", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "This result establishes a connection between matrix approximation and projective clustering, with the selection of a subsets columns being similar to the construction of a coreset [1, 15].", "startOffset": 180, "endOffset": 187}, {"referenceID": 14, "context": "This result establishes a connection between matrix approximation and projective clustering, with the selection of a subsets columns being similar to the construction of a coreset [1, 15].", "startOffset": 180, "endOffset": 187}, {"referenceID": 21, "context": "Independently, determinantal point processes (DPPs) were introduced for modeling probabilistic mutual exclusion [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "An interesting construction of DPPs is based on L-ensembles [9].", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "To date, applications using determinantal point processes have assumed Euclidean geometry [8, 18].", "startOffset": 90, "endOffset": 97}, {"referenceID": 17, "context": "To date, applications using determinantal point processes have assumed Euclidean geometry [8, 18].", "startOffset": 90, "endOffset": 97}, {"referenceID": 29, "context": "A common solution is to use geodesic distances [30], which can be approximated by the graph shortest path algorithm.", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "In [11], an approximative sampling based on the Markov chain Monte Carlo method is proposed to circumvent the combinatorial problem with ( n k ) possible subsets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Further approximations include sampling proportional to the diagonal elements Kii or its squared version K ii, leading to additive error bounds [5, 13].", "startOffset": 144, "endOffset": 151}, {"referenceID": 12, "context": "Further approximations include sampling proportional to the diagonal elements Kii or its squared version K ii, leading to additive error bounds [5, 13].", "startOffset": 144, "endOffset": 151}, {"referenceID": 11, "context": "In [12], an algorithm is proposed that yields a k! approximation to volume sampling, worsening the approximation from (k + 1) to (k + 1)!.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Algorithm 1 DPP sampling equivalent to [18]", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "An exact sampling algorithm for DPPs was presented in [17, 18], which requires the eigen decomposition of K = \u2211n i=1 \u03bbiviv > i .", "startOffset": 54, "endOffset": 62}, {"referenceID": 17, "context": "An exact sampling algorithm for DPPs was presented in [17, 18], which requires the eigen decomposition of K = \u2211n i=1 \u03bbiviv > i .", "startOffset": 54, "endOffset": 62}, {"referenceID": 17, "context": "The update formulation differs from [18], where an orthonormal basis of the eigenvectors in V to the i-th basis vector ei \u2208 R is constructed.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "An approach for efficient sampling proposed in [18] works with the dual representation of K = Y >Y to obtain Q = Y Y >, with Q being hopefully smaller than the matrix K.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "It is noted in the literature that the Gaussian kernel corresponds to an infinite dimensional feature space [27].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "Note that the cardinality of the subset cannot be set in the original DPP sampling algorithm, which led to the introduction of k-DPPs [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "We initialize the vector D = 1n, since it was noted in [18] that the squared norm of the vectors \u2016Bi\u2016 gives rise initially to a fairly uniform distribution because no points have yet been selected.", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "We can therefore draw the analogy to multidimensional scaling (MDS) [35] with a Gaussian kernel, where MDS selects the top eigenvectors.", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "Remark: The proposed algorithm bears similarities to K-means++ [2], which replaces the initialization through uniform sampling of K-means by a new seeding algorithm.", "startOffset": 63, "endOffset": 66}, {"referenceID": 35, "context": "We compare the efficient DPP sampling result with uniform sampling [36] and K-means clustering with uniform seeding [37], which performed best in several studies including a recent one [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 36, "context": "We compare the efficient DPP sampling result with uniform sampling [36] and K-means clustering with uniform seeding [37], which performed best in several studies including a recent one [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "We compare the efficient DPP sampling result with uniform sampling [36] and K-means clustering with uniform seeding [37], which performed best in several studies including a recent one [20].", "startOffset": 185, "endOffset": 189}, {"referenceID": 25, "context": "The fish bowl dataset is a punctured sphere proposed in [26], which is sparsely sampled at the bottom and densely at the top, as shown in the supplementary material.", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "where K\u0303 is the normalized kernel for Laplacian eigenmaps and the expectation is calculated over landmark points [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 20, "context": "We work with the MNIST dataset [21], consisting of 60,000 binary images of handwritten digits for training and 10,000 for testing, with a resolution of 28\u00d7 28 pixels.", "startOffset": 31, "endOffset": 35}], "year": 2015, "abstractText": "High computational costs of manifold learning prohibit its application for large point sets. A common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the Nystr\u00f6m method. The two main challenges that arise are: (i) the landmarks selected in non-Euclidean geometries must result in a low reconstruction error, (ii) the graph constructed from sparsely sampled landmarks must approximate the manifold well. We propose the sampling of landmarks from determinantal distributions on non-Euclidean spaces. Since current determinantal sampling algorithms have the same complexity as those for manifold learning, we present an efficient approximation running in linear time. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Bhattacharyya distance improves the embedding of sparsely sampled manifolds. Our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques.", "creator": "LaTeX with hyperref package"}}}