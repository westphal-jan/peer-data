{"id": "1503.03712", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2015", "title": "On Graduated Optimization for Stochastic Non-Convex Problems", "abstract": "The professorship hilbert adopting, included known there be continues method, example after popular variational that solving strictly - symmetric problems even and received immediate raise even the been decade. Despite rather popularity, very little given unknown in position same theoretical rational component. In this paper better describe small new both - order algorithm companies on adjunct optimiza - tion and experiments its skill. We logically time parameterized elder several encourage - algebraic functions the that to theorem provably totality while a corporate maximizes. In although, know prove also the algorithm converges decided own {\\ ethniki} - corresponding consider within O (90 / \\ casita ^ six) gradient - addition consider. We to our encoding making methodology supposed the own on synchronous non - planar cryptography close festive turbidity feedback, attaining soon same coherent higher. Additionally, make discussion present setting of price - ordered optimization, taking devise from old variant beyond without algorithm given approximated started fixed of O (shelby ^ first / \\ kappa ^ 4 ).", "histories": [["v1", "Thu, 12 Mar 2015 13:39:28 GMT  (231kb,D)", "http://arxiv.org/abs/1503.03712v1", "17 pages"], ["v2", "Wed, 8 Jul 2015 05:14:22 GMT  (231kb,D)", "http://arxiv.org/abs/1503.03712v2", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["elad hazan", "kfir yehuda levy", "shai shalev-shwartz"], "accepted": true, "id": "1503.03712"}, "pdf": {"name": "1503.03712.pdf", "metadata": {"source": "CRF", "title": "On Graduated Optimization for Stochastic Non-Convex Problems", "authors": ["Elad Hazan", "Kfir Y. Levy", "Shai Shalev-Swartz"], "emails": ["ehazan@cs.princeton.edu.", "kfiryl@tx.technion.ac.il.", "shais@cs.huji.ac.il."], "sections": [{"heading": null, "text": "In this paper we describe a new first-order algorithm based on graduated optimization and analyze its performance. We characterize a parameterized family of nonconvex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an \u03b5-approximate solution within O(1/\u03b52) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of \u201czero-order optimization\u201d, and devise a a variant of our algorithm which converges at rate of O(d2/\u03b54)."}, {"heading": "1 Introduction", "text": "Non-convex optimization programs are ubiquitous in machine learning and computer vision. Of particular interest are non-convex optimization problem that arise in the training of deep neural networks Bengio (2009). Often, such problems admit a multimodal structure, and therefore, the use of convex optimization machinery may lead to a local optima.\nGraduated optimization (a.k.a. continuation), Blake and Zisserman (1987), is a methodology that attempts to overcome such numerous local optima. At first, a coarse-grained version of the problem is generated by a local smoothing operation. This coarse-grained version is easier to solve. Then, the method advances in stages by gradually refining the problem versions, using the solution of the previous stage as an initial point for the optimization in the next stage.\n\u2217Princeton University; ehazan@cs.princeton.edu. \u2020Technion; kfiryl@tx.technion.ac.il. \u2021The Hebrew University; shais@cs.huji.ac.il.\nar X\niv :1\n50 3.\n03 71\n2v 1\n[ cs\n.L G\n] 1\n2 M\nDespite its popularity, there are still many gaps concerning both theoretical and practical aspects of graduated optimization, and in particular we are not aware of a rigorous running time analysis to find a global optimum, or even conditions in which a global optimum is reached. Nor are we familiar with graudated optimization in the stochastic setting, in which only a noisy gradient or value oracle to the objective is given. Moreover, any practical application of graduated optimization requires an efficient construction of coarse-grained versions of the original function. For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) . However, in the general case, it is commonly suggested in the literature to convolve the original function with a gaussian kernel Wu (1996). Yet, this operation is prohibitively inefficient in high dimensions.\nIn this paper we take an algorithmic / analytic approach to graduated optimization and show the following.\n\u2022 We characterise a family of non-convex multimodal functions that allows convergence to a global optimum. This parametrized family we call \u03c3-nice (see Definition 4.2 ).\n\u2022 We provide a stochastic algorithm inspired by graduated optimization, that performs only gradient updates and is ensured to find an \u03b5-optimal solution of \u03c3-nice functions within O(1/\u03c32\u03b52) iterations. The algorithm doesn\u2019t require expensive convolutions and access the smoothed version of any function using random sampling. The algorithm only requires access to the objective function through a noisy gradient oracle.\n\u2022 We extend our method to the \u201czero-order optimization\u201d model (a.k.a. \u201cbandit feedback\u201d model), in which the objective is only accessible through a noisy value oracle. We devise a variant of our algorithm that is guaranteed to find an \u03b5-optimal solution within O(d2/\u03c32\u03b54) iterations.\nInterestingly, the next question is raised in Bengio (2009) which reviews recent developments in the field of deep learning: \u201cCan optimization strategies based on continuation methods deliver significantly improved training of deep architectures?\u201d\nAs an initial empirical study, we examine the task of training a NN (Neural Network) over the MNIST data set. Our experiments support the theoretical guarantees, demonstrating that graduated optimization according to the methodology proposed accelerates convergence in training the NN. Moreover, we show examples in which \u03c3-nice functions capture nonconvex structure/phenomena that exists in natural data."}, {"heading": "1.1 Related Work", "text": "Among the machine vision community, the idea of graduated optimization was known since the 80\u2019s. The term \u201cGraduated Non-Convexity\u201d (GNC) was coined by Blake and Zisserman (1987), who were the first to establish this idea explicitly. Similar attitudes in the machine vision literature appeared later in Yuille (1989); Yuille et al. (1990), and Terzopoulos (1988).\nConcepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990).\nOver the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation. These approaches, in the spirit of the continuation method, offer no guarantees on the quality of the obtained solution, and are tailored to specific applications.\nA comprehensive survey of the graduated optimization literature can be found in Mobahi and Fisher III (2015a).\nA recent work Mobahi and Fisher III (2015b) advances our theoretical understanding, by analyzing a continuation algorithm in the general setting. Yet, they offer no way to perform the smoothing efficiently, nor a way to optimize the smoothed versions; but rather assume that these are possible. Moreover, their guarantee is limited to a fixed precision that depends on the objective function and does not approach zero. In contrast, our approach can generate arbitrarily precise solutions."}, {"heading": "2 Setting", "text": "We discuss an optimization of a non-convex loss function f : K 7\u2192 R, where K \u2286 Rd is a convex set. We assume that optimization lasts for T rounds; On each round t = 1, . . . , T , we may query a point xt \u2208 K, and receive a feedback. After the last round, we choose x\u0304T \u2208 K, and our performance measure is the excess loss, defined as:\nf(x\u0304T )\u2212min x\u2208K f(x)\nIn Section 4.2 we characterize a family of non-convex multimodal functions we call \u03c3-nice. Given such a \u03c3-nice loss f , we are interested in algorithms that with a high probability ensure a \u03b5-excess loss within poly(1/\u03b5) rounds.\nWe consider two kinds of feedback:\n1. Noisy Gradient feedback: Upon querying xt we receive \u2207f(xt) + \u03bet, where {\u03be\u03c4}T\u03c4=1 are independent zero mean and bounded r.v.\u2019s.\n2. Noisy Value feedback (Bandit feedback): Upon querying xt we receive f(xt)+\u03bet, where {\u03be\u03c4}T\u03c4=1 are independent zero mean and bounded r.v.\u2019s."}, {"heading": "3 Preliminaries and Notation", "text": "Notation: During this paper we use B,S to denote the unit Euclidean ball/sphere in Rd, and also Br(x),Sr(x) as the Euclidean r-ball/sphere in Rd centered at x. For a set A \u2282 Rd\n, u \u223c A denotes a random variable distributed uniformly over A."}, {"heading": "3.1 Strong-Convexity", "text": "Recall the definition of strongly-convex functions:\nDefinition 3.1. (Strong Convexity) We say that a function F : Rn \u2192 R is \u03c3-strongly convex over the set K if for all x,y \u2208 K it holds that,\nF (y) \u2265 F (x) +\u2207F (x)>(y \u2212 x) + \u03c3 2 \u2016x\u2212 y\u20162\nLet F be a \u03c3-strongly convex over convex set K, and let x\u2217 be a point in K where F is minimized, then the following inequality is satisfied:\n\u03c3 2 \u2016x\u2212 x\u2217\u20162 \u2264 F (x)\u2212 F (x\u2217) (1)\nThis is immediate by the definition of strong convexity combined with \u2207F (x\u2217)>(x\u2212 x\u2217) \u2265 0, \u2200x \u2208 K."}, {"heading": "4 Smoothing and \u03c3-Nice functions", "text": "Constructing finer and finer approximations to the original objective function is at the heart of the continuation approach. In Section 4.1 we define the smoothed versions that we will employ. Next, in Section 4.1.1 we describe an efficient way to implicitly access the smoothed versions, which will enable us to perform optimization. Finally, in Section 4.2 we define a class of non-convex multimodal functions we denote as \u03c3-nice. As we will see in Section 7, these functions are rich enough to capture non-convex structure that exists in natural data. Additionally, these functions lend themselves to an efficient optimization, and we can ensure a convergence to \u03b5-solution within poly(1/\u03b5) iterations, as described in Sections 5,6."}, {"heading": "4.1 Smoothing", "text": "Smoothing by local averaging is formally defined next.\nDefinition 4.1. Given an L-Lipschitz function f : Rd 7\u2192 R define it\u2019s \u03b4-smooth version to be\nf\u0302\u03b4(x) = Eu\u223cB[f(x + \u03b4u)].\nThe next lemma bounds the bias between f\u0302\u03b4 and f .\nLemma 4.1. Let f\u0302\u03b4 be the \u03b4-smoothed version of f , then,\n\u2200x \u2208 Rd : |f\u0302\u03b4(x)\u2212 f(x)| \u2264 \u03b4L\nProof of Lemma 4.1.\n|f\u0302\u03b4(x)\u2212 f(x)| = |Eu\u223cB [f(x + \u03b4u)]\u2212 f(x)| \u2264 Eu\u223cB [|f(x + \u03b4u)\u2212 f(x)|] \u2264 Eu\u223cB [L\u2016\u03b4u\u2016] \u2264 L\u03b4\nin the first inequality we used Jensen\u2019s inequality, and in the last inequality we used \u2016u\u2016 \u2264 1, since u \u2208 B."}, {"heading": "4.1.1 Implicit Smoothing using Sampling", "text": "A direct way to optimize a smoothed version is by direct calculation of its gradients, nevertheless this calculation might be very costly in high dimensions. A much more efficient approach is to produce an unbiased estimate for the gradients of the smoothed version by sampling the function gradients/values. These estimates could then be used by a stochastic optimization algorithms such as SGD (Stochastic Gradient Descent). This sampling approach is outlined in Figures 1,2.\nThe following two Lemmas state that the resulting estimates are unbiased and bounded:\nLemma 4.2. Let x \u2208 Rd, \u03b4 \u2265 0, and suppose that f is L-Lipschitz, then the output of SGOG (Figure 1) is bounded by L and is an unbiased estimate for \u2207f\u0302\u03b4(x).\nProof. SGOG outputs \u2207f(x + \u03b4u) for some u \u2208 B, so the first part is immediate by the Lipschitzness of f . Now, by definition, f\u0302\u03b4(x) = Eu\u223cB[f(x + \u03b4u)], deriving both sides we get the second part of the Lemma.\nLemma 4.3. Let x \u2208 K \u2286 Rd, \u03b4 \u2265 0, and suppose that maxx |f(x)| \u2264 C, then the output of SGOV (Figure 2) is bounded by dC \u03b4 and is an unbiased estimate for \u2207f\u0302\u03b4(x).\nProof. SGOV outputs d \u03b4 f(x+\u03b4v)v for some v \u2208 S, since f is C-Bounded over K the first part of the lemma is immediate. In order to prove the second part, we can use Stokes theorem to show that if v \u223c S, then:\n\u2200x \u2208 Rd . Ev\u223cS[f(x + \u03b4v)v] = \u03b4\nd \u2207f\u0302\u03b4(x) (2)\nA proof of Equation (2) is found in Flaxman et al. (2005).\nNote that the oracles depicted in Figures 1, 2 may require sampling function values outside K, (specifically in K + \u03b4B). We assume that this is possible, and that the bounds over the function gradients/values inside K, also apply in K + \u03b4B.\nExtensions to the noisy feedback settings: Note that for ease of notation, the oracles that appear in Figures 1, 2, assume we can access exact gradients/values of f . Given that we may only access noisy and bounded gradient/value estimates of f (Sec. 2), we could use these instead of the exact ones that appear in Figures 1,2, and still produce unbiased and bounded gradient estimates for the smoothed versions of f as shown in Lemmas 4.2,4.3.\nParticularly, in the case we may only access noisy gradients of f , then SGOG (Figure 1) will return \u2207f(x + \u03b4u) + \u03be instead of \u2207f(x + \u03b4u), where \u03be is a noise term. Since we assume zero bias and bounded noise this implies that \u2207f(x + \u03b4u) + \u03be is an unbiased estimate of \u2207f\u0302\u03b4(x), bounded by L+K where K is the bound on the noise and L is the Lipschitz constant of f . We can show the same for SGOV (Figure 2), given a noisy value feedback."}, {"heading": "4.2 \u03c3-Nice Functions", "text": "Following is our main definition\nDefinition 4.2. A function f : K 7\u2192 R is said to be \u03c3-nice if the following two conditions hold:\n1. Centering property: For every \u03b4 > 0, and every x\u2217\u03b4 \u2208 arg minx\u2208K f\u0302\u03b4(x), there exists x\u2217\u03b4/2 \u2208 arg minx\u2208K f\u0302\u03b4/2(x), such that:\n\u2016x\u2217\u03b4 \u2212 x\u2217\u03b4/2\u2016 \u2264 \u03b4\n2\n2. Local strong convexity of the smoothed function: For every \u03b4 > 0 let r\u03b4 = 3\u03b4, and denote x\u2217\u03b4 = arg minx\u2208K f\u0302\u03b4(x), then over Br\u03b4(x \u2217 \u03b4), the function f\u0302\u03b4(x) is \u03c3-strongly-\nconvex.\nHence, \u03c3-nice is a combination of two properties. Both together imply that optimizing the smoothed version on a scale \u03b4 is a good start for optimizing a finer version on a scale of \u03b4/2, which is sufficient for a scheme based on graduated optimization to work as we show next. In Section 7 we show that \u03c3-nice functions arise naturally in data. An illustration of \u03c3-nice function in 1-dimension appears in Figure 3."}, {"heading": "5 Graduated Optimization with a Gradient Oracle", "text": "In this section we assume that we can access a noisy gradient oracle for f . Thus, given x \u2208 Rd, \u03b4 \u2265 0 we can use SGOG (Figure 1) to obtain an unbiased and bounded estimate for \u2207f\u0302\u03b4(x), as ensured by Lemma 4.2. Note that for ease of notation SGOG (Figure 1) is listed using an exact gradient oracle for f . As described at the end of Section 4.1.1, this could be replaced with a noisy gradient oracle for f , and Lemma 4.2, will still hold.\nFollowing is our main Theorem:\nTheorem 5.1. Let \u03b5 \u2208 (0, 1) and p \u2208 (0, 1/e), also let K be a convex set, and f be an LLipschitz \u03c3-nice function. Suppose that we apply Algorithm 1, then after O\u0303(1/\u03c32\u03b52) rounds Algorithm 1 outputs a point x\u0304M+1 which is \u03b5 optimal with a probability greater than 1\u2212 p.\nAlgorithm 1 is divided into epochs, at epoch m it uses SGOG to obtain unbiased estimates for the gradients of f\u0302\u03b4m which are then employed by Suffix-SGD (Algorithm 2), to optimize this smoothed version. This optimization over f\u0302\u03b4m is performed until we are ensured to reach a point close enough to x\u2217m+1 := arg minx\u2208K f\u0302\u03b4m+1(x), i.e., the minimum of f\u0302\u03b4m+1 . Also note that at epoch m the optimization over f\u0302\u03b4m is initialized at x\u0304m which is the point reached at the previous epoch.\nSuffix-SGD (Algorithm 2), is a stochastic optimization algorithm for strongly convex functions. Its guarantees are presented in Section 5.1."}, {"heading": "5.1 Analysis", "text": "Let us first discuss Suffix-SGD (Algorithm 2). This algorithm performs projected gradient descent using the gradients received from GradOracle(\u00b7). The projection operator \u03a0K, is defined \u2200y \u2208 Rd as\n\u03a0K(y) := arg min x\u2208K\n\u2016x\u2212 y\u2016 .\nAlgorithm 1 GradOptG Input: target error \u03b5, maximal failure probability p, decision set K Choose x\u03041 \u2208 K uniformly at random. Set \u03b41 = diam(K)/2, p\u0303 = p/M , and M = log2 1\u03b10\u03b5 where \u03b10 = min{ 1 2Ldiam(K) , 2 \u221a 2\u221a\n\u03c3diam(K)} for m = 1 to M do\n// Perform SGD over f\u0302\u03b4m Set \u03b5m := \u03c3\u03b4 2 m/32, and\nTF = 12480L2 \u03c3\u03b5m log (2 p\u0303 + 2 log 12480L2 \u03c3\u03b5m ) Set shrinked decision set,\nKm := K \u2229B(x\u0304m, 1.5\u03b4m)\nSet gradient oracle for f\u0302\u03b4m ,\nGradOracle(\u00b7) = SGOG(\u00b7, \u03b4m)\nUpdate: x\u0304m+1 \u2190 Suffix-SGD(TF ,Km, x\u0304m,GradOracle)\n\u03b4m+1 = \u03b4m/2 end for Return: x\u0304M+1\nAlgorithm 2 Suffix-SGD\nInput: total time TF , decision set K, initial point x1 \u2208 K, gradient oracle GradOracle(\u00b7) for t = 1 to TF do\nSet \u03b7t = 1/\u03c3t Query the gradient oracle at xt:\ngt \u2190 GradOracle(xt)\nUpdate: xt+1 \u2190 \u03a0K(xt \u2212 \u03b7tgt) end for Return: x\u0304TF := 2 TF ( xTF /2+1 + . . .+ xTF ) Now consider a \u03c3-strongly convex function F : K \u2192 R, and suppose that we have an oracle, GradOracle(\u00b7), that upon querying a point x \u2208 K returns an unbiased and bounded\ngradient estimate, g, i.e., E[g] = \u2207F (x), and \u2016g\u2016 \u2264 G. Note the following result from Rakhlin et al. (2011) regarding stochastic optimization of \u03c3-strongly-convex functions, given such an oracle:\nTheorem 5.2. Let p \u2208 (0, 1/e), and F be a \u03c3-strongly convex function. Suppose that GradOracle(\u00b7) produces G-bounded, and unbiased estimates of \u2207F . Then after no more than TF rounds, the final point x\u0304TF returned by Suffix-SGD (Algorithm 2 ) ensures that with a probability \u2265 1\u2212 p, we have:\nF (x\u0304TF )\u2212min x\u2208K\nF (x) \u2264 6240 log\n( 2 log(TF )/p ) G2\n\u03c3TF\nCorollary 5.1. The latter means that for TF \u2265 12480G 2 \u03c3\u03b5 log ( 2/p + 2 log(12480G2/\u03c3\u03b5) ) we will have an excess loss smaller than \u03b5.\nNotice that at each epoch m of GradOptG, it initiates Suffix-SGD with a gradient oracle SGOG(\u00b7, \u03b4m). According to Lemma 4.2, SGOG(\u00b7, \u03b4m) produces an unbiased an L-bounded estimates of f\u0302\u03b4m , thus in the analysis of each epoch we can use Theorem 5.2 for f\u0302\u03b4m , taking G = L.\nFollowing is our key Lemma:\nLemma 5.1. Consider M , Km and x\u0304m+1 as defined in Algorithm 1. Also denote by x\u2217m the minimizer of f\u0302\u03b4m in K. Then the following holds for all 1 \u2264 m \u2264M w.p.\u2265 1\u2212 p:\n1. The smoothed version f\u0302\u03b4mis \u03c3-strongly convex over Km, and x\u2217m \u2208 Km.\n2. Also, f\u0302\u03b4m(x\u0304m+1)\u2212 f\u0302\u03b4m(x\u2217m) \u2264 \u03c3\u03b42m+1/8\nProof. We will prove by induction, let us prove it holds form = 1. Note that \u03b41 = diam(K)/2, therefore K1 = K, and also x\u22171 \u2208 K1. Also recall that \u03c3-niceness of f implies that f\u0302\u03b41 is \u03c3strongly convex in K, thus by Corollary 5.1, after less than TF = O\u0303( 12480L 2\n\u03c3(\u03c3\u03b421/32) ) optimization\nsteps of Suffix-SGD with a probability greater than 1\u2212 p/M , we will have:\nf\u0302\u03b41(x\u03042)\u2212 f\u0302\u03b41(x\u22171) \u2264 \u03c3\u03b421/32 = \u03c3\u03b422/8\nwhich establishes the case of m = 1. Now assume that lemma holds for m > 1. By this assumption, f\u0302\u03b4m(x\u0304m+1) \u2212 f\u0302\u03b4m(x\u2217m) \u2264 \u03c3\u03b42m+1/8, f\u0302\u03b4m is \u03c3-strongly convex in Km, and also x\u2217m \u2208 Km. Hence, we can use Equation (1) to get:\n\u2016x\u0304m+1 \u2212 x\u2217m\u2016 \u2264 \u221a 2\n\u03c3\n\u221a f\u0302\u03b4m(x\u0304m+1)\u2212 f\u0302\u03b4m(x\u2217m) =\n\u03b4m+1 2\nCombining the latter with the centering property of \u03c3-niceness yields:\n\u2016x\u0304m+1 \u2212 x\u2217m+1\u2016 \u2264 \u2016x\u0304m+1 \u2212 x\u2217m\u2016+ \u2016x\u2217m \u2212 x\u2217m+1\u2016 \u2264 1.5\u03b4m+1\nand it follows that, x\u2217m+1 \u2208 B(x\u0304m+1, 1.5\u03b4m+1) \u2282 B(x\u2217m+1, 3\u03b4m+1)\nRecalling that Km+1 := B(x\u0304m+1, 1.5\u03b4m+1), and the local strong convexity property of f (which is \u03c3-nice), then the induction step for first part of the lemma holds. Now, by Corollary 5.1, after less than TF = O\u0303( 12480L 2\n\u03c3(\u03c3\u03b42m+1/32) ) optimization steps of Suffix-SGD over f\u0302\u03b4m+1 , we\nwill have: f\u0302\u03b4m+1(x\u0304m+2)\u2212 f\u0302\u03b4m+1(x\u2217m+1) \u2264 \u03c3\u03b42m+2/8\nwhich establishes the induction step for the second part of the lemma. An analysis of fail probability: since we have M epochs in total and at each epoch the fail probability is smaller than p/M , then the total fail probability of our algorithm is smaller than p.\nWe are now ready to prove Theorem 5.1:\nproof of Theorem 5.1. Algorithm 1 terminates after M = log2 1 \u03b10\u03b5 epochs meaning, \u03b4M = diam(K)\u03b10\u03b5/2. According to Lemma 5.1 the following holds w.p.\u2265 1\u2212 p ,\nf\u0302\u03b4M (x\u0304M+1)\u2212 f\u0302\u03b4M (x\u2217M) \u2264 \u03c3\u03b42M+1/8\n=\n(\u221a \u03c3diam(K)\u03b10\u03b5\n8 \u221a 2 )2 Due to Lemma 4.1, f\u0302\u03b4M is L\u03b4M biased from f , thus:\nf(x\u0304M+1)\u2212 f(x) \u2264 Ldiam(K)\u03b10\u03b5+ (\u221a\n\u03c3diam(K)\u03b10\u03b5 8 \u221a 2 )2 \u2264 \u03b5\nwe used \u03b10 = min{ 12Ldiam(K) , 2 \u221a 2\u221a\n\u03c3diam(K)}, and \u03b5 < 1. Let Ttotal, be the total number of queries made by by Algorithm 1, then we have:\nTtotal \u2264 M\u2211 m=1 12480L2 \u03c3\u03b5m log \u0393\n\u2264 M\u2211 m=1 12480L2 \u03c3(\u03c3\u03b42m/32) log \u0393\n\u2264 4 \u00b7 10 5L2 log \u0393\n\u03c32\nM\u2211 i=1 4i\u22121 \u03b421\n\u2264 14 \u00b7 10 4L2 log \u0393 \u03c32 4M\n\u03b421\n\u2264 14 \u00b7 10 4L2 log \u0393 \u03c32 max{16L2, \u03c3/2} 1 \u03b52\nhere we used the notation:\n\u0393 := 2M\np + 2 log(12480L2/\u03c3\u03b5M)\n\u2264 2M p + 2 log(4 \u00b7 105L2 max{16L2, \u03c3 2 }/\u03c32\u03b52)"}, {"heading": "6 Graduated Optimization with a Value Oracle", "text": "In this section we assume that we can access a noisy value oracle for f . Thus, given x \u2208 Rd, \u03b4 \u2265 0 we can use SGOV (Figure 2) as an oracle that outputs an unbiased and bounded estimates for \u2207f\u0302\u03b4(x), as ensured by Lemma 4.3. Note that for ease of notation SGOV (Figure 2) is listed using an exact value oracle for f . As described at the end of Section 4.1.1, this could be replaced with a noisy value oracle for f , and Lemma 4.3, will still hold.\nFollowing is our main Theorem:\nTheorem 6.1. Let \u03b5 > 0 and p \u2208 (0, 1/e), also let K be a convex set, and f be an L-Lipschitz \u03c3-nice function. Assume also that maxx |f(x)| \u2264 C. Suppose that we apply Algorithm 3, then after after O\u0303(d2/\u03c32\u03b54) rounds Algorithm 3 outputs a point x\u0304M+1 which is \u03b5 optimal with a probability greater than 1\u2212 p.\nAn SGD algorithm for \u03b5-approximating \u03c3-nice functions is listed in Algorithm 1."}, {"heading": "6.1 Analysis", "text": "Notice that at each epoch m of GradOptV , it initiates Suffix-SGD with a gradient oracle SGOV (\u00b7, \u03b4m). According to Lemma 4.3, SGOV (\u00b7, \u03b4m) produces an unbiased and dC/\u03b4mbounded estimates for the gradients of f\u0302\u03b4m , thus in the analysis of each epoch we can use Corollary 5.1 for f\u0302\u03b4m , taking G = dC/\u03b4m.\nFollowing is our key Lemma:\nLemma 6.1. Consider M , Km and x\u0304m+1 as defined in Algorithm 3. Also denote by x\u2217m the minimizer of f\u0302\u03b4m in K. Then the following holds for all 1 \u2264 m \u2264M w.p.\u2265 1\u2212 p:\n1. The smoothed version f\u0302\u03b4mis \u03c3-strongly convex over Km, and x\u2217m \u2208 Km.\n2. Also, f\u0302\u03b4m(x\u0304m+1)\u2212 f\u0302\u03b4m(x\u2217m) \u2264 \u03c3\u03b42m+1/8\nThe proof of Lemma 6.1 is similar to the proof of Lemma 5.1 given in Section 5.1, we therefore omit the details.\nWe are now ready to prove Theorem 6.1:\nAlgorithm 3 GradOptV Input: target error \u03b5, maximal failure probability p, decision set K Choose x\u03041 \u2208 K uniformly at random. Set \u03b41 = diam(K)/2, p\u0303 = p/M , and M = log2 1\u03b10\u03b5 where \u03b10 = min{ 1 2Ldiam(K) , 2 \u221a 2\u221a\n\u03c3diam(K)} for m = 1 to M do\n// Perform SGD over f\u0302\u03b4m Set \u03b5m := \u03c3\u03b4 2 m/32, and\nTF = 12480\n\u03c3\u03b5m\nd2C2 \u03b42m log (2 p\u0303 + 2 log 12480d2C2 \u03c3\u03b5m\u03b42m ) Set shrinked decision set,\nKm := K \u2229B(x\u0304m, 1.5\u03b4m)\nSet gradient oracle for f\u0302\u03b4m ,\nGradOracle(\u00b7) = SGOV (\u00b7, \u03b4m)\nUpdate: x\u0304m+1 \u2190 Suffix-SGD(TF ,Km, x\u0304m,GradOracle)\n\u03b4m+1 = \u03b4m/2 end for Return: x\u0304M+1\nproof of Theorem 6.1. Let x\u0304M+1 be the output of Algorithm 3. Similarly to the proof of Theorem 5.1, we can show that:\nf(x\u0304M+1)\u2212 f(x) \u2264 \u03b5\nLet Ttotal, be the total number of queries made by by Algorithm 3, then we have:\nTtotal \u2264 M\u2211 m=1 12480d2C2 \u03c3\u03b5m\u03b42m log \u0393\n\u2264 M\u2211 m=1 12480d2C2 \u03c3(\u03c3\u03b42m/32)\u03b4 2 m log \u0393\n\u2264 4 \u00b7 10 5d2C2 log \u0393\n\u03c32\nM\u2211 i=1 8i\u22121 \u03b441\n\u2264 6 \u00b7 10 4d2C2 log \u0393 \u03c32 8M\n\u03b441\n\u2264 6 \u00b7 10 4d2C2 log \u0393 \u03c32 max{256L4, \u03c32/4} 1 \u03b54\nhere we used the notation:\n\u0393 := 2M\np + 2 log(12480d2C2/\u03c3\u03b5M\u03b4 2 M)\n\u2264 2M p + 2 log(4 \u00b7 105d2C2 max{256L4, \u03c3 2 4 }/\u03c32\u03b54)"}, {"heading": "7 Experiments", "text": "In the last two decades, performing complex learning tasks using Neural-Network (NN) architectures has become an active and promising line of research. Since learning NN architectures essentially requires to solve a hard non-convex program, we have decided to focus our empirical study on this type of tasks. As a test case, we train a NN with a single hidden layer of 30 units over the MNIST data set. We adopt the experimental setup of Dauphin et al. (2014) and train over a down-scaled version of the data, i.e., the original 28\u00d728 images of MNIST were down-sampled to the size of 10 \u00d7 10. We use a ReLU activation function, and minimize the square loss."}, {"heading": "7.1 Smoothing the NN", "text": "First, we were interested in exploring the non-convex structure of the above NN learning task, and check whether our definition of \u03c3-nice complies with this structure. We started by running MSGD (Minibatch Stochastic Gradient Descent) on the problem, while using a batch size of 100, and a step size rule of the form \u03b7t = \u03b70(1 + \u03b3t) \u22123/4, where \u03b70 = 0.01, \u03b3 = 10 \u22124. This choice of step size rule was the most effective among a grid of rules that we examined. We have found out that MSGD frequently \u201cstalls\u201d in areas with a relatively high loss, here we relate to points at the end of such run as stall-points.\nIn order to learn about the non-convex nature of the problem, we examined the objective values along two directions around stall-points. The first direction was the gradient at the stall point, and the second direction was the line connecting the stall-point to x\u2217, where x\u2217 is the best weights configuration of the NN that we were able to find. A drawing depicting typical results appears on the left side of Figure 4. The stall-point appears in red, and x\u2217 in green; also the axis marked as X is the gradient direction, and one marked Y is the direction between stall-point and x\u2217. Note that the stall-point is inside a narrow \u201cvalley\u201d, which prevents MSGD from \u201cseeing\u201d x\u2217, and so it seems that MSGD slowly progresses downstream. Interestingly, the objective around x\u2217 seems strongly-convex in the direction of the stall point.\nOn the middle of Figure 4, we present the \u03b4 = 3 smoothed version of the same objective that appears on the left side of Figure 4. We can see that the \u201cvalley\u201d has not vanished, but the gradient of the smoothed version leads us slightly towards x\u2217 and out of the original \u201cvalley\u201d. On the right side of Figure 4, we present the \u03b4 = 7 smoothed version of the objective. We can see that due to the coarse smoothing, the \u201cvalley\u201d in which MSGD was stalled, has completely dissolved, and the gradient of this version leads us towards x\u2217."}, {"heading": "7.2 Graduated Optimization of NN", "text": "Here we present experiments that demonstrate the effectiveness of GradOptG (Algorithm 1) in training the NN mentioned above. First, we wanted to learn if smoothing can help us escape points where MSGD stalls. We used MSGD (\u03b4 = 0) to train the NN, and as before we found that its progress slows down, yielding relatively high error. We then took the point that MSGD reached after 5 \u00b7 104 iteration and initialized an optimization over the smoothed versions of the loss; this was done using smoothing values of {1, 3, 5, 7}. In Figure 5 we present the results of the above experiment.\nAs seen in Figure 5, small \u03b4\u2019s converge slower than large \u03b4\u2019s, but produce a much better solution. Furthermore, the initial optimization progresses in leaps, for large \u03b4\u2019s the leaps are sharper, and lower \u03b4\u2019s demonstrate smaller leaps. We believe that these leaps are associated with the advance of the optimization from one local \u201cvalley\u201d to another.\nIn Figure 6 we compare our complete graduated optimization algorithm, namely GradOptG (Alg. 1) to MSGD. We started with an initial smoothing of \u03b4 = 7, which decayed according to GradOptG. Note that GradOptG progresses very fast and yields a much better solution than MSGD."}, {"heading": "8 Discussion", "text": "We have described a family of non-convex functions which admit efficient optimization via the graduated optimization methodology, and gave the first rigorous analysis of a first-order algorithm in the stochastic setting.\nWe view it as only a first glimpse of the potential of graduated optimization to provable non-convex optimization, and amongst the interesting questions that remain we find\n\u2022 Is \u03c3-niceness necessary for convergence of first-order methods to a global optimum? Is there a more lenient property that better captures the power of graduated optimization?\n\u2022 Amongst the two properties of \u03c3-niceness, can their parameters be relaxed in terms of the ratio of smoothing to strong-convexity, or to centering?\n\u2022 Can second-order/other methods give rise to better convergence rates / faster algorithms for stochastic or offline \u03c3-nice non-convex optimization?"}], "references": [{"title": "Numerical continuation methods, volume 13", "author": ["E.L. Allgower", "K. Georg"], "venue": null, "citeRegEx": "Allgower and Georg.,? \\Q1990\\E", "shortCiteRegEx": "Allgower and Georg.", "year": 1990}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "A gnc algorithm for deblurring images with interacting discontinuities", "author": ["A. Boccuto", "M. Discepoli", "I. Gerace", "R. Pandolfi", "P. Pucci"], "venue": "Proc. VI SIMAI,", "citeRegEx": "Boccuto et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Boccuto et al\\.", "year": 2002}, {"title": "Large displacement optical flow: descriptor matching in variational motion estimation", "author": ["T. Brox", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Brox and Malik.,? \\Q2011\\E", "shortCiteRegEx": "Brox and Malik.", "year": 2011}, {"title": "Gradient descent optimization of smoothed information retrieval metrics", "author": ["O. Chapelle", "M. Wu"], "venue": "Information retrieval,", "citeRegEx": "Chapelle and Wu.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle and Wu.", "year": 2010}, {"title": "A continuation method for semi-supervised svms", "author": ["O. Chapelle", "M. Chi", "A. Zien"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Smoothing a program soundly and robustly", "author": ["S. Chaudhuri", "A. Solar-Lezama"], "venue": "In Computer Aided Verification,", "citeRegEx": "Chaudhuri and Solar.Lezama.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri and Solar.Lezama.", "year": 2011}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["D. Erhan", "P.-A. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In SODA,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "On the link between gaussian homotopy continuation and convex envelopes", "author": ["H. Mobahi", "J.W. Fisher III"], "venue": "In Energy Minimization Methods in Computer Vision and Pattern Recognition,", "citeRegEx": "Mobahi and III.,? \\Q2015\\E", "shortCiteRegEx": "Mobahi and III.", "year": 2015}, {"title": "A theoretical analysis of optimization by gaussian continuation", "author": ["H. Mobahi", "J.W. Fisher III"], "venue": null, "citeRegEx": "Mobahi and III.,? \\Q2015\\E", "shortCiteRegEx": "Mobahi and III.", "year": 2015}, {"title": "Fast nonconvex nonsmooth minimization methods for image restoration and reconstruction", "author": ["M. Nikolova", "M.K. Ng", "C.-P. Tam"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Nikolova et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nikolova et al\\.", "year": 2010}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "arXiv preprint arXiv:1109.5647,", "citeRegEx": "Rakhlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2011}, {"title": "The computation of visible-surface representations", "author": ["D. Terzopoulos"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Terzopoulos.,? \\Q1988\\E", "shortCiteRegEx": "Terzopoulos.", "year": 1988}, {"title": "The effective energy transformation scheme as a special continuation approach to global optimization with application to molecular conformation", "author": ["Z. Wu"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Wu.,? \\Q1996\\E", "shortCiteRegEx": "Wu.", "year": 1996}, {"title": "Energy functions for early vision and analog networks", "author": ["A. Yuille"], "venue": "Biological Cybernetics,", "citeRegEx": "Yuille.,? \\Q1989\\E", "shortCiteRegEx": "Yuille.", "year": 1989}, {"title": "Stereo integration, mean field theory and psychophysics", "author": ["A.L. Yuille", "D. Geiger", "H. B\u00fclthoff"], "venue": "In Computer Vision ECCV", "citeRegEx": "Yuille et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Yuille et al\\.", "year": 1990}, {"title": "A path following algorithm for the graph matching problem", "author": ["M. Zaslavskiy", "F. Bach", "J.-P. Vert"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Zaslavskiy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zaslavskiy et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Of particular interest are non-convex optimization problem that arise in the training of deep neural networks Bengio (2009). Often, such problems admit a multimodal structure, and therefore, the use of convex optimization machinery may lead to a local optima.", "startOffset": 110, "endOffset": 124}, {"referenceID": 1, "context": "Of particular interest are non-convex optimization problem that arise in the training of deep neural networks Bengio (2009). Often, such problems admit a multimodal structure, and therefore, the use of convex optimization machinery may lead to a local optima. Graduated optimization (a.k.a. continuation), Blake and Zisserman (1987), is a methodology that attempts to overcome such numerous local optima.", "startOffset": 110, "endOffset": 333}, {"referenceID": 5, "context": "For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) .", "startOffset": 66, "endOffset": 89}, {"referenceID": 5, "context": "For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) .", "startOffset": 66, "endOffset": 124}, {"referenceID": 5, "context": "For some special cases this construction can be made analytically Chapelle et al. (2006); Chaudhuri and Solar-Lezama (2011) . However, in the general case, it is commonly suggested in the literature to convolve the original function with a gaussian kernel Wu (1996). Yet, this operation is prohibitively inefficient in high dimensions.", "startOffset": 66, "endOffset": 266}, {"referenceID": 1, "context": "Interestingly, the next question is raised in Bengio (2009) which reviews recent developments in the field of deep learning: \u201cCan optimization strategies based on continuation methods deliver significantly improved training of deep architectures?\u201d As an initial empirical study, we examine the task of training a NN (Neural Network) over the MNIST data set.", "startOffset": 46, "endOffset": 60}, {"referenceID": 16, "context": "Similar attitudes in the machine vision literature appeared later in Yuille (1989); Yuille et al.", "startOffset": 69, "endOffset": 83}, {"referenceID": 16, "context": "Similar attitudes in the machine vision literature appeared later in Yuille (1989); Yuille et al. (1990), and Terzopoulos (1988).", "startOffset": 69, "endOffset": 105}, {"referenceID": 15, "context": "(1990), and Terzopoulos (1988).", "startOffset": 12, "endOffset": 31}, {"referenceID": 7, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990).", "startOffset": 68, "endOffset": 78}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al.", "startOffset": 118, "endOffset": 144}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al.", "startOffset": 118, "endOffset": 301}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011).", "startOffset": 118, "endOffset": 344}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al.", "startOffset": 118, "endOffset": 384}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al.", "startOffset": 118, "endOffset": 549}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010).", "startOffset": 118, "endOffset": 590}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al.", "startOffset": 118, "endOffset": 626}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al.", "startOffset": 118, "endOffset": 644}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al.", "startOffset": 118, "endOffset": 741}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation.", "startOffset": 118, "endOffset": 762}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation. These approaches, in the spirit of the continuation method, offer no guarantees on the quality of the obtained solution, and are tailored to specific applications. A comprehensive survey of the graduated optimization literature can be found in Mobahi and Fisher III (2015a). A recent work Mobahi and Fisher III (2015b) advances our theoretical understanding, by analyzing a continuation algorithm in the general setting.", "startOffset": 118, "endOffset": 1063}, {"referenceID": 0, "context": "Concepts of the same nature appeared in the optimization literature Wu (1996), and in the field of numerical analysis Allgower and Georg (1990). Over the last two decades, this concept was successfully applied to numerous problems in computer vision; among are: image deblurring Boccuto et al. (2002) , image restoration Nikolova et al. (2010), and optical flow Brox and Malik (2011). The method was also adopted by the machine learning community, demonstrating effective performance in tasks such as semi-supervised learning Chapelle et al. (2006), graph matching Zaslavskiy et al. (2009), and ranking Chapelle and Wu (2010). In Bengio (2009), it is suggested to consider some developments in deep belief architectures Hinton et al. (2006); Erhan et al. (2009) as a kind of continuation. These approaches, in the spirit of the continuation method, offer no guarantees on the quality of the obtained solution, and are tailored to specific applications. A comprehensive survey of the graduated optimization literature can be found in Mobahi and Fisher III (2015a). A recent work Mobahi and Fisher III (2015b) advances our theoretical understanding, by analyzing a continuation algorithm in the general setting.", "startOffset": 118, "endOffset": 1108}, {"referenceID": 9, "context": "A proof of Equation (2) is found in Flaxman et al. (2005).", "startOffset": 36, "endOffset": 58}, {"referenceID": 14, "context": "Note the following result from Rakhlin et al. (2011) regarding stochastic optimization of \u03c3-strongly-convex functions, given such an oracle: Theorem 5.", "startOffset": 31, "endOffset": 53}, {"referenceID": 7, "context": "We adopt the experimental setup of Dauphin et al. (2014) and train over a down-scaled version of the data, i.", "startOffset": 35, "endOffset": 57}], "year": 2017, "abstractText": "The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade. Despite its popularity, very little is known in terms of theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimization and analyze its performance. We characterize a parameterized family of nonconvex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an \u03b5-approximate solution within O(1/\u03b52) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of \u201czero-order optimization\u201d, and devise a a variant of our algorithm which converges at rate of O(d2/\u03b54).", "creator": "LaTeX with hyperref package"}}}