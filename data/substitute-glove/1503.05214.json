{"id": "1503.05214", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "Analysis of PCA Algorithms in Distributed Environments", "abstract": "Web sites, changes directly, computerized, making scientific scientific separate enabled efforts required of application. Owners present this maps achieve to checks insights went turn, similarly as necessary guns working algorithms. Many hard lesson regression, as, give might unprecedented well turn survive only of unfortunately increasing volumes of data. To agenda means unfortunately, go identify carried optimizations that each determined brought trajectory amongst or classes processes through addition virtual. We seek be formalisms if but popular Principal Component Analysis (PCA) algorithm. PCA much another work tool following least areas some image industrial, computer retrieval, information input/output, , dimensionality reduction. We \" decided full proposed calibration PCA arithmetic also integrates PCA, well sPCA. sPCA achieves driveability network 1,500 efficient covered matrix based, otherwise leveraging matrix re-activation, and burdens intermediate analyzed. We requirements sPCA from the frequently - typically MapReduce frame, on given particular - based Spark sharing. We compare sPCA against same hold PCA implementations, also such the though three Mahout / MapReduce them MLlib / Spark. Our prototypes show that sPCA swankier both Mahout - PCA and MLlib - PCA eventually wide deficit also full as breadth, on probably, and short of intermediate inventory generated part place evolutionary.", "histories": [["v1", "Tue, 17 Mar 2015 20:38:15 GMT  (3393kb)", "http://arxiv.org/abs/1503.05214v1", null], ["v2", "Wed, 13 May 2015 12:05:02 GMT  (748kb)", "http://arxiv.org/abs/1503.05214v2", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.NA", "authors": ["tarek elgamal", "mohamed hefeeda"], "accepted": false, "id": "1503.05214"}, "pdf": {"name": "1503.05214.pdf", "metadata": {"source": "CRF", "title": "sPCA: Scalable Principal Component Analysis for Big Data on Distributed Platforms", "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n05 21\n4v 1\n[ cs\n.D C\n] 1\n7 M\nar 2\n01 5"}, {"heading": "1. INTRODUCTION", "text": "Internet-scale web services collect terabytes of data from their users\u2019 activities such as clicks, visits, likes, and ratings. This data offers opportunities for extracting valuable insights about users and their interests which can enable service providers to improve their services and attract more customers. Making sense of tera-scale data is, however, a challenging task, because many current machine learning algorithms were designed for centralized computing systems where the entire dataset can fit in the memory of one computing node. This highlights the need for designing distributed machine learning algorithms that can process large volumes of data.\nDistributed machine learning algorithms, however, introduce a new set of challenges. For example, most machine learning algorithms involve quite complex and inter-dependent computations, and dividing these computations among multiple computing nodes\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nwhile preserving the accuracy and theoretical guarantees is a nontrivial task. More importantly, this division of load introduces a new problem, namely that is partial results and intermediate data may need to be exchanged among computing nodes. If not carefully managed, this intermediate data may actually become the main bottleneck for scaling machine learning algorithms, regardless of the available number of computing nodes. This is in addition to the other common challenges in all distributed settings, such as scheduling tasks, handling failures, and balancing load.\nIn this paper, we take Principal Component Analysis (PCA) [?] as an important, complex machine learning algorithm to show several techniques that can be applied to address the challenges of big data analysis on distributed systems. PCA is a popular machine learning tool in many areas, including image processing [?, ?], data visualization [?], compression [?], and information retrieval [?]. Moreover, since PCA reduces the dimensionality of the data, it is a key step in many other machine learning algorithms that do not perform well with high-dimensional data such as k-means clustering [?]. We start by conducting a thorough analysis of existing PCA algorithms and their scalability in distributed settings. Then, we design our distributed PCA algorithm starting from one of the current PCA algorithms that promises the best theoretical scalability. We propose a set of simple, but highly effective, optimizations that achieve substantial performance gains for the proposed distributed PCA algorithm. Most of these optimizations are applicable to other machine learning algorithms as they target primitive matrix operations commonly used in machine learning algorithms, such as matrix multiplication, matrix centralization, and computing matrix norms.\nAlthough traditional libraries such as ScaLAPACK [?] offer implementations for PCA and various other machine learning algorithms, they are targeted towards high-end HPC platforms. In contrast, although our proposed optimizations can also benefit HPC machine learning libraries, we focus on designing a scalable PCA algorithm for commodity distributed clusters that are available to almost all academic and industrial organizations. In addition, we consider recent distributed programming platforms that run on such clusters, such as MapReduce [?] and Spark [?]. These programming platforms offer many advantages over traditional ones such as MPI [?], including transparent handling of failures, load balancing, and task scheduling, which greatly facilitate the development of distributed code. There are currently multiple libraries that offer PCA for distributed clusters. Two quite popular examples are Mahout [?] on MapReduce and MLlib [?] and Spark. Our experiments, however, show that the PCA algorithms in these two libraries does not scale well to support big data analysis. For example, the implementation of PCA in Mahout finished processing a 1 GB dataset in less than an hour on an 8-node cluster, where each\nnode has 8 cores. When we applied the same algorithm on a dataset of 94 GB, we had to wait for five days for the algorithm to finish; our algorithm in contrast finished in less than five hours. The situation is not much better for PCA in MLlib, which even failed to process datasets with more than 6,000 dimensions.\nThis paper addresses the challenges of PCA for large-scale data, and makes the following contributions:\n\u2022 Analysis of different methods for performing PCA and their limitations in handling large-scale datasets on distributed clusters. To the best of our knowledge, such a rigorous analysis was never done before, and it is crucial for selecting the proper PCA method for different environments and datasets.\n\u2022 Design and implementation of an efficient PCA algorithm called scalable PCA, or sPCA, for large datasets on distributed commodity clusters. The design is general and can be implemented on different platforms. We implemented sPCA on the disk-based MapReduce and the memory-based Spark programming platforms.\n\u2022 Extensive empirical study using large and diverse datasets to assess the performance of sPCA and compare it against other distributed PCA implementations. Our results show that sPCA can be several orders of magnitude faster than two solid and widely used state-of-the-art competitors: MahoutPCA on MapReduce and MLlib-PCA on Spark. In addition, sPCA has better scalability and accuracy than these competitors. An important property of sPCA is that it generates a very small amount of intermediate data. This is useful for MapReduce since it means that sPCA has a low disk footprint, resulting in less disk and network I/O. For Spark, this property not only decreases network I/O, but also allows analysis of much larger datasets in the limited aggregate memory of the cluster. For example, on a dataset of more than 1.26 billion tweets from Twitter, sPCA generates 131 MB of intermediate data whereas Mahout-PCA generates 961 GB of intermediate data.\nThe rest of this paper is organized as follows. In Section 2, we present our analysis of different PCA methods in the literature. In Section 3, we present the proposed design of sPCA. We present our MapReduce and Spark implementations in Section 4. Section 5 presents our experimental evaluation. Section 6 summarizes the related work, and Section 7 concludes the paper."}, {"heading": "2. ANALYSIS OF PCA ALGORITHMS", "text": "In this section, we analyze different methods for computing the principal components of a given dataset represented as a matrix. Although several PCA algorithms exist and are well known in the literature, to the best of our knowledge, they have never been analyzed and compared in a systematic manner, especially in the context of large-scale datasets and distributed processing environments. Due to space limitations, we only present the summary of our analysis. Detailed step-by-step derivations are presented in the companion technical report [?].\nDistributed Execution Cost Model. We analyze all methods across two important metrics: time complexity and communication complexity. We consider the worst-case scenarios for both metrics. The time complexity is the upper bound on the number of computational steps needed by the algorithm to terminate. Some PCA algorithms run multiple iterations of the same code, where each iteration improves the accuracy of its predecessor by starting from a better initial state. The time complexity that we present is for\na single iteration, as the number of iterations is typically a small constant.\nDuring the distributed execution of a PCA algorithm, processing nodes may need to exchange data among each other, which we call intermediate data. The worst-case total size of the intermediate data is considered as the communication complexity. We note that most PCA algorithms work in multiple synchronous phases, and the intermediate data is exchanged at the end of each phase. That is, a phase must wait for the entire intermediate data produced by its predecessor to be received before its execution starts. Therefore, a large amount of intermediate data will introduce delays and enlarge the total execution time of the PCA algorithm, and hence the intermediate data can become a major bottleneck. The exact delay will depend on the cluster hardware (network topology, link speed, I/O speed, etc.) as well as the software platform used to manage the cluster and run the PCA code. Some software platforms, e.g., Hadoop/MapReduce, exchange intermediate data through the distributed storage system, while others, e.g., Spark, exchange data through a shared virtual memory. For our analysis of communication complexity to be general, we consider the total number of bytes that need to be exchanged, and we abstract away the details of the underlying hardware/software architecture.\nIn addition, during our analysis, we identify the methods implemented in common libraries such as Mahout, MLlib, and ScaLAPACK. Mahout [?] is a collection of machine learning algorithms implemented on top of Hadoop MapReduce. MLlib [?] is a Spark implementation of some common machine learning algorithms. ScaLAPACK [?] is a library of linear algebra algorithms implemented for parallel distributed memory machines.\nThe notation we use in this paper is mostly consistent with the Matlab\u2019s programing language. Variable names, including matrices, are composed of one or more letters. Multiplication is indicated with a star (\u2217) between the variables. M\u2032 and M\u22121 are the transpose and inverse of matrix M, respectively. I is the identity matrix and ||M||2F = N \u2211\ni=1\nD \u2211\nj=1 (M ji ) 2 is the square of the Frobenius\nnorm of the matrix M. Furthermore, Mi denotes row i of matrix M. We use M ji to refer to the jth element of vector Mi."}, {"heading": "2.1 Basic PCA", "text": "Given a matrix Y of size N \u00d7D (N rows and D columns), a PCA algorithm obtains d principal components (d \u2264 D) that explain the most variance (and hence information) of the data in matrix Y [?, ?]. To be useful in practice, d is chosen to be much smaller than D, that is d \u226a D. The principal components can be used to get better insights about the data. For example, in the image processing domain, PCA is used to obtain the principal facial components whose linear combination could recreate any face in the image dataset [?]. In information retrieval, the principal components explain the principal terms in a set of documents [?]. In addition, PCA could be used as a dimensionality reduction technique [?] when dealing with high-dimensional data. For example, the data of a matrix Y can be mapped on the principal components, without losing much information. The resulting matrix X (of size N \u00d7 d) can be obtained using the following formula: X =Y \u2217C, where C is a D\u00d7d matrix containing the d principal components as its columns. Since matrix X is much smaller than the original matrix Y , it can be used as input to other machine learning algorithms such as k-means clustering.\nA simple method to perform PCA is to compute the covariance matrix of the input matrix Y . Then, compute the eigen-decomposition of the covariance matrix, and choose the eigenvectors that correspond to the largest d eigenvalues. Our analysis shows that the computational cost of this method is dominated by the computa-\ntion of the covariance matrix, which is O(ND\u00d7min(N,D)). This is a very high computational cost, and thus this method is not suitable for large datasets. In addition to the computational cost, this method requires generating a large and dense covariance matrix of size D\u00d7D which incurs substantial communication cost making the method not suitable for large datasets especially, when the input matrix is sparse. This method is implemented in MLlib [?], and we refer to this method as MLlib-PCA. The method is also implemented in RScaLAPACK, which is an add-on package for the widely-used R programing language. RScaLAPACK uses the parallel linear algebra routines implemented in the ScaLAPACK library."}, {"heading": "2.2 Computing PCA Using SVD", "text": "Another approach to PCA is using singular value decomposition (SVD) [?]. SVD decomposes a matrix into three matrices:\nY c =U \u2217\u03a3\u2217V \u2032.\nWhen the input matrix is mean-centered, i.e., Y c=Y \u2212Y m (where Y m is a vector of all the column means of Y ), V gives the principal components of Y c. For the sake of simplicity, we use Y \u2212Y m to indicate that vector Y m is subtracted from each row of matrix Y . Some libraries, such as Mahout, provide PCA by performing SVD on the mean-centered input matrix. by first subtracting the mean from the input matrix and then performing SVD on the resulting matrix.\nSeveral methods have been proposed to compute the SVD of a matrix. We describe the two most common methods: the first method is suitable for dense matrices [?] and the second is suitable for sparse matrices [?].\nSVD for Dense Matrices. Golub and Kahan [?] introduced a two-step approach for computing SVD: convert the input matrix to a bidiagonal one and then perform SVD on the bidiagonal matrix. Demmel and Kahan [?] improved this approach by adding another step before bidiagonalization, which is QR decomposition. We refer to this method as SVD-Bidiag, which has the following three steps for a given matrix Y : (i) compute QR decomposition of Y , which results in an orthogonal matrix Q and an upper triangular matrix R; (ii) transform R to a bidiagonal matrix B; and (iii) compute SVD on B.\nThe SVD-Bidiag algorithm is implemented in RScaLAPACK. Our analysis shows that the computational complexity of the SVDBidiag algorithm is dominated by the QR decomposition and bidiagonalization steps, and is given by O(ND2 +D3). Therefore, the SVD-Bidiag algorithm is only suitable when D is small.\nNext, we analyze the communication overhead of the SVD-Bidiag algorithm. The algorithm involves the three main steps mentioned above, and each produces intermediate data that needs to be communicated to different computing nodes to continue the computation. Specifically, the QR decomposition step results in two matrices, N \u00d7 d matrix Q and d \u00d7D matrix R. Thus, the intermediate data for this step is O(Nd +Dd). The bidiagonalization step of R results in three matrices: d \u00d7 d matrix U1, d \u00d7D matrix B, and D\u00d7D matrix V1, which makes the intermediate data for this step O(d2 +Dd +D2) = O(D2). The SVD computation on the bidiagonal matrix B results in three matrices of the same dimensions as the ones computed in the bidiagonalization step, and thus has the same order of intermediate data O(D2). Therefore, the maximum amount of intermediate data between any two of the three steps is in the order of O(max((N +D)d,D2)), which is substantial for large datasets. Therefore, our analysis reveals a serious issue with the SVD-Bidiag algorithm, namely the communication complexity,\nAlgorithm 1 PPCA (Matrix Y , int N, int D, int d)\n1: C = normrnd(D,d) 2: ss = normrnd(1,1) 3: Y m = columnMean(Y ) 4: Y c = Y \u2212Y m 5: while not STOP_CONDITION do 6: M =C\u2032 \u2217C+ ss\u2217 I 7: X = Y c\u2217C \u2217M\u22121 8: XtX = X \u2032 \u2217X + ss\u2217M\u22121\n9: YtX =Y c\u2032 \u2217X 10: C = YtX/XtX 11: ss2 = trace(XtX \u2217C\u2032 \u2217C) 12: ss3 = N \u2211\nn=1 Xn \u2217C\u2032 \u2217Y c\u2032n\n13: ss = (||Y c||2F + ss2\u22122\u2217 ss3)/N/D 14: end while\nwhich will be the bottleneck for scalability if this algorithm is used for processing big data on a distributed platforms.\nSVD for Sparse Matrices. SVD can be computed efficiently for sparse matrices using Lanczos\u2019 algorithm [?], which has a computational complexity of O(Nz2), where z is the number of nonzero dimensions (out of D dimensions). We refer to this method as SVD-Lanczos, and it is implemented in popular libraries such as Mahout and GraphLab [?]. The SVD-Lanczos algorithm, however, is not efficient for performing PCA on large datasets, because the matrix must be mean-centered in order to obtain the principal components as a result of SVD. Since in many applications the mean of the matrix is not zero, subtracting the mean from a sparse matrix substantially decreases its sparsity. In this case, z will approach the full dimensionality D, and the cost for computing PCA using SVDLanczos will be O(ND2), which is prohibitive for large datasets."}, {"heading": "2.3 Computing PCA Using Stochastic SVD", "text": "Randomized sampling techniques have recently gained popularity in solving large-scale linear algebra problems. Halko [?] describes a randomized method to compute approximate decomposition of matrices, which is referred to as stochastic SVD (SSVD). SSVD has two steps: (i) it uses randomized techniques to compute a low-dimensional approximation of the input matrix, and (ii) it performs SVD on the approximation matrix. The accuracy of the results depends on the performance of the randomized techniques and the size of the approximation matrix. Accuracy can be improved through running the randomization step multiple times. Therefore, SSVD has the flexibility of trading off the accuracy of the results with the required computational resources.\nOur analysis shows that the computational complexity of SSVD is dominated by the first step, which is O(DNd). This is a much better complexity than the previous techniques, because d is typically much smaller than D and is usually a constant. However, SSVD requires exchanging multiple intermediate matrices, which may cause a problem for scalability. Our analysis shows that the amount of intermediate data can be up to O(max(Nd,d2)).\nThe Mahout library implements PCA using SSVD on the meancentered input matrix. We call this algorithm Mahout-PCA. MahoutPCA is a close algorithm to our work, and is one of the algorithms against which we compare our proposed sPCA."}, {"heading": "2.4 Probabilistic PCA", "text": "Probabilistic PCA (PPCA) [?] is a probabilistic approach to computing principal components of a dataset. PPCA is the basis for our scalable PCA (sPCA), and thus we present it in some de-\ntail. In this probabilistic approach, PCA is presented as a latent (unobserved) variable model that seeks a linear relation between a D-dimensional observed data vector y and a d-dimensional latent variable x. The model is defined by:\ny = C\u2217x+\u00b5 + \u03b5,\nwhere C is a D\u00d7d transformation matrix (i.e, the columns of C are the principal directions), x \u223c N (0,I), \u00b5 is the vector mean of y, and \u03b5 \u223c N (0,ss \u2217 I) is a white noise to compensate for errors. The value ss used in \u03b5 is a scalar value representing the average variance and it is estimated from the data. N (u,\u03a3) denotes the Normal distribution with u mean and \u03a3 covariance matrix.\nThe work in [?] shows that, given N observations {yr}N1 as the input data, the log likelihood of data is given by:\nL ({yr}) = N\n\u2211 n=1 ln{p(yr)}.\nThus, the Maximum Likelihood Estimate (MLE) of C is obtained by optimizing\nargmax C L ({yr}). (1)\nThe main idea behind the Probablistic PCA algorithm described in [?] is that the MLE solution of Equation (1) is equivalent to the solution of PCA. Moreover, [?] proposed an Expectation Maximization (EM) [?] algorithm to optimize the likelihood of Equation (1). EM is a well-known method to optimize the likelihood of models when a closed form solution does not exit. This algorithm is the basis for our sPCA algorithm and it will be described in detail later in this section and in the rest of the paper.\nIn the following steps, we show how the likelihood term L ({yr}) is derived. It is shown in [?] that the conditional distribution of y given x is:\np(y|x) = (2\u03c0 \u2217 ss)\u2212D/2 exp{\u2212 1\n2ss \u2016y\u2212C\u2217x\u2212\u00b5\u20162}.\nWith the assumed prior distribution on x as:\np(x) = (2\u03c0)\u2212d/2 exp{\u2212 1 2 x\u2032 \u2217x},\nwe can obtain the marginal distribution of y, p(y), by first obtaining the joint distribution p(x,y),\np(x,y) = p(y|x)p(x).\nThen, we integrate the joint distribution over x to get\np(y) = \u222b p(y|x)p(x)dx\n= (2\u03c0)\u2212D/2|M|\u22121/2 exp{\u2212 1 2 (y\u2212\u00b5)\u2032 \u2217M\u22121 \u2217 (y\u2212\u00b5)},\nwhich is a Normal distribution with mean \u00b5 and covariance matrix M defined by:\nM = ss\u2217 I+C\u2217C\u2032 .\nHence, the log likelihood of data is given by:\nL ({yr}) = N\n\u2211 n=1 ln{p(yr)},\n=\u2212 N 2 {D\u2217 ln(2\u03c0)+ ln |M|+ tr(M\u22121 \u2217S)},\nwhere S is the sample covariance matrix of {yn} given by\nS = 1 N\nN\n\u2211 n=1 (yr \u2212\u00b5)\u2217 (yr \u2212\u00b5)\u2032.\nThe work in [?] shows that MLE solution of Equation (1) is equivalent to the solution of PCA, namely the eigenvectors of the sample covariance matrix S, up to an arbitrary rotation matrix. In addition, PPCA offers two desirable properties. First, large datasets often have missing values. Since PPCA uses expectation maximization, the projections of principal components can be obtained even when some data values are missing. Second, multiple PPCA models can be combined as a probabilistic mixture for better accuracy and to express complex models.\nAlgorithm 1 depicts the pseudo code of PPCA in [?]. In Algorithm 1, Y is the input matrix of size N \u00d7D and d is the desired number of principal components. In other words, matrix Y has the N observations yr as its rows. The function normrnd(r,c) gives a random matrix of size r\u00d7c with Normal distribution. The function trace obtains the trace of the matrix, which is the sum of elements on the diagonal. ||Y c||2F is the square of the Frobenius norm of the mean-centered input matrix. The algorithm requires computing many intermediate variables, among which we have X , the matrix that has N latent variables xn as its rows. The algorithm initializes the transformation matrix C and the variance ss with random values. At each iteration, it improves the values of C and ss until it reaches the STOP_CONDITION. Our analysis shows that the time complexity of PPCA is O(NDd). Section 3 uses Algorithm 1 as the starting point for the design of sPCA."}, {"heading": "2.5 Summary of the Analysis", "text": "The summary of our analysis is shown in Table 1; details are given in [?]. As the table shows, the time complexities of the top two methods (eigen decomposition of covariance matrix and SVD of bi-diagonalized matrix) are approximately cubic in terms of the dimensions of the input matrix, assuming N \u2248 D. Such high time complexities prevent these methods from scaling to large datasets. Even if D < N, the time complexity is a function of N (number of data points) multiplied by D2 (number of dimensions of each data point), which is still quite high for many datasets with a large number of dimensions. is a function of N (number of data points) multiplied by D2 (number of dimensions of each data point), which is quite high for many datasets with a large number of dimensions.\nIn addition, the communication complexities of these two methods are also quite high, especially for high dimensional datasets. Therefore, even if there are enough nodes to handle the high computational costs, the communication costs can still hinder the scalability of these two methods.\nThe last two methods in Table 1 (stochastic SVD and probabilistic PCA) have a more efficient time complexity of O(ND), assuming that d is a relatively small constant, which is typically the case in many real applications. Thus, these two approaches are potential candidates for performing PCA for large datasets. Our analysis and experimental evaluation (in Section 5), however, reveal that even though the time complexity of the stochastic SVD approach can be handled by employing more computing nodes, it can suffer from high communication complexity. For example, our experiments show that the high communications complexity of MahoutPCA (which uses SSVD) prevents it from processing datasets in the order of tens of GBs. Therefore, based on our analysis, the most promising PCA approach for large datasets is the probabilistic PCA.\nNext, we present our sPCA, which is based on probabilistic PCA. sPCA runs in a distributed environment in a way that minimizes the communication complexity while maintaining all the theoretical guarantees provided by the original probabilistic PCA on accuracy and time complexity. Therefore, we believe that sPCA can effectively process large datasets. In addition, our design and optimization method is useful in its own right to scale other machine learning algorithms."}, {"heading": "3. DESIGN OF SPCA", "text": "In this section, we present the design of sPCA, our scalable implementation of PPCA for distributed platforms such as MapReduce and Spark. A naive approach for implementing PPCA is to have a distributed (e.g., MapReduce) job for each linear algebra operation in Algorithm 1. The dependency between these jobs is depicted in the job graph in Figure 1. Each node is labeled with the variable that the job produces. A link from node A to node B indicates that data of variable A must be computed before starting the job that computes variable B. Variables carried over from the previous iteration are distinguished with the index i. Variable Y is the input to the algorithm and does not change between iterations. The output of the algorithm (the principal components of Y ) is in Ci.\nThis simple PPCA implementation works as follows. Matrix M is computed using matrix Ci\u22121 and variance ssi\u22121 that are carried over from the previous iteration. For the first iteration (i = 1), C0 and ss0 are initialized randomly from a Normal distribution. Then, matrices M and Ci\u22121 as well as the input matrix Y are used to generate the intermediate matrix X . Matrix X is used for three other computations. First, it is used to generate covariance matrix X \u2032 \u2217X , which together with the variance ssi\u22121 and the computed matrix M, create matrix XtX . The second consumer of X is its product with the transpose of input matrix Y (YtX). This matrix is divided by matrix XtX to produce the next version of principal components, Ci. The third consumer of X is ss3, part of the variance, which needs Ci and YtX that were computed in the last two steps. Eventually, the variance ssi is updated for the next iteration based on 3 components: (i) Frobenius norm of the input matrix, (ii) ss2, which is the trace of the product of XtX and C\u2032i \u2217Ci, and (iii) ss3, which is computed in the last step.\nAs depicted in Figure 1, there are many linear algebra operations per iteration and the naive approach results in poor performance. In the following, we present our proposed sPCA algorithm. We present our design as successive optimization ideas in separate sub-\nsections. Then, we put all optimizations together in the final subsection. We emphasize that our optimization ideas do not change any theoretical properties of PPCA.\nWe first note that not all operations in Figure 1 need to be performed in distributed manner. In fact, after careful inspection of the algorithm and its various data structures, we found that only three jobs need to be computed in distributed manner because they operate on very large matrices that cannot fit in the memory of a single machine. These jobs are X , YtX , and ss3, and we highlight them in Figure 1 by dotted rectangles. All other operations can easily run on a single machine, even for very large datasets. Specifically, our implementation of sPCA has one main driver program, which implements all of the control flow, launches parallel operations for the three jobs X , YtX , and ss3, and executes all other operations locally."}, {"heading": "3.1 Mean Propagation to Leverage Sparsity", "text": "The first optimization we propose is the mean propagation idea, which preserves and utilizes the sparsity of the input matrix Y . PPCA requires the input matrix to be mean-centered (denoted by Y c), meaning that the mean vector Y m must be subtracted from each row of the original matrix Y . Large matrices, however, are mostly sparse, with many zero elements. Sparse matrices can achieve a small disk and memory footprint by storing only non-zero elements, and performing operations only over non-zero elements. Subtracting the non-zero mean from the matrix would make many elements non-zero, so the advantage of sparsity is lost. The algorithm would incur much more (i) disk I/O operations, (ii) network I/O operations, and (iii) CPU time for operations that could otherwise be skipped for zero elements.\nTo avoid the problems of subtracting the mean, we keep the original matrix Y and the mean Y m in two separate data structures. We do not subtract the mean Y m from Y . Rather, we propagate the mean throughout the different algebra operations. For example, if the algorithm has a step like Y c\u2217C, we change it to be:\nY c\u2217C = (Y \u2212Y m)\u2217C\n=Y \u2217C\u2212Y m\u2217C.\nThat is, the mean Y m is propagated and multiplied with C, and\nat the same time the sparse matrix Y is efficiently multiplied with C. We apply the same technique on all algebra operations of Algorithm 1. This optimization is quite useful for algorithms that require a matrix to be mean-centered."}, {"heading": "3.2 Minimizing Intermediate Data", "text": "As explained in Section 2, intermediate data can slow down the distributed execution of any PCA algorithm, because they need to be transferred to other nodes for processing to continue. For example, at each iteration of running the basic PPCA on a 94 GB input dataset with 50 principal components, nearly 500 GB of intermediate data was created in our initial implementation, which was one of the main bottlenecks.\nThrough analysis and profiling of early implementations of sPCA, we found that the intermediate matrix X can potentially have large size. And as shown in Figure 1, X has to be fed to all the three distributed jobs, and therefore it can become a major scalability bottleneck. To minimize the size of the intermediate data X , we propose two ideas: redundant computation and distributed job consolidation. First, we note that while storing and exchanging X is expensive due to its large size, computing it is a relatively light operation when we use in-memory matrix multiplication: each row of X requires multiplying a sparse row of Y with a small, in-memory, matrix C \u2217M\u22121 (Algorithm 1). To leverage this property, we redesign the algorithm by redundantly recomputing X at each job that consumes it as input. This approach essentially trades intermediate data footprint with computation. Figure 2 illustrates this optimization. The figure shows only the part of the original job graph in Figure 1, where matrix X is recomputed in each of the three distributed jobs.\nOur second optimization is job consolidation, which means merging multiple distributed jobs into one in order to reduce the communication between these jobs. Since there is no dependency between the XtX and YtX jobs in Figure 2, we consolidate them into one job. This also reduces the number of times that X is redundantly computed. Figure 3 shows the final job graph of the distributed part of sPCA. This job graph is plugged into the complete job graph in Figure 1."}, {"heading": "3.3 Efficient Matrix Multiplication", "text": "As shown in Algorithm 1, PPCA requires many matrix multiplications, which are expensive operations in a distributed setting. To appreciate the techniques that sPCA employs to overcome the inefficiency of matrix multiplication, we briefly explain different possible implementations of this operation. Semantically, the product of two matrices A of size N\u00d7D and B of size D\u00d7M is matrix A\u2217B\nof size N \u00d7M and can be defined as:\n(A\u2217B) ji = D\n\u2211 k=1 Aki \u2217B j k.\nThis computation requires many random accesses to the two matrices, which makes it inefficient when the two matrices are distributed. There are, nevertheless, variants of matrix multiplication that can be implemented efficiently. For example, if instead of A\u2217B we want to compute A\u2032 \u2217B, then we can use the following equivalent formula:\n(A\u2032 \u2217B) = D\n\u2211 k=1 (Ai) \u2032 \u2217Bi, (2)\nwhich requires accessing one row at a time from A\u2032 and B. Some libraries, e.g., Mahout, use this approach for matrix multiplication. Mahout obtains the transpose of matrix A and then uses map-side join to multiply the corresponding rows from the two matrices. The result is then sent to reducers, which sum up the received partial matrices. This approach still requires an extra matrix transpose operation as well as transferring a large amount of data between the mappers and reducers. Map-side join also requires non-trivial initialization time to align partitions of the two matrices.\nFor sPCA, we seek a more efficient matrix multiplication operation. Notice that if matrix B can entirely fit in memory, we can benefit from the following equivalent equation:\n(A\u2217B)i = Ai \u2217B.\nUsing the above equation, matrix multiplication could be implemented by distributing only the first matrix among different nodes and loading the entire second matrix into the memory of each node. Specifically, we partition the large matrix, A, among multiple nodes and matrix B is loaded into the memory of each node. Each node reads a row from its partition of A, multiplies it with the in-memory matrix B, and produces one row of the result matrix. This approach does not require matrix transpose, and is more efficient.\nAn example of a matrix multiplication operation in Algorithm 1 that can benefit from in-memory matrix multiplication is the product of the input matrix Y c and matrix C, which is of size D \u00d7 d (recall that d is typically small). For example, in our experiments with a 94 GB dataset, the size of matrix C was 30 MB, which can easily fit in memory.\nIt is, nevertheless, not always possible to benefit from this technique since the second matrix could be large. For example, in the PPCA algorithm, the calculation of matrix YtX requires a product between the transpose of Y c and X :\nYtX = Y c\u2032 \u2217X .\nThis operation was actually a bottleneck in our first prototype of sPCA. In the new design, however, since we generate matrix X on-demand, one row of X is generated at a time which allows for efficient implementation of matrix multiplication using Equation (2)."}, {"heading": "3.4 Efficient Frobenius Norm Computation", "text": "The PPCA algorithm requires computing the Frobenius norm of the mean-centered input matrix Y c = Y \u2212Y m. Recall that we store the mean vector Y m separately from the matrix Y to avoid creating the dense matrix Y c. Applying the same technique, we can compute the rows of Y c online, right before computing the Frobenius norm. Algorithm 2 shows this approach.\nAlgorithm 2 Frobenius-simple (Matrix Y , Vector Ym) : double 1: for all Ys in Y.rows do 2: Yd = Ys \u2212Ym 3: for all Y jd in Yd do\n4: sum += (Y jd ) 2 5: end for 6: end for 7: return sum\nAlthough Algorithm 2 has the advantage of requiring a small amount of memory to maintain only one dense row at a time, it still requires iterating over the dense row Yd , which is much larger than the original sparse row Ys. To circumvent this problem, we design Algorithm 3 which does not even require creating the dense vector. We note that many machine learning algorithms compute various norms of matrices. The proposed method for optimizing the computation of the Frobenius norm can be extended to other matrix norms using similar ideas. Thus, this simple optimization can benefit several other machine learning algorithms.\nAlgorithm 3 Frobenius (Matrix Y , Vector Ym) : double\n1: for all Y jm in Ym do 2: msum+= (Y jm)2 3: end for 4: for all Ys in Y.rows do 5: for all Y js in Ys do 6: sum += (Y js \u2212Y j m) 2 7: sum \u2212= (Y jm)2 8: end for 9: sum += msum\n10: end for 11: return sum\nIn this approach, we first compute the Frobenius norm of the mean matrix, which would be equal to the norm of the sparse matrix if all elements were zero. We then subtract the mean value only from non-zero elements and add the square of the results to the norm being computed. We also cancel the effect of considering the square of the mean for each non-zero element by subtracting it from the norm being computed."}, {"heading": "3.5 sPCA: Putting it All Together", "text": "Algorithm 4 shows the pseudo code of sPCA including all the techniques that we described in the previous sections. The parts of the algorithm that are run with distributed jobs are highlighted with bold font. The C and ss variables are initialized with random values. Before starting the iterations, we run two lightweight jobs to compute the column mean and Frobenius norm of the input\nAlgorithm 4 sPCA (Matrix Y , int N, int D, int d)\n1: C = normrnd(D,d) 2: ss = normrnd(1,1) 3: Y m = meanJob(Y ) 4: ss1 = FnormJob(Y ) 5: while not STOP_CONDITION do 6: M =C\u2032 \u2217C+ ss\u2217 I 7: CM =C \u2217M\u22121 8: Xm = Y m\u2217CM 9: {XtX ,YtX}= YtXJob(Y,Y m,Xm,CM)\n10: XtX+= ss\u2217M\u22121 11: C = YtX/XtX 12: ss2 = trace(XtX \u2217C\u2032 \u2217C) 13: ss3 = ss3Job(Y,Y m,Xm,CM,C) 14: ss = (ss1+ ss2\u22122\u2217 ss3)/N/D 15: end while\nmatrix. YtXJob computes both XtX and YtX variables. It generates row r of X on demand using row r of the input matrix Y , the in-memory matrix CM, as well as the mean Y m and its effect on X (Xm). ss3Job computes the third part of variance. Similar to YtXJob, ss3Job generates the rows of X on-demand."}, {"heading": "4. IMPLEMENTATION OF sPCA", "text": "In this section, we show that the design of sPCA and the optimizations it uses are not restricted to a specific platform; they are valid for the disk-based MapReduce and the in-memory Spark."}, {"heading": "4.1 Implementation in MapReduce", "text": "This section provides a brief description on the MapReduce implementation of the two main parallel jobs in sPCA (YtXJob and ss3Job). These two jobs are run in each iteration of sPCA. There are other two MapReduce jobs (meanJob and FnormJob) which are lighter weight and run once before the main loop of the algorithm.\nAt high level, programs in the MapReduce framework are divided into mappers, reducers, and optionally combiners. Mappers execute user-specified function on different parts of the dataset. The results are then sorted and directed to reducers, which will aggregate and produce the final results. Combiners can be applied to the mappers\u2019 output before feeding them to reducers.\nIn sPCA, the mapper of the YtXJob operates on each row of the input matrix Y , Yi, and then generates Xi, the corresponding row of intermediate matrix X . It then uses Equation (2) to generate a partial result for XtX and YtX . The partial results have to be summed up in the combiners and eventually in the reducers to generate the full results. This, however, makes each mapper generate an entire dense matrix after processing each sparse row. To solve this problem, we make the mapper keep two in-memory matrices XtX-p and YtX-p to maintain the partial results and add them with the partial sums after processing each row. At the end of processing for a mapper, when the MapReduce framework calls the cleanup method, the mapper writes the entire partial matrices XtX-p and YtX-p to the output. We refer to this technique as using a Stateful Combiner. This technique results in much less load on the combiners, and allows the CPU cycles to be used for useful work.\nTo send both XtX-p and YtX-p to reducers we use a composite key. Since XtX-p is of size d \u00d7 d, where d is the number of principal components and thus small, we define the composite key to send all the partial XtX-p matrices to the same reducer. That reducer then sums them up in memory and writes the resulting XtX into HDFS. Matrix YtX , on the other hand, is generated using the normal output interface of reducers.\nSimilar to YtXJob, ss3Job generates the rows of X on demand. After producing each row Xi, it has to do the following computation:\nXi \u2217C \u2032 \u2217Y \u2032i . (3)\nThe default way to do this computation is to first perform Xi \u2217C\u2032 and then multiply the result by Y \u2032i . This, however, is not efficient, because vector Y \u2032i is sparse, and thus most of the work to compute elements in (Xi \u2217C\u2032) will be wasted since most of these elements will be multiplied with zero elements in Y \u2032i . Using the associativity property of matrix multiplication, we perform this operation as Xi \u2217 (C\u2032 \u2217Y \u2032i ), i.e., first multiply matrix C\n\u2032 with the sparse vector Y \u2032i , and then obtain its dot product with Xi, which is efficient since both are of small size d. In addition, the mapper output of this job is a scalar, which reduces the amount of intermediate data."}, {"heading": "4.2 Implementation in Spark", "text": "Spark is a framework for large-scale data-intensive computation. Spark provides two main abstractions for parallel programming: resilient distributed datasets (RDDs) and parallel operations on these datasets. An RDD is a collection of records that can be operated on in parallel. An RDD is partitioned across multiple machines and users can control its persistence (e.g., cache in memory or store on disk) and its partitioning (e.g., partition by key). Developers typically define one or more RDDs through transformations on data in stable storage. Examples of transformations include map (which returns a new distributed dataset formed by passing each element of the source through a user-defined function) and filter (which returns a new dataset formed by selecting those elements of the source on which a user-defined function returns true.). Developers can then use these RDDs in actions, which are operations that return a value to the application or export data to a storage system. Examples of actions include count (which returns the number of elements in the dataset), collect (which returns the elements themselves), and save (which outputs the dataset to a storage system).\nsPCA is designed to leverage the in-memory computations provided by Spark through making the input matrix Y persistent in the memory of the cluster nodes and performing distributed operations on it repeatedly. This approach translates to much less disk and network I/O. The disk I/O is limited to the amount of data that does not fit in the aggregate memory of the cluster.\nAlgorithm 5 presents the pseudo code of the YtXJob implemented on Spark. The code makes use of a special type of variables provided by Spark called accumulators. Accumulators are variables that workers can only add to using an associative operation, and that only the driver can read. The map operation of the YtXJob operates on each row of the input matrix Y , Yi, and then generates Xi, the corresponding row of intermediate matrix X . It then uses Equation (2) to generate the partial result XtX i and YtX i. We note that the partial results are summed up in the same map operation using the accumulators XtXSum and YtXSum, thus eliminating the need for reduce operations and achieving good scalability. The results of the accumulators are read later in the driver program after all map tasks finish execution. We note that YtX i is the product of the sparse vector Y \u2032i of length D\u00d71 and the vector Xi of length 1\u00d7 d and hence, YtX i is a sparse D\u00d7 d matrix. In order to make use of this sparsity we only pass the indices of the sparse entries of YtX i to the accumulator YtXSum. This results in significant improvement in the running time since the complexity of this operation was reduced from O(D\u00d7d) to O(z\u00d7d), where z is the number of non-zero dimensions (out of D dimensions).\nThe implementation of the ss3Job in Spark follows the same\nAlgorithm 5 YtXSparkJob (Matrix Y , Vector Y m ,Vector Xm, Matrix CM, int D, int d)\n1: YtXSum = spark.accumultor(newMatrix(D,d)) 2: XtXSum = spark.accumulator(newMatrix(d,d)) 3: Y.map{Yi => \u22b2 runs in parallel 4: Xi = Yi\u2217CM\u2212Y m\u2217CM 5: (YtX)i =Yi\u2032 \u2217 (Xi\u2212Xm)\u2212Y m\u2032 \u2217 (Xi\u2212Xm) 6: (XtX)i = Xi\u2032 \u2217 (Xi\u2212Xm)\u2212Xm\u2032 \u2217 (Xi\u2212Xm) 7: YtXSum.add((YtX)i) 8: XtXSum.add((XtX)i)} 9: YtX=YtXSum.value()\n10: XtX=XtXSum.value()\nsteps described for the MapReduce implementation (Section 4.1) to optimize the computation described in Equation (3)."}, {"heading": "5. EVALUATION", "text": "In this section, we present our rigorous evaluation of sPCA and compare it against the closest algorithms using multiple real datasets from different domains.\nAlgorithms Compared. We compare four methods for computing PCA:\n\u2022 sPCA-MapReduce: sPCA implementation on MapReduce,\n\u2022 sPCA-Spark: sPCA implementation on Spark,\n\u2022 Mahout-PCA: PCA implementation in Mahout [?] on MapReduce, and\n\u2022 MLlib-PCA: PCA implementation in MLlib [?] on Spark.\nBoth Mahout-PCA and MLlib-PCA implementations are quite popular and optimized, and we found them to be the best options for their respective platforms. For comparison, all PCA algorithms compute 50 principal components.\nDatasets. We use three real datasets, which are quite diverse in terms of the domain they come from, size, number of dimensions in the data, sparsity, and ranges/types of values for each data item. Furthermore, for each dataset, we use various subsets of it to assess the scalability and performance of the considered PCA algorithms with increasing data sizes. The datasets used in the experiments are:\n\u2022 Tweets: A large set of tweets from the Twitter social network. We construct a matrix such that the rows represent the tweets and the columns represent all words that appear in each tweet. The matrix is of size 1,264,812,931\u00d7 71,503, and each element is either 1 or 0, where 1 means the corresponding word appeared in that tweet, and 0 means otherwise. When we store only the non-zero elements, this matrix occupies about 94 GB.\n\u2022 Bio-Text: A set of 8 million biomedical documents collected from the U.S. National Library of Medicine, which is the largest medical library in the world. The collection includes books, journals, technical reports, and manuscripts on medicine and related sciences. We construct a matrix from this dataset, where the rows represent the documents and the columns represent the distinct words in each document. The matrix size is 8,200,000\u00d7141,043, and each element is either 1 or 0, where 1 means the corresponding word appeared in that document, and 0 means otherwise. The non-zero elements of this matrix occupies about 4.9 GB.\n\u2022 Diabetes: This dataset is collected from 353 patients. A urine sample is taken from each patient. Then, a magnetic field is applied on the urine samples. The nuclear magnetic resonance (NMR) is then measured for the molecules (metabolites) in the urine. NMR is a phenomenon where the molecules absorb and re-emit electromagnetic radiation. This energy is at a specific resonance frequency that depends on the strength of the magnetic field and the magnetic properties of the atoms. The data represents the magnitude of this energy at each frequency. The magnitude is measured at 65,669 different frequencies for each patient. Thus, we construct a matrix of size 353 \u00d7 65,669, where rows represent patients and columns represent sample frequencies. Unlike previous datasets that have binary elements, the elements in this dataset are real values representing the magnitude of radiation at different frequencies.\n\u2022 Images: A dataset of 160 million data vectors. These vectors are visual features extracted from 1 million images downloaded from ImageNet [?]. From each image, we extract an average of 160 SIFT [?] features, where each SIFT feature is a vector of 128 dimenstions. This results in a dataset of 160 million vector. The matrix is of size 160,000,000\u00d7128, where rows represent the data vectors, columns represent dimensions of each vector, and each element is a real value that represents the texture of some part of an image.\nPerformance Metrics. We consider three performance metrics: accuracy, running time to achieve a target accuracy, and size of the intermediate data.\nWe measure the accuracy by computing the 1-Norm of the reconstruction error, which is given by: e = ||Y \u2212X \u2217C\u22121||1. Although this provides a common way to compare the accuracy of different algorithms, the reconstruction error is a big, dense matrix which is costly to store and process. We reduce the cost of storage by computing the error row by row, avoiding the need to store the large reconstruction matrix in the file system. Nevertheless, iterating over the resulting dense rows is still time consuming. We reduce this time by measuring the error only on a random subset of the rows, Y r. To have a unique way to interpret the measured error, independent of the sampling rate or matrix size, we report the norm of the reconstruction error divided by the norm of the matrix made up of the randomly selected rows, which is:\ne = ||Y r\u2212Xr \u2217C\u22121||1/||Y r||1.\nIn addition, we measure the ideal accuracy that can be achieved with 50 principal components after a large number of iterations.\nAfter each iteration, we report the percentage of the ideal accuracy that is achieved.\nSome PCA algorithms, e.g., sPCA-MapReduce, sPCA-Spark, and Mahout-PCA, run multiple iterations until a target accuracy is achieved. Thus, given a target accuracy, we measure the time required to reach that accuracy. For example, in some of our experiments, we measure the time to reach at least 95% of the ideal accuracy.\nThe intermediate data size is the amount of data generated by each algorithm during its execution. We note that in many cases the intermediate data generated by the algorithm far exceeds the size of the input data, and thus becomes a major bottleneck.\nCluster Specifications. We run the experiments on the Amazon EC2 cloud. We created a cluster of 8 Amazon EC2 m3.2xlarge instances, where each node has 8 cores and 32 GB of memory. The cluster runs Linux Red Hat 4.6.3. Amazon Hadoop distribution 0.20.205 and Apache Spark 1.0 were installed on the cluster. The Amazon Hadoop distribution is based on Apache Hadoop, with patches and improvements added that make it work efficiently with Amazon Web Services (AWS).\nExperiments Conducted. We first compare the running times of all algorithms on the three datasets. Then, we conduct detailed evaluation and comparison on MapReduce and Spark, separately. Finally, we isolate and study the effect of each of the proposed optimizations on the performance of sPCA."}, {"heading": "5.1 Comparison of all Algorithms", "text": "We measure the running time of all algorithms on the three datasets, and for each dataset we choose several sizes. A representative sample of our results is shown in Table 2. We note that MLlib-PCA is a deterministic algorithm that terminates after performing a fixed number of matrix operations, unlike sPCA and Mahout-PCA which are iterative algorithms that keep refining the principal components until they reach a target accuracy. Hence, we compare the running time of MLlib-PCA with that of sPCA and Mahout-PCA based on the time needed for sPCA and Mahout-PCA to reach at least 95% of the ideal accuracy.\nWe make three two observations on the results in Table 2. First, sPCA outperforms the other two algorithms by wide margins in most of the all cases. For example, on the MapReduce platform, for a large dataset of tweets of size 1.26B\u00d771.5K, sPCA-MapReduce finishes in less than 5 hours (16,200 sec), while Mahout-PCA takes almost 5 days (430,200 sec) to finish. The second observation is that MLlib-PCA fails to compute the principal components for high dimensional datasets. This is because MLlib-PCA requires storing a D \u00d7 D covariance matrix in the memory of one machine, and hence the algorithm fails when the size of this matrix exceeds the\navailable memory of one machine (not the aggregate memory in the cluster). In our experiments with 32 GB memory machines, MLlibPCA fails when D exceeds 6,000. Even in the cases that MLlibPCA succeeds to produce results, it takes about twice the time of our sPCA-Spark, as shown in rows 3, 5, and 8 in Table 2. The third observation is that MLlib-PCA outperfroms other approaches in the specific case of low-dimentional and dense matrix such as the Images dataset shown in row 10 in Table 2. The Images dataset has relatively low dimentionality (128 dimensions). In this case, MLlib-PCA computes a 128 \u00d7 128 intermediate matrix and then does further computations in one machine, hence, it finishes the workload faster. On the other hand, the disadvantages of MLlibPCA that it does not leverage sparsity and it does not scale when the dimensionality exceeds 6,000 dimensions are, however, not revealed in such dataset.\nTo summarize, our results show that sPCA offers much better scalability and performance than its competitors on both MapReduce and Spark."}, {"heading": "5.2 Detailed Evaluation on MapReduce", "text": "In this section, we present an in-depth comparison of sPCAMapReduce and Mahout-PCA. Accuracy. The accuracy of both sPCA-MapReduce and Mahout-\nPCA algorithms depends on the number of iterations that they run. We run both algorithms on different datasets and measure the accuracy after each iteration. Two samples of our results are shown in Figures 4 and 5, for the Bio-Text and Tweets datasets, respectively. Figure 4 shows that sPCA quickly offers 93% accuracy after 715 sec in the second iteration. In addition, sPCA converges fast, in less than 1,500 sec. On the other hand, Mahout-PCA takes much longer time to converge, which is more than 5,000 sec.\nFigure 5 reports the accuracy for the larger Tweets dataset. The figure shows that sPCA achieves much higher accuracy than MahoutPCA. For example, at time 1,000 sec, the accuracy of sPCA is at least 20% higher than that of Mahout-PCA, and the accuracy gap keeps increasing with time as sPCA achieves almost 100% accuracy before 10,000 sec, whereas the accuracy of Mahout-PCA reaches up to 70% after more than 259,000 sec of running. That is, after running for about 26 times longer, Mahout-PCA achieves up to 30% less accuracy than sPCA.\nsPCA starts with random initialization for the variance (ss) and the principal components (C), and improves upon them at each iteration. If we feed the algorithm with smart guesses for these variables, it converges faster. We use this optimization by first running the algorithm on a much smaller sample matrix, randomly selected from the original input. We then feed the resulting ss and C variables to the algorithm to be run on the original dataset. This operation is quite useful for processing large data sets as the time to compute the smart guesses is offset by the much larger savings in time to reach the target accuracy. We refer to sPCA by sPCA-SR when this initialization process is used. The results on the Tweets dataset indicate that this initialization technique adds 527 sec of delay. However, as shown in Figure 5, after the first iteration, the technique produces a much higher accuracy of 92% compared to that of sPCA, which is 62%. We note that Mahout-PCA cannot use this optimization because Mahout-PCA requires a large random matrix that has the same number of rows (1.26 billion) as the input matrix, unlike sPCA in which a small D\u00d7 d random matrix is initialized which does not depend on the number of rows N, so it can be solved easily for a small number of rows before solving for the whole input matrix.\nTime to Achieve Target Accuracy. We compare sPCA-MapReduce and Mahout-PCA based on the time needed to reach 95% of the ideal accuracy. We vary the size of the input dataset and measure\nthe time needed for both sPCA-MapReduce and Mahout-PCA to achieve 95% accuracy. A sample of our results is shown in Figure 6 for the Tweets dataset; other results are similar. In this figure, we vary the number of rows in the input matrix, but we use the same number of columns which is the full 71,503 columns of the dataset. The results in Figure 6 show that the running times for both algorithms are close for small datasets (i.e., up to 10 million rows). For larger datasets, however, sPCA-MapReduce reaches 95% accuracy two orders of magnitude faster than Mahout-PCA. The reason for this is that the benefits of our optimizations in sPCA pay off better when we scale to larger datasets. More importantly, unlike Mahout-PCA, the running time of sPCA-MapReduce increases at a much smaller rate as the size of the input dataset increases, which allows it to scale well.\nIntermediate Data Size. We measure the size of intermediate data generated by sPCA-MapReduce and Mahout-PCA. Our results (figures not shown due to space limitations) show that sPCAMapReduce generates much smaller intermediate data in all cases compared to Mahout-PCA. For example, for the Bio-Text dataset, Mahout-PCA generates 8 GB of intermediate data, whereas sPCAMapReduce generates only 240 MB of such data, a factor of 35x less data. This property pays off better when we scale to larger datasets. Our results show that for the Tweets dataset Mahout-PCA generates 961 GB of intermediate data, whereas sPCA-MapReduce produces 131 MB of such data, which is a factor of 3,511x less data. Notice that sPCA-MapReduce generates less data as a fraction of the dataset size for the larger Tweets dataset since it has fewer columns compared to the Bio-Text dataset.\nAnalysis of sPCA and Mahout-PCA Jobs. We analyze the individual jobs of sPCA and Mahout-PCA in terms of running time of each job and the amount of data generated. This analysis helps in understanding the performance differences observed in the previous sections. Our analysis shows the following: For sPCA, we notice that although the Tweets dataset is larger than the Bio-Text dataset by a factor of 20x, the durations of the jobs increase by a factor less than 4. This is because the overheads of the Hadoop framework and job initialization have a larger relative impact in the smaller case. More importantly, the execution time depends also on the matrix sparsity. Although the Tweets dataset is 20x larger in size, it is much sparser.\nOn the other hand, Mahout-PCA\u2019s jobs are significantly slower in relative terms when the input size increases. For example, the execution time for the Bt job in Mahout-PCA increases by a factor of 654x when we increase the data size 20x by switching from the BioText to the Tweets dataset. Most of this time is spent in the mappers. To understand why Mahout-PCA suffers from this inefficiency, we looked at the mappers\u2019 output data and we observed that the mappers produce 15.6x more output than that of the Bio-Text dataset, resulting in 4 terabytes of data. The combiners, therefore, are overloaded with a large amount of input. This mapper output size is extremely large compared to the aggregate output size of the mappers of the YtX job in sPCA, which increases by only 2.3x times when we switch from Bio-Text to Tweets. This moderate mapper output size contributes to the scalability of sPCA."}, {"heading": "5.3 Detailed Evaluation on Spark", "text": "We evaluate sPCA on Spark with MLlib-PCA. To the best our knowledge, MLlib-PCA is the only available Spark implementation of PCA that was added starting from Spark version 1.0.0, which is the version we use in our experiments.\nTime to Achieve Target Accuracy. As described in Section 2.1, MLlib-PCA is a deterministic algorithm that terminates after performing a fixed number of matrix operations. We compare the\nrunning time of MLlib-PCA with that of sPCA based on the time needed for sPCA to reach at least 95% of the ideal accuracy. We run multiple experiments using the Tweets dataset. In each experiment, we use the same number of rows, but we vary the number of columns, and we measure the total running time for both algorithms. We plot the results in Figure 7. The results show that MLlib-PCA fails when the number of columns D exceeds 6,000. As discussed before, this is due to the fact that MLlib-PCA loads a D\u00d7D covariance matrix in the memory of one machine. Hence, the algorithm is not scalable except for up to a few thousand columns. On the other hand, sPCA requires a small O(D\u00d7 d) matrix to be stored in memory, and d is typically a small constant. This important difference makes sPCA much more scalable than MLlib-sPCA.\nRegarding the running time, the figure shows that sPCA is much faster than MLlib-PCA. For example, the running time of sPCA is nearly half of MLlib-PCA for D = 6,000 and the difference in speed increases with increasing number of columns. This happens because MLlib-PCA performs dense matrix operations on the covariance matrix. Since, the covariance matrix has D2 elements, the running time of MLlib-PCA increases quadratically with D, unlike sPCA in which there is a linear relationship between the running time and the number of input dimensions D.\nFinally, we observe that the running time of sPCA does not increase with the same factor as the input size. For example, the running time increases by a factor of 10x with increasing the input size by a factor of 70x. This gain is due to the efficient use of sparse matrices in sPCA.\nIntermediate Data Size. Intermediate data in Spark can be created in different ways. It could be (i) RDDs distributed in the memory or the disks of different machines in the cluster, or (ii) intermediate data loaded in the memory of the master machine which is the machine that runs the driver program and handles the workflow of the Spark jobs (launching distributed jobs, aggregating results, etc.). Since both sPCA and MLlib-PCA cache only one RDD in the aggregate memory of the cluster and this RDD is used for the input matrix, other intermediate data is loaded in the driver program. We therefore measure the amount of memory consumed by the process that runs the driver program for both algorithms.\nWe monitor the memory used by the Java process that runs the driver program in intervals of 5 seconds using the JVM utilities jmap and jstat and we report the maximum resident memory throughout the running time of the process. Figure 8 compares the memory consumption of both algorithms for the Tweets dataset. The results show that the memory consumption of sPCA is almost constant. However, the memory consumption of MLlib-PCA increases drastically with increasing the number of columns. For example, MLlib-PCA consumes more than 26 GB of memory for an input matrix of 6,000 columns. This explains the results shown in Figure 7 and shows why MLlib-PCA fails to process more than 6,000 columns on a machine with 32 GB of memory."}, {"heading": "5.4 Effect of Individual Optimizations", "text": "In Section 3, we presented our design as successive optimization ideas. Then, we put all optimizations together to form the final sPCA algorithm. In this section, we analyze how much each of these optimizations contributes to the speedup achieved by sPCA. We analyze the three core optimization ideas: mean propagation to leverage sparsity (Section 3.1), minimizing intermediate data (Section 3.2), and optimizing the computation of the Frobenius norm (Section 3.4). These optimizations are used in the operations on lines 7,8 and 13 of Algorithm 1, respectively. Each operation corresponds to one optimization and they are all distributed operations. Therefore, we use these operations to test these optimizations by\ncomparing the optimized and unoptimized versions of the operations.\nWe use a subset of the Tweets dataset consisting of 100,000 rows, and we measure the running time of each operation with and without applying the optimization. The experiments are done using the sPCA implementation on Spark. Table 3 shows the results of the comparison. The results show that the careful design and optimization ideas of sPCA provide us with orders of magnitude speedup over the unoptimized implementation. The results also show that mean propagation is the optimization that provides the biggest benefit out of the three optimizations. This is because it greatly preserves the sparsity of the input matrix, which has a major effect on performance. The second most important optimization is minimizing the intermediate data. The results show that it takes 3 seconds to compute matrices X and XtX from the input matrix Y compared to 44 minutes needed to compute matrix XtX from the stored large matrix X . The Frobenius norm optimization in Algorithm 3 is faster than the simple implementation in Algorithm 2 by a factor of 270x."}, {"heading": "5.5 Speedup", "text": "In this section, we analyze the performance of sPCA when running it on clusters of different sizes (16, 32, and 64 cores). Figure 9 shows the speedup in the execution time with increasing the number of cores. We measure the speedup as S = T16 cores/Tn cores, where T16 cores is the execution time of running sPCA on the smallest cluster (16 cores), and Tn cores is the exection time of running sPCA on a cluster with n cores."}, {"heading": "6. RELATED WORK", "text": "Implementing efficient machine learning algorithms on big data is an active field of research. Many ongoing works approach the problem from different perspectives. Several recent works [?, ?] attempt to leverage the observation that the iterative nature of machine learning algorithms does not perfectly match the MapReduce framework. Such works usually (i) add language support to en-\nable the developer to express the iterations, and (ii) provide compiler support to leverage the knowledge about iterations for better scheduling and caching policies. Haloop [?] extends the MapReduce programming model with the notion of iteration. The knowledge of iteration is then taken into consideration to affect scheduling (running on local data obtained from the previous iteration) and caching policies (caching the output if it is going to be used in the next iteration). Twister [?] suggests modifications to the MapReduce framework to make it efficient for machine learning algorithms. A different approach to iterative machine learning is adopted by Hogwild! [?]. Hogwild! parallelizes the stochastic gradient descent (SGD) algorithm on a shared memory machine by running SGD without locks. Interestingly, convergence is still guaranteed. SGD is useful for many machine learning tasks, but it cannot be used to compute PCA.\nSome related works take a top-down approach and define the minimum language requirements to express machine learning algorithms on top of distributed systems [?, ?]. Borkar et al. [?] use DataLog to define a language expressive enough to cover many of the existing machine learning algorithms. They argue that the general query optimization techniques from the database literature could be applied to compile the declarative programs into efficient executions plans. MLbase [?] argues for a DBMS approach for machine learning algorithms, in which the algorithm is expressed in an expressive language (similar to SQL) and the MLbase will take care of optimization and query planning. A similar approach is taken in SystemML [?], in which the user expresses a computation in a language similar to R, and the system automatically compiles the computation to an optimized workflow of MapReduce jobs. The SciDB system [?] focuses on parallel array processing for scientific workloads. The main focus of SciDB is effective storage and retrieval of arrays in cluster environments [?], and a computation like PCA would be implemented in an application on top of SciDB.\nIn this paper, we take a bottom-up approach: we study the bottlenecks in a complex machine learning algorithm and provide solutions for each one. The insights and rules that we presented in this paper could be leveraged by any of the above systems.\nChu et al. [?] list many machine learning algorithms that can be parallelized on multiple cores using MapReduce. For PCA, they suggest using the classic technique of first obtaining the covariance matrix, and then computing its eigenvectors. Then, they show that the covariance matrix can efficiently be computed in the MapReduce model using only one pass on the data. Afterwards, they use a centralized algorithm to obtain the eigenvectors. The disadvantage of this approach is that it requires storing the covariance matrix in the memory of one machine. Although this is possible in the case of \u201cthin\u201d matrices that have a small number of dimensions, it is not a feasible solution for matrices with large dimensionality, which we target. However, we employ their approach for computing the covariance matrix in sPCA when we compute matrix XtX . We presented a comprehensive overview of computing PCA in Section 2."}, {"heading": "7. CONCLUSION", "text": "In this paper, we analyzed different methods for computing the principal components of a given dataset, which is referred to as principal component analysis (PCA). The analysis showed the computational complexity as well as the communication complexity. Both are important aspects for processing large-scale datasets on distributed platforms. Our analysis indicated that all current algorithms for PCA have significant bottlenecks that prevent them from scaling to large datasets.\nWe presented a scalable design and implementation for PCA, which we call sPCA. Based on our analysis, we chose the prob-\nabilistic PCA (PPCA) algorithm [?] as the starting point for our design of sPCA. sPCA does not change any theoretical guarantee offered by PPCA. Rather, sPCA employs several optimizations to support large datasets on distributed clusters. These include efficient large matrix operations, effectively leveraging matrix sparsity, and minimizing intermediate data. These optimizations are applicable to various algorithms needed to process large datasets, and hence they are useful in their own right.\nWe implemented sPCA on the MapReduce and Spark platforms and compared these implementations against the closest counterparts available: Mahout-PCA on MapReduce and MLlib-PCA on Spark. Our experiments on diverse realistic datasets showed that, overall, sPCA achieves multiple orders of magnitudes speedup over Mahout-PCA and MLlib-PCA as well as much higher accuracy in the computed principal components. For example, it takes sPCA about 1.72 hours to achieve a target accuracy of 95% on a dataset of of more than 1.26 billion tweets, while Mahout-PCA needs about 5 days to reach that level of accuracy. Furthermore, in our experiments, MLlib-PCA could not process any dataset with more than 6,000 dimensions, while sPCA can process datasets with more than 70,000 dimensions.\nar X\niv :1\n50 3.\n05 21\n4v 1\n[ cs\n.D C\n] 1\n7 M\nar 2\n01 5\n1 Title"}, {"heading": "1.1 Subtitle", "text": "Plain text.\n1.2 Another subtitle\nMore plain text.\n1"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Web sites, social networks, sensors, and scientific experiments currently generate massive amounts of data. Owners of this data strive to obtain insights from it, often by applying machine learning algorithms. Many machine learning algorithms, however, do not scale well to cope with the ever increasing volumes of data. To address this problem, we identify several optimizations that are crucial for scaling various machine learning algorithms in distributed settings. We apply these optimizations to the popular Principal Component Analysis (PCA) algorithm. PCA is an important tool in many areas including image processing, data visualization, information retrieval, and dimensionality reduction. We refer to the proposed optimized PCA algorithm as scalable PCA, or sPCA. sPCA achieves scalability via employing efficient large matrix operations, effectively leveraging matrix sparsity, and minimizing intermediate data. We implement sPCA on the widely-used MapReduce platform and on the memory-based Spark platform. We compare sPCA against the closest PCA implementations, which are the ones in Mahout/ MapReduce and MLlib/Spark. Our experiments show that sPCA outperforms both Mahout-PCA and MLlib-PCA by wide margins in terms of accuracy, running time, and volume of intermediate data generated during the computation.", "creator": "gnuplot 4.6 patchlevel 4"}}}