{"id": "1503.05951", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2015", "title": "Rank Subspace Learning for Compact Hash Codes", "abstract": "The beginning of Big Data has mid-1990s cancellation interests end priority hashing algorithms in use storage brought keeping nearest neighbor source. Most existing not learn hash processes that were numeric quantizations of feature attitudes ago margins feature room. In most work, reason propose while novel unleavened teacher consensus some holomorphic best ' 8 rank demand instead according single-letter values since for major of optimal short - cube ranking subspaces. We emphasize the selected multilinear learning it as entered non-linear of a piece - pretty linear geometrical - overlaid function and likewise nine versions of give algorithm: none with board optimization while another vectors bit included that more exploiting a sequential learning framework. Our now in giving analogous result although Winner - Take - All (WTA) hash family and naturally enjoys because where codice_6 future benefits of rank observable priority all taking configuration to promise high caliber at quite into code duration. We write first throughout kansas - seen - where - century hashing non-standard entire both federally by schoolwork extracellular, showing appeals very in yet nine related google makes.", "histories": [["v1", "Thu, 19 Mar 2015 21:34:33 GMT  (685kb,D)", "http://arxiv.org/abs/1503.05951v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["kai li", "guojun qi", "jun ye", "kien a hua"], "accepted": false, "id": "1503.05951"}, "pdf": {"name": "1503.05951.pdf", "metadata": {"source": "CRF", "title": "Rank Subspace Learning for Compact Hash Codes", "authors": ["Kai Li", "Guojun Qi", "Jun Ye", "Kien A. Hua"], "emails": ["KAILI@EECS.UCF.EDU", "GUOJUN.QI@UCF.EDU", "JYE@EECS.UCF.EDU", "KIENHUA@EECS.UCF.EDU"], "sections": [{"heading": "1. Introduction", "text": "Massive amount of social multimedia data are being generated by billions of users every day. The advent of mul-\ntimedia big data presents a number of challenges and opportunities for research and development of efficient storage, indexing and retrieval techniques. Hashing is recognized by many researchers as a promising solution to the above Big Data problem, thus attracting significant amount of research in the past few years (Datar et al., 2004; Kulis & Darrell, 2009; Tschopp & Diggavi, 2009; Weiss et al., 2009). Most hashing algorithms encode high-dimensional data into binary codes by quantizing numeric projections (Norouzi & Fleet, 2011; Liu et al., 2011; 2014; 2012). In contrast, hashing schemes based on feature\u2019s ranking order (i.e. comparisons) are relatively underresearched and will be the focus of this paper.\nRanking-based hashing, such as Winner-Take-All (WTA) (Yagnik et al., 2011) and Min-wise Hashing (MinHash) (Broder et al., 2000), ranks the random permutation of input features and uses the index of maximal/minimal feature dimensions to encode a compact representation of the input features. The benefit of ranking-based hashing lies in the fact that these algorithms are insensitive to the magnitude of features, and thus are more robust against many types of random noises universal in real applications ranging from information retrieval (Salakhutdinov & Hinton, 2007), image classification (Fan, 2013) to object recognition (Torralba et al., 2008). In addition, the magnitudeindependence also makes the resultant hash codes scaleinvariant, which is critical to compare and align the features from heterogeneous spaces, e.g., revealing the multi-modal correlations (Li et al., 2014).\nUnfortunately, the existing ranking-based hashing is dataagnostic. In other words, the obtained hash codes are not learned by exploring the intrinsic structure of data distribution, making it suboptimal in its efficiency of coding the input features with compact codes of minimal length. For\nar X\niv :1\n50 3.\n05 95\n1v 1\n[ cs\n.L G\n] 1\n9 M\nexample, WTA encodes the data with the indices of the maximum dimensions chosen from a number of random permutations of input features. Although WTA has generated leading performances in many tasks (Dean et al., 2013; Yagnik et al., 2011; Li et al., 2014), it is constrained in the sense that it only ranks the existing features of input data, while incapable of combining multiple features to generate new feature subspaces to rank. A direct consequence of such limitation is that this sort of ranking-based hashing usually needs a very large number of permutations and rankings to generate useful codes, especially with a high dimensional input feature space (Yagnik et al., 2011).\nTo address this challenge, we abandon the use of ranking random permutations of existing features in ranking-based hashing algorithms. Instead, we propose to generate compact ranking-based hashing codes by learning a set of new subspaces and ranking the newly projected features in these subspaces. At each step, an input data is encoded by the index of the maximal value over the projected points onto these subspaces. The subspace projections are jointly optimized to generate the ranking indices that are most discriminative to the metric structure and/or the data labels. Then a vector of codes are iteratively generated to represent the input data from the maximal indices over a sequence of sets of subspaces.\nThis method generalizes ranking-based hashing from restricted random permutations to perform encoding by ranking a set of arbitrary subspaces learned by mixing multiple original features. This greatly extends its flexibility so that much shorter bits can be generated to encode input data, while retaining the benefits of noise insensitivity and scale invariance inherent in such algorithms.\nIn the remainder of this paper, we first review the related hashing algorithms in Section 2, then the rank subspace hash learning problem is formulated and solved in Section 3. In Section 4, we present an improved learning algorithm based on sequential learning. The experimental results are presented in Section 5 and the paper is concluded in Section 6."}, {"heading": "2. Related Work", "text": "Since the focus of this paper is on data-dependent hashing, we limit our review to two categories of this line of research \u2013 unsupervised and supervised hashing. Interested reader can refer to (Bondugula, 2013) and (Wang et al., 2014) for a comprehensive review of this research area.\nThe most representative works on unsupervised hashing includes Spectral Hashing (Weiss et al., 2009) and kernelized variant Locality Sensitive Hashing (Kulis & Grauman, 2009). In detail, SH learns linear projections through an eigenvalue decomposition so that distance between pairs\nof similar training samples are minimized in the projected subspaces when these samples are binarized. Similarly, KLSH also makes use of an eigensystem solution, but it manipulates data items in the kernel space in an effort to generalize LSH to accommodate arbitrary kernel functions. Binary Reconstructive Embedding (BRE) (Kulis & Darrell, 2009) explicitly minimizes the reconstruction error between the input space and Hamming space to preserve the metric structure in input space, which has demonstrated improved performance over SH and LSH. Iterative Quantization (ITQ) (Gong & Lazebnik, 2011) iteratively learns uncorrelated hash bits to minimize quantization error between hash bodes and the dimension-reduced data. (Wang et al., 2012) presents a sequential projection learning method that fits the eigenvector solution into a boosting framework and uses pseudo labels in learning each hash bit. More recently, Anchor Graph Hashing (AGH) (Liu et al., 2011) and Discrete Graph Hashing (DGH) (Liu et al., 2014) use anchor graphs to capture the neighborhood structure inherent in a given dataset and adopt a discrete optimization procedure to achieve nearly balanced and uncorrelated hash bits.\nOn the other hand, supervised hashing methods take advantage of the data labels to learn data-dependent hash functions. It has been shown that supervised hashing methods are good at incorporating data labels to learn more discriminative hash codes in many tasks. For example, (Salakhutdinov & Hinton, 2007) uses Restricted Boltzman Machine (RBM) to learn nonlinear binary hash codes for document retrieval and demonstrates better precision and recall than other methods. Similar deep learning hash methods have also been applied to the task of image retrieval in very large databases (Torralba et al., 2008). However, deep learning methods typically need large data sets, cost long training times and have been outperformed by other methods exclusively designed for learning hashing functions. For instance, (Norouzi & Fleet, 2011) proposes the Minimal Loss Hashing (MLH) with a structural SVM-like formulation and minimizes the loss-adjusted upper bound of a hinge-like loss function defined on pairwise similarity labels. The resulting hash codes have shown to give superior performance over the state-of-the-art. This method is further extend to minimize loss functions defined with triplet similarity comparisons (Norouzi et al., 2012). Similarly, (Li et al., 2013) also learns hash functions based on triplet similarity. On the contrary, the formulation is a convex optimization within the large-margin learning framework rather than structural SVM.\nRecently, (Fan, 2013) theoretically proved the convergence properties of arbitrary sequential learning algorithms and proposed the Jensen Shannon Divergence (JSD) sequential learning method with a multi-class classification formulation. Supervised Hashing with Kernels (KSH) is another sequential learning algorithm. This method maps\nthe data to compact hash codes by minimizing Hamming distances of similar pairs and maximizing that of dissimilar pairs simultaneously (Liu et al., 2012). The sequential part of our algorithm is similar to Boosting Similarity Sensitive Coding (BSSC) (Shakhnarovich et al., 2003) and Forgiving Hash (FH) (Baluja & Covell, 2008), both of which treat each hash bit as a week classifier and learn a series of hash functions in a AdaBoost framework. However, the rank-based hash function we learn at each step is significantly different from that in BSSC and FH, resulting in completely different objective functions and learning steps. Existing hashing schemes based on rank orders (e.g. (Tschopp & Diggavi, 2009), (Pele & Werman, 2008) and (Ozuysal et al., 2007)) are mostly restricted to approximating nearest neighbors given a distance metric to speed up large scale lookup. To the best of our knowledge, there has been no previous work explicitly exploiting the rank-based hash functions in a supervised hash learning setting."}, {"heading": "3. Formulation", "text": ""}, {"heading": "3.1. Winner-Take-All Hashing", "text": "The WTA hashing is a subfamily of hashing functions introduced by (Yagnik et al., 2011). WTA is specified by two parameters: the number of random permutations L and the window size K. Each permutation \u03c0 rearranges the entries of an input vector x \u2208 Rd to x\u03c0 in the order specified by \u03c0. Then the index of the maximum dimension of the feature among the first K elements of x\u03c0 is used as the hash code. This process is repeatedL times, resulting in aK-nary hash code of length L, which can be compactly represented using L\u00d7 dlog2Ke bits.\nWTA is considered as a ranking-based hashing algorithm, which uses the rank order among permuted entries of a vector rather than their values of features. This property has given WTA certain degree of stability to perturbations in numeric values. Thus the WTA hash codes usually generate more robust metric structure to measure the similarity between input vectors than other types of hash codes which often contain inherent noises from quantizing the input feature spaces. With theoretical soundness, however, the hash codes generated by WTA often must be sufficiently long to represent the original data in high fidelity.\nThis is caused by twofold limitations: (1) the entries of input vectors are permuted in a random fashion before the comparison is applied to find the largest entry out of the first K ones; (2) the comparison and the ranking are restricted to be made between the original features. The random permutations are very inefficient to find the most discriminative entries to compare the similarity between the input vectors, and the restriction of the ranking to original features is too strong to generate the compact representa-\ntions. In the next, we relax the two limitations."}, {"heading": "3.2. Rank Subspace Hashing", "text": "Rather than randomly permuting the input data vector x, we project it onto a set of K one-dimensional subspaces. Then the input vector is encoded by the index of the subspace that generates the largest projected value. In other words, we have\nh(x;W) = arg max 1\u2264k\u2264K\nwTk x, (1)\nwhere wk \u2208 Rd, 1 \u2264 k \u2264 K are vectors specifying the subspace projections, and W = [w1,w2, \u00b7 \u00b7 \u00b7 ,wK ]T .\nWe use a linear projection to map an input vector into subspaces to form its hash code. At first glance, this idea is similar to the family of learning-based hashing algorithms based on linear projection (Datar et al., 2004; Norouzi & Fleet, 2011). However, different from these existing algorithms, the proposed method instead ranks the obtained subspaces to encode each input vector with the index of the dimension with the maximum value. This makes the obtained hash codes highly nonlinear to the input vector, invariant to the scaling of the vector, as well as insensitive to the input noises to a larger degree than the linear hashing codes. In this paper, we name this method Rank Subspace Hashing (RSH) to distinguish it from the other compared methods.\nWTA is a special case of the RSH algorithm, if we restrict the projections onto K axis-aligned linear subspaces, i.e., wk is set to a column vector ek randomly chosen from an identity matrix I of size d\u00d7 d.\nRSH extends WTA by relaxing the axis aligned linear subspaces in (1) to arbitrary K-dimensional linear subspaces in Rd. Such relaxation greatly increases the flexibility to learn a set of subspaces to optimize the hash codes resulting from the projections to these subspaces.\nNow our objective boils down to learn hash functions characterized by the projections W as in Eq. (1). Specifically, let D be the set of N d-dimensional data points {xi}Ni=1 and let S = {sij}1\u2264i,j\u2264N be the set of pair-wise similarity labels satisfying sij \u2208 {0, 1}, where sij = 1 means the pair (xi,xj) is similar and vice verse. The pair-wise similarity labels S can be obtained either from the nearest neighbors in a metric space or by human annotation that denotes whether a pair of data points come from the same class.\nGiven a similarity label sij for each training pair, we can define an error incurred by a hash function like (1) below\ne(hi, hj , sij) = { \u03c1I(hi 6= hj), sij = 1 \u03bb(1\u2212 I(hi 6= hj)), sij = 0\n(2)\nwhere I(\u00b7) is the indicator function outputting 1 when the condition holds and 0 otherwise, hi(j) is h(xi(j);W) for short, and \u03c1 and \u03bb are two hyper-parameters that penalize false negative and false positive respectively.\nThe learning objective is to find W to minimize the cumulative error function over the training set:\nE(W) = \u2211\n1\u2264i\u2264j\u2264N\ne(hi, hj , sij) (3)\nNote that W factors into the above objective function because both hi and hj are a function of W."}, {"heading": "3.3. Reformulation", "text": "The above objective function is straightforward to formulate, but hard to optimize because it involves the indicator function and arg max function which are typically nonconvex and highly discontinuous. Motivated by (Norouzi & Fleet, 2011), we reformulate the objective function and seek a piecewise linear upper bound of E(W).\nFirst, the hash function in (1) can be equivalently reformulated as\nh(x;W) = arg max g\ngTWx,\nsubject to g \u2208 {0, 1}K ,1Tg = 1, (4)\nwhich outputs an 1-of-K binary code h for an input feature vector x. The constraint enforces there must exist and only exist a nonzero entry of 1 in the resultant hash code. We enforce this constraint in the following optimization problems without meaning it explicitly to avoid notational clutter. It is easy to find the equivalence to the hashing function (1): the only nonzero element in h encodes the index of dimension with the maximum value in Wx.\nGiven a pairwise similarity label sij between two vectors xi and xj , hi and hj are their hash codes obtained by solving the arg max problem (i.e. h(xi;W) and h(xj ;W)). Then the error function (1) can be upper bounded by\ne(hi,hj , sij) \u2264max gi,gj [e(gi,gj , sij) + g T i Wxi + g T j Wxj ]\n\u2212 hTi Wxi \u2212 hTj Wxj\nThis inequality is easy to prove by noting that the following inequality\nmaxgi,gj [e(gi,gj , sij) + g T i Wxi + g T j Wxj ]\n\u2265 e(hi,hj , sij) + hTi Wxi + hTj Wxj\nWith the above upper bound of error function, we seek to solve the MinMax problem of minimizing the following\nfunction with respect to W \u2126(W) = \u2211\n1\u2264i<j\u2264N\n{max gi,gj [e(gi,gj , sij) + g T i Wxi + g T j Wxj ]\n\u2212 hTi Wxi \u2212 hTj Wxj}"}, {"heading": "3.4. Optimization", "text": "Consider W is fixed. The first step is a discrete optimization problem that is guaranteed to have global optimal solution. Specifically, given the values of Wxi(j), the RSH codes hi(j) in the second and third term of (5) can be found straightforwardly in O(K). For the adjusted error e(gi,gj , sij) + gTi Wxi + g T j Wxj of the first term in the square bracket, it is not hard to derive its maximum value can be obtained by scanning the elements in matrix [mkl]K\u00d7K , defined as\nmkl =\n{ y (k) i + y (l) j + \u03bb(1\u2212 sij) if k = l\ny (k) i + y (l) j + \u03c1sij otherwise\n(5)\nwhere y(k)i is the k th element of Wxi. Assuming the (k\u2217, l\u2217)th element of the above matrix achieves the maximum value, the maxima (g\u2217i ,g \u2217 j ) of the adjusted error are 1-of-K binary vectors with the k\u2217th and the l\u2217th dimension set to 1. The above procedure can be computed in O(K2). Since K is normally very small (e.g. 2 to 8), the above discrete optimization problem can be computed efficiently.\nNow consider the optimization of W. Fixing the maxima (g\u2217i ,g \u2217 j ) of the first term, and the RSH codes hi and hj in (4), W can be updated in the direction of the negative gradient\n\u2212 \u2202\u2126(W) W = \u2211 i,j (hi \u2212 g\u2217i )xTi + (hj \u2212 g\u2217j )xTj (6)\nBatch update can be made using (6) when the training data can be loaded into the memory all at once. Otherwise, W can also be done in an online fashion with one training pair at a time, leading to the following iterative learning procedure\nW\u2190W + \u03b7 [ (hi \u2212 g\u2217i )xTi + (hj \u2212 g\u2217j )xTj ] , (7)\nwhere \u03b7 is the learning rate.\nThe learning algorithm is shown as Algorithm 1. The algorithm learnsL projection matrices by starting with different random initializations from Gaussian distribution. Because the convex-concavity nature of the objective function, the solutions have multiple local minima. This is a desired property in our application, because each local minimum, corresponding to a RSH function, reflects a distinct perspective of ranked subspaces underlying the training examples. In addition, each hash function is learned independently and thus can be done in parallel. The convergence\nAlgorithm 1 Rank Subspace Learning Input: data [xi], pairwise similarity labels [sij ], length of hash code L, subspace dimension K for l = 1 to L do\nInitialize wk, 1 \u2264 k \u2264 K from Gaussian distribution repeat\nPick a pair (xi,xj) and compute hi, hj , g\u2217i , g \u2217 j Update projection matrix W according to W\u2190W + \u03b7[(hi \u2212 g\u2217i )xTi + (hj \u2212 g\u2217j )xTj ] until Convergence\nend for\nof the learning algorithm has been explored and empirically studied in (McAllester et al., 2010; Norouzi & Fleet, 2011)."}, {"heading": "4. The Sequential Learning", "text": "In Algorithm 1, since each hash function is learned independently, the entire hash code may be suboptimal. This is because different random starting points may lead to the same local minima, resulting in redundant hash bits. In order to maximize the information contained in a L-bit hash code, we propose to learn the hash functions sequentially so each hash function can provide complementary information to previous ones.\nIn order to motivate our sequential learning algorithm, we can view each hash bit as a week classifier that assigns similarity labels to an input pair, and the obtained ensemble classifier is related with the Hamming distance between hashing codes. Formally, each week classifier corresponding to the lth bit is\nsiml(xi,xj) = 1\u2212Hm(h(xi;Wl),h(xj ;Wl)) (8)\nWhere Hm(x, y) = I(x 6= y) is the bitwise Hamming distance, and Wl is the projection matrix for this bit. Then, the Hamming distance between two L-bit hash codes can be seen as the vote of an ensemble of L week classifiers on them. Clearly, the sequential learning problem naturally fits into the AdaBoost framework.\nThe AdaBoost-based sequential learning algorithm is shown in Algorithm 2. In detail, a sampling weight \u03b1(l)ij is assigned to each training pair and is updated before training each new hash function. In particular, pairs that are misclassified by the current hash function will be given more weight in training the next hash function. The projection matrix is updated in the similar online fashion as in (7) but weighted by the sampling weight.\nWhen all the hash functions have been trained, the voting results of the related week classifiers are fused with a\nAlgorithm 2 Sequential Rank Subspace Learning Input: data [xi], pairwise similarity labels[sij ], length of hash code L, subspace dimension K Initialize: set all the sampling weights {\u03b1ij} to 1 for l = 1 to L do\nInitialize Wl from Gaussian distribution repeat\nPick a pair (xi,xj) and compute hi, hj , g\u2217i , g \u2217 j based on the current estimate of Wl; Update projection matrix Wl according to Wl \u2190Wl + \u03b7\u03b1(l)ij [ (hi\u2212 g\u2217i )xTi + (hj \u2212 g\u2217j )xTj ] ;\nuntil Convergence Compute the weighted errors\nl =\n\u2211 i,j \u03b1\n(l) ij e(hi,hj , sij)\u2211 i,j \u03b1 (l) ij\nEvaluate the quantity\n\u03b8l = ln {1\u2212 l\nl } Update the pair weighting coefficients using\n\u03b1 (l+1) ij \u221d \u03b1 (l) ij exp{\u03b8le(hi,hj , sij)}\nNormalize the sampling weights such that\u2211 i,j \u03b1 (l+1) ij = \u2211 ij \u03b1 (l) ij .\nend for\nweighted combination\nsim(xi,xj) = L\u2211 l=1 \u03b8l(1\u2212Hm(h(xi;Wl),h(xj ;Wl)),\n(9) where \u03b8l are the weighted training error of the lth hash function.\nWe name this AdaBoost-inspired sequential learning by Sequential RSH (SRSH), in contrast to the RSH algorithm with independently composed hash codes."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Dataset and Compared Methods", "text": "In order to evaluate the proposed hashing approaches, Rank Subspace Hashing (RSH) and Sequential Rank Space Hashing (SRSH), we use three well-known datasets: LabelMe and Peekaboom, two collections of images represented as 512D Gist vectors designed for object recognition tasks; and MNIST, a corpus of handwritten digits in 24 \u00d7 24 greyscale image. The above datasets are assembled by (Kulis & Darrell, 2009) and also used in (Norouzi & Fleet, 2011).\nFollowing the settings of (Norouzi & Fleet, 2011), we randomly picked 1000 points for training and a separate set of 3000 points as test queries. The groundtruth neighbors for test queries are defined by thresholding the Euclidean distance such that each query point has an average of 50 neighbors. Similarly, we define the neighbors and non-neighbors of each data point in the training set in order to create the similarity matrix. All the datasets are mean-centered and normalized prior to training and testing. Some methods (e.g. SH) often perform better after dimensionality reduction, we therefore apply PCA to all datasets and retain the top 40 directions for a fair comparison.\nFor comparison, we choose several state-of-the-art methods: Minimal Loss Hashing (MLH (Norouzi & Fleet, 2011)), Spectral Hashing (SH (Weiss et al., 2009)), Locality Sensitive Hashing (LSH (Datar et al., 2004)), and Winner-Take-All (WTA (Yagnik et al., 2011)) hash. For MLH and SH, we use the publicly available source code provided by their original authors, while we implemented our own version of LSH and WTA since they are rather straightforward to implement. Those methods cover both supervised (e.g., MLH) and unsupervised (e.g., SH) hashing as well as data-agnostic ones (e.g., LSH and WTA), and\nare considered most representative in their own category."}, {"heading": "5.2. Methodology", "text": "In evaluating Approximate Nearest Neighbor (ANN) search, two methods are frequently adopted in the literature, that is, hash table based lookup and Hamming distance based kNN search. We use both methods in our evaluation. In hash table lookup, the hash code is used to index all the points in a database, and the data points with the same hash key fall into the same bucket. Typically, hash buckets that fall within a Hamming ball of radius R (i.e. the hash code differs by only 2 or 3 bits) of the target query are considered to contain relevant query results. A big advantage of hash table lookup lies in that it can be done in constant time. In contrast, Hamming distance based kNN search performs a standard kNN searching procedure based on Hamming distance which involves a linear scan of the entire database. However, since Hamming distance can be computed efficiently, the kNN search in Hamming space is also very fast in practice.\nIn our experiments, we evaluate the retrieval quality by setting R = 2 and 3 in the hash table lookup and k = 50 and 100 in Hamming distance based kNN search. For\nboth evaluation protocols, we compute the retrieval precision that is defined as the percentage of true neighbors among those returned by the query. The precision reflects the quality of hash codes to a large extent and it can be critical for many applications. In addition, we also evaluate the average precision for hash table lookup by varying R, which approximates the area under precision-recall curve. For all benchmarks (unless otherwise specified), we run every algorithm 10 independent times and report the mean and the standard deviation.\nAs for parameter settings, MLH requires a loss scaling factor , and two loss function hyper-parameters \u03c1 and \u03bb. We follow the practice of MLH and perform cross-validation on a number of combinations to get the best performance at each code length. Similarly, our algorithm also has three hyper-parameter, that is, the subspace dimension K and two error term hyper-parameters as defined in (2). A similar cross validation procedure is used to find the best model. For WTA, we use the polynomial kernel extension and set window size K = 4 and polynomial degree p = 4, as suggested by (Yagnik et al., 2011). SH and LSH are essentially parameter free and therefore do not require special handling."}, {"heading": "5.3. Results", "text": "Figure 1 shows the average precision using different hash code length. We aim to compare the performance of different hashing methods in generating compact hash code, therefore the code length is restricted below 64. It can be observed from the Figure 1 that the average precision of almost all methods increases monotonically when codes become longer, which is reasonable since longer codes retains more information of original data. The only exception to this trend is SH whose performance doesn\u2019t increase or even slightly drops after exceeding certain number of bits (e.g. 24 to 32). This can be explained by fact that unsupervised learning methods tend to overfit more easily with longer codes, which is consistent with the observation by (Wang et al., 2012).\nWe note that RSH shows significant improvement over WTA, another representative ranking-based hashing algorithm, as a result of the generalization of projection directions and the supervised learning process. Compared with RSH, SRSH further boosts the performance with large gains across all the tested datasets, demonstrating the effectiveness of the sequential learning method. In general, SRSH achieves the best performance, with about 10%\nlead over MLH. We also note that both of our algorithms demonstrate exceptional performance with extremely short code (e.g. of length less than 12) as a result of using rank order encoding.\nIn addition to the average precision, we also show a more detailed precision-recall profile when the code length L is fixed to 32 in Figure 2. In the precision-recall curve, better performance is shown by larger area under the curve. Again, both of our algorithms perform significantly better than WTA with SRSH consistently being the best, which is consistent with the previous results.\nThe results of hash table lookups are shown in Fig. 3 and Fig. 4, for R = 2 and R = 3 respectively. As explained in the previous section, precision alone is more critical than average precision that is an overall evaluation of both precision and recall. Therefore, the results in Figure 3 and Figure 4 can be more important for such applications. In those tests, rank order based techniques (i.e. WTA, RSH and SRSH) generally perform better than numeric value based hashing schemes because of certain degree of resilience to numeric noises/perturbations. For example, although both WTA and LSH are based on data-agnostic random methods, WTA clearly outperforms LSH for most of the tests, which is similar to the results obtained in ((Yagnik et al., 2011)). However, we find that WTA sometimes fails to retrieve any neighbor within a small Hamming ball, resulting in large standard deviation in precision at large code length (e.g. Fig. 3(b) and 4(b)). This is a natural result of applying randomness to a highly selective hash function. Such limitation is effectively addressed by providing certain supervision in obtaining the hash functions. Therefore, both RSH and SRSH produce more stable results than WTA, as demonstrated by the consistently smaller standard deviations. Overall, SRSH performs the best in all the tests, again demonstrating its superiority in generating high quality hash codes.\nThe last group of experiments is the Hamming distance based kNN search, where we evaluate the precision of true neighbors among the 50 and 100 nearest neighbors measured by Hamming distance. As shown in Figure 5 and Figure 6, the results are similar to those in the hash table lookups, except that there are no missed retrievals for any of the compared algorithms because all queries are guaranteed to return the specified number of results. The proposed algorithms both give competitive results as compared with the others."}, {"heading": "6. Conclusion", "text": "In this paper, a new reformulation of the Winner-Take-All hashing scheme is first presented. Based on this formulation, we propose a novel hash learning objective that aims\nto optimize a number of low-dimensional linear subspaces for high quality rank order-based hash encoding. A simple yet effective learning algorithm is then provided to optimize the objective function, leading to a number of optimal rank subspaces. The effectiveness of the proposed learning method in addressing the limitations of WTA is verified in a number of experiments. We also embed our learning method into a sequential learning framework that pushes the performance of the basic learning algorithm even further. Extensive experiments on several well-known datasets demonstrated our superior performance over state-of-theart."}], "references": [{"title": "Learning to hash: forgiving hash functions and applications", "author": ["Baluja", "Shumeet", "Covell", "Michele"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Baluja et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Baluja et al\\.", "year": 2008}, {"title": "Survey of hashing techniques for compact bit representations of images", "author": ["Bondugula", "Sravanthi"], "venue": "Ph.D. dissertation,,", "citeRegEx": "Bondugula and Sravanthi.,? \\Q2013\\E", "shortCiteRegEx": "Bondugula and Sravanthi.", "year": 2013}, {"title": "Min-wise independent permutations", "author": ["Broder", "Andrei Z", "Charikar", "Moses", "Frieze", "Alan M", "Mitzenmacher", "Michael"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Broder et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Broder et al\\.", "year": 2000}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Datar", "Mayur", "Immorlica", "Nicole", "Indyk", "Piotr", "Mirrokni", "Vahab S"], "venue": "In Proceedings of the twentieth annual symposium on Computational geometry,", "citeRegEx": "Datar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Datar et al\\.", "year": 2004}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["Dean", "Thomas", "Ruzon", "Mark A", "Segal", "Mark", "Shlens", "Jonathon", "Vijayanarasimhan", "Sudheendra", "Yagnik", "Jay"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Dean et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2013}, {"title": "Supervised binary hash code learning with jensen shannon divergence", "author": ["Fan", "Lixin"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Fan and Lixin.,? \\Q2013\\E", "shortCiteRegEx": "Fan and Lixin.", "year": 2013}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Gong", "Yunchao", "Lazebnik", "Svetlana"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Gong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2011}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["Kulis", "Brian", "Darrell", "Trevor"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kulis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2009}, {"title": "What\u2019s making that sound", "author": ["Li", "Kai", "Ye", "Jun", "Hua", "Kien A"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Learning hash functions using column generation", "author": ["Li", "Xi", "Lin", "Guosheng", "Shen", "Chunhua", "Hengel", "Anton van den", "Dick", "Anthony"], "venue": "arXiv preprint arXiv:1303.0339,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Hashing with graphs", "author": ["Liu", "Wei", "Wang", "Jun", "Kumar", "Sanjiv", "Chang", "Shih-Fu"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["Liu", "Wei", "Wang", "Jun", "Ji", "Rongrong", "Jiang", "Yu-Gang", "Chang", "Shih-Fu"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Direct loss minimization for structured prediction", "author": ["McAllester", "David", "Hazan", "Tamir", "Keshet", "Joseph"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "McAllester et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McAllester et al\\.", "year": 2010}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Norouzi", "Mohammad", "Fleet", "David J"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Norouzi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2011}, {"title": "Hamming distance metric learning", "author": ["Norouzi", "Mohammad", "Fleet", "David J", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Fast keypoint recognition in ten lines of code", "author": ["Ozuysal", "Mustafa", "Fua", "Pascal", "Lepetit", "Vincent"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Ozuysal et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ozuysal et al\\.", "year": 2007}, {"title": "Robust real-time pattern matching using bayesian sequential hypothesis testing", "author": ["Pele", "Ofir", "Werman", "Michael"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Pele et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pele et al\\.", "year": 2008}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["Shakhnarovich", "Gregory", "Viola", "Paul", "Darrell", "Trevor"], "venue": "In Computer Vision,", "citeRegEx": "Shakhnarovich et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Shakhnarovich et al\\.", "year": 2003}, {"title": "Small codes and large image databases for recognition", "author": ["Torralba", "Antonio", "Fergus", "Robert", "Weiss", "Yair"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Torralba et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2008}, {"title": "Approximate nearest neighbor search through comparisons", "author": ["Tschopp", "Dominique", "Diggavi", "Suhas"], "venue": "arXiv preprint arXiv:0909.2194,", "citeRegEx": "Tschopp et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tschopp et al\\.", "year": 2009}, {"title": "Hashing for similarity search: A survey", "author": ["Wang", "Jingdong", "Shen", "Heng Tao", "Song", "Jingkuan", "Ji", "Jianqiu"], "venue": "CoRR, abs/1408.2927,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Semisupervised hashing for large-scale search", "author": ["Wang", "Jun", "Kumar", "Sanjiv", "Chang", "Shih-Fu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Spectral hashing", "author": ["Weiss", "Yair", "Torralba", "Antonio", "Fergus", "Rob"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Weiss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2009}, {"title": "The power of comparative reasoning", "author": ["Yagnik", "Jay", "Strelow", "Dennis", "Ross", "David A", "Lin", "Ruei-sung"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Yagnik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yagnik et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "Hashing is recognized by many researchers as a promising solution to the above Big Data problem, thus attracting significant amount of research in the past few years (Datar et al., 2004; Kulis & Darrell, 2009; Tschopp & Diggavi, 2009; Weiss et al., 2009).", "startOffset": 166, "endOffset": 254}, {"referenceID": 22, "context": "Hashing is recognized by many researchers as a promising solution to the above Big Data problem, thus attracting significant amount of research in the past few years (Datar et al., 2004; Kulis & Darrell, 2009; Tschopp & Diggavi, 2009; Weiss et al., 2009).", "startOffset": 166, "endOffset": 254}, {"referenceID": 10, "context": "Most hashing algorithms encode high-dimensional data into binary codes by quantizing numeric projections (Norouzi & Fleet, 2011; Liu et al., 2011; 2014; 2012).", "startOffset": 105, "endOffset": 158}, {"referenceID": 23, "context": "Ranking-based hashing, such as Winner-Take-All (WTA) (Yagnik et al., 2011) and Min-wise Hashing (MinHash) (Broder et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 2, "context": ", 2011) and Min-wise Hashing (MinHash) (Broder et al., 2000), ranks the random permutation of input features and uses the index of maximal/minimal feature dimensions to encode a compact representation of the input features.", "startOffset": 39, "endOffset": 60}, {"referenceID": 18, "context": "The benefit of ranking-based hashing lies in the fact that these algorithms are insensitive to the magnitude of features, and thus are more robust against many types of random noises universal in real applications ranging from information retrieval (Salakhutdinov & Hinton, 2007), image classification (Fan, 2013) to object recognition (Torralba et al., 2008).", "startOffset": 336, "endOffset": 359}, {"referenceID": 8, "context": ", revealing the multi-modal correlations (Li et al., 2014).", "startOffset": 41, "endOffset": 58}, {"referenceID": 4, "context": "Although WTA has generated leading performances in many tasks (Dean et al., 2013; Yagnik et al., 2011; Li et al., 2014), it is constrained in the sense that it only ranks the existing features of input data, while incapable of combining multiple features to generate new feature subspaces to rank.", "startOffset": 62, "endOffset": 119}, {"referenceID": 23, "context": "Although WTA has generated leading performances in many tasks (Dean et al., 2013; Yagnik et al., 2011; Li et al., 2014), it is constrained in the sense that it only ranks the existing features of input data, while incapable of combining multiple features to generate new feature subspaces to rank.", "startOffset": 62, "endOffset": 119}, {"referenceID": 8, "context": "Although WTA has generated leading performances in many tasks (Dean et al., 2013; Yagnik et al., 2011; Li et al., 2014), it is constrained in the sense that it only ranks the existing features of input data, while incapable of combining multiple features to generate new feature subspaces to rank.", "startOffset": 62, "endOffset": 119}, {"referenceID": 23, "context": "A direct consequence of such limitation is that this sort of ranking-based hashing usually needs a very large number of permutations and rankings to generate useful codes, especially with a high dimensional input feature space (Yagnik et al., 2011).", "startOffset": 227, "endOffset": 248}, {"referenceID": 20, "context": "Interested reader can refer to (Bondugula, 2013) and (Wang et al., 2014) for a comprehensive review of this research area.", "startOffset": 53, "endOffset": 72}, {"referenceID": 22, "context": "The most representative works on unsupervised hashing includes Spectral Hashing (Weiss et al., 2009) and kernelized variant Locality Sensitive Hashing (Kulis & Grauman, 2009).", "startOffset": 80, "endOffset": 100}, {"referenceID": 21, "context": "(Wang et al., 2012) presents a sequential projection learning method that fits the eigenvector solution into a boosting framework and uses pseudo labels in learning each hash bit.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "More recently, Anchor Graph Hashing (AGH) (Liu et al., 2011) and Discrete Graph Hashing (DGH) (Liu et al.", "startOffset": 42, "endOffset": 60}, {"referenceID": 18, "context": "Similar deep learning hash methods have also been applied to the task of image retrieval in very large databases (Torralba et al., 2008).", "startOffset": 113, "endOffset": 136}, {"referenceID": 14, "context": "This method is further extend to minimize loss functions defined with triplet similarity comparisons (Norouzi et al., 2012).", "startOffset": 101, "endOffset": 123}, {"referenceID": 9, "context": "Similarly, (Li et al., 2013) also learns hash functions based on triplet similarity.", "startOffset": 11, "endOffset": 28}, {"referenceID": 11, "context": "the data to compact hash codes by minimizing Hamming distances of similar pairs and maximizing that of dissimilar pairs simultaneously (Liu et al., 2012).", "startOffset": 135, "endOffset": 153}, {"referenceID": 17, "context": "The sequential part of our algorithm is similar to Boosting Similarity Sensitive Coding (BSSC) (Shakhnarovich et al., 2003) and Forgiving Hash (FH) (Baluja & Covell, 2008), both of which treat each hash bit as a week classifier and learn a series of hash functions in a AdaBoost framework.", "startOffset": 95, "endOffset": 123}, {"referenceID": 15, "context": "(Tschopp & Diggavi, 2009), (Pele & Werman, 2008) and (Ozuysal et al., 2007)) are mostly restricted to approximating nearest neighbors given a distance metric to speed up large scale lookup.", "startOffset": 53, "endOffset": 75}, {"referenceID": 23, "context": "The WTA hashing is a subfamily of hashing functions introduced by (Yagnik et al., 2011).", "startOffset": 66, "endOffset": 87}, {"referenceID": 3, "context": "At first glance, this idea is similar to the family of learning-based hashing algorithms based on linear projection (Datar et al., 2004; Norouzi & Fleet, 2011).", "startOffset": 116, "endOffset": 159}, {"referenceID": 12, "context": "of the learning algorithm has been explored and empirically studied in (McAllester et al., 2010; Norouzi & Fleet, 2011).", "startOffset": 71, "endOffset": 119}, {"referenceID": 22, "context": "For comparison, we choose several state-of-the-art methods: Minimal Loss Hashing (MLH (Norouzi & Fleet, 2011)), Spectral Hashing (SH (Weiss et al., 2009)), Locality Sensitive Hashing (LSH (Datar et al.", "startOffset": 133, "endOffset": 153}, {"referenceID": 3, "context": ", 2009)), Locality Sensitive Hashing (LSH (Datar et al., 2004)), and Winner-Take-All (WTA (Yagnik et al.", "startOffset": 42, "endOffset": 62}, {"referenceID": 23, "context": ", 2004)), and Winner-Take-All (WTA (Yagnik et al., 2011)) hash.", "startOffset": 35, "endOffset": 56}, {"referenceID": 23, "context": "For WTA, we use the polynomial kernel extension and set window size K = 4 and polynomial degree p = 4, as suggested by (Yagnik et al., 2011).", "startOffset": 119, "endOffset": 140}, {"referenceID": 21, "context": "This can be explained by fact that unsupervised learning methods tend to overfit more easily with longer codes, which is consistent with the observation by (Wang et al., 2012).", "startOffset": 156, "endOffset": 175}, {"referenceID": 23, "context": "For example, although both WTA and LSH are based on data-agnostic random methods, WTA clearly outperforms LSH for most of the tests, which is similar to the results obtained in ((Yagnik et al., 2011)).", "startOffset": 178, "endOffset": 199}], "year": 2015, "abstractText": "The era of Big Data has spawned unprecedented interests in developing hashing algorithms for efficient storage and fast nearest neighbor search. Most existing work learn hash functions that are numeric quantizations of feature values in projected feature space. In this work, we propose a novel hash learning framework that encodes feature\u2019s rank orders instead of numeric values in a number of optimal low-dimensional ranking subspaces. We formulate the ranking subspace learning problem as the optimization of a piecewise linear convex-concave function and present two versions of our algorithm: one with independent optimization of each hash bit and the other exploiting a sequential learning framework. Our work is a generalization of the Winner-TakeAll (WTA) hash family and naturally enjoys all the numeric stability benefits of rank correlation measures while being optimized to achieve high precision at very short code length. We compare with several state-of-the-art hashing algorithms in both supervised and unsupervised domain, showing superior performance in a number of data sets.", "creator": "LaTeX with hyperref package"}}}