{"id": "1503.06745", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "Online classifier adaptation for cost-sensitive learning", "abstract": "In how paper, could propose the problem there subscribers cost - sensitive jessee - sifier adaptation through the first coding to solve it. We certainty how still given base handshape full once cost - rather defined mean, enough way particular trained into conscience whether hand costing work separate should same preference still. Moreover, think making them so medical data samples streaming the the deterministic each called actually. The prob - faraj is must inevitably entered even base classifier one as desired operating setting sometimes entire steam teams substances electronic. To learn certainly problem, too apply to appreciate a include referent by adding rather adaptation formula_10 to the base valva, over / the movies function waveform according could put streaming identification animals. Given whose impedance databases color to the cost of misclassifying it, we up - until next describes function parameter by considerations some higher materialize loss, respecting previous when formula_13 simultaneously. The proposed algorithm is increase to both online brought again - well cost - sensitive computational on two cost - sensitive classification already, for the experiments show however how not but outperforms them one layout performances, way. requires decreasing equally only ever.", "histories": [["v1", "Mon, 23 Mar 2015 17:47:00 GMT  (41kb)", "http://arxiv.org/abs/1503.06745v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["junlin zhang", "jose garcia"], "accepted": false, "id": "1503.06745"}, "pdf": {"name": "1503.06745.pdf", "metadata": {"source": "CRF", "title": "Online classifier adaptation for cost-sensitive learning", "authors": ["Junlin Zhang", "Jos\u00e9 Gar\u0107\u0131a"], "emails": ["junlinzhang1@yahoo.com", "josegarciajaumei@outlook.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n06 74\n5v 1\n[ cs\n.L G\n] 2\n3 M\nKeywords Cost-sensitive learning \u00b7 Classifier adaptation \u00b7 Online learning \u00b7 Fact detection \u00b7 Car detection\nJ. Zhang\u2217 Xi\u2019an University of Architecture and Technology, Xi\u2019an, Shaanxi 710055, P.R. China E-mail: junlinzhang1@yahoo.com \u2217J. Zhang is the corresponding author.\nJ. Garc\u0301\u0131a Department of Computer Languages and Systems, Universitat Jaume I, Av. Sos Baynat s/n, 12071 Castel\u0301\u0131o de la Plana, Spain E-mail: josegarciajaumei@outlook.com"}, {"heading": "1 Introduction", "text": "In pattern recognition problems, we try to design a classification function to predict the class label of a data sample, so that the misclassification errors of a set of training samples can be minimized [3,11,12,20,30,4]. A popular assumption for the learning of a classifier is that the loss of misclassifying any data sample in the training set is equal. However, in real-world applications, different misclassifications may result in significant different costs. For example, in the problem of breast cancer diagnosis, misclassifying a malignant tumor sample may cause much more cost than misclassifying a benign tumor sample. Thus it is necessary to take the costs of different types of misclassifications into account when a classifier is trained. This problem is named cost-sensitive learning in machine learning community [33,16,48,13,58]. Given the cost setting, i.e., costs of different misclassifications, the target of cost-sensitive learning is to train a classifier so that the cost of overall misclassification can be minimized. In cost-sensitive binary classification, we can have different costs for misclassifications of positive and negative samples. In this case, misclassifying a positive sample to a negative sample incorrectly may results much higher cost than misclassifying a negative sample to a positive sample. So we must design a classify to correctly classify most of the positive samples, while allow some misclassification of negative samples. In this way, the overall misclassification cost can be minimized. Lots of cost-sensitive learning algorithms have been proposed to take account of different misclassification costs. For example, Zhou et al. [58] proposed to train cost-sensitive neural networks by using technologies of sampling and threshold-moving (STM), so that the distribution of the training data samples can be modify, and the costs of different types of misclassifications can be conveyed by the appearances of the examples. Sun et al. [32] provided a comprehensive analysis of the AdaBoost algorithm regarding its application in the class imbalance problem, and developed three cost-sensitive boosting algorithms (CSB), by introducing cost items into the learning framework of AdaBoost. Masnadi-Shirazi and Vasconcelos [25] also proposed a AdaBoost-based cost-sensitive learning algorithm (ABC) to design cost-sensitive boosting algorithms, by considering two necessary conditions for optimal cost-sensitive learning, which are the minimization of expected losses by optimal cost-sensitive decision rules, and the minimization of empirical loss to emphasize the neighborhood of the desired cost-sensitive boundary. Ting [34] introduced a sample-weighting method (SW) to induce cost-sensitive trees, by generalizing the standard tree induction process and initial instance weights determine the type of tree to be induced-minimum error trees or minimum high cost error trees. Chen eta al. [10] proposed a supervised learning algorithm fast flux discriminant, for large-scale nonlinear cost-sensitive classification problems, by decomposing the kernel density estimation in the original feature space into selected low-dimensional subspaces. This method archives the efficiency, interpretability and accuracy simultaneously, and meanwhile it is also sparse and naturally handles mixed data types.\nWith rapid development of internet technology, more and more data is generated continuously, and the training set of data samples is been increased every day with new data samples added to the training set. Moreover, the cost setting can also be changed from time to time. This proposed tow new challenges to the cost sensitive learning problems:\n1. When the cost setting is changed, the learned classifier cannot be adapted to the new cost setting. A possible strategy to solve this problem is to learn a new classifier according to the new cost setting using the entire training set from the very beginning and ignores the previous learned classifier with previous cost setting. However this strategy is time-consuming, especially when the training set is large. When we already have a classifier learned according to a cost setting, can we utilize it to learn another classifier with regard to a different cost setting? This problem is defined as classifier adaption. Actually, classifier adaptation has been applied to performance measures optimization [21] and cross-domain learning [47]. In [21], a classifier is learned to optimize a performance measure, and then adapted to optimize another performance measure, while in [47], a classifier is learned from a domain, and then adapted to a different domain. In this paper, we propose the problem of adapting a learned classifier to a different cost setting. 2. When the data samples are generated and added to the training set one by one, the transitional cost sensitive learning methods cannot be applied, since they assumes that the entire training set is given to the algorithm once. Recently, cost-sensitive online classification (CSOC) method was proposed by Wang et al. [39]. This method takes the training set one by one and update the cost sensitive classifier online [29,49,14]. However, CSOC is also constrained to fixed cost setting. When a cost setting is given, it learn a new classifier online, and ignores the other classifiers learned with different cost settings. Can we learn a classifier from a base classifier trained with different cost sensitive setting online? This problem remains an open problem.\nTo solve the above two problems simultaneously, in this paper, we propose the first online cost-sensitive classier adaption method. We assume that we have a existed cost-sensitive classifier, and we try to adapt it to another classifier with regard to a different cost setting, with help of data samples appearing one by one in an online way. The adaptation is implemented by adding an adaptation function, and the it is learned by updating the adaptation function parameter with the coming training samples with different misclassification costs. We construct an objective function by considering the respecting previous learned and minimizing cost weighted hinge loss with coming training samples. By solving the objective function with a gradient descent method and we develop an iterative algorithm. The contributions of this paper are of two folds:\n1. We proposed the problem of online cost-sensitive classifier adaptation.\n2. We proposed a novel algorithm to solve this problem.\nThe rest parts of this paper are organized as follows: in Section 2, we introduce the proposed novel method. In Section 3, the proposed method is evaluated on some benchmark data sets. In Section 4, the paper is concluded with some future works.\n2 Proposed Method\n2.1 Problem Formulation\nIn this paper, instead of learning a novel cost-sensitive classifier from the given training set and the cost setting, we hope to use the existed classifier by employing the framework of classifier adaptation to learn the cost-sensitive classifier effectively. Suppose that we already have a classifier f0(x) learned without consider the different costs of misclassifications of positive and negative samples, or a classifier learned with different cost setting, we want to adapt it to a problem with a new cost setting. To this end, we construct a new classifier f(x) by adding a linear adaptation function w\u22a4x to f0(x), i.e.\nf(x) = f0(x) +w \u22a4x (1)\nwhere w \u2208 Rd is the adaptation function parameter. Please note that f0(x) can be any type of classifier, for example, SVM, Adaboost, etc. In this way, we transfer the problem of cost-sensitive classifier adaptation to the learning of w.\nIn the traditional cost-sensitive learning problem, a training data set composed of many positive and negative training samples are given. The cost factors of misclassification of positive and negative samples are denoted as C+ and C\u2212 respectively. Please note that when we train f0(x), C+ and C\u2212 are set to different values. The target of cost-sensitive learning is to learn a classifier which could minimize the overall cost of misclassification of the training samples. However, in the online learning scene, we do not have the entire training data set during the training procedure. Instead, the training data samples are given sequentially, and the algorithm is run in an iterative way. In each iteration, only one training sample is given, and the classifier is updated only with regard to this training sample. In the t-th iteration, we assume that wa have a training sample (xt, yt), where xt \u2208 R\nd is its d-dimensional feature vector, and yt \u2208 {+1,\u22121} is its corresponding class label. The corresponding misclassification cost is also given as Ct,\nCt =\n{\nC+,if yi = +1; C\u2212,if yi = \u22121. (2)\nMoreover, we also assume that we already learned an adaptation function parameter from the previous iteration wt\u22121. To update w, we consider the following two problems.\n\u2013 Respecting previous learned wt\u22121: To make the learned w consistent, we hope the updated wt to respect the previous wt\u22121. To this end, we minimize the squared \u21132 distance between them,\nmin w\n1 2 \u2016w\u2212wt\u22121\u2016 2 2 . (3)\n\u2013 Minimizing Cost Weighted Hinge Loss: To measure the loss of misclassification, we apply the hinge loss function to (xt, yt), which is defined as\nL(yt, f(xt)) = max(0, 1\u2212 ytf(xt))\n= max ( 0, 1\u2212 yt ( f0(xt) +w \u22a4xt )) . (4)\nSince positive and negative samples have different misclassification costs, we weight the hinge loss of the t-th sample by its corresponding cost factor Ct, and minimize the weighted loss,\nmin w\nCt \u00d7max(0, 1\u2212 yt ( f0(xt) +w \u22a4xt ) ) (5)\nBy introducing a nonnegative slack variable \u03be, the optimization problem with a cost weighted hinge loss is transferred to\nmin w,\u03be Ct\u03be,\ns.t. 1\u2212 yt ( f0(xt) +w \u22a4xt ) \u2264 \u03be, 0 \u2264 \u03be. (6)\nHere the cost factor Ct is similar to the penalty factor of SVM. However, we must note that this penalty factor is cost-sensitive.\nBy considering the problems in (3) and (6) simultaneously, we obtain the optimization problem for the updating of w in the t-th iteration,\n(wt, \u03bet) = argmin w,\u03be\n1 2 \u2016w\u2212wt\u22121\u2016 2 2 + \u03b1Ct\u03be,\ns.t. 1\u2212 yt ( f0(xt) +w \u22a4xt ) \u2264 \u03be, 0 \u2264 \u03be.\n(7)\nwhere \u03b1 is a tradeoff parameter, and it is chosen by cross-fold validation on a training set. By solving this problem, we can obtain an adaptation function parameter wt with regard to the training sample input in the t-th iteration. We should note that the obtained wt is learned from both the previous wt\u22121 and the training sample (xt, yt). Most importantly, the updating of wt relies on the cost of misclassification cost of (xt, yt) by considering the cost factor as a loss weight. When different samples come, different cost factor is used and the hinge loss is weighted correspondingly.\n2.2 Optimization\nTo optimize the objective function in (7), we use the Lagrange multiplier method. The Lagrange function is\nL(w, \u03be, \u03c4, \u03bb) = 1\n2 \u2016w\u2212wt\u22121\u2016\n2 2 + \u03b1Ct\u03be\n+ \u03c4 ( 1\u2212 yt ( f0(xt) +w \u22a4xt ) \u2212 \u03be ) \u2212 \u03bb\u03be. (8)\nwhere \u03c4 is the nonnegative Lagrangemultiplier for the constrain of 1\u2212yt ( f0(xt) +w \u22a4xt )\n\u2264 \u03be, and \u03bb is the nonnegative Lagrange multiplier for the constrain of 0 \u2264 \u03be. According to the dual theory of optimization, the minimization of (7) can be achieved by solving the following dual problem,\nmax \u03c4,\u03bb min w,\u03be L(w, \u03be, \u03c4, \u03bb)\ns.t. \u03c4 \u2265 0, \u03bb \u2265 0. (9)\nTo solve this problem, we set the divertive of the Lagrange function L(w, \u03be, \u03c4, \u03bb) with regard to w to zero, and we have\n\u2202L \u2202w =(w\u2212wt\u22121)\u2212 \u03c4ytxt = 0 \u21d2 w = wt\u22121 + \u03c4ytxt. (10)\nRemark: The motivation of setting the divertive of the Lagrange function with regard to w to zero is to solve the inner minimization problem. In equation (9), the problem is coupled with two optimization problems, which is a inner minimization problem, and an outer maximization problem. To solve this coupled problem, our strategy is first solving the inner problem, and then substituting the results to the objective to solve the outer problem. According to the optimization theory, the minimization of L with regard to w is reached at a solution making its divertive zero, thus we should set the divertive of the L with regard to w to zero to obtain the optimal w.\nMoreover, we also set its divertive with regard to \u03be to zero, and obtain\n\u2202L \u2202\u03be =\u03b1Ct \u2212 \u03c4 \u2212 \u03bb = 0 \u21d2 \u03b1Ct \u2212 \u03c4 = \u03bb \u2265 0 \u21d2 \u03c4 \u2264 \u03b1Ct. (11)\nSubstituting results of both (10) and (11) to the Lagrange function in (8), we can rewrite it as the function of only variable \u03c4 ,\nL(\u03c4) = 1\n2 \u2016\u03c4ytxt\u2016\n2 2 + \u03c4\n[\n1\u2212 yt\n(\nf0(xt) + (wt\u22121 + \u03c4ytxt) \u22a4 xt\n)]\n= 1\n2 \u03c42x\u22a4t xt + \u03c4\n[ 1\u2212 yt ( f0(xt) +w \u22a4 t\u22121xt )] \u2212 \u03c42x\u22a4t xt\n=\u2212 1\n2 \u03c42x\u22a4t xt + \u03c4\n[ 1\u2212 yt ( f0(xt) +w \u22a4 t\u22121xt )] .\n(12)\nBy setting the divertive of L(\u03c4) with regard to \u03c4 to zero, we have the initial solution of \u03c4 ,\n\u2202L(\u03c4)\n\u2202\u03c4 = \u2212\u03c4x\u22a4t xt +\n[ 1\u2212 yt ( f0(xt) +w \u22a4 t\u22121xt )] = 0\n\u21d2 \u03c4 = 1\u2212 yt\n(\nf0(xt) +w \u22a4 t\u22121xt\n)\nx\u22a4t xt .\n(13)\nMoreover, we should also note that in (8), we have a constrain \u03c4 \u2265 0, and in (11) we have another constrain \u03c4 \u2264 \u03b1Ct. Thus the solution of \u03c4 must fall in the following range,\n0 \u2264 \u03c4 \u2264 \u03b1Ct. (14)\nIn this way, the solution of \u03c4 t can be obtained by discussing the following three cases:\n1. Case I: When 1\u2212yt(f0(xt)+w\u22a4t\u22121xt)\nx \u22a4\nt xt\n\u2264 0, the solution of \u03c4t is\n\u03c4t = 0, (15)\nso that the constrain \u03c4 \u2265 0 can be satisfied.\n2. Case II: When 0 < 1\u2212yt(f0(xt)+w\u22a4t\u22121xt)\nx \u22a4\nt xt\n\u2264 \u03b1Ct, the solution of \u03c4t is\n\u03c4t = 1\u2212 yt\n(\nf0(xt) +w \u22a4 t\u22121xt\n)\nx\u22a4t xt , (16)\nso that the minimization of (7) can be archived.\n3. Case III: When \u03b1Ct < 1\u2212yt(f0(xt)+w\u22a4t\u22121xt)\nx \u22a4\nt xt\n, we have the solution of \u03c4t as\n\u03c4t = \u03b1Ct, (17)\nso that the constrain \u03c4t \u2264 \u03b1Ct can be satisfied.\nAfter \u03c4t is determined, we can then update wt using the result in (10) as follows,\nwt = wt\u22121 + \u03c4tytxt. (18)\nIt could be note that the new classifier adaptation function parameter is obtained by adding a bias term ytxt determined by the t-th sample to the previous wt\u22121. The bias term is weighted by a Lagrange multiplier \u03c4t which is further controlled by the cost factor of the t-th sample.\n2.3 Algorithm\nBased on the optimization results, we can develop an online cost-sensitive classifier adaptation algorithm which can take training samples one by one. The algorithm takes an initial classifier f(x) as an input, and operates on a iterative way. In each iteration, one new training sample is input to update the classifier adaptation function parameter, based on the updating rules in (15) - (17), and (18).\nAlgorithm 1 Online Cost-Sensitive Classifier Adaptation algorithm (OCSCA).\nInput: An initial classifier function f0(x); Input: Tradeoff parameter \u03b1; Initialize t = 0 and w0 = 0 while A new training sample (xt, yt) with its corresponding misclassification cost Ct is input do\nCompute the initial solution of Lagrange multiplier \u03c4 as\n\u03c4 \u2032 t =\n1\u2212 yt ( f0(xt) +w\u22a4t\u22121xt )\nx\u22a4 t xt\n. (19)\nUpdate \u03c4t as\n\u03c4t =\n  \n \n0,if \u03c4 \u2032 t \u2264 0,\n\u03c4 \u2032 t ,if 0 < \u03c4 \u2032 t \u2264 \u03b1Ct\n\u03b1Ct,if \u03c4 \u2032 t > \u03b1Ct.\n(20)\nUpdate wt as\nwt = wt\u22121 + \u03c4tytxt. (21)\nUpdate t = t+ 1; end while Output: Output the learned cost-sensitive classifier function f(x) = f0(x) +wt\u22121\u22a4x"}, {"heading": "3 Experiments", "text": "In this section, we studied the proposed algorithm experimentally.\n3.1 Data sets\nIn the experiments, we used two cost-sensitive learning data sets, which are introduced as follows."}, {"heading": "3.1.1 Face detection data set", "text": "The first data set is a face detection data set used in [35]. This data set is a large data set, and it contains 9832 face images and 9832 non-face images. Each face image is treated as a positive sample, while each non-face image is treated as a negative sample. Moreover, each image is represented as 50,000 dimensional visual feature vector. The problem of face detection is to classify a given candidate image to face or non-face. Moreover, we set the cost of misclassifying a face to non-face as 5, and that of misclassifying a non-face to fact to 1."}, {"heading": "3.1.2 Car detection data set", "text": "The second data set we used is a car detection data set [1]. This data set contains 500 car images and 500 non-car images. The problem of car detection\nis to classify a given candidate image to car or non-car. In this problem car images are defined as positive images, and the non-car images are defined as negative images. In this case, we set the cost of misclassification of a car image to 8, and that of a non-car image to 1.\n3.2 Experiment setup\nTo conduct the experiment, we used the 10-fold cross validation. An entire data set was split into 10 folds randomly, and then each set is used as a test set, while the remaining 9 sets were combined as a training set. Moreover, since the proposed method is based on the adaption of a classifier f0(x) trained with different cost setting, we further split the training set to two subsets. The first subset contains 2 folds, and we used it to train f0(x) with different cost setting. For the first data set, we used the cost setting of C+ = 2 and C\u2212 = 1 to train f0(x), and for the second data set, used C+ = 3 and C\u2212 = 1. The second subset contains 7 folds, and we used it to learn w using the proposed online learning algorithm, by inputting the training samples of the second training subset to the algorithm one by one.\nThe classification performances were measured by the average classification accuracies and the average misclassification costs. They are defined as follows,\nAverage classification accuracy =\n\u2211\ni:xin\u2208T I(yi = y \u2217 i )\n\u2211\ni:xi\u2208T 1\n,\nAverage misclassification cost =\n\u2211\ni:xin\u2208T CiI(yi 6= y \u2217 i )\n\u2211\ni:xi\u2208T 1\n,\n(22)\nwhere T is the test set, y\u2217i is the predicted class label, and I(yi = y \u2217 i ) = 1 if yi = y \u2217 i , and 0 otherwise.\n3.3 Results\nWe first compared the purposed online cost sensitive learning algorithm based on classifier adaptation to an online cost sensitive learning algorithm without considering the existed classifier f0(x), and then compared it to some transitional cost sensitive learning algorithm. Because the proposed algorithm is the only method that can take advantage of f0(x), for fear comparison, when we used the other algorithm, both f0(x) and the 2 folds in the training set used to train f0(x) were ignored."}, {"heading": "3.3.1 Comparison to online cost sensitivity classification method", "text": "The boxplots of the classification accuracies and misclassification of the proposed online cost-sensitive classifier adaptation algorithm (OCSCA) and the CSOC algorithm over 10-fold cross validation are given in Fig. 1. From this\nfigure, we can see that the proposed method outperforms the CSOC algorithm on both average accuracy and misclassification cost. Especially in the case of misclassification cost, the proposed algorithm achieves completely lower average misclassification cost then CSOC. This is because the proposed method takes advantage of an existing predictor learned from more data points by adapting it to a given cost setting. Even the existing predictor is learned according to a different cost setting. This is an strong evidence of the fact that classifier adaptation can benefit cost sensitive learning."}, {"heading": "3.3.2 Comparison to off-line cost sensitivity classification method", "text": "We also compare the proposed OCSCA to four most popular off-line costsensitive learning algorithms, which are STM proposed by Zhou et al. [58], CSB proposed by Sun et al. [32], ABC proposed by Masnadi-Shirazi and Vas-\nconcelos [25], and SW proposed by Ting [34]. The boxplots the classification accuracies and misclassification costs are given in Fig. 2. It is clear that in both the two figures, the proposed algorithm outperforms the compared algorithms on both classification accuracies and misclassification costs. The outperforming is even more significant on the misclassification costs. A main reason for this phenomenon lies on the fact that the proposed OCSCA algorithm starts learning from a base classifier f0(x), and then adapt it to the given cost setting via a training set, while the rest algorithms ignores f0(x) and directly learn the classifier from the training set. This means using a base classifier and adapting it to a training set can significantly boost the performance of cost-sensitive learning. Moreover, among the compared algorithms, it seems ABC and CSB performs slightly better than the other two ones. A possible reason is that they use the formula of Adaboost algorithm [31,26,2,9], which performs well on detection problems."}, {"heading": "3.3.3 Running time", "text": "An important advantage of the proposed OCSCA algorithm is its low time complexity compared to off-line algorithms. Thus we also compared the running time of these methods and the results are given in Fig. 3. It is obverse that the running time of the two online learning algorithms OCSCA and CSOC is much less than that of the off-line learning algorithms. Both of OCSCA and CSOC take less than 200 seconds, while all the off-line learning algorithms take more than 800 seconds. This is not surprising because in each iteration, OCSCA and CSOC update the classifier using only one data sample, while the off-line learning algorithms needs to consider all the training samples."}, {"heading": "4 Conclusions and future works", "text": "In this paper, we propose the problem of adapting an existing base classifier to a cost-sensitive classification problem. The base classifier is trained using different cost settings. Moreover, we proposed a novel online learning algorithm for the adaptation of the classifier. The algorithm takes one data sample at one time to update the adaptation parameter. The advantages of this method are of two folds:\n1. It can use the base classifier to boost the classification performance, and 2. its running time is low due to its online learning nature.\nIn this work, we used the SVM as the formulation of learning. In the future, we will study other classification methods, such as Adaboost. We will design an iterative algorithm to learn the classifier online by adapting an existing classifier trained with a different cost setting, and the adaptation function is an combination of some candidate weak classifiers. In each iteration, we have select a weak classifier according to the classification cost of the coming data point, and update its weight. Moreover, the loss function of Adaboost will be\nmodified to consider the classification costs. Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60]."}], "references": [{"title": "Learning to detect objects in images via a sparse, partbased representation", "author": ["S. Agarwal", "A. Awan", "D. Roth"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 26(11), 1475\u20131490", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Neighborhood guided smoothed emphasis for real adaboost ensembles", "author": ["A. Ahachad", "A. Omari", "A. Figueiras-Vidal"], "venue": "Neural Processing Letters", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Algorithm selection based on exploratory landscape analysis and cost-sensitive learning", "author": ["B. Bischl", "O. Mersmann", "H. Trautmann", "M. Preuss"], "venue": "GECCO\u201912 - Proceedings of the 14th International Conference on Genetic and Evolutionary Computation, pp. 313\u2013320", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Pairwise optimization of bayesian classifiers for multi-class cost-sensitive learning", "author": ["C. Charnay", "N. Lachiche", "A. Braud"], "venue": "Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI, pp. 499\u2013505", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Backend dielectric reliability simulator for microprocessor system", "author": ["C.C. Chen", "F. Ahmed", "D.H. Kim", "S.K. Lim", "L. Milor"], "venue": "Microelectronics Reliability 52(9), 1953\u20131959", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Impact of nbti/pbtion srams within microprocessor systems: Modeling, simulation, and analysis", "author": ["C.C. Chen", "F. Ahmed", "L. Milor"], "venue": "Microelectronics Reliability 53(9), 1183\u2013 1188", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Simulation of system backend dielectric reliability", "author": ["C.C. Chen", "M. Bashir", "L. Milor", "D.H. Kim", "S.K. Lim"], "venue": "Microelectronics Journal 45(10), 1327\u20131334", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Microprocessor aging analysis and reliability modeling due to back-end wearout mechanisms", "author": ["C.C. Chen", "L. Milor"], "venue": "Very Large Scale Integration (VLSI) Systems, IEEE Transactions on PP(99), 1\u20131", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Driving behavior analysis of multiple information fusion based on adaboost", "author": ["S.H. Chen", "J.S. Pan", "K. Lu", "H. Xu"], "venue": "Advances in Intelligent Systems and Computing 329, 277\u2013285", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast flux discriminant for large-scale sparse nonlinear classification", "author": ["W. Chen", "Y. Chen", "K.Q. Weinberger"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 621\u2013630. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Practical selection of svm parameters and noise estimation for svm regression", "author": ["V. Cherkassky", "Y. Ma"], "venue": "Neural networks 17(1), 113\u2013126", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Pattern classification models for classifying and indexing audio signals", "author": ["P. Dhanalakshmi", "S. Palanivel", "V. Ramalingam"], "venue": "Engineering Applications of Artificial Intelligence 24(2), 350\u2013357", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "The foundations of cost-sensitive learning", "author": ["C. Elkan"], "venue": "IJCAI International Joint Conference on Artificial Intelligence, pp. 973\u2013978", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Online learning system for biomedical engineering", "author": ["G. Fonseca", "J. Destro-Filho", "T. Vieira Borges"], "venue": "2007 IWSSIP and EC-SIPMCS - Proc. 2007 14th Int. Workshop on Systems, Signals and Image Processing, and 6th EURASIP Conf. Focused on Speech and Image Processing, Multimedia Communications and Services, pp. 169\u2013172", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Kinship measurement on salient facial features", "author": ["G.Guo", "X.Wang"], "venue": "IEEE Transactions on Instrumentation and Measurement 61(8), 2322\u20132325", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning cost-sensitive active classifiers", "author": ["R. Greiner", "A. Grove", "D. Roth"], "venue": "Artificial Intelligence 139(2), 137\u2013174", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Trusted Block as a Service: Towards Sensitive Applications on the Cloud", "author": ["J. Hao", "W. Cai"], "venue": "Proceedings of the 10th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, pp. 73\u201382", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Measuring Information Exposure Attacks on Interest Management", "author": ["J. Hao", "W. Cai"], "venue": "Proceedings of the 2012 ACM/IEEE/SCS 26th Workshop on Principles of Advanced and Distributed Simulation, PADS \u201912, pp. 133\u2013144", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "vTRUST: A Formal Modeling and Verification Framework for Virtualization Systems", "author": ["J. Hao", "Y. Liu", "W. Cai", "G. Bai", "J. Sun"], "venue": "Formal Methods and Software Engineering, vol. 8144, pp. 329\u2013346", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-cost sensitive learning with submodular trees of classifiers", "author": ["M. Kusner", "W. Chen", "Q. Zhou", "Z. Xu", "K. Weinberger", "Y. Chen"], "venue": "Proceedings of the National Conference on Artificial Intelligence, vol. 3, pp. 1939\u20131945", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient optimization of performance measures by classifier adaptation", "author": ["N. Li", "I. Tsang", "Z.H. Zhou"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35(6), 1370\u20131382", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-aligned harmonic volumetric mapping using mfs", "author": ["X. Li", "H. Xu", "S. Wan", "Z. Yin", "W. Yu"], "venue": "Computers & Graphics 34(3), 242\u2013251", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Symmetry and template guided completion of damaged skulls", "author": ["X. Li", "Z. Yin", "L. Wei", "S. Wan", "W. Yu", "M. Li"], "venue": "Computers & Graphics 35(4), 885\u2013893", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Structure design of vascular stents", "author": ["Y. Liu", "J. Yang", "Y. Zhou", "J. Hu"], "venue": "Multiscale Simulations and Mechanics of Biological Materials pp. 301\u2013317", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Cost-sensitive boosting", "author": ["H. Masnadi-Shirazi", "N. Vasconcelos"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 33(2), 294\u2013309", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaboost based multi-instance transfer learning for predicting proteome-wide interactions between salmonella and human proteins", "author": ["S. Mei", "H. Zhu"], "venue": "PLoS ONE 9(10)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "A study of reinforcement learning in a new multiagent domain", "author": ["H. Min", "J. Zeng", "J. Chen", "J. Zhu"], "venue": "Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02, WI-IAT \u201908, pp. 154\u2013161. IEEE Computer Society, Washington, DC, USA", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Fuzzy cmac with automatic state partition for reinforcementlearning", "author": ["H. Min", "J. Zeng", "R. Luo"], "venue": "Proceedings of the First ACM/SIGEVO Summit on Genetic and Evolutionary Computation, GEC \u201909, pp. 421\u2013428. ACM, New York, NY, USA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-supervised learning algorithm for online electricity data streams", "author": ["P. Patil", "Y. Fatangare", "P. Kulkarni"], "venue": "Advances in Intelligent Systems and Computing 324, 349\u2013358", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimizing the ground sample collection with cost-sensitive active learning for tree species classification using hyperspectral images", "author": ["C. Persello", "M. Dalponte", "T. Gobakken", "E. Naesset"], "venue": "International Geoscience and Remote Sensing Symposium (IGARSS), pp. 2091\u20132094", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Cost-sensitive adaboost algorithm for ordinal regression based on extreme learning machine", "author": ["A. Riccardi", "F. Fernndez-Navarro", "S. Carloni"], "venue": "IEEE Transactions on Cybernetics 44(10), 1898\u20131909", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Cost-sensitive boosting for classification of imbalanced data", "author": ["Y. Sun", "M.S. Kamel", "A.K. Wong", "Y. Wang"], "venue": "Pattern Recognition 40(12), 3358\u20133378", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Cost-sensitive learning of classification knowledge and its applications in robotics", "author": ["M. Tan"], "venue": "Machine Learning 13(1), 7\u201333", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1993}, {"title": "An instance-weighting method to induce cost-sensitive trees", "author": ["K.M. Ting"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 14(3), 659\u2013665", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "International journal of computer vision 57(2), 137\u2013154", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient spherical parametrization using progressive optimization", "author": ["S. Wan", "T. Ye", "M. Li", "H. Zhang", "X. Li"], "venue": "Computational Visual Media, pp. 170\u2013177. Springer", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "A topology-preserving optimization algorithm for polycube mapping", "author": ["S. Wan", "Z. Yin", "K. Zhang", "H. Zhang", "X. Li"], "venue": "Computers & Graphics 35(3), 639\u2013649", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "An effective image representation method using kernel classification", "author": ["H. Wang", "J. Wang"], "venue": "Tools with Artificial Intelligence (ICTAI), 2014 IEEE 26th International Conference on, pp. 853\u2013858", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Cost-sensitive online classification", "author": ["J. Wang", "P. Zhao", "S. Hoi"], "venue": "Proceedings - IEEE International Conference on Data Mining, ICDM, pp. 1140\u20131145", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Computational modeling of magnetic nanoparticle targeting to stent surface under high gradient field", "author": ["S. Wang", "Y. Zhou", "J. Tan", "J. Xu", "J. Yang", "Y. Liu"], "venue": "Computational mechanics 53(3), 403\u2013412", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "An evasion and counter-evasion study in malicious websites detection", "author": ["L. Xu", "Z. Zhan", "S. Xu", "K. Ye"], "venue": "Communications and Network Security (CNS), 2014 IEEE Conference on, pp. 265\u2013273. IEEE", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive epidemic dynamics in networks: Thresholds and control", "author": ["S. Xu", "W. Lu", "L. Xu", "Z. Zhan"], "venue": "ACM Transactions on Autonomous and Adaptive Systems (TAAS) 8(4), 19", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "A stochastic model of multivirus dynamics", "author": ["S. Xu", "W. Lu", "Z. Zhan"], "venue": "Dependable and Secure Computing, IEEE Transactions on 9(1), 30\u201345", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Gender classification of depth images based on shape and texture analysis", "author": ["X.Wang", "C.Kambhamettu"], "venue": "GlobalSIP, pp. 1077\u20131080", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "A new approach for 2d-3d heterogeneous face recognition", "author": ["X.Wang", "V.Ly", "G.Guo", "C.Kambhamettu"], "venue": "ISM, pp. 301\u2013304", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Can we minimize the influence due to gender and race in age estimation? In: 2013 12th International Conference on Machine Learning and Applications (ICMLA), vol", "author": ["X.Wang", "V.Ly", "G.Lu", "C.Kambhamettu"], "venue": "2, pp. 309\u2013314", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-domain video concept detection using adaptive svms", "author": ["J. Yang", "R. Yan", "A. Hauptmann"], "venue": "Proceedings of the ACM International Multimedia Conference and Exhibition, pp. 188\u2013197", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Cost-sensitive learning by cost-proportionate example weighting", "author": ["B. Zadrozny", "J. Langford", "N. Abe"], "venue": "Proceedings - IEEE International Conference on Data Mining, ICDM, pp. 435\u2013442", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2003}, {"title": "Can the online social networks be used as a learning tool? a case study in twitter", "author": ["L. Zaina", "T. Ameida", "G. Torres"], "venue": "Communications in Computer and Information Science 446 CCIS, 114\u2013123", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Data pipeline in mapreduce", "author": ["J. Zeng", "P. Beth"], "venue": "2013 IEEE 9th International Conference on eScience, eScience \u201913, pp. 164\u2013171. IEEE", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Cloud computing data capsules for non-consumptiveuse of texts", "author": ["J. Zeng", "G. Ruan", "A. Crowell", "A. Prakash", "B. Plale"], "venue": "Proceedings of the 5th ACMWorkshop on Scientific Cloud Computing, ScienceCloud \u201914, pp. 9\u201316. ACM", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Characterizing honeypot-captured cyber attacks: Statistical framework and case study", "author": ["Z. Zhan", "M. Xu", "S. Xu"], "venue": "Information Forensics and Security, IEEE Transactions on 8(11), 1775\u20131789", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust 2-d/3-d registration of ct volumes with contrast-enhanced x-ray sequences in electrophysiology based on a weighted similarity measure and sequential subspace optimization", "author": ["X. Zhao", "S. Miao", "L. Du", "R. Liao"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 934\u2013938", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust shape-constrained active contour for whole heart segmentation in 3-d ct images for radiotherapy planning", "author": ["X. Zhao", "Y. Wang", "G. Jozsef"], "venue": "Image Processing (ICIP), 2014 IEEE International Conference on, pp. 1\u20135", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "A support vector machine (svm) for predicting preferred treatment position in radiotherapy of patients with breast cancer", "author": ["X. Zhao", "E.K. Wong", "Y. Wang", "S. Lymberis", "B. Wen", "S. Formenti", "J. Chang"], "venue": "Medical Physics 37(10), 5341\u20135350", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2010}, {"title": "Biomarker binding on an antibody-functionalized biosensor surface: The influence of surface properties, electric field, and coating density", "author": ["Y. Zhou", "W. Hu", "B. Peng", "Y. Liu"], "venue": "The Journal of Physical Chemistry C 118(26), 14,586\u201314,594", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Training cost-sensitive neural networks with methods addressing the class imbalance problem", "author": ["Z.H. Zhou", "X.Y. Liu"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 18(1), 63\u201377", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2006}, {"title": "Predictable runtime monitoring", "author": ["H. Zhu", "M.B. Dwyer", "S. Goddard"], "venue": "Real-Time Systems, 2009. ECRTS\u201909. 21st Euromicro Conference on, pp. 173\u2013183. IEEE", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2009}, {"title": "Selecting server parameters for predictable runtime monitoring", "author": ["H. Zhu", "S. Goddard", "M. Dwyer"], "venue": "Real-Time and Embedded Technology and Applications Symposium (RTAS), 2010 16th IEEE, pp. 227\u2013236. IEEE", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "Response time analysis of hierarchical scheduling: The synchronized deferrable servers approach", "author": ["H. Zhu", "S. Goddard", "M.B. Dwyer"], "venue": "Real-Time Systems Symposium (RTSS), 2011 IEEE 32nd, pp. 239\u2013248. IEEE", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "In pattern recognition problems, we try to design a classification function to predict the class label of a data sample, so that the misclassification errors of a set of training samples can be minimized [3,11,12,20,30,4].", "startOffset": 204, "endOffset": 221}, {"referenceID": 10, "context": "In pattern recognition problems, we try to design a classification function to predict the class label of a data sample, so that the misclassification errors of a set of training samples can be minimized [3,11,12,20,30,4].", "startOffset": 204, "endOffset": 221}, {"referenceID": 11, "context": "In pattern recognition problems, we try to design a classification function to predict the class label of a data sample, so that the misclassification errors of a set of training samples can be minimized [3,11,12,20,30,4].", "startOffset": 204, "endOffset": 221}, {"referenceID": 19, "context": "In pattern recognition problems, we try to design a classification function to predict the class label of a data sample, so that the misclassification errors of a set of training samples can be minimized [3,11,12,20,30,4].", "startOffset": 204, "endOffset": 221}, {"referenceID": 29, "context": "In pattern recognition problems, we try to design a classification function to predict the class label of a data sample, so that the misclassification errors of a set of training samples can be minimized [3,11,12,20,30,4].", "startOffset": 204, "endOffset": 221}, {"referenceID": 3, "context": "In pattern recognition problems, we try to design a classification function to predict the class label of a data sample, so that the misclassification errors of a set of training samples can be minimized [3,11,12,20,30,4].", "startOffset": 204, "endOffset": 221}, {"referenceID": 32, "context": "This problem is named cost-sensitive learning in machine learning community [33,16,48,13,58].", "startOffset": 76, "endOffset": 92}, {"referenceID": 15, "context": "This problem is named cost-sensitive learning in machine learning community [33,16,48,13,58].", "startOffset": 76, "endOffset": 92}, {"referenceID": 47, "context": "This problem is named cost-sensitive learning in machine learning community [33,16,48,13,58].", "startOffset": 76, "endOffset": 92}, {"referenceID": 12, "context": "This problem is named cost-sensitive learning in machine learning community [33,16,48,13,58].", "startOffset": 76, "endOffset": 92}, {"referenceID": 56, "context": "This problem is named cost-sensitive learning in machine learning community [33,16,48,13,58].", "startOffset": 76, "endOffset": 92}, {"referenceID": 56, "context": "[58] proposed to train cost-sensitive neural networks by using technologies of sampling and threshold-moving (STM), so that the distribution of the training data samples can be modify, and the costs of different types of misclassifications can be conveyed by the appearances of the examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] provided a comprehensive analysis of the AdaBoost algorithm regarding its application in the class imbalance problem, and developed three cost-sensitive boosting algorithms (CSB), by introducing cost items into the learning framework of AdaBoost.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Masnadi-Shirazi and Vasconcelos [25] also proposed a AdaBoost-based cost-sensitive learning algorithm (ABC) to design cost-sensitive boosting algorithms, by considering two necessary conditions for optimal cost-sensitive learning, which are the minimization of expected losses by optimal cost-sensitive decision rules, and the minimization of empirical loss to emphasize the neighborhood of the desired cost-sensitive boundary.", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "Ting [34] introduced a sample-weighting method (SW) to induce cost-sensitive trees, by generalizing the standard tree induction process and initial instance weights determine the type of tree to be induced-minimum error trees or minimum high cost error trees.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "[10] proposed a supervised learning algorithm fast flux discriminant, for large-scale nonlinear cost-sensitive classification problems, by decomposing the kernel density estimation in the original feature space into selected low-dimensional subspaces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Actually, classifier adaptation has been applied to performance measures optimization [21] and cross-domain learning [47].", "startOffset": 86, "endOffset": 90}, {"referenceID": 46, "context": "Actually, classifier adaptation has been applied to performance measures optimization [21] and cross-domain learning [47].", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "In [21], a classifier is learned to optimize a performance measure, and then adapted to optimize another performance measure, while in [47], a classifier is learned from a domain, and then adapted to a different domain.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "In [21], a classifier is learned to optimize a performance measure, and then adapted to optimize another performance measure, while in [47], a classifier is learned from a domain, and then adapted to a different domain.", "startOffset": 135, "endOffset": 139}, {"referenceID": 38, "context": "[39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "This method takes the training set one by one and update the cost sensitive classifier online [29,49,14].", "startOffset": 94, "endOffset": 104}, {"referenceID": 48, "context": "This method takes the training set one by one and update the cost sensitive classifier online [29,49,14].", "startOffset": 94, "endOffset": 104}, {"referenceID": 13, "context": "This method takes the training set one by one and update the cost sensitive classifier online [29,49,14].", "startOffset": 94, "endOffset": 104}, {"referenceID": 34, "context": "The first data set is a face detection data set used in [35].", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "The second data set we used is a car detection data set [1].", "startOffset": 56, "endOffset": 59}, {"referenceID": 56, "context": "[58], CSB proposed by Sun et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32], ABC proposed by Masnadi-Shirazi and Vas-", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "concelos [25], and SW proposed by Ting [34].", "startOffset": 9, "endOffset": 13}, {"referenceID": 33, "context": "concelos [25], and SW proposed by Ting [34].", "startOffset": 39, "endOffset": 43}, {"referenceID": 30, "context": "A possible reason is that they use the formula of Adaboost algorithm [31,26,2,9], which performs well on detection problems.", "startOffset": 69, "endOffset": 80}, {"referenceID": 25, "context": "A possible reason is that they use the formula of Adaboost algorithm [31,26,2,9], which performs well on detection problems.", "startOffset": 69, "endOffset": 80}, {"referenceID": 1, "context": "A possible reason is that they use the formula of Adaboost algorithm [31,26,2,9], which performs well on detection problems.", "startOffset": 69, "endOffset": 80}, {"referenceID": 8, "context": "A possible reason is that they use the formula of Adaboost algorithm [31,26,2,9], which performs well on detection problems.", "startOffset": 69, "endOffset": 80}, {"referenceID": 40, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 101, "endOffset": 124}, {"referenceID": 42, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 101, "endOffset": 124}, {"referenceID": 51, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 101, "endOffset": 124}, {"referenceID": 41, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 101, "endOffset": 124}, {"referenceID": 40, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 101, "endOffset": 124}, {"referenceID": 17, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 101, "endOffset": 124}, {"referenceID": 39, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 141, "endOffset": 151}, {"referenceID": 23, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 141, "endOffset": 151}, {"referenceID": 55, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 141, "endOffset": 151}, {"referenceID": 54, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 168, "endOffset": 178}, {"referenceID": 53, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 168, "endOffset": 178}, {"referenceID": 52, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 168, "endOffset": 178}, {"referenceID": 37, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 22, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 21, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 35, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 36, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 18, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 16, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 45, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 43, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 44, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 14, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 196, "endOffset": 230}, {"referenceID": 26, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 255, "endOffset": 262}, {"referenceID": 27, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 255, "endOffset": 262}, {"referenceID": 49, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 280, "endOffset": 287}, {"referenceID": 50, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 280, "endOffset": 287}, {"referenceID": 7, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 328, "endOffset": 346}, {"referenceID": 5, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 328, "endOffset": 346}, {"referenceID": 6, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 328, "endOffset": 346}, {"referenceID": 4, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 328, "endOffset": 346}, {"referenceID": 57, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 328, "endOffset": 346}, {"referenceID": 59, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 328, "endOffset": 346}, {"referenceID": 58, "context": "Moreover, we will also investigate the application of the proposed algorithm to information security [41,43,53,42, 41,52,18], bioinformatics [40,24,57], medial imaging [56,55,54], computer vision [38,23,22,36,37,19,17,46,44,45,15], reinforcement learning [27,28], cloud computing [50,51] and microprocessor reliability modeling [8,6,7,5,59,61,60].", "startOffset": 328, "endOffset": 346}], "year": 2015, "abstractText": "In this paper, we propose the problem of online cost-sensitive classifier adaptation and the first algorithm to solve it. We assume we have a base classifier for a cost-sensitive classification problem, but it is trained with respect to a cost setting different to the desired one. Moreover, we also have some training data samples streaming to the algorithm one by one. The problem is to adapt the given base classifier to the desired cost setting using the steaming training samples online. To solve this problem, we propose to learn a new classifier by adding an adaptation function to the base classifier, and update the adaptation function parameter according to the streaming data samples. Given a input data sample and the cost of misclassifying it, we update the adaptation function parameter by minimizing cost weighted hinge loss and respecting previous learned parameter simultaneously. The proposed algorithm is compared to both online and off-line cost-sensitive algorithms on two cost-sensitive classification problems, and the experiments show that it not only outperforms them one classification performances, but also requires significantly less running time.", "creator": "LaTeX with hyperref package"}}}