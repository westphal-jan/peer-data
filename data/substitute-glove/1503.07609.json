{"id": "1503.07609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2015", "title": "An Evolutionary Algorithm for Error-Driven Learning via Reinforcement", "abstract": "Although elements instruction software are coordinated to afford nature regarding, little the known about how of depends. This papers kind a abstraction proposals that specifies because complex strategies that nor instead thought next requires error - driven lesson thought time same holdings through description reinforcement. This agenda three specific inference up now enabling probably contribute which took evolution. (artificial) interconnect networks already generate photomultipliers that negotiate the segment to learn well - scale beyond problems using without news about the quality both put surprising. The appropriate being humanistic implications of the framework different topics, recently are consider biological analogs more the emphasis.", "histories": [["v1", "Thu, 26 Mar 2015 03:33:47 GMT  (498kb)", "http://arxiv.org/abs/1503.07609v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["yanping liu", "erik d reichle"], "accepted": false, "id": "1503.07609"}, "pdf": {"name": "1503.07609.pdf", "metadata": {"source": "CRF", "title": "An Evolutionary Algorithm for Error-Driven Learning via Reinforcement", "authors": ["Yanping Liu", "Erik D. Reichle"], "emails": ["liuyp@psych.ac.cn."], "sections": [{"heading": null, "text": "An Evolutionary Algorithm for Error-Driven Learning via\nReinforcement\nYanping Liu Key Laboratory of Behavioral Science\nInstitute of Psychology, Chinese Academy of Sciences\n&\nErik D. Reichle School of Psychology, University of Southampton, UK.\nNote: This is a draft; please do not cite without permission. Address correspondence to: Yanping Liu, 16 Lincui Road, Key Laboratory of Behavioral Science, Institute of Psychology, Chinese Academy of Sciences, Beijing, China. Email: liuyp@psych.ac.cn.\nAlthough different learning systems are coordinated to afford complex behavior, little is known about how this occurs. This article describes a theoretical framework that specifies how complex behaviors that might be thought to require error-driven learning might instead be acquired through simple reinforcement. This framework includes specific assumptions about the mechanisms that contribute to the evolution of (artificial) neural networks to generate topologies that allow the networks to learn large-scale complex problems using only information about the quality of their performance. The practical and theoretical implications of the framework are discussed, as are possible biological analogs of the approach.\nModern cognitive science has dispelled the long-held belief that the behavior\nof higher organisms is governed by a unitary learning system (e.g., Skinner, 1938) and has instead shown that behavior is controlled by several different learning mechanisms, each operating according to its own principles (Doya, 1999, 2000; Squire, 1993). With this progress, however, have come new questions about how these learning systems are coordinated so that organisms can learn the complex behaviors that are necessary to live and reproduce (e.g., see McClelland, McNaughton, & O\u2019Reilly, 1995). The goal of this article is to address this question by considering two specific types of learning that have been widely studied and that are known to play critical roles in complex behavior: reinforcement learning and error-driven learning.\nReinforcement learning refers to a type of learning in which an organism\u2019s\nbehavior is gradually shaped through contingencies of reward and punishment. In other words, the organism performs actions that result in rewards and/or punishments and then, over time, learns to perform those actions that maximize/minimize the amount of reward/punishment obtained from those actions and from the states that result from those actions. By learning to associate reward/punishment values with specific actions and their resulting states, the organism essentially learns which actions are most appropriate from each possible state. When viewed in this way, reinforcement learning refers to a general class of learning algorithms that has been extensively studied in psychology and that includes both classical (Pavlov, 1927) and operant (Thorndike, 1901) conditioning. And in the field of machine learning, reinforcement learning refers to a class of\nalgorithms that\u2014like their biological analogs\u2014allow artificial systems to learn how to optimize their performance in various tasks (e.g., navigating through mazes; Sutton & Barto, 1998) by associating values with various actions and states. However, because reinforcement learning is contingent upon a one-dimensional error signal, its applicability to complex high-dimensional (e.g., non-linear) tasks has been severely limited (Sutton & Barto, 1998), as has its application to psychology (Gershman, Cohen, & Niv, 2010).\nIn contrast to reinforcement learning, error-driven learning refers to a type of\nlearning in which the organism uses information about how its behavior differs from some \u201ctarget\u201d or desired behavior to modify its actual behavior so that the latter comes to resemble the former (McClelland et al., 1995; Rumelhart, Hinton, & Willams, 1986). Because error-driven learning uses the difference or error between the actual and desired behavior, and because this error can vary along multiple dimensions, error-driven learning is not limited like reinforcement learning and can be used to learn complex, high-dimensional problems (e.g., learning English spelling-to-sound correspondences; Seidenberg & McClelland, 1989). But this enhanced learning requires an explicit error or teaching signal that has often undermined its applicability in both psychology (Dayan & Niv 2008; Niv, 2009; Sutton, 1991) and machine learning (Alonso & Mondrag\u00f3n, 2005; Ye, Yung, & Wang, 2003). The strengths and weaknesses of error-driven learning thus complement those of reinforcement learning, and neither class of learning algorithms is sufficient to learn complex behaviors in the absence of an explicit \u201cteacher\u201d that provides information about how the behavior that is currently being performed differs from\nthe behavior that must eventually be learned. Of course, the obvious way to redress this limitation is to somehow combine the two types of learning and thereby allow error-driven learning via reinforcement. We suspect that this \u201csolution\u201d was the one arrived at by evolution through natural selection, and in the remainder of this article we will describe an evolutionary-based algorithm that does exactly that\u2014it allows error-driven learning to occur via simple reinforcement. As will be discussed below, this new algorithm: (1) allows artificial systems to learn problems through reinforcement that have heretofore been too complex to acquire, and (2) provides a hypothesis about the functional roles played by both error-driven and reinforcement learning in the acquisition of complex behaviors by real biological organisms. 1. General Overview\nIn biological organisms, the processes that support all types of learning are\ninstantiated within networks of highly interconnected neurons. These neural networks receive input from the perceptual systems and then propagate this activity as output to motor effectors, allowing the organism to both perceive and interact with the physical environment. Learning in such a network thus occurs through the modification of the connection strengths among the neurons. For example, a onelayer network must learn the correspondences the between stimuli and responses by adjusting its connection strengths so that it can execute an appropriate response to each stimulus. By modifying its connection strengths in this manner, a network retains information in the structure that mediates the \u201cmappings\u201d between stimulus inputs and response outputs. However, it is important to note that this structure or\ntopology is derived from two sources\u2014the macrostructure of the network (e.g., the number of neurons, their pattern of connectivity to the perceptual and motor systems, etc.) that is presumably acquired very slowly through evolution, and the microstructure of the network (e.g., the strengths of individual connections) that is acquired more rapidly through learning. Both types of structure are therefore important determinants of the types of behaviors that a neural network is capable of learning.\nThe manner in which topology constrains learning has been conclusively\ndemonstrated using the machine-learning analogs of the biological neural networks (e.g., for discussions of these constraints, see McClelland & Rumelhart and the PDP Resarch Group, 1986; Rumelhart & McClelland and the PDP Research Group, 1986). In these artificial neural networks, the connection strengths between \u201cneurons\u201d are modified using a variety of different algorithms, with perhaps the most widely used of these algorithms being back-propagation (Rumelhart et al., 1986). The primary virtues of back-propagation are its relative simplicity and the fact that it is sufficient to train multi-layer networks (i.e., networks in which input units are separated from output units by one or more layers of intermediate \u201chidden\u201d units). Such multilayered networks are necessary to learn tasks that require non-linear mappings between input and output, such as the classic exclusive-or problem, where either of two options results to one response but neither or both of the options results in the opposite response (Rumelhart et al., 1986). The main limitation of backpropagation is that it is a form of error-driven learning, and as such requires an explicit teaching signal during training. As already indicated, this restricts back-\npropagation to certain classes of problems (i.e., problems in which there is a teaching signal) and thereby undermines its possible utility in learning behaviors in which the only feedback is whether those behaviors resulted in positive or negative outcomes\u2014the types of behaviors that can be acquired via reinforcement learning.\nThis restriction is unfortunate because, in addition to offering some degree of\nbiological plausibility, artificial neural networks convey one other advantage that would otherwise make them ideal for learning complex behavior\u2014the graded nature of their processing allows the networks to approximate functions very effectively. For example, in contrast to standard reinforcement-learning algorithms (e.g., value-iteration; Sutton & Barto, 1998), where each state and its associated value are discretely represented (e.g., as a collection of values in a look-up table), networks represent information about all states and their values within a single set of interconnected processing units, so that the network is able to interpolate and extrapolate from its knowledge and thereby generalize from known states to novel states.\nAs will become evident below, our new algorithm to enable error-driven\nlearning via reinforcement exploits this characteristic of neural networks by allowing information about states and their values to be stored in a single set of connections, thus allowing the algorithm to generalize from situations used during training to completely new situations. The essence of this approach is that it acknowledges the constraints that are imposed by a network\u2019s macro- and microstructures, and in so doing incorporates an evolutionary-based method for developing macrostructures that are specialized for certain types of learning (i.e.,\nmicrostructures). More specifically, this approach uses a genetic algorithm (Holland, 1975) to evolve artificial neural networks that are capable of error-driven learning via reinforcement. We will now describe this approach in four sections, explaining how our hybrid algorithm: (1) selects the appropriate macrostructure that supports learning; (2) simulates the emergence of the microstructure that reflects learning; and (3) implements reinforcement learning. The final section (4) will then provide an overview of how these three processes are coordinated to allow artificial neural networks to perform error-driven learning via reinforcement."}, {"heading": "2. Macroscopic Evolution of the Network Architecture", "text": "Efficient biological evolution entails safeguards to protect any innovations of\nthe phenotype that offer a selective advantage (in terms of ecological fitness) to the genotype, while simultaneously detecting homology between genotypes and minimizing the structural complexity of the genotype. These safeguards maximize the efficiency of biological evolution by minimizing the overall size of the evolutionary \u201csearch space\u201d that must be \u201ctraversed\u201d to produce organisms that are fit enough to compete and survive in specific ecological niches. Our method of generating network topologies uses an algorithm (NeuroEvolution of Augmenting Topologies, or NEAT; Stanley & Miikkulainene, 2002) that was specifically designed to instantiate these three efficiency strategies as follows: First, innovations in network topology are protected (so that they have a reasonable chance of propagating from one generation of networks to the next) by speciation, or the evolution of separate species that comprise distinct populations of networks that have unique topologies and that only reproduce within their own population.\nSecond, homologous genotypes can be identified by historical \u201cmarkers\u201d (i.e., identifiers that reflect each network\u2019s evolutionary history) to allow for efficient identification and alignment of the genomes during the crossover stage of reproduction. And finally, the structural complexity of the genotypes is minimized by beginning the process of evolution with the simplest possible network topology\u2014single-layer networks containing only input and output units.\nIn our approach, each genome is a linear array of genes that represents each\nnetwork\u2019s topology. There are two basic types of genes: (1) node genes that determine the functional role of each node (i.e., input, output, or hidden unit) in the network, and (2) connection genes that determine the patterns of connectivity among the nodes in the network. Figure 1 is a schematic diagram showing the relationship between a single example genome (i.e., the genotype) and the network that it instantiates (i.e., the phenotype). As indicated, each node gene indicates the functional role of a node and provides a unique identifier for that node (e.g., represented by integers in Fig. 1). Likewise, each connection gene indicates the identities of the two nodes that are being connected, the initial weight or strength (i.e., represented by the parameter ) of the connection and whether or not that connection has been enabled (i.e., set equal to a value of 0 or ), and a unique identifier for that connection (again, as represented by the integers in Fig. 1). The unique mutation identifiers provide a numerical index that can be used to reference each new innovation that occurs across successive generations. (In the exposition below, this index will be called the innovation number.)\n-------------------------- Insert Figure 1 here\n\u03c9\n\u03c9\n--------------------------\nThe complexity of the network topology increases over successive\ngenerations through genetic mutation. These mutations can affect both the number and type of nodes in a network, as well as the pattern of connectivity among the nodes. These two basic types of \u201cconnective\u201d mutations are shown in Figure 2. New nodes are added to a network by replacing the connection between two existing nodes with an intermediary node that connects to the two original nodes (e.g., as shown in the left panel of Fig. 2). The new connections joining the new node to the original two are automatically enabled to ensure that the mutation affects the network\u2019s overall fitness. And similarly, mutations can affect the pattern of network connectivity by joining two previously unconnected nodes (e.g., as shown in the right panel of Fig. 2) or by changing the strength of an existing connection between two nodes. The probability that each type of mutation (i.e., adding a node, adding a connection, or modifying a connection) will occur is determined probabilistically, with the overall rates of each respective type of mutation being controlled by the\nparameters \u03c0add-node, \u03c0add-link, and \u03c0mutate-link (for a complete list of the parameter values associated with macroscopic evolution, see Appendix A).\nThe precise manner in which connections are modified also depends upon\nthe \u201cseverity\u201d of a mutation, which is determined probabilistically using the parameter . With probability , the mutation is considered to be \u201csevere\u201d\nand the connection is modified in one of the following mutually exclusive ways. With probability ccold-gauss, the mutation in the existing connection is simply canceled, and with probability 1 \u2013 cgauss, the connection strength is sampled from a\nseverity\u03b4 severity\u03b4\nGaussian distribution with \u00b5w = (i.e., the initial weight value) and \u03c3w = 0.5;\notherwise, the connection strength is sampled from a Gaussian distribution with \u00b5w\n= 0 and \u03c3w = 0.5. On the other side, some connection genes, which are selected randomly, have probability cturn-on-off to change their connected condition (i.e., if the connection gene is disabled, it become enabled, vice versa). Finally, with the insertion of a new connection, there is some probability that two previously\nunconnected nodes cannot be found; when this happens, the parameter \u03c0attemptmutation specifies the number of attempts that are made to locate the nodes before the effort is halted.\n-------------------------- Insert Figure 2 here --------------------------\nThe problem of aligning two genomes during crossover is computationally\ninexpensive because innovation numbers provide a basis for rapidly comparing any two genomes. Those genes that are identical between two parent genomes are said to match, and those that do not match are in either disjoint or excess, depending on whether they occur within or outside (respectively) of the range of innovation numbers of the other parent\u2019s genome. Using such information, crossover between the genomes of any two parents can occur in three different ways, as shown in Figure 3.\n-------------------------- Insert Figure 3 here --------------------------\nIn single-point crossover, an innovation number is randomly selected from\nthe set of matching innovation numbers. All of the genes to the left of this point in\n\u03c9\none parent\u2019s genome (including disjoint genes but not excess genes because the latter by definition do not exist to the left of the crossover point) are then copied to the genome of the offspring, and all of the genes to the right of this point in the other parent\u2019s genome (including both disjoint and excess genes) are also copied to the genome of the offspring. If the gene at the crossover point happens to correspond to a connection weight, then that particular connection weight in the offspring is set equal to the mean of those connection weights in the two parents.\nIn multipoint crossover, each of the two possible values of all matching genes\nhas an equal probability of being copied to the offspring\u2019s genome. And because all disjoint and excess genes can also be copied to the offspring\u2019s genome, the offspring\u2019s genome can become longer that the genome of either parent. One way to curtail this growth is to only copy disjoint and/or excess genes from the parent having the higher fitness value. With or without such precautions, however, this crossover method ensures that the offspring inherits both the shared and unique genes from both of its parents, thereby increasing both the combinatory and exploratory potential of this method relative to single-point crossover.\nFinally, multipoint-average crossover is identical to multipoint crossover\nexcept that, rather than randomly assigning those matching genes representing the connection weights of one of the parents to the offspring, the offspring will instead inherit a gene for a connection weight that is the mean of the parents\u2019 two connection weights.\nEach successive generation of networks is generated using some\ncombination of the three aforementioned methods of crossover in conjunction with\nboth cloning and mutation. More specifically, with each new generation, individual offspring can be produced in three ways, determined in a probabilistic manner: (1) via cloning or copying the genome of a network from one generation to the next; (2) via mutating a genome (using one of the methods described above) and then copying the mutated genome to the next generation; and (3) via one of the three crossover methods that were described above. As will be indicated below, the genome of the fittest individual in each species is simply cloned from one generation to the next; each remaining individual has a probability equal to pmutate-only of having a mutation introduced into its genome (as described above) and that genome being copied to the next generation, and a probability of 1 - pmutate-only of reproducing via crossover. With the latter, the method of crossover is determined probabilistically, with the probability of single-point, multipoint, and multipoint-average being specified by the parameters psingle-point, pmultipoint, and pmultipoint-average, respectively. Following crossover, the genome is left intact with probability pmate-only and a mutation is introduced to the genome with probability 1 \u2013 pmate-only. Finally, although crossover typically occurs between two individuals from the same species, it can with some small probability, cinter-species, occur between two individual from different species, sometimes resulting is offspring that are more fit than either parent.\nUsing these evolutionary methods, populations of networks having complex\nand diverse topologies can evolve. However, because networks having simpler topologies tend to optimize more rapidly than networks having more complex topologies, the process of adding nodes and connections initially causes more\ncomplex networks to be less fit. One method for preventing more complex network topologies from being prematurely removed from the population is to allow speciation, or the emergence of new \u201cspecies\u201d of networks (i.e., networks having more complex topologies) so that they can compete within their own more specialized ecological niches. By doing this, more complex networks are protected so that they have time to optimize their structures to their particular niches.\nThe process of speciation is also made computationally efficient by using the\ninnovation numbers and by using the number of disjoint (D) and excess (E) genes as a metric of genome compatibility. The main intuition of this method is that pairs of genomes that contain large numbers of disjoint and/or excess genes are unlikely to share much of their evolutionary history, and are thus more likely to represent distinct species. This intuition is instantiated using a measure of the compatibility\ndistance of two genomes. This distance, \u03b4, is a linear combination of D, E, and , or\nthe mean weight differences of matching genes, and is specified by Equation 2.1:\n(2.1)\nIn Equation 2.1, c1, c2, and c3 are coefficients to weight the relative\ncontributions of the three relevant factors (i.e., excess genes, disjoint genes, and the mean weight difference of matching genes), and N is the number of genes in the longer of the two genomes and is used to normalize genome length. Any two\ngenomes having a compatibility distance exceeding some pre-specified threshold, \u03b4c, are considered to belong to separate species and thus (usually) prohibited from\nW\n\u03b4 = c1E N + c2D N + c3W\nbreeding. In sorting genomes into species, each genome is grouped with the first\nspecies for which \u03b4 < \u03b4c, so that no genome belongs to more than one species.\nAnother potential problem associated with speciation is that one species can\ngrow without bounds, taking over the entire population. To prevent this from happening, the number of individuals within a species is increased/decreased according to whether its average total fitness is above/below the mean fitness of the population of species. This is done using Equation 2.2:\n(2.2)\nwhere Nj\u2032 is the number of individuals in species j adjusted for its mean fitness, as specified by the right side of the equation. There, Nj is the non-adjusted number of individuals in species j, fi,j is the adjusted (relative) fitness of individual i of species j\n(as is given by Equation 2.3), and is the mean fitness (i.e., the mean of fi,j) of the\nentire population.\n(2.3) fi, j = f i, j raw \u2212 fworst log N j( )\nIn Equation 2.3, is the raw fitness of individual i of species j (which\nnormal takes on a negative value) and fworst is the worst raw fitness in the population. Equation 2.3 differs from the original NEAT algorithm (Stanley & Miikkulainene, 2002) in that the denominator is the logarithm of Nj rather than Nj to take advantage of fitter species. The size of each species is thus adjusted so that the number of individuals in more fit species tends to increase and the number of individuals in less fit species tends to decline. With each new generation, a certain\n\u2032 N j = f i, ji=1 N j\u2211 f\nf\nraw jif ,\npercentage (determined by the parameter csurvival) of the best performing (i.e., most fit) of each species is allowed to randomly produce the next generation of for their species.\nA fitness amplification assumption is also added to the original NEAT\nalgorithm that allows the fittest individual in the population to have the unique opportunity to contribute a copy of its genome (i.e., a clone of itself) to the next generation. This is done using Equation 2.4, where fbest is the adjusted fitness value of the best-performing member of a species, and cbest is an amplification coefficient that enhances that fitness value so as to ensure that that individual contributes a copy of itself to the next generation. (2.4) bestbestbest fcf \u2190 To further enhance the reproductive advantage of better species and thereby increase the probability of evolution being successful, a \u201cdelta coding\u201d procedure is introduced that \u201csteals\u201d or eliminates a certain number of offspring, doffspring-stolen, from whatever species has improved the least. In a similar manner, when the least fit species has not improved over a certain number of generations, ddrop-off-age, its overall fitness is reduced; to product protect younger species, their fitness is increased by an amplification factor, dage-signifance.\nIt is worth emphasizing that the algorithm as described so far is biased to\nfavor networks having simple topologies. The reason for this is that the addition of nodes and/or connections is not without cost: As the topology of a network increases, so too does the inherent difficult associated with evaluating its contribution to a network\u2019s fitness. Thus, by starting the evolutionary process with\nthe simplest possible networks (i.e., networks without hidden units), more complex topologies are only retained if they are justified on the grounds that they increase a network\u2019s fitness.\nFinally, the probability of mutation is determined by a simulated annealing\nprocess (\u010cern\u00fd, 1985; Kirkpatrick, Gelatt, & Vecchi, 1983) in which the overall probability is initial some large value but then declines with each successive generation. (The \u201csimulated annealing\u201d metaphor comes from metallurgy, were the initial high temperature of a metal is slow decreased over time so that the atoms can align in a way that ensures high tensile strength.) The rate of this decline is controlled by a \u201ctemperature\u201d parameter, T (i.e., to simplify, T is set equal to\ngenerations from last reset), and the probabilities of adding either a new node, \u03c0add-\nnode, or a new connection link, \u03c0add-link, to a network are given by Equations 2.5 and 2.6, respectively:\n(2.5) , where x \u2190 x \u2212 1 k1T (2.6) , where\nx \u2190 x \u2212 1 k2T\nwhere \u03c81 and \u03c82 respectively represent the lower and upper limits for the\nprobability of adding a node via mutation, \u03c83 and \u03c84 respectively represent the lower and upper limits for the probability of adding a link via mutation, and k1 and k2 represent coefficients for adjusting the probabilities of adding nodes and links, respectively. Similarly, in Equation 2.7, \u03c85 and \u03c86 respectively represent the lower\nand upper limits for the probability of reproduction via mutation only, \u2206 is a\nparameter that control how much the probability is incremented per generation,\n\u03c0 add \u2212node = max \u03c81,min \u03c82,x( )[ ] \u03c0 add \u2212 link = max \u03c83,min \u03c84, x( )[ ]\nand k3 is a dynamic coefficient that is set equal to -1 if the fitness of the population fails to improve by some criterion, cannealing (otherwise, k3 is set equal to 1). Finally, if the fitness of the population fails to improve, the values of the \u03c0add-node and \u03c0add-link are set equal to the starting values. (2.7)\npmutate\u2212only = max \u03c85,min \u03c86,x( )[ ], where x \u2190 x + k3\u2206\nIn the next section of this article, we explain how the process of microscopic\nevolution or learning also contributes to network topology and how this, in turn, determines the fitness of the network and hence its likelihood of contributing to the gene pool of the next generation."}, {"heading": "3. Microscopic Evolution (or Learning) of the Network Structure", "text": "Our algorithm for microscopic evolution of network connection weights is\nbased on the Covariance Matrix Adaptation Evolution Strategy (CMA-ES; Hansen, 2006; Hansen & Kern, 2004; Hansen, M\u00fcller, & Koumoutsakos, 2003; Hansen & Ostermeier, 2001; Iger, Hansen, & Roth, 2007; Suttorp, Hansen, & Igel, 2009). This algorithm provides a stochastic method for parameter optimization of non-linear, non-convex functions. As such, it is particularly useful for \u201crugged\u201d parameter landscapes comprised of discontinuities and local optima (e.g., sharp \u201cridges\u201d) and is thus well suited to solve ill-conditioned and non-separable problems. Some modifications of the CMA-ES algorithm were necessary, however, to make it more amenable to the problem of evolving network connection weights.\nThe general intuition behind the algorithm is that, rather than training a\nnetwork across a series of trials to find the set of connection weights that allow a network to solve some problem, the connection weights are instead evolved. To\nunderstand how this is done, it is first necessary to understand that the weights themselves can be represented as a vector, and that the elements of this vector can be sampled to find values that allow a network to solve a particular problem. In essence, this is what the CMA-ES algorithm does: During each learning trial, a set of vectors representing possible connection weight solutions to the problem are sampled from a sampling distribution, and then a new mean and covariance of a sampling distribution are computed that reflect the fitness of these sampled vectors. This whole process is repeated until either a solution meeting some goodness-ofsolution criterion has been reached, or a stopping criterion has been reached.\nDuring each trial t+1, the \u03bb individual vectors (i.e., candidate solutions\nconsisting the connection weights) are sampled using Equation 3.1:\n(3.1)\nwhere is a normally distributed vector with mean , sampling\nvariance , and covariance matrix . (For a complete list of the parameter values associated with microscopic evolution, see Appendix B.) This provides a simple method for sampling candidate vectors of connection weights using a multinormal distribution, with the covariance matrix determining the degree to which sampling proceeds in a cautious versus audacious manner. The mean of the sampling distribution is computed using the weighted average of the individual sampled vectors using Equation 3.2:\n(3.2)\nxk t +1 ~ N x w t( ),\u03c3 2 t( )C t( )( ),k =1,...\u03bb\nN x w t( ),\u03c3 2 t( )C t( )( )\nx w t( )\n\u03c3 2 t( )\nC t( )\nx x t( ) = wixi:\u03bb t( ) i=1 \u00b5\u2211\nwith the constraints that wi > 0 for all values of i and , and with the index\ni:\u03bb denoting the i-th best individual. By combining the best connection-weight\nvectors in this weighted manner, the mean of the sampling distribution approaches the target solution over time.\nIn Equation 3.1, , represents the correlation between different connection\nweights within a neural network, and modulates the variability associated with the sampling distribution. Both terms are therefore vital to the success of the\nalgorithm. Over trials, changes as a function of both the evolutionary path, which as its name suggests, controls the global orientation or trajectory of the evolutionary\nprocess, thereby allowing it to converge towards a solution. is updated using\nEquation 3.3, were ccov is a parameter that controls the rate of change, \u00b5cov is a parameter for weighting between the rank-one and rank-\u03bc update, and the terms\nand are specified by Equations 3.4 and 3.5, respectively.\n(3.3) C t +1( ) = 1\u2212 ccov( )C t( ) + ccov 1\n\u00b5cov Pc\nt +1( ) + ccov 1\u2212 1\n\u00b5cov\n  \n   S t +1( )\n(3.4)\n(3.5)\nIn Equation 3.4, the term is specified by Equation 3.6, which\naccumulates the differences between the mean connection-weight samplingdistribution vectors across successive trials:\n(3.6) ( ) ( ) ( ) ( ) ( ) ( ) ( )111 21 +++ \u2212+\u2212= tt eff cc tt cc t c DccHpcp \u03c3 \u00b5 \u03c3\nwii=1 \u00b5\u2211 =1\nC t( )\n\u03c3 2 t( )\nC t( )\nC t( )\nPc t +1( )\nS t +1( )\nPc t +1( ) = pc t +1( ) pc t +1( )( )T\nS t +1( ) = wi \u03c3 2 t( )i=1 \u00b5\u2211 xi:\u03bbt +1( ) \u2212 x w t( )( )xi:\u03bbt +1( ) \u2212 x wt +1( )( ) T\npc t +1( )\nwhere cc is learning rate for accumulation for the rank-one update of the covariance\nmatrix, and H\u03c3 t +1( ) =1 if\np\u03c3 t +1( )\n1\u2212 1\u2212 c\u03c3( ) 2\u03c4 < 1.4 +\n2 n +1       E N 0,I( )( ); otherwise, H\u03c3 t +1( ) = 0 .\n(In the preceding conditional statement, the index \u03c4 denotes the number of\ncompleted trails.) The term \u00b5eff is the variance effective selection mass, which is\nrelated to recombination weights and is constrained so that . Finally,\nthe last term in Equation 3.6 is the actual difference between mean sampling distribution vectors across successive trials and is specified by Equation 3.7:\n(3.7) D t +1( ) = x w t +1( ) \u2212 x w t( )( ) Finally, the sampling variance in Equation 3.1, , controls the overall rate\nof change in the evolutionary process. The value of during any given trial is\ngiven by Equation 3.8, in which the term is specified by Equation 3.9.\n(3.8) \u03c3 t +1( ) = \u03c3 t( ) exp c\u03c3 d\u03c3\np\u03c3 t +1( ) E N 0,I( )( )         \u22121        \n(3.9) where the orthogonal matrix, , and the diagonal matrix, , are obtained via\nprincipal component analysis of using the matrix theorem: . In\nEquation 3.8, is the expected length of p\u03c3 under random selection and is\ngiven by Equation 3.10:\n\u00b5eff =1 wi 2\ni=1\n\u00b5\u2211\n\u03c3 2 t( )\n\u03c3 2 t( )\np\u03c3 t +1( )\np\u03c3 t +1( ) = 1\u2212 c\u03c3( )p\u03c3t + c\u03c3 2 \u2212 c\u03c3( )B t( )E \u22121 t( )BT t( )\nB t( )\nE t( )\nC t( )\nC t( ) = B t( )E 2 t( )BT t( )\nE N 0,I( )( )\n(3.10)\nwhere n denotes the number of search-space dimensions, which in this application simply corresponds to the number of connection weights.\nAs indicated above, our algorithm modifies the standard CMA-ES algorithm\n(as described so far) to automatically inflate the population size, \u03bb, and the number\nof trial iterations for the stopping condition, \u03c4stop, whenever the algorithm stagnates, or stops converging towards a better solution. This is done using Equations 3.11 and 3.12, respectively, where denotes the number of generations over which the\nalgorithm has stagnated, and \u03c1 is a parameter that denotes the default number of\ngenerations.\n(3.11) (3.12)\n\u03c4 stop = \u03c1 1+ o( ) Furthermore, to improve microscopic evolution, especially when the\npopulation is trapped in local maxima, the sampling variance in initial trial, , is defined according to its stagnation level using Equation 3.13. By increasing the sample, the population may thereby escape the local maxima point. (3.13) ( ) ( )( )1log,min max0 +\u22c5+= ood \u03c3\u03c3\u03c3\u03c3 where \u03c3max represents the upper limit of initial sampling variance, \u03c3d represents the\ndefault sampling variance and o\u03c3 represents coefficient for scaling stagnation effect.\nFinally, a fitness criterion can also be used to stop the evolutionary process.\nThat is, if the fitness of the network, \u03d1, exceeds some fitness threshold, fstop, or the\nE N 0,I( )( )= 2\u0393 n +1 2      \n\u0393 n 2      \n\u2248 n 1\u2212 1 4n + 1 21n2      \n\u03bf\n( )[ ]\u03bf\u03bb ++= nln34\n( )0\u03c3\ntrial iteration exceeds the stop condition, \u03c4stop, the microscopic evolution of the connection weights is halted."}, {"heading": "4. Reinforcement Learning of Connection Weights", "text": "The previous two sections have described how network topology is\ngenerated through the processes of macro- and microscopic evolution. In this final section, we describe the algorithm that is used to train a network based on reinforcement learning. As indicated, reinforcement learning refers to a general class of machine-learning algorithms in which performance is \u201cshaped\u201d using a single training signal corresponding to the reward/punishment that is associated with specific actions and/or the states that result from those actions (Sutton & Barto, 1998). Of central importance to this notion is the idea of reward prediction\nerror, usually denoted by \u03b4, which represents the reward that an artificial agent\nanticipates in response to a particular action and the state that then results from that action. This is represented in Equation 4.1, in which R represents the immediate reward that is received from executing the action, x represents a particular state that the agent can be in at time t, and the value function V represents\nthe value associated with the state. The parameter \u03b3 is a discount parameter that\ndetermine how much the reward that is anticipated to result from the next state,\nxt+1, is weighed against the immediate reward; small values of \u03b3 thus make the agent \u201cgreedy\u201d in that it tends to prefer actions that result in large immediate rewards,\nwhereas large values of \u03b3 cause the agents to prefer actions that result in rewards\nover the long run. (For a complete list of the parameter values associated with reinforcement learning, see Appendix C.)\n(4.1) \u03b4 = R + \u03b3V xt +1( )\u2212V xt( ) One way to implement this algorithm within a neural network is to use the standard error-driven back-propagation algorithm (Rumelhart et al., 1986), training the network using each state at time t, xt, as the input, and allowing the resulting state at time t+1, xt+1, as the output. The teaching signal for the desire output is then\n, and the weight for any given connection in the network is adjusted\nduring learning using Equation 4.2:\n(4.2) \u2206wdirect = \u03b1 R + \u03b3V xt +1( )\u2212V xt( )[ ] \u2202V xt( )\n\u2202w\nwhere \u03b1 is a learning rate parameter that controls the rate of convergence and the\nsubscript \u201cdirect\u201d denotes the name of this algorithm. Although this direct algorithm has been used successfully in many applications (Tesauro, 1990, 1992), it is not guaranteed to converge for general function-approximation systems (Baird, 1995).\nTo develop such an algorithm, the problem can be restated as being one of\npredicting the outcome of a deterministic Markov chain, with the goal being to specify a value function that, for any given state, xt, will give the value of the immediate reward and the successor state, xt+1, thereby satisfying the Bellman equation (Bellman, 1957): (4.3)\nV xt( )= R + \u03b3V xt( ) where is the expected value of all possible successor states, xt+1. For a system\nhaving a finite number of states, the optimal value function, V*, will provide a unique solution to the above equation, and any value function that is suboptimal will result in an inequality called the Bellman residual. For a system with n states, the mean\nR +\u03b3V xt +1( )\nsquared Bellman residual is given by Equation 4.4, and provides a direct measure, E, of the degree to which a given policy is suboptimal. And because the value of E is bounded, it suggests an alternative to the direct algorithm for adjusting the connection weights in a neural network function-approximation system: by performing stochastic gradient descent on E.\n(4.4)\nUnder the assumption that V is parameterized by the set of connection\nweights, the adjustment to any given weight w following a transition from xt to xt+1 with reward R is specified by Equation 4.5:\n(4.5) ( ) ( )[ ] ( ) ( )    \u2212\u2212+\u2212=\u2206 ++\u2212 ttttgradientresidual xVw xV w xVxVRw \u2202 \u2202 \u2202 \u2202\u03b3\u03b3\u03b1 11\nwhere the \u201cresidual-gradient\u201d subscript denotes the name of the algorithm, residualgradient. For a system with a finite number of states, E will equal 0 only if V = V*. And critically, this residual-gradient algorithm is guaranteed to converge, thus making it ideally suited for training neural networks to be function-approximation systems. The one limitation of this algorithm, however, is that it is slow (Baird, 1995; Williams & Baird, 1993). This limitation makes the algorithm impractical for large-scale problems of the type that might be of interest to psychologists (e.g., Reichle & Laurent, 2006). This limitation results in the following quandary: Whereas the direct algorithm is rapid, it is not guaranteed to converge, especially for large problems; in contrast, the residual-gradient algorithm is guaranteed to converge, but is slow, especially for large problems. Because both algorithms are\nE = 1 n R +\u03b3V xt +1( )\u2212V xt( )x\u2211 2\nbased on gradient descent, however, the solution to this problem is fairly straightforward: One simply combines the two algorithms so that the adjustment to any given connection weight w is simply some weighted average of the adjustments given by the direct and residual-gradient algorithms. This is done using Equation 4.6:\n(4.6) gradientresidualdirectresidual www \u2212\u2206+\u2206\u2212=\u2206 \u03c6\u03c6)1( where the subscript \u201cresidual\u201d denotes that this is the residual algorithm. In the\nequation, the parameter \u03c6 modulates the degree to which the direct and residual-\ngradient algorithms contribute to the adjustment of a given weight, w. Our algorithm thus benefits from the strengths of the direct and residual-gradient algorithms by combining the speed of the former with the convergence of the latter. In the next section, we will indicate how this residual algorithm is used in conjunction with our algorithms for macro- and microscopic evolution to enable artificial neural networks that are capable of rapidly learning very large and complex problems. 5. Combining Macro- and Microscopic Evolution with the Residual Algorithm\nFigure 4 is a schematic diagram illustrating how our algorithms for\nimplementing macro- and microscopic evolution to generate artificial network topologies are combined with the residual algorithm to generate networks that are capable of solving large, complex problems via error-driven reinforcement learning. This process starts by first generating a population of simple genomes that express themselves as individual networks (i.e., as phenotypes). Each network is then trained on the same problem using the residual gradient to adjust the network\u2019s\nconnection weights. After training, each network\u2019s performance is then evaluated by summing the reward that it received (i.e., this is the measure of a network\u2019s fitness). If the overall fitness of the population improves, then the individual networks are allowed to generate the next generation via cloning the fittest individual, and via mutation and crossover. If, however, the fitness does not improve (i.e., the generation stagnates), then the microscopic evolutionary algorithm is used. (Because the latter is computationally expensive, it is only used as necessary, to \u201cnudge\u201d the evolutionary process towards a better solution.)\n-------------------------- Insert Figure 4 here --------------------------"}, {"heading": "6. General Discussion", "text": "This article has described a procedure whereby macro- and microscopic\nevolutionary algorithms are jointly used to specify the topology of artificial neural networks that are capable of learning complex, large-scale problems via errordriven reinforcement learning. In this final section, we will briefly consider both the applied and theoretical implications of this new approach to simulating learning.\nThe main practical benefit of our approach is that is provides a novel way to\nsolve problems that have here-to-fore either been too large or complex to effectively solve using standard reinforcement-learning algorithms. To cite just one example, the algorithm has been used to simulate and understand the complex patterns of eye movements that are observed when people read text (Liu & Reichle, 2010; Reichle, Liu, & Laurent, 2011). The artificial networks in these simulations were generated as described in this article and were given the task of learning to move\ntheir \u201ceyes\u201d and \u201cattention\u201d so at to \u201cread\u201d as efficiently as possible (i.e., identify sequences of virtual \u201cwords\u201d in the minimal amount of time). The networks were rewarded for each word it identified, and punished for each time step that it spent processing a given sentence. With these incentives, the agents learned to direct their eyes towards the centers of words because, with limited visual acuity, those locations afforded to most rapid identification of words. The agents also learned the modest relationship between word length and word-processing difficulty (i.e., longer words on average required more time to identify) and then learned to exploit this knowledge by initiating saccadic programming (which required some amount of time to complete) so that the eyes would move off of a word just as it was being identified. This \u201cstrategy\u201d for deciding where and when to move the eyes was optimal in that it allowed the networks to fixate each word only as long as necessary; initiating saccadic programming any sooner would cause the eyes to move too soon, whereas initiating programming any later would cause the fixations to be unnecessarily long in duration. Importantly, these simulations replicated earlier results that had been completed using a reinforcement-learning algorithm (i.e., value-iteration) that had been instantiated using a look-up table (Reichle & Laurent, 2006), but extended these results by using more complex and realistic assumptions that would not have been practically feasible using a look-up table. These simulations also extended the earlier ones by showing that the agents were able to generalize their eye-movement behaviors to novel stimuli (i.e., sentences and words that had not been encountered during training). We suspect that the capacity to simulate complex cognitive tasks and to generalize behavior to novel experiences\nwill prove to be one of the main benefits of error-driven reinforcement learning. Returning to the example of eye-movement control in reading, the networks are currently being used (Liu, 2011) to examine the issue of how attention is allocated during reading\u2014to one word at a time, in parallel to multiple words, or via some more complex, dynamic allocation scheme.\nFrom a theoretical perspective, the main contribution of our approach is that\nis suggests the important role that evolution might play in generating network topologies that are capable of integrating the principles of error-driven and reinforcement learning. The reason for this is that the network topology is an important determinant of the types of problems a network is able to learn; by evolving topologies capable of implementing error-driven learning through reinforcement, our networks are able to exploit the function-approximation capabilities of artificial neural networks using a one-dimension error signal. This effectively provides a way to exploit the strengths of artificial neural networks while sidestepping one of their main practical and theoretical limitations\u2014the requirement to provide an explicit error (i.e., \u201cteaching\u201d) signal to the networks during their training. By instantiating error-driven reinforcement learning, our networks are capable of solving the types of large-scale and complex problems that biological organisms face in their struggle to survive and reproduce, and that are of interest to researchers in various problem domains (e.g., engineering; Sejnowski, 2010).\nFinally, while both the macroscopic evolutionary and reinforcement learning\ncomponents of our approach have obvious direct analogs in biology and psychology,\nrespectively, one might wonder about the possible analog of our algorithm for microscopic evolution. As it turns out, the central assumption of this algorithm\u2014 that network connection weights are sampled from a distribution that is updated across time to reflect how well the sampled weights solve a particular problem of interest\u2014may have a direct analog in the spontaneous oscillation that is very commonly observed in many neuronal systems (Sejnowski, 2006). Such oscillation is known to occur during sleep (Sejnowski, 2000; Takashima, 2006) and has been posited to play a significant functional role in memory consolidation by organizing pre- and post-synaptic spike times, thereby facilitating spike-timing-dependent plasticity (Paulsen & Sejnowski, 2000). Although it is not known exactly how these synaptic weights changes occur, spontaneous oscillation may contribute to optimal network performance by, for example, reducing the dimensionality of the weight space to permit more efficient learning (Montague & Sejnowski, 1994; Rao & Sejnowski, 2003). Thus, on some abstract level, our microscopic evolution algorithm might be analogous to the spontaneous oscillation that has been observed in real networks and that has been posited to play an important optimization role. Future work will be necessary, however, to validate this potential correspondence."}, {"heading": "7. References", "text": "Alonso, E. & Mondrag\u00f3n, E. (2005). Associative learning and reinforcement learning:\nWhere animal learning and machine learning meet. In Z. Guessoum & E. Alonso (Eds.), Proceedings of the Fifth Symposium on Adaptive Agents and Multi-Agent Systems (AAMAS-05), 87-99.\nBaird, L. (1995). Residual algorithms: Reinforcement learning with function\napproximation. Proceedings of the Twelfth International Conference on Machine Learning (pp. 30-37). San Francisco, CA: Morgan Kaufman.\nBellman, R. (1957). Dynamic programming. Princeton, NJ: Princeton University\nPress.\n\u010cern\u00fd, V. (1985). Thermodynamical approach to the traveling salesman problem: An\nefficient simulation algorithm. Journal of Optimization Theory and Applications, 45, 41-51.\nDayan, P. & Niv, Y. (2008). Reinforcement learning and the brain: The Good, The Bad\nand The Ugly. Current Opinion in Neurobiology, 18, 185-196.\nDoya K. (1999). What are the computations of the cerebellum, the basal ganglia, and\nthe cerebral cortex. Neural Networks, 12, 961-974.\nDoya K. (2000). Complementary roles of basal ganglia and cerebellum in learning\nand motor control. Current Opinion in Neurobiology, 10, 732-739.\nGershman, S. J., Cohen, J. D., & Niv, Y. (2010). Learning to selectively attend.\nProceedings of the 32nd Annual Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society.\nHansen, N., M\u00fcller, S. D., & Koumoutsakos, P. (2003). Reducing the time complexity\nof the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary Computation, 11, 1-18.\nHansen, N. & Kern, S. (2004). Evaluating the CMA evolution strategy on multimodal\ntest functions. In X. Yao, E. Burke, J. A. Lozano, J. Smith, J. J. Merelo-Guerv\u00f3s, J. A. Bullinaria, J. Rowe, P. Tino, A. Kab\u00e1n, & H.-P. Schwefel (Eds.), Parallel Problem Solving from Nature - PPSN VIII (Vol. 3242, pp. 282-291): Springer Berlin / Heidelberg.\nHansen, N. (2006). The CMA evolution strategy: A comparing review. In J. Lozano, P.\nLarra\u00f1aga, I. Inza, & E. Bengoetxea (Eds.), Towards a New Evolutionary Computation (Vol. 192, pp. 75-102): Springer Berlin / Heidelberg.\nHolland, J. H. (1975). Adaptation in natural and artificial systems: An introductory\nanalysis with applications to biology, control, and artificial intelligence. Ann Arbor, MI: University of Michigan Press.\nKirkpatrick, S., Gelatt, C. D., Jr., & Vecchi, M. P. (1983). Optimization by simulated\nannealing. Science, 220, 671-680.\nLiu, Y.-P. (2011). Dynamic attention allocation during reading. Unpublished doctoral\ndissertation. Sun Yat-Sen University, China.\nLiu, Y.-P. & Reichle, E. D. (2010). The emergence of adaptive eye movements in\nreading. In S. Ohlsson & R. Catrabone (Eds.), Proceedings of the 32nd Annual Conference of the Cognitive Science Society (pp. 1136-1131). Austin, TX: Cognitive Science Society.\nMcClelland, J.L., McNaughton, B. L., & O'Reilly, R.C. (1995). Why there are\ncomplementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological Review, 102, 419-457.\nMcClelland, J. L., Rumelhart, D. E., & the PDP research group. (1986). Parallel\ndistributed processing: Explorations in the microstructure of cognition, vol. II. Cambridge, MA: MIT Press.\nMontague, P. R. & Sejnowski, T. J. (1994). The predictive brain: temporal coincidence\nand temporal order in synaptic learning mechanisms. Learning & Memory, 1, 1\u201333.\nNiv, Y. (2009). Reinforcement learning in the brain. The Journal of Mathematical\nPsychology, 53, 139-154.\nPaulsen, O. & Sejnowski, T. J. (2000). Natural patterns of activity and long-term\nsynaptic plasticity. Current Opinions in Neurobiology, 10, 172\u2013179.\nPavlov, I. P. (1927/1960). Conditional Reflexes. Oxford: Oxford University Press. Rao, R. P. & Sejnowski, T. J. (2003). Self-organizing neural systems based on\npredictive learning. Philosophical Transactions: Mathematical, Physical and Engineering Sciences, 361, 1149-1175.\nReichle, E. D. & Laurent, P. A. (2006). Using reinforcement learning to understand the\nemergence of \u201cintelligent\u201d eye-movement behavior during reading. Psychological Review, 113, 390-408.\nReichle, E. D., Liu, Y.-P., & Laurent, P. A. (2011). The emergence of adaptive eye\nmovement control in reading: Theory and data. Studies of Psychology and Behavior.\nRumelhart, D. E., McClelland, J. L., & the PDP research group. (1986). Parallel\ndistributed processing: Explorations in the microstructure of cognition, vol. I. Cambridge, MA: MIT Press.\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal\nrepresentations by error propagation. In Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1: Foundations (pp. 318- 362). Cambridge, MA: MIT Press.\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by\nback-propagating errors. Nature, 323, 533-536.\nSeidenberg, M. S. & McClelland, J. L. (1989). A distributed, developmental model of\nword recognition and naming. Psychological Review, 96, 523-568.\nSejnowski, T. J. & Destexhe, A. (2000). Why do we sleep? Brain Research, 886, 208-\n223.\nSejnowski, T. J. & Paulsen, O. (2006). Network oscillations: Emerging computational\nprinciples. The Journal of Neuroscience, 26, 1673-1676.\nSejnowski, T. J. (2010). Learning optimal stragies in complex environments.\nProceedings of the National Academy of Science, 107, 20151-20152.\nSkinner. B.F., (1938). The Behavior of Organisms: An Experimental Analysis.\nCambridge, Massachusetts: B.F.Skinner Foundation.\nSquire, L.R. (1993). Declarative and nondeclarative memory: Multiple brain systems\nsupporting learning and memory. In Memory Concepts: Basic and Clinical Aspects, 7th Nova Nordisk Foundation Symposium (pp. 3-25). Elsevier Science Publishers: Amsterdam.\nStanley, K. O. & Miikkulainen, R. (2002). Evolving neural networks through\naugmenting topologies. Evolutionary Computation, 10, 99-127.\nSutton R.S. (1991). Reinforcement Learning Architecture for Animals. Proceedings of\nthe first international conference on simulation of adaptive behavior on From animals to animats (pp.288-296), Cambridge, MA: MIT Press.\nSutton, R. S. & Barto, A. G. (1998). Reinforcement learning: An introduction.\nCambridge, MA: MIT Press.\nSuttorp, T., Hansen, N., & Igel, C. (2009). Efficient covariance matrix update for\nvariable metric evolution strategies. Machine Learning, 75, 167-197.\nTakashima, A., Petersson, K. M., Rutters, F., Tendolkar, I., Jensen, O., Zwarts, M. J.,\nMcNaughton, B. L. (2006). Declarative memory consolidation in humans: A prospective functional magnetic resonance imaging study. Proceedings of the National Academy of Science, 103, 756-761.\nTesauro, G. (1990). Neurogammon: A neural-network backgammon program.\nProceedings of the International Joint Conference on Neural Networks (pp. 33- 40). San Diego, CA: IEEE Press.\nTesauro, G. (1992). Practical issues in temporal difference learning. Machine\nLearning, 8, 257-277.\nThorndike, E. L. (1901). Animal intelligence: An experimental study of the\nassociative processes in animals. Psychological Review Monograph Supplement, 2, 1-109.\nWilliams, R. J. and Baird, L. C. (1993). Tight performance bounds on greedy policies\nbased on imperfect value functions. Northeastern University Technical Report NU-CCS-93-14, November.\nYe, C., Yung, N.-C., & Wang D. (2003). A fuzzy controller with supervised learning\nassisted reinforcement learing algorithm for obstacle avoidance. IEEE Transctions on Systems, Man, and Cybernectics \u2013 PART B: Cybernetics, 33, 17- 27."}, {"heading": "8. Acknowledgements", "text": "Correspondence regarding this article should be addressed to Erik Reichle, University of Pittsburgh, 635 LRDC, 3939 O\u2019Hara St., Pittsburgh, PA, 15260; or via email to: reichle@pitt.edu. The work described in this article was supported by an award from the Chinese Scholarship Council to the first author and an NIH R01 grant (HD053639) that was awarded to the second author. The source code for both evolutionary algorithm and error-driven reinforcement learning networks is available upon request from the first author."}, {"heading": "9. Appendix A", "text": "This appendix contains tables listing the parameters for the macroscopic\nevolution algorithm (see Section 2), their functional roles, and their default values.\nc2 scaling factor for excess genes 1 c3 scaling factor for gene differences 2 \u03b4c minimal distance for 2 genomes to be in same species 3"}, {"heading": "10. Appendix B", "text": "This appendix contains a table listing the parameters for the microscopic\nevolution algorithm (see Section 3), their functional roles, and their default values. (Note that n represents the number of network connections and represents the number of generations that have stagnated.)\n\u03bf\nd\u03c3 damping for step-size update\n1+ 2max 0, \u00b5eff \u22121 n +1 \u22121         + c\u03c3"}, {"heading": "11. Appendix C", "text": "This appendix describes default parameters for reinforcement learning of\nconnection weights (section 4)."}, {"heading": "11. Figure Captions", "text": "Figure 1. An example showing the coding and mapping of a single genotype (in the top panel) onto its corresponding phenotype, an artificial neural network (in the bottom panel). As indicated, the genotype codes node identity and type, as well as information about connections between nodes, including their pattern of connectivity, weight, innovation number, and whether or not they are enabled.\nFigure 2. An example illustrating two types of mutations: On the left, the mutation results in the addition of a node, while on the right, the mutation results in the addition of a connection weight between two nodes.\nFigure 3. The top panel shows the genomes of two parents, along with the corresponding networks. The middle panel shows the two genomes aligned, as would occur during crossover. The bottom panel shows the three different crossover methods (single-point, multipoint, and multipoint-average) and the resulting offspring phenotypes.\nFigure 4. A schematic diagram of the evolutionary process, in its entirety. The top box shows the population, which consists of N species of genotypes and their resulting phenotype networks. Each generation is trained on a problem using the residual-gradient reinforcement-learning algorithm. If the overall fitness of that generation improves (i.e., it does not stagnate), production procedure are allowed to occur directly (mutation or crossover or clone depended on random probability); otherwise, the CMA-ES algorithm is used to sample network connection weights and thereby produce the next generation.\nFigure 1.\nFigure 2.\nFigure 3.\nFigure 4."}], "references": [{"title": "Associative learning and reinforcement learning: Where animal learning and machine learning meet", "author": ["E. Alonso", "E. Mondrag\u00f3n"], "venue": "Proceedings of the Fifth Symposium on Adaptive Agents and Multi-Agent Systems", "citeRegEx": "Alonso and Mondrag\u00f3n,? \\Q2005\\E", "shortCiteRegEx": "Alonso and Mondrag\u00f3n", "year": 2005}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning (pp. 30-37)", "citeRegEx": "Baird,? \\Q1995\\E", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Dynamic programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm", "author": ["V. \u010cern\u00fd"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "\u010cern\u00fd,? \\Q1985\\E", "shortCiteRegEx": "\u010cern\u00fd", "year": 1985}, {"title": "Reinforcement learning and the brain: The Good, The Bad and The Ugly", "author": ["P. Dayan", "Y. Niv"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "Dayan and Niv,? \\Q2008\\E", "shortCiteRegEx": "Dayan and Niv", "year": 2008}, {"title": "What are the computations of the cerebellum, the basal ganglia, and the cerebral cortex", "author": ["K. Doya"], "venue": "Neural Networks,", "citeRegEx": "Doya,? \\Q1999\\E", "shortCiteRegEx": "Doya", "year": 1999}, {"title": "Complementary roles of basal ganglia and cerebellum in learning and motor control", "author": ["K. Doya"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "Doya,? \\Q2000\\E", "shortCiteRegEx": "Doya", "year": 2000}, {"title": "Learning to selectively attend", "author": ["S.J. Gershman", "J.D. Cohen", "Y. Niv"], "venue": "Proceedings of the 32nd Annual Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society", "citeRegEx": "Gershman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2010}, {"title": "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)", "author": ["N. Hansen", "S.D. M\u00fcller", "P. Koumoutsakos"], "venue": "Evolutionary Computation,", "citeRegEx": "Hansen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2003}, {"title": "Evaluating the CMA evolution strategy on multimodal test functions", "author": ["N. Hansen", "S. Kern"], "venue": "Parallel Problem Solving from Nature - PPSN VIII (Vol", "citeRegEx": "Hansen and Kern,? \\Q2004\\E", "shortCiteRegEx": "Hansen and Kern", "year": 2004}, {"title": "The CMA evolution strategy: A comparing review", "author": ["N. Hansen"], "venue": null, "citeRegEx": "Hansen,? \\Q2006\\E", "shortCiteRegEx": "Hansen", "year": 2006}, {"title": "Adaptation in natural and artificial systems: An introductory analysis with applications to biology, control, and artificial intelligence", "author": ["J.H. Holland"], "venue": "Ann Arbor,", "citeRegEx": "Holland,? \\Q1975\\E", "shortCiteRegEx": "Holland", "year": 1975}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "Jr.", "M.P. Vecchi"], "venue": null, "citeRegEx": "Kirkpatrick et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 1983}, {"title": "Dynamic attention allocation during reading", "author": ["Liu", "Y.-P"], "venue": "Unpublished doctoral dissertation. Sun Yat-Sen University,", "citeRegEx": "Liu and Y..P.,? \\Q2011\\E", "shortCiteRegEx": "Liu and Y..P.", "year": 2011}, {"title": "The emergence of adaptive eye movements in reading", "author": ["Liu", "Y.-P", "E.D. Reichle"], "venue": "Proceedings of the 32nd Annual Conference of the Cognitive Science Society (pp. 1136-1131)", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory", "author": ["J.L. McClelland", "B.L. McNaughton", "R.C. O'Reilly"], "venue": "Psychological Review,", "citeRegEx": "McClelland et al\\.,? \\Q1995\\E", "shortCiteRegEx": "McClelland et al\\.", "year": 1995}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. II", "author": ["J.L. McClelland", "D.E. Rumelhart"], "venue": "PDP research group", "citeRegEx": "McClelland and Rumelhart,? \\Q1986\\E", "shortCiteRegEx": "McClelland and Rumelhart", "year": 1986}, {"title": "The predictive brain: temporal coincidence and temporal order in synaptic learning mechanisms", "author": ["P.R. Montague", "T.J. Sejnowski"], "venue": "Learning & Memory,", "citeRegEx": "Montague and Sejnowski,? \\Q1994\\E", "shortCiteRegEx": "Montague and Sejnowski", "year": 1994}, {"title": "Reinforcement learning in the brain", "author": ["Y. Niv"], "venue": "The Journal of Mathematical Psychology,", "citeRegEx": "Niv,? \\Q2009\\E", "shortCiteRegEx": "Niv", "year": 2009}, {"title": "Natural patterns of activity and long-term synaptic plasticity", "author": ["O. Paulsen", "T.J. Sejnowski"], "venue": "Current Opinions in Neurobiology,", "citeRegEx": "Paulsen and Sejnowski,? \\Q2000\\E", "shortCiteRegEx": "Paulsen and Sejnowski", "year": 2000}, {"title": "Conditional Reflexes", "author": ["I.P. Pavlov"], "venue": null, "citeRegEx": "Pavlov,? \\Q1927\\E", "shortCiteRegEx": "Pavlov", "year": 1927}, {"title": "Self-organizing neural systems based on predictive learning", "author": ["R.P. Rao", "T.J. Sejnowski"], "venue": "Philosophical Transactions: Mathematical, Physical and Engineering Sciences,", "citeRegEx": "Rao and Sejnowski,? \\Q2003\\E", "shortCiteRegEx": "Rao and Sejnowski", "year": 2003}, {"title": "Using reinforcement learning to understand the emergence of \u201cintelligent\u201d eye-movement behavior during reading", "author": ["E.D. Reichle", "P.A. Laurent"], "venue": "Psychological Review,", "citeRegEx": "Reichle and Laurent,? \\Q2006\\E", "shortCiteRegEx": "Reichle and Laurent", "year": 2006}, {"title": "The emergence of adaptive eye movement control in reading: Theory and data", "author": ["E.D. Reichle", "Liu", "Y.-P", "P.A. Laurent"], "venue": "Studies of Psychology and Behavior", "citeRegEx": "Reichle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reichle et al\\.", "year": 2011}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. I", "author": ["D.E. Rumelhart", "J.L. McClelland"], "venue": "PDP research group", "citeRegEx": "Rumelhart and McClelland,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart and McClelland", "year": 1986}, {"title": "Learning internal representations by error propagation. In Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1: Foundations (pp. 318362)", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "A distributed, developmental model of word recognition and naming", "author": ["M.S. Seidenberg", "J.L. McClelland"], "venue": "Psychological Review,", "citeRegEx": "Seidenberg and McClelland,? \\Q1989\\E", "shortCiteRegEx": "Seidenberg and McClelland", "year": 1989}, {"title": "Why do we sleep", "author": ["T.J. Sejnowski", "A. Destexhe"], "venue": "Brain Research,", "citeRegEx": "Sejnowski and Destexhe,? \\Q2000\\E", "shortCiteRegEx": "Sejnowski and Destexhe", "year": 2000}, {"title": "Network oscillations: Emerging computational principles", "author": ["T.J. Sejnowski", "O. Paulsen"], "venue": "The Journal of Neuroscience,", "citeRegEx": "Sejnowski and Paulsen,? \\Q2006\\E", "shortCiteRegEx": "Sejnowski and Paulsen", "year": 2006}, {"title": "Learning optimal stragies in complex environments", "author": ["T.J. Sejnowski"], "venue": "Proceedings of the National Academy of Science,", "citeRegEx": "Sejnowski,? \\Q2010\\E", "shortCiteRegEx": "Sejnowski", "year": 2010}, {"title": "The Behavior of Organisms: An Experimental Analysis", "author": ["B.F. Skinner"], "venue": "B.F.Skinner Foundation", "citeRegEx": "Skinner.,? \\Q1938\\E", "shortCiteRegEx": "Skinner.", "year": 1938}, {"title": "Declarative and nondeclarative memory: Multiple brain systems supporting learning and memory. In Memory Concepts: Basic and Clinical Aspects, 7th Nova Nordisk Foundation Symposium (pp. 3-25)", "author": ["L.R. Squire"], "venue": null, "citeRegEx": "Squire,? \\Q1993\\E", "shortCiteRegEx": "Squire", "year": 1993}, {"title": "Evolving neural networks through augmenting topologies", "author": ["K.O. Stanley", "R. Miikkulainen"], "venue": "Evolutionary Computation,", "citeRegEx": "Stanley and Miikkulainen,? \\Q2002\\E", "shortCiteRegEx": "Stanley and Miikkulainen", "year": 2002}, {"title": "Reinforcement Learning Architecture for Animals. Proceedings of the first international conference on simulation of adaptive behavior on From animals to animats", "author": ["R.S. Sutton"], "venue": null, "citeRegEx": "Sutton,? \\Q1991\\E", "shortCiteRegEx": "Sutton", "year": 1991}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Efficient covariance matrix update for variable metric evolution strategies", "author": ["T. Suttorp", "N. Hansen", "C. Igel"], "venue": "Machine Learning,", "citeRegEx": "Suttorp et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Suttorp et al\\.", "year": 2009}, {"title": "Declarative memory consolidation in humans: A prospective functional magnetic resonance imaging study", "author": ["A. Takashima", "K.M. Petersson", "F. Rutters", "I. Tendolkar", "O. Jensen", "M.J. Zwarts", "B.L. McNaughton"], "venue": "Proceedings of the National Academy of Science,", "citeRegEx": "Takashima et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Takashima et al\\.", "year": 2006}, {"title": "Neurogammon: A neural-network backgammon program", "author": ["G. Tesauro"], "venue": "Proceedings of the International Joint Conference on Neural Networks (pp. 3340)", "citeRegEx": "Tesauro,? \\Q1990\\E", "shortCiteRegEx": "Tesauro", "year": 1990}, {"title": "Practical issues in temporal difference learning", "author": ["G. Tesauro"], "venue": "Machine Learning,", "citeRegEx": "Tesauro,? \\Q1992\\E", "shortCiteRegEx": "Tesauro", "year": 1992}, {"title": "Animal intelligence: An experimental study", "author": ["E.L. Thorndike"], "venue": null, "citeRegEx": "Thorndike,? \\Q1901\\E", "shortCiteRegEx": "Thorndike", "year": 1901}, {"title": "Tight performance bounds on greedy policies", "author": ["R. J", "L.C. Baird"], "venue": "Supplement,", "citeRegEx": "J. and Baird,? \\Q1993\\E", "shortCiteRegEx": "J. and Baird", "year": 1993}, {"title": "A fuzzy controller with supervised learning", "author": ["C. November. Ye", "Yung", "N.-C", "Wang D"], "venue": null, "citeRegEx": "Ye et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 32, "context": ", Skinner, 1938) and has instead shown that behavior is controlled by several different learning mechanisms, each operating according to its own principles (Doya, 1999, 2000; Squire, 1993).", "startOffset": 156, "endOffset": 188}, {"referenceID": 20, "context": "When viewed in this way, reinforcement learning refers to a general class of learning algorithms that has been extensively studied in psychology and that includes both classical (Pavlov, 1927) and operant (Thorndike, 1901) conditioning.", "startOffset": 178, "endOffset": 192}, {"referenceID": 40, "context": "When viewed in this way, reinforcement learning refers to a general class of learning algorithms that has been extensively studied in psychology and that includes both classical (Pavlov, 1927) and operant (Thorndike, 1901) conditioning.", "startOffset": 205, "endOffset": 222}, {"referenceID": 15, "context": "In contrast to reinforcement learning, error-driven learning refers to a type of learning in which the organism uses information about how its behavior differs from some \u201ctarget\u201d or desired behavior to modify its actual behavior so that the latter comes to resemble the former (McClelland et al., 1995; Rumelhart, Hinton, & Willams, 1986).", "startOffset": 277, "endOffset": 338}, {"referenceID": 18, "context": "But this enhanced learning requires an explicit error or teaching signal that has often undermined its applicability in both psychology (Dayan & Niv 2008; Niv, 2009; Sutton, 1991) and machine learning (Alonso & Mondrag\u00f3n, 2005; Ye, Yung, & Wang, 2003).", "startOffset": 136, "endOffset": 179}, {"referenceID": 34, "context": "But this enhanced learning requires an explicit error or teaching signal that has often undermined its applicability in both psychology (Dayan & Niv 2008; Niv, 2009; Sutton, 1991) and machine learning (Alonso & Mondrag\u00f3n, 2005; Ye, Yung, & Wang, 2003).", "startOffset": 136, "endOffset": 179}, {"referenceID": 25, "context": "In these artificial neural networks, the connection strengths between \u201cneurons\u201d are modified using a variety of different algorithms, with perhaps the most widely used of these algorithms being back-propagation (Rumelhart et al., 1986).", "startOffset": 211, "endOffset": 235}, {"referenceID": 25, "context": "Such multilayered networks are necessary to learn tasks that require non-linear mappings between input and output, such as the classic exclusive-or problem, where either of two options results to one response but neither or both of the options results in the opposite response (Rumelhart et al., 1986).", "startOffset": 277, "endOffset": 301}, {"referenceID": 11, "context": "More specifically, this approach uses a genetic algorithm (Holland, 1975) to evolve artificial neural networks that are capable of error-driven learning via reinforcement.", "startOffset": 58, "endOffset": 73}, {"referenceID": 3, "context": "Finally, the probability of mutation is determined by a simulated annealing process (\u010cern\u00fd, 1985; Kirkpatrick, Gelatt, & Vecchi, 1983) in which the overall probability is initial some large value but then declines with each successive generation.", "startOffset": 84, "endOffset": 134}, {"referenceID": 10, "context": "Microscopic Evolution (or Learning) of the Network Structure Our algorithm for microscopic evolution of network connection weights is based on the Covariance Matrix Adaptation Evolution Strategy (CMA-ES; Hansen, 2006; Hansen & Kern, 2004; Hansen, M\u00fcller, & Koumoutsakos, 2003; Hansen & Ostermeier, 2001; Iger, Hansen, & Roth, 2007; Suttorp, Hansen, & Igel, 2009).", "startOffset": 195, "endOffset": 362}, {"referenceID": 25, "context": "One way to implement this algorithm within a neural network is to use the standard error-driven back-propagation algorithm (Rumelhart et al., 1986), training the network using each state at time t, xt, as the input, and allowing the resulting state at time t+1, xt+1, as the output.", "startOffset": 123, "endOffset": 147}, {"referenceID": 1, "context": "Although this direct algorithm has been used successfully in many applications (Tesauro, 1990, 1992), it is not guaranteed to converge for general function-approximation systems (Baird, 1995).", "startOffset": 178, "endOffset": 191}, {"referenceID": 2, "context": "To develop such an algorithm, the problem can be restated as being one of predicting the outcome of a deterministic Markov chain, with the goal being to specify a value function that, for any given state, xt, will give the value of the immediate reward and the successor state, xt+1, thereby satisfying the Bellman equation (Bellman, 1957):", "startOffset": 324, "endOffset": 339}, {"referenceID": 1, "context": "The one limitation of this algorithm, however, is that it is slow (Baird, 1995; Williams & Baird, 1993).", "startOffset": 66, "endOffset": 103}, {"referenceID": 30, "context": "By instantiating error-driven reinforcement learning, our networks are capable of solving the types of large-scale and complex problems that biological organisms face in their struggle to survive and reproduce, and that are of interest to researchers in various problem domains (e.g., engineering; Sejnowski, 2010).", "startOffset": 278, "endOffset": 314}], "year": 2015, "abstractText": null, "creator": "Acrobat PDFMaker 11 for Word"}}}