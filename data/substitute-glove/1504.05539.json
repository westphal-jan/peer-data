{"id": "1504.05539", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2015", "title": "Temporal-Difference Networks", "abstract": "We promote turned algorithm of antecedent - point (TD) emphasizes however networks important subclasses predictions. Rather longer relating a longer prediction not itself morning gives saw ever, as in used TD methods, every TD operating premise but prediction prior put set including dire because certain predictions in the set in made turned on. TD telecommunications can though example apply TD understand both a kind wider example within expect few under provided others possible. Using piece simultaneous - out example, we show that because broadcast reason be system to learn alone predict was a customer variable, however true not rather by example TD methods. Secondly, let show it everyone it side - end-user relationships are made conditional weeks against, back an frequent methods - improve advantage of TD methods raised Monte Carlo (establishing aspects) approach becomes among probably. Thirdly, we demonstrate more TD wireless why understand bayesian government syntax question enable sequence solution to gave non - Markov something. A very broad range it 3-0 - predictive domains relationships can very expressed in these networks. Overall find argue that TD tv represent has meant long of the acumen of TD combining one country america end to early ninth of nine world demonstrate only ultimately predictive, grounding terms.", "histories": [["v1", "Tue, 21 Apr 2015 18:33:39 GMT  (80kb,D)", "http://arxiv.org/abs/1504.05539v1", "8 pages, 3 figures, presented at the 2004 conference on Neural Information Processing Systems. in Advances in Neural Information Processing Systems 17 (proceedings of the 2004 conference), Saul, L. K., Weiss, Y., and Bottou, L. (Eds)"]], "COMMENTS": "8 pages, 3 figures, presented at the 2004 conference on Neural Information Processing Systems. in Advances in Neural Information Processing Systems 17 (proceedings of the 2004 conference), Saul, L. K., Weiss, Y., and Bottou, L. (Eds)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard s sutton", "brian tanner"], "accepted": true, "id": "1504.05539"}, "pdf": {"name": "1504.05539.pdf", "metadata": {"source": "CRF", "title": "Temporal-Difference Networks", "authors": ["Richard S. Sutton"], "emails": ["sutton@cs.ualberta.ca", "btanner@cs.ualberta.ca"], "sections": [{"heading": null, "text": "Temporal-difference (TD) learning is widely used in reinforcement learning methods to learn moment-to-moment predictions of total future reward (value functions). In this setting, TD learning is often simpler and more data-efficient than other methods. But the idea of TD learning can be used more generally than it is in reinforcement learning. TD learning is a general method for learning predictions whenever multiple predictions are made of the same event over time, value functions being just one example. The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999). In these works, TD learning is used to predict future values of many observations or state variables of a dynamical system.\nThe essential idea of TD learning can be described as \u201clearning a guess from a guess\u201d. In all previous work, the two guesses involved were predictions of the same quantity at two points in time, for example, of the discounted future reward at successive time steps. In this paper we explore a few of the possibilities that open up when the second guess is allowed to be different from the first.\nar X\niv :1\n50 4.\n05 53\n9v 1\n[ cs\n.L G\n] 2\n1 A\npr 2\nTo be more precise, we must make a distinction between the extensive definition of a prediction, expressing its desired relationship to measurable data, and its TD definition, expressing its desired relationship to other predictions. In reinforcement learning, for example, state values are extensively defined as an expectation of the discounted sum of future rewards, while they are TD defined as the solution to the Bellman equation (a relationship to the expectation of the value of successor states, plus the immediate reward). It\u2019s the same prediction, just defined or expressed in different ways. In past work with TD methods, the TD relationship was always between predictions with identical or very similar extensive semantics. In this paper we retain the TD idea of learning predictions based on others, but allow the predictions to have different extensive semantics."}, {"heading": "1 The Learning-to-predict Problem", "text": "The problem we consider in this paper is a general one of learning to predict aspects of the interaction between a decision making agent and its environment. At each of a series of discrete time steps t, the environment generates an observation ot \u2208 O, and the agent takes an action at \u2208 A. Whereas A is an arbitrary discrete set, we assume without loss of generality that ot can be represented as a vector of bits. The action and observation events occur in sequence, o1, a1, o2, a2, o3 \u00b7 \u00b7 \u00b7, with each event of course dependent only on those preceding it. This sequence will be called experience. We are interested in predicting not just each next observation but more general, action-conditional functions of future experience, as discussed in the next section.\nIn this paper we use a random-walk problem with seven states, with left and right actions available in every state:\n11 1 2 3 4 5 6 7 00000\nThe observation upon arriving in a state consists of a special bit that is 1 only at the two ends of the walk and, in the first two of our three experiments, seven additional bits explicitly indicating the state number (only one of them is 1). This is a continuing task: reaching an end state does not end or interrupt experience. Although the sequence depends deterministically on action, we assume that the actions are selected randomly with equal probability so that the overall system can be viewed as a Markov chain.\nThe TD networks introduced in this paper can represent a wide variety of predictions, far more than can be represented by a conventional TD predictor. In this paper we take just a few steps toward more general predictions. In particular, we consider variations of the problem of prediction by a fixed interval. This is one of the simplest cases that cannot otherwise be handled by TD methods. For the seven-state random walk, we will predict the special observation bit some numbers of discrete steps in advance, first unconditionally and then conditioned on action sequences."}, {"heading": "2 TD Networks", "text": "A TD network is a network of nodes, each representing a single scalar prediction. The nodes are interconnected by links representing the TD relationships among the predictions and to the observations and actions. These links determine the extensive semantics of each prediction\u2014its desired or target relationship to the data. They represent what we seek to predict about the data as opposed to how we try to predict it. We think of these links as determining a set of questions being asked about the data, and accordingly we call them the question network. A separate set of interconnections determines the actual\ncomputational process\u2014the updating of the predictions at each node from their previous values and the current action and observation. We think of this process as providing the answers to the questions, and accordingly we call them the answer network. The question network provides targets for a learning process shaping the answer network and does not otherwise affect the behavior of the TD network. It is natural to consider changing the question network, but in this paper we take it as fixed and given.\nFigure 1a shows a suggestive example of a question network. The three squares across the top represent three observation bits. The node labeled 1 is directly connected to the first observation bit and represents a prediction that that bit will be 1 on the next time step. The node labeled 2 is similarly a prediction of the expected value of node 1 on the next step. Thus the extensive definition of Node 2\u2019s prediction is the probability that the first observation bit will be 1 two time steps from now. Node 3 similarly predicts the first observation bit three time steps in the future. Node 4 is a conventional TD prediction, in this case of the future discounted sum of the second observation bit, with discount parameter \u03b3. Its target is the familiar TD target, the data bit plus the node\u2019s own prediction on the next time step (with weightings 1\u2212\u03b3 and \u03b3 respectively). Nodes 5 and 6 predict the probability of the third observation bit being 1 if particular actions a or b are taken respectively. Node 7 is a prediction of the average of the first observation bit and Node 4\u2019s prediction, both on the next step. This is the first case where it is not easy to see or state the extensive semantics of the prediction in terms of the data. Node 8 predicts another average, this time of nodes 4 and 5, and the question it asks is even harder to express extensively. One could continue in this way, adding more and more nodes whose extensive definitions are difficult to express but which would nevertheless be completely defined as long as these local TD relationships are clear. The thinner links shown entering some nodes are meant to be a suggestion of the entirely separate answer network determining the actual computation (as opposed to the goals) of the network. In this paper we consider only simple question networks such as the left column of Figure 1a and of the action-conditional tree form shown in Figure 1b.\nMore formally and generally, let yit \u2208 [0, 1], i = 1, . . . , n, denote the prediction of the ith node at time step t. The column vector of predictions yt = (y1t , . . . , ynt )T is updated according to a vector-valued function u with modifiable parameter W:\nyt = u(yt\u22121, at\u22121, ot,Wt) \u2208 <n. (1) The update function u corresponds to the answer network, with W being the weights on its links. Before detailing that process, we turn to the question network, the defining TD relationships between nodes. The TD target zit for y i t is an arbitrary function z\ni of the successive predictions and observations. In vector form we have1\nzt = z(ot+1, y\u0303t+1) \u2208 <n, (2) where y\u0303t+1 is just like yt+1, as in (1), except calculated with the old weights before they are updated on the basis of zt: y\u0303t = u(yt\u22121, at\u22121, ot,Wt\u22121) \u2208 <n. (3) (This temporal subtlety also arises in conventional TD learning.) For example, for the nodes in Figure 1a we have z1t = o 1 t+1, z 2 t = y 1 t+1, z 3 t = y 2 t+1, z 4 t = (1\u2212 \u03b3)o2t+1 + \u03b3y4t+1, z5t = z 6 t = o 3 t+1, z 7 t = 1 2o 1 t+1 + 1 2y 4 t+1, and z 8 t = 1 2y 4 t+1 + 1 2y 5 t+1. The target functions zi are only part of specifying the question network. The other part has to do with making them potentially conditional on action and observation. For example, Node 5 in Figure 1a predicts what the third observation bit will be if action a is taken. To arrange for such semantics we introduce a new vector ct of conditions, cit, indicating the extent to which y i t is held responsible for matching zit, thus making the ith prediction conditional on c i t. Each cit is determined as an arbitrary function c i of at and yt. In vector form we have: ct = c(at, yt) \u2208 [0, 1]n. (4) For example, for Node 5 in Figure 1a, c5t = 1 if at = a, otherwise c 5 t = 0.\nEquations (2\u20134) correspond to the question network. Let us now turn to defining u, the update function for yt mentioned earlier and which corresponds to the answer network. In general u is an arbitrary function approximator, but for concreteness we define it to be of a linear form yt = \u03c3(Wtxt) (5) where xt \u2208 <m is a feature vector, Wt is an n \u00d7 m matrix, and \u03c3 is the n-vector form of the identity function (Experiments 1 and 2) or the S-shaped logistic function \u03c3(s) =\n1 1+e\u2212s (Experiment 3). The feature vector is an arbitrary function of the preceding action, observation, and node values:\nxt = x(at\u22121, ot, yt\u22121) \u2208 <m. (6) For example, xt might have one component for each observation bit, one for each possible action (one of which is 1, the rest 0), and n more for the previous node values yt\u22121. The learning algorithm for each component wijt of Wt is\nwijt+1 \u2212 w ij t = \u03b1(z i t \u2212 yit)cit\n\u2202yit\n\u2202wijt , (7)\nwhere \u03b1 is a step-size parameter. The timing details may be clarified by writing the sequence of quantities in the order in which they are computed:\nyt at ct ot+1 xt+1 y\u0303t+1 zt Wt+1 yt+1. (8) Finally, the target in the extensive sense for yt is\ny\u2217t = Et,\u03c0 { (1\u2212 ct) \u00b7 y\u2217t + ct \u00b7 z(ot+1, y\u2217t+1) } , (9)\nwhere \u00b7 represents component-wise multiplication and \u03c0 is the policy being followed, which is assumed fixed.\n1In general, z is a function of all the future predictions and observations, but in this paper we treat only the one-step case.\n3 Experiment 1: n-step Unconditional Prediction\nIn this experiment we sought to predict the observation bit precisely n steps in advance, for n = 1, 2, 5, 10, and 25. In order to predict n steps in advance, of course, we also have to predict n \u2212 1 steps in advance, n \u2212 2 steps in advance, etc., all the way down to predicting one step ahead. This is specified by a TD network consisting of a single chain of predictions like the left column of Figure 1a, but of length 25 rather than 3. Random-walk sequences were constructed by starting at the center state and then taking random actions for 50, 100, 150, and 200 steps (100 sequences each).\nWe applied a TD network and a corresponding Monte Carlo method to this data. The Monte Carlo method learned the same predictions, but learned them by comparing them to the actual outcomes in the sequence (instead of zit in (7)). This involved significant additional complexity to store the predictions until their corresponding targets were available. Both algorithms used feature vectors of 7 binary components, one for each of the seven states, all of which were zero except for the one corresponding to the current state. Both algorithms formed their predictions linearly (\u03c3(\u00b7) was the identity) and unconditionally (cit = 1 \u2200i, t). In an initial set of experiments, both algorithms were applied online with a variety of values for their step-size parameter \u03b1. Under these conditions we did not find that either algorithm was clearly better in terms of the mean square error in their predictions over the data sets. We found a clearer result when both algorithms were trained using batch updating, in which weight changes are collected \u201con the side\u201d over an experience sequence and then made all at once at the end, and the whole process is repeated until convergence. Under batch updating, convergence is to the same predictions regardless of initial conditions or \u03b1 value (as long as \u03b1 is sufficiently small), which greatly simplifies comparison of algorithms. The predictions learned under batch updating are also the same as would be computed by least squares algorithms such as LSTD(\u03bb) (Bradtke & Barto, 1996; Boyan, 2000; Lagoudakis & Parr, 2003). The errors in the final predictions are shown in Table 1.\nFor 1-step predictions, the Monte-Carlo and TD methods performed identically of course, but for longer predictions a significant difference was observed. The RMSE of the Monte Carlo method increased with prediction length whereas for the TD network it decreased. The largest standard error in any of the numbers shown in the table is 0.008, so almost all of the differences are statistically significant. TD methods appear to have a significant data-efficiency advantage over non-TD methods in this prediction-by-n context (and this task) just as they do in conventional multi-step prediction (Sutton, 1988)."}, {"heading": "4 Experiment 2: Action-conditional Prediction", "text": "The advantage of TD methods should be greater for predictions that apply only when the experience sequence unfolds in a particular way, such as when a particular sequence of actions are made. In a second experiment we sought to learn n-step-ahead predictions conditional on action selections. The question network for learning all 2-step-ahead pre-\ndictions is shown in Figure 1b. The upper two nodes predict the observation bit conditional on taking a left action (L) or a right action (R). The lower four nodes correspond to the two-step predictions, e.g., the second lower node is the prediction of what the observation bit will be if an L action is taken followed by an R action. These predictions are the same as the e-tests used in some of the work on predictive state representations (Littman, Sutton & Singh, 2002; Rudary & Singh, 2003).\nIn this experiment we used a question network like that in Figure 1b except of depth four, consisting of 30 (2+4+8+16) nodes. The conditions for each node were set to 0 or 1 depending on whether the action taken on the step matched that indicated in the figure. The feature vectors were as in the previous experiment. Now that we are conditioning on action, the problem is deterministic and \u03b1 can be set uniformly to 1. A Monte Carlo prediction can be learned only when its corresponding action sequence occurs in its entirety, but then it is complete and accurate in one step. The TD network, on the other hand, can learn from incomplete sequences but must propagate them back one level at a time. First the one-step predictions must be learned, then the two-step predictions from them, and so on. The results for online and batch training are shown in Tables 2 and 3.\nAs anticipated, the TD network learns much faster than Monte Carlo with both online and batch updating. Because the TD network learns its n step predictions based on its n \u2212 1 step predictions, it has a clear advantage for this task. Once the TD Network has seen each action in each state, it can quickly learn any prediction 2, 10, or 1000 steps in the future. Monte Carlo, on the other hand, must sample actual sequences, so each exact action sequence must be observed."}, {"heading": "5 Experiment 3: Learning a Predictive State Representation", "text": "Experiments 1 and 2 showed advantages for TD learning methods in Markov problems. The feature vectors in both experiments provided complete information about the nominal state of the random walk. In Experiment 3, on the other hand, we applied TD networks to a non-Markov version of the random-walk example, in particular, in which only the special observation bit was visible and not the state number. In this case it is not possible to make\naccurate predictions based solely on the current action and observation; the previous time step\u2019s predictions must be used as well.\nAs in the previous experiment, we sought to learn n-step predictions using actionconditional question networks of depths 2, 3, and 4. The feature vector xt consisted of three parts: a constant 1, four binary features to represent the pair of action at\u22121 and observation bit ot, and nmore features corresponding to the components of yt\u22121. The features vectors were thus of length m = 11, 19, and 35 for the three depths. In this experiment, \u03c3(\u00b7) was the S-shaped logistic function. The initial weights W0 and predictions y0 were both 0.\nFifty random-walk sequences were constructed, each of 250,000 time steps, and presented to TD networks of the three depths, with a range of step-size parameters \u03b1. We measured the RMSE of all predictions made by the networks (computed from knowledge of the task) and also the \u201cempirical RMSE,\u201d the error in the one-step prediction for the action actually taken on each step. We found that in all cases the errors approached zero over time, showing that the problem was completely solved. Figure 2 shows some representative learning curves for the depth-2 and depth-4 TD networks.\nIn ongoing experiments on other non-Markov problems we have found that TD networks do not always find such complete solutions. Other problems seem to require more than one step of history information (the one-step-preceding action and observation), though less than would be required using history information alone. Our results as a whole suggest that TD networks may provide an effective alternative learning algorithm for predictive state representations (Littman et al., 2000). Previous algorithms have been found to be effective on some tasks but not on others (e.g, Singh et al., 2003; Rudary & Singh, 2004; James & Singh, 2004). More work is needed to assess the range of effectiveness and learning rate of TD methods vis-a-vis previous methods, and to explore their combination with history information."}, {"heading": "6 Conclusion", "text": "TD networks suggest a large set of possibilities for learning to predict, and in this paper we have begun exploring the first few. Our results show that even in a fully observable setting there may be significant advantages to TD methods when learning TD-defined predictions. Our action-conditional results show that TD methods can learn dramatically faster than other methods. TD networks allow the expression of many new kinds of predictions whose extensive semantics is not immediately clear, but which are ultimately fully grounded in data. It may be fruitful to further explore the expressive potential of TD-defined predictions.\nAlthough most of our experiments have concerned the representational expressiveness and efficiency of TD-defined predictions, it is also natural to consider using them as state, as in predictive state representations. Our experiments suggest that this is a promising direction and that TD learning algorithms may have advantages over previous learning methods. Finally, we note that adding nodes to a question network produces new predictions and thus may be a way to address the discovery problem for predictive representations."}, {"heading": "Acknowledgments", "text": "The authors gratefully acknowledge the ideas and encouragement they have received in this work from Satinder Singh, Doina Precup, Michael Littman, Mark Ring, Vadim Bulitko, Eddie Rafols, Anna Koop, Tao Wang, and all the members of the rlai.net group."}], "references": [{"title": "Technical update: Least-squares temporal difference learning. Machine Learning 49:233\u2013246", "author": ["J.A. Boyan"], "venue": null, "citeRegEx": "Boyan,? \\Q2000\\E", "shortCiteRegEx": "Boyan", "year": 2000}, {"title": "Linear least-squares algorithms for temporal difference learning. Machine Learning 22(1/2/3):33\u201357", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": null, "citeRegEx": "Bradtke and Barto,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto", "year": 1996}, {"title": "Improving generalization for temporal difference learning: The successor representation", "author": ["P. Dayan"], "venue": "Neural Computation", "citeRegEx": "Dayan,? \\Q1993\\E", "shortCiteRegEx": "Dayan", "year": 1993}, {"title": "Learning and discovery of predictive state representations in dynamical systems with reset", "author": ["M. James", "S. Singh"], "venue": "In Proceedings of the Twenty-First International Conference on Machine Learning,", "citeRegEx": "James and Singh,? \\Q2004\\E", "shortCiteRegEx": "James and Singh", "year": 2004}, {"title": "Hierarchical learning in stochastic domains: Preliminary results", "author": ["L.P. Kaelbling"], "venue": "In Proceedings of the Tenth International Conference on Machine Learning,", "citeRegEx": "Kaelbling,? \\Q1993\\E", "shortCiteRegEx": "Kaelbling", "year": 1993}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research 4(Dec):1107\u20131149", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Predictive representations of state", "author": ["M.L. Littman", "R.S. Sutton", "S. Singh"], "venue": "In Advances In Neural Information Processing Systems", "citeRegEx": "Littman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2002}, {"title": "A nonlinear predictive state representation", "author": ["M.R. Rudary", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Rudary and Singh,? \\Q2004\\E", "shortCiteRegEx": "Rudary and Singh", "year": 2004}, {"title": "Learning predictive state representations", "author": ["S. Singh", "M.L. Littman", "N.K. Jong", "D. Pardoe", "P. Stone"], "venue": "In Proceedings of the Twentieth Int. Conference on Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2003}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Sutton,? \\Q1995\\E", "shortCiteRegEx": "Sutton", "year": 1995}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}], "referenceMentions": [{"referenceID": 2, "context": "The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999).", "startOffset": 121, "endOffset": 195}, {"referenceID": 4, "context": "The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999).", "startOffset": 121, "endOffset": 195}, {"referenceID": 10, "context": "The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999).", "startOffset": 121, "endOffset": 195}, {"referenceID": 0, "context": "The predictions learned under batch updating are also the same as would be computed by least squares algorithms such as LSTD(\u03bb) (Bradtke & Barto, 1996; Boyan, 2000; Lagoudakis & Parr, 2003).", "startOffset": 128, "endOffset": 189}, {"referenceID": 9, "context": "TD methods appear to have a significant data-efficiency advantage over non-TD methods in this prediction-by-n context (and this task) just as they do in conventional multi-step prediction (Sutton, 1988).", "startOffset": 188, "endOffset": 202}], "year": 2015, "abstractText": "We introduce a generalization of temporal-difference (TD) learning to networks of interrelated predictions. Rather than relating a single prediction to itself at a later time, as in conventional TD methods, a TD network relates each prediction in a set of predictions to other predictions in the set at a later time. TD networks can represent and apply TD learning to a much wider class of predictions than has previously been possible. Using a random-walk example, we show that these networks can be used to learn to predict by a fixed interval, which is not possible with conventional TD methods. Secondly, we show that if the interpredictive relationships are made conditional on action, then the usual learning-efficiency advantage of TD methods over Monte Carlo (supervised learning) methods becomes particularly pronounced. Thirdly, we demonstrate that TD networks can learn predictive state representations that enable exact solution of a non-Markov problem. A very broad range of inter-predictive temporal relationships can be expressed in these networks. Overall we argue that TD networks represent a substantial extension of the abilities of TD methods and bring us closer to the goal of representing world knowledge in entirely predictive, grounded terms. Temporal-difference (TD) learning is widely used in reinforcement learning methods to learn moment-to-moment predictions of total future reward (value functions). In this setting, TD learning is often simpler and more data-efficient than other methods. But the idea of TD learning can be used more generally than it is in reinforcement learning. TD learning is a general method for learning predictions whenever multiple predictions are made of the same event over time, value functions being just one example. The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999). In these works, TD learning is used to predict future values of many observations or state variables of a dynamical system. The essential idea of TD learning can be described as \u201clearning a guess from a guess\u201d. In all previous work, the two guesses involved were predictions of the same quantity at two points in time, for example, of the discounted future reward at successive time steps. In this paper we explore a few of the possibilities that open up when the second guess is allowed to be different from the first. ar X iv :1 50 4. 05 53 9v 1 [ cs .L G ] 2 1 A pr 2 01 5 To be more precise, we must make a distinction between the extensive definition of a prediction, expressing its desired relationship to measurable data, and its TD definition, expressing its desired relationship to other predictions. In reinforcement learning, for example, state values are extensively defined as an expectation of the discounted sum of future rewards, while they are TD defined as the solution to the Bellman equation (a relationship to the expectation of the value of successor states, plus the immediate reward). It\u2019s the same prediction, just defined or expressed in different ways. In past work with TD methods, the TD relationship was always between predictions with identical or very similar extensive semantics. In this paper we retain the TD idea of learning predictions based on others, but allow the predictions to have different extensive semantics. 1 The Learning-to-predict Problem The problem we consider in this paper is a general one of learning to predict aspects of the interaction between a decision making agent and its environment. At each of a series of discrete time steps t, the environment generates an observation ot \u2208 O, and the agent takes an action at \u2208 A. Whereas A is an arbitrary discrete set, we assume without loss of generality that ot can be represented as a vector of bits. The action and observation events occur in sequence, o1, a1, o2, a2, o3 \u00b7 \u00b7 \u00b7, with each event of course dependent only on those preceding it. This sequence will be called experience. We are interested in predicting not just each next observation but more general, action-conditional functions of future experience, as discussed in the next section. In this paper we use a random-walk problem with seven states, with left and right actions available in every state: 1 1 1 2 3 4 5 6 7 0 0 0 0 0 The observation upon arriving in a state consists of a special bit that is 1 only at the two ends of the walk and, in the first two of our three experiments, seven additional bits explicitly indicating the state number (only one of them is 1). This is a continuing task: reaching an end state does not end or interrupt experience. Although the sequence depends deterministically on action, we assume that the actions are selected randomly with equal probability so that the overall system can be viewed as a Markov chain. The TD networks introduced in this paper can represent a wide variety of predictions, far more than can be represented by a conventional TD predictor. In this paper we take just a few steps toward more general predictions. In particular, we consider variations of the problem of prediction by a fixed interval. This is one of the simplest cases that cannot otherwise be handled by TD methods. For the seven-state random walk, we will predict the special observation bit some numbers of discrete steps in advance, first unconditionally and then conditioned on action sequences.", "creator": "TeX"}}}