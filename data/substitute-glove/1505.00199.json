{"id": "1505.00199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2015", "title": "Theory of Optimizing Pseudolinear Performance Measures: Application to F-measure", "abstract": "Non - clustering record limit few early used offered set application but learning coding. For usually, $ F $ - measure any short commonly devices surprisingly measure for classification those in static curriculum through reports on-board groups. We study the theoretical of among set linearly within any - linear performance require same masquerading - generalized for measures which includes $ F $ - measure, \\ emph {Jaccard Index }, two many there. We establish that many realism of $ F $ - measures like \\ emph {Jaccard Index} any orwellian - formula_8 functions of before per - one statements negatives and false realistically without ternary, sitc and multilabel prefix. Based on this space, we the, administration reduction there notably strength measure visualization possibility take cash - noting classification problem another victim extra. We be propose any algorithm few illogical guarantees to needed an 2,500 optimal classifier next it $ F $ - measure two solving another series included only - rather classification affecting. The strength among nothing analyze an meant otherwise verify this without inhomogeneity turn without longer following inversive, steps the existing mathematics results instead dummies - linear tougher, have are finite central critical. We are policy well multi - proving indeed that made $ F $ - finish maximisation not turned linking by interpolation full set weighted - amounting simple same followed multi - objective optimization. We because empirical artificial although motivations the pressures greatly for cost discernible other physostigmine kept different linear postpositions began various $ F $ - any logarithmic tasks.", "histories": [["v1", "Fri, 1 May 2015 15:25:59 GMT  (955kb,D)", "https://arxiv.org/abs/1505.00199v1", null], ["v2", "Tue, 19 May 2015 19:21:58 GMT  (0kb,I)", "http://arxiv.org/abs/1505.00199v2", "This paper has been withdrawn by the authors. There are some serious errors in the article, theoretical results were wrong"], ["v3", "Mon, 17 Aug 2015 18:53:59 GMT  (951kb,D)", "http://arxiv.org/abs/1505.00199v3", "Extended Version"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shameem a puthiya parambath", "nicolas usunier", "yves grandvalet"], "accepted": false, "id": "1505.00199"}, "pdf": {"name": "1505.00199.pdf", "metadata": {"source": "CRF", "title": "Optimizing Pseudo-Linear Performance Measures: Application to F-measure", "authors": ["Shameem A. Puthiya Parambath", "Nicolas Usunier", "Yves Grandvalet"], "emails": ["shameem.puthiya-parambath@utc.fr", "nusunier@utc.fr", "yves.grandvalet@utc.fr"], "sections": [{"heading": null, "text": "Keywords: machine learning, cost-sensitive classification, pseudo-linear performance measures, F -score, Jaccard index"}, {"heading": "1. Introduction", "text": "Different performance measures exist to assess the efficiency of learning algorithms. Misclassification rate is the most commonly used performance measure in classification systems. Like many other measures; which we will investigate in this paper, it is defined over the set of classification outcomes. The four possible outcomes of a classifier are True Positive (tp), True Negative (tn), False Negative (fn) and False Positive (fp). Misclassification rate is a linear function of these outcomes, defined as the sum of fp and fn. Conceptually, classification algorithms solve optimization problems where we optimize a loss function corresponding\nc\u00a92015 Shameem A. Puthiya Parambath, Nicolas Usunier and Yves Grandvalet.\nar X\niv :1\n50 5.\n00 19\n9v 3\n[ cs\n.L G\n] 1\n7 A\nto the performance measure (see ??). For example, the loss function that corresponds to misclassfication rate is 0-1 loss.\nAs mentioned, misclassification rate is a commonly used performance measure, albeit unsuitable for specific class of problems. For example, consider the classification (binary) of an imbalanced dataset of size 100 with 95 being samples of one specific class (let us say negative) and 5 being other class (say positive). A trivial classifier of the form \u2018always predict negative\u2019 results in a high accuracy albeit useless classifier. In this specific example, F\u03b2 (?) can be considered as a more meaningful performance measure than misclassification rate. In general, performance measures, like F\u03b2, are extensively used in practical problems (??). One of the striking characteristics of these performance measures is the non-linearity with respect to the in-class false negatives and false positives; whereas misclassification rate is a linear function of false negatives and false positives. Moreover, there is no convex surrogate loss function that exists for such non-linear measures; specifically, there is no surrogate loss function that exists for F -measure. Another interesting property specific to F -measure and Jaccard index is: it is a sample level measure and does not decompose over individual examples. These three aspects makes the optimization problem a difficult and interesting one.\nIn the current paper, we study the theoretical and algorithmic aspects pertaining to the optimization of a set of non-linear performance measures called pseudo-linear performance measures. The commonly used performance measure F1 is an example of pseudo-linear performance measure. Less commonly used measures like Jaccard index also come under this title; among many others. Here, we focus primarily on pseudo-linear notions of F - measure. We consider the setting in which a dataset, given as a set of feature vectors, is to be classified such that the F -measure (restricted to pseudo-linear functions) of the resulting classification is (approximately) optimal. In the literature, F -measures are also often called F -scores. Here we will stick to the first terminology, which refers to the measurement of performance, in order to avoid any confusion with classification scores, that is, the realvalued scores that may be provided by classifiers and that are thresholded to produce decisions. Unless otherwise explicitly stated, all the discussion in this paper refers to F - measure optimization. At a later point, we generalize the results to other pseudo-linear measures.\nOur principle goal is to study the algorithms for empirical optimality of pseudo-linear F -measures. Given a training set, our analysis proves that Optimal F Classifier for pseudolinear F -measures can be found by minimizing the total misclassification cost of a costsensitive classification for each value of cost in an inner loop and select the best among the set of costs. Optimality in the state of the art algorithms for pseudo-linear F -measures are asymptotic whereas our results are valid in the non-asymptotic regime also. Furthermore, our analysis can be linked to the weighted-sum approach used in the multi-objective optimization. Additionally, in case of binary F\u03b2 and multilabel-macro-F\u03b2, our experimental results suggest that selecting a classifier based on minimizing the total misclassification cost is same as selecting the optimal F -measure a posteriori. Our experiments also reveals the importance of thresholding classification scores to optimize F -measures.\nThis article is an extended version of an already published conference paper (?). The article is organized as follows. Section 2 introduces basic definitions and notations used throughout the paper. It also present earlier works in F -measure optimization. Section 3\npresents the theoretical analysis, where we establish the pseudo-linearity of different practical F -measures, and prove that Optimal F Classifier can be found by minimizing the total misclassification cost of a cost-sensitive classification for a specific cost value. We derive the values for the cost vector for many pseudo-linear F -measures. We establish the multi-objective view of the F -measure optimization problem and link our cost-minimization approach to the popular weighted-sum approach for solving multi-objective optimization problems. Section 5 presents the experimental results. We study the importance of thresholding for finding optimal solutions. We conclude the paper in Section 6. The proofs of all the propositions stated in Section 3 are deferred to Appendix A."}, {"heading": "2. Background and Related Work", "text": "Here we give a brief review of the state-of-the-art methods for F -measure maximization. We start by introducing the notations used throughout in the paper; we also give the definitions of some basic quantities like F\u03b2-measure."}, {"heading": "2.1 Notation and Basic Definitions", "text": "We are given (i) a measurable space X\u00d7Y, where X is the feature space and Y is the (finite) prediction set, (ii) a probability measure \u00b5 over X \u00d7 Y, and (iii) a set of (measurable) classifiers H from the feature space X to Y. We distinguish here the prediction set Y from the label space L = {1, ..., L}: in binary or single-label multiclass classification, the prediction set Y is the label set L, but in multilabel classification, Y = 2L is the powerset of the set of possible labels. In that framework, we assume that we have an i.i.d. sample drawn from an underlying data distribution P on X \u00d7Y. The empirical distribution of this finite training (or test) sample will be denoted by P\u0302. Then, we may take P as measure \u00b5 to get results at the population level (concerning expected errors), or we may take \u00b5 = P\u0302 to get results on a finite sample. Likewise, the set of classifiers H can be a restricted set of functions such as linear classifiers if X is a finite-dimensional vector space, or may be the set of all measurable classifiers from X to Y to get results in terms of Bayes-optimal classifiers. Finally, when required, we will use bold characters for vectors and normal font with subscript for indexing.\nMost of the previous work on pseudo-linear metric is centered around F\u03b2-measure in binary settings. F\u03b2-measure is defined as the weighted harmonic mean of precision and recall. Precision is defined as the fraction of predicted positive instances that are indeed positive and recall is defined as the fraction of positive instances that are correctly predicted as positive. Formally, we can define these metrics using classifier outcomes. Given a binary dataset and classifier, tp corresponds to the correct prediction of a positive label, tn corresponds to the correct prediction of a negative label, fn corresponds to the incorrect prediction of a positive label as a negative label, and fp corresponds to the incorrect prediction of the negative label as positive. In general, these outcomes are depicted using a confusion matrix, also called contingency table (See Table 2). In terms of the classification outcomes (tp, tn, fn, fp), we formally define precision, recall and F\u03b2 associated with a binary classifier h \u2208 H for a given sample (x,y) \u2208 (X \u00d7 Y)n as:\n(precision) Precision(h(x),y) = \u2211n i=1 tp(h(xi))\u2211n\ni=1[tp(h(xi)) + fp(h(xi))\n(recall) Recall(h(x),y) = \u2211n i=1 tp(h(xi))\u2211n\ni=1[tp(h(xi)) + fn(h(xi))]\n(binary\u2212F\u03b2) F\u03b2(h(x),y) = (1 + \u03b22) \u2211n i=1 tp(h(xi))\u2211n\ni=1[(1 + \u03b2 2)tp(h(xi)) + \u03b22fn(h(xi)) + fp(h(xi))]\nIn the above, dependence of label vector y on classification outcome is omitted for convenience. The parameter \u03b2 weights precision and recall in F\u03b2: F0 corresponds to precision, F\u221e corresponds to recall, and F1, the most widely used, corresponds to equal weights. In case of the example mentioned in the introduction, classifying a sample of 100 instances, the trivial classifier gives precision, recall and F1 values to 0. Precision does not consider false negatives, and recall does not consider false positives. So in practical problems, F\u03b2 is preferred. One thing to note: unlike misclassification rate, F -measure is not invariant under label switching i.e. if we change the positive label to negative, we get a different F -measure. Hence it is used in problems where correct classification of minority label is of vital importance. In multilabel and multiclass settings, three different definitions of F - measure can be found; namely instance-wise, macro and micro F -measures. We will give formal definition of these in Section 3 in connection with our theoretical framework."}, {"heading": "2.2 Related Work", "text": "F -measure optimization had been studied on a limited basis in the past (?????). Last couple of years witnessed an increasing interest in this domain (?????????). Majority of the work was confined to F -measure maximization in binary classification settings, whereas very little work was done on multilabel and multiclass F -measure maximization tasks (??). ? suggested an algorithm for finding locally maximal F1-measure for binary classification problems by approximating the classification outcomes using logistic models. Since the objective function used is non-convex, the algorithm does not guarantee optimality. This issue is addressed by running the procedure multiple times and selecting the best in hand. The orthogonal problem of infering the hypothesis with optimal F1 from a probabilistic model is discussed by (?). In the scientific literature, the two problem formulation has been referred to as empirical utility maximization (EUM) and decision-theoretic aproach (DTA) respectively (?).\nThe two formulations differ with respect to the definition of the expected F -measure. In case of the EUM based approach, population F -measure is defined as the F -measure of the expected tp,fp and fn. Formally, In EUM, expected F -measure is defined as,\nF EUM\u03b2 (h) = (1 + \u03b22)E[tp(h(x))]\n(1 + \u03b22)E[tp(h(x))] + \u03b22E[fn(h(x))] + E[fp(h(x))]\nAn optimal EUM classifier can be defined as,\nh\u2217 = argmax h\u2208H F EUM\u03b2 (h)\nIn DTA, assuming a probability distribution p(Y ) on {0, 1}n, expected F -measure is formally defined as,\nFDTA\u03b2 (h) = Ey\u223cp(Y )[F\u03b2(h(x),y)] An optimal DTA classifier is of the form\nh\u2217 = argmax h\u2208H \u2211 y\u2208{0,1}n F\u03b2(h(x),y)p(y)\nFrom an algorithmic point of view, DTA based algorithms are computationally more expensive than EUM algorithms. DTA based algorithms require an efficient method to estimate the joint probability and iterate over exponentially many combinations of h and y; and the problem of estimating exact probabilities is as hard as the original problem. But assuming i.i.d samples and considering the functional properties of F -measure (it is a function of integer counts (tp, fp, fn)), the above problem can be solved more efficiently. The algorithm given by ? runs in O(n4), where n is the number of examples. ? improved the efficiency of this algorithm, leading to a complexity in O(n3), using dynamic programming methodology. They also remark that the optimal classifier for binary F1 is of the form sign(p(y = 1|x)\u2212\u03b4\u2217), where \u03b4\u2217 is a threshold score dependent on the underlying distribution. ? extended the algorithm given by ? with dependence assumption and given a method to calculate optimal F classifier with O(n3) complexity in time, given n2 +1 parameters of the joint distribution p(y). This algorithm was used in a multilabel setting for instance-wise F -measure (see Remark 3). In addition to the high computational footprint, there is no optimality guarantee on finite samples. In general, optimality in DTA algorithms are asymptotic in nature (?).\nOn the other hand, EUM based approaches are computationally less demanding, and are based on structured risk minimization (SRM) principle. Here we minimize an approximate surrogate loss function, and select the hypothesis with minimal error on the validation set. The most commonly employed EUM approach is to threshold the score obtained using linear classifiers like logistic regression or support vector machines (SVM) such that F1 is maximized. An approximate surrogate function based approach named SVMperf is given by ?, based on the observation that F1 is a sample level measure. In the suggested method, the discriminant function is defined over the linear combination of the feature vectors, where the scalar multiplier is the label associated with each feature vector in the training sample. Even though the reported experimental results were promising, the method does not offer any theoretical optimality guarantee. Moreover, our experiments establish that SVMperf is a sub-optimal method. ? also advocated for SVMs with asymmetric costs (that is, with different costs for false negatives and false positives) for F1-measure optimization in binary classification. However, their argument, specific to SVMs, is not methodological but technical (relaxation of the maximization problem).\nIn case of multilabel classification, ? argued that the multilabel-micro-F -measure can be optimized by thresholding the class confidence score, one label at a time. ? used knearest neighbours and SVM to generate scores. In general, thresholding cost-insensitive SVM scores does not guarantee empirical optimality, and the paper does not address the issue of hyperparameter selection of the backend algorithm (k of k-nearest neighbor and regularization co-efficient of SVM).\n? tackle the problem by combining different classification models. They combined two logistic models, (i) maximum likelihood logistic regression and (ii) approximate logistic\napproximation (see ?) to maximize multilabel micro, macro and instance-wise F -measure. This line of work comes under multiple classifier systems. Multiple classifier systems are not widely used for F -measure maximization, and are still in nascent stages. In our knowledge, no proper statistical study regarding the optimality of the multiple classifier systems for F -measure maximization is done so far.\nApart from F -measure, some of the most recent work discusses non-linear performance measures like Jaccard index (???). Following the footsteps of ?, ?? proposed algorithms to maximize linear-fractional performance performance measure by thresholding the class confidence score. But as mentioned earlier, results hold only asymptotically.\nIn this work, we aim to perform empirical risk minimization-type learning, that is, to find a classifier with highest population level F -measure by maximizing its empirical counterpart. In that sense, we follow the EUM framework. Nonetheless, regardless of how we define the generalization performance, our results can be used to maximize the empirical value of the F\u03b2-measure."}, {"heading": "3. Theoretical Framework and Analysis", "text": "In this section, we present the theoretical framework which is at the heart of this work. Our results are mainly motivated by the maximization of F -measures for binary, multiclass, and multilabel classification. They rely on a general property of these performance measures, namely their pseudo-linearity with respect to the false negative and false positive probabilities.\nFor binary classification, we prove that, in order to optimize the F -measure, it is sufficient to solve a binary classification problem with different costs allocated to false positive and false negative errors (Proposition 4). However, these costs are not known a priori, so in practice we propose to learn several classifiers with different costs, and to select the best one according to the F -measure in a second step. Propositions 5 and 6 provide approximation guarantees on the F -measure we can obtain by following this principle depending on the granularity of the search in the cost interval.\nWe first establish the results for the F\u03b2-measures in binary classification, and then extend to other cases of F -measures with similar functional forms that are used in multiclass and multilabel classification. We also briefly describe pseudo-linear notions of Jaccard index, which can also be solved using our framework. For that reason, we present the results and proofs for the binary case, succeeded by multiclass and multilabel F -measures."}, {"heading": "3.1 Error Profiles and Pseudo-Linearity", "text": ""}, {"heading": "3.1.1 Error Profiles", "text": "The performance of a classifier h on distribution \u00b5 can be summarized by the elements of the contingency table (See Table 2) which contains the summary of errors. For all classification tasks (binary, multiclass and multilabel), the F -measures we consider here are functions of this non-diagonal elements of contingency table, which themselves are defined in terms of the marginal probabilities of classes and the per-class false negative/false positive probabilities. The marginal probabilities of label k will be denoted by Pk, and the per-class false negative/false positive probabilities of a classifier h are denoted by FNk(h) and FPk(h).\nTheir definitions are given below:\n( binary/multiclass) Pk = \u00b5({(x, y)|y = k}), FNk(h) = \u00b5({(x, y)|y = k and h(x) 6= k}) , FPk(h) = \u00b5({(x, y)|y 6= k and h(x) = k}) .\n(multilabel) Pk = \u00b5({(x, y)|y \u2208 k}), FNk(h) = \u00b5({(x, y)|k \u2208 y and k 6\u2208 h(x)}) , FPk(h) = \u00b5({(x, y)|y 6\u2208 k and k \u2208 h(x)}) .\nThese probabilities of a classifier h are then summarized by the error profile E(h):\nE(h) = ( FN1(h) , FP1(h) , ..., FNL(h) , FPL(h) ) \u2208 R2L ."}, {"heading": "3.1.2 Pseudo-Linear Functions", "text": "Throughout the paper, we rely on the notion of pseudo-linearity of a function, which is itself defined from the notion of pseudo-convexity (See ?, Definition 3.2.1): a differentiable function F : D \u2282 Rd \u2192 R, defined on a convex open subset of Rd, is pseudo-convex if\n\u2200e, e\u2032 \u2208 D , F (e) > F (e\u2032) \u21d2 \u2329 \u2207F (e), e\u2032 \u2212 e \u232a < 0 ,\nwhere \u3008., .\u3009 is the canonical dot product on Rd. Moreover, F is pseudo-linear if both F and \u2212F are pseudo-convex. In practice, working with gradients of non-linear functions may be cumbersome, so we will use the following characterization, which is a rephrasing of ?, Theorem 3.3.9, basically stating that level sets of pseudo-linear functions are hyperplanes:\nTheorem 1 (?) A non-constant function F : D \u2192 R, defined and differentiable on the open convex set D \u2286 Rd, is pseudo-linear on D if and only if \u2200e \u2208 D , \u2207F (e) 6= 0 , and: \u2203a : R\u2192 Rd and \u2203b : R\u2192 R such that, for any t in the image of F :\nF (e) \u2265 t \u21d4 \u3008a(t), e\u3009+ b(t) \u2264 0 and F (e) \u2264 t \u21d4 \u3008a(t) , e\u3009+ b(t) \u2265 0 .\nPseudo-linearity is the main property of linear-fractional functions (ratios of linear functions).\nProposition 2 (Linear-fractional function) A linear-fractional function F : D \u2286 Rd \u2192 R is the ratio of linear functions, F (e) = \u03b10+\u3008\u03b3,e\u3009\u03b11+\u3008\u03b4,e\u3009 . A non-constant linear-fractional function is pseudo-linear on the open half-space D = { e \u2208 Rd|\u03b11 + \u3008\u03b4, e\u3009 > 0, \u03b11 6= 0 } ."}, {"heading": "3.2 Pseudo-Linearity of F -measures", "text": "Several notions of F -measures used in practical problems are pseudo-linear. Here, we establish that binary F\u03b2 and multiclass/multilabel macro/micro F -measures are pseudo-linear functions."}, {"heading": "3.2.1 Binary Classification", "text": "In binary classification, we have FN2 = FP1 and we can write F -measures only by reference to class 1. Then, for any \u03b2 > 0 and any binary classifier h, the F\u03b2-measure is\nF\u03b2(h) = (1 + \u03b22)(P1 \u2212 FN1(h))\n(1 + \u03b22)P1 + FP1(h)\u2212 FN1(h) .\nWe can immediately notice that F\u03b2 is linear-fractional and hence by Proposition 2 it is pseudo-linear in FN1 and FP1. Thus, with a slight (yet convenient) abuse of notation, we write the F\u03b2-measure for binary classification as a function of vectors in R4 = R2L:\n(binary) \u2200e \u2208 R4, F\u03b2(e) = (1 + \u03b22)(P1 \u2212 e1)\n(1 + \u03b22)P1 + e2 \u2212 e1\nwhere ei represents the i th element of the error profile e. A surface plot of F1 as a function of FN1 and FP1 with level sets is given in Figure 1. As the Theorem 1 states, it can be easily verified from the plot that level sets are hyperplanes.\nIn the above, ei represents the i th element of the error profile e \u2208 E. A surface plot of F1 as a function of FN1 and FP1 is given in Figure 1. It can be easily verified from the plot that level sets are hyperplanes."}, {"heading": "3.2.2 Multilabel Classification", "text": "In multilabel classification, there are several definitions of F -measures. For those based on the error profiles, we first have the macro-F -measure (denoted by MF\u03b2), which is the average over class labels of the F\u03b2-measure of each binary classification problem associated to the prediction of the presence/absence of a given class:\n(multilabel\u2013Macro) MF\u03b2(e) = 1\nL L\u2211 k=1 (1 + \u03b22)(Pk \u2212 e2k\u22121) (1 + \u03b22)Pk + e2k \u2212 e2k\u22121 .\nMF\u03b2 is not a pseudo-linear function of an error profile e. However, if the multilabel classification algorithm learns independent binary classifiers for each class (a method known as one-vs-rest or binary relevance, see e.g. ?), then the k-th binary problem depends only\non e2k\u22121 and e2k. The maximization of the macro-F -measure with respect to all binary classifiers is then a separable problem which boils down to independently maximizing the F\u03b2-measure for L binary classification problems. In other words, optimizing MF\u03b2 consists in maximizing the pseudo-linear functions in e2k\u22121 and e2k that correspond to each F\u03b2 optimization. There are also micro-F -measures for multilabel classification. They correspond to F\u03b2-measures for a new binary classification problem over X \u00d7 L, in which one maps a multilabel classifier h : X \u2192 Y (Y is here the power set of L) to the following binary classifier h\u0303 : X \u00d7 L \u2192 {0, 1}: we have h\u0303(x, k) = 1 if k \u2208 h(x), and 0 otherwise. The micro-F\u03b2-measure, written as a function of an error profile e and denoted by mF\u03b2(e), is the F\u03b2-measure of h\u0303 and can be written as:\n(multilabel\u2013micro) mF\u03b2(e) = (1 + \u03b22)\n\u2211L k=1(Pk \u2212 e2k\u22121)\n(1 + \u03b22) \u2211L k=1 Pk + \u2211L k=1(e2k \u2212 e2k\u22121) .\nThis function is also linear-fractional, and thus pseudo-linear in e."}, {"heading": "3.2.3 Multiclass Classification", "text": "The last example we take is from multiclass classification. It differs from multilabel classification in that a single class must be predicted for each example. This restriction imposes strong global constraints that make the multiclass classification significantly harder. As for the multilabel case, there are many definitions of F -measures for multiclass classification, and in fact several definitions for the micro-F -measure itself. We will focus on the following one, which is used in information extraction (e.g in the BioNLP Challenge ?). Given L class labels, we will assume that label 1 corresponds to a \u201cdefault\u201d class, the prediction of which is considered as not important. In information extraction, the default class corresponds to the (majority) case where no information should be extracted. Then, a false negative is an example (x, y) such that y 6= 1 and h(x) 6= y, while a false positive is an example (x, y) such that y = 1 and h(x) 6= y. This micro-F -measure, denoted mcF\u03b2 can be written as:\n(multiclass\u2013micro) mcF\u03b2(e) = (1 + \u03b22)(1\u2212 P1 \u2212\n\u2211L k=2 e2k\u22121)\n(1 + \u03b22)(1\u2212 P1)\u2212 \u2211L k=2 e2k\u22121 + e1 .\nOnce again, this kind of micro-F\u03b2-measure is linear-fractional and hence pseudo-linear in e.\nRemark 3 (Non-pseudo-linear F-measures) In multilabel settings, notion of instancewise F\u03b2 has been used in the past (??????). It is similar to the micro-F -measure (mF\u03b2) for multilabel case defined above, but defined over samples (instances) instead of labels. It is defined as the average of the per-instance F -measure. Hence, we calculate the F - measures for each instance independently (i.e. estimate mF\u03b2 for each individual example by calculating tp, fp, fn for each example in the sample) and take the average (arithmetic mean) over the number of samples. This measure can not be written as a linear-fractional function of \u201cerror profile\u201d terms, hence it can not be solved using our framework."}, {"heading": "3.3 Optimizing F -Measure by Reduction to Cost-Sensitive Classification", "text": "The F\u03b2-measures presented above are non-linear aggregations of false negative/positive propotions that can not be written in the usual expected loss minimization framework;\nusual learning algorithms are thus, intrinsically, not designed to optimize this kind of performance measures. We show in Proposition 4 that the optimal classifier for a cost-sensitive classification problem with label dependent costs (??) is also an optimal classifier for the pseudo-linear F -measures (within a specific, yet arbitrary classifier set H). In cost-sensitive classification, each entry of the error profile is weighted asymmetrically by a non-negative cost, and the goal is to minimize the weighted average error. Efficient, consistent algorithms exist for such cost-sensitive problems (???). Even though the costs corresponding to the optimal F -measure are not known a priori, we show in Proposition 5 that we can approximate the optimal classifier with approximate costs. These costs, explicitly expressed in terms of the optimal F -measure, motivate a practical algorithm. Even though the discussion in this section is more general and applies to any pseudo-linear functions, we start with the discussion in binary settings. We give the proofs and results for binary F\u03b2 and extend the results to multilabel and multiclass F -measures in Section 3.4."}, {"heading": "3.3.1 Reduction to Cost-Sensitive Classification", "text": "Let F : D \u2282 Rd \u2192 R be a fixed pseudo-linear function. We denote by a : R \u2192 Rd the function mapping values of F to the corresponding level set of Theorem 1. We assume that the distribution \u00b5 is fixed, as well as the (arbitrary) set of classifier H. We denote by E (H) the closure of the image of H under E, i.e. E (H) = cl({E(h) , h \u2208 H}) (the closure ensures that E (H) is compact and that minima/maxima are well-defined), and we assume E (H) \u2286 D. Finally, for the sake of discussion with cost-sensitive classification, we assume that a(t) \u2208 Rd+ for any e \u2208 E (H), that is, lower values of errors entail higher values of F .\nProposition 4 Let F ? = max e\u2208E(H) F (e). We have: e? \u2208 argmin e\u2208E(H)\n\u2329 a ( F ? ) , e \u232a \u21d4 F (e?) = F ?.\nThis proposition shows that a ( F ? )\nare the cost vectors, which are orthogonal to the level set of F at F ? and may not need to be unique, that should be assigned to the error profile in order to find the optimal classifier in H with respect to the measure F . Hence maximizing F amounts to minimizing \u2329 a ( F ? ) ,E(h) \u232a with respect to h, that is, amounts to solving a cost-sensitive classification problem. This observation suggests that the optimization of pseudo-linear measures could be a wrapper of cost-sensitive classification algorithms. The costs a ( F ? )\nare, however, not known a priori. The following result shows that having only approximate costs is sufficient to have an approximately optimal solution, which gives us the main step towards a practical solution.\nProposition 5 Let \u03b50 \u2265 0 and \u03b51 \u2265 0, and assume that there exists \u03a6 > 0 such that for all e, e\u2032 \u2208 E (H) satisfying F (e\u2032) > F (e), we have:\nF ( e\u2032 ) \u2212 F (e) \u2264 \u03a6 \u2329 a ( F (e\u2032) ) , e\u2212 e\u2032 \u232a .\nThen, let us take e? \u2208 argmaxe\u2032\u2208E(H) F (e\u2032), and denote a? = a(F (e?)). Let furthermore a\u0302 \u2208 Rd+ and h \u2208 H satisfying the following conditions:\n(i) \u2016a\u0302\u2212 a?\u20162 \u2264 \u03b50 , (ii) \u3008a\u0302, e\u3009 \u2264 min e\u2032\u2208E(H)\n\u2329 a\u0302, e\u2032 \u232a + \u03b51 .\nWe have: \u2200e \u2208 E (H) , F (e) \u2265 F (e?)\u2212 \u03a6 \u00b7 (2\u03b50M + \u03b51) , where M = max e\u2032\u2208E(H)\n\u2225\u2225e\u2032\u2225\u2225 2 .\nThe above proposition suggests that pseudo-linear measures could be optimized by wrapping cost-sensitive classification in an inner loop with an outer loop setting the appropriate costs. This proposition also gives an upper bound on the achievable optimal F -score. This value depends on the size of the maximum error associated with the given hypothesis space,M , measured in `2 sense and the constant \u03a6. The value of M depends on the selected hypothesis class (E (H)). We call \u03a6 as discretization factor as it defines the granularity of the approximation. It depends on the specific form of F -measure and training sample. We can find an approximately optimal classifier using a procedure, where we search for an approximately optimal cost and associated error profile by iterating through the preselected cost interval in small steps. Thus searching for a cost such that \u03b50 is close to zero, we can find an approximately optimal F classifier. \u03b51 can be regarded as the approximation guarantee provided by the underlying cost-sensitive classification algorithm. Practical implementations use convex surrogate loss instead of the non-convex 0-1 loss. A discussion on convex approxmiation of 0-1 loss can be found in (?). \u03a6, the discretization factor gives the magnitude of the step size. A larger value of \u03a6 indicates more fine-grained discretization (very small step size), and a smaller value of \u03a6 indicates coarse- grained discretization. Later, we will derive the exact values of \u03a6 and the cost interval for specific F -measures."}, {"heading": "3.3.2 Discretization Factor and Cost Interval for F\u03b2", "text": "Here, we derive the values of the discretization factor (\u03a6) and the range of the cost interval (a) for binary F\u03b2-measure.\nProposition 6 F\u03b2 defined in Section 3.2.1 satisfy the conditions of Proposition 5 with:\n(binary) F\u03b2 : \u03a6 = 1\n\u03b22P1 and a : t \u2208 [0, 1] 7\u2192 (1+\u03b22\u2212t, t, 0, 0) .\nThis proposition gives the exact values of \u03a6 and the range for a in binary settings. Here the discretization factor depends on the marginal probability of the positive class (assume label 1 represents positive class). A larger value of the discretization factor demands smaller step size in the cost interval. Looking at the approximation guarantee in proposition 5, with a larger value of \u03a6, reasonable approximation can be obtained by taking \u03b50 close to zero. Intuitively, we can think of this as follows, higher values of \u03a6 indicates a highly imbalanced data with very few positive examples, hence to eliminate the influence of class-imbalance, we need to discretize in smaller step through cost interval. Given the error profile (in the form of contingency table) and associated costs as a matrix, as shown in in Figure 2, corresponding F\u03b2-measure is the sum of the elements of the Hadamard product of the two matrices.\nCorollary 7 For the F1-measure, the optimal classifier is the solution to the cost-sensitive binary classifier with costs ( 1\u2212 F ?2 , F ? 2 ) This proposition extends the result obtained by ? to the non-asymptotic regime. If we take H as the set of all measurable functions, the Bayes-optimal classifier for this cost is to predict class 1 when \u00b5(y = 1|x) \u2265 F ?2 (see ??)."}, {"heading": "3.3.3 Algorithm for F\u03b2 Maximization", "text": "Based on the above results, we give a practical algorithm to find optimal F\u03b2. In case of F\u03b2, the cost function a : [0, 1] \u2192 Rd, which assigns costs to probabilities of error, is Lipschitzcontinuous with Lipschitz constant (\u03c6) = max(1, \u03b22). Hence it is sufficient to discretize the interval [0, 1] to have a set of evenly spaced values {t1, ..., tC} (say, tj+1 \u2212 tj = \u03b50/\u03c6) to obtain an \u03b50-cover {a(t1), ...,a(tC)} of the possible costs. Using the approximate guarantee of Proposition 5, learning a cost-sensitive classifier (hi) for each a(ti) and selecting the one with minimum total misclassification cost(\u3008a(ti), hi(e)\u3009) on a validation set is sufficient to obtain a \u03a6(2\u03b50M + \u03b51)-optimal solution. Our experimental results suggest that, in binary classification choosing a classifier by our proposed method is same as selecting a classifier with optimal F -measure a posteriori. Hence our final algorithm consists of selecting a cost-sensitive classifier with optimal F -score.Our suggested algorithm is presented in Algorithm 1.\nAlgorithm 1 Optimization of the F\u03b2-measure\n1: procedure Optimize F\u03b2(D,\u03b2) . D = Data, \u03b2 = \u03b2 in F\u03b2 2: bF = 0 3: Split Training Data into two Dtra, Dval 4: for t = (0 . . . 1 + \u03b22) do . approximate cost 5: \u03c6, \u03b8, F = F cs learner(Dtra, Dval, t); . learn cost-sensitive model 6: if F > bF then 7: \u03a6 = \u03c6, \u0398 = \u03b8, bF = F ; 8: end if 9: end for\n10: return (\u03a6, \u0398) 11: end procedure\nThe cost-sensitive classification algorithms that are used in the inner loop (step 5) returns the trained model. The predict score method in the meta-algorithm simply returns the scores (score can be posterior probability, or geometric margin etc) on the validation set and computeF\u03b2 returns the optimal F -measure and a score threshold (if any) on the validation data. Even though our theoretical results do not suggest thresholding the scores\nAlgorithm 2 Cost-Sensitive Learner for F\u03b2\n1: procedure F cs learner(Dtra, Dval, t) . Dtra = Training Data, Dval = Validation Data, t=cost 2: bF = 0 3: for \u03c8 \u2208 \u03a8 do . \u03a8 = set of tunable cost-sensitive algorithm hyper-parameter 4: \u03c6 = cost sensitive learner(Dtra, t, \u03c8); . generic cost-sensitive learner 5: \u03b8, F= computeF\u03b2(\u03c6,Dval, \u03b2) . get optimal threshold and F\u03b2 6: if F > bF then 7: \u03a6 = \u03c6, \u0398 = \u03b8, bF = F ; 8: end if 9: end for\n10: return (\u03a6, \u0398, F ) 11: end procedure\na posteriori, experimental results indicate the need for a posterior thresholding of the scores. We will elaborate on this point in Section 5. This meta-algorithm can be instantiated with any cost-sensitive learning algorithm. The actual algorithm may simply consist of adjusting the hyper-parameters of a cost-insensitive classifier so as to optimize cost-sensitive classification, as in many practical implementation of cost-sensitive algorithm. This rudimentary approach results in considerable savings in computation time."}, {"heading": "3.4 Beyond Binary F -measure", "text": "As mentioned earlier, many notions of F -measures in multiclass and multilabel problems are pseudo-linear and can be solved using our framework. Here, we derive the values of cost vector (a) and discretization factor (\u03a6), and give optimal F -measure algorithm for pseudo-linear F -measures described in Sections 3.2.2 and 3.2.3.\n3.4.1 Multilabel micro-F -measure\nProposition 8 multilabel micro-F (mF\u03b2) defined in Section 3.2.2 satisfies the conditions of Proposition 5 with:\n(multilabel\u2013micro) mF\u03b2 : \u03a6 = 1 \u03b22 \u2211L k=1 Pk and ai(t) = { 1 + \u03b22 \u2212 t if i is odd t if i is even .\nHere the discretization factor depends on the sum of marginal probabilities of each label. A large value of \u03a6 indicates that majority of the labels are rare, and smaller value of \u03a6 indicates that few labels are rare. Since the impact of misclassification of rare labels does not influence the micro-F -measure to a greater extend (F -score is independent of true negatives), we have to discretize in a smaller step only if the majority of the classes are rare. Given the above result on cost vector a and discretization factor \u03a6, and following the arguments given for F\u03b2 (here also the cost function a is Lipschitz-continuous with Lipschitz constant taking value max(1, \u03b22)), we can develop an algorithm for finding optimal classifier formF\u03b2. Unlike in binary case, here we run cost-sensitive learner with discretized cost values\nto find the classifier with lowest total misclassification cost(\u3008a(ti), hi(e)\u3009). Our proposed algorithm is given in Algorithm 3. The algorithm is similar to the F\u03b2 algorithm given in Algorithm 1, except for the fact that here we minimize the total misclassification cost instead of maximixing empirical F\u03b2 in the inner loop. Also, here we need the cardinality of the label space as an additional input parameter. Here the outer loop calculates the cost (a(t)) for each value of t as given in proposition 8. The selected threshold is the one which minimizes the total misclassification cost (\u3008a(t), e\u3009) over all possible values of a(t) and e.\nAlgorithm 3 Optimization of the mF\u03b2-measure\n1: procedure Optimize mF\u03b2(D,L,\u03b2) . D = Data, L = |L|, \u03b2 = \u03b2 in F\u03b2 2: bC = +\u221e 3: bmF = 0 4: Split Training Data into two Dtra, Dval 5: for t = (0 . . . 1 + \u03b22) do . Approximate Cost 6: \u03a0 = gen mF\u03b2 cost vector(L, t, \u03b2) . Cost Vector 7: \u03c6, \u03b8 = mF cs learner(Dtra, Dval,\u03a0) . learn cost-sensitive model 8: \u03b8,mF = computemF\u03b2(\u03c6,Dval, \u03b8, \u03b2) . get the optimal threshold and mF\u03b2 9: if (mF > bmF ) then\n10: bmF = mF, \u03a6 = \u03c6, \u0398 = \u03b8; 11: end if 12: end for 13: return (\u03a6,\u0398) 14: end procedure\nAlgorithm 4 Cost-Sensitive Learner for mF\u03b2\n1: procedure mF cs learner(Dtra, Dval,\u03a0) . Dtra = Training Data, Dval = Validation Data, \u03a0=cost 2: bC = +\u221e 3: for \u03c8 \u2208 \u03a8 do . \u03a8 = set of tunable cost-sensitive algorithm hyper-parameter 4: \u03c6 = cost sensitive learner(Dtra,\u03a0, \u03c8); . generic cost-sensitive learner 5: \u03b8, C= compute cost(\u03c6,Dval,\u03a0) . get optimal threshold and total\nmisclassification cost 6: if (C < bC) then 7: \u03a6 = \u03c6, \u0398 = \u03b8; 8: end if 9: end for\n10: return (\u03a6, \u0398) 11: end procedure\n3.4.2 Multiclass micro-F -measure\nProposition 9 multiclass micro-F (mcF\u03b2) defined in Section 3.2.3 satisfies the conditions of Proposition 5 with:\n(multiclass\u2013micro) mcF\u03b2 : \u03a6 = 1\n\u03b22(1\u2212 P1) and ai(t) =  1 + \u03b22 \u2212 t if i is odd and i 6= 1 t if i = 1\n0 otherwise\n.\nFollowing the arguments given for multilabel micro-F -measure, we can use the Algorithm 3 for finding optimal mcF\u03b2 with a small modification to the gen mF\u03b2 cost vector method. The new cost generation method for multiclass micro-F -measure follows result of proposition 9.\nRemark 10 (Beyond F -Measures) Jaccard index is a set-based similarity measure. Given two sets, Jaccard index is defined as the ratio of intersection to union. Like F1-measure, it ranges from 0 to 1, where 0 indicates distinct sets and 1 indicates identical sets (?). It is used in cluster analysis and co-citation analysis to name a few. Some recent work ((??)) examined the use of Jaccard index as a performance measure in classification problems. The Jaccard index is a pseudo-linear performance function of per-class false negatives and false positives. We can define Jaccard indexes for binary, multiclass and multilabel problems in terms of the error profile entries,\n(binary) \u2200e \u2208 R4, Jac(e) = P1 \u2212 e1 P1 + e2\n(multilabel\u2013micro) \u2200e \u2208 R2L, mJac(e) = \u2211L\nk=1(Pk \u2212 e2k\u22121)\u2211L k=1 Pk + \u2211L k=1 e2k\n(multiclass\u2013micro) \u2200e \u2208 R2L, mcJac(e) = 1\u2212 P1 \u2212\n\u2211L k=2 e2k\u22121\n(1\u2212 P1) + e1\nAs we can infer from the above equations, these quantities are pseudo-linear and hence, we can use the methodology developed in Section 3.3.1,thresholding cost-sensitive scores, to find optimal Jaccard index classifier. Our analysis proves the remark of ? \u201cWe also see that algorithms maximizing the F-measure perform the best for Jaccard index\u201d."}, {"heading": "4. Relationship to Multi-Objective Optimization", "text": "Finding \u201cgood\u201d classifiers amounts to find good trade-offs between the different types of errors. In any case, it is a natural requirement that the chosen classifier has an error profile that is a minimal element of E (H) according to the partial order of Pareto dominance, which is denoted by and is defined as:\n\u2200e, e\u2032 \u2208 Rd , e e\u2032 \u21d4 \u2200k \u2208 {1, ..., d} , ek \u2264 e\u2032k .\nThe set of optimal solutions defines the Pareto front. error profile that is a minimal element of E (H) according to Pareto-dominance (where e e\u2032 iff ek \u2265 e\u2032k for all k). This set of optimal solutions defines the Pareto front.\nMulti-objective optimization defines methods for finding the Pareto front, or approximations of it (?), and one of the motivations is to find (approximately) optimal solutions of a vector function that is hard to optimize. The process is to generate candidate points in the Pareto front, and take the candidate with optimal value of the vector function. The advantage is generating candidate points is faster than the direct optimization of the vector function. In our case, goal is to find h \u2208 E (H) that achieves small values of \u3008a, e(h)\u3009 for a predefined cost vector a.\nThe reduction from pseudo-linear functions to solving a series of cost-sensitive classification problems exactly corresponds to this Pareto front method. In fact, a general way of finding Pareto-optimal solutions of a multi-objective problems is called the weighted-sum method (see e.g. ??). Applied to error profiles, the weighted-sum method would minimize positive weighted combinations of the elements of the error profiles, which corresponds to solving a cost-sensitive classification problem. In usual multi-objective optimization settings, such a Pareto set method is not useful for pseudo-linear aggregation functions, because most such functions are linear-fractional, and single-objective problems with a linear-fractional objective function can be rewritten in terms of a linear objective with linear constraints (see e.g. ?). In our context however, the linearization would not help because it would introduce constraints involving values of the error profiles, which are not linear in general. What we gain with the reduction to cost-sensitive classification (or, equivalently, with the weighted-sum method), is that efficient algorithms for cost-sensitive classification, which are known to work in practice and are asymptotically optimal, are already known. In addition, weighted-sum method require the users to know the relative preferences of the objectives in advance, which is not known in general. Hence the weight components are unbounded. Our reduction clearly defines a bound on the possible weights (a(t)).\nThe relationship between the reduction to cost-sensitive classification and the weightedsum method allows us to discuss pseudo-linear F-measures in terms of Pareto-optimal solutions. It is well-known that in general, not all Pareto-optimal solutions can be found by the weighted-sum method; in fact, only those that are on the boundary of the convex hull of the feasible set can be reached. In general however, many classification problems have Pareto-optimal solutions that do not lie on this boundary, especially if the input space is finite (as is the case on any finite dataset). Figure 3 gives the example of the Pareto front of a binary classification problem with 3 examples. The pareto front can be depicted on a 2D plane where the axis are false positives and false negatives; up to a change of basis, this Pareto front is the ROC curve (??) for the problem. In the figure, the blue points on the left plot correspond to Pareto-optimal classifiers (none of them can be improved both in terms of proportion of false positives and false negatives), while the red curve is the Pareto set of the convex hull of the error profiles of the 8 classifiers. Our result of reduction to cost-sensitive classification proves that only the classifiers whose error profile is both Pareto-optimal and on the boundary of the convex hull are candidates as optimal classifiers for any pseudo-linear aggregation function (here, the candidates are cA, cD, cF ), even though all classifiers are optimal for some trade-off rule. For instance, cB is the optimal classifier for the rule \u201d\u2018minimize the proportion of false negatives under the constraint that the proportion of false positives is smaller than 0.1\u201d\u2019."}, {"heading": "5. Experiments", "text": "This section illustrates of the accuracy of the algorithms suggested by our theoretical framework, using the F1-measure, in binary and multilabel classification. Our experimental results for binary and multilabel-macro F -measure (using binary relevance) shows that (i) choosing Optimal F Classifier by minimizing \u3008a, e\u3009 is same as choosing classifier with optimal F -measure a posteriori (ii) selecting a classifier by thresholding cost-sensitive scores is preferable to algorithms based on thresholding cost-insensitive classification scores: to maximize F -measure (iii) In case of multilabel-micro F -measure, Optimal F Classifier is the one with lowest \u3008a, e\u3009 value.\nWe compare thresholded cost-sensitive classification, as implemented by SVMs and logistic regression (LR), with asymmetric costs, to thresholded linear classifiers (SVMs and logistic regression, with a decision threshold set a posteriori by maximizing the F1-score on the validation set). Besides, the structured SVM approach to F1-measure maximization of ?, SVMperf, provides another baseline. For completeness, we also report results for nonthresholded cost-sensitive SVMs, non-thresholded cost-sensitive logistic regression, and for the thresholded versions of SVMperf.\nSince the practical cost-sensitive algorithms are based on convex surrogate loss optimization (?), the approximate cost approximation we presented in proposition 5 will not hold in general. We call the cost given in proposition 5 as actual cost and cost used in the practical surrogate loss based algorithm as surrogate cost. Since there is no one-to-one mapping between actual cost and surrogate cost, in practical implementations we have to iterate over the convex surrogate loss for each value of the actual cost.\nSVM and LR differ in the loss they optimize (weighted hinge loss for SVMs, weighted log-loss for LR), and even though both losses are calibrated in the cost-sensitive setting\n(that is, converging toward a Bayes-optimal classifier as the number of examples and the capacity of the class of function grow to infinity) (?), they behave differently on finite datasets or with restricted classes of functions. We may also note that asymptotically, the Bayes-classifier for a cost-sensitive binary classification problem is a classifier which thresholds the posterior probability of being class 1. Thus, all methods but SVMperf are asymptotically equivalent, and our goal here is to analyze their non-asymptotic behavior on a restricted class of functions.\nFor each experiment, the training set was split at random, keeping 1/3 for the validation set used to select all hyper-parameters, based on the maximization of the F1-measure on this set. For datasets that do not come with a separate test set, the data was first split to keep 1/4 for test. All results are averaged over five random splits i.e. hold-out validation with five random splits. The algorithms have from one to four hyper-parameters: (i) all algorithms are run with L2 regularization, with a regularization parameter C \u2208 {2\u22126, 2\u22125, ..., 26}; (ii) for the cost-sensitive algorithms, the cost for false negatives is chosen in {2\u2212tt , t \u2208 {0.1, 0.2, ..., 1.9}} of Proposition 4 1; (iii) for the thresholded algorithms, the threshold is chosen among all the scores of the validation examples; (iv) for kernel based SVM, we used radial basis function (RBF) kernel with \u03b3 (measure of influence of a single training example) value \u03b3 \u2208 {2\u22126, 2\u22125, ..., 26}.\nThe library LIBLINEAR (?) was used to implement non-kernel SVMs2 and logistic regression. LIBSVM (?) library was used for the kernel SVM. A constant feature with value 100 (to simulate an unregularized offset) was added to each dataset."}, {"heading": "5.1 Importance of Thresholding", "text": "Although our theoretical developments do not indicate any need to threshold the scores of classifiers, the practical benefits of a post-hoc adjustment of these scores can be important in terms of F1-measure maximization, as already noted in cost-sensitive learning scenarios (??). We study the importance thresholding clasification scores a posteriori using a didactic data called \u201cGalaxy\u201d. The data can be visualized as given in Figure 4. The data distribution consist in four clusters of 2D-examples, indexed by z \u2208 {1, 2, 3, 4}, with prior probability \u00b5(z = 1) = 0.01, \u00b5(z = 2) = 0.1, \u00b5(z = 3) = 0.001, and \u00b5(z = 4) = 0.889, with respective class prior probabilities \u00b5(y = 1|z = 1) = 0.9, \u00b5(y = 1|z = 2) = 0.09, \u00b5(y = 1|z = 3) = 0.9, and \u00b5(y = 1|z = 4) = 0. \u201cGalaxy\u201d is an example of highly imbalanced dataset.\nWe drew a very large sample (100,000 examples) from the distribution, whose optimal F1-measure is 67.5%. Without thresholding the scores of the classifiers, the best F1-measure among the classifiers is 58.0%, obtained by cost-sensitive SVM, whereas tuning thresholds enables to reach the optimal F1-measure for SVM\nperf and cost-sensitive SVM. On the other hand, LR is severely affected by the non-linearity of the level sets of the posterior probability distribution, and does not reach this limit (best F1-measure of 56.5%). Note also that, even with this very large sample size, the SVM and LR classifiers are very different. This result suggests that thresholding the classification scores a posteriori may improve the optimal F -scores, especially thresholding the cost-sensitive classifier scores.\n1. We take t greater than 1 in case the training asymmetry would be different from the true asymmetry (?). 2. The maximum number of iteration for SVMs was set to 50,000 instead of the default 1,000."}, {"heading": "5.2 Binary F\u03b2 and Multilabel MF\u03b2", "text": "The other datasets we use are Adult, RCV1, Scene, Siam and Yeast. In addition, we used a subsample from the Galaxy data to demonstrate the empirical validity of the algorithm. Adult, RCV1 and Yeast are obtained from the UCI repository3, and Scene and Siam from the Libsvm repository4. The attributes of the data used in our empirical study are given in Table 1.\nThe results for binary-F\u03b2 and multilabel-macro-F (MF\u03b2) are reported in Table 2 and 3 respectively. As it is evident from the experimental results, cost-sensitive learning and thresholded cost-sensitive learning give optimal results, whereas other methods performs suboptimally. But the difference between methods is less extreme than on the artificial Galaxy dataset. The Adult dataset is an example where all methods perform nearly iden-\n3. https://archive.ics.uci.edu/ml/datasets.html 4. http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html\ntical; the surrogate loss used in practice seems unimportant. On the other datasets, we observe that thresholding has relatively large impact, especially for SVMperf and costinsensitive classifiers. The unthresholded and cost-insensitive SVM and LR results are very poor compared to thresholded and cost-sensitive versions. The cost-sensitive classifiers (thresholded and unthresholded) outperforms all other methods, as suggested by the theory. Te cost-sensitive SVM is probably the method of choice to optimize binary-F\u03b2 or multilabelmacro-F(MF\u03b2) when predictive performance is a must. On these datasets, thresholded LR still performs reasonably well considering its relatively low computational cost. In general, on the computational cost front, LR converges faster than SVM or SVMperf.\nTable 4 presents the optimal MF\u03b2-measure with kernel SVM. We used Radial Basis Function (RBF) as the kernel function and trained RBF SVM without a bias term. Our experiments exemplify our theoretical findings in kernel settings. In case of Scene, thresholding the cost-sensitive scores marginally improves the MF1-score whereas in case of Yeast data, cost-sensitive kernel SVM outperforms other methods. In both cases, thresholding the cost-insensitive scores deteriorates the MF1-scores."}, {"heading": "5.3 Multilabel mF\u03b2", "text": "In case of multilabel-micro-F-measure, we compare our algorithm with a commonly used method to find best mF\u03b2-score suggested by ?. In the proposed method, one assumes that an optimal classifier for macro-F-measure is an optimal classifier for micro-F-measure.\nHence, the micro-F-score corresponds to optimal macro-F-score is deemed as the optimal micro-F-score. We compare our algorithm for micro-F-score against the micro-F-score corresponds to the optimal macro-F-score obtained by running binary relevance as explained in section 3.2.2.\nTable 5 contains the multilabel-micro-F (mcF\u03b2) results for the multilabel datasets. The results clearly demonstrates that selecting micro-F corresponds to maximal macro-F (correspond to Fmaxin table) always return suboptimal results. So in practice, algorithms based on per-label macro-F optimization should be avoided for micro-F optimization. In case of micro-F, effect due to thresholding is not very significant, except for RCV1 data. The unthresholded classifiers performs nearly as good as the thresholded versions. This is true for SVMperf also. As suggested by theory, cost-sensitive classification is the preferred method to optimize multilabel-micro-F. Here also, thresholded LR can be considered as an alternate option considering the computational cost.\nTable 6 presents the optimal mcF\u03b2-measure with RBF kernel SVM. Similar to the MF\u03b2 results, thresholding the cost-sensitive score gives better mFbeta results for kernel SVM."}, {"heading": "5.4 Cost Space Search Overhead", "text": "Since the actual cost associated misclassification differs from the cost associated with surrogate loss, it introduces an extra loop in our algorithm. Hence searching for optimal cost vector in the discretized cost interval might not be a good idea, especially when the value of \u03a6 is large. Here we do an empirical analysis of the functional dependencies between the actual cost and corresponding F -measure, and devise an improved version of the algorithms discussed in Section 3.4.\nFigure 5 contains the plot of micro-F -measure against false negative cost. From the plot, it is evident that micro-F -measure is a quasi-concave function of false negative cost. A function is quasi-concave, if every superlevel set of the function is convex (?). Formally, a function g : D \u2282 Rd \u2192 R, is quasi-concave if {x \u2208 D | g(x) \u2265 a} is convex. It can be verified from the plot that superlevel sets are convex. Bracketing methods (?) are extensively used to find global maxima of unimodal functions like quasi-concave function. We will not be able to use the exact bracketing algorithm to find the optimal cost, since it requires the\nknowledge of error profile associated with each value of F -measure). But we can use the idea of bracketing to limit the discretization interval.\nHere, we find three points (p, q, r), such that g(p) < g(q) > g(r), then instead of discretizing the whole interval, we can limit the discretization only to the sub-interval (p, r). We start with two intervals defined by the three points: start of the interval (0), median of the interval (1+\u03b2 2\n2 ) and the end of the interval (1 + \u03b2 2). Then we search for the\ntriplets (p, q, r) of given minimum sub-interval size inside the two intervals. In the simplest case, we find F -measure values corresponding to five points, two start points, midpoint (1+\u03b2 2\n2 ) and two midpoints of the intervals (0, 1+\u03b22 2 ) and ( 1+\u03b22 2 , 1 + \u03b2 2). Since the function is quasi-concave, the global maxima can be either on the mid point or on left or right of the mid point. Depending up on the F -measure values at the five points, we can limit the discretization only to one half. This way we can reduce the discretization space at least by half."}, {"heading": "6. Conclusion", "text": "We presented an analysis of F -measures, leveraging the property of pseudo-linearity of specific notions of F -measures to obtain a strong non-asymptotic reduction to cost-sensitive classification. The results hold on any dataset, for any class of function and on any data distribution assumptions (label dependent or label independent). We suggested algorithms for F -measure optimization based on minimizing the total misclassification cost of the costsensitive classification. We demonstrated experiments on linear classifiers, showing the theoretical interest of using cost-sensitive classification algorithms rather than probability thresholding. It is also shown that for F -measure maximization, thresholding even the cost-sensitive algorithms helps to achieve good performances.\nEmpirically and algorithmically, we only explored the simplest case of our result (F\u03b2measure in binary classification and macro-F\u03b2-measure and micro-F\u03b2-measure in multilabel classification), but much more remains to be done. Algorithms for the optimization of the non-pseudo-linear notions of F -measures like instance-wise-F\u03b2-measure in multilabel classification received interest recently as well (??), but are for now limited. We also believe that our result can lead to progresses towards optimizing the micro-F\u03b2 measure in multiclass classification."}, {"heading": "Acknowledgments", "text": "This work was carried out and funded in the framework of the Labex MS2T. It was supported by the Picardy Region and the French Government, through the program \u201cInvestments for the future\u201d managed by the National Agency for Research (Reference ANR-11IDEX-0004-02)"}, {"heading": "Appendix A. Proofs of Propositions and Corollaries", "text": "Proposition 2 A linear-fractional function F : D \u2286 Rd \u2192 R is the ratio of linear functions F (e) = \u03b10+\u3008\u03b3,e\u3009\u03b11+\u3008\u03b4,e\u3009 . A non-constant linear-fractional function is pseudo-linear on the open\nhalf-space D = { e \u2208 Rd|\u03b11 + \u3008\u03b4, e\u3009 > 0 } .\nProof A linear-fractional function F : e \u2208 Rd 7\u2192 \u03b10+\u3008\u03b3,e\u3009\u03b11+\u3008\u03b4,e\u3009 , \u03b11 + \u3008\u03b4, e\u3009 > 0 is pseudo-linear.\nF (e) \u2264 t\u21d4\u03b10 + \u3008\u03b3, e\u3009 \u2264 t(\u03b11 + \u3008\u03b4, e\u3009) \u21d2(\u03b10 \u2212 t\u03b11) + \u3008\u03b3 \u2212 t\u03b4, e\u3009 \u2264 0\nNow reversing the inequality, we obtain;\nF (e) \u2265 t\u21d4 (\u03b10 \u2212 t\u03b11) + \u3008\u03b3 \u2212 t\u03b4, e\u3009 \u2265 0\nAbove equations represent open hyperplanes.\n\u2207F (e) = (\u03b11 + \u3008\u03b4, e\u3009)\u03b3 \u2212 (\u03b10 + \u3008\u03b3, e\u3009)\u03b4 (\u03b11 + \u3008\u03b4, e\u3009)2 6= 0\nThe gradient term is constant if \u03b4 and \u03b3 are propotional and non-zero otherwise. The above conditions confirm the requirements for the pseudo-linearity given in Theorem 1 and hence the result.\nProposition 4 Let F ? = max e\u2208E(H) F (e), we have: e? \u2208 argmin e\u2208E(H)\n\u2329 a ( F ? ) , e \u232a \u21d4 F (e?) = F ? .\nProof Let e? \u2208 argmaxe\u2032\u2208E(H) F (e\u2032), and let a? = a(F (e?)) = a ( F ? ) . We first notice that pseudo-linearity implies that the set of e \u2208 D such that \u3008a?, e\u3009 = \u3008a?, e?\u3009 corresponds to the level set {e \u2208 D|F (e) = F (e?) = F ?}. Thus, we only need to show that e? is a minimizer of e\u2032 7\u2192 \u3008a?, e\u2032\u3009 in E (H). To see this, we notice that pseudo-linearity of F (see Theorem 1) implies\n\u2200e\u2032 \u2208 D, F (e?) \u2265 F ( e\u2032 ) \u21d2 \u3008a?, e?\u3009 \u2264 \u2329 a?, e\u2032 \u232a ,\nand since e? maximizes F in E (H), we get e? \u2208 argmine\u2032\u2208E(H) \u3008a?, e\u2032\u3009 .\nProposition 5 Let \u03b50 \u2265 0 and \u03b51 \u2265 0, and assume that there exists \u03a6 > 0 such that for all e, e\u2032 \u2208 E (H) satisfying F (e\u2032) > F (e), we have:\nF ( e\u2032 ) \u2212 F (e) \u2264 \u03a6 \u2329 a ( F (e\u2032) ) , e\u2212 e\u2032 \u232a . (1)\nThen, let us take e? \u2208 argmaxe\u2032\u2208E(H) F (e\u2032), and denote a? = a(F (e?)). Let furthermore a\u0302 \u2208 Rd+ and h \u2208 H satisfying the following conditions:\n(i) \u2016a\u0302\u2212 a?\u20162 \u2264 \u03b50 , (ii) \u3008a\u0302, e\u3009 \u2264 min e\u2032\u2208E(H)\n\u2329 a\u0302, e\u2032 \u232a + \u03b51 .\nWe have: \u2200e \u2208 E (H) , F (e) \u2265 F (e?)\u2212 \u03a6 \u00b7 (2\u03b50M + \u03b51) , where M = max e\u2032\u2208E(H)\n\u2225\u2225e\u2032\u2225\u2225 2 .\nProof Let e\u2032 \u2208 E (H), we can write \u3008a\u0302, e\u2032\u3009 = \u3008a?, e\u2032\u3009 + \u3008a\u0302\u2212 a?, e\u2032\u3009. Applying CauchySchwarz inequality and condition (i), we get\u2329\na\u0302, e\u2032 \u232a \u2264 \u2329 a?, e\u2032 \u232a + \u2016a\u0302\u2212 a?\u20162 \u2225\u2225e\u2032\u2225\u2225 2\n\u2264 \u2329 a?, e\u2032 \u232a + \u03b50M .\nIn particular, we have:\nmin e\u2032\u2208E(H)\n\u2329 a\u0302, e\u2032 \u232a \u2264 min\ne\u2032\u2208E(H)\n\u2329 a?, e\u2032 \u232a + \u03b50M\n\u2264 \u3008a?, e?\u3009+ \u03b50M , (2)\nsince e? \u2208 argmine\u2032\u2208E(H) \u3008a?, e\u2032\u3009 as shown in Proposition 4. Similarly, we have \u3008a?, e\u3009 = \u3008a\u0302, e\u3009+\u3008a? \u2212 a\u0302, e\u3009; applying Cauchy-Schwarz and conditions (i) and (ii), we have:\n\u2200e \u2208 E (H) , \u3008a?, e\u3009 \u2264 \u3008a\u0302, e\u3009+ \u2016a? \u2212 a\u0302\u20162 \u2016e\u20162 \u2264 \u3008a\u0302, e\u3009+ \u03b50M \u2264 min\ne\u2032\u2208E(H)\n\u2329 a\u0302, e\u2032 \u232a + \u03b51 + \u03b50M . (3)\nCombining Inequalities (2) and (3), we get\n\u2200e \u2208 E (H) , \u3008a?, e\u3009 \u2264 \u3008a?, e?\u3009+ \u03b51 + 2\u03b50M \u2200e \u2208 E (H) , \u3008a?, e\u2212 e?\u3009 \u2264 \u03b51 + 2\u03b50M ,\nand the final result follows from Assumption (1).\nProposition 6 F\u03b2-measures defined in Section 3.2.1 satisfy the conditions of Proposition 5 with:\n(binary) F\u03b2 : \u03a6 = 1\n\u03b22P1 and a : t \u2208 [0, 1] 7\u2192 (1+\u03b22\u2212 t, t, 0, 0) .\nProof Since F\u03b2 is linear-fractional as a function of the error profile, it is pseudo-linear on the open convex set {e \u2208 Rd|(1+\u03b22)P1\u2212e1 +e2 > 0} (i.e. when the denominator is strictly positive). Moreover, for every set of classifiers H, we have E (H) \u2286 D0 = [O,P1] \u00d7 [0, 1 \u2212 P1]\u00d7 [1\u2212 P1]\u00d7 [1, P1].\nNow, by the definition of F\u03b2, we have\n\u2200e \u2208 D0, F\u03b2(e) \u2264 t \u21d4 (1 + \u03b22 \u2212 t)e1 + te2 + (1 + \u03b22)P1(t\u2212 1) \u2265 0 ,\nand the equation still holds by reversing the inequalities. We thus have that a(t) = (1 + \u03b22 \u2212 t, t, 0, 0) satisfy the condition of Theorem 1 (with b(t) = (1 + \u03b22)P1(t\u2212 1)).\nWe now show that the condition of Equation 1 is satisfied for a(t) = (1 + \u03b22 \u2212 t, t, 0, 0) and all e, e\u2032 \u2208 D0 by taking \u03a6 = 1\u03b22P1 . To that end, let e and e \u2032 in E (H) and t and t\u2032 in\nR such that t\u2032 = F\u03b2(e\u2032) > F\u03b2(e) = t. Denote by \u03b5 the quantity \u3008a(t\u2032), e\u2212 e\u2032\u3009. Note that \u03b5 > 0 and that:\n0 = \u3008a(t), e\u3009 + b(t) = (1 + \u03b22 \u2212 t)e1 + te2 + (1 + \u03b22)P1(t\u2212 1) 0 = \u3008a(t\u2032), e\u2032\u3009 + b(t\u2032) = (1 + \u03b22 \u2212 t\u2032)e\u20321 + t\u2032e\u20322 + (1 + \u03b22)P1(t\u2032 \u2212 1) \u03b5 = \u3008a(t\u2032), e\u2212 e\u2032\u3009 = (1 + \u03b22 \u2212 t\u2032)e1 + t\u2032e2 + (1 + \u03b22)P1(t\u2032 \u2212 1)\nwhere the first two equalities are given by the definition of hyperplane corresponds to F\u03b2(e) = t and F\u03b2(e\n\u2032) = t\u2032, and the last one is obtained from the definition of \u3008a(t\u2032), e\u2212 e\u2032\u3009. Taking the difference of the third and first equality, we obtain:\n\u03b5 = (t\u2212 t\u2032)e1 + (t\u2032 \u2212 t)e2 + (1 + \u03b22)P1(t\u2032 \u2212 t)\nFrom which we get, since (1 + \u03b22)P1 \u2212 e1 + e2 > 0 for e \u2208 D0:\nF\u03b2(e \u2032)\u2212 F\u03b2(e) = t\u2032 \u2212 t = \u03b5 ( (1 + \u03b22)P1 \u2212 e1 + e2 )\u22121 \u2264 \u03b5 \u03b22P1 ,\nbecause \u03b22P1 the minimum of (1 + \u03b2 2)P1 \u2212 e1 + e2 on D0 (taking e1 = P1 and e2 = 0). We obtain the result since \u03b5 = \u3008a(t\u2032), e\u2212 e\u2032\u3009 by definition.\nCorollary 7 For the F1-measure, the optimal classifier is the solution to the cost-sensitive binary classifier with costs ( 1\u2212 F ?2 , F ? 2 ) Proof From Proposition 4, by putting \u03b2 = 1, we have\n(2\u2212 F ?)e1 + e2F ? + 2P1(F ? \u2212 1) \u2265 0\ndividing by 2, we get\n(1\u2212 F ?\n2 )e1 + e2\nF ?\n2 + P1(F\n? \u2212 1) \u2265 0\nCost vector, a(t), according to Theorem 1 is (1\u2212 F ?2 , F ? 2 ).\nProposition 8 multilabel micro-F (mF\u03b2) measures defined in Section 3.2.2 satisfy the conditions of Proposition 5 with:\n(multilabel\u2013micro) mF\u03b2 : \u03a6 = 1 \u03b22 \u2211L k=1 Pk and ai(t) = { 1 + \u03b22 \u2212 t if i is odd t if i is even .\nProof\nmF\u03b2(e) \u2264 t =\u21d2 (1 + \u03b22)\n\u2211L k=1(Pk \u2212 e2k\u22121)\n(1 + \u03b22) \u2211L k=1 Pk + \u2211L k=1(e2k \u2212 e2k\u22121) \u2264 t\n=\u21d2 (1 + \u03b22 \u2212 t) L\u2211 k=1 e2k\u22121 + t L\u2211 k=1 e2k + (1 + \u03b2 2)(t\u2212 1) L\u2211 k=1 Pk \u2265 0\nThus, we have that\nai(t) = { 1 + \u03b22 \u2212 t if i is odd t if i is even\nFollowing the same arguments as in Proposition: 4, we get\nmF\u03b2(e \u2032)\u2212mF\u03b2(e) = t\u2032 \u2212 t = \u03b5 [ (1 + \u03b22) L\u2211 k=1 Pk \u2212 L\u2211 k=1 e2k\u22121 + L\u2211 k=1 e2k ]\u22121 \u2264 \u03b5 \u03b22 \u2211L k=1 Pk ,\nbecause \u03b22 \u2211L k=1 Pk the minimum of (1 + \u03b2 2) \u2211L k=1 Pk \u2212 \u2211L k=1 e2k\u22121 + \u2211L\nk=1 e2k in the respective domain (taking e2k\u22121 = Pk and e2k = 0). We obtain the result since \u03b5 = \u3008a(t\u2032), e\u2212 e\u2032\u3009 by definition.\nProposition 9 multiclass micro-F (mcF\u03b2) defined in Section 3.2.3 satisfy the conditions of Proposition 5 with:\n(multiclass\u2013micro) mcF\u03b2 : \u03a6 = 1\n\u03b22(1\u2212 P1) and ai(t) =  1 + \u03b22 \u2212 t if i is odd and i 6= 1 t if i = 1\n0 otherwise\n.\nProof\n(multiclass\u2013micro) mcF\u03b2 : \u03a6 = 1\n\u03b22(1\u2212 P1) and ai(t) =  1 + \u03b22 \u2212 t if i is odd and i 6= 1 t if i = 1\n0 otherwise\n.\nmcF\u03b2(e) \u2264 t =\u21d2 (1 + \u03b22)(1\u2212 P1 \u2212\n\u2211L k=2 e2k\u22121)\n(1 + \u03b22)(1\u2212 P1)\u2212 \u2211L k=2 e2k\u22121 + e1 \u2264 t\n=\u21d2 (1 + \u03b22 \u2212 t) L\u2211 k=2 e2k\u22121 + te1 + (1 + \u03b2 2)(t\u2212 1)(1\u2212 P1) \u2265 0\nThus, we have that\nai(t) =  1 + \u03b22 \u2212 t if i is odd and i 6= 1 t if i = 1\n0 otherwise\nFollowing the same arguments as in Proposition:4, we get\nmcF\u03b2(e \u2032)\u2212mcF\u03b2(e) = t\u2032 \u2212 t = \u03b5 [ (1 + \u03b22)(1\u2212 P1)\u2212 L\u2211 k=2 e2k\u22121 + e1 ]\u22121 \u2264 \u03b5 \u03b22(1\u2212 P1) ,\nbecause \u03b22(1\u2212P1) the minimum of (1+\u03b22)(1\u2212P1)\u2212 \u2211L k=2 e2k\u22121+e1 in the respective domain\n(taking \u2211L\nk=2 e2k\u22121 = 1\u2212 P1 and e1 = 0). We obtain the result since \u03b5 = \u3008a(t\u2032), e\u2212 e\u2032\u3009 by definition."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "State of the art classification algorithms are designed to minimize the misclassification error of the system, which is a linear function of the per-class false negatives and false positives. Nonetheless non-linear performance measures are widely used for the evaluation of learning algorithms. For example, F -measure is a commonly used non-linear performance measure in classification problems. We study the theoretical properties of a subset of non-linear performance measures called pseudo-linear performance measures which includes F -measure, Jaccard index, among many others. We establish that many notions of F -measures and Jaccard index are pseudo-linear functions of the per-class false negatives and false positives for binary, multiclass and multilabel classification. Based on this observation, we present a general reduction of such performance measure optimization problem to cost-sensitive classification problem with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the F -measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on binary F -score, which are asymptotic in nature. Our analysis shows that thresholding cost-insensitive scores, a common technique employed to optimize F -measure, yields sub-optimal results. We also establish the multi-objective nature of the F -measure maximization problem by linking the algorithm with the weighted-sum approach used in multi-objective optimization. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various F -measure optimization tasks.", "creator": "LaTeX with hyperref package"}}}