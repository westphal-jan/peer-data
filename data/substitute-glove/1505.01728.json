{"id": "1505.01728", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2015", "title": "Integrating K-means with Quadratic Programming Feature Selection", "abstract": "Several data bumi possible particularly characterized already critical in below varying. One still the popular rather to reduce second tomography of been estimates whose to rehearse well selection, 're. files, board its subset a relevant and means - redundant displays. Recently, Quadratic Programming Feature Selection (QPFS) put same allow may implementer as feature fair difference fact part quadratic complete. It has been impression next analyst are means main commercial studio selection involves five a works of applications. Though, come compared many services complicated, the pulling time illustrates given QPFS is cubic having given numbers between example, which want those happy computationally expensive sometimes one lethargic sized datasets. In so paper anything propose full masterpiece required on different selection by integrating k - would schemas by QPFS. The basic model of our balanced ran matrix - give to bring while the number this video called need leave same legislature following to QPFS. We then enhanced both idea, wherein anyone continuously refine the feature space from brought very spiced clustering on every room - grained place, latter interleaving allow given QPFS with k - hence clustering. Every step of QPFS helps in identifying from proteins of irrelevant distinctive (other necessarily then more thrown letting ), whereas maybe progress a width - means resulted refines form cylindrical first actually potentially standards. We show they our iterative embellishment of foliage thought holders to dominate. We provide bounds saturday the other include meters analogous involved in brought th - well algorithm. Further, each QPFS third form now 2.8 in unlike another clusters, another can although longer smaller enough meaning previously more unique. Experiments take sixteen publicly well cross-referenced show though our steps allows of computational pushed (has 1999 both besides displays ), over system QPFS instead working one there state of only designer disc lists methods, but while commitment the overall accuracy.", "histories": [["v1", "Thu, 7 May 2015 14:45:11 GMT  (668kb)", "https://arxiv.org/abs/1505.01728v1", "17 pages, 11 figures"], ["v2", "Tue, 11 Aug 2015 18:06:36 GMT  (668kb)", "http://arxiv.org/abs/1505.01728v2", "17 pages, 11 figures"]], "COMMENTS": "17 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yamuna prasad", "k k biswas"], "accepted": false, "id": "1505.01728"}, "pdf": {"name": "1505.01728.pdf", "metadata": {"source": "CRF", "title": "Integrating K-means with Quadratic Programming Feature Selection", "authors": ["Yamuna Prasada", "K. K. Biswasa"], "emails": ["yprasad@cse.iitd.ac.in", "kkb@cse.iitd.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n01 72\n8v 2\n[ cs\n.C V\n] 1\n1 A\nug 2\n01 5\nSeveral data mining problems are characterized by data in high dimensions. One of the popular ways to reduce the dimensionality of the data is to perform feature selection, i.e, select a subset of relevant and non-redundant features. Recently, Quadratic Programming Feature Selection (QPFS) has been proposed which formulates the feature selection problem as a quadratic program. It has been shown to outperform many of the existing feature selection methods for a variety of applications. Though, better than many existing approaches, the running time complexity of QPFS is cubic in the number of features, which can be quite computationally expensive even for moderately sized datasets.\nIn this paper we propose a novel method for feature selection by integrating k-means clustering with QPFS. The basic variant of our approach runs k-means to bring down the number of features which need to be passed on to QPFS. We then enhance this idea, wherein we gradually refine the feature space from a very coarse clustering to a fine-grained one, by interleaving steps of QPFS with k-means clustering. Every step of QPFS helps in identifying the clusters of irrelevant features (which can then be thrown away), whereas every step of k-means further refines the clusters which are potentially relevant. We show that our iterative refinement of clusters is guaranteed to converge. We provide bounds on the number of distance computations involved in the k-means algorithm. Further, each QPFS run is now cubic in number of clusters, which can be much smaller than actual number of features. Experiments on eight publicly available datasets show that our approach gives significant computational gains (both in time and memory), over standard QPFS as well as other state of the art feature selection methods, even while improving the overall accuracy.\nKeywords: Feature Selection, Support Vector Machine (SVM), Quadratic Programming Feature Selection (QPFS)\n\u2217Corresponding author Email addresses: yprasad@cse.iitd.ac.in\n(Yamuna Prasad), kkb@cse.iitd.ac.in (K. K. Biswas) URL: http://www.cse.iitd.ernet.in/\u02dcyprasad (Yamuna Prasad), http://www.cse.iitd.ernet.in/\u02dckkb (K. K. Biswas)\n1Working as a research scholar in Department of CSE at Indian institute of Technology Delhi.Main areas of interests are machine learning, soft computing, optimization etc.\n2Working as Professor in Department of CSE at Indian institute of Technology Delhi.Main areas of interests are AI, machine learning, soft computing, vision etc."}, {"heading": "1. Introduction", "text": "Many data mining tasks are characterized by data in high dimensions. Directly dealing with such data leads to several problems including high computational costs and overfitting. Dimensionality reduction is used to deal with these problems by bringing down the data to a lower dimensional space. For many scientific applications, each of the dimensions (features) have an inherent meaning and one needs to keep the original features (or a representative subset) around to perform any meaningful analysis on the data [1]. Hence, some of the standard dimensionality reduction techniques such as PCA which transform the original feature space can not be di-\nPreprint submitted to Expert Systems with Applications August 12, 2015\nrectly applied. Dimensionality reduction in such scenarios reduces to the problem of feature selection. The goal is to select a subset of features which are relevant and non-redundant. Searching for such an optimal subset is computationally intractable (search space is exponential) [2, 3]. Amongst the current feature selection techniques, filter based methods are more popular because of the possibility of use with alternate classifiers and their reduced computational complexity (like Maximal relevance (MaxRel), Maximal Dependency (MaxDep), minimal-RedundancyMaximal-Relevance (mRMR) etc.) [4, 5]) [6].\nRecently, a new filter based quadratic programming feature selection (QPFS) method [6] has been proposed which has been shown to outperform many other existing feature selection methods. In this approach, a similarity matrix representing the redundancy among the features and a feature relevance vector are computed. These together are fed into a quadratic program to get a ranking on the features. The computation of the similarity matrix requires quadratic time and space in the number of features. Ranking requires cubic time in the number of features. This cubic time complexity can be prohibitively expensive for carrying out feature selection task in many datasets of practical interest. To deal with this problem, Lujan et al. [6] combine Nystro\u0308m sampling method, which reduces the space and time requirement at the cost of accuracy.\nIn this paper, we propose a feature selection approach by first clustering the set of features using two-level k-means clustering [7] and then applying QPFS over the cluster representatives (called Twolevel K-Means QPFS). The key intuition is to identify the redundant sets of features using k-means and use a single representative from each cluster for the ensuing QPFS run. This makes the feature selection task much more scalable since k-means has linear time complexity in the number of points to be clustered. The QPFS run is now cubic only in number of clusters, which typically is much smaller than actual number of features. Our approach is motivated by the work of Chitta and Murty [7], which proposes a two-level k-means algorithm for clustering the set of similar data points and uses it for improving classification accuracy in SVMs. Chitta and Murty [7] show that their approach yields linear time complex-\nity in contrast to standard cubic time complexity for SVM training.\nWe further enhance our feature selection approach by realizing that instead of simply doing one pass of k-means followed by QPFS, we can run them repeatedly to get better feature clusters. Specifically, we propose a novel method for feature selection by interleaving steps of QPFS with MacQueen\u2019s [8] k-means clustering (called Interleaved K-means QPFS). We gradually refine the feature space from a very coarse clustering to a fine-grained one. While every step of QPFS helps in identifying the clusters of irrelevant features (i.e. having 0 weights for the representative features), every step of k-means refines the potentially relevant clusters. Clusters of irrelevant features are thrown away after every QPFS step reducing the time requirements. This process is repeated recursively for a fixed number of levels or until each cluster has sufficiently small radius. Each QPFS run is now cubic in the number of clusters (which are much smaller than actual number of features and may be assumed to be constant). We show that our algorithm is guaranteed to converge. Further, we can bound the number of distance computations employed during the k-means algorithm.\nWe perform extensive evaluation of our proposed approach on eight publicly available benchmark datasets. We compare the performance with standard QPFS as well as other state of the art feature selection methods. Our experiments show that our approach gives significant computational gains (both in time and memory), even while improving the overall accuracy.\nIn addition to Chitta and Murty [7], there is other prior literature which uses clustering to reduce the dimensionality of the data for classification and related tasks. Examples include Clustering based SVM (CB-SVM) [9], clustering based trees for k-nearest neighbor classification [10] and use of PCA for efficient Gaussian kernel summation [11]. [12] presents a framework for categorizing existing feature selection algorithms and chosing the right algorithm for an application based on data characteristics. To the best of our knowledge, ours is the first work which integrates the use of clustering with existing feature selection methods to boost up their performance. Unlike most previous approaches, which use clustering as a one pass algorithm, our work interleaves steps\nof clustering with feature selection, thereby, reaping the advantage of clustering at various levels of granularity. The key contributions of our work can be summarized as follows:\n\u2022 A novel way to integrate the use of clustering (k-means) with existing feature selection methods (QPFS)\n\u2022 Bounds on the performance of the proposed algorithm\n\u2022 An extensive evaluation on eight different publicly available datasets\nThe rest of the paper is organized as follows: We describe the background for QPFS approach and the two level k-means algorithm in Section 2. Our proposed Two-level k-means QPFS and Interleaved KMeans QPFS approaches are presented in Sections 3 and 4, respectively. Experimental results are described in Section 5. We conclude our work in Section 6."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. QPFS [6]", "text": "Given a dataset withM features (fi, i = 1, ...,M) and N training instances (xi, i = 1, ..., N) with Class Y labels (yi, i = 1, ..., Y ) the standard QPFS formulation [6] is:\nf (\u03b1) = min \u03b1\n1 2 \u03b1TQ\u03b1\u2212 sT\u03b1\nSubject to \u03b1i > 0, i = 1, ...,M ; I T\u03b1 = 1.\n(1) where, \u03b1 is an M dimensional vector, I is the vector of all ones and Q is an M \u00d7 M symmetric positive semi-definite matrix, which represents the redundancy among the features; s is an M size vector representing relevance score of features with respective class labels. In this formulation, the quadratic term captures the dependence between each pair of features, and the linear term captures the relevance between each of the features and the class labels. The task of feature selection involves optimizing the twin goal of selecting features with high relevance and low redundancy. Considering the relative importance of non-redundancy amongst the features and\ntheir relevance,a scalar quantity \u03b8 \u2208 [0, 1] is introduced in the above formulation resulting in [6]:\nf (\u03b1) = min \u03b1\n1 2 (1\u2212 \u03b8)\u03b1TQ\u03b1 \u2212 \u03b8sT\u03b1\nSubject to \u03b1i > 0, i = 1, ...,M ; I T\u03b1 = 1.\n(2) In the above equation, \u03b8 = 1 corresponds to the formulation where only relevance is considered. In this case the QPFS formulation becomes equivalent to Maximum relevance criterion. When \u03b8 is set to zero, the formulation considers only non-redundancy among the features, that is, features with low redundancy with the rest of the features are likely to be selected. A reasonable value of \u03b8 can be computed using\n\u03b8 = q\u0304/(q\u0304 + m\u0304) (2a)\nwhere, q\u0304 is the mean value of the elements of matrix Q and m\u0304 is the mean value of the elements of vector s. As \u03b8 is a scalar, the similarity matrix Q and the feature relevance vector s in (2) can be scaled according to the value of \u03b8, resulting in the equivalent QPFS formulation of Equation (1). The QPFS can be solved by using any of the standard quadratic programming implementations but it raises space and computational time issues. Time complexity of QPFS approach is O(M3 +NM2) and space complexity is O(M2). To handle large scale data, Lujan et al. [6] proposes to combine QPFS with Nystro\u0308m method by working on subsamples of the data set for faster convergence. This often comes at the cost of trade-off with accuracy. The details are available in [6]."}, {"heading": "2.2. Similarity Measure", "text": "Various measures have been employed to represent similarities among features [13, 3, 14, 2]. Among these, correlation and mutual information (MI) based similarity measures are more popular. The classification accuracy can be improved with MI as it captures nonlinear dependencies between pair of variables unlike correlation coefficient which only measures linear relationship between a pair of variables [15, 6]. The mutual information between a pair of features fi and fj can be computed as follows:\nMI(fi, fj) = H(fi) +H(f2)\u2212H(fi, f2) (3)\nwhere H(fi) reperents entropy of feature vector fi and H(fi, fj) represents the joint entropy between feature vectors fi and fj [16]. Following variant of mutual information can be used as distance metric [16]:\nd(fi, fj) = 1\u2212 MI(fi, fj)\nmax(H(fi), H(fj)) (4)"}, {"heading": "2.3. MacQueen\u2019s K-Means Algorithm [8]", "text": "This is a k-means clustering algorithm which runs in two passes. In the first pass, it chooses first k samples as the initial k centers and assigns each of the remaining N \u2212 k samples to the cluster whose center is nearest and updates the centers. In the second pass, each of the N samples is assigned to the clusters whose center is closest and centers are updated. The number of distance computations in the first and second passes are k(N \u2212 k) and Nk respectively. Thus, the number of distance computations needed in MacQueen\u2019s k-means algorithm is 2Nk\u2212k2. This in effect means that the complexity is O(Nk) [7]."}, {"heading": "2.4. Two-level K-means[7] Algorithm", "text": "Recently, a two-level k-means algorithm has been developed using MacQueen\u2019s k-means algorithm [7]. This clustering algorithm ensures that radii of the clusters produced is less than a pre-defined threshold \u03c4 . The algorithm is outlined below:\nAlgorithm Two-level K-means(D, k, \u03c4 ) Input: Data Set D, Initial Number of Clusters k and Radius Threshold \u03c4 . Output: Set of clusters C (c1, c2, . . . , ci, . . .) (with radius ri 6 \u03c4 ) and the set of cluster centers \u00b5. 1. (level 1: ) Cluster the given set of data points\ninto an arbitarily chosen k\u2032 clusters using MacQueen\u2019s k-means algorithm.\n2. Calculate the radius ri of ith cluster using ri = maxxj\u2208ci d(xj , ci), where, d(., .) is the similarity metric. 3. (level 2: ) If the radius ri of the cluster ci is greater than the user defined threshold \u03c4 , split it using MacQueen\u2019s k-means with the number of clusters set to ( ri\n\u03c4 )M , where M is the dimension\nof the data. 4. return the set of clusters (C) and corresponding\ncenters (\u00b5) obtained after level 2.\n[7] shows that the above two-level k-means algorithm reduces the number of distance calculations as required by the MacQueen\u2019s k-means algorithm, while guaranteeing a bound on the clustering error (details below). The difference between the number of distance computations by MacQueen\u2019s k-means algorithm and the two-level k-means algorithm follows the inequality:\nU \u2212 N\u03b1MR\n\u03c4 6 ND1 \u2212ND2 6 U +\nk\u2032\u03b12MR\n2\u03c4 (5)\nwhere, ND1 and ND2 are the distance computations in MacQueen\u2019s k-means algorithm and two-level kmeans algorithm respectively,\u03b1 > 0 is some constant, R is the radius of the ball enclosing all the data points and U is (k \u2212 k\u2032)(2N \u2212 k \u2212 k\u2032). If k\u2032 \u226a k, then the expected number of distance computations in level 2 is upper bounded by N\u03b1MR/\u03c4 . The parameter \u03c4 obeys the following inequality:\nN\u03b1MR\nU 6 \u03c4 6 R\nAn appropriate choice of \u03c4 is obtained using the inequality\nmax ( R\n(k)1/M ,\nR\n(2N \u2212 k)1/M ) 6 \u03c4 6 R (6)\nThe clustering error in two-level k-means algorithm is upper bounded by twice the error of optimal clustering [7]. The time complexity of two-level k-means algorithm is O(Nk) and the space complexity isO(N+ k) [7]. The detailed analysis of these bounds can be found in [7]."}, {"heading": "3. Two-level K-means QPFS", "text": "Authors in [7] employ two-level k-means clustering for reducing the number of data points for classification using SVM. We use similar idea except that we cluster a set of features instead of the set of data points. We then apply QPFS on representative set of features. Thus, the problem is transformed into the feature space in contrast with their formultaion in the space of data points. Another key distinction is that we need to work with actual features unlike cluster means as in the case of [7]. This is because the\nmeans of feature clusters are abstract points and may not correspond to an actual features over which feature selection could be carried out. Towards this end, we develop two algorithms, the first one by modifying the MacQueen\u2019s k-means algorithm and the other one by modifying the two-level k-means algorithm [7] to return cluster representatives (features) in place of cluster means. Each feature is represented as an N-dimensional vector where N denotes the number of training instances (see Section 2.1). The kth component of this vector denotes the value of the feature in the kth data point. The distance metric between a pair features is defined using mutual information as in Equation (4).\nIn the following sections, M is cardinality of the (feature) space to be clustered. This takes the place of N which is the cardinality of (data) space in the case of Chitta and Murty [7]. Similarly, N denotes the dimensionality of the (feature) space to be clustered. This takes place of n which is the dimensionality of the (data) space in case of Chitta and Murty [7]."}, {"heading": "3.1. Variant MacQueen\u2019s K-means", "text": "We propose a variant of MacQueen\u2019s K-means algorithm for clustering the features to produce set of clusters with redundant features instead of clustering datapoints. In each iteration of the MacQueen\u2019s K-means algorithm, the nearest point from the updated mean is selected as the new center (called the cluster representative). Each iteration needs to compute distance from M \u2212 k features to k centers and distance from center to nearest feature in its cluster. Thus, each iteration needs k(M \u2212 k) + M distance computations. As MacQueen\u2019s k-means uses two iterations, the total number of distance compuations would be 2Mk \u2212 2k2 + 2M . The complexity is thus O(Mk)."}, {"heading": "3.2. Variant Two-level K-Means(TLKM)", "text": "We propose two-level k-Means algorithm (TLKM) by replacing MacQueen\u2019s k-means algorithm with its variant in the two-level k-Means algorithm as given in Section 2.4. It is important to note that we are clustering features rather than the data points. TLKM returns the feature clusters along with corresponding representatives. Following the arguments in [7], we\ncan derive the bounds on number of distance computations for our proposed TLKM algorithm in a similar manner. The only difference is that we have an additional 2M \u2212 k2 term as explained in Section 2.4 The bounds for difference in the number of distance computations between variant MacQueen\u2019s k-means and TLKM is\nU\u2212M( (\u03b1N + 1)R\n\u03c4 ) 6 ND1\u2212ND2 6 U+\nk\u2032\u03b12NR\n\u03c4 (7)\nHere, U is 2(k\u2212 k\u2032)(M \u2212 k\u2212 k\u2032) and other parameters have same definitions as in Equation (5) of Section 2.4. Further, if k\u2032 \u226a k, then the expected number of distance computations in the second level is upper bounded by M(2+ (\u03b1N +1)R/\u03c4) and parameter \u03c4 obeys the following inequality\nM ((\u03b1N + 1)R\nU\n)\n6 \u03c4 6 R\nFollowing [7], for reducing the number of computations in TLKM algoritm, it is necessary that\nmax ( R\n(k)1/N ,\nR\n(M \u2212 k)1/N ) 6 \u03c4 6 R (8)\nOR,\n\u03c4 6 min ( R\n(k)1/N ,\nR\n(M \u2212 k)1/N ) 6 R (9)\nFollowing the arguments in [7], it can be shown that the time and space complexities of the modified twolevel k-means for clustering features are O(Mk) and O(M + k), respectively."}, {"heading": "3.3. Two-level K-Means QPFS (TLKM-QPFS) Algorithm", "text": "We are now ready to present the QPFS based feature selection method using TLKM. We named this algorithm TLKM-QPFS, henceforth. We employ TLKM approach to cluster the features in a given dataset followed by a run of QPFS. Algorithm TLKM-QPFS illustrates our proposed Two-level k-means QPFS (TLKMQPFS) approach.\nAlgorithm TLKM-QPFS(FS, k, \u03c4 ) Input: Feature Set FS, Initial Number of Clusters k\nand Radius Threshold \u03c4 .\nOutput: Final representative feature set F (features) in order of their \u03b1 values. 1. Find the representatives F using TLKM algorithm as defined in Section 3.2 2. Apply QPFS on the cluster representatives F . 3. return Ranked F in the order of \u03b1\nTime and space complexities for TLKM approach in Step 1 are O(Mk) and O(M + k) respectively. In step 2 of Algorithm TLKM-QPFS, QPFS approach is used to rank the k cluster representatives (features) obtained in step 1. Time and space complexities for this step are O(k3 + Nk2) and O(k2), respectively. Therefore, the total time and space complexities of the algorithm TLKM-QPFS are O(Mk) + O(k3 + Nk2) \u223c O(M) and O(M + k) + O(k2) \u223c O(M), respectively. It is clear from this analysis that both the time and space complexities of this algorithm are O(M) as k \u226a M ."}, {"heading": "4. Interleaved K-Means QPFS (IKM-QPFS)", "text": "We now propose a new algorithm by combining the benefits of clustering approach with QPFS. In this proposed algorithm, we strive to refine relevant feature space from coarse to fine-grained clusters to improve accuracy while still preserving some of the computational gains obtained by TLKM-QPFS. Algorithm TLKM-QPFS uses k-means to identify cluster of features which are similar to each other (redundant). A representative is chosen for each of the clusters and then fed into QPFS. QPFS in turn returns a ranking on these cluster representatives. Many of the representatives are deemed irrelevant for classification (\u03b1 = 0). Amongst the sets of clusters whose representatives were deemed irrelevant, consider those with cluster radius r < \u03c4 . All the features in the these clusters can be considered irrelevant (since the cluster representative was irrelevant and cluster radius is sufficiently small) and can be thrown away. This also gives us an opportunity to further refine the larger clusters(r > \u03c4 ) potentially improving accuracy by identifying a larger subset of relevant features. This process of executing QPFS after initial run of k-means clustering can be repeated recursively. Each run of k-means further refines the relevant subclusters whereas each run of QPFS helps in identifying relevant set of features. This leads to the fol-\nlowing algorithm for feature selection which we have named Interleaved K-Means QPFS (IKM-QPFS)."}, {"heading": "4.1. Interleaved K-Means QPFS (IKM-QPFS) Algorithm", "text": "To start with, we first employ k-means to find the a set of cluster representatives. These cluster representatives are then fed into QPFS to get feature ranking on them. The cluster with sufficiently small radius (r < \u03c4 ) need not be refined further and can be directly use for final level of feature selection. Here, we throw away those representatives whose \u03b1 values are zero(irrelevant for classification). At the same time, clusters with radius greater than \u03c4 need to be refined further. This can be done recursively using above steps. In practice, we need to run the recursive splitting of clusters only upto a user defined level. In our approach, we split each cluster into a fixed number (k) of sub-clusters during k-means splitting. The proposed Interleaved K-Means-QPFS algorithm is presented in Algorithm IKM-QPFS.\nAlgorithm IKM-QPFS(FS, k, L, \u03c4 ) Input: Feature Set FS, Number of Sub-Clusters k\nthat each Cluster is split into, Radius Threshold \u03c4 and Number of Interleaved Levels L.\nOutput: Ordered set of relevant features F 1. Apply variant MacQueen\u2019s algorithm to features\nin FS; Obtain clusters C, cluster representatives f .\n2. Apply QPFS on the cluster representatives f and obtain \u03b1. 3. l \u2190 1 4. F \u2190 IRR(C, f, \u03b1, k, \u03c4, l, L) 5. Apply QPFS on F and rank F according to \u03b1. 6. return F\nThe sub procedure IRR(Identify Relevant and Refine) is illustrated in Algorithm IRR.\nAlgorithm IRR(C, f, \u03b1, k, \u03c4, l, L) Input: Cluster Set C, Cluster representatives f , \u03b1\nobtained by QPFS, Number of Clusters k, Radius Threshold \u03c4 , Number of level l, and Maximum Number of Levels L\nOutput: Final centers F (features) in order of their \u03b1 values. 1. for each cluster ci \u2208 C\n2. do 3. find the radius ri = maxfj\u2208ci d(fj, fi); d(., .) is the distance metric. 4. if (ri < \u03c4 or l = L) 5. then if (\u03b1i > 0) 6. then F = \u222a{fi} 7. else 8. Apply variant MacQueen\u2019s k-means\nalgorithm to features in cluster ci; Obtain clusters C \u2032, cluster representatives f \u2032\n9. Apply QPFS on the cluster centers C \u2032 and get \u03b1\u2032. 10. l \u2190 l + 1 11. F \u2032 \u2190 IRR(C \u2032, f \u2032, \u03b1\u2032, k, \u03c4, l) 12. F \u2190 F \u222a {F \u2032} 13. return F\nIn the above algorithm, if condition in step 4 checks if the boundary condition has been reached and no more splitting needs to be done (i.e. maximum number of levels L has been reached or ri < \u03c4 ). In which case, if the cluster is relevant (\u03b1i > 0), then corresponding features are added to the feature set to be returned (step 6). Else, they are discarded. Else condition in step 7 goes on to recursively refine the clusters when boundary condition is not yet reached.\nThe recursive approach for a sub-cluster at ith level can be visualized as follows. In Figure 1, sub-\nclusters 1 and k have radii greater than \u03c4 . They are split further independent of \u03b1 values. Their contribution to the final feature set is calculated by refining them recursively. Sub-clusters 2, 3 and k \u2212 1 have\nradii less than \u03c4 . They don\u2019t need to be split further. Amongst these, representatives for 2 and k \u2212 1 contribute to the final set of features. Sub-cluster 3 is discarded since \u03b13 = 0."}, {"heading": "4.2. Convergence", "text": "In every recursive call of Algorithm IRR, all the clusters whose radius is greater than \u03c4 are further split into k sub-clusters. Since every split is guaranteed to decrease the size of the original cluster, and we have a finite number of features, the algorithm is guaranteed to terminate and find clusters each of whose radius is less than \u03c4 , given sufficiently large L. Note that in the extreme case, a cluster will have only one point in it and hence, its radius will be zero. Now, let us try to analyze what happens in an average case i.e. when the sub-cluster split induced by the MacQueen\u2019s algorithm results in uniform-sized clusters. More formally, let ri denote the radius of the cluster i (at some level) which needs to be split further. Then, the volume enclosed by this cluster is C \u2217 riN . Here, N is the number of original data points (this is the space in which features are embedded). By the assumption of uniform size, this volume is divided equally amongst all the sub-clusters. Hence, the volume of each sub-cluster is going to be C \u2217 ri\nN/k. This volume corresponds to a sub-cluster of radius ri/k1/N . Hence, at every level, the cluster radius is reduced by a factor of k1/N . If the starting radius is R, then, after l levels the radius of a sub-cluster is given by R/kl/N . We would like this quantity to be less than equal to \u03c4 . This results in the following bound on l.\nR\nk l N\n\u2264 \u03c4 =\u21d2 k l N \u2265\nR \u03c4 =\u21d2 kl \u2265\n(\nR\n\u03c4\n)N\n=\u21d2 l \u2265 N \u2217 logk\n(\nR\n\u03c4\n)\n(taking log)\nHence, under the assumption of uniform splitting, continuation up to N \u2217 logk(R/\u03c4) levels will guarantee that each sub-cluster has radius \u2264 \u03c4 . If N \u2248 M , then features are very sparsely distributed in the data space, and above is a very loose bound. On the other hand if N \u226a M (as is the case with many Microarray datasets), then, above bound can be put to practical use.\nMacQueen\u2019s algorithm starts with the first set of k points as the cluster representatives, followed by another pass of assigning the points to each cluster and then recalculating the cluster representatives. In general, the assumption of uniform sub-cluster may only be an approximation to the actual clusters which are obtained, and hence, above bound will also be an approximation. A detailed analysis of whether one can bound this approximation is proposed to be carried out in future."}, {"heading": "4.3. Distance Computations", "text": "Distance computations done by interleaved steps of k-means in Algorithm 4 can be bounded as follows. In worst case, none of the clusters will be discarded and also, their radii will be greater than or equal to threshold (\u03c4 ) at each level. This will lead to recursive splitting of each cluster upto level L. Now, consider cluster cj at level i. The number of distance computations required by the MacQueen\u2019s algorithm to split this cluster further is given by 2|cj|k \u2212 2k2 + 2|cj| (see Section 3.1). Thus, total number of distance computations at level i is given as \u03a3j2|cj|k \u2212 2k2 + 2|cj| = 2Mk \u2212 k2 + 2M . The equality follows from the fact that total number of points in clusters at any level is \u03a3j |cj| = M (since each cluster is split upto the last level). Therefore, the number of distance computations in worst case is independent of the particular level. Hence, the total number of distance computations for Algorithm 4 can be bounded by L(2Mk \u2212 k2 + 2M)."}, {"heading": "4.4. Time Complexity Analysis", "text": "Time required in step 1 and step 2 of Algorithm IKM-QPFS is O(Mk) and O(k3) respectively. In step 4 of Algorithm IKM-QPFS, Algorithm IRR is called which is executed recursively. The time required for its execution can be computed as follows: If the maximum number of levels is L, number of cluster is k then it can be easily shown that the time complexity of Algorithm IRR is upper bounded by O(LMk + k3+L). The first term comes from the number distance computations in k-means, and the second term comes from the O(kL) calls to QPFS (ki\u22121 calls at level i), where each call takes O(k3) time. Thus, time required in step 4 of Algorithm IKM-QPFS\nis O(LMk+k3+L) and time required in step 5 of Algorithm IKM-QPFS is O(kL+3).\nAs L and k are very small constants, total time required by Algorithm IKM-QPFS is O(Mk)+O(k3)+ O(LMk+k3+L)+O(kL+3) which in effect is O(M)."}, {"heading": "4.5. Interleaved K-Means Aggressive QPFS (IKMAQPFS)", "text": "In this section, we present a variation on the IKMQPFS algorithm described above. The key idea is that after every step of QPFS run, we throw away all the clusters whose representatives are deemed irrelevant during a QPFS run (i.e., \u03b1 = 0), indepedent of the radii of the corresponding clusters. This is a deviation from the original proposed algorithm, wherein, we throw away a cluster only if the corresponding \u03b1 = 0 and the cluster radius r \u2264 \u03c4 . We call this variation Interleaved K-Means Aggressive QPFS (IKMA-QPFS) since it is aggressive about discarding the clusters whose representatives are deemed irrelevant. This potentially leads to even larger gain in terms of computational complexity since IKMAQPFS tries to identify the irrelvant feature clusters early enough in the process and throws them away. But since some of these clusters can be large in size (r \u2265 \u03c4 ), we might trade-off the additional computational gain by a loss in accuracy. But interestingly, in our analysis, we found that almost always this aggressive throwing away of clusters happened only towards the deeper levels of clustering(i.e., very few representatives were deemed irrelevant in the beginning levels of clustering), where the clusters were already sufficiently small. Hence, as we will see in our experiments, not only this variant performs better in terms of computational efficiency than IKM-QPFS, it even simplifies the feature selection problem, giving improved accuracy in some cases.\nFor IKMA-QPFS, the only change in the Algorithm IRR is before step 3 (i.e., right after the for loop starts), where we need to put another check if(\u03b1i = 0). If this condition is satisfied, we simply return out of the function. Rest of the algorithm remains the same. The convergence, the distance computations and the time complexity analyses presented above also remain the same as for IKM-QPFS. This is because all the analyses have been done in the worst\ncase when no clusters might be thrown away at intermediate levels."}, {"heading": "5. Experiments", "text": "We compare the performance of our proposed approaches TLKM-QPFS, IKM-QPFS and IKMA-QPFS with QPFS, FGM and GDM on eight publicly available benchmark datasets. We compare all methods for their time and memory requirements and also for their error rates at various numbers of top-k features selected. FGM and GDM methods works for binary classification datasets, therefore comparison with FGM and GDM is not carried out for SRBCT multi-class classification datasets. We observe an improved accuracy for FGM and GDM on normalized dataset in range [ -1, 1]. Therefore, we normalized all the datasets in range [ -1, 1].\nWe plot the accuracy graphs for varying (1 to 100) the number of top-k features selected for all the datasets except WDBC. For WDBC dataset, we have reported the results up-till 30 top features as this dataset has only 30 features. Next we describe the details of the datasets and our experimental methodology followed by our actual results."}, {"heading": "5.1. Datasets", "text": "For our experimental study, we have used eight publicly available benchmark datasets used by other researchers for feature selection. The description of these datasets is presented in Table 1. WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets."}, {"heading": "5.2. Methodology", "text": "WDBC and USPS datasets are divided into 60% and 40% sized splits for training and testing, respectively as in [18]. MNIST dataset is divided in 11,982 training and 1984 testing instances following [19]. The reported results are the average over 100 random splits of the data. The number of samples is very small (less than 100) in microarray datasets, so leaveone-out cross-validation is used for these datasets. We use mutual information as in [6] for redundancy and relevance measures in the experiments. The data\nis discretized using three segments and one standard deviation for computing mutual information as in [6]. For QPFS, the value of scale parameter (\u03b8) is computed using cross-validation from the set of \u03b8 values {0.0, 0.1, 0.3, 0.5, 0.7, 0.9}. The error rates obtained were very similar to the ones obtained using the scale parameter based on Equation (2a). For TLKM-QPFS, we used cross validation to determine the values of expected number of clusters k (to get \u03c4 ) and for IKM-QPFS and IKMA-QPFS, we used cross validation to determine the good values \u03c4 (threshold parameter) and k\u2032 (initial number of clusters). Threshold parameter \u03c4 is choosen from the set {0.70, . . .,0.99} with step size of 0.01. k is choosen from the set { 5,...,1000 } with step size of 5 and k\u2032 parameter in IKM-QPFS (as well in IKMA-QPFS) is choosen from the set [ 3, 150 ].\nAfter feature selection is done, linear SVM (L2regularized L2-loss support vector classification in primal) [20] is used to train a classifier using the optimal set of features output by QPFS, TLKM-QPFS, IKM-QPFS and IKMA-QPFS methods. FGM and GDM are embedded methods, so accuracy for both of these methods are obtained according to [18, 19]. The experiments were run on a Intel CoreTM i7 (3.10 GHz) machine with 8 GB RAM.\nWe have presented the variation in the error rates on varying the values of \u03c4 with a fixed value of initial number of clusters k\u2032=15 in figure 2 and on varying the values of initial number of clusters k\u2032 with a fixed value of \u03c4=0.8 in figure 3 for Colon dataset. On\nother datasets, it shows a similar trend."}, {"heading": "5.3. Results", "text": ""}, {"heading": "5.3.1. Time and Memory", "text": "Tables 2 and 3 show the time and memory requirements for feature selection done using each of the methods for all datasets respectively. On all the datasets, TLKM-QPFS,IKM-QPFS and IKMA-QPFS are orders of magnitude faster than QPFS. TLKMQPFS is three times faster than the GDM on RAC, MNIST and USPS datasets while three to ten times slower than the GDM on WDBC, Colon, Lymphoma and Leukemia datasets. Further, TLKM-QPFS is an order of magnitude faster than the FGM on MNIST\nand USPS datasets. IKM-QPFS and IKMA-QPFS are three to five times faster than FGM and GDM on RAC, MNIST and USPS datasets while two to five times slower on WDBC, Colon, Lymphoma and Leukemia datasets. The performance of TLKM-QPFS and IKM-QPFS are comparable while IKMA-QPFS is two to fifteen times faster than TLKM-QPFS and two to six times faster than the IKM-QPFS. This achievement of reduction in time of IKMA-QPFS is due to aggressive throwing of clusters when \u03b1 becomes zero.\nQPFS ran out of memory for RAC dataset in contrast to TLKM-QPFS and IKM-QPFS approaches. Therefore, we use QPFS with Nystro\u0308m method at Nystro\u0308m sampling rate \u03c1 =0.05 for RAC dataset. The results are appended with \u2217 for QPFS with Nystro\u0308m method in all the tables. For RAC dataset, TLKMQPFS and IKM-QPFS are more than two orders of magnitude faster than QPFS with Nystro\u0308m on RAC dataset.\nTLKM-QPFS, IKM-QPFS and IKMA-QPFS require more than an order of magnitude less memory compared to QPFS on all the datasets, except MNIST. On MNIST, they require about as much memory as QPFS. The memory required by FGM and GDM are marginally less than TLKM-QPFS, IKMQPFS and IKMA-QPFS. The memory required by TLKM-QPFS, IKM-QPFS and IKMA-QPFS are comparable on all datasets.\nThe results in tables 2 and 3, experimentally validates the theoretical complexities for time and memory."}, {"heading": "5.3.2. Accuracy", "text": "To compare the error rates across various methods, we varied the number of top features to be selected in the range from 1 to 100. For RAC dataset, we varied the number of top features at an interval of 5 in the range from 5 to 100. In tables 4 and 5, \u2212 corresponding to a method represents that the experiment was not done with that method. From table 4, it can be observed that our proposed IKMAQPFS and IKM-QPFS methods achieves lowest error rates for all datasets. In general, IKMA-QPFS and IKM-QPFS achieves lowest error rates earlier than the QPFS for WDBC, SRBCT, Lymphoma, Leukemia and USPS datasets and achieves lowest error rates\nearlier than FGM and GDM for all datasets except Colon dataset (tables 4 and 5). QPFS achieves lowest error rates earlier than FGM and GDM for WDBC, Colon, Lymphoma and Leukemia datasets. TLKMQPFS achieves lowest error rates earlier than the FGM for WDBC, Colon, Lymphoma and USPS datasets and also achieves lowest error rates earlier than the GDM for WDBC, Lymphoma and USPS datasets.\nFigure 4 plots the error rates for each datasets as the number of top selected features is varied from 1 to 100. The baseline here represents the accuracy obtained when all the features are used for classification and k-means-baseline represents the accuracy obtained when all the representative features (after two-level k-means) are used for classification. It is evident from figures 5.4-5.4 that error rates achieved by TLKM-QPFS, IKM-QPFS and IKMA-QPFS methods are improved over QPFS, FGM and GDM for each of the datasets. In all the datasets, TLKM-QPFS and IKM-QPFS achieve lower error rates with a less\nnumber of top selected features than QPFS. Usually, IKM-QPFS and IKMA-QPFS achieves lower error rates early than the TLKM-QPFS. In figure 4 and figure 5, plots of IKM-QPFS and IKMA-QPFS significantly overlaps.\nAs expected, the error rates come down as relevant features are added to the set. Once the relevant set has been added, any more additional (irrelevant) features lead to loss in accuracy. Tables 6 and 7 present the average test set error rates for each of the methods for top ranked k features (k being 10, 20, 30,50, 100), where top k features are chosen as output by the respective feature selection method. On all the datasets, IKMA-QPFS performs significantly better than QPFS, FGM and GDM at all the values of top k features selected. Further, error rates for TLKM-QPFS, IKM-QPFS and IKMA-QPFS are comparable on all datasets. This is particularly evident early on i.e. for a smaller number of top-k features. This points to the fact that IKM-QPFS and\nIKMA-QPFS are able to rank the relevant set of features right at the top.\nTLKM-QPFS performs better than QPFS in all the cases (dataset and number of top k feature combination), except on Colon data at k = 10 3. Among the two proposed approaches (TLKM-QPFS, IKMQPFS and IKMA-QPFS), both IKM-QPFS and IKMAQPFS are clear winner in terms of the accuracy."}, {"heading": "5.4. Summary", "text": "It is clearly evident from above results that our all the three proposed approaches for feature selection, TLKM-QPFS, IKM-QPFS and IKMA-QPFS, give significant gains in computational requirements (both time and memory), even while improving the overall accuracy in all cases when compared with\n3it performs marginally worse in couple of cases (k = 10, k = 30) for SRBCT\nQPFS and significantly low error rates when compared with FGM and GDM. Especially, our proposed approaches help reach the relevant set of features early on, which is a very important property of a good feature selection method. The computational requirements of TLKM-QPFS, IKM-QPFS and IKMA-QPFS are similar to each other. On the large microarray dataset our proposed approaches are faster than the FGM and GDM. As for performance, IKMA-QPFS is a clear winner among the three variants\nIn tables 6 and 7, TLKM, IKM and IKMA represent TLKM-QPFS, IKM-QPFS and IKMA-QPFS, respectively."}, {"heading": "6. Conclusion", "text": "In this paper, we proposed an approach for integrating k-means based clustering with Quadratic Programming Feature Selection (QPFS). The key idea involved using k-means to cluster together redundant\nsets of features. Only one representative from each cluster needed to be considered during the QPFS run for feature selection, reducing the complexity of QPFS from cubic in number of features to cubic in number of clusters (which is much smaller). We presented two variations of our approach. TLKM-QPFS used two level k-means to identify a set of representative features followed by a run of QPFS. In the more sophisticated variant, IKMA-QPFS, we interleaved the steps of k-means with QPFS, leading to a very fine grained selection of relevant features. Extensive evaluation on eight publicly available datasets showed the superior performance of our approach relative to existing state of the art feature selection methods.\nOne of the key directions for future work involves providing a generic framework for integrating a given clustering algorithm with a filter based feature selection method. Other direction includes extending our approach to sparse representations to deal with data in very high dimensions (millions of features, such as in vision). A third direction deals with coming up with a parallel formulation of our proposed approach."}, {"heading": "Acknowledgment", "text": "The authors would like to thank Dr. Parag Singla, Department of Computer Science & Engineering, In-\ndian Institute of Technology, Delhi, India for his valuable suggestions and support."}], "references": [{"title": "Feature selection in scientific applications", "author": ["E. Cant\u00fa-Paz", "S. Newsam", "C. Kamath"], "venue": "in: Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Stable feature selection via dense feature groups, in: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201908", "author": ["L. Yu", "C. Ding", "S. Loscalzo"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "An extensive empirical study of feature selection metrics for text classification", "author": ["G. Forman", "I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Two-level k-means clustering algorithm for k-\u03c4 relationship establishment and linear-time classification, Pattern Recogn", "author": ["R. Chitta", "M. Narasimha Murty"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Classifying large data sets using svms with hierarchical clusters", "author": ["H. Yu", "J. Yang", "J. Han"], "venue": "in: Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Fast k-nearest neighbor classification using cluster-based trees", "author": ["B. Zhang", "S. Srihari"], "venue": "IEEE Transactions on Pattern Analysis and Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Fast high-dimensional kernel summations using the monte carlo multipole method", "author": ["D. Lee", "A. Gray"], "venue": "in: Proceedings of Twenty Second Annual Conference on Neural Information Processing Systems (NIPS 08),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["H. Liu", "L. Yu"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Attribute clustering for grouping, selection, and classification of gene expression", "author": ["W.-H. Au", "K.C.C. Chan", "A.K.C. Wong", "Y. Wang"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Simultaneous gene clustering and subset selection for sample classification via mdl., Bioinformatics", "author": ["R. Jornsten", "B. Yu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Feature selection based on mutual information: criteria of max-dependency, maxrelevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Information-theoretic measures for knowledge 15  discovery and data mining, in: Karmeshu (Ed.), Entropy Measures, Maximum Entropy Principle and Emerging Applications", "author": ["Y.Y. Yao"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Design of fuzzy expert system for microarray data classification using a novel genetic swarm algorithm", "author": ["P. Ganesh Kumar", "T. Aruldoss Albert Victoire", "P. Renukadevi", "D. Devaraj"], "venue": "Expert Syst. Appl", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Learning sparse svm for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Discovering support and affiliated features from very high dimensions", "author": ["Y. Zhai", "M. Tan", "Y.S. Ong", "I.W. Tsang"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For many scientific applications, each of the dimensions (features) have an inherent meaning and one needs to keep the original features (or a representative subset) around to perform any meaningful analysis on the data [1].", "startOffset": 220, "endOffset": 223}, {"referenceID": 1, "context": "Searching for such an optimal subset is computationally intractable (search space is exponential) [2, 3].", "startOffset": 98, "endOffset": 104}, {"referenceID": 2, "context": ") [4, 5]) [6].", "startOffset": 2, "endOffset": 8}, {"referenceID": 3, "context": "In this paper, we propose a feature selection approach by first clustering the set of features using two-level k-means clustering [7] and then applying QPFS over the cluster representatives (called Twolevel K-Means QPFS).", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "Our approach is motivated by the work of Chitta and Murty [7], which proposes a two-level k-means algorithm for clustering the set of similar data points and uses it for improving classification accuracy in SVMs.", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "Chitta and Murty [7] show that their approach yields linear time complexity in contrast to standard cubic time complexity for SVM training.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "In addition to Chitta and Murty [7], there is other prior literature which uses clustering to reduce the dimensionality of the data for classification and related tasks.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Examples include Clustering based SVM (CB-SVM) [9], clustering based trees for k-nearest neighbor classification [10] and use of PCA for efficient Gaussian kernel summation [11].", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "Examples include Clustering based SVM (CB-SVM) [9], clustering based trees for k-nearest neighbor classification [10] and use of PCA for efficient Gaussian kernel summation [11].", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "Examples include Clustering based SVM (CB-SVM) [9], clustering based trees for k-nearest neighbor classification [10] and use of PCA for efficient Gaussian kernel summation [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 7, "context": "[12] presents a framework for categorizing existing feature selection algorithms and chosing the right algorithm for an application based on data characteristics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Considering the relative importance of non-redundancy amongst the features and their relevance,a scalar quantity \u03b8 \u2208 [0, 1] is introduced in the above formulation resulting in [6]:", "startOffset": 117, "endOffset": 123}, {"referenceID": 8, "context": "Various measures have been employed to represent similarities among features [13, 3, 14, 2].", "startOffset": 77, "endOffset": 91}, {"referenceID": 1, "context": "Various measures have been employed to represent similarities among features [13, 3, 14, 2].", "startOffset": 77, "endOffset": 91}, {"referenceID": 9, "context": "Various measures have been employed to represent similarities among features [13, 3, 14, 2].", "startOffset": 77, "endOffset": 91}, {"referenceID": 10, "context": "The classification accuracy can be improved with MI as it captures nonlinear dependencies between pair of variables unlike correlation coefficient which only measures linear relationship between a pair of variables [15, 6].", "startOffset": 215, "endOffset": 222}, {"referenceID": 11, "context": "where H(fi) reperents entropy of feature vector fi and H(fi, fj) represents the joint entropy between feature vectors fi and fj [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "Following variant of mutual information can be used as distance metric [16]:", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "This in effect means that the complexity is O(Nk) [7].", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "Two-level K-means[7] Algorithm", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "Recently, a two-level k-means algorithm has been developed using MacQueen\u2019s k-means algorithm [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "[7] shows that the above two-level k-means algorithm reduces the number of distance calculations as required by the MacQueen\u2019s k-means algorithm, while guaranteeing a bound on the clustering error (details below).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The clustering error in two-level k-means algorithm is upper bounded by twice the error of optimal clustering [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "The time complexity of two-level k-means algorithm is O(Nk) and the space complexity isO(N+ k) [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "The detailed analysis of these bounds can be found in [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Authors in [7] employ two-level k-means clustering for reducing the number of data points for classification using SVM.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "Another key distinction is that we need to work with actual features unlike cluster means as in the case of [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Towards this end, we develop two algorithms, the first one by modifying the MacQueen\u2019s k-means algorithm and the other one by modifying the two-level k-means algorithm [7] to return cluster representatives (features) in place of cluster means.", "startOffset": 168, "endOffset": 171}, {"referenceID": 3, "context": "This takes the place of N which is the cardinality of (data) space in the case of Chitta and Murty [7].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "This takes place of n which is the dimensionality of the (data) space in case of Chitta and Murty [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "Following the arguments in [7], we can derive the bounds on number of distance computations for our proposed TLKM algorithm in a similar manner.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "Following [7], for reducing the number of computations in TLKM algoritm, it is necessary that", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Following the arguments in [7], it can be shown that the time and space complexities of the modified twolevel k-means for clustering features are O(Mk) and O(M + k), respectively.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "We observe an improved accuracy for FGM and GDM on normalized dataset in range [ -1, 1].", "startOffset": 79, "endOffset": 87}, {"referenceID": 0, "context": "Therefore, we normalized all the datasets in range [ -1, 1].", "startOffset": 51, "endOffset": 59}, {"referenceID": 12, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 130, "endOffset": 144}, {"referenceID": 1, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 130, "endOffset": 144}, {"referenceID": 13, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 130, "endOffset": 144}, {"referenceID": 13, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 174, "endOffset": 182}, {"referenceID": 14, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 174, "endOffset": 182}, {"referenceID": 13, "context": "WDBC and USPS datasets are divided into 60% and 40% sized splits for training and testing, respectively as in [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "MNIST dataset is divided in 11,982 training and 1984 testing instances following [19].", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": ",1000 } with step size of 5 and k parameter in IKM-QPFS (as well in IKMA-QPFS) is choosen from the set [ 3, 150 ].", "startOffset": 103, "endOffset": 113}, {"referenceID": 13, "context": "FGM and GDM are embedded methods, so accuracy for both of these methods are obtained according to [18, 19].", "startOffset": 98, "endOffset": 106}, {"referenceID": 14, "context": "FGM and GDM are embedded methods, so accuracy for both of these methods are obtained according to [18, 19].", "startOffset": 98, "endOffset": 106}], "year": 2015, "abstractText": "Several data mining problems are characterized by data in high dimensions. One of the popular ways to reduce the dimensionality of the data is to perform feature selection, i.e, select a subset of relevant and non-redundant features. Recently, Quadratic Programming Feature Selection (QPFS) has been proposed which formulates the feature selection problem as a quadratic program. It has been shown to outperform many of the existing feature selection methods for a variety of applications. Though, better than many existing approaches, the running time complexity of QPFS is cubic in the number of features, which can be quite computationally expensive even for moderately sized datasets. In this paper we propose a novel method for feature selection by integrating k-means clustering with QPFS. The basic variant of our approach runs k-means to bring down the number of features which need to be passed on to QPFS. We then enhance this idea, wherein we gradually refine the feature space from a very coarse clustering to a fine-grained one, by interleaving steps of QPFS with k-means clustering. Every step of QPFS helps in identifying the clusters of irrelevant features (which can then be thrown away), whereas every step of k-means further refines the clusters which are potentially relevant. We show that our iterative refinement of clusters is guaranteed to converge. We provide bounds on the number of distance computations involved in the k-means algorithm. Further, each QPFS run is now cubic in number of clusters, which can be much smaller than actual number of features. Experiments on eight publicly available datasets show that our approach gives significant computational gains (both in time and memory), over standard QPFS as well as other state of the art feature selection methods, even while improving the overall accuracy.", "creator": "LaTeX with hyperref package"}}}