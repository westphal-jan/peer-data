{"id": "1505.01757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2015", "title": "Contextual Analysis for Middle Eastern Languages with Hidden Markov Models", "abstract": "Displaying made explaining in Middle Eastern languages requires contextual approaches due next use off-beat forms for normally narrative among second text. The why of called page will be formed were while joining of the interpret positional glyphs representing corresponding mudejar forms of while characters. A open mainly requires constitutes for joining it the prefixed. As there, important rules specific two context though derived and are case take philosophical by the functionality networking.", "histories": [["v1", "Thu, 7 May 2015 16:03:02 GMT  (209kb,D)", "http://arxiv.org/abs/1505.01757v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["kazem taghva"], "accepted": false, "id": "1505.01757"}, "pdf": {"name": "1505.01757.pdf", "metadata": {"source": "CRF", "title": "Contextual Analysis for Middle Eastern Languages with Hidden Markov Models", "authors": ["Kazem Taghva"], "emails": ["kazem.taghva@unlv.edu"], "sections": [{"heading": null, "text": "In this paper, we propose a machine learning approach for contextual analysis based on the first order Hidden Markov Model. We will design and build a model for the Farsi language to exhibit this technology. The Farsi model achieves 94 % accuracy with the training based on a short list of 89 Farsi vocabularies consisting of 2780 Farsi characters.\nThe experiment can be easily extended to many languages including Arabic, Urdu, and Sindhi. Furthermore, the advantage of this approach is that the same software can be used to perform contextual analysis without coding complex rules for each specific language. Of particular interest is that the languages with fewer speakers can have greater representation on the web, since they are typically ignored by software developers due to lack of financial incentives.\nKey Words: Unicode, Contextual Analysis, Hidden Markov Models, Big Data, Middle Eastern Languages, Farsi, Arabic"}, {"heading": "1 Introduction", "text": "One of the main objectives of the Unicode is to provide a setting that non-English documents can be easily created and displayed on modern electronic devices such as laptops and cellular phones. Consequently, this\nencoding has led to development of many software tools for text editing, font design, storage, and management of data in foreign languages. For commercial reasons, the languages with high speaking populations and large economies have enjoyed much more rapid advancement in Unicode based technologies. On the other hand, less spoken languages such as Pushtu is barely given attention. According to [11], approximately 40 to 60 million people speak Pushtu worldwide.\nMany Unicode based technologies are based on proprietary and patented methods and thus are not available to the general open source software developers\u2019 communities. For example, BIT [9] does not reveal its contextual analysis algorithm for Farsi[10]. Many software engineers need to redevelop new methods to implement tools to mimic these commercial technologies. The new contextual analysis for Farsi developed by Moshfeghi in Iran Telecommunication Research Center is an example of these kinds of efforts[10].\nThe Unicode also introduces a challenge for the internationalization of any software regardless of being commercial or open source. Tim Bray [2] writes:\n\u201c Whether you\u2019re doing business or academic research or public service, you have to deal with people, and these days, it\u2019s quite likely that some of the people you want to deal with come from somewhere else, and you\u2019ll sometimes want to deal with them in their own language. And if your software is unable to collect, store, and display a name, an address, or a part description in Chinese, Bengali, or Greek, there\u2019s a good chance that this could become very painful very quickly.\nThere are a few organizations that as a matter of principle operate in one language only (The US Department of Defense, the Acadmie franaise) but as a proportion of the\nar X\niv :1\n50 5.\n01 75\n7v 1\n[ cs\n.C L\n] 7\nM ay\nworld, they shrink every year.\u201d\nThis internationalization is a costly effort and subject to availability of resources. As mentioned above, languages with high speaking population such as Mandarin attract a lot of the efforts. The availability of data in Unicode represents an opportunity to employ machine learning techniques to advance software internationalization and foreign text manipulation. The language translation technologies heavily use Hidden Markov Models to improve translation accuracy [3] [1].\nIn this paper, we propose the use of HMM for contextual analysis. In particular, we design and build a generic HMM for Farsi that can be easily adapted to other Middle Eastern languages.\nIn section 2, we provide some background and related work on contextual analysis. Section 3 will provide a brief introduction to first order HMM. In section 4, we describe the design and implementation of our HMM for Farsi contextual analysis. The training and testing of HMM will be explained in section 5. Finally, section 6 describes our conclusion and proposes future work."}, {"heading": "2 Background", "text": "In 2002, the Center for Intelligent Information Retrieval at the University of Massachusetts, Amherst, held a workshop on Challenges in Information Retrieval and Language Model [7]. The premise of this workshop was to promote the use of the Language Model technology for various natural languages. The aim is to use the same software for indexing and retrieval regardless of the language. It was pointed out that, by using training materials such as document collections, we can automatically build retrieval engines for all languages. This report was one of the reasons that we decided to start a couple of projects on Farsi and Arabic [16] [19].\nConsequently, these projects led to developments of the two widely used Farsi and Arabic Stemmers [15][17]. One of the difficulties we had was the lack of technologies for input and display of Farsi and Arabic documents[19]. For example, we needed an input/display method that would allow us to enter Farsi query words in a Latin-based operating system without any special software or hardware. It was further necessary to have a standard character encoding for text representation and searching. At the time, we developed a system that provides the following capabilities:\n\u2022 a web-browser based keyboard applet for input\n\u2022 if the web-browser has the ability to process and display Unicode content, it will be used\n\u2022 if the browser cannot display Unicode content, an auxiliary process will be invoked to render the Unicode content into a portable bitmap image with associated HTML to display the image in the browser.\nAnother area of difficulty that we encountered is that the presence of white space used to separate words in the document is dependent on the display geometry of the glyphs. Since Farsi and Arabic are written using a cursive form, each character can have up to four different display glyphs. These glyphs represent the four different presentation forms:\nisolated: the standalone character initial: the character at the beginning of a word medial: the character in the middle of a word final: the character at the end of a word\nWe found that depending on the amount of trailing white space following a final form glyph, a space character may or may not be found in the text. This situation came to light when our subject matter experts were developing our test queries. We found that since the glyphs used to display the final form of characters had very little trailing space, they were manually adding space characters to improve the look of the displayed queries."}, {"heading": "2.1 Keyboard Applet", "text": "The keyboard applet was written in java script. The applet displays a Farsi keyboard image with the ability to enter characters from both the keyboard and mouse. The applet also handles character display conversion and joining of the input data.\nThe keyboard layout is based on the ISIRI 2901:1994 standard layout as documented in an email by Pournader [12]. Figure 1 shows the keyboard applet being used to define our test queries for search and retrieval.\nDisplay of the input data is normally performed by using the preloaded glyph images. However, if a character has not been preloaded, it can be generated on the fly. Most of the time, these generated characters are \u201ccompound\u201d characters. Farsi (and other Arabic script languages) may use \u201dcompound\u201d characters which are a combination of two or more separate characters. For\nexample, the rightmost character of A \u00d3Q k , the Farsi word for \u201cdate\u201d (that is, the fruit), is a combination of a kh with a damma.\nThe complications associated with our work on Farsi and Arabic convinced us that we need to develop generic machine learning tools if we want to develop display and search technologies for most of the Middle Eastern languages. In the next few sections, we will offer a solution to contextual analysis to display the correct presentational forms of characters."}, {"heading": "3 Hidden Markov Model", "text": "An HMM is a finite state automaton with probabilistic transitions and symbol emissions [13][14]. An HMM consists of:\n\u2022 A set of states S = {s1, \u00b7 \u00b7 \u00b7 sn}.\n\u2022 An emission vocabulary V = {w1 \u00b7 \u00b7 \u00b7wn}.\n\u2022 Probability distributions over emission symbols where the probability that a state s emits symbol w is given by P (w|s). This is denoted by matrix B.\n\u2022 Probability distributions over the set of possible outgoing transitions. The probability of moving from state si to sj is given by P (sj |si). This is denoted by matrix A.\n\u2022 A subset of the states that are considered start states, and to each of these is associated an \u201cinitial\u201d probability that the state will be a start state. This is denoted by \u03a0.\nAs an example, consider the widely used HMM [21] that decodes weather states based on a friend\u2019s activities. Assume there are only two states of weather: Sunny, Rainy. Also assume there are only three activities: Walking, Shopping, Cleaning.\nYou regularly call your friend who lives in another city to find out about his activity and the weather status. He may respond by saying \u201cI am cleaning and it is rainy\u201d, or \u201cI am shopping and it is sunny\u201d. If you collect a good number of these weather states and activities, you then can summarize your data as the HMM shown in Figure 3.\nThis HMM states that on rainy days, your friend walks 10% of the days while on sunny days, he walks 60%. The statistics associated with this HMM is obtained by simply counting the activities on rainy and sunny days.\nYou also notice arrows from states to states that keeps track of weather changes. For example, our HMM reflects the fact that on a rainy day, there is a 70% chance of rain next day while 30% chance of sunshine.\nIn addition, one can keep track of how many days in the data are sunny or rainy. This will be the initial probabilities. Formally these statistics are calculated by Maximum Likelihood Estimates (MLE). Formally, transition probabilities were estimated as:\nP (si, sj) = Number of transitions from si to sj Total number of transitions out of si (1)\nThe emission probabilities are estimated with Maximum Likelihood supplemented by smoothing. Smoothing is required because Maximum Likelihood estima-\ntion will sometimes assign a zero probability to unseen emission-state combinations.\nPrior to smoothing, emission probabilities are estimated by:\nP (w|s)ml = Number of times w is emitted at s\nTotal number of symbols emitted by s (2)\nThe most interesting part of an HMM is the decoding aspect. We may be told that our friend\u2019s activities for the last four days were cleaning, cleaning, shopping, cleaning and we want to know what the weather patterns were for those four days. This essentially translate to finding a sequence of four states s1s2s3s4 that maximizes p(s1s2s3s4| cleaning cleaning shopping cleaning). This amounts to choosing the highest probability among 16 choices for s1s2s3s4. This is computationally very expensive as the number of states and symbols increases. The solution is given by the Viterbi algorithm that finds an optimal path using dynamic programming [14]. The algorithm 1 is a modification of the pseudo code from [21].\nIn the next section we will describe the design and implementation of an HMM for Farsi contextual analysis."}, {"heading": "4 Farsi Hidden Markov Model", "text": "The Farsi HMM is very similar to the example of HMM described in the previous section. The HMM has a state for each presentation form of Farsi alphabet. Also the HMM has a vocabulary of size 32, one for each character in Farsi alphabet. A simple calculation reveals that the Farsi HMM should have 128 states and 32 vocabulary. The HMM has fewer than 128 states since some of the characters do not have 4 presentational form. For example, there are only two\nALGORITHM 1: Viterbi Algorithm\nData: Given K states and M vocabularies, and a sequence of vocabularies Y = w1w2 . . . wn\u22121wn Result: The most likely state sequence R = r1r2 \u00b7 \u00b7 \u00b7 rn\u22121rn that maximizes the above probability Function Viterbi(V, S, \u03a0, Y, A, B) : X for each state si do\nT1[i, 1] = \u03a0i \u2217Biw1 ; T2[i, 1] = 0;\nend for i = 2, 3, . . . , n do\nfor each state sj do do T1[j, i] = maxk(T1[k, i\u2212 1] \u2217Akj \u2217Bjwi); T2[j, i] = argmaxk(T1[k, i\u2212 1] \u2217Akj \u2217Bjwi); end\nend zn = argmaxk(T1[k, n]) rn = sZn for i = n, n\u2212 1, . . . , 2 do\nzi\u22121 = T2[zi, 1]; ri\u22121 = Szi\u22121;\nend Return R\nstates for the character @ , as there are no medial or initial form for this character.\nAs an example, suppose we want to type the word \u00c8A \u00aa , in English jackal. On the keyboard, we type\nfour isolated characters , \u00a8 , @ , and \u00c8 . The HMM should decode these four characters as initial, medial, final, and isolated, respectively. In other words, the sequence of the four isolated characters (or\nvocabulary in HMM terminology) should be decoded in the four states as shown in Figure 4.\nThe part of the HMM as displayed in Figure. 4 shows how Viterbi algorithm takes the path to decode the correct form of the characters by choosing the appropriate states. As we observe, there are four states for\nthe character \u00a8 representing the four shapes of this character. We also observe that there are only two\nstates for the character @ , as there are no medial or initial form for this character.\nA typical implementation of HMM adds states and vocabularies as being trained[20]. The training is done by providing pairs of the form ([w1w2 . . . wn\u22121wn], [s1s2 . . . sn\u22121sn]) similar to the [vocabularies, states] sequences as shown in Figure 4."}, {"heading": "5 Training and Testing of HMM", "text": "We trained the HMM with 89 words ( 2780 characters ) chosen from the list of the frequent words from Kayhan newspaper published in 2005 [4]. There are over 10,000 words in this collection. We limited the training to this short list to save time. The list of these words are shown in Figure 5.\nThe test data is a small number of words selected randomly from a small dictionary and shown in Figure 5. This list contains 32 words ( 350 characters ). The training file contains pairs of words separated by a vertical bar. The first word is the isolated form and the second word is the correct presentational form of the word. We read the file one line at a time and submit the two words for training as seen in the following Ruby code:\nf = File.open(\u201d./training-data\u201d) farsi.train([\u201d \u201d],[\u201d \u201d]) f.each do \u2014 line \u2014\nseq1,seq2 = line.chomp.split(/s*\u2014s*/) farsi.train(seq1.split(\u201d \u201d), seq2.split(\u201d \u201d))\nend\nAs it is seen, we have added a blank vocabulary and state to our HMM. The HMM adds vocabulary and states as a part of the training. The HMM has 32\nvocabulary and 74 states. It is anticipated that the HMM will have more states as the size of the training data increases.\nThe test correctly decoded 94% of all the characters. Most of the mistakes are due to the fact that the HMM has not seen enough combination examples of\ncharacters. For example, in the word @, the initial\nform of H was not decoded correctly. A closer examination of the training data reveals that there are no\noccurrence of in the set. Similarly, there are other\nerrors of this form such as the initial form of \u00e0 in the\nword \u00e9 K @ Q K. There are also a few errors attributed to\nthe double combination of the character \u00f8 as in \u00f8 C \u00a3. We believe most of these errors will be corrected with a larger training sets."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, We have presented a machine learning approach to the contextual analysis of script languages. It is shown that an ergodic HMM can be easily trained to automatically decode presentational forms of the script languages.\nAlthough the paper is developed based on Farsi, it can be easily extended to other middle eastern languages. Further training and research in this area can improve the character accuracy. A successful program for contextual analysis may have to include a list of exceptional words that do not fall into the normal combination of the characters. It is also important to notice that most of the Arabic and Farsi type setting technologies such as ArabTex [8] or FarsiTex [5] have problems with contextual analysis. This is mainly due to the fact that it is practically impossible to devise an algorithm that has 100% accuracy for tasks associated with natural languages.\nFinally, a higher order HMM may also improve the contextual analysis. For example, it is shown that the second order HMM improves the hand written character recognition [6]. It may also worth mentioning that the second order HMM does not improve error detection and correction for post processing of printed documents [18]."}], "references": [{"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Jan A. Botha", "Phil Blunsom"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Element sets: A minimal basis for an XML query engine", "author": ["Tim Bray"], "venue": "In QL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer"], "venue": "Comput. Linguist.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "Extended viterbi algorithm for second order hidden markov process", "author": ["Y.H.Y. He"], "venue": "In Proceedings 9th International Conference on Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "Challenges in information retrieval and language modeling: Report of a workshop held at the center for intelligent information retrieval, university of massachusetts amherst,  september", "author": ["Allan James"], "venue": "SIGIR Forum,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Design and implementation of a bilingual information entrance and edit environment. technical report, Iran", "author": ["Fallah Moshfeghi", "Kourosh Shadsari"], "venue": "Telecommunication Research Center,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "A new algorithm for contextual analysis of farsi characters and its implementation in java", "author": ["Kourosh Fallah Moshfeghi"], "venue": "In 17th International Unicode Conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "A Grammar of Pashto a Descriptive Study of the Dialect of Kandahar, Afghanistan", "author": ["Herbert Penzl", "Ismail Sloan"], "venue": "Ishi Press International,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "National iranian standard isiri 6219, information technology persian information interchange and display mechanism, using unicode", "author": ["Roozbeh Pournader"], "venue": "In Technical Report,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R. Rabiner"], "venue": "In PROCEEDINGS OF THE IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Readings in speech recognition. chapter A Tutorial on Hidden Markov Models and Selected Applications in Speech Recogni- 8  tion, pages 267\u2013296", "author": ["Lawrence R. Rabiner"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1990}, {"title": "A stemming algorithm for the farsi language", "author": ["Kazem Taghva", "Russell Beckley", "Mohammad Sadeh"], "venue": "In International Symposium on Information Technology: Coding and Computing (ITCC 2005),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Language model-based retieval for farsi documents", "author": ["Kazem Taghva", "Jeffrey S. Coombs", "Ray Pereda", "Thomas A. Nartker"], "venue": "In International Conference on Information Technology: Coding and Computing (ITCC\u201904),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Arabic stemming without A root dictionary", "author": ["Kazem Taghva", "Rania Elkhoury", "Jeffrey S. Coombs"], "venue": "In International Symposium on Information Technology: Coding and Computing (ITCC 2005),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Malreddy. Post processing with first- and secondorder hidden markov models", "author": ["Kazem Taghva", "Srijana Poudel", "Spandana"], "venue": "In DRR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Farsi searching and display technologies", "author": ["Kazem Taghva", "Ron Young", "Jeff Coombs", "Russell Beckley", "Mohammad Sadeh"], "venue": "In SDIUT,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Probability bracket notation: Markov state chain projector, hidden markov models and dynamic bayesian networks", "author": ["Xing M. Wang"], "venue": "CoRR, abs/1212.3817,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "According to [11], approximately 40 to 60 million people speak Pushtu worldwide.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "For example, BIT [9] does not reveal its contextual analysis algorithm for Farsi[10].", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "For example, BIT [9] does not reveal its contextual analysis algorithm for Farsi[10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 6, "context": "The new contextual analysis for Farsi developed by Moshfeghi in Iran Telecommunication Research Center is an example of these kinds of efforts[10].", "startOffset": 142, "endOffset": 146}, {"referenceID": 1, "context": "Tim Bray [2] writes:", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "The language translation technologies heavily use Hidden Markov Models to improve translation accuracy [3] [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 0, "context": "The language translation technologies heavily use Hidden Markov Models to improve translation accuracy [3] [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "In 2002, the Center for Intelligent Information Retrieval at the University of Massachusetts, Amherst, held a workshop on Challenges in Information Retrieval and Language Model [7].", "startOffset": 177, "endOffset": 180}, {"referenceID": 12, "context": "This report was one of the reasons that we decided to start a couple of projects on Farsi and Arabic [16] [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "This report was one of the reasons that we decided to start a couple of projects on Farsi and Arabic [16] [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "Consequently, these projects led to developments of the two widely used Farsi and Arabic Stemmers [15][17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "Consequently, these projects led to developments of the two widely used Farsi and Arabic Stemmers [15][17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "One of the difficulties we had was the lack of technologies for input and display of Farsi and Arabic documents[19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "The keyboard layout is based on the ISIRI 2901:1994 standard layout as documented in an email by Pournader [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 9, "context": "An HMM is a finite state automaton with probabilistic transitions and symbol emissions [13][14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "An HMM is a finite state automaton with probabilistic transitions and symbol emissions [13][14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "As an example, consider the widely used HMM [21] that decodes weather states based on a friend\u2019s activities.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "The solution is given by the Viterbi algorithm that finds an optimal path using dynamic programming [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "The algorithm 1 is a modification of the pseudo code from [21].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "For example, it is shown that the second order HMM improves the hand written character recognition [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 14, "context": "It may also worth mentioning that the second order HMM does not improve error detection and correction for post processing of printed documents [18].", "startOffset": 144, "endOffset": 148}], "year": 2015, "abstractText": "Displaying a document in Middle Eastern languages requires contextual analysis due to different presentational forms for each character of the alphabet. The words of the document will be formed by the joining of the correct positional glyphs representing corresponding presentational forms of the characters. A set of rules defines the joining of the glyphs. As usual, these rules vary from language to language and are subject to interpretation by the software developers. In this paper, we propose a machine learning approach for contextual analysis based on the first order Hidden Markov Model. We will design and build a model for the Farsi language to exhibit this technology. The Farsi model achieves 94 % accuracy with the training based on a short list of 89 Farsi vocabularies consisting of 2780 Farsi characters. The experiment can be easily extended to many languages including Arabic, Urdu, and Sindhi. Furthermore, the advantage of this approach is that the same software can be used to perform contextual analysis without coding complex rules for each specific language. Of particular interest is that the languages with fewer speakers can have greater representation on the web, since they are typically ignored by software developers due to lack of financial incentives.", "creator": "LaTeX with hyperref package"}}}