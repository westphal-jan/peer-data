{"id": "1505.03410", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2015", "title": "Mind the duality gap: safer rules for the Lasso", "abstract": "Screening rules permit to years removes irrelevant variables from from optimization as Lasso needs, type comes buybacks, them solvers faster. In all paper, we recommendations setting reworked and the so - appears $ \\ textit {simply apply} $ in same Lasso. Based early supersymmetric widening considerations, my new enforcing putting must test regions whose thicknesses drift directly exceeds, provided though out relies opened a persuasions tinkering. This poor ability testing out perhaps variables, years a wider range especially state-space inverse change. In some and faster harmonic, do never never still differ them making active sets (means) of the solutions during cyclic time. While our proposed capabilities can vulnerable others give fallacy, its musical both skepticism for but reinforce descent algorithm particularly compositions both machine learning such cases. Significant web time reductions are information while achieve, previous ready limits.", "histories": [["v1", "Wed, 13 May 2015 14:50:34 GMT  (263kb,D)", "http://arxiv.org/abs/1505.03410v1", "to appear in ICML 2015"], ["v2", "Tue, 25 Aug 2015 14:52:12 GMT  (263kb,D)", "http://arxiv.org/abs/1505.03410v2", "to appear in ICML 2015"], ["v3", "Thu, 3 Dec 2015 21:12:34 GMT  (263kb,D)", "http://arxiv.org/abs/1505.03410v3", "erratum to ICML 2015, \"The authors would like to thanks Jalal Fadili and Jingwei Liang for helping clarifying some misleading statements on the equicorrelation set\""]], "COMMENTS": "to appear in ICML 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC stat.CO", "authors": ["olivier fercoq", "alexandre gramfort", "joseph salmon"], "accepted": true, "id": "1505.03410"}, "pdf": {"name": "1505.03410.pdf", "metadata": {"source": "META", "title": "Mind the duality gap: safer rules for the Lasso", "authors": ["Olivier Fercoq", "Alexandre Gramfort", "Joseph Salmon"], "emails": ["OLIVIER.FERCOQ@TELECOM-PARISTECH.FR", "ALEXANDRE.GRAMFORT@TELECOM-PARISTECH.FR", "JOSEPH.SALMON@TELECOM-PARISTECH.FR"], "sections": [{"heading": "1. Introduction", "text": "Since the mid 1990\u2019s, high dimensional statistics has attracted considerable attention, especially in the context of linear regression with more explanatory variables than observations: the so-called p \u0105 n case. In such a context, the least squares with `1 regularization, referred to as the Lasso (Tibshirani, 1996) in statistics, or Basis Pursuit (Chen et al., 1998) in signal processing, has been one of the most popular tools. It enjoys theoretical guarantees (Bickel et al., 2009), as well as practical benefits: it provides sparse solutions and fast convex solvers are available. This has made the Lasso a popular method in modern data-science toolkits. Among successful fields where it has been applied,\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\none can mention dictionary learning (Mairal, 2010), biostatistics (Haury et al., 2012) and medical imaging (Lustig et al., 2007; Gramfort et al., 2012) to name a few.\nMany algorithms exist to approximate Lasso solutions, but it is still a burning issue to accelerate solvers in high dimensions. Indeed, although some other variable selection and prediction methods exist (Fan & Lv, 2008), the best performing methods usually rely on the Lasso. For stability selection methods (Meinshausen & Bu\u0308hlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved. For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Cande\u0300s et al., 2008).\nAmong possible algorithmic candidates for solving the Lasso, one can mention homotopy methods (Osborne et al., 2000), LARS (Efron et al., 2004), and approximate homotopy (Mairal & Yu, 2012), that provide solutions for the full Lasso path, i.e., for all possible choices of tuning parameter \u03bb. More recently, particularly for p \u0105 n, coordinate descent approaches (Friedman et al., 2007) have proved to be among the best methods to tackle large scale problems.\nFollowing the seminal work by El Ghaoui et al. (2012), screening techniques have emerged as a way to exploit the known sparsity of the solution by discarding features prior to starting a Lasso solver. Such techniques are coined safe rules when they screen out coefficients guaranteed to be zero in the targeted optimal solution. Zeroing those coefficients allows to focus more precisely on the non-zero ones (likely to represent signal) and helps reducing the computational burden. We refer to (Xiang et al., 2014) for a concise introduction on safe rules. Other alternatives have tried to screen the Lasso relaxing the \u201csafety\u201d. Potentially, some variables are wrongly disregarded and post-processing is needed to recover them. This is for instance the strategy adopted for the strong rules (Tibshirani et al., 2012).\nThe original basic safe rules operate as follows: one\nar X\niv :1\n50 5.\n03 41\n0v 1\n[ st\nat .M\nL ]\n1 3\nM ay\nchooses a fixed tuning parameter \u03bb, and before launching any solver, tests whether a coordinate can be zeroed or not (equivalently if the corresponding variable can be disregarded or not). We will refer to such safe rules as static safe rules. Note that the test is performed according to a safe region, i.e., a region containing a dual optimal solution of the Lasso problem. In the static case, the screening is performed only once, prior any optimization iteration. Two directions have emerged to improve on static strategies.\n\u2022 The first direction is oriented towards the resolution of the Lasso for a large number of tuning parameters. Indeed, practitioners commonly compute the Lasso over a grid of parameters and select the best one in a data-driven manner, e.g., by cross-validation. As two consecutive \u03bb1s in the grid lead to similar solutions, knowing the first solution may help improve screening for the second one. We call sequential safe rules such strategies, also referred to as recursive safe rules in (El Ghaoui et al., 2012). This road has been pursued in (Wang et al., 2013; Xu & Ramadge, 2013; Xiang et al., 2014), and can be thought of as a \u201cwarm start\u201d of the screening (in addition to the warm start of the solution itself). When performing sequential safe rules, one should keep in mind that generally, only an approximation of the previous dual solution is computed. Though, the safety of the rule is guaranteed only if one uses the exact solution. Neglecting this issue, leads to \u201cunsafe\u201d rules: relevant variables might be wrongly disregarded.\n\u2022 The second direction aims at improving the screening by interlacing it throughout the optimization algorithm itself: although screening might be useless at the beginning of the algorithm, it might become (more) efficient as the algorithm proceeds towards the optimal solution. We call these strategies dynamic safe rules following (Bonnefoy et al., 2014a;b).\nBased on convex optimization arguments, we leverage duality gap computations to propose a simple strategy unifying both sequential and dynamic safe rules. We coined GAP SAFE rules such safe rules.\nThe main contributions of this paper are 1) the introduction of new safe rules which demonstrate a clear practical improvement compared to prior strategies 2) the definition of a theoretical framework for comparing safe rules by looking at the convergence of their associated safe regions.\nIn Section 2, we present the framework and the basic concepts which guarantee the soundness of static and dynamic screening rules. Then, in Section 3, we introduce the new concept of converging safe rules. Such rules identify in\nfinite time the active variables of the optimal solution (or equivalently the inactive variables), and the tests become more and more precise as the optimization algorithm proceeds. We also show that our new GAP SAFE rules, built on dual gap computations, are converging safe rules since their associated safe regions have a diameter converging to zero. We also explain how our GAP SAFE tests are sequential by nature. Application of our GAP SAFE rules with a coordinate descent solver for the Lasso problem is proposed in Section 4. Using standard data-sets, we report the time improvement compared to prior safe rules."}, {"heading": "1.1. Model and notation", "text": "We denote by rds the set t1, . . . , du for any integer d P N. Our observation vector is y P Rn and the design matrix X \u201c rx1, \u00a8 \u00a8 \u00a8 , xps P Rn\u02c6p has p explanatory variables (or features) column-wise. We aim at approximating y as a linear combination of few variables xj\u2019s, hence expressing y as X\u03b2 where \u03b2 P Rp is a sparse vector. The standard Euclidean norm is written } \u00a8 }, the `1 norm } \u00a8 }1, the `8 norm } \u00a8 }8, and the matrix transposition of a matrix Q is denoted by QJ. We denote ptq` \u201c maxp0, tq.\nFor such a task, the Lasso is often considered (see Bu\u0308hlmann & van de Geer (2011) for an introduction). For a tuning parameter \u03bb \u0105 0, controlling the trade-off between data fidelity and sparsity of the solutions, a Lasso estimator \u03b2\u0302p\u03bbq is any solution of the primal optimization problem\n\u03b2\u0302p\u03bbq P arg min \u03b2PRp\n1 2 \u2016X\u03b2 \u00b4 y\u201622 ` \u03bb \u2016\u03b2\u20161 looooooooooooomooooooooooooon\n\u201cP\u03bbp\u03b2q\n. (1)\nDenoting \u2206X \u201c \u03b8 P Rn : \u2223\u2223xJj \u03b8\u2223\u2223 \u010f 1,@j P rps( the dual feasible set, a dual formulation of the Lasso reads (see for instance Kim et al. (2007) or Xiang et al. (2014)):\n\u03b8\u0302p\u03bbq \u201c arg max \u03b8P\u2206X\u0102Rn\n1 2 \u2016y\u201622 \u00b4\n\u03bb2\n2 \u2225\u2225\u2225\u03b8 \u00b4 y \u03bb \u2225\u2225\u22252 2\nlooooooooooooomooooooooooooon\n\u201cD\u03bbp\u03b8q\n. (2)\nWe can reinterpret Eq. (2) as \u03b8\u0302p\u03bbq \u201c \u03a0\u2206X py{\u03bbq, where \u03a0C refers to the projection onto a closed convex set C. In particular, this ensures that the dual solution \u03b8\u0302p\u03bbq is always unique, contrarily to the primal \u03b2\u0302p\u03bbq."}, {"heading": "1.2. A KKT detour", "text": "For the Lasso problem, a primal solution \u03b2\u0302p\u03bbq P Rp and the dual solution \u03b8\u0302p\u03bbq P Rn are linked through the relation:\ny \u201c X\u03b2\u0302p\u03bbq ` \u03bb\u03b8\u0302p\u03bbq . (3)\nThe Karush-Khun-Tucker (KKT) conditions state:\n@j P rps, xJj \u03b8\u0302p\u03bbq P # tsignp\u03b2\u0302p\u03bbqj qu if \u03b2\u0302 p\u03bbq j \u2030 0,\nr\u00b41, 1s if \u03b2\u0302p\u03bbqj \u201c 0. (4)\nSee for instance (Xiang et al., 2014) for more details. The KKT conditions lead to the fact that for \u03bb \u011b \u03bbmax \u201c }XJy}8, 0 P Rp is a primal solution. It can be considered as the mother of all safe screening rules. So from now on, we assume that \u03bb \u010f \u03bbmax for all the considered \u03bb\u2019s."}, {"heading": "2. Safe rules", "text": "Safe rules exploit the KKT condition (4). This equation implies that \u03b2\u0302p\u03bbqj \u201c 0 as soon as |xJj \u03b8\u0302p\u03bbq| \u0103 1. The main challenge is that the dual optimal solution is unknown. Hence, a safe rule aims at constructing a set C \u0102 Rn containing \u03b8\u0302p\u03bbq. We call such a set C a safe region. Safe regions are all the more helpful that for many j\u2019s, \u00b5Cpxjq :\u201c sup\u03b8PC |xJj \u03b8| \u0103 1, hence for many j\u2019s, \u03b2\u0302p\u03bbqj \u201c 0.\nPractical benefits are obtained if one can construct a region C for which it is easy to compute its support function, denoted by \u03c3C and defined for any x P Rn by:\n\u03c3Cpxq \u201c max \u03b8PC\nxJ\u03b8 . (5)\nCast differently, for any safe region C, any j P rps, and any primal optimal solution \u03b2\u0302p\u03bbq, the following holds true:"}, {"heading": "If \u00b5Cpxjq \u201c maxp\u03c3Cpxjq, \u03c3Cp\u00b4xjqq \u0103 1 then \u03b2\u0302p\u03bbqj \u201c 0.", "text": "(6) We call safe test or safe rule, a test associated to C and screening out explanatory variables thanks to Eq. (6). Remark 1. Reminding that the support function of a set is the same as the support function of its closed convex hull (Hiriart-Urruty & Lemare\u0301chal, 1993)[Proposition V.2.2.1], we restrict our search to closed convex safe regions.\nBased on a safe region C one can partition the explanatory variables into a safe active set A\u03bbpCq and a safe zero set Z\u03bbpCq where:\nAp\u03bbqpCq \u201c tj P rps : \u00b5Cpxjq \u011b 1u, (7) Zp\u03bbqpCq \u201c tj P rps : \u00b5Cpxjq \u0103 1u. (8)\nNote that for nested safe regions C1 \u0102 C2 then Ap\u03bbqpC1q \u0102 Ap\u03bbqpC2q. Consequently, a natural goal is to find safe regions as small as possible: narrowing safe regions can only increase the number of screened out variables.\nRemark 2. If C \u201c t\u03b8\u0302p\u03bbqu, the safe active set is the equicorrelation set Ap\u03bbqpCq \u201c E\u03bb :\u201c tj P rps : |xJj \u03b8\u0302p\u03bbq| \u201c 1u (in most cases (Tibshirani, 2013) it is exactly the active set of \u03b2\u0302p\u03bbq). If the Lasso has a unique solution, its support is exactly the equicorrelation set. If it is not unique, the equicorrelation set contains all the solutions\u2019 supports and there exists a Lasso solution whose support is exactly this set (Tibshirani, 2013)[Lemma 12]. The other extreme case is when C \u201c \u2206X , and Ap\u03bbqpCq \u201c rps. Here, no variable is screened out: Zp\u03bbqpCq \u201c H and the screening is useless.\nWe now consider common safe regions whose support functions are easy to obtain in closed form. For simplicity we focus only on balls and domes, though more complicated regions could be investigated (Xiang et al., 2014)."}, {"heading": "2.1. Sphere tests", "text": "Following previous work on safe rules, we call sphere tests, tests relying on balls as safe regions. For a sphere test, one chooses a ball containing \u03b8\u0302p\u03bbq with center c and radius r, i.e., C \u201c Bpc, rq. Due to their simplicity, safe spheres have been the most commonly investigated safe regions (see for instance Table 1 for a brief review). The corresponding test is defined as follows:\nIf \u00b5Bpc,rqpxjq \u201c |xJj c| ` r}xj} \u0103 1, then \u03b2\u0302 p\u03bbq j \u201c 0. (9)\nNote that for a fixed center, the smaller the radius, the better the safe screening strategy.\nExample 1. The first introduced sphere test (El Ghaoui et al., 2012) consists in using the center c \u201c y{\u03bb and radius r \u201c |1{\u03bb \u00b4 1{\u03bbmax|}y}. Given that \u03b8\u0302p\u03bbq \u201c \u03a0\u2206X py{\u03bbq, this is a safe region since y{\u03bbmax P \u2206X and }y{\u03bbmax \u00b4 \u03a0\u2206X py{\u03bbq} \u010f }y}|1{\u03bb \u00b4 1{\u03bbmax|. However, one can check that this static safe rule is useless as soon as\n\u03bb\n\u03bbmax \u010f min jPrps\n\u02dc\n1` |xJj y|{p}xj}}y}q 1` \u03bbmax{p}xj}}y}q\n\u00b8\n. (10)"}, {"heading": "2.2. Dome tests", "text": "Other popular safe regions are domes, the intersection between a ball and a half-space. This kind of safe region has\nbeen considered for instance in (El Ghaoui et al., 2012; Xiang & Ramadge, 2012; Xiang et al., 2014; Bonnefoy et al., 2014b). We denote Dpc, r, \u03b1, wq the dome with ball center c, ball radius r, oriented hyperplane with unit normal vector w and parameter \u03b1 such that c \u00b4 \u03b1rw is the projection of c on the hyperplane (see Figure 1 for an illustration in the interesting case \u03b1 \u0105 0). Remark 3. The dome is non-trivial whenever \u03b1 P r\u00b41, 1s. When \u03b1 \u201c 0, one gets simply a hemisphere.\nFor the dome test one needs to compute the support function for C \u201c Dpc, r, \u03b1, wq. Interestingly, as for balls, it can be obtained in a closed form. Due to its length though, the formula is deferred to the Appendix (see also (Xiang et al., 2014)[Lemma 3] for more details)."}, {"heading": "2.3. Dynamic safe rules", "text": "For approximating a solution \u03b2\u0302p\u03bbq of the Lasso primal problem P\u03bb, iterative algorithms are commonly used. We denote \u03b2k P Rp the current estimate after k iterations of any iterative algorithm (see Section 4 for a specific study on coordinate descent). Dynamic safe rules aim at discovering safe regions that become narrower as k increases. To do so, one first needs dual feasible points: \u03b8k P \u2206X . Following El Ghaoui et al. (2012) (see also (Bonnefoy et al., 2014a)), this can be achieved by a simple transformation of the current residuals \u03c1k \u201c y \u00b4X\u03b2k, defining \u03b8k as\n#\n\u03b8k\u201c\u03b1k\u03c1k, \u03b1k\u201cmin \u201d max \u00b4\nyJ\u03c1k \u03bb\u2016\u03c1k\u20162 , \u00b41\u2016XJ\u03c1k\u20168\n\u00af\n, 1\u2016XJ\u03c1k\u20168\n\u0131\n.\n(11) Such dual feasible \u03b8k is proportional to \u03c1k, and is the closest point (for the norm } \u00a8 }) to y{\u03bb in \u2206X with such a property, i.e., \u03b8k \u201c \u03a0\u2206XXSpanp\u03c1kqpy{\u03bbq. A reason for choosing this dual point is that the dual optimal solution \u03b8\u0302p\u03bbq is the projection of y{\u03bb on the dual feasible set \u2206X , and the optimal \u03b8\u0302p\u03bbq is proportional to y \u00b4X\u03b2\u0302p\u03bbq, cf. Equation (3). Remark 4. Note that if limk\u00d1`8 \u03b2k \u201c \u03b2\u0302p\u03bbq (convergence\nof the primal) then with the previous display and (3), we can show that limk\u00d1`8 \u03b8k \u201c \u03b8\u0302p\u03bbq. Moreover, the convergence of the primal is unaltered by safe rules: screening out unnecessary coefficients of \u03b2k, can only decrease the distance between \u03b2k and \u03b2\u0302p\u03bbq.\nExample 2. Note that any dual feasible point \u03b8 P \u2206X immediately provides a ball that contains \u03b8\u0302p\u03bbq since\u2225\u2225\u2225\u03b8\u0302p\u03bbq \u00b4 y\n\u03bb \u2225\u2225\u2225 \u201c min \u03b81P\u2206X \u2225\u2225\u2225\u03b81 \u00b4 y \u03bb \u2225\u2225\u2225 \u010f \u2225\u2225\u2225\u03b8 \u00b4 y \u03bb \u2225\u2225\u2225 :\u201c qR\u03bbp\u03b8q. (12)\nThe ball B ` y{\u03bb, qR\u03bbp\u03b8kq \u02d8\ncorresponds to the simplest safe region introduced in (Bonnefoy et al., 2014a;b) (cf. Figure 2 for more insights). When the algorithm proceeds, one expects that \u03b8k gets closer to \u03b8\u0302p\u03bbq, so }\u03b8k \u00b4 y{\u03bb} should get closer to }\u03b8\u0302p\u03bbq \u00b4 y{\u03bb}. Similarly to Example 1, this dynamic rule becomes useless once \u03bb is too small. More precisely, this occurs as soon as\n\u03bb\n\u03bbmax \u010f min jPrps\n\u02dc\n1` |xJj y|{p}xj}}y}q \u03bbmax}\u03b8\u0302p\u03bbq}{}y} ` \u03bbmax{p}xj}}y}q\n\u00b8\n.\n(13) Noticing that }\u03b8\u0302p\u03bbq} \u010f }y{\u03bb} (since \u03a0\u2206X is a contraction and 0 P \u2206X ) and proceeding as for (10), one can show that this dynamic safe rule is inefficient when:\n\u03bb\n\u03bbmax \u010f min jPrps\n\u02dc\n|xJj y| \u03bbmax\n\u00b8\n. (14)\nThis is a critical threshold, yet the screening might stop even at a larger \u03bb thanks to Eq. (13). In practice the bound in Eq. (13) cannot be evaluated a priori due to the term }\u03b8\u0302p\u03bbq}). Note also that the bound in Eq. (14) is close to the one in Eq. (10), explaining the similar behavior observed in our experiments (see Figure 3 for instance)."}, {"heading": "3. New contributions on safe rules", "text": ""}, {"heading": "3.1. Support discovery in finite time", "text": "Let us first introduce the notions of converging safe regions and converging safe tests.\nDefinition 1. Let pCkqkPN be a sequence of closed convex sets in Rn containing \u03b8\u0302p\u03bbq. It is a converging sequence of safe regions for the Lasso with parameter \u03bb if the diameters of the sets converge to zero. The associated safe screening rules are referred to as converging safe tests.\nNot only converging safe regions are crucial to speed up computation, but they are also helpful to reach exact active set identification in a finite number of steps. More precisely, we prove that one recovers the equicorrelation set of the Lasso (cf. Remark 2) in finite time with any converging strategy: after a finite number of steps, the equicorrelation set E\u03bb is exactly identified. Such a property is\nsometimes referred to as finite identification of the support (Liang et al., 2014). This is summarized in the following.\nTheorem 1. Let pCkqkPN be a sequence of converging safe regions. The estimated support provided by Ck, Ap\u03bbqpCkq \u201c tj P rps : max\u03b8PCk |\u03b8Jxj | \u011b 1u, satisfies limk\u00d18A\np\u03bbqpCkq \u201c E\u03bb, and there exists k0 P N such that @k \u011b k0 one gets Ap\u03bbqpCkq \u201c E\u03bb.\nProof. The main idea of the proof is to use that limk\u00d18 Ck \u201c t\u03b8\u0302p\u03bbqu, limk\u00d18 \u00b5Ckpxq \u201c \u00b5t\u03b8\u0302p\u03bbqupxq \u201c |xJ\u03b8\u0302p\u03bbq| and that the set Ap\u03bbqpCkq is discrete. Details are delayed to the Appendix.\nRemark 5. A more general result is proved for a specific algorithm (Forward-Backward) in Liang et al. (2014). Interestingly, our scheme is independent of the algorithm considered (e.g., Forward-Backward (Beck & Teboulle, 2009), Primal Dual (Chambolle & Pock, 2011), coordinatedescent (Tseng, 2001; Friedman et al., 2007)) and relies only on the convergence of a sequence of safe regions."}, {"heading": "3.2. GAP SAFE regions: leveraging the duality gap", "text": "In this section, we provide new dynamic safe rules built on converging safe regions.\nTheorem 2. Let us take any p\u03b2, \u03b8q P Rp \u02c6 \u2206X . Denote pR\u03bbp\u03b2q :\u201c 1\u03bb ` \u2016y\u20162\u00b4\u2016X\u03b2 \u00b4 y\u20162\u00b42\u03bb \u2016\u03b2\u20161 \u02d81{2 ` ,\nqR\u03bbp\u03b8q :\u201c \u2016\u03b8 \u00b4 y{\u03bb\u2016 , \u03b8\u0302p\u03bbq the dual optimal Lasso solution and r\u0303\u03bbp\u03b2, \u03b8q :\u201c b qR\u03bbp\u03b8q2 \u00b4 pR\u03bbp\u03b2q2, then\n\u03b8\u0302p\u03bbq P B \u00b4 \u03b8, r\u0303\u03bbp\u03b2, \u03b8q \u00af . (15)\nProof. The construction of the ball Bp\u03b8, r\u0303\u03bbp\u03b2, \u03b8qq is based on the weak duality theorem (cf. (Rockafellar & Wets,\n1998) for a reminder on weak and strong duality). Fix \u03b8 P \u2206X and \u03b2 P Rp, then it holds that\n1 2 \u2016y\u20162 \u00b4 \u03bb\n2\n2 \u2225\u2225\u2225\u03b8 \u00b4 y \u03bb \u2225\u2225\u22252 \u010f 1 2 \u2016X\u03b2 \u00b4 y\u20162 ` \u03bb \u2016\u03b2\u20161 .\nHence,\n\u2225\u2225\u2225\u03b8 \u00b4 y \u03bb \u2225\u2225\u2225 \u011b c \u00b4 \u2016y\u20162 \u00b4 \u2016X\u03b2 \u00b4 y\u20162 \u00b4 2\u03bb \u2016\u03b2\u20161 \u00af ` \u03bb . (16)\nIn particular, this provides }\u03b8\u0302p\u03bbq \u00b4 y{\u03bb} \u011b pR\u03bbp\u03b2q. Combining (12) and (16), asserts that \u03b8\u0302p\u03bbq belongs to the annulus Apy{\u03bb, qR\u03bbp\u03b8q, pR\u03bbp\u03b2qq :\u201c tz P Rn : pR\u03bbp\u03b2q \u010f }z \u00b4 y{\u03bb} \u010f qR\u03bbp\u03b8qu (the light blue zone in Figure 2).\nRemind that the dual feasible set \u2206X is convex, hence \u2206X X Bpy{\u03bb, qR\u03bbp\u03b8qq is also convex. Thanks to (16), \u2206XXBpy{\u03bb, qR\u03bbp\u03b8qq \u201c \u2206XXApy{\u03bb, qR\u03bbp\u03b8q, pR\u03bbp\u03b2qq, and then \u2206X X Apy{\u03bb, qR\u03bbp\u03b8q, pR\u03bbp\u03b2qq is convex too. Hence, \u03b8\u0302p\u03bbq is inside the annulus Apy{\u03bb, qR\u03bbp\u03b8q, pR\u03bbp\u03b2qq and so is r\u03b8, \u03b8\u0302p\u03bbqs \u010e Apy{\u03bb, qR\u03bbp\u03b8q, pR\u03bbp\u03b2qq by convexity (see Figure 2,(a) and Figure 2,(b)). Moreover, \u03b8\u0302p\u03bbq is the point of r\u03b8, \u03b8\u0302p\u03bbqs which is closest to y{\u03bb. The farthest where \u03b8\u0302p\u03bbq can be according to this information would be if r\u03b8, \u03b8\u0302p\u03bbqs were tangent to the inner ball Bpy{\u03bb, pR\u03bbp\u03b2qq and }\u03b8\u0302p\u03bbq \u00b4 y{\u03bb} \u201c pR\u03bbp\u03b2q. Let us denote \u03b8int such a point. The tangency property reads \u2016\u03b8int \u00b4 y{\u03bb\u2016 \u201c pR\u03bbp\u03b2q and p\u03b8 \u00b4 \u03b8intqJpy{\u03bb \u00b4 \u03b8intq \u201c 0. Hence, with the later and the definition of qR\u03bbp\u03b8q, \u2016\u03b8 \u00b4 y{\u03bb\u20162 \u201c \u2016\u03b8 \u00b4 \u03b8int\u20162 ` \u2016\u03b8int \u00b4 y{\u03bb\u20162and \u2016\u03b8 \u00b4 \u03b8int\u20162 \u201c qR\u03bbp\u03b8q2 \u00b4 pR\u03bbp\u03b2q2.\nSince by construction \u03b8\u0302p\u03bbq cannot be further away from \u03b8 than \u03b8int (again, insights can be gleaned from Figure 2), we conclude that \u03b8\u0302p\u03bbq P B ` \u03b8, p qR\u03bbp\u03b8q2 \u00b4 pR\u03bbp\u03b2q2q1{2 \u02d8 .\nRemark 6. Choosing \u03b2 \u201c 0 and \u03b8 \u201c y{\u03bbmax, then one recovers the static safe rule given in Example 1.\nWith the definition of the primal (resp. dual) objective for P\u03bbp\u03b2q, (resp. D\u03bbp\u03b8q), the duality gap reads as G\u03bbp\u03b2, \u03b8q \u201c P\u03bbp\u03b2q\u00b4D\u03bb p\u03b8q. Remind that ifG\u03bbp\u03b2, \u03b8q \u010f , then one has P\u03bbp\u03b2q \u00b4 P\u03bbp\u03b2\u0302p\u03bbqq \u010f , which is a standard stopping criterion for Lasso solvers. The next proposition establishes a connection between the radius r\u03bbp\u03b2, \u03b8q and the duality gap G\u03bbp\u03b2, \u03b8q. Proposition 1. For any p\u03b2, \u03b8q P Rp \u02c6 \u2206X , the following holds\nr\u0303\u03bbp\u03b2, \u03b8q2 \u010f r\u03bbp\u03b2, \u03b8q2 :\u201c 2\n\u03bb2 G\u03bbp\u03b2, \u03b8q. (17)\nProof. Use the fact that qR\u03bbp\u03b8q2 \u201c \u2016\u03b8 \u00b4 y{\u03bb\u20162 and pR\u03bbp\u03b2q2 \u011b p\u2016y\u20162 \u00b4 \u2016X\u03b2 \u00b4 y\u20162 \u00b4 2\u03bb \u2016\u03b2\u20161q{\u03bb2.\nIf we could choose the \u201coracle\u201d \u03b8 \u201c \u03b8\u0302p\u03bbq and \u03b2 \u201c \u03b2\u0302p\u03bbq in (15) then we would obtain a zero radius. Since those quantities are unknown, we rather pick dynamically the current available estimates given by an optimization algorithm: \u03b2 \u201c \u03b2k and \u03b8 \u201c \u03b8k as in Eq. (11). Introducing GAP SAFE spheres and domes as below, Proposition 2 ensures that they are converging safe regions.\nGAP SAFE sphere:\nCk \u201c B p\u03b8k, r\u03bbp\u03b2, \u03b8qq . (18)\nGAP SAFE dome:\nCk \u201c D\n\u00a8\n\u02dd\ny \u03bb ` \u03b8k 2 , qR\u03bbp\u03b8kq 2 , 2\n\u02dc\npR\u03bbp\u03b2kq qR\u03bbp\u03b8kq\n\u00b82\n\u00b4 1, \u03b8k \u00b4 y\u03bb }\u03b8k \u00b4 y\u03bb}\n\u02db\n\u201a.\n(19)\nProposition 2. For any converging primal sequence p\u03b2kqkPN, and dual sequence p\u03b8kqkPN defined as in Eq. (11), then the GAP SAFE sphere and the GAP SAFE dome are converging safe regions.\nProof. For the GAP SAFE sphere the result follows from strong duality, Remark 4 and Proposition 1 yield limk\u00d18 r\u03bbp\u03b2k, \u03b8kq \u201c 0, since limk\u00d18 \u03b8k \u201c \u03b8\u0302p\u03bbq and limk\u00d18 \u03b2k \u201c \u03b2\u0302p\u03bbq. For the GAP SAFE dome, one can check that it is included in the GAP SAFE sphere, therefore inherits the convergence (see also Figure 2,(c) and (d)).\nRemark 7. The radius r\u03bbp\u03b2k, \u03b8kq can be compared with the radius considered for the Dynamic Safe rule and Dynamic ST3 (Bonnefoy et al., 2014a) respectively: qR\u03bbp\u03b8kq \u201c }\u03b8k \u00b4 y{\u03bb}2 and p qR\u03bbp\u03b8kq2 \u00b4 \u03b42q1{2, where \u03b4 \u201c p\u03bbmax{\u03bb \u00b4 1q{ \u2016xj\u2039\u2016. We have proved that limk\u00d18 r\u03bbp\u03b2k, \u03b8kq \u201c 0, but a weaker property is satisfied by the two other radius: limk\u00d18 qR\u03bbp\u03b8kq \u201c qR\u03bbp\u03b8\u0302p\u03bbqq \u201c } qR\u03bbp\u03b8\u0302p\u03bbqq \u00b4 y{\u03bb}2 and limk\u00d18p qR\u03bbp\u03b8kq2 \u00b4 \u03b42q1{2 \u201c p qR\u03bbp\u03b8\u0302p\u03bbqq2 \u00b4 \u03b42q1{2 \u0105 0."}, {"heading": "3.3. GAP SAFE rules : sequential for free", "text": "As a byproduct, our dynamic screening tests provide a warm start strategy for the safe regions, making our GAP SAFE rules inherently sequential. The next proposition shows their efficiency when attacking a new tuning parameter, after having solved the Lasso for a previous \u03bb, even only approximately. Handling approximate solutions is a critical issue to produce safe sequential strategies: without taking into account the approximation error, the screening might disregard relevant variables, especially the one near the safe regions boundaries. Except for \u03bbmax, it is unrealistic to assume that one can dispose of exact solutions.\nConsider \u03bb0 \u201c \u03bbmax and a non-increasing sequence of T \u00b4 1 tuning parameters p\u03bbtqtPrT\u00b41s in p0, \u03bbmaxq. In practice, we choose the common grid (Bu\u0308hlmann & van de Geer, 2011)[2.12.1]): \u03bbt \u201c \u03bb010\u00b4\u03b4t{pT\u00b41q (for instance in Figure 3, we considered \u03b4 \u201c 3). The next result controls how the duality gap, or equivalently, the diameter of our GAP SAFE regions, evolves from \u03bbt\u00b41 to \u03bbt.\nProposition 3. Suppose that t \u011b 1 and p\u03b2, \u03b8q P Rp\u02c6\u2206X . Reminding r2\u03bbtp\u03b2, \u03b8q \u201c 2G\u03bbtp\u03b2, \u03b8q{\u03bb 2 t , the following holds\nr2\u03bbtp\u03b2, \u03b8q \u201c \u02c6 \u03bbt\u00b41 \u03bbt \u02d9 r2\u03bbt\u00b41p\u03b2, \u03b8q (20)\n` p1\u00b4 \u03bbt \u03bbt\u00b41 q \u2225\u2225\u2225\u2225X\u03b2 \u00b4 y\u03bbt \u2225\u2225\u2225\u22252 \u00b4 p\u03bbt\u00b41\u03bbt \u00b4 1q \u2016\u03b8\u20162 . Proof. Details are given in the Appendix.\nThis proposition motivates to screen sequentially as follows: having p\u03b2, \u03b8q P Rp\u02c6\u2206X such that G\u03bbt\u00b41p\u03b2, \u03b8q \u010f , then, we can screen using the GAP SAFE sphere with center \u03b8 and radius r\u03bbp\u03b2, \u03b8q. The adaptation to the GAP SAFE dome is straightforward and consists in replacing \u03b8k, \u03b2k, \u03bb by \u03b8, \u03b2, \u03bbt in the GAP SAFE dome definition.\nRemark 8. The basic sphere test of (Wang et al., 2013) requires the exact dual solution \u03b8 \u201c \u03b8\u0302p\u03bbt\u00b41q for center, and has radius |1{\u03bbt\u00b41{\u03bbt\u00b41| \u2016y\u2016, which is strictly larger than ours. Indeed, if one has access to dual and primal optimal solutions at \u03bbt\u00b41, i.e., p\u03b8, \u03b2q \u201c p\u03b8\u0302p\u03bbt\u00b41q, \u03b2\u0302p\u03bbt\u00b41qq, then r2\u03bbt\u00b41p\u03b2, \u03b8q \u201c 0, \u03b8 \u201c py \u00b4X\u03b2q{\u03bbt\u00b41 and\nr2\u03bbtp\u03b2, \u03b8q \u201c \u02c6 \u03bb2t\u00b41 \u03bb2t p1\u00b4 \u03bbt \u03bbt\u00b41 q \u00b4 p\u03bbt\u00b41 \u03bbt \u00b4 1q \u02d9 \u2016\u03b8\u20162 ,\n\u010f \u02c6 1 \u03bbt \u00b4 1 \u03bbt\u00b41\n\u02d92\n\u2016y\u20162 ,\nsince }\u03b8} \u010f }y}{\u03bbt\u00b41 for \u03b8 \u201c \u03b8\u0302p\u03bbt\u00b41q.\nNote that contrarily to former sequential rules (Wang et al., 2013), our introduced GAP SAFE rules still work when one has only access to approximations of \u03b8\u0302p\u03bbt\u00b41q."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Coordinate Descent", "text": "Screening procedures can be used with any optimization algorithm. We chose coordinate descent because it is well suited for machine learning tasks, especially with sparse and/or unstructured design matrix X . Coordinate descent requires to extract efficiently columns of X which is typically not easy in signal processing applications where X is commonly an implicit operator (e.g. Fourier or wavelets).\nAlgorithm 1 Coordinate descent with GAP SAFE rules input X, y, ,K, f, p\u03bbtqtPrT\u00b41s\nInitialization: \u03bb0 \u201c \u03bbmax \u03b2\u03bb0 \u201c 0 for t P rT \u00b4 1s do \u03b2 \u00d0 \u03b2\u03bbt\u00b41 (previous -solution) for k P rKs do\nif k mod f \u201c 1 then Compute \u03b8 and C thanks to (11) and (18) or (19) Get A\u03bbtpCq \u201c tj P rps : \u00b5Cpxjq \u011b 1u as in (7) if G\u03bbtp\u03b2, \u03b8q \u010f then \u03b2\u03bbt \u00d0 \u03b2 break for j P A\u03bbtpCq do \u03b2j \u00d0 ST `\n\u03bbt \u2016xj\u20162\n, \u03b2j \u00b4 xJj pX\u03b2\u00b4yq\n\u2016xj\u20162 \u02d8\n# STpu, xq \u201c signpxq p|x| \u00b4 uq` (softthreshold)\noutput p\u03b2\u03bbtqtPrT\u00b41s\nWe implemented the screening rules of Table 1 based on the coordinate descent in Scikit-learn (Pedregosa et al., 2011). This code is written in Python and Cython to generate low level C code, offering high performance. A low level language is necessary for this algorithm to scale. Two implementations were written to work efficiently with both dense data stored as Fortran ordered arrays and sparse data stored in the compressed sparse column (CSC) format. Our pseudo-code is presented in Algorithm 1. In practice, we perform the dynamic screening tests every f \u201c 10 passes through the entire (active) variables. Iterations are stopped when the duality gap is smaller than the target accuracy.\nThe naive computation of \u03b8k in (11) involves the computation of \u2225\u2225XJ\u03c1k\u2225\u22258 (\u03c1k being the current residual), which costs Opnpq operations. This can be avoided as one knows when using a safe rule that the index achieving the maximum for this norm is in A\u03bbtpCq. Indeed, by construction arg maxjPA\u03bbt pCq |xJj \u03b8k| \u201c arg maxjPrps |xJj \u03b8k| \u201c arg maxjPrps |xJj \u03c1k|. In practice the evaluation of the dual gap is therefore not a Opnpq but Opnqq where q is the size of A\u03bbtpCq. In other words, using screening also speeds up\nthe evaluation of the stopping criterion.\nWe did not compare our method against the strong rules of Tibshirani et al. (2012) because they are not safe and therefore need complex post-processing with parameters to tune. Also we did not compare against the sequential rule of Wang et al. (2013) (e.g., EDDP) because it requires the exact dual optimal solution of the previous Lasso problem, which is not available in practice and can prevent the solver from actually converging: this is a phenomenon we always observed on our experiments."}, {"heading": "4.2. Number of screened variables", "text": "Figure 3 presents the proportion of variables screened by several safe rules on the standard Leukemia dataset. The screening proportion is presented as a function of the number of iterations K. As the SAFE screening rule of El Ghaoui et al. (2012) is sequential but not dynamic, for a given \u03bb the proportion of screened variables does not depend on K. The rules of Bonnefoy et al. (2014a) are more efficient on this dataset but they do not benefit much from the dynamic framework. Our proposed GAP SAFE tests screen much more variables, especially when the tuning parameter \u03bb gets small, which is particularly relevant in practice. Moreover, even for very small \u03bb\u2019s (notice the logarithmic scale) where no variable is screened at the beginning of the optimization procedure, the GAP SAFE rules manage to screen more variables, especially when K increases. Finally, the figure demonstrates that the GAP SAFE dome test only brings marginal improvement over the sphere."}, {"heading": "4.3. Gains in the computation of Lasso paths", "text": "The main interest of variable screening is to reduce computation costs. Indeed, the time to compute the screening itself should not be larger than the gains it provides. Hence, we compared the time needed to compute Lasso paths to prescribed accuracy for different safe rules. Figures 4, 5 and 6 illustrate results on three datasets. Figure 4 presents results on the dense, small scale, Leukemia dataset. Figure 5 presents results on a medium scale sparse dataset obtained with bag of words features extracted from the 20newsgroup dataset (comp.graphics vs. talk.religion.misc with TF-IDF removing English stop words and words occurring only once or more than 95% of the time). Text feature extraction was done using Scikit-Learn. Figure 6 focuses on the large scale sparse RCV1 (Reuters Corpus Volume 1) dataset, cf. (Schmidt et al., 2013).\nIn all cases, Lasso paths are computed as required to estimate optimal regularization parameters in practice (when using cross-validation one path is computed for each fold). For each Lasso path, solutions are obtained for T \u201c 100 values of \u03bb\u2019s, as detailed in Section 3.3. Remark that the grid used is the default one in both Scikit-Learn and the glmnet R package. With our proposed GAP SAFE screening we obtain on all datasets substantial gains in computational time. We can already get an up to 3x speedup when we require a duality gap smaller than 10\u00b44. The interest of the screening is even clearer for higher accuracies: GAP SAFE sphere is 11x faster than its competitors on the Leukemia dataset, at accuracy 10\u00b48. One can observe that with the parameter grid used here, the larger is p compared to n, the higher is the gain in computation time.\nIn our experiments, the other safe screening rules did not show much speed-up. As one can see on Figure 3, those screening rules keep all the active variables for a wide range of \u03bb\u2019s. The algorithm is thus faster for large \u03bb\u2019s but slower afterwards, since we still compute the screening tests. Even if one can avoid some of these useless computations thanks to formulas like (14) or (10), the corresponding speed-up\nwould not be significant."}, {"heading": "5. Conclusion", "text": "We have presented new results on safe rules for accelerating algorithms solving the Lasso problem (see Appendix for extension to the Elastic Net). First, we have introduced the framework of converging safe rules, a key concept independent of the implementation chosen. Our second contribution was to leverage duality gap computations to create two safer rules satisfying the aforementioned convergence properties. Finally, we demonstrated the important practical benefits of those new rules by applying them to standard dense and sparse datasets using a coordinate descent solver. Future works will extend our framework to generalized linear model and group-Lasso."}, {"heading": "Acknowledgment", "text": "We acknowledge the support from Chair Machine Learning for Big Data at Te\u0301le\u0301com ParisTech and from the Orange/Te\u0301le\u0301com ParisTech think tank phi-TAB. This work benefited from the support of the \u201dFMJH Program Gaspard Monge in optimization and operation research\u201d, and from the support to this program from EDF."}, {"heading": "A. Supplementary materials", "text": "We provided in this Appendix some more details on the theoretical results given in the main part.\nA.1. Dome test\nLet us consider the case where the safe region C is the dome Dpc, r, \u03b1, wq, with parameters: center c, radius r, relative distance ratio \u03b1 and unit normal vector w.\nThe computation of the dome test formula proceeds as follows:\n\u03c3Cpxjq \u201c # cJxj ` r}xj} if wJxj \u0103 \u00b4\u03b1}xj}, cJxj \u00b4 r\u03b1wJxj ` r a p}xj}2 \u00b4 |wJxj |2qp1\u00b4 \u03b12q otherwise. (21)\nand so\n\u03c3Cp\u00b4xjq \u201c # \u00b4cJxj ` r}xj} if \u00b4 wJxj \u0103 \u00b4\u03b1}xj}, \u00b4cJxj ` r\u03b1wJxj ` r a p}xj}2 \u00b4 |wJxj |2qp1\u00b4 \u03b12q otherwise. (22)\nWith the previous display we can now compute \u00b5Cpxjq :\u201c maxp\u03c3Cpxjq, \u03c3Cp\u00b4xjqq. Thanks to the Eq. (6), we express our dome test as:\nIf Mmin \u0103 cJxj \u0103Mmax, then \u03b2\u0302p\u03bbqj \u201c 0. (23)\nUsing the former notation:\nMmax \u201c # 1\u00b4 r}xj} if wJxj \u0103 \u00b4\u03b1}xj}, 1` r\u03b1wJxj \u00b4 r a p}xj}2 \u00b4 |wJxj |2qp1\u00b4 \u03b12q otherwise. (24)\nMmin \u201c # \u00b41` r}xj} if \u00b4 wJxj \u0103 \u00b4\u03b1}xj}, \u00b41` r\u03b1wJxj ` r a p}xj}2 \u00b4 |wJxj |2qp1\u00b4 \u03b12q otherwise. (25)\nLet us introduce the following dome parameters, for any \u03b8 P \u2206X :\n\u2022 Center: c \u201c py{\u03bb` \u03b8q{2.\n\u2022 Radius: r \u201c qR\u03bbp\u03b8q{2.\n\u2022 Ratio: \u03b1 \u201c \u00b41` 2 pR\u03bbp\u03b8q2{ qR\u03bbp\u03b8q2.\n\u2022 Normal vector: w \u201c py{\u03bb\u00b4 \u03b8q{ qR\u03bbp\u03b8q.\nReminding that the support function of a set is the same as the support function of its closed convex hull (Hiriart-Urruty & Lemare\u0301chal, 1993)[Proposition V.2.2.1] means that we only need to optimize over the dome introduced. Therefore, one cannot improve our previous result by optimizing the problem on the intersection of the ball of radius qR\u03bbp\u03b8q and the complement of the ball of radius pR\u03bbp\u03b2q (i.e., the blue region in Figure 2).\nA.2. Proof of Theorem 1\nProof. Define maxjRE\u03bb |xJj \u03b8\u0302p\u03bbq| \u201c t \u0103 1. Fix \u0105 0 such that \u0103 p1\u00b4tq{pmaxjRE\u03bb }xj}q. As Ck is a converging sequence containing \u03b8\u0302p\u03bbq, its diameter is converging to zero, and there exists k0 P N such that @k \u011b k0,@\u03b8 P Ck, }\u03b8 \u00b4 \u03b8\u0302p\u03bbq} \u010f . Hence, for any j R E\u03bb and any \u03b8 P Ck, |xJj p\u03b8\u00b4 \u03b8\u0302p\u03bbqq| \u010f pmaxjRE\u03bb }xj}q}\u03b8\u00b4 \u03b8\u0302p\u03bbq} \u010f pmaxjRE\u03bb }xj}q . Using the triangle inequality, one gets\n|xJj \u03b8| \u010fpmax jRE\u03bb }xj}q `max jRE\u03bb |xJj \u03b8\u0302p\u03bbq|\n\u010fpmax jRE\u03bb }xj}q ` t \u0103 1,\nprovided that \u0103 p1\u00b4 tq{pmaxjRE\u03bb }xj}q. Thus, for any k \u011b k0, Ec\u03bb \u0102 Zp\u03bbqpCkq \u201c Ap\u03bbqpCkqc and Ap\u03bbqpCkq \u0102 E\u03bb.\nFor the reverse inclusion take j P E\u03bb, i.e., |xJj \u03b8\u0302p\u03bbq| \u201c 1. Since for all k P N, \u03b8\u0302p\u03bbq P Ck, then j P Ap\u03bbqpCkq \u201c tj P rps : max\u03b8PCk |xJj \u03b8| \u011b 1u and the result holds.\nA.3. Proof of Proposition 3\nWe detail here the proof of Proposition 3.\nProof. We first use the fact that\nG\u03bbt\u00b41p\u03b2, \u03b8q \u201c 1\n2 \u2016X\u03b2 \u00b4 y\u201622 ` \u03bbt\u00b41 \u2016\u03b2\u20161 \u00b4\n1 2 \u2016y\u201622 ` \u03bb2t\u00b41 2 \u2225\u2225\u2225\u2225\u03b8 \u00b4 y\u03bbt\u00b41 \u2225\u2225\u2225\u22252\n2\n,\nto obtain\n\u2016\u03b2\u20161 \u201c 1\n\u03bbt\u00b41\n\u00b41\n2 \u2016y\u201622 \u00b4 \u2016X\u03b2 \u00b4 y\u2016 2 2 \u00b4 \u03bb2t\u00b41 2 \u2225\u2225\u2225\u2225\u03b8 \u00b4 y\u03bbt\u00b41 \u2225\u2225\u2225\u22252\n2\n`G\u03bbt\u00b41p\u03b2, \u03b8q \u00af .\nThen,\nG\u03bbtp\u03b2,\u03b8q \u201c 1\n2 \u2016X\u03b2 \u00b4 y\u201622 ` \u03bbt \u03bbt\u00b41 \u00b41 2 \u2016y\u201622 \u00b4 1 2 \u2016X\u03b2 \u00b4 y\u201622 \u00b4 \u03bb2t\u00b41 2 \u2225\u2225\u2225\u2225\u03b8 \u00b4 y\u03bbt\u00b41 \u2225\u2225\u2225\u22252\n2\n`G\u03bbt\u00b41p\u03b2, \u03b8q \u00af\n\u00b4 1 2 \u2016y\u201622 ` \u03bb2t 2 \u2225\u2225\u2225\u2225\u03b8 \u00b4 y\u03bbt \u2225\u2225\u2225\u22252\n2\n\u201c1 2 p \u03bbt \u03bbt\u00b41 \u00b4 1q \u2016y\u201622 ` 1 2 p1\u00b4 \u03bbt \u03bbt\u00b41 q \u2016X\u03b2 \u00b4 y\u201622 ` \u03bbt \u03bbt\u00b41 G\u03bbt\u00b41p\u03b2, \u03b8q ` 1 2 ` \u2016\u03bbt\u03b8 \u00b4 y\u201622 \u00b4 \u03bbt \u03bbt\u00b41 \u2016\u03bbt\u00b41\u03b8 \u00b4 y\u201622 \u02d8\n\u201c1 2 p \u03bbt \u03bbt\u00b41 \u00b4 1q \u2016y\u201622 ` 1 2 p1\u00b4 \u03bbt \u03bbt\u00b41 q \u2016X\u03b2 \u00b4 y\u201622 ` \u03bbt \u03bbt\u00b41 G\u03bbt\u00b41p\u03b2, \u03b8q\n` 1 2 \u00b4 \u2016\u03bbt\u03b8 \u00b4 y\u201622 \u00b4 \u03bbt \u03bbt\u00b41 ` \u2016\u03bbt\u03b8 \u00b4 y\u201622 ` \u2016p\u03bbt\u00b41 \u00b4 \u03bbtq\u03b8\u2016 2 2 ` 2p\u03bbt\u03b8 \u00b4 yq Jp\u03bbt\u00b41 \u00b4 \u03bbtq\u03b8 \u02d8 \u00af .\nWe deal with the dot product as\n2\u03bbtp\u03bbt\u00b41 \u00b4 \u03bbtqp\u03b8 \u00b4 y\n\u03bbt qJ\u03b8 \u201c \u03bbtp\u03bbt\u00b41 \u00b4 \u03bbtq ` \u2016\u03b8\u201622 ` \u2225\u2225\u2225\u2225\u03b8 \u00b4 y\u03bbt \u2225\u2225\u2225\u22252 2 \u00b4 \u2225\u2225\u2225\u2225 y\u03bbt \u2225\u2225\u2225\u22252 2 \u02d8 .\nHence,\nG\u03bbtp\u03b2, \u03b8q \u201c 1 2 p \u03bbt \u03bbt\u00b41 \u00b4 1` 1 \u03bbt\u00b41 p\u03bbt\u00b41 \u00b4 \u03bbtqq \u2016y\u201622 ` 1 2 p1\u00b4 \u03bbt \u03bbt\u00b41 q \u2016X\u03b2 \u00b4 y\u201622\n\u00b4 \u03bbt 2 p\u03bbt\u00b41 \u00b4 \u03bbtqq \u2016\u03b8\u201622 ` 1 2 p1\u00b4 \u03bbt \u03bbt\u00b41 \u00b4 1 \u03bbt\u00b41 p\u03bbt\u00b41 \u00b4 \u03bbtqq \u2016\u03bbt\u03b8 \u00b4 y\u201622 ` \u03bbt \u03bbt\u00b41 G\u03bbt\u00b41p\u03b2, \u03b8q\n\u201c1 2\n\u02c6\n1\u00b4 \u03bbt \u03bbt\u00b41\n\u02d9\n\u2016X\u03b2 \u00b4 y\u201622 \u00b4 \u03bbt 2 p\u03bbt\u00b41 \u00b4 \u03bbtqq}\u03b8}2 ` \u03bbt \u03bbt\u00b41 G\u03bbt\u00b41p\u03b2, \u03b8q.\nWe observe in the end that\n2\n\u03bb2t G\u03bbtp\u03b2, \u03b8q \u201c\n\u02c6\n1\u00b4 \u03bbt \u03bbt\u00b41 \u02d9 \u2225\u2225\u2225\u2225X\u03b2 \u00b4 y\u03bbt \u2225\u2225\u2225\u22252 2 \u00b4 \u02c6 \u03bbt\u00b41 \u03bbt \u00b4 1 \u02d9 \u2016\u03b8\u201622 ` 2 \u03bbt\u00b41\u03bbt G\u03bbt\u00b41p\u03b2, \u03b8q.\nA.4. Elastic-Net\nThe previously proposed tests can be adapted straightforwardly to the Elastic-Net estimator (Zou & Hastie, 2005). We provide here some more details for the interested reader.\nmin \u03b2PRp\n1 2 \u2016X\u03b2 \u00b4 y\u201622 ` \u03bb\u03b1 \u2016\u03b2\u20161 ` \u03bb 2 p1\u00b4 \u03b1q \u2016\u03b2\u201622 . (26)\nOne can reformulate this problem as a Lasso problem\nmin \u03b2PRp\n1\n2 \u2225\u2225\u2225X\u0303\u03b2 \u00b4 y\u0303\u2225\u2225\u22252 2 ` \u03bb\u03b1 \u2016\u03b2\u20161 , (27)\nwhere X\u0303 \u201c \u02c6 X a\np1\u00b4 \u03b1q\u03bbIp\n\u02d9 P Rn`p,p and y\u0303 \u201c \u02c6\ny 0\n\u02d9\nP Rn`p. With this modification all the tests introduced for the\nLasso can be adapted for the Elastic-Net."}], "references": [{"title": "Bolasso: model consistent Lasso estimation through the bootstrap", "author": ["F. Bach"], "venue": "In ICML,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": "Ann. Statist.,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "A dynamic screening principle for the lasso", "author": ["A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval"], "venue": "In EUSIPCO,", "citeRegEx": "Bonnefoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bonnefoy et al\\.", "year": 2014}, {"title": "Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso", "author": ["A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval"], "venue": "ArXiv e-prints,", "citeRegEx": "Bonnefoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bonnefoy et al\\.", "year": 2014}, {"title": "Statistics for highdimensional data", "author": ["P. B\u00fchlmann", "S. van de Geer"], "venue": null, "citeRegEx": "B\u00fchlmann and Geer,? \\Q2011\\E", "shortCiteRegEx": "B\u00fchlmann and Geer", "year": 2011}, {"title": "Enhancing sparsity by reweighted l1 minimization", "author": ["E.J. Cand\u00e8s", "M.B. Wakin", "S.P. Boyd"], "venue": "J. Fourier Anal. Applicat.,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2008}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["A. Chambolle", "T. Pock"], "venue": "J. Math. Imaging Vis.,", "citeRegEx": "Chambolle and Pock,? \\Q2011\\E", "shortCiteRegEx": "Chambolle and Pock", "year": 2011}, {"title": "Atomic decomposition by basis pursuit", "author": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Chen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1998}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I.M. Johnstone", "R. Tibshirani"], "venue": "Ann. Statist.,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Safe feature elimination in sparse supervised learning", "author": ["L. El Ghaoui", "V. Viallon", "T. Rabbani"], "venue": "J. Pacific Optim.,", "citeRegEx": "Ghaoui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ghaoui et al\\.", "year": 2012}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Fan and Li,? \\Q2001\\E", "shortCiteRegEx": "Fan and Li", "year": 2001}, {"title": "Sure independence screening for ultrahigh dimensional feature space", "author": ["J. Fan", "J. Lv"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Fan and Lv,? \\Q2008\\E", "shortCiteRegEx": "Fan and Lv", "year": 2008}, {"title": "Pathwise coordinate optimization", "author": ["J. Friedman", "T. Hastie", "H. H\u00f6fling", "R. Tibshirani"], "venue": "Ann. Appl. Stat.,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "Mixednorm estimates for the M/EEG inverse problem using accelerated gradient methods", "author": ["A. Gramfort", "M. Kowalski", "M. H\u00e4m\u00e4l\u00e4inen"], "venue": "Physics in Medicine and Biology,", "citeRegEx": "Gramfort et al\\.,? \\Q1937\\E", "shortCiteRegEx": "Gramfort et al\\.", "year": 1937}, {"title": "TIGRESS: Trustful Inference of Gene REgulation using Stability Selection", "author": ["Haury", "A.-C", "F. Mordelet", "P. Vera-Licona", "J.P. Vert"], "venue": "BMC systems biology,", "citeRegEx": "Haury et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Haury et al\\.", "year": 2012}, {"title": "Convex analysis and minimization algorithms. I, volume 305", "author": ["Hiriart-Urruty", "J.-B", "C. Lemar\u00e9chal"], "venue": "SpringerVerlag, Berlin,", "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 1993}, {"title": "An interior-point method for large-scale l1regularized least squares", "author": ["Kim", "S.-J", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky"], "venue": "IEEE J. Sel. Topics Signal Process.,", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "Local linear convergence of forward\u2013backward under partial smoothness", "author": ["J. Liang", "J. Fadili", "G. Peyr\u00e9"], "venue": "In NIPS, pp. 1970\u20131978,", "citeRegEx": "Liang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2014}, {"title": "Sparse MRI: The application of compressed sensing for rapid MR imaging", "author": ["M. Lustig", "D.L. Donoho", "J.M. Pauly"], "venue": "Magnetic Resonance in Medicine,", "citeRegEx": "Lustig et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lustig et al\\.", "year": 2007}, {"title": "Sparse coding for machine learning, image processing and computer vision", "author": ["J. Mairal"], "venue": "PhD thesis, E\u0301cole normale supe\u0301rieure de Cachan,", "citeRegEx": "Mairal,? \\Q2010\\E", "shortCiteRegEx": "Mairal", "year": 2010}, {"title": "Complexity analysis of the lasso regularization path", "author": ["J. Mairal", "B. Yu"], "venue": "In ICML,", "citeRegEx": "Mairal and Yu,? \\Q2012\\E", "shortCiteRegEx": "Mairal and Yu", "year": 2012}, {"title": "A new approach to variable selection in least squares problems", "author": ["M.R. Osborne", "B. Presnell", "B.A. Turlach"], "venue": "IMA J. Numer. Anal.,", "citeRegEx": "Osborne et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2000}, {"title": "Variational analysis, volume 317 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences", "author": ["R.T. Rockafellar", "Wets", "R.J.-B"], "venue": null, "citeRegEx": "Rockafellar et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Rockafellar et al\\.", "year": 1998}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M. Schmidt", "N. Le Roux", "F. Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R.J. Tibshirani"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Tibshirani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2012}, {"title": "The lasso problem and uniqueness", "author": ["R.J. Tibshirani"], "venue": "Electron. J. Stat.,", "citeRegEx": "Tibshirani,? \\Q2013\\E", "shortCiteRegEx": "Tibshirani", "year": 2013}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl.,", "citeRegEx": "Tseng,? \\Q2001\\E", "shortCiteRegEx": "Tseng", "year": 2001}, {"title": "Smallsample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering", "author": ["G. Varoquaux", "A. Gramfort", "B. Thirion"], "venue": "In ICML,", "citeRegEx": "Varoquaux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Varoquaux et al\\.", "year": 2012}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"], "venue": "In NIPS,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Fast lasso screening tests based on correlations", "author": ["Z.J. Xiang", "P.J. Ramadge"], "venue": "In ICASSP, pp", "citeRegEx": "Xiang and Ramadge,? \\Q2012\\E", "shortCiteRegEx": "Xiang and Ramadge", "year": 2012}, {"title": "Learning sparse representations of high dimensional data on large scale dictionaries", "author": ["Z.J. Xiang", "H. Xu", "P.J. Ramadge"], "venue": "In NIPS,", "citeRegEx": "Xiang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2011}, {"title": "Screening tests for lasso problems", "author": ["Z.J. Xiang", "Y. Wang", "P.J. Ramadge"], "venue": "arXiv preprint arXiv:1405.4897,", "citeRegEx": "Xiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2014}, {"title": "Three structural results on the lasso problem", "author": ["P. Xu", "P.J. Ramadge"], "venue": "In ICASSP, pp", "citeRegEx": "Xu and Ramadge,? \\Q2013\\E", "shortCiteRegEx": "Xu and Ramadge", "year": 2013}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["Zhang", "C.-H"], "venue": "Ann. Statist.,", "citeRegEx": "Zhang and C..H.,? \\Q2010\\E", "shortCiteRegEx": "Zhang and C..H.", "year": 2010}, {"title": "A general theory of concave regularization for high-dimensional sparse estimation problems", "author": ["Zhang", "C.-H", "T. Zhang"], "venue": "Statistical Science,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "The adaptive lasso and its oracle properties", "author": ["H. Zou"], "venue": "J. Am. Statist. Assoc.,", "citeRegEx": "Zou,? \\Q2006\\E", "shortCiteRegEx": "Zou", "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Zou and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie", "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "In such a context, the least squares with `1 regularization, referred to as the Lasso (Tibshirani, 1996) in statistics, or Basis Pursuit (Chen et al.", "startOffset": 86, "endOffset": 104}, {"referenceID": 8, "context": "In such a context, the least squares with `1 regularization, referred to as the Lasso (Tibshirani, 1996) in statistics, or Basis Pursuit (Chen et al., 1998) in signal processing, has been one of the most popular tools.", "startOffset": 137, "endOffset": 156}, {"referenceID": 2, "context": "It enjoys theoretical guarantees (Bickel et al., 2009), as well as practical benefits: it provides sparse solutions and fast convex solvers are available.", "startOffset": 33, "endOffset": 54}, {"referenceID": 20, "context": "one can mention dictionary learning (Mairal, 2010), biostatistics (Haury et al.", "startOffset": 36, "endOffset": 50}, {"referenceID": 15, "context": "one can mention dictionary learning (Mairal, 2010), biostatistics (Haury et al., 2012) and medical imaging (Lustig et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 19, "context": ", 2012) and medical imaging (Lustig et al., 2007; Gramfort et al., 2012) to name a few.", "startOffset": 28, "endOffset": 72}, {"referenceID": 0, "context": "For stability selection methods (Meinshausen & B\u00fchlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved.", "startOffset": 32, "endOffset": 98}, {"referenceID": 29, "context": "For stability selection methods (Meinshausen & B\u00fchlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved.", "startOffset": 32, "endOffset": 98}, {"referenceID": 37, "context": "For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Cand\u00e8s et al., 2008).", "startOffset": 133, "endOffset": 186}, {"referenceID": 6, "context": "For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Cand\u00e8s et al., 2008).", "startOffset": 133, "endOffset": 186}, {"referenceID": 22, "context": "Among possible algorithmic candidates for solving the Lasso, one can mention homotopy methods (Osborne et al., 2000), LARS (Efron et al.", "startOffset": 94, "endOffset": 116}, {"referenceID": 9, "context": ", 2000), LARS (Efron et al., 2004), and approximate homotopy (Mairal & Yu, 2012), that provide solutions for the full Lasso path, i.", "startOffset": 14, "endOffset": 34}, {"referenceID": 13, "context": "More recently, particularly for p \u0105 n, coordinate descent approaches (Friedman et al., 2007) have proved to be among the best methods to tackle large scale problems.", "startOffset": 69, "endOffset": 92}, {"referenceID": 33, "context": "We refer to (Xiang et al., 2014) for a concise introduction on safe rules.", "startOffset": 12, "endOffset": 32}, {"referenceID": 26, "context": "This is for instance the strategy adopted for the strong rules (Tibshirani et al., 2012).", "startOffset": 63, "endOffset": 88}, {"referenceID": 0, "context": "For stability selection methods (Meinshausen & B\u00fchlmann, 2010; Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso problems need to be solved. For non-convex approaches such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010), solving the Lasso is often a required preliminary step (Zou, 2006; Zhang & Zhang, 2012; Cand\u00e8s et al., 2008). Among possible algorithmic candidates for solving the Lasso, one can mention homotopy methods (Osborne et al., 2000), LARS (Efron et al., 2004), and approximate homotopy (Mairal & Yu, 2012), that provide solutions for the full Lasso path, i.e., for all possible choices of tuning parameter \u03bb. More recently, particularly for p \u0105 n, coordinate descent approaches (Friedman et al., 2007) have proved to be among the best methods to tackle large scale problems. Following the seminal work by El Ghaoui et al. (2012), screening techniques have emerged as a way to exploit the known sparsity of the solution by discarding features prior to starting a Lasso solver.", "startOffset": 63, "endOffset": 847}, {"referenceID": 30, "context": "This road has been pursued in (Wang et al., 2013; Xu & Ramadge, 2013; Xiang et al., 2014), and can be thought of as a \u201cwarm start\u201d of the screening (in addition to the warm start of the solution itself).", "startOffset": 30, "endOffset": 89}, {"referenceID": 33, "context": "This road has been pursued in (Wang et al., 2013; Xu & Ramadge, 2013; Xiang et al., 2014), and can be thought of as a \u201cwarm start\u201d of the screening (in addition to the warm start of the solution itself).", "startOffset": 30, "endOffset": 89}, {"referenceID": 17, "context": "Denoting \u2206X \u201c \u03b8 P R : \u2223\u2223xJj \u03b8\u2223\u2223 \u010f 1,@j P rps( the dual feasible set, a dual formulation of the Lasso reads (see for instance Kim et al. (2007) or Xiang et al.", "startOffset": 125, "endOffset": 143}, {"referenceID": 17, "context": "Denoting \u2206X \u201c \u03b8 P R : \u2223\u2223xJj \u03b8\u2223\u2223 \u010f 1,@j P rps( the dual feasible set, a dual formulation of the Lasso reads (see for instance Kim et al. (2007) or Xiang et al. (2014)):", "startOffset": 125, "endOffset": 166}, {"referenceID": 32, "context": ", 2012) y{\u03bb q R\u03bbp y \u03bbmax q \u03bbmax \u201c }X y}8\u201c|xj\u2039y| Dynamic ST3 (Xiang et al., 2011) y{\u03bb \u0301 \u03b4xj\u2039 p q R\u03bbp\u03b8kq  \u0301 \u03b4q 1 2 \u03b4 \u201c ` \u03bbmax \u03bb \u0301 1 \u0306", "startOffset": 60, "endOffset": 80}, {"referenceID": 30, "context": ", as in (11) ) Sequential (Wang et al., 2013) \u03b8\u0302p\u03bbt \u03011q \u02c7", "startOffset": 26, "endOffset": 45}, {"referenceID": 33, "context": "See for instance (Xiang et al., 2014) for more details.", "startOffset": 17, "endOffset": 37}, {"referenceID": 27, "context": "If C \u201c t\u03b8\u0302p\u03bbqu, the safe active set is the equicorrelation set Ap\u03bbqpCq \u201c E\u03bb :\u201c tj P rps : |xj \u03b8\u0302p\u03bbq| \u201c 1u (in most cases (Tibshirani, 2013) it is exactly the active set of \u03b2\u0302p\u03bbq).", "startOffset": 121, "endOffset": 139}, {"referenceID": 27, "context": "If it is not unique, the equicorrelation set contains all the solutions\u2019 supports and there exists a Lasso solution whose support is exactly this set (Tibshirani, 2013)[Lemma 12].", "startOffset": 150, "endOffset": 168}, {"referenceID": 33, "context": "For simplicity we focus only on balls and domes, though more complicated regions could be investigated (Xiang et al., 2014).", "startOffset": 103, "endOffset": 123}, {"referenceID": 33, "context": "been considered for instance in (El Ghaoui et al., 2012; Xiang & Ramadge, 2012; Xiang et al., 2014; Bonnefoy et al., 2014b).", "startOffset": 32, "endOffset": 123}, {"referenceID": 33, "context": "Due to its length though, the formula is deferred to the Appendix (see also (Xiang et al., 2014)[Lemma 3] for more details).", "startOffset": 76, "endOffset": 96}, {"referenceID": 8, "context": "Following El Ghaoui et al. (2012) (see also (Bonnefoy et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 18, "context": "sometimes referred to as finite identification of the support (Liang et al., 2014).", "startOffset": 62, "endOffset": 82}, {"referenceID": 28, "context": ", Forward-Backward (Beck & Teboulle, 2009), Primal Dual (Chambolle & Pock, 2011), coordinatedescent (Tseng, 2001; Friedman et al., 2007)) and relies only on the convergence of a sequence of safe regions.", "startOffset": 100, "endOffset": 136}, {"referenceID": 13, "context": ", Forward-Backward (Beck & Teboulle, 2009), Primal Dual (Chambolle & Pock, 2011), coordinatedescent (Tseng, 2001; Friedman et al., 2007)) and relies only on the convergence of a sequence of safe regions.", "startOffset": 100, "endOffset": 136}, {"referenceID": 17, "context": "A more general result is proved for a specific algorithm (Forward-Backward) in Liang et al. (2014). Interestingly, our scheme is independent of the algorithm considered (e.", "startOffset": 79, "endOffset": 99}, {"referenceID": 30, "context": "The basic sphere test of (Wang et al., 2013) requires the exact dual solution \u03b8 \u201c \u03b8\u0302p\u03bbt \u03011q for center, and has radius |1{\u03bbt \u03011{\u03bbt \u03011| \u2016y\u2016, which is strictly larger than ours.", "startOffset": 25, "endOffset": 44}, {"referenceID": 30, "context": "Note that contrarily to former sequential rules (Wang et al., 2013), our introduced GAP SAFE rules still work when one has only access to approximations of \u03b8\u0302p\u03bbt \u03011q.", "startOffset": 48, "endOffset": 67}, {"referenceID": 25, "context": "We did not compare our method against the strong rules of Tibshirani et al. (2012) because they are not safe and therefore need complex post-processing with parameters to tune.", "startOffset": 58, "endOffset": 83}, {"referenceID": 25, "context": "We did not compare our method against the strong rules of Tibshirani et al. (2012) because they are not safe and therefore need complex post-processing with parameters to tune. Also we did not compare against the sequential rule of Wang et al. (2013) (e.", "startOffset": 58, "endOffset": 251}, {"referenceID": 8, "context": "As the SAFE screening rule of El Ghaoui et al. (2012) is sequential but not dynamic, for a given \u03bb the proportion of screened variables does not depend on K.", "startOffset": 33, "endOffset": 54}, {"referenceID": 3, "context": "The rules of Bonnefoy et al. (2014a) are more efficient on this dataset but they do not benefit much from the dynamic framework.", "startOffset": 13, "endOffset": 37}, {"referenceID": 24, "context": "(Schmidt et al., 2013).", "startOffset": 0, "endOffset": 22}], "year": 2015, "abstractText": "Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the socalled safe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.", "creator": "LaTeX with hyperref package"}}}