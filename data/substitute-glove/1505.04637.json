{"id": "1505.04637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2015", "title": "Ensemble of Example-Dependent Cost-Sensitive Decision Trees", "abstract": "Several seems - today variants continuing unlike both - particularly limit - sensitive present creating, away has reducing due to photoionization and/or may instances and not only within various. However, standard classification subject 've n't take these premium itself data, these likewise a constant spending on authoritativeness fault. In number art, some methods that next as example, financial wages into the service of particularly predictive people partly approving, with the example - relatively cost - vulnerable decision tree fourier being during one for little taken reaching savings. In does pages be promised he but framework made piano called example - dependent cost - sensitive decision - trees. The principle portion its creating whereas unlike - consequence value - particularly any trees in measurement subsamples of entered training allowed, and out combining them using 30 they combination evolving. Moreover, good modify pair came only - suggests combination discussion; cost - problematic trades passed example minimum - sensitive ramekin, the latter being based, the estimated - specific cohesion regression forms. Finally, making weeks different automate, over four real - making standard: credit card fraud detection, eyeballs modeling, credit 3-pointers that directly consultancy, be evaluations an proposed method threat state - of - to - theater example - constrained putting - sensitive techniques, form, expensive - proportionate comparison, Bayes reducing concern and only - pose deal conifers. The outcomes events that long accepted optimization none makes this no all databases, , the mind still growth savings.", "histories": [["v1", "Mon, 18 May 2015 13:43:53 GMT  (150kb,D)", "http://arxiv.org/abs/1505.04637v1", "13 pages, 6 figures, Submitted for possible publication"]], "COMMENTS": "13 pages, 6 figures, Submitted for possible publication", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alejandro correa bahnsen", "djamila aouada", "bjorn ottersten"], "accepted": false, "id": "1505.04637"}, "pdf": {"name": "1505.04637.pdf", "metadata": {"source": "CRF", "title": "Ensemble of Example-Dependent Cost-Sensitive Decision Trees", "authors": ["Alejandro Correa Bahnsen", "Djamila Aouada", "Bj\u00f6rn Ottersten"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Cost-sensitive classification, ensemble methods, credit scoring, fraud detection, churn modeling, direct marketing.\nF"}, {"heading": "1 INTRODUCTION", "text": "C LASSIFICATION, in the context of machine learning,deals with the problem of predicting the class of a set of examples given their features. Traditionally, classification methods aim at minimizing the misclassification of examples, in which an example is misclassified if the predicted class is different from the true class. Such a traditional framework assumes that all misclassification errors carry the same cost. This is not the case in many real-world applications. Methods that use different misclassification costs are known as cost-sensitive classifiers. Typical costsensitive approaches assume a constant cost for each type of error, in the sense that, the cost depends on the class and is the same among examples [1].\nThis class-dependent approach is not realistic in many real-world applications. For example in credit card fraud detection, failing to detect a fraudulent transaction may have an economical impact from a few to thousands of Euros, depending on the particular transaction and card holder [2]. In churn modeling, a model is used for predicting which customers are more likely to abandon a service provider. In this context, failing to identify a profitable or unprofitable churner has a significant different economic result [3]. Similarly, in direct marketing, wrongly predicting that a customer will not accept an offer when in fact he will, may have a different financial impact, as not all customers generate the same profit [4]. Lastly, in credit scoring, accepting loans from bad customers does not have the same economical loss, since customers have different credit lines, therefore, different profit [5].\n\u2022 The authors are with the Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg. E-mail: {alejandro.correa, djamila.aouada, bjorn.ottersten}@uni.lu\nIn order to deal with these specific types of cost-sensitive problems, called example-dependent cost-sensitive, some methods have been proposed. Standard solutions consist in re-weighting the training examples based on their costs, either by cost-proportionate rejection-sampling [4], or cost-proportionate-sampling [1]. The rejection-sampling approach consists in selecting a random subset of the training set, by randomly selecting examples and accepting them with a probability proportional to the misclassification cost of each example. The over-sampling method consists in creating a new training set, by making copies of each example taking into account the misclassification cost. However, costproportionate over-sampling increases the training set and it also may result in over-fitting [6]. Also, none of these methods uses take into account the cost of correct classification. Moreover, the literature on example-dependent costsensitive methods is limited, often because there is a lack of publicly available datasets that fit the problem [7]. Recently, we have proposed different methods that take into account the different example-dependent costs, in particular: Bayes minimum risk (BMR) [8], cost-sensitive logistic regression [9], and cost-sensitive decision tree (CSDT ) [10].\nThe CSDT method is based on a new splitting criteria which is cost-sensitive, used during the tree construction. Then, after the tree is fully grown, it is pruned by using a cost-based pruning criteria. This method was shown to have better results than traditional approaches, in the sense of lower financial costs across different real-world applications, such as in credit card fraud detection and credit scoring. However, the CSDT algorithm only creates one tree in order to make a classification, and as noted in [11], individual decision trees typically suffer from high variance. A very efficient and simple way to address this flaw is to use them in the context of ensemble methods.\nar X\niv :1\n50 5.\n04 63\n7v 1\n[ cs\n.L G\n] 1\n8 M\nay 2\n01 5\n2 Ensemble learning is a widely studied topic in the machine learning community. The main idea behind the ensemble methodology is to combine several individual base classifiers in order to have a classifier that outperforms each of them [12]. Nowadays, ensemble methods are one of the most popular and well studied machine learning techniques [13], and it can be noted that since 2009 all the first-place and second-place winners of the KDD-Cup competition1 used ensemble methods. The core principle in ensemble learning, is to induce random perturbations into the learning procedure in order to produce several different base classifiers from a single training set, then combining the base classifiers in order to make the final prediction. In order to induce the random permutations and therefore create the different base classifiers, several methods have been proposed, in particular: bagging [14], pasting [15], random forests [16] and random patches [11]. Finally, after the base classifiers are trained, they are typically combined using either majority voting, weighted voting or stacking [13].\nIn the context of cost-sensitive classification, some authors have proposed methods for using ensemble techniques. In [17], the authors proposed a framework for costsensitive boosting that is expected to minimized the losses by using optimal cost-sensitive decision rules. In [18], a bagging algorithm with adaptive costs was proposed. In his doctoral thesis, Nesbitt [19], proposed a method for costsensitive tree-stacking. In this method different decision trees are learned, and then combined in a way that a cost function is minimize. Lastly in [20], a survey of application of cost-sensitive learning with decision trees is shown, in particular including other methods that create cost-sensitive ensembles. However, in all these methods, the misclassification costs only dependent on the class, therefore, assuming a constant cost across examples. As a consequence, these methods are not well suited for example-dependent costsensitive problems.\nIn this paper we propose a new framework of ensembles of example-dependent cost-sensitive decision-trees, by training example-dependent cost-sensitive decision trees using four different random inducer methods and then blending them using three different combination approaches. Moreover, we propose two new cost-sensitive combination approaches, cost-sensitive weighted voting and costsensitive stacking. The latter being an extension of our previously proposed cost-sensitive logistic regression. We evaluate the proposed framework using five different databases from four real-world problems. In particular, credit card fraud detection, churn modeling, credit scoring and direct marketing. The results show that the proposed method outperforms state-of-the-art example-dependent cost-sensitive methods in three databases, and have a similar result in the other two. Furthermore, our source code, as used for the experiments, is publicly available as part of the CostSensitiveClassification2 library.\nThe remainder of the paper is organized as follows. In Section 2, we explain the background behind exampledependent cost-sensitive classification and ensemble learning. In Section 3, we present the proposed ensembles\n1. https //www.sigkdd.org/kddcup/ 2. https://github.com/albahnsen/CostSensitiveClassification\nof cost-sensitive decision-trees framework. Moreover, in Section 4, we prove theoretically that combining individual cost-sensitive classifiers achives better results in the sense of higher financial savings. Then the experimental setup and the different datasets are described in Section 5. Subsequently, the proposed algorithms are evaluated and compared against state-of-the-art methods on these different datasets. Finally, conclusions are given in Section 7."}, {"heading": "2 BACKGROUND AND PROBLEM FORMULATION", "text": "This work is related to two groups of research in the field of machine learning: (i) example-dependent cost-sensitive classification, and (ii) ensemble learning."}, {"heading": "2.1 Example-dependent cost-sensitive classification", "text": "Classification deals with the problem of predicting the class yi of a set S of examples or instances, given their k features xi \u2208 Rk. The objective is to construct a function f(\u00b7) that makes a prediction ci of the class of each example using its variables xi. Traditionally, machine learning classification methods are designed to minimize some sort of misclassification measure such as the F1Score [21]; therefore, assuming that different misclassification errors have the same cost. As discussed before, this is not suitable in many real-world applications. Indeed, two classifiers with equal misclassification rate but different numbers of false positives and false negatives do not have the same impact on cost since CFPi 6= CFNi ; therefore, there is a need for a measure that takes into account the actual costs of each example i.\nIn this context, binary classification costs can be represented using a 2x2 cost matrix [1], that introduces the costs associated with two types of correct classification, true positives (CTPi ), true negatives (CTNi ), and the two types of misclassification errors, false positives (CFPi ), false negatives (CFNi ), as defined in TABLE 1. Conceptually, the cost of correct classification should always be lower than the cost of misclassification. These are referred to as the reasonableness conditions [1], and are defined as CFPi > CTNi and CFNi > CTPi .\nLet S be a set of N examples xi, where each example is represented by the augmented feature vector x\u2217i = [xi, CTPi , CFPi , CFNi , CTNi ] and labelled using the class label yi. A classifier f which generates the predicted label ci for each example i is trained using the set S . Using the cost matrix, an example-dependent cost statistic [8], is defined as:\nCost(f(x\u2217i )) =yi(ciCTPi + (1\u2212 ci)CFNi)+ (1\u2212 yi)(ciCFPi + (1\u2212 ci)CTNi), (1)\n3 leading to a total cost of:\nCost(f(S)) = N\u2211 i=1 Cost(f(x\u2217i )). (2)\nHowever, the total cost may not be easy to interpret. In [22], a normalized cost measure was proposed, by dividing the total cost by the theoretical maximum cost, which is the cost of misclassifying every example. The normalized cost is calculated using\nCostn(f(S)) = Cost(f(S))\u2211N\ni=1 CFNi \u00b7 10(yi) + CFPi \u00b7 11(yi) , (3)\nwhere 1c(z) is an indicator function that takes the value of one if z = c and zero if z 6= c.\nWe proposed a similar approach in [9], where the savings corresponding to using an algorithm are defined as the cost of the algorithm versus the cost of using no algorithm at all. To do that, the cost of the costless class is defined as\nCostl(S) = min{Cost(f0(S)), Cost(f1(S))}, (4)\nwhere\nfa(S) = a, with a \u2208 {0, 1}. (5)\nThe cost improvement can be expressed as the cost of savings as compared with Costl(S).\nSavings(f(S)) = Costl(S)\u2212 Cost(f(S)) Costl(S) . (6)"}, {"heading": "2.2 Ensemble learning", "text": "Ensemble learning is a widely studied topic in the machine learning community. The main idea behind the ensemble methodology is to combine several individual classifiers, referred to as base classifiers, in order to have a classifier that outperforms everyone of them [12]. There are three main reasons regarding why ensemble methods perform better than single models: statistical, computational and representational [23]. First, from a statistical point of view, when the learning set is too small, an algorithm can find several good models within the search space, that arise to the same performance on the training set S . Nevertheless, without a validation set, there is risk of choosing the wrong model. The second reason is computational; in general, algorithms rely on some local search optimization and may get stuck in a local optima. Then, an ensemble may solve this by focusing different algorithms to different spaces across the training set. The last reason is representational. In most cases, for a learning set of finite size, the true function f cannot be represented by any of the candidate models. By combining several models in an ensemble, it may be possible to obtain a model with a larger coverage across the space of representable functions.\nThe most typical form of an ensemble is made by combining T different base classifiers. Each base classifier M(Sj) is trained by applying algorithm M to a random subset Sj of the training set S . For simplicity we define Mj \u2261 M(Sj) for j = 1, . . . , T , and M = {Mj}Tj=1 a set\nof base classifiers. Then, these models are combined using majority voting to create the ensemble H as follows\nfmv(S,M) = arg max c\u2208{0,1} T\u2211 j=1 1c(Mj(S)). (7)\nMoreover, if we assume that each one of the T base classifiers has a probability \u03c1 of being correct, the probability of an ensemble making the correct decision, denoted by Pc, can be calculated using the binomial distribution [24]\nPc = T\u2211\nj>T/2\n( T\nj\n) \u03c1j(1\u2212 \u03c1)T\u2212j . (8)\nFurthermore, as shown in [25], if T \u2265 3 then:\nlim T\u2192\u221e Pc =  1 if \u03c1 > 0.5 0 if \u03c1 < 0.5 0.5 if \u03c1 = 0.5,\n(9)\nleading to the conclusion that\n\u03c1 \u2265 0.5 and T \u2265 3 \u21d2 Pc \u2265 \u03c1. (10)"}, {"heading": "3 ENSEMBLES OF COST-SENSITIVE DECISIONTREES", "text": "In this section, we present our proposed framework for ensembles of example-dependent cost-sensitive decisiontrees (ECSDT ). The framework is based on expanding our previous contribution on example-dependent cost-sensitive decision trees (CSDT ) [10]. In particular, we create many different CSDT on random subsamples of the training set, and then combine them using different combination methods. Moreover, we propose two new cost-sensitive combination approaches, cost-sensitive weighted voting and cost-sensitive stacking. The latter being an extension of our previously proposed cost-sensitive logistic regression (CSLR) [9].\nThe remainder of the section is organized as follows: First, we introduce the example-dependent cost-sensitive decision tree. Then we present the different random inducers and combination methods. Finally, we define our proposed algorithms.\n3.1 Cost-sensitive decision tree (CSDT )\nIntroducing the cost into the training of a decision tree has been a widely study way of making classifiers costsensitive [20]. However, in most cases, approaches that have been proposed only deal with the problem when the cost depends on the class and not on the example [26]\u2013[31]. In [10], we proposed an example-dependent cost-sensitive decision trees (CSDT ) algorithm, that takes into account the example-dependent costs during the training and pruning of a tree.\nIn the CSDT method, a new splitting criteria is used during the tree construction. In particular, instead of using a traditional splitting criteria such as Gini, entropy or misclassification, the cost as defined in (1), of each tree node is calculated, and the gain of using each split evaluated as the decrease in total cost of the algorithm.\n4 The cost-based impurity measure is defined by comparing the costs when all the examples in a leaf are classified as negative and as positive,\nIc(S) = min { Cost(f0(S)), Cost(f1(S)) } . (11)\nThen, using the cost-based impurity, the gain of using the splitting rule (xj , lj), that is the rule of splitting the set S on feature xj on value lj , is calculated as:\nGain(xj , lj) = Ic(S)\u2212 |Sl| |S| Ic(Sl)\u2212 |Sr| |S| Ic(Sr), (12)\nwhere Sl = {x\u2217i |x\u2217i \u2208 S \u2227x j i \u2264 lj}, Sr = {x\u2217i |x\u2217i \u2208 S \u2227x j i > lj}, and | \u00b7 | denotes the cardinality. Afterwards, using the cost-based gain measure, a decision tree is grown until no further splits can be made.\nLastly, after the tree is constructed, it is pruned by using a cost-based pruning criteria\nPCc = Cost(f(S))\u2212 Cost(f\u2217(S)), (13)\nwhere f\u2217 is the classifier of the tree without the pruned node."}, {"heading": "3.2 Algorithms", "text": "With the objective of creating an ensemble of exampledependent cost-sensitive decision trees, we first create T different random subsamples Sj for j = 1, . . . , T , of the training set S , and train a CSDT algorithm on each one. In particular we create the different subsets using four different methods: bagging [14], pasting [15], random forests [16] and random patches [11].\nIn bagging [14], base classifiers are built on randomly drawn bootstrap subsets of the original data, hence producing different base classifiers. Similarly, in pasting [15], the base classifiers are built on random samples without replacement from the training set. In random forests [16], using decision trees as the base learner, bagging is extended and combined with a randomization of the input features that are used when considering candidates to split internal nodes. In particular, instead of looking for the best split among all features, the algorithm selects, at each node, a random subset of features and then determines the best split only over these features. In the random patches algorithm [11], base classifiers are created by randomly drawn bootstrap subsets of both examples and features.\nLastly, the base classifiers are combined using either majority voting, cost-sensitive weighted voting and costsensitive stacking. Majority voting consists in collecting the predictions of each base classifier and selecting the decision with the highest number of votes, see (7).\nCost-sensitive weighted voting This method is an extension of weighted voting. First, in the traditional approach, a similar comparison of the votes of the base classifiers is made, but giving a weight \u03b1j to each classifier Mj during the voting phase [13]\nfwv(S,M, \u03b1) = arg max c\u2208{0,1} T\u2211 j=1 \u03b1j1c(Mj(S)), (14)\nwhere \u03b1 = {\u03b1j}Tj=1. The calculation of \u03b1j is related to the performance of each classifier Mj . It is usually defined as the normalized misclassification error of the base classifier Mj in the out of bag set Soobj = S \u2212 Sj\n\u03b1j = 1\u2212 (Mj(Soobj ))\u2211T\nj1=1 1\u2212 (Mj1(Soobj1 ))\n. (15)\nHowever, as discussed in Section 2.1, the misclassification measure is not suitable in many real-world classification problems. We herein propose a method to calculate the weights \u03b1j taking into account the actual savings of the classifiers. Therefore using (6), we define\n\u03b1j = Savings(Mj(Soobj ))\u2211T\nj1=1 Savings(Mj1(Soobj ))\n. (16)\nThis method guaranties that the base classifiers that contribute to a higher increase in savings have more importance in the ensemble.\nCost-sensitive stacking The staking method consists in combining the different base classifiers by learning a second level algorithm on top of them [32]. In this framework, once the base classifiers are constructed using the training set S , a new set is constructed where the output of the base classifiers are now considered as the features while keeping the class labels.\nEven though there is no restriction on which algorithm can be used as a second level learner, it is common to use a linear model [13], such as\nfs(S,M, \u03b2) = g  T\u2211 j=1 \u03b2jMj(S)  , (17) where \u03b2 = {\u03b2j}Tj=1, and g(\u00b7) is the sign function g(z) = sign(z) in the case of a linear regression or the sigmoid function, defined as g(z) = 1/(1 + e\u2212z), in the case of a logistic regression.\nMoreover, following the logic used in [19], we propose learning the set of parameters \u03b2 using our proposed costsensitive logistic regression (CSLR) [9]. The CSLR algorithm consists in introducing example-dependent costs into a logistic regression, by changing the objective function of the model to one that is cost-sensitive. For the specific case of cost-sensitive stacking, we define the cost function as:\nJ(S,M, \u03b2) = N\u2211 i=1 [ yi ( fs(xi,M, \u03b2) \u00b7 (CTPi \u2212 CFNi) + CFNi ) +\n(1\u2212 yi) ( fs(xi,M, \u03b2) \u00b7 (CFPi \u2212 CTNi) + CTNi )] . (18)\nThen, the parameters \u03b2 that minimize the logistic cost function are used in order to combine the different base classifiers. However, as discussed in [9], this cost function is not convex for all possible cost matrices, therefore, we use genetic algorithms to minimize it.\nSimilarly to cost-sensitive weighting, this method guarantees that the base classifiers that contribute to a higher increase in savings have more importance in the ensemble.\n5 Algorithm 1 The proposed ECSDT algorithms. Input: CSDT (an example-dependent cost-sensitive deci-\nsion tree algorithm), T the number of iterations, S the training set, inducer, Ne number of examples for each base classifier, Nf number of examples for each base classifier, combinator. Step 1: Create the set of base classifiers for j \u2190 1 to T do\nswitch (inducer) case Bagging: Sj \u2190 Sample Ne examples from S with replacement. case Pasting: Sj \u2190 Sample Ne examples from S without replacement. case Random forests: Sj \u2190 Sample Ne examples from S with replacement. case Random patches: Sj \u2190 Sample Ne examples and Nf features from S with replacement. end switch Mj \u2190 CSDT (Sj) Soobj \u2190 S \u2212 Sj \u03b1j \u2190 Savings(Mj(Soobj ))\nend for Step 2: Combine the different base classifiers switch (combinator) case Majority voting: H \u2190 fmv(S,M) case Cost-sensitive weighted voting: H \u2190 fwv(S,M, \u03b1) case Cost-sensitive stacking: \u03b2 \u2190 argmin\u03b2\u2208RT J(S,M, \u03b2) H \u2190 fs(S,M, \u03b2)\nOutput: H (Ensemble of cost-sensitive decision trees)\nFurthermore, by learning an additional second level costsensitive method, the combination is made such that the overall savings measure is maximized.\nFinally, Algorithm 1 summarizes the proposed ECSDT methods. In total, we evaluate 12 different algorithms, as four different random inducers (bagging, pasting, random forest and random patches) and three different combinators (majority voting, cost-sensitive weighted voting and costsensitive stacking) can be selected in order to construct the cost-sensitive ensemble."}, {"heading": "4 THEORETICAL ANALYSIS OF THE COSTSENSITIVE ENSEMBLE", "text": "Although the above proposed algorithm is simple, there is no work that has formally investigated ensemble performance in terms other than accuracy. In this section, our aim is to prove theoretically that combining individual costsensitive classifiers achieves better results in the sense of higher savings.\nWe denote Sa, where a \u2208 {0, 1}, as the subset of S where the examples belong to the class a:\nSa = {x\u2217i |yi = a, i \u2208 1, . . . , N}, (19)\nwhere S = S0 \u222a S1, S0 \u2229 S1 = \u2205, and Na = |Sa|. Also, we define the average cost of the base classifiers as:\nCost(M(S)) = 1 T T\u2211 j=1 Cost(Mj(S)). (20)\nFirstly, we prove the following lemma that states the cost of an ensemble H on the subset Sa is lower than the average cost of the base classifiers on the same set for a \u2208 {0, 1}.\nLemma 1. Let H be an ensemble of T \u2265 3 classifiers M = {M1,M2, . . . ,MT }, and S a testing set of size N . If each one of the base classifiers has a probability of being correct higher or equal than one half, \u03c1 \u2265 12 , and the reasonableness conditions of the cost matrix are satisfied, then the following holds true\nCost(H(Sa)) \u2264 Cost(M(Sa)), a \u2208 {0, 1}, (21)\nProof. First, we decompose the total cost of the ensemble by applying equations (1) and (2). Additionally, we separate the analysis for a = 0 and a = 1: \u2022 a = 0 :\nCost(H(S0)) = N0\u2211 i=1 yi(ciCTPi + (1\u2212 ci)CFNi)+\n(1\u2212 yi)(ciCFPi + (1\u2212 ci)CTNi). (22)\nMoreover, we know from (8) that the probability of an ensemble making the right decision, i.e., yi = ci, for any given example, is equal to Pc. Therefore, we can use this probability to estimate the expected savings of an ensemble:\nCost(H(S0)) = N0\u2211 i=1 PcCTNi + (1\u2212 Pc)CFPi . (23)\n\u2022 a = 1 : In the case of S1, and following the same logic as when a = 0, the cost of an ensemble is:\nCost(H(S1)) = N1\u2211 i=1 PcCTPi + (1\u2212 Pc)CFNi . (24)\nThe second part of the proof consists in analyzing the right hand side of (21), specifically, the average cost of the base classifiers on set Sa. To do that, with the help of (2) and (20), we may express the average cost of the base classifiers as:\nCost(M(Sa)) = 1\nT T\u2211 j=1 Na\u2211 i=1 Cost(Mj(x \u2217 i )). (25)\nWe define the set of base classifiers that make a negative prediction as Ti0 = {Mj(x\u2217i )|Mj(x\u2217i ) = 0, j \u2208 1, . . . , T}, similarly, the set of classifiers that make a positive prediction as Ti1 = {Mj(x\u2217i )|Mj(x\u2217i ) = 1, j \u2208 1, . . . , T}. Then, by taking the cost of negative and positive predictions from (5), the average cost of the base learners becomes:\nCost(M(Sa)) = 1\nT Na\u2211 i=1 ( |Ti0| \u00b7 Cost(f0(x\u2217i ))\n+ |Ti1| \u00b7 Cost(f1(x\u2217i )) ) . (26)\n6 We separate the analysis for a = 0 and a = 1: \u2022 a = 0 :\nCost(M(S0)) = N0\u2211 i=1 ( |Ti0| T \u00b7 CTNi + |Ti1| T \u00b7 CFPi ) . (27)\nFurthermore, we know from (8) that an average base classifier will have a correct classification probability of \u03c1, then |Ti0| T = \u03c1, leading to:\nCost(M(S0)) = N0\u2211 i=1 \u03c1 \u00b7 CTNi + (1\u2212 \u03c1) \u00b7 CFPi . (28)\n\u2022 a = 1 : Similarly, for the set S1, the average classifier will have a correct classification probability of \u03c1, then |Ti1|T = \u03c1. Therefore,\nCost(M(S1)) = N1\u2211 i=1 \u03c1 \u00b7 CTPi + (1\u2212 \u03c1) \u00b7 CFNi . (29)\nFinally, by replacing in (21) the expected savings of an ensemble with (23) for a = 0 and (24) for a = 1, and the average cost of the base learners with (28) for a = 0 and (29) for a = 1, (21) is rewritten as: for a = 0: N0\u2211 i=1 PcCTNi + (1\u2212 Pc)CFPi \u2264 N0\u2211 i=1 \u03c1CTNi + (1\u2212 \u03c1)CFPi ,\n(30)\nfor a = 1: N1\u2211 i=1 PcCTPi + (1\u2212 Pc)CFNi \u2264 N1\u2211 i=1 \u03c1CTPi + (1\u2212 \u03c1)CFNi .\n(31)\nSince \u03c1 \u2265 12 , then Pc \u2265 \u03c1 from (10), and using the reasonableness conditions described in Section 2.1, i.e, CFPi > CTNi and CFNi > CTPi , we find that (30) and (31) are True.\nLemma 1 separates the costs on sets S0 and S1. We are interested in analyzing the overall savings of an ensemble. In this direction, we demonstrate in the following theorem, that the expected savings of an ensemble of classifiers are higher than the expected average savings of the base learners.\nTheorem 1. Let H be an ensemble of T \u2265 3 classifiers M = {M1, . . . ,MT }, and S a testing set of size N , then the expected savings of using H in S are lower than the average expected savings of the base classifiers, in other words,\nSavings(H(S)) \u2265 Savings(M(S)). (32)\nProof. Given (6), (32) is equivalent to\nCost(H(S)) \u2264 Cost(M(S)). (33)\nAfterwards, by applying the cost definition (1), and grouping the sets of negative and positive examples using (19), (33) becomes\u2211\na\u2208{0,1}\nCost(H(Sa)) \u2264 \u2211\na\u2208{0,1}\nCost(M(Sa)), (34)\nwhich can be easily proved using Lemma 1, since, if the cost of an ensemble H is lower than the average cost of the base classifiers on both S0 and S1, implies that it is also lower on the sum of the cost on both sets, therefore, proving Theorem 1."}, {"heading": "5 EXPERIMENTAL SETUP", "text": "In this section we present the datasets used to evaluate the propose Ensembles of Example-Dependent Cost-Sensitive Decision-Trees algorithms. We used five datasets from four different real world example-dependent cost-sensitive problems: Credit card fraud detection, churn modeling, credit scoring and direct marketing.\nFor each dataset we used a pre-defined a cost matrix that we previously proposed in different publications. Additionally, we perform an under-sampling, cost-proportionate rejection-sampling and cost-proportionate over-sampling procedures."}, {"heading": "5.1 Credit card fraud detection", "text": "A credit card fraud detection algorithm, consist in identifying those transactions with a high probability of being fraud, based on historical fraud patterns. Different detection systems that are based on machine learning techniques have been successfully used for this problem, for a review see [2].\nCredit card fraud detection is by definition a cost sensitive problem, since the cost of failing to detect a fraud is significantly different from the one when a false alert is made. We used the fraud detection example-dependent cost matrix we proposed in [8], in which the cost of failing to detect a fraud is equal to the amount of the transaction (Amti), and the costs of correct classification and false positives is equal to the administrative cost of investigating a fraud alert (Ca). The cost table is presented in TABLE 2. For a further discussion see [8], [33].\nFor this paper we used a dataset provided by a large European card processing company. The dataset consists of fraudulent and legitimate transactions made with credit and debit cards between January 2012 and June 2013. The total dataset contains 236,735 individual transactions, each one with 27 attributes, including a fraud label indicating whenever a transaction is identified as fraud. This label was created internally in the card processing company, and can be regarded as highly accurate."}, {"heading": "5.2 Churn modeling", "text": "Customer churn predictive modeling deals with estimating the probability of a customer defecting using historical, behavioral and socio-economical information. The problem\n7\nof churn predictive modeling has been widely studied by the data mining and machine learning communities. It is usually tackled by using classification algorithms in order to learn the different patterns of both the churners and non-churners. For a review see [34]. Nevertheless, current state-of-the-art classification algorithms are not well aligned with commercial goals, in the sense that, the models miss to include the real financial costs and benefits during the training and evaluation phases [3].\nWe then follow the example-dependent cost-sensitive methodology for churn modeling we proposed in [35]. When a customer is predicted to be a churner, an offer is made with the objective of avoiding the customer defecting. However, if a customer is actually a churner, he may or not accept the offer with a probability \u03b3i. If the customer accepts the offer, the financial impact is equal to the cost of the offer (Coi ) plus the administrative cost of contacting the customer (Ca). On the other hand, if the customer declines the offer, the cost is the expected income that the clients would otherwise generate, also called customer lifetime value (CLVi), plus Ca. Lastly, if the customer is not actually a churner, he will be happy to accept the offer and the cost will be Coi plus Ca. In the case that the customer is predicted as non-churner, there are two possible outcomes. Either the customer is not a churner, then the cost is zero, or the customer is a churner and the cost is CLVi. In TABLE 3, the cost matrix is shown.\nFor this paper we used a dataset provided by a TV cable provider. The dataset consists of active customers during the first semester of 2014. The total dataset contains 9,410 individual registries, each one with 45 attributes, including a churn label indicating whenever a customer is a churner."}, {"heading": "5.3 Credit scoring", "text": "The objective in credit scoring is to classify which potential customers are likely to default a contracted financial obligation based on the customer\u2019s past financial experience, and with that information decide whether to approve or decline a loan [36]. When constructing credit scores, it is a common practice to use standard cost-insensitive binary classification algorithms such as logistic regression, neural networks, discriminant analysis, genetic programing, decision tree, among others [37]. However, in practice, the cost associated with approving a bad customer is quite different from the cost associated with declining a good customer. Furthermore, the costs are not constant among customers, as customers have different credit line amounts, terms, and even interest rates.\nIn this paper, we used the credit scoring exampledependent cost-sensitive cost matrix we proposed in [9]. The cost matrix is shown in TABLE 4. First, the costs of a\ncorrect classification are zero for every customer. Then, the cost of a false negative is defined as the credit line Cli times the loss given default Lgd. On the other hand, in the case of a false positive, the cost is the sum of ri and CaFP , where ri is the loss in profit by rejecting what would have been a good customer. The second term CaFP , is related to the assumption that the financial institution will not keep the money of the declined customer idle. It will instead give a loan to an alternative customer, and it is calculated as CaFP = \u2212r \u00b7 \u03c00 + Cl \u00b7 Lgd \u00b7 \u03c01.\nFor this paper we use two different publicly available credit scoring datasets. The first dataset is the 2011 Kaggle competition Give Me Some Credit3, in which the objective is to identify those customers of personal loans that will experience financial distress in the next two years. The second dataset is from the 2009 Pacific-Asia Knowledge Discovery and Data Mining conference (PAKDD) competition4. Similarly, this competition had the objective of identifying which credit card applicants were likely to default and by doing so deciding whether or not to approve their applications.\nThe Kaggle Credit dataset contains 112,915 examples, each one with 10 features and the class label. The proportion of default or positive examples is 6.74%. On the other hand, the PAKDD Credit dataset contains 38,969 examples, with 30 features and the class label, with a proportion of 19.88% positives. This database comes from a Brazilian financial institution, and as it can be inferred from the competition description, the data was obtained around 2004."}, {"heading": "5.4 Direct Marketing", "text": "In direct marketing the objective is to classify those customers who are more likely to have a certain response to a marketing campaign [34]. We used a direct marketing dataset from [38]. The dataset contains 45,000 clients of a Portuguese bank who were contacted by phone between March 2008 and October 2010 and received an offer to open a long-term deposit account with attractive interest rates. The dataset contains features such as age, job, marital status, education, average yearly balance and current loan status and the label indicating whether or not the client accepted the offer.\nThis problem is example-dependent cost sensitive, since there are different costs of false positives and false negatives. Specifically, in direct marketing, false positives have the cost of contacting the client, and false negatives have the cost due to the loss of income by failing to contact a client that otherwise would have opened a long-term deposit.\nWe used the direct marketing example-dependent cost matrix we proposed in [33]. The cost matrix is shown in\n3. http://www.kaggle.com/c/GiveMeSomeCredit/ 4. http://sede.neurotech.com.br:443/PAKDD2009/\n8\nTABLE 5, where Ca is the administrative cost of contacting the client, and Inti is the expected income when a client opens a long-term deposit. This last term is defined as the long-term deposit amount times the interest rate spread."}, {"heading": "5.5 Database partitioning", "text": "For each database, 3 different datasets are extracted: training, validation and testing. Each one containing 50%, 25% and 25% of the transactions, respectively. Afterwards, because classification algorithms suffer when the label distribution is skewed towards one of the classes [21], an under-sampling of the positive examples is made, in order to have a balanced class distribution. Additionally, we perform the cost-proportionate rejection-sampling and costproportionate over-sampling procedures. TABLE 6, summarizes the different datasets. It is important to note that the sampling procedures were only applied to the training dataset since the validation and test datasets must reflect the real distribution."}, {"heading": "6 RESULTS", "text": "For the experiments we first used three classification algorithms, decision tree (DT ), logistic regression (LR) and random forest (RF ). Using the implementation of Scikitlearn [39], each algorithm is trained using the different training sets: training (t), under-sampling (u), cost-proportionate rejection-sampling (r) [4] and cost-proportionate oversampling (o) [1]. Afterwards, we evaluate the results of the algorithms using BMR [33]. Then, the cost-sensitive logistic regression (CSLR) [9] and cost-sensitive decision tree (CSDT ) [10] were also evaluated. Lastly, we calculate the proposed ensembles of cost-sensitive decision trees algorithms. In particular, using each of the random inducer methods, bagging (CSB), pasting (CSP ), random forests (CSRF ) and random patches (CSRP ), and then blending the base classifiers using each one of the combination methods; majority voting (mv), cost-sensitive weighted voting (wv) and cost-sensitive stacking (s). Unless otherwise stated, the random selection of the training set was repeated 50 times, and in each time the models were trained and results collected, this allows us to measure the stability of the results.\nThe results are shown in TABLE 7. First, when observing the results of the cost-insensitive methods (CI), that is, DT , LR and RF algorithms trained on the t and u sets, the RF algorithm produces the best result by savings in three out of the five sets, followed by the LR\u2212 u. It is also clear that the results on the t dataset are not as good as the ones on the u, this is highly related to the unbalanced distribution of the positives and negatives in all the databases.\nIn the case of cost-proportionate sampling methods (CPS), specifically the cost-proportionate rejection sampling (r) and cost-proportionate over sampling (o). It is observed than in four cases the savings increases quite significantly. It is on the fraud detection database where these methods do not outperform the algorithms trained on the under-sampled set. This may be related to the fact that in this database the initial percentage of positives is 1.5% which is similar to the percentage in the r and o sets. However it is 50.42% in the u set, which may help explain why this method performs much better as measured by savings.\nAfterwards, in the case of the BMR algorithms, the results show that this method outperforms the previous ones in four cases and has almost the same result in the other set. In the fraud detection set, the results are quite better, since the savings of the three classification algorithms increase when using this methodology. The next family of algorithms is the cost-sensitive training, which includes the CSLR and CSDT techniques. In this case, only in two databases the results are improved. Lastly, we evaluate the proposed ECSDT algorithms. The results show that these methods arise to the best overall results in three sets, while being quite competitive in the others.\nSubsequently, in order to statistically sort the classifiers we computed the Friedman ranking (F-Rank) statistic [40]. This rank increases with the cost of the algorithms. We also calculate the average savings of each algorithm compared\n9\nwith the highest savings in each set (perBest), as a measure of how close are the savings of an algorithm to the best result. In TABLE 8, the results are shown. It is observed that the first six algorithms, according to the F-Rank, belong to the ECSDT family. In particular, the best three classifiers is the ensemble of cost-sensitive decision trees using the random patches approach. Giving the best result the one that blends the base classifiers using weighted voting method. Moreover as shown in TABLE 9, this method ranks on each\ndataset 1st, 2nd, 2nd, 5th and 3rd, respectively. For comparison the best method from an other family is the RF with BMR, which ranks 14th, 14th, 8th, 2nd and 9th.\n10\nMoreover, when analyzing the perBest statistic, it is observed that it follows almost the same order as the F-Rank. Notwithstanding, there are cases in which algorithms ranks are different in the two statistics, for example the CSDT \u2212 t algorithm has a lower F-Rank than the RF \u2212 BMR, but the perBest if better. This happens because, the F-Rank does not take into account the difference in savings within algo-\nrithms. This can be further investigated in Fig. 1. Even that ranks of the BMR models are better than the CSDT , the latter is on average closer to the best performance method in each set. Moreover, it is confirmed that the CSRP \u2212 wt is very close to the best result in all cases. Lastly, it is shown why the F-Rank of the LR \u2212 BMR is high, given the fact that is the best model in two databases. The reason for that, is because the performance on the other sets is very poor.\nFurthermore Fig. 2a, shows the Friedman ranking of each family of classifiers. The ECSDT methods are overall better, followed by the BMR and the CST methods. As expected, the CI family is the one that performs the worst. Nevertheless, there is a significant variance within the ranks in the ECSDT family, as the best one has a Friedman ranking of 2.6 and the worst 16. Similar results are found when observing the perBest shown in Fig. 2b. However, in the case of the perBest, the CST methods perform better\n11\nthan the BMR. It is important, in both cases it is confirmed that the ECSDT family of methods is the one that arise to the best results as measured by savings.\nWe further investigate the different methods that compose the ECSDT family, first by inducer methods and by the combination approach. In Fig. 3a, the Friedman ranking of the ECSDT methods grouped by inducer algorithm are shown. It is observed that the worst method is the random forest methodology. This may be related to the fact that within the random inducer methods, this is the only one that also modified the learner algorithm in the sense that it randomly select features for each step during the decision tree growing. Moreover, as expected the bagging and pasting methods perform quite similar, after all the only difference is that in bagging the sampling is done with replacement, while it is not the case in pasting. In general the best methodology is random patches. Additionally, in Fig. 3b, a similar analysis is made taking into account the combination of base classifiers approach. In this case, the best combination method is weighted voting, while majority voting and staking have a similar performance.\nFinally, in TABLE 10 the results of the algorithms measured by F1Score are shown. It is observed that the model with the highest savings is not the same as the one with the highest F1Score in all of the databases, corroborating the conclusions from [8], as selecting a method by a traditional statistic does not give the same result as selecting it using a business oriented measure such as financial savings. This can be further examined in Fig. 4, where the ranking of the F1Score and savings are compared. It is observed that the best two algorithms according to their Friedman rank of F1Score are indeed the best ones measured by the Fried-\nman rank of the savings. However, this relation does not consistently hold for the other algorithms as the correlation between the rankings is just 65.10%.\n12"}, {"heading": "7 CONCLUSIONS AND FUTURE WORK", "text": "In this paper we proposed a new framework of ensembles of example-dependent cost-sensitive decision-trees by creating cost-sensitive decision trees using four different random inducer methods and then blending them using three different combination approaches. The proposed method was tested using five databases, from four real-world applications: credit card fraud detection, churn modeling, credit scoring and direct marketing. We have shown theoretically and experimentally that our method ranks the best and outperforms state-of-the-art example-dependent cost-sensitive methodologies, when measured by financial savings.\nIn total, our framework is composed of 12 different algorithms, since the example-dependent cost-sensitive ensemble can be constructed by inducing the base classifiers using either bagging, pasting, random forest or random patches, and then blending them using majority voting, cost-sensitive weighted voting or cost-sensitive stacking. When analyzing the results within our proposed framework, it is observed that the inducer method that performs the best is random patches algorithm. Furthermore, the random patches algorithm is the one with the lowest complexity as each base classifier is learned on a smaller subset than with the other inducer methods. Nevertheless, there is no clear winner among the different combination methods. Since the most time consuming step is inducing and constructing the base classifiers, testing all combination methods does not add a significant additional complexity.\nOur results show the importance of using the real example-dependent financial costs associated with realworld applications. In particular, we found significant differences in the results when evaluating a model using a traditional cost-insensitive measure such as the accuracy or F1Score, than when using the savings. The final conclusion is that it is important to use the real practical financial costs of each context.\nTo improve the results, future research should be focused on developing an example-dependent cost-sensitive boosting approach. For some applications boosting methods have proved to outperform the bagging algorithms. Moreover, the methods covered in this work are all batch, in the sense that the batch algorithms keeps the system weights constant while calculating the evaluation measures. However in some applications such as fraud detection, the evolving patters due to change in the fraudsters behavior is not capture by using batch methods. Therefore, the need for investigate this problem from an online-learning perspective."}, {"heading": "ACKNOWLEDGMENTS", "text": "Funding for this research was provided by the Fonds National de la Recherche, Luxembourg."}], "references": [{"title": "The Foundations of Cost-Sensitive Learning", "author": ["C. Elkan"], "venue": "Seventeenth International Joint Conference on Artificial Intelligence, 2001, pp. 973\u2013978.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "The application of data mining techniques in financial fraud detection: A classification framework and an academic review of literature", "author": ["E. Ngai", "Y. Hu", "Y. Wong", "Y. Chen", "X. Sun"], "venue": "Decision Support Systems, vol. 50, no. 3, pp. 559\u2013569, Feb. 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A novel profit maximizing metric for measuring classification performance of customer churn prediction models", "author": ["T. Verbraken", "W. Verbeke", "B. Baesens"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 5, pp. 961\u2013973, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Cost-sensitive learning by cost-proportionate example weighting", "author": ["B. Zadrozny", "J. Langford", "N. Abe"], "venue": "Third IEEE International Conference on Data Mining. IEEE Comput. Soc, 2003, pp. 435\u2013442.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Development and application of consumer credit scoring models using profitbased classification measures", "author": ["T. Verbraken", "C. Bravo", "R. Weber", "B. Baesens"], "venue": "European Journal of Operational Research, vol. 238, no. 2, pp. 505\u2013513, Oct. 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "C4.5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling", "author": ["C. Drummond", "R. Holte"], "venue": "Workshop on Learning from Imbalanced Datasets II, ICML, Washington, DC, USA, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees", "author": ["O.M. Aodha", "G.J. Brostow"], "venue": "2013 IEEE International Conference on Computer Vision, Washington, DC, USA, Dec. 2013, pp. 193\u2013200.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Cost Sensitive Credit Card Fraud Detection Using Bayes Minimum Risk", "author": ["A. Correa Bahnsen", "A. Stojanovic", "D. Aouada", "B. Ottersten"], "venue": "2013 12th International Conference on Machine Learning and Applications. Miami, USA: IEEE, Dec. 2013, pp. 333\u2013338.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Example- Dependent Cost-Sensitive Logistic Regression for Credit Scoring", "author": ["A. Correa Bahnsen", "D. Aouada", "B. Ottersten"], "venue": "2014 13th International Conference on Machine Learning and Applications. Detroit, USA: IEEE, 2014, pp. 263\u2013269.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Example-Dependent Cost-Sensitive Decision Trees", "author": ["\u2014\u2014"], "venue": "Expert Systems with Applications, vol. inpress, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Ensembles on random patches", "author": ["G. Louppe", "P. Geurts"], "venue": "ECML PKDD\u201912 Proceedings of the 2012 European conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2012, pp. 346\u2013361.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artificial Intelligence Review, vol. 33, no. 1-2, pp. 1\u201339, Nov. 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Ensemble Methods Foundations and Algorithms", "author": ["Z.-H. Zhou"], "venue": "Boca Raton, FL, US: CRC Press,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 24, no. 2, pp. 123\u2013140, Aug. 1996.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Pasting small votes for classification in large databases and on-line", "author": ["\u2014\u2014"], "venue": "Machine Learning, vol. 103, pp. 85\u2013103, 1999.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Random Forests", "author": ["\u2014\u2014"], "venue": "Machine Learning, vol. 45, pp. 5\u201332, 2001.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Cost-Sensitive Boosting", "author": ["H. Masnadi-shirazi", "N. Vasconcelos"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 2, pp. 294\u2013309, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Bagging with Adaptive Costs", "author": ["W. Street"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 20, no. 5, pp. 577\u2013588, May 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Cost-Sensitive Tree-Stacking : Learning with Variable Prediction Error Costs", "author": ["T.A. Nesbitt"], "venue": "Ph.D. dissertation, University of California, Los Angeles, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey of cost-sensitive decision tree induction algorithms", "author": ["S. Lomax", "S. Vadera"], "venue": "ACM Computing Surveys, vol. 45, no. 2, pp. 1\u201335, Feb. 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd ed", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Transaction aggregation as a strategy for credit card fraud detection", "author": ["C. Whitrow", "D.J. Hand", "P. Juszczak", "D.J. Weston", "N.M. Adams"], "venue": "Data Mining and Knowledge Discovery, vol. 18, no. 1, pp. 30\u201355, Jul. 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Ensemble methods in machine learning", "author": ["T. Dietterich"], "venue": "Multiple classifier systems. Springer, 2000, pp. 1\u201315.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 12, no. October, pp. 993\u20131001, 1990.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Application of majority voting to pattern recognition: an analysis of its behavior and performance", "author": ["L. Lam", "S.Y. Suen"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part A Systems and Humans, vol. 27, no. 5, pp. 553\u2013568, 1997.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Goal-directed classification using linear machine decision trees", "author": ["B. Draper", "C. Brodley", "P. Utgoff"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 16, no. 9, pp. 888\u2013893, 1994.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "An instance-weighting method to induce cost-sensitive trees", "author": ["K. Ting"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 14, no. 3, pp. 659\u2013665, 2002.  13", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Decision trees with minimal costs", "author": ["C.X. Ling", "Q. Yang", "J. Wang", "S. Zhang"], "venue": "Twenty-first international conference on Machine learning - ICML \u201904, no. Icml. New York, New York, USA: ACM Press, 2004, p. 69.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Cost-Sensitive Classification with Genetic Programming", "author": ["J. Li", "X. Li", "X. Yao"], "venue": "2005 IEEE Congress on Evolutionary Computation, vol. 3. IEEE, 2005, pp. 2114\u20132121.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Evolutionary induction of costsensitive decision trees", "author": ["M. Kretowski", "M. Grze\u015b"], "venue": "Foundations of Intelligent Systems. Springer Berlin Heidelberg, 2006, pp. 121\u2013126.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "CSNL: A cost-sensitive non-linear decision tree algorithm", "author": ["S. Vadera"], "venue": "ACM Transactions on Knowledge Discovery from Data, vol. 4, no. 2, pp. 1\u201325, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural Networks, vol. 5, pp. 241\u2013259, 1992.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1992}, {"title": "Improving Credit Card Fraud Detection with Calibrated Probabilities", "author": ["A. Correa Bahnsen", "A. Stojanovic", "D. Aouada", "B. Ottersten"], "venue": "Proceedings of the fourteenth SIAM International Conference on Data Mining, Philadelphia, USA, 2014, pp. 677 \u2013 685.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Application of data mining techniques in customer relationship management: A literature review and classification", "author": ["E. Ngai", "L. Xiu", "D. Chau"], "venue": "Expert Systems with Applications, vol. 36, no. 2, pp. 2592\u20132602, Mar. 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "A novel costsensitive framework for customer churn predictive modeling", "author": ["A. Correa Bahnsen", "D. Aouada", "B. Ottersten"], "venue": "Decision Analytics, vol. inpress, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain-Driven Classification Based on Multiple Criteria and Multiple Constraint- Level Programming for Intelligent Credit Scoring", "author": ["J. He", "Y. Zhang", "Y. Shi", "S. Member", "G. Huang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 6, pp. 826\u2013838, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey of the issues in consumer credit modelling research", "author": ["L.C. Thomas", "R.M. Oliver", "D.J. Hand"], "venue": "Journal of the Operational Research Society, vol. 56, no. 9, pp. 1006\u20131015, Jul. 2005.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Using data mining for bank direct marketing: An application of the crisp-dm methodology", "author": ["S. Moro", "R. Laureano", "P. Cortez"], "venue": "European Simulation and Modelling Conference, Guimares, Portugal, 2011, pp. 117\u2013121.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical Comparisons of Classifiers over Multiple Data Sets", "author": ["J. Dem\u0161ar"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 1\u201330, 2006.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Typical costsensitive approaches assume a constant cost for each type of error, in the sense that, the cost depends on the class and is the same among examples [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "For example in credit card fraud detection, failing to detect a fraudulent transaction may have an economical impact from a few to thousands of Euros, depending on the particular transaction and card holder [2].", "startOffset": 207, "endOffset": 210}, {"referenceID": 2, "context": "In this context, failing to identify a profitable or unprofitable churner has a significant different economic result [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "Similarly, in direct marketing, wrongly predicting that a customer will not accept an offer when in fact he will, may have a different financial impact, as not all customers generate the same profit [4].", "startOffset": 199, "endOffset": 202}, {"referenceID": 4, "context": "Lastly, in credit scoring, accepting loans from bad customers does not have the same economical loss, since customers have different credit lines, therefore, different profit [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 3, "context": "Standard solutions consist in re-weighting the training examples based on their costs, either by cost-proportionate rejection-sampling [4], or cost-proportionate-sampling [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "Standard solutions consist in re-weighting the training examples based on their costs, either by cost-proportionate rejection-sampling [4], or cost-proportionate-sampling [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 5, "context": "However, costproportionate over-sampling increases the training set and it also may result in over-fitting [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "Moreover, the literature on example-dependent costsensitive methods is limited, often because there is a lack of publicly available datasets that fit the problem [7].", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "Recently, we have proposed different methods that take into account the different example-dependent costs, in particular: Bayes minimum risk (BMR) [8], cost-sensitive logistic regression [9], and cost-sensitive decision tree (CSDT ) [10].", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "Recently, we have proposed different methods that take into account the different example-dependent costs, in particular: Bayes minimum risk (BMR) [8], cost-sensitive logistic regression [9], and cost-sensitive decision tree (CSDT ) [10].", "startOffset": 187, "endOffset": 190}, {"referenceID": 9, "context": "Recently, we have proposed different methods that take into account the different example-dependent costs, in particular: Bayes minimum risk (BMR) [8], cost-sensitive logistic regression [9], and cost-sensitive decision tree (CSDT ) [10].", "startOffset": 233, "endOffset": 237}, {"referenceID": 10, "context": "However, the CSDT algorithm only creates one tree in order to make a classification, and as noted in [11], individual decision trees typically suffer from high variance.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "The main idea behind the ensemble methodology is to combine several individual base classifiers in order to have a classifier that outperforms each of them [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "Nowadays, ensemble methods are one of the most popular and well studied machine learning techniques [13], and it can be noted that since 2009 all the first-place and second-place winners of the KDD-Cup competition1 used ensemble methods.", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "In order to induce the random permutations and therefore create the different base classifiers, several methods have been proposed, in particular: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 14, "context": "In order to induce the random permutations and therefore create the different base classifiers, several methods have been proposed, in particular: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 169, "endOffset": 173}, {"referenceID": 15, "context": "In order to induce the random permutations and therefore create the different base classifiers, several methods have been proposed, in particular: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 190, "endOffset": 194}, {"referenceID": 10, "context": "In order to induce the random permutations and therefore create the different base classifiers, several methods have been proposed, in particular: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 214, "endOffset": 218}, {"referenceID": 12, "context": "Finally, after the base classifiers are trained, they are typically combined using either majority voting, weighted voting or stacking [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "In [17], the authors proposed a framework for costsensitive boosting that is expected to minimized the losses by using optimal cost-sensitive decision rules.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [18], a bagging algorithm with adaptive costs was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In his doctoral thesis, Nesbitt [19], proposed a method for costsensitive tree-stacking.", "startOffset": 32, "endOffset": 36}, {"referenceID": 19, "context": "Lastly in [20], a survey of application of cost-sensitive learning with decision trees is shown, in particular including other methods that create cost-sensitive ensembles.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "Traditionally, machine learning classification methods are designed to minimize some sort of misclassification measure such as the F1Score [21]; therefore, assuming that different misclassification errors have the same cost.", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "In this context, binary classification costs can be represented using a 2x2 cost matrix [1], that introduces the costs associated with two types of correct classification, true positives (CTPi ), true negatives (CTNi ), and the two types of misclassification errors, false positives (CFPi ), false negatives (CFNi ), as defined in TABLE 1.", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "These are referred to as the reasonableness conditions [1], and are defined as CFPi > CTNi and CFNi > CTPi .", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "Using the cost matrix, an example-dependent cost statistic [8], is defined as:", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "In [22], a normalized cost measure was proposed, by dividing the total cost by the theoretical maximum cost, which is the cost of misclassifying every example.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "We proposed a similar approach in [9], where the savings corresponding to using an algorithm are defined as the cost of the algorithm versus the cost of using no algorithm at all.", "startOffset": 34, "endOffset": 37}, {"referenceID": 11, "context": "The main idea behind the ensemble methodology is to combine several individual classifiers, referred to as base classifiers, in order to have a classifier that outperforms everyone of them [12].", "startOffset": 189, "endOffset": 193}, {"referenceID": 22, "context": "There are three main reasons regarding why ensemble methods perform better than single models: statistical, computational and representational [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 23, "context": "Moreover, if we assume that each one of the T base classifiers has a probability \u03c1 of being correct, the probability of an ensemble making the correct decision, denoted by Pc, can be calculated using the binomial distribution [24]", "startOffset": 226, "endOffset": 230}, {"referenceID": 24, "context": "Furthermore, as shown in [25], if T \u2265 3 then:", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "The framework is based on expanding our previous contribution on example-dependent cost-sensitive decision trees (CSDT ) [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "The latter being an extension of our previously proposed cost-sensitive logistic regression (CSLR) [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 19, "context": "Introducing the cost into the training of a decision tree has been a widely study way of making classifiers costsensitive [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 25, "context": "However, in most cases, approaches that have been proposed only deal with the problem when the cost depends on the class and not on the example [26]\u2013[31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 30, "context": "However, in most cases, approaches that have been proposed only deal with the problem when the cost depends on the class and not on the example [26]\u2013[31].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "In [10], we proposed an example-dependent cost-sensitive decision trees (CSDT ) algorithm, that takes into account the example-dependent costs during the training and pruning of a tree.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In particular we create the different subsets using four different methods: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "In particular we create the different subsets using four different methods: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "In particular we create the different subsets using four different methods: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "In particular we create the different subsets using four different methods: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "In bagging [14], base classifiers are built on randomly drawn bootstrap subsets of the original data, hence producing different base classifiers.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "Similarly, in pasting [15], the base classifiers are built on random samples without replacement from the training set.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "In random forests [16], using decision trees as the base learner, bagging is extended and combined with a randomization of the input features that are used when considering candidates to split internal nodes.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "In the random patches algorithm [11], base classifiers are created by randomly drawn bootstrap subsets of both examples and features.", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "First, in the traditional approach, a similar comparison of the votes of the base classifiers is made, but giving a weight \u03b1j to each classifier Mj during the voting phase [13]", "startOffset": 172, "endOffset": 176}, {"referenceID": 31, "context": "The staking method consists in combining the different base classifiers by learning a second level algorithm on top of them [32].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "Even though there is no restriction on which algorithm can be used as a second level learner, it is common to use a linear model [13], such as fs(S,M, \u03b2) = g \uf8eb\uf8ed T \u2211", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": "Moreover, following the logic used in [19], we propose learning the set of parameters \u03b2 using our proposed costsensitive logistic regression (CSLR) [9].", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "Moreover, following the logic used in [19], we propose learning the set of parameters \u03b2 using our proposed costsensitive logistic regression (CSLR) [9].", "startOffset": 148, "endOffset": 151}, {"referenceID": 8, "context": "However, as discussed in [9], this cost function is not convex for all possible cost matrices, therefore, we use genetic algorithms to minimize it.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Different detection systems that are based on machine learning techniques have been successfully used for this problem, for a review see [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "We used the fraud detection example-dependent cost matrix we proposed in [8], in which the cost of failing to detect a fraud is equal to the amount of the transaction (Amti), and the costs of correct classification and false positives is equal to the administrative cost of investigating a fraud alert (Ca).", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "For a further discussion see [8], [33].", "startOffset": 29, "endOffset": 32}, {"referenceID": 32, "context": "For a further discussion see [8], [33].", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "TABLE 2 Credit card fraud detection cost matrix [8]", "startOffset": 48, "endOffset": 51}, {"referenceID": 34, "context": "TABLE 3 Churn modeling cost matrix [35]", "startOffset": 35, "endOffset": 39}, {"referenceID": 33, "context": "For a review see [34].", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "Nevertheless, current state-of-the-art classification algorithms are not well aligned with commercial goals, in the sense that, the models miss to include the real financial costs and benefits during the training and evaluation phases [3].", "startOffset": 235, "endOffset": 238}, {"referenceID": 34, "context": "We then follow the example-dependent cost-sensitive methodology for churn modeling we proposed in [35].", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "The objective in credit scoring is to classify which potential customers are likely to default a contracted financial obligation based on the customer\u2019s past financial experience, and with that information decide whether to approve or decline a loan [36].", "startOffset": 250, "endOffset": 254}, {"referenceID": 36, "context": "When constructing credit scores, it is a common practice to use standard cost-insensitive binary classification algorithms such as logistic regression, neural networks, discriminant analysis, genetic programing, decision tree, among others [37].", "startOffset": 240, "endOffset": 244}, {"referenceID": 8, "context": "In this paper, we used the credit scoring exampledependent cost-sensitive cost matrix we proposed in [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "First, the costs of a TABLE 4 Credit scoring cost matrix [9]", "startOffset": 57, "endOffset": 60}, {"referenceID": 33, "context": "In direct marketing the objective is to classify those customers who are more likely to have a certain response to a marketing campaign [34].", "startOffset": 136, "endOffset": 140}, {"referenceID": 37, "context": "We used a direct marketing dataset from [38].", "startOffset": 40, "endOffset": 44}, {"referenceID": 32, "context": "We used the direct marketing example-dependent cost matrix we proposed in [33].", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "TABLE 5 Direct marketing cost matrix [33]", "startOffset": 37, "endOffset": 41}, {"referenceID": 20, "context": "Afterwards, because classification algorithms suffer when the label distribution is skewed towards one of the classes [21], an under-sampling of the positive examples is made, in order to have a balanced class distribution.", "startOffset": 118, "endOffset": 122}, {"referenceID": 38, "context": "Using the implementation of Scikitlearn [39], each algorithm is trained using the different training sets: training (t), under-sampling (u), cost-proportionate rejection-sampling (r) [4] and cost-proportionate oversampling (o) [1].", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "Using the implementation of Scikitlearn [39], each algorithm is trained using the different training sets: training (t), under-sampling (u), cost-proportionate rejection-sampling (r) [4] and cost-proportionate oversampling (o) [1].", "startOffset": 183, "endOffset": 186}, {"referenceID": 0, "context": "Using the implementation of Scikitlearn [39], each algorithm is trained using the different training sets: training (t), under-sampling (u), cost-proportionate rejection-sampling (r) [4] and cost-proportionate oversampling (o) [1].", "startOffset": 227, "endOffset": 230}, {"referenceID": 32, "context": "Afterwards, we evaluate the results of the algorithms using BMR [33].", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Then, the cost-sensitive logistic regression (CSLR) [9] and cost-sensitive decision tree (CSDT ) [10] were also evaluated.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "Then, the cost-sensitive logistic regression (CSLR) [9] and cost-sensitive decision tree (CSDT ) [10] were also evaluated.", "startOffset": 97, "endOffset": 101}, {"referenceID": 39, "context": "Subsequently, in order to statistically sort the classifiers we computed the Friedman ranking (F-Rank) statistic [40].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "It is observed that the model with the highest savings is not the same as the one with the highest F1Score in all of the databases, corroborating the conclusions from [8], as selecting a method by a traditional statistic does not give the same result as selecting it using a business oriented measure such as financial savings.", "startOffset": 167, "endOffset": 170}], "year": 2015, "abstractText": "Several real-world classification problems are example-dependent cost-sensitive in nature, where the costs due to misclassification vary between examples and not only within classes. However, standard classification methods do not take these costs into account, and assume a constant cost of misclassification errors. In previous works, some methods that take into account the financial costs into the training of different algorithms have been proposed, with the example-dependent cost-sensitive decision tree algorithm being the one that gives the highest savings. In this paper we propose a new framework of ensembles of example-dependent cost-sensitive decision-trees. The framework consists in creating different example-dependent cost-sensitive decision trees on random subsamples of the training set, and then combining them using three different combination approaches. Moreover, we propose two new cost-sensitive combination approaches; cost-sensitive weighted voting and cost-sensitive stacking, the latter being based on the cost-sensitive logistic regression method. Finally, using five different databases, from four real-world applications: credit card fraud detection, churn modeling, credit scoring and direct marketing, we evaluate the proposed method against state-of-the-art example-dependent cost-sensitive techniques, namely, cost-proportionate sampling, Bayes minimum risk and cost-sensitive decision trees. The results show that the proposed algorithms have better results for all databases, in the sense of higher savings.", "creator": "LaTeX with hyperref package"}}}