{"id": "1505.05561", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "Why Regularized Auto-Encoders learn Sparse Representation?", "abstract": "Although set five addition auto - encoder models rule bulkiness explicitly well already learned representation he alone joel ' t, there taken been little approval analysis following doubt benefit kookiness during these feature however colonel. Therefore, our objective here means the chosen research what interim trouble for concomitantly auto - actuators. We scenes be are electron-electron having modulating proper play an important also in providing over-use. We build consideration pressure on make be evaluation addition wonder any example variety models - - have De - noising took Contractive auto - lzma - - has counterpropaganda - - like Rectified Linear and Sigmoid - - respect except failure; thus correct establishement in their learned discrete. Our theoretical bringing empirical analysis only, touchdown light on of acquire from maldacena / activation that represent advantageous rest impressiveness. As a whose - produce of the insight respectively he putting analysis, might made propose a recently conduction define how typecasting the individual drawbacks much subsequent existing deregistration (in terms means flimsiness) where consequently produces full at bogey (or better) with created successful theater activation for supposed sector - encoder models leaders.", "histories": [["v1", "Thu, 21 May 2015 00:10:46 GMT  (966kb,D)", "http://arxiv.org/abs/1505.05561v1", null], ["v2", "Fri, 29 May 2015 19:22:37 GMT  (967kb,D)", "http://arxiv.org/abs/1505.05561v2", "10 pages of content, 1 page of reference, 3 pages of supplementary. Minor changes to theorem 1 and more empirical results added"], ["v3", "Wed, 2 Mar 2016 15:29:29 GMT  (379kb,D)", "http://arxiv.org/abs/1505.05561v3", "8 pages of content, 1 page of reference, 4 pages of supplementary"], ["v4", "Mon, 23 May 2016 23:04:21 GMT  (1246kb,D)", "http://arxiv.org/abs/1505.05561v4", "8 pages of content, 1 page of reference, 4 pages of supplementary. ICML 2016"], ["v5", "Fri, 17 Jun 2016 23:01:20 GMT  (1835kb,D)", "http://arxiv.org/abs/1505.05561v5", "8 pages of content, 1 page of reference, 4 pages of supplementary. ICML 2016; bug fix in lemma 1"]], "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["devansh arpit", "yingbo zhou", "hung q ngo 0001", "venu govindaraju"], "accepted": true, "id": "1505.05561"}, "pdf": {"name": "1505.05561.pdf", "metadata": {"source": "CRF", "title": "Why Regularized Auto-Encoders learn Sparse Representation?", "authors": ["Devansh Arpit", "Yingbo Zhou", "Hung Ngo", "Venu Govindaraju"], "emails": ["devansha@buffalo.edu", "yingbozh@buffalo.edu", "hungngo@buffalo.edu", "govind@buffalo.edu"], "sections": [{"heading": "1 Introduction", "text": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5]. A key commonality between these models is finding distributed representation [13, 11] for observed data although the former focuses on finding sparse distributed representation while the latter focuses on learning complex functions. When combined together, some of the main advantages of sparse distributed representation in the context of deep neural networks [1] has been shown to be information disentangling and manifold flattening [2], and better linear separability and representational power [7].\nFor these reasons, our objective in this paper is to investigate why a number of regularized AutoEncoders (AE) learn sparse representation. AEs are specially interesting for this matter because of the clear distinction between their learned encoder representation and decoder output. This is in contrast with other deep models where there is no clear distinction between the encoder and decoder parts. The idea of AEs learning SR is not new. Due to the aforementioned biological connection between SR and NNs, a natural follow-up pursued by a number of researchers was to propose AE variants that encouraged sparsity in their learned representation [18, 14, 24, 21]. On the other hand, there has also been work on empirically analyzing/suggesting the sparseness of hidden representations learned after pre-training with unsupervised models [22, 19, 23]. However, there has been no prior work on formally analyzing why regularized auto-encoders learn sparse representation in general, to the best of our knowledge. The main challenge behind doing so is the analysis of nonlinear, and even worse, non-convex objective functions. In addition, questions regarding the efficacy of activation functions and the choice of regularization on AE objective are often raised since there\nar X\niv :1\n50 5.\n05 56\n1v 1\n[ st\nat .M\nL ]\n2 1\nM ay\nare multiple available choices for both. We also try to address these questions in regards with SR in this paper.\nWe address these problems in two parts. We first provide sufficient conditions on AE regularizations that encourage low pre-activations in hidden units. We then analyze the properties of activation functions that coupled with such regularizations result in sparse representation and find that multiple popular activations have these desirable properties. Finally the second part shows multiple popular AE objectives including De-noising auto-encoder (DAE) [31] and Contractive auto-encoder (CAE) [28] indeed have the suggested form of regularization; thus explaining why existing AEs encourage sparsity in their latent representation. Based on our theoretical analysis, we empirically study multiple popular AE models and activation functions in order to analyze their behaviour in terms of sparsity in the learned representations. Besides confirming our analysis, these experiments also serve to illustrate a comparative performance of these AE models and activation functions. Thus our analysis provides novel tools that can be used for both, comparing existing AEs under the same hood, as well as predicting the behavior of (new) regularizations and activation functions (in terms of sparsity) leading to a deeper understanding of AEs."}, {"heading": "2 Auto-Encoders and Sparse Representation", "text": "Auto-Encoders (AE) [29, 3] are a class of single hidden layer neural networks that minimize the data reconstruction error while learning an intermediate mapping between the input and output space. An auto-encoder consists of two parts \u2013 an encoder and a decoder. An input (x \u2208 Rn) is first mapped to the latent space by an encoder function fe : Rn \u2192 Rm given by h := fe(x) = se(Wx + be) where h is the hidden representation vector, se is the encoder activation function, W \u2208 Rm\u00d7n is the weight matrix, and be is the encoder bias. Then we map the hidden output back to the original space by a decoder function fd : Rm \u2192 Rn given by y = fd(h) = sd(WTh + bd) where y is the reconstructed counterpart of x, sd is the decoder activation function, bd is the decoder bias. The objective of a basic auto-encoder is to minimize the following with respect to {W,be,bd}\nJAE = Ex[`(x, fd(fe(x)))] (1) where `(\u00b7) is a loss function. The motivation behind this objective is to capture predominant repeating patterns in data. Thus even though the auto-encoder optimization learns to map an input back to itself, the focus is on learning a noise invariant representation (manifold) of data.\n2.1 Part I: What encourages sparsity during Auto-Encoder training?\nLearning a dictionary adapted to a set of training data such that the latent code is sparse is generally formulated as the following optimization problem [25]\nmin W,h N\u2211 i=1 \u2016xi \u2212WThi\u20162 + \u03bb\u2016hi\u20161 (2)\nThe above optimization is convex in each one of W and h when the other is fixed and hence this objective is generally solved alternately in each variable while fixing the other. Note that `1 penalty is the driving force in the above objective and forces the latent variable to be sparse. In this section we will analyze the factors that encourage sparsity in the case of AEs. For our analysis, we will use linear decoding function which addresses the more general case of continuous real valued data distributions. We will now show that both regularization as well as activation function play an important role for achieving sparsity. Proposition 1. Let {Wt \u2208 Rm\u00d7n,bte \u2208 Rm} be the parameters of a regularized auto-encoder (\u03bb > 0)\nJRAE = JAE + \u03bbR(W,be) (3)\nat training iteration t with regularization term R(W,be), activation function se(.) and define preactivation atj = W t jx+b t ej (thus h t j = se(a t j)). If \u2202R \u2202bej\n> 0, where j \u2208 {1, 2, . . . ,m}, and Ex[x] = 0, then updating {Wt,bte} along the negative gradient of \u03bbR, results in Ex[at+1j ] < Ex[atj ] and var[at+1j ] \u2264 \u03bb2x\u2016W t+1 j \u20162 for all t \u2265 0. Here \u03bb2x denotes the maximum eigenvalues of Ex[xxT ].\nThe above proposition gives a general sufficient condition on regularizations for forcing the average pre-activation value (E[(atj)]) to go on reducing every training iteration. Of course in practice, gradients from both regularization and loss function are used. Hence in the practical version, the interpretation of the above proposition is that as long as the gradient of the objective with respect to be is dominated by the regularization term\u2013 thus \u2202JRAE/\u2202bej \u2265 0 (strong bias gradient), the expected value of pre-activation (atj) over the data distribution goes on reducing for every hidden unit (htj) with iteration t. As we will see in a bit, this is a sparsity inducing property. For the sake of concreteness, we will now analyze the bias gradient resulting from AE squared loss function. This gradient is given by:\n\u2202`/\u2202bej = 2Ex [ (bd + W T se(Wx + be)\u2212 x)TWj .\u2202se(aj)/\u2202aj ]\n(4) Note the absolute value of first derivative of most activation functions is upper bounded (usually at most 1). Also, the optimal value of bd = Ex[x \u2212WTh] at every iteration. Suppose we assume the weight lengths are upper bounded and we only consider monotonically increasing se(\u00b7) with finite negative saturation. Then coupled with the natural implicit assumption on bounded length data distribution, there will exist a set of \u03bb values for which the bias gradient fromR outweighs that from JAE for any finite valued initialization of be. For all following iterations the bias value is always non-increasing and hence the value of se will be bounded because of being monotonically increasing with finite negative saturation. Thus the value of \u2202JAE/\u2202bej is upper bounded independent of the regularization coefficient \u03bb as long as its value is chosen from the aforementioned set. This shows that the dominance of \u2202R/\u2202bej on \u2202JRAE/\u2202bej can be controlled by the choice of \u03bb. Intuitively higher values of \u03bb should lead to lower average pre-activation; however, the last argument may not strictly be true in practice due to non-linearities in the regularization (see experiments in section 4.2 and 4.3). Finally, the upper bound on weight vectors\u2019 length can easily be guaranteed1 using Maxnorm Regularization or Weight Decay which are widely used tricks while training deep networks [10]. In the prior case every weight vector is simply constrained to lie within an `2 ball (\u2016Wj\u20162 \u2264 c \u2200j \u2208 {1, 2, . . . ,m}, where c is a fixed constant) after every gradient update. Corollary 1. If se is a monotonically increasing activation function and Ex[x] = 0, then updating {Wt,bte} along the negative gradient of R = \u2211m j=1 f(Ex[hj ]), results in Ex[a t+1 j ] \u2264 Ex[atj ] and var[at+1j ] \u2264 \u03bb2x\u2016W t+1 j \u20162 for all t \u2265 0. Here \u03bb2x denotes the maximum eigenvalues of Ex[xxT ] and f(.) is any monotonically increasing function. Corollary 2. If se(.) is a monotonically increasing convex activation function and Ex[x] = 0, then updating {Wt,bte} along the negative gradient of R = Ex [\u2211m j=1 (( \u2202hj \u2202aj )q \u2016Wtj\u2016 p 2 )] , q \u2208 N , p \u2208W, results in Ex[at+1j ] \u2264 Ex[atj ] and var[a t+1 j ] \u2264 \u03bb2x\u2016W t+1 j \u20162 for all t \u2265 0. Here \u03bb2x denotes the maximum eigenvalues of Ex[xxT ].\nThe above corollaries show that specific regularizations encourage the pre-activation of hidden units in auto-encoders towards zero values on average across the data distribution, with assumptions made only on activation function and distribution mean. We will show in section 2.2 that multiple existing AEs have regularizations of the form above.\n2.1.1 Which Activation functions are good for Sparse Representation?\nThe above analysis in general suggests that monotonically increasing convex activation functions encourage lower expected pre-activation for both corollaries. Also note that a reduction in the expected pre-activation value (E[(atj)]) does not necessarily imply a reduction in the hidden unit value (htj) and thus sparsity. However, these regularizations become immediately useful if we consider activation functions with negative saturation at 0, i.e., lima\u2192\u2212\u221e se(a) = 0. Now a lower average pre-activation value directly implies higher sparsity! Under this light, we analyze the properties of monotonically increasing convex activation functions as suggested above.\nBefore proceeding, we would like to mention that although the general notion of sparsity in AEs entails majority of units are de-activated, i.e., their value is less than a certain threshold (\u03b4min), in practice, a representation that is truly sparse (large number of hard zeros) usually yields better performance [7, 32, 33]. We will keep this in mind during our discussion.\n1In practice the weight lengths remain bounded from the regularized objective itself and so it is not imperative to apply this trick.\nTheorem 1. (Desirable properties of Activation function) When proposition 1 applies and a monotonically increasing convex function se(.) with lima\u2192\u2212\u221e se(a) = 0 is used, then,\n1. Both \u2202hj\u2202aj , \u22022hj \u2202a2j \u2265 0 for all values of the argument of se(.) due to monotonically increasing\nand convexity property of se(.) and hence both corollary 2 and 1 apply to se(.).\n2. ptj := 1 \u2212 var[atj ]\n(s\u22121e (\u03b4min)\u2212E[atj ])2 lower bounds Pr(htj \u2264 \u03b4min) over data distribution D at\niteration t.\n3. If \u2016Wtj\u20162 is upper bounded independent of \u03bb values then \u2203S \u2286 R+ and \u2203Tmin \u2208 N such that pt+1j \u2265 ptj \u2200t \u2265 Tmin, \u2200\u03bb \u2208 S.\nThe above theorem shows a general property of any monotonically increasing convex activation function se(.). While part 1 implies that the average pre-activation monotonically decreases with every iteration for all such activation functions for the suggested regularizations, part 2 shows that this decrease in average pre-activation directly implies a higher probability of de-activated hidden units. These results coupled with the fact that lima\u2192\u2212\u221e se(a) = 0 imply that the average sparsity of hidden units keeps increasing after a sufficient number of iterations. On the other hand, the condition \u2016Wtj\u20162 be upper bounded is a straightforward result of applying common tricks like Maxnorm Regularization or Weight Decay and hence does not undermine our result.\nThe most popular choices of activation function se(.) are ReLU, Maxout [8], Sigmoid, Tanh and Softplus. Since Maxout and Tanh don\u2019t satisfy the negative saturation property, they are not applicable to our framework.\nReLU: It is a monotonically increasing convex function; thus both corollary 1 and 2 apply. Note that ReLU does not have a second derivative2. Thus, in practice, this may lead to poor sparsity for the regularization in corollary 2 due to lack of bias gradients from the regularization, i.e. \u2202R/\u2202bej = 0. On the flip side, the advantage of ReLU is that it enforces hard zeros in the representations learned by AEs.\nSoftplus: It is also a monotonically increasing convex function and hence in general encourages sparsity for the suggested AE regularizations. In contrast to ReLU, Softplus has positive bias gradients (hence better sparsity for corollary 2) because of its smoothness. On the other hand, note that Softplus does not produce hard zeros due to asymptotic left saturation at 0.\nSigmoid: While Corollary 1 applies unconditionally to Sigmoid, corollary 2 doesn\u2019t apply in general. However, we empirically found that Sigmoid activation learns hidden unit values such that majority of them are either close to 0 or lie in the linear region (see experiments in section 4.1); thus a very small fraction concentrates in the positive saturation region (+1). In other words, AEs only use the convex part of Sigmoid function, which empirically supports Ex[\u22022hj/\u2202a2j ] \u2265 0 for practical purposes. This suggests Sigmoid should encourage sparsity for regularization even in corollary 2. On the other hand, in general, the non-convexity of Sigmoid makes it difficulty to apply the 2nd part of the above theorem.\nIn conclusion, while Maxout and Tanh do not satisfy the negative saturation property at 0 and hence do not guarantee sparsity, all others\u2013 ReLU, Softplus and Sigmoid\u2013 have properties (at least in principal) that encourage sparsity in learned representations for the suggested regularizations."}, {"heading": "2.2 Part II: Do existing Auto-Encoders learn Sparse Representation?", "text": "At this point, a natural question to ask is whether existing AEs learn Sparse Representation. To complete the loop, we show that most of the popular AE objectives have regularization term similar to what we have proposed in corollary 1 and 2 and thus they indeed learn sparse representation.\n2In other words, \u2202 2hj \u2202a2j = \u03b4(Wjx+ bej ), where \u03b4(.) is the Dirac delta function. Although strictly speaking,\n\u22022hj \u2202a2j is always non-negative, this value is zero everywhere except when the argument is exactly 0, in which case it is +\u221e"}, {"heading": "2.2.1 De-noising Auto-Encoder (DAE)", "text": "DAE [31] aims at minimizing the reconstruction error between every sample x and the reconstructed vector using its corresponding corrupted version x\u0303. The corrupted version x\u0303 is sampled from a conditional distribution p(x\u0303i|xi). The usual choices for this distribution are additive Gaussian and independent Bernoulli (multiplicative masking). The original DAE objective is given by\nJDAE = Ex [ Ep(x\u0303|x)[`(x, fd(fe(x\u0303)))] ] (5)\nwhere p(x\u0303i|x) denotes the conditional distribution of x\u0303 given x. Since the above objective is analytically intractable due to the corruption process, we take a second order Taylor\u2019s approximation of the DAE objective3 around the distribution mean \u00b5x = Ep(x\u0303|x)[x\u0303] in order to overcome this difficulty,\nTheorem 2. Let {W,be} represent the parameters of a DAE with squared loss, linear decoding, and i.i.d. Gaussian corruption with zero mean and \u03c32 variance, at any point of training over data sampled from distribution D. Let aj := Wjx + bej so that hj = se(aj) corresponding to sample x \u223c D. Then,\nJDAE = JAE + \u03c32Ex  m\u2211 j=1 (( \u2202hj \u2202aj )2 \u2016Wj\u201642 ) + m\u2211 j,k=1 j 6=k ( \u2202hj \u2202aj \u2202hk \u2202ak (WTj Wk) 2 ) +\nn\u2211 i=1 ( (bd + W Th\u2212 x)TWT ( \u22022h \u2202a2 Wi Wi )) + o(\u03c32) (6)\nwhere \u2202 2h \u2202a2 \u2208 R m is the element-wise 2nd derivative of h w.r.t. a and is element-wise product.\nThe first term of the above regularization is of the form stated in corollary 2. Even though the second term doesn\u2019t have the exact suggested form, it is straight forward to see that this term generates nonnegative bias gradients for monotonically increasing convex activation functions. Note the last term depends on the reconstruction error which practically becomes small after a few epochs of training and the other two regularization terms take over. Besides, this term is usually ignored as it is not positive-definite. This suggests that DAE is capable of learning sparse representation."}, {"heading": "2.2.2 Contractive Auto-Encoder (CAE)", "text": "CAE [28] objective is given by JCAE = JAE + \u03bbEx [ \u2016J(x)\u20162F ] (7)\nwhere J(x) = \u2202h\u2202x denotes the Jacobian matrix and the objective aims at minimizing the sensitivity of the hidden representation to slight changes in input.\nRemark 1. Let {W \u2208 Rm\u00d7n,be \u2208 Rm} represent the parameters of a Contractive Auto-Encoder (CAE) with se(.) activation function, squared loss or cross-entropy loss and regularization coefficient \u03bb, at any point of training over data sampled from some distribution D. Let aj := Wjx + bej so that hj = se(aj) corresponding to sample x \u223c D. Then,\nJCAE = JAE + \u03bbEx  m\u2211 j=1 (( \u2202hj \u2202aj )2 \u2016Wj\u201622 ) (8) Thus CAE regularization also has a form identical to the form suggested in corollary 2. Thus the hidden representation learned by CAE should also be sparse. In addition, since the first order regularization term in Higher order CAE (CAE+H) [27] is same as CAE, this suggests that CAE+H objective should have similar properties in term of sparsity.\n3As a note, even though we derive this equivalent form of DAE, we use explicit corruption for experiments."}, {"heading": "2.2.3 Marginalized De-noising Auto-Encoder (mDAE)", "text": "mDAE [4] objective is given by:\nJmDAE = JAE + 1\n2 Ex  n\u2211 i=1 \u03c32xi m\u2211 j=1 \u22022` \u2202hj 2 ( \u2202hj \u2202x\u0303i )2 (9) where \u03c32xi denotes the corruption variance intended for the i\nth input dimension. The authors of mDAE proposed this algorithm with the primary goal of speeding up the training of DAE by deriving an approximate form that omits the need to iterate over a large number of explicitly corrupted instances of every training sample.\nRemark 2. Let {W \u2208 Rm\u00d7n,be \u2208 Rm} represent the parameters of a Marginalized De-noising Auto-Encoder (mDAE) with se(.) activation function, linear decoding, squared loss and \u03c32xi = \u03bb \u2200i \u2208 {1, . . . , n}, at any point of training over data sampled from some distribution D. Let aj := Wjx + bej so that hj = se(aj) corresponding to sample x \u223c D. Then,\nJmDAE = JAE + \u03bbEx  m\u2211 j=1 (( \u2202hj \u2202aj )2 \u2016Wj\u201642 ) (10) Apart from justifying sparsity in the above AEs, these equivalences also expose the similarity between DAE, CAE and mDAE regularization as they all follow the form in corollary 2. Note how the goal of achieving invariance in hidden and original representation respectively in CAE and mDAE show up as a mere factor of weight length in their regularization in the case of linear decoding."}, {"heading": "2.2.4 Sparse Auto-Encoder (SAE)", "text": "Sparse AEs are given by:\nJSAE = JAE + \u03bb m\u2211 j=1 (\u03c1 log(\u03c1/\u03c1j) + (1\u2212 \u03c1) log((1\u2212 \u03c1)/(1\u2212 \u03c1j))) (11)\nwhere \u03c1j = Ex[hj ] and \u03c1 is the desired average activation (typically close to 0). Thus SAE requires one additional parameter (\u03c1) that needs to be pre-determined. To make SAE follow our paradigm, we set \u03c1 = 0 and thus tuning the value of \u03bb would automatically enforce a balance between the final level of average sparsity and reconstruction error. Thus the SAE objective becomes\nJSAE = JAE \u2212 \u03bb m\u2211 j=1 log(1\u2212 \u03c1j) (when \u03c1 = 0) (12)\nNote for small values of \u03c1j , log(1 \u2212 \u03c1j) \u2248 \u2212\u03c1j . Thus the above objective has a very close resemblance with sparse coding (equation 2, except that SC has a non-parametric encoder). On the other hand, the above regularization has a form as specified in corollary 1 which we have showed enforces sparsity. Thus, although it is expected of the SAE regularization to enforce sparsity from an intuitive standpoint, our results show that it indeed does so from a more theoretical perspective. Notice only Sigmoid is applicable to SAE in general due to the log function. So we clip Ex[hj ] at 1 during AE training process for applying other activations as well."}, {"heading": "3 Improving Bias Gradient of ReLU with Rectified Softplus (ReS)", "text": "True sparsity (large number of hard zeros) is generally desired in feature representation for purposes like robustness, separability, compactness and performance. As discussed in section 2.1.1, while Maxout and Tanh do not satisfy the negative saturation property (and hence do not guarantee sparsity), ReLU, Softplus and Sigmoid satisfy the required properties (theorem 1) in general and thus encourage sparsity. However, the latter three activations each have some individual drawbacks. While ReLU does not have bias gradient which may lead to poor sparsity for regularizations in corollary 2 (eg. DAE, CAE, mDAE), Softplus and Sigmoid do not produce hard zeros.\n0 2 4 6 8 10\nIteration\n0\n20\n40\n60\n80\nBi as\nG ra\ndi en\nt M ea\nn\nReS (MNIST) ReLu (MNIST) ReS (CIFAR-10) ReLu (CIFAR-10)\nBias Gradient Mean/Std for CAE/mDAE\n0 2 4 6 8 10 Iteration 0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBi as\nM ea\nn\nReS (MNIST) ReLu (MNIST) ReS (CIFAR-10) ReLu (CIFAR-10)\nBias Mean/Std for CAE/mDAE\nFigure 2: Bias Gradient mean (left) and Bias mean (right) for CAE/mDAE. ReLU lacks bias gradient, thus larger bias values. This results in poor sparsity for ReLU compared to ReS.\nAs a by-product of the insights gained from our analysis, we propose an activation function Rectified Softplus (ReS), that overcomes the aforementioned drawbacks in individual activations (leading to better sparsity). We define ReS as se(aj) = max(0, log( 1+exp aj\n2 )). Note that ReS is a monotonically increasing convex function;thus theorem 1 applies.\nApart from the advantage of ReS over ReLU, Softplus and Sigmoid apparent from discussions above, one additional advantage of ReS (shared with ReLU and Softplus) over Sigmoid is its large linear range which helps in overcoming the problem of diminished weight gradients as against Sigmoid (due to saturation at 1), leading to faster training"}, {"heading": "4 Empirical Analysis and Observations", "text": "We use the following two datasets for our experiments: 1. MNIST [17]: It is a 10 class dataset of handwritten digit images (binary valued) of which 50, 000 images are provided as train, 10, 000 validation and 10, 000 as test set. 2. CIFAR-10 [16]: It consists of 60000 32\u00d7 32 real world (continuous valued) color images in 10 classes, with 6000 images per class. There are 50, 000 train images and 10, 000 test images. For CIFAR-10, we randomly crop 50, 000 patches of size 8\u00d78 (since CIFAR-10 has real world images) for training the auto-encoders.\nExperimental Protocols: For all experiments, we use RMS Prop [30] for objective optimization, learning rate 0.001, pre-training epochs 100, batch size 50 and hidden units 1000 (for MNIST) and 500 (for CIFAR-10) unless specified otherwise. We train DAE, CAE, mDAE and SAE (using eq. 12) with the same aforementioned hyper-parameters for all the experiments. For regularization coefficient (\u03c32), we use the values in the set {0.001, 0.12, 0.22, 0.32, 0.42, 0.52, 0.62, 0.72, 0.82, 0.92, 1.0} for all models except DAE where \u03c32 values represent the variance of Gaussian noise added. For all models and activation functions, we use squared loss and linear decoding. We initialize the bias to zeros and use normalized initialization [6] for weight vectors. Further, we subtract the mean from all the training samples."}, {"heading": "4.1 Behaviour of Sigmoid Activation", "text": "We say that a Sigmoid unit has fired if hj \u2265 0.1 and we say a unit has saturated if hj \u2265 0.9. Thus a unit is in the linear region if 0.1 < hj \u2264 0.9. We perform two types of experiments both of which confirm our argument in section 2.1.1 that AEs only make use of the linear range of Sigmoid activation and hence Ex [ \u22022hj/\u2202a 2 j ] \u2265 0 for practical purposes which explains why Sigmoid learns sparse representation for DAE, CAE and mDAE (recall that corollary 1 always applies to Sigmoid; justifying sparsity for SAE): 1. Ex [ \u22022hj/\u2202a 2 j ] vs. iterations: In order for corollary 2 to apply to Sigmoid, Ex [ \u22022hj/\u2202a 2 j ] =\nEx [ h2j (1\u2212 hj)2(1\u2212 2hj) ] should be non-negative, which would lead sparsity to increase every it-\neration for the jth unit. So we record the fraction of units for which Ex [ \u22022hj/\u2202a 2 j ] is non-negative vs. training iterations for all the AE models. We found that this value is always close to 1 throughout the training iterations. 2. Percentage of active units in Linear Range: This is the ratio of the number of units in linear range to the number of units fired on percentage scale. The result as seen in figure 1 suggests more\nthan 92% of activated units are in linear range for majority cases. As a result of the above analysis, it is expected of Sigmoid activation to enforce low average activation fraction in the learned representation for all the four AE models. This is empirically verified in figures 3 and 4.\n4.2 Sparsity vs. Regularization coefficient / Comparison of Activation functions\nWe analyze the effect of increasing regularization coefficient (\u03c32) on the sparsity of representations learned by all the four AE models studied above on both the datasets using all activations4. As suggested by proposition 1, higher values of regularization coefficient should intuitively lead to lower average pre-activation, and based on our further analysis, it should ultimately lead to sparser rep-\n4We don\u2019t plot Softplus because its trend is very similar to Sigmoid. An explanation for this is that AEs use the convex region of Sigmoid, so both activations become similar in terms of their representational power.\nresentation. However, recall that the regularization in case of DAE, CAE and mDAE has the form \u03c32Ex [\u2211m j=1 ( (\u2202hj/\u2202aj) 2 \u2016Wtj\u2016 p 2 )] . Note, only the term (\u2202hj/\u2202aj) 2 has effect on the gradient w.r.t the encoding bias be (hence sparsity). Thus the term \u03c32\u2016Wtj\u2016 p 2 forms the effective regularization coefficient for the term (\u2202hj/\u2202aj) 2. Therefore, in order to exclusively analyze the trend of sparsity vs. regularization coefficient (\u03c32), we constraint 5 \u2016Wtj\u2016 p 2 = 1 and train all the AE objectives. The result on MNIST and CIFAR-10 can be seen in figure 3. Clearly, both Sigmoid and ReS have a stable decreasing trend for all the AE models. On the other hand, ReLU activation is sensitive towards CAE/mDAE. As suggested in section 2.1.1, an explanation for this observation from the perspective of activation function is that ReLU lacks gradient from the regularization w.r.t. the encoding bias be, which is needed for regularizations in corollary 2 (eg. CAE, mDAE, DAE). So the bias gradient for ReLU in these cases entirely depends on the loss function. This can be verified in figure 2. Here, we compute the mean and standard-deviation of bias gradient (w.r.t. regularization) values across all units and samples during every training iteration6. We do the same for bias values as well. ReLU completely lacks bias gradient resulting in larger bias values (leading to poor sparsity) compared to ReS. This result clearly confirms our analysis on ReLU and supports the proposed advantage of ReS."}, {"heading": "4.3 Comparison of Auto-Encoder objectives", "text": "Here we are interested in analysing how sensitive different AE objectives are across various values of regularization coefficient (\u03c32), activation functions and datasets. We do this in light of our analysis of the equivalent forms of AE objectives that we derive in section 2.2. So we train all the AE models without any extra constraint (as against section 4.2) and plot sparsity vs. \u03c32. The plots are shown in figure 4 for MNIST AND CIFAR-10. We find that the sparsity trend for both CAE and mDAE is sensitive to the value of regularization coefficient while that for DAE and SAE is stable and has a smooth decreasing trend. As already mentioned in section 4.2, a plausible explanation for this observation from the perspective of AE objective function is that (\u2202hj/\u2202aj)\n2 is effectively regularized with the coefficient \u03c32\u2016Wtj\u2016 p 2 and hence \u03c3 2 has a non-linear effect on the bias gradient."}, {"heading": "4.3.1 Why is DAE less sensitive compared to CAE/mDAE?", "text": "The surprising part of the above experiments is that DAE has a stable sparsity trend (across different values of \u03c32) for ReLU although DAE (similar to CAE, mDAE) has a regularization form given in corollary 2. The fact that ReLU practically does not generate bias gradients from this form of regularization brings our attention to an interesting possibility: ReLU is generating the positive bias gradient due to the first order regularization term in DAE. Recall that we marginalize out the first order term in DAE (during Taylor\u2019s expansion, see proof in section 2.2.1) while taking expectation over all corrupted versions of a training sample. However, the mathematically equivalent objective of DAE obtained by this analytical marginalization is not what we optimize in practice. While optimizing with explicit corruption in a batch-wise manner, we indeed get a non-zero first order term, which does not vanish due to finite sampling (of corrupted versions); thus explaining sparsity for ReLU. We test this hypothesis by optimizing the explicit Taylor\u2019s expansion of DAE (eDAE) with only the first order term on MNIST and CIFAR-10 using our standard experimental protocols:\nJeDAE = N\u2211 i=1 `(xi, fd(fe(xi))) + (x\u0303i \u2212 xi)T\u2207x\u0303i` (13)\nwhere x\u0303i is a Gaussian corrupted version of xi and N denotes number of training samples. The activation fraction vs. corruption variance (\u03c32) for eDAE is shown in table 2 which confirms that the first order term contributes towards sparsity. On the other hand, both CAE and mDAE have regularizations with only second order terms, which as already discussed, do not generate bias gradients for ReLU and hence fail at sparsity. On a more general note, lower order terms (in Taylor\u2019s expansion) of highly non-linear functions generally change slower (hence less sensitive) compared to higher order terms. In conclusion we find that explicit corruption may have advantages at times compared to marginalization because it captures the effect of both lower and higher order terms together.\n5Note that CAE and mDAE objectives become identical in this case. 6Both bias gradient and thus bias values converge after the first 10 iterations."}, {"heading": "4.4 Effect of True Sparsity on Supervised Performance", "text": "As mentioned in section 3, true sparse representation generally leads to better performance. On the same note, ReS overcomes the individual drawbacks of all the three activations\u2013 ReLU, Softplus and Sigmoid, thus encouraging true sparsity. Even though the focus of this paper is not on supervised evaluation on AEs, in order to test the effectiveness of true sparsity in the learned representations of AEs, we quantitatively analyse the robustness of the proposed activation function ReS against others. Since our paper focuses on analysing unsupervised representation learning using single layer AEs, we train different AE models and use them as feature extractors, i.e., we do not fine-tune the weight vectors of the AEs. The hyper-parameters for training all the AEs are chosen on the validation set. These include the learning rate for pre-training (candidate set{0.0001, 0.0005, 0.001, 0.005, 0.01}) and regularization coefficient \u03c32 (candidate set {0.001, 0.12, 0.32, 0.52}). Finally, we use linear SVM for classification using the extracted features with penalty C from candidate set {0.01, 0.1, 1, 10}. The test errors chosen on the best validation performance for MNIST can be seen in table 1. We find a couple of interesting observations:\n1) The performance of ReLU is worse than Sigmoid and Softplus for CAE and mDAE due to weak bias gradient. Thus even though ReLU is potentially capable of generating hard zeros, it never does so due to weak gradients while Sigmoid and Softplus (although incapable of producing hard zeros) learn sparse representation and hence get better results.\n2) ReLU has better performance than Sigmoid and Softplus for DAE and SAE. Even though all three activations learn sparse representations, ReLU enforces true sparsity. Recall that ReLU doesn\u2019t have weak bias gradient problem for DAE and SAE.\n3) Finally, ReS retains the benefits of all\u2013 ReLU, Sigmoid and Softplus; thus produces performance at par (or better) with the best performing activation for all AE models consistently.\n0.0 0.2 0.4 0.6 0.8 1.0 \u03c32\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nAv g.\nA ct\niv at\nio n\nFr ac\ntio n\nMNIST CIFAR-10\neDAE with ReLU\nTable 2: Activation fraction vs. \u03c32 for eDAE using ReLU showing 1st order term in DAE contributes towards sparsity."}, {"heading": "5 Conclusion", "text": "Inspired from the fact that neurons in brain exhibit sparse distributed behavior, we establish a connection between auto-encoders and sparse representation. Our contribution is multi-fold, we show: a) AE regularizations with positive encoding bias gradient encourage low pre-activation values (proposition 1); b) monotonically increasing convex activation functions with negative saturation at zero encourage sparsity for such regularizations (theorem 1) and that multiple existing activations satisfy them; c) existing AEs have regularizations of the form suggested in corollary 1 and 2, justifying why they learn sparse representation. d) Based on these insights, we propose a new activation function (ReS in section 3) which overcomes the individual drawbacks of existing activations in terms of sparsity leading to better empirical performance.\nOn the empirical side, a) we find that AEs only use the convex region of Sigmoid (section 4.1) which justifies learning sparse representation using this activation; b) ReLU leads to poor sparsity for CAE and mDAE due to the absence of bias gradients(section 4.2) compared with Sigmoid and ReS; c) DAE and SAE are less sensitive to regularization coefficient compared with CAE and mDAE (section 4.3) in terms of sparsity; d) explicit corruption (eg. DAE) may have advantages over marginalizing it out (eg. mDAE, see section 4.3.1) because it captures both first and second order effects. In conclusion, our analysis combined together yields new insights into AEs and provides novel tools for analyzing existing (and new) regularization/activation functions that help predicting whether the resulting AE learns sparse representations."}, {"heading": "A1 Supplementary Material", "text": ""}, {"heading": "A1 .1 Supplementary Proofs", "text": "Proposition 1. Let {Wt \u2208 Rm\u00d7n,bte \u2208 Rm} be the parameters of a regularized auto-encoder (\u03bb > 0)\nJRAE = JAE + \u03bbR(W,be) (14)\nat training iteration t with regularization term R(W,be), activation function se(.) and define preactivation atj = W t jx+b t ej (thus h t j = se(a t j)). If \u2202R \u2202bej\n> 0, where j \u2208 {1, 2, . . . ,m}, and Ex[x] = 0, then updating {Wt,bte} along the negative gradient of \u03bbR, results in Ex[at+1j ] < Ex[atj ] and var[at+1j ] \u2264 \u03bb2x\u2016W t+1 j \u20162 for all t \u2265 0. Here \u03bb2x denotes the maximum eigenvalues of Ex[xxT ].\nProof. At iteration t+ 1,\nat+1j = a t j \u2212 \u03b7\u03bb \u2202R \u2202Wj x\u2212 \u03b7\u03bb \u2202R \u2202bej\n(15)\nfor any step size \u03b7. Thus\nEx [ at+1j ] = Ex [ atj ] \u2212 \u03b7\u03bb \u2202R\n\u2202bej (16)\nThus if \u2202R\u2202bej > 0, then Ex[at+1j ] < Ex[atj ].\nFinally, var[at+1j ] = Ex[a t+1 j \u2212 Ex[a t+1 j ]] 2 = Ex[Wt+1j x]2 \u2264 \u03bb2x\u2016W t+1 j \u20162\nCorollary 1. If se is a monotonically increasing activation function and Ex[x] = 0, then updating {Wt,bte} along the negative gradient of R = \u2211m j=1 f(Ex[hj ]), results in Ex[a t+1 j ] \u2264 Ex[atj ] and var[at+1j ] \u2264 \u03bb2x\u2016W t+1 j \u20162 for all t \u2265 0. Here f(.) is any monotonically increasing function.\nProof. We need one additional argument other than proposition 1. \u2202R\u2202bej = \u2202f(Ex[hj ]) \u2202Ex[hj ] Ex [ \u2202hj \u2202aj ] . Since both se(.) and f(.) are monotonically increasing functions, \u2202R\u2202bej \u2265 0 in all cases.\nCorollary 2. If se(.) is a monotonically increasing convex activation function and Ex[x] = 0, then updating {Wt,bte} along the negative gradient of R = Ex [\u2211m j=1 (( \u2202hj \u2202aj )q \u2016Wtj\u2016 p 2 )] , q \u2208 N , p \u2208W, results in Ex[at+1j ] \u2264 Ex[atj ] and var[a t+1 j ] \u2264 \u03bb2x\u2016W t+1 j \u20162 for all t \u2265 0.\nProof. We need one additional argument other than proposition 1. \u2202R\u2202bej = Ex [ q ( \u2202hj \u2202aj )q\u22121 \u22022hj \u2202a2j \u2202aj \u2202bej \u2016Wtj\u2016 p 2 ] . Since se(.) is a monotonically increasing convex function, both \u2202 2se(aj)\n\u2202a2j \u2265 0 and \u2202se(aj)\u2202aj \u2265 0 \u2200aj \u2208 R. Finally, \u2202aj \u2202bej = 1 by definition. Thus \u2202R\u2202bej \u2265 0\nin all cases.\nTheorem 1. (Desirable properties of Activation function) When proposition 1 applies and a monotonically increasing convex function se(.) is used, the following arguments hold true,\n1. Both \u2202hj\u2202aj , \u22022hj \u2202a2j \u2265 0 for all values of the argument of se(.) due to monotonically increasing\nand convexity property of se(.) and hence both corollary 2 and 1 apply to se(.).\n2. ptj := 1 \u2212 var[atj ]\n(s\u22121e (\u03b4min)\u2212E[atj ])2 lower bounds Pr(htj \u2264 \u03b4min) over data distribution D at\niteration t.\n3. If \u2016Wtj\u20162 is upper bounded independent of \u03bb values then \u2203S \u2286 R+ and \u2203Tmin \u2208 N such that pt+1j \u2265 ptj \u2200t \u2265 Tmin, \u2200\u03bb \u2208 S.\nProof. From proposition 1, E[at+1j ] < E[atj ] \u2200t \u2265 0. Define amin such that \u03b4min = maxamin se(amin). (We have used amin = s \u22121 e (\u03b4min) as an abuse of notation in the statement of theorem for notational convenience; s\u22121e (.) doesn\u2019t exist for non-strictly monotonically increasing functions) Thus \u2203To \u2208 N, such that \u2200t \u2265 To, E[atj ] < amin. Since E[atj ] < amin \u2200t \u2265 To, in the case monotonically increasing convex activation functions, using Chebyshev\u2019s bound,\nPr(htj \u2264 \u03b4min) = Pr(atj \u2264 amin) \u2265 Pr(|atj \u2212 E[atj ]| \u2264 amin \u2212 E[atj ])\n\u2265 1\u2212 var[atj ] (amin \u2212 E[atj ])2 (17)\nThus Pr(htj \u2264 \u03b4min) \u2265 1 \u2212 var[atj ]\n(amin\u2212E[atj ])2 . In other words, ptj := 1 \u2212\nvar[atj ]\n(amin\u2212E[atj ])2 lower bounds\nPr(htj \u2264 \u03b4min). Now consider the difference\nD := var[at+1j ]\n(amin \u2212 E[atj ])2 \u2212\nvar[atj ]\n(amin \u2212 E[atj ])2 (18)\nand recall that Ex [ at+1j ] = Ex [ atj ] \u2212 \u03b7\u03bb \u2202R\n\u2202bej (19)\nwhere both the step size \u03b7 and \u2202R\u2202bej are positive. Thus, since var[aj ] \u2264 \u03bb2x\u2016Wtj\u20162 and \u2200t \u2265 Tmin, we can always choose a fixed S \u2286 R+ such that D < 0 \u2200\u03bb \u2208 S and t \u2265 Tmin.\nTheorem 2. Let {W,be} represent the parameters of a DAE with squared loss, linear decoding, and i.i.d. Gaussian corruption with zero mean and \u03c32 variance, at any point of training over data sampled from distribution D. Let aj := Wjx + bej so that hj = se(aj) corresponding to sample x \u223c D. Then,\nJDAE = JAE + \u03c32Ex  m\u2211 j=1 (( \u2202hj \u2202aj )2 \u2016Wj\u201642 ) + m\u2211 j,k=1 j 6=k ( \u2202hj \u2202aj \u2202hk \u2202ak (WTj Wk) 2 ) +\nn\u2211 i=1 ( (bd + W Th\u2212 x)TWT ( \u22022h \u2202a2 Wi Wi )) + o(\u03c32) (20)\nwhere \u2202 2h \u2202a2 \u2208 R m is the element-wise 2nd derivative of h w.r.t. a and is element-wise product.\nProof. Using 2nd order Taylor\u2019s expansion of the loss function, we get\n`(x, fd(fe(x\u0303))) = `(x, fd(fe(\u00b5x))) + (x\u0303\u2212 \u00b5x)T\u2207x\u0303`+ 1\n2 (x\u0303\u2212 \u00b5x)T\u22072x\u0303` (x\u0303\u2212 \u00b5x) + o(\u03c32)\n(21)\nSince the corruption process is Gaussian with zero mean, taking the expectation of this approximation yields\nE[`(x, fd(fe(x\u0303)))] = E[`(x, fd(fe(\u00b5x)))] + 1\n2 tr(\u03a3x\u22072x\u0303`) + o(\u03c32) (22)\nwhere \u03a3x := E[(x\u0303\u2212\u00b5x)(x\u0303\u2212\u00b5x)T ]. Since the corruption is i.i.d., assume the covariance \u03a3x = \u03c32I, where I is the identity matrix.\nThus we can rewrite equation (22) as\nJDAE = JAE + 1\n2 \u03c32 n\u2211 i=1 \u22022` \u2202x\u03032i + o(\u03c32) (23)\nExpanding the second order term in the above equation, we get\n\u22022` \u2202x\u03032i = \u2202h \u2202x\u0303i T \u22022` \u2202h2 \u2202h \u2202x\u0303i + \u2202` \u2202h T \u22022h \u2202x\u03032i (24)\nFor linear decoding and squared loss,\n\u2202`\n\u2202h\nT \u22022h\n\u2202x\u03032i = n\u2211 i=1 ( (bd + W Th\u2212 x)TWT ( \u22022h \u2202a2 Wi Wi )) (25)\nwhere \u2202 2h \u2202a2 \u2208 R m is the element-wise 2nd derivative of h w.r.t. a, represents element-wise product and Wi denotes the ith column of W. Let vector dh \u2208 Rm be defined such that dhj = \u2202hj \u2202aj \u2200j \u2208 {1, 2, . . . ,m}. Then, n\u2211 i=1 \u2202h \u2202x\u0303i T \u22022` \u2202h2 \u2202h \u2202x\u0303i = 2 n\u2211 j=1 n\u2211 k=1 ( (dh (W)j)T (W)k )2 (26)\nwhere (W)j represents the jth column of W and denotes element-wise product. Let Dh = diag(dh). Then,\nn\u2211 j=1 n\u2211 k=1 ( (dh (W)j)T (W)k )2 = \u2016(DhW)TW\u20162F (27)\nFinally, using the cyclic property of trace operator, we get, \u2016(DhW)TW\u20162F = tr(WTDhWW TDhW) = tr(DhWW TDhWW T ). Thus DAE objective becomes,\nJDAE = JAE + \u03c32Ex[tr(DhWWTDhWWT )]+ n\u2211 i=1 ( (bd + W Th\u2212 x)TWT ( \u22022h \u2202a2 Wi Wi )) + o(\u03c32)\n(28)\nUpon expansion of the second term above, we get the final form.\nRemark 3. Let {W \u2208 Rm\u00d7n,be \u2208 Rm} represent the parameters of a Marginalized De-noising Auto-Encoder (mDAE) with se(.) activation function, linear decoding, squared loss and \u03c32xi = \u03bb \u2200i \u2208 {1, . . . , n}, at any point of training over data sampled from some distribution D. Let aj := Wjx + bej so that hj = se(aj) corresponding to sample x \u223c D. Then,\nJmDAE = JAE + \u03bbEx  m\u2211 j=1 (( \u2202hj \u2202aj )2 \u2016Wj\u201642 ) (29) Proof. For linear decoding and squared loss, \u2202\n2` \u2202hj2 = 2\u2016Wj\u201622 and \u2202hj \u2202x\u0303i = \u2202hj \u2202aj Wji. Thus\n1\n2 n\u2211 i=1 \u03c32xi m\u2211 j=1 \u22022` \u2202hj 2 ( \u2202hj \u2202x\u0303i )2 = n\u2211 i=1 \u03bb m\u2211 j=1 \u2016Wj\u201622 ( \u2202hj \u2202aj Wji )2\n= \u03bb m\u2211 j=1 \u2016Wj\u201622 ( \u2202hj \u2202aj )2 n\u2211 i=1 W 2ji = \u03bb m\u2211 j=1 ( \u2202hj \u2202aj )2 \u2016Wj\u201642\n(30)"}], "references": [{"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Better mixing via deep representations", "author": ["Yoshua Bengio", "Gr\u00e9goire Mesnil", "Yann Dauphin", "Salah Rifai"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological Cybernetics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["Minmin Chen", "Kilian Q. Weinberger", "Fei Sha", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Adam Coates", "Andrew Y. Ng"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Theory of the backpropagation neural network", "author": ["Robert Hecht-Nielsen"], "venue": "In Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Receptive fields of single neurones in the cat\u2019s striate cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "The Journal of physiology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1959}, {"title": "Distributed representation of objects in the human ventral visual pathway", "author": ["Alumit Ishai", "Leslie Ungerleider", "Alex Martin", "Jennifer Schouten", "James Haxby"], "venue": "Proc Natl Acad Sci,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["Koray Kavukcuoglu", "Yann Lecun"], "venue": "Technical report, Courant Institute,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["Koray Kavukcuoglu", "Marc\u2019Aurelio Ranzato", "Yann LeCun"], "venue": "arXiv preprint arXiv:1010.3467,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Sparse deep belief net model for visual area v2", "author": ["Honglak Lee", "Chaitanya Ekanadham", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Why does the unsupervised pretraining encourage moderate-sparseness", "author": ["Jun Li", "Wei Luo", "Jian Yang", "Xiaotong Yuan"], "venue": "CoRR, abs/1312.5813,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "An introduction to computing with neural nets", "author": ["Richard P Lippmann"], "venue": "ASSP, IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1987}, {"title": "Zero-bias autoencoders and the benefits of co-adapting features", "author": ["Roland Memisevic", "Kishore Reddy Konda", "David Krueger"], "venue": "In ICLR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Sparse autoencoder", "author": ["Andrew Ng"], "venue": "Lecture notes,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by v1", "author": ["Bruno A. Olshausen", "David J. Fieldt"], "venue": "Vision Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Where do you know what you know? the representation of semantic knowledge in the human brain", "author": ["Karalyn Patterson", "Peter Nestor", "Timothy Rogers"], "venue": "Nature Rev. Neuroscience,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Higher order contractive auto-encoder", "author": ["Salah Rifai", "Gr\u00e9goire Mesnil", "Pascal Vincent", "Xavier Muller", "Yoshua Bengio", "Yann Dauphin", "Xavier Glorot"], "venue": "In ECML/PKDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Salah Rifai", "Pascal Vincent", "Xavier Muller", "Xavier Glorot", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Learning representations by back-propagating errors", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "venue": "Nature, pages 533\u2013536,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1986}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T Tieleman", "G Hinton"], "venue": "In COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Yi Ma"], "venue": "IEEEE TPAMI,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 82, "endOffset": 97}, {"referenceID": 20, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 82, "endOffset": 97}, {"referenceID": 21, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 82, "endOffset": 97}, {"referenceID": 7, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 82, "endOffset": 97}, {"referenceID": 16, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 138, "endOffset": 149}, {"referenceID": 12, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 138, "endOffset": 149}, {"referenceID": 4, "context": "Both Sparse Representation (SR) and Neural Network (NN) are biologically inspired [12, 25, 26, 9] models used heavily in machine learning [20, 15, 5].", "startOffset": 138, "endOffset": 149}, {"referenceID": 10, "context": "A key commonality between these models is finding distributed representation [13, 11] for observed data although the former focuses on finding sparse distributed representation while the latter focuses on learning complex functions.", "startOffset": 77, "endOffset": 85}, {"referenceID": 0, "context": "When combined together, some of the main advantages of sparse distributed representation in the context of deep neural networks [1] has been shown to be information disentangling and manifold flattening [2], and better linear separability and representational power [7].", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "When combined together, some of the main advantages of sparse distributed representation in the context of deep neural networks [1] has been shown to be information disentangling and manifold flattening [2], and better linear separability and representational power [7].", "startOffset": 203, "endOffset": 206}, {"referenceID": 6, "context": "When combined together, some of the main advantages of sparse distributed representation in the context of deep neural networks [1] has been shown to be information disentangling and manifold flattening [2], and better linear separability and representational power [7].", "startOffset": 266, "endOffset": 269}, {"referenceID": 14, "context": "Due to the aforementioned biological connection between SR and NNs, a natural follow-up pursued by a number of researchers was to propose AE variants that encouraged sparsity in their learned representation [18, 14, 24, 21].", "startOffset": 207, "endOffset": 223}, {"referenceID": 11, "context": "Due to the aforementioned biological connection between SR and NNs, a natural follow-up pursued by a number of researchers was to propose AE variants that encouraged sparsity in their learned representation [18, 14, 24, 21].", "startOffset": 207, "endOffset": 223}, {"referenceID": 19, "context": "Due to the aforementioned biological connection between SR and NNs, a natural follow-up pursued by a number of researchers was to propose AE variants that encouraged sparsity in their learned representation [18, 14, 24, 21].", "startOffset": 207, "endOffset": 223}, {"referenceID": 17, "context": "On the other hand, there has also been work on empirically analyzing/suggesting the sparseness of hidden representations learned after pre-training with unsupervised models [22, 19, 23].", "startOffset": 173, "endOffset": 185}, {"referenceID": 15, "context": "On the other hand, there has also been work on empirically analyzing/suggesting the sparseness of hidden representations learned after pre-training with unsupervised models [22, 19, 23].", "startOffset": 173, "endOffset": 185}, {"referenceID": 18, "context": "On the other hand, there has also been work on empirically analyzing/suggesting the sparseness of hidden representations learned after pre-training with unsupervised models [22, 19, 23].", "startOffset": 173, "endOffset": 185}, {"referenceID": 26, "context": "Finally the second part shows multiple popular AE objectives including De-noising auto-encoder (DAE) [31] and Contractive auto-encoder (CAE) [28] indeed have the suggested form of regularization; thus explaining why existing AEs encourage sparsity in their latent representation.", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "Finally the second part shows multiple popular AE objectives including De-noising auto-encoder (DAE) [31] and Contractive auto-encoder (CAE) [28] indeed have the suggested form of regularization; thus explaining why existing AEs encourage sparsity in their latent representation.", "startOffset": 141, "endOffset": 145}, {"referenceID": 24, "context": "Auto-Encoders (AE) [29, 3] are a class of single hidden layer neural networks that minimize the data reconstruction error while learning an intermediate mapping between the input and output space.", "startOffset": 19, "endOffset": 26}, {"referenceID": 2, "context": "Auto-Encoders (AE) [29, 3] are a class of single hidden layer neural networks that minimize the data reconstruction error while learning an intermediate mapping between the input and output space.", "startOffset": 19, "endOffset": 26}, {"referenceID": 20, "context": "Learning a dictionary adapted to a set of training data such that the latent code is sparse is generally formulated as the following optimization problem [25]", "startOffset": 154, "endOffset": 158}, {"referenceID": 8, "context": "Finally, the upper bound on weight vectors\u2019 length can easily be guaranteed1 using Maxnorm Regularization or Weight Decay which are widely used tricks while training deep networks [10].", "startOffset": 180, "endOffset": 184}, {"referenceID": 6, "context": ", their value is less than a certain threshold (\u03b4min), in practice, a representation that is truly sparse (large number of hard zeros) usually yields better performance [7, 32, 33].", "startOffset": 169, "endOffset": 180}, {"referenceID": 27, "context": ", their value is less than a certain threshold (\u03b4min), in practice, a representation that is truly sparse (large number of hard zeros) usually yields better performance [7, 32, 33].", "startOffset": 169, "endOffset": 180}, {"referenceID": 26, "context": "DAE [31] aims at minimizing the reconstruction error between every sample x and the reconstructed vector using its corresponding corrupted version x\u0303.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "CAE [28] objective is given by", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "In addition, since the first order regularization term in Higher order CAE (CAE+H) [27] is same as CAE, this suggests that CAE+H objective should have similar properties in term of sparsity.", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "mDAE [4] objective is given by:", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": "CIFAR-10 [16]: It consists of 60000 32\u00d7 32 real world (continuous valued) color images in 10 classes, with 6000 images per class.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "Experimental Protocols: For all experiments, we use RMS Prop [30] for objective optimization, learning rate 0.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "We initialize the bias to zeros and use normalized initialization [6] for weight vectors.", "startOffset": 66, "endOffset": 69}], "year": 2015, "abstractText": "Although a number of auto-encoder models enforce sparsity explicitly in their learned representation while others don\u2019t, there has been little formal analysis on what encourages sparsity in these models in general. Therefore, our objective here is to formally study this general problem for regularized auto-encoders. We show that both regularization and activation function play an important role in encouraging sparsity. We provide sufficient conditions on both these criteria and show that multiple popular models\u2013 like De-noising and Contractive auto-encoder\u2013 and activations\u2013 like Rectified Linear and Sigmoid\u2013 satisfy these conditions; thus explaining sparsity in their learned representation. Our theoretical and empirical analysis together, throws light on the properties of regularization/activation that are conducive to sparsity. As a by-product of the insights gained from our analysis, we also propose a new activation function that overcomes the individual drawbacks of multiple existing activations (in terms of sparsity) and hence produces performance at par (or better) with the best performing activation for all auto-encoder models discussed.", "creator": "LaTeX with hyperref package"}}}