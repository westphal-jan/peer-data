{"id": "1505.05969", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2015", "title": "Learning Program Embeddings to Propagate Feedback on Student Code", "abstract": "Providing functionality, both assessing final time one would sweetness even stuck reading, kind take will set - third assignments in collapse online curricula which or range from thousands out millions of athletes. We introduce a helical network method also encoding programme later when linear reconnaissance back an cables precondition space able full server postcondition solar all measures leading algorithm set feedback at rapid not besides parameters geographic such styles. We apply our algorithm given assessments 30 the Code. gds Hour of Code and Stanford University ' nasdaq100tr CS1 time, except we electromagnetic human comments on received procedures to purchasing many 4.7 for individually.", "histories": [["v1", "Fri, 22 May 2015 07:03:45 GMT  (2433kb,D)", "http://arxiv.org/abs/1505.05969v1", "Accepted to International Conference on Machine Learning (ICML 2015)"]], "COMMENTS": "Accepted to International Conference on Machine Learning (ICML 2015)", "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.SE", "authors": ["chris piech", "jonathan huang", "andy nguyen", "mike phulsuksombati", "mehran sahami", "leonidas j guibas"], "accepted": true, "id": "1505.05969"}, "pdf": {"name": "1505.05969.pdf", "metadata": {"source": "META", "title": "Learning Program Embeddings to Propagate Feedback on Student Code", "authors": ["Chris Piech", "Jonathan Huang", "Andy Nguyen", "Mike Phulsuksombati", "Mehran Sahami"], "emails": ["PIECH@CS.STANFORD.EDU", "JONATHANHUANG@GOOGLE.COM", "TANONEV@CS.STANFORD.EDU", "MIKEP15@CS.STANFORD.EDU", "SAHAMI@CS.STANFORD.EDU", "GUIBAS@CS.STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "Online computer science courses can be massive with numbers ranging from thousands to even millions of students. Though technology has increased our ability to provide content to students at scale, assessing and providing feedback (both for final work and partial solutions) remains difficult. Currently, giving personalized feedback, a staple of quality education, is costly for small, in-person classrooms and prohibitively expensive for massive classes. Autonomously providing feedback is therefore a central challenge for at scale computer science education.\nIt can be difficult to apply machine learning directly to data in the form of programs. Program representations such as the Abstract Syntax Tree (AST) are not directly conducive to standard statistical methods and the edit distance metric between such trees are not discriminative enough to be used to share feedback accurately since programs with similar ASTs can behave quite differently and require different comments. Moreover, though unit tests are a useful way to\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\ntest if final solutions are correct they are not well suited for giving help to students with an intermediate solution and they are not able to give feedback on stylistic elements.\nThere are two major goals of our paper. The first is to automatically learn a feature embedding of student submitted programs that captures functional and stylistic elements and can be easily used in typical supervised machine learning systems. The second is to use these features to learn how to give automatic feedback to students. Inspired by recent successes of deep learning for learning features in other domains like NLP and vision, we formulate a novel neural network architecture that allows us to jointly optimize an embedding of programs and memory-state in a feature space. See Figure 1 for an example program and corresponding matrix embeddings.\nTo gather data, we exploit the fact that programs are executable \u2014 that we can evaluate any piece of code on an arbitrary input (i.e., the precondition), and observe the state after, (the postcondition). For a program and its constituent parts we can thus collect arbitrarily many such precondition/postcondition mappings. This data provides the training set from which we can learn a shared representation for programs. To evaluate our program embeddings we test our ability to amplify teacher feedback. We use real student data from the Code.org Hour of Code which has been attempted by over 27 million learners making it, to the best of our knowledge, the largest online course to-date. We then show how the same approach can be used for submissions in Stanford University\u2019s Programming Methodologies course which has thousands of students and assignments that are substantially more complex. The programs we analyze are written in a Turing-complete language but do not allow for user-defined variables.\nOur main contributions are as follows. First, we present a method for computing features of code that capture both functional and stylistic elements. Our model works by simultaneously embedding precondition and postcondition spaces of a set of programs into a feature space where programs can be viewed as linear maps on this space. Second,\nar X\niv :1\n50 5.\n05 96\n9v 1\n[ cs\n.L G\n] 2\n2 M\nay 2\n01 5\nwe show how our code features can be useful for automatically propagating instructor feedback to students in a massive course. Finally, we demonstrate the effectiveness of our methods on large scale datasets. Learning embeddings of programs is fertile ground for machine learning research and if such embeddings can be useful for the propagation of teacher feedback this line of investigation will have a sizable impact on the future of computer science education."}, {"heading": "2. Related Work", "text": "The advent of massive online computer science courses has made the problem of automated reasoning with large code collections an important problem. There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback. The volume of work speaks to the importance of this problem. Despite the research efforts, however, providing quality feedback at scale remains an open problem.\nA central challenge that a number of papers address is that of measuring similarity between source code. Some authors have done this without an explicit featurization of the code \u2014 for example, the AST edit distance has been a popular choice (Huang et al., 2013; Rogers et al., 2014). (Mokbel et al., 2013) explicitly hand engineered a small collection of features on ASTs that are meant to be domainindependent.\nTo incorporate functionality, (Nguyen et al., 2014) proposed a method that discovers program modifications that do not appear to change the semantic meaning of code. The embedded representations of programs used in this paper also capture semantic similarities and are more amenable to prediction tasks such as propagating feedback. We ran feedback propagation on student data using methods from\nNguyen et al and observe that embeddings enabled notable improvement (see section 6.3).\nEmbedding programs has many crossovers with embedding natural language artifacts, given the similarity between the AST representation and parse trees. Our models are related to recent work from the NLP and deep learning communities on recursive neural networks, particularly for modeling semantics in sentences or symbolic expressions (Socher et al., 2013; 2011; Zaremba et al., 2014; Bowman, 2013).\nFinally, representing a potentially complicated function (which in our case is a program) as a linear operator acting on a nonlinear feature space has also been explored in different communities. The computer graphics community have represented pairings of nonlinear geometric shapes as linear maps between shape features, called functional maps (Ovsjanikov et al., 2012; 2013). From the kernel methods literature, there has also been recent work on representations of conditional probability distributions as operators on a Hilbert space (Song et al., 2013; 2009). From this point of view, our work is novel in that it focuses on the joint optimization of feature embeddings together with a collection of maps so that the maps simultaneously \u201clook linear\u201d with respect to the feature space."}, {"heading": "3. Embedding Hoare Triples", "text": "Our core problem is to represent a program as a point in a fixed-dimension real-valued space that can then be used directly as input for typical supervised learning algorithms.\nWhile there are many dimensions that \u201ccharacterize\u201d a program including aspects such as style or time/space complexity, we begin by first focussing on capturing the most basic aspect of a program \u2014 its function. While capturing the function of the program ignores aspects that can be useful in application (such as giving stylistic feedback in CS education), we discuss in later sections how elements of style can be recaptured by modeling the function of subprograms that correspond to each subtree of an AST. Given a program A (where we consider a program to generally be any executable code whether a full submission or a subtree of a submission), and a precondition P , we thus would like to learn features of A that are useful for predicting the outcome of running A when P holds. In other words, we want to predict a postcondition Q out of some space of possible postconditions. Without loss of generality we let P and Q be real-valued vectors encapsulating the \u201cstate\u201d of the program (i.e., the values of all program variables) at a particular time. For example, in a grid world, this vector would contain the location of the agent, the direction the agent is facing, the status of the board and whether the program has crashed. Figure 2 visualizes two preconditions, and the corresponding postconditions for a simple program.\nWe propose to learn program features using a training set of\n(P,A,Q)-triples \u2014 so-called Hoare triples (Hoare, 1969) obtained via historical runs of a collection of programs on a collection of preconditions. We discuss the process by which such a dataset can be obtained in Section 5. The main approach that we espouse in this paper is to simultaneously find an embedding of states and programs into feature space where pre and postconditions are points in this space and programs are mappings between them.\nThe simple way that we propose to relate preconditions to postconditions is through a linear transformation. Explicitly, given a (P,A,Q)-triple, if fP and fQ are mdimensional nonlinear feature representations of the pre and postconditions P and Q, respectively, then we relate the embeddings via the equation\nfQ = MA \u00b7 fP . (1)\nWe then take the m \u00d7m matrix of coefficients MA as our feature representation of the program A and refer to it as the program embedding matrix. We will want to learn the mapping into feature space f as well as the linear map MA such that this equality holds for all observed triples and can generalize to predict postcondition Q given P and A.\nAt first blush, this linear relationship may seem too limiting as programs are not linear nor continuous in general. By learning a nonlinear embedding function f for the pre and postcondition spaces, however, we can capture a rich family of nonlinear relationships much in the same way that kernel methods allow for nonlinear decision boundaries.\nAs described so far, there remain a number of modeling choices to be made. In the following, we elaborate further on how we model the feature embeddings fP , and fQ of the pre and postconditions, and how to model the program embedding matrix MA."}, {"heading": "3.1. Neural network encoding and decoding of states", "text": "We assume that preconditions have some base encoding as a d-dimensional vector, which we refer to as P . For example, in image processing courses, the state space could simply be the pixel encoding of an image, whereas in the discrete gridworld-type programming problems that we use in our experiments, we might choose to encode the (x, y)-\ncoordinate and discretized heading of a robot using a concatenation of one-hot encodings. Similarly, we assume that there is a base encoding Q of the postcondition.\nWe will focus our exposition in the remainder of our paper on the case where the precondition space and postcondition spaces share a common base encoding. This is particularly appropriate to our experimental setting in which both the preconditions and postconditions are representations of a gridworld. In this case, we can use the same decoder parameters (i.e., W dec and bdec) to decode both from precondition space and postcondition space \u2014 a fact that we will exploit in the following section.\nInspired by nonlinear autoencoders, we parameterize a mapping, called the encoder from precondition P to a nonlinear m-dimensional feature representation fP . As with traditional autoencoders, we use an affine mapping composed with an elementwise nonlinearity:\nfP = \u03c6(W enc \u00b7 P + benc), (2)\nwhereW enc \u2208 Rm\u00d7d, benc \u2208 Rm, and \u03c6 is an elementwise nonlinear function (such as tanh). At this point, we can use the representation fP to decode or reconstruct the original precondition as a traditional autoencoder would do using:\nP\u0302 = \u03c8(W dec \u00b7 fP + bdec), (3)\nwhere W dec \u2208 Rd\u00d7m, bdec \u2208 Rd, and \u03c8 is some (potentially different) elementwise nonlinear function. Moreover, we can push the precondition embedding fP through Equation 1, and decode the postcondition embedding fQ = MA \u00b7 fP . This mapping which reconstructs the postcondition Q, the decoder, takes the form:\nQ\u0302 = \u03c8(W dec \u00b7 fQ + bdec), (4)\n= \u03c8(W dec \u00b7MA \u00b7 fP + bdec). (5)\nFigure 2 diagrams our model on a simple program. Note that it is possible to swap in alternative feature representations. We have experimented with using a deep, stacked autoencoder however our results have not shown these to help much in the context of our datasets."}, {"heading": "3.2. Nonparametric model of program embedding", "text": "To encode the program embedding matrix, we propose a simple nonparametric model in which each program in the training set is associated with its own embedding matrix. Specifically, if the collection of unique programs is {A1, . . . , Am}, then for each Ai, we will associate a matrix Mi. The entire parameter set for our nonparametric matrix model (henceforth abbreviated NPM) is thus: \u0398 = {W dec,W enc, benc, bdec} \u222a {Mi : i = 1, . . . ,m}.\nTo learn the parameters, we minimize a sum of three terms: (1) a prediction loss `pred which quantifies how well we can predict postcondition of a program given a precondition, (2) an autoencoding loss `auto which quantifies how good the encoder and decoder parameters are for reconstructing given preconditions, and (3) a regularization term R. Formally, given training triples {(Pi, Ai, Qi)}ni=1, we can minimize the following objective function:\nL(\u0398) = 1\nn n\u2211 i=1 `pred(Qi, Q\u0302i(Pi, Ai; \u0398))\n+ 1\nn n\u2211 i=1 `auto(Pi, P\u0302i(Pi,\u0398)) + \u03bb 2 R(\u0398),\n(6)\nwhere R is a regularization term on the parameters, and \u03bb a regularization parameter. In our experiments, we use R to penalize the sum of the L2 norms of the weight matrices (excluding the bias terms benc and bdec).\nAny differentiable loss can conceptually be used for `pred and `auto. For example, when the top level predictions, P\u0302 or Q\u0302, can be interpreted as probabilities (e.g., when \u03c6 is the Softmax function), we use a cross-entropy loss function.\nInformally speaking, one can think of our optimization problem (Equation 6) as trying to find a good shared representation of the state space \u2014 shared in the sense that even though programs are clearly not linear maps over the original state space, the hope is that we can discover some nonlinear encoding of the pre and postconditions such that most programs simultaneously \u201clook\u201d linear in this new projected feature space. As we empirically show in Section 6, such a representation is indeed discoverable.\nWe run joint optimization using minibatch stochastic gradient descent without momentum, using ordinary backpropagation to calculate the gradient. We use random search (Bergstra & Bengio, 2012) to optimize over hyperparameters (e.g, regularization parameters, matrix dimensions, and minibatch size). Learning rates are set using Adagrad (Duchi et al., 2011). We seed our parameters using a \u201csmart\u201d initialization in which we first learn an autoencoder on the state space, and perform a vector-valued ridge regression for each unique program to extract a matrix mapping the features of the precondition to the features\nof the postcondition. The encoder and decoder parameters and the program matrices are then jointly optimized."}, {"heading": "3.3. Triple Extraction", "text": "For a given program S we extract Hoare triples by executing it on an exemplar set of unit tests. These tests span a variety of reasonable starting conditions. We instrument the execution of the program such that each time a subtree A \u2282 S is executed, we record the value, P , of all variables before execution, and the value, Q, of all variables after execution and save the triple (P , A, Q). We run all programs on unit tests, collecting triples for all subtrees. Doing so results in a large dataset {(Pi, Ai, Qi)}ni=1 from which we collapse equivalent triples. In practice, some subtrees, especially the body of loops, generate a large (potentially infinite) number of triples. To prevent any subtree from having undue influence on our model we limit the number of triples for any subtree.\nCollecting triples on subtrees, as opposed to just collecting triples on complete programs, is critical since it allows us to learn embeddings not just for the root of a program AST but also for the constituent parts. As a result, we retain data on how a program was implemented, and not just on its overall functionality, which is important for student feedback as we discuss in the next section. Collecting triples on subtrees also means we are able to optimize our embeddings with substantially more data."}, {"heading": "4. Feedback Propagation", "text": "The result of jointly learning to embed states and a corpus of programs is a fixed dimensional, real-valued matrix MA for each subtreeA of any program in our corpus. These matrices can be cooperative with machine learning algorithms that can perform tasks beyond predicting what a program does. The central application in this paper is the force multiplication of teacher-provided feedback where an active learning algorithm interacts with human graders such that feedback is given to many more assignments than the grader annotates. We propose a two phase interaction. In the first phase, the algorithm selects a subset of exemplar programs for graders to apply a finite set of annotations. Then in the second phase, the algorithm uses the human provided annotations as supervised labels with which it can learn to predict feedback for unlabelled submissions. Each program is annotated with a set H \u2282 L where L is a discrete collection ofN possible annotations. The annotations are meant to cover a range of comments a grader could apply, including feedback on style, strategy and functionality. For each ungraded submission, we must then decide which of the N labels to apply. As such, we view feedback propagation as N binary classification tasks.\nOne way of propagating feedback would be to use the el-\nements of the embedding matrix of the root of a program as features and then train a classifier to predict appropriate feedback for a given program. However, the matrices we have learned for programs and their subtrees have been trained only to predict functionality. Consequently, any two programs that are functionally indistinguishable would be given the same instructor feedback under this approach, ignoring any strategic or stylistic differences between the programs."}, {"heading": "4.1. Incorporating structure via recursive embedding", "text": "To recapture the elements of program structure and style that are critical for student feedback, our approach to predict feedback uses the embedding matrices learned for the NPM model, but incorporates all constituent subtrees of a given AST. Specifically, using the embedding matrices learned in the NPM model (which we henceforth denote as MNPMA for a subtree A), we now propose a new model based on recursive neural networks (called the NPM-RNN model) in which we parametrize a matrix MA in this new model with an RNN whose architecture follows the abstract syntax tree (similar to the way in which RNN architectures might take the form of a parse tree in an NLP setting (Socher et al., 2013)).\nIn our RNN based model, a subtree of the AST rooted at node j is represented by a matrix which is computed by combining (1) representations of subtrees rooted at the children of j, and (2) the embedding matrix of the subtree rooted at node j learned via the NPM model. By incorporating the embedding matrix from the NPM model, we are able to capture the function of every subtree in the AST.\nFormally, we will assume each node is associated with some type in set T = {\u03c91, \u03c92, . . . }. Concretely, the type set might be the collection of keywords or built-in functions that can be called from a program in the dataset, e.g., T = {repeat,while, if , . . . }. A node with type \u03c9 is assumed to have a fixed number, a\u03c9 , of children in the AST \u2014 for example, a repeat node has two children, with one child holding the body of a repeat loop and the second representing the number of times the body is to be repeated.\nThe representation of node j with type \u03c9 is then recursively computed in the NPM-RNN model via:\na(j) = \u03c6 ( a\u03c9\u2211 i=1 W\u03c9i \u00b7 a(ci[j]) + b\u03c9 + \u00b5MNPMj ) , (7)\nwhere: \u03c6 is a nonlinearity (such as tanh), ci[j] indexes over the a\u03c9 children of node j, and MNPMj is the program embedding matrix learned in the NPM model for the subtree rooted at node j. We remind the reader that the activation a(j) at each node is anm\u00d7mmatrix. Leaf nodes of type \u03c9 are simply associated with a single parameter matrix W\u03c9 .\nIn the NPM-RNN model, we have parameter matrices\nW\u03c9, b\u03c9 \u2208 Rm\u00d7m for each possible type \u03c9 \u2208 T . To train the parameters, we first use the NPM model to compute the embedding matrix MNPMj for each subtree. After fixing Mj , we optimize (as with the NPM model) with minibatch stochastic gradient descent using backpropagation through structure (Goller & Kuchler, 1996) to compute gradients. Instead of optimizing for predicting postcondition, for NPM-RNN, we optimize for each of the binary prediction tasks that are used for feedback propagation given the vector embedding at the root of a program. We used hyper-parameters learned in the RNN model optimization since feedback optimization is performed over few examples and without a holdout set.\nFinally, feedback propagation has a natural active learning component: intelligently selecting submissions for human annotation can potentially save instructors significant time. We find that in practice, running k-means on the learned embeddings, and selecting the cluster centroids as the set of submissions to be annotated works well and leads to significant improvements in feedback propagation over random subset selection. Surprisingly, having humans annotate the most common programs performs worse than the alternatives, which we observe to be due to the fact that the most common submissions are all quite similar to one another."}, {"heading": "5. Datasets", "text": "We evaluate our model on three assignments from two different courses, Code.org\u2019s Hour of Code (HOC) which has submissions from over 27 million students and Stanfords Programming Methodology course, a first-term introductory programming course, which has collected submissions over many years from almost three thousand students. From these two classes, we look at three different assignments. As in many introductory programming courses, the first assignments have the students write standard programming control flow (if/else statements, loops, methods) but do not introduce user-defined variables. The programs for these assignments operate in maze worlds where an agent can move, turn, and test for conditions of its current location. In the Stanford assignments, agents can also put down and pick up beepers, making the language Turing complete.\nSpecifically, we study the following three problems:\n\u21261: The 18th problem in the Hour of Code (HOC). Students solve a task which requires an if/else block inside of a while loop, the most difficult concept in the Hour of Code.\n\u21262: The first assignment in Stanford\u2019s course. Students program an agent to retrieve a beeper in a fixed world.\n\u21263: The fourth assignment in Stanford\u2019s course. Students program an agent to find the midpoint of a world with unknown dimension. There are multiple strategies for this problem and many require O(n2) operations where n is the size of the world. The task is challenging even for those who already know how to program.\nIn addition to the final submission to any problem, from each student we also collect partial solutions as they progress from starter code to final answer. Table 1 summarizes the sizes of each of the datasets. For all three assignments studied, students take multiple steps to reach their final answer and as a result most programs in our datasets are intermediate solutions that are not responsive to unit tests that simply evaluate correctness. The code.org dataset is available at code.org/research.\nFor all assignments we have both functional and stylistic feedback based on class rubrics which range from observations of solution strategy, to notes on code decomposition, and tests for correctness. The feedback is generated for all submissions (including partial solutions) via a complex script. The script analyzes both the program trees and the series of steps a student took to assign annotations. In general, a script, no matter how complex, does not provide perfect feedback. However the ability to recreate these complex annotations allows us to rigorously evaluate our methods. An algorithm that is able to propagate such feedback should also be able to propagate human quality labels."}, {"heading": "6. Results", "text": "We rely on a few baselines against which to evaluate our methods, but the main baseline that we compare to is a simplification of the NPM-RNN model (which we will call, simply, RNN) in which we drop the program embedding terms Mj from each node (cf. Eqn. 7).\nThe RNN model can be trained to predict postconditions as well as to propagate feedback. It has much fewer parameters than the NPM (and thus NPM-RNN) model being a strictly parametric model, and is thus expected to have an advantage in smaller training set regimes. On the other hand, it is also a strictly less expressive model and so the question is: how much does the expressive power of the NPM and NPM-RNN models actually help in practice? We address this question amongst others using two tasks: predicting postcondition and propagating feedback."}, {"heading": "6.1. Prediction of postcondition", "text": "To understand how much functionality of a program is captured in our embeddings, we evaluate the accuracy to which we can use the program embedding matrices learned by the NPM model to predict postconditions \u2014 note, however, that we are not proposing to use the embeddings to predict post-conditions in practice. We split our observed Hoare triples into training and test sets and learn our NPM model using the training set. Then for each triple (P,A,Q) in the test set we measure how well we can predict the postcondition Q given the corresponding program A and precondition P . We evaluate accuracy as the average number of state variables (e.g. row, column, orientation and location of beepers) that are correctly predicted per triple, and in addition to the RNN model, compare against the baseline method \u201cCommon\u201d where we select the most common postcondition for a given precondition observed in the training set. As our results in Table 2 show, the NPM model achieves the best training accuracy (with 98%, 98% and 94% accuracy respectively, for the three problems). For the two simpler problems, the parametric (RNN) model achieves slightly better test accuracy, especially for problem \u21262 where the training set is much smaller. For the most complex programming problem, \u21263, however, the NPM model substantially outperforms other approaches."}, {"heading": "6.2. Composability of program embeddings", "text": "If we are to represent programs as matrices that act on a feature space, then a natural desiderata is that they \u201ccompose well\u201d. That is, if program C is functionally equivalent to running program B followed by program A, then it should be the case that MC \u2248 MB \u00b7MA. To evaluate the extent to which our program embedding matrices are composable, we use a corpus of 5000 programs that are composed of a subprogram A followed by another subprogram B (Compose-2). We then compare the accuracy of postcondition prediction using the embedding of an entire program MC against the product of embeddings MB \u00b7MA. As Table 3 shows, the accuracy using the NPM model for predicting postcondition is 94% when using the matrix for the root embedding. Using the product of two embedding matrices, we see that accuracy does not fall dramatically, with a decoding accuracy of 92%. When we test programs that are composed of three subprograms, A followed by B, then C (Compose-3), we see accuracy drop only to 83%.\nBy comparison, the embeddings computed using the RNN, a more constrained model, do not seem to satisfy composability. We also compare against NPM-0, which is the NPM model using just the weights set by the smart initialization (see Section 3.2). While NPM-0 outperforms the RNN, the full nonparametric model (NPM) performs much better, suggesting that the joint optimization (of state and program embeddings) allows us to learn an embedding of the state space that is more amenable to composition."}, {"heading": "6.3. Prediction of Feedback", "text": "We now use our program embedding matrices in the feedback propagation application described in Section 4. The central question is: given a budget of K human annotated programs (we set K = 500), what fraction of unannotated programs can we propagate these annotations to using the labelled programs, and at what precision? Alternatively, we are interested in the \u201cforce multiplication factor\u201d \u2014 the ratio of students who receive feedback via propagation to students to receive human feedback.\nFigure 3 visualizes recall and precision of our experiment on each of the three problems. The results translate to 214\u00d7, 12\u00d7 and 45\u00d7 force multiplication factors of teacher effort for \u21261, \u21262 and \u21263 respectively while maintaining 90% precision. The amount to which we can force multiply feedback depends both on the recall of our model and the size of the corpus to which we are propagating feedback. For example, though \u21262 had substantially higher recall than \u21261, in \u21262 the grading task was much smaller. There were only 6,700 unique programs to propagate feedback to, compared to \u21261 which had over 210,000. As with the previous experiment, we observe that for both \u21261 and \u21262, the NPM-RNN and RNN models perform similarly. However for \u21263, the NPM-RNN model substantially outperforms all alternatives.\nIn addition to the RNN, we compare our results to three other baselines: (1) Running unit tests, (2) a \u201cBag-ofTrees\u201d approach and (3) k-nearest neighbor (KNN) with AST edit distances. The unit tests unsurprisingly are perfect at recognizing correct solutions. However, since our dataset is largely composed of intermediate solutions and not final submissions (especially for \u21261 and \u21263), unit tests are not a particularly effective way to propagate annotations. The Bag-of-Trees approach, where we trained a Na\u0131\u0308ve Bayes model to predict feedback conditioned on the\nset of subtrees in a program, is useful for feedback propagation but we observe that it underperforms the embedding solutions on each problem. Moreover, we extended this baseline by amalgamating functionally equivalent code (Nguyen et al., 2014). Using equivalences found using similar amount of effort as in previous work, we are able to achieve 90% precision with recall of 39%, 48% and 13%, for the three problems respectively. While this improves the baseline, NPM-RNN obtains almost twice as much recall on all problems. Finally, we find KNN with AST edit distances to be computationally expensive to run and highly ineffective at propagating feedback \u2014 calculating edit distance between all trees requires 20 billion comparisons for \u21261 and 1.5 billion comparisons for \u21263. Moreover, the highest precision achieved by KNN for \u21263 is only 43% (note that the cut-off for the x-axis in Figure 3 is 80%) and at that precision only has a recall of 1.3%.\nThe feedback that we propagate covers a range of stylistic and functional annotations. To further understand the strengths and weaknesses of our solution, we explore the performance of the NPM-RNN model on each of the nine possible annotations for \u21263. As we see in Figure 4(c), our model performs best on functional feedback with an average 44% recall at 90% precision, followed by strategic feedback and performs worst at propagating purely stylistic annotations with averages of 31% and 8% respectively. Overall propagation for \u21263 is 33% recall at 90% precision."}, {"heading": "6.4. Code complexity and performance", "text": "The results from the above experiments are suggestive that the nonparametric models perform better on more complex code while the parametric (RNN) model performs better on simpler code. To dig deeper, we now look specifically into how our performance depends on the complexity of programs in our corpus \u2014 a question that is also central to understanding how our models might apply to other assignments. We focus on submissions for \u21263, which cover a range of complexities, from simple programs to ones with over 50 decision points (loops and if statements). The distribution of cyclomatic complexity (McCabe, 1976), a measure of code structure, reflects this wide range (shown in gray in Figures 4(a),(b)). We first sort and bin all submissions to \u21263 by cyclomatic complexity into ten groups of equal size. Figures 4(a),(b) plot the results of the postcondition prediction and force multiplication experiments run individually on these smaller bins (still using a holdout set, and a budget of 500 graded submissions). While the RNN model performs better for simple programs (with cyclomatic complexity\u2264 6), both train and test accuracies for the RNN degrade dramatically as programs become more complicated. On the other hand, while the NPM model overfits, it maintains steady (and better) performance in test accuracy as complexity increases. This pattern may help to\nexplain our observations that the RNN is more accurate for force multiplying feedback on simple problems."}, {"heading": "7. Discussion", "text": "In this paper we have presented a method for finding simultaneous embeddings of preconditions and postconditions into points in shared Euclidean space where a program can be viewed as a linear mapping between these points. These embeddings are predictive of the function of a program, and as we have shown, can be applied to the the tasks of propagating teacher feedback. The courses we evaluate our model on are compelling case studies for different reasons. Tens of millions of students are expected to use Code.org next year, meaning that the ability to autonomously provide feedback could impact an enormous number of people. The Stanford course, though much smaller, highlights the complexity of the code that our method can handle.\nThere remains much work towards making these embeddings more generally applicable, particularly for domains where we do not have tens of thousands of submissions per problem or the programs are more complex. For settings where users can define their own variables it would be nec-\nessary to find a novel method for mapping program memory into vector space. An interesting future direction might be to jointly find embeddings across multiple homeworks from the same course, and ultimately, to even learn using arbitrary code outside of a classroom environment. To do so may require more expressive models. From the standpoint of purely predicting program output, the approaches described in this paper are not capable of representing arbitrary computation in the sense of the Church-Turing thesis. However, there has been recent progress in the deep learning community towards models capable of simulating Turing machines (Graves et al., 2014). While this \u201cNeural Turing Machines\u201d line of work approaches quite a different problem than our own, we remark that such expressive representations may indeed be important for statistical reasoning with arbitrary code databases.\nFor the time being, feature embeddings of code can at least be learned using the massive online education datasets that have only recently become available. And we believe that these features will be useful in a variety of ways \u2014 not just in propagating feedback, but also in tasks such as predicting future struggles and even student dropout."}, {"heading": "Acknowledgments", "text": "We would like to thank Kevin Murphy, John Mitchell, Vova Kim, Roland Angst, Steve Cooper and Justin Solomon for their critical feedback and useful discussions. We appreciate the generosity of the Code.Org team, especially Nan Li and Ellen Spertus, who providing data and support. Chris is supported by NSF-GRFP grant number DGE-114747."}], "references": [{"title": "Powergrading: a clustering approach to amplify human effort for short answer grading", "author": ["Basu", "Sumit", "Jacobs", "Chuck", "Vanderwende", "Lucy"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Basu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2013}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "James", "Bengio", "Yoshua"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Can recursive neural tensor networks learn logical reasoning", "author": ["Bowman", "Samuel R"], "venue": "arXiv preprint arXiv:1312.6192,", "citeRegEx": "Bowman and R.,? \\Q2013\\E", "shortCiteRegEx": "Bowman and R.", "year": 2013}, {"title": "Divide and correct: Using clusters to grade short answers at scale", "author": ["Brooks", "Michael", "Basu", "Sumit", "Jacobs", "Charles", "Vanderwende", "Lucy"], "venue": "In Proceedings of the first ACM conference on Learning@ scale conference,", "citeRegEx": "Brooks et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brooks et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Learning taskdependent distributed representations by backpropagation through structure", "author": ["Goller", "Christoph", "Kuchler", "Andreas"], "venue": "In Neural Networks,", "citeRegEx": "Goller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Goller et al\\.", "year": 1996}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "An axiomatic basis for computer programming", "author": ["Hoare", "Charles Antony Richard"], "venue": "Communications of the ACM,", "citeRegEx": "Hoare and Richard.,? \\Q1969\\E", "shortCiteRegEx": "Hoare and Richard.", "year": 1969}, {"title": "Syntactic and functional variability of a million code submissions in a machine learning mooc", "author": ["Huang", "Jonathan", "Piech", "Chris", "Nguyen", "Andy", "Guibas", "Leonidas J"], "venue": "In The 16th International Conference on Artificial Intelligence in Education (AIED", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "A complexity measure", "author": ["McCabe", "Thomas J"], "venue": "Software Engineering, IEEE Transactions on,", "citeRegEx": "McCabe and J.,? \\Q1976\\E", "shortCiteRegEx": "McCabe and J.", "year": 1976}, {"title": "Domainindependent proximity measures in intelligent tutoring systems", "author": ["Mokbel", "Bassam", "Gross", "Sebastian", "Paassen", "Benjamin", "Pinkwart", "Niels", "Hammer", "Barbara"], "venue": "In Proceedings of the 6th International Conference on Educational Data Mining (EDM),", "citeRegEx": "Mokbel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mokbel et al\\.", "year": 2013}, {"title": "Codewebs: Scalable homework search for massive open online programming courses", "author": ["Nguyen", "Andy", "Piech", "Christopher", "Huang", "Jonathan", "Guibas", "Leonidas"], "venue": "In Proceedings of the 23rd International World Wide Web Conference (WWW", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Functional maps: a flexible representation of maps between shapes", "author": ["Ovsjanikov", "Maks", "Ben-Chen", "Mirela", "Solomon", "Justin", "Butscher", "Adrian", "Guibas", "Leonidas"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "Ovsjanikov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ovsjanikov et al\\.", "year": 2012}, {"title": "Analysis and visualization of maps between shapes", "author": ["Ovsjanikov", "Maks", "Ben-Chen", "Mirela", "Chazal", "Frederic", "Guibas", "Leonidas"], "venue": "In Computer Graphics Forum,", "citeRegEx": "Ovsjanikov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ovsjanikov et al\\.", "year": 2013}, {"title": "Autonomously generating hints by inferring problem solving policies", "author": ["Piech", "Chris", "Sahami", "Mehran", "Huang", "Jonathan", "Guibas", "Leonidas"], "venue": "In Proceedings of the Second", "citeRegEx": "Piech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Piech et al\\.", "year": 2015}, {"title": "ACES: Automatic evaluation of coding style", "author": ["Rogers", "Stephanie", "Garcia", "Dan", "Canny", "John F", "Tang", "Steven", "Kang", "Daniel"], "venue": "PhD thesis,", "citeRegEx": "Rogers et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rogers et al\\.", "year": 2014}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["Song", "Le", "Huang", "Jonathan", "Smola", "Alex", "Fukumizu", "Kenji"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Kernel embeddings of conditional distributions: A unified kernel framework for nonparametric inference in graphical models", "author": ["Song", "Le", "Fukumizu", "Kenji", "Gretton", "Arthur"], "venue": "Signal Processing Magazine,", "citeRegEx": "Song et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Song et al\\.", "year": 2013}, {"title": "Learning to discover efficient mathematical identities", "author": ["Zaremba", "Wojciech", "Kurach", "Karol", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 0, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 11, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 3, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 14, "context": "There have been a number of recent papers (Huang et al., 2013; Basu et al., 2013; Nguyen et al., 2014; Brooks et al., 2014; Lan et al., 2015; Piech et al., 2015) on using large homework submission datasets to improve student feedback.", "startOffset": 42, "endOffset": 161}, {"referenceID": 8, "context": "Some authors have done this without an explicit featurization of the code \u2014 for example, the AST edit distance has been a popular choice (Huang et al., 2013; Rogers et al., 2014).", "startOffset": 137, "endOffset": 178}, {"referenceID": 15, "context": "Some authors have done this without an explicit featurization of the code \u2014 for example, the AST edit distance has been a popular choice (Huang et al., 2013; Rogers et al., 2014).", "startOffset": 137, "endOffset": 178}, {"referenceID": 10, "context": "(Mokbel et al., 2013) explicitly hand engineered a small collection of features on ASTs that are meant to be domainindependent.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "To incorporate functionality, (Nguyen et al., 2014) proposed a method that discovers program modifications that do not appear to change the semantic meaning of code.", "startOffset": 30, "endOffset": 51}, {"referenceID": 18, "context": "Our models are related to recent work from the NLP and deep learning communities on recursive neural networks, particularly for modeling semantics in sentences or symbolic expressions (Socher et al., 2013; 2011; Zaremba et al., 2014; Bowman, 2013).", "startOffset": 184, "endOffset": 247}, {"referenceID": 12, "context": "The computer graphics community have represented pairings of nonlinear geometric shapes as linear maps between shape features, called functional maps (Ovsjanikov et al., 2012; 2013).", "startOffset": 150, "endOffset": 181}, {"referenceID": 17, "context": "From the kernel methods literature, there has also been recent work on representations of conditional probability distributions as operators on a Hilbert space (Song et al., 2013; 2009).", "startOffset": 160, "endOffset": 185}, {"referenceID": 4, "context": "Learning rates are set using Adagrad (Duchi et al., 2011).", "startOffset": 37, "endOffset": 57}, {"referenceID": 11, "context": "Moreover, we extended this baseline by amalgamating functionally equivalent code (Nguyen et al., 2014).", "startOffset": 81, "endOffset": 102}, {"referenceID": 6, "context": "However, there has been recent progress in the deep learning community towards models capable of simulating Turing machines (Graves et al., 2014).", "startOffset": 124, "endOffset": 145}], "year": 2015, "abstractText": "Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University\u2019s CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.", "creator": "LaTeX with hyperref package"}}}