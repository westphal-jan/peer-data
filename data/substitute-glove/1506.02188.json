{"id": "1506.02188", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2015", "title": "Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach", "abstract": "In this press we answer under problem of request needed within has Markov appeal step (MDP) framework several cause when perceptual failures are this along account. Our approach though bring sufficient with failure - sensitive prerequisite - valuation - sunday - levels (CVaR) critical, as minority now a standard determined - stance downside. We refer taking including any part CVaR MDP. Our first contribution still to show that a CVaR objective, various capturing risk tolerance, all his new expression as expected funds however since - criminal designing errors, for gave nevertheless error surpluses. This result, which only of responsible interest, appreciate CVaR MDPs created kind premise structured for mean - unusual much robust move making. Our in brings of decided, comes exceeds values - iteration derivation years CVaR MDPs and replicate country corresponds below. To sense personal, this name the first negotiate diagram before CVaR MDPs that respect score obtaining. Finally, go present predicting leaving numerical experiments likely corroborated sense scholarly investigation which attention where thoroughness of our approach.", "histories": [["v1", "Sat, 6 Jun 2015 19:52:23 GMT  (569kb,D)", "http://arxiv.org/abs/1506.02188v1", "Submitted to NIPS 15"]], "COMMENTS": "Submitted to NIPS 15", "reviews": [], "SUBJECTS": "cs.AI math.OC", "authors": ["yinlam chow", "aviv tamar", "shie mannor", "marco pavone"], "accepted": true, "id": "1506.02188"}, "pdf": {"name": "1506.02188.pdf", "metadata": {"source": "CRF", "title": "Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach", "authors": ["Yinlam Chow", "Aviv Tamar", "Shie Mannor", "Marco Pavone"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Decision making within the Markov decision process (MDP) framework typically involves the minimization of a risk-neutral performance objective, namely the expected total discounted cost [3]. This approach, while very popular, natural, and attractive from a computational standpoint, neither takes into account the variability of the cost (i.e., fluctuations around the mean), nor its sensitivity to modeling errors, which may significantly affect overall performance [12]. Risk-sensitive MDPs [9] address the first aspect by replacing the risk-neutral expectation with a risk-measure of the total discounted cost, such as variance, Value-at-Risk (VaR), or Conditional-VaR (CVaR). Robust MDPs [15], on the other hand, address the second aspect by defining a set of plausible MDP parameters, and optimize decision with respect to the worst-case scenario.\nIn this work we consider risk-sensitive MDPs with a CVaR objective, referred to as CVaR MDPs. CVaR [1, 19] is a risk-measure that is rapidly gaining popularity in \u2217Institute of Computational & Mathematical Engineering, Stanford University. Stanford CA, USA 94305 \u2020Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa, Israel 32000 \u2021Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa, Israel 32000 \u00a7Aeronautics and Astronautics Department, Stanford University, Stanford CA, USA 94305\nar X\niv :1\n50 6.\n02 18\n8v 1\n[ cs\n.A I]\n6 J\nun 2\nvarious engineering applications, e.g., finance, due to its favorable computational properties [1] and superior ability to safeguard a decision maker from the \u201coutcomes that hurt the most\u201d [21]. In this paper, by relating risk to robustness, we derive a novel result that further motivates the usage of a CVaR objective in a decision-making context. Specifically, we show that the CVaR of a discounted cost in an MDP is equivalent to the expected value of the same discounted cost in presence of worst-case perturbations of the MDP parameters (specifically, transition probabilities), provided that such perturbations are within a certain error budget. This result suggests CVaR MDP as a method for decision making under both cost variability and model uncertainty, motivating it as unified framework for planning under uncertainty.\nLiterature review: Risk-sensitive MDPs have been studied for over four decades, with earlier efforts focusing on exponential utility [9], mean-variance [23], and percentile risk criteria [7] . Recently, for the reasons explained above, several authors have investigated CVaR MDPs [19]. Specifically, in [4], the authors propose a dynamic programming algorithm for finite-horizon risk-constrained MDPs where risk is measured according to CVaR. The algorithm is proven to asymptotically converge to an optimal risk-constrained policy. However, the algorithm involves computing integrals over continuous variables (Algorithm 1 in [4]) and, in general, its implementation appears particularly difficult. In [2], the authors investigate the structure of CVaR optimal policies and show that a Markov policy is optimal on an augmented state space, where the additional (continuous) state variable is represented by the running cost. In [8], the authors leverage such result to design an algorithm for CVaR MDPs that relies on discretizing occupation measures in the augmented-state MDP. This approach, however, involves solving a non-convex program via a sequence of linear-programming approximations, which can only shown to converge asymptotically. A different approach is taken by [5] and [24], which consider a finite dimensional parameterization of control policies, and show that a CVaR MDP can be optimized to a local optimum using stochastic gradient descent (policy gradient). A recent result by Pflug and Pichler [17] showed that CVaR MDPs admit a dynamic programming formulation by using a state-augmentation procedure different from the one in [2]. The augmented state is also continuous, making the design of a solution algorithm challenging.\nContributions: The contribution of this paper is twofold. First, as discussed above, we provide a novel interpretation for CVaR MDPs in terms of robustness to modeling errors. This result is of independent interest and further motivates the usage of CVaR MDPs for decision making under uncertainty. Second, we provide a new optimization algorithm for CVaR MDPs, which leverages the state augmentation procedure introduced by Pflug and Pichler [17]. We overcome the aforementioned computational challenges (due to the continuous augmented state) by designing an algorithm that merges approximate value iteration [3] with linear interpolation. Remarkably, we are able to provide explicit error bounds and convergence rates based on contraction-style arguments. In comparison to the algorithms in [4, 8, 5, 24], our approach leads to finite-time error guarantees, with respect to the globally optimal policy. In addition, our algorithm is significantly simpler than previous methods, and calculates the optimal policy for all CVaR confidence intervals and initial states simultaneously. The practicality of our approach is demonstrated in numerical experiments involving planning a path on a grid with thousand of states. To the best of our knowledge, this is the\nfirst algorithm to compute globally-optimal policies for non-trivial CVaR MDPs. Organization: This paper is structured as follows. In Section 2 we provide background on CVaR and MDPs, we state the problem we wish to solve (i.e., CVaR MDPs), and motivate the CVaR MDP formulation by establishing a novel relation between CVaR and model perturbations. Section 3 provides the basis for our solution algorithm, based on a Bellman-style equation for the CVaR. Then, in Section 4 we present our algorithm and correctness analysis. in Section 5 we evaluate our approach via numerical experiments. Finally, in Section 6, we draw some conclusions and discuss directions for future work."}, {"heading": "2 Preliminaries, Problem Formulation, and Motivation", "text": ""}, {"heading": "2.1 Conditional Value-at-Risk", "text": "Let Z be a bounded-mean random variable, i.e., E[|Z|] < \u221e, on a probability space (\u2126,F ,P), with cumulative distribution function F (z) = P(Z \u2264 z). In this paper we interpret Z as a cost. The value-at-risk (VaR) at confidence level \u03b1 \u2208 (0, 1) is the 1\u2212\u03b1 quantile of Z, i.e., VaR\u03b1(Z) = min { z | F (z) \u2265 \u03b1 } . The conditional value-at-risk (CVaR) at confidence level \u03b1 \u2208 (0, 1) is defined as [19]:\nCVaR\u03b1(Z) = min w\u2208R\n{ w + 1 \u03b1 E [ (Z \u2212 w)+ ]} , (1)\nwhere (x)+ = max(x, 0) represents the positive part of x. If there is no probability atom at VaR\u03b1(Z), it is well known that CVaR\u03b1(Z) = E [ Z | Z \u2265 VaR\u03b1(Z) ] . Therefore, CVaR\u03b1(Z) may be interpreted as the worst case expected value of Z, conditioned on the \u03b1-portion of the tail distribution. It is well known that CVaR\u03b1(Z) is decreasing in \u03b1, CVaR1(Z) equals to E(Z), and CVaR\u03b1(Z) tends to max(Z) as \u03b1 \u2193 0. During the last decade, the CVaR risk-measure has gained popularity in financial applications, among others. It is especially useful for controlling rare, but potentially disastrous events, which occur below the 1\u2212 \u03b1 quantile, and are neglected by the VaR [21]. Furthermore, CVaR enjoys desirable axiomatic properties, such as coherence [1]. We refer to [25] for further motivation about CVaR and a comparison with other risk measures such as VaR.\nA useful property of CVaR, which we exploit in this paper, is its alternative dual representation [1]:\nCVaR\u03b1(Z) = max \u03be\u2208UCVaR(\u03b1,P) E\u03be[Z], (2)\nwhere E\u03be[Z] denotes the \u03be-weighted expectation of Z, and the risk envelop UCVaR is given by UCVaR(\u03b1,P) = { \u03be : \u03be(\u03c9) \u2208 [ 0, 1\u03b1 ] , \u222b \u03c9\u2208\u2126 \u03be(\u03c9)P(\u03c9)d\u03c9 = 1 } . Thus, the\nCVaR of a random variable Z may be interpreted as the worst-case expectation of Z, under a perturbed distribution \u03beP.\nIn this paper, we are interested in the CVaR of the total discounted cost in a sequential decision-making setting, as discussed next."}, {"heading": "2.2 Markov Decision Processes", "text": "An MDP is a tuple M = (X ,A, C, P, x0, \u03b3), where X and A are finite state and action spaces; C(x, a) \u2208 [\u2212Cmax, Cmax] is a bounded deterministic cost; P (\u00b7|x, a) is the transition probability distribution; \u03b3 \u2208 [0, 1) is the discounting factor, and x0 is the initial state. (Our results easily generalize to random initial states and random costs.)\nLet the space of admissible histories up to time t beHt = Ht\u22121\u00d7X , for t \u2265 1, and H0 = X . A generic element ht \u2208 Ht is of the form ht = (x0, a0, . . . , xt\u22121, at\u22121, xt). Let \u03a0H,t be the set of all deterministic history-dependent policies with the property that at each time t the control is a function of ht. In other words, \u03a0H,t := { \u00b50 : H0 \u2192\nA, \u00b51 : H1 \u2192 A, . . . , \u00b5t : Ht \u2192 A}|\u00b5j(hj) \u2208 A for all hj \u2208 Hj , 1 \u2264 j \u2264 t }\n. We also let \u03a0H = limt\u2192\u221e\u03a0H,t be the set of all history dependent policies."}, {"heading": "2.3 Problem Formulation", "text": "Let the sequence of random variables Zt denote the stage-wise costs observed along a state/control trajectory in the MDP model, and let C0,T = \u2211T t=0 \u03b3\ntZt denote the total discounted cost up to time T . The risk-sensitive discounted-cost problem we wish to address is as follows:\nmin \u00b5\u2208\u03a0H\nCVaR\u03b1 (\nlim T\u2192\u221e\nC0,T \u2223\u2223\u2223x0, \u00b5) , (3)\nwhere \u00b5 = {\u00b50, \u00b51, . . .} is the policy sequence with actions at = \u00b5t(ht) for t \u2208 {0, 1, . . .}. We refer to problem (3) as CVaR MDP. (One may also consider a related formulation combining mean and CVaR, the details of which are presented in the supplementary material.)\nThe problem formulation in (3) directly addresses the aspect of risk sensitivity, as demonstrated by the numerous applications of CVaR optimization in finance (see, e.g., [20, 11, 6]) and the recent approaches for CVaR optimization in MDPs [4, 8, 5, 24]. In the following, we show a new result providing additional motivation for CVaR MDPs, from the point of view of robustness to modeling errors."}, {"heading": "2.4 Motivation - Robustness to Modeling Errors", "text": "We show a new result relating the CVaR objective in (3) to the worst-case expected discounted-cost in presence of worst-case perturbations of the MDP parameters, where the perturbations are budgeted according to the \u201cnumber of things that can go wrong.\u201d Thus, by minimizing CVaR, the decision maker also guarantees robustness of the policy.\nConsider a trajectory (x0, . . . , xT ) in a finite-horizon MDP problem with transitions Pt(xt|xt\u22121). We explicitly denote the time index of the transition matrices for reasons that will become clear shortly. The total probability of the trajectory is P (x0, . . . , xT ) = P0(x0)P1(x1|x0) \u00b7 \u00b7 \u00b7PT (xT |xT\u22121), and we let C0,T (x1, . . . , xT ) denote its discounted cost, as defined above.\nWe consider an adversarial setting, where an adversary is allowed to change the transition probabilities at each stage, under some budget constraints. We will show that, for a specific budget and perturbation structure, the expected cost under the worstcase perturbation is equivalent to the CVaR of the cost. Thus, we shall establish that,\nin this perspective, being risk sensitive is equivalent to being robust against model perturbations.\nFor each stage 1 \u2264 t \u2264 T , consider a perturbed transition matrix P\u0302t = Pt \u25e6 \u03b4t, where \u03b4t \u2208 RX\u00d7X is a multiplicative probability perturbation and \u25e6 is the Hadamard product, under the condition that P\u0302t is a stochastic matrix. Let \u2206t denote the set of perturbation matrices that satisfy this condition, and let \u2206 = \u22061 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7\u2206T the set of all possible perturbations to the trajectory distribution.\nWe now impose a budget constraint on the perturbations as follows. For some budget \u03b7 \u2265 1, we consider the constraint\n\u03b41(x1|x0)\u03b42(x2|x1) \u00b7 \u00b7 \u00b7 \u03b4T (xT |xT\u22121) \u2264 \u03b7, \u2200x1, . . . , xT \u2208 X , t = 0, . . . , T. (4)\nEssentially, the product in Eq. (4) states that the worst cannot happen at each time. Instead, the perturbation budget has to be split (multiplicatively) along the trajectory. We note that Eq. (4) is in fact a constraint on the perturbation matrices, and we denote by \u2206\u03b7 \u2282 \u2206 the set of perturbations that satisfy this constraint with budget \u03b7. The following result shows an equivalence between the CVaR and the worst-case expected loss.\nProposition 1 (Interpretation of CVaR as a Robustness Measure) It holds\nCVaR 1 \u03b7 (C0,T (x1, . . . , xT )) = sup (\u03b41,...,\u03b4T )\u2208\u2206\u03b7 EP\u0302 [C0,T (x1, . . . , xT )] , (5)\nwhere EP\u0302 [\u00b7] denotes expectation with respect to a Markov chain with transitions P\u0302t.\nThe proof of Proposition 1 is in the supplementary material. It is instructive to compare Proposition 1 with the dual representation of CVaR in (2). Note, in particular, that the perturbation budget in Proposition 1 has a temporal structure, which constrains the adversary from choosing the worst perturbation at each time step.\nRemark 1 An equivalence between robustness and risk-sensitivity was previously suggested by Osogami [16]. In that study, the iterated (dynamic) coherent risk was shown to be equivalent to a robust MDP [10] with a rectangular uncertainty set. The iterated risk (and, correspondingly, the rectangular uncertainty set) is very conservative [26], in the sense that the worst can happen at each time step. In contrast, the perturbations considered here are much less conservative. In general, solving robust MDPs without the rectangularity assumption is NP-hard. Nevertheless, Mannor et. al. [13] showed that, for cases where the number of perturbations to the parameters along a trajectory is upper bounded (budget-constrained perturbation), the corresponding robust MDP problem is tractable. Analogous to the constraint set (1) in [13], the perturbation set in Proposition 1 limits the total number of log-perturbations along a trajectory. Accordingly, we shall later see that optimizing problem (3) with perturbation structure (4) is indeed also tractable.\nNext section provides the fundamental theoretical ideas behind our approach to the solution of (3)."}, {"heading": "3 Bellman Equation for CVaR", "text": "In this section, by leveraging a recent result from [17], we present a dynamic programming (DP) formulation for the CVaR MDP problem in (3). As we shall see, the value function in this formulation depends on both the state and the CVaR confidence level \u03b1. We then establish important properties of such DP formulation, which will later enable us to derive an efficient DP-based approximate solution algorithm and provide correctness guarantees on the approximation error. All proofs are presented in the supplementary material.\nOur starting point is a recursive decomposition of CVaR, whose proof is detailed in Theorem 10 of [17].\nTheorem 2 (CVaR Decomposition Theorem, [17]) For any t \u2265 0, denote by Z = (Zt+1, Zt+2, . . . ) the cost sequence from time t + 1 onwards. The conditional CVaR under policy \u00b5, i.e., CVaR\u03b1(Z | Ht, \u00b5), obeys the following decomposition:\nCVaR\u03b1(Z | Ht, \u00b5) = max \u03be\u2208UCVaR(\u03b1,P (\u00b7|xt,at)) E[\u03be(xt+1)\u00b7CVaR\u03b1\u03be(xt+1)(Z | Ht+1, \u00b5) | Ht, \u00b5],\nwhere at is the action induced by policy \u00b5t(ht), and the expectation is with respect to xt+1. Theorem 2 concerns a fixed policy \u00b5; we now extend it to a general DP formulation. Note that in the recursive decomposition in Theorem 2 the right-hand side involves CVaR terms with different confidence levels than that in the left-hand side. Accordingly, we augment the state space X with an additional continuous state Y = (0, 1], which corresponds to the confidence level. For any x \u2208 X and y \u2208 Y , the valuefunction V (x, y) for the augmented state (x, y) is defined as:\nV (x, y) = min \u00b5\u2208\u03a0H\nCVaRy (\nlim T\u2192\u221e\nC0,T | x0 = x, \u00b5 ) .\nSimilar to standard DP, it is convenient to work with operators defined on the space of value functions [3]. In our case, Theorem 2 leads to the following definition of CVaR Bellman operator T : X \u00d7 Y \u2192 X \u00d7 Y:\nT[V ](x, y) = min a\u2208A\n[ C(x, a) + \u03b3 max\n\u03be\u2208UCVaR(y,P (\u00b7|x,a)) \u2211 x\u2032\u2208X \u03be(x\u2032)V (x\u2032, y\u03be(x\u2032))P (x\u2032|x, a)\n] .\n(6) We now establish several useful properties for the Bellman operator T[V ].\nLemma 3 (Properties of CVaR Bellman Operator) The Bellman operator T[V ] has the following properties:\n1. (Contraction.) \u2016T[V1]\u2212T[V2]\u2016\u221e \u2264 \u03b3\u2016V1\u2212V2\u2016\u221e,where \u2016f\u2016\u221e=supx\u2208X ,y\u2208Y |f(x, y)|.\n2. (Concavity preserving in y.) For any x \u2208 X , suppose yV (x, y) is concave in y \u2208 Y . Then the maximization problem in (6) is concave. Furthermore, yT[V ](x, y) is concave in y.\nThe first property in Lemma 3 is similar to standard DP [3], and is instrumental to the design of a converging value-iteration approach. The second property is nonstandard and specific to our approach. It will be used to show that the computation of value-iteration updates involves concave, and therefore tractable optimization problems. Furthermore, it will be used to show that a linear-interpolation of V (x, y) in the augmented state y has a bounded error.\nEquipped with the results in Theorem 2 and Lemma 3, we can now show that the fixed point solution of T[V ](x, y) = V (x, y) is unique, and equals to the solution of the CVaR MDP problem (3) with x0 = x and \u03b1 = y.\nTheorem 4 (Optimality Condition) For any x \u2208 X and y \u2208 (0, 1], the solution to T[V ](x, y) = V (x, y) is unique, and equals to V \u2217(x, y) = min\u00b5\u2208\u03a0H CVaRy (limT\u2192\u221e C0,T | x0 = x, \u00b5). Next, we show that the optimal value of the CVaR MDP problem (3) can be attained by a stationary Markov policy, defined as a greedy policy with respect to the value function V \u2217(x, y). Thus, while the original problem is defined over the intractable space of history-dependent policies, a stationary Markov policy (over the augmented state space) is optimal, and can be readily derived from V \u2217(x, y). Furthermore, an optimal history-dependent policy can be readily obtained from an (augmented) optimal Markov policy according to the following theorem.\nTheorem 5 (Optimal Policies) Let \u03c0\u2217H = {\u00b50, \u00b51, . . .} \u2208 \u03a0H be a history-dependent policy recursively defined as:\n\u00b5k(hk) = u \u2217(xk, yk), \u2200k \u2265 0, (7)\nwith initial conditions x0 and y0 = \u03b1, and state transitions\nxk \u223c P (\u00b7 | xk\u22121, u\u2217(xk\u22121, yk\u22121)), yk = yk\u22121\u03be\u2217xk\u22121,yk\u22121,u\u2217(xk),\u2200k \u2265 1, (8)\nwhere the stationary Markovian policy u\u2217(x, y) and risk factor \u03be\u2217x,y,u\u2217(\u00b7) are solution to the min-max optimization problem in the CVaR Bellman operator T[V \u2217](x, y). Then, \u03c0\u2217H is an optimal policy for problem (3) with initial state x0 and CVaR confidence level \u03b1.\nTheorems 4 and 5 suggest that a value-iteration DP method [3] can be used to solve the CVaR MDP problem (3). Let an initial value-function guess V0 : X \u00d7 Y \u2192 R be chosen arbitrarily. Value iteration proceeds recursively as follows:\nVk+1(x, y) = T[Vk](x, y), \u2200(x, y) \u2208 X \u00d7 Y, k \u2208 {0, 1, . . . , }. (9)\nSpecifically, by combining the contraction property in Lemma 3 and uniqueness result of fixed point solutions from Theorem 4, one concludes that limk\u2192\u221e Vk(x, y) = V \u2217(x, y). By selecting x = x0 and y = \u03b1, one immediately obtains V \u2217(x0, \u03b1) = min\u00b5\u2208\u03a0H CVaR\u03b1 (limT\u2192\u221e C0,T | x0, \u00b5). Furthermore, an optimal policy may be derived from V \u2217(x, y) according to the policy construction procedure in Theorem 5.\nUnfortunately, while value iteration is conceptually appealing, its direct implementation in our setting is generally impractical since, e.g., the state y is continuous. In the following, we pursue an approximation to the value iteration algorithm (9), based on a linear interpolation scheme for y.\nAlgorithm 1 CVaR Value Iteration with Linear Interpolation"}, {"heading": "1: Given:", "text": "\u2022 N(x) interpolation points Y(x) = { y1, . . . , yN(x) } \u2208 [0, 1]N(x) for every x \u2208\nX with yi < yi+1, y1 = 0 and yN(x) = 1.\n\u2022 Initial value function V0(x, y) that satisfies Assumption 1."}, {"heading": "2: For t = 1, 2, . . .", "text": "\u2022 For each x \u2208 X and each yi \u2208 Y(x), update the value function estimate as follows:\nVt(x, yi) = TI [Vt\u22121](x, yi),\n3: Set the converged value iteration estimate as V\u0302 \u2217(x, yi), for any x \u2208 X , and yi \u2208 Y(x)."}, {"heading": "4 Value Iteration with Linear Interpolation", "text": "In this section we present an approximate DP algorithm for solving CVaR MDPs, based on the theoretical results of Section 3. The value iteration algorithm in Eq. (9) presents two main implementation challenges. The first is due to the fact that the augmented state y is continuous. We handle this challenge by using interpolation, and exploit the concavity of yV (x, y) to bound the error introduced by this procedure. The second challenge stems from the the fact that applying T involves maximizing over \u03be. Our strategy is to exploit the concavity of the maximization problem to guarantee that such optimization can indeed be performed effectively.\nAs discussed, our approach relies on the fact that the Bellman operator T preserves concavity as established in Lemma 3. Accordingly, we require the following assumption for the initial guess V0(x, y),\nAssumption 1 The guess for the initial value function V0(x, y) satisfies the following properties: 1) yV0(x, y) is concave in y \u2208 Y and 2) V0(x, y) is continuous in y \u2208 Y for any x \u2208 X .\nAssumption 1 may easily be satisfied, for example, by choosing V0(x, y) = CVaRy(Z | x0 = x), where Z is any arbitrary bounded random variable. As stated earlier, a key difficulty in applying value iteration (9) is that, for each state x \u2208 X , the Bellman operator has to be calculated for each y \u2208 Y , and Y is continuous. As an approximation, we propose to calculate the Bellman operator only for a finite set of values y, and interpolate the value function in between such interpolation points.\nFormally, let N(x) denote the number of interpolation points. For every x \u2208 X , denote by Y(x) = { y1, . . . , yN(x) } \u2208 [0, 1]N(x) the set of interpolation points. We denote by Ix[V ](y) the linear interpolation of the function yV (x, y) on these points, i.e.,\nIx[V ](y) = yiV (x, yi) + yi+1V (x, yi+1)\u2212 yiV (x, yi)\nyi+1 \u2212 yi (y \u2212 yi),\nwhere yi = max {y\u2032 \u2208 Y(x) : y\u2032 \u2264 y}. The interpolation of yV (x, y) instead of V (x, y) is key to our approach. The motivation is twofold: first, it can be shown [19] that for a discrete random variable Z, yCVaRy(Z) is piecewise linear in y. Second, one can show that the Lipschitzness of y V (x, y) is preserved during value iteration, and exploit this fact to bound the linear interpolation error.\nWe now define the interpolated Bellman operator TI as follows:\nTI [V ](x, y) = min a\u2208A\n[ C(x, a) + \u03b3 max\n\u03be\u2208UCVaR(y,P (\u00b7|x,a)) \u2211 x\u2032\u2208X Ix\u2032 [V ](y\u03be(x\u2032)) y P (x\u2032|x, a)\n] . (10)\nRemark 2 Notice that by L\u2019Hospital\u2019s rule one has limy\u21920 Ix[V ](y\u03be(x))/y = V (x, 0)\u03be(x). This implies that at y = 0 the interpolated Bellman operator is equivalent to the original Bellman operator, i.e., T[V ](x, 0) = mina\u2208A { C(x, a) + \u03b3maxx\u2032\u2208X :P (x\u2032|x,a)>0 V (x \u2032, 0) }\n= TI [V ](x, 0).\nAlgorithm 1 presents CVaR value iteration with linear interpolation. The only difference between this algorithm and standard value iteration (9) is the linear interpolation procedure described above. In the following, we show that Algorithm 1 converges, and bound the error due to interpolation. We begin by showing that the useful properties established in Lemma 3 for the Bellman operator T extend to the interpolated Bellman operator TI .\nLemma 6 (Properties of Interpolated Bellman Operator) TI [V ] has the same properties of T[V ] as in Lemma 3, namely 1) contraction and 2) concavity preservation.\nLemma 6 implies several important consequences for Algorithm 1. The first one is that the maximization problem in (10) is concave, and thus may be solved efficiently at each step. This guarantees that the algorithm is tractable. Second, the contraction property in Lemma 6 guarantees that Algorithm 1 converges, i.e., there exists a value function V\u0302 \u2217 \u2208 R|X |\u00d7|Y| such that limn\u2192\u221eTnI [V0](x, yi) = V\u0302 \u2217(x, yi). In addition, the convergence rate is geometric and equals to \u03b3.\nThe following theorem provides an error bound between approximate value iteration and exact value iteration (3) in terms of the interpolation resolution.\nTheorem 7 (Convergence and Error Bound) Suppose the initial value function V0(x, y) satisfies Assumption 1 and let > 0 be an error tolerance parameter. For any state x \u2208 X and step t \u2265 0, choose y2 > 0 such that Vt(x, y2)\u2212 Vt(x, 0) \u2265 \u2212 and update the interpolation points according to the logarithmic rule: yi+1 = \u03b8yi, \u2200i \u2265 2, with uniform constant \u03b8 \u2265 1. Then, Algorithm 1 has the following error bound:\n0 \u2265 V\u0302 \u2217(x0, \u03b1)\u2212 min \u00b5\u2208\u03a0H\nCVaR\u03b1 (\nlim T\u2192\u221e\nC0,T | x0, \u00b5 ) \u2265 \u2212\u03b3\n1\u2212 \u03b3O ((\u03b8 \u2212 1) + ) ,\nand the following finite time convergence error bound:\u2223\u2223\u2223\u2223TnI [V0](x0, \u03b1)\u2212 min\u00b5\u2208\u03a0H CVaR\u03b1 ( lim T\u2192\u221e C0,T | x0, \u00b5 )\u2223\u2223\u2223\u2223 \u2264 O ((\u03b8 \u2212 1) + ) +O(\u03b3n)1\u2212 \u03b3 .\nTheorem 7 shows that 1) the interpolation-based value function is a conservative estimate for the optimal solution to problem (3); 2) the interpolation procedure is consistent, i.e., when the number of interpolation points is arbitrarily large (specifically, \u2192 0 and yi+1/yi \u2192 1), the approximation error tends to zero; and 3) the approximation error bound is O((\u03b8\u22121)+ ), where log \u03b8 is the log-difference of the interpolation points, i.e., log \u03b8 = log yi+1 \u2212 log yi, \u2200i.\nFor a pre-specified , the condition Vt(x, y2) \u2212 Vt(x, 0) \u2265 \u2212 may be satisfied by a simple adaptive procedure for selecting the interpolation points Y(x). At each iteration t > 0, after calculating Vt(x, yi) in Algorithm 1, at each state x in which the condition does not hold, add a new interpolation point y\u20322 = y2 |Vt(x,y2)\u2212Vt(x,0)| , and additional points between y\u20322 and y2 such that the condition log \u03b8 \u2265 log yi+1 \u2212 log yi is maintained. Since all the additional points belong to the segment [y1, y2], the linearly interpolated Vt(x, yi) remains unchanged, and Algorithm 1 proceeds as is. For bounded costs and > 0, the number of additional points required is bounded.\nThe full proof of Theorem 7 is detailed in the supplementary material; we highlight the main ideas and challenges involved. In the first part of the proof we bound, for all t > 0, the Lipschitz constant of yVt(x, y) in y. The key to this result is to show that the Bellman operator T preserves the Lipschitz property for yVt(x, y). Using the Lipschitz bound and the concavity of yVt(x, y), we then bound the error Ix[Vt](y) y \u2212 Vt(x, y) for all y. The condition on y2 is required for this bound to hold when y \u2192 0. Finally, we use this result to bound \u2016TI [Vt](x, y)\u2212T[Vt](x, y)\u2016\u221e. The results of Theorem 7 follow from contraction arguments, similar to approximate dynamic programming [3]."}, {"heading": "5 Experiments", "text": "We validate Algorithm 1 on a rectangular grid world, where states represent grid points on a 2D terrain map. An agent (e.g., a robotic vehicle) starts in a safe region and its objective is to travel to a given destination. At each time step the agent can move to any of its four neighboring states. Due to sensing and control noise, however, with probability \u03b4 a move to a random neighboring state occurs. The stage-wise cost of each move until reaching the destination is 1, to account for fuel usage. In between the starting point and the destination there are a number of obstacles that the agent should avoid. Hitting an obstacle costs M >> 1 and terminates the mission. The objective is to compute a safe (i.e., obstacle-free) path that is fuel efficient.\nFor our experiments, we choose a 64 \u00d7 53 grid-world (see Figure 1), for a total of 3,312 states. The destination is at position (60, 2), and there are 80 obstacles plotted in yellow. By leveraging Theorem 7, we use 21 log-spaced interpolation points for Algorithm 1 in order to achieve a small value function error. We choose \u03b4 = 0.05, and a discount factor \u03b3 = 0.95 for an effective horizon of 200 steps. Furthermore, we set the penalty cost equal to M = 2/(1 \u2212 \u03b3)\u2013such choice trades off high penalty for collisions and computational complexity (that increases as M increases).\nIn Figure 1 we plot the value function V (x, y) for three different values of the CVaR confidence parameter \u03b1, and the corresponding paths starting from the initial position (60, 50). The first three figures in Figure 1 show how by decreasing the confidence parameter \u03b1 the average travel distance (and hence fuel consumption) slightly increases but the collision probability decreases, as expected. We next discuss robust-\nness to modeling errors. We conducted simulations in which with probability 0.5 each obstacle position is perturbed in a random direction to one of the neighboring grid cells. This emulates, for example, measurement errors in the terrain map. We then trained both the risk-averse (\u03b1 = 0.11) and risk-neutral (\u03b1 = 1) policies on the nominal (i.e., unperturbed) terrain map, and evaluated them on 400 perturbed scenarios (20 perturbed maps with 20 Monte Carlo evaluations each). While the risk-neutral policy finds a shorter route (with average cost equal to 18.137 on successful runs), it is vulnerable to perturbations and fails more often (with over 120 failed runs). In contrast, the risk-averse policy chooses slightly longer routes (with average cost equal to 18.878 on successful runs), but is much more robust to model perturbations (with only 5 failed runs).\nFor the computation of Algorithm 1 we represented the concave piecewise linear maximization problem in (10) as a linear program, and concatenated several problems to reduce repeated overhead stemming from the initialization of the CPLEX linear programming solver. This resulted in a computation time on the order of two hours. We believe there is ample room for improvement, for example by leveraging parallelization and sampling-based methods. Overall, we believe our proposed approach is currently the most practical method available for solving CVaR MDPs (as a comparison, the recently proposed method in [8] involves infinite dimensional optimization). The Matlab code used for the experiments is provided in the supplementary material."}, {"heading": "6 Conclusion", "text": "In this paper we presented an algorithm for CVaR MDPs, based on approximate valueiteration on an augmented state space. We established convergence of our algorithm, and derived finite-time error bounds. These bounds are useful to stop the algorithm at a desired error threshold.\nIn addition, we uncovered an interesting relationship between the CVaR of the total cost and the worst-case expected cost under adversarial model perturbations. In this formulation, the perturbations are correlated in time, and lead to a robustness framework significantly less conservative than the popular robust-MDP framework, where the uncertainty is temporally independent.\nCollectively, our work suggests CVaR MDPs as a unifying and practical framework for computing control policies that are robust with respect to both stochasticity\nand model perturbations. Future work should address extensions to large state-spaces. We conjecture that a sampling-based approximate DP approach [3] should be feasible since, as proven in this paper, the CVaR Bellman equation is contracting (as required by approximate DP methods)."}, {"heading": "A Proofs of Theoretical Results", "text": ""}, {"heading": "A.1 Proof of Proposition 1", "text": "By definition, we have that\nEP\u0302 [C(x1, . . . , xT )] = \u2211\n(x1,...,xT )\nP1(x1)\u03b41(x1) \u00b7 \u00b7 \u00b7PT (xT |xT\u22121)\u03b4T (xT |xT\u22121)C(x1, . . . , xT )\n= \u2211\n(x1,...,xT )\nP (x1, . . . , xT )\u03b41(x1)\u03b42(x2|x1) \u00b7 \u00b7 \u00b7 \u03b4T (xT |xT\u22121)C(x1, . . . , xT )\n. = \u2211 (x1,...,xT ) P (x1, . . . , xT )\u03b4(x1, . . . , xT )C(x1, . . . , xT ).\nNote that by definition of the set \u2206, for any (\u03b41, . . . , \u03b4T ) \u2208 \u2206 we have thatP (x1, . . . , xT ) > 0\u2192 \u03b4(x1, . . . , xT ) \u2265 0, and\nE [\u03b4(x1, . . . , xT )] . = \u2211 (x1,...,xT ) P (x1, . . . , xT )\u03b4(x1, . . . , xT ) = 1.\nThus,\nsup (\u03b41,...,\u03b4T )\u2208\u2206\u03b7 EP\u0302 [C(x1, . . . , xT )] = sup 0\u2264\u03b4(x1,...,xT )\u2264\u03b7, E[\u03b4(x1,...,xT )]=1 \u2211 (x1,...,xT ) P (x1, . . . , xT )\u03b4(x1, . . . , xT )C(x1, . . . , xT )\n= CVaR 1 \u03b7 (C(x1, . . . , xT )) ,\nwhere the last equality is by the representation theorem for CVaR [22]."}, {"heading": "A.2 Proof of Lemma 3", "text": "The proof of monotonicity and constant shift properties follow directly from the definitions of the Bellman operator, by noting that \u03be(x\u2032)P (x\u2032|x, a) is non-negative and\u2211 x\u2032\u2208X \u03be(x\n\u2032)P (x\u2032|x, a)] = 1 for any \u03be \u2208 UCVaR(y, P (\u00b7|x, a)). For the contraction property, denote c = \u2016V1 \u2212 V2\u2016\u221e. Since\nV2(x, y)\u2212 \u2016V1 \u2212 V2\u2016\u221e \u2264 V1(x, y) \u2264 V2(x, y) + \u2016V1 \u2212 V2\u2016\u221e, \u2200x \u2208 X , y \u2208 Y,\nby monotonicity and constant shift property,\nT[V2](x, y)\u2212\u03b3\u2016V1\u2212V2\u2016\u221e \u2264 T[V1](x, y) \u2264 T[V2](x, y)+\u03b3\u2016V1\u2212V2\u2016\u221e \u2200x \u2208 X , y \u2208 Y.\nThis further implies that\n|T[V1](x, y)\u2212T[V2](x, y)| \u2264 \u03b3\u2016V1 \u2212 V2\u2016\u221e \u2200x \u2208 X , y \u2208 Y\nand the contraction property follows.\nNow, we prove the concavity preserving property. Assume that yV (x, y) is concave in y for any x \u2208 X . Let y1, y2 \u2208 Y , and \u03bb \u2208 [0, 1], and define y\u03bb = (1\u2212 \u03bb)y1 + \u03bby2. We have\n(1\u2212 \u03bb)y1T[V ](x, y1) + \u03bby2T[V ](x, y2)\n=(1\u2212 \u03bb)y1 min a1\u2208A\n[ C(x, a1) + \u03b3 max\n\u03be1\u2208UCVaR(y1,P (\u00b7|x,a1)) \u2211 x\u2032\u2208X \u03be1(x \u2032)V (x\u2032, y1\u03be1(x \u2032))P (x\u2032|x, a1)\n]\n+ \u03bby2 min a2\u2208A\n[ C(x, a2) + \u03b3 max\n\u03be2\u2208UCVaR(y2,P (\u00b7|x,a2)) \u2211 x\u2032\u2208X \u03be2(x \u2032)V (x\u2032, y2\u03be2(x \u2032))P (x\u2032|x, a2)\n]\n= min a1\u2208A\n[ (1\u2212 \u03bb)y1C(x, a1) + \u03b3 max\n\u03be1\u2208UCVaR(y1,P (\u00b7|x,a1)) \u2211 x\u2032\u2208X \u03be1(x \u2032)V (x\u2032, y1\u03be1(x \u2032))P (x\u2032|x, a1)(1\u2212 \u03bb)y1\n]\n+ min a2\u2208A\n[ \u03bby2C(x, a2) + \u03b3 max\n\u03be2\u2208UCVaR(y2,P (\u00b7|x,a2)) \u2211 x\u2032\u2208X \u03be2(x \u2032)V (x\u2032, y2\u03be2(x \u2032))P (x\u2032|x, a2)\u03bby2\n]\n\u2264min a\u2208A y\u03bbC(x, a) + \u03b3 max \u03be1\u2208UCVaR(y1,P (\u00b7|x,a)) \u03be2\u2208UCVaR(y2,P (\u00b7|x,a)) \u2211 x\u2032\u2208X P (x\u2032|x, a) ((1\u2212\u03bb)y1\u03be1(x\u2032)V (x\u2032, y1\u03be1(x\u2032)) + \u03bby2\u03be2(x\u2032)V (x\u2032, y2\u03be2(x\u2032)))  \u2264min a\u2208A y\u03bbC(x, a) + \u03b3 max \u03be1\u2208UCVaR(y1,P (\u00b7|x,a)) \u03be2\u2208UCVaR(y2,P (\u00b7|x,a)) \u2211 x\u2032\u2208X P (x\u2032|x, a) ((1\u2212\u03bb)y1\u03be1(x\u2032)+\u03bby2\u03be2(x\u2032))V (x\u2032, ((1\u2212\u03bb)y1\u03be1(x\u2032) + \u03bby2\u03be2(x\u2032)))\n where the first inequality is by concavity of the min, and the second is by the concavity assumption. Now, define \u03be = (1\u2212\u03bb)y1\u03be1+\u03bby2\u03be2y\u03bb . When \u03be1 \u2208 UCVaR(y1, P (\u00b7|x, a)) and\n\u03be2 \u2208 UCVaR(y2, P (\u00b7|x, a)), we have that \u03be \u2208 [ 0, 1y\u03bb ] and \u2211 x\u2032\u2208X \u03be(x\n\u2032)P(x\u2032|x, a) = 1. We thus have\n(1\u2212 \u03bb)y1T[V ](x, y1) + \u03bby2T[V ](x, y2)\n\u2264min a\u2208A\n[ y\u03bbC(x, a) + \u03b3 max\n\u03be\u2208UCVaR(y\u03bb,P (\u00b7|x,a)) \u2211 x\u2032\u2208X P (x\u2032|x, a)y\u03bb\u03be(x\u2032)V (x\u2032, y\u03bb\u03be(x\u2032))\n]\n=y\u03bb min a\u2208A\n[ C(x, a) + \u03b3 max\n\u03be\u2208UCVaR(y\u03bb,P (\u00b7|x,a)) \u2211 x\u2032\u2208X P (x\u2032|x, a)\u03be(x\u2032)V (x\u2032, y\u03bb\u03be(x\u2032))\n] = y\u03bbT[V ](x, y\u03bb).\nFinally, to show that the inner problem in (6) is a concave maximization, we need to show that\n\u039bx,y,a(z) :=\n{ zV (x\u2032, z)P (x\u2032|x, a)/y if y 6= 0\n0 otherwise\nis a concave function in z \u2208 R for any given x \u2208 X , y \u2208 Y and a \u2208 A. Suppose zV (x, z) is a concave function in z. Immediately we can see that \u039bx,y,a(z) is concave in z when y = 0. Also notice that when y \u2208 Y \\ {0}, since the transition probability\nP (x\u2032|x, a) is non-negative, we have the result that \u039bx,y,a(z) is concave in z. This further implies\u2211\nx\u2032\u2208X\nP (x\u2032|x, a) y \u039bx,y,a(y\u03be(x \u2032)) = \u2211 x\u2032\u2208X \u03be(x\u2032)V (x\u2032, y\u03be(x\u2032))P (x\u2032|x, a)\nis concave in \u03be. Furthermore by combining the result with the fact that the feasible set of \u03be is a polytope, we complete the proof of this claim."}, {"heading": "A.3 Proof of Theorem 4", "text": "The first part of the proof is to show that for any (x, y) \u2208 X \u00d7 Y ,\nVn(x, y) := T n[V0](x, y)= min \u00b5\u2208\u03a0M CVaRy (C0,n + \u03b3nV0 | x0 = x, \u00b5) , (11)\nby induction, where the initial condition is (x0, y0) = (x, y) and control action at is induced by \u00b5(xt, yt). For n = 1, we have that V1(x, y) = T[V0](x, y) = min\u00b5\u2208\u03a0M C(x0, a0)+ \u03b3CVaRy (C(x1, a1) + V0(x1) | x0 = x, \u00b5) from definition. By induction hypothesis, assume the above expression holds at n = k. For n = k + 1,\nVk+1(x, y) := T k+1[V0](x, y) = T[Vk](x, y)\n= min a\u2208A C(x, a) + \u03b3 max \u03be\u2208UCVaR(y,P (\u00b7|x,a)) \u2211 x\u2032\u2208X \u03be(x\u2032)Vk ( x\u2032, y\u03be(x\u2032)\ufe38 \ufe37\ufe37 \ufe38\ny\u2032\n) P (x\u2032|x, a)  = min a\u2208A [ C(x, a) + \u03b3 max\n\u03be\u2208UCVaR(y,P (\u00b7|x,a)) \u2211 x\u2032\u2208X \u03be(x\u2032)P (x\u2032|x, a) min \u00b5\u2208\u03a0M CVaRy\u2032 ( C0,k + \u03b3kV0 | x0 = x\u2032, \u00b5\n) ]\n= min a\u2208A\n[ C(x, a) + max \u03be\u2208UCVaR(y,P (\u00b7|x,a)) E\u03be [ min \u00b5\u2208\u03a0M CVaRy1 ( C1,k+1 + \u03b3k+1V0 | x1, \u00b5 ) ]] = min \u00b5\u2208\u03a0M CVaRy ( C0,k+1 + \u03b3k+1V0 | x0 = x, \u00b5 ) ,\n(12)\nwhere the initial state condition is given by (x0, y0) = (x, y). Thus, the equality in (11) is proved by induction.\nThe second part of the proof is to show that V \u2217(x0, y0)=min\u00b5\u2208\u03a0M CVaRy0 (limn\u2192\u221e C0,n | x0, \u00b5). Recall T[V ](x, y) = mina\u2208A C(x, a) + \u03b3max\u03be\u2208UCVaR(y,P (\u00b7|x,a)) E\u03be[V |x, y, a]. Since T is a contraction and V0 is bounded, one obtains\nV \u2217(x, y) = T[V \u2217](x, y) = lim n\u2192\u221e Tn[V0](x, y) = lim n\u2192\u221e Vn(x, y)\nfor any (x, y) \u2208 X \u00d7 Y . The first and the second equality follow directly from Proposition 2.1 and Proposition 2.2 in [3] and the third equality follows from the definition of Vn. Furthermore since V0(x, y) is bounded for any (x, y) \u2208 X \u00d7 Y , the result in (12) implies\n\u2212 lim n\u2192\u221e \u03b3n\u2016V0\u2016\u221e \u2264 V \u2217(x0, y0)\u2212 min \u00b5\u2208\u03a0M\nCVaRy0 (\nlim n\u2192\u221e C0,n | x0, \u00b5 ) \u2264 lim n\u2192\u221e \u03b3n\u2016V0\u2016\u221e.\nTherefore, by taking n \u2192 \u221e, we have just shown that for any (x0, y0) \u2208 X \u00d7 Y , V \u2217(x0, y0) = min\u00b5\u2208\u03a0M CVaRy0 (limn\u2192\u221e C0,n | x0, \u00b5).\nThe third part of the proof is to show that for the initial state x0 and confidence interval y0, we have that\nV \u2217(x0, y0) = min \u00b5\u2208\u03a0H\nCVaRy0 (\nlim n\u2192\u221e\nC0,n | x0, \u00b5 ) .\nAt any (xt, yt) \u2208 X \u00d7 Y , we first define the tth tail-subproblem of problem (3) as follows:\nV(xt, yt)= min \u00b5\u2208\u03a0H\nCVaRyt (\nlim n\u2192\u221e\nCt,n | xt, \u00b5 )\nwhere the tail policy sequence is equal to \u00b5 = {\u00b5t, \u00b5t+1, . . .} and the action is given by aj = \u00b5j(hj) for j \u2265 t. For any history depend policy \u00b5\u0303 \u2208 \u03a0H , we also define the \u00b5\u0303\u2212induced value function as CVaRyt (limn\u2192\u221e Ct,n | xt, \u00b5\u0303) where \u00b5\u0303 = {\u00b5\u0303t, \u00b5\u0303t+1, . . .} and aj = \u00b5\u0303j(hj) for j \u2265 t.\nNow let \u00b5\u2217 be the optimal policy of the above tth tail-subproblem. Clearly, the truncated policy \u00b5\u0303 = {\u00b5\u2217t+1, \u00b5\u2217t+2, . . .} is a feasible policy for the (t+ 1)th tail subproblem at any state xt+1 and confidence interval yt+1:\nmin \u00b5\u2208\u03a0H\nCVaRyt+1 (\nlim n\u2192\u221e\nCt+1,n | xt+1, \u00b5 ) .\nCollecting the above results, for any pair (xt, yt) \u2208 X \u00d7 Y and with at = \u00b5\u2217t (xt) we can write\nV(xt, yt) =C(xt, at) + \u03b3 max \u03be\u2208UCVaR(yt,P (\u00b7|xt,at)) E \u03be(xt+1) \u00b7 CVaRyt+1 ( limn\u2192\u221e Ct+1,n | xt+1, \u00b5\u0303)\ufe38 \ufe37\ufe37 \ufe38 V\u00b5\u0303(xt+1,yt+1),yt+1=yt\u03be(xt+1)  \u2265C(xt, at)+\u03b3 max\n\u03be\u2208UCVaR(yt,P (\u00b7|xt,at)) E\u03be[V(xt+1, yt\u03be(xt+1)) |xt, yt, at]\u2265T[V](xt, yt).\nThe first equality follows from the definition of V(xt, yt) and the decomposition of CVaRs (Theorem 2). The first inequality uses the inequality: V\u00b5\u0303(x, y) \u2265 V(x, y), \u2200(x, y) \u2208 X \u00d7 Y . The second inequality follows from the definition of Bellman operator T.\nOn the other hand, starting at any state xt+1 and confidence interval yt+1, let \u00b5\u2217 = {\u00b5\u2217t+1, \u00b5\u2217t+2, . . .} \u2208 \u03a0H be an optimal policy for the tail subproblem:\nmin \u00b5\u2208\u03a0H\nCVaRyt+1 (\nlim n\u2192\u221e\nCt+1,n | xt+1, \u00b5 ) .\nFor a given pair of (xt, yt) \u2208 X\u00d7Y , construct the \u201cextended\u201d policy \u00b5\u0303 = {\u00b5\u0303t, \u00b5\u0303t+1, . . .} \u2208 \u03a0H as follows:\n\u00b5\u0303t(xt) = u \u2217(xt, yt), and \u00b5\u0303j(hj) = \u00b5\u2217j (hj) for j \u2265 t+ 1,\nwhere u\u2217(xt, yt) is the minimizer of the fixed point equation\nu\u2217(xt, yt) \u2208 argmin a\u2208A C(xt, a)+\u03b3 max \u03be\u2208UCVaR(yt,P (\u00b7|xt,a)) E\u03be[V(xt+1, yt\u03be(xt+1)) |xt, yt, a],\nwith yt is the given confidence interval to the tth tail-subproblem and the transition from yt to yt+1 is given by yt+1 = yt\u03be\u2217(xt+1) where\n\u03be\u2217 \u2208 arg max \u03be\u2208UCVaR(yt,P (\u00b7|xt,a\u2217))\nE [ \u03be(xt+1) \u00b7 CVaRyt\u03be(xt+1) ( lim n\u2192\u221e Ct+1,n | xt+1, \u00b5\u0303 )]\nSince \u00b5\u2217 is an optimal, and a fortiori feasible policy for the tail subproblem (from time t + 1), the policy \u00b5\u0303 \u2208 \u03a0H is a feasible policy for the tail subproblem (from time t): min\u00b5\u2208\u03a0H CVaRyt (limn\u2192\u221e Ct,n | xt, \u00b5). Hence, we can write\nV(xt, yt) \u2264 C(xt, \u00b5\u0303t(xt)) + \u03b3CVaRyt (\nlim n\u2192\u221e\nCt+1,n | xt, \u00b5\u0303 ) .\nHence from the definition of \u00b5\u2217, one easily obtains:\nV(xt, yt)\n\u2264C(xt, u\u2217(xt, yt)) + \u03b3 max \u03be\u2208UCVaR(yt,P (\u00b7|xt,u\u2217(xt,yt)))\nE [ \u03be(xt+1) \u00b7 CVaRyt\u03be(xt+1) ( lim n\u2192\u221e Ct+1,n | xt+1, \u00b5\u0303 ) |xt, yt, u\u2217(xt, yt) ] =C(xt, u\n\u2217(xt, yt)) + \u03b3 max \u03be\u2208UCVaR(yt,P (\u00b7|xt,u\u2217(xt,yt))) E\u03be[V(xt+1, yt\u03be(xt+1)) |xt, yt, u\u2217(xt, yt)]\n=T[V](xt, yt).\nCollecting the above results, we have shown that V is a fixed point solution to V (x, y) = T[V ](x, y) for any (x, y) \u2208 X \u00d7Y . Since the fixed point solution is unique, combining both of these arguments implies V \u2217(x, y) = V(x, y) for any (x, y) \u2208 X\u00d7Y . Therefore, it follows that with initial state (x, y), we have V \u2217(x, y) = V(x, y) = min\u00b5\u2208\u03a0H CVaRy (limT\u2192\u221e C0,T | x0 = x, \u00b5).\nCombining the above three parts of the proof, the claims of this theorem follows."}, {"heading": "A.4 Proof of Theorem 5", "text": "Similar to the definition of the optimal Bellman operator T, for any augmented stationary Markovin policy u : X \u00d7 Y \u2192 A, we define the policy induced Bellman operator Tu as\nTu[V ](x, y) = C(x, u(x, y))+\u03b3 max \u03be\u2208UCVaR(y,P (\u00b7|x,u(x,y))) \u2211 x\u2032\u2208X \u03be(x\u2032)V (x\u2032, y\u03be(x\u2032))P (x\u2032|x, u(x, y)).\nAnalogous to Theorem 4, we can easily show that the fixed point solution to Tu[V ](x, y) = V (x, y) is unique and the CVaR decomposition theorem (Theorem 2) further implies this solution equals to\nCVaRy (\nlim T\u2192\u221e\nC0,T | x0 = x, uH ) ,\nwhere the history dependent policy \u03c0H = {\u00b50, \u00b51, . . .} is given by \u00b5k(hk) = u(xk, yk) for any k \u2265 0, with initial states x0, y0 = \u03b1, state transitions (8), but with augmented stationary Markovian policy u\u2217 replaced by u.\nTo complete the proof of this theorem, we need to show that the augmented stationary Markovian policy u\u2217 is optimal if and only if\nT[V \u2217](x, y) = Tu\u2217 [V \u2217](x, y), \u2200x \u2208 X , y \u2208 Y, (13)\nwhere V \u2217(x, y) is the unique fixed point solution of T[V ](x, y) = V (x, y). Here an augmented stationary Markovian policy u\u2217 is optimal if and only if the induced history dependent policy u\u2217H in (7) is optimal to problem (3).\nFirst suppose u\u2217 is an optimal augmented stationary Markvoian policy. Then using the definition of u\u2217 and the result from Theorem 4 that\nV \u2217(x, y) = min \u00b5\u2208\u03a0H\nCVaRy (\nlim T\u2192\u221e\nC0,T | x0 = x, \u00b5 ) ,\nwe immediately show that V \u2217(x, y) = Vu\u2217(x, y). By the fixed point equation T[V \u2217](x, y) = V \u2217(x, y) and Tu\u2217 [Vu\u2217 ](x, y) = Vu\u2217(x, y), this further implies (13) holds.\nSecond suppose u\u2217 satisfies the equality in (13). Then by the fixed point equality T[V \u2217](x, y) = V \u2217(x, y), we immediately obtain the equation V \u2217(x, y) = Tu\u2217 [V \u2217](x, y) for any x \u2208 X and y \u2208 Y . since the fixed point solution to Tu\u2217 [V ](x, y) = V (x, y) is unique, we further show that T[V \u2217](x, y) = V \u2217(x, y) = Vu\u2217(x, y) and Vu\u2217(x, y) = min\u00b5\u2208\u03a0H CVaRy (limT\u2192\u221e C0,T | x0 = x, \u00b5) from Theorem 4. By using the policy construction formula in (7) to obtain the history dependent policy u\u2217H and following the above arguments at which the augmented Markovian stationary policy u is replaced by u\u2217, this further implies\nmin \u00b5\u2208\u03a0H\nCVaRy (\nlim T\u2192\u221e\nC0,T | x0 = x, \u00b5 ) = CVaRy (\nlim T\u2192\u221e\nC0,T | x0 = x, u\u2217H ) ,\ni.e., u\u2217 is an optimal augmented stationary Markovian policy."}, {"heading": "A.5 Proof of Lemma 6", "text": "We first proof the monotonicity property. Based on the definition of Ix[V ](y), if V1(x, y) \u2265 V2(x, y) \u2200x \u2208 X and y \u2208 Y , we have that\nIx[V1](y) = yi+1V1(x, yi+1)(y \u2212 yi) + yiV1(x, yi)(yi+1 \u2212 y)\nyi+1 \u2212 yi , if y \u2208 Ii(x).\nSince yi, yi+1 \u2208 Y and (yi+1 \u2212 y), (y \u2212 yi) \u2265 0 (because y \u2208 Ii(x)), we can easily see that Ix[V1](y) \u2265 Ix[V2](y). As y \u2208 Y and \u03be(\u00b7)P (\u00b7|x, a) \u2265 0 for any \u03be \u2208 UCVaR(y, P (\u00b7|x, a), this further implies TI [V1](x, y) \u2265 TI [V2](x, y).\nNext we prove the constant shift property. Note from the definition of Ix[V ](y)\nthat\nIx[V +K](y)\n=yi(V (x, yi) +K) + yi+1(V (x, yi+1) +K)\u2212 yi(V (x, yi) +K)\nyi+1 \u2212 yi (y \u2212 yi), if y \u2208 Ii(x),\n=yK + yiV (x, yi) + yi+1V (x, yi+1)\u2212 yiV (x, yi)\nyi+1 \u2212 yi (y \u2212 yi), if y \u2208 Ii(x)\n=Ix[V ](y) + yK.\nTherefore by definition of TI [V ](x, y), the constant shift property: TI [V+K](x, y) = TI [V ](x, y) + \u03b3K for any x \u2208 X , y \u2208 Y , follows directly from the above arguments.\nEquipped with both properties in monotonicity and constant shift, the proof of contraction of TI directly follows from the analogous proof in Lemma 3.\nFinally we prove the concavity preserving property. Assume yV (x, y) is concave in y \u2208 Y for any x \u2208 X . Then for yi+2 > yi+1 > yi, \u2200i \u2208 {1, . . . , N(x) \u2212 2} the following inequality immediately follows from the definition of a concave function:\ndIx[V ](y) dy \u2223\u2223\u2223\u2223 y\u2208Ii+1(x) = yi+1V (x, yi+1)\u2212 yiV (x, yi) yi+1 \u2212 yi\n\u2265yi+2V (x, yi+2)\u2212 yi+1V (x, yi+1) yi+2 \u2212 yi+1 = dIx[V ](y) dy \u2223\u2223\u2223\u2223 y\u2208Ii+2(x) .\n(14)\nWe then show that the following inequality in each of the following cases, whenever the slope exists:\nIx[V ](z1) \u2264 Ix[V ](z2) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y=z2 (z1 \u2212 z2), \u2200z1, z2 \u2208 Y \\ {0}.\n(1) There exists i \u2208 {1, . . . , N(x) \u2212 1} such that z1, z2 \u2208 Ii+1(x). In this case we have that\ndIx[V ](y) dy \u2223\u2223\u2223\u2223 y=z1 = dIx[V ](y) dy \u2223\u2223\u2223\u2223 y=z2 ,\nand this further implies\nIx[V ](z1) = Ix[V ](z2) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y=z2 (z1 \u2212 z2).\n(2) There exists i, j \u2208 {1, . . . , N(x) \u2212 2}, i + 1 < j such that z1 \u2208 Ii+1(x) and z2 \u2208 Ij(x). In this case, without loss of generality we assume j = i+ 1. The proof for case: j > i+ 2 is omitted for the sake of brevity, as it can be completed by iteratively applying the same arguments from case: j = i + 2. Since z1 \u2208 Ii(x), z2 \u2208 Ij(x), we have z2 \u2212 z1 \u2265 0 and\ndIx[V ](y) dy \u2223\u2223\u2223\u2223 y=z1 \u2265 dIx[V ](y) dy \u2223\u2223\u2223\u2223 y=z2 .\nBased on the definition of the linear interpolation function, we have that\nIx[V ](yi+1) = yi+1V (x, yi+1) = Ix[V ](yi) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y\u2208Ii+1(x) (yi+1 \u2212 yi).\nFurthermore, combining previous arguments with the definitions of Ix[V ](z1), Ix[V ](z2) implies that for (z2 \u2212 yi+1) \u2265 0,\nIx[V ](z2) =Ix[V ](yi+1) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y=z2 (z2 \u2212 yi+1)\n\u2264Ix[V ](yi+1) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y=z1 (z2 \u2212 yi+1)\n=Ix[V ](yi) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y\u2208Ii+1(x) (z2 \u2212 yi)\n=Ix[V ](z1) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y=z1 (z2 \u2212 z1).\n(3) There exists i, j \u2208 {1, . . . , N(x) \u2212 2}, i + 1 < j such that z2 \u2208 Ii+1(x) and z1 \u2208 Ij(x). In this case, without loss of generality we assume j = i+ 1. The proof for case: j > i+ 2 is omitted for the sake of brevity, as it can be completed by iteratively applying the same arguments from case: j = i + 2. Since z2 \u2208 Ii+1(x), z1 \u2208 Ij(x), we have z1 \u2212 z2 \u2265 0 and\ndIx[V ](y) dy \u2223\u2223\u2223\u2223 y=z1 \u2264 dIx[V ](y) dy \u2223\u2223\u2223\u2223 y=z2 .\nSimilar to the analysis in the previous case, we have that\nIx[V ](yi) = yiV (x, yi) = Ix[V ](yi+1) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y\u2208Ii+1(x) (yi \u2212 yi+1)\nFurthermore, combining previous arguments with the definitions of Ix[V ](z1), Ix[V ](z2) implies that for (z2 \u2212 z1) \u2264 0,\nIx[V ](z2) =Ix[V ](yi) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y=z2 (z2 \u2212 yi)\n=Ix[V ](yi+1) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y=z2 (z2 \u2212 yi+1)\n=Ix[V ](z1) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y=z2 (z2 \u2212 z1)\n\u2264Ix[V ](z1) + dIx[V ](y)\ndy\n\u2223\u2223\u2223\u2223 y=z1 (z2 \u2212 z1).\nThus we have just shown that the first order sufficient condition for concave functions, corresponding to Ix[V ](y), holds, i.e., Ix[V ](y) is concave in y \u2208 Y \\ {0} for\nany given x \u2208 X . Now since Ix[V ](y) is a continuous piecewise linear function in y \u2208 Y and a concave function when the domain is restricted to Y \\ {0}. By continuity this immediately implies that Ix[V ](y) is concave in y \u2208 Y as well. Then following the identical arguments in the proof of Lemma 3 for the concavity preserving property, we can thereby show that\nyTI [V ](x, y) = min a\u2208A\n{ yC(x, a) + max\n\u03be\u2208UCVaR(y,P (\u00b7|x,a)) \u2211 x\u2032\u2208X Ix\u2032 [V ](y\u03be(x\u2032))P (x\u2032|x, a)\n}\nis concave in y \u2208 Y for any given x \u2208 X ."}, {"heading": "A.6 Useful Intermediate Results", "text": "Lemma 8 Let f(y) : [0, 1] \u2192 R be a concave function, differentiable almost everywhere, with Lipschitz constant M . Then the linear interpolation I[f ](y) is also concave, and with Lipschitz constant MI \u2264M .\nProof For every segment [yj , yj+1] in the linear interpolation, f(y) is concave, and with Lipschitz constantM , and I[f ](y) is linear. Also, f(yj) = I[f ](yj), and f(yj+1) = I[f ](yj+1), by definition of the linear interpolation. Denote by cj the magnitude of the slope of I[f ](y) at y \u2208 [yj , yj+1].\nAssume by contradiction that cj > maxy\u2208[yj ,yj+1] |f \u2032(y)| whenever f \u2032(y) exists. Consider the case when f(yj+1) \u2265 f(yj). This implies cj is the slope of the interpolation function I[f ](y) at y \u2208 [yj , yj+1]. Then by the fundamental theorem of calculus, we have\nf(yj+1)\u2212f(yj) = \u222b yj+1 yj f \u2032(y)dy \u2264 \u222b yj+1 yj |f \u2032(y)|dy < \u222b yj+1 yj cjdy = (I[f ](yj+1)\u2212I[f ](yj)),\ncontradicting f(yj+1) = I[f ](yj+1) and f(yj) = I[f ](yj). On the other hand, consider the case when f(yj+1) \u2264 f(yj). This implies \u2212cj is the slope of the interpolation function I[f ](y) at y \u2208 [yj , yj+1]. Again by fundamental theorem of calculus,\n0 \u2264 f(yj+1)\u2212f(yj) = \u222b yj+1 yj f \u2032(y)dy \u2265 \u222b yj+1 yj \u2212|f \u2032(y)|dy > \u222b yj+1 yj \u2212cjdy = I[f ](yj)\u2212I[f ](yj+1).\nSince f(yj+1) = I[f ](yj+1) and f(yj) = I[f ](yj), which implies I[f ](yj)\u2212I[f ](yj+1) \u2265 0, the above expression clearly leads to a contradiction.\nWe finally have that maxy\u2208[yj ,yj+1] |f \u2032(y)| \u2265 cj for segment j \u2208 {1, . . . , N(x) \u2212 1}. As this argument holds for each segment, by maximizing over j over {1, . . . , N(x)\u2212 1}, we have that\nM \u2265 max j\u2208{1,...,N(x)\u22121} max y\u2208[yj ,yj+1] |f \u2032(y)| \u2265 max j\u2208{1,...,N(x)\u22121} cj = MI .\nThe concavity property (thus differentiability almost everywhere) are well-known results of linear interpolation [18].\nLemma 9 Let yV (x, y) be Lipschitz with constant M , concave, and differentiable almost everywhere, for every x \u2208 X and y \u2208 [0, 1]. Then yT[V ](x, y) is also Lipschitz with constant Cmax + \u03b3M .\nProof For any given state-action pair x \u2208 X , and a \u2208 A, let P (x\u2032) = P (x\u2032|x, a) be the transition kernel. Consider the function\nH(y) . = max \u03be\u2208UCVaR(y,P (\u00b7)) \u2211 x\u2032\u2208X y\u03be(x\u2032)V (x\u2032, y\u03be(x\u2032))P (x\u2032).\nNote that, by definition of UCVaR, and a change of variables z(x\u2032) = y\u03be(x\u2032), we can write H(y) as follows:\nH(y) = max 0\u2264z(x\u2032)\u22641,\u2211 x\u2032 P (x \u2032)z(x\u2032)=y\n\u2211 x\u2032\u2208X z(x\u2032)V (x\u2032, z(x\u2032))P (x\u2032). (15)\nThe Lagrangian of the above maximization problem is\nL(z, \u03bb; y) = \u2211 x\u2032\u2208X z(x\u2032)V (x\u2032, z(x\u2032))P (x\u2032)\u2212 \u03bb( \u2211 x\u2032 P (x\u2032)z(x\u2032)\u2212 y).\nSince yV (x, y) is concave, the maximum is attained. By first order optimality condition the following expression holds:\n\u2202L(z, \u03bb; y)\n\u2202z(x\u2032) = P (x\u2032)\n\u2202 [z(x\u2032)V (x\u2032, z(x\u2032))]\n\u2202z(x\u2032) \u2212 \u03bbP (x\u2032) = 0.\nSumming the last expression over x\u2032, we obtain:\u2211 x\u2032\u2208X P (x\u2032) \u2202 [z(x\u2032)V (x\u2032, z(x\u2032))] \u2202z(x\u2032) = \u2211 x\u2032\u2208X \u03bbP (x\u2032) = \u03bb.\nNow, from the Lipschitz property of yV (x, y), we have\u2223\u2223\u2223\u2223\u2223\u2211 x\u2032\u2208X \u03bbP (x\u2032) \u2223\u2223\u2223\u2223\u2223 \u2264 \u2211 x\u2032\u2208X P (x\u2032) \u2223\u2223\u2223\u2223\u2202 [z(x\u2032)V (x\u2032, z(x\u2032))]\u2202z(x\u2032) \u2223\u2223\u2223\u2223 \u2264 \u2211 x\u2032\u2208X P (x\u2032)M = M.\nThus,\n|\u03bb| \u2264 \u2211 x\u2032\u2208X P (x\u2032) \u2223\u2223\u2223\u2223\u2202 [z(x\u2032)V (x\u2032, z(x\u2032))]\u2202z(x\u2032) \u2223\u2223\u2223\u2223 \u2264M.\nNote that the objective in (15) does not depend on y. From the envelope theorem [14], it follows that\ndH(y)\ndy = \u03bb,\ntherefore, H(y) is Lipschitz, with constant M .\nNow, by definition,\nyT[V ](x, y) = min a\u2208A\n[ yC(x, a) + \u03b3 max\n\u03be\u2208UCVaR(y,P (\u00b7|x,a)) \u2211 x\u2032\u2208X y\u03be(x\u2032)V (x\u2032, y\u03be(x\u2032))P (x\u2032|x, a)\n] .\nUsing our Lipschitz result for H(y), we have that for any a \u2208 A, the function\nyC(x, a) + \u03b3 max \u03be\u2208UCVaR(y,P (\u00b7|x,a)) \u2211 x\u2032\u2208X y\u03be(x\u2032)V (x\u2032, y\u03be(x\u2032))P (x\u2032|x, a)\nis Lipschitz in y, with constant C(x, a) +\u03b3M . Using again the envelope theorem [14], we obtain that yT[V ](x, y) is Lipschitz, with constant Cmax + \u03b3M .\nLemma 10 Consider Algorithm 1. Assume that for any x \u2208 X , the initial value function satisfies that yV0(x, y) is Lipschitz (in y), with uniform constant M0. We have that for any t \u2208 {0, 1, . . . , }, the function yVt(x, y) is Lipschitz in y for any x \u2208 X , with Lipschitz constant\nMt = 1\u2212 \u03b3t\n1\u2212 \u03b3 Cmax + \u03b3 tM0 \u2264 Cmax 1\u2212 \u03b3 +M0, \u2200t.\nProof Let TI [V ] denote the application of the Bellman operator T to the linearlyinterpolated version of yV (x, y). We have, by definition, that\nV1(x, y) = TI [V0](x, y).\nUsing Lemma 8 and Lemma 9, we have that V1(x, y) is Lipschitz, with M1 \u2264 Cmax + \u03b3M0.\nNote now, that V2(x, y) = TI [V1](x, y). Thus, by induction, we have\nMt \u2264 1\u2212 \u03b3t\n1\u2212 \u03b3 Cmax + \u03b3\ntM0,\nand the result follows."}, {"heading": "A.7 Proof of Theorem 7", "text": "The proof of this theorem is split into three parts. In the first part, we bound the difference Ix[Vt](y)/y \u2212 Vt(x, y) at each state (x, y) \u2208 X \u00d7 Y using the previous technical lemmas and Lipschitz property.\nIn the second part, we bound the difference of TI [Vt](x, y)\u2212T[Vt](x, y). In the third part we bound the interpolation error using contraction properties of Bellman recursions. First we analyze the bounds for Ix[Vt](y)/y\u2212Vt(x, y) in the following four cases. Notice that from Lemma 10, we have that |dIx[Vt](y)/dy| \u2264 M := Cmax/(1\u2212 \u03b3) + M0.\n(1) When y = 0 (for which y \u2208 I1(x)). Using previous analysis and L\u2019Hospital\u2019s rule we have that limy\u21920 Ix[Vt](y)/y = Vt(x, 0). This further implies limy\u21920 Ix[Vt](y)/y \u2212 Vt(x, 0) = 0. (2) When y \u2208 Ii+1(x), 2 \u2264 i < N(x)\u2212 1. Similar to the inequality in (14), by concavity of yVt(x, y) in y \u2208 Y , we have that\ndIx[Vt](y) dy \u2223\u2223\u2223\u2223 y\u2208Ii+1(x) = yi+1Vt(x, yi+1)\u2212 yiVt(x, yi) yi+1 \u2212 yi \u2264 yVt(x, y)\u2212 yiVt(x, yi) y \u2212 yi ,\nand\ndIx[Vt](y) dy \u2223\u2223\u2223\u2223 y\u2208Ii+2(x) = yi+2Vt(x, yi+2)\u2212 yi+1Vt(x, yi+1) yi+2 \u2212 yi+1 \u2264 yi+1Vt(x, yi+1)\u2212 yVt(x, y) yi+1 \u2212 y .\nFrom the first inequality, for each (x, y) \u2208 X \u00d7 Y we get,\nIx[Vt](y) y \u2212Vt(x, y) \u2264 1 y\n( yiVt(x, yi) +\nyi+1Vt(x, yi+1)\u2212 yiVt(x, yi) yi+1 \u2212 yi\n(y \u2212 yi)\u2212 yVt(x, y) ) \u2264 0.\n(16) On the other hand, rearranging the second inequality gives\n1 y (Ix[Vt](y)\u2212 yVt(x, y))\n\u22651 y\n( yiVt(x, yi) +\ndIx[Vt](y) dy \u2223\u2223\u2223\u2223 y\u2208Ii+1(x) (y \u2212 yi)\u2212 yi+1Vt(x, yi+1)\u2212 dIx[Vt](y) dy \u2223\u2223\u2223\u2223 y\u2208Ii+2(x) (y \u2212 yi+1) )\n=\n( dIx[Vt](y)\ndy\n\u2223\u2223\u2223\u2223 y\u2208Ii+1(x) \u2212 dIx[Vt](y) dy \u2223\u2223\u2223\u2223 y\u2208Ii+2(x) ) y \u2212 yi+1 y \u2265 \u22122M ( yi+1 y \u2212 1 ) .\n(17)\nFurthermore by the Lipschitz property, we also have the following inequality as well:\n1 y (Ix[Vt](y)\u2212 yVt(x, y))\n= yi+1Vt(x, yi+1)(y \u2212 yi) + yiVt(x, yi)(yi+1 \u2212 y)\n(yi+1 \u2212 yi)y \u2212 Vt(x, y)\n\u2265yiVt(x, yi)(y \u2212 yi) + yiVt(x, yi)(yi+1 \u2212 y)\u2212M(yi+1 \u2212 yi)(y \u2212 yi) (yi+1 \u2212 yi)y \u2212 Vt(x, y) = yiVt(x, yi)\u2212M(y \u2212 yi)\ny \u2212 Vt(x, y) \u2265 \u22122M\n( 1\u2212 yi\ny\n) .\n(18)\nCombining the inequalities (17) and (18), the following lower bound for Ix[Vt](y)/y\u2212 Vt(x, y) holds:\n1 y (Ix[Vt](y)\u2212yVt(x, y)) \u2265 \u03b4 := \u22122M min\n{ 1\u2212 yi y , yi+1 y \u2212 1 } , \u2200y \u2208 Ii+1(x), i \u2265 2.\nFrom the above definition, when yi \u2264 y \u2264 (yi + yi+1)/2, the lower bound becomes \u03b4 = \u22122M(1 \u2212 yi/y) and when (yi + yi+1)/2 \u2264 y \u2264 yi+1, the corresponding lower bound is \u03b4 = \u22122M(yi+1/y\u22121). In both cases, \u03b4 is minimized when y = (yi+yi+1)/2. Therefore, the above analysis implies the following lower bound:\n1 y (Ix[Vt](y)\u2212 yVt(x, y)) \u2265 \u22122M yi+1 \u2212 yi yi+1 + yi , \u2200y \u2208 Ii+1(x), i \u2265 2.\nWhen yi+1 = \u03b8yi for i \u2208 {2, . . . , N(x) \u2212 1} for some constant \u03b8 \u2265 1, this further implies that\n1 y (Ix[Vt](y)\u2212 yVt(x, y)) \u2265 \u22122M \u03b8 \u2212 1 \u03b8 + 1 \u2265 \u2212M(\u03b8 \u2212 1), \u2200y \u2208 Y \\ [0, ].\nThen combining the results, here we get the following bound for Ix[Vt](y)/y\u2212Vt(x, y):\n\u2212M(\u03b8 \u2212 1) \u2264 Ix[Vt](y) y \u2212 Vt(x, y) \u2264 0, \u2200y \u2208 Ii+1(x), i \u2265 2.\n(3) When y \u2208 IN(x)(x), i.e., y \u2208 (yN(x)\u22121, 1]. Similar to the proof of case (2), we can show that for any x \u2208 X and y \u2208 IN(x)(x), the same lines of arguments in inequality (16) and (18) hold, which implies\n\u22122M ( 1\u2212 yN(x)\u22121 ) \u2264 \u22122M ( 1\u2212 yN(x)\u22121\ny\n) \u2264 1 y (Ix[Vt](y)\u2212 yVt(x, y)) \u2264 0.\nWhen yN(x) = 1 = \u03b8yN(x)\u22121, this further shows that \u22122MyN(x)\u22121(\u03b8 \u2212 1) = \u22122M ( yN(x) \u2212 yN(x)\u22121 ) \u2264 1 y (Ix[Vt](y)\u2212 yVt(x, y)) \u2264 0,\nand \u22122M(\u03b8 \u2212 1) \u2264 \u22122\n\u03b8 M(\u03b8 \u2212 1) \u2264 1 y (Ix[Vt](y)\u2212 yVt(x, y)) \u2264 0.\n(4) When y \u2208 I2(x), i.e., y \u2208 (0, y2]. From inequality (16), the definition of Ix[Vt](y), we have that\n0 \u2265 Ix[Vt](y)\u2212 yVt(x, y) y = y(Vt(x, y2)\u2212 Vt(x, y)) y = Vt(x, y2)\u2212Vt(x, y) \u2265 Vt(x, y2)\u2212Vt(x, 0).\nThe first inequality is due to the fact that yVt(x, y) is concave in y \u2208 Y for any x \u2208 X , thus the first order condition implies\ny2Vn(x, y2)\u2212 y1Vn(x, y1) y2 \u2212 y1 \u2264 yVn(x, y)\u2212 y1Vn(x, y1) y \u2212 y1 , \u2200y \u2208 I2(x),\nand the last inequality is due to the similar fact that\nVt(x,w) = wVt(x,w)\u2212 0 \u00b7 Vt(x, 0) w \u2212 0 \u2264 zVt(x, z)\u2212 0 \u00b7 Vt(x, 0) z \u2212 0 = Vt(x, z), \u2200z, w \u2208 Y, z \u2264 w.\nTherefore the condition of this theorem implies\n0 \u2265 Ix[Vt](y)\u2212 yVt(x, y) y \u2265 \u2212 , \u2200t \u2265 0, x \u2208 X , y \u2208 Y.\nCombining the above four cases, we have that for each state (x, y) \u2208 X \u00d7 Y ,\n0 \u2265 Ix[Vt](y) y \u2212 Vt(x, y) \u2265 \u22122M(\u03b8 \u2212 1)\u2212 , \u2200t.\nSecond, we bound the difference of TI [Vt](x, y) \u2212 T[Vt](x, y). By recalling that \u03be(\u00b7)P (\u00b7|x, a) is a probability distribution for any \u03be \u2208 UCVaR(y, P (\u00b7|x, a)), we then combine all previous arguments and show that at any t \u2208 {0, 1, . . . , } and any x \u2208 X , a \u2208 A, y \u2208 Y(x),\nmax \u03be\u2208UCVaR(y,P (\u00b7|x,a)) \u2211 x\u2032\u2208X ,\u03be(x\u2032)6=0 ( Ix\u2032 [Vt](y\u03be(x\u2032)) y\u03be(x\u2032) \u2212 Vt(x\u2032, y\u03be(x\u2032)) ) \u03be(x\u2032)P (x\u2032|x, a) \u2265 \u22122M(\u03b8\u22121)\u2212 .\nThis further implies\nT[Vt](x, y)\u2212 \u03b3(2M(\u03b8 \u2212 1) + ) \u2264 TI [Vt](x, y) \u2264 T[Vt](x, y). (19)\nThird, we prove the error bound of interpolation based value iteration using the above properties. By putting t = 0 in (19), we have that\n\u2212\u03b3(2M(\u03b8 \u2212 1) + ) \u2264 TI [V0](x, y)\u2212T[V0](x, y) \u2264 0.\nApplying the Bellman operator T on all sides of the above inequality and noting that T is a translational invariant mapping, the above expression implies\nT2[V0](x, y)\u2212\u03b32(2M(\u03b8\u22121)+ ) \u2264 T[TI [V0]](x, y) = T[V1](x, y) \u2264 T2[V0](x, y).\nBy adding the inequality: \u2212\u03b3(2M(\u03b8 \u2212 1) + ) \u2264 TI [V1](x, y) \u2212T[V1](x, y) \u2264 0 to the above expression, this further implies the following expression:\nT2[V0](x, y)\u2212\u03b3(1+\u03b3)(2M(\u03b8\u22121)+ ) \u2264 TI [V1](x, y) = T2I [V0](x, y) \u2264 T2[V0](x, y).\nThen, by repeating this process, we can show that for any n \u2208 N, the following inequality holds:\nTn[V0](x, y)\u2212 \u03b3 1\u2212 \u03b3n\n1\u2212 \u03b3 (2M(\u03b8 \u2212 1) + ) \u2264 TnI [V0](x, y) \u2264 Tn[V0](x, y).\nNote that when n \u2192 \u221e, we have that \u03b3n converges to 0, Tn[V0](x, y) converges to min\u00b5\u2208\u03a0H CVaRy (limT\u2192\u221e C0,T | x, \u00b5) (follow from Theorem 4) and TnI [V0](x, y) converges to V\u0302 \u2217(x, y) (follow from the contraction property in Lemma 6).\nFurthermore, from Proposition 1.6.4 in [3], the contraction property of Bellman operator T implies that for any x \u2208 X , y \u2208 Y , the following expression holds:\n|Tn[V0](x, y)\u2212 V \u2217(x, y)| \u2264 \u03b3n\n1\u2212 \u03b3 (Cmax + \u2016Z\u2016\u221e)\nwhere Z is the bounded random variable of the initial value function V0(x, y) = CVaRy(Z | x0 = x) such that \u2016V0\u2016\u221e \u2264 \u2016Z\u2016\u221e, and V \u2217(x, y) = min\u00b5\u2208\u03a0H CVaRy (limT\u2192\u221e C0,T | x, \u00b5). This further implies for any x \u2208 X , y \u2208 Y ,\n|TnI [V0](x, y)\u2212 V \u2217(x, y)| \u2264 \u03b3 1\u2212 \u03b3n 1\u2212 \u03b3 (2M(\u03b8 \u2212 1) + ) + \u03b3 n 1\u2212 \u03b3 (Cmax + \u2016Z\u2016\u221e).\nThen, by combining all the above arguments, we prove the claim of this theorem."}, {"heading": "B Trajectory Plots", "text": "In Figure 2 we demonstrate simulated trajectories according to a policy that is greedy w.r.t. the value function, according to Theorem 5."}, {"heading": "C Generalization to Mean-CVaR Optimization", "text": "In this section we extend our approach to MDPs with a mean-CVaR objective of the form:\nmin \u00b5\u2208\u03a0H\n\u03bbE (\nlim T\u2192\u221e\nC0,T | x0, \u00b5 ) + (1\u2212 \u03bb)CVaR\u03b1 (\nlim T\u2192\u221e\nC0,T | x0, \u00b5 ) , (20)\nwhere \u03bb \u2208 [0, 1]. Such an objective is common in practice [11], and is also useful for solving CVaR-constrained objectives using standard Lagrangian methods (see, e.g., [5]).\nNow for any \u03b11, \u03b12 \u2208 [0, 1], define\n\u03c1\u03b1\u0304(Z | Ht, \u00b5) = \u03bbCVaR\u03b11(Z | Ht, \u00b5) + (1\u2212 \u03bb)CVaR\u03b12(Z | Ht, \u00b5)\nand notice that \u03c1\u03b1\u0304(Z | Ht, \u00b5) = \u03bbE (Z | Ht, \u00b5) + (1 \u2212 \u03bb)CVaR\u03b1 (Z | Ht, \u00b5) when the vector of CVaR confidence intervals is given by \u03b1\u0304 = (1, \u03b1).\nTheorem 11 For any t \u2265 0, denote by Z .= (Zt+1, Zt+2, . . . ) the cost sequence from time t + 1 onwards. The conditional mean-CVaR risk metric under policy \u00b5 obeys the following decomposition:\n\u03c1\u03b1\u0304(Z | Ht, \u00b5) = max \u03be\u2208U2CVaR(\u03b1\u0304,P (\u00b7|xt,at)) E[S\u03bb(\u03be(xt+1)) \u00b7 \u03c1\u03b1\u0304S\u03bb(\u03be(xt+1))(Z | Ht+1) | Ht]\nwhere \u03b1\u0304 = (\u03b11, \u03b12) is the vector of CVaR confidence intervals. The risk envelop is given by\nU2CVaR(\u03b1\u0304, P (\u00b7|xt, at))= \u03be = (\u03be1, \u03be2) : \u03bei(xt+1)\u2208 [ 0, 1\n\u03b1i\n] , \u2211\nxt+1\u2208X \u03bei(xt+1)P (xt+1|xt, at) = 1, \u2200i  , S\u03bb(\u03be) : R2 7\u2192 R is a linear operator given by \u03bb\u03be1 + (1 \u2212 \u03bb)\u03be2 and at is the control input induced by policy \u00b5t(ht).\nNow we extend the above analysis to Bellman recursion. With the generic state space Y = [0, 1]2, we now define the optimal Bellman operator at any (x, y) \u2208 X \u00d7 Y ,\nT[V ](x, y) = min a\u2208A\n[ C(x, a) + \u03b3 max\n\u03be\u2208U2CVaR(y,P (\u00b7|x,a)) \u2211 x\u2032\u2208X S\u03bb(\u03be(x \u2032))V (x\u2032, yS\u03bb(\u03be(x \u2032)))P (x\u2032|x, a)\n] .\n(21) Based on the decomposition result from Theorem 11, we now have the result on the convergence of Bellman recursion, analogous to Theorem 4 and 5, showing that the fixed point solution of T[V ](x, y) = V (x, y) is unique and equals to the solution of (20) with x0 = x and y0 = (1, \u03b1).\nTheorem 12 For any state x \u2208 X and y = (y1, y2) \u2208 [0, 1]2, the fixed point solution of T[V ](x, y) = V (x, y) is unique and is equal to V (x, y) := min\u00b5\u2208\u03a0H \u03bbCVaRy1 (limT\u2192\u221e C0,T | x0, \u00b5)+ (1\u2212 \u03bb)CVaRy2 (limT\u2192\u221e C0,T | x0, \u00b5). Furthermore, let \u00b5\u2217 = {\u00b50, \u00b51, . . .} \u2208 \u03a0H be a policy recursively defined as in (7) with two-dimensional augmented state {yj} and initial condition y0 = (1, \u03b1). Then \u00b5\u2217 is an optimal policy for the mean-CVaR problem (20) with initial condition x0 and CVaR confidence level \u03b1.\nExtending the interpolation-based CVaR value iteration (Algorithm 1) for this case is straightforward, using a 2-D linear interpolation for yV (x, y)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>In this paper we address the problem of decision making within a Markov de-<lb>cision process (MDP) framework where risk and modeling errors are taken into<lb>account. Our approach is to minimize a risk-sensitive conditional-value-at-risk<lb>(CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to<lb>such problem as CVaR MDP. Our first contribution is to show that a CVaR objec-<lb>tive, besides capturing risk sensitivity, has an alternative interpretation as expected<lb>cost under worst-case modeling errors, for a given error budget. This result, which<lb>is of independent interest, motivates CVaR MDPs as a unifying framework for<lb>risk-sensitive and robust decision making. Our second contribution is to present<lb>an approximate value-iteration algorithm for CVaR MDPs and analyze its conver-<lb>gence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs<lb>that enjoys error guarantees. Finally, we present results from numerical exper-<lb>iments that corroborate our theoretical findings and show the practicality of our<lb>approach.", "creator": "LaTeX with hyperref package"}}}