{"id": "1506.02632", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control", "abstract": "Cumulative prospect theory (CPT) that prominent though model physical issues well, on improvements probabilistic investigators strong well reason. CPT done on distorting probabilities taking unlike alone governor than beginning versions cuts cab and narrative rate plan. We bring this idea to a worse - sensitive reinforcement learning (RL) setting only design graphical for both estimation being system. The estimation multimillion we anyway enact system first empirical its on certain coming comparison the CPT - value part a specific voltage. We once same this scheme since the part boundary part argued prototyping permit for a Markov declared steps (MDP ). We propose three vertical - based turned well as reactance - free concerns optimizing algorithms. The onetime notable such year - or where 14 - not methods any are international saturday part well - known realtime dynamical convinced of setting regression semantic infinitesimal (SPSA ), while the terms long based on kind reference pricing that complementary close way global optima. Using similar descriptive marketing the the policy capacity in integrated this Kullback - Leibler (KL) mutation to the reference source, nothing get also increased reforms optimization procurement. We offer analyses continuum pledges from besides the funding algorithms.", "histories": [["v1", "Mon, 8 Jun 2015 19:37:55 GMT  (40kb)", "http://arxiv.org/abs/1506.02632v1", null], ["v2", "Sun, 20 Sep 2015 04:19:53 GMT  (46kb,D)", "http://arxiv.org/abs/1506.02632v2", null], ["v3", "Fri, 26 Feb 2016 21:30:04 GMT  (79kb,D)", "http://arxiv.org/abs/1506.02632v3", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["prashanth l a", "cheng jie", "michael c fu", "steven i marcus", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1506.02632"}, "pdf": {"name": "1506.02632.pdf", "metadata": {"source": "CRF", "title": "Cumulative Prospect Theory Meets Reinforcement Learning: Estimation and Control", "authors": ["Prashanth L.A", "Jie Cheng", "Michael Fu", "Steve Marcus"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n02 63\n2v 1\n[ cs\n.L G"}, {"heading": "1 Introduction", "text": "For a random variable X, let pi, i = 1, . . . ,K denote the probability of incurring a gain/loss xi, i = 1, . . . ,K . Given a utility function u and weighting function w, Prospect theory (PT) value is defined as V (X) = \u2211K i=1 u(xi)w(pi). The idea is to take an utility function that is S-shaped, so that it satisfies the diminishing sensitivity property. If we take the weighting function w to be the identity, then one recovers the classic expected utility. A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects). However, PT is lacking in some theoretical aspects as it violates first-order stochastic dominance1.\nCumulative prospect theory (CPT) (Tversky and Kahneman 1992) uses a similar measure as PT, except that the weights are a function of cumulative probabilities. First, separate the gains and losses as\n1Consider the following example from Fennema and Wakker (1997): Suppose there are 20 prospects (outcomes) ranging from \u221210 to 180, each with probability 0.05. If the weight function is such that w(0.05) > 0.05, then it uniformly overweights all low-probability prospects and the resulting PT value is higher than the expected value 85. This violates stochastic dominance, since a shift in the probability mass from bad outcomes did not result in a better prospect.\nx1 \u2264 . . . \u2264 xl \u2264 0 \u2264 xl+1 \u2264 . . . \u2264 xK . Then, the CPT-value is defined as\nV (X) =\nl\u2211\ni=1\nu\u2212(xi) ( w\u2212( i\u2211\nj=1\npj)\u2212 w\u2212( i\u22121\u2211\nj=1\npj) ) + K\u22121\u2211\ni=l+1\nu+(xi) ( w+( K\u2211\nj=i\npj)\u2212 w\u2212( K\u2211\nj=i+1\npj) ) ,\nwhere u+, u\u2212 are utility functions and w+, w\u2212 are weight functions corresponding to gains and losses, respectively. The utility functions u+ and u\u2212 are non-decreasing, while the weight functions are continuous, non-decreasing and have the range [0, 1] with w+(0) = w\u2212(0) = 0 and w+(1) = w\u2212(1) = 1 . Unlike PT, the CPT-value does not violate stochastic dominance2."}, {"heading": "CPT-value estimation.", "text": "With this background, the first aim of this paper is develop a scheme for estimating the CPT-value, given only samples of the random variable X. In particular, we want to estimate the following equivalent form of CPT-value:\nV (X) =\n\u222b +\u221e\n0 w+(P (u+(X)) > x)dx\u2212\n\u222b +\u221e\n0 w\u2212(P (u\u2212(X)) > x)dx. (1)\nWe derive an estimate of the first integral above as follows: first compute the empirical distribution function for u+(X), then compose it with the weight function w+ and finally, integrate the resulting composition to obtain the final estimate. The second integral in (1) is estimated in a similar fashion and the CPT-value estimate is the difference in the estimates of the two integrals in (1). Assuming that the weight functions are Lipschitz, we establish convergence (asymptotic) of our CPT-value estimate to the true CPT-value. We also provide a sample complexity result that establishes that O ( 1 \u01eb2 ) samples are required to be \u01eb-close to the CPT-value with high probability."}, {"heading": "Optimizing CPT-value in MDPs.", "text": "The second aim of this paper is use a performance measure inspired by the CPT-value in a risk-sensitive reinforcement learning setting. In particular, we consider a stochastic shortest path (SSP) problem and instead of the classic expected utility criterion, we employ a weight function to distort the probabilities and then, aim to minimize the CPT-value. For this purpose, we first parameterize the policies such that they are continuously differentiable and then, design policy optimization algorithms in order to find a good-enough policy that optimizes the CPT-value. There are three challenges involved in the design of a policy-optimizing algorithm:\nBiased policy evaluation: For a fixed policy, a finite sample run results in a biased estimate of its CPTvalue, although the bias is bounded.\nSimulation optimization: Given only sample values of the CPT-value for any policy, it is necessary to devise an adaptive search scheme that improves the policy iteratively.\n2In the aforementioned example, increasing w\u2212(0.05) and w+(0.05) does not impact outcomes other than those on the extreme, i.e., \u221210 and 180, respectively. For instance, the weight for outcome 100 would be w+(0.45) \u2212w+(0.40). Thus, CPT formalizes the intuitive notion that humans are sensitive to extreme outcomes and relatively insensitive to intermediate ones.\nNon-dynamic programming: The CPT-value is a non-coherent and non-convex risk measure, unlike traditional objectives such as expected utility and coherent risk measures (e.g. conditional value-at-risk (CVaR)). The implication is that it is non-trivial to develop dynamic programming based algorithms here.\nFor the first problem, we increase the number of samples as the policy update progresses and this gradually takes the bias in CPT-value estimates to zero. Using two well-known ideas from the simulation optimization literature Fu (2015), we propose three policy optimization algorithms that overcome the second and third problems. Our proposed algorithms are summarized as follows:\nGradient-based methods: We propose two algorithms in this class. The first is a policy gradient algorithm that employs simultaneous perturbation stochastic approximation (SPSA)-based estimates for the gradient of the CPT-value, while the second is a policy Newton algorithm that also uses SPSAbased estimates of the gradient and also the Hessian. We remark here that, unlike traditional settings for SPSA, our estimates for CPT-value have a non-zero (albeit bounded) bias. We establish that our algorithms converge to a locally CPT-value optimal policy.\nGradient-free method: We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to devise a globally optimizing policy update scheme. The idea is to use a reference model that eventually concentrates on the global minimum and then empirically approximate this reference distribution well-enough. The latter is achieved via natural exponential families in conjunction with KullbackLeibler (KL) divergence to measure the \u201cdistance\u201d from the reference distribution. Unlike the setting of Chang et al. (2013), we neither observe the objective function (CPT-value) perfectly nor with zero-mean noise. We establish that our algorithm converges to a globally CPT-value optimal policy (assuming it exists)."}, {"heading": "Related work", "text": "Risk-sensitive reinforcement learning (RL) has received a lot of attention recently (cf. Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function. Unlike previous applications of SPSA and the algorithm from Chang et al. (2013), our he policy optimization algorithms suffer from biased estimates from the policy evaluation procedure, where the bias is non-zero and bounded. We overcome the bias asymptotically by slowly increasing the number of samples and show that the resulting policy optimization algorithms converge.\nThe closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)). While the CPT-value (1) that we aim to optimize is based on that in Lin (2013), we extend the latter work in several ways:\n(i) Unlike Lin (2013), we do not assume model information and develop an estimation scheme for the CPT-value function;\n(ii) Further, we also propose control algorithms using SPSA and model-based policy search in order to find a policy that optimizes the CPT-value.\nThe rest of the paper is organized as follows: In Section 2, we describe the empirical distribution based scheme for estimating the CPT-value of any random variable. In Section 4, we present the gradient-based algorithms for optimizing the CPT-value of an MDP. Next, in Section 5, we present a gradient-free modelbased algorithm for CPT-value optimization in an MDP. We provide the proofs of convergence for all the proposed algorithms in Section 6. We present the results from numerical experiments for the CPT-value estimation scheme in Section 7 and finally, provide the concluding remarks in Section 8."}, {"heading": "2 CPT-value estimation", "text": "In traditional settings, one is trying to estimate an expected value by obtaining samples from the distribution w.r.t. which the expectation is taken. However, in our setting, one obtains samples of the underlying random variable X using its distribution, but the CPT-value integral in (1) distorts this distribution using a non-linear weight function w. Thus, one cannot employ classic stochastic approximation schemes Robbins and Monro (1951) in our setting. Earlier works on risk-sensitive RL (cf. Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.\nSince the integral in (1) requires the CDF estimate (over the entire domain), our approach is to use the empirical distribution function (EDF) to approximate the CDF and then perform an integration of the weight-distorted EDF. We establish later in Propositions 1 and 2 that the resulting estimate converges and also with the canonical Monte Carlo convergence rate."}, {"heading": "2.1 Basic algorithm", "text": "Let Xi, i = 1, . . . , n denote n samples of the random variable X. Using conventional notation, we define the empirical distribution function (EDF) for u+(X) and u\u2212(X), for any given real-valued functions u+ and u\u2212, as follows:\nF\u0302+n (x) = 1\nn\nn\u2211\ni=1\n1(u+(Xi)\u2264x), and F\u0302 \u2212 n (x) =\n1\nn\nn\u2211\ni=1\n1(u\u2212(Xi)\u2264x).\nUsing EDFs, the CPT-value (1) is estimated as follows:\nV\u0302n(X) =\n\u222b +\u221e\n0 w+(1\u2212 F\u0302+n (x))dx\u2212\n\u222b +\u221e\n0 w\u2212(1\u2212 F\u0302\u2212n (x))dx. (2)\nNotice that we have substituted 1\u2212 F\u0302+n (x) (resp.1\u2212 F\u0302\u2212n (x)) for P (u+(X) > x) (resp. P (u\u2212(X) > x)) in (1) and then performed an integration of the complementary EDF composed with the weight function."}, {"heading": "2.2 Main results", "text": "Without any additional assumption, the integral in (2) may not even be finite, even though the weight functions w+, w\u2212 are bounded in [0, 1]. In order to overcome this difficulty, we make the following assumption:\nAssumption (A1). The weight functions w+, w\u2212 are Lipschitz with common constant L.\nThe above assumption covers a broad class of functions that are encountered in practice. For the asymptotic rate and sample complexity results below, we require the following assumption:\nAssumption (A2). The utility functions u+(X) and u\u2212(X) are bounded above by M < \u221e. The following result shows that the estimate (2) converges to the true CPT value almost surely and at the (nearly) canonical Monte Carlo asymptotic rate.\nProposition 1. (Asymptotic convergence and rate.) Under (A1), we have\nV\u0302n(X) \u2192 V (X) a.s. as n \u2192 \u221e. (3)\nIn addition, if we assume (A2), then we have\nlim sup n\u2192\u221e\n\u221a n\n2 ln lnn ||V\u0302n(X)\u2212 V (X)||\u221e \u2264 LM a.s.\nProof. Section 6.1.\nWhile the result in Proposition 1 establishes that (2) is unbiased estimate in the asymptotic sense, it is important to know the rate at which the estimate in (2) converges to the CPT-value. The following sample complexity result shows that O ( 1 \u01eb2 ) number of samples are required to be \u01eb-close to the CPT-value in high probability.\nProposition 2. (Sample Complexity) Under (A1) and (A2), for any \u01eb, \u03b4 > 0, we have\nP (|V\u0302n(X)\u2212 V (X)| \u2264 \u01eb) \u2265 1\u2212 \u03b4, for all n \u2265 2L2M2\n\u01eb2 ln\n4 \u03b4 .\nProof. Section 6.1."}, {"heading": "3 CPT-value objective for MDPs", "text": ""}, {"heading": "3.1 Setting", "text": "We consider a stochastic shortest path (SSP) problem with state space X = {0, 1, . . . , l}, with 0 denoting the terminating state that is absorbing. For any x \u2208 S , let A(x) denote the set of actions in state x. Let g(x, a) denote the single-stage cost incurred by choosing action a in state s. A stationary randomized policy \u03c0 maps states to probability distributions over the actions. We parameterize the polices and assume that each policy (identified by its parameter \u03b8) in this set \u0398 \u2208 Rd is continuously differentiable3 . Moreover, we make the standard assumption that all policies in the parameterized class that we consider are proper, i.e., there is a positive probability that the terminal state 0 is reached from any state x \u2208 X . In other words, a proper policy makes the state 0 recurrent and the rest of the states transient for the underlying Markov chain."}, {"heading": "3.2 CPT-value objective", "text": "An episode is a simulated sample path of the SSP that starts in state x0 and ends in the recurrent state 0. Let D\u03c0(x0) be a random variable (r.v) that denotes the total cost from an episode simulated using policy \u03c0, i.e.,\nD\u03c0(x0) = \u03c4\u2211\nm=0\ng(sm, am),\n3In this paper, we use \u03c0 and \u03b8 interchangeably to denote a policy.\nwhere am \u223c \u03c0(\u00b7, sm),\u2200m, i.e., the actions are chosen using policy \u03c0 and \u03c4 is the first passage time to state 0.\nThe traditional objective is to minimize the expected value of the total cost defined above. In this paper, we adopt the CPT approach and aim to minimize the CPT-value function, defined as\nV \u03c0(x0) =\n\u222b +\u221e\n0 w+(P (u+(D\u03c0(x0))) > z)dz \u2212\n\u222b +\u221e\n0 w\u2212(P (u\u2212(D\u03c0(x0))) > z)dz. (4)\nThe first component in (4) relates to the gains, while the second component handles the losses. If we assume that the single-stage rewards are positive (and bounded), then we have only the first component above. If both the weight and utility functions are identity maps, then it is easy to see that (4) is just the traditional value function (i.e., expectation of the return).\nHaving defined the CPT-value, a natural policy search objective is\nmin \u03b8\u2208\u0398\nV \u03b8(x0).\nGiven only biased estimates of the CPT-value function, one requires a scheme for obtaining gradients of the CPT-value in order to descend in the policy parameter \u03b8. We next introduce SPSA-based schemes for this purpose."}, {"heading": "4 Gradient-based algorithms for optimizing CPT-value", "text": ""}, {"heading": "4.1 Policy gradient algorithm (PG-CPT-SPSA)", "text": "Update rule. We update the policy parameter in the descent direction as follows:\n\u03b8n+1 = \u03b8n \u2212 an\u2207\u0302V \u03b8n (x0), (5)\nwhere \u2207\u0302V \u03b8n (x0) is an estimate of the gradient of the CPT-value function (4) and an is a step-size chosen to satisfy (6) below.\nFig. 1 illustrates the overall flow of the policy gradient algorithm based on SPSA, while Algorithm 1 presents the pseudocode."}, {"heading": "Gradient estimation", "text": "Given that we operate in a learning setting and only have biased estimates of the CPT-value from (2), we require a simulation optimization scheme that outputs \u2207\u0302V \u03b8n (x0). Simultaneous perturbation methods are a general class of stochastic gradient schemes that optimize a function given only noisy sample values - see\nAlgorithm 1 Structure of PG-CPT-SPSA algorithm. Input: initial parameter \u03b80, perturbation constants \u03b4n > 0, trajectory lengths {mn}, step-sizes {an}. for n = 0, 1, 2, . . . do\nGenerate {\u2206in, i = 1, . . . , d} using Rademacher distribution, independent of {\u2206m,m = 0, 1, . . . , n\u2212 1}\nPolicy Evaluation (Trajectory 1) Simulate mn episodes of the SSP using policy (\u03b8n + \u03b4n\u2206n)\nObtain CPT-value estimate V\u0302 (\u03b8n+\u03b4n\u2206n)n (x0) using (2)\nPolicy Evaluation (Trajectory 2) Simulate mn episodes of the SSP using policy \u03b8n \u2212 \u03b4n\u2206n Obtain CPT-value estimate V\u0302 \u03b8n+\u03b4n\u2206nn (x 0)) using (2) Policy Improvement (Gradient descent)\nSPSA-based gradient estimate \u2207\u0302V \u03b8(x0) = V\u0302 \u03b8n+\u03b4n\u2206n(x0)\u2212V\u0302 \u03b8n\u2212\u03b4n\u2206n (x0)2\u03b4n \u2206 \u22121 n\nUpdate \u03b8n+1 = \u03b8n \u2212 an\u2207\u0302V \u03b8(x0) end for Return \u03b8n\nBhatnagar et al. (2013) for textbook introduction. SPSA is a well-known scheme that estimates the gradient using two sample values as follows:\n\u2207V \u03b8(x0) \u2248 V\u0302 \u03b8n+\u03b4n\u2206n n (x 0)\u2212 V\u0302 \u03b8n\u2212\u03b4n\u2206nn (x0) 2\u03b4n \u2206\u22121n ,\nwhere \u03b4n is a positive scalar that satisfies (6) below and \u2206\u22121n = ( 1 \u22061n , . . . , 1 \u2206dn )T, where {\u2206in, i = 1, . . . , d, n = 1, 2, . . .} are i.i.d. Rademacher random variables. This idea of using two-point feedback for estimating the gradient has been employed in various settings. Machine learning applications include bandit/stochastic convex optimization - see Hazan (2015), Flaxman et al. (2005), Duchi et al. (2013). However, the idea applies to non-convex functions as well - see Spall (2005), Bhatnagar et al. (2013). The correctness of the gradient estimate is briefly sketched below (see Lemma 7 for a formal proof): By using suitable Taylor\u2019s expansions, we obtain\nV \u03b8n+\u03b4n\u2206n(x0)\u2212 V \u03b8n\u2212\u03b4n\u2206n(x0) 2\u03b4n\u2206in \u2212\u2207iV \u03b8n(x0) = N\u2211\nj=1,j 6=i\n\u2206jn(n) \u2206in(n) \u2207jV \u03b8n(x0)\n\ufe38 \ufe37\ufe37 \ufe38 (I)\n+O(\u03b42n).\nThe conditional expectation of term (I) above is zero since \u2206n are Rademacher. Hence, in expectation, SPSA-based gradient estimate is only an O(\u03b42n) term away from the true gradient.\nJustification for increasing number mn of episodes. Recall that the CPT-value estimation scheme is biased, i.e., providing samples with policy \u03b8n at instant n, we obtain its CPT-value estimate as V \u03b8(x0)+ \u01eb\u03b8n. Here \u01eb\u03b8n denotes the bias. Now, rewrite the update rule (5) as follows:\n\u03b8n+1 = \u03b8n \u2212 an ( (V \u03b8n+\u03b4n\u2206n(x0)\u2212 V \u03b8n\u2212\u03b4n\u2206n(x0))\n2\u03b4n \u2206\u22121n + \u03b7n\n) ,\nwhere \u03b7n = (\u01eb\u03b8n+\u03b4n\u2206nn \u2212 \u01eb\u03b8n\u2212\u03b4n\u2206nn )\n2\u03b4n \u2206\u22121n . Let \u03b6n =\n\u2211n l=0 al\u03b7l. Then, a critical requirement that allows us\nto ignore the bias term \u03b6n is the following condition (cf. Lemma 1 in Chapter 2 of Borkar (2008)):\nsup l\u22650\n(\u03b6n+l \u2212 \u03b6n) \u2192 0 as n \u2192 \u221e.\nWhile Theorems 1\u20132 show that the bias \u01eb\u03b8 is bounded above, it is insufficient to establish convergence of the policy gradient recursion (5) and hence, we increase the number of samples mn so that the bias vanishes asymptotically - see assumption (A3) below for the precise condition on the rate at which mn has to increase.\nAssumption (A3). The step-sizes an and the perturbation constants \u03b4n are positive \u2200n and satisfy\nan, \u03b4n \u2192 0, 1\u221a\nmn\u03b4n \u2192 0 as n \u2192 \u221e,\n\u2211\nn\nan = \u221e and \u2211\nn\na2n \u03b42n < \u221e. (6)\nWhile the conditions on an and \u03b4n are standard for SPSA-based algorithms, the condition on mn is motivated by the earlier discussion. A simple choice that satisfies the above conditions is an = a0/n, mn = m0n\u03bd and \u03b4n = \u03b40/n\u03b3 , for some \u03bd, \u03b3 > 0 with \u03b3 < \u03bd/2."}, {"heading": "Convergence result", "text": "Theorem 1. (Strong convergence) Assume (A1)-(A3). Let \u03b8n be bounded almost surely 4 and let \u03b8\u2217 be an asymptotically stable equilibrium of the ordinary differential equation (ODE): \u03b8\u0307t = \u2212\u2207V \u03b8t(x0), with domain of attraction D(\u03b8\u2217). Then, if there exists a compact subset D of D(\u03b8\u2217) such that \u03b8n visits D infinitely often, we have \u03b8n \u2192 \u03b8\u2217 a.s. as n \u2192 \u221e.\nProof. See Section 6.2."}, {"heading": "4.2 Policy Newton algorithm (PN-CPT-SPSA)", "text": "Need for second order methods. While stochastic gradient descent methods are useful in minimizing the CPT-value given biased estimates, they are sensitive to the choice of the step-size sequence {an}. In particular, for a step-size choice an = a0/n, if a0 is not chosen to be greather than 1/3\u03bbmin(\u22072V \u03b8 \u2217 (x0)), then the optimum rate of convergence is not achieved. Here \u03bbmin denotes the minimum eigenvalue, while \u03b8\u2217 corresponds to the optimal policy (see Theorem 1). A standard approach to overcome this step-size dependency is to use iterate averaging, suggested independently by Polyak Polyak and Juditsky (1992) and Ruppert Ruppert (1991). The idea is to use larger step-sizes an = 1/n\u03b1, where \u03b1 \u2208 (1/2, 1), and then combine it with averaging of the iterates. However, it is well-known that iterate averaging is optimal only in an asymptotic sense, while finite time bounds show that the initial condition is not forgotten sub- exponentially fast (see Theorem 2.2 in Fathi and Frikha (2013)). Thus, it is optimal to average iterates only after a sufficient number of iterations have passed and all the iterates are very close to the optimum. However, the latter situation serves as a stopping condition in practice. An alternative approach is to employ step-sizes of the form an = (a0/n)Mn, where Mn converges to( \u22072V \u03b8\u2217(x0) )\u22121 , i.e., the inverse of the Hessian of the CPT-value at the optimum \u03b8\u2217. Such a scheme gets rid of the step-size dependency (one can set a0 = 1) and still obtains optimal convergence rates. This is the motivation behind having a second-order policy optmization scheme and we next adapt the scheme from Spall (2000) for estimating the Hessian inverse.\n4This is not a restrictive assumption, as one can project to a compact set to ensure boundedness. See the discussion pp. 40-41 of Kushner and Clark (1978) and also remark E.1 of Bhatnagar et al. (2013).\nUpdate rule. A second-order method would update the policy parameter as follows:\n\u03b8n+1 =\u03b8n \u2212 an\u03a5(Hn)\u22121\u2207\u0302V \u03b8n (x0), (7)\nHn = n\nn+ 1 Hn\u22121 +\n1\nn+ 1 H\u0302n, (8)\nwhere \u2207\u0302V \u03b8n (x0) is an estimate of the gradient of the CPT-value function and H\u0302n and Hn denote the Hessian estimate and its smooth counterpart, respectively. Notice that we invert Hn in each iteration, and to ensure that this inversion is feasible (so that the \u03b8-recursion descends), we project Hn onto the set of positive definite matrices using the operator \u03a5. The operator has to be such that asymptotically \u03a5(Hn) should be the same as Hn (since the latter would converge to the true Hessian), while ensuring inversion is feasible in the initial iterations. A simple way is to have \u03a5(Hn) as a diagonal matrix and then add a positive scalar \u03b4n to the diagonal elements so as to ensure invertibility - see Gill et al. (1981), Spall (2000) for a similar operator.\nAlgorithm 2 presents the pseudocode. Fig. 1 illustrates the overall structure of PN-CPT-SPSA, except that three system trajectories with a different perturbation sequence are used.\nAlgorithm 2 Structure of PN-CPT-SPSA algorithm. Input: initial parameter \u03b80, perturbation constants \u03b4n > 0, trajectory lengths {mn}, step-sizes {an}. for n = 0, 1, 2, . . . do\nGenerate {\u2206in, \u2206\u0302in, i = 1, . . . , d} using Rademacher distribution, independent of {\u2206m, \u2206\u0302m,m = 0, 1, . . . , n\u2212 1}\nPolicy Evaluation (Trajectory 1) Simulate mn episodes of the SSP using policy (\u03b8n + \u03b4n(\u2206n + \u2206\u0302n))\nObtain CPT-value estimate V\u0302 (\u03b8n+\u03b4n(\u2206n+\u2206\u0302n))n (x0) using (2)\nPolicy Evaluation (Trajectory 2) Simulate mn episodes of the SSP using policy (\u03b8n \u2212 \u03b4n(\u2206n + \u2206\u0302n)) Obtain CPT-value estimate V\u0302 \u03b8n+\u03b4n(\u2206n+\u2206\u0302n)n (x0)) using (2)\nPolicy Evaluation (Trajectory 3) Simulate mn episodes of the SSP using policy \u03b8n Obtain CPT-value estimate V\u0302 \u03b8nn (x 0)) using (2) Policy Improvement (Newton decrement)\nSPSA-based gradient estimate \u2207\u0302iV \u03b8n (x0) = V\u0302\n\u03b8n+\u03b4n(\u2206n+\u2206\u0302n) n (x0)\u2212 V\u0302 \u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n)n (x0)\n2\u03b4n\u2206in\nSPSA-based Hessian estimate H\u0302n = V\u0302\n\u03b8n+\u03b4n(\u2206n+\u2206\u0302n) n (x0) + V\u0302 \u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n) n (x0)\u2212 2V\u0302 \u03b8nn (x0) \u03b42n\u2206 i n\u2206\u0302 j n\nPolicy update: \u03b8n+1 = \u03b8n \u2212 an\u03a5(Hn)\u22121\u2207\u0302V \u03b8n (x0) Hessian update: Hn = nn+1Hn\u22121 + 1 n+1H\u0302n\nend for Return \u03b8n"}, {"heading": "Gradient and Hessian estimation", "text": "We estimate the Hessian of the CPT-value function using the scheme suggested by Bhatnagar and Prashanth (2015). As in the case of the first-order method, we use Rademacher random variables to simultaneously\nperturb all the coordinates. However, in this case, we require three system trajectories with corresponding policy parameters \u03b8n + \u03b4n(\u2206n + \u2206\u0302n), \u03b8n \u2212 \u03b4n(\u2206n + \u2206\u0302n) and \u03b8n, where {\u2206in, \u2206\u0302in, i = 1, . . . , d, n = 1, 2, . . .} are i.i.d. Rademacher random variables. Using the CPT-value estimates for the aforementioned policy parameters, we estimate the Hessian and the gradient of the CPT-value function as follows: For i, j = 1, . . . , d, set\nGradient: \u2207\u0302iV \u03b8nn (x0) = V\u0302\n\u03b8n+\u03b4n(\u2206n+\u2206\u0302n) n (x0)\u2212 V\u0302 \u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n)n (x0)\n2\u03b4n\u2206in ,\nHessian: H\u0302 i,jn = V\u0302\n\u03b8n+\u03b4n(\u2206n+\u2206\u0302n) n (x0) + V\u0302 \u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n) n (x0)\u2212 2V\u0302 \u03b8nn (x0) \u03b42n\u2206 i n\u2206\u0302 j n .\nNotice that the above estimates require three samples, while the the second order SPSA algorithm proposed first in Spall (2000) required four. Both the gradient estimate \u2207\u0302V \u03b8nn (x0) and the Hessian estimate H\u0302n can be shown to be an O(\u03b42n) term away from the true gradient \u2207V \u03b8n (x0) and Hessian \u22072V \u03b8n (x0), respectively (see Lemmas 8\u20139 in Appendix 6.3)."}, {"heading": "Convergence result", "text": "Theorem 2. (Strong convergence) Assume (A1)-(A3). Let \u03b8n be bounded almost surely, an, \u03b4n satisfy (6) and also \u2211 n 1 (n+1)2\u03b42n\n< \u221e. Further, assume the following: (i) \u2200n, \u03b8, \u2203 \u03c1 > 0 independent of n and \u03b8, such that (\u03b8 \u2212 \u03b8\u2217)T\u03a5(Hn)\u22121\u2207V \u03b8(x0) \u2265 \u03c1 \u2016\u03b8n \u2212 \u03b8\u2217\u2016. (ii) \u03a5(Hn)\u22121 exists a.s. \u2200n, \u03b42n\u03a5(Hn)\u22121 \u2192 0 and E \u2225\u2225\u03a5(H\u0304n)\u22121 \u2225\u22252+\u03b7 \u2264 \u03b10, for some \u03b7, \u03b10 > 0.\nThen, we have \u03b8n \u2192 \u03b8\u2217 and Hn \u2192 \u22072V \u03b8 \u2217 (x0) a.s. as n \u2192 \u221e.\nProof. See Section 6.3."}, {"heading": "5 Gradient-free algorithm for optimizing CPT-value", "text": "We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to our setting of optimizing CPT-value in MDPs. We require that there exists a unique global optimum \u03b8\u2217 for the problem min\u03b8\u2208\u0398 V \u03b8(x0).\nTo illustrate the main idea in the algorithm, assume we know the form of V \u03b8(x0). Then, the idea is to generate a sequence of reference distributions gk(\u03b8) on the policy space \u0398, such that it eventually concentrates on the global optimum \u03b8\u2217. One simple way, suggested in Chapter 4 of Chang et al. (2013) is\ngk(\u03b8) = H(V \u03b8(x0))gk\u22121(\u03b8)\u222b\n\u0398H(V \u03b8 \u2032(x0))gk\u22121(\u03b8\u2032)\u03bd(d\u03b8\u2032)\n, \u2200 \u03b8 \u2208 \u0398, (9)\nwhere \u03bd is the Lebesgue/counting measure on \u0398 and H is a strictly decreasing function. The above construction for gk\u2019s assigns more weight to policies having lower CPT-values and it is easy to show that gk converges to a point-mass concentrated at \u03b8\u2217.\nNext, consider a setting where one can obtain the CPT-value V \u03b8(x0) (without any noise) for any policy \u03b8. In this case, we consider a family of parameterized distributions, say {f(\u00b7, \u03b7), \u03b7 \u2208 C} and incrementally update the distribution parameter \u03b7 such that it minimizes the following KL divergence:\nD(gk, f(\u00b7, \u03b7)) := Egk [ ln gk(R(\u0398)) f(R(\u0398), \u03b7) ] = \u222b \u0398 ln gk(\u03b8) f(\u03b8, \u03b7) gk(\u03b8)\u03bd(d\u03b8),\nwhere R(\u0398) is a random vector taking values in the policy space \u0398. An algorithm to optimize CPT-value in this noise-less setting would perform the following update for the parameter \u03b7n:\n\u03b7n+1 \u2208 argmin \u03b7\u2208C E\u03b7n [ [H(V R(\u0398)(x0))]n f(R(\u0398), \u03b7n) ln f(R(\u0398), \u03b7) ] , (10)\nwhere E\u03b7n [V R(\u0398)(x0)] = \u222b \u0398 V\n\u03b8(x0)f(\u03b8, \u03b7n)\u03bd(d\u03b8). Finally, we get to our setting where we only obtain biased estimate of the CPT-value V \u03b8(x0) for any policy \u03b8. Recall that the bias is due to a finite sample run followed by estimation scheme (2). As in the case of SPSA-based algorithms, it is easy to see that the number of samples mn (in iteration n) should asymptotically increase to infinity. Assuming this setup, the gradient-free model-based policy search algorithm would involve the following steps (see Algorithm 3 for the pseudocode):\nStep 1 (Candidate policies): Generate Nn policy parameters {\u03b81n, . . . , \u03b8Nn} using the distribution f(\u00b7, \u03b7n).\nStep 2 (CPT-value estimation): Run mn SSP episodes for each of the policies in \u03b8in, i = 1, . . . , Nn and return CPT-value estimates V\u0302 \u03b8 i n(x0).\nStep 3 (Parameter update):\n\u03b7n+1 \u2208 argmin \u03b7\u2208C\n1\nNn\nNn\u2211\ni=1\n[H(V\u0302 \u03b8in(x0))]n f(\u03b8in, \u03b7n) ln f(\u03b8in, \u03b7). (11)\nA few remarks are in order.\nRemark 1. (Choice of sampling distribution) A natural question is how to compute the KL-distance (10) in order to update the policy. A related question is how to choose the family of distributions f(\u00b7, \u03b8), so that the update (10) can be done efficiently. One choice is to employ the natural exponential family (NEF) since it ensures that the KL distance in (10) can be computed analytically.\nRemark 2. (Elite sampling) In practice, it is efficient to use only an elite portion of the candidate policies that have been sampled in order to update the sampling distribution f(\u00b7, \u03b7). This can be achieved by using a quantile estimate of the CPT-value function corresponding to candidate policies that were estimated in a particular iteration. The intuition here is that using policies that have performed well guides the policy search procedure towards better regions more efficiently in comparison to an alternative that uses all the candidate policies for updating \u03b7."}, {"heading": "Convergence result", "text": "Theorem 3. Assume (A1)-(A2). Suppose that multivariate normal densities are used for the sampling distribution, i.e., \u03b7n = (\u00b5n,\u03a3n), where \u00b5n and \u03a3n denote the mean and covariance of the normal densities. Then,\nlim n\u2192\u221e\n\u00b5n = \u03b8 \u2217 and lim\nn\u2192\u221e \u03a3n = 0d\u00d7d a.s. (13)\nProof. See Section 6.4.\n1Here V\u0302 \u03b8 (i) n n (x 0) denotes the ith order statistic.\n2Here I\u0303(z, \u03c7) :=\n \n 0 if z \u2264 \u03c7\u2212 \u03b5, (z \u2212 \u03c7+ \u03b5)/\u03b5 if \u03c7\u2212 \u03b5 < z < \u03c7, 1 if z \u2265 \u03c7.\nAlgorithm 3 Structure of gradient free model-based policy optimization algorithm. Input: family of distributions {f(\u00b7, \u03b7)}, initial parameter vector \u03b70 s.t. f(\u03b8, \u03b70) > 0 \u2200 \u03b8 \u2208 \u0398, trajectory lengths {mn}, \u03c10 \u2208 (0, 1], N0 > 1, \u03b5 > 0, \u03b1 > 1, \u03bb \u2208 (0, 1), strictly decreasing function H. for n = 0, 1, 2, . . . do\nCandidate Policies Generate Nn policy parameters using the mixed distribution f\u0303(\u00b7, \u03b7n) = (1\u2212\u03bb)f(\u00b7, \u03b7\u0303n)+\u03bbf(\u00b7, \u03b70). Denote these candidate policies by \u039bn = {\u03b81n, . . . , \u03b8Nn}. CPT-value Estimation for i = 1, 2, . . . , Nn do\nSimulate mn episodes of the SSP using policy \u03b8in Obtain CPT-value estimate V\u0302 \u03b8 i n\nn (x0) using (2) end for\nElite Sampling\nOrder the CPT-value estimates1 {V\u0302 \u03b8(1)nn (x0), . . . , V\u0302 \u03b8 (Nn) n n (x 0)}. Compute the (1\u2212 \u03c1n)-quantile from the above samples as follows:\n\u03c7\u0303n(\u03c1n, Nn) = V\u0302 \u03b8 \u2308(1\u2212\u03c1k)Nk\u2309 n n (x 0). (12)\nThresholding if n = 0 or \u03c7\u0303n(\u03c1n, Nn) \u2265 \u03c7\u0304n\u22121 + \u03b5 then\nSet \u03c7\u0304k = \u03c7\u0303k(\u03c1n, Nn), \u03c1k+1 = \u03c1n, Nk+1 = Nk and Set \u03b8\u2217n = \u03b81\u2212\u03c1n , where \u03b81\u2212\u03c1n is the policy that corresponds to the (1\u2212 \u03c1n)-quantile in (12).\nelse find the largest \u03c1\u0304 \u2208 (0, \u03c1n) such that \u03c7\u0303n(\u03c1\u0304, Nn) \u2265 \u03c7\u0304n\u22121 + \u03b5; if \u03c1\u0304 exists then\nSet \u03c7\u0304n = \u03c7\u0303n(\u03c1\u0304, Nn), \u03c1k+1 = \u03c1\u0304, Nn+1 = Nn and \u03b8\u2217n = \u03b81\u2212\u03c1\u0304 else\nSet \u03c7\u0304n = V\u0302 \u03b8\u2217n\u22121 n (x0),\u03c1n+1 = \u03c1n,Nn+1 = \u2308\u03b1Nn\u2309, and \u03b8\u2217n = \u03b8\u2217n\u22121.\nend if end if\nSampling Distribution Update Parameter update2:\n\u03b7n+1 \u2208 argmin \u03b7\u2208C\n1\nNn\nNn\u2211\ni=1\n[H(V\u0302 \u03b8in(x0))]n f\u0303(\u03b8, \u03b7n) I\u0303 ( V\u0302 \u03b8 i n(x0), \u03c7\u0304n ) ln f(\u03b8, \u03b7).\nend for Return \u03b8n"}, {"heading": "6 Convergence Proofs", "text": ""}, {"heading": "6.1 Proofs for CPT-value estimator", "text": "In order to prove Proposition 1, we require the dominated convergence theorem in its generalized form, which is provided below.\nTheorem 4. (Generalized Dominated Convergence theorem) Let {fn}\u221en=1 be a sequence of measurable functions on E that converge pointwise a.e. on a measurable space E to f . Suppose there is a sequence {gn} of integrable functions on E that converge pointwise a.e. on E to g such that |fn| \u2264 gn for all n \u2208 N. If lim\nn\u2192\u221e\n\u222b E gn = \u222b E g, then limn\u2192\u221e \u222b E fn = \u222b E f .\nProof. This is a standard result that can be found in any textbook on measure theory. For instance, see Theorem 2.3.11 in Athreya and Lahiri (2006)."}, {"heading": "Proof of Proposition 1: Asymptotic convergence", "text": "For notational convenience, we shall henceforth denote u+(X) and u\u2212(X) by U+ and U\u2212, respectively.\nProof. Recall that the CPT-value for any r.v. X is defined as\nV (X) =\n\u222b +\u221e\n0 w+(P (U+ > x)dx\u2212\n\u222b +\u221e\n0 w\u2212(P (U\u2212 > x)dx.\nAlso, recall that we estimate V (X) using the empirical distribution as follows:\nV\u0302n(X) =\n\u222b +\u221e\n0 w+(1\u2212 F\u0302+n (x))dx\u2212\n\u222b +\u221e\n0 w\u2212(1\u2212 F\u0302\u2212n (x))dx, (14)\nwhere\nF\u0302+n (x) = 1\nn\nn\u2211\ni=1\n1(U+\u2264x), and F\u0302 \u2212 n (x) =\n1\nn\nn\u2211\ni=1\n1(U\u2212\u2264x).\nWe first prove the claim for the first integral in (14), i.e., we show\n\u222b +\u221e\n0 w+(1\u2212 F\u0302+n (x))dx \u2192\n\u222b +\u221e\n0 w+(P (U+ > x)dx. (15)\nSince w+ is Lipschitz continuous with constant L, we have almost surely that w+(1\u2212 F\u0302n(x)) \u2264 L(1\u2212 F\u0302n(x)), for all n and w+((P (U+ > x)) \u2264 L \u00b7 (P (U+ > x), since w+(0) = 0.\nNotice that the empirical distribution function F\u0302+n (x) generates a Stieltjes measure which takes mass 1/n on each of the sample points U+i .\nWe have \u222b +\u221e\n0 (P (U+ > x))dx = E(U+)\nand\n\u222b +\u221e\n0 (1\u2212 F\u0302+n (x))dx =\n\u222b +\u221e\n0\n\u222b \u221e\nx dF\u0302n(t)dx. (16)\nSince F\u0302+n (x) has bounded support on R \u2200n, the integral in (16) is finite. Applying Fubini\u2019s theorem to the RHS of (16), we obtain\n\u222b +\u221e\n0\n\u222b \u221e\nx dF\u0302n(t)dx =\n\u222b +\u221e\n0\n\u222b t\n0 dxdF\u0302n(t) =\n\u222b +\u221e\n0 tdF\u0302n(t) =\n1\nn\nn\u2211\ni=1\nU+ [i] , (17)\nwhere U+[i], i = 1, . . . , n denote the order statistics, i.e., U + [1] \u2264 . . . \u2264 U + [n].\nNow, notice that\n1\nn\nn\u2211\ni=1\nU+[i] = 1\nn\nn\u2211\ni=1\nU+i a.s\u2212\u2192 E(U+),\nFrom the foregoing,\nlim n\u2192\u221e\n\u222b +\u221e\n0 L \u00b7 (1\u2212 F\u0302n(x))dx a.s\u2212\u2192\n\u222b +\u221e\n0 L \u00b7 (P (U+ > x))dx.\nHence, we have \u222b \u221e\n0 w(+)(1\u2212 F\u0302n(x))dx a.s.\u2212\u2212\u2192\n\u222b \u221e\n0 w(+)(P (U+) > x)dx.\nThe claim in (15) now follows by invoking the generalized dominated convergence theorem by setting fn = w +(1 \u2212 F\u0302+n (x)) and gn = L \u00b7 (1 \u2212 F\u0302n(x)), and noticing that L \u00b7 (1 \u2212 F\u0302n(x)) a.s.\u2212\u2212\u2192 L(P (U+ > x)) uniformly \u2200x. The latter fact is implied by the Glivenko-Cantelli theorem (cf. Chapter 2 of Wasserman (2015)).\nFollowing similar arguments, it is easy to show that\nw\u2212(1\u2212 F\u0302\u2212n (x))dx \u2192 \u222b +\u221e\n0 w\u2212(P (U\u2212) > x)dx.\nThe final claim regarding convergence of V\u0302n(X) to V (X) now follows.\nProof of Proposition 1: Asymptotic convergence rate\nIn order to prove the convergence rate of the policy optimization algorithms, we need a uniform bound on the distance between Vn(X) and the CPT-value V (X), i.e., ||V\u0302n(X) \u2212 V (X)||\u221e. For this purpose, Proposition 1 had a law of the iterated logarithm type result, which states that ||V\u0302n(X)\u2212 V (X)||\u221e is of the order O(n\u22121/2) (ignoring log-factors) for sufficiently large n. Before proving this result, we recall the law of the iterated logarithm when empirical distribution function is in one dimension (see Van der Vaart (2000) for a detailed description):\nTheorem 5. (Law of the iterated logarithm.) Let F\u0302n denote the empirical distribution and F the true distribution. Then, for sufficiently large n, we have\nlim sup n\u2192\u221e\n\u221a n\n2loglogn ||F\u0302n \u2212 F ||\u221e \u2264\n1 2 , a.s.\nProof. (Proof of Proposition 1: Asymptotic convergence rate)\nLet\u2019s focus on the difference \u2223\u2223\u2223 \u222b +\u221e 0 w +(P (u+((X))) > x)dx\u2212 \u222b +\u221e 0 w +(1\u2212 F\u0302+n (x))dx \u2223\u2223\u2223. We have\n\u2223\u2223\u2223\u2223 \u222b +\u221e\n0 w+(P (U+) > x)dx\u2212\n\u222b +\u221e\n0 w+(1\u2212 F\u0302+n (x))dx\n\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223 \u222b M\n0 w+(P (U+) > x)dx\u2212\n\u222b M\n0 w+(1\u2212 F\u0302+n (x))dx\n\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223 \u222b M\n0 L \u00b7 |P (U+ < x)\u2212 F\u0302+n (x)|dx\n\u2223\u2223\u2223\u2223\n\u2264 LM sup x\u2208R\n\u2223\u2223\u2223P (U+ < x)\u2212 F\u0302+n (x) \u2223\u2223\u2223 .\nUsing the law of iterated logarithm, we obtain\nlim sup n\u2192\u221e\n\u221a n\n2loglogn\n\u2225\u2225\u2225\u2225 \u222b +\u221e\n0 w+(1\u2212 F\u0302+n (x))\u2212\n\u222b +\u221e\n0 P (U+ > x)dx \u2225\u2225\u2225\u2225 \u221e \u2264 1 2 LM, a.s.\nAlong similar lines, we obtain\nlim sup n\u2192\u221e\n\u221a n\n2loglogn\n\u2225\u2225\u2225\u2225 \u222b +\u221e\n0 w\u2212(1\u2212 F\u0302\u2212n (x))\u2212\n\u222b +\u221e\n0 P (U\u2212 > x)dx \u2225\u2225\u2225\u2225 \u221e \u2264 1 2 LM, a.s.\nThe main claim follows by summing the above two inequalities."}, {"heading": "Proof of Proposition 2", "text": "For proving Proposition 2, we require the following well-known inequality that provide a finite-time bound on the distance between empirical distribution and the true distribution:\nLemma 6. (Dvoretzky-Kiefer-Wolfowitz (DKW) inequality) Let F\u0302n(x) = 1n \u2211n i=1 1((Xi)\u2264x) denote the empirical distribution of a r.v. X, with X1, . . . ,Xn being sampled from the true distribution F (X). The, for any n and \u01eb > 0, we have\nP (sup x\u2208R\n|F\u0302n(x)\u2212 F (x)| > \u01eb) \u2264 2e\u22122n\u01eb 2 .\nThe reader is referred to Chapter 2 of Wasserman (2015) for more on empirical distributions in general and DKW inequality in particular.\nProof. (Theorem 2) Since U+ is bounded above by M and w+ is Lipschitz with constant L, we have\n\u2223\u2223\u2223\u2223 \u222b +\u221e\n0 w+(P (U+) > x)dx\u2212\n\u222b +\u221e\n0 w+(1\u2212 F\u0302+n (x))dx\n\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223 \u222b M\n0 w+(P (U+) > x)dx\u2212\n\u222b M\n0 w+(1\u2212 F\u0302+n (x))dx\n\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223 \u222b M\n0 L \u00b7 |P (U+ < x)\u2212 F\u0302+n (x)|dx\n\u2223\u2223\u2223\u2223\n\u2264LM sup x\u2208R\n\u2223\u2223\u2223P (U+ < x)\u2212 F\u0302+n (x) \u2223\u2223\u2223 .\nNow, plugging in the DKW inequality, we obtain\nP (\u2223\u2223\u2223\u2223 \u222b +\u221e\n0 w+(P (U+) > x)dx\u2212\n\u222b +\u221e\n0 w+(1\u2212 F\u0302+n (x))dx\n\u2223\u2223\u2223\u2223 > \u01eb/2 )\n\u2264 P ( LM sup\nx\u2208R\n\u2223\u2223\u2223(P (U+ < x)\u2212 F\u0302+n (x) \u2223\u2223\u2223 > \u01eb/2 ) \u2264 2e\u2212n \u01eb2 2L2M2 . (18)\nAlong similar lines, we obtain\nP (\u2223\u2223\u2223\u2223 \u222b +\u221e\n0 w\u2212(P (U\u2212) > x)dx\u2212\n\u222b +\u221e\n0 w\u2212(1\u2212 F\u0302\u2212n (x))dx\n\u2223\u2223\u2223\u2223 > \u01eb/2 ) \u2264 2e\u2212n \u01eb2 2L2M2 . (19)\nCombining (18) and (19), we obtain P (|V\u0302n(X) \u2212 V (X)| > \u01eb) \u2264 P (\u2223\u2223\u2223\u2223 \u222b +\u221e\n0 w+(P (U+) > x)dx\u2212\n\u222b +\u221e\n0 w+(1\u2212 F\u0302+n (x))dx\n\u2223\u2223\u2223\u2223 > \u01eb/2 )\n+ P (\u2223\u2223\u2223\u2223 \u222b +\u221e\n0 w\u2212(P (U\u2212) > x)dx\u2212\n\u222b +\u221e\n0 w\u2212(1\u2212 F\u0302\u2212n (x))dx\n\u2223\u2223\u2223\u2223 > \u01eb/2 )\n\u2264 4e\u2212n \u01eb2 2L2M2 .\nAnd the claim follows."}, {"heading": "6.2 Proofs for PG-CPT-SPSA", "text": "To prove the main result in Theorem 1, we first show that the gradient estimate using SPSA is only an order O(\u03b42n) term away from the true gradient. The following lemma establishes this claim and its proof can be inferred from Spall (1992), though we give it here for the sake of completeness. Following this lemma, we complete the proof of Theorem 1 by invoking the well-known Kushner-Clark lemma Kushner and Clark (1978) and this involves verfying conditions A2.2.1-A2.2.4 there.\nLemma 7. Let Fn = \u03c3(\u03b8m,m \u2264 n,\u2206m,m < n), n \u2265 1. Then, for any i = 1, . . . , d, we have almost surely,\n\u2223\u2223\u2223\u2223\u2223E [ V\u0302 \u03b8n+\u03b4n\u2206nn (x 0)\u2212 V\u0302 \u03b8n\u2212\u03b4n\u2206nn (x0) 2\u03b4n\u2206in \u2223\u2223\u2223\u2223\u2223Fn ] \u2212\u2207iV \u03b8n(x0) \u2223\u2223\u2223\u2223\u2223 \u2192 0 as n \u2192 \u221e. (20)\nProof. Recall that the CPT-value estimation scheme is biased, i.e., providing samples with policy \u03b8, we obtain its CPT-value estimate as V \u03b8(x0) + \u01eb\u03b8. Here \u01eb\u03b8 denotes the bias.\nWe claim\nE\n[ V\u0302 \u03b8n+\u03b4n\u2206nn (x\n0)\u2212 V\u0302 \u03b8n\u2212\u03b4n\u2206nn (x0) 2\u03b4n\u2206in\n| Fn ] = E [ V \u03b8n+\u03b4n\u2206nn (x\n0)\u2212 V \u03b8n\u2212\u03b4n\u2206nn (x0) 2\u03b4n\u2206in\n| Fn ] + E [\u03b7n | Fn] ,\n(21)\nwhere \u03b7n =\n( \u01eb\u03b8n+\u03b4n\u2206 \u2212 \u01eb\u03b8n\u2212\u03b4n\u2206\n2\u03b4n\n) \u2206\u22121n is the bias arising out of the empirical distribution based CPT-\nvalue estimation scheme. From Proposition 1, we see that, \u01eb\u03b8 = LM \u221a\n2 log logmn mn , provided mn is suffi-\nciently large. Thus, \u03b7n = O (\u221a\n2 log logmn mn 1 \u03b4n ) and since 1\u221amn\u03b4n \u2192 0 by assumption (see (6) in the main\npaper), we have that \u03b7n goes to zero asymptotically. In other words,\nE\n[ V\u0302 \u03b8n+\u03b4n\u2206nn (x\n0)\u2212 V\u0302 \u03b8n\u2212\u03b4n\u2206nn (x0) 2\u03b4n\u2206in\n| Fn ] n\u2192\u221e\u2212\u2212\u2212\u2192 E [ V \u03b8n+\u03b4n\u2206n(x0)\u2212 V \u03b8n\u2212\u03b4n\u2206n(x0)\n2\u03b4n\u2206in | Fn\n] . (22)\nWe now analyse the RHS of the above. By using suitable Taylor\u2019s expansions,\nV \u03b8n+\u03b4n\u2206n(x0) = V \u03b8n(x0) + \u03b4n\u2206 T n\u2207V \u03b8n(x0) + \u03b42\n2 \u2206Tn\u22072V \u03b8n(x0)\u2206n +O(\u03b43n),\nV \u03b8n\u2212\u03b4n\u2206n(x0) = V \u03b8n(x0)\u2212 \u03b4n\u2206Tn\u2207V \u03b8n(x0) + \u03b42\n2 \u2206Tn\u22072V \u03b8n(x0)\u2206n +O(\u03b43n).\nFrom the above, it is easy to see that\nV \u03b8n+\u03b4n\u2206n(x0)\u2212 V \u03b8n\u2212\u03b4n\u2206n(x0) 2\u03b4n\u2206in \u2212\u2207iV \u03b8n(x0) = N\u2211\nj=1,j 6=i\n\u2206jn \u2206in\n\u2207jV \u03b8n(x0) \ufe38 \ufe37\ufe37 \ufe38\n(I)\n+O(\u03b42n).\nTaking conditional expectation on both sides, we obtain\nE\n[ V \u03b8n+\u03b4n\u2206n(x0)\u2212 V \u03b8n\u2212\u03b4n\u2206n(x0)\n2\u03b4n\u2206in | Fn\n] =\u2207iV \u03b8n(x0) + E   N\u2211\nj=1,j 6=i\n\u2206jn \u2206in\n \u2207jV \u03b8n(x0) +O(\u03b42n)\n=\u2207iV \u03b8n(x0) +O(\u03b42n). (23)\nThe first equality above follows from the fact that \u2206n is distributed according to a d-dimensional vector of Rademacher random variables and is independent of Fn. The second inequality follows by observing that \u2206in is independent of \u2206 j n, for any i, j = 1, . . . , d.\nThe claim follows by using the fact that \u03b4n \u2192 0 as n \u2192 \u221e."}, {"heading": "Proof of Theorem 1", "text": "Proof. We first rewrite the update rule (5) as follows:\n\u03b8n+1 = \u03b8n \u2212 an(\u2207V \u03b8n(x0) + \u03b2n + \u03b7n), (24)\nwhere\n\u03b2n = E\n( (V\u0302 \u03b8n+\u03b4n\u2206n(x0)\u2212 V\u0302 \u03b8n\u2212\u03b4n\u2206n(x0))\n2\u03b4n \u2206\u22121n | Fn\n) \u2212\u2207V \u03b8(x0), and\n\u03b7n =\n( V\u0302 \u03b8n+\u03b4n\u2206n(x0)\u2212 V\u0302 \u03b8n\u2212\u03b4n\u2206n(x0)\n2\u03b4n\n) \u2206\u22121n \u2212 E ( (V\u0302 \u03b8n+\u03b4n\u2206n(x0)\u2212 V\u0302 \u03b8n\u2212\u03b4n\u2206n(x0))\n2\u03b4n \u2206\u22121n | Fn\n) .\nIn the above, \u03b2n is the bias in the gradient estimate due to SPSA and \u03b7n is a martingale difference sequence.. Convergence of (24) can be inferred from Theorem 2.3.1 on pp. 29 of Kushner and Clark (1978), provided we verify that the assumptions A2.2.1 to A2.2.3 and A2.2.4\u201d of Kushner and Clark (1978) are satisfied for \u03b8n governed by (5). We verify them below:\n\u2022 A2.2.1 requires that \u2207V \u03b8(x0) is a continuous Rd-valued function and this holds by assumption in our setting.\n\u2022 A2.2.2 requires that the sequences \u03b2n and \u03b7n, n \u2265 0 are bounded and converge to zero asymptotically. Lemma 7 above establishes that the bias \u03b2n is O(\u03b42n) and since \u03b4n \u2192 0 as n \u2192 \u221e, it is easy to see that A2.2.2 is satisfied for \u03b2n. As noted in the proof of Lemma 7, \u03b7n \u2192 0 as n \u2192 \u221e and hence, A2.2.2 is satisfied for \u03b7n as well. \u2022 A2.2.3 requires that the step-sizes an, n \u2265 0 satisfy a(n) \u2192 0 as n \u2192 \u221e and \u2211\nn an = \u221e. These step-size conditions hold by assumption in our setting - see (6) in the main paper.\n\u2022 Finally, we verify A2.2.4\u201d using arguments similar to those used in Spall (1992) for the classic SPSA algorithm: We first recall Doob\u2019s martingale inequality (see (2.1.7) on pp. 27 of Kushner and Clark (1978)):\nP ( sup m\u22650 \u2016Wm\u2016 \u2265 \u01eb ) \u2264 1 \u01eb2 lim m E \u2016Wm\u20162 . (25)\nApplying the above inequality to the martingale sequence {Wn}, where Wn := \u2211n\u22121\ni=0 ai\u03b7i, n \u2265 1, we obtain\nP ( sup m\u2265n \u2225\u2225\u2225\u2225\u2225 m\u2211\ni=n\nai\u03b7i \u2225\u2225\u2225\u2225\u2225 \u2265 \u01eb ) \u2264 1 \u01eb2 E \u2225\u2225\u2225\u2225\u2225 \u221e\u2211\ni=n\nai\u03b7i \u2225\u2225\u2225\u2225\u2225 2 = 1 \u01eb2 \u221e\u2211\ni=n\na2iE \u2016\u03b7i\u20162 . (26)\nWe now bound E \u2016\u03b7i\u20162 as follows:\nE\n( V\u0302 \u03b8n+\u03b4n\u2206n(x0)\u2212 V\u0302 \u03b8n\u2212\u03b4n\u2206n(x0)\n2\u03b4n\u2206in\n)2\n\u2264 1 4\u03b42n\n[ E ( 1\n(\u2206in) 2+2\u03b11\n)] 1 1+\u03b11 [ E [ (V\u0302 \u03b8n+\u03b4n\u2206n(x0)\u2212 V\u0302 \u03b8n\u2212\u03b4n\u2206n(x0)) ]2+2\u03b12] 11+\u03b12 (27)\n\u2264 1 4\u03b42n\n([ E [ (V\u0302 \u03b8n+\u03b4n\u2206n(x0)) ]2+2\u03b12] 11+\u03b12 + [ E [ (V\u0302 \u03b8n\u2212\u03b4n\u2206n(x0)) ]2+2\u03b12] 11+\u03b12 )\n(28)\n\u2264C \u03b42n , for some C < \u221e. (29)\nThe inequality in (27) uses Holder\u2019s inequality, with \u03b11, \u03b12 > 0 satisfying 11+\u03b11 + 1 1+\u03b12 = 1. The\nequality in (28) above follows owing to the fact that E (\n1 (\u2206in) 2+2\u03b11\n) = 1 as \u2206in is Rademacher. The\ninequality in (29) follows by using the fact that, for any \u03b8, the CPT-value estimate V\u0302 \u03b8(x0) = V \u03b8(x0)+ \u01eb\u03b8. We assume a finite state-action spaced SSP (which implies that the costs maxs,a g(s, a) < \u221e) and consider only proper policies (which implies that the total cost D\u03b8(x0) is bounded for any policy \u03b8) and finally, by (A1), the weight functions are Lipschitz - these together imply that V \u03b8(x0) is bounded for any policy \u03b8. The bias \u01eb\u03b8 is bounded by Proposition 1 in the main paper.\nThus, E \u2016\u03b7i\u20162 \u2264 C\u03b42n for some C < \u221e. Plugging this in (26), we obtain\nlim n\u2192\u221e P ( sup m\u2265n \u2225\u2225\u2225\u2225\u2225 m\u2211\ni=n\nai\u03b7i \u2225\u2225\u2225\u2225\u2225 \u2265 \u01eb ) \u2264 dC \u01eb2 lim n\u2192\u221e \u221e\u2211\ni=n\na2i \u03b42i = 0.\nThe equality above follows from (A3) in the main paper.\nThe claim follows."}, {"heading": "6.3 Proofs for PN-CPT-SPSA", "text": "Before proving Theorem 2, we bound the bias in the SPSA based estimate of the Hessian in the following lemma.\nLemma 8. Let Fn = \u03c3(\u03b8m,m \u2264 n,\u2206m, \u2206\u0302m,m < n), n \u2265 1. Then, for any i, j = 1, . . . , d, we have almost surely, \u2223\u2223\u2223\u2223\u2223\u2223 E   V\u0302 \u03b8n+\u03b4n(\u2206n+\u2206\u0302n) n (x0) + V\u0302 \u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n) n (x0)\u2212 2V\u0302 \u03b8nn (x0) \u03b42n\u2206 i n\u2206\u0302 j n \u2223\u2223\u2223\u2223\u2223\u2223 Fn  \u2212\u22072i,jV \u03b8n(x0) \u2223\u2223\u2223\u2223\u2223\u2223 \u2192 0 as n \u2192 \u221e.\n(30)\nProof. As in the proof of Lemma 7, we can ignore the bias from the CPT-value estimation scheme and conclude that,\nE\n  V\u0302 \u03b8n+\u03b4n(\u2206n+\u2206\u0302n) n (x0) + V\u0302\n\u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n) n (x0)\u2212 2V\u0302 \u03b8nn (x0) \u03b42n\u2206 i n\u2206\u0302 j n | Fn\n \nn\u2192\u221e\u2212\u2212\u2212\u2192 E [ V \u03b8n+\u03b4n(\u2206n+\u2206\u0302n)(x0) + V \u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n)(x0)\u2212 2V \u03b8n(x0)\n\u03b42n\u2206 i n\u2206\u0302 j n\n| Fn ] . (31)\nNow, the RHS of the above approximates the true gradient with only an O(\u03b42n) error and this can be inferred using arguments similar to that used in the proof of Proposition 4.2 of Bhatnagar and Prashanth (2015). We provide the proof here for the sake of completeness. Using Taylor\u2019s expansion as in Lemma 7, we obtain\nV \u03b8n+\u03b4n(\u2206n+\u2206\u0302n)(x0) + V \u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n)(x0)\u2212 2V \u03b8n(x0) \u03b42n\u2206 i n\u2206\u0302 j n\n= (\u2206n + \u2206\u0302n) T\u22072V \u03b8n(x0)(\u2206n + \u2206\u0302n) \u25b3i(n)\u25b3\u0302j(n) +O(\u03b42n)\n=\nN\u2211\nl=1\nN\u2211\nm=1\n\u2206ln\u22072l,mV \u03b8n(x0)\u2206mn \u2206in\u2206\u0302 j n + 2 N\u2211\nl=1\nN\u2211\nm=1\n\u2206ln\u22072l,mV \u03b8n(x0)\u2206\u0302mn \u2206in\u2206\u0302 j n + N\u2211\nl=1\nN\u2211\nm=1\n\u2206\u0302ln\u22072l,mV \u03b8n(x0)\u2206\u0302mn \u2206in\u2206\u0302 j n +O(\u03b42n).\nTaking conditional expectation, we observe that the first and last term above become zero, while the second term becomes \u22072ijV \u03b8n(x0). The claim follows by using the fact that \u03b4n \u2192 0 as n \u2192 \u221e.\nLemma 9. Let Fn = \u03c3(\u03b8m,m \u2264 n,\u2206m,m < n), n \u2265 1. Then, for any i = 1, . . . , d, we have almost surely,\n\u2223\u2223\u2223\u2223\u2223E [ V\u0302 \u03b8n+\u03b4n(\u2206n+\u2206\u0302n) n (x0)\u2212 V\u0302 \u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n)n (x0)\n2\u03b4n\u2206in\n\u2223\u2223\u2223\u2223\u2223Fn ] \u2212\u2207iV \u03b8n(x0) \u2223\u2223\u2223\u2223\u2223 \u2192 0 as n \u2192 \u221e. (32)\nProof. As in the proof of Lemma 7, we can ignore the bias from the CPT-value estimation scheme and conclude that,\nE\n  V\u0302 \u03b8n+\u03b4n(\u2206n+\u2206\u0302n) n (x0)\u2212 V\u0302 \u03b8n\u2212\u03b4n(\u2206n+\u2206\u0302n)n (x0)\n2\u03b4n\u2206in | Fn\n  n\u2192\u221e\u2212\u2212\u2212\u2192 E [ V \u03b8n+\u03b4n\u2206n(x0)\u2212 V \u03b8n\u2212\u03b4n\u2206n(x0)\n2\u03b4n\u2206in | Fn\n] .\nThe rest of the proof amounts to showing that the RHS of the above approximates the true gradient with an O(\u03b42n) correcting term and this can be done in a similar manner as the proof of Lemma 7."}, {"heading": "Proof of Theorem 2", "text": "Proof. The claim related to convergence of \u03b8n can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(\u03b42n) term away from the true gradient \u2207V \u03b8n(x0).\nFor proving the claim related to convergence of the Hessian recursion (8), we use the proof technique from Spall (2000). The proof proceeds as follows:\nLet Wm = H\u0302m \u2212 E [ H\u0302m \u2223\u2223\u2223 \u03b8m ] . Then, EWm = 0 and \u2211 m E\u2016Wm\u20162 m2 < \u221e, since E [ \u03b42m \u2225\u2225\u2225H\u0302m \u2225\u2225\u2225 2 ] < \u221e,\u2200m and \u2211n 1(n+1)2\u03b42n < \u221e by assumption. Now, applying a martingale convergence result from p. 397 of Laha and Rohatgi (1979) to Wm, we obtain\n1\nn+ 1\nn\u2211\nm=0\nH\u0302m \u2212 E [ H\u0302m \u2223\u2223\u2223 \u03b8m ] \u2192 0 a.s. (33)\nFrom (31) and Lemma 8, we know that E [ H\u0302n \u2223\u2223\u2223 \u03b8n ] = \u22072V \u03b8n(x0) +O(\u03b42n). Hence,\n1\nn+ 1\nn\u2211\nm=0\nE [ H\u0302m \u2223\u2223\u2223 \u03b8m ] = 1\nn+ 1\nn\u2211\nm=0\n\u22072V \u03b8m(x0) +O(\u03b42m) \u2192 \u22072V \u03b8 \u2217 (x0) a.s.\nThe final step above follows from the fact that the Hessian is continuous near \u03b8n and the fact that \u03b8n converges almost surely to \u03b8\u2217. Thus, we obtain\n1\nn+ 1\nn\u2211\nm=0\nH\u0302m \u2192 \u22072V \u03b8 \u2217 (x0) a.s.\nand the claim follows by observing that Hn = 1\nn+ 1\n\u2211n m=0 H\u0302m."}, {"heading": "6.4 Proofs for gradient-free policy optimization algorithm", "text": "We begin by remarking that there is one crucial difference between our algorithm and MRAS2 from Chang et al. (2013): MRAS2 has an expected function value objective, i.e., it aims to minimize a function by using sample observations that have zero-mean noise. On the other hand, the objective in our setting is the CPT-value, which distorts the underlying transition probabilities. The implication here is that MRAS2 can estimate the expected value using sample averages, while we have to resort to integrating the empirical distribution.\nSince we obtain samples of the objective (CPT) in a manner that differs from MRAS2, we need to establish that the thresholding step in Algorithm 3 achieves the same effect as it did in MRAS2. This is achieved by the following lemma, which is a variant of Lemma 4.13 from Chang et al. (2013), adapted to our setting.\nLemma 10. The sequence of random variables {\u03b8\u2217n, n = 0, 1, . . .} in Algorithm 3 converges w.p.1 as n \u2192 \u221e.\nProof. Let An be the event that either the first if statement (see 16) is true or the second if statement in the else clause (see 21) is true within the Thresholding step of Algorithm 3. Let Bn := {V \u03b8 \u2217 n(x0)\u2212V \u03b8\u2217n\u22121(x0) \u2264 \u03b5 2}. Whenever An holds, we have V\u0302 \u03b8\u2217n n (x0)\u2212 V\u0302 \u03b8\u2217n\u22121 n (x0) \u2265 \u03b5 and hence, we obtain\nP (An \u2229 Bn) \u2264P ({ V\u0302 \u03b8 \u2217 n n (x 0)\u2212 V\u0302 \u03b8 \u2217 n\u22121 n\u22121 (x 0) \u2265 \u03b5 } \u2229 { V \u03b8 \u2217 n(x0)\u2212 V \u03b8\u2217n\u22121(x0) \u2264 \u03b5\n2\n})\n\u2264P ( \u22c3\n\u03b8\u2208\u039bn,\u03b8\u2032\u2208\u039bn\u22121\n{{ V\u0302 \u03b8n (x 0)\u2212 V\u0302 \u03b8\u2032n\u22121(x0) \u2265 \u03b5 } \u2229 { V \u03b8(x0)\u2212 V \u03b8\u2032(x0) \u2264 \u03b5\n2\n}})\n\u2264 \u2211 \u03b8\u2208\u039bn,\u03b8\u2032\u2208\u039bk\u22121 P ({ V\u0302 \u03b8n (x 0)\u2212 V\u0302 \u03b8\u2032n\u22121(x0) \u2265 \u03b5 } \u2229 { V \u03b8(x0)\u2212 V \u03b8\u2032(x0) \u2264 \u03b5 2 })\n\u2264|\u039bn||\u039bn\u22121| sup \u03b8,\u03b8\u2032\u2208\u0398\nP ({\nV\u0302 \u03b8n (x 0)\u2212 V\u0302 \u03b8\u2032n\u22121(x0) \u2265 \u03b5\n} \u2229 { V \u03b8(x0)\u2212 V \u03b8\u2032(x0) \u2264 \u03b5\n2\n})\n\u2264|\u039bn||\u039bn\u22121| sup \u03b8,\u03b8\u2032\u2208\u0398\nP ( V\u0302 \u03b8n (x 0)\u2212 V\u0302 \u03b8\u2032n\u22121(x0)\u2212 V \u03b8(x0) + V \u03b8 \u2032 (x0) \u2265 \u03b5\n2\n)\n\u2264|\u039bn||\u039bn\u22121| sup \u03b8,\u03b8\u2032\u2208\u0398\n( P ( V\u0302 \u03b8n (x\n0)\u2212 V \u03b8(x0) \u2265 \u03b5 4\n) + P ( V\u0302 \u03b8 \u2032 n\u22121(x 0)\u2212 V \u03b8\u2032(x0) \u2265 \u03b5\n4\n))\n\u22644|\u039bn||\u039bk\u22121|e\u2212mn \u01eb2 8L2M2 .\nFrom the foregoing, we have \u2211\u221e\nn=1 P (An \u2229 Bn) < \u221e since mn \u2192 \u221e as n \u2192 \u221e. Applying the BorelCantelli lemma, we obtain\nP (An \u2229 Bn i.o.) = 0.\nFrom the above, it is implied that if An happens infinitely often, then Bcn will also happen infinitely often. Hence,\n\u221e\u2211\nn=1\n[ V \u03b8 \u2217 n(x0)\u2212 V \u03b8\u2217n\u22121(x0) ] =\n\u2211\nn: Anoccurs\n[ V \u03b8 \u2217 n(x0)\u2212 V \u03b8\u2217n\u22121(x0) ] +\n\u2211\nn: Acnoccurs\n[ V \u03b8 \u2217 n(x0)\u2212 V \u03b8\u2217n\u22121(x0) ]\n= \u2211\nn: Anoccurs\n[ V \u03b8 \u2217 n(x0)\u2212 V \u03b8\u2217n\u22121(x0) ]\n= \u2211\nn: An\u2229Bnoccurs\n[ V \u03b8 \u2217 n(x0)\u2212 V \u03b8\u2217n\u22121(x0) ] +\n\u2211\nn: An\u2229Bcnoccurs\n[ V \u03b8 \u2217 n(x0)\u2212 V \u03b8\u2217n\u22121(x0) ]\n=\u221e w.p.1, since \u03b5 > 0.\nIn the above, the first equality follows from the fact that if the else clause in the second if statement (see 23) in Algorithm 3 is hit, then \u03b8\u2217n = \u03b8 \u2217 n\u22121. From the last equality above, we conclude that it is a contradiction because, V \u03b8(x0) > V \u03b8 \u2217 (x0) for any \u03b8 (since \u03b8\u2217 is the global minimum). The main claim now follows since An can happen only a finite number of times."}, {"heading": "Proof of Theorem 3", "text": "Proof. Once we have established Lemma 10, the rest of the proof follows in an identical fashion as the proof of Corollary 4.18 of Chang et al. (2013). This is because our algorithm operates in a similar manner as MRAS2 w.r.t. generating the candidate solution using a parameterized family f(\u00b7, \u03b7) and updating the distribution parameter \u03b7. The difference, as mentioned earlier, is the manner in which the samples are generated and the objective (CPT-value) function is estimated. The aforementioned lemma established that the elite sampling and thresholding achieve the same effect as that in MRAS2 and hence the rest of the proof follows from Chang et al. (2013)."}, {"heading": "7 Numerical Experiments for CPT-value estimation scheme", "text": "Setup. We consider a random variable X that is uniformly distributed in [0, 5]. We set the utility function to be identity and define the weight function w as follows:\nw(x) = { 2 3 (2x\u2212 x2) 0 \u2264 x < 12 1 3 + 2 3x 2 1 2 \u2264 x \u2264 1\nThe graph of w(x) can be seen in Fig. 2. Thus, the CPT-value of X can be seen to be\nV (X) =\n\u222b \u221e\n0 w(P (X > x))dx, (34)\nSince we consider gains only, the second integral component in V (X) from (1) is zero.\nBackground. Let X1, . . . ,Xn be i.i.d. random variables with underlying distribution U [0, 5]. Then, the empirical distribution function is defined as\nF\u0302n(x) = 1\nn\nn\u2211\ni=1\n1(Xi\u2264x). (35)\nThus, 1 \u2212 F\u0302n(x) is an unbiased estimator of P (X > x). From (35), it is clear that F\u0302n(x) generates a Lebesgue Stieljes measure which takes mass 1n at each of the points Xi, i = 1, . . . , n. So does w(1\u2212F\u0302n(x)). Based on this observation, one can see the weight function equivalently as follows:\nw(1\u2212 F\u0302n(x)) = \u222b \u221e\nx dw(1 \u2212 F\u0302n(y))\nHence, the estimate V\u0302n(X) of V (X) is arrived at as follows:\nV\u0302n(X) =\n\u222b \u221e\n0 w(1 \u2212 F\u0302n(x))dx =\u2212\n\u222b +\u221e\n0\n\u222b \u221e\nx dw(1 \u2212 F\u0302n(y)) (36)\n=\u2212 \u222b +\u221e\n0 xdw(1 \u2212 F\u0302n(x))\n=\nn\u2211\ni=1\nX[i]\n( w ( i+ 1\nn\n) \u2212 w ( i\nn\n)) , (37)\nwhere X[i] denotes the ith order statistic of the sample set {X1, . . . ,Xn}. Note that, we let w+ ( n+1 n ) = 1, \u2200n. We use (37) to estimate the CPT-value V (X). Analytically, V (X) = 2.5 for a U [0, 5] distributed random variable X. In the following, we report the accuracy of the estimator V\u0302n(X) using simulation experiments.\nResults. Fig. 3 shows the estimation error obtained for a random variable X with distribution U [0.5]. Here, the estimation error denotes the absolute difference between the estimated CPT-value (37) and true CPT-value, which is 2.5. From Fig. 3, it is evident that the CPT-value estimation scheme (37) converges rapidly to the true CPT-value."}, {"heading": "8 Conclusions and Future Work", "text": "We considered the problem of optimizing the CPT-value of an MDP. For this purpose, we first designed an estimation scheme that evaluates the CPT-value of any given policy. Next, using this estimator as the inner loop, we proposed three policy optimization algorithms. The first two algorithms use SPSA to estimate the gradient and/or Hessian of the CPT-value function and then perform a descent in the policy parameter, whereas the third algorithm is gradient-free and is based on a reference distribution that concentrates on the global optimum. Using an empirical distribution over the policy space in conjunction with KL-divergence to the reference distribution, we get a global policy optimization scheme. We provided theoretical convergence guarantees for all the proposed algorithms. In particular, we first showed that the CPT-value estimator\nconverges asymptotically and at the optimal rate as well. Next, the SPSA based policy optimization algorithms were shown to converge to a locally CPT-value optimal policy, while the gradient-free algorithm was shown to converge to a globally CPT-value optimal policy. As future work, it would be interesting to test our algorithms on simple as well as sophisticated empirical domains."}], "references": [{"title": "Measure theory and probability theory", "author": ["K.B. Athreya", "S.N. Lahiri"], "venue": "Springer Science & Business Media,", "citeRegEx": "Athreya and Lahiri.,? \\Q2006\\E", "shortCiteRegEx": "Athreya and Lahiri.", "year": 2006}, {"title": "Abstract Dynamic Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q2013\\E", "shortCiteRegEx": "Bertsekas.", "year": 2013}, {"title": "Simultaneous perturbation Newton algorithms for simulation optimization", "author": ["S. Bhatnagar", "L.A. Prashanth"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Bhatnagar and Prashanth.,? \\Q2015\\E", "shortCiteRegEx": "Bhatnagar and Prashanth.", "year": 2015}, {"title": "Stochastic Recursive Algorithms for Optimization, volume 434", "author": ["S. Bhatnagar", "H.L. Prasad", "L.A. Prashanth"], "venue": null, "citeRegEx": "Bhatnagar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bhatnagar et al\\.", "year": 2013}, {"title": "Stochastic Approximation: A Dynamical Systems Viewpoint", "author": ["V. Borkar"], "venue": null, "citeRegEx": "Borkar.,? \\Q2008\\E", "shortCiteRegEx": "Borkar.", "year": 2008}, {"title": "Learning algorithms for risk-sensitive control", "author": ["V. Borkar"], "venue": "In Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems\u2013MTNS,", "citeRegEx": "Borkar.,? \\Q2010\\E", "shortCiteRegEx": "Borkar.", "year": 2010}, {"title": "Risk-constrained Markov decision processes", "author": ["V. Borkar", "R. Jain"], "venue": "In IEEE Conference on Decision and Control (CDC),", "citeRegEx": "Borkar and Jain.,? \\Q2010\\E", "shortCiteRegEx": "Borkar and Jain.", "year": 2010}, {"title": "Simulation-based Algorithms for Markov Decision Processes", "author": ["H.S. Chang", "J. Hu", "M.C. Fu", "S.I. Marcus"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2013}, {"title": "Optimal rates for zero-order convex optimization: the power of two function evaluations", "author": ["J.C. Duchi", "M.I. Jordan", "M.J. Wainwright", "A. Wibisono"], "venue": "arXiv preprint arXiv:1312.2139,", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Transport-entropy inequalities and deviation estimates for stochastic approximation schemes", "author": ["M. Fathi", "N. Frikha"], "venue": "Electron. J. Probab,", "citeRegEx": "Fathi and Frikha.,? \\Q2013\\E", "shortCiteRegEx": "Fathi and Frikha.", "year": 2013}, {"title": "Original and cumulative prospect theory: A discussion of empirical differences", "author": ["H. Fennema", "P. Wakker"], "venue": "Journal of Behavioral Decision Making,", "citeRegEx": "Fennema and Wakker.,? \\Q1997\\E", "shortCiteRegEx": "Fennema and Wakker.", "year": 1997}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A.D. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In SODA,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Practical Optimization", "author": ["P.E. Gill", "W. Murray", "M.H. Wright"], "venue": null, "citeRegEx": "Gill et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Gill et al\\.", "year": 1981}, {"title": "Online Convex Optimization", "author": ["E. Hazan"], "venue": null, "citeRegEx": "Hazan.,? \\Q2015\\E", "shortCiteRegEx": "Hazan.", "year": 2015}, {"title": "Prospect theory: An analysis of decision under risk", "author": ["D. Kahneman", "A. Tversky"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "Kahneman and Tversky.,? \\Q1979\\E", "shortCiteRegEx": "Kahneman and Tversky.", "year": 1979}, {"title": "Stochastic Approximation Methods for Constrained and Unconstrained Systems", "author": ["H. Kushner", "D. Clark"], "venue": null, "citeRegEx": "Kushner and Clark.,? \\Q1978\\E", "shortCiteRegEx": "Kushner and Clark.", "year": 1978}, {"title": "Stochastic Systems with Cumulative Prospect Theory", "author": ["K. Lin"], "venue": "Ph.D. Thesis,", "citeRegEx": "Lin.,? \\Q2013\\E", "shortCiteRegEx": "Lin.", "year": 2013}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Polyak and Juditsky.,? \\Q1992\\E", "shortCiteRegEx": "Polyak and Juditsky.", "year": 1992}, {"title": "Policy Gradients for CVaR-Constrained MDPs. In Algorithmic Learning Theory, pages 155\u2013169", "author": ["L.A. Prashanth"], "venue": null, "citeRegEx": "Prashanth.,? \\Q2014\\E", "shortCiteRegEx": "Prashanth.", "year": 2014}, {"title": "Actor-critic algorithms for risk-sensitive MDPs", "author": ["L.A. Prashanth", "M. Ghavamzadeh"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Prashanth and Ghavamzadeh.,? \\Q2013\\E", "shortCiteRegEx": "Prashanth and Ghavamzadeh.", "year": 2013}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Optimization of conditional value-at-risk", "author": ["R.T. Rockafellar", "S. Uryasev"], "venue": "Journal of risk,", "citeRegEx": "Rockafellar and Uryasev.,? \\Q2000\\E", "shortCiteRegEx": "Rockafellar and Uryasev.", "year": 2000}, {"title": "Stochastic approximation. Handbook of Sequential Analysis, pages 503\u2013529", "author": ["D. Ruppert"], "venue": null, "citeRegEx": "Ruppert.,? \\Q1991\\E", "shortCiteRegEx": "Ruppert.", "year": 1991}, {"title": "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation", "author": ["J.C. Spall"], "venue": "IEEE Trans. Auto. Cont.,", "citeRegEx": "Spall.,? \\Q1992\\E", "shortCiteRegEx": "Spall.", "year": 1992}, {"title": "Adaptive stochastic approximation by the simultaneous perturbation method", "author": ["J.C. Spall"], "venue": "IEEE Trans. Autom. Contr.,", "citeRegEx": "Spall.,? \\Q2000\\E", "shortCiteRegEx": "Spall.", "year": 2000}, {"title": "Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control, volume 65", "author": ["J.C. Spall"], "venue": null, "citeRegEx": "Spall.,? \\Q2005\\E", "shortCiteRegEx": "Spall.", "year": 2005}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Variance adjusted actor-critic algorithms", "author": ["A. Tamar", "S. Mannor"], "venue": "arXiv preprint arXiv:1310.3697,", "citeRegEx": "Tamar and Mannor.,? \\Q2013\\E", "shortCiteRegEx": "Tamar and Mannor.", "year": 2013}, {"title": "Policy gradients with variance related risk criteria", "author": ["A. Tamar", "D.D. Castro", "S. Mannor"], "venue": "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,", "citeRegEx": "Tamar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2012}, {"title": "Advances in prospect theory: Cumulative representation of uncertainty", "author": ["A. Tversky", "D. Kahneman"], "venue": "Journal of Risk and Uncertainty,", "citeRegEx": "Tversky and Kahneman.,? \\Q1992\\E", "shortCiteRegEx": "Tversky and Kahneman.", "year": 1992}], "referenceMentions": [{"referenceID": 13, "context": "A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects).", "startOffset": 159, "endOffset": 187}, {"referenceID": 10, "context": "A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects).", "startOffset": 188, "endOffset": 214}, {"referenceID": 10, "context": "A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects). However, PT is lacking in some theoretical aspects as it violates first-order stochastic dominance1. Cumulative prospect theory (CPT) (Tversky and Kahneman 1992) uses a similar measure as PT, except that the weights are a function of cumulative probabilities. First, separate the gains and losses as Consider the following example from Fennema and Wakker (1997): Suppose there are 20 prospects (outcomes) ranging from \u221210 to 180, each with probability 0.", "startOffset": 188, "endOffset": 654}, {"referenceID": 7, "context": "Gradient-free method: We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to devise a globally optimizing policy update scheme.", "startOffset": 80, "endOffset": 100}, {"referenceID": 7, "context": "Gradient-free method: We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to devise a globally optimizing policy update scheme. The idea is to use a reference model that eventually concentrates on the global minimum and then empirically approximate this reference distribution well-enough. The latter is achieved via natural exponential families in conjunction with KullbackLeibler (KL) divergence to measure the \u201cdistance\u201d from the reference distribution. Unlike the setting of Chang et al. (2013), we neither observe the objective function (CPT-value) perfectly nor with zero-mean noise.", "startOffset": 80, "endOffset": 525}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al.", "startOffset": 0, "endOffset": 14}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al.", "startOffset": 0, "endOffset": 38}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).", "startOffset": 0, "endOffset": 59}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).", "startOffset": 0, "endOffset": 93}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf.", "startOffset": 0, "endOffset": 189}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).", "startOffset": 0, "endOffset": 342}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).", "startOffset": 0, "endOffset": 376}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)).", "startOffset": 0, "endOffset": 501}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)).", "startOffset": 0, "endOffset": 519}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000).", "startOffset": 0, "endOffset": 828}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function.", "startOffset": 0, "endOffset": 1012}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function. Unlike previous applications of SPSA and the algorithm from Chang et al. (2013), our he policy optimization algorithms suffer from biased estimates from the policy evaluation procedure, where the bias is non-zero and bounded.", "startOffset": 0, "endOffset": 1282}, {"referenceID": 3, "context": "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function. Unlike previous applications of SPSA and the algorithm from Chang et al. (2013), our he policy optimization algorithms suffer from biased estimates from the policy evaluation procedure, where the bias is non-zero and bounded. We overcome the bias asymptotically by slowly increasing the number of samples and show that the resulting policy optimization algorithms converge. The closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)).", "startOffset": 0, "endOffset": 1615}, {"referenceID": 1, "context": "The closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)).", "startOffset": 113, "endOffset": 130}, {"referenceID": 1, "context": "The closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)). While the CPT-value (1) that we aim to optimize is based on that in Lin (2013), we extend the latter work in several ways:", "startOffset": 113, "endOffset": 211}, {"referenceID": 16, "context": "(i) Unlike Lin (2013), we do not assume model information and develop an estimation scheme for the CPT-value function; (ii) Further, we also propose control algorithms using SPSA and model-based policy search in order to find a policy that optimizes the CPT-value.", "startOffset": 11, "endOffset": 22}, {"referenceID": 16, "context": "Thus, one cannot employ classic stochastic approximation schemes Robbins and Monro (1951) in our setting.", "startOffset": 65, "endOffset": 90}, {"referenceID": 4, "context": "Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.", "startOffset": 0, "endOffset": 14}, {"referenceID": 4, "context": "Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.", "startOffset": 0, "endOffset": 39}, {"referenceID": 4, "context": "Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.", "startOffset": 0, "endOffset": 73}, {"referenceID": 4, "context": "Lemma 1 in Chapter 2 of Borkar (2008)): sup l\u22650 (\u03b6n+l \u2212 \u03b6n) \u2192 0 as n \u2192 \u221e.", "startOffset": 24, "endOffset": 38}, {"referenceID": 14, "context": "A standard approach to overcome this step-size dependency is to use iterate averaging, suggested independently by Polyak Polyak and Juditsky (1992) and Ruppert Ruppert (1991).", "startOffset": 121, "endOffset": 148}, {"referenceID": 14, "context": "A standard approach to overcome this step-size dependency is to use iterate averaging, suggested independently by Polyak Polyak and Juditsky (1992) and Ruppert Ruppert (1991). The idea is to use larger step-sizes an = 1/n\u03b1, where \u03b1 \u2208 (1/2, 1), and then combine it with averaging of the iterates.", "startOffset": 121, "endOffset": 175}, {"referenceID": 8, "context": "2 in Fathi and Frikha (2013)).", "startOffset": 5, "endOffset": 29}, {"referenceID": 8, "context": "2 in Fathi and Frikha (2013)). Thus, it is optimal to average iterates only after a sufficient number of iterations have passed and all the iterates are very close to the optimum. However, the latter situation serves as a stopping condition in practice. An alternative approach is to employ step-sizes of the form an = (a0/n)Mn, where Mn converges to ( \u22072V \u03b8\u2217(x0) )\u22121 , i.e., the inverse of the Hessian of the CPT-value at the optimum \u03b8\u2217. Such a scheme gets rid of the step-size dependency (one can set a0 = 1) and still obtains optimal convergence rates. This is the motivation behind having a second-order policy optmization scheme and we next adapt the scheme from Spall (2000) for estimating the Hessian inverse.", "startOffset": 5, "endOffset": 681}, {"referenceID": 8, "context": "2 in Fathi and Frikha (2013)). Thus, it is optimal to average iterates only after a sufficient number of iterations have passed and all the iterates are very close to the optimum. However, the latter situation serves as a stopping condition in practice. An alternative approach is to employ step-sizes of the form an = (a0/n)Mn, where Mn converges to ( \u22072V \u03b8\u2217(x0) )\u22121 , i.e., the inverse of the Hessian of the CPT-value at the optimum \u03b8\u2217. Such a scheme gets rid of the step-size dependency (one can set a0 = 1) and still obtains optimal convergence rates. This is the motivation behind having a second-order policy optmization scheme and we next adapt the scheme from Spall (2000) for estimating the Hessian inverse. This is not a restrictive assumption, as one can project to a compact set to ensure boundedness. See the discussion pp. 40-41 of Kushner and Clark (1978) and also remark E.", "startOffset": 5, "endOffset": 871}, {"referenceID": 3, "context": "1 of Bhatnagar et al. (2013).", "startOffset": 5, "endOffset": 29}, {"referenceID": 12, "context": "A simple way is to have \u03a5(Hn) as a diagonal matrix and then add a positive scalar \u03b4n to the diagonal elements so as to ensure invertibility - see Gill et al. (1981), Spall (2000) for a similar operator.", "startOffset": 146, "endOffset": 165}, {"referenceID": 12, "context": "A simple way is to have \u03a5(Hn) as a diagonal matrix and then add a positive scalar \u03b4n to the diagonal elements so as to ensure invertibility - see Gill et al. (1981), Spall (2000) for a similar operator.", "startOffset": 146, "endOffset": 179}, {"referenceID": 2, "context": "Gradient and Hessian estimation We estimate the Hessian of the CPT-value function using the scheme suggested by Bhatnagar and Prashanth (2015). As in the case of the first-order method, we use Rademacher random variables to simultaneously", "startOffset": 112, "endOffset": 143}, {"referenceID": 23, "context": "Notice that the above estimates require three samples, while the the second order SPSA algorithm proposed first in Spall (2000) required four.", "startOffset": 115, "endOffset": 128}, {"referenceID": 7, "context": "5 Gradient-free algorithm for optimizing CPT-value We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to our setting of optimizing CPT-value in MDPs.", "startOffset": 109, "endOffset": 129}, {"referenceID": 7, "context": "5 Gradient-free algorithm for optimizing CPT-value We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to our setting of optimizing CPT-value in MDPs. We require that there exists a unique global optimum \u03b8\u2217 for the problem min\u03b8\u2208\u0398 V \u03b8(x0). To illustrate the main idea in the algorithm, assume we know the form of V \u03b8(x0). Then, the idea is to generate a sequence of reference distributions gk(\u03b8) on the policy space \u0398, such that it eventually concentrates on the global optimum \u03b8\u2217. One simple way, suggested in Chapter 4 of Chang et al. (2013) is gk(\u03b8) = H(V (x))gk\u22121(\u03b8) \u222b \u0398H(V \u03b8 (x0))gk\u22121(\u03b8\u2032)\u03bd(d\u03b8\u2032) , \u2200 \u03b8 \u2208 \u0398, (9) where \u03bd is the Lebesgue/counting measure on \u0398 and H is a strictly decreasing function.", "startOffset": 109, "endOffset": 569}, {"referenceID": 0, "context": "11 in Athreya and Lahiri (2006).", "startOffset": 6, "endOffset": 32}, {"referenceID": 22, "context": "The following lemma establishes this claim and its proof can be inferred from Spall (1992), though we give it here for the sake of completeness.", "startOffset": 78, "endOffset": 91}, {"referenceID": 15, "context": "Following this lemma, we complete the proof of Theorem 1 by invoking the well-known Kushner-Clark lemma Kushner and Clark (1978) and this involves verfying conditions A2.", "startOffset": 104, "endOffset": 129}, {"referenceID": 15, "context": "29 of Kushner and Clark (1978), provided we verify that the assumptions A2.", "startOffset": 6, "endOffset": 31}, {"referenceID": 15, "context": "29 of Kushner and Clark (1978), provided we verify that the assumptions A2.2.1 to A2.2.3 and A2.2.4\u201d of Kushner and Clark (1978) are satisfied for \u03b8n governed by (5).", "startOffset": 6, "endOffset": 129}, {"referenceID": 22, "context": "4\u201d using arguments similar to those used in Spall (1992) for the classic SPSA algorithm: We first recall Doob\u2019s martingale inequality (see (2.", "startOffset": 44, "endOffset": 57}, {"referenceID": 15, "context": "27 of Kushner and Clark (1978)):", "startOffset": 6, "endOffset": 31}, {"referenceID": 2, "context": "2 of Bhatnagar and Prashanth (2015). We provide the proof here for the sake of completeness.", "startOffset": 5, "endOffset": 36}, {"referenceID": 23, "context": "The claim related to convergence of \u03b8n can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(\u03b42 n) term away from the true gradient \u2207V \u03b8n(x0).", "startOffset": 90, "endOffset": 103}, {"referenceID": 23, "context": "The claim related to convergence of \u03b8n can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(\u03b42 n) term away from the true gradient \u2207V \u03b8n(x0). For proving the claim related to convergence of the Hessian recursion (8), we use the proof technique from Spall (2000). The proof proceeds as follows: Let Wm = \u0124m \u2212 E [ \u0124m \u2223\u2223 \u03b8m ] .", "startOffset": 90, "endOffset": 374}, {"referenceID": 23, "context": "The claim related to convergence of \u03b8n can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(\u03b42 n) term away from the true gradient \u2207V \u03b8n(x0). For proving the claim related to convergence of the Hessian recursion (8), we use the proof technique from Spall (2000). The proof proceeds as follows: Let Wm = \u0124m \u2212 E [ \u0124m \u2223\u2223 \u03b8m ] . Then, EWm = 0 and \u2211 m E\u2016Wm\u2016 m2 < \u221e, since E [ \u03b42 m \u2225\u2225\u0124m \u2225\u2225 2 ] < \u221e,\u2200m and n 1 (n+1)2\u03b42 n < \u221e by assumption. Now, applying a martingale convergence result from p. 397 of Laha and Rohatgi (1979) to Wm, we obtain 1 n+ 1 n \u2211", "startOffset": 90, "endOffset": 630}, {"referenceID": 7, "context": "4 Proofs for gradient-free policy optimization algorithm We begin by remarking that there is one crucial difference between our algorithm and MRAS2 from Chang et al. (2013): MRAS2 has an expected function value objective, i.", "startOffset": 153, "endOffset": 173}, {"referenceID": 7, "context": "4 Proofs for gradient-free policy optimization algorithm We begin by remarking that there is one crucial difference between our algorithm and MRAS2 from Chang et al. (2013): MRAS2 has an expected function value objective, i.e., it aims to minimize a function by using sample observations that have zero-mean noise. On the other hand, the objective in our setting is the CPT-value, which distorts the underlying transition probabilities. The implication here is that MRAS2 can estimate the expected value using sample averages, while we have to resort to integrating the empirical distribution. Since we obtain samples of the objective (CPT) in a manner that differs from MRAS2, we need to establish that the thresholding step in Algorithm 3 achieves the same effect as it did in MRAS2. This is achieved by the following lemma, which is a variant of Lemma 4.13 from Chang et al. (2013), adapted to our setting.", "startOffset": 153, "endOffset": 885}, {"referenceID": 7, "context": "18 of Chang et al. (2013). This is because our algorithm operates in a similar manner as MRAS2 w.", "startOffset": 6, "endOffset": 26}, {"referenceID": 7, "context": "18 of Chang et al. (2013). This is because our algorithm operates in a similar manner as MRAS2 w.r.t. generating the candidate solution using a parameterized family f(\u00b7, \u03b7) and updating the distribution parameter \u03b7. The difference, as mentioned earlier, is the manner in which the samples are generated and the objective (CPT-value) function is estimated. The aforementioned lemma established that the elite sampling and thresholding achieve the same effect as that in MRAS2 and hence the rest of the proof follows from Chang et al. (2013).", "startOffset": 6, "endOffset": 540}], "year": 2017, "abstractText": "Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control. The estimation scheme that we propose uses the empirical distribution in order to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of policy optimization procedures for a Markov decision process (MDP). We propose both gradient-based as well as gradient-free policy optimization algorithms. The former includes both first-order and second-order methods that are based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA), while the latter is based on a reference distribution that concentrates on the global optima. Using an empirical distribution over the policy space in conjunction with Kullback-Leibler (KL) divergence to the reference distribution, we get a global policy optimization scheme. We provide theoretical convergence guarantees for all the proposed algorithms.", "creator": "LaTeX with hyperref package"}}}