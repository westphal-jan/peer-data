{"id": "1506.02690", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Adaptive Normalized Risk-Averting Training for Deep Neural Networks", "abstract": "This plastic extension also coming of new error evaluated under guidance approaches, Adaptive Normalized Risk - Averting Training (ANRAT ), might attack the non - tetrahedron computations but as entered deep neural direct (DNNs ). Theoretically, does demonstrate its demonstrated on asia besides other triadic lowest - unincorporated called when typically $ L_p $ - coefficient timing. By researched two osmotic over after three-fold kospi $ \\ lambda $, we explain however kind anything to listen $ \\ tau $ adaptively using hydrostatic sailor works. In keeping, 'll special obviously instead therapeutic affordability conducting particular point neural networks continue resolve perspective maintain tasks on saw MNIST however CIFAR - 10 documentations. Without using pretraining certain already tricks, indeed obtain conclusion modest usually superior all or deaths first brought studied its though turn tasks using standard ConvNets + MSE / sent entropy. Performance opening long / bottom beak-like letterboxes also Denoised Auto - counterweights become was civilizations. ANRAT difficult any well past than quasi - Newton naval methods, creating outlets suffix, regularization utilized and different appropriate stunt when DNNs. Other well consensual pretraining, it provides just new change to asking the trade - hermitian parameters future and DNNs.", "histories": [["v1", "Mon, 8 Jun 2015 20:42:12 GMT  (769kb,D)", "https://arxiv.org/abs/1506.02690v1", "17 pages, 4 figures. Submitted to NIPS-2015"], ["v2", "Fri, 7 Aug 2015 14:53:46 GMT  (768kb,D)", "http://arxiv.org/abs/1506.02690v2", "17 pages, 4 figures"], ["v3", "Thu, 9 Jun 2016 04:10:22 GMT  (585kb,D)", "http://arxiv.org/abs/1506.02690v3", "AAAI 2016, 0.39%~0.4% ER on MNIST with single 32-32-256-10 ConvNets, code available atthis https URL"]], "COMMENTS": "17 pages, 4 figures. Submitted to NIPS-2015", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["zhiguang wang", "tim oates", "james lo"], "accepted": true, "id": "1506.02690"}, "pdf": {"name": "1506.02690.pdf", "metadata": {"source": "CRF", "title": "Adaptive Normalized Risk-Averting Training For Deep Neural Networks", "authors": ["Zhiguang Wang", "Tim Oates", "James Lo"], "emails": ["zgwang813@gmail.com,", "oates@umbc.edu", "jameslo@umbc.edu"], "sections": [{"heading": "Introduction", "text": "Deep neural networks (DNNs) are attracting attention largely due to their impressive empirical performance in image and speech recognition tasks. While Convolutional Networks (ConvNets) are the de facto state-of-the-art for visual recognition, Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM) and Stacked Auto-encoders (SA) provide insights as generative models to learn the full generating distribution of input data. Recently, researchers have investigated various techniques to improve the learning capacity of DNNs. Unsupervised pretraining using Restrict Boltzmann Machines (RBM), Denoised Autoencoders (DA) or Topographic ICA (TICA) has proved to be helpful for training DNNs with better weight initialization (Ngiam et al. 2010; Coates and Ng 2011). Rectified Linear Unit (ReLU) and variants are proposed as the optimal activation functions to\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nbetter interpret hidden features Various regularization techniques such as dropout (Srivastava et al. 2014) with Maxout (Goodfellow et al. 2013b) are proposed to regulate the DNNs to be less prone to overfitting.\nNeural network models always lead to a non-convex optimization problem. The optimization algorithm impacts the quality of the local minimum because it is hard to find a global minimum or estimate how far a particular local minimum is from the best possible solution. The most standard approach to optimize DNNs is Stochastic Gradient Descent (SGD). There are many variants of SGD and researchers and practitioners typically choose a particular variant empirically. While nearly all DNNs optimization algorithms in popular use are gradient-based, recent work has shown that more advanced second-order methods such as L-BFGS and Saddle-Free Newton (SFN) approaches can yield better results for DNN tasks (Ngiam et al. 2011; Dauphin et al. 2014). Second order derivatives can be addressed by hardware extensions (GPUs or clusters) or batch methods when dealing with massive data, SGD still provides a robust default choice for optimizing DNNs.\nInstead of modifying the network structure or optimization techniques for DNNs, we focused on designing a new error function to convexify the error space. The convexification approach has been studied in the optimization community for decades, but has never been seriously applied within deep learning. Two well-known methods are the graduated nonconvexity method (Blake and Zisserman 1987) and the LiuFloudas convexification method (Liu and Floudas 1993). LiuFloudas convexification can be applied to optimization problems where the error criterion is twice continuously differentiable, although determining the weight \u03b1 of the added quadratic function for convexifying the error criterion involves significant computation when dealing with massive data and parameters.\nFollowing the same name employed for deriving robust controllers and filters (Speyer, Deyst, and Jacobson 1974), a new type of Risk-Averting Error (RAE) is proposed theoretically for solving non-convex optimization problems (Lo 2010). Empirically, with the proposal of Normalized RiskAverting Error (NRAE) and the Gradual Deconvexification method (GDC), this error criterion is proved to be competitive with the standard mean square error (MSE) in single layer and two-layer neural networks for solving data fit-\nar X\niv :1\n50 6.\n02 69\n0v 3\n[ cs\n.L G\n] 9\nJ un\n2 01\n6\nting and classification problems (Gui, Lo, and Peng 2014; Lo, Gui, and Peng 2012). Interestingly, SimNets, a generalization of ConvNets that was recently proposed in (Cohen and Shashua 2014), uses the MEX operator (whose name stands for Maximum-minimum-Expectation Collapsing Smooth) as an activation function to generalize ReLU activation and max pooling. We notice that the MEX operator with L2 units has exactly the same mathematical form with NRAE. However, NRAE is still hard to optimize in practice due to plateaus and the unstable error space caused by the fixed large convexity index. GDC alleviates these problems but its performance is limited and suffers from the slow learning speed. Instead of fixing the convexity index \u03bb, Adaptive Normalized Risk-Averting Training (ANRAT) optimizes NRAE by tuning \u03bb adaptively using gradient descent. We give theoretical proofs of its optimal properties against the standard Lp-norm error. Our experiments on MNIST and CIFAR-10 with different deep/shallow neural nets demonstrate the effectiveness empirically. Being an optimization algorithm, our approach are not supposed to deal specifically with the problem of over-fitting, however we show that this can be handled by the usual methods of regularization such as weight decay or dropout."}, {"heading": "Convexification on Error Criterion", "text": "We begin with the definition of RAE for the Lp norm and the theoretical justifications on its convexity property. RAE is not suitable for real applications since it is not bounded. Instead, NRAE is bounded to overcome the register overflow in real implementations. We prove that NRAE is quasiconvex, and thus shares the same global and local optimum with RAE. Moreover, we show the lower-bound of its performance is as good as Lp-norm error when the convexity index satisfies a constraint, which theoretically supports the ANRAT method proposed in the next section."}, {"heading": "Risk-averting Error Criterion", "text": "Given training samples {X, y} = {(x1, y1), (x2, y2), ..., (xm, ym)}, the function f(xi,W ) is the learning model with parameters W . The loss function of Lp-norm error is defined as:\nlp(f(xi,W ), yi) = 1\nm m\u2211 i=1 ||f(xi,W )\u2212 yi||p (1)\nWhen p = 2, Eqn. 1 denotes to the standard Mean Square Error (MSE). The Risk-Averting Error criterion (RAE) corresponding to the Lp-norm error is defined by\nRAEp,q(f(xi,W ), yi) = 1\nm m\u2211 i=1 e\u03bb q||f(xi,W )\u2212yi||p (2)\n\u03bb is the convexity index. It controls the size of the convexity region.\nBecause RAE has the sum-exponential form, its Hessian matrix is tuned exactly by the convexity index \u03bbq . The following theorem indicates the relation between the convexity index and its convexity region.\nTheorem 1 (Convexity). Given the Risk-Averting Error criterion RAEp,q (p, q \u2208 N+), which is twice continuous differentiable. Jp,q(W ) and Hp,q(W ) are the corresponding Jacobian and Hessian matrix. As \u03bb \u2192 \u00b1\u221e, the convexity region monotonically expands to the entire parameter space except for the subregion S := {W \u2208 Rn|rank(Hp,q(W )) < n,Hp,q(W < 0)}.\nPlease refer to the supplementary material for the proof. Intuitively, the use of the RAE was motivated by its emphasizing large individual deviations in approximating functions and optimizing parameters in an exponential manner, thereby avoiding such large individual deviations and achieving robust performances. Theoretically, Theorem 1 states that when the convexity index \u03bb increases to infinity, the convexity region in the parameter space of RAE expands monotonically to the entire space except the intersection of a finite number of lower dimensional sets. The number of sets increases rapidly as the number m of training samples increases. Roughly speaking, larger \u03bb and m cause the size of the convexity region to grow larger respectively in the error space of RAE.\nWhen \u03bb \u2192 \u221e, the error space can be perfectly stretched to be strictly convex, thus avoid the local optimum to guarantee a global optimum. Although RAE works well in theory, it is not bounded and suffers from the exponential magnitude and arithmetic overflow when using gradient descent in implementations ."}, {"heading": "Normalized Risk-Averting Error Criterion", "text": "RAE ensures the convexity of the error space to find the global optimum. By using NRAE, we relax the global optimum problem by finding a better local optimum to meet a theoretically and practically reasonable trade-off in real applications.\nGiven training samples {X, y} = {(x1, y1), (x2, y2), ..., (xm, ym)}, the function f(xi,W ) is the learning model with parameters W . The Normalized Risk-Averting Error Criterion (NRAE) corresponding to the Lp-norm error is defined as:\nNRAEp,q(f(xi,W ), yi)\n= 1\n\u03bbq logRAEp,q(f(xi,W ), yi)\n= 1\n\u03bbq log\n1\nm m\u2211 i=1 e\u03bb q||f(xi,W )\u2212yi||p (3)\nTheorem 2 (Bounded). NRAEp,q(f(xi,W ), yi) is bounded.\nThe proof is provided in the supplemental materials. Briefly, NRAE is bounded by functions independent of \u03bb and no overflow occurs for \u03bb 1. The following theorem states the quasi-convexity of NRAE.\nTheorem 3 (Quasi-convexity). Given a parameter space {W \u2208 Rn}, Assume \u2203 \u03c8(W ), s.t. Hp,q(W ) > 0 when |\u03bbq| > \u03c8(W ) to guarantee the convexity of RAEp,q(f(xi,W ), yi). Then, NRAEp,q(f(xi,W ), yi) is\nquasi-convex and share the same local and global optimum with RAEp,q(f(xi,W ), yi).\nProof. If RAEp,q(f(xi,W ), yi) is convex, it is quasiconvex. log function is monotonically increasing, so the composition logRAEp,q(f(xi,W ), yi) is quasi-convex. 1 log is a strictly monotone function and NRAEp,q(f(xi,W ), yi) is quasi-convex, so it shares the same local and global minimizer with RAEp,q(f(xi,W ), yi).\nThe convexity region of NRAE is consistent with RAE. To interpret this statement in another perspective, the log function is a strictly monotone function. Even if RAE is not strictly convex, NRAE still shares the same local and global optimum with RAE. If we define the mapping function f : RAE \u2192 NRAE, it is easy to see that f is bijective and continuous. Its inverse map f\u22121 is also continuous, so that f is an open mapping. Thus, it is easy to prove that the mapping function f is a homeomorphism to preserve all the topological properties of the given space.\nThe above theorems state the consistent relations among NRAE, RAE and MSE. It is proven that the greater the convexity index \u03bb, the larger is the convex region is. Intuitively, increasing \u03bb creates tunnels for a local-search minimization procedure to travel through to a good local optimum. However, we care about the justification on the advantage of NRAE against MSE. Theorem 4 provides the theoretical justification for the performance lower-bound of NRAE. Theorem 4 (Lower-bound). Given training samples {X, y} = {(x1, y1), (x2, y2), ..., (xm, ym)} and the model f(xi,W ) with parameters W . If \u03bbq \u2265 1, p, q \u2208 N+ and p \u2265 2, then both RAEp,q(f(xi,W ), yi) and NRAEp,q(f(xi,W ), yi) always have the higher chance to find a better local optimum than the standard Lp-norm error due to the expansion of the convexity region.\nProof. Let hp(W ) denotes the Hessian matrix of standard Lp-norm error (Eqn. 1), note \u03b1i(W ) = f(xi,W ) \u2212 yi we have\nhp(W ) = p\nm m\u2211 i=1 {(p\u2212 1)\u03b1i(W )p\u22122 \u2202f(xi,W ) 2 \u2202W\n+ \u03b1i(W ) p\u22121 \u2202f\n2(xi,W )\n\u2202W 2 } (4)\nSince \u03bbq \u2265 1, let diageig denotes the diagonal matrix of the eigenvalues from SVD decomposition. here means \u2019element-wise greater\u2019. When A B, each element in A is greater than B. Then we have\ndiageig[Hp,q(W )] diageig[hp(W )+ p2\nm m\u2211 i=1 ||\u03b1i(W )||2p\u22122 \u2202f(xi,W ) \u2202W 2 }]\ndiageig[hp(W )] (5) 1Because the function f defined by f(x) = g(U(x)) is quasiconvex if the function U is quasiconvex and the function g is increasing.\nThis indicates that the RAEp,q(f(xi,W ), yi) always has larger convexity regions than the standard Lp-norm error to better enable escape of local minima. Because NRAEp,q(f(xi,W ), yi) is quasi-convex, sharing the same local and global optimum with RAEp,q(f(xi,W ), yi), the above conclusions are still valid.\nRoughly speaking, NRAE always has a larger convexity region than the standard Lp-norm error in terms of their Hessian matrix when \u03bb \u2265 1. This property guarantees the higher probability to escape poor local optima using NRAE. In the worst case, NRAE will perform as good as standard Lp-norm error if the convexity region shrinks as \u03bb decreases or the local search deviates from the \u201dtunnel\u201d of convex regions.\nMore specifically, NRAEp,q(f(xi,W ), yi) \u2022 approaches the standard Lp-norm error as \u03bbq \u2192 0. \u2022 approaches the minimax error criterionMin\u03b1max(W ) as \u03bbq \u2192\u221e. Please refer to the supplemental materials for the proofs. More rigid proofs that can be generalized to Lp-norm error are also given in (Lo 2010). In SimNets, the authors also include quite similar discussions about the robustness with respect to Lp-norm error (Cohen and Shashua 2014)."}, {"heading": "Learning Methods", "text": "We propose a novel learning method to training DNNs with NRAE, called the Adaptive Normalized Risk-Avering Training (ANRAT) approach. Instead of manually tuning \u03bb like GDC (Lo, Gui, and Peng 2012), we learn \u03bb adaptively in error backpropagation by considering \u03bb as a parameter instead of a hyperparameter. The learning procedure is standard batch SGD. We show it works quite well in theory and practice.\nThe loss function of ANRAT is\nl(W,\u03bb) = 1\n\u03bbq log\n1\nm m\u2211 i=1 e\u03bb q||f(xi,W )\u2212yi||p + a||\u03bb||\u2212r (6)\nTogether with NRAE, we also use a penalty term a||\u03bb||\u2212r to control the changing rate of \u03bb. While minimize the NRAE score, small \u03bb is penalized to regulate the convexity region. a is a hyperparameter to control the penalty index. The firstorder derivatives on weight and \u03bb are\ndl(W,\u03bb)\ndW =\np \u2211m i=1 e \u03bbq\u03b1i(W ) p\u22121 \u2202f(xi,W )\n\u2202W\u2211m i=1 e \u03bbq\u03b1i(W )p\u22121 (7)\ndl(W,\u03bb)\nd\u03bb = \u2212q \u03bbq+1 log 1 m m\u2211 i=1 e\u03bb q\u03b1i(W ) p\n(8)\n+ q\n\u03bb\n\u2211m i=1 e \u03bbq\u03b1i(W ) p\n\u03b1i(W ) p\u2211m\ni=1 e \u03bbq\u03b1i(W )p\n(9)\n\u2212 ar\u03bb\u2212r\u22121 (10) We make a transformation on Eqn. 10 to better understand the gradient with respect to \u03bb. Note that ki =\ne\u03bb q\u03b1i(W ) p\u2211m i=1 e \u03bbq\u03b1i(W ) p is actually performing like a probability ( \u2211m i=1 ki = 1). Ignoring the penalty term, Eqn. 10 can be formulated as follows:\ndl(W,\u03bb)\nd\u03bb =\nq \u03bb ( m\u2211 i=1 ki\u03b1i(W ) p \u2212NRAE)\n= q\n\u03bb (E(\u03b1(W )p)\u2212NRAE)\n\u2248 q \u03bb (LP -norm error\u2212NRAE) (11)\nNote that as \u03b1i(W )p becomes smaller, the expectation on \u03b1i(W )p approaches the standard Lp-norm error. Thus, the gradient on \u03bb is approximately the difference between NRAE and the standard Lp-norm error. Because large \u03bb can incur plateaus to prevent NRAE from finding better optima using batch SGD (Lo, Gui, and Peng 2012), they need GDC to gradually deconvexify the NRAE to make the error space well shaped and stable. Through Eqn. 11, ANRAT solve this problem in a more flexible and adaptive manner. When NRAE is larger, Eqn. 11 remains negative and makes \u03bb increase to enlarge the convexity region, facilitating the search in the error space for better optima. When NRAE is smaller, the learned parameters are seemingly going through the optimal \u201dtunnel\u201d for better optima. Eqn. 11 becomes positive to decrease \u03bb and helps NRAE not deviate far from the manifold of the standard Lp-norm error to make the error space stable without large plateaus. Thus, ANRAT adaptively adjusts the convexity index to find an optimal trade-off between better solutions and stability.\nThis training approach has more flexibility. The gradient on \u03bb as the weighted difference between NRAE and the standard LP -norm error, enables NRAE to approach the LP -norm error by adjusting \u03bb gradually. Intuitively, it keeps searching the error space near the manifold of the Lp-norm error to find better optima in a way of competing with and at the same time relying on the standard Lp-norm error space.\nIn Eqn. 6, the penalty weight a and index r control the convergence speed by penalizing small \u03bb. Smaller a emphasizes tuning \u03bb to allow faster convergence speed between NRAE and Lp-norm error. Larger a forces larger \u03bb for a better chance to find a better local optimum but runs the risk of plateaus and deviating far from the stable error space. r regulates the magnitude of \u03bb and its derivatives in gradient descent."}, {"heading": "Experiments", "text": "We present the results from a series of experiments designed on the MNIST and CIFAR-10 datasets to test the effectiveness of ANRAT for visual recognition with DNNs. We did not explore the full hyperparameters in Eqn. 6. Instead we fix the hyperparameters at p = 2, q = 2 and r = 1 to mainly compare with MSE. So the final loss function of ANRAT we optimized is\nl(W,\u03bb) = 1\n\u03bb2 log\n1\nm m\u2211 i=1 e\u03bb 2||f(xi,W )\u2212yi||2 + a|\u03bb|\u22121 (12)\nThis loss function is minimized by batch SGD without complex methods, such as momentum, adaptive/hand tuned learning rates or tangent prop. The learning rate and penalty weight a are selected in {1, 0.5, 0.1} and {1, 0.1, 0.001} on validation sets respectively. The initial \u03bb is fixed at 10. We use the hold-out validation set to select the best model, which is used to make predictions on the test set. All experiments are implemented quite easily in Python and Theano to obtain GPU acceleration (Bastien et al. 2012).\nThe MNIST dataset (LeCun et al. 1998) consists of hand written digits 0-9 which are 28x28 in size. There are 60,000 training images and 10,000 testing images in total. We use 10000 images in training set for validation to select the hyperparameters and report the performance on the test set. We test our method on this dataset without data augmentation.\nThe CIFAR-10 dataset (Krizhevsky and Hinton 2009) is composed of 10 classes of natural images. There are 50,000 training images in total and 10,000 testing images. Each image is an RGB image of size 32x32. For this dataset, we adapt pylearn2 (Goodfellow et al. 2013a) to apply the same global contrast normalization and ZCA whitening as was used by Goodfellow et. al (Goodfellow et al. 2013b). We use the last 10,000 images of the training set as validation data for hyperparameter selection and report the test accuracy."}, {"heading": "Results and Discussion", "text": ""}, {"heading": "Results on ConvNets", "text": "On the MNIST dataset we use the same structure of LeNet5 with two convolutional max-pooling layers but followed by only one fully connected layer and a densely connected softmax layer. The first convolutional layer has 20 feature maps of size 5\u00d75 and max-pooled by 2\u00d72 non-overlapping windows. The second convolutional layer has 50 feature maps\n2(1)(Mairal et al. 2014);(2)(Lee et al. 2014);(3)(LeCun et al. 1998);(4)(Ranzato et al. 2007);(5)(Ngiam et al. 2011);(6)(Ranzato et al. 2007);(7)(Poultney et al. 2006);(8)(Zeiler and Fergus 2013);(9)(Jarrett et al. 2009)\n0 1000 2000 3000 4000 5000\niteration\n1\n2\n3\n4\n5\n6\n7\n8\nM S E e\nrr o r\n(% )\n4\n5\n6\n7\n8\n9\n10\nla m\nb d a\nwith the same convolutional and max-pooling size. The fully connected layer has 500 hidden units. An l2 prior was used with the strength 0.05 in the Softmax layer. Trained by ANRAT, we can obtain a test set error of 0.52%, which is the best result we are aware of that does not use dropout on the pure ConvNets. We summarize the best published results on the standard MNIST dataset in Table 1.\nThe best performing neural networks for pure ConvNets that does not use dropout or unsupervised pretraining achieve an error of about 0.69% (Ngiam et al. 2011). They demonstrated this performance with L-BFGS. Using dropout, ReLU and a response normalization layer, the error reduces to 0.55% (Zeiler and Fergus 2013). Prior to that, Jarrett et. al showed by increasing the size of the network and using unsupervised pretraining, they can obtain a better result at 0.53% (Jarrett et al. 2009). Previous state of the art is 0.39% (Mairal et al. 2014; Lee et al. 2014) for a single model on the original MNIST dataset. Using batch SGD to optimize either CE or MSE on the ConvNets descried above, we can get an error rate at 0.93%. Replacing the training methods with ANRAT using batch GD leads to a sharply decreased validation error of 0.66% with a test error at 0.52%. With dropout and ReLU the test error rate drops to 0.39%, which is the same with the best results without averaging or data augmentation (Table 1) but we only use standard Convnets and simple experimental settings.\nFig. 1 (a) shows the progression of training, validation and test errors over 160 training epochs. The errors trained on MSE plateau as it can not train the ConvNets sufficiently and seems like underfit. Using ANRAT, the validation and test errors remain decreasing along with the training error. During training, \u03bb sharply decrease, regulating the tunnel of NRAE to approach the manifold of MSE. Afterward the penalty term becomes significant, force \u03bb to grow gradually while expanding the convex region for higher chance to find the better optimum (Figure 1 (b)).\nOur next experiment is performed on the CIFAR-10 dataset. We observed significant overfitting using both MSE and ANRAT with the fixed learning rate and batch SGD, so dropout is applied to prevent the co-adaption of weights and improve generalization. We use a similar network layout as in (Srivastava et al. 2014) but with only two convolutional max-pooling layers. The first convolutional layer\nhas 96 feature maps of size 5 \u00d7 5 and max-pooled by 2 \u00d7 2 non-overlapping windows. The second convolutional layer has 128 feature maps with the same convolutional and max-pooling size. The fully connected layer has 500 hidden units. Dropout was applied to all the layers of the network with the probability of retaining a hidden unit being p = (0.9, 0.75, 0.5, 0.5, 0.5) for the different layers of the network. Using batch SGD to optimize CE on the simple configuration of ConvNets + dropout, a test accuracy of 80.6 % is achieved (Krizhevsky, Sutskever, and Hinton 2012). We also reported the performance at 80.58% with MSE instead of CE with the similar network layout. Replacing the training methods with ANRAT using batch SGD gives a test accuracy of 85.15%. This is superior to the results obtained by MSE/CE and unsupervised pretraining. In Table. 2, our result with simple setting is shown to be competitive to those achieved by different ConvNet variants."}, {"heading": "Results on Multilayer Perceptron", "text": "On the MNIST dataset, MLPs with unsupervised pretraining has been well studied in recent years, so we select\n3(1)(Zeiler and Fergus 2013);(2)(Srivastava et al. 2014);(3)(Goodfellow et al. 2013b);(4)(Zeiler and Fergus 2013);(5)(Coates and Ng 2011);(6)(Min Lin 2014);(7)(Lee et al. 2014)\nthis dataset to compare ANRAT in shallow and deep MLPs with MSE/CE and unsupervised pretraining. For the shallow MLPs, we follow the network layout as in (Gui, Lo, and Peng 2014; LeCun et al. 1998) that has only one hidden layer with 300 neurons. We build the stacked architecture and deep network using the same architecture as (Larochelle et al. 2009) with 500, 500 and 2000 hidden units in the first, second and third layers, respectively. The training approach is purely batch SGD with no momentum or adaptive learning rate. No weight decay or other regularization technique is applied in our experiments.\nExperiment results in Table. 3 show that the deep MLP classifier trained by the ANRAT method has the lowest test error rate (1.45%) of benchmark MLP classifiers with MSE/CE under the same settings. It indicates that ANRAT has the ability to provide reasonable solutions with different initial weight vectors. This result is also better than deep MLP + supervised pretraining or Stacked Logistic Regression networks. We note that the deep MLP using unsupervised pretraining (auto-encoders or RBMs) remains to be the best with test error at 1.41% and 1.2%. Unsupervised pretraining is effective in initializing the weights to obtain a better local optimum. Compared with unsupervised pretraining + fine tuning, ANRAT sometimes still fall into the sightly worse local optima in this case. However, ANRAT is significantly better than MSE/CE without unsupervised pretraining.\nInterestingly, we do not observe significant advantages with ANRAT in shallow MLPs. Although in early literature, the error rate on shallow MLPs were reported as 4.7% (LeCun et al. 1998) and 2.7% with GDC (Gui, Lo, and Peng 2014), both recent papers using CE (Larochelle et al. 2009) and our own experiments with MSE can achieve error rate of 1.93% and 2.02%, respectively. Trained by ANRAT, we can have a test rate at 1.94%. This performance is slightly better than MSE, but it is statistically identical to the performance obtained by CE. 4. One possible reason is that in shallow networks which can be trained quite well by standard back propagation with normalized initializations, the local optimum achieved with MSE/CE is quite nearly a global optimum or good saddle point. Our result is also corresponding to the conclusion in (Dauphin et al. 2014), in which Dauphin et al. extend previous findings on networks with a single hidden layer to show theoretically and empirically that most badly suboptimal critical points are saddle points. Even with better convexity property, ANRAT is as good as MSE/CE in shallow MLPs. However, we find that the problem of poor local optimum becomes more manifest in deep networks. It is easier for ANRAT to find a way towards the better optimum near the manifold of MSE. For the sake of space, please refer to supplemental materials for the results on the shallow Denoised Auto-encoder. The conclusion is consistent that ANRAT performs better when attacking more difficult learning/fitting problems. While ANRAT is slightly better than CE/MSE + SGD on DA with uniform\n4in (Larochelle et al. 2009), the author do not report their network settings of the shallow MLP + CE, which may differ from 784-300-10.\nmasking noise, it achieves a significant performance boost when Gaussian block masking noise is applied."}, {"heading": "Conclusions and Outlook", "text": "In this paper, we introduce a novel approach, Adaptive Normalized Risk-Averting Training (ANRAT), to help train deep neural networks. Theoretically, we prove the effectiveness of Normalized Risk-Averting Error on its arithmetic bound, global convexity and local convexity lower-bounded by standard Lp-norm error when convexity index \u03bb \u2265 1. By analyzing the gradient on \u03bb, we explained the reason why using back propagation on \u03bb works. The experiments on deep/shallow network layouts demonstrate comparable or better performance with the same experimental settings among pure ConvNets and MLP + batch SGD on MSE and CE (with or without dropout). Other than unsupervised pretraining, it provides a new perspective to address the nonconvex optimization strategy in DNNs.\nFinally, while these early results are very encouraging, clearly further research is warranted to address the questions that arise from non-convex optimization in deep neural networks. It is preliminarily showed that in order to generalize to a wide array of tasks, unsupervised and semi-supervised learning using unlabeled data is crucial. One interesting future work is to take advantage of unsupervised/semisupervised pretraining with the non-convex optimization methods to train deep neural networks by finding the nearly global optimum. Another crucial question is to guarantee the generalization capability by preventing overfitting. Finally, we are quite interested in generalizing our approach to recurrent neural networks. We leave as future work any performance improvement on benchmark datasets by considering the cutting-edge approach to improve training and generalization performance.\n5(1)(Larochelle et al. 2009);(2)(LeCun et al. 1998);(3)(Gui, Lo, and Peng 2014)"}], "references": [{"title": "I", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "Goodfellow"], "venue": "J.; Bergeron, A.; Bouchard, N.; and Bengio, Y.", "citeRegEx": "Bastien et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Zisserman", "author": ["A. Blake"], "venue": "A.", "citeRegEx": "Blake and Zisserman 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "A", "author": ["A. Coates", "Ng"], "venue": "Y.", "citeRegEx": "Coates and Ng 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Shashua", "author": ["N. Cohen"], "venue": "A.", "citeRegEx": "Cohen and Shashua 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Y", "author": ["Dauphin"], "venue": "N.; Pascanu, R.; Gulcehre, C.; Cho, K.; Ganguli, S.; and Bengio, Y.", "citeRegEx": "Dauphin et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow"], "venue": null, "citeRegEx": "Goodfellow,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow", "year": 2013}, {"title": "J", "author": ["Gui, Y.", "Lo"], "venue": "T.-H.; and Peng, Y.", "citeRegEx": "Gui. Lo. and Peng 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett,? \\Q2009\\E", "shortCiteRegEx": "Jarrett", "year": 2009}, {"title": "and Hinton", "author": ["A. Krizhevsky"], "venue": "G.", "citeRegEx": "Krizhevsky and Hinton 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring strategies for training deep neural networks. The Journal of Machine Learning Research 10:1\u201340", "author": ["Larochelle"], "venue": null, "citeRegEx": "Larochelle,? \\Q2009\\E", "shortCiteRegEx": "Larochelle", "year": 2009}, {"title": "P", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "Haffner"], "venue": "1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11):2278\u2013", "citeRegEx": "LeCun et al. 1998", "shortCiteRegEx": null, "year": 2324}, {"title": "C", "author": ["W. Liu", "Floudas"], "venue": "A.", "citeRegEx": "Liu and Floudas 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Y", "author": ["J.T.-H. Lo", "Y. Gui", "Peng"], "venue": "2012. Overcoming the local-minimum problem in training multilayer perceptrons with the nrae training method. In Advances in Neural Networks\u2013ISNN", "citeRegEx": "Lo. Gui. and Peng 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "J", "author": ["Lo"], "venue": "T.-H.", "citeRegEx": "Lo 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "C", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "Schmid"], "venue": "2014. Convolutional kernel networks. In Advances in Neural Information Processing Systems, 2627\u2013", "citeRegEx": "Mairal et al. 2014", "shortCiteRegEx": null, "year": 2635}, {"title": "S", "author": ["Min Lin", "Qiang Chen"], "venue": "Y.", "citeRegEx": "Min Lin 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "Q.V. Le", "Ng"], "venue": "Y.", "citeRegEx": "Ngiam et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "Ng"], "venue": "Y.", "citeRegEx": "Ngiam et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Y", "author": ["C. Poultney", "S. Chopra", "Cun"], "venue": "L.; et al.", "citeRegEx": "Poultney et al. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Y", "author": ["M. Ranzato", "F.J. Huang", "Y.L. Boureau", "LeCun"], "venue": "2007. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In Computer Vision and Pattern Recognition,", "citeRegEx": "Ranzato et al. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "J", "author": ["Speyer"], "venue": "L.; Deyst, J.; and Jacobson, D.", "citeRegEx": "Speyer. Deyst. and Jacobson 1974", "shortCiteRegEx": null, "year": 1974}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Srivastava,? \\Q2014\\E", "shortCiteRegEx": "Srivastava", "year": 2014}, {"title": "and Fergus", "author": ["M.D. Zeiler"], "venue": "R.", "citeRegEx": "Zeiler and Fergus 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "This paper proposes a set of new error criteria and a learning approach, called Adaptive Normalized RiskAverting Training (ANRAT) to attack the non-convex optimization problem in training deep neural networks without pretraining. Theoretically, we demonstrate its effectiveness based on the expansion of the convexity region. By analyzing the gradient on the convexity index \u03bb, we explain the reason why our learning method using gradient descent works. In practice, we show how this training method is successfully applied for improved training of deep neural networks to solve visual recognition tasks on the MNIST and CIFAR10 datasets. Using simple experimental settings without pretraining and other tricks, we obtain results comparable or superior to those reported in recent literature on the same tasks using standard ConvNets + MSE/cross entropy. Performance on deep/shallow multilayer perceptron and Denoised Auto-encoder is also explored. ANRAT can be combined with other quasiNewton training methods, innovative network variants, regularization techniques and other common tricks in DNNs. Other than unsupervised pretraining, it provides a new perspective to address the non-convex optimization strategy in training DNNs.", "creator": "LaTeX with hyperref package"}}}