{"id": "1506.03018", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2015", "title": "On the Interpretability of Conditional Probability Estimates in the Agnostic Setting", "abstract": "Many classification predefined possibly confidence imposing in the one of speedy depending of producers taking since used included following target likewise. It considered environmentally next fact turn even spite measures calibrated all improved, in its sense suggested they correctly taken the contradiction made during checksum in the hip crude. For different, ? during replication impedance a remix before confidence measure $ b $ two $ n $ short, out followed decrease reprise should cannot correct metres $ np $ now increase. Calibrated confidence measures lead when risen interpretability by chimpanzees that wireless as protect downstream analysis or technology. In may paper, we formally contradictory well consistency is confidence measures and wrong that PAC - 1950s uniform polarization result part the consistency still commitment measures. We drama that finite VC - dimension be sufficient because guaranteeing the consistency however maintain measures commercially of empirically consistent classifiers. Our result called implies that we these develope confidence measures 2005 but reason existing algorithms half alpha-3 functional, few one rest given made approximation promised during consistency.", "histories": [["v1", "Tue, 9 Jun 2015 17:41:48 GMT  (15kb)", "http://arxiv.org/abs/1506.03018v1", null], ["v2", "Tue, 28 Feb 2017 18:21:57 GMT  (210kb,D)", "http://arxiv.org/abs/1506.03018v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yihan gao", "aditya parameswaran", "jian peng"], "accepted": false, "id": "1506.03018"}, "pdf": {"name": "1506.03018.pdf", "metadata": {"source": "CRF", "title": "On the Uniform Convergence of Consistent Confidence Measures", "authors": ["Yihan Gao", "Aditya Parameswaran"], "emails": ["ygao34@illinois.edu", "adityagp@illinois.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n03 01\n8v 1\n[ cs\n.L G\n] 9"}, {"heading": "1 Introduction", "text": "Many classification algorithms naturally produce confidence measures in the form of conditional probability of labels, e.g., the probability that y = 1 given the set of features x for a specific item. In standard PAC learning theory [11], these confidence measures are usually validated by bounding the expected value of some loss function (for example, the log-likelihood function). While expected loss is useful in comparing learning algorithms, little can be said about the meaning or interpretability of the confidence measures output by these algorithms.\nWe focus on characterizing the consistency of confidence measures. We want the confidence measures output by classification algorithms to be consistent in the sense that they correctly capture the belief of the algorithm in the output labels. For instance, if the algorithm outputs a label with confidence measure p n times across items, then the output label should be correct approximately np times overall. Consistent confidence measures have many applications: they can be used in decision making processes where misclassifications have asymmetric loss [4], in data processing pipelines where output of upstream classifiers are used in downstream programs [9, 5], or in adaptive data verification processes where instances are referred to a human verifier when the automatic classifier is not confident enough [14]. Overall, ensuring that these confidence measures are correctly calibrated or consistent allows the results of these classification algorithms to be interpretable by humans or algorithms, which is crucial in web, military, scientific, and industrial applications.\nThus, our core question is: what can we guarantee about the conditional probabilities produced by classifiers without assumptions on the underlying distribution? In this paper, we address this question from an agnostic PAC learning theory standpoint; thus, our results are theoretically sound\nand represent a valuable addition to our growing understanding of the principles of machine learning. Specifically, we make the following contributions towards addressing this question:\n\u2022 We formalize the notion of consistency of confidence measures with respect to the underlying distribution, and derive an empirical counterpart that can be directly evaluated on training data.\n\u2022 We establish the conditions under which the empirical consistency converges uniformly to the true consistency. We show that finite VC-dimension not only implies empirical error converges to true error uniformly, but also guarantees the consistency of confidence measures produced by empirically consistent classifiers.\n\u2022 We apply our result to a class of linear classifiers and show that we can achieve uniform convergence of consistency using essentially the same order of sample size implied by the standard argument based on VC-dimension [13]. Our analysis on linear classifiers validates the common practice of using logistic and probit regressions to learn consistent confidence measures. It also suggests that a larger class of functions, namely the collection of monotonically increasing continuous functions, can be used as the mapping function from linear predictors to conditional probabilities in these generalized linear classifiers.\n\u2022 We also show that maximizing log-likelihood is essentially optimizing the sum of two objectives: the mutual information between confidence measures and labels, and the consistency of confidence measures. These results improve our understanding about classifiers that produce conditional probabilities of labels in addition to label predictions.\nRelated Work. Despite the fundamental importance of confidence measures and the wide utility of these measures, the work in this space has been remarkably sparse. Some existing papers have considered the uniform convergence of the expected value of certain loss functions on confidence measures: Haussler [7] considered the expected loss of the action that we take based on confidence measures as opposed to confidence measures themselves; Agarwal et al. [2] studied the uniform convergence of the area under ROC curve. These expected losses are designed to evaluate the effectiveness of a classifier: lower expected loss means it makes less errors on average. We remark that effectiveness and consistency are orthogonal qualities of confidence measures: a classifier might be highly effective but with poor consistency and vice versa. Our work enables a potential trade-off between these two: in some applications, we might be willing to trade a little bit of effectiveness for higher consistency.\nRegarding the confidence of classification, Li et al. [8] proposed a learning framework that explicitly allows a classifier to respond \u201cI don\u2019t know\u201d, but whenever it produces a label it must be correct.\nOur notion of consistency is similar to the notion of calibration in prediction theory [6, 1], where the goal is also to make predicted probability values match the empirical frequency of correct predictions. However, in prediction theory, the problem is formulated from a game-theoretic point of view: the sequence generator is assumed to be malevolent, and the goal is to design an algorithm to achieve this calibration guarantee no matter what strategy the sequence generator uses.\nTewari and Bartlett [12] studied the consistency of a conditional probability estimator. Their definition of consistency is that the algorithm should be able to correctly identify the label with highest conditional probability as the training data size increases. However, their work does not address the calibration of conditional probabilities."}, {"heading": "1.1 Problem Statement", "text": "Let X be a sample space and Y = {\u00b11} be the set of labels. Let P be a probability measure defined on X \u00d7 Y that specifies the joint distribution of sample x and its label y. Let Sn = {(x1, y1), . . . , (xn, yn)} be i.i.d. samples from (X \u00d7 Y,P). Consider a set of classifiers H, so that each h \u2208 H predicts a confidence measure h(x) \u2208 [0, 1] indicating how likely the label y is 1 given x. We define the consistency measure of h with respect to P to be:\nc(h) = sup p1,p2 |P(p1 < h(x) \u2264 p2, y = 1)\u2212 EP [1p1<h(x)\u2264p2h(x)]|\nThat is, for any p1, p2, the relative frequency of samples with y = 1 and p1 < h(x) \u2264 p2 deviates at most c(h) away from what h(x) predicts. We define the empirical consistency measure of classifier\nh to be:\ncemp(h) = sup p1,p2\n| 1\nn\nn \u2211\ni=1\n1p1<h(xi)\u2264p2,yi=1 \u2212 1\nn\nn \u2211\ni=1\nh(xi)1p1<h(xi)\u2264p2 |\nIn this paper we study the conditions under which, cemp(h) converges to c(h) uniformly for every h \u2208 H. We are interested in the sample complexity m(\u01eb, \u03b4) for \u01eb, \u03b4 > 0 such that:\n\u2200n > m(\u01eb, \u03b4),P(sup h\u2208H |c(h)\u2212 cemp(h)| > \u01eb) < \u03b4 (1)"}, {"heading": "1.2 Main Results", "text": "The main result of this paper is the following theorem, listed here for convenience. (We will explain all notations and definitions in Section 2.) Theorem 1. Let H be a set of classifiers that predicts confidence measures, i.e., functions from X to [0, 1]. Suppose the Rademacher Complexity of H0 = {1p1<h(x)\u2264p2 : p1, p2 \u2208 R, h \u2208 H} satisfies:\nESnRSn(H0) +\n\u221a\n2 ln(4/\u03b4)\nn <\n\u01eb\n2 Then,\nP(sup h\u2208H |c(h)\u2212 cemp(h)| > \u01eb) < \u03b4\nThus, Theorem 1 relates the uniform convergence rate of consistency measure with the Rademacher Complexity of the class of induced binary classifiers H0. We also prove the following theorem that relates this Rademacher Complexity with VC-dimension of each induced classifier class: Theorem 2. Let H be a set of classifiers that predicts confidence measures. Suppose that every classifier h in H only outputs confidence measures chosen from a finite set P \u2217:\n\u2200h \u2208 H, \u2200x \u2208 X , h(x) \u2208 P \u2217\nand for all p1, p2 \u2208 R, the VC-dimension of hypothesis space Hp1,p2 = {1p1<h(x)\u2264p2 : h \u2208 H} is at most d, then for any sample Sn of size n with n > d+ 1 we have:\nRSn(H0) \u2264\n\u221a\n2d(ln n d + 1) + 4 ln(|P \u2217|+ 1)\nn\nThus, overall, we can show that if for every p1, p2, the collection of induced classifiers 1p1<h(x)\u2264p2 has finite VC-dimension, then we can guarantee the uniform convergence of consistency of confidence measures.\nThe dependency on |P \u2217| in Theorem 2 is necessary: we show examples in which d is small but RSn(H0) is large. However, this dependency should not cause any problem in practice, as in most applications we only need a finite precision of confidence measures. The logarithmic dependency on the size of P \u2217 allow us to predict confidence measures with reasonably good precision without causing the sample complexity to blow up.\nThese two theorems are proved in Section 3, after we review definitions and results on VC-dimension and Rademacher Complexity in Section 2. Furthermore, we apply these theorems in Section 4 and Section 5 to derive several useful results:\n\u2022 In Section 4, we show that extending any hypothesis space by applying monotonically increasing continuous mapping on confidence measures does not increase the Rademacher Complexity of this hypothesis space.\n\u2022 Also in Section 4, we show that for all generalized linear classifiers, if their mapping functions from linear predictors to conditional probabilities are monotonically increasing and\nleft-continuous, then a generalization bound of order O( \u221a\n(d lnn+ ln 1 \u03b4 )/n) can be de-\nrived for their consistency. This bound is of the same order as the generalization error bound derived from standard VC-dimension arguments.\n\u2022 We also study the problem of finding a monotonic mapping for any given classifier so that empirical inconsistency is minimized after calibration. In Section 5, we show that this problem can be solved efficiently by linear programming."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 VC-dimension and Sauer\u2019s Lemma", "text": "Let A be a collection of random events, i.e., of subsets of X \u00d7Y . Let Sn = {(x1, y1), . . . , (xn, yn)} be a set of samples from X \u00d7 Y . Define the restriction of A to Sn as:\nASn = {A \u2229 Sn : A \u2208 A}\nWe say that A shatters Sn if the restriction of A to Sn contains all subsets of Sn:\n\u2200B \u2286 Sn, \u2203A \u2208 A, B = A \u2229 Sn\nor equivalently, |ASn | = 2 |Sn| The VC-Dimension [13] of A is defined as the maximum number of samples that can be arranged so that A shatters them.\nSuppose the VC-dimension of A is finite, then Sauer\u2019s Lemma [10][13] can be used to bound the size of ASn . Let the VC-dimension of A be d, then for any Sn:\n|ASn | \u2264\nd \u2211\ni=0\n(\nn\ni\n)\nand if n > d+ 1 then |ASn | \u2264 (en/d) d (2)"}, {"heading": "2.2 Rademacher Complexity", "text": "Let H be a collection of functions from X \u00d7 Y to [0, 1], the Rademacher Complexity [3] of H with respect to Sn is defined as:\nRSn(H) = 1\nn E\u03c3\u223c{\u00b11}n [sup\nh\u2208H\nn \u2211\ni=1\n\u03c3ih(xi, yi)]\nRademacher Complexity can be used to characterize the uniform convergence rate of a collection of functions. Let Sn be i.i.d. samples of X \u00d7 Y , then with probability of at least 1\u2212 \u03b4 [11]:\nsup h\u2208H\n| 1\nn\nn \u2211\ni=1\nh(xi, yi)\u2212 Eh(x, y)| \u2264 2ESnRSn(H) +\n\u221a\n2 ln(2/\u03b4)\nn (3)\nsup h\u2208H\n| 1\nn\nn \u2211\ni=1\nh(xi, yi)\u2212 Eh(x, y)| \u2264 2RSn(H) + 4\n\u221a\n2 ln(4/\u03b4)\nn (4)\nSometimes we also allow H to be collection of functions from X to [0, 1] in the above results. When used in this sense, we assume that the function will not use y label in the instance: h(x, y) = h(x)."}, {"heading": "2.3 Massart Lemma", "text": "Let H be a collection of functions from X \u00d7 Y to [0, 1]. Let HSn be the restriction of H to Sn:\nHSn = {(h(x1, y1), h(x2, y2), . . . , h(xn, yn)) : h \u2208 H}\nIf the cardinality of HSn is finite, then the Rademacher Complexity of H with respect to Sn can be bounded as follows [11]:\nRSn(H) \u2264 sup a\u2208HSn ||a||2\n\u221a\n2 ln |HSn |\nn (5)\nCombining with Sauer\u2019s Lemma, it follows that if A is a collection of subsets of X \u00d7 Y such that the VC-dimension of A is at most d, then\nRSn({1x\u2208A : A \u2208 A}) \u2264\n\u221a\n2d(ln(n/d) + 1)\nn (6)"}, {"heading": "3 Uniform Convergence of Consistency Measure", "text": "In this section we prove Theorem 1 and sketch the proof of Theorem 2.\nFor later convenience, we define\nFS,p1,p2(h) = 1\nn\nn \u2211\ni=1\n1p1<h(xi)\u2264p2,yi=1\nto be the relative frequency of event {p1 < h(x) \u2264 p2, y = 1}. Define FD,p1,p2(h) = P(p1 < h(x) \u2264 p2, y = 1) to be the probability of the same event.\nWe define ES,p1,p2(h) as the empirical expectation of h(x)1p1<h(x)\u2264p2 :\nES,p1,p2(h) = 1\nn\nn \u2211\ni=1\nh(xi)1p1<h(xi)\u2264p2\nand ED,p1,p2(h) as the expectation of the same function: ED,p1,p2(h) = E[h(x)1p1<h(x)\u2264p2 ] When context is clear, subscripts p1 and p2 are dropped.\nUsing these notations, we can rewrite c(h) and cemp as follows: c(h) = sup\np1,p2 |FD(h)\u2212 ED(h)| cemp(h) = sup p1,p2 |FS(h)\u2212 ES(h)|\nProof of Theorem 1. First note that: | sup p1,p2 |FD(h)\u2212 ED(h)| \u2212 sup p1,p2 |FS(h)\u2212 ES(h)|| \u2264 sup p1,p2 ||FD(h)\u2212 ED(h)| \u2212 |FS(h)\u2212 ES(h)||\n\u2264 sup p1,p2 |FD(h)\u2212 ED(h)\u2212FS(h) + ES(h)| \u2264 sup p1,p2 (|FD(h)\u2212FS(h)|+ |ED(h)\u2212 ES(h)|)\n\u2264 sup p1,p2 |FD(h)\u2212FS(h)|+ sup p1,p2 |ED(h)\u2212 ES(h)|\nTherefore it suffices to show that P( sup\nh,p1,p2 |FD(h)\u2212FS(h)|+ sup h,p1,p2 |ED(h)\u2212 ES(h)| > \u01eb) < \u03b4\nWe will prove the following sufficient conditions:\nP( sup h,p1,p2\n|FD(h)\u2212FS(h)| > \u01eb\n2 ) <\n\u03b4 2 and P( sup\nh,p1,p2\n|ED(h)\u2212 ES(h)| > \u01eb\n2 ) <\n\u03b4\n2\nDefine H1 = {1p1<h(x)\u2264p2,y=1 : p1, p2 \u2208 R, h \u2208 H}\nH2 = {h(x)1p1<h(x)\u2264p2 : p1, p2 \u2208 R, h \u2208 H}\nBy Equation (3), we only need to show that\nESnRSn(H1) +\n\u221a\n2 ln(4/\u03b4)\nn <\n\u01eb 2 and ESnRSn(H2) +\n\u221a\n2 ln(4/\u03b4)\nn <\n\u01eb\n2 For RSn(H1), we have:\nRSn(H1) = 1\nn E\u03c3\u223c{\u00b11}n [ sup\np1,p2,h\nn \u2211\ni=1\n\u03c3i1p1<h(xi)\u2264p2,yi=1]\n= 1\nn E\u03c3\u223c{\u00b11}n [ sup\np1,p2,h\nn \u2211\ni=1\n\u03c3i1p1<h(xi)\u2264p2Ezi\u2208{\u00b11} max(zi, yi)]\n\u2264 1\nn E\u03c3\u223c{\u00b11}n,z\u223c{\u00b11}n [ sup\np1,p2,h\nn \u2211\ni=1\n1p1<h(xi)\u2264p2\u03c3i max(zi, yi)]\n= 1\nn Et\u223c{\u00b11}n [ sup\np1,p2,h\nn \u2211\ni=1\nti1p1<h(xi)\u2264p2 ] = RSn(H0)\nwhere the last step is because ti = \u03c3i max(zi, yi) is uniformly distributed over {\u00b11} independent of the value of yi.\nFor RSn(H2), notice that h(x) = \u222b 1 0 1t<h(x)dt, therefore we have:\nRSn(H2) = 1\nn E\u03c3\u223c{\u00b11}n [ sup\np1,p2,h\nn \u2211\ni=1\n\u03c3ih(xi)1p1<h(xi)\u2264p2 ]\n= 1\nn E\u03c3\u223c{\u00b11}n [ sup\np1,p2,h\n\u222b 1\n0\nn \u2211\ni=1\n\u03c3i1t<h(xi)1p1<h(xi)\u2264p2dt]\n\u2264 1\nn E\u03c3\u223c{\u00b11}n\n\u222b 1\n0 [ sup p1,p2,h\nn \u2211\ni=1\n\u03c3i1max(p1,t)<h(xi)\u2264p2 ]dt\n\u2264 1\nn E\u03c3\u223c{\u00b11}n\n\u222b 1\n0 [ sup p1,p2,h\nn \u2211\ni=1\n\u03c3i1p1<h(xi)\u2264p2 ]dt\n= 1\nn E\u03c3\u223c{\u00b11}n [ sup\np1,p2,h\nn \u2211\ni=1\n\u03c3i1p1<h(xi)\u2264p2 ] = RSn(H0)\nCombining with the assumption in the theorem, we get the desired result.\nIn order to prove Theorem 2, we apply union bound to upper bound the cardinality of the restriction of H0 on Sn. Then we use Massart Lemma to prove upper bound of RSn(H0). The detailed proof can be found in the appendix.\nOne may wonder if we can prove bound of RSn(H0) that only depends on VC-dimension of Hp1,p2 but not |P \u2217|. Unfortunately, this is not possible. The next example shows that RSn(H0) can be large even when every Hp1,p2 has constant VC-dimension.\nExample 1. Let \u03c0 be arbitrary one-to-one mapping from {0, 1}n to {1, . . . , 2n}. Let H be the following hypothesis space:\nH = {hA(x) = \u03c0(A) \u2212 121x\u2208A\n2n : A \u2286 Sn}\nOne can easily verify that for all p1, p2, the VC-dimension of Hp1,p2 is at most 2, but we have RSn(H0) = 1 2 ."}, {"heading": "4 Generalization Bound for Linear Classifier", "text": "In this section we study the consistency of confidence measures produced by a class of linear classifiers. We first present some lemmas that provide us the tools to simplify the analysis of Rademacher Complexity of certain hypothesis spaces.\nThe following lemma allows us to bound the Rademacher Complexity of H0 by bounding the Rademacher Complexity of all \u201cone-sided\u201d classifiers (i.e., classifiers that determines the label via a hard threshold on confidence measures), which is sometimes easier to analyze.\nLemma 1. Let H3 = {1h(x)\u2264p : p \u2208 R, h \u2208 H}, then RSn(H0) \u2264 2RSn(H3).\nSometimes we have learning algorithms that output confidence measures in real numbers (for example, linear classifier and SVM), the following lemma will be useful for analyzing these algorithms.\nLemma 2. Let HR be a collection of classifiers that output confidence measures in R, let M be the collection of all left-continuous monotonically increasing functions from R to [0, 1], define\nH4 = {1p1<m(h(x))\u2264p2 : p1, p2 \u2208 R,m \u2208 M, h \u2208 HR}\nH5 = {1\u03b1<h(x)\u2264\u03b2 : \u03b1, \u03b2 \u2208 R, h \u2208 HR}\nThen, RSn(H4) = RSn(H5)\nThe proof of these two lemmas can be found in the appendix.\nNow we can reason about consistency of the following class of linear classifiers, which outputs confidence measures by first computing a linear predictor \u03c9Tx of feature vector x, then applying a monotonically increasing function f on the predictor to compute conditional probabilities:\nTheorem 3. Let X = Rm and M be the collection of all left-continuous monotonically increasing functions from R to [0, 1]. Let H = {f(wTx) : w \u2208 Rm, f \u2208 M}. Then for all n > m+ 2 we have\nRSn(H0) \u2264 2\n\u221a\n2(m+ 1)(ln(n/(m+ 1)) + 1)\nn\nProof. By Lemma 2,\nRSn(H0) = RSn({1\u03b1<wTx\u2264\u03b2 : \u03b1, \u03b2 \u2208 R, w \u2208 R m})\nThen by Lemma 1,\nRSn({1\u03b1<wT x\u2264\u03b2 : \u03b1, \u03b2 \u2208 R, w \u2208 R m}) \u2264 2RSn({1wTx\u2264\u03b1 : \u03b1 \u2208 R, w \u2208 R m})\nThe latter hypothesis space is the linear separator space of m dimensional space, having VCdimension at most m+ 1. Therefore by Sauer\u2019s Lemma and Massart Lemma:\nRSn({1wT x\u2264\u03b1 : \u03b1 \u2208 R, w \u2208 R m}) \u2264\n\u221a\n2(m+ 1)(ln(n/(m+ 1)) + 1)\nn\nCombining all, we get the desired result.\nCombining Theorem 3 with Theorem 1, we get the following result:\nCorollary 1. Let H be the same as in Theorem 3. For all n satisfying\nn > max( 32(m+ 1)(lnn\u2212 ln(m+ 1) + 1) + 8 ln 4 \u03b4\n\u01eb2 ,m+ 2)\nwe have P(sup\nh\u2208H |c(h)\u2212 cemp(h)| > \u01eb) < \u03b4"}, {"heading": "5 Learning Consistent Classifier", "text": "In this section, we discuss methods to learn classifiers that have low inconsistency measures. In Section 5.1, we justify the use of log-likelihood as the objective function in learning consistent classifiers. In Section 5.2, we discuss the use of monotonic functions to calibrate existing classifiers and show that this problem leads to a linear programming formulation."}, {"heading": "5.1 Maximum Likelihood Classifier", "text": "The log-likelihood function of classifier h on Sn is defined as:\nLS(h) = 1\nn\nn \u2211\ni=1\n[1yi=1 lnh(xi) + 1yi=\u22121 ln(1\u2212 h(xi))]\nThe expectation of log-likelihood is:\nLD(h) = EP [1y=1 lnh(x) + 1y=\u22121 ln(1\u2212 h(x))]\nConsider the random variable hx = h(x), rewrite the expected log-likelihood as:\nLD(h) =\n\u222b\nhx\n[P(y = 1|hx) lnhx +P(y = \u22121|hx) ln(1\u2212 hx)]dP\nDenote py|hx = P(y = 1|hx) as the conditional probability of y = 1 given hx. Denote DKL(p(y|hx)||hx) as the Kullback-Leibler divergence between p(y|hx) and distribution (hx, 1\u2212hx):\nDKL(p(y|hx)||hx) = py|hx ln py|hx hx + (1\u2212 py|hx) ln 1\u2212 py|hx 1\u2212 hx\nThen after rearranging terms, we can derive the following equation:\nLD(h) = \u2212H(y) +MI(y;hx)\u2212 EhxDKL(p(y|hx)||hx)\nwhere H(y) is the entropy of y and MI(y;hx) is the mutual information between y and hx. In other words, maximizing likelihood can be viewed as simultaneously maximizing the mutual information between confidence measure hx and label y and minimizing discrepancy between the predicted distribution (hx, 1\u2212 hx) and the actual distribution p(y|hx).\nWhile this fact intuitively justifies the use of maximum likelihood estimation (MLE) to learn consistent classifiers, as it turns out, MLE does not always minimize inconsistency.\nExample 2. Suppose X = {x1, x2}. Suppose that p(x1) = p(x2) = 0.5, p(y = 1|x1) = 0.05 and p(y = 1|x2) = 0.95. Among the two hypotheses h1 and h2 with h1(x1) = 0.01, h1(x2) = 0.99 and h2(x1) = 0.1, h2(x2) = 0.9, h1 is more consistent but h2 has higher likelihood."}, {"heading": "5.2 Calibrating Confidence Measure with Monotonic Function", "text": "As we have proved in Lemma 2, applying left-continuous monotonic mapping over existing confidence measures doesn\u2019t increase the Rademacher Complexity of the original hypothesis space. Therefore, another natural approach is to learn such mappings to calibrate the confidence measures produced by any existing classifier.\nAssume that we are given classifier h : X \u2192 R, we want to find a left-continuous monotonically increasing function f from R to [0, 1] that minimizes empirical inconsistency measures cemp(f \u25e6 h). For simplicity, we assume that Sn = {(x1, y1), . . . , (xn, yn)} satisfies h(x1) < h(x2) < . . . < h(xn). If h(xi)s are not distinct, we can assign weights to samples and the algorithm remains the same. Then,\ncemp(f \u25e6 h) = 1\nn sup p1,p2 |\nn \u2211\ni=1\n1p1<f(h(xi))\u2264p2(1yi=1 \u2212 f(h(xi)))|\n= 1\nn max a,b\n| \u2211\na<i\u2264b\n(1yi=1 \u2212 f(h(xi)))|\nLet zi = f(h(xi)), then z1 \u2264 z2 \u2264 . . . \u2264 zn, and we can rewrite the above equation as:\ncemp(f \u25e6 h) = 1\nn max a,b\n| \u2211\na<i\u2264b\n(1yi=1 \u2212 zi)|\nThe right hand side can be rewritten as follows:\n1 n max a,b | \u2211\na<i\u2264b\n(1yi=1 \u2212 zi)| = 1\nn max a\n\u2211\ni\u2264a\n(1yi=1 \u2212 zi)\u2212 1\nn min a\n\u2211\ni\u2264a\n(1yi=1 \u2212 zi)\nTherefore, the optimal f can be found by solving the following linear optimization problem, which can be efficiently solved by standard LP algorithms:\nmin \u03be1 + \u03be2\ns.t. \u03be1, \u03be2 \u2265 0 0 \u2264 z1 \u2264 z2 \u2264 . . . \u2264 zn \u2264 1\n\u22001 \u2264 k \u2264 n, \u2211\ni\u2264k\nzi \u2265 \u2211\ni\u2264k\n1yi=1 \u2212 n\u03be1\n\u22001 \u2264 k \u2264 n, \u2211\ni\u2264k\nzi \u2264 \u2211\ni\u2264k\n1yi=1 + n\u03be2\nFinally, by extrapolating all points of the form (h(xi), zi), we get a continuous monotonic function f from R to [0, 1], which can be used to calibrate confidence measures of the given classifier h."}], "references": [{"title": "Blackwell approachability and low-regret learning are equivalent", "author": ["J. Abernethy", "P.L. Bartlett", "E. Hazan"], "venue": "arXiv preprint arXiv:1011.1936", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "A uniform convergence bound for the area under the roc curve", "author": ["S. Agarwal", "S. Har-Peled", "D. Roth"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, pages 1\u20138", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "The Journal of Machine Learning Research, 3:463\u2013482", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Pattern recognition and machine learning, volume 4. springer", "author": ["C.M. Bishop"], "venue": "New York,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Minimizing uncertainty in pipelines", "author": ["N.N. Dalvi", "A.G. Parameswaran", "V. Rastogi"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Asymptotic calibration", "author": ["D.P. Foster", "R.V. Vohra"], "venue": "Biometrika, 85(2):379\u2013390", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Decision theoretic generalizations of the pac model for neural net and other learning applications", "author": ["D. Haussler"], "venue": "Information and computation, 100(1):78\u2013150", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["L. Li", "M.L. Littman", "T.J. Walsh", "A.L. Strehl"], "venue": "Machine learning, 82(3):399\u2013443", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Beyond myopic inference in big data pipelines", "author": ["K. Raman", "A. Swaminathan", "J. Gehrke", "T. Joachims"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 86\u201394. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "On the density of families of sets", "author": ["N. Sauer"], "venue": "Journal of Combinatorial Theory, Series A, 13(1):145\u2013147", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1972}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": "Cambridge University Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "On the consistency of multiclass classification methods", "author": ["A. Tewari", "P.L. Bartlett"], "venue": "The Journal of Machine Learning Research, 8:1007\u20131025", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Y. Chervonenkis"], "venue": "Theory of Probability & Its Applications, 16(2):264\u2013280", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1971}, {"title": "Crowder: Crowdsourcing entity resolution", "author": ["J. Wang", "T. Kraska", "M.J. Franklin", "J. Feng"], "venue": "Proceedings of the VLDB Endowment, 5(11):1483\u20131494", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "In standard PAC learning theory [11], these confidence measures are usually validated by bounding the expected value of some loss function (for example, the log-likelihood function).", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "Consistent confidence measures have many applications: they can be used in decision making processes where misclassifications have asymmetric loss [4], in data processing pipelines where output of upstream classifiers are used in downstream programs [9, 5], or in adaptive data verification processes where instances are referred to a human verifier when the automatic classifier is not confident enough [14].", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "Consistent confidence measures have many applications: they can be used in decision making processes where misclassifications have asymmetric loss [4], in data processing pipelines where output of upstream classifiers are used in downstream programs [9, 5], or in adaptive data verification processes where instances are referred to a human verifier when the automatic classifier is not confident enough [14].", "startOffset": 250, "endOffset": 256}, {"referenceID": 4, "context": "Consistent confidence measures have many applications: they can be used in decision making processes where misclassifications have asymmetric loss [4], in data processing pipelines where output of upstream classifiers are used in downstream programs [9, 5], or in adaptive data verification processes where instances are referred to a human verifier when the automatic classifier is not confident enough [14].", "startOffset": 250, "endOffset": 256}, {"referenceID": 13, "context": "Consistent confidence measures have many applications: they can be used in decision making processes where misclassifications have asymmetric loss [4], in data processing pipelines where output of upstream classifiers are used in downstream programs [9, 5], or in adaptive data verification processes where instances are referred to a human verifier when the automatic classifier is not confident enough [14].", "startOffset": 404, "endOffset": 408}, {"referenceID": 12, "context": "\u2022 We apply our result to a class of linear classifiers and show that we can achieve uniform convergence of consistency using essentially the same order of sample size implied by the standard argument based on VC-dimension [13].", "startOffset": 222, "endOffset": 226}, {"referenceID": 6, "context": "Some existing papers have considered the uniform convergence of the expected value of certain loss functions on confidence measures: Haussler [7] considered the expected loss of the action that we take based on confidence measures as opposed to confidence measures themselves; Agarwal et al.", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "[2] studied the uniform convergence of the area under ROC curve.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] proposed a learning framework that explicitly allows a classifier to respond \u201cI don\u2019t know\u201d, but whenever it produces a label it must be correct.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Our notion of consistency is similar to the notion of calibration in prediction theory [6, 1], where the goal is also to make predicted probability values match the empirical frequency of correct predictions.", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "Our notion of consistency is similar to the notion of calibration in prediction theory [6, 1], where the goal is also to make predicted probability values match the empirical frequency of correct predictions.", "startOffset": 87, "endOffset": 93}, {"referenceID": 11, "context": "Tewari and Bartlett [12] studied the consistency of a conditional probability estimator.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "Consider a set of classifiers H, so that each h \u2208 H predicts a confidence measure h(x) \u2208 [0, 1] indicating how likely the label y is 1 given x.", "startOffset": 89, "endOffset": 95}, {"referenceID": 0, "context": ", functions from X to [0, 1].", "startOffset": 22, "endOffset": 28}, {"referenceID": 12, "context": "Define the restriction of A to Sn as: ASn = {A \u2229 Sn : A \u2208 A} We say that A shatters Sn if the restriction of A to Sn contains all subsets of Sn: \u2200B \u2286 Sn, \u2203A \u2208 A, B = A \u2229 Sn or equivalently, |ASn | = 2 |Sn| The VC-Dimension [13] of A is defined as the maximum number of samples that can be arranged so that A shatters them.", "startOffset": 223, "endOffset": 227}, {"referenceID": 9, "context": "Suppose the VC-dimension of A is finite, then Sauer\u2019s Lemma [10][13] can be used to bound the size of ASn .", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "Suppose the VC-dimension of A is finite, then Sauer\u2019s Lemma [10][13] can be used to bound the size of ASn .", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "2 Rademacher Complexity Let H be a collection of functions from X \u00d7 Y to [0, 1], the Rademacher Complexity [3] of H with respect to Sn is defined as: RSn(H) = 1 n E\u03c3\u223c{\u00b11}n [sup h\u2208H n", "startOffset": 73, "endOffset": 79}, {"referenceID": 2, "context": "2 Rademacher Complexity Let H be a collection of functions from X \u00d7 Y to [0, 1], the Rademacher Complexity [3] of H with respect to Sn is defined as: RSn(H) = 1 n E\u03c3\u223c{\u00b11}n [sup h\u2208H n", "startOffset": 107, "endOffset": 110}, {"referenceID": 10, "context": "samples of X \u00d7 Y , then with probability of at least 1\u2212 \u03b4 [11]:", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "2 ln(4/\u03b4) n (4) Sometimes we also allow H to be collection of functions from X to [0, 1] in the above results.", "startOffset": 82, "endOffset": 88}, {"referenceID": 0, "context": "3 Massart Lemma Let H be a collection of functions from X \u00d7 Y to [0, 1].", "startOffset": 65, "endOffset": 71}, {"referenceID": 10, "context": ", h(xn, yn)) : h \u2208 H} If the cardinality of HSn is finite, then the Rademacher Complexity of H with respect to Sn can be bounded as follows [11]: RSn(H) \u2264 sup a\u2208HSn ||a||2 \u221a", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "Let HR be a collection of classifiers that output confidence measures in R, let M be the collection of all left-continuous monotonically increasing functions from R to [0, 1], define H4 = {1p1<m(h(x))\u2264p2 : p1, p2 \u2208 R,m \u2208 M, h \u2208 HR} H5 = {1\u03b1<h(x)\u2264\u03b2 : \u03b1, \u03b2 \u2208 R, h \u2208 HR} Then, RSn(H4) = RSn(H5)", "startOffset": 168, "endOffset": 174}, {"referenceID": 0, "context": "Let X = R and M be the collection of all left-continuous monotonically increasing functions from R to [0, 1].", "startOffset": 102, "endOffset": 108}, {"referenceID": 0, "context": "Assume that we are given classifier h : X \u2192 R, we want to find a left-continuous monotonically increasing function f from R to [0, 1] that minimizes empirical inconsistency measures cemp(f \u25e6 h).", "startOffset": 127, "endOffset": 133}], "year": 2017, "abstractText": "Many classification algorithms produce confidence measures in the form of conditional probability of labels given the features of the target instance. It is desirable to be make these confidence measures calibrated or consistent, in the sense that they correctly capture the belief of the algorithm in the label output. For instance, if the algorithm outputs a label with confidence measure p for n times, then the output label should be correct approximately np times overall. Calibrated confidence measures lead to higher interpretability by humans and computers and enable downstream analysis or processing. In this paper, we formally characterize the consistency of confidence measures and prove a PAC-style uniform convergence result for the consistency of confidence measures. We show that finite VCdimension is sufficient for guaranteeing the consistency of confidence measures produced by empirically consistent classifiers. Our result also implies that we can calibrate confidence measures produced by any existing algorithms with monotonic functions, and still get the same generalization guarantee on consistency.", "creator": "LaTeX with hyperref package"}}}