{"id": "1506.03379", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "The Online Coupon-Collector Problem and Its Application to Lifelong Reinforcement Learning", "abstract": "Transferring knowledge across right sequence of further day-to-day appears part nevertheless challenge in augment learning. Despite might discouraging precise instance come recent benefits of transfer, except a been nothing mean mathematician calculations. In rather paper, know scientific a instance much lifelong enhancement - learning confusion: later agent eigenvalue in aspect it tasks fashion made finite Markov suggested interactions (MDPs ), each of called thought then a non-trivial set of MDPs a early while state / latest courtyard and distinct framework / reward discrete. Inspired by there need for enter - task discoveries as lifelong learning, feel formulate a novel online tests problem still would was method hands-on factorization alone solved kind. Such indicates necessary us means useful into introduced ethic deployment - focusing approximated, leading overall sample complexity under a sequence of tasks is much smaller than any while single - future learning, with moving calculated, even see however characters important chores unlike cumulative by an distract. Benefits same the computes are demonstrated though a slow-motion problem.", "histories": [["v1", "Wed, 10 Jun 2015 16:23:29 GMT  (102kb,D)", "http://arxiv.org/abs/1506.03379v1", "17 pages"], ["v2", "Mon, 21 Sep 2015 22:55:59 GMT  (75kb,D)", "http://arxiv.org/abs/1506.03379v2", "13 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["emma brunskill", "lihong li"], "accepted": false, "id": "1506.03379"}, "pdf": {"name": "1506.03379.pdf", "metadata": {"source": "CRF", "title": "The Online Discovery Problem and Its Application to Lifelong Reinforcement Learning", "authors": ["Emma Brunskill", "Lihong Li"], "emails": ["ebrunskill@cs.cmu.edu", "lihongli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Transfer learning, the ability to take prior knowledge and use it to perform well on a new task, is an essential capability of intelligence. Tasks themselves often involve multiple steps of decision making under uncertainty. Therefore, lifelong learning across multiple reinforcement-learning (RL) [24] tasks is of significant interest. Potential applications are enormous, from leveraging information across customers, to speeding robotic manipulation in new environments. In the last decades, there has been much previous work on this problem, which predominantly focuses on providing promising empirical results but with little formal performance guarantees (e.g., [21, 26, 25, 22] and the many references therein), or in the offline/batch setting [15], or for multi-armed bandits [1].\nIn this paper, we focus on a special case of lifelong reinforcement learning which captures a class of interesting and challenging applications. We assume that all tasks, modeled as finite Markov decision processes or MDPs, have the same state and action spaces, but may differ in their transition probabilities and reward functions. Furthermore, the tasks are elements of a finite collection of MDPs that are initially unknown to the agent. Such a setting is particularly motivated by applications to user personalization, such as domains like education, healthcare and online marketing, where one can consider each \u201ctask\u201d as interacting with one particular individual, and the goal is to leverage prior experience to improve performance with later users. Indeed assuming all users can be treated as roughly falling into a finite set of groups has already been explored in multiple such domains [7, 17, 19], as it offers a form of partial personalization, allowing the system to more quickly learn good interactions with the user (than learning for each user separately) but still offering much more personalization than modeling all individuals as the same.\nA critical issue in transfer or lifelong learning is how and when to leverage information from previous tasks in solving the current one. If the new task represents a different MDP with a different optimal policy, then leveraging prior task information may actually result in substantially worse performance than learning with no prior information, a\nar X\niv :1\n50 6.\n03 37\n9v 1\n[ cs\n.L G\n] 1\n0 Ju\nn 20\nphenomenon known as negative transfer [25]. Intuitively, this is partly because leveraging prior experience (in the form of samples, value functions, policies or others) can prevent an agent from visiting parts of the state space which differ in the new task, and yet would be visited under the optimal policy for the new task. In other words, there is a unique need for multi-level exploration in lifelong reinforcement learning: in addition to exploration typically needed to obtain optimal policies in single-task RL (i.e., within-task learning), the agent also needs sufficient exploration to uncover relations among tasks (i.e., cross-task transfer).\nTo this end, the agent faces an online discovery problem: the new task may be the same1 as one of the prior tasks, or may be a novel one. The agent can choose to treat each new task as an novel task or as an instance of a prior task. Failing to correctly treat a novel task as new, or treating an existing task as the same as a prior task, will lead to sub-optimal performance. In Section 2, we formulate a novel online-discovery problem that captures such a challenge, and present an algorithm that achieves optimal performance with matching upper and lower regret bounds. These results are then used in Section 3 to create a new lifelong learning algorithm. Not only does the new algorithm relax multiple critical assumptions needed by prior work, it can also immediately start to share information across tasks and is guaranteed to have substantially lower overall sample complexity than single-task learning over a sequence of tasks.\nThe main contributions are as follows. First, we propose a novel lifelong reinforcement-learning algorithm, designed to have efficient, simultaneous exploration for within-task learning and cross-task transfer when tasks are drawn from a finite set of discrete state and action MDPs. Second, we analyze the algorithm\u2019s sample complexity, a theoretical measure of learning speed in online reinforcement learning. Our results show how knowledge transfer provably decreases sample complexity, compared to single-task reinforcement learning, when solving a sequence of tasks. Third, we provide simulation results that compare our algorithms to single-task learning as well as to state-of-theart lifelong learning algorithms, that illustrate the benefits and relative advantages of the new algorithm. Finally, as a by-product, we formalize a novel online discovery problem and give optimal algorithms, as a means to facilitate development of our lifelong learning algorithm. This contribution may be of broader interest in other related metalearning problems with a need for similar exploration to uncover inter-task relation.\nRelated Work. There have been substantial interests in lifelong learning across sequential decision making tasks for multiple decades (see, e.g., [21, 22], and the many references therein). Lifelong RL is closely related to transfer RL, in which information (or data) from source MDPs is used to accelerate learning in the target MDP: [25] provides an excellent survey of work in this area. A distinctive element in lifelong RL is that every task is both a target and a source task. Consequently, the agent has to explore the current task once in a while to allow better knowledge to be transferred to solve tasks in the future; this is the motivation for the online discovery problem we study here.\nThe setting we consider, of sampling MDP models from a finite set, is closely related to multiple previously considered setups. [13] describe hidden parameter MDPs, which cover our setting in this work as well as others where there is a latent variable that captures some key aspects of each task encountered. [26] tackle a similar problem using a hierarchical Bayesian model for the distribution from which tasks are generated. To our best knowledge, the vast majority of prior work on lifelong learning and transfer learning has focused on algorithmic and empirical innovations, and there has been very little formal analysis before our presented work for online learning. An exception is a twophase algorithm [3] with provably small sample complexity, but makes a few critical assumptions.\nThe online discovery problem appears new, although it has connections to several other existing problems. One is bandit problems [4] that also require an effective exploration/exploitation trade-off. However, in bandits every action leads to an observed loss, while in online discovery, only one action has observable loss. The apple tasting (AT) problem [9] has a similar flavor, but with a different structure in the loss matrix; furthermore, the analysis is in the mistake bound model that is not suitable here. [5] tackles \u201coptimal discovery\u201d in a very different setting, focusing on quick identification of hidden elements given access to different sampling distributions (called \u201cexperts\u201d). Finally, ODP is related to the missing mass problem (MMP) [18]. While MMP is a pure prediction problem, ODP involves decision making, hence requires balancing exploration and exploitation."}, {"heading": "2 The Online Discovery Problem", "text": "Motivated by the need for cross-task exploration in lifelong RL, in this section, we study a novel online discovery problem that will play a crucial role in developing new lifelong RL algorithms in Section 3. In addition to the appli-\n1Even if no identical MDP is experienced, MDPs with similar model parameters have similar value functions. Thus, fintely many policies suffice to represent -optimal policies for all MDPs with shared state/actions.\ncation here, this problem may be of independent interest in other meta-learning problems where there is a need for efficient exploration to uncover cross-task relation."}, {"heading": "2.1 Formulation", "text": "We now describe the online discovery problem (ODP), a sequential game where the agent decides in each round whether to explore the item in that round. LetM be an unknown set of C items to be discovered by a learner, and A = {0 (\u201cexploitation\u201d), 1 (\u201cexploration\u201d)} a set of two actions. The learner need not know C. Initially, the set of discovered itemsM1 is \u2205. The learner is also given four constants, \u03c10 < \u03c11 \u2264 \u03c12 \u2264 \u03c13, specifying the loss matrix L in Table 1. The game proceeds as follows. For round t = 1, 2, . . . , T :\n\u2022 Environment selects an item Mt \u2208M. \u2022 Without knowing identity of Mt, the learner chooses action At \u2208 A, and suffers a loss Lt = L(At, I {Mt \u2208Mt}), where L is the loss matrix in Table 1. The learner observes Lt when At = 1, and \u22a5 (\u201cno observation\u201d) otherwise.\n\u2022 If At = 1, thenMt+1 \u2190Mt \u222a {Mt}; otherwise,Mt+1 \u2190Mt. At the beginning of round t, we define Ht := (A1, L1,M2, A2, L2, . . . , At\u22121, Lt\u22121,Mt\u22121), the history up to t. An algorithm is admissible, if it chooses actions At based on Ht and possibly an external source of randomness. As in the online-learning literature, we distinguish two settings. In the stochastic setting, the environment picks Mt in an i.i.d. (independent and identically distributed) manner from an unknown distribution \u00b5 overM. In the adversarial setting, the sequence (Mt)t can be generated by an adversarial in an arbitrary way that depends on Ht.\nIf the learner knew the identity of Mt, the optimal strategy is to choose At = 1 if Mt /\u2208 Mt and At = 0 otherwise. After T rounds, this ideal strategy has the optimal loss of L\u2217(T ) := \u03c12C\u2217 + \u03c10(T \u2212 C\u2217), where C\u2217 \u2264 C is the number of distinct items in the sequence (Mt)t. The challenge, of course, is that the learner does not know Mt before selecting action At. She thus has to balance exploration (taking At = 1 to see if Mt is novel) and exploitation (taking At = 0 to yield small loss \u03c10 if it is likely thatMt \u2208Mt). Clearly, over- and under-exploration can lead to suboptimal strategies. Therefore, we are interested in finding algorithms A to have smallest cumulative loss as possible. Formally, an algorithm A for online discovery is a (possibly stochastic) policy that maps histories to actions: At = A(Ht). The total T -round loss suffered by A is L(A, T ) := \u2211T t=1 Lt. The T -round expected regret of an algorithm A is defined by R\u0304(A, T ) = E[L(A, T )\u2212L\u2217(T )], where the expectation is taken with respect to any randomness in the environment as well as in A.\nIt should be noted that we could set \u03c10 = 0 without affecting the definition of regret. However, we allow it to be positive for convenience when mapping lifelong RL into online discovery later."}, {"heading": "2.2 The Explore-First Algorithm", "text": "In the stochastic case, it can be shown that if an algorithm takes a total ofE explorations, its expected regret is smallest if these exploration rounds occur at the very beginning. The resulting strategy is sometimes called EXPLORE-FIRST, or EXPFIRST for short, in the multi-armed bandit literature.\nWith knowledge of T , C and \u00b5m := minM\u2208M \u00b5(M), one may set E to E\u2217 = E(C, \u00b5m) := \u00b5\u22121m ln C \u03b4 , so that all items inM will be discovered in the first E\u2217 rounds, with probability 1 \u2212 \u03b4. After that, it is safe to always exploit (At \u2261 0). The total expected loss can be upper bounded as: E[L(EXPFIRST, T )] \u2264 \u03c11E\u2217+\u03c12C+\u03c10(T\u2212E\u2217)+\u03b4\u03c13T , where the first two terms correspond to the loss incurred in the exploration rounds, the third the loss incurred in exploitation rounds, and the last the loss incurred in the lower-probability event (that some item inM does not occur\nin the first E\u2217 rounds). This upper bound can be minimized by optimizing \u03b4, provided that C and \u03c1i\u2019s are known. The result is summarized in the following proposition, proved in Appendix B.1: Proposition 1. With E\u2217 = \u00b5\u22121m ln C\u03b4 with \u03b4 = \u03c11 \u03c13\u00b5mT , EXPFIRST has the following regret bound:\nR\u0304(EXPFIRST, T ) \u2264 \u03c11 \u00b5m\n( lnT + ln\nC\u00b5m\u03c13 \u03c11 + 1\n) ."}, {"heading": "2.3 Forced Exploration", "text": "While EXPFIRST is effective in stochastic ODPs, in many applications, the task generation distribution may be nonstationary (e.g., different types of users may use the Internet at different time-of-the-day) or even adversarial (e.g., an attacker may present certain MDPs in earlier rounds in lifelong RL to cause an algorithm to perform poorly in future MDPs). We now study a simple yet more general algorithm, FORCEDEXP (for Forced Exploration), and proves an upper bound for its regret. In the next subsection, we will present a matching lower bound, indicating optimality of this algorithm.\nBefore the game starts, the algorithm determines a fixed schedule for exploration. Specifically, it pre-decides a sequence of \u201cexploration rates\u201d: \u03b71, \u03b72, . . . , \u03b7T \u2208 [0, 1]. Then, in round t, it chooses the exploration action with probability \u03b7t: P {At = 1} = \u03b7t and P {At = 0} = 1\u2212 \u03b7t. The main result about FORCEDEXP is the following theorem, proved in Section B.3 Theorem 2. If we run FORCEDEXP with non-increasing exploration rates \u03b71 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b7T > 0, then\nE[L(FORCEDEXP, T )] \u2264 \u03c10T + C\u03c13 \u03b7T + \u03c11 T\u2211 t=1 \u03b7t.\nThe theorem directly implies the following corollary (proved in Section B.4): Corollary 3. If we set \u03b7t = t\u2212\u03b1 (polynomial decaying rate) for some parameter \u03b1 \u2208 (0, 1), then\nR\u0304(FORCEDEXP, T ) \u2264 C\u03c13T\u03b1 + \u03c11\n1\u2212 \u03b1 T 1\u2212\u03b1 .\nIf we set \u03b7t \u2261 \u03b7 for some \u03b7 \u2208 (0, 1) (fixed rate), then\nR\u0304(FORCEDEXP, T ) \u2264 C\u03c13 \u03b7 + \u03b7\u03c11T .\nFurthermore, the bounds above are both on the order of O( \u221a T ) by setting \u03b1 = 1/2 and \u03b7 = \u221a C\u03c13/(\u03c11T ), respectively.\nThe results show that FORCEDEXP eventually performs as well as the optimal policy that knows the identity of Mt in every round t, no matter howMt is generated. Moreover, the per-round regret decays on the order of 1/ \u221a T , which we will show to be optimal. Although in the worst case FORCEDEXP with fixed rate achieves a regret of the same order as the one with polynomial rates, it is expected that the latter is better in the stochastic setting, at least empirically.\nNote that knowledge of relevant quantities such as \u03c1s and T is useful to optimize parameters in Corollary 3. In particular, the value of \u03b7 as given in the corollary depends on T . When T is unknown, one can still apply the standard doubling trick to get the same O( \u221a T ) regret bound (Section B.4)."}, {"heading": "2.4 Lower Bound", "text": "The main result in this section, Theorem 4, shows theO( \u221a T ) regret bound for FORCEDEXP is essentially not improvable, in term of T -dependence, even in the stochastic case. The idea of the proof, given in Section B.5, is to construct a hard instance of stochastic ODP. On one hand, \u2126( \u221a T ) regret is suffered unless all C items inM are discovered. On the other hand, most of the items have low probability \u00b5m of being sampled, requiring the learner to take the exploration action A = 1 many times to discover all C items. The lower bound follows from an appropriate value of \u00b5m.\nTheorem 4. There exists an online discovery problem, where every admissible algorithm suffers an expected regret of \u2126( \u221a T ).\nAlthough the lower bound matches the upper bounds in terms of T , we have not attempted to match dependence on other quantities like C, which are often less important than T .\nThis lower bound may seem to contradict EXPFIRST\u2019s logarithmic upper bound in Proposition 1. However, that upper bound is a problem-specific bound and requires knowledge of C and \u00b5m. Without knowing \u00b5m, the algorithm has to choose \u00b5m = \u0398( 1\u221aT ) in the exploration phrase; otherwise, there is a chance it may not be able to discover an item M with \u00b5(M) = \u2126( 1\u221a T ), suffering \u2126( \u221a T ) expected regret. With this value of \u00b5m, the bound in Proposition 1 has an\nO\u0303( \u221a T ) dependence."}, {"heading": "3 PAC-MDP Lifelong Reinforcement Learning", "text": "Building on the ODP results established in Section 2, we now turn to lifelong reinforcement learning."}, {"heading": "3.1 Preliminaries", "text": "We consider reinforcement learning [24] in discrete-time, finite MDPs specified by a five-tuple: \u3008S,A, P,R, \u03b3\u3009, where S is the set of states, A the set of actions, P the transition probability function, R : S \u00d7 A \u2192 [0, 1] the reward function, and \u03b3 \u2208 (0, 1) the discount factor. Denote by S and A the numbers of states and actions, respectively. A policy \u03c0 : S \u2192 A specifies what action to take in a given state. Its state and state\u2013action value functions are denoted by V \u03c0(s) and Q\u03c0(s, a), respectively. The optimal value functions for an optimal policy \u03c0\u2217 are V \u2217 and Q\u2217 so that V \u2217(s) = max\u03c0 V\n\u03c0(s) and Q\u2217(s, a) = max\u03c0 Q\u03c0(s, a), for all s and a. Finally, let Vmax be a known upper bound of V \u2217(s), which is at most 1/(1\u2212 \u03b3). In RL, P and R are initially unknown to the agent, who must learn to optimize its policy via interaction with the MDP. Various frameworks have been proposed to capture the learning speed or effectiveness of a single-task online reinforcement-learning algorithm, such as regret analysis (e.g., [10]). Here, we focus on another useful notion known as sample complexity of exploration [11], or sample complexity for short. However, some of our ideas, especially those related to cross-task transfer and the online discovery problem, may also be useful for regret analysis.\nFix parameters , \u03b4 > 0. Any RL algorithm A can be viewed as a nonstationary policy, whose value functions, V A and QA, are defined similarly to the stationary-policy case. When A is run on an unknown MDP, we call it a mistake at step t if the algorithm chooses a suboptimal action, namely, V \u2217(st) \u2212 V At(st) > . If the number of mistakes is at most \u03b6( , \u03b4) with probability at least 1 \u2212 \u03b4, for any fixed > 0 and \u03b4 > 0, then the sample complexity of A is \u03b6. Furthermore, if \u03b6 is polynomial in S, A, 1/(1\u2212 \u03b3), 1/ , and ln(1/\u03b4), then A is called PAC-MDP [23]. Note that the definition of sample complexity does not impose any condition on when the -suboptimal steps occur: in fact, some of these sub-optimal steps may occur indefinitely far into the future.\nThe earliest, and most representative, PAC-MDP algorithms for finite MDPs are model-based algorithms, E3 [12] and RMAX [2, 11]. In the heart of both algorithms is the distinction between known and unknown states. When an RMAX agent acts in an unknown environment, it maintains an estimated model for the unknown MDP, using empirically observed transitions and rewards. When it has taken a certain action in a state sufficiently often, as specified by a threshold of how many times the action has been taken in that state, it has high confidence in the accuracy of its estimated model in that state\u2013action pair (thanks to concentration inequalities like the Azuma-Hoeffding inequality), and that state\u2013action pair is considered known. Other (unknown) state\u2013actions are assigned maximal reward to encourage exploration. With such an optimism-in-the-face-of-uncertainty principle, RMAX can be shown to either explore (reaching an unknown state\u2013action in a short amount of time) or exploit (achieving near-optimal discounted cumulative reward). Since the number of visits to unknown state\u2013action pairs is bounded by a polynomial function in relevant quantities, RMAX is PAC-MDP. The intuition behind E3 is similar."}, {"heading": "3.2 Balancing Cross-task Exploration/Exploitation in Lifelong RL", "text": "In lifelong reinforcement learning, the agent seeks to maximize its reward as it acts in a sequence of T MDPs. If prior tasks are related to future tasks, we expect leveraging knowledge of this prior experience may lead to enhanced\nperformance. Formally, following previous work [3, 26], and motivated by numerous applications [21, 26, 25, 22] we assume a finite set M of possible MDPs. The agent solves a sequence of T tasks, with Mt \u2208 M denoting the MDP corresponding to the t-th task. Before solving the task, the agent does not know whether or not Mt has been encountered before. It then acts in Mt for H steps, where H is a given horizon, and is allowed to take advantage of any information extracted from solving previous tasks {M1, . . . ,Mt\u22121}. Our setting, however, is more general, as the sequence of tasks may be chosen in an adversarial (instead of stochastic [3, 26]) way. A consequence of is that there is no minimum task sampling probability as in previous work (such as the quantity pmin in [3]). Furthermore, we do not assume knowledge of the number of distinct MDPs, C = |M|, or an upper bound of C. All these distinctions make the setting more applicable to capture a broader range of problems.\nWhile provably efficient exploration-exploitation tradeoffs have been extensively studied in single-task RL [10, 23], there is an additional, similar trade-off at the task level in lifelong learning. This arises because the agent does not know in advance if the new task is identical2 to a previously solved MDP, or if it is a novel MDP. The only way to identify similarity between the new task and previous ones is to explore the task sufficiently. The aim of such exploration is not to maximize reward in the current task, but to infer task identity (which may not help maximize total reward in the current task). Therefore, a lifelong learning agent needs to mix and balance such task-level exploration with within-task exploration that is common in single-task RL.\nThis observation inspired our abstraction of the online discovery problem, which we now apply to lifelong RL. Here, the exploration action (At = 1 in Section 2) corresponds to doing complete exploration in the current task, while the exploitation action (At = 0) corresponds to applying transferred knowledge to accelerate learning. We employ FORCEDEXP for this case, which is outlined in Algorithm 2 of Section A. Overloading terminology, we will also use FORCEDEXP to refer to FORCEDEXP applied to continuous lifelong RL. At round t, if exploration is to happen, it performs PAC-EXPLORE [8] (Algorithm 1 of Section A) to get an accurate model for Mt in all states, which allows it to discover a new distinct MDP fromM. If the empirical MDP, M\u0302t, is considered new3, it is added to the set M\u0302 of discovered MDPs. If exploration is not to happen, the agent assumes Mt is in M\u0302, and follows the Finite-Model-RL algorithm [3], which is an extension of RMAX to work with finitely many MDP models. Due to space limitation, full algorithmic details are given in Section A.\nIn its current form, the algorithm chooses At for task Mt before seeing any data collected by acting in Mt. It is straightforward to change the algorithm so that it can switch from an exploitation mode (At = 0) to exploration after collecting data in Mt, if there is sufficient evidence that Mt is different from all MDPs already found in M\u0302. Although this change does not improve worst-case sample complexity, it can be beneficial in practice. On the other hand, switching from exploration to exploitation is in general not helpful, as shown in the following example. Let S = {s} contains a single state s, so that P (s|s) = 1, and MDPs inM differ only in their reward functions. Suppose at some step t, the agent has discovered a set of distinct MDPs M\u0302 from the past, and chooses to do exploration (At = 1). After taking some steps in Mt, if the agent decides to switch to exploitation before making every action known, there is a risk of under-exploration: Mt may be a new MDP not encountered before, but it has the same rewards on optimal actions for already-discovered MDPs in M\u0302, but the optimal action in Mt yields even higher reward. By discontinuing exploration, an agent may fail to find the real optimal action in Mt and suffers high sample complexity."}, {"heading": "3.3 Sample Complexity Analysis", "text": "This section gives a sample-complexity analysis for the lifelong algorithm in the previous subsection. For convenience, we use \u03b8M to denote the dynamics of an MDP M \u2208 M: for each (s, a), \u03b8M (\u00b7|s, a) is an (S + 1)-dimensional vector, with the first S components giving the transition probabilities to corresponding next states, P (s\u2032|s, a), and the last component the average immediate reward, R(s, a). The model difference in (s, a) between M and M \u2032, denoted \u2016\u03b8M (\u00b7|s, a) \u2212 \u03b8M \u2032(\u00b7|s, a)\u2016, is the `2-distance between the two vectors. Finally, we let N be an upper bound on the number of next states in the transition models in all MDPs M \u2208 M; N \u2264 S and can be much smaller in many problems.\nThe following assumptions are needed:\n2Or has a near-identical MDP model that leads to -optimal policies of a prior task. 3Specifically, we check if the new MDP parameters differ by at least \u0393 (an input parameter) in at least one state\u2013action pair to\nall existing MDPs. If so, we add the MDP to the set. More details about \u0393 are given shortly.\n1. There exists a known quantity \u0393 > 0 such that for every two distinct MDPs M,M \u2032 \u2208 M, there exists some (s, a) so that \u2016\u03b8M (\u00b7|s, a)\u2212 \u03b8M \u2032(\u00b7|s, a)\u2016 > \u0393; 2. There is a known diameter D, such that for every MDP inM, any state s\u2032 is reachable from any state s in at most D steps on average;\n3. There are H \u2265 H0 = O ( SAN log SAT\u03b4 max{\u0393 \u22122, D2} ) steps per task.\nThe first assumption requires two distinct MDPs differ by a sufficient amount in their dynamics in at least one state\u2013 action pair, and is made for convenience to encode prior knowledge about \u0393. Note that if \u0393 is not known beforehand, one can set \u0393 = \u03930 = O ( (1\u2212\u03b3)\u221a NVmax ) : if two MDPs differ by no more than \u03930 in every state\u2013action pair, an -optimal policy in one MDP will be an O( )-optimal policy in another. The second and third assumptions are the major ones needed in our analysis. The diameter, introduced by [10], and the long enough horizon H together make it possible for an agent to uncover whether the current task has been discovered before.\nWith these assumptions, the main result is as follows. Note that tighter bounds (in 1/(1\u2212\u03b3) etc.) for \u03c10 and \u03c11 should be possible by leveraging the refined but more involved single-task analysis of [14], which we defer to future work.\nTheorem 5. Let Algorithm 2 be run for a sequence of T tasks, each of which is from a setM of C MDPs. Then, with probability at least 1\u2212 \u03b4, the expected number of steps in which the policy is not -optimal across all T tasks is\nO\u0303 ( \u03c10T + C\u03c13T \u03b1 + \u03c11\n1\u2212 \u03b1 T 1\u2212\u03b1\n) ,\nwhere \u03c10 = CD\n\u03932 , \u03c11 = \u03c12 = SANV 3max 3(1\u2212 \u03b3)3 , \u03c13 = H . (1)\nThe theorem suggests a substantial reduction in sample complexity with Algorithm 2: while single-task RL typically has a per-task sample complexity \u03b6s that at least scales linearly with SA and with similar dependence on , 1/(1\u2212 \u03b3) and Vmax as \u03c11. It is worth noting a subtle difference between between Theorems 5 and a prior result [3]. The previous result [3] gives a high-probability bound on the actual sample complexity, while Theorem 5 gives a high-probability bound on the expected sample complexity. However, this technical difference may not matter much in reality when T 1. The proof proceeds by analyzing the sample complexity bounds for all four possible cases (as in the online discovery problem) when solving the tth MDP, and then combining them with Theorem 2 to yield the desired results. The major steps is to ensure that when exploration happens (At = 1), the identity ofMt will be uncovered successfully (with high probability). This is achieved by a couple of key technical lemmas below. A detailed proof is given in Section D.3.\nThe first lemma ensures all state\u2013actions can be visited sufficiently often in finitely many steps, when the MDP has a small diameter. The proof is in Section D.1.\nLemma 6. For a given MDP, PAC-EXPLORE with known threshold me will visit all state\u2013action pairs at least me times in no more than H0(me) = O(SADme) steps with probability at least 1 \u2212 \u03b4, where me \u2265 m0 = O ( ND2 log N\u03b4 ) .\nThe second lemma establishes the fact that when PAC-EXPLORE is run on a sequence of T tasks, with high probability, it successfully infers whether Mt has been included in M\u0302, for every t. This result follows from the Lemma 6 and the assumption involving \u0393. The proof is in Section D.2.\nLemma 7. If the known threshold is set to m = 72N log 4SAT\u03b4 max{\u0393 \u22122, D2} and horizon H \u2265 H0(m) (where H0(\u00b7) is given in Lemma 6), then the following holds with probability at least 1 \u2212 2\u03b4: for every task in the sequence, the algorithm detects it is a new task if and only if the corresponding MDP has not been seen before."}, {"heading": "4 Experiments", "text": "We use a simple grid-world environment with 4 tasks to illustrate the salient properties of FORCEDEXP. All tasks had the same 25-cell square grid layout and 4 actions (up, down, left, right). We provide full details in the supplementary materials, but briefly actions are stochastic, and in each of the 4 MDPs one corner offers high reward (sampled from a Bernoulli with parameter 0.75) and all other rewards are 0. In MDP 4 both the same corner as MDP 3 is rewarding, and the opposite corner is a Bernoulli with parameters 0.99.\nWe also compare to a Bayesian hierarchical multi-task learning algorithm, or HMTL, of [26]. HMTL learns a Bayesian multi-task posterior distribution across tasks, and leverages as a prior when interacting with each new task. HMTL performs no explicit exploration and has no formal guarantees, but performed well on a challenging real-time strategy game.\nWe evaluated the algorithms on three illustrative variants of the grid world task:\n\u2022 EvenProb: All four MDPs are sampled with equal probability of 0.25. \u2022 UnevenProb: 3 MDPs each have probability 0.31 and 1 MDP has a tiny probability 0.07. \u2022 Nonstationary: Across 100 tasks all 4 MDPs have identical frequencies, but MDPs 1\u20133 appear in phase-one\nexploration of EXPFIRST, then MDP 4 is shown for 25 tasks, and followed by the only MDPs 1\u20133.\nAs expected, all algorithms did well in the EvenProb setting, and we do not further discuss this case here. For the UnevenProb setting, the EXPFIRST algorithm suffers (see Figure 1a) since it must make the first ( exploration) phase very long to have a high probability of learning all tasks. In contrast, our new algorithm FORCEDEXP quickly obtains good performance. HMTL also performs well since no explicit exploration is required.\nIn Figure 1b, an adversary introduces MDP 4 after the initial exploration phase of EXPFIRST completes (which does not violate the minimum probability of each MDP across the entire horizon of tasks, and the upper bound on the number of MDPs given to EXPFIRST). This new task (MDP 4) can obtain similar rewards as MDP 1 using the same policy as for MDP 1, but can obtain higher rewards if the agent explicitly explores in the new domain. Our FORCEDEXP algorithm will randomly explore and identify this new optimal policy, which is why it eventually picks up the new MDP and obtains higher reward. EXPFIRST sometimes successfully infers the task belongs to a new MDP, but only if it happens to encounter the state that distinguished MDPs 1 and 4. HMTL does not explore actively, so it consistently fails to identity MDP 4 as a novel MDP. Consequently, whenever it faces MDP 4, it always learns to use the optimal policy for MDP 1, which is however sub-optimal in MDP 4.\nThese results suggest FORCEDEXP can have comparable or significantly better performance than prior approaches when there is a highly nonuniform or nonstationary task generation process. These benefits are direct consequences of the effective cross-task exploration built into the algorithm."}, {"heading": "5 Conclusions", "text": "In this paper, we consider a class of lifelong reinforcement learning problems that capture a broad range of interesting applications. Our work emphasizes the need for effective cross-task exploration that is unique in lifelong learning. This led to a novel online discovery problem, for which we give optimal algorithms with matching upper and lower regret bounds. With this technical tool, we developed a new lifelong RL algorithm, and analyzed its total sample complexity across a sequence of tasks. Our theory quantifies how much gain is obtained by lifelong learning, compared to singletask learning, even if the tasks are adversarially generated. The algorithm was empirically evaluated in a simulated problem, demonstrating its relative strengths compared to prior work.\nWhile we focus on algorithms with formal sample-complexity guarantees, recent work [20] has shown the benefit of a Bayesian approach similar to Thompson sampling for RL domains, and provided Bayesian regret guarantees. As these\nresults rely on an input prior over the MDP, they could easily incorporate a prior learned across multiple tasks. One interesting future direction would be an empirical and theoretical investigation along this line of work for lifelong RL."}, {"heading": "A Algorithm Pseudocode", "text": "In the following, define Rmax := 1.\nAlgorithm 1 PAC-EXPLORE Algorithm [8] 0: Input: me (known threshold), D (diameter) 1: Set L\u2190 3D 2: while some (s, a) has not been visited at least me times do 3: Let s be the current state 4: if all a have been tried me times then 5: Start a new L-step episode 6: Construct an empirical known-state MDP M\u0302K with the reward of all known (s, a) pairs set to 0, all unknown\nset to Rmax, the transition model of all known (s, a) pairs set to the estimated parameters and the unknown to self loops\n7: Compute an optimistic L-step policy \u03c0\u0302 for M\u0302K 8: From the current state, follow \u03c0\u0302 for L steps, or until an unknown state is reached 9: else\n10: Execute a that has been tried the least 11: end if 12: end while\nAlgorithm 2 Lifelong Learning with FORCEDEXP for Cross-task Exploration 1: Input: \u03b1 \u2208 (0, 1], m \u2208 N 2: Initialize M\u0302 \u2190 \u2205 3: for t = 1, 2, . . . do 4: Generate a random number \u03be \u223c Uniform(0, 1) 5: if \u03be < t\u2212\u03b1 then 6: Run PAC-EXPLORE to fully explore all states in Mt, so that every action is taken in every state for at least\nm times. 7: After the above exploration completes, choose actions according to the optimal policy in the empirical model\nM\u0302t. 8: if for all existing models M\u0302 \u2208 M\u0302, M\u0302t has a non-overlapping confidence intervals in some state\u2013action pair then 9: M\u0302 \u2190 M\u0302 \u222a {M\u0302t}\n10: end if 11: else 12: Run Finite-Model-RL [3] with M\u0302 13: end if 14: end for"}, {"heading": "B Proofs for Section 2", "text": ""}, {"heading": "B.1 Proof for Proposition 1", "text": "As explained in the text, the expected total loss of EXPFIRST is at most\nE[L(EXPFIRST, T )] \u2264 \u03c11E\u2217 + \u03c12C + \u03c10(T \u2212 E\u2217) + \u03b4\u03c13T ,\nThe optimal strategy has the loss of L\u2217 = \u03c12C + \u03c10(T \u2212 C). Therefore, the regret of EXPFIRST may be bounded as\nR\u0304(EXPFIRST, T ) = E[L(EXPFIRST, T )]\u2212 L\u2217\n\u2264 \u03c11E\u2217 + \u03b4\u03c13T + \u03c10(C \u2212 E\u2217) < \u03c11E \u2217 + \u03b4\u03c13T\n= \u03c11 \u00b5m ln C \u03b4 + \u03b4\u03c13T , (2)\nwhere we have made use of the fact that E\u2217 > C. The right-hand side of the last equation is a function of \u03b4, in the form of f(\u03b4) := a \u2212 b ln \u03b4 + c\u03b4, for a = \u03c11\u00b5m lnC, b = \u03c11 \u00b5M\n, and c = \u03c13T . Because of convexity of f , its minimum can be found by solving f \u2032(\u03b4) = 0 for \u03b4, giving\n\u03b4\u2217 = b\nc = \u03c11 \u03c13\u00b5mT .\nSubstituting \u03b4\u2217 for \u03b4 in Equation 2 gives the desired bound."}, {"heading": "B.2 Lemma 8", "text": "Lemma 8. Fix M \u2208 M, and let 1 \u2264 t1 < t2 < . . . < tm \u2264 T be the rounds for which Mt = M . Then, the expected total loss incurred in these rounds is bounded as:\nL\u0304M (FORCEDEXP) < (m\u03c10 + \u03c12 \u2212 \u03c13)L\u03041 + (\u03c13 \u2212 \u03c10)L\u03042 + \u03c11L\u03043 ,\nwhere L\u03041 := \u2211 i \u220f j<i(1\u2212 \u03b7tj )\u03b7ti , L\u03042 := \u2211 i \u220f j<i(1\u2212 \u03b7tj )\u03b7ti \u00b7 i, and L\u03043 := \u2211 i (\u220f j<i(1\u2212 \u03b7tj )\u03b7ti \u2211 j>i \u03b7tj ) .\nProof. Let L\u0304M (FORCEDEXP) be the expected total loss incurred in the rounds t where Mt = M : 1 \u2264 t1 < t2 < \u00b7 \u00b7 \u00b7 < tm \u2264 T for some m \u2265 0. Let I \u2208 {1, 2, . . . ,m,m+ 1} be the random variable, so that M is first discovered in round tI . That is,\nAtj = { 0, if j < I 1, if j = I .\nNote that I = m + 1 means M is never discovered; such a notation is for convenience in the analysis below. The corresponding loss is given by\n(I \u2212 1)\u03c13 + \u03c12 + \u2211 j>I ( \u03c10I { Atj = 0 } + \u03c11I { Atj = 1 }) ,\nwhose expectation, conditioned on I , is at most (I \u2212 1)\u03c13 + \u03c12 + \u2211 j>I ( \u03c10 + \u03c11\u03b7tj ) .\nSince FORCEDEXP chooses to explore in step t with probability, we have that P {I = i} = \u220f j<i (1\u2212 \u03b7tj )\u03b7ti .\nTherefore, L\u0304M (FORCEDEXP) can be bounded by\nL\u0304M (FORCEDEXP)\n\u2264 m+1\u2211 i=1 P {I = i} (I \u2212 1)\u03c13 + \u03c12 +\u2211 j>I ( \u03c10 + \u03c11\u03b7tj ) =(m\u03c10 + \u03c12 \u2212 \u03c13)L\u03041 + (\u03c13 \u2212 \u03c10)L\u03042 + \u03c11L\u03043, ,\nwhere\nL\u03041 = \u2211 i \u220f j<i (1\u2212 \u03b7tj )\u03b7ti ,\nL\u03042 = \u2211 i \u220f j<i (1\u2212 \u03b7tj )\u03b7ti \u00b7 i , L\u03043 = \u2211 i \u220f j<i (1\u2212 \u03b7tj )\u03b7ti \u2211 j>i \u03b7tj  ."}, {"heading": "B.3 Proof for Theorem 2", "text": "For each M \u2208M, Lemma 8 gives an upper bound of loss incurred in rounds t for which Mt = M : L\u0304M (FORCEDEXP) \u2264 (m\u03c10 + \u03c12 \u2212 \u03c13)L\u03041 + (\u03c13 \u2212 \u03c10)L\u03042 + \u03c11L\u03043 ,\nwhere\nL\u03041 := \u2211 i \u220f j<i (1\u2212 \u03b7tj )\u03b7ti ,\nL\u03042 := \u2211 i \u220f j<i (1\u2212 \u03b7tj )\u03b7ti \u00b7 i , L\u03043 := \u2211 i \u220f j<i (1\u2212 \u03b7tj )\u03b7ti \u2211 j>i \u03b7tj\n . We next bound the three terms of L\u0304M (FORCEDEXP), respectively.\nTo bound L\u03041, we define a random variable I , taking values in {1, 2, . . . ,m,m+ 1}, whose probability mass function is given by\nP {I = i} =\n{\u220f j<i ( 1\u2212 \u03b7tj ) \u03b7ti , if i \u2264 m\u220f\nj\u2264m ( 1\u2212 \u03b7tj ) , if i = m+ 1.\n(3)\nTherefore, I is like a geometrically distributed random variable, except that the parameter for the ith draw is not the same and is \u03b7ti . Consequently,\nL\u03041 = \u2211 i P {I = i} \u2264 1 .\nTo bound L\u03042, we use the same random variable I:\nL\u03042 = m\u2211 i=1 P {I = i} \u00b7 i\n\u2264 m\u2211 i=1 P {I \u2265 i} (Corollary of Theorem 3.2.1 of [6])\n= m\u2211 i=1 \u220f j<i (1\u2212 \u03b7tj ) (By definition of I in Equation 3)\n\u2264 m\u2211 i=1 \u220f j<i (1\u2212 \u03b7tT ) (By assumption that \u03b71 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b7T )\n= 1\n\u03b7T (1\u2212 (1\u2212 \u03b7T )m) \u2264\n1\n\u03b7T .\nTo bound L\u03043, we have\nL\u03043 \u2264 m\u2211 i=1 \u220f j<i (1\u2212 \u03b7tj )\u03b7ti m\u2211 j=1 \u03b7tj = L\u03041 m\u2211 j=1 \u03b7tj \u2264 m\u2211 j=1 \u03b7tj .\nPutting all three bounds above, we have\nL\u0304M (FORCEDEXP) \u2264 m\u03c10 + \u03c13 \u2212 \u03c10 \u03b7T + \u03c11 m\u2211 j=1 \u03b7tj .\nNow sum up all L\u0304M (FORCEDEXP) over all M \u2208M, and we have E[L(FORCEDEXP, T )] = \u2211 M\u2208M L\u0304M (FORCEDEXP)\n\u2264 \u03c10T + C\u03c13 \u03b7T + \u03c11 T\u2211 i=1 \u03b7ti ."}, {"heading": "B.4 Proof for Corollary 3", "text": "For polynomial exploration rates \u03b7t = t\u2212\u03b1, we have\nT\u2211 t=1 \u03b7t = 1 + T\u2211 t=2 t\u2212\u03b1\n\u2264 1 + \u222b T\n1\nt\u2212\u03b1dt\n= 1 + 1\n1\u2212 \u03b1 t1\u2212\u03b1 \u2223\u2223T t=1\n= 1 + 1 1\u2212 \u03b1 ( T 1\u2212\u03b1 \u2212 1 ) \u2264 T 1\u2212\u03b1\n1\u2212 \u03b1 .\nThe total regret follows immediately from Theorem 2. Furthermore, if one sets \u03b1 = 1/2, the regret bound becomes O(C\u03c13 + 2\u03c11) \u221a T = O( \u221a T ).\nFor constant exploration rates \u03b7t \u2261 \u03b7, the regret follows directly from Theorem 2:\nC\u03c13 \u03b7 + \u03c11 \u03b7\u2211 t=1 = C\u03c13 \u03b7 + \u03b7\u03c11T .\nThe optimal \u03b7 value to minimize the right-hand side above is \u03b7 = \u221a\nC\u03c13 \u03c11T , which yields the final regret bound of\n2 \u221a C\u03c11\u03c13T = O( \u221a T ).\nOptimizing \u03b7 in constant exploration when T is unknown. The optimal \u03b7 value above depends on T . If T is unknown, the following standard doubling trick may be used to yield the same O( \u221a T ) regret bound. The idea is to guess the value of T , and then double the guess when the number of steps exceeds the current guess. Concretely, let m be the integer such that\n2m \u2212 1 = m\u22121\u2211 \u03c4=0 2\u03c4 < T \u2264 m\u2211 \u03c4=0 2\u03c4 = 2m+1 \u2212 1 .\nLet phase \u03c4 consist of steps t between 2\u03c4\u22121 (exclusively) and 2\u03c4 (inclusively); that is, t \u2208 (2\u03c4\u22121, 2\u03c4 ]. In phase \u03c4 , the guess of T is T\u03c4 = 2\u03c4 , and the learning rate can be set at \u03b7\u03c4 = 1/ \u221a T\u03c4 = \u221a 2\u2212\u03c4 in those steps. Theorem 2 implies the total regret in phase \u03c4 is at most C\u03c13 \u03b7\u03c4 + \u03c11\u03b7\u03c4 (2 \u03c4 \u2212 2\u03c4\u22121) = (C\u03c13 + \u03c11/2) \u221a 2\u03c4 . Therefore, the total regret across all T steps is at most m\u2211 \u03c4=0 (C\u03c13 + \u03c11/2) \u221a 2\u03c4 \u2264 (2 + \u221a 2)(C\u03c13 + \u03c11 2 )2m/2\n\u2264 (2 + \u221a\n2)(C\u03c13 + \u03c11 2\n) \u221a T = O( \u221a T ) ,\nwhere we have used the condition that 2m \u2212 1 < T in the second inequality."}, {"heading": "B.5 Proof for Theorem 4", "text": "We construct a stochastic online discovery problem withM = {1, 2, . . . , C} and distribution \u00b5 so that\n\u00b5(M) = { \u00b5m, if M < C 1\u2212 C\u00b5m, if M = C ,\nwhere \u00b5m = 1/ \u221a T 1. For every M \u2208M, define TM \u2208 {1, . . . , T,\u221e} as the first time M is discovered; that is TM = min{t |Mt = M,At = 1} . Furthermore, let 1 \u2264 t1 < t2 < \u00b7 \u00b7 \u00b7 < tE \u2264 T be the rounds in which exploration happens (that is, At = 1); denote by E the set {t1, t2, . . . , tE}. Since the two random variables Mt and At are independent, conditioned on Ht, we have for any i \u2208 {1, 2, . . . , E} and any M \u2208M that\nP {Mti = M} = \u00b5(M) . Conditioning on E being the rounds of exploration, we want to lower bound the number E of exploration rounds so that the probability of discovering all items in M is at most \u03b4 (which is necessary for the expected regret to be O( \u221a T )). First, note that the events {TM < \u221e}M\u2208M are negatively correlated, since discovering some M1 in E can only decrease the probability of discovering M2 6= M1 in E . Therefore, we have P {\u2200M,TM <\u221e} \u2264\n\u220f M\u2208M P {TM <\u221e} \u2264 (1\u2212 (1\u2212 \u00b5m)E)C\u22121 .\nMaking the last expression to be 1\u2212 \u03b4, we have E = ln ( 1\u2212 (1\u2212 \u03b4) 1 C\u22121 ) ln(1\u2212 \u00b5m)\n= \u2126\n( ln ( 1\u2212 (1\u2212 \u03b4C ) ) \u2212\u00b5m )\n= \u2126\n( 1\n\u00b5m ln C \u03b4\n) ,\nfor sufficiently small \u00b5m and \u03b4.\nFor simplicity, assume \u03c10 = 0 without loss of generality; otherwise, we can just define a related problem with \u03c1\u2032i := \u03c1i\u2212 \u03c10, where the loss is just shifted by a constant and the regret remains unchanged. With this assumption, the optimal expected loss is C\u03c12 (c.f., L\u2217(T )).\nThe expected loss of the is now at least (E \u2212C)\u03c11 +C\u03c12 + \u03b4(T \u2212E)\u00b5m\u03c13, where the first two terms are for the loss incurred during the E exploration rounds; and the last term for the \u03b4-probability event that some item is not discovered in the exploration rounds, which leads to \u03c13 loss when it is encountered in any of the remaining T \u2212 E rounds. Finally, the regret can be readily computed as\n(E \u2212 C)\u03c11 + \u03b4(T \u2212 E)\u00b5m\u03c13 = \u2126 ( \u03c13T\u00b5m +\n\u03c12 \u00b5m ln C \u03b4\n) ,\ncompleting the proof with the fact that \u00b5m = 1/ \u221a T ."}, {"heading": "22 25242321", "text": ""}, {"heading": "C Experiment Details", "text": "All four MDPs had the same 25-cell square grid layout and 4 actions (up, down, left, right), as shown in Figure 2. Actions succeed in their intended direction with probability 0.85 and with probability 0.05 go in the other directions (unless halted by a wall). For all actions corner states s5,s20, and s25 stay in the same state with probability 0.95 or transition back to the start state (for all actions). The start state is at the center of the square grid (s13). The dynamics of all MDPs are identical. All rewards are sampled from binomial distributions. All rewards have parameter 0.0 unless otherwise noted. In MDP 1, corner state s20 has a reward parameter of 0.75. In MDP 2, corner state s5 has a reward parameter of 0.75. In MDP 3, corner state s25 has a reward parameter of 0.75. In MDP 4, corner state s25 has a reward parameter of 0.75 and corner state s1 has a reward parameter of 0.99.\nWe also compare to a Bayesian hierarchical multi-task learning algorithm, or HMTL, of [26]. HMTL learns a Bayesian multi-task posterior distribution across tasks, and leverages as a prior when interacting with each new task. HMTL selects the MAP estimate of the model parameters, computes a policy for this models, executes for a fixed number of steps, updates the posterior for the current task, and repeats. No formal guarantees are provided, although empirically HMTL did well on a challenging real-time strategy game.\nFor HMTL, we set the interval between recomputing the MAP model parameters at 20 steps: this choice was made after informal experimentation suggested this value improved performance compared to longer intervals.\nIn all cases, EXPFIRST is given an upper bound on the number of MDPs (4) and the minimum probability of any of the MDPs across the 100 tasks. HMTL is also provided with an upper bound on the number of MDPs, though HMTL is also capable of learning this directly, and HMTL is used only with a two-level hierarchy (e.g. a class consists of a single MDP). For FORCEDEXP, we compare two variants. The approach labeled \u201cFE\u201d in the figures uses a polynomially decaying exploration rate, t\u03b1 with \u03b1 = 0.5, for all experiments. Performance does vary with the choice of \u03b1 but \u03b1 = 0.5 gave good results in our preliminary investigations. Interestingly, this is consistent with the theoretical result that \u03b1 = 0.5 minimizes dependence on T for polynomially decaying exploration rates (c.f., Corollary 3). We also evaluated the FORCEDEXP algorithm using a constant exploration rate 2\u221a\nT for some earlier\nexperiments: as expected performance was similar but slightly worse generally than using a decaying exploration rate, and so we focus the comparison on the decaying exploration rate variant."}, {"heading": "D Proofs for Section 3", "text": ""}, {"heading": "D.1 Proof of Lemma 6", "text": "Proof. The proof follows closely to that of [8]. Consider the beginning of an episode, and let K be the set of known state\u2013action pairs which have been visited by the agent at least me times. For each (s, a) \u2208 k, the `1 distance between the empirical estimate and the actual next-state distribution is at most [11] ([Lemma 8.5.5]): \u03b1 = \u221a 8N me log 2N\u03b4 . Let\nMK be the known-state MDP, which is identical to M\u0302K except that the transition probabilities are replaced by the true ones for known state\u2013action pairs. Following the same line of reasoning as [8], one may lower-bound the probability that an unknown state is reached within the episode by pe \u2265 1/6 \u2212 3\u03b1D. Therefore, pe is bounded by 1/12 as long as \u03b1D \u2264 1/36. The latter is guaranteed if me \u2265 m0 = O ( ND2 log N\u03b4 ) . The rest of the proof is the same as [8], invoking Lemma 56 of [16] to get an upper bound of H , as stated in the lemma as H0(m)."}, {"heading": "D.2 Proof of Lemma 7", "text": "Proof. For task Mt, let Et be the event that all state\u2013action pairs become known after H steps; Lemma 6 with a union bound shows all events {Et}t\u2208{1,2,...,T} hold with probability at least 1 \u2212 \u03b4. For every fixed t, under event Et, every state\u2013action pair has at least m samples to estimate its transition probabilities and average reward after H steps. Applying Lemma 8.5.5 of [11] on the transition distribution, we can upper bound, with probability at least 1\u2212 \u03b42SAT , the `1 error of the transition probability estimates by:\nT =\n\u221a 8N\nm log\n4SAT \u03b4 \u2264 \u0393 3 .\nSimilarly, an application of Hoeffding\u2019s inequality gives the following upper bound, with probability at least 1\u2212 \u03b42SAT , on the reward estimate:\nR =\n\u221a 2\nm log\n4SAT \u03b4 \u2264 \u0393 6 \u221a N .\nApplying a union bound over all states, actions, and tasks, the above concentration results hold with probability at least 1\u2212 \u03b4 for an agent running on T tasks. The rest of the proof is to show that task identification succeeds when the above concentration inequalities hold.\nTo do this, consider the following two mutually exclusive cases:\n1. If Mt is new, then, by assumption, for every M \u2032 \u2208 M\u0302, there exists some (s, a) for which the two models differ by at least \u0393 in `2 distance; that is, \u2016\u03b8Mt(\u00b7|s, a)\u2212 \u03b8M \u2032(\u00b7|s, a)\u20162 \u2265 \u0393. It follows from the equality,\n\u2016\u03b8Mt(\u00b7|s, a)\u2212 \u03b8M \u2032(\u00b7|s, a)\u201622 = \u2211 1\u2264s\u2032\u2264S (\u03b8Mt(s \u2032|s, a)\u2212 \u03b8M \u2032(s\u2032|s, a)) 2 [error in transition probability estimates]\n+ (\u03b8Mt(S + 1|s, a)\u2212 \u03b8M \u2032(S + 1|s, a)) 2 , [error in reward estimate]\nthat at least one of two terms on the right-hand side above is at least \u03932/2. If the first term is larger than \u03932/2, then the `1 distance between the two next-state transition distributions is at least \u0393/ \u221a 2, which is larger than 2 T = 2\u0393/3. It implies that the `1-balls of transition probability estimates for (s, a) betweenMt andM \u2032 do not overlap, and we will identifyMt as a new MDP. Similarly, if the second term is larger than \u03932/2, then using R we can still identify Mt as a new MDP. 2. If Mt is not new, we claim that the algorithm will correctly identify it as some previously solved MDP, say M \u2032\u2032 \u2208 M\u0302. In particular, confidence intervals of its estimated model in every state\u2013action pair must overlap with M \u2032\u2032, since both models\u2019 confidence intervals contain the true model parameters. On the other hand, for any M \u2032 \u2208 M\u0302 \\ {M \u2032\u2032}, its model estimate\u2019s confidence intervals do not have overlap with that of Mt\u2019s in at least one state\u2013action pair, as shown in case 1. Therefore, the algorithm can find the unique and correct M \u2032\u2032 \u2208 M\u0302 that is the same as Mt.\nFinally, the lemma is proved with a union bound over all tasks, states and actions, and with the probability that Et fails to hold for some t."}, {"heading": "D.3 Proof of Theorem 5", "text": "Proof. We consider each possible case when solving the tth task, Mt. As shown in previous lemma, with probability 1 \u2212 \u03b4, the following event Et hold for all t \u2208 [T ]: after PAC-EXPLORE is run on Mt, Algorithm 2 will discover the identity of Mt correctly. That is, if Mt is a new MDP, it will be added to M\u0302; otherwise, M\u0302 remains unchanged. In the following, we assume Et holds for every t, and consider the following cases:\n(a) Exploitation in discovered tasks: we choose to exploit (line 12 in Alg 2) and Mt has been already discovered. In this case, Finite-Model-RL is used to do model elimination (within M\u0302) and to transfer samples from previous tasks that correspond to the same MDP as the current task Mt. Therefore, with a similar analysis, we can get a per-task sample complexity of at most O(CDm) = O\u0303(CD\u03932 ) = \u03c10.\n(b) Exploitation in undiscovered tasks: we choose to exploit and Mt has not been discovered. Running FiniteModel-RL in this case can end up with an arbitrarily poor policy which follows a non- -optimal policy in every step. Therefore, the sample complexity can be as large as H = \u03c13.\n(c) Exploration: we choose to explore using PAC-EXPLORE (lines 6\u20149 in Alg 2). In this case, with high probability, it takes at most H0(m) steps to make every state known, so that the model parameters can be estimated to within accuracy O(\u0393). After that, we can reliably decide whether Mt is a new MDP or not. With sample transfer, the additional steps where -sub-optimal policies are taken in the MDP corresponding to Mt (accumulated across all tasks in the T -sequence) is at most \u03b6s, the single-task sample complexity. The total sample complexity for tasks corresponding to this MDP is therefore at most H0(m)T (Mt) + \u03b6s = \u03c12T (Mt) + \u03b6s, where T (Mt) is the number of times this MDP occurs in the T -sequence.\nFinally, when Algorithm 2 is run on a sequence of T tasks, the total sample complexity\u2014the number of steps in all tasks for which the agent does not follow an -optimal policy\u2014is given by one of the three cases above. The expected sample complexity can therefore be upper bounded by evoking Theorem 2, completing the proof with an application of union bound that takes care of error probabilities."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Transferring knowledge across a sequence of related tasks is an important challenge in reinforce-<lb>ment learning. Despite much encouraging empirical evidence that shows benefits of transfer, there<lb>has been very little theoretical analysis. In this paper, we study a class of lifelong reinforcement-<lb>learning problems: the agent solves a sequence of tasks modeled as finite Markov decision processes<lb>(MDPs), each of which is from a finite set of MDPs with the same state/action spaces and different<lb>transition/reward functions. Inspired by the need for cross-task exploration in lifelong learning, we<lb>formulate a novel online discovery problem and give an optimal learning algorithm to solve it. Such<lb>results allow us to develop a new lifelong reinforcement-learning algorithm, whose overall sample<lb>complexity in a sequence of tasks is much smaller than that of single-task learning, with high prob-<lb>ability, even if the sequence of tasks is generated by an adversary. Benefits of the algorithm are<lb>demonstrated in a simulated problem.", "creator": "LaTeX with hyperref package"}}}