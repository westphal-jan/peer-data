{"id": "1506.04334", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2015", "title": "A Bayesian Model for Generative Transition-based Dependency Parsing", "abstract": "We reductions single using, scalable, fully dogmatics car for moves - according dependency prakrit with contrast difference. The differs, polymorphic both Hierarchical Pitman - Yor Processes, predicament the defining of final alchemy wheels well extend easier while accurate hypotheses. We propose action create decoding polynomial based on measurement filtering should better adapt the coil size which the uncertainty in present model while jointly predicting POS tags part bamboos trees. The UAS of part carboxylation actually taken 10-footer with though than, greedy discriminative pivot. As his language used, it unbiased better disapprobation than making n - phosphodiesterase features earlier instrumental semi - supervised learning pushed which part unlabelled corpus. We fashion another the concept is whatever means generate supplied though phenomenologically pragmatic indictment, opening during talking up immediate applications 20 written success.", "histories": [["v1", "Sat, 13 Jun 2015 23:39:09 GMT  (26kb,D)", "https://arxiv.org/abs/1506.04334v1", "10 pages"], ["v2", "Sun, 28 Jun 2015 21:18:50 GMT  (29kb,D)", "http://arxiv.org/abs/1506.04334v2", "Depling 2015"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan buys", "phil blunsom"], "accepted": false, "id": "1506.04334"}, "pdf": {"name": "1506.04334.pdf", "metadata": {"source": "CRF", "title": "A Bayesian Model for Generative Transition-based Dependency Parsing", "authors": ["Jan Buys", "Phil Blunsom"], "emails": ["jan.buys@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower.\nIn this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency\nparsing (Collins, 1997; Petrov et al., 2006), their use in dependency parsing has been limited. Generative models offer a principled approach to semiand unsupervised learning, and can also be applied to natural language generation tasks.\nDependency grammar induction models (Klein and Manning, 2004; Blunsom and Cohn, 2010) are generative, but not expressive enough for high-accuracy parsing. A previous generative transition-based dependency parser (Titov and Henderson, 2007) obtains competitive accuracies, but training and decoding is computationally very expensive. Syntactic language models have also been shown to improve performance in speech recognition and machine translation (Chelba and Jelinek, 2000; Charniak et al., 2003). However, the main limitation of most existing generative syntactic models is their inefficiency.\nWe propose a generative model for transitionbased parsing (\u00a72). The model, parameterized by Hierarchical Pitman-Yor Processes (HPYPs) (Teh, 2006), learns a distribution over derivations of parser transitions, words and POS tags (\u00a73).\nTo enable efficient inference, we propose a novel algorithm for linear-time decoding in a generative transition-based parser (\u00a74). The algorithm is based on particle filtering (Doucet et al., 2001), a method for sequential Monte Carlo sampling. This method enables the beam-size during decoding to depend on the uncertainty of the model.\nExperimental results (\u00a75) show that the model obtains 88.5% UAS on the standard WSJ parsing task, compared to 88.9% for a greedy discriminative model with similar features. The model can accurately parse up to 200 sentences per second. Although this performance is below state-of-theart discriminative models, it exceeds existing generative dependency parsing models in either accuracy, speed or both.\nAs a language model, the transition-based parser offers an inexpensive way to incorporate\nar X\niv :1\n50 6.\n04 33\n4v 2\n[ cs\n.C L\n] 2\n8 Ju\nn 20\n15\nMs. Waleson is a free-lance writer based NNP NNP VBZ DT JJ NN VBN\nNAME VMOD NMOD\nNMOD\nFigure 1: A partially-derived dependency tree for the sentence Ms. Waleson is a free-lance writer based in New York. The next word to be predicted by the generative model is based. Words in bold are on the stack.\nsyntactic structure into incremental word prediction. With supervised training the model\u2019s perplexity is comparable to that of n-gram models, although generated examples shows greater syntactic coherence. With semi-supervised learning over a large unannotated corpus its perplexity is considerably better than that of a n-gram model."}, {"heading": "2 Generative Transition-based Parsing", "text": "Our parsing model is based on transition-based projective dependency parsing with the arcstandard parsing strategy (Nivre and Scholz, 2004). Parsing is restricted to (labelled) projective trees. An arc (i, l, j) \u2208 A encodes a dependency between two words, where i is the head node, j the dependent and l is the dependency type of j. In our generative model a word can be represented by its lexical (word) type and/or its POS tag. We add a root node to the beginning of the sentence (although it could also be added at the end of the sentence), such that the head word of the sentence is the dependent of the root node.\nA parser configuration (\u03c3, \u03b2,A) for sentence s consists of a stack \u03c3 of indices in s, an index \u03b2 to the next word to be generated, and a set of arcs A. The stack elements are referred to as \u03c31, . . . , \u03c3|\u03c3|, where \u03c31 is the top element. For any node a, lc1(a) refers to the leftmost child of a in A, and rc1(a) to its rightmost child.\nThe initial configuration is ([], 0, \u2205). A terminal configuration is reached when \u03b2 > |s|, and \u03c3 consists only of the root. A sentence is generated leftto-right by performing a sequence of transitions. As a generative model it assigns probabilities to sentences and dependency trees: A word w (including its POS tag) is generated when it is shifted on to the stack, similar to the generative models proposed by Titov and Henderson (2007) and Cohen et al. (2011), and the joint tagging and parsing model of Bohnet and Nivre (2012).\nThe types of transitions in this model are shift (sh), left-arc (la) and right-arc (ra):\n(shw) (\u03c3, i, A) ` (\u03c3|i, i+ 1, A)\n(lal) (\u03c3|i|j, \u03b2,A) ` (\u03c3|j, \u03b2,A \u222a {(j, l, i)})\n(ral) (\u03c3|i|j, \u03b2,A) ` (\u03c3|i, \u03b2, A \u222a {(i, l, j)})\nLeft-arc and right-arc (reduce) transitions add an arc between the top two words on the stack, and also generate an arc label l. The parsing strategy adds arcs bottom-up. No arc that would make the root node the dependent of another node may be added. To illustrate the generative process, the configuration of a partially generated dependency tree is given in Figure 1.\nIn general parses may have multiple derivations. In transition-based parsing it is common to define an oracle o(c,G) that maps the current configuration c and the gold parse G to the next transition that should be performed. In our probabilistic model we are interested in performing inference over all latent structure, including spurious derivations. Therefore we propose a non-deterministic oracle which allows us to find all derivations of G. In contrast to dynamic oracles (Goldberg and Nivre, 2013), we are only interested in derivations of the correct parse tree, so the oracle can assume that given c there exists a derivation for G.\nFirst, to enforce the bottom-up property our oracle has to ensure that an arc (i, j) inGmay only be added once j has been attached to all its children \u2013 we refer to these arcs as valid. Most deterministic oracles add valid arcs greedily. Second, we note that if there exists a valid arc between \u03c32 and \u03c31 and the oracle decides to shift, the same pair will only occur on the top of the stack again after a right dependent has been attached to \u03c31. Therefore right arcs have to be added greedily if they are valid, while adding a valid left arc may be delayed if \u03c31 has unattached right dependents in G."}, {"heading": "3 Probabilistic Generative Model", "text": "Our model defines a joint probability distribution over a parsed sentence with POS tags t1:n, words w1:n and a transition sequence a1:2n as\np(t1:n,w1:n,a1:2n)\n= n\u220f i=1 ( p(ti|htmi)p(wi|ti,h w mi) mi+1\u220f j=mi+1 p(aj |haj ) ) ,\nwhere mi is the number of transitions that have been performed when (ti, wi) is generated and ht,hw and ha are sequences representing the conditioning contexts for the tag, word and transition distributions, respectively.\nIn the generative process a shift transition is followed by a sequence of 0 or more reduce transitions. This is repeated until all the words have been generated and a terminal configuration of the parser has been reached. We shall also consider unlexicalised models, based only on POS tags."}, {"heading": "3.1 Hierarchical Pitman-Yor processes", "text": "The probability distributions for predicting words, tags and transitions are drawn from hierarchical Pitmar-Yor Process (HPYP) priors. HPYP models were originally proposed for n-gram language modelling (Teh, 2006), and have been applied to various NLP tasks. A version of approximate inference in the HPYP model recovers interpolated Kneser-Ney smoothing (Kneser and Ney, 1995), one of the best preforming n-gram language models. The Pitman-Yor Process (PYP) is a generalization of the Dirichlet process which defines a distribution over distributions over a probability space X , with discount parameter 0 \u2264 d < 1, strength parameter \u03b8 > \u2212d and base distribution B. PYP priors encode the power-law distribution found in the distribution of words.\nSampling from the posterior is characterized by the Chinese Restaurant Process analogy, where each variable in a sequence is represented by a customer entering a restaurant and sitting at one of an infinite number of tables. Let ck be the number of customers sitting at table k and K the number of occupied tables. The customer chooses to sit at a table according to the probability\nP (zi = k|z1:i\u22121) = { ck\u2212d i\u22121+\u03b8 1 \u2264 k \u2264 K Kd+\u03b8 i\u22121+\u03b8 k = K + 1,\nwhere zi is the index of the table chosen by the ith customer and z1:i\u22121 is the seating arrangement of the previous i\u2212 1 customers.\nAll customers at a table share the same dish, corresponding to the value assigned to the variables they represent. When a customer sits at an empty table, a dish is assigned to the table by drawing from the base distribution of the PYP.\nFor HPYPs, the PYP base distribution can itself be drawn from a PYP. The restaurant analogy is extended to the Chinese Restaurant Franchise,\nwhere the base distribution of a PYP corresponds to another restaurant. So when a customer sits at a new table, the dish is chosen by letting a new customer enter the base distribution restaurant. All dishes can be traced back to a uniform base distribution at the top of the hierarchy.\nInference over seating arrangements in the model is performed with Gibbs sampling, based on routines to add or remove a customer from a restaurant. In our implementation we use the efficient data structures proposed by Blunsom et al. (2009). In addition to sampling the seating arrangement, the discount and strength parameters are also sampled, using slice sampling.\nIn our model Tht ,Whw and Aha are HPYPs for the tag, word and transition distributions, respectively. The PYPs for the transition prediction distribution, with conditioning context sequence ha1:L, are defined hierarchically as\nAha1:L \u223c PYP(d A L , \u03b8 A L , Aha1:L\u22121) Aha1:L\u22121 \u223c PYP(d A L\u22121, \u03b8 A L\u22121, Aha1:L\u22122)\n. . . . . .\nA\u2205 \u223c PYP(dA0 , \u03b8A0 ,Uniform),\nwhere dAk and \u03b8 A k are the discount and strength discount parameters for PYPs with conditioning context length k. Each back-off level drops one context element. The distribution given the empty context backs off to the uniform distribution over all predictions. The word and tag distributions are defined by similarly-structured HPYPs.\nThe prior specifies an ordering of the symbols in the context from most informative to least informative to the distributions being estimated. The choice and ordering of this context is crucial in the formulation of our model. The contexts that we use are given in Table 1."}, {"heading": "4 Decoding", "text": "In the standard approach to beam search for transition-based parsing (Zhang and Clark, 2008), the beam stores partial derivations with the same number of transitions performed, and the lowestscoring ones are removed when the size of the beam exceeds a set threshold. However, in our model we cannot compare derivations with the same number of transitions but which differ in the number of words shifted. One solution is to keep n separate beams, each containing only derivations with i words shifted, but this approach leads to\nO(n2) decoding complexity. Another option is to prune the beam every time after the next word is shifted in all derivations \u2013 however the number of reduce transitions that can be performed between shifts is bounded by the stack size, so decoding complexity remains quadratic.\nWe propose a novel linear-time decoding algorithm inspired by particle filtering (see Algorithm 1). Instead of specifying a fixed limit on the size of the beam, the beam size is controlled by setting the number of particles K. Every partial derivation dj in the beam is associated with kj particles, such that \u2211 j kj = K. Each pass through the beam advances each dj until the next word is shifted. At each step, to predict the next transition for dj , kj is divided proportionally between taking a shift or reduce transition, according to p(a|dj .ha). If a non-zero number of particles are assigned to reduce, the highest scoring left-arc and rightarc transitions are chosen deterministically, and derivations that execute them are added to the beam. In practice we found that adding only the highest scoring reduce transition (left-arc or rightarc) gives very similar performance. The shift transition is performed on the current derivation, and the derivation weight is also updated with the word generation probability.\nA POS tag is also generated along with a shift transition. Up to three candidate tags are assigned (more do not improve performance) and corresponding derivations are added to the beam, with particles distributed relative to the tag probability (in Algorithm 1 only one tag is predicted).\nA pass is complete once the derivations in the beam, including those added by reduce transitions during the pass, have been iterated through. Then a selection step is performed to determine which\nInput: Sentence w1:n, K particles. Output: Parse tree of argmaxd in beam d.\u03b8. Initialize the beam with parser configuration d with weight d.\u03b8 = 1 and d.k = K particles; for i\u2190 1 to N do\nSearch step; foreach derivation d in beam do\nnShift = round(d.k \u00b7 p(sh|d.ha)); nReduce = d.k \u2212 nShift; if nReduce > 0 then\na = argmaxa6=sh p(a|d.ha); beam.append(dd\u2190 d); dd.k \u2190 nReduce; dd.\u03b8 \u2190 dd.\u03b8 \u00b7 p(a|d.ha); dd.execute(a);\nend d.k \u2190 nShift; if nShift > 0 then\nd.\u03b8 \u2190 d.\u03b8 \u00b7 p(sh|d.ha) \u00b7 maxti p(ti|d.ht)p(wi|d.hw); d.execute(sh);\nend end Selection step; foreach derivation d in beam do\nd.\u03b8\u2032 \u2190 d.k\u00b7d.\u03b8\u2211 d\u2032 d\n\u2032.k\u00b7d\u2032.\u03b8 ; end foreach derivation d in beam do\nd.k = bd.\u03b8\u2032 \u00b7Kc; if d.k = 0 then\nbeam.remove(d); end\nend end\nAlgorithm 1: Beam search decoder for arcstandard generative dependency parsing.\nderivations are kept. The number of particles for each derivation are reallocated based on the normalised weights of the derivations, each weighted by its current number of particles. Derivations to which zero particles are assigned are eliminated. The selection step allows the size of the beam to depend on the uncertainty of the model during decoding. The selectional branching method proposed by Choi and McCallum (2013) for discriminative beam-search parsing has a similar goal.\nAfter the last word in the sentence has been shifted, reduce transitions are performed on each derivation until it reaches a terminal configuration. The parse tree corresponding to the highest scoring final derivation is returned.\nThe main differences between our algorithm and particle filtering are that we divide particles proportionally instead of sampling with replacement, and in the selection step we base the redistribution on the derivation weight instead of the importance weight (the word generation probability). Our method can be interpreted as maximizing\nby sampling from a peaked version of the distribution over derivations."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Parsing Setup", "text": "We evaluate our model as a parser on the standard English Penn Treebank (Marcus et al., 1993) setup, training on WSJ sections 02-21, developing on section 22, and testing on section 23. We use the head-finding rules of Yamada and Matsumoto (2003) (YM)1 for constituencyto-dependency conversion, to enable comparison with previous results. We also evaluate on the Stanford dependency representation (De Marneffe and Manning, 2008) (SD)2.\nWords that occur only once in the training data are treated as unknown words. We classify unknown words according to capitalization, numbers, punctuation and common suffixes into classes similar to those used in the implementation of generative constituency parsers such as the Stanford parser (Klein and Manning, 2003).\nAs a discriminative baseline we use MaltParser (Nivre et al., 2006), a discriminative, greedy transition-based parser, performing arcstandard parsing with LibLinear as classifier. Although the accuracy of this model is not state-ofthe-art, it does enable us to compare our model against an optimised discriminative model with a feature-set based on the same elements as we include in our conditioning contexts.\nOur HPYP dependency parser (HPYP-DP) is trained with 20 iterations of Gibbs sampling, resampling the hyper-parameters after every iteration, except when performing inference over latent structure, in which case they are only resampled every 5 iterations. Training with a deterministic oracle takes 28 seconds per iteration (excluding resampling hyper-parameters), while a nondeterministic oracle (sampling with 100 particles) takes 458 seconds."}, {"heading": "5.2 Modelling Choices", "text": "We consider several modelling choices in the construction of our generative dependency parsing model. Development set parsing results are given in Table 2. We report unlabelled attachment score\n1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 2Converted with version 3.4.1 of the Stanford parser,\navailable at http:/nlp.stanford.edu/software/lex-parser.shtml.\n(UAS) and labelled attachment score (LAS), excluding punctuation.\nHPYP priors The first modelling choice is the selection and ordering of elements in the conditioning contexts of the HPYP priors. Table 3 shows how the development set accuracy increases as more elements are added to the conditioning context. The first two words on the stack are the most important, but insufficient \u2013 second-order dependencies and further elements on the stack should also be included in the contexts. The challenge is that the back-off structure of each HPYP specifies an ordering of the elements based on their importance in the prediction. We are therefore much more restricted than classifiers with large, sparse featuresets which are commonly used in transition-based parsers. Due to sparsity, the word types are the first elements to be dropped in the back-off structure, and elements such as third-order dependencies, which have been shown to improve parsing performance, cannot be included successfully in our model.\nSampling over parsing derivations during training further improves performance by 0.16% to\n89.09 UAS. Adding the root symbol at the end of the sentence rather than at the front gives very similar parsing performance. When unknown words are not clustered according to surface features, performance drops to 88.60 UAS.\nPOS tags and lexicalisation It is standard practice in transition-based parsing to obtain POS tags with a stand-alone tagger before parsing. However, as we have a generative model, we can use the model to assign POS tags in decoding, while predicting the transition sequence. We compare predicting tags against using gold standard POS tags and tags obtain using the Stanford POS tagger3 (Toutanova et al., 2003). Even though the predicted tags are slightly less accurate than the Stanford tags on the development set (95.6%), jointly predicting tags and decoding increases the UAS by 1.1%. The jointly predicted tags are a better fit to the generative model, which can be seen by an improvement in the likelihood of the test data. Bohnet and Nivre (2012) found that joint prediction increases both POS and parsing accuracy. However, their model rescored a k-best list of tags obtained with an preprocessing tagger, while our model does not use the external tagger at all during joint prediction.\nWe train lexicalised and unlexicalised versions of our model. Unlexicalised parsing gives us a strong baseline (85.6 UAS) over which to consider our model\u2019s ability to predict and condition on words. Unlexicalised parsing is also considered to be robust for applications such as crosslingual parsing (McDonald et al., 2011). Additionally, we consider a version of the model that don\u2019t include lexical elements in the conditioning context. This model performs only 1% UAS lower than the best lexicalised model, although it makes much stronger independence assumptions. The main benefit of lexicalised conditioning contexts are to make incremental decoding easier.\nSpeed vs accuracy trade-offs We consider a number of trade-offs between speed and accuracy in the model. We compare using different numbers of particles during decoding, as well as jointly predicting POS tags against using pre-obtained tags (Table 4).\n3We use the efficient \u201cleft 3 words\u201d model, trained on the same data as the parsing model, excluding distributional features. Tagging accuracy is 95.9% on the development set and 96.5% on the test set.\nThe optimal number of particles is found to be 1000 - more particles only increase accuracy by about 0.1 UAS. Although jointly predicting tags is more accurate, using pre-obtained tags provides a better trade-off between speed and accuracy \u2013 87.59 against 85.27 UAS at around 100 sentences per second. In comparison, the MaltParser parses around 500 sentences per second.\nWe also compare our particle filter-based algorithm against a more standard beam-search algorithm that prunes the beam to a fixed size after each word is shifted. This algorithm is much slower than the particle-based algorithm \u2013 to get similar accuracy it parses only 3 sentences per second (against 27) when predicting tags jointly, and 29 (against 108) when using pre-obtained tags."}, {"heading": "5.3 Parsing Results", "text": "Test set results comparing our model against existing discriminative and generative dependency parsers are given in Table 5. Our HPYP model performs much better than Eisner\u2019s generative model as well as the Bayesian version of that model proposed by Wallach et al. (2008) (the result for Eis-\nner\u2019s model is given as reported by Wallach et al. (2008) on the WSJ). The accuracy of our model is only 0.8 UAS below the generative model of Titov and Henderson (2007), despite that model being much more powerful. The Titov and Henderson model takes 3 days to train, and its decoding speed is around 1 sentence per second.\nThe UAS of our model is very close to that of the MaltParser. However, we do note that our model\u2019s performance is relatively worse on LAS than on UAS. An explanation for this is that as we do not include labels in the conditioning contexts, the predicted labels are independent of words that have not yet been generated.\nWe also test the model on the Stanford dependencies, which have a larger label set. Our model obtains 87.9/83.2 against the MaltParser\u2019s 88.9/86.2 UAS/LAS.\nDespite these promising results, our model\u2019s performance still lags behind recent discriminative parsers (Zhang and Nivre, 2011; Choi and McCallum, 2013) with beam-search and richer feature sets than can be incorporated in our model. In terms of speed, Zhang and Nivre (2011) parse 29 sentences per second, against the 110 sentences per second of Choi and McCallum (2013). Recently proposed neural networks for dependency parsers have further improved performance (Dyer et al., 2015; Weiss et al., 2015), reaching up to 94.0% UAS with Stanford dependencies.\nWe argue that the main weakness of the HPYP parser is sparsity in the large conditioning contexts composed of tags and words. The POS tags in the parser configuration context already give a very strong signal for predicting the next transition. As a result it is challenging to construct PYP reduction lists that also include word types without making the back-off contexts too sparse.\nThe other limitation is that our decoding algorithm, although efficient, still prunes the search space aggressively, while not being able to take advantage of look-ahead features as discriminative models can. Interestingly, we note that a discriminative parser cannot reach high performance without look-ahead features."}, {"heading": "5.4 Language Modelling", "text": "Next we evaluate our model as a language model. First we use the standard WSJ language modelling setup, training on sections 00 \u2212 20, developing on 21 \u2212 22 and testing on 23 \u2212 24. Punctua-\ntion is removed, numbers and symbols are mapped to a single symbol and the vocabulary is limited to 10, 000 words. Second we consider a semisupervised setup where we train the model, in addition to the WSJ, on a subset of 1 million sentences (24.1 million words) from the WMT English monolingual training data4. This model is evaluated on newstest2012.\nWhen training our models for language modelling, we first perform standard supervised training, as for parsing (although we don\u2019t predict labels). This is followed by a second training stage, where we train the model only on words, regarding the tags and parse trees as latent structure. In this unsupervised stage we train the model with particle Gibbs sampling (Andrieu et al., 2010), using a particle filter to sample parse trees. When only training on the WSJ, we perform this step on the same data, now allowing the model to learn parses that are not necessarily consistent with the annotated parse trees.\nFor semi-supervised training, unsupervised learning is performed on the large unannotated corpus. However, here we find the highest scoring parse trees, rather than sampling. Only the word prediction distribution is updated, not the tag and transition distributions.\nLanguage modelling perplexity results are given in Table 6. We note that the perplexities reported are upper bounds on the true perplexity of the model, as it is intractable to sum over all possible parses of a sentence to compute the marginal probability of the words. As an approximation we sum over the final beam after decoding.\nThe results show that on the WSJ the model performs slightly better than a HPYP n-gram model. One disadvantage of evaluating on this dataset is that due to removing punctuation and restricting the vocabulary, the model parsing accuracy drops to 84.6 UAS. Also note that in contrast to many other evaluations, we do not interpolate with a ngram model \u2013 this will improve perplexity further.\nOn the big dataset we see a larger improvement over the n-gram model. This is a promising result, as it shows that our model can successfully generalize to larger vocabularies and unannotated datasets.\n4Available at http://www.statmt.org/wmt14/translationtask.html."}, {"heading": "5.5 Generation", "text": "To support our claim that our generative model is a good model for sentences, we generate some examples. The samples given here were obtained by generating 1000 samples, and choosing the 10 highest scoring ones with length greater or equal to 10. The models are trained on the standard WSJ training set (including punctuation).\nThe examples are given in Table 7. The quality of the sentences generated by the dependency model is superior to that of the n-gram model, despite the models have similar test set perplexities. The sentences generated by the dependency model tend to have more global syntactic structure (for examples having verbs where expected), while retaining the local coherence of n-gram models. The dependency model was also able to generate balanced quotation marks."}, {"heading": "6 Related work", "text": "One of the earliest graph-based dependency parsing models (Eisner, 1996) is generative, estimating the probability of dependents given their head and previously generated siblings. To counter sparsity in the conditioning context of the distributions, backoff and smoothing are performed. Wallach et al. (2008) proposed a Bayesian HPYP parameterisation of this model.\nOther generative models for dependency trees have been proposed mostly in the context of unsupervised parsing. The first successful model was the dependency model with valence (DMV) (Klein and Manning, 2004). Several extensions have been proposed for this model, for example using structural annaeling (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al., 2010) or richer contexts (Blunsom and Cohn, 2010). However, these models are not powerful enough for either accurate parsing or language modelling with\nrich contexts (they are usually restricted to firstorder dependencies and valency).\nAlthough any generative parsing model can be applied to language modelling by marginalising out the possible parses of a sentence, in practice the success of such models has been limited. Lexicalised PCFGs applied to language modelling (Roark, 2001; Charniak, 2001) show improvements over n-gram models, but decoding is prohibitively expensive for practical integration in language generation applications.\nChelba and Jelinek (2000) as well as Emami and Jelinek (2005) proposed incremental syntactic language models with some similarities to our model. Those models predict binarized constituency trees with a transition-based model, and are parameterized by deleted interpolation and neural networks, respectively. Rastrow et al. (2012) applies a transition-based dependency language model to speech recognition, using hierarchical interpolation and relative entropy pruning. However, the model perplexity only improves over an n-gram model when interpolated with one.\nTitov and Henderson (2007) introduced a generative latent variable model for transition-based parsing. The model is based on an incremental sigmoid belief networks, using the arc-eager parsing strategy. Exact inference is intractable, so neural networks and variational mean field methods are proposed to perform approximate inference. However, this is much slower and therefore less scalable than our model.\nA generative transition-based parsing model for non-projective parsing is proposed in (Cohen et al., 2011), along with a dynamic program for inference. The parser is similar to ours, but the dynamic program restricts the conditioning context to the top 2 or 3 words on the stack. No experimental results are included.\nLe and Zuidema (2014) proposed a recursive neural network generative model over dependency trees. However, their model can only score trees, not perform parsing, and its perplexity (236.58 on the PTB development set) is worse than model\u2019s, despite using neural networks to combat sparsity.\nFinally, incremental parsing with particle filtering has been proposed previously (Levy et al., 2009) to model human online sentence processing."}, {"heading": "7 Conclusion", "text": "We presented a generative dependency parsing model that, unlike previous models, retains most of the speed and accuracy of discriminative parsers. Our models can accurately estimate probabilities conditioned on long context sequences. The model is scalable to large training and test sets, and even though it defines a full probability distribution over sentences and parses, decoding speed is efficient. Additionally, the generative model gives strong performance as a language model. For future work we believe that this model can be applied successfully to natural language generation tasks such as machine translation."}], "references": [{"title": "Particle Markov chain Monte Carlo methods", "author": ["Christophe Andrieu", "Arnaud Doucet", "Roman Holenstein."], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269\u2013342.", "citeRegEx": "Andrieu et al\\.,? 2010", "shortCiteRegEx": "Andrieu et al\\.", "year": 2010}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Phil Blunsom", "Trevor Cohn."], "venue": "EMNLP, pages 1204\u20131213.", "citeRegEx": "Blunsom and Cohn.,? 2010", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2010}, {"title": "A note on the implementation of hierarchical Dirichlet processes", "author": ["Phil Blunsom", "Trevor Cohn", "Sharon Goldwater", "Mark Johnson."], "venue": "ACL/IJCNLP (Short Papers), pages 337\u2013340.", "citeRegEx": "Blunsom et al\\.,? 2009", "shortCiteRegEx": "Blunsom et al\\.", "year": 2009}, {"title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bernd Bohnet", "Joakim Nivre."], "venue": "EMNLP-CoNLL, pages 1455\u20131465.", "citeRegEx": "Bohnet and Nivre.,? 2012", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "Syntax-based language models for statistical machine translation", "author": ["Eugene Charniak", "Kevin Knight", "Kenji Yamada."], "venue": "Proceedings of MT Summit IX, pages 40\u201346.", "citeRegEx": "Charniak et al\\.,? 2003", "shortCiteRegEx": "Charniak et al\\.", "year": 2003}, {"title": "Immediate-head parsing for language models", "author": ["Eugene Charniak."], "venue": "Proceedings of ACL, pages 124\u2013131.", "citeRegEx": "Charniak.,? 2001", "shortCiteRegEx": "Charniak.", "year": 2001}, {"title": "Structured language modeling", "author": ["Ciprian Chelba", "Frederick Jelinek."], "venue": "Computer Speech & Language, 14(4):283\u2013332.", "citeRegEx": "Chelba and Jelinek.,? 2000", "shortCiteRegEx": "Chelba and Jelinek.", "year": 2000}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Transition-based dependency parsing with selectional branching", "author": ["Jinho D. Choi", "Andrew McCallum."], "venue": "ACL.", "citeRegEx": "Choi and McCallum.,? 2013", "shortCiteRegEx": "Choi and McCallum.", "year": 2013}, {"title": "Exact inference for generative probabilistic non-projective dependency parsing", "author": ["Shay B. Cohen", "Carlos G\u00f3mez-Rodr\u0131\u0301guez", "Giorgio Satta"], "venue": "In EMNLP,", "citeRegEx": "Cohen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["Michael Collins."], "venue": "ACL, pages 16\u201323.", "citeRegEx": "Collins.,? 1997", "shortCiteRegEx": "Collins.", "year": 1997}, {"title": "The Stanford typed dependencies representation", "author": ["Marie-Catherine De Marneffe", "Christopher D Manning."], "venue": "Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1\u20138.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Sequential Monte Carlo methods in practice", "author": ["Arnaud Doucet", "Nando De Freitas", "Neil Gordon."], "venue": "Springer.", "citeRegEx": "Doucet et al\\.,? 2001", "shortCiteRegEx": "Doucet et al\\.", "year": 2001}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of ACL 2015.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason Eisner."], "venue": "COLING, pages 340\u2013345.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "A neural syntactic language model", "author": ["Ahmad Emami", "Frederick Jelinek."], "venue": "Machine Learning, 60(13):195\u2013227.", "citeRegEx": "Emami and Jelinek.,? 2005", "shortCiteRegEx": "Emami and Jelinek.", "year": 2005}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "TACL, 1:403\u2013414.", "citeRegEx": "Goldberg and Nivre.,? 2013", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2013}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Liang Huang", "Kenji Sagae."], "venue": "ACL, pages 1077\u20131086.", "citeRegEx": "Huang and Sagae.,? 2010", "shortCiteRegEx": "Huang and Sagae.", "year": 2010}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "ACL, pages 423\u2013430.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D Manning."], "venue": "ACL, pages 478\u2013586.", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney."], "venue": "ICASSP, volume 1, pages 181\u2013184. IEEE.", "citeRegEx": "Kneser and Ney.,? 1995", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "Efficient thirdorder dependency parsers", "author": ["Terry Koo", "Michael Collins."], "venue": "ACL, pages 1\u201311.", "citeRegEx": "Koo and Collins.,? 2010", "shortCiteRegEx": "Koo and Collins.", "year": 2010}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "EMNLP, pages 729\u2013739.", "citeRegEx": "Le and Zuidema.,? 2014", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of ACL (Volume 1: Long Papers), pages 1381\u20131391.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Modeling the effects of memory on human online sentence processing with particle filters", "author": ["Roger P Levy", "Florencia Reali", "Thomas L Griffiths."], "venue": "Advances in neural information processing systems, pages 937\u2013944.", "citeRegEx": "Levy et al\\.,? 2009", "shortCiteRegEx": "Levy et al\\.", "year": 2009}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan T. McDonald", "Koby Crammer", "Fernando C.N. Pereira."], "venue": "ACL.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Ryan McDonald", "Slav Petrov", "Keith Hall."], "venue": "EMNLP, pages 62\u201372. Association for Computational Linguistics.", "citeRegEx": "McDonald et al\\.,? 2011", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Deterministic dependency parsing of English text", "author": ["Joakim Nivre", "Mario Scholz."], "venue": "COLING.", "citeRegEx": "Nivre and Scholz.,? 2004", "shortCiteRegEx": "Nivre and Scholz.", "year": 2004}, {"title": "Maltparser: A data-driven parser-generator for dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson."], "venue": "Proceedings of LREC, volume 6, pages 2216\u20132219.", "citeRegEx": "Nivre et al\\.,? 2006", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre."], "venue": "Computational Linguistics, 34(4):513\u2013553.", "citeRegEx": "Nivre.,? 2008", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "COLING-ACL, pages 433\u2013440.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Efficient structured language modeling for speech recognition", "author": ["Ariya Rastrow", "Mark Dredze", "Sanjeev Khudanpur."], "venue": "INTERSPEECH.", "citeRegEx": "Rastrow et al\\.,? 2012", "shortCiteRegEx": "Rastrow et al\\.", "year": 2012}, {"title": "Probabilistic top-down parsing and language modeling", "author": ["Brian Roark."], "venue": "Computational linguistics, 27(2):249\u2013276.", "citeRegEx": "Roark.,? 2001", "shortCiteRegEx": "Roark.", "year": 2001}, {"title": "Annealing structural bias in multilingual weighted grammar induction", "author": ["Noah A. Smith", "Jason Eisner."], "venue": "Proceedings of COLING-ACL, pages 569\u2013576.", "citeRegEx": "Smith and Eisner.,? 2006", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "Viterbi training improves unsupervised dependency parsing", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky", "Christopher D. Manning."], "venue": "CoNLL, pages 9\u201317.", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Yee Whye Teh."], "venue": "ACL.", "citeRegEx": "Teh.,? 2006", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "A latent variable model for generative dependency parsing", "author": ["Ivan Titov", "James Henderson."], "venue": "Proceedings of the Tenth International Conference on Parsing Technologies, pages 144\u2013155.", "citeRegEx": "Titov and Henderson.,? 2007", "shortCiteRegEx": "Titov and Henderson.", "year": 2007}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "HLT-NAACL, pages 173\u2013180.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Bayesian modeling of dependency trees using hierarchical Pitman-Yor priors", "author": ["Hanna M Wallach", "Charles Sutton", "Andrew McCallum."], "venue": "ICML Workshop on Prior Knowledge for Text and Language Processing.", "citeRegEx": "Wallach et al\\.,? 2008", "shortCiteRegEx": "Wallach et al\\.", "year": 2008}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of ACL 2015.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proceedings of IWPT.", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}, {"title": "A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing", "author": ["Yue Zhang", "Stephen Clark."], "venue": "EMNLP, pages 562\u2013571.", "citeRegEx": "Zhang and Clark.,? 2008", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "ACL-HLT short papers-Volume 2, pages 188\u2013193.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}], "referenceMentions": [{"referenceID": 30, "context": "Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014).", "startOffset": 161, "endOffset": 221}, {"referenceID": 43, "context": "Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014).", "startOffset": 161, "endOffset": 221}, {"referenceID": 7, "context": "Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014).", "startOffset": 161, "endOffset": 221}, {"referenceID": 42, "context": "Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time.", "startOffset": 50, "endOffset": 121}, {"referenceID": 17, "context": "Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time.", "startOffset": 50, "endOffset": 121}, {"referenceID": 8, "context": "Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time.", "startOffset": 50, "endOffset": 121}, {"referenceID": 26, "context": "Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower.", "startOffset": 19, "endOffset": 83}, {"referenceID": 21, "context": "Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower.", "startOffset": 19, "endOffset": 83}, {"referenceID": 23, "context": "Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower.", "startOffset": 19, "endOffset": 83}, {"referenceID": 10, "context": "While generative models have been used widely and successfully for constituency parsing (Collins, 1997; Petrov et al., 2006), their use in dependency parsing has been limited.", "startOffset": 88, "endOffset": 124}, {"referenceID": 31, "context": "While generative models have been used widely and successfully for constituency parsing (Collins, 1997; Petrov et al., 2006), their use in dependency parsing has been limited.", "startOffset": 88, "endOffset": 124}, {"referenceID": 19, "context": "Dependency grammar induction models (Klein and Manning, 2004; Blunsom and Cohn, 2010) are generative, but not expressive enough for high-accuracy parsing.", "startOffset": 36, "endOffset": 85}, {"referenceID": 1, "context": "Dependency grammar induction models (Klein and Manning, 2004; Blunsom and Cohn, 2010) are generative, but not expressive enough for high-accuracy parsing.", "startOffset": 36, "endOffset": 85}, {"referenceID": 37, "context": "A previous generative transition-based dependency parser (Titov and Henderson, 2007) obtains competitive accuracies, but training and decoding is computationally very expensive.", "startOffset": 57, "endOffset": 84}, {"referenceID": 6, "context": "Syntactic language models have also been shown to improve performance in speech recognition and machine translation (Chelba and Jelinek, 2000; Charniak et al., 2003).", "startOffset": 116, "endOffset": 165}, {"referenceID": 4, "context": "Syntactic language models have also been shown to improve performance in speech recognition and machine translation (Chelba and Jelinek, 2000; Charniak et al., 2003).", "startOffset": 116, "endOffset": 165}, {"referenceID": 36, "context": "The model, parameterized by Hierarchical Pitman-Yor Processes (HPYPs) (Teh, 2006), learns a distribution over derivations of parser transitions, words and POS tags (\u00a73).", "startOffset": 70, "endOffset": 81}, {"referenceID": 12, "context": "The algorithm is based on particle filtering (Doucet et al., 2001), a method for sequential Monte Carlo sampling.", "startOffset": 45, "endOffset": 66}, {"referenceID": 28, "context": "Our parsing model is based on transition-based projective dependency parsing with the arcstandard parsing strategy (Nivre and Scholz, 2004).", "startOffset": 115, "endOffset": 139}, {"referenceID": 34, "context": "As a generative model it assigns probabilities to sentences and dependency trees: A word w (including its POS tag) is generated when it is shifted on to the stack, similar to the generative models proposed by Titov and Henderson (2007) and Cohen et al.", "startOffset": 209, "endOffset": 236}, {"referenceID": 8, "context": "As a generative model it assigns probabilities to sentences and dependency trees: A word w (including its POS tag) is generated when it is shifted on to the stack, similar to the generative models proposed by Titov and Henderson (2007) and Cohen et al. (2011), and the joint tagging and parsing model of Bohnet and Nivre (2012).", "startOffset": 240, "endOffset": 260}, {"referenceID": 3, "context": "(2011), and the joint tagging and parsing model of Bohnet and Nivre (2012). The types of transitions in this model are shift (sh), left-arc (la) and right-arc (ra):", "startOffset": 51, "endOffset": 75}, {"referenceID": 16, "context": "In contrast to dynamic oracles (Goldberg and Nivre, 2013), we are only interested in derivations of the correct parse tree, so the oracle can assume that given c there exists a derivation for G.", "startOffset": 31, "endOffset": 57}, {"referenceID": 36, "context": "HPYP models were originally proposed for n-gram language modelling (Teh, 2006), and have been applied to various NLP tasks.", "startOffset": 67, "endOffset": 78}, {"referenceID": 20, "context": "A version of approximate inference in the HPYP model recovers interpolated Kneser-Ney smoothing (Kneser and Ney, 1995), one of the best preforming n-gram language models.", "startOffset": 96, "endOffset": 118}, {"referenceID": 2, "context": "In our implementation we use the efficient data structures proposed by Blunsom et al. (2009). In addition to sampling the seating arrangement, the discount and strength parameters are also sampled, using slice sampling.", "startOffset": 71, "endOffset": 93}, {"referenceID": 42, "context": "In the standard approach to beam search for transition-based parsing (Zhang and Clark, 2008), the beam stores partial derivations with the same number of transitions performed, and the lowestscoring ones are removed when the size of the beam exceeds a set threshold.", "startOffset": 69, "endOffset": 92}, {"referenceID": 8, "context": "The selectional branching method proposed by Choi and McCallum (2013) for discriminative beam-search parsing has a similar goal.", "startOffset": 45, "endOffset": 70}, {"referenceID": 25, "context": "We evaluate our model as a parser on the standard English Penn Treebank (Marcus et al., 1993) setup, training on WSJ sections 02-21, developing on section 22, and testing on section 23.", "startOffset": 72, "endOffset": 93}, {"referenceID": 24, "context": "We evaluate our model as a parser on the standard English Penn Treebank (Marcus et al., 1993) setup, training on WSJ sections 02-21, developing on section 22, and testing on section 23. We use the head-finding rules of Yamada and Matsumoto (2003) (YM)1 for constituencyto-dependency conversion, to enable comparison with previous results.", "startOffset": 73, "endOffset": 247}, {"referenceID": 18, "context": "We classify unknown words according to capitalization, numbers, punctuation and common suffixes into classes similar to those used in the implementation of generative constituency parsers such as the Stanford parser (Klein and Manning, 2003).", "startOffset": 216, "endOffset": 241}, {"referenceID": 29, "context": "As a discriminative baseline we use MaltParser (Nivre et al., 2006), a discriminative, greedy transition-based parser, performing arcstandard parsing with LibLinear as classifier.", "startOffset": 47, "endOffset": 67}, {"referenceID": 38, "context": "We compare predicting tags against using gold standard POS tags and tags obtain using the Stanford POS tagger3 (Toutanova et al., 2003).", "startOffset": 111, "endOffset": 135}, {"referenceID": 3, "context": "Bohnet and Nivre (2012) found that joint prediction increases both POS and parsing accuracy.", "startOffset": 0, "endOffset": 24}, {"referenceID": 27, "context": "Unlexicalised parsing is also considered to be robust for applications such as crosslingual parsing (McDonald et al., 2011).", "startOffset": 100, "endOffset": 123}, {"referenceID": 29, "context": "41 Zhang and Nivre (2011) 92.", "startOffset": 13, "endOffset": 26}, {"referenceID": 8, "context": "8 Choi and McCallum (2013) 92.", "startOffset": 2, "endOffset": 27}, {"referenceID": 37, "context": "Titov and Henderson (2007) was retrained to enable direct comparison.", "startOffset": 0, "endOffset": 27}, {"referenceID": 14, "context": "Our HPYP model performs much better than Eisner\u2019s generative model as well as the Bayesian version of that model proposed by Wallach et al. (2008) (the result for Eis-", "startOffset": 41, "endOffset": 147}, {"referenceID": 38, "context": "ner\u2019s model is given as reported by Wallach et al. (2008) on the WSJ).", "startOffset": 36, "endOffset": 58}, {"referenceID": 37, "context": "8 UAS below the generative model of Titov and Henderson (2007), despite that model being much more powerful.", "startOffset": 36, "endOffset": 63}, {"referenceID": 43, "context": "Despite these promising results, our model\u2019s performance still lags behind recent discriminative parsers (Zhang and Nivre, 2011; Choi and McCallum, 2013) with beam-search and richer feature sets than can be incorporated in our model.", "startOffset": 105, "endOffset": 153}, {"referenceID": 8, "context": "Despite these promising results, our model\u2019s performance still lags behind recent discriminative parsers (Zhang and Nivre, 2011; Choi and McCallum, 2013) with beam-search and richer feature sets than can be incorporated in our model.", "startOffset": 105, "endOffset": 153}, {"referenceID": 13, "context": "Recently proposed neural networks for dependency parsers have further improved performance (Dyer et al., 2015; Weiss et al., 2015), reaching up to 94.", "startOffset": 91, "endOffset": 130}, {"referenceID": 40, "context": "Recently proposed neural networks for dependency parsers have further improved performance (Dyer et al., 2015; Weiss et al., 2015), reaching up to 94.", "startOffset": 91, "endOffset": 130}, {"referenceID": 8, "context": "Despite these promising results, our model\u2019s performance still lags behind recent discriminative parsers (Zhang and Nivre, 2011; Choi and McCallum, 2013) with beam-search and richer feature sets than can be incorporated in our model. In terms of speed, Zhang and Nivre (2011) parse 29 sentences per second, against the 110 sentences per second of Choi and McCallum (2013).", "startOffset": 129, "endOffset": 276}, {"referenceID": 8, "context": "Despite these promising results, our model\u2019s performance still lags behind recent discriminative parsers (Zhang and Nivre, 2011; Choi and McCallum, 2013) with beam-search and richer feature sets than can be incorporated in our model. In terms of speed, Zhang and Nivre (2011) parse 29 sentences per second, against the 110 sentences per second of Choi and McCallum (2013). Recently proposed neural networks for dependency parsers have further improved performance (Dyer et al.", "startOffset": 129, "endOffset": 372}, {"referenceID": 0, "context": "In this unsupervised stage we train the model with particle Gibbs sampling (Andrieu et al., 2010), using a particle filter to sample parse trees.", "startOffset": 75, "endOffset": 97}, {"referenceID": 6, "context": "22 Chelba and Jelinek (2000) 146.", "startOffset": 3, "endOffset": 29}, {"referenceID": 6, "context": "22 Chelba and Jelinek (2000) 146.1 Emami and Jelinek (2005) 131.", "startOffset": 3, "endOffset": 60}, {"referenceID": 14, "context": "One of the earliest graph-based dependency parsing models (Eisner, 1996) is generative, estimating the probability of dependents given their head and previously generated siblings.", "startOffset": 58, "endOffset": 72}, {"referenceID": 14, "context": "One of the earliest graph-based dependency parsing models (Eisner, 1996) is generative, estimating the probability of dependents given their head and previously generated siblings. To counter sparsity in the conditioning context of the distributions, backoff and smoothing are performed. Wallach et al. (2008) proposed a Bayesian HPYP parameterisation of this model.", "startOffset": 59, "endOffset": 310}, {"referenceID": 19, "context": "The first successful model was the dependency model with valence (DMV) (Klein and Manning, 2004).", "startOffset": 71, "endOffset": 96}, {"referenceID": 34, "context": "Several extensions have been proposed for this model, for example using structural annaeling (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al.", "startOffset": 93, "endOffset": 117}, {"referenceID": 35, "context": "Several extensions have been proposed for this model, for example using structural annaeling (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al., 2010) or richer contexts (Blunsom and Cohn, 2010).", "startOffset": 139, "endOffset": 164}, {"referenceID": 1, "context": ", 2010) or richer contexts (Blunsom and Cohn, 2010).", "startOffset": 27, "endOffset": 51}, {"referenceID": 33, "context": "Lexicalised PCFGs applied to language modelling (Roark, 2001; Charniak, 2001) show improvements over n-gram models, but decoding is prohibitively expensive for practical integration in language generation applications.", "startOffset": 48, "endOffset": 77}, {"referenceID": 5, "context": "Lexicalised PCFGs applied to language modelling (Roark, 2001; Charniak, 2001) show improvements over n-gram models, but decoding is prohibitively expensive for practical integration in language generation applications.", "startOffset": 48, "endOffset": 77}, {"referenceID": 9, "context": "A generative transition-based parsing model for non-projective parsing is proposed in (Cohen et al., 2011), along with a dynamic program for inference.", "startOffset": 86, "endOffset": 106}, {"referenceID": 24, "context": "Finally, incremental parsing with particle filtering has been proposed previously (Levy et al., 2009) to model human online sentence processing.", "startOffset": 82, "endOffset": 101}], "year": 2015, "abstractText": "We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy. The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference. We propose an efficient decoding algorithm based on particle filtering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation.", "creator": "TeX"}}}