{"id": "1506.05439", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "Learning with a Wasserstein Loss", "abstract": "Learning to predict multi - label bytes appears difficult, same. making problems least being a waste metric moving the waveform that get one some to effective troubling. In this put we strategies just faced function for multi - lp example, based on the Wasserstein meter. The Wasserstein anywhere means a developed notion of similitude for probability measures. Although methodologies through respect then the regardless Wasserstein thus meaning costly, recent began same a made inconspicuously approximation how is continuously calculating. We aware providing emphasis measurement based on part regularization, strengthening the Wasserstein loss 20 probability measures able unnormalized implementation. We directly notion a csa practical bound this it final and show connections with the 45 spatial norm and the Jaccard trading. The Wasserstein against if need straightforwardness mainly second predictions half needs may every different 2.5 taking with exports space. We inspire this moreover on full yet - software tag adjustment complicated, are has Yahoo Flickr Creative Commons backend, achieving missouri career few puts scrolling that oold ' t might given metric.", "histories": [["v1", "Wed, 17 Jun 2015 19:36:41 GMT  (2371kb,D)", "http://arxiv.org/abs/1506.05439v1", null], ["v2", "Fri, 6 Nov 2015 03:46:05 GMT  (2370kb,D)", "http://arxiv.org/abs/1506.05439v2", "NIPS 2015"], ["v3", "Wed, 30 Dec 2015 01:08:11 GMT  (2376kb,D)", "http://arxiv.org/abs/1506.05439v3", "NIPS 2015; v3 updates Algorithm 1 and Equations 6, 8"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["charlie frogner", "chiyuan zhang", "hossein mobahi", "mauricio araya-polo", "tomaso a poggio"], "accepted": true, "id": "1506.05439"}, "pdf": {"name": "1506.05439.pdf", "metadata": {"source": "CRF", "title": "Learning with a Wasserstein Loss", "authors": ["Charlie Frogner", "Chiyuan Zhang", "Hossein Mobahi", "Mauricio Araya-Polo"], "emails": ["frogner@mit.edu,", "chiyuan@mit.edu,", "tp@ai.mit.edu", "hmobahi@csail.mit.edu", "Mauricio.Araya@shell.com"], "sections": [{"heading": null, "text": "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe efficient learning algorithms based on this regularization, extending the Wasserstein loss from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss and show connections with the total variation norm and the Jaccard index. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, achieving superior performance over a baseline that doesn\u2019t use the metric."}, {"heading": "1 Introduction", "text": "We consider the problem of learning to predict a measure over a finite set. This problem includes many widely-used machine learning scenarios. For example, in multiclass classification, the set consists of the classes and a predicted distribution over classes is used to determine the top-K most likely classes (as in the ImageNet Large Scale Visual Recognition Challenge [ILSVRC]) or to do subsequent inference (as with acoustic modeling in speech recognition). Another example is semantic segmentation [1], where the set consists of the pixel locations, and a segment can be modeled as a uniform measure supported on a subset.\nIn practice, many learning problems have natural similarity or metric structure on the output space. For example, in semantic segmentation, spatial adjacency between pixel locations provides a strong cue for similarity of their labels, due to contiguity of segmented regions. Such spatial adjacency can be captured, for example, by the Euclidean distance between the pixel locations. And in the ILSVRC image classification task, the output set comprises 1000 visual categories that are organized in a hierarchy, from which various semantic similarity measures are derived. Hierarchical structure in the label space is also prevalent in document categorization problems. In the following, we call the similarity structure in the label space the ground metric or semantic similarity.\n\u2217Authors contributed equally.\nar X\niv :1\n50 6.\n05 43\n9v 1\n[ cs\n.L G\n] 1\nThe presence of a ground metric can be taken into account when measuring the prediction performance. For example, confusing dogs with cats might be a more severe error than confusing breeds of dogs. Intuitively, a loss incorporating this metric should encourage the algorithm to favor predictions that are, if not completely accurate, at least semantically similar to the ground truth.\nIn this paper, we develop a loss function for multi-label learning\nthat incorporates a metric on the output space by measuring the Wasserstein distance between a prediction and the target label, with respect to that metric. The Wasserstein distance is defined as the cost of the optimal transport plan for moving the mass in the predicted measure to match that in the target, and has been applied to a wide range of problems, including barycenter estimation [2], label propagation [3], and clustering [4]. To our knowledge, this paper represents the first use of the Wasserstein distance as a loss for supervised learning.\nIncorporating an output metric into the loss can meaningfully impact learning performance. Take, for example, a multiclass classification problem containing semantically near-equivalent categories. Figure 1 shows such a case from the ILSVRC, in which the categories Siberian husky and Eskimo dog are nearly indistinguishable. Such categories can introduce noise in human-labeled data, as the labelers may fail to make fine distinctions between the categories. We simulate this problem by identifying the classes with points on a grid in the two-dimensional plane and randomly switching the labels to neighboring classes. We compare the standard multiclass logistic loss to the Wasserstein loss, and measure the prediction performance with the Euclidean distance between the predicted class and the true class. As shown in Figure 2, The prediction performance of both losses degrades as more labels are perturbed. Importantly, by incorporating the ground metric, the Wasserstein loss yields predictions that are closer to the ground truth, across all noise levels. Section D.1 of the Appendix describes the experiment in more detail.\nThe main contributions of this paper are as follows. We formulate the problem of learning with knowledge of the ground metric, and propose the Wasserstein loss as an alternative to traditional information divergence-based loss functions. Specifically, we focus on empirical risk minimization (ERM) with the Wasserstein loss, and describe efficient learning algorithms based on entropic regularization of the optimal transport problem. Moreover, we justify ERM with the Wasserstein loss by showing a statistical learning bound and we draw connections with existing measures of performance. Finally, we evaluate the proposed loss on both synthetic examples and a real-world image annotation problem, demonstrating benefits for incorporating an output metric into the loss."}, {"heading": "2 Related work", "text": "Decomposable loss functions like KL Divergence and `p distances are very popular for probabilistic [1] or vector-valued [5] predictions, as each component can be evaluated independently, often leading to simple and efficient algorithms. The idea of exploiting smoothness in the label space according to a prior metric has been explored in many different forms, including regularization [6] and post-processing with graphical models [7]. Optimal transport provides a natural distance for probability distributions over metric spaces. In [2, 8], the optimal transport is used to formulate the Wasserstein Barycenter as a probability distribution with minimum Wasserstein distance to a set of\ngiven points on the probability simplex. [9] propagates histogram values on a graph by minimizing a Dirichlet energy induced by the optimal transport. The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13]. However, to our knowledge, this is the first time it is used as a loss function in a discriminative learning framework. The closest work to this paper is a theoretical study [14] of an estimator that minimizes the optimal transport cost between the empirical distribution and the estimated distribution in the setting of statistical parameter estimation."}, {"heading": "3 Learning with a Wasserstein loss", "text": ""}, {"heading": "3.1 Problem setup and notation", "text": "Consider the problem of learning a map from X \u2208 RDX to the space Y = RK+ of measures over a finite set K of size |K| = K. Assume K is a subset of a metric space with metric dK(\u00b7, \u00b7). dK is called the ground metric, and it measures the semantic similarity in the label space. We perform learning over a hypothesis spaceH of predictors h\u03b8 : X \u2192 Y , parameterized by \u03b8 \u2208 \u0398. In the standard statistical learning setting, we get an i.i.d. sequence of training examples S = ((x1, y1), . . . , (xN , yN )), sampled from an unknown joint distribution PX\u00d7Y . Given a measure of performance (a.k.a. risk) E(\u00b7, \u00b7), the goal is to find the predictor h\u03b8 \u2208 H that minimizes the expected risk E[E(h\u03b8(x), y)]. Typically E(\u00b7, \u00b7) is difficult to optimize directly and the joint distribution PX\u00d7Y is unknown, so learning is performed via empirical risk minimization. Specifically, we solve\nmin h\u03b8\u2208H\n{ E\u0302S [`(h\u03b8(x), y) = 1\nN N\u2211 i=1 `(h\u03b8(xi), yi) } with a loss function `(\u00b7, \u00b7) acting as a surrogate of E(\u00b7, \u00b7)."}, {"heading": "3.2 Optimal transport and the exact Wasserstein loss", "text": "Information divergence-based loss functions are widely used in learning with probability-valued outputs. But along with other popular measures like Hellinger distance and \u03c72 distance, these divergences are invariant to permutation of the elements in K, ignoring any metric structure on K. Given a cost function c : K \u00d7 K \u2192 R, the optimal transport distance [15] measures the cheapest way to transport a probability measure \u00b51 to match \u00b52 with respect to c:\nWc(\u00b51, \u00b52) = inf \u03b3\u2208\u03a0(\u00b51,\u00b52) \u222b K\u00d7K c(\u03ba1, \u03ba2)\u03b3(d\u03ba1, d\u03ba2) (1)\nwhere \u03a0(\u00b51, \u00b52) is the set of joint probability measures on K \u00d7 K having \u00b51 and \u00b52 as marginals. An important case is when the cost is given by a metric dK(\u00b7, \u00b7) or its p-th power dpK(\u00b7, \u00b7) with p \u2265 1. In this case, they are called Wasserstein distances [16], also known as the earth mover\u2019s distances [11]. In this paper, we only work with discrete measures. In the case of probability measures, these are histograms in the simplex \u2206K.\nWhen the ground truth y and the output of h both lie in the simplex \u2206K, we can define a Wasserstein loss at x. Definition 3.1 (Exact Wasserstein Loss). For any h\u03b8 \u2208 H, h\u03b8 : X \u2192 \u2206K, let h\u03b8(\u03ba|x) = h\u03b8(x)\u03ba be the predicted value at element \u03ba \u2208 K, given input x \u2208 X . Let y(\u03ba) be the ground truth value for \u03ba given by the corresponding label y. Then we define the Wasserstein loss as\nW pp (h(\u00b7|x), y(\u00b7)) = inf T\u2208\u03a0(h(x),y) \u3008T,M\u3009 (2)\nwhere M \u2208 RK\u00d7K+ is the distance matrix M\u03ba,\u03ba\u2032 = d p K(\u03ba, \u03ba \u2032), and the set of valid transport plans is\n\u03a0(h(x), y) = {T \u2208 RK\u00d7K+ : T1 = h(x), T>1 = y} (3) where 1 is the all-one vector.\nW pp is the cost of the optimal plan for transporting the predicted mass distribution h(x) to match the target distribution y. The penalty increases as more mass is transported over longer distances, according to the ground metric M ."}, {"heading": "4 Efficient optimization", "text": "The Wasserstein loss (2) is a linear program and Lagrangian duality gives a means of computing descent direction with respect to h(x). The dual LP of (2) is\ndW pp (h(x), y) = sup \u03b1,\u03b2\u2208CM \u03b1>h(x) + \u03b2>y, CM = {(\u03b1, \u03b2) \u2208 RK\u00d7K : \u03b1\u03ba + \u03b2\u03ba\u2032 \u2264M\u03ba,\u03ba\u2032}. (4)\nAs (2) is a linear program, at an optimum the values of the dual and the primal are equal (see, e.g. [17]), hence the dual optimal \u03b1 is a subgradient of the loss with respect to its first argument.\nComputing \u03b1 is costly, as it entails solving a linear program with O(K2) contraints, with K being the dimension of the output space. This cost can be prohibitive when optimizing by gradient descent."}, {"heading": "4.1 Entropic regularization of optimal transport", "text": "Cuturi [18] proposes a smoothed transport objective that enables efficient approximation of both the transport matrix in (2) and the subgradient of the loss. [18] introduces an entropic regularization term that results in a strictly convex problem:\n\u03bbW pp (h(\u00b7|x), y(\u00b7)) = inf T\u2208\u03a0(h(x),y) \u3008T,M\u3009+ \u03bbH(T ), H(T ) = \u2212 \u2211 \u03ba,\u03ba\u2032 T\u03ba,\u03ba\u2032 log T\u03ba,\u03ba\u2032 . (5)\nImportantly, the transport matrix that solves (5) is a diagonal scaling of a matrix K = e\u2212\u03bbM\u22121:\nT \u2217 = diag(u)Kdiag(v) (6)\nfor u = e\u03bb\u03b1 and v = e\u03bb\u03b2 , where \u03b1 and \u03b2 are the Lagrangian dual variables for (5).\nIdentifying such a matrix subject to equality constraints on the row and column sums is exactly a matrix balancing problem, which is well-studied in numerical linear algebra and for which efficient iterative algorithms exist [19]. [18] and [2] use the well-known Sinkhorn-Knopp algorithm."}, {"heading": "4.2 Extending smoothed transport to the learning setting", "text": "When the output vectors h(x) and y lie in the simplex, (5) can be used directly as a surrogate for (2). In this case, \u03b1 is a subgradient of the objective and can be obtained from the optimal scaling vector u as \u03b1 = 1\u03bb log u. Note that there is a translation ambiguity here: any upscaling of the vector u can be paired with a corresponding downscaling of the vector v without altering the matrix T \u2217 (and vice versa). This means that \u03b1 is only defined up to a constant shift. In [2] the authors recommend choosing \u03b1 = 1\u03bb log u\u2212 1 K\u03bb log u >1 so that \u03b1 is tangent to the simplex.\nFor many learning problems, however, a normalized output assumption is unnatural. In image segmentation, for example, the target shape is not naturally represented as a histogram. And even when the prediction and the ground truth are constrained to the simplex, the observed label can be subject to noise that violates the constraint.\nThere is more than one way to generalize optimal transport to unnormalized measures. The objective we choose should deal effectively with the difference in total mass between h(x) and y while still being efficient to optimize."}, {"heading": "4.3 Relaxed transport", "text": "We propose a novel relaxation that extends smoothed transport to unnormalized measures. By replacing the equality constraints on the transport marginals in (5) with soft penalties with respect to KL divergence, we get an unconstrained approximate transport problem. The resulting objective is:\n\u03bb,\u03b3a,\u03b3bWKL(h(\u00b7|x), y(\u00b7)) = min T\u2208RK\u00d7K+\n\u3008T,M\u3009+\u03bbH(T ) + \u03b3aK\u0303L (T1\u2016h(x)) + \u03b3bK\u0303L ( T>1\u2016y ) (7)\nwhere K\u0303L (w\u2016z) = w> log(w z) \u2212 1>w + 1>z is the generalized KL divergence between w, z \u2208 RK+ . Here represents element-wise division. As with the previous formulation, the optimal transport matrix with respect to (7) is a diagonal scaling of the matrix K.\n(a) Convergence to smoothed transport. (b) Approximation of exact Wasserstein. (c) Convergence of alternating projections (\u03bb = 50).\nFigure 3: The relaxed transport problem (7) for unnormalized measures.\nProposition 4.1. The transport matrix T \u2217 optimizing (7) satisfies T \u2217 = diag(u)Kdiag(v), where u = (h(x) T \u22171)\u03bb\u03b3a , v = ( y (T \u2217)>1 )\u03bb\u03b3b , and K = e\u2212\u03bbM\u22121. And the optimal transport matrix is a fixed point for a Sinkhorn-like iteration.\nProposition 4.2. T \u2217 = diag(u)Kdiag(v) optimizing (7) satisfies: i) u = h(x) \u03b3a\u03bb \u03b3a\u03bb+1 (Kv)\u2212 \u03b3a\u03bb \u03b3a\u03bb+1 , and ii) v = y \u03b3b\u03bb \u03b3b\u03bb+1 ( K>u )\u2212 \u03b3b\u03bb\u03b3b\u03bb+1 , where represents element-wise multiplication. Unlike the previous formulation, (7) is unconstrained and differentiable with respect to h(x). The gradient is given by \u2207h(x)WKL(h(\u00b7|x), y(\u00b7)) = \u03b3a (1\u2212 T \u22171 h(x)). When restricted to normalized measures, the relaxed problem (7) approximates smoothed transport (5). Figure 3a shows, for normalized h(x) and y, the relative distance between the values of (7) and (5) 1. For \u03bb large enough, (7) converges to (5) as \u03b3a and \u03b3b increase.\n(7) also retains two properties of smoothed transport (5). Figure 3b shows that, for normalized outputs, the relaxed loss converges to the unregularized Wasserstein distance as \u03bb, \u03b3a and \u03b3b increase 2. And Figure 3c shows that convergence of the iterations in (4.2) is nearly independent of the dimension K of the output space."}, {"heading": "5 Properties of the Wasserstein loss", "text": "In this section, we study the statistical properties of learning with the exact Wasserstein loss (2) as well as connections with two standard measures. Full proofs can be found in the appendix."}, {"heading": "5.1 Generalization error", "text": "Let S = ((x1, y1), . . . , (xN , yN )) be i.i.d. samples and h\u03b8\u0302 be the empirical risk minimizer\nh\u03b8\u0302 = argmin h\u03b8\u2208H\n{ E\u0302S [ W pp (h\u03b8(\u00b7|x), y) ] = 1\nN N\u2211 i=1 W pp (h\u03b8(\u00b7|xi), yi)\n} .\nFurther assume H = s \u25e6 Ho is the composition of a softmax s and a base hypothesis space Ho of functions mapping into RK . The softmax layer outputs a prediction that lies in the simplex \u2206K. Theorem 5.1. For p = 1, and any \u03b4 > 0, with probability at least 1\u2212 \u03b4, it holds that\nE [ W 11 (h\u03b8\u0302(\u00b7|x), y) ] \u2264 inf h\u03b8\u2208H E [ W 11 (h\u03b8(\u00b7|x), y) ] + 32KCMRN (Ho) + 2CM\n\u221a log(1/\u03b4)\n2N (8)\nwith the constant CM = max\u03ba,\u03ba\u2032M\u03ba,\u03ba\u2032 . RN (Ho) is the Rademacher complexity [21] measuring the complexity of the hypothesis spaceHo.\n1In figures 3a-c, h(x), y and M are generated as described in [18] section 5. In 3a-b, h(x) and y have dimension 256. In 3c, convergence is defined as in [18]. Shaded regions are 95% intervals.\n2The unregularized Wasserstein distance was computed using FastEMD [20].\nThe Rademacher complexity RN (Ho) for commonly used models like neural networks and kernel machines [21] decays with the training set size. This theorem guarantees that the expected Wasserstein loss of the empirical risk minimizer approaches the best achievable loss forH. As an important special case, minimizing the empirical risk with Wasserstein loss is also good for multiclass classification. Let y = e\u03ba be the \u201cone-hot\u201d encoded label vector for the groundtruth class. Proposition 5.2. In the multiclass classification setting, for p = 1 and any \u03b4 > 0, with probability at least 1\u2212 \u03b4, it holds that\nEx,\u03ba [ dK(\u03ba\u03b8\u0302(x), \u03ba) ] \u2264 inf h\u03b8\u2208H KE[W 11 (h\u03b8(x), y)] + 32K2CMRN (Ho) + 2CMK \u221a log(1/\u03b4) 2N (9)\nwhere the predictor is \u03ba\u03b8\u0302(x) = argmax\u03ba h\u03b8\u0302(\u03ba|x), with h\u03b8\u0302 being the empirical risk minimizer.\nNote that instead of the classification error Ex,\u03ba[1{\u03ba\u03b8\u0302(x) 6= \u03ba}], we actually get a bound on the expected semantic distance between the prediction and the groundtruth."}, {"heading": "5.2 Connection with other standard measures", "text": "The special case in which no prior similarity is assumed between the points is captured by the 0-1 ground metric, defined by M0\u22121\u03ba,\u03ba\u2032 = 1\u03ba 6=\u03ba\u2032 = 1\u2212 \u03b4\u03ba,\u03ba\u2032 . In this case, it is known that the Wasserstein distance reduces to the total variation distance TV(\u00b7, \u00b7): Proposition 5.3. For the 0-1 ground metric, \u2200 probability measures \u00b5, \u03bd, W 10\u22121(\u00b5, \u03bd) = TV(\u00b5, \u03bd). The Wasserstein loss is also closely related to the Jaccard index [22], also known as intersectionover-union (IoU), which is a popular measure of performance in segmentation [23]. For two regions A and B in the image plane, the Jaccard index is defined as J(A,B) = |A \u2229 B|/|A \u222a B|. The associated Jaccard distance dJ(A,B) = 1\u2212 J(A,B) is a metric on the space of all finite sets [22]. If we treat each region A as a uniform probability distribution UA supported on A, then it holds that Proposition 5.4. The Wasserstein loss W 10\u22121 is a proxy of dJ in the sense that for any 0 \u2264 \u03b5 \u2264 1, W 10\u22121(U A,UB) \u2264 \u03b5 if dJ(A,B) \u2264 \u03b5; conversely, dJ(A,B) \u2264 2\u03b5 if W 10\u22121(UA,UB) \u2264 \u03b5.\nWhen the Euclidean distance in the image plane is used as the ground metric, the general Wasserstein loss W pp is still a surrogate of dJ : Corollary 5.5. For any 0 \u2264 \u03b5 \u2264 1, and p \u2265 1, under the ground metric d(\u03ba, \u03ba\u2032) = \u2016\u03ba\u2212 \u03ba\u2032\u2016pp over the set of pixel coordinates, W pp (U A,UB) \u2264 \u03b5 implies that dJ(A,B) \u2264 2\u03b5.\nUnlike W 10\u22121, W p p is stronger than dJ because it ensures not only that the incorrectly predicted region is small, but also that it is not far away. The connection with the Jaccard distance can also be characterized for the case of non-uniform distributions. See section C.4 in the Appendix for details."}, {"heading": "6 Empirical study", "text": ""}, {"heading": "6.1 Impact of the ground metric", "text": "In this section, we show that the Wasserstein loss encourages smoothness with respect to an artificial metric on the MNIST handwritten digit dataset. This is a multi-class classification problem with output dimensions corresponding to the 10 digits, and we apply a ground metric dp(\u03ba, \u03ba\u2032) = |\u03ba \u2212 \u03ba\u2032|p, where \u03ba, \u03ba\u2032 \u2208 {0, . . . , 9} and p \u2208 [0,\u221e). This metric encourages the recognized digit to be numerically close to the true one. We train a model independently for each value of p and plot the average predicted probabilities of the different digits on the test set in Figure 4.\nNote that as p \u2192 0, the metric approaches the 0 \u2212 1 metric d0(\u03ba, \u03ba\u2032) = 1\u03ba6=\u03ba\u2032 , which treats all incorrect digits as being equally unfavorable. In this case, as can be seen in the figure, the predicted probability of the true digit goes to 1 while the probability for all other digits goes to 0. As p increases, the predictions become more evenly distributed over the neighboring digits, converging to a uniform distribution as p\u2192\u221e 3.\n3To avoid numerical issues, we scale down the ground metric such that all of the distance values are in the interval [0, 1)."}, {"heading": "6.2 Flickr tag prediction", "text": "We apply the Wasserstein loss to a real world multi-label learning problem, using the recently released Yahoo/Flickr Creative Commons 100M dataset [24]. Our goal is tag prediction: we select 1000 descriptive tags along with two random sets of 10,000 images each, associated with these tags, for training and testing. We derive a distance metric between tags by using word2vec [25] to embed the tags as unit vectors, then taking their Euclidean distances. To extract image features we use MatConvNet [26]. Note that the set of tags is highly redundant and often many semantically equivalent or similar tags can apply to an image. The images are also incompletely tagged, as different users may prefer different tags. We therefore measure the prediction performance by the top-K cost, defined as CK = 1/K \u2211K k=1 minj dK(\u03ba\u0302k, \u03baj), where {\u03baj} is the set of groundtruth tags, and {\u03ba\u0302k} are the tags with highest predicted probability. We find that a linear combination of the Wasserstein loss W pp and a KL divergence-based loss yields the best prediction results. Specifically, we train a linear model by minimizing W pp + \u03b1KL on the training set, where \u03b1 controls the relative weight of KL. Figure 5a shows the top-K cost on the test set for the combined loss and the baseline KL loss. We additionally create a second dataset by removing redundant labels from the original dataset: this simulates the potentially more difficult case in which a single user tags each image, by selecting one tag to apply from amongst each cluster of applicable, semantically similar tags. Figure 3b shows that performance for both algorithms decreases on the harder dataset, while the combined Wasserstein loss continues to outperform the baseline.\nIn Figure 6, we show the effect on performance of varying the weight \u03b1 on the KL loss. We observe that the optimum of the top-K cost is achieved when the Wasserstein loss is weighted more heavily than at the optimum of the AUC. This is consistent with a semantic smoothing effect of Wasserstein, which during training will favor mispredictions that are semantically similar to the ground truth, sometimes at the cost of lower AUC 4. We finally show two selected images from the test set in\n4The Wasserstein loss can achieve a similar trade-off alone as discussed in Section 6.1. However, the achievable range is usually limited by numerical stability when dealing with large values of the metric. In practice it is often easier to implement the trade-off by combining with a KL loss.\nFigure 7. These illustrate cases in which both algorithms make predictions that are semantically relevant, despite overlapping very little with the ground truth. The image on the left shows semantically irrelevant errors made by the baseline algorithm. More examples can be found in the appendix."}, {"heading": "7 Conclusions and future work", "text": "In this paper we have described a loss function for learning to predict a measure over a finite set, based on the Wasserstein distance. Optimizing with respect to the exact Wasserstein loss is computationally costly and we describe efficient algorithms based on entropic regularization, for learning both normalized and unnormalized measures. We have also described a statistical learning bound for the loss and shown connections with both the total variation norm and the Jaccard index. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space, and we demonstrate this property on a real-data tag prediction problem, achieving superior performance over a baseline that doesn\u2019t incorporate the metric.\nAn interesting direction for future work may be to explore the connection between the Wasserstein loss and Markov random fields, as the latter are often used to encourage smoothness of predictions, via inference at prediction time."}, {"heading": "A Relaxed transport", "text": "Equation (7) gives the relaxed transport objective as \u03bb,\u03b3a,\u03b3bWKL(h(\u00b7|x), y(\u00b7)) = min\nT\u2208RK\u00d7K+ \u3008T,M\u3009+ \u03bbH(T ) + \u03b3aK\u0303L (T1\u2016h(x)) + \u03b3bK\u0303L\n( T>1\u2016y ) with K\u0303L (w\u2016z) = w> log(w z)\u2212 1>w + 1>z.\nProof of Proposition 4.1. The first order condition for T \u2217 optimizing (7) is\nMij + 1\n\u03bb\n( log T \u2217ij + 1 ) + \u03b3a (log T \u22171 h(x))i + \u03b3b ( log(T \u2217)>1 y ) j = 0.\n\u21d2 log T \u2217ij + \u03b3a\u03bb log (T \u22171)i + \u03b3b\u03bb log ( (T \u2217)>1 ) j = \u2212\u03bbMij + \u03b3a\u03bb log h(x)i + \u03b3b\u03bb log yj \u2212 1 \u21d2T \u2217ij(T \u22171)\u03b3a\u03bb((T \u2217)>1)\u03b3b\u03bb = exp (\u2212\u03bbMij + \u03b3a\u03bb log h(x)i + \u03b3b\u03bb log yj \u2212 1)\n\u21d2T \u2217ij = (h(x) T \u22171) \u03b3a\u03bb i\n( y (T \u2217)>1 )\u03b3b\u03bb j exp (\u2212\u03bbMij \u2212 1)\nHence T \u2217 (if it exists) is a diagonal scaling of K = exp (\u2212\u03bbM \u2212 1).\nProof of Proposition 4.2. Let u = (h(x) T \u22171)\u03b3a\u03bb and v = ( y (T \u2217)>1 )\u03b3b\u03bb, so T \u2217 = diag(u)Kdiag(v). We have\nT \u22171 = diag(u)Kv\n\u21d2 (T \u22171)\u03b3a\u03bb+1 h(x)\u03b3a\u03bb = Kv\nwhere we substituted the expression for u. Re-writing T \u22171,\n(diag(u)Kv)\u03b3a\u03bb+1 = diag(h(x)\u03b3a\u03bb)Kv\n\u21d2u\u03b3a\u03bb+1 = h(x)\u03b3a\u03bb (Kv)\u2212\u03b3a\u03bb\n\u21d2u = h(x) \u03b3a\u03bb \u03b3a\u03bb+1 (Kv)\u2212 \u03b3a\u03bb \u03b3a\u03bb+1 .\nA symmetric argument shows that v = y \u03b3b\u03bb \u03b3b\u03bb+1 (K>u)\u2212 \u03b3b\u03bb \u03b3b\u03bb+1 ."}, {"heading": "B Statistical Learning Bounds", "text": "We establish the proof of Theorem 5.1 in this section. For simpler notation, for a sequence S = ((x1, y1), . . . , (xN , yN )) of i.i.d. training samples, we denote the empirical risk R\u0302S and risk R as\nR\u0302S(h\u03b8) = E\u0302S [ W pp (h\u03b8(\u00b7|x), y(\u00b7)) ] , R(h\u03b8) = E [ W pp (h\u03b8(\u00b7|x), y(\u00b7)) ] (10)\nLemma B.1. Let h\u03b8\u0302, h\u03b8\u2217 \u2208 H be the minimizer of the empirical risk R\u0302S and expected risk R, respectively. Then\nR(h\u03b8\u0302) \u2264 R(h\u03b8\u2217) + 2 sup h\u2208H |R(h)\u2212 R\u0302S(h)|\nProof. By the optimality of h\u03b8\u0302 for R\u0302S ,\nR(h\u03b8\u0302)\u2212R(h\u03b8\u2217) = R(h\u03b8\u0302)\u2212 R\u0302S(h\u03b8\u0302) + R\u0302S(h\u03b8\u0302)\u2212R(h\u03b8\u2217) \u2264 R(h\u03b8\u0302)\u2212 R\u0302S(h\u03b8\u0302) + R\u0302S(h\u03b8\u2217)\u2212R(h\u03b8\u2217) \u2264 2 sup\nh\u2208H |R(h)\u2212 R\u0302S(h)|\nTherefore, to bound the risk for h\u03b8\u0302, we need to establish uniform concentration bounds for the Wasserstein loss. Towards that goal, we define a space of loss functions induced by the hypothesis spaceH as\nL = { `\u03b8 : (x, y) 7\u2192W pp (h\u03b8(\u00b7|x), y(\u00b7)) : h\u03b8 \u2208 H } (11)\nThe uniform concentration will depends on the \u201ccomplexity\u201d of L, which is measured by the empirical Rademacher complexity defined below. Definition B.2 (Rademacher Complexity [21]). Let G be a family of mapping from Z to R, and S = (z1, . . . , zN ) a fixed sample from Z . The empirical Rademacher complexity of G with respect to S is defined as\nR\u0302S(G) = E\u03c3 [ sup g\u2208G 1 N n\u2211 i=1 \u03c3ig(zi) ] (12)\nwhere \u03c3 = (\u03c31, . . . , \u03c3N ), with \u03c3i\u2019s independent uniform random variables taking values in {+1,\u22121}. \u03c3i\u2019s are called the Rademacher random variables. The Rademacher complexity is defined by taking expectation with respect to the samples S,\nRN (G) = ES [ R\u0302S(G) ] (13)\nTheorem B.3. For any \u03b4 > 0, with probability at least 1\u2212 \u03b4, the following holds for all `\u03b8 \u2208 L, E[`\u03b8]\u2212 E\u0302S [`\u03b8] \u2264 2RN (L) + \u221a C2M log(1/\u03b4)\n2N (14)\nwith the constant CM = max\u03ba,\u03ba\u2032M\u03ba,\u03ba\u2032 .\nBy the definition of L, E[`\u03b8] = R(h\u03b8) and E\u0302S [`\u03b8] = R\u0302S [h\u03b8]. Therefore, this theorem provides a uniform control for the deviation of the empirical risk from the risk. Theorem B.4 (McDiarmid\u2019s Inequality). Let S = {X1, . . . , XN} \u2282 X be N i.i.d. random variables. Assume there exists C > 0 such that f : X N \u2192 R satisfies the following stability condition\n|f(x1, . . . , xi, . . . , xN )\u2212 f(x1, . . . , x\u2032i, . . . , xN )| \u2264 C (15) for all i = 1, . . . , N and any x1, . . . , xN , x\u2032i \u2208 X . Then for any \u03b5 > 0, denoting f(X1, . . . , XN ) by f(S), it holds that\nP (f(S)\u2212 E[f(S)] \u2265 \u03b5) \u2264 exp ( \u2212 2\u03b5 2\nNC2\n) (16)\nLemma B.5. Let the constant CM = max\u03ba,\u03ba\u2032M\u03ba,\u03ba\u2032 , then 0 \u2264W pp (\u00b7, \u00b7) \u2264 CM .\nProof. For any h(\u00b7|x) and y(\u00b7), let T \u2217 \u2208 \u03a0(h(x), y) be the optimal transport plan that solves (2), then W pp (h(x), y) = \u3008T \u2217,M\u3009 \u2264 CM \u2211 \u03ba,\u03ba\u2032 T\u03ba,\u03ba\u2032 = CM\nProof of Theorem B.3. For any `\u03b8 \u2208 L, note the empirical expectation is the empirical risk of the corresponding h\u03b8:\nE\u0302S [`\u03b8] = 1\nN N\u2211 i=1 `\u03b8(xi, yi) = 1 N N\u2211 i=1 W pp (h\u03b8(\u00b7|xi), yi(\u00b7)) = R\u0302S(h\u03b8)\nSimilarly, E[`\u03b8] = R(h\u03b8). Let \u03a6(S) = sup\n`\u2208L E[`]\u2212 E\u0302S [`] (17)\nLet S\u2032 be S with the i-th sample replaced by (x\u2032i, y \u2032 i), by Lemma B.5, it holds that\n\u03a6(S)\u2212 \u03a6(S\u2032) \u2264 sup `\u2208L E\u0302S\u2032 [`]\u2212 E\u0302S [`] = sup h\u03b8\u2208H\nW pp (h\u03b8(x \u2032 i), y \u2032 i)\u2212W pp (h\u03b8(xi), yi) N \u2264 CM N\nSimilarly, we can show \u03a6(S\u2032)\u2212\u03a6(S) \u2264 CM/N , thus |\u03a6(S\u2032)\u2212\u03a6(S)| \u2264 CM/N . By Theorem B.4, for any \u03b4 > 0, with probability at least 1\u2212 \u03b4, it holds that\n\u03a6(S) \u2264 E[\u03a6(S)] + \u221a C2M log(1/\u03b4)\n2N (18)\nTo bound E[\u03a6(S)], by Jensen\u2019s inequality, ES [\u03a6(S)] = ES [ sup `\u2208L E[`]\u2212 E\u0302S [`] ] = ES [ sup `\u2208L ES\u2032 [ E\u0302S\u2032 [`]\u2212 E\u0302S [`] ]] \u2264 ES,S\u2032 [ sup `\u2208L E\u0302S\u2032 [`]\u2212 E\u0302S [`] ]\nHere S\u2032 is another sequence of i.i.d. samples, usually called ghost samples, that is only used for analysis. Now we introduce the Rademacher variables \u03c3i, since the role of S and S\u2032 are completely symmetric, it follows\nES [\u03a6(S)] \u2264 ES,S\u2032,\u03c3 [ sup `\u2208L 1 N N\u2211 i=1 \u03c3i(`(x \u2032 i, y \u2032 i)\u2212 `(xi, yi)) ]\n\u2264 ES\u2032,\u03c3 [ sup `\u2208L 1 N N\u2211 i=1 \u03c3i`(x \u2032 i, y \u2032 i) ] + ES,\u03c3 [ sup `\u2208L 1 N N\u2211 i=1 \u2212\u03c3i`(xi, yi) ] = ES [ R\u0302S(L) ] + ES\u2032 [ R\u0302S\u2032(L)\n] = 2RN (L)\nThe conclusion follows by combing (17) and (18).\nTo finish the proof of Theorem 5.1, we combine Lemma B.1 and Theorem B.3, and relate RN (L) to RN (H) via the following generalized Talagrand\u2019s lemma [27]. Lemma B.6. Let F be a class of real functions, and H \u2282 F = F1 \u00d7 . . . \u00d7 FK be a K-valued function class. If m : RK \u2192 R is a Lm-Lipschitz function and m(0) = 0, then R\u0302S(m \u25e6 H) \u2264 2Lm \u2211K k=1 R\u0302S(Fk). Theorem B.7 (Theorem 6.15 of [15]). Let \u00b5 and \u03bd be two probability measures on a Polish space (K, dK). Let p \u2208 [1,\u221e) and \u03ba0 \u2208 K. Then\nWp(\u00b5, \u03bd) \u2264 21/p \u2032 (\u222b K dK(\u03ba0, \u03ba)d|\u00b5\u2212 \u03bd|(\u03ba) )1/p , 1 p + 1 p\u2032 = 1 (19)\nCorollary B.8. The Wasserstein loss is Lipschitz continuous in the sense that for any h\u03b8 \u2208 H, and any (x, y) \u2208 X \u00d7 Y ,\nW pp (h\u03b8(\u00b7|x), y) \u2264 2p\u22121CM \u2211 \u03ba\u2208K |h\u03b8(\u03ba|x)\u2212 y(\u03ba)| (20)\nIn particular, when p = 1, we have W 11 (h\u03b8(\u00b7|x), y) \u2264 CM \u2211 \u03ba\u2208K |h\u03b8(\u03ba|x)\u2212 y(\u03ba)| (21)\nWe cannot apply Lemma B.6 directly to the Wasserstein loss class, because the Wasserstein loss is only defined on probability distributions, so 0 is not a valid input. To get around this problem, we assume the hypothesis spaceH used in learning is of the form\nH = {s \u25e6 ho : ho \u2208 Ho} (22) where Ho is a function class that maps into RK , and s is the softmax function defined as s(o) = (s1(o), . . . , sK(o)), with\nsk(o) = eok\u2211 j e oj , k = 1, . . . ,K (23)\nThe softmax layer produce a valid probability distribution from arbitrary input, and this is consistent with commonly used models such as Logistic Regression and Neural Networks. By working with the log of the groundtruth labels, we can also add a softmax layer to the labels.\nLemma B.9 (Proposition 2 of [28]). The Wasserstein distances Wp(\u00b7, \u00b7) are metrics on the space of probability distributions of K, for all 1 \u2264 p \u2264 \u221e. Proposition B.10. The map \u03b9 : RK \u00d7 RK \u2192 R defined by \u03b9(y, y\u2032) = W 11 (s(y), s(y\u2032)) satisfies\n|\u03b9(y, y\u2032)\u2212 \u03b9(y\u0304, y\u0304\u2032)| \u2264 4CM\u2016(y, y\u2032)\u2212 (y\u0304, y\u0304\u2032)\u20162 (24)\nfor any (y, y\u2032), (y\u0304, y\u0304\u2032) \u2208 RK \u00d7 RK . And \u03b9(0, 0) = 0.\nProof. For any (y, y\u2032), (y\u0304, y\u0304\u2032) \u2208 RK \u00d7 RK , by Lemma B.9, we can use triangle inequality on the Wasserstein loss,\n|\u03b9(y, y\u2032)\u2212 \u03b9(y\u0304, y\u0304\u2032)| = |\u03b9(y, y\u2032)\u2212 \u03b9(y\u0304, y\u2032) + \u03b9(y\u0304, y\u2032)\u2212 \u03b9(y\u0304, y\u0304\u2032)| \u2264 \u03b9(y, y\u0304) + \u03b9(y\u2032, y\u0304\u2032)\nFollowing Corollary B.8, it continues as\n|\u03b9(y, y\u2032)\u2212 \u03b9(y\u0304, y\u0304\u2032)| \u2264 CM (\u2016s(y)\u2212 s(y\u0304)\u20161 + \u2016s(y\u2032)\u2212 s(y\u0304\u2032)\u20161) (25)\nNote for each k = 1, . . . ,K, the gradient\u2207ysk satisfies\n\u2016\u2207ysk\u20162 = \u2225\u2225\u2225\u2225\u2225 ( \u2202sk \u2202yj )K j=1 \u2225\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225(\u03b4kjsk \u2212 sksj)Kj=1\u2225\u2225\u2225 2 = \u221a\u221a\u221a\u221as2k K\u2211 j=1 s2j + s 2 k(1\u2212 2sk) (26)\nBy mean value theorem, \u2203\u03b1 \u2208 [0, 1], such that for y\u03b8 = \u03b1y + (1\u2212 \u03b1)y\u0304, it holds that\n\u2016s(y)\u2212 s(y\u0304)\u20161 = K\u2211 k=1 \u2223\u2223\u2223\u3008\u2207ysk|y=y\u03b1k , y \u2212 y\u0304\u3009\u2223\u2223\u2223 \u2264 K\u2211 k=1 \u2016\u2207ysk|y=y\u03b1k \u20162\u2016y \u2212 y\u0304\u20162 \u2264 2\u2016y \u2212 y\u0304\u20162\nbecause by (26), and the fact that \u221a\u2211\nj s 2 j \u2264 \u2211 j sj = 1 and \u221a a+ b \u2264 \u221a a + \u221a b for a, b \u2265 0, it\nholds K\u2211 k=1 \u2016\u2207ysk\u20162 = \u2211\nk:sk\u22641/2\n\u2016\u2207ysk\u20162 + \u2211\nk:sk>1/2\n\u2016\u2207ysk\u20162\n\u2264 \u2211\nk:sk\u22641/2\n( sk + sk \u221a 1\u2212 2sk ) + \u2211 k:sk>1/2 sk \u2264 K\u2211 k=1 2sk = 2\nSimilarly, we have \u2016s(y\u2032)\u2212 s(y\u0304\u2032)\u20161 \u2264 2\u2016y\u2032 \u2212 y\u0304\u2032\u20162, so from (25), we know\n|\u03b9(y, y\u2032)\u2212 \u03b9(y\u0304, y\u0304\u2032)| \u2264 2CM (\u2016y \u2212 y\u0304\u20162 + \u2016y\u2032 \u2212 y\u0304\u2032\u20162) \u2264 2 \u221a 2CM ( \u2016y \u2212 y\u0304\u201622 + \u2016y\u2032 \u2212 y\u0304\u2032\u201622 )1/2 then (24) follows immediately. The second conclusion follows trivially as s maps the zero vector to a uniform distribution.\nProof of Theorem 5.1. Consider the loss function space preceded with a softmax layer\nL = {\u03b9\u03b8 : (x, y) 7\u2192W 11 (s(ho\u03b8(x)), s(y)) : ho\u03b8 \u2208 Ho}\nWe apply Lemma B.6 to the 4CM -Lipschitz continuous function \u03b9 in Proposition B.10 and the function space\nHo \u00d7 . . .\u00d7Ho\ufe38 \ufe37\ufe37 \ufe38 K copies \u00d7I \u00d7 . . .\u00d7 I\ufe38 \ufe37\ufe37 \ufe38 K copies\nwith I a singleton function space with only the identity map. It holds R\u0302S(L) \u2264 8CM ( KR\u0302S(Ho) +KR\u0302S(I) ) = 8KCM R\u0302S(Ho) (27)\nbecause for the identity map, and a sample S = (y1, . . . , yN ), we can calculate\nR\u0302S(I) = E\u03c3 [ sup f\u2208I 1 N N\u2211 i=1 \u03c3if(yi) ] = E\u03c3 [ 1 N N\u2211 i=1 \u03c3iyi ] = 0\nThe conclusion of the theorem follows by combining (27) with Theorem B.3 and Lemma B.1."}, {"heading": "C Connection with other standard measures", "text": "C.1 Connection with multiclass classification\nProof of Proposition 5.2. Given that the label is a \u201cone-hot\u201d vector y = e\u03ba, the set of transport plans (3) degenerates. Specifically, the constraint T>1 = e\u03ba means that only the \u03ba-th column of T could be non-zero. Above that, the constraint T1 = h\u03b8\u0302(\u00b7|x) ensures that the \u03ba-th column of T actually equals to h\u03b8\u0302(\u00b7|x). In other words, the set \u03a0(h\u03b8\u0302(\u00b7|x), e\u03ba) contains only one feasible transport plan, so (2) can be computed directly as\nW pp (h\u03b8\u0302(\u00b7|x), e\u03ba) = \u2211 \u03ba\u2032\u2208K M\u03ba\u2032,\u03bah\u03b8\u0302(\u03ba \u2032|x) = \u2211 \u03ba\u2032\u2208K dpK(\u03ba \u2032, \u03ba)h\u03b8\u0302(\u03ba \u2032|x)\nNow let \u03ba\u0302 = argmax\u03ba h\u03b8\u0302(\u03ba|x) be the prediction, we have h\u03b8\u0302(\u03ba\u0302|x) = 1\u2212 \u2211 \u03ba6=\u03ba\u0302 h\u03b8\u0302(\u03ba|x) \u2265 1\u2212 \u2211 \u03ba 6=\u03ba\u0302 h\u03b8\u0302(\u03ba\u0302|x) = 1\u2212 (K \u2212 1)h\u03b8\u0302(\u03ba\u0302|x)\nTherefore, h\u03b8\u0302(\u03ba\u0302|x) \u2265 1/K, so\nW pp (h\u03b8\u0302(\u00b7|x), e\u03ba) \u2265 d p K(\u03ba\u0302, \u03ba)h\u03b8\u0302(\u03ba\u0302|x) \u2265 d p K(\u03ba\u0302, \u03ba)/K\nThe conclusion follows by applying Theorem 5.1 with p = 1.\nC.2 Connection with the total variation distance\nThe total variation distance between two distributions \u00b5 and \u03bd is defined as TV(\u00b5, \u03bd) = supA\u2282K |\u00b5(A)\u2212 \u03bd(A)|. It can be shown that\nTV(\u00b5, \u03bd) = 1\n2 \u2211 \u03ba\u2208K |\u00b5(\u03ba)\u2212 \u03bd(\u03ba)| = 1\u2212 \u2211 \u03ba\u2208K min(\u00b5(\u03ba), \u03bd(\u03ba)) (28)\nProof of Proposition 5.3. In the case of 0-1 ground metric, the transport cost becomes \u3008T,M\u3009 = \u2211 \u03ba,\u03ba\u2032 T\u03ba,\u03ba\u2032M\u03ba,\u03ba\u2032 = 1\u2212 \u2211 \u03ba T\u03ba,\u03ba (29)\nMoreover, by the constraint T \u2208 \u03a0(\u00b5, \u03bd), we have \u00b5(\u03ba) = \u2211 \u03ba\u2032 T\u03ba,\u03ba\u2032 = T\u03ba,\u03ba + \u2211 \u03ba\u2032 6=\u03ba T\u03ba,\u03ba\u2032 \u2265 T\u03ba,\u03ba, \u03bd(\u03ba) = \u2211 \u03ba\u2032 T\u03ba\u2032,\u03ba = T\u03ba,\u03ba + \u2211 \u03ba\u2032 6=\u03ba T\u03ba\u2032,\u03ba \u2265 T\u03ba,\u03ba\nTherefore, the minimum of (29) is achieved by T \u2217\u03ba,\u03ba = min(\u00b5(\u03ba), \u03bd(\u03ba)) for the diagonal, with offdiagonal entries assigned arbitrarily as long as the constraints for \u03a0(\u00b5, \u03bd) are met. As a result, it holds\nW 10\u22121(\u00b5, \u03bd) = \u3008T \u2217,M0\u22121\u3009 = 1\u2212 \u2211 \u03ba min(\u00b5(\u03ba), \u03bd(\u03ba)) (30)\nFollowing (28), we can seeW 10\u22121 equals to the total variation distance, which is also a scaled version of the `1 distance.\nC.3 Connection with the Jaccard distance\nLet W 10\u22121(\u00b7, \u00b7) be the Wasserstein distance under the 0-1 metric defined by d(\u03ba, \u03ba\u2032) = 1\u03ba6=\u03ba\u2032 , then we have the following characterization of the Wasserstein distance between two uniform distributions over regions. Lemma C.1. Let A,B \u2282 K, then\n1\u2212W 10\u22121(UA,UB) = |A \u2229B|\nmax(|A|, |B|) (31)\nProof. The overlapping mass that does not need to transport is given by\nm0 = min\n( 1\n|A| ,\n1\n|B|\n) |A \u2229B| (32)\nSince under 0-1 metric, any transport of mass has unit cost, so the minimum attainable transport cost is\n1\u00d7 (1\u2212m0) = 1\u2212 |A \u2229B|\nmax(|A|, |B|) (33)\nProof of Proposition 5.4. Given that dJ(A,B) \u2264 \u03b5, by Lemma C.1, it holds\nW 10\u22121(U A,UB) = 1\u2212 |A \u2229B| max(|A|, |B|) \u2264 1\u2212 |A \u2229B| |A \u222aB| = dJ(A,B) \u2264 \u03b5\nConversely, given that W 10\u22121(U A,UB) \u2264 \u03b5, again by Lemma C.1, it holds\n|A \u2229B| max(|A|, |B|) \u2265 1\u2212 \u03b5 (34)\nBy symmetry, without loss of generality, assume |A| \u2265 |B|, then\n|A \u2229B| \u2265 (1\u2212 \u03b5)|A| (35)\nAs a result, we have\nJ(A,B) = |A \u2229B| |A \u222aB| = |A \u2229B| |A|+ |B| \u2212 |A \u2229B| \u2265 (1\u2212 \u03b5)|A| |A|+ |A| \u2212 (1\u2212 \u03b5)|A| = 1\u2212 \u03b5 1 + \u03b5\nThe conclusion follows as\ndJ(A,B) = 1\u2212 J(A,B) \u2264 1\u2212 1\u2212 \u03b5 1 + \u03b5 = 2\u03b5 1 + \u03b5 \u2264 2\u03b5\nProof of Corollary 5.5. Note when the alphabet consists of integer coordinates of pixels, for any \u03ba 6= \u03ba\u2032, \u2016\u03ba\u2212 \u03ba\u2032\u2016pp \u2265 1 for any p \u2265 1. Therefore, M0\u22121\u03ba,\u03ba\u2032 \u2264M p \u03ba,\u03ba\u2032 , i.e. the 0-1 ground metric matrix is bounded by the p-Euclidean ground metric matrix, elementwise. Let T \u2217 be the optimal transport plan under the p-Euclidean ground metric, which is also a feasible transport plan under the 0 \u2212 1 ground metric. So\nW 10\u22121(U A,UB) = inf T\u2208\u03a0(UA,UB) \u3008T,M0\u22121\u3009 \u2264 \u3008T \u2217,M0\u22121\u3009 \u2264 \u3008T \u2217,Mp\u3009 = W pp (UA,UB)\nThe conclusion follows by a direct application of Proposition 5.4.\nC.4 Relation to the Jaccard distance (non-uniform case)\nLemma C.2. Let A,B \u2282 K, and \u00b5A, \u03bdB are two probability distributions supported on A,B, respectively, then\nW 10\u22121(\u00b5 A, \u03bdB) = 1\u2212min\n{ \u00b5A(A \u2229B), \u03bdB(A \u2229B) } (36)\nProof. The amount of mass that completely matches is min{\u00b5A(A \u2229 B), \u03bdB(A \u2229 B)}. The rest of the mass needs to be moved around with unit cost 1, so the total minimum transport cost is 1\u2212min{\u00b5A(A \u2229B), \u03bdB(A \u2229B)}.\nProposition C.3. Let \u00b5A and \u03bdB be two probability measures supported on A and B, respectively. Denote\n\u00b5\u2217 = max \u03ba\u2208A \u00b5A(\u03ba), \u00b5o = min \u03ba\u2208A \u00b5A(\u03ba)\n\u03bd\u2217 = max \u03ba\u2208B \u03bdB(\u03ba), \u03bdo = min \u03ba\u2208B \u00b5B(\u03ba)\nthen W 10\u22121(\u00b5 A, \u03bdB) \u2264 \u03b5 implies\ndJ(A,B) \u2264 2C\u2217 \u2212 2Co(1\u2212 \u03b5) 2C\u2217 \u2212 (1\u2212 \u03b5)Co\n(37)\nwhere C\u2217 \u2265 max(\u00b5\u2217, \u03bd\u2217) and 0 < Co \u2264 min(\u00b5o, \u03bdo).\nProof. Notice obviously, for any X \u2282 K, we have the following properties \u00b5o|X| \u2264 \u00b5A(X) \u2264 \u00b5\u2217|X| (38) \u03bdo|X| \u2264 \u03bdB(X) \u2264 \u03bd\u2217|X| (39)\n(40)\nLet us first assume that W 10\u22121(\u00b5 A, \u03bdB) \u2264 \u03b5, following Lemma C.2, it implies 1\u2212W 10\u22121(\u00b5A, \u03bdB) = min ( \u00b5A(A \u2229B), \u03bdB(A \u2229B) ) \u2265 1\u2212 \u03b5 (41)\nOn the other hand,\ndJ(A,B) = 1\u2212 |A \u2229B| |A \u222aB|\nSo we can get an upper bound of dJ(A,B) by deriving a lower bound on |A \u2229 B| and an upper bound on |A \u222aB|. By (38), (39), and (41), it holds\n|A \u2229B| \u2265 max { \u00b5A(A \u2229B)\n\u00b5\u2217 , \u03bdB(A \u2229B) \u03bd\u2217\n} \u2265 max { 1\u2212 \u03b5 \u00b5\u2217 , 1\u2212 \u03b5 \u03bd\u2217 } \u2265 1\u2212 \u03b5 C\u2217\nwhere C\u2217 \u2265 max{\u00b5\u2217, \u03bd\u2217}. Similarly, we have\n|A \u222aB| = |A|+ |B| \u2212 |A \u2229B| \u2264 1 \u00b5o + 1 \u03bdo \u2212 1\u2212 \u03b5 C\u2217 \u2264 2 Co \u2212 1\u2212 \u03b5 C\u2217\nwhere 0 < Co \u2264 min{\u00b5o, \u03bdo}. It then follows that\ndJ(A,B) \u2264 1\u2212 1\u2212\u03b5 C\u2217\n2 Co \u2212 1\u2212\u03b5C\u2217\n\u2264 1\u2212 Co(1\u2212 \u03b5) 2C\u2217 \u2212 (1\u2212 \u03b5)Co \u2264 2C \u2217 \u2212 2Co(1\u2212 \u03b5) 2C\u2217 \u2212 (1\u2212 \u03b5)Co\nProposition C.4. Following the same notation of Proposition C.3, dJ(A,B) \u2264 \u03b5 implies\nW 10\u22121(\u00b5 A, \u03bdB) \u2264 2(C \u2217 \u2212 Co) + \u03b5(2Co \u2212 C\u2217) C\u2217(2\u2212 \u03b5)\n(42)\nProof. By Lemma C.2, in order to upper bound W 10\u22121(\u00b5 A, \u03bdB), we only need to derive lower bounds for \u00b5A(A \u2229B) and \u03bdB(A \u2229B). By dJ(A,B) \u2264 \u03b5, it holds\n1\u2212 \u03b5 \u2264 |A \u2229B| |A \u222aB| = |A \u2229B| |A|+ |B| \u2212 |A \u2229B| \u2264 |A \u2229B|\n1/\u00b5\u2217 + 1/\u03bd\u2217 \u2212 |A \u2229B| where the inequality is from (38) and (39). As a result,\n|A \u2229B| \u2265 1\u2212 \u03b5 2\u2212 \u03b5\n( 1\n\u00b5\u2217 +\n1\n\u03bd\u2217\n) \u2265 1\u2212 \u03b5\n2\u2212 \u03b5 2 C\u2217\nwhere C\u2217 \u2265 max{\u00b5\u2217, \u03bd\u2217}. By (38) again, we get\n\u00b5A(A \u2229B) \u2265 \u00b5o|A \u2229B| \u2265 1\u2212 \u03b5 2\u2212 \u03b5 2\u00b5o C\u2217\nSimilarly, we have\n\u03bdB(A \u2229B) \u2265 1\u2212 \u03b5 2\u2212 \u03b5 2\u03bdo C\u2217\nCombining, we get\n1\u2212W 10\u22121(\u00b5A, \u03bdB) = min { \u00b5A(A \u2229B), \u03bdB(A \u2229B) } \u2265 2Co\nC\u2217 1\u2212 \u03b5 2\u2212 \u03b5\nwhere 0 < Co \u2264 min{\u00b5o, \u03bdo}. The conclusion follows immediately.\nREMARK: For the case of uniform distributions, C\u2217 = Co, Proposition C.3 and Proposition C.4 fall back to Proposition 5.4."}, {"heading": "D Empirical study", "text": "D.1 Noisy label example\nWe illustrate the phenomenon of semantic label noise of human labelers with a synthetic example. Consider a multiclass classification problem, where the labels corresponds to the vertices on aD\u00d7D lattice on the 2D plane. The Euclidean distance in R2 is used to measure the semantic similarity between labels. The observations for each category are samples of a isotropic Gaussian distribution centered at the corresponding vertex. Given a noise level t, we choose to flip the label of each training sample to one of the neighboring categories5 with probability t. Figure 8 shows the training set for 3\u00d7 3 lattice with noise level t = 0.1 and t = 0.5, respectively. We repeat 10 times for noise levels t = 0.1, 0.2, . . . , 0.9 andD = 3, 4, . . . , 7. A multiclass classifier based on logistic regression is trained with the standard divergenced based loss6 and the proposed Wasserstein loss7, respectively. The performance is measured by the Euclidean distance between the predicted class and the true class, averaged on the test set. Figure 2 compares the performance of the two loss functions.\nD.2 Full figure for the MNIST example\nThe full version of Figure 4 from Section 6.1 is shown in Figure 9.\nD.3 Details of the Flickr tag prediction experiment\nFrom the tags in the Yahoo Flickr Creative Commons dataset, we filtered out those not occurring in the WordNet8 database, as well those whose dominant lexical category was \u201dnoun.location\u201d or \u201dnoun.time.\u201d We also filtered out by hand nouns referring to geographical location or nationality, proper nouns, numbers, photography-specific vocabulary, and several words not generally descriptive of visual content (such as \u201dannual\u201d and \u201ddemo\u201d). From the remainder, the 1000 most frequently occurring tags were used.\nWe list some of the 1000 selected tags here. The 50 most frequently occurring tags: travel, square, wedding, art, flower, music, nature, party, beach, family, people, food, tree, summer, water, concert, winter, sky, snow, street, portrait, architecture, car, live, trip, friend, cat, sign, garden, mountain, bird, sport, light, museum, animal, rock, show, spring, dog, film, blue, green, road, girl, event, red, fun, building, new, cloud. . . . and the 50 least frequent tags: arboretum, chick, sightseeing, vineyard, animalia, burlesque, key, flat, whale, swiss, giraffe, floor, peak, contemporary, scooter, society, actor, tomb, fabric, gala, coral, sleeping, lizard, performer, album, body, crew, bathroom, bed, cricket, piano, base, poetry, master, renovation, step, ghost, freight, champion, cartoon, jumping, crochet, gaming, shooting, animation, carving, rocket, infant, drift, hope.\n5Connected vertices on the lattice are considered neighbors, and the Euclidean distance between neighbors is set to 1.\n6Corresponds to maximum likelihood estimation of the logistic regression model. 7In this special case, corresponds to weighted maximum likelihood estimation, c.f. Section C.1. 8http://wordnet.princeton.edu\nWe train a multiclass linear logistic regression model with a linear combination of the Wasserstein loss and the KL divergence-based loss. The Wasserstein loss between the prediction and the normalized groundtruth is computed with a entropic regularization formulation using 10 iterations of the Sinkhorn-Knopp algorithm. Based on our observation of the ground metric matrix, we use p-norm with p = 13, and set \u03bb = 50. This makes sure that the matrix K is reasonable sparse, enforcing semantic smoothness only in each local neighborhood. Stochastic gradient descent with a mini-batch size of 100, and momentum 0.7 is run for 100,000 iterations to optimize the objective function on the training set. The baseline is trained under the same setting, but with only the KL loss function.\nTo create the dataset with reduced redundancy, for each image in the training set, we compute the pairwise semantic distance for the groundtruth tags, and cluster them into \u201cequivalent\u201d tag-sets with a threshold of semantic distance 1.3. Within each tag-set, one random tag is selected.\nFigure 10 shows some more test images and predictions randomly picked from the test set.\n(a) Flickr user tags: zoo, run, mark; our proposals: running, summer, fun; baseline proposals: running, country, lake. (b) Flickr user tags: travel, architecture, tourism; our proposals: sky, roof, building; baseline proposals: art, sky, beach. (c) Flickr user tags: spring, race, training; our proposals: road, bike, trail; baseline proposals: dog, surf, bike.\n(d) Flickr user tags: family, trip, house; our proposals: family, girl, green; baseline proposals: woman, tree, family. (e) Flickr user tags: education, weather, cow, agriculture; our proposals: girl, people, animal, play; baseline proposals: concert, statue, pretty, girl.\n(f) Flickr user tags: garden, table, gardening; our proposals: garden, spring, plant; baseline proposals: garden, decoration, plant. (g) Flickr user tags: nature, bird, rescue; our proposals: bird, nature, wildlife; baseline proposals: ature, bird, baby.\nFigure 10: Examples of images in the Flickr dataset. We show the groundtruth tags and as well as tags proposed by our algorithm and baseline."}], "references": [{"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "CVPR (to appear),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Fast Computation of Wasserstein Barycenters", "author": ["Marco Cuturi", "Arnaud Doucet"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Wasserstein Propagation for Semi-Supervised Learning", "author": ["Justin Solomon", "Raif M Rustamov", "Leonidas J Guibas", "Adrian Butscher"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Kernels for vector-valued functions: A review", "author": ["Neil D. Lawrence"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["Leonid I Rudin", "Stanley Osher", "Emad Fatemi"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille"], "venue": "In ICLR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A Smoothed Dual Approach for Variational Wasserstein Problems", "author": ["Marco Cuturi", "Gabriel Peyr\u00e9", "Antoine Rolet"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Wasserstein propagation for semi-supervised learning", "author": ["Justin Solomon", "Raif Rustamov", "Leonidas Guibas", "Adrian Butscher"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Comparing clusterings in space", "author": ["Michael Coen", "Hidayath Ansari", "Nathanael Fillmore"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "The earth mover\u2019s distance as a metric for image retrieval", "author": ["Yossi Rubner", "Carlo Tomasi", "Leonidas J Guibas"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Fast contour matching using approximate earth mover\u2019s distance", "author": ["Kristen Grauman", "Trevor Darrell"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Approximate earth mover\u2019s distance in linear time", "author": ["S Shirdhonkar", "D W Jacobs"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "On minimum kantorovich distance estimators", "author": ["Federico Bassetti", "Antonella Bodini", "Eugenio Regazzini"], "venue": "Stat. Probab. Lett., 76(12):1298\u20131302,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Optimal Transport: Old and New", "author": ["C\u00e9dric Villani"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "The Monge-Kantorovich problem: achievements, connections, and perspectives", "author": ["Vladimir I Bogachev", "Aleksandr V Kolesnikov"], "venue": "Russian Math. Surveys, 67(5):785,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Introduction to Linear Optimization", "author": ["Dimitris Bertsimas", "John N. Tsitsiklis", "John Tsitsiklis"], "venue": "Athena Scientific, Boston, third printing edition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport", "author": ["Marco Cuturi"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A fast algorithm for matrix balancing", "author": ["Philip A Knight", "Daniel Ruiz"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Fast and robust Earth Mover\u2019s Distances", "author": ["Ofir Pele", "Michael Werman"], "venue": "ICCV, pages 460\u2013467,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Optimal Decisions from Probabilistic Models: The Intersection-over-Union Case", "author": ["Sebastian Nowozin"], "venue": "CVPR, pages 548\u2013555,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "The new data and new challenges in multimedia research", "author": ["Bart Thomee", "David A. Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": "arXiv preprint arXiv:1503.01817,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": "CoRR, abs/1412.4564,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes. Classics in Mathematics", "author": ["M. Ledoux", "M. Talagrand"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Another example is semantic segmentation [1], where the set consists of the pixel locations, and a segment can be modeled as a uniform measure supported on a subset.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "The Wasserstein distance is defined as the cost of the optimal transport plan for moving the mass in the predicted measure to match that in the target, and has been applied to a wide range of problems, including barycenter estimation [2], label propagation [3], and clustering [4].", "startOffset": 234, "endOffset": 237}, {"referenceID": 2, "context": "The Wasserstein distance is defined as the cost of the optimal transport plan for moving the mass in the predicted measure to match that in the target, and has been applied to a wide range of problems, including barycenter estimation [2], label propagation [3], and clustering [4].", "startOffset": 257, "endOffset": 260}, {"referenceID": 0, "context": "2 Related work Decomposable loss functions like KL Divergence and `p distances are very popular for probabilistic [1] or vector-valued [5] predictions, as each component can be evaluated independently, often leading to simple and efficient algorithms.", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "2 Related work Decomposable loss functions like KL Divergence and `p distances are very popular for probabilistic [1] or vector-valued [5] predictions, as each component can be evaluated independently, often leading to simple and efficient algorithms.", "startOffset": 135, "endOffset": 138}, {"referenceID": 4, "context": "The idea of exploiting smoothness in the label space according to a prior metric has been explored in many different forms, including regularization [6] and post-processing with graphical models [7].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "The idea of exploiting smoothness in the label space according to a prior metric has been explored in many different forms, including regularization [6] and post-processing with graphical models [7].", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "In [2, 8], the optimal transport is used to formulate the Wasserstein Barycenter as a probability distribution with minimum Wasserstein distance to a set of", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "In [2, 8], the optimal transport is used to formulate the Wasserstein Barycenter as a probability distribution with minimum Wasserstein distance to a set of", "startOffset": 3, "endOffset": 9}, {"referenceID": 7, "context": "[9] propagates histogram values on a graph by minimizing a Dirichlet energy induced by the optimal transport.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "The Wasserstein distance is also used to formulate a metric for comparing clusters in [10], as well as applied for image retrieval [11], contour matching [12], and many other problems that can be formulated as histogram matching [13].", "startOffset": 229, "endOffset": 233}, {"referenceID": 12, "context": "The closest work to this paper is a theoretical study [14] of an estimator that minimizes the optimal transport cost between the empirical distribution and the estimated distribution in the setting of statistical parameter estimation.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "Given a cost function c : K \u00d7 K \u2192 R, the optimal transport distance [15] measures the cheapest way to transport a probability measure \u03bc1 to match \u03bc2 with respect to c: Wc(\u03bc1, \u03bc2) = inf \u03b3\u2208\u03a0(\u03bc1,\u03bc2) \u222b", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "In this case, they are called Wasserstein distances [16], also known as the earth mover\u2019s distances [11].", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "In this case, they are called Wasserstein distances [16], also known as the earth mover\u2019s distances [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "[17]), hence the dual optimal \u03b1 is a subgradient of the loss with respect to its first argument.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "1 Entropic regularization of optimal transport Cuturi [18] proposes a smoothed transport objective that enables efficient approximation of both the transport matrix in (2) and the subgradient of the loss.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "[18] introduces an entropic regularization term that results in a strictly convex problem: W p p (h(\u00b7|x), y(\u00b7)) = inf T\u2208\u03a0(h(x),y) \u3008T,M\u3009+ \u03bbH(T ), H(T ) = \u2212 \u2211", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Identifying such a matrix subject to equality constraints on the row and column sums is exactly a matrix balancing problem, which is well-studied in numerical linear algebra and for which efficient iterative algorithms exist [19].", "startOffset": 225, "endOffset": 229}, {"referenceID": 16, "context": "[18] and [2] use the well-known Sinkhorn-Knopp algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[18] and [2] use the well-known Sinkhorn-Knopp algorithm.", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "In [2] the authors recommend choosing \u03b1 = 1 \u03bb log u\u2212 1 K\u03bb log u >1 so that \u03b1 is tangent to the simplex.", "startOffset": 3, "endOffset": 6}, {"referenceID": 19, "context": "RN (H) is the Rademacher complexity [21] measuring the complexity of the hypothesis spaceH.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "In figures 3a-c, h(x), y and M are generated as described in [18] section 5.", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "In 3c, convergence is defined as in [18].", "startOffset": 36, "endOffset": 40}, {"referenceID": 18, "context": "The unregularized Wasserstein distance was computed using FastEMD [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "The Rademacher complexity RN (H) for commonly used models like neural networks and kernel machines [21] decays with the training set size.", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "The Wasserstein loss is also closely related to the Jaccard index [22], also known as intersectionover-union (IoU), which is a popular measure of performance in segmentation [23].", "startOffset": 174, "endOffset": 178}, {"referenceID": 21, "context": "We apply the Wasserstein loss to a real world multi-label learning problem, using the recently released Yahoo/Flickr Creative Commons 100M dataset [24].", "startOffset": 147, "endOffset": 151}, {"referenceID": 22, "context": "We derive a distance metric between tags by using word2vec [25] to embed the tags as unit vectors, then taking their Euclidean distances.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "To extract image features we use MatConvNet [26].", "startOffset": 44, "endOffset": 48}], "year": 2015, "abstractText": "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe efficient learning algorithms based on this regularization, extending the Wasserstein loss from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss and show connections with the total variation norm and the Jaccard index. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, achieving superior performance over a baseline that doesn\u2019t use the metric.", "creator": "LaTeX with hyperref package"}}}