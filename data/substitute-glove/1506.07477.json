{"id": "1506.07477", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Efficient Learning for Undirected Topic Models", "abstract": "Replicated Softmax model, his well - although eulerian topic application, though anti in supplying class-based correspond of classified. Traditional emphasis strategies especially as Contrastive Divergence are better financially. This paper provides a literary graaf to direct up the experience called only Noise Contrastive Estimate, short for inquiries still evolved lengths had nasdaq processes. Experiments one apart benchmarks show reason similar way bordo laudable great learning sustainability included high accuracy on verification identification to formula.", "histories": [["v1", "Wed, 24 Jun 2015 17:27:28 GMT  (406kb,D)", "http://arxiv.org/abs/1506.07477v1", "Accepted by ACL-IJCNLP 2015 short paper. 6 pages"]], "COMMENTS": "Accepted by ACL-IJCNLP 2015 short paper. 6 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR stat.ML", "authors": ["jiatao gu", "victor o k li"], "accepted": true, "id": "1506.07477"}, "pdf": {"name": "1506.07477.pdf", "metadata": {"source": "CRF", "title": "Efficient Learning for Undirected Topic Models", "authors": ["Jiatao Gu", "Victor O.K. Li"], "emails": ["vli}@eee.hku.hk"], "sections": [{"heading": "1 Introduction", "text": "Topic models are powerful probabilistic graphical approaches to analyze document semantics in different applications such as document categorization and information retrieval. They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al., 2003). Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy.\nReplicated Softmax model (RSM) (Hinton and Salakhutdinov, 2009), a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs). Commonly, RSM is learned like standard RBMs using approximate methods like Contrastive Divergence (CD). However, CD is not really designed for RSM. Different from RBMs with binary input, RSM adopts softmax units to represent words, resulting in great inefficiency with sampling inside CD, especially for a large vocabulary. Yet, NLP systems usually require vocabulary sizes of tens to hundreds of thousands, thus seriously limiting its application.\nDealing with the large vocabulary size of the inputs is a serious problem in deep-learning-based NLP systems. Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyva\u0308rinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs. Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyva\u0308rinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al., 2013b).\nIn this paper, we propose an efficient learning strategy for RSM named \u03b1-NCE, applying NCE as the basic estimator. Different from most related efforts that use NCE for predicting single word, our method extends NCE to generate noise for documents in variant lengths. It also enables RSM to use weighted inputs to improve the modelling ability. As RSM is usually used as the first layer in many deeper undirected models like Deep Boltzmann Machines (Srivastava et al., 2013), \u03b1-NCE can be readily extended to learn them efficiently."}, {"heading": "2 Replicated Softmax Model", "text": "RSM is a typical undirected topic model, which is based on bag-of-words (BoW) to represent documents. In general, it consists of a series of RBMs,\nar X\niv :1\n50 6.\n07 47\n7v 1\n[ cs\n.L G\n] 2\n4 Ju\nn 20\n15\neach of which contains variant softmax visible units but the same binary hidden units.\nSuppose K is the vocabulary size. For a document with D words, if the ith word in the document equals the kth word of the dictionary, a vector vi \u2208 {0, 1}K is assigned, only with the kth element vik = 1. An RBM is formed by assigning a hidden state h \u2208 {0, 1}H to this document V = {v1, ...,vD}, where the energy function is:\nE\u03b8(V ,h) = \u2212hTWv\u0302 \u2212 bT v\u0302 \u2212D \u00b7 aTh (1)\nwhere \u03b8 = {W , b,a} are parameters shared by all the RBMs, and v\u0302 = \u2211D i=1 vi is commonly referred to as the word count vector of a document. The probability for the document V is given by:\nP\u03b8(V ) = 1 ZD e\u2212F\u03b8(V ), ZD = \u2211 V e\u2212F\u03b8(V )\nF\u03b8(V ) = log \u2211\nh e\u2212E\u03b8(V ,h)\n(2)\nwhere F\u03b8(V ) is the \u201cfree energy\u201d, which can be analytically integrated easily, and ZD is the \u201cpartition function\u201d for normalization, only associated with the document length D. As the hidden state and document are conditionally independent, the conditional distributions are derived:\nP\u03b8 (vik = 1|h) = exp\n( W Tk h+ bk )\u2211K k=1 exp ( W Tk h+ bk\n) (3) P\u03b8 (hj = 1|V ) = \u03c3 (Wj v\u0302 +D \u00b7 aj) (4)\nwhere \u03c3(x) = 1 1+e\u2212x . Equation (3) is the softmax units describing the multinomial distribution of the words, and Equation (4) serves as an efficient inference from words to semantic meanings, where we adopt the probabilities of each hidden unit \u201cactivated\u201d as the topic features."}, {"heading": "2.1 Learning Strategies for RSM", "text": "RSM is naturally learned by minimizing the negative log-likelihood function (ML) as follows:\nL(\u03b8) = \u2212EV \u223cPdata [logP\u03b8(V )] (5)\nHowever, the gradient is intractable for the combinatorial normalization term ZD. Common strategies to overcome this intractability are MCMCbased approaches such as Contrastive Divergence (CD) (Hinton, 2002) and Persistent CD (PCD) (Tieleman, 2008), both of which require repeating Gibbs steps of h(i) \u223c P\u03b8(h|V (i)) and V (i+1) \u223c P\u03b8(V |h(i)) to generate model samples to approximate the gradient. Typically, the performance and\nconsistency improve when more steps are adopted. Notwithstanding, even one Gibbs step is time consuming for RSM, since the multinomial sampling normally requires linear time computations. The \u201calias method\u201d (Kronmal and Peterson Jr, 1979) speeds up multinomial sampling to constant time while linear time is required for processing the distribution. Since P\u03b8(V |h) changes at every iteration in CD, such methods cannot be used."}, {"heading": "3 Efficient Learning for RSM", "text": "Unlike (Dahl et al., 2012) that retains CD, we adopted NCE as the basic learning strategy. Considering RSM is designed for documents, we further modified NCE with two novel heuristics, developing the approach \u201cPartial Noise Uniform Contrastive Estimate\u201d (or \u03b1-NCE for short)."}, {"heading": "3.1 Noise Contrastive Estimate", "text": "Noise Contrastive Estimate (NCE), similar to CD, is another estimator for training models with intractable partition functions. NCE solves the intractability through treating the partition function ZD as an additional parameter ZcD added to \u03b8, which makes the likelihood computable. Yet, the model cannot be trained through ML as the likelihood tends to be arbitrarily large by setting ZcD to huge numbers. Instead, NCE learns the model in a proxy classification problem with noise samples.\nGiven a document collection (data) {Vd}Td , and another collection (noise) {Vn}Tn with Tn = kTd, NCE distinguishes these (1+k)Td documents simply based on Bayes\u2019 Theorem, where we assumed data samples matched by our model, indicating P\u03b8 ' Pdata, and noise samples generated from an artificial distribution Pn. Parameters are learned by minimizing the cross-entropy function:\nJ(\u03b8) = \u2212EVd\u223cP\u03b8 [log \u03c3k(X(Vd))] \u2212kEVn\u223cPn [log \u03c3k\u22121(\u2212X(Vn))]\n(6)\nand the gradient is derived as follows,\n\u2212\u2207\u03b8J(\u03b8) =EVd\u223cP\u03b8 [\u03c3k\u22121(\u2212X)\u2207\u03b8X(Vd)] \u2212kEVn\u223cPn [\u03c3k(X)\u2207\u03b8X(Vn)] (7)\nwhere \u03c3k(x) = 11+ke\u2212x , and the \u201clog-ratio\u201d is:\nX(V ) = log [P\u03b8(V )/Pn(V )] (8)\nJ(\u03b8) can be optimized efficiently with stochastic gradient descent (SGD). Gutmann and Hyva\u0308rinen (2010) showed that the NCE gradient\u2207\u03b8J(\u03b8) will reach the ML gradient when k \u2192 \u221e. In practice, a larger k tends to train the model better."}, {"heading": "3.2 Partial Noise Sampling", "text": "Different from (Mnih and Teh, 2012), which generates noise per word, RSM requires the estimator to sample the noise at the document level. An intuitive approach is to sample from the empirical distribution p\u0303 forD times, where the log probability is computed: logPn(V ) = \u2211 v\u2208V [ vT log p\u0303 ] .\nFor a fixed k, Gutmann and Hyva\u0308rinen (2010) suggested choosing the noise close to the data for a sufficient learning result, indicating full noise might not be satisfactory. We proposed an alternative \u201cPartial Noise Sampling (PNS)\u201d to generate noise by replacing part of the data with sampled words. See Algorithm 1, where we fixed the\nAlgorithm 1 Partial Noise Sampling 1: Initialize: k, \u03b1 \u2208 (0, 1) 2: for each Vd = {v}D \u2208 {Vd}Td do 3: Set: Dr = d\u03b1 \u00b7De 4: Draw: Vr = {vr}Dr \u2286 V uniformly 5: for j = 1, ..., k do 6: Draw: V (j)n = {v(j)n }D\u2212Dr \u223c p\u0303 7: V (j) n = V (j) n \u222a Vr\n8: end for 9: Bind: (Vd,Vr), (V (1) n ,Vr), ..., (V (k) n ,Vr)\n10: end for\nproportion of remaining words at \u03b1, named \u201cnoise level\u201d of PNS. However, traversing all the conditions to guess the remaining words requiresO(D!) computations. To avoid this, we simply bound the remaining words with the data and noise in advance and the noise logPn(V ) is derived readily:\nlogP\u03b8(Vr) + \u2211\nv\u2208V \\Vr\n[ vT log p\u0303 ] (9)\nwhere the remaining words Vr are still assumed to be described by RSM with a smaller document length. In this way, it also strengthens the robustness of RSM towards incomplete data.\nSampling the noise normally requires additional computational load. Fortunately, since p\u0303 is fixed, sampling is efficient using the \u201calias method\u201d. It also allows storing the noise for subsequent use, yielding much faster computation than CD."}, {"heading": "3.3 Uniform Contrastive Estimate", "text": "When we initially implemented NCE for RSM, we found the document lengths terribly biased the log-ratio, resulting in bad parameters. Therefore \u201cUniform Contrastive Estimate (UCE)\u201d was proposed to accommodate variant document lengths\nby adding the uniform assumption:\nX\u0304(V ) = D\u22121 log [P\u03b8(V )/Pn(V )] (10)\nwhere UCE adopts the uniform probabilities D \u221a P\u03b8\nand D \u221a Pn for classification to average the modelling ability at word-level. Note that D is not necessarily an integer in UCE, and allows choosing a real-valued weights on the document such as idf -weighting (Salton and McGill, 1983). Typically, it is defined as a weighting vector w, where wk = log\nTd |V \u2208{Vd}:vik=1,vi\u2208V | is multiplied to the\nkth word in the dictionary. Thus for a weighted input V w and corresponding length Dw, we derive:\nX\u0303(V w) = Dw\u22121 log [P\u03b8(V w)/Pn(V w)] (11)\nwhere logPn(V w) = \u2211 vw\u2208V w [ vwT log p\u0303 ] . A specific ZcDw will be assigned to P\u03b8(V w).\nCombining PNS and UCE yields a new estimator for RSM, which we simply call \u03b1-NCE1."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets and Details of Learning", "text": "We evaluated the new estimator to train RSMs on two text datasets: 20 Newsgroups and IMDB.\nThe 20 Newsgroups2 dataset is a collection of the Usenet posts, which contains 11,345 training and 7,531 testing instances. Both the training and testing sets are labeled into 20 classes. Removing stop words as well as stemming were performed.\nThe IMDB dataset3 is a benchmark for sentiment analysis, which consists of 100,000 movie reviews taken from IMDB. The dataset is divided into 75,000 training instances (1/3 labeled and 2/3 unlabeled) and 25,000 testing instances. Two types of labels, positive and negative, are given to show sentiment. Following (Maas et al., 2011), no stop words are removed from this dataset.\nFor each dataset, we randomly selected 10% of the training set for validation, and the idf -weight vector is computed in advance. In addition, replacing the word count v\u0302 by dlog (1 + v\u0302)e slightly improved the modelling performance for all models.\nWe implemented \u03b1-NCE according to the parameter settings in (Hinton, 2010) using SGD in minibatches of size 128 and an initialized learning rate of 0.1. The number of hidden units was fixed\n1\u03b1 comes from the noise level in PNS, but UCE is also the vital part of this estimator, which is absorbed in \u03b1-NCE.\n2Available at http://qwone.com/\u02dcjason/20Newsgroups 3Available at http://ai.stanford.edu/\u02dcamaas/data/sentiment\nat 128 for all models. Although learning the partition function ZcD separately for every length D is nearly impossible, as in (Mnih and Teh, 2012) we also surprisingly found freezing ZcD as a constant function of D without updating never harmed but actually enhanced the performance. It is probably because the large number of free parameters in RSM are forced to learn better when ZcD is a constant. In practise, we set this constant function as ZcD = 2 H \u00b7 (\u2211 k e bk )D. It can readily extend to learn RSM for real-valued weighted length Dw. We also implemented CD with the same settings. All the experiments were run on a single GPU GTX970 using the library Theano (Bergstra et al., 2010). To make the comparison fair, both \u03b1-NCE and CD share the same implementation."}, {"heading": "4.2 Evaluation of Efficiency", "text": "To evaluate the efficiency in learning, we used the most frequent words as dictionaries with sizes ranging from 100 to 20, 000 for both datasets, and test the computation time both for CD of variant Gibbs steps and \u03b1-NCE of variant noise sample sizes. The comparison of the mean running\ntime per minibatch is clearly shown in Figure 1, which is averaged on both datasets. Typically, \u03b1-NCE achieves 10 to 500 times speed-up compared to CD. Although both CD and \u03b1-NCE run slower when the input dimension increases, CD tends to take much more time due to the multinomial sampling at each iteration, especially when more Gibbs steps are used. In contrast, running time stays reasonable in \u03b1-NCE even if a larger noise size or a larger dimension is applied."}, {"heading": "4.3 Evaluation of Performance", "text": "One direct measure to evaluate the modelling performance is to assess RSM as a generative model\nto estimate the log-probability per word as perplexity. However, as \u03b1-NCE learns RSM by distinguishing the data and noise from their respective features, parameters are trained more like a feature extractor than a generative model. It is not fair to use perplexity to evaluate the performance. For this reason, we evaluated the modelling performance with some indirect measures.\nFor 20 Newsgroups, we trained RSMs on the training set, and reported the results on document retrieval and document classification. For retrieval, we treated the testing set as queries, and retrieved documents with the same labels in the training set by cosine-similarity. Precision-recall (P-R) curves and mean average precision (MAP) are two metrics we used for evaluation. For classification, we trained a softmax regression on the training set, and checked the accuracy on the testing set. We use this dataset to show the modelling ability of RSM with different estimators.\nFor IMDB, the whole training set is used for learning RSMs, and an L2-regularized logistic regression is trained on the labeled training set. The error rate of sentiment classification on the testing set is reported, compared with several BoW-based baselines. We use this dataset to show the general modelling ability of RSM compared with others.\nWe trained both \u03b1-NCE and CD, and naturally NCE (without UCE) at a fixed vocabulary size (2000 for 20 Newsgroups, and 5000 for IMDB). Posteriors of the hidden units were used as topic features. For \u03b1-NCE , we fixed noise level at 0.5 for 20 Newsgroups and 0.3 for IMDB. In comparison, we trained CD from 1 up to 5 Gibbs steps.\nFigure 2 and Table 1 show that a larger noise size in \u03b1-NCE achieves better modelling perfor-\nmance, and \u03b1-NCE greatly outperforms CD on retrieval tasks especially around large recall values. The classification results of \u03b1-NCE is also comparable or slightly better than CD. Simultaneously, it is gratifying to find that the idf -weighting inputs achieve the best results both in retrieval and classification tasks, as idf -weighting is known to extract information better than word count. In addition, naturally NCE performs poorly compared to others in Figure 2, indicating variant document lengths actually bias the learning greatly.\nOn the other hand, Table 2 shows the performance of RSM in sentiment classification, where model combinations reported in previous efforts are not considered. It is clear that \u03b1-NCE learns RSM better than CD, and outperforms BoW and other BoW-based models4 such as LDA. The idf -\n4Accurately, WRRBM uses \u201cbag of n-grams\u201d assumption.\nweighting inputs also achieve the best performance. Note that RSM is also based on BoW, indicating \u03b1-NCE has arguably reached the limits of learning BoW-based models. In future work, RSM can be extended to more powerful undirected topic models, by considering more syntactic information such as word-order or dependency relationship in representation. \u03b1-NCE can be used to learn them efficiently and achieve better performance."}, {"heading": "4.4 Choice of Noise Level-\u03b1", "text": "In order to decide the best noise level (\u03b1) for PNS, we learned RSMs using \u03b1-NCE with different noise levels for both word count and idf -weighting inputs on the two datasets. Figure 3 shows that \u03b1-NCE learning with partial noise (\u03b1 > 0) outperforms full noise (\u03b1 = 0) in most situations, and achieves better results than CD in retrieval and classification on both datasets. However, learning tends to become extremely difficult if the noise becomes too close to the data, and this explains why the performance drops rapidly when \u03b1 \u2192 1. Furthermore, curves in Figure 3 also imply the choice of \u03b1 might be problem-dependent, with larger sets like IMDB requiring relatively smaller \u03b1. Nonetheless, a systematic strategy for choosing optimal \u03b1 will be explored in future work. In practise, a range from 0.3 \u223c 0.5 is recommended."}, {"heading": "5 Conclusions", "text": "We propose a novel approach \u03b1-NCE for learning undirected topic models such as RSM efficiently, allowing large vocabulary sizes. It is new a estimator based on NCE, and adapted to documents with variant lengths and weighted inputs. We learn RSMs with \u03b1-NCE on two classic benchmarks, where it achieves both efficiency in learning and accuracy in retrieval and classification tasks."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Training restricted boltzmann machines on word observations", "author": ["George E Dahl", "Ryan P Adams", "Hugo Larochelle."], "venue": "arXiv preprint arXiv:1202.5695.", "citeRegEx": "Dahl et al\\.,? 2012", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Stochastic ratio matching of rbms for sparse high-dimensional inputs", "author": ["Yann Dauphin", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems, pages 1340\u20131348.", "citeRegEx": "Dauphin and Bengio.,? 2013", "shortCiteRegEx": "Dauphin and Bengio.", "year": 2013}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 297\u2013304.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Replicated softmax: an undirected topic model", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov."], "venue": "Advances in neural information processing systems, pages 1607\u20131614.", "citeRegEx": "Hinton and Salakhutdinov.,? 2009", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2009}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey Hinton."], "venue": "Neural computation, 14(8):1771\u20131800.", "citeRegEx": "Hinton.,? 2002", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["Geoffrey Hinton."], "venue": "Momentum, 9(1):926.", "citeRegEx": "Hinton.,? 2010", "shortCiteRegEx": "Hinton.", "year": 2010}, {"title": "Learning the similarity of documents: An information-geometric approach to document retrieval and categorization", "author": ["Thomas Hofmann"], "venue": null, "citeRegEx": "Hofmann.,? \\Q2000\\E", "shortCiteRegEx": "Hofmann.", "year": 2000}, {"title": "Some extensions of score matching", "author": ["Aapo Hyv\u00e4rinen."], "venue": "Computational statistics & data analysis, 51(5):2499\u20132512.", "citeRegEx": "Hyv\u00e4rinen.,? 2007", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2007}, {"title": "On the alias method for generating random variables from a discrete distribution", "author": ["Richard A Kronmal", "Arthur V Peterson Jr."], "venue": "The American Statistician, 33(4):214\u2013218.", "citeRegEx": "Kronmal and Jr.,? 1979", "shortCiteRegEx": "Kronmal and Jr.", "year": 1979}, {"title": "A probabilistic model for semantic word vectors", "author": ["Andrew L Maas", "Andrew Y Ng."], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning.", "citeRegEx": "Maas and Ng.,? 2010", "shortCiteRegEx": "Maas and Ng.", "year": 2010}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1081\u20131088.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "arXiv preprint arXiv:1206.6426.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Proceedings of the international workshop on artificial intelligence and statistics, pages 246\u2013252. Citeseer.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Semantic hashing", "author": ["Ruslan Salakhutdinov", "Geoffrey Hinton."], "venue": "International Journal of Approximate Reasoning, 50(7):969\u2013978.", "citeRegEx": "Salakhutdinov and Hinton.,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Introduction to modern information retrieval", "author": ["Gerard Salton", "Michael J McGill"], "venue": null, "citeRegEx": "Salton and McGill.,? \\Q1983\\E", "shortCiteRegEx": "Salton and McGill.", "year": 1983}, {"title": "Modeling documents with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov", "Geoffrey E Hinton."], "venue": "arXiv preprint arXiv:1309.6865.", "citeRegEx": "Srivastava et al\\.,? 2013", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["Tijmen Tieleman."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 1064\u2013 1071. ACM.", "citeRegEx": "Tieleman.,? 2008", "shortCiteRegEx": "Tieleman.", "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al.", "startOffset": 60, "endOffset": 75}, {"referenceID": 2, "context": "They are mainly constructed by directed structure like pLSA (Hofmann, 2000) and LDA (Blei et al., 2003).", "startOffset": 84, "endOffset": 103}, {"referenceID": 19, "context": "Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy.", "startOffset": 96, "endOffset": 153}, {"referenceID": 21, "context": "Accompanied by the vast developments in deep learning, several undirected topic models, such as (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013), have recently been reported to achieve great improvements in efficiency and accuracy.", "startOffset": 96, "endOffset": 153}, {"referenceID": 6, "context": "Replicated Softmax model (RSM) (Hinton and Salakhutdinov, 2009), a kind of typical undirected topic model, is composed of a family of Restricted Boltzmann Machines (RBMs).", "startOffset": 31, "endOffset": 63}, {"referenceID": 16, "context": "A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a).", "startOffset": 61, "endOffset": 107}, {"referenceID": 14, "context": "A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a).", "startOffset": 61, "endOffset": 107}, {"referenceID": 5, "context": "Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv\u00e4rinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al.", "startOffset": 59, "endOffset": 88}, {"referenceID": 17, "context": "Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv\u00e4rinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al., 2013b).", "startOffset": 197, "endOffset": 240}, {"referenceID": 15, "context": "Recently, a new estimator Noise Contrastive Estimate (NCE) (Gutmann and Hyv\u00e4rinen, 2010) is proposed for unnormalized models, and shows great efficiency in learning word representations such as in (Mnih and Teh, 2012; Mikolov et al., 2013b).", "startOffset": 197, "endOffset": 240}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree.", "startOffset": 0, "endOffset": 152}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation.", "startOffset": 0, "endOffset": 456}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv\u00e4rinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs.", "startOffset": 0, "endOffset": 645}, {"referenceID": 0, "context": "Bengio et al. (2003) pointed this problem out when normalizing the softmax probability in the neural language model (NNLM), and Morin and Bengio (2005) solved it based on a hierarchical binary tree. A similar architecture was used in word representations like (Mnih and Hinton, 2009; Mikolov et al., 2013a). Directed tree structures cannot be applied to undirected models like RSM, but stochastic approaches can work well. For instance, Dahl et al. (2012) found that several Metropolis Hastings sampling (MH) approaches approximate the softmax distribution in CD well, although MH requires additional complexity in computation. Hyv\u00e4rinen (2007) proposed Ratio Matching (RM) to train unnormalized models, and Dauphin and Bengio (2013) added stochastic approaches in RM to accommodate high-dimensional inputs.", "startOffset": 0, "endOffset": 734}, {"referenceID": 21, "context": "As RSM is usually used as the first layer in many deeper undirected models like Deep Boltzmann Machines (Srivastava et al., 2013), \u03b1-NCE can be readily extended to learn them efficiently.", "startOffset": 104, "endOffset": 129}, {"referenceID": 7, "context": "Common strategies to overcome this intractability are MCMCbased approaches such as Contrastive Divergence (CD) (Hinton, 2002) and Persistent CD (PCD) (Tieleman, 2008), both of which require repeating Gibbs steps of h(i) \u223c P\u03b8(h|V (i)) and V (i+1) \u223c P\u03b8(V |h(i)) to generate model samples to approximate the gradient.", "startOffset": 111, "endOffset": 125}, {"referenceID": 22, "context": "Common strategies to overcome this intractability are MCMCbased approaches such as Contrastive Divergence (CD) (Hinton, 2002) and Persistent CD (PCD) (Tieleman, 2008), both of which require repeating Gibbs steps of h(i) \u223c P\u03b8(h|V (i)) and V (i+1) \u223c P\u03b8(V |h(i)) to generate model samples to approximate the gradient.", "startOffset": 150, "endOffset": 166}, {"referenceID": 3, "context": "Unlike (Dahl et al., 2012) that retains CD, we adopted NCE as the basic learning strategy.", "startOffset": 7, "endOffset": 26}, {"referenceID": 5, "context": "Gutmann and Hyv\u00e4rinen (2010) showed that the NCE gradient\u2207\u03b8J(\u03b8) will reach the ML gradient when k \u2192 \u221e.", "startOffset": 0, "endOffset": 29}, {"referenceID": 17, "context": "Different from (Mnih and Teh, 2012), which generates noise per word, RSM requires the estimator to sample the noise at the document level.", "startOffset": 15, "endOffset": 35}, {"referenceID": 5, "context": "For a fixed k, Gutmann and Hyv\u00e4rinen (2010) suggested choosing the noise close to the data for a sufficient learning result, indicating full noise might not be satisfactory.", "startOffset": 15, "endOffset": 44}, {"referenceID": 20, "context": "Note that D is not necessarily an integer in UCE, and allows choosing a real-valued weights on the document such as idf -weighting (Salton and McGill, 1983).", "startOffset": 131, "endOffset": 156}, {"referenceID": 13, "context": "Following (Maas et al., 2011), no stop words are removed from this dataset.", "startOffset": 10, "endOffset": 29}, {"referenceID": 8, "context": "We implemented \u03b1-NCE according to the parameter settings in (Hinton, 2010) using SGD in minibatches of size 128 and an initialized learning rate of 0.", "startOffset": 60, "endOffset": 74}, {"referenceID": 17, "context": "Although learning the partition function Zc D separately for every length D is nearly impossible, as in (Mnih and Teh, 2012) we also surprisingly found freezing Zc D as a constant function of D without updating never harmed but actually enhanced the performance.", "startOffset": 104, "endOffset": 124}, {"referenceID": 1, "context": "All the experiments were run on a single GPU GTX970 using the library Theano (Bergstra et al., 2010).", "startOffset": 77, "endOffset": 100}, {"referenceID": 12, "context": "Bag of Words (BoW) (Maas and Ng, 2010) 86.", "startOffset": 19, "endOffset": 38}, {"referenceID": 13, "context": "75% LDA (Maas et al., 2011) 67.", "startOffset": 8, "endOffset": 27}, {"referenceID": 13, "context": "42% LSA (Maas et al., 2011) 83.", "startOffset": 8, "endOffset": 27}, {"referenceID": 3, "context": "44% WRRBM (Dahl et al., 2012) 87.", "startOffset": 10, "endOffset": 29}, {"referenceID": 11, "context": "Bag of Words (BoW) (Maas and Ng, 2010) 86.75% LDA (Maas et al., 2011) 67.42% LSA (Maas et al., 2011) 83.96% Maas et al. (2011)\u2019s \u201cfull\u201d model 87.", "startOffset": 20, "endOffset": 127}], "year": 2015, "abstractText": "Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefficient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.", "creator": "TeX"}}}