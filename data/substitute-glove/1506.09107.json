{"id": "1506.09107", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2015", "title": "A complex network approach to stylometry", "abstract": "Statistical methods have been be contractors one medicine form ethical land of mandarin. In have years, practical the complex including dynamical components proved ideal to change several literature models. Despite the of amount of faculty devoted to represent texts with physical engine, much right limited related well doctoral likely shown you making land within created stress physical controls can some employed to needs the performance another particularly uses device transition. In this press, I requests this change by devising complex networks use nor most able to emphasis made debut of significant surveys methods. Using was fuzzy classification financing, I what that into formula_10 properties extracted moved texts functionality as such textual familiar. In separate cases, entire surprising revealed with suvs precisely margins but results obtained when only important or offline purely claimed these. Because the proposed instance means generic, the mechanism devised taken have fact straightforwardly commonly to expert example textual tools place present topology plays when outset role present the description of the interacting contacted.", "histories": [["v1", "Tue, 30 Jun 2015 14:32:30 GMT  (2437kb,D)", "https://arxiv.org/abs/1506.09107v1", null], ["v2", "Mon, 17 Aug 2015 13:59:39 GMT  (2437kb,D)", "http://arxiv.org/abs/1506.09107v2", "PLoS ONE, 2015 (to appear)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["diego r amancio"], "accepted": false, "id": "1506.09107"}, "pdf": {"name": "1506.09107.pdf", "metadata": {"source": "CRF", "title": "A complex network approach to stylometry", "authors": ["Diego R. Amancio"], "emails": ["diego@icmc.usp.br,", "diego.raphael@gmail.com"], "sections": [{"heading": null, "text": "properties of language. In recent years, methods from complex and dynamical systems proved useful to create several language models. Despite the large amount of studies devoted to represent texts with physical models, only a limited number of studies have shown how the properties of the underlying physical systems can be employed to improve the performance of natural language processing tasks. In this paper, I address this problem by devising complex networks methods that are able to improve the performance of current statistical methods. Using a fuzzy classification strategy, I show that the topological properties extracted from texts complement the traditional textual description. In several cases, the performance obtained with hybrid approaches outperformed the results obtained when only traditional or networked methods were used. Because the proposed model is generic, the framework devised here could be straightforwardly used to study similar textual applications where the topology plays a pivotal role in the description of the interacting agents.\nPACS numbers: 89.75.Hc,02.40.Pc,02.50.-r\nKeywords: complex networks, network topology, pattern recognition"}, {"heading": "1. Introduction", "text": "The human language represents a major factor responsible for the success of our species; and its written form is one of the main expression used to convey and share information. Owing to the ubiquity of language in several contexts, many linguistic aspects have been studied via the application of methods and tools borrowed from diverse scientific fields. As a consequence, several findings related to the origins, organization and structure of the language have been unveiled. One of the most fundamental patterns arising from statistical analysis of huge amounts of text is the Zipf\u2019s law, which states that the frequency of the words decreases inversely to their rank [1\u20133]. Other fundamental recurrent pattern is the Heap\u2019s law, which states that the vocabulary size grows slowly with the number of tokens of the document [4\u20136]. More recently, concepts from Physics have been applied to model several language features. For example, with regard to longscale properties, concepts from dynamical systems have successfully been employed to ar X\niv :1\n50 6.\n09 10\n7v 2\n[ cs\n.C L\n] 1\n7 A\nug 2\n01 5\n2 compare the burstiness of the spatial disbribution of words in documents [7, 8]. In a similar fashion, the spatial distribution of words in documents and analogous systems have also been studied in terms of level statistics [9], entropy [10] and intermittency measurements [11]\nA well-known approach to study written texts is the word adjacency network\nmodel [12\u201314], which considers short-scale textual properties to form the networks. In this representation, relevant words conveying meaning are modeled as nodes and adjacency relationships are used to establish links. Using this model, several characteristics of texts and languages have been inferred from statistical analyses performed in the structure of networks [15]. Language networks have been increasingly employed to understand theoretical linguistic aspects, such as the origins of fundamental properties [16] and the underlying mechanisms behind language acquisition in early years [17]. In practical terms, networks have been applied in the context of machine translations [18], autommatic summarization [19], sense disambiguation [20,21], complexity/quality analysis [22,23] and document classification [24]. Despite the relative success of applying networks concepts to better understand language phenomena, in many real-world applications attributes extracted from networked models have not contributed to the advancement of the state of art. For example, when one considers the document classification task, a strong dependency of network features on textual characteristics have been observed. However, the best performance are still achieved with traditional statistical natural language processing features. In this context, the present study address this problem by devising methods that effectively take advantage of network properties to boost the performance of the textual classification task. Here I focus on the text classification based on stylistic features, where a text is classified according to stylistic marks left by specific authors [25] or literary genres [26]. Upon introducing a hybrid classifier relying on the fuzzy definition of supervised pattern recognition methods [27], I show that the performance of style-based classifications can be significantly improved when topological information is included in traditional models. Given the generality of the proposed method, the framework devised here could be applied to improve the characterization and description of many related textual applications."}, {"heading": "2. Representing texts as networks", "text": "There are several ways to map texts into networks [12,15,19,28\u201330]. The most suitable form depends on the context of the application. In occasions where the semantics is relevant, the words sharing some semantical relationship are linked [20, 22, 31\u201333]. In a similar fashion, other semantic-based models link the words appearing in a given context [34, 35] (e.g. in the same sentence or paragraph). A general model for establishing significant \u201csemantical\u201d links between co-occurring elements was devised in [36]. More specifically, the model considers the existence of a set of elements V = {v1, . . . , vn}, which may occur in one or more sets of a given collection \u03be = {S1, . . . , SN}.\n3 Given two elements \u03b1 \u2208 V and \u03b2 \u2208 V , the model computes the probability p that more than r sets in \u03be contain both elements \u03b1 and \u03b2 as\npt = \u2211 j\u2265r p(j), (1)\nwhere p(j) is the probability that \u03b1 and \u03b2 co-occur exactly j times in the same set. Given a confidence level p0, the strength of the link between \u03b1 and \u03b2 is s = log(p0/pt).\nIn applications where the style (or structure) plays an important role, the links\namong words are established according to syntactical relationships [19,30]. A well-known approach for grasping stylistic features of texts is the word adjacency model [13,37,38], which basically connect adjacent words in the text. Differently from the model devised in [36], the word adjacency model captures the stylistic features of texts [39]. It has been shown that the adjacency model is able to capture most of the syntactical links with the benefit of being language independent. Despite being a simplification of the syntactical analysis, the adjacency model has been employed in several contexts because the topological properties of word adjacency and syntactical networks are similar. Such high degree of similarity can be explained by the fact that most of the syntactical links occurs among neighbouring words [30]. In the current paper, the traditional word adjacency representation was adopted.\nBefore mapping the text into a word adjacency network, some pre-processsing steps\nare usually applied. First, words conveying low semantic content, such as articles and prepositions, are removed from the text. These words, referred to as stopwords, are disregarded because they just serve to link content words. As a consequence, they can be straightforwardly replaced by network edges. The remaining words are then lemmatized, i.e. they are transformed to their canonical forms. To assist the lemmatization process, all words are labeled with their part-of-speech tags. Particularly, in this study, I used a model based on maximum entropy [40]. To exemplify the construction of a word adjacency network, I show in table 1 the pre-processing steps performed in a short text. The corresponding network is shown in figure 1."}, {"heading": "3. Topological characterization of networks", "text": "A network can be defined as G = {V,E}, where V denotes the set of nodes and E denotes the set of edges, which serve to link nodes. An unweighted network can be represented by an adjacency matrix A = {aij}, where each element aij stores the information concerning the connectivity of nodes i and j. If i and j are connected, then aij = 1. Otherwise, aij = 0. Note that, in a undirected network devoid of self loops, AT = A. Currently, there are several network measurements available to characterize the topology of complex networks [41]. Here, I describe the main measurements applied for text analysis:\n\u2022 Degrees: the degree (k) is the number of edges connected to the node, i.e. ki = \u2211 j aij. A relevant feature related to the degree that has been useful for\n4\nRETINA\nNAME\n5 \u2206k (n) i = [ 1\nki \u2211 j aij ( kj \u2212 1 ki \u2211 m aimkm )2]1/2 . (3)\nIn text networks, both k(n) and \u2206k(n) have been useful to quantify the structural organization of texts [42].\n\u2022 Accessibility: the accessibility measurement is a extension of the node degree centrality [43]. It is defined as a normalization of the diversity measurement [43],\nwhich quantifies the irregularity of a accessing neighbors through self-avoiding random walks [43]. To define the accessibility, let Ph(i, j) be the probability of a random walker starting at node i to reach node j in exactly h steps. The heterogeneity of access to neighbors can be quantified with the diversity measurement:\n\u03b4 (h) i = \u2212 \u2211 j Ph(i, j) logPh(i, j), (4)\nGiven eq. 4, the accessibility (\u03b1) is computed as\n\u03b1 (h) i = exp(\u03b4 (h) i ). (5)\nIt can be shown that the accessibility is bounded according to the relation 0 \u2264 \u03b1(h) \u2264 nh, where nh is the number of nodes at the h-th concentric level [44]. An example of the computation of the accessibility in a small network is shown in figure 2. In the example, nodes 2, 3, 4 and 5 belong to the first concentric level and nodes 6, 7, 8, 9 and 10 belong to the second concentric level. When one considers a regular access to the second level (red configuration), the accessibility corresponds to the total number of nodes located at the h-th concentric level. When some nodes are more accessed than others, the accessibility decreases because less nodes are effectively accessed. In textual networks, it has been shown that the accessibility is more advantageous than other traditional centrality measurements as it is able to capture more information at further hierarchical levels [44]. This measurement has been successfully applied to detect core concepts in texts [19]. Furthermore, it has also been employed to generate informative summaries [19]. A dependency of the distribution of this measurement with stylistic features of text was observed in [45].\n\u2022 Betweenness: the betweenness is a centrality measurement that considers that a node is relevant if it is highly accessed via shortest paths. If nsit is the number of\ngeodesic paths between s and t passing through node i; and nst is the total number of shortest paths linking s and t, then the betweenness is defined as:\nBi = 1\nN2 \u2211 s \u2211 t nsit nst\n(6)\nIn word adjacency networks, high frequent words usually take high values of betweenness. However, some words may act as articulation points whenever they link two semantical contexts or communities [46]. It has been shown that\n1 = 5. When blue edges are included, nodes 7 and 9 tend to receive more visits\nthan the other nodes, according to the considered probabilities. For this reason, the effective number of accessed nodes drops to \u03b1 (h=2) 1 = 4.71.\nthe betweenness is able to identify the generality of contexts in which a word appears [47]. More specifically, domain-specific words tend to assume lower values of betweenness when compared with more generic words.\n\u2022 Assortativity: several real-world networks are formed of nodes with a specific type of classification. For example, in social networks, individuals may be classified\nby considering their age, sex or race. When analyzing the connectivity patterns of networks, it might be relevant to study how distinct classes connect to each other. This type of analysis is usually performed with the so-called assortativity measurement [48]. The assortativity can be computed as the Pearson correlation coefficient (r):\nr = e\u22121 \u2211 j>i kikjaij \u2212 [e\u22121 \u2211 j>i 1 2 (ki + kj)aij] 2\ne\u22121 \u2211\nj>i 1 2 (k2i + k 2 j )aij \u2212 [e\u22121 \u2211 j>i 1 2 (ki + kj)aij] 2 , (7)\nwhere e is the total number of edges. In word adjacency networks, the assortativity quantifies how words with distinct frequency appear as neighbors.\n7 \u2022 Clustering coefficient: the clustering coefficient (C) quantifies the local density of neighbors of a given node. The local definition of the clustering coefficient is\ngiven by the fraction of the number of triangles among all possible connected sets of three nodes:\nC = 3 \u2211 k>j>i aijaikajk [ \u2211 k>j>i aijaik + ajiajk + akiakj ]\u22121 . (8)\nSimilarly to the betweenness, the clustering coefficient is useful to detect words appearing in generic contexts [47]. However, differently from the betweenness, the clustering coefficient analyzes only the local neighborhood of nodes.\n\u2022 Average shortest path length: the average shortest path length (l) is the typical distance between any two nodes in the network. This measurement was used in this\npaper because it has been useful in stylistic-based applications [47]. In texts, the average shortest path length quantifies words relevance. More specifically, according to this measurement, the most relevant words are those that are close to the hubs.\nMost of the measurements described in this section are local measurements, i.e.\neach node i possesses a value X\u0303i, where X\u0303 = {k, k(n),\u2206k(n), \u03b1, B, C, l}. For the purposes of this paper, it is necessary to sum up the local measurements. The most natural choice is to characterize the documents by computing the average \u3008X\u0303\u3009, where \u3008. . .\u3009 = N\u22121 \u2211N i=1 . . . stands for the average computed over the N distinct words of the text. A disadvantage associated with this type of summing procedure is that all words receive the same weight, regardless of their number of occurrences in the text. To avoid\nthis potential problem, I also computed the average value \u3008. . .\u3009\u2217 = \u03b7\u22121 \u2211\u03b7\ni=1 . . . obtained\nwhen only the \u03b7 = 50 most frequent words are considered. The standard deviations \u2206X\u0303 and the skewness \u03b3(X\u0303) were also used to characterize the documents.\nA drawback associated to the computation of local topological measurements in\nword adjacency networks is the high correlation found between these measurements and the node degree (i.e. the word frequency). To minimize this correlation, the following procedure was adopted. Each of the measurements was normalized by the average obtained over 30 texts produced using a word shuffling technique, where the frequencies of words are preserved. If \u00b5(X\u0303(R)) and \u03c3(X\u0303(R)) are the average and deviation obtained over the random realizations, the normalized measurement X and the error (X) related to X are\nX = X\u0303\n\u00b5(X\u0303(R)) (9)\n(X) = \u03c3(X\u0303(R))\n\u00b5(X\u0303(R))2 X\u0303 =\n\u03c3(X\u0303(R)) \u00b5(X\u0303(R)) X. (10)\n8"}, {"heading": "4. Traditional stylistic features", "text": ""}, {"heading": "Frequency of words and characters", "text": "Traditional methods usually perform statistical analysis using specific textual features [49]. An important contribution to the stylometry was introduced by Mosteller and Wallace [50], which showed that the frequency of function words (such as any, of, a and on) is useful to characterize the style in texts. Frequent words have also been used in strategies devised by physicists where the distance of frequency ranks was used to compute the similarity between texts [51\u201353]. Another relevant feature for characterizing styles in texts is the frequency of character bigrams (i.e. a sequence of two adjacent characters). These attributes have proven useful e.g. to detect the stylistic marks of specific authors [54]."}, {"heading": "Intermittence", "text": "The uneven spatial distribution of words along texts is a feature useful for characterizing the style of texts [10]. The quantification of the homogeneity of the distribution of words along texts can be performed by using recurrence times, a standard measure employed to study time series [55]. In texts, the concept of time is represented in terms of the number of words occuring in a given interval. For each word i, the recurrence time Tj is defined as the number of words appearing between two successive occurrences of i plus one. For example, the recurrence times of the word \u201cstone\u201d in the lemmatized text shown in table 1 are T1 = 1, T2 = 3, T3 = 3, T4 = 11, T5 = 1 and T6 = 5. If a word i occurs Ni times in a text comprising NT words, it generates a sequence of Ni \u2212 1 recurrence times {T1, T2, . . . , TNi\u22121}. In order to consider the time Tf until the first occurrence of i and the time Tl, the number of words between the last occurrence of i and the last word of the text, the recurrence time TN = Tf + Tl is added to the set of recurrence times of word i. As such, \u3008T \u3009 = NT/Ni, where \u3008\u00b7\u3009 is the average over distinct Tj\u2019s. Note that the average recurrence time \u3008T \u3009 does not provide additional information, since it only depends on the frequency. The intermittence (or burstiness) of the word i is obtained from the coefficient of the variation of the recurrence times:\nI = \u03c3T/\u3008T \u3009 = [ \u3008T 2\u3009 \u3008T \u30092 \u2212 1 ]1/2 . (11)\nIt has been shown that the intermittence has been useful to identify core concepts in texts, even when a large corpus is not available. Relevant words usually take high values of intermittency, i.e. I 1. Stopwords, on the other hand, are evenly distributed along the text [45]. To illustrate the properties of the intermittency measurement, figure 3 shows the spatial distribution of the words long and Hobson in the book \u201cAdventures of Sally\u201d, by P.G. Wodehouse. The frequency of these words are similar, yet their values of intermittency are quite different. Note that, while the distribution of long is relatively homogeneous, Hobson is unevenly distributed along the text. Because bursty concepts\nare the most relevant words [10], in the example of figure 3, Hobson would be considered a relevant character in the plot."}, {"heading": "5. Supervised classification", "text": "In this section, I describe the two methods employed to combine distinct strategies of classification. The objective here is to combine evidences from both traditional and CN-based methods. Before introducing the methods for combining classifiers, the main concepts concerning the supervised learning task are presented.\nIn a typical supervised learning task, two datasets are employed: the training and\nthe test dataset. The training dataset Xtr = {(x1, y1), (x2, y2), . . . , (xl, yl)} is the set of instances whose classes are known beforehand. The first component of the tuples xi = (f1 = a1, f2 = a2, . . .) represents the values of attributes used to describe the i-th instance. The second element yi \u2208 Y = {y1, y2, . . .} represents the class label of the i-th training instance. In the supervised learning task, the objective is to obtain the map x 7\u2192 y. The quality of the map obtained is evaluated with the test dataset Xts = {(xl, yl), (xl+1, yl+1) . . . , (xl+u, yl+u)}. The technique used to evaluate the performance of the classifiers used in this study is the well-known 10-fold crossvalidation [56].\nAn usual procedure in classification tasks is the quantification of the relevance of\neach attribute for the task. To quantify the relevance of attributes to discriminate the data, several indices have been proposed [56]. A well-known index, the information gain, quantifies the homogeneity of the set of instances in Xtr when the value of the attribute is specified [57]. Whenever a single class prevails when the value of an attribute fk is specified, the information gain associated to fk takes high values. Mathematically, the information gain \u2126 is defined as\n\u2126(Xtr, fk) = H(Xtr)\u2212H(Xtr|fk) (12)\nwhere H(Xtr) is the entropy of the training dataset Xtr and H(Xtr|fk) is the entropy\n10\nof the training dataset when the k-th attribute is specified. The quantity H(Xtr|fk) is computed as\nH(Xtr|fk) = 1 |Xtr| \u2211\nv\u2208V (fk)\n|{x \u2208 Xtr|fk = v}| H({x \u2208 Xtr|fk = v}) (13)\nwhere V (Fk) is the set comprising all values taken by fk in Xtr."}, {"heading": "Hybrid classifier", "text": "To define this classifier, consider the following definitions. Let mij be the strength associating the i-th instance to the j-th class, where 0 \u2264 mij \u2264 1. The term mij, henceforth referred to as membership strength, can be interpret as the likelihood of instance i to belong to class j. Note that the quantification of mij depends on the classifier being used. A detailed description of methods for computing the membership strength can be found in [27, 58\u201361]. Let m (R) ij be the membership strength obtained when topological features of complex networks are used and m (T ) ij the membership strength obtained when traditional statistical features are analyzed. According to the hybrid strategy, the combination of both topological and traditional methods is achieved according to the following convex combination\nm (H) ij = \u03bbm (R) ij + (1\u2212 \u03bb)m (T ) ij , (14)\nwhere \u03bb \u2208 [0, 1] accounts for the weight associated to the topological strategy. Note that the combination of evidences performed in equation 14 yields a membership strength m (H) ij ranging in the interval [0, 1]. The final decision is then made according to the rule\ny\u0303i = max j m\n(H) ij , (15)\nwhere y\u0303i \u2208 Y denotes the correct class label associated to the i-th test instance. In practical applications, a screening on the \u03bb can be useful to find its best value. However, this process might be computationally unaffordable for very large datasets. In this case, the process of finding the adequate value of the parameter can be made via application of optimization heuristics [62,63].\nTo illustrate how the decision is performed with the hybrid algorithm, figure 4\nshows the classification of a text modeled as a network. In the top panel, the central network is the text whose class is to be inferred. The left and right networks represent two networks in the training dataset. The node color denotes the node label, i.e. the word associated to the node. In this example, two texts are semantically similar if they share the same words. In the central panel, the scenario where the hybrid classified is set with \u03bb = 0.15 is considered. Because \u03bb is close to zero, the classification is mostly based on the number of shared words. In fact, the decision boundary in this case is created so that the test instance is classified in the same class as the right training instance. The bottom panel illustrates the decision made with \u03bb = 0.85. Now, the topological features of the text is the most prevalent feature used for the classification. As a consequence, the boundary decision moves to the right side so that the test instance is classified as\n11"}, {"heading": "INSTANCE OF THE TEST DATASET", "text": "the same class as the left training instance. Note that, the test instance as belonging to class c1 because the r1 and r? are topologically similar."}, {"heading": "Tiebreaker classifier", "text": "Similarly to the hybrid classifier, this classification scheme uses as attributes both traditional and topological features of texts modelled as networks. The objective of this approach is to use the topological attributes only when the classification performed with\n12\ntraditional features is not reliable, as revealed by the values of membership strength. Consider that the two most likely classes to which the unknown instance belongs are j and k, according to traditional features. As a consequence, m (T ) ij \u2265 m (T ) ik \u2265 m (T ) il , for each class l \u2208 Y \u2212 {i, j}. Let \u2206 be the difference between the probabilities associated to the two most likely classes, i.e. \u2206 = m\n(T ) ij \u2212 m (T ) ik . If such difference surpasses a\ngiven threshold \u03b8, the tiebreaker performs the classification by using only traditional attributes. Conversely, topological attributes are employed to infer the class of the unknown instance. Equivalently,\ny\u0303i =\n{ maxjm (T ) ij , if \u2206 = m (T ) ij \u2212m (T ) ik \u2265 \u03b8\nmax{m(R)ij ,m (R) ik }, otherwise.\n(16)\nNote that the threshold \u03b8 ultimately decides which attributes (traditional or topological) are used to perform the classification. An example of classification using this approach is shown in figure 5."}, {"heading": "6. Results and discussion", "text": "The effectiveness of the combination of traditional textual features and topological measurements of networks was evaluated in the context of two natural language processing tasks. Both tasks rely upon the characterization of stylistic marks of texts. The studied tasks were the authorship attribution and genre detection problems. The\n13\nclassifiers chosen to compound the combining techniques were: kNN, Support Vector Machines (SVM), Random Forest (RFO) and Multilayer Perceptron (MLP). These classifiers were chosen because they usually yield good accuracy rates with default parameters [64]."}, {"heading": "6.1. Authorship recognition", "text": "In the authorship attribution task, the objective is to identify the authors of texts whose identity is lacking [65]. The dataset employed here comprises books written by eight authors: Arthur Conan Doyle (ACD), Bram Stoker (BRS), Charles Dickens (CHD), Thomas Hardy (THH), Pelham Grenville Wodehouse (PGW), Hector Hugh Munro (HHM) and Herman Melville (HME). To show how the topological properties of the networks modelling books can be useful to improve the characterization of authors\u2019 styles in texts, the following combination of attributes were considered:\n\u2022 INT+CN: the intermittence of stopwords were considered along with the topological measurements of the network modeling the text. As stopwords, I\nconsidered all the words that appeared at least once in all books of the dataset. According to the dataset, there is a total of 340 stopwords.\n\u2022 FR+CN: the frequency of stopwords were considered along with the topological attributes of the networks.\n\u2022 BG+CN: the frequency of character bigrams were considered along with the topological attributes of the networks. All the possibilities of character bigrams\n(634) were considered in this analysis.\nFor each combination of attributes, the gain in performance when the topology is considered as an additional feature is represented as:\n\u2206\u0393(x) = \u0393H(x)\u2212 \u0393T\n\u0393T , (17)\nwhere \u0393H(x) denotes the accuracy rate obtained with either the hybrid or tiebreaker classifier and x is the corresponding parameter employed, i.e. \u03bb for the hybrid classifier and \u03b8 for the tiebreaker classifier. Figure 6 shows the values of \u2206\u0393(\u03bb) when the kNN is used to compose the hybrid classifier. The results obtained for additional values of the parameter k are shown in table 2. Figure 6 also shows the maximum accuracy rate obtained with the variation of \u03bb:\n\u2206\u0393max = max \u03bb\u2208[0,1] \u2206\u0393(\u03bb). (18)\nWith regard to the INT+CN combination, the addition of topological features of texts yielded accuracy rates much higher than the ones obtained with intermittency features alone, since \u2206\u0393max > 2 for k = {3, 4, 5}. Note that, in this case, the network approach alone performed better than the method based solely on intermittency features, because \u2206\u0393(\u03bb = 1) > 1. This observation might explain the high gain obtained when networks are included as an additional feature. As for the FR+CN\n14\ncombination, the highest gain in performance was \u2206\u0393max = 1.197, which was obtained for k = 5. The lowest gain in accuracy occurred for the BIG+CN combination. In this case, the maximum gain was \u2206\u0393max = 1.070 for k = 5. When other classifiers were used to compose the hybrid classifier, similar results have been found (see table 3): the highest and lowest improvement in performance occurred for the INT+CN and BIG+CN combinations, respectively. In general, the hybrid classification that combined traditional statistical features and topological measurements of networks improved the classification performance in the authorship recognition task. Interestingly, in several cases, the combination outperformed the accuracy rates obtained when the compounding strategies were analyzes separately, i.e. \u0393H(\u03bb \u2208]0, 1[) > \u0393H(\u03bb = 0) and \u0393H(\u03bb \u2208]0, 1[) > \u0393H(\u03bb = 1).\ntiebreaker classifier. The results obtained with the kNN classifier for k = {3, 4, 5} are shown in figure 7. The results obtained for other values of the parameter k are displayed in table 4. The tiebreaker classifier built using the INT+CN combination provided an increase in performance of up to 31%. The maximum gain, obtained with the combinations FR+CN and BG+CN, were respectively 34.2% and 4.8%. With regard to the other classifiers compounding the tiebreaker technique, the results obtained are shown in table 5. Interestingly, there was no improvement when the SVM was used (\u2206\u0393max = 1). Differently, when the RFO classifier was used, an improvement of\n15\n16\nperformance was observed in all three combinations of attributes. Finally, the tiebreaker classifier built with the MLP only provided a gain in accuracy for the combination INT+CN. While providing a gain in performance in several scenarios, the tiebreaker technique usually performed worst than the hybrid classifications performed with the same compound classifiers and parameters.\nable to improve the characterization of texts because such textual description grasps relevant patterns that are mostly disregarded by traditional statistical approaches. This can be confirmed by optimized results obtained for \u03bb\u2217 > 0 and \u03b8\u2217 > 0. To better understand the factors behind the discriminability power of the network model, the relevance of each topological for discriminating authors was computed with the information gain criterion. The best topological features were the standard deviation of the accessibility at the third level (\u2126(\u2206\u03b1(h=3)) = 1.07), the standard deviation of the average neighboorhood degree (\u2126(\u2206k(n)) = 1.05) and the average clustering coefficient (\u2126(\u3008C\u3009) = 0.60). An example of discriminability provided by the topological strategy is shown in figure 8."}, {"heading": "6.2. Style identification", "text": "In this section, I investigate if the topological features of complex networks are useful to complement the traditional textual characterization for the style identification task [66].\n17\n18\nThe dataset used was the Brown corpus [67], which comprises documents that are classified either as informative prose (e.g. press reportage, popular folklore and scientific manuscripts) or imaginative prose (e.g general fiction, romance and love stories). Differently from the authorship attribution task, the objective here is to cluster together documents written using the same style, regardless of their authorship.\nThe results obtained for the style identification in the Brown corpus using the\nhybrid technique are shown in tables 6 and 7. Considering the INT+CN combination, the highest gain in performance were 29.0% and 25.8% for the kNN (k = 4) and SVM methods. Minor improvements in accuracy were observed for other combinations, which might be explained by the high discriminability rates observed for the traditional techniques based on the frequency of stopwords (\u0393 = 95.6%) and character bigrams (\u0393 = 94.05%). The results obtained with the tiebreaker technique were inferior to those obtained with the hybrid technique (results not shown). All in all, the results confirm that the inclusion of topological attributes might also improve the characterization of documents in the context of stylistic-based classification tasks. In this task, the most relevant topological features were the skewness and deviation of the clustering coefficient (\u2126(\u03b3(C)) = 0.172 and \u2126(\u2206C) = 0.157). A visualization of the discriminability provided by topological features is shown in figure 9.\n19"}, {"heading": "7. Conclusions", "text": "In this study, I have shown that the styles of texts can be captured by measuring the topological properties of the corresponding network representation. More important than just to note a dependency between the structure of networks and stylistic features of texts, this study devised techniques to combine traditional and topological\n20\nfeatures, which were found to be of paramount importance to enhance the quality of current classification strategies. Two traditional stylometry tasks were studied: the authorship attribution and genre identification problems. In both tasks, the addition of topological features provided an improvement in classification performance. The highest improvement occurred with the hybrid classifier, which uses a linear convex combination of features from distinct textual evidences. The different nature of the quantities used to characterize texts suggests a complementary role in capturing distinct aspects of written texts. Because the adequate choice of parameters for the proposed technique is far from being a trivial task, I intend, as a future work, to devise a method that automatically assigns a value of \u03bb and \u0398 for each test instance."}, {"heading": "Acknowledgments", "text": "I acknowledge financial support from Sa\u0303o Paulo Research Foundation (FAPESP-Brazil) (grant number 14/20830-0)."}], "references": [{"title": "Human Behavior and the Principle of Least Effort", "author": ["GK Zipf"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1949}, {"title": "The city as a giant component: a random graph approach to Zipf\u2019s law", "author": ["R Kali"], "venue": "Appl. Econ. Lett", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "The evolution of the exponent of Zipf\u2019s law in language ontogeny", "author": ["J Baixeries", "B Elvevag", "R Ferrer-i-Cancho"], "venue": "PLoS ONE", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Deviations in the zipf and heaps laws in natural languages", "author": ["VV Bochkarev", "EY Lerner", "AV Shevlyakova"], "venue": "J. Phys.: Conf. Ser", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A (2013) A scaling law beyond Zipf\u2019s law and its relation to Heaps", "author": ["F Font-Clos", "G Boleda", "Corral"], "venue": "law. New J. Phys", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Deviation of Zipf\u2019s and Heaps\u2019 laws in human languages with limited dictionary", "author": ["L Lu", "Z Zhang", "T Zhou"], "venue": "sizes. Sci. Rep", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Distribution of content words and phrases in text and language modelling", "author": ["SM Katz"], "venue": "Nat. Lang. Eng", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Authorship recognition via fluctuation analysis of network topology and word intermittency", "author": ["DR Amancio"], "venue": "J. Stat. Mech. P03005", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Level statistics of words: Finding keywords in literary texts and symbolic sequences", "author": ["P Carpena", "P Bernaola-Galvan", "M Hackenberg", "AV Coronado", "JL Oliver"], "venue": "Phys. Rev. E", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Statistical keyword detection in literary corpora", "author": ["JP Herrera", "PA Pury"], "venue": "EPJ B", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Intermittency and scale-free networks: a dynamical model for human language complexity", "author": ["P Allegrini", "P Grigolini", "L Palatella"], "venue": "Chaos Soliton Fract", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Modeling the average shortest-path length in growth of word-adjacency networks", "author": ["A Kulig", "S Drozdz", "J Kwapien", "P Oswiecimka"], "venue": "Phys. Rev. E", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Prose and poetry classification and boundary detection using word adjacency network analysis", "author": ["RM Roxas", "G Tapang"], "venue": "Int. J. Mod. Phys. C", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Network properties of written human language", "author": ["AP Masucci", "GJ Rodgers"], "venue": "Phys. Rev. E", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Approaching human language with complex networks", "author": ["J Cong", "H Liu"], "venue": "Phys. Life Rev", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Least effort and the origins of scaling in human language", "author": ["R Ferrer i Cancho", "RV Sole"], "venue": "Proc. Natl. Acad. Sci. USA", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "The ontogeny of scale-free syntax networks: phase transitions in early language acquisition", "author": ["B Corominas-Murtra", "S Balverde", "R Sol\u00e9"], "venue": "Advs. Complex Syst", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Extractive summarization using complex networks and syntactic dependency", "author": ["DR Amancio", "MGV Nunes", "ON Oliveira Jr.", "LF Costa"], "venue": "Physica A", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Unveiling the relationship between complex networks metrics and word senses", "author": ["DR Amancio", "ON Oliveira Jr.", "LF Costa"], "venue": "Europhys. Lett", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Word sense disambiguation: a survey", "author": ["R Navigli"], "venue": "ACM Comput. Surv", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Complex networks analysis of language complexity", "author": ["DR Amancio", "SM Aluisio", "ON Oliveira Jr.", "LF Costa"], "venue": "Europhys. Lett", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Strong correlations between text quality and complex networks features", "author": ["L Antiqueira", "MGV Nunes", "ON Oliveira Jr.", "LF Costa"], "venue": "Physica A", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Graph-based natural language processing and information retrieval", "author": ["R Mihalcea", "D Radev"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "A survey of modern authorship attribution methods", "author": ["E Stamatatos"], "venue": "J. Am. Soc. Inf. Sci. Technol", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "An Evaluation of Text Classification Methods for Literary Study", "author": ["B Yu"], "venue": "Ph.D. Dissertation. University of Illinois at Urbana-Champaign,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "A fuzzy k-nearest neighbor algorithm", "author": ["JM Keller", "MR Gray", "JA Givens Jr."], "venue": "IEEE Trans. Syst. Man, Cybern", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1985}, {"title": "Networks in cognitive science", "author": ["A Baronchelli", "R Ferrer-i-Cancho", "R Pastor-Satorras", "N Chater", "MH Christiansen"], "venue": "Trends Cogn. Sci", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Patterns in syntactic dependency networks", "author": ["R Ferrer i Cancho", "R Sol\u00e9", "R Kohler"], "venue": "Phys. Rev. E", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "A (2012) Local-based semantic navigation on a networked representation of information", "author": ["JA Capitan", "J Borge-Holthoefer", "S Gomez", "J Martinez-Romo", "L Araujo", "JA Cuesta", "Arenas"], "venue": "PLoS ONE", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Complex structures and semantics in free word association", "author": ["P Gravino", "VDP Servedio", "A Barrat", "V Loreto"], "venue": "Adv. Complex Syst", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Word sense disambiguation via high order of learning in complex networks", "author": ["TC Silva", "DR Amancio"], "venue": "Europhys. Lett", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Hyperlex: Lexical cartography for information retrieval", "author": ["J Veronis"], "venue": "Comput. Speech Lang", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "A graph model for unsupervised lexical acquisition", "author": ["D Widdows", "B Dorow"], "venue": "Proceedings of the 19th International Conference on Computational Linguistics", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "Disentangling categorical relationships through a graph of co-occurrences", "author": ["J Martinez-Romo", "L Araujo", "J Borge-Holthoefer", "A Arenas", "JA Capitan", "JA Cuesta"], "venue": "Phys. Rev. E", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Identification of literary movements using complex networks to represent texts", "author": ["DR Amancio", "ON Oliveira Jr.", "LF Costa"], "venue": "New J. Phys", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Using metrics from complex networks to evaluate machine translation", "author": ["DR Amancio", "MGV Nunes", "ON Oliveira Jr.", "TAS Pardo", "L Antiqueira", "LF Costa"], "venue": "Physica A", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "A maximum entropy approach to natural language processing", "author": ["AL Berger", "VJ Della Pietra", "SA Della Pietra"], "venue": "Comput. Linguist", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1996}, {"title": "Some issues on complex networks for author characterization", "author": ["L Antiqueira", "TAS Pardo", "MGV Nunes", "Jr. Oliveira"], "venue": "ON", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Accessibility in complex networks", "author": ["B Travencolo", "LF Costa"], "venue": "Phys. Lett. A", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "Concentric characterization and classification of complex network nodes: application to an institutional collaboration network", "author": ["LF Costa", "MAR Tognetti", "FN Silva"], "venue": "Physica A", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Probing the topological properties of complex networks modeling short written texts", "author": ["DR Amancio"], "venue": "PLoS ONE", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Community detection in graphs", "author": ["S Fortunato"], "venue": "Phys. Rep", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Comparing intermittency and network measurements of words and their dependence on authorship", "author": ["DR Amancio", "EG Altmann", "ON Oliveira Jr.", "LF Costa"], "venue": "New J. Phys", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Mixing patterns in networks", "author": ["MEJ Newman"], "venue": "Phys. Rev. E", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2003}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["CD Manning", "H Schutze"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1999}, {"title": "Inference and Disputed Authorship: The Federalist", "author": ["F Mosteller", "D Wallace"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1964}, {"title": "The distance between zipf plots", "author": ["S Havlin"], "venue": "Physica A", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1995}, {"title": "Can analysis of word frequency distinguish between writings of different authors", "author": ["B Vilensky"], "venue": "Physica A", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1996}, {"title": "Information categorization approach to literary authorship disputes", "author": ["ACC Yang", "CK Peng", "WK Yien", "AL Goldberger"], "venue": "Physica A", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2003}, {"title": "Relative n-gram signatures: document visualization at the level of character n-grams", "author": ["M Jankowska", "V Keselij", "E Milios"], "venue": "IEEE Conference on Visual Analytics Science and Technology,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Burstiness and memory in complex systems", "author": ["KI Goh", "AL Barab\u00e1si"], "venue": "Europhys. Lett", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2008}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["IH Witten", "E Frank"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2005}, {"title": "Machine Learning. The Mc-Graw-Hill Companies, Inc. ISBN 0070428077", "author": ["TM Mitchell"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1997}, {"title": "Fuzzy support vector machines", "author": ["Lin C-F abd Wang S-D"], "venue": "IEEE Trans. Neural Netw. Learn. Syst", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2002}, {"title": "A complete fuzzy decision tree technique", "author": ["C Olaru", "L Wehenkel"], "venue": "Fuzzy Set. Syst", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2003}, {"title": "A fuzzy random forest", "author": ["P Bonissone", "JM Cadenas", "MC Garrido", "RA Daz-Valladares"], "venue": "Int. J. Approx. Reason", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "Multilayer perceptron, fuzzy sets, and classification", "author": ["SK Pal", "S Mitra"], "venue": "IEEE Trans. Neural Netw. Learn. Syst", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1992}, {"title": "Random search for hyper-parameter optimization", "author": ["J Bergstra", "Y Bengio"], "venue": "J. Machine Learning Research", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["J Bergstra", "R Bardenet", "Y Bengio", "B Kegl"], "venue": "Adv. Neural Inform. Process. Syst", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2011}, {"title": "A systematic comparison of supervised classifiers", "author": ["DR Amancio", "CH Comin", "D Casanova", "G Travieso", "OM Bruno", "FA Rodrigues", "LF Costa"], "venue": "PLoS ONE", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2014}, {"title": "Ant colony optimisation for stylometry: the federalist papers", "author": ["M Oakes"], "venue": "Proc. 5th Int. Conf. on Recent Advances in Soft Computing", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2004}, {"title": "A stylometric analysis of Yasar Kemal\u2019s", "author": ["JM Patton", "F Can"], "venue": "Ince Memed tetralogy. Computers and the Humanities", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2004}, {"title": "Discriminant Analysis and Statistical Pattern Recognition", "author": ["GJ McLachlan"], "venue": "Wiley Interscience", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "One of the most fundamental patterns arising from statistical analysis of huge amounts of text is the Zipf\u2019s law, which states that the frequency of the words decreases inversely to their rank [1\u20133].", "startOffset": 193, "endOffset": 198}, {"referenceID": 1, "context": "One of the most fundamental patterns arising from statistical analysis of huge amounts of text is the Zipf\u2019s law, which states that the frequency of the words decreases inversely to their rank [1\u20133].", "startOffset": 193, "endOffset": 198}, {"referenceID": 2, "context": "One of the most fundamental patterns arising from statistical analysis of huge amounts of text is the Zipf\u2019s law, which states that the frequency of the words decreases inversely to their rank [1\u20133].", "startOffset": 193, "endOffset": 198}, {"referenceID": 3, "context": "Other fundamental recurrent pattern is the Heap\u2019s law, which states that the vocabulary size grows slowly with the number of tokens of the document [4\u20136].", "startOffset": 148, "endOffset": 153}, {"referenceID": 4, "context": "Other fundamental recurrent pattern is the Heap\u2019s law, which states that the vocabulary size grows slowly with the number of tokens of the document [4\u20136].", "startOffset": 148, "endOffset": 153}, {"referenceID": 5, "context": "Other fundamental recurrent pattern is the Heap\u2019s law, which states that the vocabulary size grows slowly with the number of tokens of the document [4\u20136].", "startOffset": 148, "endOffset": 153}, {"referenceID": 6, "context": "compare the burstiness of the spatial disbribution of words in documents [7, 8].", "startOffset": 73, "endOffset": 79}, {"referenceID": 7, "context": "compare the burstiness of the spatial disbribution of words in documents [7, 8].", "startOffset": 73, "endOffset": 79}, {"referenceID": 8, "context": "In a similar fashion, the spatial distribution of words in documents and analogous systems have also been studied in terms of level statistics [9], entropy [10] and intermittency measurements [11] A well-known approach to study written texts is the word adjacency network model [12\u201314], which considers short-scale textual properties to form the networks.", "startOffset": 143, "endOffset": 146}, {"referenceID": 9, "context": "In a similar fashion, the spatial distribution of words in documents and analogous systems have also been studied in terms of level statistics [9], entropy [10] and intermittency measurements [11] A well-known approach to study written texts is the word adjacency network model [12\u201314], which considers short-scale textual properties to form the networks.", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "In a similar fashion, the spatial distribution of words in documents and analogous systems have also been studied in terms of level statistics [9], entropy [10] and intermittency measurements [11] A well-known approach to study written texts is the word adjacency network model [12\u201314], which considers short-scale textual properties to form the networks.", "startOffset": 192, "endOffset": 196}, {"referenceID": 11, "context": "In a similar fashion, the spatial distribution of words in documents and analogous systems have also been studied in terms of level statistics [9], entropy [10] and intermittency measurements [11] A well-known approach to study written texts is the word adjacency network model [12\u201314], which considers short-scale textual properties to form the networks.", "startOffset": 278, "endOffset": 285}, {"referenceID": 12, "context": "In a similar fashion, the spatial distribution of words in documents and analogous systems have also been studied in terms of level statistics [9], entropy [10] and intermittency measurements [11] A well-known approach to study written texts is the word adjacency network model [12\u201314], which considers short-scale textual properties to form the networks.", "startOffset": 278, "endOffset": 285}, {"referenceID": 13, "context": "In a similar fashion, the spatial distribution of words in documents and analogous systems have also been studied in terms of level statistics [9], entropy [10] and intermittency measurements [11] A well-known approach to study written texts is the word adjacency network model [12\u201314], which considers short-scale textual properties to form the networks.", "startOffset": 278, "endOffset": 285}, {"referenceID": 14, "context": "Using this model, several characteristics of texts and languages have been inferred from statistical analyses performed in the structure of networks [15].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "Language networks have been increasingly employed to understand theoretical linguistic aspects, such as the origins of fundamental properties [16] and the underlying mechanisms behind language acquisition in early years [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": "Language networks have been increasingly employed to understand theoretical linguistic aspects, such as the origins of fundamental properties [16] and the underlying mechanisms behind language acquisition in early years [17].", "startOffset": 220, "endOffset": 224}, {"referenceID": 17, "context": "In practical terms, networks have been applied in the context of machine translations [18], autommatic summarization [19], sense disambiguation [20,21], complexity/quality analysis [22,23] and document classification [24].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "In practical terms, networks have been applied in the context of machine translations [18], autommatic summarization [19], sense disambiguation [20,21], complexity/quality analysis [22,23] and document classification [24].", "startOffset": 144, "endOffset": 151}, {"referenceID": 19, "context": "In practical terms, networks have been applied in the context of machine translations [18], autommatic summarization [19], sense disambiguation [20,21], complexity/quality analysis [22,23] and document classification [24].", "startOffset": 144, "endOffset": 151}, {"referenceID": 20, "context": "In practical terms, networks have been applied in the context of machine translations [18], autommatic summarization [19], sense disambiguation [20,21], complexity/quality analysis [22,23] and document classification [24].", "startOffset": 181, "endOffset": 188}, {"referenceID": 21, "context": "In practical terms, networks have been applied in the context of machine translations [18], autommatic summarization [19], sense disambiguation [20,21], complexity/quality analysis [22,23] and document classification [24].", "startOffset": 181, "endOffset": 188}, {"referenceID": 22, "context": "In practical terms, networks have been applied in the context of machine translations [18], autommatic summarization [19], sense disambiguation [20,21], complexity/quality analysis [22,23] and document classification [24].", "startOffset": 217, "endOffset": 221}, {"referenceID": 23, "context": "Here I focus on the text classification based on stylistic features, where a text is classified according to stylistic marks left by specific authors [25] or literary genres [26].", "startOffset": 150, "endOffset": 154}, {"referenceID": 24, "context": "Here I focus on the text classification based on stylistic features, where a text is classified according to stylistic marks left by specific authors [25] or literary genres [26].", "startOffset": 174, "endOffset": 178}, {"referenceID": 25, "context": "Upon introducing a hybrid classifier relying on the fuzzy definition of supervised pattern recognition methods [27], I show that the performance of style-based classifications can be significantly improved when topological information is included in traditional models.", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "There are several ways to map texts into networks [12,15,19,28\u201330].", "startOffset": 50, "endOffset": 66}, {"referenceID": 14, "context": "There are several ways to map texts into networks [12,15,19,28\u201330].", "startOffset": 50, "endOffset": 66}, {"referenceID": 17, "context": "There are several ways to map texts into networks [12,15,19,28\u201330].", "startOffset": 50, "endOffset": 66}, {"referenceID": 26, "context": "There are several ways to map texts into networks [12,15,19,28\u201330].", "startOffset": 50, "endOffset": 66}, {"referenceID": 27, "context": "There are several ways to map texts into networks [12,15,19,28\u201330].", "startOffset": 50, "endOffset": 66}, {"referenceID": 18, "context": "In occasions where the semantics is relevant, the words sharing some semantical relationship are linked [20, 22, 31\u201333].", "startOffset": 104, "endOffset": 119}, {"referenceID": 20, "context": "In occasions where the semantics is relevant, the words sharing some semantical relationship are linked [20, 22, 31\u201333].", "startOffset": 104, "endOffset": 119}, {"referenceID": 28, "context": "In occasions where the semantics is relevant, the words sharing some semantical relationship are linked [20, 22, 31\u201333].", "startOffset": 104, "endOffset": 119}, {"referenceID": 29, "context": "In occasions where the semantics is relevant, the words sharing some semantical relationship are linked [20, 22, 31\u201333].", "startOffset": 104, "endOffset": 119}, {"referenceID": 30, "context": "In occasions where the semantics is relevant, the words sharing some semantical relationship are linked [20, 22, 31\u201333].", "startOffset": 104, "endOffset": 119}, {"referenceID": 31, "context": "In a similar fashion, other semantic-based models link the words appearing in a given context [34, 35] (e.", "startOffset": 94, "endOffset": 102}, {"referenceID": 32, "context": "In a similar fashion, other semantic-based models link the words appearing in a given context [34, 35] (e.", "startOffset": 94, "endOffset": 102}, {"referenceID": 33, "context": "A general model for establishing significant \u201csemantical\u201d links between co-occurring elements was devised in [36].", "startOffset": 109, "endOffset": 113}, {"referenceID": 17, "context": "In applications where the style (or structure) plays an important role, the links among words are established according to syntactical relationships [19,30].", "startOffset": 149, "endOffset": 156}, {"referenceID": 27, "context": "In applications where the style (or structure) plays an important role, the links among words are established according to syntactical relationships [19,30].", "startOffset": 149, "endOffset": 156}, {"referenceID": 12, "context": "A well-known approach for grasping stylistic features of texts is the word adjacency model [13,37,38], which basically connect adjacent words in the text.", "startOffset": 91, "endOffset": 101}, {"referenceID": 34, "context": "A well-known approach for grasping stylistic features of texts is the word adjacency model [13,37,38], which basically connect adjacent words in the text.", "startOffset": 91, "endOffset": 101}, {"referenceID": 35, "context": "A well-known approach for grasping stylistic features of texts is the word adjacency model [13,37,38], which basically connect adjacent words in the text.", "startOffset": 91, "endOffset": 101}, {"referenceID": 33, "context": "Differently from the model devised in [36], the word adjacency model captures the stylistic features of texts [39].", "startOffset": 38, "endOffset": 42}, {"referenceID": 27, "context": "Such high degree of similarity can be explained by the fact that most of the syntactical links occurs among neighbouring words [30].", "startOffset": 127, "endOffset": 131}, {"referenceID": 36, "context": "Particularly, in this study, I used a model based on maximum entropy [40].", "startOffset": 69, "endOffset": 73}, {"referenceID": 37, "context": "In text networks, both k and \u2206k have been useful to quantify the structural organization of texts [42].", "startOffset": 98, "endOffset": 102}, {"referenceID": 38, "context": "\u2022 Accessibility: the accessibility measurement is a extension of the node degree centrality [43].", "startOffset": 92, "endOffset": 96}, {"referenceID": 38, "context": "It is defined as a normalization of the diversity measurement [43], which quantifies the irregularity of a accessing neighbors through self-avoiding random walks [43].", "startOffset": 62, "endOffset": 66}, {"referenceID": 38, "context": "It is defined as a normalization of the diversity measurement [43], which quantifies the irregularity of a accessing neighbors through self-avoiding random walks [43].", "startOffset": 162, "endOffset": 166}, {"referenceID": 39, "context": "(5) It can be shown that the accessibility is bounded according to the relation 0 \u2264 \u03b1 \u2264 nh, where nh is the number of nodes at the h-th concentric level [44].", "startOffset": 153, "endOffset": 157}, {"referenceID": 39, "context": "In textual networks, it has been shown that the accessibility is more advantageous than other traditional centrality measurements as it is able to capture more information at further hierarchical levels [44].", "startOffset": 203, "endOffset": 207}, {"referenceID": 17, "context": "This measurement has been successfully applied to detect core concepts in texts [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "Furthermore, it has also been employed to generate informative summaries [19].", "startOffset": 73, "endOffset": 77}, {"referenceID": 40, "context": "A dependency of the distribution of this measurement with stylistic features of text was observed in [45].", "startOffset": 101, "endOffset": 105}, {"referenceID": 41, "context": "However, some words may act as articulation points whenever they link two semantical contexts or communities [46].", "startOffset": 109, "endOffset": 113}, {"referenceID": 42, "context": "the betweenness is able to identify the generality of contexts in which a word appears [47].", "startOffset": 87, "endOffset": 91}, {"referenceID": 43, "context": "This type of analysis is usually performed with the so-called assortativity measurement [48].", "startOffset": 88, "endOffset": 92}, {"referenceID": 42, "context": "Similarly to the betweenness, the clustering coefficient is useful to detect words appearing in generic contexts [47].", "startOffset": 113, "endOffset": 117}, {"referenceID": 42, "context": "This measurement was used in this paper because it has been useful in stylistic-based applications [47].", "startOffset": 99, "endOffset": 103}, {"referenceID": 44, "context": "Traditional methods usually perform statistical analysis using specific textual features [49].", "startOffset": 89, "endOffset": 93}, {"referenceID": 45, "context": "An important contribution to the stylometry was introduced by Mosteller and Wallace [50], which showed that the frequency of function words (such as any, of, a and on) is useful to characterize the style in texts.", "startOffset": 84, "endOffset": 88}, {"referenceID": 46, "context": "Frequent words have also been used in strategies devised by physicists where the distance of frequency ranks was used to compute the similarity between texts [51\u201353].", "startOffset": 158, "endOffset": 165}, {"referenceID": 47, "context": "Frequent words have also been used in strategies devised by physicists where the distance of frequency ranks was used to compute the similarity between texts [51\u201353].", "startOffset": 158, "endOffset": 165}, {"referenceID": 48, "context": "Frequent words have also been used in strategies devised by physicists where the distance of frequency ranks was used to compute the similarity between texts [51\u201353].", "startOffset": 158, "endOffset": 165}, {"referenceID": 49, "context": "to detect the stylistic marks of specific authors [54].", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "The uneven spatial distribution of words along texts is a feature useful for characterizing the style of texts [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 50, "context": "The quantification of the homogeneity of the distribution of words along texts can be performed by using recurrence times, a standard measure employed to study time series [55].", "startOffset": 172, "endOffset": 176}, {"referenceID": 40, "context": "Stopwords, on the other hand, are evenly distributed along the text [45].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "are the most relevant words [10], in the example of figure 3, Hobson would be considered a relevant character in the plot.", "startOffset": 28, "endOffset": 32}, {"referenceID": 51, "context": "The technique used to evaluate the performance of the classifiers used in this study is the well-known 10-fold crossvalidation [56].", "startOffset": 127, "endOffset": 131}, {"referenceID": 51, "context": "To quantify the relevance of attributes to discriminate the data, several indices have been proposed [56].", "startOffset": 101, "endOffset": 105}, {"referenceID": 52, "context": "A well-known index, the information gain, quantifies the homogeneity of the set of instances in Xtr when the value of the attribute is specified [57].", "startOffset": 145, "endOffset": 149}, {"referenceID": 25, "context": "A detailed description of methods for computing the membership strength can be found in [27, 58\u201361].", "startOffset": 88, "endOffset": 99}, {"referenceID": 53, "context": "A detailed description of methods for computing the membership strength can be found in [27, 58\u201361].", "startOffset": 88, "endOffset": 99}, {"referenceID": 54, "context": "A detailed description of methods for computing the membership strength can be found in [27, 58\u201361].", "startOffset": 88, "endOffset": 99}, {"referenceID": 55, "context": "A detailed description of methods for computing the membership strength can be found in [27, 58\u201361].", "startOffset": 88, "endOffset": 99}, {"referenceID": 56, "context": "A detailed description of methods for computing the membership strength can be found in [27, 58\u201361].", "startOffset": 88, "endOffset": 99}, {"referenceID": 0, "context": "m (H) ij = \u03bbm (R) ij + (1\u2212 \u03bb)m (T ) ij , (14) where \u03bb \u2208 [0, 1] accounts for the weight associated to the topological strategy.", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "Note that the combination of evidences performed in equation 14 yields a membership strength m (H) ij ranging in the interval [0, 1].", "startOffset": 126, "endOffset": 132}, {"referenceID": 57, "context": "In this case, the process of finding the adequate value of the parameter can be made via application of optimization heuristics [62,63].", "startOffset": 128, "endOffset": 135}, {"referenceID": 58, "context": "In this case, the process of finding the adequate value of the parameter can be made via application of optimization heuristics [62,63].", "startOffset": 128, "endOffset": 135}, {"referenceID": 59, "context": "These classifiers were chosen because they usually yield good accuracy rates with default parameters [64].", "startOffset": 101, "endOffset": 105}, {"referenceID": 60, "context": "In the authorship attribution task, the objective is to identify the authors of texts whose identity is lacking [65].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Figure 6 also shows the maximum accuracy rate obtained with the variation of \u03bb: \u2206\u0393max = max \u03bb\u2208[0,1] \u2206\u0393(\u03bb).", "startOffset": 94, "endOffset": 99}, {"referenceID": 61, "context": "In this section, I investigate if the topological features of complex networks are useful to complement the traditional textual characterization for the style identification task [66].", "startOffset": 179, "endOffset": 183}, {"referenceID": 62, "context": "The linear discriminant analysis [68] was employed to generate the visualization.", "startOffset": 33, "endOffset": 37}], "year": 2015, "abstractText": "Statistical methods have been widely employed to study the fundamental properties of language. In recent years, methods from complex and dynamical systems proved useful to create several language models. Despite the large amount of studies devoted to represent texts with physical models, only a limited number of studies have shown how the properties of the underlying physical systems can be employed to improve the performance of natural language processing tasks. In this paper, I address this problem by devising complex networks methods that are able to improve the performance of current statistical methods. Using a fuzzy classification strategy, I show that the topological properties extracted from texts complement the traditional textual description. In several cases, the performance obtained with hybrid approaches outperformed the results obtained when only traditional or networked methods were used. Because the proposed model is generic, the framework devised here could be straightforwardly used to study similar textual applications where the topology plays a pivotal role in the description of the interacting agents. PACS numbers: 89.75.Hc,02.40.Pc,02.50.-r", "creator": "LaTeX with hyperref package"}}}