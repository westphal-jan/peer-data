{"id": "1507.01127", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2015", "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes", "abstract": "We addition \\ textit {AutoExtend }, a computer put supposed planar for synsets taken csfs. It still useful in that the can opportunity any comes satsang as input two neither instead certain an total facility corpus. The synset / long-snouted speleothems document live in however same vector visible considered the note culturas. A surprisingly quantization tri-partite payment efficiency addition parallelizability. We use WordNet called just linguistic resource, well AutoExtend else cannot once applied come certain depend like Freebase. AutoExtend unmatched city -, - similar - sculpture performance leaving word similarity and meaning perspective swahr responsibilities.", "histories": [["v1", "Sat, 4 Jul 2015 16:59:30 GMT  (369kb,D)", "http://arxiv.org/abs/1507.01127v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sascha rothe", "hinrich sch\u00fctze"], "accepted": true, "id": "1507.01127"}, "pdf": {"name": "1507.01127.pdf", "metadata": {"source": "CRF", "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes", "authors": ["Sascha Rothe"], "emails": ["sascha@cis.lmu.de"], "sections": [{"heading": "1 Introduction", "text": "Unsupervised methods for word embeddings (also called \u201cdistributed word representations\u201d) have become popular in natural language processing (NLP). These methods only need very large corpora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space. Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some context. A lexeme pairs a particular spelling or pro-\nnunciation with a particular meaning, i.e., a lexeme is a conjunction of a word and a synset. Our premise is that many NLP applications will benefit if the non-word data types of resources \u2013 e.g., synsets in WordNet \u2013 are also available as embeddings. For example, in machine translation, enriching and improving translation dictionaries (cf. Mikolov et al. (2013b)) would benefit from these embeddings because they would enable us to create an enriched dictionary for word senses. Generally, our premise is that the arguments for the utility of embeddings for word forms should carry over to the utility of embeddings for other data types like synsets in WordNet.\nThe insight underlying the method we propose is that the constraints of a resource can be formalized as constraints on embeddings and then allow us to extend word embeddings to embeddings of other data types like synsets. For example, the hyponymy relation in WordNet can be formalized as such a constraint.\nThe advantage of our approach is that it decouples embedding learning from the extension of embeddings to non-word data types in a resource. If somebody comes up with a better way of learning embeddings, these embeddings become immediately usable for resources. And we do not rely on any specific properties of embeddings that make them usable in some resources, but not in others.\nAn alternative to our approach is to train embeddings on annotated text, e.g., to train synset embeddings on corpora annotated with synsets. However, successful embedding learning generally requires very large corpora and sense labeling is too expensive to produce corpora of such a size.\nAnother alternative to our approach is to add up all word embedding vectors related to a particular node in a resource; e.g., to create the synset vector of lawsuit in WordNet, we can add the word vectors of the three words that are part of the synset (lawsuit, suit, case). We will call this approach\nar X\niv :1\n50 7.\n01 12\n7v 1\n[ cs\n.C L\n] 4\nJ ul\n2 01\n5\nnaive and use it as a baseline (Snaive in Table 3).\nWe will focus on WordNet (Fellbaum, 1998) in this paper, but our method \u2013 based on a formalization that exploits the constraints of a resource for extending embeddings from words to other data types \u2013 is broadly applicable to other resources including Wikipedia and Freebase.\nA word in WordNet can be viewed as a composition of several lexemes. Lexemes from different words together can form a synset. When a synset is given, it can be decomposed into its lexemes. And these lexemes then join to form words. These observations are the basis for the formalization of the constraints encoded in WordNet that will be presented in the next section: we view words as the sum of their lexemes and, analogously, synsets as the sum of their lexemes.\nAnother motivation for our formalization stems from the analogy calculus developed by Mikolov et al. (2013a), which can be viewed as a group theory formalization of word relations: we have a set of elements (our vectors) and an operation (addition) satisfying the properties of a mathematical group, in particular, associativity and invertibility. For example, you can take the vector of king, subtract the vector of man and add the vector of woman to get a vector near queen. In other words, you remove the properties of man and add the properties of woman. We can also see the vector of king as the sum of the vector of man and the vector of a gender-neutral ruler. The next thing to notice is that this does not only work for words that combine several properties, but also for words that combine several senses. The vector of suit can be seen as the sum of a vector representing lawsuit and a vector representing business suit. AutoExtend is designed to take word vectors as input and unravel the word vectors to the vectors of their lexemes. The lexeme vectors will then give us the synset vectors.\nThe main contributions of this paper are: (i) We present AutoExtend, a flexible method that extends word embeddings to embeddings of synsets and lexemes. AutoExtend is completely general in that it can be used for any set of embeddings and for any resource that imposes constraints of a certain type on the relationship between words and other data types. (ii) We show that AutoExtend achieves state-of-the-art word similarity and word sense disambiguation (WSD) performance. (iii) We publish the AutoExtend code for extending\nword embeddings to other data types, the lexeme and synset embeddings and the software to replicate our WSD evaluation.\nThis paper is structured as follows. Section 2 introduces the model, first as a general tensor formulation then as a matrix formulation making additional assumptions. In Section 3, we describe data, experiments and evaluation. We analyze AutoExtend in Section 4 and give a short summary on how to extend our method to other resources in Section 5. Section 6 discusses related work."}, {"heading": "2 Model", "text": "We are looking for a model that extends standard embeddings for words to embeddings for the other two data types in WordNet: synsets and lexemes. We want all three data types \u2013 words, lexemes, synsets \u2013 to live in the same embedding space.\nThe basic premise of our model is: (i) words are sums of their lexemes and (ii) synsets are sums of their lexemes. We refer to these two premises as synset constraints. For example, the embedding of the word bloom is a sum of the embeddings of its two lexemes bloom(organ) and bloom(period); and the embedding of the synset flower-bloomblossom(organ) is a sum of the embeddings of its three lexemes flower(organ), bloom(organ) and blossom(organ).\nThe synset constraints can be argued to be the simplest possible relationship between the three WordNet data types. They can also be motivated by the way many embeddings are learned from corpora \u2013 for example, the counts in vector space models are additive, supporting the view of words as the sum of their senses. The same assumption is frequently made; for example, it underlies the group theory formalization of analogy discussed in Section 1.\nWe denote word vectors as w(i) \u2208 Rn, synset vectors as s(j) \u2208 Rn, and lexeme vectors as l(i,j) \u2208 Rn. l(i,j) is that lexeme of wordw(i) that is a member of synset s(j). We set lexeme vectors l(i,j) that do not exist to zero. For example, the non-existing lexeme flower(truck) is set to zero. We can then formalize our premise that the two constraints (i) and (ii) hold as follows:\nw(i) = \u2211 j l(i,j) (1)\ns(j) = \u2211 i l(i,j) (2)\nThese two equations are underspecified. We therefore introduce the matrix E(i,j) \u2208 Rn\u00d7n:\nl(i,j) = E(i,j)w(i) (3)\nWe make the assumption that the dimensions in Eq. 3 are independent of each other, i.e., E(i,j) is a diagonal matrix. Our motivation for this assumption is: (i) This makes the computation technically feasible by significantly reducing the number of parameters and by supporting parallelism. (ii) Treating word embeddings on a per-dimension basis is a frequent design choice (e.g., Kalchbrenner et al. (2014)). Note that we allow E(i,j) < 0 and in general the distribution weights for each dimension (diagonal entries of E(i,j)) will be different. Our assumption can be interpreted as word w(i) distributing its embedding activations to its lexemes on each dimension separately. Therefore, Eqs. 1-2 can be written as follows:\nw(i) = \u2211 j E(i,j)w(i) (4)\ns(j) = \u2211 i E(i,j)w(i) (5)\nNote that from Eq. 4 it directly follows that:\u2211 j E(i,j) = In \u2200i (6)\nwith In being the identity matrix. Let W be a |W | \u00d7 n matrix where n is the dimensionality of the embedding space, |W | is the number of words and each row w(i) is a word embedding; and let S be a |S|\u00d7nmatrix where |S| is the number of synsets and each row s(j) is a synset embedding. W and S can be interpreted as linear maps and a mapping between them is given by the rank 4 tensor E \u2208 R|S|\u00d7n\u00d7|W |\u00d7n. We can then write Eq. 5 as a tensor product:\nS = E\u2297W (7)\nwhile Eq. 6 states, that\u2211 j Ei,d1j,d2 = 1 \u2200i, d1, d2 (8)\nAdditionally, there is no interaction between different dimensions, so Ei,d1j,d2 = 0 if d1 6= d2. In other words, we are creating the tensor by stacking the diagonal matrices E(i,j) over i and j. Another sparsity arises from the fact that many lexemes do\nnot exist: Ei,d1j,d2 = 0 if l (i,j) = 0; i.e., l(i,j) 6= 0 only if word i has a lexeme that is a member of synset j. To summarize the sparsity:\nEi,d1j,d2 = 0\u21d0 d1 6= d2 \u2228 l (i,j) = 0 (9)"}, {"heading": "2.1 Learning", "text": "We adopt an autoencoding framework to learn embeddings for lexemes and synsets. To this end, we view the tensor equation S = E \u2297W as the encoding part of the autoencoder: the synsets are the encoding of the words. We define a corresponding decoding part that decodes the synsets into words as follows:\ns(j) = \u2211 i l (i,j) , w(i) = \u2211 j l (i,j)\n(10)\nIn analogy toE(i,j), we introduce the diagonal matrix D(j,i):\nl (i,j) = D(j,i)s(j) (11)\nIn this case, it is the synset that distributes itself to its lexemes. We can then rewrite Eq. 10 to:\ns(j) = \u2211 i D(j,i)s(j), w(i) = \u2211 j D(j,i)s(j) (12)\nand we also get the equivalent of Eq. 6 for D(j,i):\u2211 i D(j,i) = In \u2200j (13)\nand in tensor notation:\nW = D\u2297 S (14)\nNormalization and sparseness properties for the decoding part are analogous to the encoding part:\u2211\ni\nDj,d2i,d1 = 1 \u2200j, d1, d2 (15)\nDj,d2i,d1 = 0\u21d0 d1 6= d2 \u2228 l (i,j) = 0 (16)\nWe can state the learning objective of the autoencoder as follows:\nargmin E,D\n\u2016D\u2297E\u2297W \u2212W\u2016 (17)\nunder the conditions Eq. 8, 9, 15 and 16. Now we have an autoencoder where input and output layers are the word embeddings and the hidden layer represents the synset vectors. A simplified version is shown in Figure 1. The tensors E\nand D have to be learned. They are rank 4 tensors of size\u22481015. However, we already discussed that they are very sparse, for two reasons: (i) We make the assumption that there is no interaction between dimensions. (ii) There are only few interactions between words and synsets (only when a lexeme exists). In practice, there are only \u2248107 elements to learn, which is technically feasible."}, {"heading": "2.2 Matrix formalization", "text": "Based on the assumption that each dimension is fully independent from other dimensions, a separate autoencoder for each dimension can be created and trained in parallel. Let W \u2208 R|W |\u00d7n be a matrix where each row is a word embedding and w(d) = W\u00b7,d the d-th column of W , i.e., a vector that holds the d-th dimension of each word vector. In the same way, s(d) = S\u00b7,d holds the d-th dimension of each synset vector and E(d) = E\u00b7,d\u00b7,d \u2208 R|S|\u00d7|W |. We can write S = E\u2297W as:\ns(d) = E(d)w(d) \u2200d (18)\nwithE(d)i,j = 0 if l (i,j) = 0. The decoding equation W = D\u2297 S takes this form:\nw(d) = D(d)s(d) \u2200d (19)\nwhere D(d) = D\u00b7,d\u00b7,d \u2208 R |W |\u00d7|S| and D(d)j,i = 0 if l(i,j) = 0. So E and D are symmetric in terms of non-zero elements. The learning objective becomes:\nargmin E(d),D(d)\n\u2016D(d)E(d)w(d) \u2212 w(d)\u2016 \u2200d (20)"}, {"heading": "2.3 Lexeme embeddings", "text": "The hidden layer S of the autoencoder gives us synset embeddings. The lexeme embeddings are defined when transitioning from W to S, or more explicitly by:\nl(i,j) = E(i,j)w(i) (21)\nHowever, there is also a second lexeme embedding in AutoExtend when transitioning form S to W :\nl (i,j) = D(j,i)s(j) (22)\nAligning these two representations seems natural, so we impose the following lexeme constraints:\nargmin E(i,j),D(j,i)\n\u2225\u2225\u2225E(i,j)w(i) \u2212D(j,i)s(j)\u2225\u2225\u2225 \u2200i, j (23)\nThis can also be expressed dimension-wise. The matrix formulation is given by:\nargmin E(d),D(d) \u2225\u2225\u2225E(d) diag(w(d))\u2212 (D(d) diag(s(d)))T\u2225\u2225\u2225\u2200d (24) with diag(x) being a square matrix having x on the main diagonal and vector s(d) defined by Eq. 18. While we try to align the embeddings, there are still two different lexeme embeddings. In all experiments reported in Section 4 we will use the average of both embeddings and in Section 4 we will analyze the weighting in more detail."}, {"heading": "2.4 WN relations", "text": "Some WordNet synsets contain only a single word (lexeme). The autoencoder learns based on the synset constraints, i.e., lexemes being shared by different synsets (and also words); thus, it is difficult to learn good embeddings for single-lexeme synsets. To remedy this problem, we impose the constraint that synsets related by WordNet (WN) relations should have similar embeddings. Table 1 shows relations we used. WN relations are entered in a new matrixR \u2208 Rr\u00d7|S|, where r is the number of WN relation tuples. For each relation tuple, i.e., row in R, we set the columns corresponding to the first and second synset to 1 and \u22121, respectively. The values of R are not updated during training. We use a squared error function and 0 as target value. This forces the system to find similar values for related synsets. Formally, the WN relation constraints are:\nargmin E(d)\n\u2016RE(d)w(d)\u2016 \u2200d (25)"}, {"heading": "2.5 Implementation", "text": "Our training objective is minimization of the sum of synset constraints (Eq. 20), weighted by \u03b1, the lexeme constraints (Eq. 24), weighted by \u03b2, and the WN relation constraints (Eq. 25), weighted by 1\u2212 \u03b1\u2212 \u03b2.\nThe training objective cannot be solved analytically because it is subject to constraints Eq. 8,\nEq. 9, Eq. 15 and Eq. 16. We therefore use backpropagation. We do not use regularization since we found that all learned weights are in [\u22122, 2].\nAutoExtend is implemented in MATLAB. We run 1000 iterations of gradient descent. On an Intel Xeon CPU E7-8857 v2 3.00GHz, one iteration on one dimension takes less than a minute because the gradient computation ignores zero entries in the matrix."}, {"heading": "2.6 Column normalization", "text": "Our model is based on the premise that a word is the sum of its lexemes (Eq. 1). From the definition of E(i,j), we derived that E \u2208 R|S|\u00d7n\u00d7|W |\u00d7n is normalized over the first dimension (Eq. 8). So E(d) \u2208 R|S|\u00d7|W | is also normalized over the first dimension. In other words, E(d) is a column normalized matrix. Another premise of the model is that a synset is the sum of its lexemes. Therefore, D(d) is also column normalized. A simple way to implement this is to start the computation with column normalized matrices and normalize them again after each iteration as long as the error function still decreases. When the error function starts increasing, we stop normalizing the matrices and continue with a normal gradient descent. This respects that while E(d) and D(d) should be column normalized in theory, there are a lot of practical issues that prevent this, e.g., OOV words."}, {"heading": "3 Data, experiments and evaluation", "text": "We downloaded 300-dimensional embeddings for 3,000,000 words and phrases trained on Google News, a corpus of \u22481011 tokens, using word2vec CBOW (Mikolov et al., 2013c). Many words in the word2vec vocabulary are not in WordNet, e.g., inflected forms (cars) and proper nouns (Tony Blair). Conversely, many WordNet lemmas are not in the word2vec vocabulary, e.g., 42 (digits were converted to 0). This results in a number of empty synsets (see Table 2). Note however that AutoExtend can produce embeddings for empty synsets because we use WN relation constraints in addition to synset and lexeme constraints.\nWe run AutoExtend on the word2vec vectors. As we do not know anything about a suitable weighting for the three different constraints, we set \u03b1 = \u03b2 = 0.33. Our main goal is to produce compatible embeddings for lexemes and synsets. Thus, we can compute nearest neighbors across all three data types as shown in Figure 2.\nWe evaluate the embeddings on WSD and on similarity performance. Our results depend directly on the quality of the underlying word embeddings, in our case word2vec embeddings. We would expect even better evaluation results as word representation learning methods improve. Using a new and improved set of underlying embeddings is simple: it is a simple switch of the input file that contains the word embeddings."}, {"heading": "3.1 Word Sense Disambiguation", "text": "For WSD we use the shared tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004) and a system named IMS (Zhong and Ng, 2010). Senseval-2 contains 139, Senseval-3\n57 different words. They provide 8,611, respectively 8,022 training instances and 4,328, respectively 3,944 test instances. For the system, we use the same setting as in the original paper. Preprocessing consists of sentence splitting, tokenization, POS tagging and lemmatization; the classifier is a linear SVM. In our experiments (Table 3), we run IMS with each feature set by itself to assess the relative strengths of feature sets (lines 1\u2013 7) and on feature set combinations to determine which combination is best for WSD (lines 8, 12\u2013 15).\nIMS implements three standard WSD feature sets: part of speech (POS), surrounding word and local collocation (lines 1\u20133).\nLetw be an ambiguous word with k senses. The three feature sets on lines 5\u20137 are based on the AutoExtend embeddings s(j), 1 \u2264 j \u2264 k, of the synsets of w and the centroid c of the sentence in which w occurs. The centroid is simply the sum of all word2vec vectors of the words in the sentence, excluding stop words.\nThe S-cosine feature set consists of the k cosines of centroid and synset vectors:\n< cos(c, s(1)), cos(c, s(2)), . . . , cos(c, s(k)) >\nThe S-product feature set consists of the nk element-wise products of centroid and synset vectors:\n< c1s (1) 1 , . . . , cns (1) n , . . . , c1s (k) 1 , . . . , cns (k) n >\nwhere ci (resp. s (j) i ) is element i of c (resp. s (j)). The idea is that we let the SVM estimate how important each dimension is for WSD instead of giving all equal weight as in S-cosine.\nThe S-raw feature set simply consists of the n(k + 1) elements of centroid and synset vectors:\n< c1, . . . , cn, s (1) 1 , . . . , s (1) n , . . . , s (k) 1 , . . . , s (k) n >\nOur main goal is to determine if AutoExtend features improve WSD performance when added to standard WSD features. To make sure that improvements we get are not solely due to the power of word2vec, we also investigate a simple word2vec baseline. For S-product, the AutoExtend feature set that performs best in the experiment (cf. lines 6 and 14), we test the alternative word2vec-based Snaive-product feature set. It has the same definition as S-product except that we replace the synset vectors s(j) with naive synset vectors z(j), defined as the sum of the word2vec vectors of the words that are members of synset j.\nLines 1\u20137 in Table 3 show the performance of each feature set by itself. We see that the synset feature sets (lines 5\u20137) have a comparable performance to standard feature sets. S-product is the strongest of them.\nLines 8\u201316 show the performance of different feature set combinations. MFS (line 8) is the most frequent sense baseline. Lines 9&10 are the winners of Senseval. The standard configuration of IMS (line 11) uses the three feature sets on lines 1\u20133 (POS, surrounding word, local collocation) and achieves an accuracy of 65.2% on the English lexical sample task of Senseval-2 and 72.3% on Senseval-3.1 Lines 12\u201316 add one additional feature set to the IMS system on line 11; e.g., the system on line 14 uses POS, surrounding word, local collocation and S-product feature sets. The system on line 14 outperforms all previous systems, most of them significantly. While S-raw performs quite reasonably as a feature set alone, it hurts the performance when used as an additional feature set. As this is the feature set that contains the largest number of features (n(k + 1)), overfitting is the likely reason. Conversely, S-cosine only adds k features and therefore may suffer from underfitting.\u2020\nWe do a grid search (step size .1) for optimal values of \u03b1 and \u03b2, optimizing the average score of Senseval-2 and Senseval-3. The best performing feature set combination is Soptimized-product with\n1Zhong and Ng (2010) report accuracies of 65.3% / 72.6% for this configuration.\n\u2020In Table 3 and Table 4, results significantly worse than the best (bold) result in each column are marked \u2020 for \u03b1 = .05 and \u2021 for \u03b1 = .10 (one-tailed Z-test).\n\u03b1 = 0.2 and \u03b2 = 0.5, with only a small improvement (line 16).\nThe main result of this experiment is that we achieve an improvement of more than 1% in WSD performance when using AutoExtend."}, {"heading": "3.2 Synset and lexeme similarity", "text": "We use SCWS (Huang et al., 2012) for the similarity evaluation. SCWS provides not only isolated words and corresponding similarity scores, but also a context for each word. SCWS is based on WordNet, but the information as to which synset a word in context came from is not available. However, the dataset is the closest we could find for sense similarity. Synset and lexeme embeddings are obtained by running AutoExtend. Based on the results of the WSD task, we set \u03b1 = 0.2 and \u03b2 = 0.5. Lexeme embeddings are the natural choice for this task as human subjects are provided with two words and a context for each and then have to assign a similarity score. But for completeness, we also run experiments for synsets.\nFor each word, we compute a context vector c by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l(ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vectors weighted by cosine similarity to c (method AvgSimC).\nTable 4 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work,\nincluding (Huang et al., 2012) and (Tian et al., 2014). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably because using a representation that is specific to the actual word being judged is more precise than using a representation that also includes synonyms.\nA simple baseline is to use the underlying word2vec embeddings directly (line 5). In this case, there is only one embedding, so there is no difference between AvgSim and AvgSimC. It is interesting that even if we do not take the context into account (method AvgSim) the lexeme embeddings outperform the original word embeddings. As AvgSim simply adds up all lexemes of a word, this is equivalent to the constraint we proposed in the beginning of the paper (Eq. 1). Thus, replacing a word\u2019s embedding by the sum of the embeddings of its senses could generally improve the quality of embeddings (cf. Huang et al. (2012) for a similar point). We will leave a deeper evaluation of this topic for future work."}, {"heading": "4 Analysis", "text": "We first look at the impact of the parameters \u03b1, \u03b2 (Section 2.5) that control the weighting of synset constraints vs lexeme constraints vs WN relation constraints. We investigate the impact for three different tasks. WSD-alone: accuracy of IMS (average of Senseval-2 and Senseval-3) if only Sproduct is used as a feature set (line 6 in Table 3). WSD-additional: accuracy of IMS (average of Senseval-2 and Senseval-3) if S-product is used together with the feature sets POS, surrounding word and local collocation (line 14 in Table 3). SCWS: Spearman correlation on SCWS (line 7 in Table 4).\nFor WSD-alone (Figure 3, center), the best performing weightings (red) all have high weights for WN relations and are therefore at the top of triangle. Thus, WN relations are very important for WSD-alone and adding more weight to the\nsynset and lexeme constraints does not help. However, all three constraints are important in WSDadditional: the red area is in the middle (corresponding to nonzero weights for all three constraints) in the left panel of Figure 3. Apparently, strongly weighted lexeme and synset constraints enable learning of representations that in their interaction with standard WSD feature sets like local collocation increase WSD performance. For SCWS (right panel), we should not put too much weight on WN relations as they artificially bring related, but not similar lexemes together. So the maximum for this task is located in the lower part of the triangle.\nThe main result of this analysis is that AutoExtend never achieves its maximum performance when using only one set of constraints. All three constraints are important \u2013 synset, lexeme and WN relation constraints \u2013 with different weights for different applications.\nWe also analyzed the impact of the four different WN relations (see Table 1) on performance. In Table 3 and Table 4, all four WN relations are used together. We found that any combination of three relation types performs worse than using all four together. A comparison of different relations must be done carefully as they differ in the POS they affect and in quantity (see Table 1). In general, relation types with more relations outperformed relation types with fewer relations.\nFinally, the relative weighting of l(i,j) and l (i,j) when computing lexeme embeddings is also a parameter that can be tuned. We use simple averaging (\u03b8 = 0.5) for all experiments reported in this paper. We found only small changes in performance for 0.2 \u2264 \u03b8 \u2264 0.8."}, {"heading": "5 Resources other than WordNet", "text": "AutoExtend is broadly applicable to lexical and knowledge resources that have certain properties. While we only run experiments with WordNet in this paper, we will briefly address other resources. For Freebase (Bollacker et al., 2008), we could replace the synsets with Freebase entities. Each entity has several aliases, e.g. Barack Obama, President Obama, Obama. The role of words in WordNet would correspond to these aliases in Freebase. This will give us the synset constraint, as well as the lexeme constraint of the system. Relations are given by Freebase types; e.g., we can add a constraint that entity embeddings of the type \u201dPresi-\ndent of the US\u201d should be similar. To explorer multilingual word embeddings we require the word embeddings of different languages to live in the same vector space, which can easily be achieved by training a transformation matrix L between two languages using known translations (Mikolov et al., 2013b). Let X be a matrix where each row is a word embedding in language 1 and Y a matrix where each row is a word embedding in language 2. For each row the words of X and Y are a translation of each other. We then want to minimize the following objective:\nargmin L \u2016LX \u2212 Y \u2016 (26)\nWe can use a gradient descent to solve this but a matrix inversion will run faster. The matrix L is given by:\nL = (XT \u2217X)\u22121(XT \u2217 Y ) (27)\nThe matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to compute synset embeddings. We can add cross-linguistic relationships to our model, e.g., aligning German and English synset embeddings of the same concept."}, {"heading": "6 Related Work", "text": "Rumelhart et al. (1988) introduced distributed word representations, usually called word embeddings today. There has been a resurgence of work on them recently (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend.\nThere are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Schu\u0308tze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was\nproposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings.\nThis prior work needs a training step to learn embeddings. In contrast, we can \u201cAutoExtend\u201d any set of given word embeddings \u2013 without (re)training them.\nThere is only little work on taking existing word embeddings and producing embeddings in the same space. Labutov and Lipson (2013) tuned existing word embeddings in supervised training, not to create new embeddings for senses or entities, but to get better predictive performance on a task while not changing the space of embeddings.\nLexical resources have also been used to improve word embeddings. In the Relation Constrained Model, Yu and Dredze (2014) use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings.\nAnother interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014). The downside of this approach is that parallel data is needed.\nWe used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al., 2014).\nAnd while we use cosine to compute similarity between synsets, there are also a lot of similarity measures that only rely on a given resource, mostly WordNet. These measures are often functions that depend on the provided information like gloss or the topology like shortest-path. Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998); Blanchard et al. (2005) give a good overview."}, {"heading": "7 Conclusion", "text": "We presented AutoExtend, a flexible method to learn synset and lexeme embeddings from word embeddings. It is completely general and can be used for any other set of embeddings and for any other resource that imposes constraints of a certain type on the relationship between words and other data types. Our experimental results show that AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation. Along with this paper, we will publish AutoExtend for extending word embeddings to other data types; the lexeme and synset embeddings used in the experiments; and the code needed to replicate our WSD evaluation2."}, {"heading": "Acknowledgments", "text": "This work was partially funded by Deutsche Forschungsgemeinschaft (DFG SCHU 2246/2-2). We are grateful to Christiane Fellbaum for discussions leading up to this paper and to the anonymous reviewers for their comments.\n2http://cistern.cis.lmu.de/"}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Knowledge-powered deep learning for word embedding", "author": ["Bian et al.2014] Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Proceedings of ECML PKDD", "citeRegEx": "Bian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "A typology of ontology-based semantic measures", "author": ["Mounira Harzallah", "Henri Briand", "Pascale Kuntz"], "venue": "In Proceedings of EMOI - INTEROP", "citeRegEx": "Blanchard et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2005}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of ACM SIGMOD", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of AAAI", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for opentext semantic parsing", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Proceedings of AISTATS", "citeRegEx": "Bordes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "WordNet: An Electronic Lexical Database. Bradford Books", "author": ["Christiane Fellbaum"], "venue": null, "citeRegEx": "Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "Proceedings of WWW", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Learning sense-specific word embeddings by exploiting bilingual resources", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In Proceedings of Coling,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Using the structure of a conceptual network in computing semantic relatedness", "author": ["Iryna Gurevych"], "venue": "In IJCNLP", "citeRegEx": "Gurevych.,? \\Q2005\\E", "shortCiteRegEx": "Gurevych.", "year": 2005}, {"title": "Germanet-a lexical-semantic net for german", "author": ["Hamp et al.1997] Birgit Hamp", "Helmut Feldweg"], "venue": "In Proceedings of ACL,", "citeRegEx": "Hamp and Feldweg,? \\Q1997\\E", "shortCiteRegEx": "Hamp and Feldweg", "year": 1997}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "arXiv preprint arXiv:1408.3456", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of ACL", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of ACL", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "English lexical sample task description", "author": ["Adam Kilgarriff"], "venue": "In Proceedings of SENSEVAL-2", "citeRegEx": "Kilgarriff.,? \\Q2001\\E", "shortCiteRegEx": "Kilgarriff.", "year": 2001}, {"title": "Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265\u2013283", "author": ["Leacock", "Chodorow1998] Claudia Leacock", "Martin Chodorow"], "venue": null, "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "The senseval-3 english lexical sample task", "author": ["Timothy Chklovski", "Adam Kilgarriff"], "venue": "In Proceedings of SENSEVAL-3", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Proceedings of NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["Miller", "Charles1991] George A Miller", "Walter G Charles"], "venue": "Language and Cognitive Processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of ICML", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Proceedings of NIPS", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Babelnet: Building a very large multilingual semantic network", "author": ["Navigli", "Ponzetto2010] Roberto Navigli", "Simone Paolo Ponzetto"], "venue": "In Proceedings of ACL", "citeRegEx": "Navigli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2010}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "Proceedings of EMNLP", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multi-prototype vectorspace models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J Mooney"], "venue": "In Proceedings of NAACL", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Learning representations by back-propagating errors", "author": ["Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive Modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Automatic word sense discrimination", "author": ["Hinrich Sch\u00fctze"], "venue": "Computational Linguistics,", "citeRegEx": "Sch\u00fctze.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00fctze.", "year": 1998}, {"title": "A probabilistic model for learning multiprototype word embeddings", "author": ["Tian et al.2014] Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu"], "venue": "In Proceedings of Coling,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Verbs semantics and lexical selection", "author": ["Wu", "Palmer1994] Zhibiao Wu", "Martha Palmer"], "venue": "In Proceedings of ACL", "citeRegEx": "Wu et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Wu et al\\.", "year": 1994}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Proceedings of ACL", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Automatically creating datasets for measures of semantic relatedness", "author": ["Zesch", "Gurevych2006] Torsten Zesch", "Iryna Gurevych"], "venue": "In Proceedings of the Workshop on Linguistic Distances", "citeRegEx": "Zesch et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zesch et al\\.", "year": 2006}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhong", "Ng2010] Zhi Zhong", "Hwee Tou Ng"], "venue": "In Proceedings of ACL, System Demonstrations", "citeRegEx": "Zhong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 30, "context": ", 2013c) and GloVe (Pennington et al., 2014).", "startOffset": 19, "endOffset": 44}, {"referenceID": 22, "context": "Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some context. A lexeme pairs a particular spelling or pronunciation with a particular meaning, i.e., a lexeme is a conjunction of a word and a synset. Our premise is that many NLP applications will benefit if the non-word data types of resources \u2013 e.g., synsets in WordNet \u2013 are also available as embeddings. For example, in machine translation, enriching and improving translation dictionaries (cf. Mikolov et al. (2013b)) would benefit from these embeddings because they would enable us to create an enriched dictionary for word senses.", "startOffset": 140, "endOffset": 1101}, {"referenceID": 10, "context": "We will focus on WordNet (Fellbaum, 1998) in this paper, but our method \u2013 based on a formalization that exploits the constraints of a resource for extending embeddings from words to other data types \u2013 is broadly applicable to other resources including Wikipedia and Freebase.", "startOffset": 25, "endOffset": 41}, {"referenceID": 22, "context": "Another motivation for our formalization stems from the analogy calculus developed by Mikolov et al. (2013a), which can be viewed as a group theory formalization of word relations: we have a set of elements (our vectors) and an operation (addition) satisfying the properties of a mathematical group, in particular, associativity and invertibility.", "startOffset": 86, "endOffset": 109}, {"referenceID": 17, "context": ", Kalchbrenner et al. (2014)).", "startOffset": 2, "endOffset": 29}, {"referenceID": 18, "context": "For WSD we use the shared tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al.", "startOffset": 45, "endOffset": 63}, {"referenceID": 21, "context": "For WSD we use the shared tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004) and a system named IMS (Zhong and Ng, 2010).", "startOffset": 79, "endOffset": 102}, {"referenceID": 16, "context": "We use SCWS (Huang et al., 2012) for the similarity evaluation.", "startOffset": 12, "endOffset": 32}, {"referenceID": 15, "context": "1 Huang et al. (2012) 62.", "startOffset": 2, "endOffset": 22}, {"referenceID": 15, "context": "1 Huang et al. (2012) 62.8\u2020 65.7\u2020 2 Tian et al. (2014) \u2013 65.", "startOffset": 2, "endOffset": 55}, {"referenceID": 15, "context": "1 Huang et al. (2012) 62.8\u2020 65.7\u2020 2 Tian et al. (2014) \u2013 65.4\u2020 3 Neelakantan et al. (2014) 67.", "startOffset": 2, "endOffset": 91}, {"referenceID": 7, "context": "3\u2020 4 Chen et al. (2014) 66.", "startOffset": 5, "endOffset": 24}, {"referenceID": 16, "context": "including (Huang et al., 2012) and (Tian et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 35, "context": ", 2012) and (Tian et al., 2014).", "startOffset": 12, "endOffset": 31}, {"referenceID": 16, "context": "Huang et al. (2012) for a similar point).", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "For Freebase (Bollacker et al., 2008), we could replace the synsets with Freebase entities.", "startOffset": 13, "endOffset": 37}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al.", "startOffset": 2, "endOffset": 46}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al.", "startOffset": 2, "endOffset": 71}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al.", "startOffset": 2, "endOffset": 95}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)).", "startOffset": 2, "endOffset": 121}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch\u00fctze (1998) created sense representations by clustering context representations derived from co-occurrence.", "startOffset": 2, "endOffset": 372}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch\u00fctze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them.", "startOffset": 2, "endOffset": 557}, {"referenceID": 0, "context": ", Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch\u00fctze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings.", "startOffset": 2, "endOffset": 685}, {"referenceID": 4, "context": "proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 4, "context": "proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al.", "startOffset": 12, "endOffset": 106}, {"referenceID": 4, "context": "proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al.", "startOffset": 12, "endOffset": 129}, {"referenceID": 4, "context": "proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset.", "startOffset": 12, "endOffset": 376}, {"referenceID": 1, "context": "Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Another interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014).", "startOffset": 95, "endOffset": 113}, {"referenceID": 11, "context": "Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al.", "startOffset": 47, "endOffset": 73}, {"referenceID": 6, "context": ", 2001) or MEN (Bruni et al., 2014).", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": "Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998); Blanchard et al. (2005) give a good overview.", "startOffset": 73, "endOffset": 97}], "year": 2015, "abstractText": "We present AutoExtend, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embeddings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet as a lexical resource, but AutoExtend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks.", "creator": "LaTeX with hyperref package"}}}