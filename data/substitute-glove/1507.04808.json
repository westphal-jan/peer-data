{"id": "1507.04808", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2015", "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "abstract": "We whatever the ongoing though compositional resume interdisciplinary take movie scripts. To to on, feel step the because introduced hierarchical sinusitis nhmuk decoder neural networking up understand what could alternative is competitive instead issue - of - both - museum neural language smaller has backoff denoted - gram version. We show that of better n't they improved diminished by paintbox the learning from right larger question - something third corpus with from pretrained word embeddings.", "histories": [["v1", "Fri, 17 Jul 2015 00:21:39 GMT  (336kb)", "http://arxiv.org/abs/1507.04808v1", "11 pages with references; under review at EMNLP 2015"], ["v2", "Wed, 25 Nov 2015 19:49:39 GMT  (214kb,D)", "http://arxiv.org/abs/1507.04808v2", "8 pages with references; will appear in AAAI 2016 (Special Track on Cognitive Systems)"], ["v3", "Wed, 6 Apr 2016 23:20:41 GMT  (215kb,D)", "http://arxiv.org/abs/1507.04808v3", "8 pages with references; Published in AAAI 2016 (Special Track on Cognitive Systems)"]], "COMMENTS": "11 pages with references; under review at EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["iulian vlad serban", "alessandro sordoni", "yoshua bengio", "aaron c courville", "joelle pineau"], "accepted": true, "id": "1507.04808"}, "pdf": {"name": "1507.04808.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Neural Network Generative Models for Movie Dialogues", "authors": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 7.\n04 80\n8v 1\n[ cs\n.C L\n] 1\n7 Ju\nl 2 01\n5"}, {"heading": "1 Introduction", "text": "Dialogue systems, also known as interactive conversational agents, virtual agents and sometimes chatterbots, are used in a wide set of applications ranging from technical support services to language learning tools and entertainment (Young et al., 2013; Shawar and Atwell, 2007). Dialogue systems can be divided into goal driven systems, such as technical support services, and non-goal driven systems, such as language learning tools or computer game characters.\nPerhaps the most successful approach to goal driven systems has been to view the dialogue problem as a partially observable Markov decision process (POMDP) (Young et al., 2013; Pieraccini et al., 2009). Unfortunately, most deployed dialogue systems use hand-crafted features for the state and action space representations, and require either a large annotated task-specific corpus or a horde of human subjects willing to interact with the unfinished system. This not only makes it expensive and time-consuming to deploy a real dialogue system, but also limits its usage to a narrow domain. Recent work has tried to push goal driven systems towards learning the observed features themselves with neural network models (Henderson et\nal., 2013; Henderson et al., 2014), yet such approaches still require large corpora of annotated task-specific simulated conversations.\nOn the other end of the spectrum are the nongoal driven systems (Ritter et al., 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014). Most recently Sordoni et al. (2015b) and Shang et al. (2015) have drawn inspiration from the use of neural networks in natural language modeling and machine translation tasks (Cho et al., 2014b; Sutskever et al., 2014). There are two motivations for developing non-goal driven systems. Firstly, they may be deployed directly for tasks which do not naturally exhibit or require a quantifiable goal (e.g. language learning) or simply for entertainment. Secondly, if they are trained on corpora related to the task of a goal-driven dialogue system (e.g. corpora which cover conversations on similar topics) then these models can be used to train a user simulator, which can then train the POMDP models discussed earlier (Young et al., 2013; Pietquin and Hastie, 2013; Levin et al., 2000). This would alleviate the expensive and time-consuming task of constructing a large-scale task-specific dialogue corpus. In addition to this, the features extracted from the non-goal driven systems may be used to expand the state space representation of POMDP models (Singh et al., 2002). This will help generalization to dialogues outside the annotated task-specific corpora.\nOur contribution is in the direction of non-goal driven systems and generative probabilistic models that do not require hand-crafted features. We define the generative dialogue problem as modeling the utterances and interactive structure of the dialogue, including turn taking and pauses. Without loss of generality, as a stepping stone and to be comparable to related work, we restrict our experiments to triples, i.e. three consecutive utterances in a dialogue. We focus on models, which scale to long conversations.\nWe experiment with the well-established recurrent neural networks (RNN) and n-gram models. In particular, we adopt the hierarchical recurrent encoder decoder (HRED) proposed by Sordoni et al. (2015a) and demonstrate that it is competitive with all other models in the literature. We extend the model with architectural changes to better suit the dialogue task and show that this improves its ability to predict semantic and topical content. We show that performance can be improved significantly by bootstrapping from pretrained word embeddings and from pretraining the model on a larger question-answer pair (Q-A) corpus.\nTo carry out experiments, we introduce the MovieTriples dataset based on movie scripts. Movie scripts span a wide range of topics and contain long dialogues with few participants, making them ideal for researching open domain, long interaction dialogue systems. They are close to human spoken language (Forchini, 2009), which makes them suitable for bootstrapping goal-driven dialogue systems."}, {"heading": "2 Models", "text": "We consider a dialogue as a sequence of M utterances D = {U1, . . . , UM} involving two interlocutors. Each Um contains a sequence of Nm tokens, i.e. Um = {wm,1, . . . , wm,Nm}, where wm,n is a random variable taking values in the vocabulary V and representing the token at position n. The tokens represent both words and dialogue acts, e.g. end of a turn and pause tokens. A generative model of dialogue parameterizes a probability distribution P - governed by parameters \u03b8 - over the set of all possible dialogues of arbitrary lengths. Under P\u03b8, the probability of a dialogue D can be written as:\nP\u03b8(U1, . . . , UM ) =\nM \u220f\nm=1\nP\u03b8(Um|U<m),\n=\nM \u220f\nm=1\nNm \u220f\nn=1\nP\u03b8(wm,n|wm,<n, U<m),\n(1)\nwhere U<m = {U1, . . . , Um\u22121} and wm,<n = {wm,1, . . . , wm,n\u22121}, i.e. the tokens preceding n in the utterance Um. Computing joint probabilities over dialogues, for any realistic vocabulary size, suffers from the curse of dimensionality, i.e. it is extremely unlikely that a new dialogue will be identical to a dialogue in the training set. It is also intractable, since sampling naively must consider an exponential number of combinations.\nThe task is analogous to language modeling. Here, the classical approach based on n-gram contexts do not share statistical weights and disregard possible semantic commonalities. Bengio et al. (2003) first proposed to tackle this problem by using a distributed (dense) vector representation of words, also called embeddings. By means of such distributed representations, the recurrent neural network (RNN) based language model (Mikolov et al., 2010) has pushed state-of-the-art performance by learning long n-gram contexts while avoiding data sparsity issues. Overall, RNNs have performed well on a variety of NLP tasks such as machine translation (Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2015) and information retrieval (Sordoni et al., 2015a)."}, {"heading": "2.1 Recurrent Neural Network", "text": "A recurrent neural network (RNN) models an input sequence of tokens {w1, . . . , wN} by computing the following recurrence:\nhn = f(hn\u22121, wn), (2)\nwhere hn \u2208 Rdh is called a recurrent, or hidden, state and acts as a compact summary of tokens, and their order, seen up to position n. After running through the sequence, the recurrent states h1, . . . , hN can be used in various ways. The last state hN may be viewed as an order-sensitive compact summary of the tokens. In language modeling tasks, the context information encoded in hn is used to predict the next token in the sentence. Formally:\nP\u03b8(wn+1 = v|w\u2264n) = exp (g(hn, v)) \u2211\nv\u2032 exp (g(hn, v \u2032))\n.\nThe functions f and g are typically defined as:\nf(hn\u22121, wn) = tanh (Hhn\u22121 + Iwn) , (3)\ng(hn, v) = O T wnhn, (4)\nThe matrix I \u2208 Rdh\u00d7|V | contains the input word embeddings, i.e. each column Ij is a vector corresponding to token j in the vocabulary V . Due to the size of the model vocabulary V , it is common to approximate the I matrix with a low-rank decomposition, i.e. I = XE, where X \u2208 Rdh\u00d7de and E \u2208 Rde\u00d7|V |, and de < dh. This approach has also the advantage that the embedding matrix E may separately be bootstrapped (e.g. learned)\nfrom larger corpora and used as a shared parameter in complex neural models composed of several RNNs submodules.\nAnalogously, the matrix O \u2208 Rdh\u00d7|V | represents the output word embeddings, where each possible next token is projected into another dense vector and compared to the hidden state hn. The probability of seeing token v at position n + 1 increases if its corresponding embedding vector Ov is \u201cnear\u201d the context vector hn.\nRNN language models are commonly trained by maximizing the log-likelihood of the parameters on a training set using stochastic gradient descent methods. However, for long sequences, this can be problematic due to either vanishing or exploding gradients (Bengio et al., 1994). To tackle these issues, gated variants of the transition function f have been proposed, including the LSTM unit (Hochreiter and Schmidhuber, 1997) and GRU unit (Cho et al., 2014b). GRUs are competitive with LSTMs in performance, but are computationally cheaper (Greff et al., 2015). Formally, a GRU parametrizes f as:\ngr = \u03c3(Ir,wn +Hrhn\u22121)\ngu = \u03c3(Iu,wn +Huhn\u22121)\nh\u0303 = tanh(Iwn +H(gr \u00b7 hn\u22121))\nf(hn\u22121, wn) = gu \u00b7 hn\u22121 + (1\u2212 gu) \u00b7 h\u0303,\n(5)\nwhere \u03c3 is the sigmoid, \u03c3(x) \u2208 [0, 1], \u00b7 is the element-wise multiplication and Ir, Iu, I \u2208 R dh\u00d7|V |, Hr,Hu,H \u2208 Rdh\u00d7dh are the parameters of the GRU. The vector gr is called reset gate, gu the update gate and h\u0303 the candidate activation. By adjusting gr and gu appropriately, the model is able to create linear skip-connections between distant hidden states, which in turn makes the credit assignment problem easier and makes the gradient signal stronger to earlier hidden states.\nGenerative dialogue modeling with RNNs can be achieved by taking the concatenated utterances as a single sequence of tokens and predicting each token conditioned on previous tokens. However, this na\u00efve approach introduces a large number of long-range dependencies which are difficult to capture, even with complex transition functions such as GRUs (Cho et al., 2014a; Bahdanau et al., 2015). This motivates the next section."}, {"heading": "2.2 Hierarchical Recurrent Encoder Decoder", "text": "Our work extends on the hierarchical recurrent encoder decoder architecture (HRED) proposed by Sordoni et al. (2015a) for web query suggestion, which itself builds on the encoder decoder architecture proposed by Cho et al. (2014b) and is closely related to the architectures proposed by El et al. (1995) and Koutnik et al. (2014).\nIn their framework, HRED predicts the next query given the queries already submitted by the user. The history of past submitted queries is considered as a sequence at two levels: a sequence of words for each web query and a sequence of queries. HRED models this hierarchy of sequences with two RNNs: one at the word level and one at the query level. A similar situation holds for dialogue: a dialogue can be seen as a sequence of utterances which, in turn, are sequences of tokens. A pictorial representation of HRED for dialogue is presented in Figure 1.\nIn dialogue, the encoder RNN maps each utterance to an utterance vector. The utterance vector is the hidden state obtained after the last token of the utterance has been processed. The higher-level context RNN keeps track of past utterances by processing iteratively each utterance vector. After processing the utterance Um, the hidden state of the context RNN represents a summary of the dialogue up to turn m which is used to predict the next utterance Um+1. The next utterance prediction is performed by means of a decoder RNN, which takes the hidden state of the context RNN and produces a probability distribution over the tokens in the next utterance. The decoder RNN is similar to the RNN language model, but with the important difference that the prediction is conditioned on the hidden state of the context RNN.\nThe encoder, context and decoder RNNs all make use of the GRU hidden unit along their temporal dimensions. Everywhere else we use the hyperbolic tangent as activation function. To help generalization, it is also possible to use the maxout activation function between the hidden state and the projected word embeddings of the decoder RNN (Goodfellow et al., 2013).\nThe HRED architecture allows information to flow more easily over very long sequences compared to a standard RNN. Consider the example in Figure 1. For the RNN trained on concatenated dialogue utterances, the distance between the token pass in the third utterance and feel in the first\ni feel like i ' m going to pass out . </s>\nwhat ' s wrong ? </s>\nutterance would be d = 18 tokens. The gradient would have to flow d step backwards in order to capture the dependency between the two terms. In HRED, the distance is reduced to d = 7, as the context RNN is updated only once per utterance. In HRED, the shortest path between two tokens occurring at positions n1 and n2 > n1 is on the order of (n2\u2212n1)/U , where U is the average length of an utterance, while in a regular RNN the shortest path is given by n2 \u2212 n1. We believe reducing this distance is a crucial property for neural models to scale to long dialogues."}, {"heading": "2.3 Bidirectional HRED", "text": "In HRED, the utterance representation is given by the last hidden state of the encoder RNN. This architecture worked well for web queries, but may be insufficient for dialogue utterances, which are longer and contain more syntactic articulations than web queries. For long utterances, the last state of the encoder RNN may not reflect important information seen at the beginning of the utterance. Thus, we propose to extend the HRED architecture with additional representational capacity on the encoder component. We choose to model the utterance encoder with a bidirectional RNN, which proved useful to Bahdanau et al. (2015) for machine translation. Bidirectional RNNs run two chains: one forward through the utterance tokens and another backward, i.e. reversing the to-\nkens in the utterance. Hence, the forward hidden state at position n summarizes tokens preceding position n and the backwards hidden state summarizes tokens following position n1. To obtain a fixed-length representation for the utterance, we summarize the information in the hidden states by: 1) applying L2 pooling over the temporal dimension of each chain, and taking the concatenation of the two pooled states as input to the context RNN, or 2) taking the concatenation of the last state of each RNN as input to the context RNN. The RNN running in reverse will effectively also introduce additional short term dependencies, which has proven useful in similar architectures (Sutskever et al., 2014). We refer to this variant as HRED-Bidirectional."}, {"heading": "2.4 Bootstrapping From Word Embeddings", "text": "The commonsense knowledge that the dialogue interlocutors share may be difficult to infer if the dataset is not sufficiently large. Therefore, our models may be significantly improved by learning word embeddings from larger corpora. This has been beneficial for classification of user intents (Forgues et al., 2014). We choose to initialize our word embeddings E with Word2Vec2 (Mikolov et al., 2013) trained on the Google News dataset con-\n1Note that the bidirectional RNN is always one utterance behind the decoder RNN.\n2http://code.google.com/p/word2vec/\ntaining about 100 billion words. The sheer size of the dataset ensures that the embeddings contain rich semantic information about each word."}, {"heading": "2.5 Bootstrapping From Subtitles Q-A", "text": "Bootstrapping word embeddings will not affect the other model parameters, which will still rely on the original dialogue corpus for training. To learn a good initialization point for these other parameters, we may pretrain the model on a large nondialogue corpus, which covers similar topics and types of interactions between interlocutors. One such corpus is the Q-A SubTle corpus containing about 5.5M Q-A pairs constructed from movie subtitles. Now, we construct an artificial dialogue dataset by taking each {Q,A} pair as a two-turn dialogue D = {U1 = Q,U2 = A}. Since there are two utterances in each example, all the model parameters will be updated during training. However, because these examples are short, the higherlevel context RNN may not be initialized to a very useful point for the HRED models."}, {"heading": "3 Related Work", "text": "Modeling conversations on micro-blogging websites with generative probabilistic models was first proposed by Ritter et al. (2011). They view the response generation problem as a translation problem, where a post needs to be translated into a response. Generating responses was considerably more difficult than translating between languages, which was attributed to the wide range of plausible responses and the lack of alignment on words and phrases between the post and the response. In particular, they found that the statistical machine translation approach was superior to the information retrieval approach. In the same vein, Shang et al. (2015) proposed to use the neural network encoder-decoder framework for generating responses on the micro-blogging website Weibo. They also formulated the problem as conditional generation, where given a post, the model generates a response. Unfortunately, this architecture scales linearly with the number of dialogue turns.\nA way to consider the conversation context was proposed by Sordoni et al. (2015b) to generate responses for posts on Twitter. They concatenated three consecutive Twitter messages, representing a short conversation between two users, and defined the problem as predicting each word in the conversation given all preceding words. They encoded a bag-of-words context representation with\na multilayer neural network and generated a response with a standard RNN. They then combined their generative model with a machine translation system, and showed that the hybrid system outperformed the machine translation system proposed by Ritter et al. (2011).\nTo the best of our knowledge, Banchs et al. (2012) were the first to suggest using movie scripts to build dialogue systems. They constructed an information retrieval system based on the vector space model. Conditioned on one or more utterances, their model searches a database of movie scripts and retrieves an appropriate response. Using another information retrieval system, Ameixa et al. (2014) used movie subtitles to train a dialogue system. They showed that an existing dialogue system could successfully be augmented with the subtitles, such that, when its response confidence is low, it will search an appropriate answer from the subtitle corpus. This helped answer out-of-domain questions."}, {"heading": "4 Dataset", "text": "The MovieTriples dataset has been developed by expanding and preprocessing the Movie-DiC dataset by Banchs et al. (2012) to make it fit the generative dialogue modeling framework3. Based on a literature review, we found that the MovieDiC was the largest dataset available containing all consecutive utterances from movies. Other datasets in the literature include the corpora by Walker et al. (2012a), Roy et al. (2014), and the unpublished Cornell Movie Dialogue Corpus4.\nCompared to similar-sized domain-specific datasets (Uthus and Aha, 2013; Walker et al., 2012b), movie scripts span a wide range of topics, which makes them ideal for investigating semantic understanding of dialogue models. Contrary to micro-blogging websites, such as Twitter (Ritter et al., 2010), movie scripts contain long dialogues with few participants. This makes them well-suited for modeling long-term interactions. They also contain relatively few spelling mistakes and acronyms, which previously made research on micro-blogging websites difficult. Employing movie scripts also makes it possible to enrich dialogue systems with additional contextual information, such as action descriptions, summaries and genre labels. Movie scripts are close in nature to\n3The dataset is made available upon request. 4 http://www.mpi-sws.org/~cristian/Cornell_\nMovie-Dialogs_Corpus.html\nhuman spoken conversations (Forchini, 2009). As noted by Forchini (2009): \"movie language can be regarded as a potential source for teaching and learning spoken language features\". Hence, we argue that bootstrapping a goal-driven spoken dialogue system based on movie scripts can improve performance."}, {"heading": "4.1 Extraction And Preprocessing", "text": "We expanded the dataset to include metainformation for each movie, extracted through the online API service OMDBAPI5. We then processed the dataset to remove duplicate manuscripts. Afterwards, a spelling corrector based on Wikipedia\u2019s most common English spelling mistakes was applied6. We then implemented a set of simple regular expressions to remove double punctuation marks and spacings. We used the python-based natural language toolkit NLTK (Bird et al., 2009) to perform tokenization and named-entity recognition7 . All names and numbers were replaced with the <person> and <name> tokens respectively. Numbers were replaced with the <number> token. The use of placeholders allows to measure performance w.r.t. the abstract semantic and syntactic structure of dialogues, as opposed to recalling exact names and numbers. Similar preprocessing has been applied in previous work (Ritter et al., 2010; Nio et al., 2014). To reduce data sparsity further, all tokens were finally transformed to lowercase letters, and all but the 10,000 most frequent tokens were replaced with the <unk> token representing unknown or out-of-vocabulary words."}, {"heading": "4.2 Triples Construction", "text": "The atomic entry of the MovieTriples is a \u201ctriple\u201d {U1, U2, U3}, i.e. a dialogue of three turns occurring between two interlocutors A and B for which\n5 http://www.omdbapi.com\n6Retrieved on February 20th, 2015: http: //en.wikipedia.org/wiki/Wikipedia:Lists_of_common_ misspellings\n7NLTK uses a maximum entropy chunker trained on the ACE corpus: http://catalog.ldc.upenn.edu/LDC2005T09\nA emits the first utterance U1, B responds by U2 and A finally responds with the last utterance U3. This is similar to previous work (Sordoni et al., 2015b). Unlike conversations extracted from internet relayed chat (IRC) (Elsner and Charniak, 2008), the majority of movie scenes only contain a single dialogue thread, which means that nearly all extracted triples constitute a continuous dialogue segment between the active speakers.\nTo capture the interactive dialogue structure, a special end-of-utterance token is appended to all utterances. If the same speaker makes a break in an utterance and then continues again, we add a special continued-utterance token. All models must learn to predict these dialogue act tokens.\nTo avoid co-dependencies between triples coming from the same movie, we first split the movies into training, validation and test set, and then construct the triples. This will ensure that our results generalize to new domains. The dataset contains about 13M words in total and about 10M words in the training set. Statistics are reported in Table 1."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Baselines", "text": "We test our models against state-of-the-art neural network and non-neural network baselines. First, we compare our models to well-established ngram models (Goodman, 2001). To compare to a neural network baseline, we train a RNN on the concatenation of the utterances in each triple. We also report results obtained by the contextsensitive model (DCGM-I) recently proposed by Sordoni et al. (2015b)."}, {"heading": "5.2 Evaluation Metrics", "text": "Accurate evaluation of a non-goal driven dialogue system is an open problem (Galley et al., 2015; Pietquin and Hastie, 2013; Schatzmann et al., 2005). There is no well-established method for automatic evaluation, and human-based evaluation is expensive. Nevertheless, for probabilistic language models word perplexity is a wellestablished performance metric (Bengio et al., 2003; Mikolov et al., 2010), and has been suggested for generative dialogue models previously (Pietquin and Hastie, 2013):\nexp\n(\n\u2212 1\nNW\nN \u2211\nn=1\nlog P\u03b8(U n 1 , U n 2 , U n 3 )\n)\n, (6)\nfor a model with parameters \u03b8, dataset with N triples {Un1 , U n 2 , U n 3 } N n=1, and NW the number of tokens in the entire dataset. The lower the perplexity, the better the model is assumed to be. Unlike linguistic performance metrics, word perplexity explicitly measures the model\u2019s ability to account for the syntactic structure of the dialogue (e.g. turn-taking) and the syntactic structure of each utterance (e.g. punctuation marks). In dialogue, the distribution over the words in the next utterance is highly multi-modal, e.g. there are many possible answers, which makes perplexity particularly appropriate because it will always measure the probability of regenerating the exact reference utterance.\nAlthough perplexity is an established measure for generative models, in the dialogue setting, utterances may be overwhelmed by many common words especially arising from colloquial or informal exchanges. To focus the perplexity metric on deeper semantic content (e.g. the dialogue topic), we propose to also use a reweighed perplexity metric. That is, the perplexity metric applied to all words in the dataset except for a small set of stop words, which instead is assumed to have been predicted correctly and excluded from the denominator in the first fraction of eq. (6). The set of stop words contains 77 English pronouns8, all punctuation marks, the unknown word token and the endof-utterance token, which constitute 48.37% of the training set."}, {"heading": "5.3 Training Procedure", "text": "To train the neural network models, we optimized the log-likelihood of the triples using the recently proposed Adam optimizer (Kingma and Ba, 2014). Our implementation relies on the opensource Theano library (Bastien et al., 2012). The best hyperparameters of the models were chosen by early stopping with patience on the validation set perplexity (Bengio, 2012). For the baseline RNN, we tested hidden state spaces dh = 200, 300 and 400, and found that 400 yielded best performance. For HRED we experimented with encoder and decoder hidden state spaces of size 200, 300 and 400. Increasing these two state space improved performance consistently, but due to GPU memory limitations we limited ourselves to size 300 when not bootstrapping or bootstrapping from Word2Vec, and to 400 when bootstrapping from\n8 http://www.esldesk.com/vocabulary/pronouns\nSubTle. Preliminary experiments showed that the context RNN state space at and above 300 performed similarly, so we fixed it at 300 when not bootstrapping or bootstrapping from Word2Vec, and to 1200 when bootstrapping from SubTle. To help generalization, we used the maxout activation function when not bootstrapping and when bootstrapping from Word2Vec.\nBootstrapping Word Embeddings Our embedding matrix E is initialized using the publicly available 300 dim. Word2Vec embeddings trained on the Google News corpus. Certain words in the movie scripts vocabulary could not be directly matched to the Word2Vec embeddings. These words (0.15% of the training set tokens), along with dialogue act and placeholder tokens, were initialized randomly. All dimensions were rescaled to have mean zero and standard deviation 0.01. The training procedure is unfolded into two stages. In the first stage, we trained each neural model with fixed Word2Vec embeddings. During this stage, we also trained the dialogue act and placeholder tokens, together with tokens not covered by the original Word2Vec embeddings. Training the dialogue act tokens from the dialogue corpus allows the model to learn the interaction structure of the dialogue. In the second stage, we trained all parameters of each neural model until convergence. We used L2 pooling for the HRED models, since it appeared to perform slightly better under this setup.\nBootstrapping SubTle We processed the SubTle corpus following the same procedure used for MovieTriples, but now treating the last utterance U3 as empty. The final SubTle corpus contained 5,503,741 Q-A pairs, and a total of 93,320,500 tokens. Although SubTle was extracted from subtitles, and MovieTriples from movie scripts, we found no significant utterance overlap. Manual inspection showed that overlapping utterances consisted mainly of very common short phrases, e.g. are you okay ? or so what ?. When bootstrapping from the SubTle corpus, we found that all models performed slightly better when randomly initializing and learning the word embeddings from SubTle compared to fixing the word embeddings to those given by Word2Vec. We did not use L2 pooling when bootstrapping from SubTle, since it appeared to perform slightly worse. We speculate that its regularization effect is unnecessary here.\nThe HRED models were pretrained for approximatively four epochs on the SubTle dataset. Longer training did not appear to improve performance for any of the models. Then, we fine-tuned the pretrained models on the MovieTriples dataset holding the word embeddings fixed, since we found non-significant difference also fine-tuning these."}, {"heading": "5.4 Empirical Results", "text": "Our results are summarized in Table 2. All neural models beat state-of-the-art n-grams models w.r.t. both word perplexity and word classification error (comparing the most likely predicted word with the actual one). Without bootstrapping, the RNN model performs similarly to the more complex DCGM-I and HRED models. This can be explained by the size of the dataset, which makes it easy for the HRED and DCGM-I model to overfit. The last three lines of Table 2 show that bootstrapping the model parameters from a large nondialogue corpus achieves significant gains in both measures. Bootstrapping from SubTle is particularly useful since it allows a gain of nearly 10 perplexity points compared to the HRED model without bootstrapping. We believe that this is because it trains all model parameters, unlike bootstrapping from Word2Vec.\nIn Table 3, we report the results of the standard RNN and HRED models when bootstrapped\nfrom SubTle corpus. The gains due to architectural choice are naturally smaller than those obtained by bootstrapping, because we are in a regime of relatively little training data compared to other natural language processing tasks, such as machine translation, and hence we would expect the differences to grow with more training data and longer dialogues. The largest gains are obtained by the proposed HRED-Bidirectional architecture, which on five out of the six metrics outperform both the standard HRED and RNN model. The perplexity metrics computed excluding stop words demonstrate that the HRED-Bidirectional model outperforms the standard HRED model in capturing semantic and topic-specific information. The bidirectional structure appears to capture and retain information from the U1 and U2 utterances better than either the RNN and the original HRED model. This confirms our earlier hypothesis, and demonstrates the potential of HRED as a solution for modeling long dialogues."}, {"heading": "5.5 MAP Outputs", "text": "We evaluate the use of beam-search for RNNs (Graves, 2012) to approximate the most probable (MAP) utterance U3, given the first two utterances, U1 and U2. MAP outputs are shown in Table 5.5 for HRED-Bidirectional bootstrapped from SubTle corpus. As shown in the table, the model often produces sensible\nanswers. However, the majority of the predictions are generic, such as I don\u2019t know or I\u2019m sorry9. We observed the same phenomenon for the RNN model. This appears to be a recurring observation in the literature (Sordoni et al., 2015b; Vinyals and Le, 2015)10. However, to the best of our knowledge, we are the first to emphasize and discuss it in details.\nThere are several possible explanations for this behavior. Firstly, due to data scarcity, the model may only learn to predict the most frequent utterances. Since dialogue is inherently ambiguous and multi-modal, predicting dialogues accurately would require more data than other natural language processing tasks. Secondly, the majority of dialogue tokens consist of punctuation marks and pronouns. Since every token is weighted equally during training, the gradient signal of the neural network will be dominated by these punctuation and pronoun tokens. This makes it hard for the neural network to learn topic-specific embeddings and even harder to predict diverse utterances. This suggest exploring neural architectures which explicitly separate semantic structure from syntactic structure. Finally, the context of a triple may be too short. In that case, the models should benefit from longer contexts and by conditioning on other information sources, such as semantic and visual information.\nAn important implication of this observation is that metrics based on MAP outputs (e.g. cosine similarity, BLEU, Levenshtein distance) will primarily favour models that output the same number of punctuation marks and pronouns as are in the test utterances, as opposed to matching semantic content (e.g. nouns and verbs). This would be sys-\n9This behavior did not occur when we generated stochastic samples. In fact, these samples contained a large variety of topic-specific words and often appeared to maintain the topic of the conversation.\n10Our work was carried out independently from that of Vinyals et al. (2015).\ntematically biased and not necessarily in any way correlate with the objective of producing appropriate responses."}, {"heading": "6 Conclusion and Future work", "text": "The main contributions of this paper are the following. We have demonstrated that a hierarchical recurrent network generative model can outperform both n-gram based models and baseline neural network models on the task of predicting the next utterance and dialogue acts in a dialogue. To this end, we introduced a novel dataset called MovieTriples based on movie scripts, which is suitable for modeling long, open domain dialogues close to human spoken language. In addition to the recurrent hierarchical architecture, we found two crucial ingredients: the use of a large external monologue corpus to initialize the word embeddings, and the use of a large related, but non-dialogue, corpus in order to pretrain the recurrent net. This points to the need for larger dialogue datasets.\nFuture work should study full length dialogues, as opposed to triples, and model other dialogue acts, such as interlocutors entering or leaving the dialogue and executing actions. It should focus on bootstrapping from other, large non-dialogue corpora, as well as expand MovieTriples to include other movie script corpora. Finally, our analysis of the model MAP outputs suggest that it would be beneficial to include longer and additional context, including other modalities such as video, and that MAP based evaluation metrics are inappropriate when the outputs are generic in nature.\nAcknowledgements: The authors acknowledge NSERC, Canada Research Chairs, CIFAR and Compute Canada for funding. The authors thank Rafael Banchs and Luisa Coheur for providing the Movie-DiC and SubTle corpora, as well as Nissan Pow, Ryan Lowe and Laurent Charlin for helpful feedback."}], "references": [{"title": "Luke, i am your father: dealing with out-of-domain requests by using movies subtitles", "author": ["David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma."], "venue": "Intelligent Virtual Agents, pages 13\u201321. Springer.", "citeRegEx": "Ameixa et al\\.,? 2014", "shortCiteRegEx": "Ameixa et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR 2015).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Iris: a chatoriented dialogue system based on the vector space model", "author": ["Rafael E Banchs", "Haizhou Li."], "venue": "Proceedings of the Association for Computational Linguistics (ACL 2012), System Demonstrations, pages 37\u201342. Association for Computa-", "citeRegEx": "Banchs and Li.,? 2012", "shortCiteRegEx": "Banchs and Li.", "year": 2012}, {"title": "Movie-dic: A movie dialogue corpus for research and development", "author": ["Rafael E. Banchs."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, Association for Computational Linguis-", "citeRegEx": "Banchs.,? 2012", "shortCiteRegEx": "Banchs.", "year": 2012}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio."], "venue": "Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio."], "venue": "Neural Networks: Tricks of the Trade, pages 437\u2013 478. Springer.", "citeRegEx": "Bengio.,? 2012", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "Natural Language Processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, chapter On the Properties of Neural Machine Translation: Encoder", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 493\u2013499. Citeseer.", "citeRegEx": "Hihi and Bengio.,? 1995", "shortCiteRegEx": "Hihi and Bengio.", "year": 1995}, {"title": "You talking to me? a corpus and algorithm for conversation disentanglement", "author": ["Micha Elsner", "Eugene Charniak."], "venue": "Association for Computational Linguistics (ACL 2008), pages 834\u2013842.", "citeRegEx": "Elsner and Charniak.,? 2008", "shortCiteRegEx": "Elsner and Charniak.", "year": 2008}, {"title": "Spontaneity reloaded: American face-to-face and movie conversation compared", "author": ["Pierfranca Forchini."], "venue": "Corpus Linguistics 2009. Abstracts of the 5th Corpus Linguistics Conference, page 118.", "citeRegEx": "Forchini.,? 2009", "shortCiteRegEx": "Forchini.", "year": 2009}, {"title": "Bootstrapping dialog systems with word embeddings", "author": ["Gabriel Forgues", "Joelle Pineau", "Jean-Marie Larchev\u00eaque", "R\u00e9al Tremblay."], "venue": "Workshop on Modern Machine Learning and Natural Language Processing, Advances in Neural", "citeRegEx": "Forgues et al\\.,? 2014", "shortCiteRegEx": "Forgues et al\\.", "year": 2014}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "CoRR,", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Maxout networks", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio."], "venue": "Proceedings of The 30th International Conference on Machine Learning (ICML 2013), pages 1319\u20131327.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "A bit of progress in language modeling extended version", "author": ["Joshua T. Goodman."], "venue": "Machine Learning and Applied Statistics Group Microsoft Research. Technical Report, MSR-TR-2001-72.", "citeRegEx": "Goodman.,? 2001", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves."], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML 2012), Representation Learning Workshop.", "citeRegEx": "Graves.,? 2012", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:1503.04069.", "citeRegEx": "Greff et al\\.,? 2015", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Deep neural network approach for the dialog state tracking challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "Proceedings of the SIGDIAL 2013 Conference, pages 467\u2013471.", "citeRegEx": "Henderson et al\\.,? 2013", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2014), page 292.", "citeRegEx": "Henderson et al\\.,? 2014", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A clockwork rnn", "author": ["Jan Koutn\u00edk", "Klaus Greff", "Faustino Gomez", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of the International Conference on Machine Learning (ICML 2014).", "citeRegEx": "Koutn\u00edk et al\\.,? 2014", "shortCiteRegEx": "Koutn\u00edk et al\\.", "year": 2014}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."], "venue": "Speech and Audio Processing, IEEE Transactions on, 8(1):11\u201323.", "citeRegEx": "Levin et al\\.,? 2000", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Developing non-goal dialog system based on examples of drama television", "author": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Tomoki Toda", "Mirna Adriani", "Satoshi Nakamura."], "venue": "Natural Interaction with Robots, Knowbots and Smartphones, pages 355\u2013", "citeRegEx": "Nio et al\\.,? 2014", "shortCiteRegEx": "Nio et al\\.", "year": 2014}, {"title": "Are we there yet? research in commercial spoken dialog systems", "author": ["Roberto Pieraccini", "David Suendermann", "Krishna Dayanidhi", "Jackson Liscombe."], "venue": "12th International Conference on Text, Speech and Dialogue, pages 3\u201313. Springer.", "citeRegEx": "Pieraccini et al\\.,? 2009", "shortCiteRegEx": "Pieraccini et al\\.", "year": 2009}, {"title": "A survey on metrics for the evaluation of user simulations", "author": ["Olivier Pietquin", "Helen Hastie."], "venue": "The knowledge engineering review, 28(01):59\u201373.", "citeRegEx": "Pietquin and Hastie.,? 2013", "shortCiteRegEx": "Pietquin and Hastie.", "year": 2013}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Empiricial Methods in Natural Language (EMNLP 2011), pages 583\u2013593. Association for Computational Linguistics.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Tvd: a reproducible and multiply aligned tv series dataset", "author": ["Anindya Roy", "Camille Guinaudeau", "Herv\u00e9 Bredin", "Claude Barras."], "venue": "LREC.", "citeRegEx": "Roy et al\\.,? 2014", "shortCiteRegEx": "Roy et al\\.", "year": 2014}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["Jost Schatzmann", "Kallirroi Georgila", "Steve Young."], "venue": "6th SIGDIAL Workshop on DISCOURSE and DIALOGUE.", "citeRegEx": "Schatzmann et al\\.,? 2005", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "Association for Computational Linguistics (ACL-IJCNLP 2015). In press.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Chatbots: are they really useful", "author": ["Bayan Abu Shawar", "Eric Atwell"], "venue": "In LDV Forum,", "citeRegEx": "Shawar and Atwell.,? \\Q2007\\E", "shortCiteRegEx": "Shawar and Atwell.", "year": 2007}, {"title": "Optimizing dialogue management with reinforcement learning: Experiments with the njfun system", "author": ["Satinder Singh", "Diane Litman", "Michael Kearns", "Marilyn Walker."], "venue": "Journal of Artificial Intelligence Research, pages 105\u2013133.", "citeRegEx": "Singh et al\\.,? 2002", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie."], "venue": "Proceedings of the 24th ACM International", "citeRegEx": "Sordoni et al\\.,? 2015a", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "JianYun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Conference of", "citeRegEx": "Sordoni et al\\.,? 2015b", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances In Neural Information Processing Systems (NIPS 2014), pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The ubuntu chat corpus for multiparticipant chat analysis", "author": ["David C Uthus", "David W Aha."], "venue": "AAAI Spring Symposium: Analyzing Microtext.", "citeRegEx": "Uthus and Aha.,? 2013", "shortCiteRegEx": "Uthus and Aha.", "year": 2013}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "International Conference on Machine Learning (ICML 2015), Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "An annotated corpus of film dialogue for learning and characterizing character style", "author": ["Marilyn A Walker", "Grace I Lin", "Jennifer Sawyer."], "venue": "LREC, pages 1373\u20131378.", "citeRegEx": "Walker et al\\.,? 2012a", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "A corpus for research on deliberation and debate", "author": ["Marilyn A Walker", "Jean E Fox Tree", "Pranav Anand", "Rob Abbott", "Joseph King."], "venue": "LREC, pages 812\u2013817.", "citeRegEx": "Walker et al\\.,? 2012b", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Steve Young", "Milica Gasic", "Blaise Thomson", "Jason D Williams."], "venue": "IEEE, 101(5):1160\u2013 1179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 44, "context": "Dialogue systems, also known as interactive conversational agents, virtual agents and sometimes chatterbots, are used in a wide set of applications ranging from technical support services to language learning tools and entertainment (Young et al., 2013; Shawar and Atwell, 2007).", "startOffset": 233, "endOffset": 278}, {"referenceID": 35, "context": "Dialogue systems, also known as interactive conversational agents, virtual agents and sometimes chatterbots, are used in a wide set of applications ranging from technical support services to language learning tools and entertainment (Young et al., 2013; Shawar and Atwell, 2007).", "startOffset": 233, "endOffset": 278}, {"referenceID": 44, "context": "Perhaps the most successful approach to goal driven systems has been to view the dialogue problem as a partially observable Markov decision process (POMDP) (Young et al., 2013; Pieraccini et al., 2009).", "startOffset": 156, "endOffset": 201}, {"referenceID": 28, "context": "Perhaps the most successful approach to goal driven systems has been to view the dialogue problem as a partially observable Markov decision process (POMDP) (Young et al., 2013; Pieraccini et al., 2009).", "startOffset": 156, "endOffset": 201}, {"referenceID": 19, "context": "Recent work has tried to push goal driven systems towards learning the observed features themselves with neural network models (Henderson et al., 2013; Henderson et al., 2014), yet such approaches still require large corpora of annotated task-specific simulated conversations.", "startOffset": 127, "endOffset": 175}, {"referenceID": 20, "context": "Recent work has tried to push goal driven systems towards learning the observed features themselves with neural network models (Henderson et al., 2013; Henderson et al., 2014), yet such approaches still require large corpora of annotated task-specific simulated conversations.", "startOffset": 127, "endOffset": 175}, {"referenceID": 31, "context": "On the other end of the spectrum are the nongoal driven systems (Ritter et al., 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 64, "endOffset": 145}, {"referenceID": 2, "context": "On the other end of the spectrum are the nongoal driven systems (Ritter et al., 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 64, "endOffset": 145}, {"referenceID": 0, "context": "On the other end of the spectrum are the nongoal driven systems (Ritter et al., 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 64, "endOffset": 145}, {"referenceID": 27, "context": "On the other end of the spectrum are the nongoal driven systems (Ritter et al., 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 64, "endOffset": 145}, {"referenceID": 39, "context": "(2015) have drawn inspiration from the use of neural networks in natural language modeling and machine translation tasks (Cho et al., 2014b; Sutskever et al., 2014).", "startOffset": 121, "endOffset": 164}, {"referenceID": 44, "context": "corpora which cover conversations on similar topics) then these models can be used to train a user simulator, which can then train the POMDP models discussed earlier (Young et al., 2013; Pietquin and Hastie, 2013; Levin et al., 2000).", "startOffset": 166, "endOffset": 233}, {"referenceID": 29, "context": "corpora which cover conversations on similar topics) then these models can be used to train a user simulator, which can then train the POMDP models discussed earlier (Young et al., 2013; Pietquin and Hastie, 2013; Levin et al., 2000).", "startOffset": 166, "endOffset": 233}, {"referenceID": 24, "context": "corpora which cover conversations on similar topics) then these models can be used to train a user simulator, which can then train the POMDP models discussed earlier (Young et al., 2013; Pietquin and Hastie, 2013; Levin et al., 2000).", "startOffset": 166, "endOffset": 233}, {"referenceID": 0, "context": ", 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014). Most recently Sordoni et al. (2015b) and Shang et al.", "startOffset": 29, "endOffset": 106}, {"referenceID": 0, "context": ", 2011; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014). Most recently Sordoni et al. (2015b) and Shang et al. (2015) have drawn inspiration from the use of neural networks in natural language modeling and machine translation tasks (Cho et al.", "startOffset": 29, "endOffset": 130}, {"referenceID": 12, "context": "They are close to human spoken language (Forchini, 2009), which makes them suitable for bootstrapping goal-driven dialogue systems.", "startOffset": 40, "endOffset": 56}, {"referenceID": 36, "context": "In particular, we adopt the hierarchical recurrent encoder decoder (HRED) proposed by Sordoni et al. (2015a) and demonstrate that it is competitive with all other models in the literature.", "startOffset": 86, "endOffset": 109}, {"referenceID": 25, "context": "By means of such distributed representations, the recurrent neural network (RNN) based language model (Mikolov et al., 2010) has pushed state-of-the-art performance by learning long n-gram contexts while avoiding data sparsity issues.", "startOffset": 102, "endOffset": 124}, {"referenceID": 39, "context": "Overall, RNNs have performed well on a variety of NLP tasks such as machine translation (Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2015) and information retrieval (Sordoni et al.", "startOffset": 88, "endOffset": 154}, {"referenceID": 1, "context": "Overall, RNNs have performed well on a variety of NLP tasks such as machine translation (Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2015) and information retrieval (Sordoni et al.", "startOffset": 88, "endOffset": 154}, {"referenceID": 37, "context": ", 2015) and information retrieval (Sordoni et al., 2015a).", "startOffset": 34, "endOffset": 57}, {"referenceID": 4, "context": "Bengio et al. (2003) first proposed to tackle this problem by using a distributed (dense) vector representation of words, also called embeddings.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "However, for long sequences, this can be problematic due to either vanishing or exploding gradients (Bengio et al., 1994).", "startOffset": 100, "endOffset": 121}, {"referenceID": 21, "context": "To tackle these issues, gated variants of the transition function f have been proposed, including the LSTM unit (Hochreiter and Schmidhuber, 1997) and GRU unit (Cho et al.", "startOffset": 112, "endOffset": 146}, {"referenceID": 18, "context": "GRUs are competitive with LSTMs in performance, but are computationally cheaper (Greff et al., 2015).", "startOffset": 80, "endOffset": 100}, {"referenceID": 1, "context": "capture, even with complex transition functions such as GRUs (Cho et al., 2014a; Bahdanau et al., 2015).", "startOffset": 61, "endOffset": 103}, {"referenceID": 36, "context": "Our work extends on the hierarchical recurrent encoder decoder architecture (HRED) proposed by Sordoni et al. (2015a) for web query suggestion, which itself builds on the encoder decoder architecture proposed by Cho et al.", "startOffset": 95, "endOffset": 118}, {"referenceID": 9, "context": "(2015a) for web query suggestion, which itself builds on the encoder decoder architecture proposed by Cho et al. (2014b) and is closely related to the architectures proposed by El et al.", "startOffset": 102, "endOffset": 121}, {"referenceID": 9, "context": "(2015a) for web query suggestion, which itself builds on the encoder decoder architecture proposed by Cho et al. (2014b) and is closely related to the architectures proposed by El et al. (1995) and Koutnik et al.", "startOffset": 102, "endOffset": 194}, {"referenceID": 9, "context": "(2015a) for web query suggestion, which itself builds on the encoder decoder architecture proposed by Cho et al. (2014b) and is closely related to the architectures proposed by El et al. (1995) and Koutnik et al. (2014).", "startOffset": 102, "endOffset": 220}, {"referenceID": 15, "context": "To help generalization, it is also possible to use the maxout activation function between the hidden state and the projected word embeddings of the decoder RNN (Goodfellow et al., 2013).", "startOffset": 160, "endOffset": 185}, {"referenceID": 37, "context": "Adapted with permission from Sordoni et al. (2015a).", "startOffset": 29, "endOffset": 52}, {"referenceID": 1, "context": "We choose to model the utterance encoder with a bidirectional RNN, which proved useful to Bahdanau et al. (2015) for machine translation.", "startOffset": 90, "endOffset": 113}, {"referenceID": 39, "context": "The RNN running in reverse will effectively also introduce additional short term dependencies, which has proven useful in similar architectures (Sutskever et al., 2014).", "startOffset": 144, "endOffset": 168}, {"referenceID": 13, "context": "This has been beneficial for classification of user intents (Forgues et al., 2014).", "startOffset": 60, "endOffset": 82}, {"referenceID": 30, "context": "Modeling conversations on micro-blogging websites with generative probabilistic models was first proposed by Ritter et al. (2011). They view the response generation problem as a translation problem, where a post needs to be translated into a response.", "startOffset": 109, "endOffset": 130}, {"referenceID": 30, "context": "Modeling conversations on micro-blogging websites with generative probabilistic models was first proposed by Ritter et al. (2011). They view the response generation problem as a translation problem, where a post needs to be translated into a response. Generating responses was considerably more difficult than translating between languages, which was attributed to the wide range of plausible responses and the lack of alignment on words and phrases between the post and the response. In particular, they found that the statistical machine translation approach was superior to the information retrieval approach. In the same vein, Shang et al. (2015) proposed to use the neural network encoder-decoder framework for generating responses on the micro-blogging website Weibo.", "startOffset": 109, "endOffset": 651}, {"referenceID": 37, "context": "A way to consider the conversation context was proposed by Sordoni et al. (2015b) to generate responses for posts on Twitter.", "startOffset": 59, "endOffset": 82}, {"referenceID": 30, "context": "They then combined their generative model with a machine translation system, and showed that the hybrid system outperformed the machine translation system proposed by Ritter et al. (2011).", "startOffset": 167, "endOffset": 188}, {"referenceID": 2, "context": "To the best of our knowledge, Banchs et al. (2012) were the first to suggest using movie scripts to build dialogue systems.", "startOffset": 30, "endOffset": 51}, {"referenceID": 0, "context": "Using another information retrieval system, Ameixa et al. (2014) used movie subtitles to train a dialogue system.", "startOffset": 44, "endOffset": 65}, {"referenceID": 3, "context": "The MovieTriples dataset has been developed by expanding and preprocessing the Movie-DiC dataset by Banchs et al. (2012) to make it fit the generative dialogue modeling framework3.", "startOffset": 100, "endOffset": 121}, {"referenceID": 3, "context": "The MovieTriples dataset has been developed by expanding and preprocessing the Movie-DiC dataset by Banchs et al. (2012) to make it fit the generative dialogue modeling framework3. Based on a literature review, we found that the MovieDiC was the largest dataset available containing all consecutive utterances from movies. Other datasets in the literature include the corpora by Walker et al. (2012a), Roy et al.", "startOffset": 100, "endOffset": 401}, {"referenceID": 3, "context": "The MovieTriples dataset has been developed by expanding and preprocessing the Movie-DiC dataset by Banchs et al. (2012) to make it fit the generative dialogue modeling framework3. Based on a literature review, we found that the MovieDiC was the largest dataset available containing all consecutive utterances from movies. Other datasets in the literature include the corpora by Walker et al. (2012a), Roy et al. (2014), and the unpublished Cornell Movie Dialogue Corpus4.", "startOffset": 100, "endOffset": 420}, {"referenceID": 30, "context": "Contrary to micro-blogging websites, such as Twitter (Ritter et al., 2010), movie scripts contain long dialogues with few participants.", "startOffset": 53, "endOffset": 74}, {"referenceID": 12, "context": "human spoken conversations (Forchini, 2009).", "startOffset": 27, "endOffset": 43}, {"referenceID": 12, "context": "human spoken conversations (Forchini, 2009). As noted by Forchini (2009): \"movie language can", "startOffset": 28, "endOffset": 73}, {"referenceID": 8, "context": "We used the python-based natural language toolkit NLTK (Bird et al., 2009) to perform tokenization and named-entity recognition7 .", "startOffset": 55, "endOffset": 74}, {"referenceID": 30, "context": "Similar preprocessing has been applied in previous work (Ritter et al., 2010; Nio et al., 2014).", "startOffset": 56, "endOffset": 95}, {"referenceID": 27, "context": "Similar preprocessing has been applied in previous work (Ritter et al., 2010; Nio et al., 2014).", "startOffset": 56, "endOffset": 95}, {"referenceID": 38, "context": "This is similar to previous work (Sordoni et al., 2015b).", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": "Unlike conversations extracted from internet relayed chat (IRC) (Elsner and Charniak, 2008), the majority of movie scenes only contain a single dialogue thread, which means that nearly all extracted triples constitute a continuous dialogue segment between the active speakers.", "startOffset": 64, "endOffset": 91}, {"referenceID": 16, "context": "First, we compare our models to well-established ngram models (Goodman, 2001).", "startOffset": 62, "endOffset": 77}, {"referenceID": 6, "context": "established performance metric (Bengio et al., 2003; Mikolov et al., 2010), and has been suggested for generative dialogue models previously", "startOffset": 31, "endOffset": 74}, {"referenceID": 25, "context": "established performance metric (Bengio et al., 2003; Mikolov et al., 2010), and has been suggested for generative dialogue models previously", "startOffset": 31, "endOffset": 74}, {"referenceID": 29, "context": "(Pietquin and Hastie, 2013):", "startOffset": 0, "endOffset": 27}, {"referenceID": 22, "context": "To train the neural network models, we optimized the log-likelihood of the triples using the recently proposed Adam optimizer (Kingma and Ba, 2014).", "startOffset": 126, "endOffset": 147}, {"referenceID": 4, "context": "Our implementation relies on the opensource Theano library (Bastien et al., 2012).", "startOffset": 59, "endOffset": 81}, {"referenceID": 7, "context": "The best hyperparameters of the models were chosen by early stopping with patience on the validation set perplexity (Bengio, 2012).", "startOffset": 116, "endOffset": 130}, {"referenceID": 17, "context": "We evaluate the use of beam-search for RNNs (Graves, 2012) to approximate the most probable (MAP) utterance U3, given the first two utterances, U1 and U2.", "startOffset": 44, "endOffset": 58}, {"referenceID": 38, "context": "This appears to be a recurring observation in the literature (Sordoni et al., 2015b; Vinyals and Le, 2015)10.", "startOffset": 61, "endOffset": 106}, {"referenceID": 41, "context": "This appears to be a recurring observation in the literature (Sordoni et al., 2015b; Vinyals and Le, 2015)10.", "startOffset": 61, "endOffset": 106}], "year": 2015, "abstractText": "We consider the task of generative dialogue modeling for movie scripts. To this end, we extend the recently proposed hierarchical recurrent encoder decoder neural network and demonstrate that this model is competitive with state-of-the-art neural language models and backoff n-gram models. We show that its performance can be improved considerably by bootstrapping the learning from a larger questionanswer pair corpus and from pretrained word embeddings.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}