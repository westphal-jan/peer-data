{"id": "1508.04734", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Fault Diagnosis of Helical Gear Box using Large Margin K-Nearest Neighbors Classifier using Sound Signals", "abstract": "Gear automatic besides came of the least widely several transmission system while fact papermaking. Sound transmit all a shape tools instances one dynamic useful about an provide remain. Not n't regarding items started new contemporary reported suitability result signal simultaneously but lying premature third-party. Maximum numbers of prose are based leaving FFT (Fast Fourier Transform) instance on well its yet limitations few non - trains coded like took ones into gears. In not published, attempt has he also in standard instruments signals formerly from gears in good being robot malfunctioned conditions give once principles of wrong diagnosis just a machine difficulty approach. The descriptive statistical features they extracted from before acquired sound signals and into connotation music were selected small J48 calls deciduous technique. The nine covers are then used though prefix type Large Margin K - link desperately approach. The reference also discusses that limiting of various specified on prefix timeliness.", "histories": [["v1", "Wed, 19 Aug 2015 18:33:07 GMT  (789kb)", "http://arxiv.org/abs/1508.04734v1", "Accepted for publication"]], "COMMENTS": "Accepted for publication", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["m amarnath", "s arunav", "hemantha kumar", "v sugumaran", "g s raghvendra"], "accepted": false, "id": "1508.04734"}, "pdf": {"name": "1508.04734.pdf", "metadata": {"source": "CRF", "title": "Fault Diagnosis of Helical Gear Box using Large Margin K- Nearest Neighbors Classifier using Sound Signals", "authors": ["M. Amarnath", "S. Arunav", "Hemantha Kumar", "V. Sugumaran"], "emails": ["hemanta76@gmail.com", "v_sugu@yahoo.com"], "sections": [{"heading": null, "text": "Gear drives are one of the most widely used transmission system in many machinery.\nSound signals of a rotating machine contain the dynamic information about its health\nconditions. Not much information available in the literature reporting suitability of\nsound signals for fault diagnosis applications. Maximum numbers of literature are\nbased on FFT (Fast Fourier Transform) analysis and have its own limitations with\nnon-stationary signals like the ones from gears. In this paper, attempt has been made\nin using sound signals acquired from gears in good and simulated faulty conditions for\nthe purpose of fault diagnosis through a machine learning approach. The descriptive\nstatistical features were extracted from the acquired sound signals and the\npredominant features were selected using J48 decision tree technique. The selected\nfeatures were then used for classification using Large Margin K-nearest neighbor\napproach. The paper also discusses the effect of various parameters on classification\naccuracy.\nKeywords: Gearbox fault diagnosis, J48 decision tree algorithm, Large Margin\nK-nearest neighbor algorithm, feature subset selection, machine learning\napproach, classification"}, {"heading": "1. Introduction", "text": "Helical gear box condition monitoring has received considerable attention for many\nyears. Gears are the most important and frequently encountered components in the\nvast majority of rotating machines. Their load carrying capacity and reliability is of\nprime importance for the overall machine performance. Hence, fault diagnosis of such\nmachine elements has been the subject matter of extensive research (Drosjack and\nHouser [9] ).Gear failures can be caused by several factors such as incorrect design or\ninstallation, acid corrosion, poor lubrication etc. Vibration and sound monitoring has\nbeen widely reported as being a useful technique for the diagnosis of the condition of\nrotating machines. It can help fault detection before significant damage occurs. More\nefficient maintenance scheduling can be planned if accurate information about a\nmachine\u2019s condition is known and if online monitoring is used .The traditional pattern\nrecognition includes a large collection of very different types of mathematical tools\n(preprocessing, extraction of features and final recognition). In many cases it is\ndifficult to say what kind of tool would be the best for a particular problem Fault\nclassification techniques have been used in a wide range of pattern recognition\napplications including sound vibration monitoring. The contributions of some authors\n(Bo et al. [5], Samanta [14] , Chen and Wang [7] and Paya et al. [10] ) reveal the application\nof neural networks to online condition monitoring of rotating machinery to have very\nhigh success rates. ANNs consequently appear to be a possible solution to gear\ndiagnostics problem as they could allow real-time online condition monitoring at a\nreasonably low cost (Yang et al. [22] ). Paya et al. [10] carried out investigations to\nstudy both bearing and gear faults introduced separately as a single fault and then\ntogether as multiple faults in the drive line. The real time signals obtained from the\ndriveline were preprocessed by wavelet transforms for the neural network to perform\nfault detection and identification of the exact kind of fault occurring in the model\ndrive line. The authors summarized the results of their research for distinguishing\nbetween different kinds of faults viz., good gear, blip gear, shaved gear and one with\ninner race defect. An overall success rate of 96% was achieved on test by back\npropagation network which gave the details of exact kind of fault in the driveline.\nBaydar and Ball [4] demonstrated the results of fault diagnosis experiments conducted\non two stage helical gearbox. Authors have considered sound and vibration signals to\ndetect local faults in helical gear tooth. Sound and vibration signals acquired from the\ngearbox were processed using Morlet wavelet. Amplitude and phase maps obtained\nfrom wavelet analysis provided a good visual inspection tool to detect faults in the\nearly stage. Vyas and Satishkumar [19] carried out experiments to automate the fault\ndetection procedure in rotating machinery. A back propagation learning algorithm and\na multilayer network were employed for fault detection. Five different types of faults\nwere introduced in the experimental setup and five statistical moments of vibration\nsignals were employed to train the network. An overall success rate of 90% was\nobtained in this work. Chen and Wang [7] dealt with multi layer perceptron (MLP)\npattern classifiers for wavelet map interpretation and their application as a tool for\nmechanical fault detection. Features for neural networks were extracted from\ninstantaneous scale distribution. This study was undertaken to simplify the difficulties\nin inspecting complicated wavelet patterns in time-scale domain. The authors\nhighlighted the details of construction, training and testing multilayer perceptron\nbased classifiers for diagnosis of gear faults. Ramroop et al. [13] conducted\nexperimental investigation to detect faults in multistage industrial gearbox, sound\nsignals were acquired from the gearbox under near field condition, and fast Fourier\ntransform (FFT) method used extract fault related features from these signals. This\npaper provided a series of best practice guidelines for implementation sound condition\nmonitoring technique to detect local faults in industrial gearbox. Wuxing et al. [21]\nconducted experiments on a gearbox to classify the gear faults using cumulants and\nthe radial basis function (RBF) network. The cumulants were calculated from the\nvibration signals collected from the inspected gearbox and were used as input features\nto an ANN. The radial basis function network was then used as a classifier for various\noperating conditions of the helical gear box. e.g., normal, spalling, one worn tooth\ncondition and two worn teeth condition. The authors concluded that the method of\nfault classification by combining cumulants and the radial basis function network is\npromising and achieves better accuracy than many of the current methods available.\nSamanta [23] presented an experimental study to compare the performance of gear fault\ndetection and classification using ANN and SVM. The time domain vibration signals\nof a rotating machine with normal and defective gears were pre-processed for feature\nextraction. The role of different vibration signals at normal and light loads were\ninvestigated in this work. SVM shows better classification accuracy than ANN. In\naddition, genetic algorithms (GA) were used to improve accuracy of fault\nclassification. With GA based selection, the performance of ANN and SVM showed\ncomparatively equal accuracy in results. Yang et al. [23] presented a novel scheme to\ndetect faults in reciprocating compressors of refrigerators. The vibration and noise\nsignals were wavelet transformed to find diagnostic information. Further the statistical\nfeatures of wavelets were used for fault classification using ANN and SVM\ntechniques. A high accuracy in classification of faults was obtained using SVM\ntechnique. Shin et al. [15] adopted SVM technique for detection and classification of\nfaults in electro mechanical machinery using vibration parameters. Multilayer\nperceptrons of ANN technique was also included in the diagnosis program. The\nresults concluded that the classification of faults using SVM was superior to that of\nMLP of ANN techniques. Sugumaran et al. [17] employed proximal support vector\nmachines (PSVM) and SVM to classify faults in bearings. The authors compared the\nresults of PSVM and SVM. PSVM was found to have less iterations and faster\nlearning as compared to SVMs in fault classification. A novel method to diagnose\nfaults in rotating machinery was proposed by Qiao et al. [11] . Improved wavelet\npackage transform was used in to extract the salient frequency band features from the\nvibration signals. SVM ensemble technique was adopted in fault classification, which\nprovides promising results in diagnosis of machinery. Amarnath and Praveen Krishna\n[3] carried out experiments to detect faults in ball bearing and gears using sound\nsignals. Emperical mode decomposition (EMD) method was used to detect local faults\nin bearings and gears, EMD based statistical parameters such as kurtosis, root mean\nsquare, skewness, crest factor and impulse factor values were extracted from sound\nsignals, these fault related features showed comparatively better fault diagnostic\ninformation than that of statistical parameter values of unprocessed sound signals.\nAmarnath et al. [2] used acoustic signals acquired from near field area of bearings in\ngood and simulated faulty conditions for the purpose of fault diagnosis through\nmachine learning approach. Sugumaran et al. [18] used vibration signals acquired from\ngears in good and simulated faulty conditions for the purpose of fault diagnosis\nthrough J48 decision tree algorithm.\nIn the present study, an attempt is made to exploit sound signals for the purpose of\nfault diagnosis of helical gear box. To extract some meaningful features, descriptive\nstatistical features like mean, median, kurtosis etc., were used. Important features\nwere selected and classification was carried out using the novel Large Margin K-\nnearest neighbor algorithm with varying number of neighbors and size of training set\nusing random sub sampling. A modified version of the pseudo code given by Bremer\net al [6] along with modifications proposed by Cost and Salzberg [8] is used in this\nstudy in order to train and test the classifier. The results for both vibration and sound\nsignals are plotted as a function of test case size versus classification accuracy\npercentage."}, {"heading": "2. Experimental Setup", "text": "Fig. 1 shows the experimental set up (Amarnath et al. [1] ). It consists of a 5 HP two\nstage helical gearbox driven by a 5.5 HP 3-phase induction motor at 1200 rpm. The\nmechanical output of the gearbox is used to drive a D.C generator and the output\npower of the generator is dissipated in a resistor bank, which provides torque load to\nthe generator and gearbox. This arrangement does not provide any additional\nvibration to the test rig. The motor, gearbox and generator are mounted on stiffened I-\nbeams, which are anchored to a massive concrete block. An accelerometer B&K 4332\nis stud-mounted to measure the vertical vibration signals generated on the bearing\nhousing of the 16 teeth pinion. Meshing gear frequencies are calculated at 320 Hz and\nmultiples. Different data sets were collected when the helical gear train was working\nat normal, 10%, 20%, 40%, 60%, 80% and 100% tooth removal conditions (Badyar\nand Ball [4] ). A total of 30 data sets were collected for each operating condition. The\nsignals were truncated to 3 kHz using a low pass filter and sampled at 8 kHz. The\naccelerometer outputs were conditioned using B&K TYPE 2626 charge amplifier.\nFig.1 Experimental setup of two stage helical gearbox\nThe pinion is connected to a D.C motor (which is used as generator) to generate 2 kW power, which is dissipated in a resistor bank. Hence, the actual load on the gearbox is only 2.6 HP which is 52% of its rated power 5 HP. In industrial environment utilization of load varies from 50% to 100%. In the case of traditional dynamometer, additional torsional vibrations can occur due to torque fluctuations. This is avoided in this case by using D.C motor and resistor bank. Tyre couplings are fitted between the electrical machines and gear box so that backlash in the system can be restricted to the gears. The motor, gear box and generator are mounted on I-beams, which are anchored to a massive foundation. Vibration signals are measured using a Bruel and Kj\u00e6r accelerometer which is installed close to the test bearing. Signals are sampled at a sampling frequency of 8.2 kHz. The experimental setup with equipment and sensors is shown in Fig. 2.\nOverhaul time of a new gear box is more than one year. It is very difficult to study the fault detection procedures without seeded fault trials. Local faults in a gear box can be classified into three categories. (a) Surface wear spalling (b) cracked tooth and (c) loss of a part of tooth due to breakage of tooth at root or at a point on working tip (broken tooth or chipped tooth). There are different methods to simulate faults in gears viz. electric discharge machining (EDM), grinding, adding iron particles in gearbox lubricant and over loading the gear box i.e. accelerated test condition. The simplest approach is partial tooth removal. This simulates the partial tooth break, which is common in many industrial applications (Staszewski et al. [16] and Yesilyurt et al. [24] ))."}, {"heading": "3. Statistical Feature Extraction", "text": "From the vibration signals, descriptive statistical parameters such as mean, median, mode, kurtosis, skewness, standard error, standard deviation, minimum, maximum, sum, and range are computed to serve as features. They are named as \u2018statistical features\u2019 here. Brief descriptions about the extracted features are given below.\nAccelerometer\n(a) Standard error: Standard error is a measure of the amount of error in the\nprediction of y for an individual x in the regression, where x and y are the sample means and \u2018n\u2019 is the sample size.\n\u221a\n\u2211(\n\u2211\n) (1)\n(b) Standard deviation: This is a measure of the effective energy or power content of the vibration signal. The following formula was used for computation of standard deviation.\n\u221a \u2211 \u2211\n(2)\n(c) Sample variance: It is variance of the signal points and the following formula was\nused for computation of sample variance.\n\u2211 \u2211\n(3)\n(d) Kurtosis: Kurtosis indicates the flatness or the spikiness of the signal. Its value is\nvery low for normal condition of the bearing and high for faulty condition of the bearing due to the spiky nature of the signal.\n(\n\u2211(\n) )\n(4)\nwhere\u2018s\u2019 is the sample standard deviation.\n(e) Skewness: Skewness characterizes the degree of asymmetry of a distribution\naround its mean. The following formula was used for computation of skewness.\n\u2211(\n)\n(5)\n(f) Range: It refers to the difference in maximum and minimum signal point values for a given signal.\n(g) Minimum value: It refers to the minimum signal point value in a given signal. As the bearing parts (inner race, outer race) get degraded, the vibration levels seem to go high. Therefore, it can be used to detect faulty bearing condition.\n(h) Maximum value: It refers to the maximum signal point value in a given signal.\n(i) Sum: It is the sum of all feature values for each sample."}, {"heading": "4. Large Margin K-Nearest Neighbors Algorithm", "text": "The large Margin K-nearest neighbor algorithm is a variant of the original K-nearest neighbour algorithm for classification [Bremer et al [6] ]. The basis of the algorithm lies in the construction of a distance metric. Given any two objects (vectors in a feature space) x and y a metric is defined as d(x, y) provided it satisfies the following criteria:-\n1. (non-negativity, or separation axiom)\n2. if and only if x = y (coincidence axiom)\n3. (symmetry)\n4. (Sub-additivity / triangle inequality).\nA distance measure which satisfies conditions 1, 2 and 4 is called a pseudo metric. In statistical data mining algorithms a metric or a pseudo metric can be used as a quantitative dissimilarity measure.\nLet each object (observation) be defined as a vector in the feature space of its dimensions. Also let an integer value k (the number of nearest neighbors to be considered) be defined. Also, let the class variable take n possible values denoted by\nwhere"}, {"heading": "4.1 Training Phase", "text": "In the training phase a certain subset of vectors are chosen from the original data set as the training set. Let this subset be T. Now the first step of the algorithm requires the construction of a pseudo metric. The metric can be written as:-\n( \u20d7  \u20d7 ) ( \u20d7  \u20d7 )  \u20d7  \u20d7 (6)\nHere  \u20d7 and  \u20d7 are two observations or instances. In the special case of M being identity, the metric is same as Euclidean distance. The algorithm makes a distinction between two types of instances called target neighbors and impostors which are described in the next two subsections.\n4.1.1 Target Neighbors\nTarget neighbors are instances which are selected before learning. Each instance  \u20d7 has exactly k different target neighbors within D, which all shares the same class label\n. The target neighbors are the data points that should become nearest neighbors\nunder the learned metric. Let us denote the set of target neighbors for a data point  \u20d7 as Ni .\n4.1.2 Impostors\nAn impostor of a data point  \u20d7 is another data point  \u20d7 with a different class label\n(i.e where  \u20d7 is one of the k nearest neighbors of  \u20d7 . During learning the\nalgorithm tries to minimize the number of impostors for all data instances in the training set.\nThe Large Margin Nearest Neighbor algorithm optimizes the matrix M using semidefinite programming. The objective has two components: For every data observation  \u20d7 , the target neighbors should be as close as possible and the impostors should be as far away as possible simultaneously. The learned pseudo metric causes the input vector  \u20d7 to be surrounded by training instances of the same class (i.e target\nneighbors). If a test point is being classified, it would be classified correctly under the new metric distance scheme, since it will be surrounded by its target neighbors."}, {"heading": "4.2 Problem Restatement and Solver Algorithm", "text": "The problem can be restated in terms of an optimization problem. The first optimization goal is achieved by minimizing the average distance between instances and their target neighbors\n\u2211  \u20d7  \u20d7 (7)\nThe second goal is achieved by constraining impostors  \u20d7 to be one unit further away than target neighbors  \u20d7 (and therefore pushing them out of the local neighborhood of\n \u20d7 ). The resulting inequality constraint can be stated as:\n( \u20d7  \u20d7 )  \u20d7  \u20d7 (8)\nThe margin of exactly one unit fixes the scale of the matrix M. Any alternative choice c > 0 would result in a rescaling of M by a factor of\nThe final optimization problem thus problem becomes:\n\u2211  \u20d7  \u20d7 \u2211 (9)\nSubject to:-\n( \u20d7  \u20d7 )  \u20d7  \u20d7 (10)\n(11)\n(12)\nHere the slack variables absorb the amount of violations of the impostor\nconstraints (i.e the slack variables minimize the errors caused by the impostor instances). The overall sum of distance and impostor errors is minimized. Constraint (12) ensures that M is positive semi-definite. This optimization problem is an instance of semi - definite programming (SDP). Although SDPs tend to suffer from high\ncomputational complexity, this particular SDP instance can be solved very efficiently due to the underlying geometric properties of the problem. In the present case a particularly efficient gradient based solver proposed by Weinberger et al. [20] is used. Certain modifications are done to the algorithm in order to guarantee termination and save computation time and resources.\nWe need to find a matrix M that minimizes the distance measure as defined in equation (6). We will call this M matrix the optimal matrix .The solver algorithm is iterative and is based on the gradient descent approach. A pseudo code description of the algorithm is given in appendix A along with detailed explanations of the data structures used. The algorithm is governed by the following equations:-\nFirstly the error introduced by impostor instances can be written in the form of equation (13),\n( ( \u20d7  \u20d7 )\n \u20d7  \u20d7 ) (13)\nHere M is the optimal matrix as required in equation 6. Also:-\n( \u20d7  \u20d7 ) ( \u20d7  \u20d7 )  \u20d7  \u20d7\n( ) (14)\n( \u20d7  \u20d7 )  \u20d7  \u20d7 (15)\nEquation (14) is just a paraphrasing of equation (6) using trace operation on matrices. Now consider \u00b5 as the rate of gradient descent. The objective function can now be paraphrased as:-\n\u2211 ( ( \u20d7  \u20d7 ) ) (16)\nHere and if  \u20d7 and  \u20d7 have the same class label and 0 otherwise. Also indicates that  \u20d7 is a target neighbor of  \u20d7 .Moreover the constraint (8)\nchanges to\n( \u20d7  \u20d7 )\n \u20d7  \u20d7 (17)\nWhereas constraints (9) and (10) remain the same.\nNow the gradient matrix is calculated using the following equation:-\n\u2211 \u2211 (18)\nThe 3 \u2013 tuple if and is the matrix M at the tth iteration.\nFinally the Matrix M is recalculated at every iteration until a user defined convergence criteria is met:-\n(19)\nHere is a user defined parameter and is the projection of M to all semi-definite cones. The projection to all semi definite cones arises from the following equations,\n(20)\n(21)\nAlso and . Here is a diagonal matrix of all the eigenvalues of the optimal matrix. is the diagonal matrix with all the negative entries of replaced with 0. In other words it is the diagonal matrix of positive eigenvalues of the optimal matrix. is the square matrix of all the eigenvectors of the optimal matrix stacked together column wise.\nOnce the pseudo metric is ascertained we move on to the Test phase."}, {"heading": "4.3 Test Phase", "text": "For do:-\n1. Determine the weighted k-nearest neighbors (k elements of the training set with least pseudo metric distance). One can do this by repeatedly picking the element with the minimum distance (closest neighbor) and repeat it k times without replacement. Moreover while picking the minimum distance incorporate the following weight measure (Cost and Salzberg [8] ),\n \u20d7  \u20d7 (22)\nHere  \u20d7 is the test instance and \u20d7 is the training instance which is considered as one of\nthe k-nearest neighbors.\n2. Determine the weight of instances of each denoted by . Now let = argmax . The previous sentence means - choose by majority voting which class has highest representation in the k-nearest neighbors and classify the test example by that.\nAs stated previously, a pseudo code description of the algorithm is given in Appendix (A) along with modifications made for this study."}, {"heading": "5. Results and Discussion", "text": "The sound signals were recorded for normal and abnormal conditions of helical gear box. Totally 420 samples were collected; out of which 60 samples were from Healthy condition. For faulty load with 10%, 20%, 40%, 60%, 80% and 100% fault, 60 samples from each condition were collected. The statistical features were treated as features (attributes) and act as inputs to the algorithm. The corresponding status or condition (10% fault, 20 % fault, 40% fault, 60% fault, 80% fault, 100% fault and healthy) of the classified data will be the required output of the algorithm. This input and corresponding output together forms the dataset. The dataset is used with decision tree J48 algorithm for generating the decision tree for the purpose of feature selection (Quinlan [12] ). Although the nodes closer to the root are more significant, all nodes in the tree are given equal importance for feature subset selection in order to maintain simplicity of the code. Of all the fourteen features extracted, only standard error was discarded.\nOnce the features were selected, the large margin k- nearest neighbor classifier was used for both training and testing purposes. The number of objects (training set size) for testing was varied from 1 to 59 and found that when it is 50 and when k = 1, the\nalgorithm gives best classification accuracy of 94.3% for sound signals. Out of 420 data points 396 data points were classified successfully.\nThe following graph is a graph for classification accuracy(in percentage) versus Test set size for a few representative values of nearest neighbors (i.e. k values).\nThe confusion matrix is plotted for k = 1 and test set size = 50. It is a quantitative summary of the classification details.\nThe interpretation of the confusion matrix is as follows:-\n The diagonal elements in the confusion matrix show the number of correctly\nclassified instances. The rows correspond to the classification achieved by the LMNN algorithm (the predicted classes) and the columns correspond to the actual classes.\n Out of the 420 data instances, 396 have been classified correctly.\n6. Conclusions\nGears are important machine elements in industrial machinery which is subjected to wear and tear. This paper presented an algorithm based interpretation of vibration signals for automated evaluation of gear condition. From acquired vibration data, a model was built using data modeling technique. Decision tree algorithm was used for feature subset selection and large margin k \u2013 nearest neighbor algorithm was used for classification of the condition of the gear. The built model was tested with all possible combinations of nearest neighbor and test set size value and an accuracy of 94.2% was achieved for k = 1 and test set size = 50. Hence, the results of the large margin k - nearest neighbor algorithm can be practically used for diagnosing the condition of the gears successfully."}, {"heading": "7. References", "text": "[1] Amarnath, M., Sujatha, C., and Swarnamani, S., Experimental studies on\nthe effects of reduction in gear tooth stiffness and lubricant film thickness in a spur geared system, Tribology International, Vol.42(2), pp. 340-352, 2009.\n[2] Amarnath, M., Sugumaran, V., and Hemantha kumar (2013), Exploiting\nSound Signals for Fault Diagnosis of Bearings Using Decision Tree, Measurement, Vol. 46, pp. 1250 \u2013 1256, 2013.\n[3] Amarnath, M., and Praveen, K., Detection and diagnosis of surface wear\nfailure in a spur geared system using EEMD based vibration signal analysis, Tribology International, Vol. 61, pp. 224-234, 2013.\n[4] Baydar, N., and Ball, A., Detection and diagnosis of gear failure via\nvibration and acoustic signals using wavelet transform, Mechanical Systems and Signal Processing, Vol. 17(4), pp. 787-804, 2003.\n[5] Bo, L., Mo, Y., and James C. H., Neural network based motor rolling\nbearing fault diagnosis, IEEE Transaction on Industrial Electronics, Vol. 47 (5), pp. 1060-1069, 2000.\n[6] Bremner, D., Demaine, E., Erickson, J., Iacono, J., Langerman, S.,\nMorin, P., and Toussaint, G., Output-sensitive algorithms for computing nearest-neighbor decision boundaries, Discrete and Computational Geometry, Vol. 33(4), pp. 593\u2013604, 2005.\n[7] Chen, D. and Wang, W. J., Classification of wavelet patterns using\nmultilayer neural networks, Mechanical Systems and Signal Processing, Vol. 16(4), pp. 695-704, 2002.\n[8] Cost, S., and Slazberg, S., A weighted Nearest Neighbor Algorithm for\nlearning with symbolic features. Machine learning, pages 10:57 \u2013 78, 1993.\n[9] Drosjack, M. J. and Houser, D. R., An experimental and theoretical\nstudies of the effect of simulated pitch line pitting on the vibration of a geared system, ASME publication report 77-DET-123, 1977.\n[10] Paya, B. A., East, I. I., and Badi, M. N. M., Artificial neural network\nbased fault diagnostics of rotating machinery using wavelet transform as a preprocessor, Mechanical Systems and Signal Processing, Vol. 11(5), pp. 751-765, 1997.\n[11] Qiao, H., Zhengjia, H., Zhousuo, Z., and YanyangZi, Fault diagnosis\nof rotating machinery based on improved Wavelet package transform and SVMs ensemble, Mechanical Systems and Signal Processing, Vol. 21(2), pp. 688-705, 2007.\n[12] Quinlan, J. R., Induction of Decision Trees, Machine Learning, Vol.1,\npp. 81-106, 1986.\n[13] Ramroop, G., Liu, K., Gu, F., Paya B. S., and Ball, A. D., Airborne\nAcoustic Condition Monitoring, www.maintenenceengineering.com (Accessed Dec. 2003).\n[14] Samanta, B., Gear fault diagnosis using artificial neural networks and\nsupport vector mechanics with genetic algorithms, Mechanical Systems and Signal Processing, 18(3), 625-649, 2004.\n[15] Shin, H.J,, Eom, D. H., and Kim, S. S., One class support vector\nmachines \u2013 an application in fault detection and classification, Computer and Industrial Engineering, Vol. 48, pp. 395 \u2013 408, 2005.\n[16] Staszewski, W. J., Worden. K., and Tomlinson, G. R., Time-frequency\nanalysis in gearbox fault detection using the Wigner-ville distribution and pattern recognition, Mechanical Systems and Signal Processing, Vol. 11(5), pp. 673-692, 1997.\n[17] Sugumaran, V., Muralidharan, V., and Ramachandran, K. I., Feature\nselection using Decision Tree and classification through Proximal Support Vector Machine for fault diagnostics of roller bearing, Mechanical Systems and Signal Processing, Vol. 21(2), pp. 930-942, 2007.\n[18] Sugumaran, V., Deepak, J., Amarnath, M., and Hemanthakumar, Fault\nDiagnosis of Helical Gear Box using Decision Tree through Vibration\nSignals, International Journal of Performability Engineering, Vol. 9(2), pp. 195- 208, 2013.\n[19] Vyas, N. S., and Satishkumar, D., Artificial neural network design for\nfault identification in a rotor-bearing system, Mechanism and Machine Theory, Vol. 36, pp. 157-175, 2001.\n[20] Weinberger, K. Q.; Blitzer J. C., Saul L. K., Distance Metric Learning\nfor Large Margin Nearest Neighbor Classification, Advances in Neural Information Processing Systems, Vol. 18, pp. 1473\u20131480, 2006.\n[21] Wuxing, L., Tse, P. W., Guiicai, Z., and Shitielin, Classification of\ngear faults using cumulant and the radial basis function, Mechanical Systems and Signal Processing, Vol. 18, pp. 381-389, 2004.\n[22] Yang, M., Stronachand, A. F., McConnel, P., Third order spectral\ntechnique for the diagnosis of motor bearing conditions using artificial neural network, Mechanical Systems and Signal Processing, Vol. 16(2- 3), pp. 391-411, 2002.\n[23] Yang, B. S., Hwang, W.W., Kim, D. J., and Tan, A. C., Condition\nclassification of small reciprocating compressor for refrigerators using artificial neural networks and support vector machines, Mechanical Systems and Signal Processing, Vol. 19, pp.371-390, 2005.\n[24] Yesilyurt, I., Fengshou and Ball, A. D.,Gear tooth stiffness\nmeasurement using modal analysis and its use in wear fault severity assessment of spur gears, NDT&E International, Vol. 36, pp. 357-372, 2003."}, {"heading": "8. Appendix", "text": "A.1. Pseudo code for the modified LMNN algorithm:The following is the pseudo code for the LMNN algorithm which is used in this paper. Certain changes were made from the original paper by Weinberger et al. [20] in order to make it better suited for fault diagnosis.\nA.1.1. Training phase algorithm: - Initialize pseudo metric, which will be used by the Test phase algorithm\nUser defined input \u2013 \u03bc (gradient step size), Training set of instances { \u20d7 , k (number of nearest neighbors), (gradient matrix weight)\nOutput \u2013 Matrix M (optimal matrix for pseudo metric)\n1: {Initialize with the identity matrix} 2: {Initialize neighborhood matrix as 0 matrix} 3 Initialize empty active set} 4: \u2211 {Initialize gradient}\n5 {Initialize integer counter variable} 6: {Initialize integer counter variables} 7: while (not converged) do 8: while < number of training instances) do 9: while {k = number of nearest neighbors} do 10: if {if  \u20d7 if a target neighbor of  \u20d7\n11: 12: end if 13: end while 14: end while 15: for do 16: while < K) do\n17: if {if  \u20d7 if a target neighbor of  \u20d7 do\n18: 19: end if 20: end while 21: end for 22: \u2211 \u2211\n23: Take gradient step and project onto SDP cone} 24: 25: {Reset the active set to empty for next iteration} 26: {Reset neighborhood matrix as 0 matrix} 26: end while 27: Output\nA.1.2. Test phase algorithm: - Classify test cases and generate accuracy and confusion matrix statistics User defined input \u2013 Test set of instances { \u20d7 , optimal matrix (M), k (number of nearest neighbors) Output \u2013 Percentage accuracy of classification, confusion matrix of classification. 1: 2: {Get the actual class of the test instances} 3: {Initialize the set of predicted classes as empty set} 4: W = 0 {Initialize set of weights = 0} 5: for each instance  \u20d7 in test set  \u20d7 do 6: K-nearest = generate k nearest neighbors from training set {training set =\ntotal observations \u2013 test set)\n7: for each \u20d7 in K-nearestdo\n8: ( \u20d7 )\n9: ( ) ( ) ( \u20d7  \u20d7 )\n10: end for\n11: = {Choose the index of W with highest\nvalue as the class label}\n12: W = 0 {Re initialize set of weights = 0} 13: end for 14: {declare accuracy variable = 0} 15: {set counter = 0} 16: while ) do 17: if ( ) do 18: 19: end if 20: end while 21: 22: Output 23: Output )\nThe explanation of the pseudo code is as the following:A.1.4. Training phase algorithm: -The input to the training phase program is the gradient step size (\u03bc) the set of training instances { \u20d7 , the number of nearest neighbors (k) and the gradient matrix weight ( In this case, \u03bc = 0.1 was chosen as the default value for the gradient step size. The training set and k were varied and chosen using random sub sampling method. Firstly an identity matrix of dimensions n * n is initialized where n = Number of features chosen via feature subset selection (in this study n = 4 for sound signals and n = 3, for vibration signals respectively). Initialize a Neighborhood matrix N with dimensions same as the optimal matrix and set all elements as 0. The neighborhood matrix is a bitmap which has the row and\ncolumn element = 1 if  \u20d7 is a target neighbor of instance  \u20d7 A set called\nActive set (A) is defined which is initially set as an empty set. The active set is a set of 3- tuples where and  \u20d7 is not a target neighbor of instance  \u20d7 . This ensures that the slack variable is always > 0 in the set N\nNow initialize the gradient matrix according to line (4) of the algorithm. The dimensions of the gradient matrix are same as the initial optimal matrix. The matrix\nis defined in equation (14). An integer counter is declared which keeps of track\nof number of iterations completed by the algorithm\u2019s main while loop (lines 7 to 26). Similarly 3 integer counters are also defined which is used to generate and index the active set. Lines 8 to 14 are used to generate the neighborhood matrix for each iteration. Since only k nearest neighbors are to be checked, for each instance  \u20d7 only k immediate neighbors are checked (line 8).Furthermore it is checked whether  \u20d7\nis a target neighbor of instance  \u20d7 (line 10). Accordingly the neighborhood matrix is updated. Lines 15 to 21 are used to generate the Active set. For this all are chosen for which the row and column element of the neighborhood matrix is 1(line 16). All of the k - nearest neighbors of instance  \u20d7 are checked and tested if\n(line 17). If yes then the 3-tuple is added to active set N (line 18). An\nalternate way of updating the active set is to check whether a set of ( \u20d7  \u20d7  \u20d7 leads to\na strictly positive value of equation (13). If it does, then the 3-tuple is added to active set A. Lines 7 to 26 is the main loop of the training phase algorithm which generated the active set and updates the optimal matrix M using it. In this study, the convergence criteria for the main while loop (line 7) was chosen to be ten iterations of the loop. The gradient matrix is updated according to equation (18) (line 22) and the optimal matrix is updated via equation (19) (line 23). The variable in line 23 is the gradient matrix weight and is set to 0.01 as a default value in this study. The function\nis projection onto semi definite cones for the matrix M. This ensures that the optimal matrix is always positive semi-definite. In order to evaluate this function, the eigen-decomposition of the optimal matrix is needed. Its procedure is given in the last paragraph of section 4.2. Lines 25 and 26 are used to reset the neighborhood matrix and the active set to empty for the next iteration. The output is the final optimal matrix after the while loop.\nA.1.5. Test phase algorithm: - The test phase algorithm runs once the training phase algorithm is completed. Inputs to the algorithm are the optimal matrix (M) computed in the training phase algorithm, and the set of test instances { \u20d7 and the number of\nnearest neighbors (k). First the Actual class labels are acquired from the test set in order to test accuracy (line 2) and put into an array called C Actual. The size of C Actual = size of test set. Now initialize an empty array called C Predicted with same size as C Actual. In order to generate the C Predicted array, a weights array (W) is initialized with value 0. The size of the array = Number of possible class labels. Lines 5 to 13 generate the C Predicted array. For each test instance  \u20d7 in the test set { \u20d7 , the k-nearest neighbors of { \u20d7 is added to an array called K-nearest (line 6). In order to achieve that, the training set (total observations \u2013 test set) can be sorted based on distance from the test instance using any sorting algorithm and the first k instances from the sorted array can be picked. The class label of each of the nearest neighbors is\ndetermined (line 8) and is stored in a variable Now the\nindex of W is updated\naccording to equation (22) which uses equation (6) as a distance measure(line 9). Then the predicted class is classified according to a majority voting scheme based on the W array (line 11). After the classes are labelled for each test instance, the accuracy is determined based on number of matches between C Actual and C Predicted (lines 14 to 20). The confusion matrix is also generated (line 23). The details of the confusion matrix are given in section 5."}], "references": [{"title": "Experimental studies on the effects of reduction in gear tooth stiffness and lubricant film thickness in a spur geared system", "author": ["M. Amarnath", "C. Sujatha", "S. Swarnamani"], "venue": "Tribology International,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Exploiting Sound Signals for Fault Diagnosis of Bearings", "author": ["M. Amarnath", "V. Sugumaran", "Hemantha kumar"], "venue": "Using Decision Tree, Measurement,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Detection and diagnosis of surface wear failure in a spur geared system using EEMD based vibration signal analysis", "author": ["M. Amarnath", "K. Praveen"], "venue": "Tribology International,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Detection and diagnosis of gear failure via vibration and acoustic signals using wavelet transform", "author": ["N. Baydar", "A. Ball"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Neural network based motor rolling bearing fault diagnosis", "author": ["L. Bo", "Y. Mo", "James C. H"], "venue": "IEEE Transaction on Industrial Electronics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Output-sensitive algorithms for computing nearest-neighbor decision boundaries", "author": ["D. Bremner", "E. Demaine", "J. Erickson", "J. Iacono", "S. Langerman", "P. Morin", "G. Toussaint"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Classification of wavelet patterns using multilayer neural networks", "author": ["D. Chen", "W.J. Wang"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "A weighted Nearest Neighbor Algorithm for learning with symbolic features", "author": ["S. Cost", "S. Slazberg"], "venue": "Machine learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "An experimental and theoretical studies of the effect of simulated pitch line pitting on the vibration of a geared system, ASME publication report", "author": ["M.J. Drosjack", "D.R. Houser"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1977}, {"title": "Artificial neural network based fault diagnostics of rotating machinery using wavelet transform as a preprocessor", "author": ["B.A. Paya", "I.I. East", "M.N.M. Badi"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Induction of Decision Trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1986}, {"title": "Airborne Acoustic Condition Monitoring, www.maintenenceengineering.com", "author": ["G. Ramroop", "K. Liu", "F. Gu", "Paya B. S", "A.D. Ball"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Gear fault diagnosis using artificial neural networks and support vector mechanics with genetic algorithms", "author": ["B. Samanta"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "One class support vector machines \u2013 an application in fault detection and classification", "author": ["Shin", "H.J", "D.H. Eom", "S.S. Kim"], "venue": "Computer and Industrial Engineering,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Time-frequency analysis in gearbox fault detection using the Wigner-ville distribution and pattern recognition", "author": ["W.J. Staszewski", "Worden. K", "G.R. Tomlinson"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Feature selection using Decision Tree and classification through Proximal Support Vector Machine for fault diagnostics of roller bearing", "author": ["V. Sugumaran", "V. Muralidharan", "K.I. Ramachandran"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Fault Diagnosis of Helical Gear Box using Decision Tree through Vibration", "author": ["V. Sugumaran", "J. Deepak", "M. Amarnath", "Hemanthakumar"], "venue": "Signals, International Journal of Performability Engineering,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Artificial neural network design for fault identification in a rotor-bearing system", "author": ["N.S. Vyas", "D. Satishkumar"], "venue": "Mechanism and Machine Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "Saul L.K. Blitzer J. C"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Third order spectral technique for the diagnosis of motor bearing conditions using artificial neural network", "author": ["M. Yang", "A.F. Stronachand", "P. McConnel"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Condition classification of small reciprocating compressor for refrigerators using artificial neural networks and support vector machines", "author": ["B.S. Yang", "W.W. Hwang", "D.J. Kim", "A.C. Tan"], "venue": "Mechanical Systems and Signal Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}], "referenceMentions": [{"referenceID": 8, "context": "Hence, fault diagnosis of such machine elements has been the subject matter of extensive research (Drosjack and Houser [9] ).", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "[5], Samanta [14] , Chen and Wang [7] and Paya et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[5], Samanta [14] , Chen and Wang [7] and Paya et al.", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "[5], Samanta [14] , Chen and Wang [7] and Paya et al.", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "[10] ) reveal the application of neural networks to online condition monitoring of rotating machinery to have very high success rates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] carried out investigations to study both bearing and gear faults introduced separately as a single fault and then together as multiple faults in the drive line.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Baydar and Ball [4] demonstrated the results of fault diagnosis experiments conducted on two stage helical gearbox.", "startOffset": 16, "endOffset": 19}, {"referenceID": 17, "context": "Vyas and Satishkumar [19] carried out experiments to automate the fault detection procedure in rotating machinery.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "Chen and Wang [7] dealt with multi layer perceptron (MLP) pattern classifiers for wavelet map interpretation and their application as a tool for mechanical fault detection.", "startOffset": 14, "endOffset": 17}, {"referenceID": 11, "context": "[13] conducted experimental investigation to detect faults in multistage industrial gearbox, sound signals were acquired from the gearbox under near field condition, and fast Fourier transform (FFT) method used extract fault related features from these signals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Samanta [23] presented an experimental study to compare the performance of gear fault detection and classification using ANN and SVM.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "[23] presented a novel scheme to detect faults in reciprocating compressors of refrigerators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] adopted SVM technique for detection and classification of faults in electro mechanical machinery using vibration parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] employed proximal support vector machines (PSVM) and SVM to classify faults in bearings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] carried out experiments to detect faults in ball bearing and gears using sound signals.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] used acoustic signals acquired from near field area of bearings in good and simulated faulty conditions for the purpose of fault diagnosis through machine learning approach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[18] used vibration signals acquired from gears in good and simulated faulty conditions for the purpose of fault diagnosis", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "A modified version of the pseudo code given by Bremer et al [6] along with modifications proposed by Cost and Salzberg [8] is used in this study in order to train and test the classifier.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "A modified version of the pseudo code given by Bremer et al [6] along with modifications proposed by Cost and Salzberg [8] is used in this study in order to train and test the classifier.", "startOffset": 119, "endOffset": 122}, {"referenceID": 0, "context": "[1] ).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Different data sets were collected when the helical gear train was working at normal, 10%, 20%, 40%, 60%, 80% and 100% tooth removal conditions (Badyar and Ball [4] ).", "startOffset": 161, "endOffset": 164}, {"referenceID": 14, "context": "[16] and Yesilyurt et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "The large Margin K-nearest neighbor algorithm is a variant of the original K-nearest neighbour algorithm for classification [Bremer et al [6] ].", "startOffset": 138, "endOffset": 141}, {"referenceID": 18, "context": "[20] is used.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Moreover while picking the minimum distance incorporate the following weight measure (Cost and Salzberg [8] ),", "startOffset": 104, "endOffset": 107}, {"referenceID": 10, "context": "The dataset is used with decision tree J48 algorithm for generating the decision tree for the purpose of feature selection (Quinlan [12] ).", "startOffset": 132, "endOffset": 136}], "year": 2013, "abstractText": "Gear drives are one of the most widely used transmission system in many machinery. Sound signals of a rotating machine contain the dynamic information about its health conditions. Not much information available in the literature reporting suitability of sound signals for fault diagnosis applications. Maximum numbers of literature are based on FFT (Fast Fourier Transform) analysis and have its own limitations with non-stationary signals like the ones from gears. In this paper, attempt has been made in using sound signals acquired from gears in good and simulated faulty conditions for the purpose of fault diagnosis through a machine learning approach. The descriptive statistical features were extracted from the acquired sound signals and the predominant features were selected using J48 decision tree technique. The selected features were then used for classification using Large Margin K-nearest neighbor 2 approach. The paper also discusses the effect of various parameters on classification accuracy.", "creator": "Microsoft\u00ae Word 2010"}}}