{"id": "1508.06717", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2015", "title": "Online Anomaly Detection via Class-Imbalance Learning", "abstract": "Anomaly adaptive is 's purpose action in sometimes it world access such as scandal detection, suspicious activity seismic, health families communication civitates. In means pointed, really tackle another solve from conduct instruction metaphor over data learning creating. We meaningful their usually \\ emph {Gmean} 350,000 for men - worsens developed in subscriptions learning outlined. Specifically, keep comes that appreciable \\ emph {Gmean} whole weight meant appreciable turn convex surrogate recover function with based on that we follow poem clients writing coding make anomaly detection. We while show, by techniques feasibility, that with performance of the provision algorithm to any once $ sum $ 9,000 always as whatever turn, early proposed Cost - Sensitive Online Classification (CSOC) algorithm which currently - imbalance learning again similar side-scrolling data sets that but right takes next one the perception algorithm. Our any conclusion is believe particular good publishers measurement do not can consistently when electronic sets of respective light. This show making future infallibility taken come suggested beyond.", "histories": [["v1", "Thu, 27 Aug 2015 03:39:39 GMT  (1150kb,D)", "http://arxiv.org/abs/1508.06717v1", "This paper is accepted for publication in IC3 2015, Jaypee Noida"]], "COMMENTS": "This paper is accepted for publication in IC3 2015, Jaypee Noida", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chandresh kumar maurya", "durga toshniwal", "gopalan vijendran venkoparao"], "accepted": false, "id": "1508.06717"}, "pdf": {"name": "1508.06717.pdf", "metadata": {"source": "CRF", "title": "Online Anomaly Detection via Class-Imbalance Learning", "authors": ["Chandresh Kumar Maurya", "Durga Toshniwal", "Gopalan Vijendran Venkoparao"], "emails": ["durgatoshniwal@gmail.com", "GopalanVijendran.Venkoparao@in.bosch.com"], "sections": [{"heading": null, "text": "Index Terms\u2014Class-Imbalance Learning, Online learning, Anomaly detection.\nI. INTRODUCTION\nAnomaly detection aims to capture behavior in data that do not conform to the normal behavior as expected by domain expert [1]. Anomaly detection in online setting is an important task in many real world applications. For example, intrusion detection in computer network, flight navigation system, credit card fraud detection and so on. It is clear that such task require detection of malicious activity on the fly. However, most existing techniques focus on offline training of the model and then use it to detect anomalies [2], [3], [4], [5].\nIn this paper, we tackle anomaly detection problem from supervised learning perspective in online setting. In supervised learning framework, anomaly detection refers to correctly classifying rare class examples as compared to majority examples. For example, out of 100 persons visiting a doctor for cancer check up, saying a person, who actually have the cancer, not having cancer is more costly than saying he has the cancer when he actually does not. Therefore, we take anomaly detection problem as class-imbalance learning problem. Hence from now and onwards, we will use class imbalance learning to refer to anomaly detection.\nIn what follows, we present related work in section 2. Section 3 is devoted to problem formulation in online learning setting and online algorithm for class-imbalance learning. Experimental results are presented in section 4 and finally section 5 concludes the paper."}, {"heading": "II. RELATED WORK", "text": "Work presented in this paper spans two main themes in data mining and machine learning: Online learning and classimbalance learning. Although there have been many works in both domain separately [6], [7], little work has been done that jointly solves online learning and class-imbalance learning. Below we briefly describe work in each domain that closely matches our work."}, {"heading": "A. Online learning", "text": "Online learning aims to process one example at a time. First, it receives an examples and then makes a prediction. If prediction goes wrong, it suffer loss and updates its parameters. Online learning has its origin from classic work of Rosenblatt on perceptron algorithm [8]. Perceptron algorithm is based on the idea of single neuron. It simply takes an input instance and learn a linear predictor of the form f(w) = wTx, where w is weight vector and x is the input instance. If it makes a wrong prediction, it updates its parameter vector as follows:\nwt+1 = wt + ytxt (1)\nwhere wt+1 is weight vector at time t+ 1. [9] proposed online learning with kernels. Their algorithm, called NORMA\u03bb, is based on regularized empirical risk minimization which they solved via regularized stochastic gradient descent. They also showed empirically how this can be used in anomaly detection scenario. However, Their algorithm requires tuning of many parameters which is costly for time critical applications. Passive-Aggressive (PA) learning [10] is another online learning algorithm based on the idea of maximizing \u201cmargin\u201d in online learning framework. PA algorithm updates the weight vector whenever \u201cmargin\u201d is below a certain threshold on the current example. Further, the same author have introduced the idea of slack variable to handle non-linearly separable data.XXX/YY/$31.00 c\u00a92015 IEEE\nar X\niv :1\n50 8.\n06 71\n7v 1\n[ cs\n.L G\n] 2\n7 A\nug 2\n01 5"}, {"heading": "B. Class-Imbalance Learning", "text": "Class-imbalance learning aims to correctly classify minority examples. In literature, there exist solutions that are either based on the idea of sampling or weighting scheme. In the former case, either majority examples are under sampled or minority examples are over sampled. In the latter case, each example is weighted differently and idea is to learn these weights optimally. Some examples of sampling based techniques are SMOTE [6], SMOTEBoost [7], AdaBoost.NC [11] and so on. Work that used weighting scheme include cost-sensitive learning [12], [13], [14], [15] and so on.\nIt is worthwhile to mention here that only a few work exists that jointly solves class-imbalance learning and online learning. Below, we mention some work that closely matches our work. In [16], author proposed sampling with online bagging (SOB) for class imbalance detection. Their idea essentially is based on resampling. That is, oversample minority class and under sample majority class from Poisson distribution with average arrival rate of N/P and Rp respectively where P is total number of positive examples, N is total number of negative examples, and Rp is recall on positive examples. [16] also tries to maximize Gmean, but the way they approached to solve the maximization problem is different from our present work. [17] proposed online cost sensitive classification of imbalanced data. One of their problem formulation is based on maximizing weighted sum of sensitivity and specificity and the other is minimizing weighted cost. Their solution is based on minimizing convex surrogate loss function (modified hinge loss) instead of non-convex 0-1 loss. [17] work closely matches our work. But, in section 3 we show the major difference between the two work."}, {"heading": "III. FRAMEWORK OF CLASS-IMBALANCE LEARNING", "text": ""}, {"heading": "A. Problem formulation", "text": "Without loss of generality, consider binary classification problem. Formally, let X be instance space in Rd and Y be label space in {\u22121,+1}. We are given samples S = {(x1, y1), (x2, y2), ..., (xT , yT )} where instance-label pair (xi, yi) \u2208 X\u00d7Y , and i \u2208 {1, 2, . . . , T}. In online learning setting, no assumption is made about the distribution of the samples and they come sequentially . Let xt be an instance received at time step t and ft be the model that is obtained from previous t \u2212 1 rounds. Let y\u0302 be the prediction for the t-th instance i.e y\u0302 = sign(ft(xt)), whereas the value |ft(xt)| known as \u201dmargin\u201d, is used as the confidence of the learner on the t-th prediction step.\nFor binary classification task, let P and N respectively denote the number of positive and negative instances received so far. Let TP, TN,FP and FN denote number of true positive, true negative, false positive and false negative so far. Mathematically, they are defined as: TPt = {y = y\u0302 = +1}, TNt = {y = y\u0302 = \u22121}, FPt = {y = \u22121, y\u0302 = +1}, FNt = {y = +1, y\u0302 = \u22121} where subscript t denote these metric value at that time step. TP is calculated by summing TPt from t = 1 to T . Similarly TN , FP and FN can be calculated.\nWithout loss of generality, assume positive instances are minority class. It is well known that maximizing accuracy as a measure of performance on class-imbalanced data leads to false conclusion. For example, suppose training data contains 100 examples having 95 negative examples and 5 positive examples. If a classifier predicts each example as negative it will have accuracy of 100%. Thus missing all the positive examples to classify correctly.\nTherefore, we require a metric that can account for classifying minority class correctly. Gmean [18] is such a metric that evaluates the degree of inductive bias in terms of a ratio of positive accuracy and negative accuracy. Mathematically, Gmean is defined as:\nGmean = \u221a recall+ \u00d7 recall\u2212 (2)\nwhere recall+ and recall\u2212 denote accuracy on positive and negative examples respectively and defined as:\nrecall+ = TP\nTP + FN , recall\u2212 =\nTN\nTN + FP\nFor simplicity, we assume that \u2016xt\u2016 \u2264 1 which says that all incoming instances lies within a unit ball. However, this restriction can be relaxed in a more general setting, that is, we can take \u2016xt\u2016 \u2264 r, where r is radious of the Euclidean ball centered at 0. This condition ensures that cumulative regret, that is, performance of online learner with respect to a fixed learner chosen in hindsight, can be bounded. Our objective is to maximize (??).\nSuppose we are given a linear classifier of the form f(x) = wTx, where w represents the weight of the linear classifier that we wish to learn in an online manner. Whenever classifier makes a mistake i.e yf(x) < 0, we update the weight. How weights are actually updated will be explained in the following section. B. Online Setting\nIn this subsection, we prove the following lemma due to [16] for the sake of completeness. Then we show the hardness of optimizing the equivalent formulation in the lemma and propose to optimize a convex surrogate loss function instead.\nLemma 1: Maximizing Gmean as given in (??) is equivalent to minimizing the following objective:\u2211\nyt=+1\nN P I(ytft(xt)<0) + \u2211 yt=\u22121 P \u2212 FN P I(ytft(xt)<0) (3)\nProof: Maximizing (??) is equivalent to maximizing its square. So we can write it as follows:\nGmean =\n( TP\nTP + FN\n) \u00d7 (\nTN\nTN + FP ) = ( P \u2212 FN\nP\n) \u00d7 ( N \u2212 FP\nN ) = ( 1\u2212 FN\nP\n) \u00d7 ( 1\u2212 FP\nN ) = 1\u2212 ( FN\nP + FP N \u2212 FN.FP P.N\n)\nwhere in step 2, we used the fact that P = TP + FN and N = FP + TN . Hence we minimize:\nGmean = FN P + FP N \u2212 (FN)(FP ) PN\n=\n( N\nP\n) FN + ( P \u2212 FN\nP\n) FP\n= \u2211 yt=+1 N\nP I(ytft(xt)<0)+\u2211\nyt=\u22121\nP \u2212 FN P I(ytft(xt)<0)\n(4)\nwhere in step 2, we have taken off the constant N from the equation 1 without changing the optima and I(C) is an indicator function that outputs 1 when argument C is true and 0 otherwise. .\nLemma 1 gives an alternative objective to maximize Gmean. But, it involves indicator function that is non-convex. Hence, we resort to convex relaxation techniques. More specifically, we use the following convex surrogate loss function:\nL(w; (x, y)) = max (0, \u03c1\u2212 yft(xt)) (5)\nwhere\n\u03c1 =\n(( N\nP\n) I(y=1) + ( P \u2212 FN\nP\n) I(y=\u22121) ) We see that (??) is similar to modified hinge loss function. Our next goal is to put the modified loss function into online learning framework. In online learning, learner receives one example at a time and makes it prediction. If it goes wrong, it updates it prediction function f . Here we will cast this problem into empirical risk minimization(ERM) learning framework. In offline learning under ERM framework, measure of quality of f is expected risk [9]\nRemp[f, S] = E(x,y)\u223cP [(y\u0302), y] (6)\nSince distribution P is unknown in general, one instead minimizes empirical risk\nRemp[f, S] = 1\nT T\u2211 t=1 L(f(xt), yt) (7)\nOne can further regularizes empirical risk to avoid overfitting.\nRemp[f, S] = 1\nT T\u2211 t=1 L(f(xt), yt) + \u03bb 2 ||f ||2 (8)\nwhere \u03bb is a regularization parameter that controls tradeoff between complexity of the model and correctness of the prediction.\n1It can be shown that problem formulation presented in this paper is similar to one presented in [17] but not equivalent since they were minimizing weighted sum of sensitivity and specificity where weights are decided by Laplace estimation. In our formulation, we have directly estimated these weights in terms of P,N , and FN .\nAlgorithm 1: Online G-mean(OGMEAN) algorithm Input: Learning rate \u03c4 Initialization w1 = 0 Output: Weight vector wT+1 for t = 1, . . . , T do\nreceive instance: xt; predict :y\u0302t = sign(wt.xt); receive correct label: yt \u2208 {\u22121,+1} ; suffer loss: Lt(wt) as given in (??); if Lt(wt) > 0 then\nwt+1 = wt + \u03c4ytxt ; else\nwt+1 = wt ; end\nend\nSince we are interested in online learning, we define an instantaneous risk [9] of using prediction function ft on example (xt, yt) as follows:\nRemp[ft; (xt, yt)] = L(ft(xt), yt) + \u03bb\n2 ||ft||2 (9)\nNext, we need to determine how our model is performing over sequence of inputs during online learning. For that, cumulative mistake is often used and defined as:\nLCUM [ft, T ] = T\u2211 t=1 L(ft(xt), yt) (10)\nWe note here that ft is tested on an example (xt, yt) which was not used for fitting ft. Thus, if we can guarantee low cumulative mistake, we can prevent overfitting without the need of regularizer [9]. We used this fact in our current work and do not provide explicit regularization parameter.\nThus our objective is to minimize (??). To this end, we use online gradient descent [19] to optimize (??).\nwt+1 = wt \u2212 \u03c4\u2207Lt(wt) (11)\nwhere \u2207Lt is loss incurred at time t on example xt. To find \u2207Lt for our loss function (??), we differentiate it with respect to wt and update rules becomes:\nwt+1 = { wt + \u03c4ytxt if Lt(wt) > 0 wt otherwise\n(12)"}, {"heading": "C. Algorithm", "text": "Our proposed algorithm Online G-MEAN (OGMEAN) is given in Algorithm 1. As we can see that OGMEAN requires only one parameter to tune; \u03c4 , the learning rate. In general it is set to 1/ \u221a t. However, for simplicity we set it to a constant in our experiments. In is clear that OGMEAN takes time proportional to O(T \u00d7 n), which is linear in the number of dimension of the input instances as well as number of received instances so far."}, {"heading": "D. Relative Loss Bound for OGMEAN", "text": "Following [17], we can bound the regret of OGMEAN algorithm. Below we just state the lemma without proof.\nLemma 2: Let S = {(xt, yt)}t=1,...,T be the sequence of T examples where xt \u2208 X , yt \u2208 Y and ||xt|| \u2264 1 for all t. Then for any w \u2208 X , by setting \u03c4 = ||w|| \u221a T , the following holds for OGMEAN:\nT\u2211 t=1 Lt(wt) \u2264 T\u2211 t=1 Lt(w) + ||w|| \u221a T ."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we experimentally validated the accuracy of the proposed OGMEAN algorithm over various bench marked data set which can be freely downloaded from LIBSVM website2. A brief summary of the data set and class-imbalance ratio is given in Table I. All the algorithms were run in MATLAB 2012a (64 bit version) on 64 bit Windows 8.1 machine. We compare our algorithm with recently proposed CSOCsum algorithm [17] with respect to a metric called sum which is defined as:\nsum = np \u00d7 sensitivity + nn \u00d7 specificity (13)\nwhere np and nn are weight parameter which in [17] are set manually to 0.5 each and sensitivity and specificity are the same as recall on positive and negative examples respectively. It is claimed in [17] that CSOCsum algorithm beats stateof-the-art online algorithms for class-imbalance problem with respect to sum metric. The algorithms compared in [17] are Perceptron [8], ROMMA, agg-ROMMA, PA-I, PA-II, CPAPB [10] and PAUM [20]. Since CSOCsum performs equal or better than all the above algorithms, we only compare OGMEAN to CSOCsum with respect to sum metric in this paper. We also show the mistake rate, number of updates and running time of all the algorithms for fair comparison. For this purpose, we used the LIBOL online learning library 3."}, {"heading": "A. Evaluation of Weighted Sum Performance", "text": "We conducted a comparative study of CSOCsum and OGMEAN algorithms where we set learning rate parameter \u03c4 equals to 0.2 and weights for \u201csum\u201d i.e np and nn equal to 0.5 over all the data sets and both algorithms as done in CSOCsum. Online \u201csum\u201d performance of the two algorithms are shown in ??????, 2 and Table II over four data sets: \u201cgerman\u201d, \u201cijcnn1\u201d,\u201csvmguide3\u201d, and \u201ccovtype\u201d. We can draw\n2http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets 3http://stevenhoi.org\nseveral conclusions from these results. First, we can see in ?????? that OGMEAN achieves equal or higher \u201csum\u201d value as compared to CSOCsum over all the data sets. It shows the potential drawback in using Laplace estimation to estimate \u03c1 in CSOCsum algorithm. From the Table II, we infer that OGMEAN beats CSOCsum algorithm in terms of Mistake rate, No of updates applied on weights and CPU time on almost all the data sets. It again justifies the potential benefits of using OGMEAN in real world applications."}, {"heading": "B. Comparative Study", "text": "We conducted another experiment for comparing mistake rate, cumulative number of updates and cumulative time cost over \u201ccovtype\u201d and \u201cgerman3\u201d data sets. For all the algorithms compared, cost parameter C is set to 1 except ALMA for which the value of C is \u221a 2. We kept the values of all other parameters same as given in LIBOL implementation. From ??, we observe that both CSOCsum and OGMEAN has cumulative running time approaching perception algorithm. In terms of the number of mistakes, both CSOCsum and OGMEAN outperform all other algorithms. On the other hand, in ??, we observe that both CSOCsum and OGMEAN did not do well in comparison to SCW-I, SCW-II, NHERD, and ALMA in terms of the number of mistakes. However, CSOCsum and OGMEAN took less time as compared to all other algorithms except PERCEPTRON.\nIn addition, we also tested comparative performance of all the algorithms over other bench marked data sets (\u201cijcnn1\u201d, \u201csvmguide3\u201d) where we found that performance of CW, SCWI, and SCW-II was better than all other algorithms. However, result presented in this paper indicates that aforementioned algorithms are not so well performing on \u201cgerman\u201d and \u201ccovtype\u201d as compared to CSOCsum and OGMEAN. This leads us to a conclusion that there is lack of consistency of performance in terms of the number of mistakes made by these algorithms except CSOCsum and OGMEAN."}, {"heading": "V. CONCLUSION", "text": "In the present work, we tackled binary class imbalance learning under online learning framework. We maximize popular Gmean metric for class imbalance problem. We showed\nthat maximizing Gmean equivalent formulation is non-convex and hence used convex surrogate loss function under empirical risk minimization framework. We compared our OGMEAN algorithm performance with recently proposed CSOCsum algorithm over various bench marked data sets. It is found that directly optimizing weighted sum of sensitivity and specificity where weights are learned using Laplace estimation techniques is less efficient as compared to directly optimizing equivalent formulation of maximizing Gmean. We also showed mistake rate, cumulative time cost and number of updates of our algorithm with respect to many other online learning algorithms and concluded that its performance is as good as or better than these recent online algorithms.\nIn our future work, we plan to extend the work to multi-class setting. Concretely, how can we optimize Gmean metric for multi-class in online scenario? The problem is that Gmean for multi-class is non-decomposable loss function that prohibits us to use any existing optimization techniques. So, further work is required in this direction."}, {"heading": "ACKNOWLEDGMENT", "text": "This research work is supported by the Prime Ministers Fellowship for Doctoral Research, joint initiative by Confederation of Indian Industry (CII) and industry partner Robert Bosch."}], "references": [{"title": "Anomaly detection: A survey,", "author": ["B.A.V. Chandola", "V. kumar"], "venue": "University of Minnesota, Tech. Rep.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Lof: Identifying density-based local outliers,", "author": ["M. Breunig", "H.P. Kriegel", "R.T. Ng", "J. Sander"], "venue": "Proceedings of the 2000 ACM SIGMOD international conference on management of data", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Toward supervised anomaly detection.", "author": ["N. Goernitz", "M. Kloft", "K. Rieck", "U. Brefeld"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Smote: Synthetic minority over-sampling technique,", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Smoteboost: Improving prediction of the minority class in boosting.", "author": ["N.V. Chawla", "A. Lazarevic", "L.O. Hall", "K.W. Bowyer"], "venue": "PKDD, ser. Lecture Notes in Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Neurocomputing: Foundations of research,", "author": ["F. Rosenblatt"], "venue": "Psychological Review, Nov 1958,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1958}, {"title": "Online learning with kernels,", "author": ["J. Kivinen", "A. Smola", "R. Williamson"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Online passive-aggressive algorithms,", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Negative correlation learning for classification ensembles,", "author": ["S. Wang", "H. Chen", "X. Yao"], "venue": "Neural Networks (IJCNN), The 2010 International Joint Conference on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "The foundations of cost-sensitive learning,", "author": ["C. Elkan"], "venue": "Proceedings of the 17th International Joint Conference on Artificial Intelligence - Volume 2, ser. IJCAI\u201901", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Test strategies for cost-sensitive decision trees,", "author": ["C. Ling", "V. Sheng", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "The influence of class imbalance on costsensitive learning: An empirical study,\u201d in ICDM \u201906", "author": ["X.-Y. Liu", "Z.-H. Zhou"], "venue": "Sixth International Conference on Data Mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Cost-sensitive multilabel learning for audio tag annotation and retrieval,", "author": ["H.-Y. Lo", "J.-C. Wang", "H.-M. Wang", "S.-D. Lin"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Online class imbalance learning and its applications in fault detection,", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "International Journal of Computational Intelligence and Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Cost-sensitive online classification,", "author": ["J. Wang", "P. Zhao", "S. Hoi"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Addressing the curse of imbalanced training sets: One-sided selection,", "author": ["M. Kubat", "S. Matwin"], "venue": "Proceedings of the Fourteenth International Conference on Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Online convex programming and generalized infinitesimal gradient ascent,", "author": ["M. Zinkevich"], "venue": "Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "The perceptron algorithm with uneven margins,", "author": ["H. Li", "H. Zaragoza", "R. Herbrich", "J. Shawe-Taylor", "J. Kandola"], "venue": "Proceedings of the Nineteenth International Conference on Machine Learning, ser. ICML \u201902,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Anomaly detection aims to capture behavior in data that do not conform to the normal behavior as expected by domain expert [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "However, most existing techniques focus on offline training of the model and then use it to detect anomalies [2], [3], [4], [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "However, most existing techniques focus on offline training of the model and then use it to detect anomalies [2], [3], [4], [5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "Although there have been many works in both domain separately [6], [7], little work has been done that jointly solves online learning and class-imbalance learning.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "Although there have been many works in both domain separately [6], [7], little work has been done that jointly solves online learning and class-imbalance learning.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "Online learning has its origin from classic work of Rosenblatt on perceptron algorithm [8].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "[9] proposed online learning with kernels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Passive-Aggressive (PA) learning [10] is another online learning algorithm based on the idea of maximizing \u201cmargin\u201d in online learning framework.", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "Some examples of sampling based techniques are SMOTE [6], SMOTEBoost [7], AdaBoost.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Some examples of sampling based techniques are SMOTE [6], SMOTEBoost [7], AdaBoost.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "NC [11] and so on.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Work that used weighting scheme include cost-sensitive learning [12], [13], [14], [15] and so on.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Work that used weighting scheme include cost-sensitive learning [12], [13], [14], [15] and so on.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Work that used weighting scheme include cost-sensitive learning [12], [13], [14], [15] and so on.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "Work that used weighting scheme include cost-sensitive learning [12], [13], [14], [15] and so on.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "In [16], author proposed sampling with online bagging (SOB) for class imbalance detection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "[16] also tries to maximize Gmean, but the way they approached to solve the maximization problem is different from our present work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] proposed online cost sensitive classification of imbalanced data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] work closely matches our work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Gmean [18] is such a metric that evaluates the degree of inductive bias in terms of a ratio of positive accuracy and negative accuracy.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "Online Setting In this subsection, we prove the following lemma due to [16] for the sake of completeness.", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "In offline learning under ERM framework, measure of quality of f is expected risk [9]", "startOffset": 82, "endOffset": 85}, {"referenceID": 14, "context": "1It can be shown that problem formulation presented in this paper is similar to one presented in [17] but not equivalent since they were minimizing weighted sum of sensitivity and specificity where weights are decided by Laplace estimation.", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "Since we are interested in online learning, we define an instantaneous risk [9] of using prediction function ft on example (xt, yt) as follows:", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "Thus, if we can guarantee low cumulative mistake, we can prevent overfitting without the need of regularizer [9].", "startOffset": 109, "endOffset": 112}, {"referenceID": 16, "context": "To this end, we use online gradient descent [19] to optimize (??).", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Relative Loss Bound for OGMEAN Following [17], we can bound the regret of OGMEAN algorithm.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "We compare our algorithm with recently proposed CSOCsum algorithm [17] with respect to a metric called sum which is defined as:", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "sum = np \u00d7 sensitivity + nn \u00d7 specificity (13) where np and nn are weight parameter which in [17] are set manually to 0.", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "It is claimed in [17] that CSOCsum algorithm beats stateof-the-art online algorithms for class-imbalance problem with respect to sum metric.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "The algorithms compared in [17] are Perceptron [8], ROMMA, agg-ROMMA, PA-I, PA-II, CPAPB [10] and PAUM [20].", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "The algorithms compared in [17] are Perceptron [8], ROMMA, agg-ROMMA, PA-I, PA-II, CPAPB [10] and PAUM [20].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "The algorithms compared in [17] are Perceptron [8], ROMMA, agg-ROMMA, PA-I, PA-II, CPAPB [10] and PAUM [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "The algorithms compared in [17] are Perceptron [8], ROMMA, agg-ROMMA, PA-I, PA-II, CPAPB [10] and PAUM [20].", "startOffset": 103, "endOffset": 107}], "year": 2015, "abstractText": "Anomaly detection is an important task in many real world applications such as fraud detection, suspicious activity detection, health care monitoring etc. In this paper, we tackle this problem from supervised learning perspective in online learning setting. We maximize well known Gmean metric for classimbalance learning in online learning framework. Specifically, we show that maximizing Gmean is equivalent to minimizing a convex surrogate loss function and based on that we propose novel online learning algorithm for anomaly detection. We then show, by extensive experiments, that the performance of the proposed algorithm with respect to sum metric is as good as a recently proposed Cost-Sensitive Online Classification(CSOC) algorithm for class-imbalance learning over various benchmarked data sets while keeping running time close to the perception algorithm. Our another conclusion is that other competitive online algorithms do not perform consistently over data sets of varying size. This shows the potential applicability of our proposed approach.", "creator": "LaTeX with hyperref package"}}}