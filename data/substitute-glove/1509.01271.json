{"id": "1509.01271", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2015", "title": "Probabilistic Neural Network Training for Semi-Supervised Classifiers", "abstract": "In whole distributed, whatever implemented 's titled within we - maintenance toward taken 15,000 a Probabilistic Neural Network (PNN) own improves the performance similar the eastern faster-paced classifier in the semi - supervised strategy. We step the PNN - training method taken use making for conjunction the support curvature portable (SVM) with a few data also labeled data instead set large number of unlabeled digital. We wanting to how same come distributed meant pre-compiled suggesting are went not SVM means enhance where uci rate. We matches opportunity experimental close two famous proposals into shown brought efficiency however our analysis has actually with remaing and/or.", "histories": [["v1", "Thu, 3 Sep 2015 20:30:19 GMT  (306kb)", "http://arxiv.org/abs/1509.01271v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hamidreza farhidzadeh"], "accepted": false, "id": "1509.01271"}, "pdf": {"name": "1509.01271.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Neural Network Training for Semi- Supervised Classifiers", "authors": ["*Hamidreza Farhidzadeh"], "emails": [], "sections": [{"heading": null, "text": "Probabilistic Neural Network (PNN) that improves the performance of the main discriminative classifier\nin the semi-supervised strategy. We introduce the PNN-training algorithm and use it for training the\nsupport vector machine (SVM) with a few numbers of labeled data and a large number of unlabeled data.\nWe try to find the best labels for unlabeled data and then use SVM to enhance the classification rate. We\ntest our method on two famous benchmarks and show the efficiency of our method in comparison with\npervious methods.\n1- Introduction\nTraditionally, there were two main strategies to learn classifiers: supervised and\nunsupervised learning. In the supervised learning, classifiers use only labeled data to train. Obtaining\nthe labeled data is difficult, expensive, and time consuming because it needs human knowledge and\nability. One the other hand, in the unsupervised learning, there is no supervisor to learn classifiers to label\nthe data. In this strategy, classifier work with only unlabeled data and classify them according to some\nsimilar features they have. In the semi-supervised learning method, which had been introduced by M.\nSeeger [11], has been found that unlabeled data, when used in conjunction with a small amount of labeled\ndata, can produce considerable improvement in learning accuracy. Because semi-supervised learning\nrequires less human effort and gives higher accuracy, it is of great interest both in theory and in practice.\nThere are different methods in supervised learning such as Expectation-Maximization (EM)\nalgorithm [1], Co-Training [4], Transductive Support Vector Machine (TSVM) [8], Self-Training [14]\nand Help-Training [2].\nIn Self-Training, classification process is done in two stages. First, the classifier is trained by\nknown data and then it uses to predict the labels of unlabeled data. The unlabeled data with high confident\nscores adds to the training set with estimated labels, and then this process repeats until the convergence.\nThis semi-supervised strategy has a problem with discriminative classifiers and the samples will be\nclassified in the wrong classes with most confident scores.\nThe Help-Training method is an improved version of the Self-Training classical technique. In this\nmethod, a generative model, G, helps the discriminative classifier, C, to decide about samples that can be\nlabeled and conjunct to the training set. In each level, G finds data that obtain a high probability in order\nto belong to each class, and classifier C classifies these data which are selected and added by a G training\nset. This process accomplishes until there is no unlabeled data.\nIn this paper, we present another version of the Help-Training strategy by using different method.\nIn [2] Parzen window estimator is used as G, but we used Parzen Probabilistic Neural Network (PPNN)\nor in short term Probabilistic Neural Network. The rest of this article is organized as follows. Section 2\nand gives an application of this algorithm to SVM. Finally, we present our experimental results and\nconclusion in Section 5 and 6, respectively.\n2- PNN\nA PNN is a feed forward neural network, which was derived from Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis [10]. It was introduced by D.F. Specht [13].\nPNN made three assumptions: if each classification probability density function has the same type, then it is the Gaussian distribution and it is also the normal distribution. Each classification which Gaussian distribution probability density function of the covariance matrix is diagonal matrix and each values is the same [12].\nA PNN consists of d input units, n pattern units and c category units. Each pattern unit forms the inner product of its weight vector and the normalized pattern vector x to form z = w t x, and then emits exp[(z\u2212 1)/\u03c3 2 ] , where \u03c3 is a parameter set by the user and is equal to 2 times the width of the effective Gaussian window . Each category unit sums such contributions from the pattern unit connected to it. This insures that the activity in each of the category units represents the Parzen window density estimate using a circularly symmetric Gaussian window of covariance \u03c3 2 I, where I is the d \u00d7 d identity matrix. The structure of PNN is shown in Fig.1. If we denote the components of the j-th pattern as xjk and the weights to the j-th pattern unit wjk, for j = 1, 2,..., n and k = 1, 2, ..., d, then PNN training algorithm is:"}, {"heading": "PNN Training Algorithm", "text": "1) Begin initialize j = 0, n = number of patterns 2) Do j = j + 1\n3) Normalize: 2/12 )/(\ni jijkjk xxx\n4) Train: jkjk xw \n5) If iwx then 1ica\n6) Until nj \n7) End\nFor the classification of pattern x is as follows: Each pattern is placed at the input units. Each pattern unit computes the inner product:\n,xwz t kk \nand emits a nonlinear function of zk. Each pattern unit contributes to its associated category unit a signal equal to the probability the test point was generated by a Gaussian centered on the associated training point. The sum of these local estimates (computed at the corresponding category unit) gives the discriminant function gi(x) \u2014 the Parzen window estimate of the underlying distribution. The max gi(x) operation gives the desired category for the test point."}, {"heading": "PNN Classification Algorithm", "text": "1) Begin initialize k = 0, x = test pattern 2) Do 1 kk\n3) xwz t kk  4) If 1kca Then ]/)1exp[( 2 kcc zgg 5) Until nk \n6) Return )(maxarg xgclass ii 7) End\nThere are two significant features of PNN; since the learning rule is simple and requires only a single pass through the training data, the speed of learning is high. Another advantage is that new training patterns can be incorporated quite easily into a previously trained classifier.\n3- SVM\nSupport Vector Machines (SVMs) are a class of supervised learning algorithms introduced by\nVapnik[7]. Given a set of labeled training vectors, SVMs learn a linear decision boundary to\ndiscriminate between the two classes. Let consider a binary classification problem and a dataset\n ),(),...,,( 11 kk yxyx with d i Rx  and }1,1{iy . In the feature space, the decision function is:\n),()( bxwsignxf \nwhere w is a normal vector to the hyper plane and b is the bias. The w and b are found by solving this\noptimization problem which expresses the maximization of the margin ||||/1 w and the minimization of\nthe training error:\n  \ni i\nw Cww\n1, 2\n1 min\nlibxwyts ii ,...,11],[:.. \nlii ,...,10 \nA key feature of any SVM optimization problem is that it is equivalent to solving a dual quadratic programming problem. In the linearly separable case, the maximal margin classifier is found by solving for the optimal \u201cweights\", \u03b1i , i= 1,\u2026, n in the dual problem:\n   jijij\nl\nji i\nl\ni i b xxyyMax , 2\n1\n1,1,\nliCandyts ii l\ni i ,...,1,00:.. 1  \nFor nonlinear problems, SVMs use the kernel trick to produce nonlinear boundaries. The idea behind kernels is to map training data nonlinearly into a higher-dimensional feature space via a mapping function \u03a6 and to construct a separating hyperplane that maximizes the margin. The construction of the linear decision surface in this feature space requires only the evaluation of dot products \u03a6 (xi).\u03a6 (xj) = k (xi, xj), where k () is called the kernel function [5].\n4- PNN-Training Algorithm This algorithm is another version of the Help-Training algorithm. In this algorithm, a generative classifier, PNN, models the whole data space and then discriminative classifier, for example SVM, classifies the data. In data space, we have a few numbers of labeled data. First, we train the PNN with this group of data and then predict the labels of unlabeled data. In this level, the PNN assigns the unlabeled data to one class based on their probability to belong to each class. Then this group of data will be added to the training set. The discriminative classifier classifies the labeled and new labeled data which the PNN estimated belonging of each sample to each class. The PNN-Training for semi-supervised SVM is described as follows:"}, {"heading": "PNN-Training for semi-supervised SVM", "text": "1)Input: Set L = Labeled data, U = Unlabeled data 2) Train PNN by L (PNN Training Algorithm) 3) Classify U by trained PNN (PNN Classification Algorithm) 4) Add new labeled data to training set 5) Make a model of training set by SVM 6) Calculate SVM test error by testing set 7)Output: Error classification rate\n5- Experimental Result In this section, we test our algorithm on two specific data sets that are common in semisupervised testing. In this section, we use LIBSVM library [6].\n5-1 Two moon\nMoons data set [3, 7]. We produce 50 samples of each class and select randomly 10 samples of each class as a labeled data. After training of PPN by 20% of whole data and estimating of other samples, the training set is ready to be trained and test by SVM. We use RBF kernel with default values of its terms in LIBSVM; gamma = (# feature) -1 .\nThe results are shown in Table I. The other test errors are from [2]:\nMethod Test error (%) Branch and Bound 0 Self-Training 33.68\nHelp-Training 15.07 PNN-Training 10.23\nThe PNN-Training result shows that it can have better performance that it\u2019s previous version, Help-Training. Branch and Bound Algorithm has best efficiency, however, it loses its performance on large datasets.\n5-2 USPS Dataset\nThe US Postal Service handwritten digits recognition corpus (USPS) is another famous benchmark to test semi-supervised algorithm. It contains normalized grey scale images of size 16\u00d716 digits, divided into a training set of 7291 images and a test set of 2007 images. 90% of samples are unlabeled. Again we apply a default value of RBF kernel in LIBSVM for supervised SVM. . The results are as following in Table II. The other test errors are from [2]:\nMethod Test error (%)\nTSVM/SVMlight 14.70\nSelf-Training 8.22\nHelp-Training 7.77 PNN-Training 7.42\nstrategy, because we know just 10% of data labels. LIBSVM is a powerful tool to implement this strategy. We compare our result with TSVM which is implemented in SVMlight.[9]. The PNNTraining method can obtain a slightly better classification rate than its previous version, HelpTraining method.\n6- Conclusion In this paper, we introduced a new type of Help-Training algorithm and name it PNN-Training. We employed this method to one discriminative classifier, SVM. The algorithm estimates the label of large numbers of unlabeled samples by using a few labeled numbers of them. Whole labeled and new estimated labeled samples contribute the main training set. Finally, the RBFSVM classifier classifies them. We tested our method on two data sets and showed it had better performance in one data set than Help-Training and other semi-supervised learning methods such as TSVM."}], "references": [{"title": "Maximum likelihood from incomplete data via the EM algorithm,", "author": ["N.M.L.A.P. Dempster", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1977}, {"title": "Help-Training for semi-supervised support vector machines", "author": ["M.M. Adankon", "M. Cheriet"], "venue": "Pattern Recogn., vol. 44, pp. 2220-2230, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399-2434, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "presented at the Proceedings of the eleventh annual conference on Computational learning theory, Madison, Wisconsin, United States, 1998.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"], "venue": "presented at the Proceedings of the fifth annual workshop on Computational learning theory, Pittsburgh, Pennsylvania, United States, 1992.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. Intell. Syst. Technol., vol. 2, pp. 1-27, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Branch and Bound for Semi-Supervised Support Vector Machines", "author": ["O. Chapelle", "V. Sindhwani", "S. Keerthi"], "venue": "Advances in Neural Information Processing Systems 19, l. B. Sch\\\"o, J. Platt, and T. Hoffman, Eds., ed Cambridge, MA: MIT Press, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Support-Vector Networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Mach. Learn., vol. 20, pp. 273-297, 1995.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Transductive Inference for Text Classification using Support Vector Machines", "author": ["T. Joachims"], "venue": "presented at the Proceedings of the Sixteenth International Conference on Machine Learning, 1999.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Fisher discriminant analysis with kernels", "author": ["G.R.S. Mika", "J. Weston", "B. Scholkopf", "K.R. Muller"], "venue": "Neural Networks for Signal Processing pp. 41-48, 1999.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning with labeled and unlabeled data", "author": ["M. Seeger"], "venue": "Technical report, Institute for Adaptive and neural Computation, University of Edinburgh, February 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural Networks for the Recognition of Traditional Chinese Handwriting", "author": ["S.-R.Z. Shang-Jen Chuang", "Yu-Lin Chou"], "venue": "presented at the 14th IEEE International Conference on Computational Science and Engineering, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic neural networks", "author": ["D.F. Specht"], "venue": "Neural Networks, vol. 3, pp. 109-118, 1990.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["D. Yarowsky"], "venue": "presented at the Proceedings of the 33rd annual meeting on Association for Computational Linguistics, Cambridge, Massachusetts, 1995.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 10, "context": "Seeger [11], has been found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "There are different methods in supervised learning such as Expectation-Maximization (EM) algorithm [1], Co-Training [4], Transductive Support Vector Machine (TSVM) [8], Self-Training [14] and Help-Training [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "There are different methods in supervised learning such as Expectation-Maximization (EM) algorithm [1], Co-Training [4], Transductive Support Vector Machine (TSVM) [8], Self-Training [14] and Help-Training [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "There are different methods in supervised learning such as Expectation-Maximization (EM) algorithm [1], Co-Training [4], Transductive Support Vector Machine (TSVM) [8], Self-Training [14] and Help-Training [2].", "startOffset": 164, "endOffset": 167}, {"referenceID": 13, "context": "There are different methods in supervised learning such as Expectation-Maximization (EM) algorithm [1], Co-Training [4], Transductive Support Vector Machine (TSVM) [8], Self-Training [14] and Help-Training [2].", "startOffset": 183, "endOffset": 187}, {"referenceID": 1, "context": "There are different methods in supervised learning such as Expectation-Maximization (EM) algorithm [1], Co-Training [4], Transductive Support Vector Machine (TSVM) [8], Self-Training [14] and Help-Training [2].", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "In [2] Parzen window estimator is used as G, but we used Parzen Probabilistic Neural Network (PPNN) or in short term Probabilistic Neural Network.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "2- PNN A PNN is a feed forward neural network, which was derived from Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis [10].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "Specht [13].", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "Each classification which Gaussian distribution probability density function of the covariance matrix is diagonal matrix and each values is the same [12].", "startOffset": 149, "endOffset": 153}, {"referenceID": 6, "context": "3- SVM Support Vector Machines (SVMs) are a class of supervised learning algorithms introduced by Vapnik[7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "\u03a6 (xj) = k (xi, xj), where k () is called the kernel function [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "In this section, we use LIBSVM library [6].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "One of the famous benchmarks in testing of semi-supervised learning algorithm is the Two Moons data set [3, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 6, "context": "One of the famous benchmarks in testing of semi-supervised learning algorithm is the Two Moons data set [3, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 1, "context": "The other test errors are from [2]:", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "The other test errors are from [2]:", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "In this paper, we propose another version of help-training approach by employing a Probabilistic Neural Network (PNN) that improves the performance of the main discriminative classifier in the semi-supervised strategy. We introduce the PNN-training algorithm and use it for training the support vector machine (SVM) with a few numbers of labeled data and a large number of unlabeled data. We try to find the best labels for unlabeled data and then use SVM to enhance the classification rate. We test our method on two famous benchmarks and show the efficiency of our method in comparison with pervious methods.", "creator": "Microsoft\u00ae Word 2010"}}}