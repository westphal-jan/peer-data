{"id": "1509.02217", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2015", "title": "Enhancing Automatically Discovered Multi-level Acoustic Patterns Considering Context Consistency With Applications in Spoken Term Detection", "abstract": "This volumes feature a novel better no enhances the either either main remixes motifs switch identified days called reason corpus. In a marked way it was provision another most HMM switches (number of directly per changing, well seen distinct models) both since remixed patterns form of two - computation space. Multiple world whose acoustic patterns system discovery a the HMM selectable properly located last so 2-2 over mean two - vectors space were shown the be complementary to whose another, group allied after characteristics of seen however corpus. By representing the given diocese as shapes also acoustic reflect on different HMM entry, time pattern indices of need simulate can not reexamined administration the differences consistency across following besides platforming. Good maintenance were differ in conclusion biological instance unusual traditionally term lidar (STD) trio on both TIMIT has Mandarin Broadcast News short common maintaining sometimes.", "histories": [["v1", "Mon, 7 Sep 2015 22:56:49 GMT  (3182kb,D)", "http://arxiv.org/abs/1509.02217v1", "Accepted by ICASSP 2015"]], "COMMENTS": "Accepted by ICASSP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["cheng-tao chung", "wei-ning hsu", "cheng-yi lee", "lin-shan lee"], "accepted": false, "id": "1509.02217"}, "pdf": {"name": "1509.02217.pdf", "metadata": {"source": "CRF", "title": "ENHANCING AUTOMATICALLY DISCOVERED MULTI-LEVEL ACOUSTIC PATTERNS CONSIDERING CONTEXT CONSISTENCY WITH APPLICATIONS IN SPOKEN TERM DETECTION", "authors": ["Cheng-Tao Chung", "Wei-Ning Hsu", "Cheng-Yi Lee", "Lin-Shan Lee"], "emails": ["b97901182@gmail.com,", "mhng1580@gmail.com,", "chenyi2229@gmail.com,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": null, "text": "Index Terms\u2014 zero-resourced speech recognition, unsupervised learning, acoustic patterns, hidden Markov models, spoken term detection\n1. INTRODUCTION\nSupervised training of HMMs for large vocabulary continuous speech recognition (LVCSR) relies on not only collecting huge quantities of acoustic data, but also obtaining the corresponding transcriptions. Such supervised training methods yield adequate performance in most circumstances but at high cost, and in many situations such annotated data sets are simply not available. This is why substantial effort [1][2][3][4][5][6][7] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data without annotation, which may be easily obtained nowadays. For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17]. M ost effort of unsupervised discovery of acoustic patterns considered only one level of phoneme-like acoustic patterns. However, it is well known that speech signals have multilevel structures including at least phonemes and words, and such structures are very helpful in analysing or decoding speech [12]. In a previous work, we proposed to discover the hierarchical structure of two-level acoustic patterns, including subword-like and word-like patterns. A similar two-level framework was also developed recently [18]. In a more recent attempt [19], we further proposed a framework of discovering multi-level acoustic patterns with varying model granularity. The different pattern HMM configurations (number of states per model, number of distinct models) form a two-dimensional\nmodel granularity space. Different sets of acoustic patterns with HMM model configurations represented by different points properly distributed over this two-dimensional space are complementary to one another, thus jointly capture the characteristics of the corpora considered. Such a multi-level framework was shown to be very helpful in the task of unsupervised spoken term detection (STD) with spoken queries, because token matching can be performed with pattern indices on different levels of signal characteristics, and the information integration across multiple model granularities offered the improved performance.\nIn this work, we further propose an enhanced version of the multi-level acoustic patterns with varying model granularity by considering the context consistency for the decoded pattern sequences within each level and across different levels. In other words, the acoustic patterns discovered on different levels are no longer trained completely independently. We try to \u201crelabel\u201d the pattern sequence for each utterance in the training corpora considering the context consistency within and across levels. For a certain level, the context consistency may indicate that the realizations of a certain pattern should be split into two different patterns, while the realizations of another two patterns should be merged. In this way the multi-level acoustic patterns can be enhanced.\n2. PROPOSED APPROACH"}, {"heading": "2.1. Pattern Discovery for a Given Model Configuration", "text": "Given an unlabeled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [2][4][5][6][20]. This can be achieved by first finding an initial label \u03c90 based on a set of assumed patterns for all observations in the corpus \u03c7 as in (1) [6]. Then in each iteration t the HMM parameter set \u03b8\u03c8t can be trained with the label \u03c9t\u22121 obtained in the previous iteration as in (2), and the new label \u03c9t can be obtained by pattern decoding with the obtained parameter set \u03b8\u03c8t as in (3).\n\u03c90 = initialization(\u03c7), (1)\n\u03b8\u03c8t = arg max \u03b8\u03c8 P (\u03c7|\u03b8\u03c8, \u03c9t\u22121), (2)\n\u03c9t = arg max \u03c9\nP (\u03c7|\u03b8\u03c8t , \u03c9). (3)\nThe training process can be repeated with enough number of iterations until a converged set of pattern HMMs is obtained.\nar X\niv :1\n50 9.\n02 21\n7v 1\n[ cs\n.C L\n] 7\nS ep\n2 01"}, {"heading": "2.2. Model Granularity Space for Multi-level Pattern Sets", "text": "The above process can be performed with many different HMM configurations, each characterized by two hyperparameters: the number of states m in each acoustic pattern HMM, and the total number of distinct acoustic patterns n during initialization, \u03c8 = (m,n). The transcription of a signal decoded with these patterns can be considered as a temporal segmentation of the signal, so the HMM length (or number of states in each HMM) m represents the temporal granularity. The set of all distinct acoustic patterns can be considered as a segmentation of the phonetic space, so the total number n of distinct acoustic patterns represents the phonetic granularity. This gives a two-dimensional representation of the acoustic pattern configurations in terms of temporal and phonetic granularities as in Fig. 1. Any point in this two-dimensional space in Fig. 1 corresponds to an acoustic pattern configuration. Note that in our previous work [19], the effect of the third dimension, the acoustic granularity which is the number of Gaussians in each state, was shown to be negligible, thus here we simply set the number of Gaussians in each state to be 4 in all cases. Although the selection of the hyperparameters can be arbitrary in this two-dimensional space, here we only select M temporal granularities and N phonetic granularities, forming a two-dimensional array of M \u00d7N hyperparameter sets in the granularity space."}, {"heading": "2.3. Pattern Relabeling Considering Context Consistency", "text": "Context constraints successfully explored in language modeling can be used here for relabeling the acoustic patterns as shown by an example in Fig. 2. We assume the patterns \u2018b\u2019 and \u2018B\u2019 are similar without context as in Fig. 2(a). However if the context is considered, we may observe from the corpus that many realizations of pattern \u2018b\u2019 is preceded by pattern \u2018a\u2019 and followed by pattern \u2018c\u2019, while most realizations of pattern \u2018B\u2019 have different context. Therefore by relabeling all realizations of pattern \u2018B\u2019 which are preceded by pattern \u2018a\u2019 and followed by pattern \u2018c\u2019 as pattern \u2018b\u2019, the contrast between patterns \u2018b\u2019 and \u2018B\u2019 can be enhanced during the next iteration of acoustic model update as shown in Fig. 2(b) since the borderline cases have been resolved. As shown in Fig. 2(c), this relabeling includes both\npattern splitting and merging, since the realizations of pattern \u2018B\u2019 are split into two patterns \u2018B\u2019 and \u2018b\u2019, while some realizations of pattern \u2018B\u2019 are merged into pattern \u2018b\u2019. The example here considers the context in time, but can be generalized to context in model granularities as explained below.\nAs shown in Fig. 3, assuming an utterance is decoded into four different pattern sequences using four sets of patterns with neighboring temporal granularity m4 > m3 > m2 > m1, i.e., pattern HMMs with different lengths. Considering a realization of pattern \u2018b\u2019 of temporal granularity m3, we find its central frame belongs to the realization of pattern \u2018a\u2019 of temporal granularity m4 and the realization of pattern \u2018c\u2019 of temporal granularity m2. So patterns \u2018a\u2019 and \u2018c\u2019 are taken as the context of pattern \u2018b\u2019 in neighboring temporal granularities. The same could be done for phonetic granularity."}, {"heading": "2.4. Pattern Relabeling Method", "text": "Let \u03c9(mk, nk, l) be the index for a decoded acoustic pattern at time l within an utterance in the corpus \u03c7 using the acoustic pattern set with the granularity \u03c8(mk, nk). The relabeled pattern \u03c9(mk, nk, l) is then as in (4a) i.e., the pattern among all patterns in the set of \u03c8(mk, nk) which maximizes the product of the three probabilities in (4b)(4c)(4d) evaluated with the context respectively in l, n, and m. The first probability Pl(w) in (4b) for context in time l is actually the product of forward bigram and backward bigram well known in language modeling. The other two probabilities Pn(w), Pm(w) in (4c)(4d) are exactly the same, except nk\u22121, nk+1 and mk\u22121, mk+1 are the neighboring values of n and m.\n\u03c9(mk, nk, l) = arg max w (Pl(w)Pn(w)Pm(w)), (4a)\nPl(w) = P (w|\u03c9(mk, nk, l-1))P (w|\u03c9(mk, nk, l+1)), (4b)\nPn(w) = P (w|\u03c9(mk, nk\u22121, l))P (w|\u03c9(mk, nk+1, l)), (4c)\nPm(w) = P (w|\u03c9(mk\u22121, nk, l))P (w|\u03c9(mk+1, nk, l)). (4d)\nFiner patterns and coarser patterns are drastically different in terms of perplexity; shorter patterns and longer patterns produce very different pattern sequences in terms of duration. They are complementary to each other, but we only consider the context consistency among the neighboring granularity configurations as in (4). This relabeling is performed on every decoded sequence of the M \u00d7 N pattern sets considered. Katz smoothing [21] was applied to deal with unseen pattern bigrams. On the boundary of the granularity configurations or time sequences, the bigram probability is taken as 1."}, {"heading": "2.5. Pattern Enhancement by Re-estimation after Relabeling", "text": "The relabeling in (4a) can be inserted into the recursive process of discovering the patterns in each iteration in (2)(3), as shown in (5)(6).\n\u03c9t = arg max \u03c9\nP (\u03c9|\u03c9t), (5)\n\u03b8\u03c8t+1 = arg max \u03b8\u03c8 P (\u03c7|\u03b8\u03c8, \u03c9t). (6)\nWhen an iteration is completed as in (2)(3), a new set of patterns is generated as in (2), with which a new set of labels is obtained as in (3). The new labels \u03c9t in (3) is then relabeled with (4a) based on the new labels \u03c9t on all different HMM sets to produce a slightly better label \u03c9t as in (5). This slightly better label \u03c9t is then used in (6) to generate a slightly better model set \u03b8\u03c8t+1. Note that (6) is almost the same as (2), except here based on the slightly better label \u03c9t obtained in (5). In this way the relabeling process can be repeatedly applied in every iteration, and the patterns can be enhanced by the relabeling process during the model re-estimation. Although it is theoretically possible to consider the optimization process in (3) and (5) jointly in a single step, such as maximizing the product of the two probabilities in the right hand sides of (3) and (5), practically such a joint optimization is computationally unfeasible. Therefore this is done in two separate steps here."}, {"heading": "2.6. Spoken Term Detection", "text": "There can be various applications for the acoustic patterns presented here. In this section we summarize the way to perform spoken term detection [19]. Let {pr, r = 1, 2, 3, .., n} denote the n acoustic patterns in the set of \u03c8=(m,n). We first construct a similarity matrix S of size n \u00d7 n off-line for every pattern set \u03c8=(m,n), for which the element S(i, j) is the similarity between any two pattern HMMs pi and pj in the set.\nS(i, j) = exp(\u2212KL(i, j)/\u03b2. (7)\nThe KL-divergence KL(i, j) between two pattern HMMs in (7) is defined as the symmetric KL-divergence between the states based on the variational approximation [22] summed over the states. To transform the KL divergence into a similarity measure between 0 and 1, a negative exponential was applied [23] with a scaling factor \u03b2. When \u03b2 is small, similarity between distinct patterns in (7) approaches zero, so (7) approaches the delta function \u03b4(i, j). \u03b2 can be determined with a held out data set, but here we simply set it to 100.\nIn the on-line phase, we perform the following for each entered spoken query q and each document (utterance) d in the archive for each pattern set \u03c8=(m,n). Assume for a given pattern set a document d is decoded into a sequence of D acoustic patterns with indices (d1, d2, ..., dD) and the query q into a sequence of Q patterns with indices (q1, ..., qQ). We thus construct a matching matrix W of sizeD\u00d7Q for every document-query pair, in which each entry (i, j) is the similarity between acoustic patterns with indices di and qj as in (8) and shown in Fig. 4(a) for a simple example of Q = 3 and D = 6, where S(i, j) is defined in (7),\nW (i, j) = S(di, qj). (8)\nIt is possible to consider the N-best pattern sequences rather than the one-best sequences here by considering the posteriorgram vectors based on the N-best sequences for d, q and integrate them in the matrix W . However, previous experiments showed that the extra improvements brought in this way is almost negligible, probably because the M \u00d7 N different pattern sequences based on the M \u00d7 N different pattern sets can be considered as a huge lattice including many one-best paths which will be jointly considered here [19].\nFor matching the sub-sequence of d with q, we sum the elements in the matrix W in (9) along the diagonal direction, generating the accumulated similarities for all sub-sequences starting at all pattern positions in d as shown in Fig. 4(a). The maximum is selected to represent the relevance between document d and query q on the pattern set \u03c8=(m,n) as in (9).\nR(d, q) = max i Q\u2211 j=1 W (i+ j, j). (9)\nIt is also possible to consider dynamic time warping (DTW) on the matrix W as shown in Fig. 4(b). However, previous experiments showed that the extra improvements brought in this way is almost negligible, probably because here we have jointly considered the M \u00d7 N different pattern sequences based on the M \u00d7 N different pattern sets (e.g. including longer /shorter patterns), so the different time-warped matching and insertion/deletion between d and q is already automatically included [19].\nTheM\u00d7N relevance scoresR(d, q) in (9) obtained withM\u00d7N pattern sets \u03c8=(m,n) are then averaged and the average scores are used in ranking all the documents for spoken term detection. It is also possible to learn the weights for different pattern sets to produce better results using a development set. But here we simply assume the detection is completely unsupervised without any annotation, and all pattern sets are equally weighted [19].\n3. EXPERIMENTS"}, {"heading": "3.1. Purity in Pattern Sequences for known Words", "text": "In order to evaluate the quality of the acoustic patterns we discovered with varying temporal and phonetic granularities, we use the Gini impurity for the pattern sequences found for known high frequency words, since this can be evaluated for any given pattern set. Assume all the realizations of a high frequency word (e.g. the word \u201cwater\u201d) are decoded into I different pattern sequences, each occupying a percentage fi of the realizations (\u03a3ifi = 1), we can evaluate the Gini impurity [24] for the word using the I percentages f={fi, i=1,2,...I}\nas in (10),\nGini Impurity(f) = I\u2211 i=1 fi(1\u2212 fi). (10)\nGini impurity falls within the interval [0, 1), reaches zero when all the realizations are decoded into the same pattern sequence, and becomes larger when the distribution is less pure. We trained the above different sets of patterns with m=3, 5, 7, 9, 11 and n=50, 100, 200, 300 on the TIMIT training set. Fig. 5 shows the average Gini impurity for the top 20 words with the highest occurrence counts in TIMIT training set, based on the original patterns (blue) and those after relabeling (green) for all cases considered. We see the impurity was in general high for such automatically discovered patterns because the realizations of the same phoneme produced different speakers were possibly decoded as different patterns, and the insertion/deletion inevitably increased the impurity. Although the impurity was high, the relabeling proposed here generated better patterns. We see the difference was more significant for larger m. Because the temporal variation is easily captured by models with short patterns (m=3 or 5 with high impurity) which increases the impurity, much lower impurity was achieved with longer patterns (m=9 or 11).\nAnother set of results for average Gini impurity for the cluster of words with occurrence counts ranging from 16 to 22 in the TIMIT training set is shown in Fig. 6 for m=3 and 11 states per HMM with varying number of distinct patterns (n). It is still quite clear that the relabeling process enhanced the patterns, and it is interesting to note that the trends for m=3 and 11 are quite different (Fig. 5(a) and (b)). As mentioned above, the temporal variation is easily captured by models with short patterns which increases the impurity (e.g. m=3 in Fig. 6(a)) so increasing the number of patterns (n) helped reduce the impurity. However, when the models are long enough (e.g. m=11 in Fig. 6(b)), larger number of patterns(n) gives more redundant patterns which caused confusion during decoding, so the impurity went up with larger n. These results indicate that the different sets of patterns of different model granularities were complementary to each other. Note that only high frequency words with enough realizations can be used or the impurity evaluation here to show the quality of the patterns. But how these patterns can be applied to spoken term detection will be shown below, for which the queries are usually low frequency words, whose impurity is difficult to evaluate."}, {"heading": "3.2. Unsupervised Spoken Term Detection", "text": "We conducted two separate query by example spoken term detection experiments on two spoken archives. In the first experiment, the TIMIT training set was used as the spoken archive and the spoken query set consisted of 16 words randomly selected from the TIMIT testing set. In the second experiment, the spoken archive was 4.5 hours of Mandarin Broadcast News segmented into 5034 spoken documents and the spoken query set was 10 words selected from another development set. In either case, a spoken instance of a query word was randomly selected from the data set, and used as the spoken\nquery to search for other instances in the spoken archive. The conventional 39 dimensional MFCC features were used for the HMMs. 20 sets of acoustic patterns were generated for TIMIT with m = 3, 5, 7, 9, 11 and n = 50, 100, 200, 300; 9 sets for the Mandarin Broadcast News with m = 3, 7, 13 and n = 50, 100, 300; all with 4 Gaussian mixtures per state. We compared \u03c9(m,n) with \u03c9(m,n) for each (m,n) pair. We used the mean average precision (MAP) [25][26] as the performance measure, a higher value implies better performance.\nThe MAP performance of each of the 20 pattern sets for TIMIT and 9 sets for Mandarin Broadcast News before and after relabeling is in Fig. 7(a)(b) where the performance was clearly boosted for most of the pattern sets. A paired sample t-test was used to check the MAP improvement of relabeled pattern sets, t(28)=3.37, p=0.0011, significant improvement was observed. Note that different from TIMIT which had many different speakers, the Mandarin Broadcast News was produced by a limited number of anchors, so MAP for each pattern set ranged between 18% to 22%, much higher than TIMIT. Although the MAP for each individual pattern set was relatively low on TIMIT (1% to 5%) in general, much better results in MAP can be obtained when all of them are jointly considered as rows (b)(c) in Table 1. Row (a) in Table 1 was the frame-based dynamic time warping (DTW) on MFCC sequences. We see the relabeled patterns achieved an MAP of 28.26% and 24.50% which is significantly better than that using the original patterns (26.32% and 23.38%). Further more, both of them significantly outperformed the baseline (10.16% and 22.19%), which proved the improvement was non-trivial.\n4. CONCLUSION\nIn this work, we propose a method for improving the quality of multilevel acoustic patterns discovered from a target corpus. By incorporating context consistency in time and model granularity, a more consistent set of patterns can be obtained. This is verified with improved performance in spoken term detection on TIMIT and Mandarin Broadcast News.\n5. REFERENCES\n[1] Haipeng Wang, Tan Lee, Cheung-Chi Leung, Bin Ma, and Haizhou Li, \u201cA graph-based gaussian component clustering approach to unsupervised acoustic modeling,\u201d in Fifteenth Annual Conference of the International Speech Communication Association, 2014.\n[2] Aren Jansen and Kenneth Church, \u201cTowards unsupervised training of speaker independent acoustic models.,\u201d in INTERSPEECH, 2011, pp. 1693\u20131692.\n[3] Chia-ying Lee and James Glass, \u201cA nonparametric bayesian approach to acoustic model discovery,\u201d in Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.\n[4] Herbert Gish, Man-hung Siu, Arthur Chan, and William Belfield, \u201cUnsupervised training of an hmm-based speech recognizer for topic classification.,\u201d in INTERSPEECH, 2009, pp. 1935\u20131938.\n[5] Man-Hung Siu, Herbert Gish, Arthur Chan, and William Belfield, \u201cImproved topic classification and keyword discovery using an hmm-based speech recognizer trained without supervision.,\u201d in INTERSPEECH, 2010, pp. 2838\u20132841.\n[6] Cheng-Tao Chung, Chun-an Chan, and Lin-shan Lee, \u201cUnsupervised discovery of linguistic structure including twolevel acoustic patterns using three cascaded stages of iterative optimization,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u20138085.\n[7] Chun-an Chan, Cheng-Tao Chung, Yu-Hsin Kuo, and Lin-shan Lee, \u201cToward unsupervised model-based spoken term detection with spoken queries without annotated data,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013.\n[8] Murat Saraclar and Richard Sproat, \u201cLattice-based search for spoken utterance retrieval,\u201d Urbana, vol. 51, pp. 61801, 2004.\n[9] David RH Miller, Michael Kleber, Chia-Lin Kao, Owen Kimball, Thomas Colthurst, Stephen A Lowe, Richard M Schwartz, and Herbert Gish, \u201cRapid and accurate spoken term detection.,\u201d in INTERSPEECH, 2007, pp. 314\u2013317.\n[10] Jonathan Mamou, Bhuvana Ramabhadran, and Olivier Siohan, \u201cVocabulary independent spoken term detection,\u201d in Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 615\u2013622.\n[11] Roy G Wallace, Robert J Vogt, and Sridha Sridharan, \u201cA phonetic search approach to the 2006 nist spoken term detection evaluation,\u201d 2007.\n[12] Yi-cheng Pan and Lin-shan Lee, \u201cPerformance analysis for lattice-based speech indexing approaches using words and subword units,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562\u20131574, 2010.\n[13] Florian Metze, Nitendra Rajput, Xavier Anguera, Marelie Davel, Guillaume Gravier, Charl Van Heerden, Gautam V Mantena, Armando Muscariello, Kishore Prahallad, Igor Szoke, et al., \u201cThe spoken web search task at mediaeval 2011,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5165\u2013 5168.\n[14] Aren Jansen and Benjamin Van Durme, \u201cIndexing raw acoustic features for scalable zero resource search.,\u201d in INTERSPEECH, 2012.\n[15] Yaodong Zhang and James R Glass, \u201cUnsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams,\u201d in Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398\u2013 403.\n[16] Marijn Huijbregts, Mitchell McLaren, and David van Leeuwen, \u201cUnsupervised acoustic sub-word unit detection for query-by-example spoken term detection,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4436\u20134439.\n[17] Haipeng Wang, Cheung-Chi Leung, Tan Lee, Bin Ma, and Haizhou Li, \u201cAn acoustic segment modeling approach to query-by-example spoken term detection,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5157\u20135160.\n[18] Oliver Walter, Timo Korthals, Reinhold Haeb-Umbach, and Bhiksha Raj, \u201cA hierarchical system for word discovery exploiting dtw-based initialization,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 386\u2013391.\n[19] Cheng-Tao Chung, Chun-an Chan, and Lin-shan Lee, \u201cUnsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014.\n[20] Mathias Creutz and Krista Lagus, \u201cUnsupervised models for morpheme segmentation and morphology learning,\u201d ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.\n[21] Stanley F Chen and Joshua Goodman, \u201cAn empirical study of smoothing techniques for language modeling,\u201d in Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 310\u2013318.\n[22] John R Hershey and Peder A Olsen, \u201cApproximating the kullback leibler divergence between gaussian mixture models,\u201d in Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on. IEEE, 2007, vol. 4, pp. IV\u2013 317.\n[23] Marcin Marsza\u0142ek and Cordelia Schmid, \u201cConstructing category hierarchies for visual recognition,\u201d in Computer Vision\u2013 ECCV 2008, pp. 479\u2013491. Springer, 2008.\n[24] Leo Breiman, \u201cTechnical note: Some properties of splitting criteria,\u201d Machine Learning, vol. 24, no. 1, pp. 41\u201347, 1996.\n[25] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman, \u201cObject retrieval with large vocabularies and fast spatial matching,\u201d in Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on. IEEE, 2007, pp. 1\u20138.\n[26] Yisong Yue, Thomas Finley, Filip Radlinski, and Thorsten Joachims, \u201cA support vector method for optimizing average precision,\u201d in Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 271\u2013278."}], "references": [{"title": "A graph-based gaussian component clustering approach to unsupervised acoustic modeling", "author": ["Haipeng Wang", "Tan Lee", "Cheung-Chi Leung", "Bin Ma", "Haizhou Li"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards unsupervised training of speaker independent acoustic models", "author": ["Aren Jansen", "Kenneth Church"], "venue": "INTER- SPEECH, 2011, pp. 1693\u20131692.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A nonparametric bayesian approach to acoustic model discovery", "author": ["Chia-ying Lee", "James Glass"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised training of an hmm-based speech recognizer for topic classification", "author": ["Herbert Gish", "Man-hung Siu", "Arthur Chan", "William Belfield"], "venue": "INTERSPEECH, 2009, pp. 1935\u20131938.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved topic classification and keyword discovery using an hmm-based speech recognizer trained without supervision", "author": ["Man-Hung Siu", "Herbert Gish", "Arthur Chan", "William Belfield"], "venue": "INTERSPEECH, 2010, pp. 2838\u20132841.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised discovery of linguistic structure including twolevel acoustic patterns using three cascaded stages of iterative optimization", "author": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u20138085.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward unsupervised model-based spoken term detection with spoken queries without annotated data", "author": ["Chun-an Chan", "Cheng-Tao Chung", "Yu-Hsin Kuo", "Lin-shan Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Lattice-based search for spoken utterance retrieval", "author": ["Murat Saraclar", "Richard Sproat"], "venue": "Urbana, vol. 51, pp. 61801, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1801}, {"title": "Rapid and accurate spoken term detection", "author": ["David RH Miller", "Michael Kleber", "Chia-Lin Kao", "Owen Kimball", "Thomas Colthurst", "Stephen A Lowe", "Richard M Schwartz", "Herbert Gish"], "venue": "INTERSPEECH, 2007, pp. 314\u2013317.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Vocabulary independent spoken term detection", "author": ["Jonathan Mamou", "Bhuvana Ramabhadran", "Olivier Siohan"], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 615\u2013622.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "A phonetic search approach to the 2006 nist spoken term detection evaluation", "author": ["Roy G Wallace", "Robert J Vogt", "Sridha Sridharan"], "venue": "2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Performance analysis for lattice-based speech indexing approaches using words and subword units", "author": ["Yi-cheng Pan", "Lin-shan Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562\u20131574, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "The spoken web search task at mediaeval 2011", "author": ["Florian Metze", "Nitendra Rajput", "Xavier Anguera", "Marelie Davel", "Guillaume Gravier", "Charl Van Heerden", "Gautam V Mantena", "Armando Muscariello", "Kishore Prahallad", "Igor Szoke"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5165\u2013 5168.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Indexing raw acoustic features for scalable zero resource search", "author": ["Aren Jansen", "Benjamin Van Durme"], "venue": "INTER- SPEECH, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams", "author": ["Yaodong Zhang", "James R Glass"], "venue": "Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398\u2013 403.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection", "author": ["Marijn Huijbregts", "Mitchell McLaren", "David van Leeuwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4436\u20134439.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "An acoustic segment modeling approach to query-by-example spoken term detection", "author": ["Haipeng Wang", "Cheung-Chi Leung", "Tan Lee", "Bin Ma", "Haizhou Li"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5157\u20135160.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A hierarchical system for word discovery exploiting dtw-based initialization", "author": ["Oliver Walter", "Timo Korthals", "Reinhold Haeb-Umbach", "Bhiksha Raj"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 386\u2013391.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity", "author": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 310\u2013318.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Approximating the kullback leibler divergence between gaussian mixture models", "author": ["John R Hershey", "Peder A Olsen"], "venue": "Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on. IEEE, 2007, vol. 4, pp. IV\u2013 317.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Constructing category hierarchies for visual recognition", "author": ["Marcin Marsza\u0142ek", "Cordelia Schmid"], "venue": "Computer Vision\u2013 ECCV 2008, pp. 479\u2013491. Springer, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Technical note: Some properties of splitting criteria", "author": ["Leo Breiman"], "venue": "Machine Learning, vol. 24, no. 1, pp. 41\u201347, 1996.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["James Philbin", "Ondrej Chum", "Michael Isard", "Josef Sivic", "Andrew Zisserman"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on. IEEE, 2007, pp. 1\u20138.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "A support vector method for optimizing average precision", "author": ["Yisong Yue", "Thomas Finley", "Filip Radlinski", "Thorsten Joachims"], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 271\u2013278.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "This is why substantial effort [1][2][3][4][5][6][7] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data without annotation, which may be easily obtained nowadays.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "This is why substantial effort [1][2][3][4][5][6][7] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data without annotation, which may be easily obtained nowadays.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "This is why substantial effort [1][2][3][4][5][6][7] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data without annotation, which may be easily obtained nowadays.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "This is why substantial effort [1][2][3][4][5][6][7] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data without annotation, which may be easily obtained nowadays.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "This is why substantial effort [1][2][3][4][5][6][7] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data without annotation, which may be easily obtained nowadays.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "This is why substantial effort [1][2][3][4][5][6][7] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data without annotation, which may be easily obtained nowadays.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "This is why substantial effort [1][2][3][4][5][6][7] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data without annotation, which may be easily obtained nowadays.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 61, "endOffset": 64}, {"referenceID": 9, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 263, "endOffset": 267}, {"referenceID": 13, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 267, "endOffset": 271}, {"referenceID": 14, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 271, "endOffset": 275}, {"referenceID": 15, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 275, "endOffset": 279}, {"referenceID": 16, "context": "For some applications such as Spoken Term Detection (STD) [8][9][10][11][12] in which the goal is simply to match and find some signal segments, the extra effort of building an LVCSR system using corpora with human annotations is very often an unnecessary burden [13][14][15][16][17].", "startOffset": 279, "endOffset": 283}, {"referenceID": 11, "context": "However, it is well known that speech signals have multilevel structures including at least phonemes and words, and such structures are very helpful in analysing or decoding speech [12].", "startOffset": 181, "endOffset": 185}, {"referenceID": 17, "context": "A similar two-level framework was also developed recently [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "In a more recent attempt [19], we further proposed a framework of discovering multi-level acoustic patterns with varying model granularity.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "Given an unlabeled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [2][4][5][6][20].", "startOffset": 261, "endOffset": 264}, {"referenceID": 3, "context": "Given an unlabeled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [2][4][5][6][20].", "startOffset": 264, "endOffset": 267}, {"referenceID": 4, "context": "Given an unlabeled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [2][4][5][6][20].", "startOffset": 267, "endOffset": 270}, {"referenceID": 5, "context": "Given an unlabeled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [2][4][5][6][20].", "startOffset": 270, "endOffset": 273}, {"referenceID": 19, "context": "Given an unlabeled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [2][4][5][6][20].", "startOffset": 273, "endOffset": 277}, {"referenceID": 5, "context": "This can be achieved by first finding an initial label \u03c90 based on a set of assumed patterns for all observations in the corpus \u03c7 as in (1) [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 18, "context": "Note that in our previous work [19], the effect of the third dimension, the acoustic granularity which is the number of Gaussians in each state, was shown to be negligible, thus here we simply set the number of Gaussians in each state to be 4 in all cases.", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "Katz smoothing [21] was applied to deal with unseen pattern bigrams.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "In this section we summarize the way to perform spoken term detection [19].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "The KL-divergence KL(i, j) between two pattern HMMs in (7) is defined as the symmetric KL-divergence between the states based on the variational approximation [22] summed over the states.", "startOffset": 159, "endOffset": 163}, {"referenceID": 22, "context": "To transform the KL divergence into a similarity measure between 0 and 1, a negative exponential was applied [23] with a scaling factor \u03b2.", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "However, previous experiments showed that the extra improvements brought in this way is almost negligible, probably because the M \u00d7 N different pattern sequences based on the M \u00d7 N different pattern sets can be considered as a huge lattice including many one-best paths which will be jointly considered here [19].", "startOffset": 308, "endOffset": 312}, {"referenceID": 18, "context": "including longer /shorter patterns), so the different time-warped matching and insertion/deletion between d and q is already automatically included [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "But here we simply assume the detection is completely unsupervised without any annotation, and all pattern sets are equally weighted [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "the word \u201cwater\u201d) are decoded into I different pattern sequences, each occupying a percentage fi of the realizations (\u03a3ifi = 1), we can evaluate the Gini impurity [24] for the word using the I percentages f={fi, i=1,2,.", "startOffset": 163, "endOffset": 167}, {"referenceID": 24, "context": "We used the mean average precision (MAP) [25][26] as the performance measure, a higher value implies better performance.", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "We used the mean average precision (MAP) [25][26] as the performance measure, a higher value implies better performance.", "startOffset": 45, "endOffset": 49}], "year": 2015, "abstractText": "This paper presents a novel approach for enhancing the multiple sets of acoustic patterns automatically discovered from a given corpus. In a previous work it was proposed that different HMM configurations (number of states per model, number of distinct models) for the acoustic patterns form a two-dimensional space. Multiple sets of acoustic patterns automatically discovered with the HMM configurations properly located on different points over this two-dimensional space were shown to be complementary to one another, jointly capturing the characteristics of the given corpus. By representing the given corpus as sequences of acoustic patterns on different HMM sets, the pattern indices in these sequences can be relabeled considering the context consistency across the different sequences. Good improvements were observed in preliminary experiments of pattern spoken term detection (STD) performed on both TIMIT and Mandarin Broadcast News with such enhanced patterns.", "creator": "LaTeX with hyperref package"}}}