{"id": "1509.02412", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2015", "title": "Unsupervised Domain Discovery using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition", "abstract": "Speech maintain integrated places often particular domain dependent, kind kind already bureau as part scholars. However the exists created corresponding is parts and not possibly may anything criteria. Hence it is frequently only particular if user should be well on better out - of - template. While both overdubs made language wheels usually be domain specific, . he unfortunately report refining month solo spatial. We due every novel method still conducted internships found of domains types Latent Dirichlet Allocation (LDA) modelling. Here in which for hidden grouped still accepted to places at the data, intended on software segment be could considered to though though rebounded mixtures of atm complexes. The classification on audio segments with entities allows into particular related url provide noise engines from carry views contribution. Experiments sometimes conducted on a cnms of themes opinion information covering presentation while radio three TV digital, telephone tape, meetings, symposiums and printed speech, up a resume intensive set it 82 while made a course on. seven time. Maximum A Posteriori (MAP) sci-fi because LDA based types was shown made 5.75 relative Word Error Rate (WER) implementing of up to six% generally, compared but pooled professionals, as up to 72% , 55 with models \u2014 with hiv - labelled established domain knowledge.", "histories": [["v1", "Tue, 8 Sep 2015 15:29:23 GMT  (70kb,D)", "http://arxiv.org/abs/1509.02412v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mortaza doulaty", "oscar saz", "thomas hain"], "accepted": false, "id": "1509.02412"}, "pdf": {"name": "1509.02412.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Domain Discovery using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition", "authors": ["Mortaza Doulaty", "Oscar Saz", "Thomas Hain"], "emails": ["t.hain}@sheffield.ac.uk"], "sections": [{"heading": null, "text": "pendent, a fact widely reported in the literature. However the concept of domain is complex and not bound to clear criteria. Hence it is often not evident if data should be considered to be out-of-domain. While both acoustic and language models can be domain specific, work in this paper concentrates on acoustic modelling. We present a novel method to perform unsupervised discovery of domains using Latent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is assumed to exist in the data, whereby each audio segment can be considered to be a weighted mixture of domain properties. The classification of audio segments into domains allows the creation of domain specific acoustic models for automatic speech recognition. Experiments are conducted on a dataset of diverse speech data covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech, with a joint training set of 60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to LDA based domains was shown to yield relative Word Error Rate (WER) improvements of up to 16% relative, compared to pooled training, and up to 10%, compared with models adapted with human-labelled prior domain knowledge. Index Terms: domain discovery, latent dirichlet allocation, adaptation, speech recognition"}, {"heading": "1. Introduction", "text": "Recently, new applications and domains are becoming the target of research in Automatic Speech Recognition (ASR), as the existing systems increase their accuracy. This has opened the issue on how to scale up existing systems when new domains are incorporated as target data, for instance \u201cfound data\u201d, such as media and historical audio archives. In this situation, training acoustic models for an unknown domain, like different YouTube recordings, can be infeasible if the origin of the target speech can not be properly assessed, and the loss of accuracy is large due to wrong modelling decisions.\nWell\u2013tailored single domain systems, where training data that properly matches the target recognition data is available, are mostly used in current speech recognisers. These domain dependent models have been usually trained via Maximum Likelihood (ML) if a sufficiently large amount of domain data existed or using adaptation techniques such as Maximum A Posteriori [1], Maximum Likelihood Linear Regression (MLLR) [2] or Cluster Adaptive Training [3]. For more recent Deep Neural Network (DNN)\u2013based systems, domain adaptation is also possible with linear transformations, conservative training and subspace methods [4] with frameworks such as Multi\u2013Level Adaptive Networks (MLAN) [5] or Deep Maxout Networks (DMN) [6].\nAn important issue when dealing with highly diverse speech data is the difficulty to appropriately categorise every speech input within a particular domain, especially the case with newly discovered data. Even when domain categories have been given manually by humans, this may be inaccurate or there may be hidden characteristics in the audio that can further subdivide these categories or cross across several of the predefined domains. Developing the ability of discovering these new and hidden acoustic domains would greatly enhance the possibility of using well\u2013targeted specific domain models in ASR. However, as most speech recognition tasks assume a single domain or well differentiated domains, the task of unsupervised discovery of acoustic domains in speech data has been of less interest so far. This paper proposes to open new areas for research in multi\u2013domain ASR by treating speech data as a set of documents where latent domains exist and can be discovered using Latent Dirichlet Allocation (LDA) models.\nLDA is an statistical approach to discover latent topics in a collection of documents in an unsupervised manner [7]. It is mostly used in Natural Language Processing (NLP) for the categorisation of text documents, but it has been used for audio and image processing as well. In audio tasks, LDA has been used for classifying unstructured audio files into onomatopoeic and semantic descriptions with successful results [8, 9]. Building on this knowledge, this work proposes to use LDA for domain adaptation in ASR tasks.\nThis paper is organised as follows: Section 2 will give an overview of LDA modelling in its original proposal for topic modelling. Then, Section 3 will describe the proposed use of LDA models for unsupervised domain discovery in speech data. Section 4 will present the experimental setup used for multi\u2013domain speech recognition, with Section 5 detailing the obtained results. Section 6 gives the conclusions to this work."}, {"heading": "2. Latent Dirichlet Allocation", "text": "Latent Dirichlet Allocation (LDA) [7] is an unsupervised probabilistic generative model for collections of discrete data. It aims to describe how every item within the collection is generated, assuming that there are a set of hidden topics and that each item is modelled as a finite mixture over those topics. Also, an infinite mixture over an underlying set of topic probabilities is used to model each topic [7]. LDA is mostly used for topic modelling of text corpora, however, the model can be applied to other tasks, such as object categorisation and localisation in image processing [10], automatic harmonic analysis in music processing [11] or acoustic information retrieval in unstructured audio analysis [9].\nIn the context of text corpora, a dataset is defined as a collection of documents and each document is a collection of words. Given a vocabulary of size V , each word is represented by a V \u2013dimensional binary vector. It is assumed that the docu-\nar X\niv :1\n50 9.\n02 41\n2v 1\n[ cs\n.C L\n] 8\nS ep\n2 01\n5\nments are generated using the following generative process:\n1. For each document dm,m \u2208 {1...M}, choose a K\u2013 dimensional topic weight vector \u03b8m from the Dirichlet distribution with scaling parameter \u03b1: p(\u03b8m|\u03b1) = Dir(\u03b1)\n2. For each word wn, n \u2208 {1...N} in document dm (a) Draw a topic zn \u2208 {1...K} from the multinomial\ndistribution p(zn = k|\u03b8m) (b) Given the topic, draw a word from p(wn|zn, \u03b2),\nwhere \u03b2 is a V \u00d7K matrix and \u03b2ij = p(wn = i|zn = j, \u03b2)\nOther assumptions include the bag\u2013of\u2013words property of the documents and the fixed and known dimensionality of the Dirichlet distribution K (and thus the dimensionality of the topic variable z)\nThe graphical representation of LDA model is shown at Figure 1, a three level hierarchical Bayesian model. In this model, the only observed variable is w and the rest are all latent. \u03b1 and \u03b2 are corpus level parameters, \u03b8m are document level variables and zn, wn are word level variables. The generative process is described formally as:\np(\u03b8, z,w|\u03b1, \u03b2) = p(\u03b8|\u03b1) N\u220f n=1 p(zn|\u03b8)p(wn|zn, \u03b2) (1)\nThe posterior distribution of the latent topic variables given the words and \u03b1 and \u03b2 parameters is:\np(\u03b8, z|w, \u03b1, \u03b2) = p(\u03b8, z,w|\u03b1, \u03b2) p(w|\u03b1, \u03b2) (2)\nComputing p(w|\u03b1, \u03b2) requires some intractable integrals. A reasonable approximate can be acquired using variational approximation which is shown to work reasonably well in various applications [7]. The approximated posterior distribution is:\nq(\u03b8, z|\u03b3, \u03c6) = q(\u03b8|\u03b3) N\u220f n=1 q(zn|\u03c6n) (3)\nwhere \u03b3 is the Dirichlet parameter that determine \u03b8 and \u03c6 is the parameter for the multinomial that generates the topics.\nTraining tries to minimise the Kullback\u2013Leiber divergence (KLD) [12] between the real and the approximated joint probabilities (equations 2 and 3) [7]:\nargmin \u03b3,\u03c6\nKLD ( q(\u03b8, z|\u03b3, \u03c6) || p(\u03b8, z|w, \u03b1, \u03b2) ) (4)\nOther training methods based on Markov\u2013Chain Monte-Carlo is also proposed, like Gibbs sampling method [13]."}, {"heading": "3. Unsupervised Domain Discovery", "text": "The proposed technique uses an LDA model to discover hidden and latent acoustic domains in multi\u2013domain speech data. Since LDA is for collections of discrete data (such as text corpora) [7], every speech segment of length T frames, x =\n{x1, ...,xt, ...,xT }, is represented as a set of discrete symbols to support modelling within this framework. For that purpose, the n\u2013dimensional audio frames, xt \u2208 Rn, are quantised into a dictionary of V acoustic \u201cwords\u201d, x\u0304t \u2208 {1...V } [8]. First a Gaussian Mixture Model (GMM) is trained using Expectation Maximisation (EM) and mix\u2013up procedure to reach the desired codebook size V (enforcing the co\u2013variance matrix to be identity, equivalent to LBG\u2013VQ [14]). Then the means of the Gaussian components are used to create the codebook and quantise the audio frames into discrete symbols. The assignment of frame xi to codebook index j is performed using:\nx\u0304t = argmin j\n||xt \u2212mj || , j \u2208 {1...V } (5)\nwhere mj is jth mixture component\u2019s mean vector. To reconcile this with the LDA terminology described in Section 2, in this work each audio segment is a \u201cdocument\u201d and each codified audio frame is a \u201cword\u201d. All the audio segments (now \u201cdocuments\u201d) then create a whole \u201ccollection\u201d or \u201ccorpus\u201d.\nOnce all the audio frames are converted to discrete \u201cwords\u201d, the parameters of the LDA model using K domains are estimated on the M audio segments from the training data using variational EM. The domain of each quantised audio segment x\u0304 is then given by the domain with the highest value of the posterior Dirichlet parameter \u03b3 for that segment.\nDomain(x\u0304) = argmax j\n\u03b3j , j \u2208 {1..K} (6)\nBased on the estimated parameters from the training set, Dirichlet parameters \u03b3 can be inferred for the test set segments as well. With every segment in both train and test sets associated to a hidden domain, it is possible to perform training and/or adaptation with the usual techniques. Acoustic models can be trained via Maximum Likelihood (ML), or domain specific models can be adapted via MAP or MLLR, in case of GMM/HMM systems."}, {"heading": "4. Experimental setup", "text": "To evaluate the proposed domain discovery and adaptation method in a multi\u2013domain and diverse ASR task, a dataset of 6 different types of data was chosen from the following sources:\n\u2022 Radio (RD): BBC Radio4 broadcasts on February 2009.\n\u2022 Television (TV): Broadcasts from BBC on May 2008.\n\u2022 Telephone speech (CT): From the Fisher corpus1 [15].\n\u2022 Meetings (MT): From AMI [16] and ICSI [17] corpora.\n\u2022 Lectures (TK): From TedTalks [18].\n\u2022 Read speech (RS): From the WSJCAM0 corpus [19].\nA subset of 10h from each domain was selected to form the training set (60h in total), and 1h from each domain was used for testing (6h in total). The selection of the domains aims to cover the most common and distinctive types of audio recordings used in ASR tasks.\nTwo types of acoustic features were used: First, 13 PLP features plus first and second derivatives for a total of 39\u2013 dimensional feature vectors; and second, a 65\u2013dimensional feature vector concatenating the 39 PLP features and 26 bottleneck (PLP+BN) features extracted from a 4\u2013hidden\u2013layer DNN trained on the full 60 hours of data. 31 adjacent frames (15\n1All of the telephone speech data was up\u2013sampled to 16 kHz to match the sampling rate of the rest of the data.\nframes to the left and 15 frames to the right) of 23 dimensional log Mel filter bank features were concatenated to form a 713\u2013 dimensional super vector; Discrete Cosine Transform (DCT) was applied to this super vector to de\u2013correlate and compress it to 368 dimensions and then fed into the neural network. The network was trained on 4,000 triphone state targets and the 26 dimensional bottleneck layer was placed before the output layer. The objective function used was frame\u2013level cross\u2013entropy and the optimisation was done with stochastic gradient descent and the backpropagation algorithm. DNN training was performed with the TNet toolkit [20] and more details can be found at [21].\nFor both types of features, baseline ML GMM\u2013HMM models were trained using HTK [22] with 5\u2013state crossword triphones and 16 gaussians per state. The language model used was based on a 50,000\u2013word vocabulary and was trained by combination of language models from the 6 domains, with interpolation weights tuned using an independent development set."}, {"heading": "4.1. Baseline results", "text": "Table 1 presents the baseline Word Error Rate (WER) results for the in\u2013domain maximum\u2013likelihood (ML) model trained with the pooled 60 hours of all domains, plus the results of ML in\u2013 domain models each trained with 10 hours of in\u2013domain data. It also includes the MAP adapted models from the pooled model to each domain. Experiments were conducted using PLP and PLP+BN features. The results using ML training on the limited in\u2013domain data underperformed MAP adaptation on such data, which set MAP as a preferred setup for domain adaptation."}, {"heading": "5. Results", "text": "The experiments performed aimed to evaluate two aspects of the proposed LDA modelling for unsupervised domain discovery. First, if LDA could be successfully used to find hidden domains and if these domains represented the hidden characteristics of the audio. Second, once hidden domains had been identified, if domain adaptation could be applied on them and improvements in ASR performance were achieved over the baselines."}, {"heading": "5.1. Unsupervised domain discovery", "text": "For using LDA models, as described in Section 2, two parameters had to be initially set up. First, the number of domains K to be found had to be decided prior to the training. Also, since the audio frames needed to be quantised, the size of the codebook V also needed to be defined. For this end, a set of experiments were conducted with different codebook sizes and number of domains. Codebooks of size 128 up to 8,192 were used and given a codebook, different LDA models with a varying number of domains from 4 to 64 were estimated [23, 24] using the training data described in Section 4.\nSince these identified domains were latent, there was no ground truth to verify them at this stage. An initial way of evaluating how the different latent domains behaved was by measuring the distribution of the data, according to manual labels,\nwhich was included in each hidden domain. Figure 2 presents this distribution for an acoustic codebook of size 2,048 and 8 hidden domains. From this Figure, it is possible to see how telephone speech was separated into two different hidden domains (D1 and D3), while meeting speech was mostly assigned to a unique hidden domain (D7). Other manually labelled domains, such as Radio and Television broadcasts were scattered across hidden domains (D2, D4 or D8), indicating the presence of previously unseen domains within these types of data.\nFollowing this, KL divergence [12] was proposed as an appropriate metric to measure the consistency of the hidden topics discovered by LDA. This measured how the distributions of data in latent domains, as in Figure 2, in different sets, for instance training and testing data, were different with each other:\nKLD(P ||Q) = \u2211 i P (i) ln P (i) Q(i) (7)\nwhere P and Q are the distributions for training and test data. To compute the divergence, since we deal with counts in the distributions and some counts can be zero, the distributions are smoothed by discounting 3% of the total mass and distributing it across zero counts.\nFigure 3 shows the divergence values of different configurations. Low values of divergence indicated a more consistent set of hidden domains found by LDA modelling and, thus, were preferred over configurations with higher values. In terms of codebook size, codebooks of 2,048 and 8,192 symbols resulted in lower divergence. For the number of domains, increasing to more than 12 resulted an increase in divergence."}, {"heading": "5.2. Domain adaptation", "text": "For the evaluation of the possibilities offered by the unsupervised discovery of domains in ASR, MAP domain adaptation was performed to each of these new domains. The experiments were conducted with domains of size 4, 6, 8, 10 and 12 and a codebook of acoustic words of size 2,048. Each MAP adapted domain specific model was used to decode the corresponding speech segments in the test set that were assigned to that domain. Figure 4 shows the overall WER on the test set with different number of topics using both types of features, PLP and PLP+BN. The lowest WER values, 30.4% for PLP features and\n25.4% for PLP+BN, were achieved with 8 domains for both types of features, which was 16% and 5% relative improvement over their respective ML baselines. Comparing with MAP adaptation to human\u2013labelled domains the relative WER reduction was 10% and 3%. The improvements in WER vanished for more than 8 hidden domains, indicating that using larger numbers of domains were not beneficial for this task.\nTable 2 presents the breakout of the results using 8 hidden domains across the manually labelled domains. Improvements occur across all of these domains, indicating that the LDA model can benefit all types of speech in this setup. The domains that achieved the highest gains from using LDA MAP adaptation (with PLP feature) were read speech, telephone speech and TV broadcasts, with relative WER reductions of 14%, 12%, 10% respectively compared to MAP adaptation on the manually labelled domains. The lowest gain, 4% relative, occurred on meeting speech. Similarly, with PLP+BN features telephone speech, lectures and read speech benefited the most, with relative WER reduction of 5%, 4% and 2% respectively.\nFinally, Table 3 shows the WER across the hidden domains for both types of features with LDA MAP models. The most relevant feature of these domains, in terms of WER, was that the domains of low WER (like Read speech) or high WER (like TV data) had been broken up in different hidden domains and hence, WERs across hidden domains were evenly distributed."}, {"heading": "6. Conclusions", "text": "A novel technique based on Latent Dirichlet Allocation (LDA) has been proposed to discover latent domains in highly\u2013diverse speech data in an un\u2013supervised manner. The data set consisted\nof data from TV and radio shows, meetings, lectures, talks and telephony speech with a 60\u2013hour training set and 6\u2013hour test set. It was assumed that there are a set of hidden domains and each audio segment is a mixture of different properties of those hidden domains with different weights. LDA models were used to discover the latent domains and then these domains were used to perform Maximum A Posteriori (MAP) domain adaptation. Results showed relative improvement of up to 16% over the baseline Maximum Likelihood trained models and up to 10% over the MAP adapted models to human labelled domains with the LDA discovered domains.\nThe bag\u2013of\u2013words assumption in LDA model does not take the order of words into account. In applying LDA for image processing, there are some variants of the original LDA model, such as Spatial LDA [25] which encodes spatial structure with the visual words. A temporal variant of LDA could better handle the temporal nature of speech and needs to be investigated as a future work. Also applying the current technique on bigger and/or less diverse data set needs to be verified to see what would be the new discovered domains and how they are related to domain adaptation. Newer sets of features, better targeted to describe background acoustic characteristics [26], could also provide an improvement over PLP features, which are known to describe well phonetic and speaker information."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the EPSRC Programme Grant EP/I031022/1 Natural Speech Technology (NST)."}, {"heading": "8. Data Access Statement", "text": "The speech data used in this paper was obtained from the following sources: Fisher Corpus (LDC catalogue number LDC2004T19), ICSI Meetings corpus (LDC2004S02), WSJCAM0 (LDC95S24), AMI corpus (DOI number 10.1007/11677482 3), TedTalks data (freely available as part of the IWSLT evaluations), BBC Radio and TV data (this data was distributed to the NST project\u2019s partners with an agreement with BBC R&D and not publicly available yet).\nThe specific file lists used for training and testing in the experiments in this paper, as well as result files can be downloaded from http://mini.dcs.shef.ac.uk/ publications/papers/is15-doulaty2."}, {"heading": "9. References", "text": "[1] J.-L. Gauvain and C.-H. Lee, \u201cMaximum a posteriori estimation\nfor multivariate gaussian mixture observations of markov chains,\u201d IEEE Transactions on Speech and audio processing, vol. 2, no. 2, pp. 291\u2013298, 1994.\n[2] C. J. Leggetter and P. C. Woodland, \u201cMaximum likelihood linear regression for speaker adaptation of continuous density hidden markov models,\u201d Computer Speech & Language, vol. 9, no. 2, pp. 171\u2013185, 1995.\n[3] M. J. Gales, \u201cCluster adaptive training for speech recognition.\u201d in Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP\u2013Interspeech), Sydney, Australia, 1998, pp. 1783\u20131786.\n[4] D. Yu and L. Deng, Automatic Speech Recognition: A Deep Learning Approach. London, UK: Springer-Verlag, 2015.\n[5] P. Bell, P. Swietojanski, and S. Renals, \u201cMulti-level adaptive networks in tandem and hybrid ASR systems,\u201d in Proceddings of the 2013 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP), Vancouver BC, Canada, 2013, pp. 6975\u20136979.\n[6] Y. Miao, F. Metze, and S. Rawat, \u201cDeep maxout networks for lowresource speech recognition,\u201d in Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Olomouc, Czech Republic, 2013, pp. 398\u2013403.\n[7] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent Dirichlet Allocation,\u201d Journal of Machine Learning Research, vol. 3, pp. 993\u2013 1022, 2003.\n[8] S. Kim, S. Sundaram, P. Georgiou, and S. Narayanan, \u201cAudio scene understanding using topic models,\u201d in Proceedings of the Neural Information Processing System (NIPS) Workshop, Whistler BC, Canada, 2009.\n[9] S. Kim, S. Narayanan, and S. Sundaram, \u201cAcoustic topic model for audio information retrieval,\u201d in Proceedings of the 2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), New Paltz NY, USA, 2009, pp. 37\u201340.\n[10] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman, \u201cDiscovering objects and their location in images,\u201d in Proceedings of the 10th International Conference on Computer Vision (ICCV), Beijing, China, 2005, pp. 370\u2013377.\n[11] D. Hu and L. K. Saul, \u201cA probabilistic topic model for unsupervised learning of musical key-profiles.\u201d in Proceedings of the 10th International Society for Music Information Retrieval Conference (ISMIR), Kobe, Japan, 2009, pp. 441\u2013446.\n[12] S. Kullback and R. A. Leibler, \u201cOn information and sufficiency,\u201d The annals of mathematical statistics, vol. 2, no. 1, pp. 79\u201386, 1951.\n[13] T. L. Griffiths and M. Steyvers, \u201cFinding scientific topics,\u201d Proceedings of the National Academy of Sciences of the United States of America, vol. 101, pp. 5228\u20135235, 2004.\n[14] A. Gersho and R. M. Gray, Vector quantization and signal compression. Berlin, Germany: Springer Science & Business Media, 1992.\n[15] C. Cieri, D. Miller, and K. Walker, \u201cThe Fisher corpus: A resource for the next generations of speech-to-text.\u201d in Proceedings of the 4th International Conference on Language Resources and Evaluation, Lisbon, Portugal, 2004, pp. 69\u201371.\n[16] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, W. Karaiskos, Vasilis Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner, \u201cThe AMI meeting corpus: A preannouncement,\u201d in Proceedings of the Third International Workshop on Machine Learning for Multimodal Interaction, Bethesda MD, USA, 2006, pp. 28\u201339.\n[17] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters, \u201cThe ICSI meeting corpus,\u201d in Proceedings of the 2003 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP), Hong Kong, 2003.\n[18] R. W. N. Ng, M. Doulaty, R. Doddipatla, O. Saz, M. Hasan, T. Hain, W. Aziz, K. Shaf, and L. Specia, \u201cThe USFD spoken language translation system for IWSLT 2014,\u201d Lake Tahoe NV, USA, 2014.\n[19] T. Robinson, J. Fransen, D. Pye, J. Foote, and S. Renals, \u201cWSJCAM0: A british english speech corpus for large vocabulary continuous speech recognition,\u201d in Proceedings of the 1995 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP), Detroit MI, USA, 1995.\n[20] K. Vesely, L. Burget, and F. Grezl, \u201cParallel training of neural networks for speech recognition,\u201d in Proceedings of the 11th Annual Conference of the International Speech Communication Association (Interspeech), Makuhari, Japan, 2010.\n[21] Y. Liu, P. Zhang, and T. Hain, \u201cUsing neural network front-ends on far field multiple microphones based speech recognition,\u201d in Proceedings of the 2014 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 2014.\n[22] S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw, X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey et al., The HTK book (for HTK version 3.4). Cambridge, UK: Cambridge University Engineering Department, 2006.\n[23] M. Hoffman, F. R. Bach, and D. M. Blei, \u201cOnline learning for latent dirichlet allocation,\u201d in Proceedings of the Neural Information Processing System (NIPS) Workshop, Vancouver BC, Canada, 2010, pp. 856\u2013864.\n[24] R. Rehurek and P. Sojka, \u201cSoftware Framework for Topic Modelling with Large Corpora,\u201d in Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC), Valletta, Malta, 2010, pp. 45\u201350.\n[25] X. Wang and E. Grimson, \u201cSpatial latent dirichlet allocation,\u201d in Proceedings of the Neural Information Processing System (NIPS) Workshop, Whistler BC, Canada, 2008, pp. 1577\u20131584.\n[26] O. Saz, M. Doulaty, and T. Hain, \u201cBackground\u2013tracking acoustic features for genre identification of broadcast shows,\u201d in Proceedings of the 2014 IEEE Workshop on Spoken Language Technologies (SLT), Lake Tahoe NV, USA, 2014."}], "references": [{"title": "Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains", "author": ["J.-L. Gauvain", "C.-H. Lee"], "venue": "IEEE Transactions on Speech and audio processing, vol. 2, no. 2, pp. 291\u2013298, 1994.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models", "author": ["C.J. Leggetter", "P.C. Woodland"], "venue": "Computer Speech & Language, vol. 9, no. 2, pp. 171\u2013185, 1995.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Cluster adaptive training for speech recognition.", "author": ["M.J. Gales"], "venue": "Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP\u2013Interspeech), Sydney, Australia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Automatic Speech Recognition: A Deep Learning Approach", "author": ["D. Yu", "L. Deng"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Multi-level adaptive networks in tandem and hybrid ASR systems", "author": ["P. Bell", "P. Swietojanski", "S. Renals"], "venue": "Proceddings of the 2013 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP), Vancouver BC, Canada, 2013, pp. 6975\u20136979.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep maxout networks for lowresource speech recognition", "author": ["Y. Miao", "F. Metze", "S. Rawat"], "venue": "Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Olomouc, Czech Republic, 2013, pp. 398\u2013403.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u2013 1022, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Audio scene understanding using topic models", "author": ["S. Kim", "S. Sundaram", "P. Georgiou", "S. Narayanan"], "venue": "Proceedings of the Neural Information Processing System (NIPS) Workshop, Whistler BC, Canada, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Acoustic topic model for audio information retrieval", "author": ["S. Kim", "S. Narayanan", "S. Sundaram"], "venue": "Proceedings of the 2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), New Paltz NY, USA, 2009, pp. 37\u201340.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Discovering objects and their location in images", "author": ["J. Sivic", "B.C. Russell", "A.A. Efros", "A. Zisserman", "W.T. Freeman"], "venue": "Proceedings of the 10th International Conference on Computer Vision (ICCV), Beijing, China, 2005, pp. 370\u2013377.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "A probabilistic topic model for unsupervised learning of musical key-profiles.", "author": ["D. Hu", "L.K. Saul"], "venue": "Proceedings of the 10th International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The annals of mathematical statistics, vol. 2, no. 1, pp. 79\u201386, 1951.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1951}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 101, pp. 5228\u20135235, 2004.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Vector quantization and signal compression", "author": ["A. Gersho", "R.M. Gray"], "venue": "Berlin, Germany: Springer Science & Business Media,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "The Fisher corpus: A resource for the next generations of speech-to-text.", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "Proceedings of the 4th International Conference on Language Resources and Evaluation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "The AMI meeting corpus: A preannouncement", "author": ["J. Carletta", "S. Ashby", "S. Bourban", "M. Flynn", "M. Guillemot", "T. Hain", "J. Kadlec", "W. Karaiskos", "Vasilis Kraaij", "M. Kronenthal", "G. Lathoud", "M. Lincoln", "A. Lisowska", "I. McCowan", "W. Post", "D. Reidsma", "P. Wellner"], "venue": "Proceedings of the Third International Workshop on Machine Learning for Multimodal Interaction, Bethesda MD, USA, 2006, pp. 28\u201339.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "The ICSI meeting corpus", "author": ["A. Janin", "D. Baron", "J. Edwards", "D. Ellis", "D. Gelbart", "N. Morgan", "B. Peskin", "T. Pfau", "E. Shriberg", "A. Stolcke", "C. Wooters"], "venue": "Proceedings of the 2003 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP), Hong Kong, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "The USFD spoken language translation system for IWSLT 2014", "author": ["R.W.N. Ng", "M. Doulaty", "R. Doddipatla", "O. Saz", "M. Hasan", "T. Hain", "W. Aziz", "K. Shaf", "L. Specia"], "venue": "Lake Tahoe NV, USA, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "WSJCAM0: A british english speech corpus for large vocabulary continuous speech recognition", "author": ["T. Robinson", "J. Fransen", "D. Pye", "J. Foote", "S. Renals"], "venue": "Proceedings of the 1995 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP), Detroit MI, USA, 1995.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Parallel training of neural networks for speech recognition", "author": ["K. Vesely", "L. Burget", "F. Grezl"], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (Interspeech), Makuhari, Japan, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition", "author": ["Y. Liu", "P. Zhang", "T. Hain"], "venue": "Proceedings of the 2014 IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "The HTK book (for HTK version 3.4)", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Online learning for latent dirichlet allocation", "author": ["M. Hoffman", "F.R. Bach", "D.M. Blei"], "venue": "Proceedings of the Neural Information Processing System (NIPS) Workshop, Vancouver BC, Canada, 2010, pp. 856\u2013864.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. Rehurek", "P. Sojka"], "venue": "Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC), Valletta, Malta, 2010, pp. 45\u201350.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatial latent dirichlet allocation", "author": ["X. Wang", "E. Grimson"], "venue": "Proceedings of the Neural Information Processing System (NIPS) Workshop, Whistler BC, Canada, 2008, pp. 1577\u20131584.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Background\u2013tracking acoustic features for genre identification of broadcast shows", "author": ["O. Saz", "M. Doulaty", "T. Hain"], "venue": "Proceedings of the 2014 IEEE Workshop on Spoken Language Technologies (SLT), Lake Tahoe NV, USA, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "These domain dependent models have been usually trained via Maximum Likelihood (ML) if a sufficiently large amount of domain data existed or using adaptation techniques such as Maximum A Posteriori [1], Maximum Likelihood Linear Regression (MLLR) [2] or Cluster Adaptive Training [3].", "startOffset": 198, "endOffset": 201}, {"referenceID": 1, "context": "These domain dependent models have been usually trained via Maximum Likelihood (ML) if a sufficiently large amount of domain data existed or using adaptation techniques such as Maximum A Posteriori [1], Maximum Likelihood Linear Regression (MLLR) [2] or Cluster Adaptive Training [3].", "startOffset": 247, "endOffset": 250}, {"referenceID": 2, "context": "These domain dependent models have been usually trained via Maximum Likelihood (ML) if a sufficiently large amount of domain data existed or using adaptation techniques such as Maximum A Posteriori [1], Maximum Likelihood Linear Regression (MLLR) [2] or Cluster Adaptive Training [3].", "startOffset": 280, "endOffset": 283}, {"referenceID": 3, "context": "For more recent Deep Neural Network (DNN)\u2013based systems, domain adaptation is also possible with linear transformations, conservative training and subspace methods [4] with frameworks such as Multi\u2013Level Adaptive Networks (MLAN) [5] or Deep Maxout Networks (DMN) [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "For more recent Deep Neural Network (DNN)\u2013based systems, domain adaptation is also possible with linear transformations, conservative training and subspace methods [4] with frameworks such as Multi\u2013Level Adaptive Networks (MLAN) [5] or Deep Maxout Networks (DMN) [6].", "startOffset": 229, "endOffset": 232}, {"referenceID": 5, "context": "For more recent Deep Neural Network (DNN)\u2013based systems, domain adaptation is also possible with linear transformations, conservative training and subspace methods [4] with frameworks such as Multi\u2013Level Adaptive Networks (MLAN) [5] or Deep Maxout Networks (DMN) [6].", "startOffset": 263, "endOffset": 266}, {"referenceID": 6, "context": "LDA is an statistical approach to discover latent topics in a collection of documents in an unsupervised manner [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 7, "context": "In audio tasks, LDA has been used for classifying unstructured audio files into onomatopoeic and semantic descriptions with successful results [8, 9].", "startOffset": 143, "endOffset": 149}, {"referenceID": 8, "context": "In audio tasks, LDA has been used for classifying unstructured audio files into onomatopoeic and semantic descriptions with successful results [8, 9].", "startOffset": 143, "endOffset": 149}, {"referenceID": 6, "context": "Latent Dirichlet Allocation Latent Dirichlet Allocation (LDA) [7] is an unsupervised probabilistic generative model for collections of discrete data.", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "Also, an infinite mixture over an underlying set of topic probabilities is used to model each topic [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "LDA is mostly used for topic modelling of text corpora, however, the model can be applied to other tasks, such as object categorisation and localisation in image processing [10], automatic harmonic analysis in music processing [11] or acoustic information retrieval in unstructured audio analysis [9].", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "LDA is mostly used for topic modelling of text corpora, however, the model can be applied to other tasks, such as object categorisation and localisation in image processing [10], automatic harmonic analysis in music processing [11] or acoustic information retrieval in unstructured audio analysis [9].", "startOffset": 227, "endOffset": 231}, {"referenceID": 8, "context": "LDA is mostly used for topic modelling of text corpora, however, the model can be applied to other tasks, such as object categorisation and localisation in image processing [10], automatic harmonic analysis in music processing [11] or acoustic information retrieval in unstructured audio analysis [9].", "startOffset": 297, "endOffset": 300}, {"referenceID": 6, "context": "A reasonable approximate can be acquired using variational approximation which is shown to work reasonably well in various applications [7].", "startOffset": 136, "endOffset": 139}, {"referenceID": 11, "context": "Training tries to minimise the Kullback\u2013Leiber divergence (KLD) [12] between the real and the approximated joint probabilities (equations 2 and 3) [7]:", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "Training tries to minimise the Kullback\u2013Leiber divergence (KLD) [12] between the real and the approximated joint probabilities (equations 2 and 3) [7]:", "startOffset": 147, "endOffset": 150}, {"referenceID": 12, "context": "Other training methods based on Markov\u2013Chain Monte-Carlo is also proposed, like Gibbs sampling method [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 6, "context": "Since LDA is for collections of discrete data (such as text corpora) [7], every speech segment of length T frames, x = {x1, .", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "V } [8].", "startOffset": 4, "endOffset": 7}, {"referenceID": 13, "context": "First a Gaussian Mixture Model (GMM) is trained using Expectation Maximisation (EM) and mix\u2013up procedure to reach the desired codebook size V (enforcing the co\u2013variance matrix to be identity, equivalent to LBG\u2013VQ [14]).", "startOffset": 213, "endOffset": 217}, {"referenceID": 14, "context": "\u2022 Telephone speech (CT): From the Fisher corpus [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "\u2022 Meetings (MT): From AMI [16] and ICSI [17] corpora.", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "\u2022 Meetings (MT): From AMI [16] and ICSI [17] corpora.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "\u2022 Lectures (TK): From TedTalks [18].", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "\u2022 Read speech (RS): From the WSJCAM0 corpus [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "DNN training was performed with the TNet toolkit [20] and more details can be found at [21].", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "DNN training was performed with the TNet toolkit [20] and more details can be found at [21].", "startOffset": 87, "endOffset": 91}, {"referenceID": 21, "context": "For both types of features, baseline ML GMM\u2013HMM models were trained using HTK [22] with 5\u2013state crossword triphones and 16 gaussians per state.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Codebooks of size 128 up to 8,192 were used and given a codebook, different LDA models with a varying number of domains from 4 to 64 were estimated [23, 24] using the training data described in Section 4.", "startOffset": 148, "endOffset": 156}, {"referenceID": 23, "context": "Codebooks of size 128 up to 8,192 were used and given a codebook, different LDA models with a varying number of domains from 4 to 64 were estimated [23, 24] using the training data described in Section 4.", "startOffset": 148, "endOffset": 156}, {"referenceID": 11, "context": "Following this, KL divergence [12] was proposed as an appropriate metric to measure the consistency of the hidden topics discovered by LDA.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "In applying LDA for image processing, there are some variants of the original LDA model, such as Spatial LDA [25] which encodes spatial structure with the visual words.", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "Newer sets of features, better targeted to describe background acoustic characteristics [26], could also provide an improvement over PLP features, which are known to describe well phonetic and speaker information.", "startOffset": 88, "endOffset": 92}], "year": 2015, "abstractText": "Speech recognition systems are often highly domain dependent, a fact widely reported in the literature. However the concept of domain is complex and not bound to clear criteria. Hence it is often not evident if data should be considered to be out-of-domain. While both acoustic and language models can be domain specific, work in this paper concentrates on acoustic modelling. We present a novel method to perform unsupervised discovery of domains using Latent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is assumed to exist in the data, whereby each audio segment can be considered to be a weighted mixture of domain properties. The classification of audio segments into domains allows the creation of domain specific acoustic models for automatic speech recognition. Experiments are conducted on a dataset of diverse speech data covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech, with a joint training set of 60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to LDA based domains was shown to yield relative Word Error Rate (WER) improvements of up to 16% relative, compared to pooled training, and up to 10%, compared with models adapted with human-labelled prior domain knowledge.", "creator": "LaTeX with hyperref package"}}}