{"id": "1509.02604", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2015", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part II: Linear Convergence Analysis and Numerical Performance", "abstract": "The alternating direction form another ketones (ADMM) has been \u2014 as made versatile emphasize for difficulties modern similar - scale car ways without signal supply conflicts smoothly. When the data size way / them the happens dimension considered raised, a distributed titled that ADMM reason having used, which it controlling part distributing the deterministic cargo few given data three to a independent entire automated cytoplasmic. Unfortunately, old direct radio-location initiatives it such polynomial does sure damage several with the problem component, as after algorithm capacity is limited by a slowest web-based nodes. To address this country, both close character paper, we so re an interfacing distributed ADMM (AD - ADMM) been art now worst - issue functional generally. In much paper, come increasing came tests whose characterizing the risk it which the AD - ADMM laudable non-linear evolution. Our pressure as well a this resulting horizontal discount answers next indeed that certain optimized parameters, channels delay and cellular similar which followed the algorithm impact. To demonstrate entered superior this adequate while all introduce AD - ADMM, need evaluation made AD - ADMM started he high - experience operating fornax though obstacles now primarily - phase connectivity regression rather.", "histories": [["v1", "Wed, 9 Sep 2015 02:07:27 GMT  (118kb)", "http://arxiv.org/abs/1509.02604v1", "submitted for publication, 28 pages"]], "COMMENTS": "submitted for publication, 28 pages", "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.SY", "authors": ["tsung-hui chang", "wei-cheng liao", "mingyi hong", "xiangfeng wang"], "accepted": false, "id": "1509.02604"}, "pdf": {"name": "1509.02604.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part II: Linear Convergence Analysis and Numerical Performance", "authors": ["Tsung-Hui Chang", "Wei-Cheng Liao", "Mingyi Hong", "Xiangfeng Wang"], "emails": ["tsunghui.chang@ieee.org.", "mhong@umn.edu", "mingyi@iastate.edu", "xfwang@sei.ecnu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n02 60\n4v 1\n[ cs\n.D C\n] 9\nS ep\nKeywords\u2212 Distributed optimization, ADMM, Asynchronous, Consensus optimization\n\u22c6Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172, E-mail: tsunghui.chang@ieee.org.\n\u2020Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: mhong@umn.edu\n\u2020Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu\n\u2021Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn\nSeptember 10, 2015 DRAFT\n2 I. INTRODUCTION\nConsider the following optimization problem\nmin x\u2208Rn\nN\u2211\ni=1\nfi(x) + h(x), (1)\nwhere each fi : Rn \u2192 R is the cost function and h : Rn \u2192 R \u222a {\u221e} is a non-smooth, convex regularization function. The regularization function is used for obtaining structured solutions (e.g., sparsity) and/or is an indicator function which enforces x to lie in a constraint set [2, Section 5]. Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.\nDistributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]\u2013[14]. Our interest in this paper lies in the distributed optimization method based on the alternating direction method of multipliers (ADMM) [2, Section 7.1.1]. The ADMM is a convenient approach of distributing the computation load of a very large-scale problem to a network of computing nodes. Specifically, consider a computer network with a star topology, where one master node coordinates the computation of a set of N distributed workers. Based on a consensus formulation, the distributed ADMM partitions the original problem into N subproblems, each of which contains either a small set of training samples or a subset of the learning parameters. At each iteration, the distributed workers solve the subproblems based on the local data and send the variable information to the master, who summarizes the variable information and broadcasts it back the workers. Through such iterative variable update and information exchange, the large-scale learning problem can be solved in a distributed and parallel manner.\nThe convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]\u2013[20]. For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number. Considering non-convex problems with smooth fi\u2019s, reference [16] presented conditions for which the distributed ADMM converges to the set of Karush-KuhnTucker (KKT) points. For problems with strongly convex and smooth fi\u2019s or problems satisfying certain error bound condition, references [17] and [21] respectively showed that the ADMM can even exhibit a linear convergence rate. References [18]\u2013[20] also showed similar linear convergence conditions for some variants of distributed ADMM in a network with a general topology. However, the distributed ADMM in [2], [16] have assumed a synchronous network, where at each iteration, the master always waits until all\nSeptember 10, 2015 DRAFT\n3 the workers report their variable information. Unfortunately, such synchronous protocol does not scale well with the problem size, as the algorithm speed is determined by the \u201cslowest\u201d workers. To improve the time efficiency, the works [22], [23] have generalized the distributed ADMM to an asynchronous network. Specifically, in the asynchronous distributed ADMM (AD-ADMM) proposed in [22], [23], the master does not necessarily wait for all the workers. Instead, the master updates its variable whenever it receives the variable information from a partial set of the workers. This prevents the master and speedy workers from spending most of the time waiting and consequently can improve the time efficiency of distributed optimization. Theoretically, it has been shown in [23] that the AD-ADMM is guaranteed to converge (to a KKT point) even for non-convex problem (1), under a bounded delay assumption only.\nThe contributions of this paper are twofold. Firstly, beyond the convergence analysis in [23], we further present the conditions for which the AD-ADMM can exhibit a linear convergence rate. Specifically, we show that for problem (1) with some structured convex fi\u2019s (e.g., strongly convex), the augmented Lagrangian function of the AD-ADMM can decrease by a constant fraction in every iteration of the algorithm, as long as the algorithm parameters are chosen appropriately according to the network delay. We give explicit expressions on the linear convergence conditions and the linear rate, which illustrate how the algorithm and network parameters impact on the algorithm performance. To the best of our knowledge, our results are novel, and are by no means extensions of the existing analyses [17]\u2013[21] for synchronous ADMM. Secondly, we present extensive numerical results to demonstrate the time efficiency of the AD-ADMM over its synchronous counterpart. In particular, we consider a large-scale LR problem and implement the AD-ADMM on a high-performance computer cluster. The presented numerical results show that the AD-ADMM significantly reduces the practical running time of distributed optimization.\nSynopsis: Section II reviews the AD-ADMM in [23]. The linear convergence analysis is presented in Section III and the proofs are presented in Section IV. Numerical results are given in Section V and conclusions are drawn in Section VI.\nII. ASYNCHRONOUS DISTRIBUTED ADMM\nIn this section, we review the AD-ADMM proposed in [23]. The distributed ADMM [2, Section 7.1.1]\nis derived based on the following consensus formulation of (1):\nmin x0,xi\u2208Rn, i=1,...,N\nN\u2211\ni=1\nfi(xi) + h(x0) (2a)\ns.t. xi = x0 \u2200i \u2208 V , {1, . . . , N}. (2b)\nSeptember 10, 2015 DRAFT\n4 By applying the standard ADMM [7] to problem (2), one obtains the following three simple steps: for iteration k = 0, 1, . . . , update\nxk+10 =arg min x0\u2208Rn\n{ h(x0)\u2212 x T 0 \u2211N i=1 \u03bb k i + \u03c1 2 \u2211N i=1 \u2016x k i \u2212 x0\u2016 2 } , (3)\nxk+1i =arg min xi\u2208Rn fi(xi) + x T i \u03bb k i + \u03c1 2\u2016xi \u2212 x k+1 0 \u2016 2 \u2200i \u2208 V, (4) \u03bbk+1i = \u03bb k i + \u03c1(x k+1 i \u2212 x k+1 0 ) \u2200i \u2208 V. (5)\nAs seen, the distributed ADMM is designed for a computing network with a star topology that consists of one master node and a set of N workers (see Fig. 1 in [23]). In particular, the master is responsible for optimizing the variable x0 by (3), while each worker i, i \u2208 V , takes charge of optimizing variables xi and \u03bbi by (4) and (5), respectively. Once the master updates x0, it broadcasts x0 to the workers; each worker i then updates (xi,\u03bbi) based on the received x0, and sends the new (xi,\u03bbi) to the master. Through such iterative variable update and message exchange, problem (2) is solved in a fully parallel and distributed fashion.\nHowever, to implement (3)-(5), the master and the workers have to be synchronized with each other. Specifically, according to (3), the master proceeds to update x0 only if it has received update-to-date (xi,\u03bbi) from all the workers. This implies that the optimization speed would be determined by the slowest worker in the network. This is in particular the case in a heterogeneous network where the workers experience different computation and communication delays, in which case the master and speedy workers would idle most of the time.\nThe distributed ADMM has been extended to an asynchronous network in [22], [23]. In the ADADMM, the master does not wait for all the workers, but updates the variable x0 as long as it receives variable information from a partial set of workers instead. This would greatly reduce the waiting time of the master, and improve the overall time efficiency of distributed optimization. The AD-ADMM is presented in Algorithm 1, which includes the algorithmic steps of the master and those of the workers. Here, we denote k as the iteration number of the master (i.e., the number of times for which the master updates x0), and assume that, at each iteration k, the master receives variable information from workers in the set Ak \u2286 V , {1, . . . , N}. Worker i is said to be \u201carrived\u201d at iteration k if i \u2208 Ak and unarrived otherwise. Notation Ack denotes the complementary set of Ak, i.e., Ak \u2229 A c k = \u2205 and Ak \u222a A c k = V . Moreover, variables di\u2019s are used to count the numbers of delayed iterations of the workers. The variables \u03c1 and \u03b3 are two penalty parameters.\nSeptember 10, 2015 DRAFT\n5 In the AD-ADMM, the master inevitably uses delayed and old variable information for updating x0. As shown in step 4 of Algorithm of the Master, to ensure the used variable information not too stale, the master would wait until it receives the update-to-date (xi,\u03bbi) from all the workers that have di \u2265 \u03c4 \u2212 1, if any (so all the workers i \u2208 Ack must have di < \u03c4 \u2212 1). This condition guarantees that the variable information is at most \u03c4 iterations old, and is known as the partially asynchronous model [7]:\nAssumption 1 (Bounded delay) Let \u03c4 \u2265 1 be a maximum tolerable delay. For all i \u2208 V and iteration k, it must be that i \u2208 Ak \u222a Ak\u22121 \u00b7 \u00b7 \u00b7 \u222a Ak\u2212\u03c4+1.\nIn [23, Theorem 1], we have shown that under Assumption 1, some smoothness conditions on the cost functions fi\u2019s (see [23, Assumption 2]) and for sufficiently large \u03c1 and \u03b3, the AD-ADMM in Algorithm 1 is provably convergent to the set of KKT points of problem (2). Notably, this convergence property holds even for non-convex fi\u2019s. In the next section, we focus on convex fi\u2019s, and further characterize the linear convergence conditions of the AD-ADMM.\nIII. LINEAR CONVERGENCE RATE ANALYSIS\nIn this section, we show that the AD-ADMM can achieve linear convergence for some structured convex functions. We first make the following convex assumption on problem (1) (or equivalently, problem (2)).\nAssumption 2 Each function fi is a proper closed convex function and is continuously differentiable; each gradient \u2207fi is Lipschitz continuous with a Lipschitz constant L > 0; the function h is convex (not necessarily smooth). Moreover, problem (1) is bounded below, i.e., F \u22c6 > \u2212\u221e where F \u22c6 denotes the optimal objective value of problem (1).\nAssumption 2 is the same as [23, Assumption 2], except that fi\u2019s are assumed convex here. Given this\nconvex property, it is well known that the augmented Lagrangian function, i.e.,\nL\u03c1(x k,xk0 ,\u03bb k) =\nN\u2211\ni=1\nfi(x k i ) + h(x k 0) +\nN\u2211\ni=1\n(\u03bbki ) T (xki \u2212 x k 0)\n+ \u03c1\n2\nN\u2211\ni=1\n\u2016xki \u2212 x k 0\u2016 2, (12)\nwould converge to F \u22c6 whenever the iterates ({xki } N i=1,x k 0 , {\u03bb k i } N i=1) approaches the optimal solution of problem (2). Therefore, our analysis is based on characterizing how L\u03c1(xk,xk0 ,\u03bb k) can converge to F \u22c6 linearly. Let us define\n\u25b3k , L\u03c1(x k,xk0 ,\u03bb k)\u2212 F \u22c6. (13)\nSeptember 10, 2015 DRAFT\n6 It has been shown in [23, Lemma 3] that \u25b3k \u2265 0 for all k as long as \u03c1 \u2265 L.\nIn the ensuing analysis, we consider two types of structured convex cost functions, respectively\ndescribed in the following two assumptions.\nAssumption 3 For all i \u2208 V, each function fi is strongly convex with modulus \u03c32 > 0.\nAssumption 4 Each function fi(x) = gi(Aix), \u2200i \u2208 V, where gi : Rm \u2192 R is a strongly convex function with modulus \u03c32 > 0 and Ai \u2208 Rm\u00d7n is a nonzero matrix with arbitrary rank. Moreover, h(x) = 0.\nNote that in Assumption 4 matrix Ai can have an arbitrary rank, so fi(x) is not necessarily strongly convex with respect to x. Interestingly, such structured cost function appears in many machine learning problems, for example, the least squared problem and the logistic regression problem [5].\nLet us first consider the strongly convex case. Under Assumption 3, the linear convergence conditions\nof the AD-ADMM are given by the following theorem.\nTheorem 1 Suppose that Assumptions 1, 2 and 3 hold true. Moreover, assume that there exists a constant S \u2208 [1, N ] such that |Ak| < S for all k and that\n\u03c1 \u2265 max\n{ (1 + L2) + \u221a (1 + L2)2 + 8L2\u03b1(\u03c4)\n2 , \u03c32 +\n1\n8N\n} , (14)\n\u03b3 \u2265 max { \u03b2(\u03c1, \u03c4) \u2212 N\u03c1\n2 + 1, 8N(\u03c1 \u2212 \u03c32)\n} , (15)\nwhere \u03b1(\u03c4) , 1 + 2+2 \u03c4 (\u03c4\u22121) 1+8N\u03c32 and \u03b2(\u03c1, \u03c4) , 2(\u03c4 \u2212 1)[( (1+\u03c12)S+S/N 2 )(2 \u03c4\u22121 \u2212 1) + (4\u03c4\u22121 \u2212 1)]. Then, the iterates generated by (6), (7) and (9) satisfy\n0 \u2264 \u25b3k+1 \u2264\n( 1\n1 + 1\u03b4\u03b3\n)k+1 \u25b30, (16)\nwhere \u03b4 is a constant satisfying\n\u03b4 \u2265 max { 1, \u03c1N + \u03b3\n\u03c32N \u2212 1\n} . (17)\nTheorem 1 asserts that, for problem (2) with strongly convex fi\u2019s, the augmented Lagrange function can decrease linearly to zero, as long as \u03c1 and \u03b3 are large enough (exponentially increasing with \u03c4 ). Equation (16) also implies that the linear rate would decrease with the delay \u03c4 and the number of workers in the worst case.\nAnalogous to Theorem 1, the following theorem shows that the AD-ADMM can achieve linear\nconvergence under Assumption 4.\nSeptember 10, 2015 DRAFT\n7 Theorem 2 Suppose that Assumptions 1, 2 and 4 hold true. Moreover, assume that there exists a constant S \u2208 [1, N ] such that |Ak| < S for all k and that\n\u03c1 \u2265 max\n{ (1 + L2) + \u221a (1 + L2)2 + 8L2\u03b1(\u03c4)\n2 , \u03c32 +\n1\n8N\n} ,\n\u03b3 \u2265 max { \u03b2(\u03c1, \u03c4) \u2212 N\u03c1\n2 + 1, 8N(\u03c1 \u2212 \u03c32/c) + 4N\u03c32\n} ,\nfor some constant c > 0. Then, the iterates generated by (6), (7) and (9) satisfy (16) with \u03b4 satisfying\n\u03b4 \u2265 max { 1, \u03c1N + \u03b3\nN\u03c32/c \u2212 1\n} .\nSince it has been known that the (synchronous) distributed ADMM [17]\u2013[21] can converge linearly given the same structured cost functions in Assumption 3 and Assumption 4, the convergence results presented above demonstrate that the linear convergence property can be preserved in the asynchronous network. We remark that (14) and (15) are sufficient conditions only. In practice, the AD-ADMM could still exhibit a linear convergence rate without exactly satisfying these conditions.\nThe proofs of Theorem 1 and Theorem 2 are presented in the next section. The readers who are more\ninterested in the numerical performance of the AD-ADMM may jump to Section V.\nIV. PROOFS OF THEOREMS\nA. Preliminaries and Key Lemmas\nLet us present some basic inequalities that will be used frequently in the ensuing analysis and key\nlemmas for proving Theorem 1 and Theorem 2.\nWe will frequently use the following inequality due to Jensen\u2019s inequality: for any ai, i = 1, . . . ,M ,\n\u2016 \u2211M\ni=1 ai\u2016 2 \u2264 M \u2211M i=1 \u2016ai\u2016 2. (18)\nMoreover, for any a, b and \u03b4 > 0,\n\u2016a + b\u20162 \u2264 (1 + \u03b4)\u2016a\u20162 + (1 + 1\n\u03b4 )\u2016b\u20162. (19)\nThe equality is also known to be true: for any vectors a, b, c and d,\n(a\u2212 b)T (c\u2212 d) = 1\n2 \u2016a\u2212 d\u20162 \u2212\n1 2 \u2016a\u2212 c\u20162\n+ 1\n2 \u2016b\u2212 c\u20162 \u2212\n1 2 \u2016b\u2212 d\u20162. (20)\nSeptember 10, 2015 DRAFT\n8 We follow [23, Algorithm 3] to write Algorithm 1 from the master\u2019s point of view as follows:\nxk+1i =    arg min xi\u2208Rn { fi(xi) + x T i \u03bb k i + \u03c1 2\u2016xi \u2212 x k\u0304i+1 0 \u2016 2 } , \u2200i \u2208 Ak\nxki \u2200i \u2208 A c k\n, (21)\n\u03bbk+1i =\n{ \u03bbki + \u03c1(x k+1 i \u2212 x k\u0304i+1 0 ) \u2200i \u2208 Ak\n\u03bbki \u2200i \u2208 A c k\n, (22)\nxk+10 =arg min x0\u2208Rn\n{ h(x0)\u2212 x T 0 \u2211N i=1 \u03bb k+1 i\n+ \u03c12 \u2211N i=1 \u2016x k+1 i \u2212 x0\u2016 2 + \u03b32\u2016x0 \u2212 x k 0\u2016 2 } . (23)\nHere, index k\u0304i in (21) and (22) represents the last iteration number before iteration k for which worker i \u2208 Ak is arrived, i.e., i \u2208 Ak\u0304i . Under Assumption 1, it must hold\nk \u2212 \u03c4 \u2264 k\u0304i < k \u2200k. (24)\nFurthermore, for workers i \u2208 Ack, let us denote k\u0303i as the last iteration number before iteration k for which worker i is arrived, i.e., i \u2208 Ak\u0303i . Then, under Assumption 1, it must hold\nk \u2212 \u03c4 < k\u0303i < k \u2200k. (25)\nIn addition, denote k\u0302i (k\u0303i\u2212 \u03c4 \u2264 k\u0302i < k\u0303i) as the last iteration number before iteration k\u0303i for which worker i \u2208 Ak\u0303i is arrived, i.e., i \u2208 Ak\u0302i . Then by (21) and (22), for all workers i \u2208 A c k, we must have\nxk\u0303i+1i = x k\u0303i+2 i = \u00b7 \u00b7 \u00b7 = x k i = x k+1 i , (26) \u03bbk\u0303i+1i = \u03bb k\u0303i+2 i = \u00b7 \u00b7 \u00b7 = \u03bb k i = \u03bb k+1 i , (27)\nSince i \u2208 Ak\u0303i for all i \u2208 A c k and by (26)-(27), we can equivalently write (21) and (22) for all i \u2208 A c k as\nxk+1i = x k\u0303i+1 i\n= arg min xi\nfi(xi) + x T i \u03bb k\u0303i i + \u03c1 2\u2016xi \u2212 x k\u0302i+1 0 \u2016 2, (28)\n\u03bbk+1i = \u03bb k\u0303i+1 i = \u03bb k\u0303i i + \u03c1(x k\u0303i+1 i \u2212 x k\u0302i+1 0 )\n= \u03bbk\u0303ii + \u03c1(x k+1 i \u2212 x k\u0302i+1 0 ). (29)\nBased on these notations, we have shown in [23, Eqn. (33)] that the following lemma is true.\nSeptember 10, 2015 DRAFT\n9 Lemma 1 Suppose that Assumption 2 holds and \u03c1 \u2265 L. Then, for all k = 0, 1, . . . ,\n0 \u2264\u25b3k+1 \u2264 \u25b3k +\n( 1 + \u03c1/\u01eb\n2\n)\u2211\ni\u2208Ak\n\u2016xk0 \u2212 x k\u0304i+1 0 \u2016 2\n\u2212\n( 2\u03b3 +N\u03c1\n2\n) \u2016xk+10 \u2212 x k 0\u2016 2 + ( L2 + (\u01eb\u2212 1)\u03c1\n2 +\nL2\n\u03c1\n) \u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k i \u2016 2, (30)\nwhere \u01eb \u2208 (0, 1) is a constant.\nIn particular, (30) is the same as [23, Eqn. (33)] except that here we have assumed convex fi\u2019s. Lemma\n1 shows how the gap between the augmented Lagrangian function L\u03c1(xk+1,x k+1 0 ,\u03bb k+1) and the optimal objective value F \u22c6 evolves with the iteration number k. Notice that it follows from [23, Lemma 3] that \u25b3k+1 \u2265 0 for all k if \u03c1 \u2265 L. As will be seen shortly, Lemma 1 is crucial in the linear convergence analysis.\nSimilar to [23, Lemma 3], we next need to bound the error terms, e.g., (1+\u03c1 2 2 ) \u2211 i\u2208Ak \u2016xk0 \u2212 x k\u0304i+1 0 \u2016 2 in (30), which is caused by asynchrony of the network. Here, we present a more general result for the latter analysis.\nLemma 2 Let \u03b7 > 0 and j \u2212 \u03bd \u2264 ji < j where \u03bd \u2208 Z++, ji \u2208 Z+ and j \u2208 {0, 1, . . . , k}. Moreover, let Nj \u2282 V be any index subset satisfying |Nj| \u2264 N\u0304 for some constant N\u0304 \u2208 (1, N ]. Then, the following inequality holds true\nk\u2211\nj=0\n\u03b7j \u2211\ni\u2208Nj\n\u2016xj0 \u2212 x ji+1 0 \u2016\n2 \u2264 (\u03bd \u2212 1)N\u0304 k\u22121\u2211\nj=0\n\u03b7j+1 ( \u03b7\u03bd\u22121 \u2212 1\n\u03b7 \u2212 1\n) \u2016xj0 \u2212 x j+1 0 \u2016 2. (31)\nProof: See Appendix B.\nNow let us consider Assumption 3. For strongly convex f \u2032is, it is known that the following first-order\ncondition holds [24]: \u2200x,y,\nfi(y) \u2265 fi(x) + (\u2207fi(x)) T (y \u2212 x) +\n\u03c32\n2 \u2016y \u2212 x\u20162. (32)\nBased on this property, we can bound \u25b3k+1 as follows.\nLemma 3 Suppose that Assumptions 2 and 3 hold and \u03c1 \u2265 \u03c32. If \u03b3 \u2265 8N(\u03c1\u2212 \u03c32) and \u03b4 satisfies (17), then it holds that\n1\n\u03b3\u03b4 \u25b3k+1 \u2264\nL2\n4\u03c12N\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k i \u2016 2\nSeptember 10, 2015 DRAFT\n10\n+ L2\n4\u03c12N\n\u2211\ni\u2208Ack\n\u2016xk\u0303i+1i \u2212 x k\u0303i i \u2016\n2 + 1\n2N\n\u2211\ni\u2208Ak\n\u2016xk0 \u2212 x k\u0304i+1 0 \u2016 2\n+ 1\n2N\n\u2211\ni\u2208Ack\n\u2016xk0 \u2212 x k\u0302i+1 0 \u2016 2 + \u2016xk+10 \u2212 x k 0\u2016 2. (33)\nInstead, if \u03b3 = 0 and \u03b4 \u2265 max{\u03c1/\u03c32 \u2212 1, 1}, then it holds (\n1\n4(\u03c1\u2212 \u03c32)N\u03b4\n) \u25b3k+1 \u2264 L2\n2\u03c12N\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k i \u2016 2\n+ L2\n2\u03c12N\n\u2211\ni\u2208Ack\n\u2016xk\u0303i+1i \u2212 x k\u0303i i \u2016\n2 + 1\nN\n\u2211\ni\u2208Ak\n\u2016xk0 \u2212 x k\u0304i+1 0 \u2016 2\n+ 1\nN\n\u2211\ni\u2208Ack\n\u2016xk0 \u2212 x k\u0302i+1 0 \u2016 2 + \u2016xk+10 \u2212 x k 0\u2016 2. (34)\nProof: See Appendix C.\nB. Proof of Theorem 1\nWe use the lemmas above to prove Theorem 1. Denote \u03b7 , 1 + 1\u03b4\u03b3 . By summing (30) and (33), we\nobtain\n\u25b3k+1 \u2264 1\n\u03b7 \u25b3k +\n1 \u03b7\n[( L2 + (\u01eb\u2212 1)\u03c1+ L 2\n2\u03c12N\n2 +\nL2\n\u03c1\n) N\u2211\ni=1\n\u2016xk+1i \u2212 x k i \u2016 2\n\u2212\n( 2\u03b3 +N\u03c1\n2 \u2212 1\n) \u2016xk+10 \u2212 x k 0\u2016 2 + 1\n2N\n\u2211\ni\u2208Ack\n\u2016xk0 \u2212 x k\u0302i+1 0 \u2016 2\n+ L2\n4\u03c12N\n\u2211\ni\u2208Ack\n\u2016xk\u0303i+1i \u2212 x k\u0303i i \u2016 2 2 +\n( 1 + \u03c1/\u01eb\n2 +\n1\n2N\n) \u2211\ni\u2208Ak\n\u2016xk0 \u2212 x k\u0304i+1 0 \u2016 2\n] . (35)\nHere, we have used the fact of \u2211\ni\u2208Ak \u2016xk+1i \u2212x k i \u2016\n2 = \u2211N\ni=1 \u2016x k+1 i \u2212x k i \u2016 2 as xk+1i = x k i \u2200i \u2208 A c k. By\ntaking the telescoping sum of (35), we further obtain \u25b3k+1 \u2264 1\n\u03b7k+1 \u25b30\n+ 1\n\u03b7\n[( L2 + (\u01eb\u2212 1)\u03c1+ L 2\n2\u03c12N + 2L2 \u03c1\n2\n) k\u2211\n\u2113=0\n1\n\u03b7\u2113\nN\u2211\ni=1\n\u2016xk\u2212\u2113+1i \u2212x k\u2212\u2113 i \u2016 2 \u2212\n( 2\u03b3 +N\u03c1\n2 \u2212 1\n) k\u2211\n\u2113=0\n1\n\u03b7\u2113 \u2016xk\u2212\u2113+10 \u2212 x k\u2212\u2113 0 \u2016 2\n+\n( 1 + \u03c1/\u01eb\n2 +\n1\n2N\n) k\u2211\n\u2113=0\n1\n\u03b7\u2113\n\u2211\ni\u2208Ak\u2212\u2113\n\u2016xk\u2212\u21130 \u2212 x (k\u2212\u2113) i +1 0 \u2016 2\n\ufe38 \ufe37\ufe37 \ufe38 (36a)\n+ 1\n2N\nk\u2211\n\u2113=0\n1\n\u03b7\u2113\n\u2211\ni\u2208Ack\u2212\u2113\n\u2016xk\u2212\u21130 \u2212 x (\u0302k\u2212\u2113) i +1 0 \u2016 2\n\ufe38 \ufe37\ufe37 \ufe38 (36b)\n+ L2\n4\u03c12N\nk\u2211\n\u2113=0\n1\n\u03b7\u2113\n\u2211\ni\u2208Ack\u2212\u2113\n\u2016x (\u0303k\u2212\u2113) i +1 i \u2212 x (\u0303k\u2212\u2113) i i \u2016 2 2\n\ufe38 \ufe37\ufe37 \ufe38 (36c)\n] . (36)\nSeptember 10, 2015 DRAFT\n11\nThe three terms (36a), (36b), and (36c) in the right hand side (RHS) of (36) can respectively be bounded as follows, using Lemma 2. Consider the change of variable k \u2212 \u2113 = j. Then, we have the following chain for (36a):\n(36a) =\nk\u2211\n\u2113=0\n1\n\u03b7\u2113\n\u2211\ni\u2208Ak\u2212\u2113\n\u2016xk\u2212\u21130 \u2212 x (k\u2212\u2113) i +1 0 \u2016 2\n= 1\n\u03b7k\nk\u2211\nj=0\n\u03b7j \u2211\ni\u2208Aj\n\u2016xj0 \u2212 x j\u0304i+1 0 \u2016 2\n\u2264 1\n\u03b7k S(\u03c4 \u2212 1)\nk\u22121\u2211\nj=0\n\u03b7j+1 ( \u03b7\u03c4\u22121 \u2212 1\n\u03b7 \u2212 1\n) \u2016xj0 \u2212 x j+1 0 \u2016 2\n= S(\u03c4 \u2212 1)\u03b7\n( \u03b7\u03c4\u22121 \u2212 1\n\u03b7 \u2212 1\n) k\u2211\n\u2113=1\n1\n\u03b7\u2113 \u2016xk\u2212\u21130 \u2212 x k\u2212\u2113+1 0 \u2016 2, (37)\nwhere the inequality is obtained by applying (31) with \u03bd = \u03c4 , Nj = Aj , N\u0304 = S, and ji = j\u0304i which satisfies j\u2212 \u03c4 \u2264 j\u0304i < j (see (24)); to obtain the last equality, the change of variable k\u2212 \u2113 = j is applied again.\nAnalogously, by applying (31) with \u03bd = 2\u03c4 \u2212 1, Nj = Acj , N\u0304 = N , and ji = j\u0302i (which satisfies j\u2212 2\u03c4 +1 \u2264 j\u0302i < j since j\u0303i\u2212 \u03c4 \u2264 j\u0302i < j\u0303i and j\u2212 \u03c4 < j\u0303i < j by (24) and (25)), one can bound (36b) as\n(36b) =\nk\u2211\n\u2113=0\n1\n\u03b7\u2113\n\u2211\ni\u2208Ack\u2212\u2113\n\u2016xk\u2212\u21130 \u2212 x (\u0302k\u2212\u2113) i +1 0 \u2016 2\n= 1\n\u03b7k\nk\u2211\nj=0\n\u03b7j \u2211\ni\u2208Acj\n\u2016xj0 \u2212 x j\u0302i+1 0 \u2016 2\n\u2264 1\n\u03b7k 2N(\u03c4 \u2212 1)\nk\u22121\u2211\nj=0\n\u03b7j+1 ( \u03b72(\u03c4\u22121) \u2212 1\n\u03b7 \u2212 1\n) \u2016xj0 \u2212 x j+1 0 \u2016 2\n= 2N(\u03c4 \u2212 1)\u03b7\n( \u03b72(\u03c4\u22121) \u2212 1\n\u03b7 \u2212 1\n) k\u2211\n\u2113=1\n1\n\u03b7\u2113 \u2016xk\u2212\u21130 \u2212 x k\u2212\u2113+1 0 \u2016 2. (38)\nThe term (36c) can be bounded as follows\n(36c) =\nk\u2211\n\u2113=0\n1\n\u03b7\u2113\n\u2211\ni\u2208Ack\u2212\u2113\n\u2016x (\u0303k\u2212\u2113) i +1 i \u2212 x (\u0303k\u2212\u2113) i i \u2016 2\n= 1\n\u03b7k\nk\u2211\nj=0\n\u03b7j \u2211\ni\u2208Acj\n\u2016xj\u0303i+1i \u2212 x j\u0303i i \u2016 2\n= 1\n\u03b7k\nk\u2211\nj=0\n\u2211\ni\u2208Acj\n\u03b7j\u2212j\u0303i\u22121\u03b7j\u0303i+1\u2016xj\u0303i+1i \u2212 x j\u0303i i \u2016 2\nSeptember 10, 2015 DRAFT\n12\n\u2264 \u03b7\u03c4\u22122 1\n\u03b7k\nk\u2211\nj=0\n\u2211\ni\u2208Acj\n\u03b7j\u0303i+1\u2016xj\u0303i+1i \u2212 x j\u0303i i \u2016 2\n\u2264 \u03b7\u03c4\u22122(\u03c4 \u2212 1) 1\n\u03b7k\nN\u2211\ni=1\nk\u2211\nj=0\n\u03b7j+1\u2016xj+1i \u2212 x j i\u2016 2\n= \u03b7\u03c4\u22121(\u03c4 \u2212 1) N\u2211\ni=1\nk\u2211\n\u2113=0\n1\n\u03b7\u2113 \u2016xk\u2212\u2113+1i \u2212 x k\u2212\u2113 i \u2016 2, (39)\nwhere, in the first inequality, we have used the fact of j\u2212 \u03c4 +1 \u2264 j\u0303i < j from (25). To show the second inequality, notice that for any i \u2208 Acj , it also satisfies i \u2208 A c \u2113 for \u2113 = j\u0303i + 1, . . . , j. So, j\u0303i = \u2113\u0303i for \u2113 = j\u0303i + 1, . . . , j. Since j \u2212 \u03c4 < j\u0303i < j, each \u03b7j\u0303+1\u2016x j\u0303+1 i \u2212 x j\u0303i i \u2016 2 appears no more than \u03c4 \u2212 1 times in the summation \u2211k\nj=0 \u2211 i\u2208Acj \u03b7j\u0303+1\u2016xj\u0303+1i \u2212 x j\u0303i i \u2016 2.\nBy substituting (39), (38) and (37) into (36), we obtain\n\u25b3k+1 \u2264 1\n\u03b7k+1 \u25b30\n+ 1\n\u03b7\n[( 1 + \u03c1/\u01eb\n2 +\n1\n2N\n) S(\u03c4 \u2212 1)\u03b7 ( \u03b7\u03c4\u22121 \u2212 1\n\u03b7 \u2212 1\n)\n+ (\u03c4 \u2212 1)\u03b7\n( \u03b72(\u03c4\u22121) \u2212 1\n\u03b7 \u2212 1\n)\n+ 1\u2212\n( 2\u03b3 +N\u03c1\n2\n)] k\u2211\n\u2113=0\n1\n\u03b7\u2113 \u2016xk\u2212\u2113+10 \u2212 x k\u2212\u2113 0 \u2016 2\n+ 1\n\u03b7\n[( L2 + (\u01eb\u2212 1)\u03c1+ L 2\n2\u03c12N + 2L2 \u03c1\n2\n)\n+ \u03b7\u03c4\u22121(\u03c4 \u2212 1) L2\n4\u03c12N\n] N\u2211\ni=1\nk\u2211\n\u2113=0\n1\n\u03b7\u2113\nN\u2211\ni=1\n\u2016xk\u2212\u2113+1i \u2212 x k\u2212\u2113 i \u2016 2. (40)\nLet \u01eb = 1/\u03c1. Therefore, we see that (16) is true if\n\u03b3 \u2265 (\u03c4 \u2212 1)\u03b7\n[( S(1 + \u03c12) + S/N\n2\n)( \u03b7\u03c4\u22121 \u2212 1\n\u03b7 \u2212 1\n)\n+\n( \u03b72(\u03c4\u22121) \u2212 1\n\u03b7 \u2212 1\n)] \u2212 N\u03c1\n2 + 1, (41)\n\u03c1 \u2265 (1 + L2) + 2L2\n\u03c1 +\nL2\n2\u03c12N\n( 1 + \u03b7\u03c4\u22121(\u03c4 \u2212 1) ) . (42)\nLet \u03c1 \u2265 18N + \u03c3 2. Then (42) holds true if\n\u03c1 \u2265 (1 + L2) + 2L2\n\u03c1\n( 1 + 2 + 2\u03b7\u03c4\u22121(\u03c4 \u2212 1)\n1 + 8N\u03c32\n) . (43)\nSeptember 10, 2015 DRAFT\n13\nMoreover, since \u03b3 \u2265 8N(\u03c1\u2212 \u03c32) and \u03b4 > 1, we see that \u03b7 has an upper bound\n\u03b7 = 1 + 1\n\u03b4\u03b3 < 1 +\n1\n8N(\u03c1\u2212 \u03c32) < 2. (44)\nTherefore, (14) and (15) are sufficient conditions for (43) and (41), respectively. The proof is thus complete.\nC. Proof of Theorem 2\nThe key is to build a similar result as Lemma 3 under Assumption 4. Now, consider Assumption 4.\nLet x\u22c6 be an optimal solution to (1), and let\ny\u22c6i = Aix \u22c6, i = 1, . . . , N.\nThen, (y\u22c61 , . . . ,y \u22c6 N ) is unique since gi\u2019s are strongly convex. So, the optimal solution set to (2) can be defined as\nX \u22c6 = { (x0,x1, . . . ,xN ) |   y\u22c61 ...\ny\u22c6N\n  =   A1x1 ...\nANxN\n  ,\nxi = x0, i = 1, . . . , N\n} . (45)\nLet 1N+1 \u2297 P\u22c6(x\u0302) be the projection point of x\u0302 , (xT0 ,x T 1 , . . . ,x T N ) T onto X \u22c6, where \u2297 denotes the Kronecker product. It can be shown that the following lemma is true.\nLemma 4 Under Assumption 4, for any x\u0302 \u2208 Rn(N+1), it holds that N\u2211\ni=1\nfi(P \u22c6(x\u0302)) \u2265\nN\u2211\ni=1\nfi(xi) +\nN\u2211\ni=1\n(\u2207fi(xi)) T (P\u22c6(x\u0302)\u2212 xi)\n+\nN\u2211\ni=1\n\u03c32 2c \u2016P\u22c6(x\u0302)\u2212 xi\u2016 2 + \u03c32 2c \u2016P\u22c6(x\u0302)\u2212 x0\u2016 2\n\u2212 \u03c32\n2\nN\u2211\ni=1\n\u2016xi \u2212 x0\u2016 2, (46)\nfor some finite constant c > 0.\nProof: See Appendix D.\nLemma 4 implies that the structured fi\u2019s in Assumption 4 own an analogous property as the strongly convex functions in (32). Based on Lemma 4, the next lemma shows that one can still bound \u25b3k+1 as in Lemma 3 under Assumption 4.\nSeptember 10, 2015 DRAFT\n14\nLemma 5 Suppose that Assumptions 2 and 4 hold, and assume that \u03b3 \u2265 8N(\u03c1\u2212 \u03c32/c) + 4N\u03c32 and \u03b4 satisfies\n\u03b4 \u2265 max { 1, \u03c1N + \u03b3\nN\u03c32/c \u2212 1\n} . (47)\nThen, (33) holds true. Instead, if \u03b3 = 0 and \u03b4 \u2265 max{(c\u03c1)/\u03c32 \u2212 1, 1}, then\n\u25b3k+1 2N [2(\u03c1 \u2212 \u03c32/c)\u03b4 + \u03c32] \u2264 L2 2\u03c12N\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k i \u2016 2\n+ L2\n2\u03c12N\n\u2211\ni\u2208Ack\n\u2016xk\u0303i+1i \u2212 x k\u0303i i \u2016\n2 + 1\nN\n\u2211\ni\u2208Ak\n\u2016xk0 \u2212 x k\u0304i+1 0 \u2016 2\n+ 1\nN\n\u2211\ni\u2208Ack\n\u2016xk0 \u2212 x k\u0302i+1 0 \u2016 2 + \u2016xk+10 \u2212 x k 0\u2016 2. (48)\nProof: See Appendix E.\nGiven Lemma 5, Theorem 2 can be proved by following exactly the same steps as for Theorem 1 in\nSection IV-B. The details are omitted here.\nV. NUMERICAL RESULTS\nIn this section, we present some simulation results to examine the practical performance of the AD-\nADMM. We consider the following LR problem\nmin w\u2208W\nm\u2211\nj=1\nlog ( 1 + exp(\u2212yja T j w) ) (49)\nwhere y1, . . . , ym are the binary labels of the m training data, w \u2208 Rn is the regression variable and Ai = [a1, . . . ,am] T \u2208 Rm\u00d7n is the training data matrix. We used the MiniBooNE particle identification Data Set1 which contains 130065 training samples (m = 130065) and the learning parameter has a size of 50 (n = 50). The constraint set W is set to W = {w \u2208 Rn | |wi| \u2264 10 \u2200i = 1, . . . , n}. The AD-ADMM is implemented on an HP ProLiant BL280c G6 Linux Cluster (Itasca HPC in University of Minnesota). The n training samples are uniformly distributed to a set of N workers (N = 10, 15, 20). For each worker, we employed the fast iterative shrinkage thresholding algorithm (FISTA) [25] to solve the corresponding subproblem (10). The stepsize of FISTA is set to 0.0001 and the stopping condition is that the 2-norm of the gradient is less than 0.001. The penalty parameter \u03c1 of the AD-ADMM is set to 0.01. Interestingly, while the theoretical convergence conditions in [23, Theorem 1] and Theorem 1\n1https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification\nSeptember 10, 2015 DRAFT\n15\nall suggest that the penalty parameter \u03b3 should be large in the worst-case, we find that, for the problem instance we test here, it is also fine to set \u03b3 = 0.\nNote that the asynchrony in our setting comes naturally from the heterogeneity of the computation times of computing nodes. In our experiments, analogous to [22], we further constrained the minimum size of the active set Ak by |Ak| \u2265 A where A \u2208 [1, N ] is an integer. When A = N , it corresponds to the synchronous case where the master is forced to wait for all the workers at every iteration.\nFigure 1(a) and Figure 1(b) respectively display the convergence curves (objective value) of the ADADMM versus the iteration number and the running time (second), for various values of N and \u03c4 . Here we\nSeptember 10, 2015 DRAFT\n16\nset A = 1. One can observe from Figure 1(a) that, in terms of the iteration number, the convergence speed of the AD-ADMM slows down when \u03c4 increases. However, as seen from Figure 1(b), the AD-ADMM is actually faster than its synchronous counterpart (\u03c4 = 1), and the running time of the AD-ADMM can be further reduced with increased \u03c4 . We also observe that, when N increases, the advantages of the AD-ADMM compared to its synchronous counterpart reduces. This is because the computation load allocated to each worker decreases with N (as n is fixed), making all the workers experience similar computation delays.\nIn Figure 1(c) and Figure 1(d), we present the convergence curves of AD-ADMM with different values of A. We see from Figure 1(c) that when A increases, it always requires fewer number of iterations to\nSeptember 10, 2015 DRAFT\n17\nachieve convergence for all choices of parameters. From Figure 1(d), however we can observe that a larger value of A is not always beneficial in reducing the running time. Specifically, one can see that for N = 10, the running time of AD-ADMM decreases when one increases A from 1 to 2, whereas the running time increases a lot if one increases A to 4. One can observe similar results for N = 15 and N = 20.\nTo look into how the values of \u03c4 and A impact on the algorithm speed, in Figure 2, we respectively plot the computation time and the waiting time of the master node for various pairs of (\u03c4,A). The setting is the same as that in Figure 1, except that here the stopping condition of the AD-ADMM is that the objective value achieves 4.56 \u00d7 104. One can observe from these figures that, when \u03c4 increases, the computing load of the master also increases but the waiting time is significantly reduced. This explains why in Figure 1(b) the AD-ADMM requires a less running time compared with the synchronous ADMM. On the other hand, when A increases, the computation time of the master always decreases. This is because the master may take a smaller number of iterations to reach the target objective value (see Figure 1(c)) and have to spend more time waiting for slow workers. However, the overall waiting time of the master does not necessarily become larger or smaller with A. As seen from Figure 2(b) and Figure 2(d), when A increases from 1 to 2, the waiting time for N = 10 in Figure 2(b) increases, whereas the waiting time for N = 20 in Figure 2(d) decreases. However, for A = 4, the waiting times always become larger. Nevertheless, when comparing to the synchronous ADMM (i.e., (\u03c4,A) = (1, N)), we can see that the waiting time of the master in the AD-ADMM is always much smaller.\nVI. CONCLUSIONS\nIn this paper, we have analytically studied the linear convergence conditions of the AD-ADMM proposed in [23]. Specifically, we have shown that for strongly convex fi\u2019s (Assumption 3) or for fi\u2019s with the composite form in Assumption 4, the AD-ADMM is guaranteed to converge linearly, provided that the penalty parameter \u03c1 and the proximal parameter \u03b3 are chosen sufficiently large depending on the delay \u03c4 . When the delay \u03c4 is bounded and N is large, we have further shown that linear convergence can be achieved with zero proximal parameter (i.e.,\u03b3 = 0), and with a delay-independent \u03c1. The linear convergence conditions and the linear rate have been given explicitly, which relate the algorithm and network parameters with the algorithm worst-case convergence performance. The presented numerical examples have shown that in practice the AD-ADMM can effectively reduce the waiting time of the master node, and as a consequence improves the overall time efficiency of distributed optimization significantly.\nSeptember 10, 2015 DRAFT\n18\nAPPENDIX A\nBOUND OF CONSENSUS ERROR\nWe bound the size of the consensus error \u2211N\ni=1 \u2016x k+1 i \u2212x k+1 0 \u2016 2 in the following lemma.\nLemma 6 Under Assumption 2, it holds that\nN\u2211\ni=1\n\u2016xk+1i \u2212x k+1 0 \u2016\n2 \u2264 2L2\n\u03c12\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k i \u2016 2\n+ 2L2\n\u03c12\n\u2211\ni\u2208Ack\n\u2016xk\u0303i+1i \u2212 x k\u0303i i \u2016\n2 + 4 \u2211\ni\u2208Ak\n\u2016xk\u0304i+10 \u2212x k 0\u2016\n2 + 4 \u2211\ni\u2208Ack\n\u2016xk\u0302i+10 \u2212x k 0\u2016 2 + 4N\u2016xk+10 \u2212 x k 0\u2016 2.\n(A.1)\nProof: It follows from (22) and (29) that the following chain is true\nN\u2211\ni=1\n\u2016xk+1i \u2212x k+1 0 \u2016\n2 = \u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k\u0304i+1 0 + x k\u0304i+1 0 \u2212x k+1 0 \u2016 2\n+ \u2211\ni\u2208Ack\n\u2016xk+1i \u2212 x k\u0302i+1 0 + x k\u0302i+1 0 \u2212x k+1 0 \u2016 2\n\u2264 2 \u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k\u0304i+1 0 \u2016\n2 + 2 \u2211\ni\u2208Ack\n\u2016xk+1i \u2212 x k\u0302i+1 0 \u2016 2\n+ 2 \u2211\ni\u2208Ak\n\u2016xk\u0304i+10 \u2212x k+1 0 \u2016\n2 + 2 \u2211\ni\u2208Ack\n\u2016xk\u0302i+10 \u2212x k+1 0 \u2016 2\n\u2264 2\n\u03c12\n\u2211\ni\u2208Ak\n\u2016\u03bbk+1i \u2212 \u03bb k i \u2016\n2 + 2\n\u03c12\n\u2211\ni\u2208Ack\n\u2016\u03bbk\u0303i+1i \u2212 \u03bb k\u0303i i \u2016 2\n+ 2 \u2211\ni\u2208Ak\n\u2016xk\u0304i+10 \u2212 x k 0 + x k 0 \u2212x k+1 0 \u2016\n2 + 2 \u2211\ni\u2208Ack\n\u2016xk\u0302i+10 \u2212 x k 0 + x k 0 \u2212x k+1 0 \u2016 2\n\u2264 2\n\u03c12\n\u2211\ni\u2208Ak\n\u2016\u03bbk+1i \u2212 \u03bb k i \u2016\n2 + 2\n\u03c12\n\u2211\ni\u2208Ack\n\u2016\u03bbk\u0303i+1i \u2212 \u03bb k\u0303i i \u2016 2\n+ 4 \u2211\ni\u2208Ak\n\u2016xk\u0304i+10 \u2212x k 0\u2016\n2 + 4 \u2211\ni\u2208Ack\n\u2016xk\u0302i+10 \u2212x k 0\u2016 2 + 4N\u2016xk+10 \u2212 x k 0\u2016 2. (A.2)\nRecall from [23, Eqn. (38)] that\n\u2207fi(x k+1 i ) + \u03bb k+1 i = 0 \u2200i \u2208 V and \u2200k. (A.3)\nBy substituting (A.3) into (A.2) and by the Lipschitz continuity of \u2207fi, we obtain (A.1).\nSeptember 10, 2015 DRAFT\n19\nAPPENDIX B\nPROOF OF LEMMA 2\nIt is easy to show the following chain is true\nk\u2211\nj=0\n\u03b7j \u2211\ni\u2208Nj\n\u2016xj0 \u2212 x ji+1 0 \u2016 2 =\nk\u2211\nj=0\n\u03b7j \u2211\ni\u2208Nj\n\u2016\nj\u22121\u2211\nq=ji+1\n(xq0 \u2212 x q+1 0 )\u2016 2\n\u2264 k\u2211\nj=0\n\u03b7j \u2211\ni\u2208Nj\n(j \u2212 ji \u2212 1)\nj\u22121\u2211\nq=ji+1\n\u2016xq0 \u2212 x q+1 0 \u2016 2\n\u2264 k\u2211\nj=0\n\u03b7j \u2211\ni\u2208Nj\n(\u03bd \u2212 1)\nj\u22121\u2211\nq=j\u2212\u03bd+1\n\u2016xq0 \u2212 x q+1 0 \u2016 2\n\u2264 (\u03bd \u2212 1)N\u0304\n( k\u2211\nj=0\n\u03b7j j\u22121\u2211\nq=j\u2212\u03bd+1\n\u2016xq0 \u2212 x q+1 0 \u2016 2\n) , (A.4)\nwhere the second inequality is owing to j \u2212 \u03bd \u2264 ji. To proceed, we list \u03b7j \u2211j\u22121 q=j\u2212\u03bd+1 \u2016x q 0 \u2212 x q+1 0 \u2016 2 for j = 1, . . . , \u03bd, . . ., below\nj = 1, \u03b7\n0\u2211\nq=2\u2212\u03bd\n\u2016xq0 \u2212 x q+1 0 \u2016 2 = \u03b7\u2016x00 \u2212 x 1 0\u2016 2\nj = 2, \u03b72 1\u2211\nq=3\u2212\u03bd\n\u2016xq0 \u2212 x q+1 0 \u2016 2 = \u03b72\u2016x00 \u2212 x 1 0\u2016 2 + \u03b72\u2016x10 \u2212 x 2 0\u2016 2\n... ...\nj = \u03bd \u2212 1, \u03b7\u03bd\u22121 \u03bd\u22122\u2211\nq=0\n\u2016xq0 \u2212 x q+1 0 \u2016 2 = \u03b7\u03bd\u22121\u2016x00 \u2212 x 1 0\u2016 2 + \u03b7\u03bd\u22121\u2016x10 \u2212 x 2 0\u2016 2 + \u00b7 \u00b7 \u00b7+ \u03b7\u03bd\u22121\u2016x\u03bd\u221220 \u2212 x \u03bd\u22121 0 \u2016 2\nj = \u03bd, \u03b7\u03bd \u03bd\u22121\u2211\nq=1\n\u2016xq0 \u2212 x q+1 0 \u2016 2 = \u03b7\u03bd\u2016x10 \u2212 x 2 0\u2016 2 + \u03b7\u03bd\u2016x20 \u2212 x 3 0\u2016 2 + \u00b7 \u00b7 \u00b7 + \u03b7\u03bd\u2016x\u03bd\u221210 \u2212 x \u03bd 0\u2016 2 (A.5)\nOne can verify that each \u2016xj0 \u2212 x j+1 0 \u2016 2 appears no more than \u03bd \u2212 1 times in the summation term \u2211k\nj=0 \u03b7 j \u2211j\u22121 q=j\u2212\u03bd+1 \u2016x q 0 \u2212 x q+1 0 \u2016 2 and therefore the total contribution of each \u2016xj0 \u2212 x j+1 0 \u2016 2 can be\nupper bounded by\n(\u03b7j+1 + \u03b7j+2 + \u00b7 \u00b7 \u00b7+ \u03b7j+\u03bd\u22121)\u2016xj0 \u2212 x j+1 0 \u2016\n2 = \u03b7j+1 ( \u03b7\u03bd\u22121 \u2212 1\n\u03b7 \u2212 1\n) \u2016xj0 \u2212 x j+1 0 \u2016 2. (A.6)\nThis shows that k\u2211\nj=0\n\u03b7j j\u22121\u2211\nq=j\u2212\u03bd+1\n\u2016xq0 \u2212 x q+1 0 \u2016\n2 \u2264 k\u22121\u2211\nj=0\n\u03b7j+1 ( \u03b7\u03bd\u22121 \u2212 1\n\u03b7 \u2212 1\n) \u2016xj0 \u2212 x j+1 0 \u2016 2. (A.7)\nBy substituting (A.7) into (A.4), we obtain (31).\nSeptember 10, 2015 DRAFT\n20\nAPPENDIX C\nPROOF OF LEMMA 3\nBy the optimality condition of (21) [24] , one has, \u2200i \u2208 Ak and \u2200xi \u2208 Rn,\n0 \u2265 (\u2207fi(x k+1 i ) + \u03bb k i + \u03c1(x k+1 i \u2212 x k\u0304i+1 0 ) T (xk+1i \u2212 xi)\n= (\u2207fi(x k+1 i )) T (xk+1i \u2212 xi) + (\u03bb k+1 i ) T (xk+1i \u2212 xi), (A.8)\nwhere the equality is due to (22). Similarly, by the optimality condition of (28) and by (29), one has, \u2200i \u2208 Ack and \u2200xi \u2208 R n,\n0 \u2265 (\u2207fi(x k+1 i ) + \u03bb k\u0303i i + \u03c1(x k+1 i \u2212 x k\u0302i+1 0 ) T (xk+1i \u2212 xi)\n= (\u2207fi(x k+1 i )) T (xk+1i \u2212 xi) + (\u03bb k+1 i ) T (xk+1i \u2212 xi). (A.9)\nSumming (A.8) and (A.9) for all i \u2208 V gives rise to\nN\u2211\ni=1\n(\u2207fi(x k+1 i )) T (xk+1i \u2212 xi) + N\u2211\ni=1\n(\u03bbk+1i ) T (xk+1i \u2212 xi)\n\u2264 0 \u2200(x1, . . . ,xN ) \u2208 R nN . (A.10)\nIn addition, by the optimality condition of (23) [7, Lemma 4.1], one has, \u2200x0 \u2208 Rn,\nh(xk+10 )\u2212 h(x0)\u2212 N\u2211\ni=1\n(\u03bbk+1i ) T (xk+10 \u2212 x0)\n\u2212 \u03c1 N\u2211\ni=1\n(xk+1i \u2212 x k+1 0 ) T (xk+10 \u2212 x0)\n+ \u03b3(xk+10 \u2212 x k 0) T (xk+10 \u2212 x0) \u2264 0. (A.11)\nDenote x\u22c6 \u2208 Rn as an optimal solution to problem (1). Let x1 = \u00b7 \u00b7 \u00b7 = xN = x0 = x\u22c6 in (A.10) and (A.11), and combine the two equations. We obtain\nN\u2211\ni=1\n(\u2207fi(x k+1 i )) T (xk+1i \u2212 x \u22c6) + h(xk+10 )\u2212 h(x \u22c6)\n+\nN\u2211\ni=1\n(\u03bbk+1i ) T (xk+1i \u2212 x k+1 0 )\n\u2212 \u03c1 N\u2211\ni=1\n(xk+1i \u2212 x k+1 0 ) T (xk+10 \u2212 x \u22c6)\n+ \u03b3(xk+10 \u2212 x k 0) T (xk+10 \u2212 x \u22c6) \u2264 0. (A.12)\nSeptember 10, 2015 DRAFT\n21\nLet y = x\u22c6 and x = xk+1i in (32) for all i \u2208 V , and apply them to (A.12). We have\n0 \u2265\n( N\u2211\ni=1\nfi(x k+1 i ) + h(x k+1 0 )\u2212\nN\u2211\ni=1\nfi(x \u22c6)\u2212 h(x\u22c6)\n)\n+\nN\u2211\ni=1\n(\u03bbk+1i ) T (xk+1i \u2212 x k+1 0 ) +\n\u03c32\n2\nN\u2211\ni=1\n\u2016xk+1i \u2212 x \u22c6\u20162\n\u2212 \u03c1 N\u2211\ni=1\n(xk+1i \u2212 x k+1 0 ) T (xk+10 \u2212 x \u22c6)\n+ \u03b3(xk+10 \u2212 x k 0) T (xk+10 \u2212 x \u22c6), (A.13)\nNote that, by (20),\n\u2212 \u03c1 N\u2211\ni=1\n(xk+1i \u2212 x k+1 0 ) T (xk+10 \u2212 x \u22c6)\n= \u2212 \u03c1\n2\nN\u2211\ni=1\n\u2016xk+1i \u2212 x \u22c6\u20162 +\n\u03c1 2\nN\u2211\ni=1\n\u2016xk+1i \u2212 x k+1 0 \u2016 2\n+ \u03c1N\n2 \u2016xk+10 \u2212 x \u22c6\u20162, (A.14)\nand that\n\u03b3(xk+10 \u2212 x k 0) T (xk+10 \u2212 x \u22c6) =\n\u03b3 2 \u2016xk+10 \u2212 x \u22c6\u20162\n\u2212 \u03b3\n2 \u2016xk0 \u2212 x\n\u22c6\u20162 + \u03b3\n2 \u2016xk+10 \u2212 x k 0\u2016 2. (A.15)\nBy substituting (A.14) and (A.15) into (A.13) and recalling L\u03c1 in (12), we obtain\n\u25b3k+1 \u2264 \u03c1\u2212 \u03c32\n2\nN\u2211\ni=1\n\u2016xk+1i \u2212 x \u22c6\u20162 +\n\u03b3 2 \u2016xk0 \u2212 x \u22c6\u20162\n\u2212 \u03b3\n2 \u2016xk+10 \u2212 x k 0\u2016\n2 \u2212 \u03b3 + \u03c1N\n2 \u2016xk+10 \u2212 x \u22c6\u20162. (A.16)\nWe bound the term \u2211N\ni=1 \u2016x k+1 i \u2212 x \u22c6\u20162 as\nN\u2211\ni=1\n\u2016xk+1i \u2212 x \u22c6\u20162 =\nN\u2211\ni=1\n\u2016xk+1i \u2212 x k+1 0 + x k+1 0 \u2212 x \u22c6\u20162\n\u2264 (1 + 1\n\u03b4 )N\u2016xk+10 \u2212 x\n\u22c6\u201622 + (1 + \u03b4) N\u2211\ni=1\n\u2016xk+1i \u2212 x k+1 0 \u2016 2 (by (20))\nSeptember 10, 2015 DRAFT\n22\n\u2264 (1 + 1\n\u03b4 )N\u2016xk+10 \u2212 x \u22c6\u201622 + (1 + \u03b4)\n[ 2L2\n\u03c12\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k i \u2016\n2 + 2L2\n\u03c12\n\u2211\ni\u2208Ack\n\u2016xk\u0303i+1i \u2212 x k\u0303i i \u2016 2\n+ 4 \u2211\ni\u2208Ak\n\u2016xk\u0304i+10 \u2212x k 0\u2016\n2 + 4 \u2211\ni\u2208Ack\n\u2016xk\u0302i+10 \u2212x k 0\u2016 2 + 4N\u2016xk+10 \u2212 x k 0\u2016 2\n] (by (A.1))\n\u2264 (1 + 1\n\u03b4 )N\u2016xk+10 \u2212 x\n\u22c6\u201622 + 4\u03b4L2\n\u03c12\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k i \u2016\n2 + 4\u03b4L2\n\u03c12\n\u2211\ni\u2208Ack\n\u2016xk\u0303i+1i \u2212 x k\u0303i i \u2016 2\n+ 8\u03b4 \u2211\ni\u2208Ak\n\u2016xk\u0304i+10 \u2212x k 0\u2016\n2 + 8\u03b4 \u2211\ni\u2208Ack\n\u2016xk\u0302i+10 \u2212x k 0\u2016 2 + 8\u03b4N\u2016xk+10 \u2212 x k 0\u2016 2, (A.17)\nwhere the last inequality is obtained by assuming \u03b4 > 1. Besides, we bound the term \u03b32\u2016x k 0 \u2212 x \u22c6\u20162 in the RHS of (A.16) as\n\u03b3 2 \u2016xk0 \u2212 x \u22c6\u20162 = \u03b3 2 \u2016xk0 \u2212 x k+1 0 + x k+1 0 \u2212 x \u22c6\u20162 \u2264 \u03b3\n2 (1 + \u03b4)\u2016xk0 \u2212 x k+1 0 \u2016\n2 + \u03b3\n2 (1 +\n1 \u03b4 )\u2016xk+10 \u2212 x \u22c6\u20162. (A.18)\nBy substituting (A.17) and (A.18) into (A.16), one obtains\n\u25b3k+1 \u2264\n( \u03c1N + \u03b3\n2\u03b4 \u2212\n\u03c32N\n2 (1 +\n1 \u03b4 )\n) \u2016xk+10 \u2212 x \u22c6\u20162\n+\n( \u03b3\u03b4\n2 + 4(\u03c1\u2212 \u03c32)N\u03b4\n) \u2016xk+10 \u2212 x k 0\u2016 2 + 2(\u03c1\u2212 \u03c32)\u03b4L2\n\u03c12\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k i \u2016 2\n+ 2(\u03c1\u2212 \u03c32)\u03b4L2\n\u03c12\n\u2211\ni\u2208Ack\n\u2016xk\u0303i+1i \u2212 x k\u0303i i \u2016 2\n+ 4(\u03c1\u2212 \u03c32)\u03b4 \u2211\ni\u2208Ak\n\u2016xk0 \u2212 x k\u0304i+1 0 \u2016\n2 + 4(\u03c1\u2212 \u03c32)\u03b4 \u2211\ni\u2208Ack\n\u2016xk0 \u2212 x k\u0302i+1 0 \u2016 2. (A.19)\nLet \u03b4 > 1 be large enough so that \u03c1N+\u03b32\u03b4 \u2212 \u03c32N 2 (1+ 1 \u03b4 ) \u2264 0 and assume that \u03b3 \u2265 8(\u03c1\u2212 \u03c3 2)N . Then, one obtains (33) from (A.19).\nTo show (34), let \u03b3 = 0 in (A.19) and assume that \u03b4 > 1 be large enough so that \u03c1\u03b4 \u2212 \u03c3 2(1 + 1\u03b4 ) \u2264 0.\nAPPENDIX D\nPROOF OF LEMMA 4\nSince X \u22c6 is a linear set, according to the Hoffman bound [26], for some constant c > 0,\ndist2(X \u22c6, x\u0302) = N\u2211\ni=1\n\u2016P\u22c6(x\u0302)\u2212 xi\u2016 2 + \u2016P\u22c6(x\u0302)\u2212 x0\u2016 2\n\u2264 c N\u2211\ni=1\n\u2016Aixi \u2212 y \u22c6 i \u2016 2 + c\nN\u2211\ni=1\n\u2016xi \u2212 x0\u2016 2. (A.20)\nSeptember 10, 2015 DRAFT\n23\nIn addition, it follows from the strong convexity of gi\u2019s that\nN\u2211\ni=1\nfi(P \u22c6(x\u0302)) =\nN\u2211\ni=1\ngi(AiP \u22c6(x\u0302))\n\u2265 N\u2211\ni=1\ngi(Aixi) +\nN\u2211\ni=1\n(\u2207gi(Aixi)) TAi(P \u22c6(x\u0302)\u2212 xi) + N\u2211\ni=1\n\u03c32\n2 \u2016AiP\n\u22c6(x\u0302)\u2212Aixi\u2016 2\n=\nN\u2211\ni=1\nfi(xi) +\nN\u2211\ni=1\n(\u2207fi(xi)) T (P\u22c6(x\u0302)\u2212 xi) +\nN\u2211\ni=1\n\u03c32\n2 \u2016y\u22c6i \u2212Aixi\u2016 2. (A.21)\nBy substituting (A.20) into (A.21), one obtains (46).\nAPPENDIX E\nPROOF OF LEMMA 5\nBy applying (46) (with xi = x k+1 i \u2200i = 0, 1, . . . , N ) to (A.12), and following the same steps as in\n(A.13)-(A.16), we have\n\u25b3k+1 \u2264 \u03c1\u2212 \u03c32/c\n2\nN\u2211\ni=1\n\u2016xk+1i \u2212 P \u22c6(x\u0302)\u20162 +\n\u03b3 2 \u2016xk0 \u2212 P \u22c6(x\u0302)\u20162\n\u2212 \u03b3\n2 \u2016xk+10 \u2212 x k 0\u2016\n2 \u2212 \u03b3 + \u03c32/c+ \u03c1N\n2 \u2016xk+10 \u2212 P \u22c6(x\u0302)\u20162\n+ \u03c32\n2\nN\u2211\ni=1\n\u2016xk+1i \u2212 x k+1 0 \u2016 2. (A.22)\nRecall (A.17), (A.18) (with x\u22c6 replaced by P\u22c6(x\u0302)) and (A.1) in Lemma 6 and apply them to (A.22).\nOne obtains\n\u25b3k+1 \u2264\n( \u03c1N + \u03b3\n2\u03b4 \u2212\nN\u03c32/c\n2 (1 +\n1 \u03b4 )\n) \u2016xk+10 \u2212 P \u22c6(x\u0302)\u20162\n+\n( \u03b3\u03b4\n2 + 4(\u03c1\u2212 \u03c32/c)N\u03b4 + 2\u03c32N\n) \u2016xk+10 \u2212 x k 0\u2016 2\n+ (2(\u03c1 \u2212 \u03c32/c)\u03b4 + \u03c32)L2\n\u03c12\n\u2211\ni\u2208Ak\n\u2016xk+1i \u2212 x k i \u2016 2\n+ (2(\u03c1\u2212 \u03c32/c)\u03b4 + \u03c32)L2\n\u03c12\n\u2211\ni\u2208Ack\n\u2016xk\u0303i+1i \u2212 x k\u0303i i \u2016 2\n+ (4(\u03c1\u2212 \u03c32/c)\u03b4 + 2\u03c32) \u2211\ni\u2208Ak\n\u2016xk0 \u2212 x k\u0304i+1 0 \u2016 2\n+ (4(\u03c1\u2212 \u03c32/c)\u03b4 + 2\u03c32) \u2211\ni\u2208Ack\n\u2016xk0 \u2212 x k\u0302i+1 0 \u2016 2. (A.23)\nSeptember 10, 2015 DRAFT\n24\nLet \u03b4 > 1 be large enough so that \u03c1N+\u03b32\u03b4 \u2212 N\u03c32/c 2 (1+ 1 \u03b4 ) \u2264 0 In addition, since \u03b3 \u2265 8N(\u03c1\u2212 \u03c3 2/c) + 4N\u03c32 implies \u03b3 \u2265 8N(\u03c1\u2212 \u03c32/c) + 4N\u03c32/\u03b4, (A.23) infers (33).\nTo obtain (48), let \u03b3 = 0 in (A.23) and assume that \u03b4 > 1 be large enough so that \u03c1\u03b4 \u2212 \u03c32 c (1+ 1 \u03b4 ) \u2264 0.\nREFERENCES\n[1] T.-H. Chang, M. Hong, W.-C. Liao, and X. Wang, \u201cAsynchronous distributed alternating direction method of multipliers:\nAlgorithm and convergence analysis,\u201d submitted to NIPS, Montreal, Canada, Dec. 7-12, 2015.\n[2] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, \u201cDistributed optimization and statistical learning via the alternating\ndirection method of multipliers,\u201d Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.\n[3] R. Tibshirani and M. Saunders, \u201cSparisty and smoothness via the fused lasso,\u201d J. R. Statist. Soc. B, vol. 67, no. 1, pp.\n91\u2013108, 2005.\n[4] J. Liu, J. Chen, and J. Ye, \u201cLarge-scale sparse logistic regression,\u201d in Proc. ACM Int. Conf. on Knowledge Discovery and\nData Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547\u2013556.\n[5] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction.\nNew York, NY, USA: Springer-Verlag, 2001.\n[6] P. Richta\u0301rik, M. Taka\u0301c\u030c, and S. D. Ahipasaoglu, \u201cAlternating maximization: Unifying framework for 8 sparse PCA\nformulations and efficient parallel codes,\u201d [Online] http://arxiv.org/abs/1212.4137.\n[7] D. P. Bertsekas and J. N. Tsitsiklis, Parallel and distributed computation: Numerical methods. Upper Saddle River, NJ,\nUSA: Prentice-Hall, Inc., 1989.\n[8] F. Niu, B. Recht, C. Re, and S. J. Wright, \u201cHogwild!: A lock-free approach to parallelizing stochastic gradient descent,\u201d\nProc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730. [9] A. Agarwal and J. C. Duchi, \u201cDistributed delayed stochastic optimization,\u201d Proc. Advances in Neural Information Processing\nSystems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.\n[10] M. Li, L. Zhou, Z. Yang, A. Li, F. Xia, D. G. Andersen, and A. Smola, \u201cParameter server for distributed machine learning,\u201d\n[Online] http://www.cs.cmu.edu/\u223cmuli/file/ps.pdf.\n[11] M. Li, D. G. Andersen, and A. Smola, \u201cDistributed delayed proximal gradient methods,\u201d [Online] http://www.cs.cmu.edu/\n\u223cmuli/file/ddp.pdf.\n[12] J. Liu and S. J. Wright, \u201cAsynchronous stochastic coordinate descent: Parallelism and convergence properties,\u201d SIAM J.\nOptim.,, vol. 25, no. 1, pp. 351\u2013376, Feb. 2015.\n[13] M. Razaviyayn, M. Hong, Z.-Q. Luo, and J. S. Pang, \u201cParallel successive convex approximation for nonsmooth nonconvex\noptimization,\u201d in the Proceedings of the Neural Information Processing (NIPS), 2014.\n[14] G. Scutari, F. Facchinei, P. Song, D. P. Palomar, and J.-S. Pang, \u201cDecomposition by partial linearization: Parallel\noptimization of multi-agent systems,\u201d IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641\u2013656, 2014.\n[15] B. He and X. Yuan, \u201cOn the o(1/n) convergence rate of Douglas-Rachford alternating direction method,\u201d SIAM J. Num.\nAnal., vol. 50, 2012.\n[16] M. Hong, Z.-Q. Luo, and M. Razaviyayn, \u201cConvergence analysis of alternating direction method of multipliers for a family\nof nonconvex problems,\u201d technical report; available on http://arxiv.org/pdf/1410.1390.pdf.\nSeptember 10, 2015 DRAFT\n25\n[17] W. Deng and W. Yin, \u201cOn the global and linear convergence of the generalized alternating direction method of multipliers,\u201d\nRice CAAM technical report 12-14, 2012.\n[18] W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin, \u201cOn the linear convergence of the ADMM in decentralized consensus\noptimization,\u201d IEEE Trans. Signal Process., vol. 62, no. 7, pp. 1750\u20131761, April 2014.\n[19] T.-H. Chang, M. Hong, and X. Wang, \u201cMulti-agent distributed optimization via inexact consensus ADMM,\u201d IEEE Trans.\nSignal Process., vol. 63, no. 2, pp. 482\u2013497, Jan. 2015.\n[20] D. Jakovetic\u0301, J. M. F. Moura, and J. Xavier, \u201cLinear convergence rate of class of distributed augmented lagrangian\nalgorithms,\u201d to appear in IEEE Trans. Automatic Control.\n[21] M. Hong and Z.-Q. Luo, \u201cOn the linear convergence of the alternating direction method of multipliers,\u201d available on\narxiv.org.\n[22] R. Zhang and J. T. Kwok, \u201cAsynchronous distributed ADMM for consensus optimization,\u201d in Proc. 31th ICML, , 2014.,\nBeijing, China, June 21-26, 2014, pp. 1\u20139.\n[23] T.-H. Chang, M. Hong, W.-C. Liao, and X. Wang, \u201cAsynchronous distributed ADMM for large-scale optimization- Part I:\nAlgorithm and convergence analysis,\u201d submitted for publication.\n[24] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, UK: Cambridge University Press, 2004. [25] A. Beck and M. Teboulle, \u201cA fast iterative shrinkage-thresholding algorithm for linear inverse problems,\u201d SIAM J. Imaging\nSci., vol. 2, no. 1, pp. 183\u2013202, 2009.\n[26] A. J. Hoffman, \u201cOn approximate solutions of systems of linear inequalities,\u201d Journal of Research of the National Bureau\nof Standards,, vol. 49, pp. 263\u2013265, 1952.\nSeptember 10, 2015 DRAFT\n26\nAlgorithm 1 Asynchronous Distributed ADMM for (2).\n1: Algorithm of the Master: 2: Given initial variable x0 and broadcast it to the workers. Set k = 0 and d1 = \u00b7 \u00b7 \u00b7 = dN = 0; 3: repeat 4: wait until receiving {x\u0302i, \u03bb\u0302i}i\u2208Ak from workers i \u2208 Ak and that di < \u03c4 \u2212 1 \u2200i \u2208 A c k. 5: update\nxk+1i =\n{ x\u0302i \u2200i \u2208 Ak\nxki \u2200i \u2208 A c k\n, (6)\n\u03bbk+1i =\n{ \u03bb\u0302i \u2200i \u2208 Ak\n\u03bbki \u2200i \u2208 A c k\n, (7)\ndi =\n{ 0 \u2200i \u2208 Ak\ndi + 1 \u2200i \u2208 A c k\n, (8)\nxk+10 =arg min x0\u2208Rn\n{ h(x0)\u2212 x T 0 \u2211N i=1 \u03bb k+1 i\n+ \u03c12 \u2211N i=1 \u2016x k+1 i \u2212 x0\u2016 2 + \u03b32\u2016x0 \u2212 x k 0\u2016 2 } , (9)\n6: broadcast xk+10 to the workers in Ak. 7: set k \u2190 k + 1. 8: until a predefined stopping criterion is satisfied.\n1: Algorithm of the ith Worker: 2: Given initial \u03bb0 and set ki = 0. 3: repeat 4: wait until receiving x\u03020 from the master node. 5: update\nxki+1i = arg min xi\u2208Rn fi(xi) + x T i \u03bb ki i + \u03c1 2\u2016xi \u2212 x\u03020\u2016 2, (10) \u03bbki+1i = \u03bb ki i + \u03c1(x ki+1 i \u2212 x\u03020). (11)\n6: send (xki+1i ,\u03bb ki+1 i ) to the master node. 7: set ki \u2190 ki + 1. 8: until a predefined stopping criterion is satisfied.\nSeptember 10, 2015 DRAFT"}], "references": [{"title": "Asynchronous distributed alternating direction method of multipliers: Algorithm and convergence analysis", "author": ["T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang"], "venue": "submitted to NIPS, Montreal, Canada, Dec. 7-12, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparisty and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders"], "venue": "J. R. Statist. Soc. B, vol. 67, no. 1, pp. 91\u2013108, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": "Proc. ACM Int. Conf. on Knowledge Discovery and Data Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547\u2013556.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Alternating maximization: Unifying framework for 8 sparse PCA formulations and efficient parallel codes", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d", "S.D. Ahipasaoglu"], "venue": "[Online] http://arxiv.org/abs/1212.4137.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1212}, {"title": "Parallel and distributed computation: Numerical methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Upper Saddle River, NJ, USA: Prentice-Hall, Inc.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Parameter server for distributed machine learning", "author": ["M. Li", "L. Zhou", "Z. Yang", "A. Li", "F. Xia", "D.G. Andersen", "A. Smola"], "venue": "[Online] http://www.cs.cmu.edu/\u223cmuli/file/ps.pdf.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 0}, {"title": "Distributed delayed proximal gradient methods", "author": ["M. Li", "D.G. Andersen", "A. Smola"], "venue": "[Online] http://www.cs.cmu.edu/ \u223cmuli/file/ddp.pdf.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 0}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["J. Liu", "S.J. Wright"], "venue": "SIAM J. Optim.,, vol. 25, no. 1, pp. 351\u2013376, Feb. 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel successive convex approximation for nonsmooth nonconvex optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo", "J.S. Pang"], "venue": "the Proceedings of the Neural Information Processing (NIPS), 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Decomposition by partial linearization: Parallel optimization of multi-agent systems", "author": ["G. Scutari", "F. Facchinei", "P. Song", "D.P. Palomar", "J.-S. Pang"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641\u2013656, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "On the o(1/n) convergence rate of Douglas-Rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM J. Num. Anal., vol. 50, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems", "author": ["M. Hong", "Z.-Q. Luo", "M. Razaviyayn"], "venue": "technical report; available on http://arxiv.org/pdf/1410.1390.pdf. September 10, 2015  DRAFT  25", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["W. Deng", "W. Yin"], "venue": "Rice CAAM technical report 12-14, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "On the linear convergence of the ADMM in decentralized consensus optimization", "author": ["W. Shi", "Q. Ling", "K. Yuan", "G. Wu", "W. Yin"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 7, pp. 1750\u20131761, April 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-agent distributed optimization via inexact consensus ADMM", "author": ["T.-H. Chang", "M. Hong", "X. Wang"], "venue": "IEEE Trans. Signal Process., vol. 63, no. 2, pp. 482\u2013497, Jan. 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Linear convergence rate of class of distributed augmented lagrangian algorithms", "author": ["D. Jakoveti\u0107", "J.M.F. Moura", "J. Xavier"], "venue": "to appear in IEEE Trans. Automatic Control.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 0}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "available on arxiv.org.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 0}, {"title": "Asynchronous distributed ADMM for consensus optimization", "author": ["R. Zhang", "J.T. Kwok"], "venue": "Proc. 31th ICML, , 2014., Beijing, China, June 21-26, 2014, pp. 1\u20139.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous distributed ADMM for large-scale optimization- Part I: Algorithm and convergence analysis", "author": ["T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang"], "venue": "submitted for publication.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 0}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sci., vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.", "startOffset": 159, "endOffset": 162}, {"referenceID": 4, "context": "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.", "startOffset": 193, "endOffset": 196}, {"referenceID": 5, "context": "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.", "startOffset": 255, "endOffset": 258}, {"referenceID": 1, "context": "Distributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]\u2013[14].", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "Distributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]\u2013[14].", "startOffset": 144, "endOffset": 147}, {"referenceID": 13, "context": "Distributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]\u2013[14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 1, "context": "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]\u2013[20].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]\u2013[20].", "startOffset": 91, "endOffset": 94}, {"referenceID": 14, "context": "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]\u2013[20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]\u2013[20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number.", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number.", "startOffset": 58, "endOffset": 61}, {"referenceID": 14, "context": "For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number.", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "Considering non-convex problems with smooth fi\u2019s, reference [16] presented conditions for which the distributed ADMM converges to the set of Karush-KuhnTucker (KKT) points.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "For problems with strongly convex and smooth fi\u2019s or problems satisfying certain error bound condition, references [17] and [21] respectively showed that the ADMM can even exhibit a linear convergence rate.", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "For problems with strongly convex and smooth fi\u2019s or problems satisfying certain error bound condition, references [17] and [21] respectively showed that the ADMM can even exhibit a linear convergence rate.", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "References [18]\u2013[20] also showed similar linear convergence conditions for some variants of distributed ADMM in a network with a general topology.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "References [18]\u2013[20] also showed similar linear convergence conditions for some variants of distributed ADMM in a network with a general topology.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "However, the distributed ADMM in [2], [16] have assumed a synchronous network, where at each iteration, the master always waits until all", "startOffset": 33, "endOffset": 36}, {"referenceID": 15, "context": "However, the distributed ADMM in [2], [16] have assumed a synchronous network, where at each iteration, the master always waits until all", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "To improve the time efficiency, the works [22], [23] have generalized the distributed ADMM to an asynchronous network.", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "To improve the time efficiency, the works [22], [23] have generalized the distributed ADMM to an asynchronous network.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "Specifically, in the asynchronous distributed ADMM (AD-ADMM) proposed in [22], [23], the master does not necessarily wait for all the workers.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "Specifically, in the asynchronous distributed ADMM (AD-ADMM) proposed in [22], [23], the master does not necessarily wait for all the workers.", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "Theoretically, it has been shown in [23] that the AD-ADMM is guaranteed to converge (to a KKT point) even for non-convex problem (1), under a bounded delay assumption only.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Firstly, beyond the convergence analysis in [23], we further present the conditions for which the AD-ADMM can exhibit a linear convergence rate.", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "To the best of our knowledge, our results are novel, and are by no means extensions of the existing analyses [17]\u2013[21] for synchronous ADMM.", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "To the best of our knowledge, our results are novel, and are by no means extensions of the existing analyses [17]\u2013[21] for synchronous ADMM.", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "Synopsis: Section II reviews the AD-ADMM in [23].", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "ASYNCHRONOUS DISTRIBUTED ADMM In this section, we review the AD-ADMM proposed in [23].", "startOffset": 81, "endOffset": 85}, {"referenceID": 6, "context": "By applying the standard ADMM [7] to problem (2), one obtains the following three simple steps: for iteration k = 0, 1, .", "startOffset": 30, "endOffset": 33}, {"referenceID": 22, "context": "1 in [23]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "The distributed ADMM has been extended to an asynchronous network in [22], [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 22, "context": "The distributed ADMM has been extended to an asynchronous network in [22], [23].", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "This condition guarantees that the variable information is at most \u03c4 iterations old, and is known as the partially asynchronous model [7]: Assumption 1 (Bounded delay) Let \u03c4 \u2265 1 be a maximum tolerable delay.", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Interestingly, such structured cost function appears in many machine learning problems, for example, the least squared problem and the logistic regression problem [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 16, "context": "Since it has been known that the (synchronous) distributed ADMM [17]\u2013[21] can converge linearly given the same structured cost functions in Assumption 3 and Assumption 4, the convergence results presented above demonstrate that the linear convergence property can be preserved in the asynchronous network.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "Since it has been known that the (synchronous) distributed ADMM [17]\u2013[21] can converge linearly given the same structured cost functions in Assumption 3 and Assumption 4, the convergence results presented above demonstrate that the linear convergence property can be preserved in the asynchronous network.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "For each worker, we employed the fast iterative shrinkage thresholding algorithm (FISTA) [25] to solve the corresponding subproblem (10).", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "In our experiments, analogous to [22], we further constrained the minimum size of the active set Ak by |Ak| \u2265 A where A \u2208 [1, N ] is an integer.", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "CONCLUSIONS In this paper, we have analytically studied the linear convergence conditions of the AD-ADMM proposed in [23].", "startOffset": 117, "endOffset": 121}], "year": 2015, "abstractText": "The alternating direction method of multipliers (ADMM) has been recognized as a versatile approach for solving modern large-scale machine learning and signal processing problems efficiently. When the data size and/or the problem dimension is large, a distributed version of ADMM can be used, which is capable of distributing the computation load and the data set to a network of computing nodes. Unfortunately, a direct synchronous implementation of such algorithm does not scale well with the problem size, as the algorithm speed is limited by the slowest computing nodes. To address this issue, in a companion paper, we have proposed an asynchronous distributed ADMM (AD-ADMM) and studied its worst-case convergence conditions. In this paper, we further the study by characterizing the conditions under which the AD-ADMM achieves linear convergence. Our conditions as well as the resulting linear rates reveal the impact that various algorithm parameters, network delay and network size have on the algorithm performance. To demonstrate the superior time efficiency of the proposed AD-ADMM, we test the ADADMM on a high-performance computer cluster by solving a large-scale logistic regression problem. Keywords\u2212 Distributed optimization, ADMM, Asynchronous, Consensus optimization \u22c6Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172, E-mail: tsunghui.chang@ieee.org. Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: mhong@umn.edu Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn September 10, 2015 DRAFT", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}