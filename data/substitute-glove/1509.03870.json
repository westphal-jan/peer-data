{"id": "1509.03870", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2015", "title": "The USFD Spoken Language Translation System for IWSLT 2014", "abstract": "The University same Sheffield (USFD) participating into next International Workshop came Spoken Language Translation (IWSLT) ago 1958. In also paper, never adding consider making USFD SLT technology the IWSLT. Automatic remark importance (ASR) is achieved widely went backbone - scoring over epithelial technology automated all theme and rescoring useful. Machine refers (MT) still objectives by a pronounced - based system. The USFD as effective uses state - has - over - cinema ASR been MT techniques which answer a BLEU finished of 57. 47 had 48. 113 first has English - have - French including English - might - German speech - leave - copied translation task five before IWSLT 2009-10 data. The USFD contrastive computing explore where strengthening especially ASR has MT other hand when skills quantify effectively. rescore by ASR configurations, co-ordinate stopping done descriptive. This gives a that 3-2. 72 they sc. 2002 BLEU declining 47 also along IWSLT 2014 also 2013-14 screening maps.", "histories": [["v1", "Sun, 13 Sep 2015 16:58:41 GMT  (108kb,D)", "http://arxiv.org/abs/1509.03870v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raymond w m ng", "mortaza doulaty", "rama doddipatla", "wilker aziz", "kashif shah", "oscar saz", "madina hasan", "ghada alharbi", "lucia specia", "thomas hain"], "accepted": false, "id": "1509.03870"}, "pdf": {"name": "1509.03870.pdf", "metadata": {"source": "CRF", "title": "The USFD Spoken Language Translation System for IWSLT 2014", "authors": ["Raymond W. M. Ng", "Mortaza Doulaty", "Rama Doddipatla", "Wilker Aziz", "Kashif Shah", "Oscar Saz", "Madina Hasan", "Ghada AlHarbi", "Lucia Specia", "Thomas Hain"], "emails": ["o.saztorralba@sheffield.ac.uk", "m.hasan@sheffield.ac.uk", "GAlHarbi1@sheffield.ac.uk", "l.specia@sheffield.ac.uk", "t.hain@sheffield.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "In this paper, the University of Sheffield (USFD) system for the International Workshop on Spoken Language Translation (IWSLT) 2014 is introduced. USFD participated in Englishto-French and English-to-German SLT tasks. The ASR and MT systems made use of state-of-the-art technologies. On the ASR side, two deep neural network systems built on partially different data and different tandem configurations were used. On the MT side, phrase-based translation models were built. ASR and MT system integration attempts were made by using a translation quality estimation system. It considered the system scores from both ASR and MT, as well as features extracted from the ASR outputs in source language. The ASR hypotheses were then rescored based on the predicted translation quality. This gives performance improvements in terms of BLEU score increase.\nIn the following, the data used for system training is introduced in \u00a72. \u00a73 and \u00a74 give the details of the ASR and MT systems. The decoding algorithm and system results are given in \u00a75. Besides the primary submission, USFD also submitted contrastive systems which implement system integration. These systems used a quality estimation module and performed ASR N -best list rescoring based on predicted translation quality. This would be described in \u00a76."}, {"heading": "2. Data processing and selection", "text": "The ASR and MT systems were primarily trained on TED lecture data [1]. For ASR, TED and the additional data form two data subsets, on which two systems were trained. For MT, out-of-domain data after data selection were incorporated in the training of translation models and target language models."}, {"heading": "2.1. ASR acoustic modelling", "text": "Two data sets were used for ASR system training. For the ease of discussion they are hereinafter referred to as ASR1 and ASR2. The composition of the two data sets is shown in Table 1.\nTED serves as a common data set in both ASR1 and ASR2. Their segmentations in ASR1 and ASR2 differ slightly and this is explained later. The two data sets are augmented by e-corner lecture data (ECRN) with a duration of 60 hours [2]. ASR1 also contains 106 hours of LLC lecture data. In ASR2, 165 hours of meeting data from the AMI, AMIDA and ICSI corpora are added so the trained model will reflect also generic domains other than lectures [3, 4].\nThe TED portions in both ASR1 and ASR2 originate from 734 TED talks published before 31 Dec 2010. Each talk has a duration of around 15 minutes. Human annotations in the form of subtitles are also available, giving rough segmentation with segment duration from 3 to 5 seconds and time accuracy to the nearest second.\nExact segmentations and transcriptions of TED were derived in different ways in ASR1 and ASR2. In ASR1, all segments from the same talk were merged and the speech was forced aligned, resegmented before another forced alignment run determined the final training set. This gave a total of 132 hours of speech for AM training. In ASR2, forced alignment\nar X\niv :1\n50 9.\n03 87\n0v 1\n[ cs\n.C L\n] 1\n3 Se\np 20\n15\nwas performed on the rough segmentation, after which contagious segments were merged when there was tight silence at the segment boundaries. A further run of forced alignment determined the final training set. This gave a total of 112 hours of speech.\nTo evaluate the performance of different segmentations, PLP-based state-tied triphone models with cepstral mean and variance normalisation were trained on these data and decoding was performed on the IWSLT 2010 evaluation data set. The WERs for the ASR1 and ASR2 settings are 25.7% and 26.2% respectively. When the models are trained directly on the roughly segmented data (no adjustment of segmentations), the total duration of training data is 109 hours and the corresponding WER is 28.1%."}, {"heading": "2.2. Language models and MT", "text": "Textual data for the training of language models and translation models were obtained from the affiliated websites of the IWSLT and WMT evaluations [5, 6]. TED was considered as the in-domain training data and the full data set was used. Four out-of-domain (OOD) data sets from News commentary v9, Common Crawl, Gigaword and Europarl v7 were also used, after a data selection process.\nThe OOD corpora were selected with the cross entropy difference criterion [7]. Given a sentence xI1 = [x1 \u00b7 \u00b7 \u00b7xI ] with I words, cross entropy values H(xI1, ID) and H(xI1, OOD) were computed using GID, the ID language model (in this case, TED) and GOOD, the OOD language model (built on the corpus from which the sentence was taken). The cross entropy difference (CED) was given by,\nCED(xI1) = H(x I 1,GID)\u2212H(xI1,GOOD) (1)\nSentences were ranked by the CED values and 25% of the sentences with the lowest CED values were selected from each corpus. Furthermore, CED values were calculated on sentence batches with increasing sizes. A line search was done to find the optimal batch giving the minimum CED value. All data selection was done on the English text. For data selection to translation model training, the corresponding sentences in the target languages were extracted after selection was done on English sentences.\nTable 2 shows the amount of the full text data set, and the\nselected text data in different systems in the English\u2192French translation task. The full data set contains 703.9M words. They were used for training the target language model in MT, which was a 5-gram interpolated LM with punctuation and out-of-vocabulary word modelling, modified KneserNey smoothing and was in standard ARPA format. The source language model for ASR was built on the full TED data set and 25% or 50% of the OOD data, making up to 322.2M words. A monolingual translation model was trained for punctuation insertion and case conversion. The training took the full TED data and 5-10% of the OOD data, resulting in a total of 37.6M words. The translation model was trained on the full TED data set and other optimally selected OOD data sets, where only around 5% of the sentences were selected. The total number of words is 31.7M."}, {"heading": "3. Automatic speech recognition", "text": "There are two DNN systems with tandem configurations in ASR [8]. Bottleneck (BN) features were derived from deep neural network (DNN)s [4], and GMM-HMM systems were trained on these bottleneck features. The two tandem systems were trained on ASR1 and ASR2 data respectively (Table 1). Different portions of data were used in different stages of training. Let DNN1 and DNN2 denote the two DNN systems for ASR1 and ASR2. DNN1 was trained on TED data only. DNN2 was trained on TED and AMI+AMIDA+ICSI data only. The remaining data listed in Table 1 were added to the training pool in the GMM-HMM training stage.\nDNN1 has 4 hidden layers, each having 1,745 hidden units. The BN layer is placed just before the output layer and has 26 units. The output layer has 4,320 units. DNN2 has 5 hidden layers, with the first 3 layers having 1,745 units and the fourth hidden layer having 65 units. A BN layer is placed just before the output layer and has 39 units. The output layer has 5,691 units.\nBoth the DNNs were trained using log filter-bank outputs and concatenating 31 adjacent frames, which were decorrelated using DCT to form a 368-dimensional feature vector. The filter-bank outputs were mean and variance normalised at the speaker level. Global mean and variance normalisation was performed on each dimension before feeding the input for training the DNN. The GMM-HMM systems trained using the BN features were different. The model for ASR1 was trained on the concatenated features with the 26-dimension BN features from DNN1 and the 39-dimension PLP features. The model for ASR2 was trained on the 39-dimension BN features from DNN2. Both the GMM-HMM models were trained as tied-state triphone systems with the final models having 16 mixture Gaussians per state.\nAll systems are vocal tract length normalised (VTLN). In the training stage, a PLP system was used to obtain the warp factors for each speaker. Then the filter-bank and PLP features were VTLN-warped, which were in turn used for DNN and GMM-HMM training in the tandem configuration. In the decoding stage, a non-VTLN DNN and GMM-HMM tandem\nsystem trained on ASR2 data replaced the PLP system for the derivation of warp factors.\nTo improve the performance of the acoustic model, minimum phone error (MPE) training was performed using the lattices which were generated using a uni-gram language model [9].\nLanguage models for ASR are all interpolated LMs built on the English text data described in Table 2 and tuned on IWSLT 2010 dev and eval data. 2-gram and 4-gram ARPA language models were trained for lattice generation and expansion. The 4-gram LM was pruned with a threshold 10\u221210 and a weighted-finite-state transducer (WFST) was constructed for fast decoding in the pre-final passes in the ASR systems.\nAll ASR LMs were based on a word-list with a 60k word vocabulary extracted based on our standard English ASR inventory and the English part of the TED MT training data for IWSLT 2014 [3, 5]. Pilot ASR experiments on the IWSLT 2011 and 2012 eval data show the drop of perplexity with the addition of Common crawl and Gigaword data. For these two corpora, the rate of data selected for LM building was set to 50%, while the rate for other OOD corpora was kept 25%. This made the total number of words 322.2M as shown in Table 2.\nPronunciation probabilities were incorporated in final stage decoding [10]. These probabilities were extracted based on the Viterbi alignment of the phoneme level transcription of the ASR1 training data. When a word allowed multiple pronunciations, the frequency of each pronunciation was calculated and stored. These frequencies were then applied to the words in the decoding dictionary for words that appeared in both training and decoding stages. Words with multiple pronunciations appearing only in the decoding stage were given equal probability."}, {"heading": "4. Machine translation", "text": "A phrase-based model using MOSES [11] in a standard setting was employed. For phrase extraction all of the TED data (3.17 million words) was used. Following previous findings [12], data selection via a cross-entropy difference criterion (detailed in \u00a72.2) was used to select the optimal batch of\nthe OOD data, which amounts to about 5% of the total data or 30.58M words. The phrase length was limited to 5 and word-alignment was obtained with FASTALIGN [13]. Lexicalised reordering models were trained using the same data. For language modelling, we used the complete sets of OOD data (i.e. no data selection). 5-gram LMs were trained using LMPLZ [14]. 100-best MIRA tuning was employed [15]. For the English-to-French system, tuning was done on the IWSLT 2010 development and evaluation data with a total of 2,551 sentences. For the English-to-German system, tuning was done on the IWSLT 2010 development data with 887 sentences.\nIn SLT, the input to the MT system was ASR output, which typically lacks casing and punctuation. Following previous work [16, 17], a monolingual translation system was trained to recover casing and punctuation from the ASR output, thus producing source sentences which are more adequate for translation. The training data for this monolingual MT system was obtained by pre-processing an actual corpus of the source language to form pseudo ASR outputs, which contained no case and punctuation information. Numbers, symbols and acronyms were also converted to their verbal forms with lookup tables. We then used this synthesised corpus of pseudo ASR as the source, and the original corpus as the target of our monolingual MT. The monolingual translation system was trained on 37.6M words (Table 2). It performed monotonic translation with phrases of as long as 7 words."}, {"heading": "5. Decoding", "text": "The evaluation systems for ASR and MT are multi-pass systems with resource optimisation and environment management capabilities [11, 18]. The ASR is a two-stream multipass system. It is illustrated in Figure 1. The two streams ASR1 and ASR2 differ by the acoustic model training data (detailed in Table 1) and also the tandem configurations (detailed in \u00a73). Both streams follow the same routine along the multi-pass decoding system. In pass 1, a unified decoding result was generated using a non-VTLN DNN and GMM-HMM tandem system with cepstral mean and variance (CMVN) normalisation trained on ASR2 data. These\nhypothesis transcripts were used for inferring the warp factors. The filterbank (for both ASR1 and ASR2) and PLP (for ASR1 only) features were then warped and CMVN normalised, and the system branched off into two streams with two VTLN decoders trained on ASR1 and ASR2 data respectively.\nAfter pass 2 decoding, speaker-based MLLR cross adaptations were carried out. The transcripts from ASR1 was used for the model transformation in ASR2 system and vice versa. The number of regression classes was set to 16. When pass 3 decoding was done, MLLR self adaptations were performed. The number of regression classes was also set to 16.\nAll pre-final stage decoding made use of weighted finite state transducers (WFSTs) for fast implementation. In a pilot experiment, PLP systems with heteroscedastic linear discriminant analysis (HLDA) were trained on the ASR2 data [19]. WFST decoding with a pruned 4-gram grammar network was compared with the standard tree search with an unpruned 3-gram LM. The WER and real-time factor (RT) on IWSLT 2011 evaluation and IWSLT 2012 evaluation data are shown in Table 3. WFST was shown to achieve the same performance as tree-search decoding, with much faster decoding speed.\nIn the final stage, acoustic and language model rescoring were performed. Base lattices were generated with 2-gram LM pruned with a threshold 10\u221210. Lattice expansion was done with 4-gram unpruned language models. Three settings were tried and the results were compared,\n(i) Language model rescoring with the 4-gram LM (ii) Considering pronunciation probability (Pron. prob.)\non top of (i) (iii) Acoustic and language model rescoring with the set-\nting of (ii) ASR performance in terms of WER are shown in Table 4. The initial non-VTLN system gave WER of 16.9% and 17.7% on IWSLT 2011 and 2012 data respectively. Moving towards the VTLN systems, when ASR1 and ASR2 branched off, it is observed that the ASR1 model gave 1.0% to 1.4% lower WER than the ASR2 model. This is because the data in ASR1 had a better match in terms of domain. Incremental performance gains can be observed in individual steps, particularly MPE, cross-adaptation and language model rescoring. The WER difference between ASR1 and ASR2 diminished to 0.4-0.5% after all optimisation steps. After system combination, the final WER is 21-25% relatively lower compared with the initial system.\nMT Decoding was performed with cube pruning [20] both in tuning and testing. Decoding was done with the min-\nimum Bayes risk criterion and reordering over punctuations was forbidden. To restore the correct case of the output the truecasing heuristic was employed. The same set of standard techniques was applied on En\u2192Fr and En\u2192De translation.\nThe MT system was tested on IWSLT 2010 development data and 2012 evaluation data, and the results are shown in Table 5. Performance are shown in terms of cased and punctuated BLEU scores. When given the reference transcript, the MT system gave 40.9 and 21.5 BLEU score for MT tasks in En\u2192Fr and En\u2192De respectively. The monolingual translation system (\u00a74) restored case and punctuation information. It was tested on pseudo ASR and real ASR output and yielded 88.0 and 69.0 BLEU score. Finally in the SLT setting, the decoded ASR result was fed to the monolingual translation system and the output were subsequently translated. The BLEU score is 31.7 and 16.8 for SLT tasks in En\u2192Fr and En\u2192De respectively.\nIn Table 6, the official IWSLT 2014 evaluation performance in terms of BLEU and TER (cased, punctuated and non-case, non-punctuated) for the USFD primary system is shown."}, {"heading": "6. System integration", "text": "The USFD primary system is a pipeline SLT system in which 1-best ASR result was directly fed to the MT system. System integration experiments were tried in the En\u2192Fr SLT task and the results were submitted as contrastive systems. Figure 2 depicts the integrated system and its comparison with the pipeline system. In the integrated system, ASR system hypotheses are expanded in the form of lattices, confusion networks or N -best lists. A quality estimation (QE) module evaluated and rescored the ASR outputs before they were fed to the MT system.\nIn our implementation, 10-best outputs from the ASR system on the IWSLT 2011 evaluation data were used for QE training. The QE module derived 117 QuEst [21, 22] features from each sentence to describe its linguistic, statistical properties as well as the statistics from the ASR and MT models. Out of the 117 features, top 58 features were selected using the Gaussian Process (GP) with RBF kernel as described in [23]. Further, GP was used to learn the relationship between the selected features and the translation performance of the sentence (in this case, sentence-based METEOR score) [24]. During testing, the estimated translation performance was used to rescore the 10-best ASR output. Details of the integrated system were described in [25].\nThe ROVER combination of ASR1 and ASR2 systems only provided 1-best output. In the integration experiment, the 10-best output from ASR1 was used instead.\nPerformance of the contrastive systems in terms of cased and punctuated BLEU score is shown in Table 7. Contrastive\n1 result is from the baseline system with pipeline setting. Contrastive 2 and 3 show the results of two different system integration settings. The baseline system gave BLEU scores 31.33 and 23.18 on IWSLT 2012 and IWSLT 2014 data. The baseline numbers are inferior to the primary system number (IWSLT 2012: 31.7; IWSLT 2014: 23.45) as shown in Table 5 and 6. This is because the baseline here did not benefit from ASR system combination.\nRescoring gives 0.18 and 0.09 BLEU improvements to IWSLT 2012 and IWSLT 2014 data respectively. By inspecting the results, it was found that rescoring generally had higher effectiveness for the sentences with low ASR confidence. Therefore, a confidence threshold was set, and rescoring was only performed when the ASR confidence dropped below this threshold. For IWSLT 2012 data, optimality was reached when 55% of the sentences were selected by this confidence criteria to rescore, resulting a further 0.36 BLEU score gain. This threshold was applied on IWSLT 2014 data, a 0.17 BLEU score gain was observed."}, {"heading": "7. Summary", "text": "In this paper, the USFD SLT system for IWSLT 2014 was described. Automatic speech recognition (ASR) is achieved by two multi-pass deep neural network systems with slightly different tandem configurations and different training data. Machine translation (MT) is achieved by a monolingual phrase-based monotonic translation system which recovers case and inserts punctuation, followed by a bilingual phrasebased translation system. The USFD contrastive systems explore the integration of ASR and MT by using a quality estimation system to rescore the ASR outputs, optimising towards better translation. This gives noticeable BLEU improvement on the IWSLT 2012 and 2014 evaluation data."}, {"heading": "8. References", "text": "[1] TED, \u201cTechnology entertainment design,\u201d http://www.\nted.com, 2006.\n[2] M. Hasan, R. Doddipatla, and T. Hain, \u201cMulti-pass sentence-end detection of lecture speech,\u201d in Proc. Interspeech, 2014.\n[3] T. Hain, L. Burget, J. Dines, P. N. Garner, A. E. Hannani, M. Huijbregts, M. Karafiat, M. Lincoln, and V. Wan, \u201cThe AMIDA 2009 meeting transcription system,\u201d in Proc. Interspeech 2010, 2010, pp. 358\u2013361.\n[4] R. Doddipatla, M. Hasan, and T. Hain, \u201cSpeaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition,\u201d 2014.\n[5] M. Cettolo, C. Girardi, and M. Federico, \u201cWIT3: Web inventory of transcribed and translated talks,\u201d in Proceedings of Conference of European Association for Machine Translation Trento (Italy), 2012, pp. 261\u2013268.\n[6] \u201cACL 2014 ninth workshop on statistical machine translation,\u201d http://www.statmt.org/wmt14/ translation-task.html, 2014.\n[7] R. C. Moore and W. Lewis, \u201cIntelligent selection of language model training data,\u201d in Proceedings of the ACL 2010 Conference Short Papers, ser. ACLShort \u201910. Stroudsburg, PA, USA: Association for Computational Linguistics, 2010, pp. 220\u2013224.\n[8] H. Hermansky, D. Ellis, and S. Sharma, \u201cTandem connectionist feature extraction for conventional HMM systems,\u201d in Proc. ICASSP, 2000.\n[9] D. Povey and P. C. Woodland, \u201cMinimum phone error and I-smoothing for improved discriminative training,\u201d in Proc. ICASSP, 2002.\n[10] T. Hain, \u201cImplicit modelling of pronunciation variation in automatic speech recognition,\u201d Speech Communication, vol. 46, pp. 171\u2013188, 2005.\n[11] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst, \u201cMoses: open source toolkit for statistical machine translation,\u201d in Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ser. ACL \u201907. Stroudsburg, PA, USA: Association for Computational Linguistics, 2007, pp. 177\u2013180.\n[12] A. Birch, N. Durrani, and P. Koehn, \u201cEdinburgh SLT and MT system description for the IWSLT 2013 evaluation,\u201d in Proceedings of International Workshop on Spoken Language Translation, 2013.\n[13] C. Dyer, V. Chahuneau, and N. A. Smith, \u201cA simple, fast, and effective reparameterization of IBM model 2,\u201d in Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta, Georgia: Association for Computational Linguistics, June 2013, pp. 644\u2013648.\n[14] K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn, \u201cScalable modified Kneser-Ney language model estimation,\u201d in Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Sofia, Bulgaria: Association for Computational Linguistics, August 2013, pp. 690\u2013696.\n[15] C. Cherry and G. Foster, \u201cBatch tuning strategies for statistical machine translation,\u201d in Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, ser. NAACL HLT \u201912. Stroudsburg, PA, USA: Association for Computational Linguistics, 2012, pp. 427\u2013436.\n[16] S. Peitz, M. Freitag, A. Mauser, and H. Ney, \u201cModelling punctuation prediction as machine translation,\u201d in Proc. IWSLT, 2011.\n[17] E. Cho, J. Niehues, and A. Waibel, \u201cSegmentation and punctuation prediction in speech language translation using a monolingual translation system,\u201d in Proc. IWSLT, 2012.\n[18] T. Hain, A. E. Hannani, S. N. Wrigley, and V. Wan, \u201cAutomatic speech recognition for scientific purposes - webASR,\u201d in Proc. Interspeech, 2008, pp. 504\u2013507.\n[19] N. Kumar and A. G. Andreou, \u201cHeteroscedastic discriminant analysis and reduced rank HMMs for improved speech recognition,\u201d Speech Communication, vol. 26, pp. 283\u2013297, 1998.\n[20] L. Huang and D. Chiang, \u201cForest rescoring: Faster decoding with integrated language models,\u201d in Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Prague, Czech Republic: Association for Computational Linguistics, June 2007, pp. 144\u2013151.\n[21] L. Specia, K. Shah, J. G. C. d. Souza, and T. Cohn, \u201cQuEst - A translation quality estimation framework,\u201d in Proceedings of 51st Annual Meeting of the Association for Computational Linguistics: Demo Session, Sofia, Bulgaria, 2013, p. 794.\n[22] K. Shah, E. Avramidis, E. Bic\u0327ici, and L. Specia, \u201cQuEst - design, implementation and extensions of a framework for machine translation quality estimation,\u201d Prague Bull. Math. Linguistics, vol. 100, pp. 19\u201330, 2013.\n[23] K. Shah, T. Cohn, and L. Specia, \u201cAn Investigation on the Effectiveness of Features for Translation Quality Estimation,\u201d in Machine Translation Summit XIV, Nice, France, 2013, pp. 167\u2013174.\n[24] M. Denkowski and A. Lavie, \u201cMeteor universal: Language specific translation evaluation for any target language,\u201d in Proceedings of WMT14, 2014.\n[25] R. W. M. Ng, K. Shah, W. Aziz, L. Specia, and T. Hain, \u201cQuality estimation for ASR K-best list rescoring in spoken language translation,\u201d Submitted to Proc. ICASSP, 2015."}], "references": [{"title": "Technology entertainment design", "author": ["TED"], "venue": "http://www. ted.com, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-pass sentence-end detection of lecture speech", "author": ["M. Hasan", "R. Doddipatla", "T. Hain"], "venue": "Proc. Interspeech, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "The AMIDA 2009 meeting transcription system", "author": ["T. Hain", "L. Burget", "J. Dines", "P.N. Garner", "A.E. Hannani", "M. Huijbregts", "M. Karafiat", "M. Lincoln", "V. Wan"], "venue": "Proc. Interspeech 2010, 2010, pp. 358\u2013361.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition", "author": ["R. Doddipatla", "M. Hasan", "T. Hain"], "venue": "2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "WIT3: Web inventory of transcribed and translated talks", "author": ["M. Cettolo", "C. Girardi", "M. Federico"], "venue": "Proceedings of Conference of European Association for Machine Translation Trento (Italy), 2012, pp. 261\u2013268.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Intelligent selection of language model training data", "author": ["R.C. Moore", "W. Lewis"], "venue": "Proceedings of the ACL 2010 Conference Short Papers, ser. ACLShort \u201910. Stroudsburg, PA, USA: Association for Computational Linguistics, 2010, pp. 220\u2013224.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["H. Hermansky", "D. Ellis", "S. Sharma"], "venue": "Proc. ICASSP, 2000.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Minimum phone error and I-smoothing for improved discriminative training", "author": ["D. Povey", "P.C. Woodland"], "venue": "Proc. ICASSP, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Implicit modelling of pronunciation variation in automatic speech recognition", "author": ["T. Hain"], "venue": "Speech Communication, vol. 46, pp. 171\u2013188, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ser. ACL \u201907. Stroudsburg, PA, USA: Association for Computational Linguistics, 2007, pp. 177\u2013180.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Edinburgh SLT and MT system description for the IWSLT 2013 evaluation", "author": ["A. Birch", "N. Durrani", "P. Koehn"], "venue": "Proceedings of International Workshop on Spoken Language Translation, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A simple, fast, and effective reparameterization of IBM model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta, Georgia: Association for Computational Linguistics, June 2013, pp. 644\u2013648.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Sofia, Bulgaria: Association for Computational Linguistics, August 2013, pp. 690\u2013696.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["C. Cherry", "G. Foster"], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, ser. NAACL HLT \u201912. Stroudsburg, PA, USA: Association for Computational Linguistics, 2012, pp. 427\u2013436.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Modelling punctuation prediction as machine translation", "author": ["S. Peitz", "M. Freitag", "A. Mauser", "H. Ney"], "venue": "Proc. IWSLT, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmentation and punctuation prediction in speech language translation using a monolingual translation system", "author": ["E. Cho", "J. Niehues", "A. Waibel"], "venue": "Proc. IWSLT, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic speech recognition for scientific purposes webASR", "author": ["T. Hain", "A.E. Hannani", "S.N. Wrigley", "V. Wan"], "venue": "Proc. Interspeech, 2008, pp. 504\u2013507.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Heteroscedastic discriminant analysis and reduced rank HMMs for improved speech recognition", "author": ["N. Kumar", "A.G. Andreou"], "venue": "Speech Communication, vol. 26, pp. 283\u2013297, 1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Forest rescoring: Faster decoding with integrated language models", "author": ["L. Huang", "D. Chiang"], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Prague, Czech Republic: Association for Computational Linguistics, June 2007, pp. 144\u2013151.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "QuEst - A translation quality estimation framework", "author": ["L. Specia", "K. Shah", "J.G.C. d. Souza", "T. Cohn"], "venue": "Proceedings of 51st Annual Meeting of the Association for Computational Linguistics: Demo Session, Sofia, Bulgaria, 2013, p. 794.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "QuEst - design, implementation and extensions of a framework for machine translation quality estimation", "author": ["K. Shah", "E. Avramidis", "E. Bi\u00e7ici", "L. Specia"], "venue": "Prague Bull. Math. Linguistics, vol. 100, pp. 19\u201330, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "An Investigation on the Effectiveness of Features for Translation Quality Estimation", "author": ["K. Shah", "T. Cohn", "L. Specia"], "venue": "Machine Translation Summit XIV, Nice, France, 2013, pp. 167\u2013174.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["M. Denkowski", "A. Lavie"], "venue": "Proceedings of WMT14, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Quality estimation for ASR K-best list rescoring in spoken language translation", "author": ["R.W.M. Ng", "K. Shah", "W. Aziz", "L. Specia", "T. Hain"], "venue": "Submitted to Proc. ICASSP, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The ASR and MT systems were primarily trained on TED lecture data [1].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "The two data sets are augmented by e-corner lecture data (ECRN) with a duration of 60 hours [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "In ASR2, 165 hours of meeting data from the AMI, AMIDA and ICSI corpora are added so the trained model will reflect also generic domains other than lectures [3, 4].", "startOffset": 157, "endOffset": 163}, {"referenceID": 3, "context": "In ASR2, 165 hours of meeting data from the AMI, AMIDA and ICSI corpora are added so the trained model will reflect also generic domains other than lectures [3, 4].", "startOffset": 157, "endOffset": 163}, {"referenceID": 4, "context": "Textual data for the training of language models and translation models were obtained from the affiliated websites of the IWSLT and WMT evaluations [5, 6].", "startOffset": 148, "endOffset": 154}, {"referenceID": 5, "context": "The OOD corpora were selected with the cross entropy difference criterion [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "There are two DNN systems with tandem configurations in ASR [8].", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "Bottleneck (BN) features were derived from deep neural network (DNN)s [4], and GMM-HMM systems were trained on these bottleneck features.", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "To improve the performance of the acoustic model, minimum phone error (MPE) training was performed using the lattices which were generated using a uni-gram language model [9].", "startOffset": 171, "endOffset": 174}, {"referenceID": 2, "context": "All ASR LMs were based on a word-list with a 60k word vocabulary extracted based on our standard English ASR inventory and the English part of the TED MT training data for IWSLT 2014 [3, 5].", "startOffset": 183, "endOffset": 189}, {"referenceID": 4, "context": "All ASR LMs were based on a word-list with a 60k word vocabulary extracted based on our standard English ASR inventory and the English part of the TED MT training data for IWSLT 2014 [3, 5].", "startOffset": 183, "endOffset": 189}, {"referenceID": 8, "context": "Pronunciation probabilities were incorporated in final stage decoding [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "A phrase-based model using MOSES [11] in a standard setting was employed.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "Following previous findings [12], data selection via a cross-entropy difference criterion (detailed in \u00a72.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "The phrase length was limited to 5 and word-alignment was obtained with FASTALIGN [13].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "5-gram LMs were trained using LMPLZ [14].", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "100-best MIRA tuning was employed [15].", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": "Following previous work [16, 17], a monolingual translation system was trained to recover casing and punctuation from the ASR output, thus producing source sentences which are more adequate for translation.", "startOffset": 24, "endOffset": 32}, {"referenceID": 15, "context": "Following previous work [16, 17], a monolingual translation system was trained to recover casing and punctuation from the ASR output, thus producing source sentences which are more adequate for translation.", "startOffset": 24, "endOffset": 32}, {"referenceID": 9, "context": "The evaluation systems for ASR and MT are multi-pass systems with resource optimisation and environment management capabilities [11, 18].", "startOffset": 128, "endOffset": 136}, {"referenceID": 16, "context": "The evaluation systems for ASR and MT are multi-pass systems with resource optimisation and environment management capabilities [11, 18].", "startOffset": 128, "endOffset": 136}, {"referenceID": 17, "context": "In a pilot experiment, PLP systems with heteroscedastic linear discriminant analysis (HLDA) were trained on the ASR2 data [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "MT Decoding was performed with cube pruning [20] both in tuning and testing.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "The QE module derived 117 QuEst [21, 22] features from each sentence to describe its linguistic, statistical properties as well as the statistics from the ASR and MT models.", "startOffset": 32, "endOffset": 40}, {"referenceID": 20, "context": "The QE module derived 117 QuEst [21, 22] features from each sentence to describe its linguistic, statistical properties as well as the statistics from the ASR and MT models.", "startOffset": 32, "endOffset": 40}, {"referenceID": 21, "context": "Out of the 117 features, top 58 features were selected using the Gaussian Process (GP) with RBF kernel as described in [23].", "startOffset": 119, "endOffset": 123}, {"referenceID": 22, "context": "Further, GP was used to learn the relationship between the selected features and the translation performance of the sentence (in this case, sentence-based METEOR score) [24].", "startOffset": 169, "endOffset": 173}, {"referenceID": 23, "context": "Details of the integrated system were described in [25].", "startOffset": 51, "endOffset": 55}], "year": 2015, "abstractText": "The University of Sheffield (USFD) participated in the International Workshop for Spoken Language Translation (IWSLT) in 2014. In this paper, we will introduce the USFD SLT system for IWSLT. Automatic speech recognition (ASR) is achieved by two multi-pass deep neural network systems with adaptation and rescoring techniques. Machine translation (MT) is achieved by a phrase-based system. The USFD primary system incorporates state-of-the-art ASR and MT techniques and gives a BLEU score of 23.45 and 14.75 on the English-to-French and English-to-German speech-totext translation task with the IWSLT 2014 data. The USFD contrastive systems explore the integration of ASR and MT by using a quality estimation system to rescore the ASR outputs, optimising towards better translation. This gives a further 0.54 and 0.26 BLEU improvement respectively on the IWSLT 2012 and 2014 evaluation data.", "creator": "LaTeX with hyperref package"}}}