{"id": "1509.03946", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2015", "title": "Parametric Maxflows for Structured Sparse Learning with Convex Relaxations of Submodular Functions", "abstract": "The cusps problem an relevant penalties collected service geometry relaxations of submodular wherein is known they so fine to minimizing resolvent convex functions pushed the input submodular polyhedra. In called paper, done material takes comprehensive class a functional penalties for with harsher be certain reason may solved via with efficiently non-trivial active following parametric maxflow benchmarking. We time series what the parametric maxflow algorithm proposed though Gallo la iraq. and example spelling, which minutes, though same days - matter, came the budget means actually a constant tends several own single computation addition where corresponding maxflow time-dependent, them be adapted to situations the margaritifer problems this rather tackle. Several existing structured penalties satisfy certain any; means, remunerated learning with these punishments problem isomorphic quickly similar way semantic maxflow algorithm. We also probes three computational java performance of 's provision basis.", "histories": [["v1", "Mon, 14 Sep 2015 04:11:02 GMT  (3462kb,D)", "http://arxiv.org/abs/1509.03946v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yoshinobu kawahara", "yutaro yamaguchi"], "accepted": false, "id": "1509.03946"}, "pdf": {"name": "1509.03946.pdf", "metadata": {"source": "CRF", "title": "Parametric Maxflows for Structured Sparse Learning with Convex Relaxations of Submodular Functions", "authors": ["Yoshinobu Kawahara", "Yutaro Yamaguchi"], "emails": ["ykawahara@sanken.osaka-u.ac.jp,", "yamaguchi@mist.i.u-tokyo.ac.jp,"], "sections": [{"heading": "1 Introduction", "text": "Learning with structural information in data has been a primary interest in machine learning. Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].\nRecently, it has been revealed that many of the existing structured sparsity-inducing penalties can be interpreted as convex relaxations of submodular functions [3, 41]. Based on this result, the calculation of the proximal operators for such penalties is known to be reduced to the minimization of separable convex functions over the corresponding submodular polyhedra, which can be solved via the iteration of submodular minimization. However, minimizing a submodular function is not effectively scalable (due to its generality); thus, an unavoidable next step is to clarify when the problem is solvable as a special case that can be calculated faster, especially cases that are solvable as an efficiently solvable class of network flow optimization. Several specific problems are known to be solvable via such network flow optimization. For example, a class of the total variation, which is equivalent to generalized fused Lasso (GFL), is known to be solved via parametric maxflows [10, 19]. Mairal et al. (2011) [36] and Mairal & Yu (2013) [37] proposed parametric maxflow algorithms for l1/l\u221e-regularization and the path-coding, respectively. In addition, Takeuchi et al. (2015) [49] recently proposed a generalization of GFL to a hyper-graph case, which they call higher-order fused Lasso, with a parametric maxflow algorithm.\nIn this paper, we first develop sufficient conditions for estimating whether a submodular function corresponding to a given structured penalty is graph-representable, i.e., realizable as a projection of a graph-cut function with auxiliary nodes. Several existing structured penalties from submodular functions, such as (overlapping) grouped penalty and (generalized) fused penalty, satisfy these conditions. Then, we show that the parametric maxflow algorithm proposed by Gallo et al. [17] and its variants (hereafter, we call those the GGT-type algorithms) is applicable to calculate the proximal problems for penalties obtained via convex\nar X\niv :1\n50 9.\n03 94\n6v 1\n[ cs\n.L G\n] 1\n4 Se\np 20\nrelaxation of such submodular functions, which runs at the cost of only a constant factor in the worst-case time bound of the corresponding maxflow optimization. Also, we empirically investigate the comparative performance of the proposed framework against existing algorithms.\nThus, the main contribution of this work is two-fold: (i) we develop sufficient conditions (with concrete ways of constructing the corresponding networks) for the class of structured penalties that can be solved via a parametric maxflow algorithm and (ii) we show that an efficient parametric flow algorithm can be applied to the proximal problem for such penalties. Note that the first one is closely related to the class of energy minimization problems that can be solved with the so-called graph-cut algorithm, which has been discussed actively in computer vision [30, 29]. Similar discussions are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 38, 16]. Our current work would give a relation to such discussions to structured regularized learning. And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.\nThe remainder of this paper is organized as follows. We first define notations and describe preliminaries in Section 2. Then, in Section 3, we give a brief review of structured penalties obtained as convex relaxations of submodular functions. In Section 4, we describe the sufficient condition for estimating whether the proximal problem for a given penalty is solvable via network flow optimization. In Section 5, we develop the parametric flow algorithm to proximal problems for penalties satisfying this condition. In Section 6, we describe related work. Finally, we show runtime comparisons for calculating the proximal problem for the penalties by the proposed and existing algorithms in Section 7, and conclude the paper in Section 8. All proofs are given in Appendix C."}, {"heading": "2 Notations and Preliminaries", "text": "In this section, we introduce notations used in this paper, and give brief reviews on submodular functions in Section 2.1 and network flow optimization in Section 2.2."}, {"heading": "2.1 Submodular Functions", "text": "Let d be a positive integer and V := {1, 2, . . . , d}. We denote the complement of A by A for A \u2286 V , i.e., A = V \\ A. For a real vector w = (wi)i\u2208V \u2208 RV and a subset A \u2286 V , define w(A) := \u2211 i\u2208A wi. A set function F : 2V \u2192 R is called submodular if\nF (A) + F (B) \u2265 F (A \u2229B) + F (A \u222aB)\nfor any A,B \u2286 V [11, 14].\nWe denote by F\u0302 the Lova\u0301sz extension of a set function F with F (\u2205) = 0, i.e., F\u0302 : RV \u2192 R is a continuous function defined as, for each w \u2208 RV ,\nF\u0302 (w) := d\u2211 i=1 wji ( F ({j1, . . . , ji})\u2212 F ({j1, . . . , ji\u22121}) ) ,\nwhere j1, j2, . . . , jd \u2208 V are the distinct indices corresponding to a permutation that arranges the entries of w in nonincreasing order, i.e., wj1 \u2265 wj2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 wjd [33]. For a submodular function F with F (\u2205) = 0, the submodular polyhedron P (F ) \u2286 RV and the base polyhedron B(F ) \u2286 RV are respectively defined as\nP (F ) : = {x \u2208 RV | x(A) \u2264 F (A) (\u2200A \u2286 V ) } and B(F ) : = {x \u2208 P (F ) | x(V ) = F (V ) }.\nWe define P+(F ) := RV+ \u2229 P (F ). For an integer i with 0 \u2264 i \u2264 d, let ( V i ) denote the set of i-element subsets of V . For any set function F ,\nthere uniquely exist functions F (i) : ( V i ) \u2192 R (i = 0, 1, . . . , d) such that\nF (A) = |A|\u2211 i=0 \u2211 Y \u2208(Ai ) F (i)(Y ) (A \u2286 V ),\nwhere, for each i = 0, 1, . . . , d, F (i)(A) = \u2211 Y\u2286A (\u22121)|A\u2212Y |F (Y ) (A \u2208 ( V i ) )\nby the Mo\u0308bius inversion formula (see, for example, [1]). A set function F is said to be of order k for an integer k with 0 \u2264 k \u2264 d if F (k) 6= 0 and F (i) = 0 (k + 1 \u2264 i \u2264 d)."}, {"heading": "2.2 Flow Terminology", "text": "Suppose we are given a directed network N = (U,E) with a finite vertex set U and an edge set E \u2286 U \u00d7 U , a distinguished source vertex s \u2208 U , a distinguished sink vertex t \u2208 U , and a nonnegative capacity c(u, v) for each edge (u, v) \u2208 E. Define c(u, v) := 0 for each pair (u, v) \u2208 (U \u00d7 U) \\ E. A flow f on N is a real-valued function on vertex pairs satisfying the following three constraints:\nf(u, v) \u2264 c(u, v) for (u, v) \u2208 U \u00d7 U (capacity), f(u, v) = \u2212f(v, u) for (u, v) \u2208 U \u00d7 U (antisymmetry), and\u2211\nu\u2208Uf(u, v) = 0 for v \u2208 U \\ {s, t} (conservation). The value of flow f is \u2211 v\u2208U f(v, t). A maximum flow is a flow of maximum value. For disjoint A,B \u2286 V ,\nthe capacity of pair (A,B) is defined as c(A,B) := \u2211 u\u2208A,v\u2208B c(u, v). A cut (C,C) is a vertex partition (i.e., C \u222a C = U , C \u2229 C = \u2205) such that s \u2208 C and t \u2208 C. A minimum cut is a cut of minimum capacity. The capacity constraint implies that for any flow f and any cut (C,C), we have f(C,C) \u2264 c(C,C), which implies that the value of a maximum flow is at most the capacity of a minimum cut. The max-flow min-cut theorem of [13] states that these two quantities are equal."}, {"heading": "3 Penalties via Convex Relaxation of Submodular Functions", "text": "We briefly review structured penalties through convex relaxations of submodular functions, which cover several known structured sparsity-inducing penalties, in Subsection 3.1, and then the existing optimization methods for those proximal problems in Subsection 3.2."}, {"heading": "3.1 Structured Penalties from Submodular Functions", "text": "Structured penalties obtained via convex relaxations of submodular functions can be categorized into two types. Here, we review these respectively in Sections 3.1.1 and 3.1.2.\n3.1.1 Penalty via `p-relaxation of Nondecreasing Submodular Function\nThe first type of the penalty from a submodular function is defined through convex relaxation with the `p-norm [3, 41, 5]. For this type, a submodular function F is required to be non-decreasing. To define this penalty, we first consider a function h : RV \u2192 R that penalizes both supports and lp-norm on the supports;\nh(w) = 1\np \u2016w\u2016pp +\n1 r F (supp(w)), (1)\nwhere 1/p + 1/r = 1. Note that when p tends to infinity, function g tends to F (supp(w)) restricted to the l\u221e-ball. The following is known for any p \u2208 (1,+\u221e).\nProposition 1 ([41]). Let F be a non-decreaing function s.t. F ({i}) > 0 for all i \u2208 V . The tightest convex homogeneous lower-bound of h(w) is a norm, denoted by \u2126\u0303F,p, such that its dual norm equals to, for s \u2208 RV ,\n\u2126\u0303\u2217F,p(s) = sup A\u2286V,A6=\u2205 \u2016sA\u2016r F (A)1/r . (2)\nNote that, if F is submodular, then only stable inseparable sets may be kept in the definition of \u2126\u0303\u2217F,p in Eq. (2). From the above definition, we obtain, for any w \u2208 RV ,\n\u2126\u0303F,p(w) = sup s\u2208RV\nw>s such that \u2126\u2217F,p(w) \u2264 1\n= sup s\u2208RV\nw>s such that \u2200A \u2286 V, \u2016sA\u2016rr \u2264 F (A)\n= sup t\u2208P+(F )\n\u2211 i\u2208V t 1/r i |wi|, (3)\nwhere we change the variables as ti = s r i . The first equality is obtained using the Fenchel duality. Consequently, the norm \u2126\u0303F,p is computed with a separable form over (the positive part of) the corresponding submodular polyhedron.\nIt is easy to check that, if we use F (A) = |A|, the \u2126\u0303F,p is equivalent to the `p-regularization. And, if we use F (A) = \u2211 g\u2208G min{|A\u2229g|, 1} for a group of variables G, then \u2126\u0303f,p is equivalent to the (possibly, overlapping) `1/`\u221e and non-overlapping `1/`p group regularizations or provides group sparsity similar to the overlapping `1/`p group regularization [41, 5]."}, {"heading": "3.1.2 Penalty by the Lova\u0301sz Extension of Submodular Function", "text": "The other type of penalty is defined as the Lova\u0301sz extension, i.e., `\u221e-relaxation, of a submodular function F with F (\u2205) = F (V ) = 0. This is known to make some of the components of w equal when used as a regularizer [4]. A representative example of this type of penalty is the generalized fused Lasso (GFL), which is defined for a given undirected network N = (V,E) as\n\u2126fl(w) = \u2211\n(i,j)\u2208E\naij |wi \u2212 wj |,\nwhere aij is the weight on each pair (i, j). This penalty is known to be equivalent to the Lova\u0301sz extension of a cut function on N , i.e., F (A) = \u2211 i\u2208A,j\u2208V \\A aij [4, 54]. This can be extended to a hypergraph H = (V,E) with non-negative weight ae for each hyperedge e \u2208 E, where the Lova\u0301sz extension of a hypergraph cut function F (A) = \u2211 e\u2208E:e\u2229A6=\u2205,e\u2229A 6=\u2205 ae gives the hypergraph regularization \u2126hr(w) = \u2211 e\u2208E ae(maxi\u2208e wi \u2212 mini\u2208e wj) p [22].\nFrom the definition, the Lova\u0301sz extension of a submodular function with F (\u2205) = 0 can be represented as a greedy solution over the submodular polyhedron [33], i.e.,\nF\u0302 (w) = sup t\u2208P+(F ) \u2211 i\u2208V ti|wi|.\nwhich is in fact the equivalent form with Eq. (3) for r = 1 (i.e., p =\u221e)."}, {"heading": "3.2 Proximal Problem for Submodular Penalties", "text": "The above penalties have a common form, for a (normalized) submodular function F ,\n\u2126F,p(w) := sup t\u2208P+(F ) \u2211 i\u2208V t 1/r i |wi|, (4)\nwhere p \u2208 (1,+\u221e) and 1/p + 1/r = 1. However, note that, if F is not nondecreasing, then \u2126F,p(w) does not necessarily has the duality as described in Section 3.1. When using the norm \u2126F,p as a regularizer, we solve the following problem for some (convex and smooth) loss l : RV \u2192 R that corresponds to the respective learning task:\nmin w\u2208RV\nl(w) + \u03bb \u00b7 \u2126F,p(w) (\u03bb > 0).\nSince the objective of this problem is the sum of smooth and non-smooth convex functions, a major option for its optimization is the proximal gradient method, such as FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) [7]. Thus, our necessary step is to compute iteratively the proximal operator\nprox\u03bb\u2126F,p(z) := argmin w\u2208Rd\n1 2 \u2016z \u2212w\u201622 + \u03bb \u00b7 \u2126F,p(w), (5)\nwhere z \u2208RV . From the definition (4), we can calculate prox\u03bb\u2126F,p by solving\nmin w\u2208RV max t\u2208P+(F )\n1 2 \u2016w \u2212 z\u201622 + \u03bb \u2211 i\u2208V t 1/r i |wi| = max t\u2208P+(F ) \u2211 i\u2208V min wi\u2208R { 1 2 (wi \u2212 zi)2 + \u03bbt1/ri |wi| } = \u2212 min\nt\u2208P+(F ) \u2211 i\u2208V \u03c8i(ti), (6)\nwhere \u03c8i(ti) = \u2212minwi\u2208R{ 12 (wi \u2212 zi) 2 + \u03bbt 1/r i |wi|}. Thus, solving the proximal problem equals minimizing a separable convex function over the submodular polyhedron.\nBased on the above formulation, Obozinski & Bach (2012) [41] recently suggested a divide-and-conquer algorithm as an adaptation of the decomposition algorithm by Groenevelt (1991) [21] for penalties from general submodular functions (for the case of p = 2). A more general version of this approach was also developed by Bach (2013) [5]. However, a straightforward implementation of this approach yields O(d)-time calculation of submodular minimization, which could be time-consuming especially in large problems.\nWe address this issue by considering it from the following two perspectives. First, in Section 4, we develop an explicit sufficient conditions for determining whether the proximal problem for a given penalty can be solved through maximum flow optimization rather than submodular minimization. Maximum flow optimization can be regarded as an efficiently-solvable special case of submodular minimization, and is known to be much faster than submodular minimization in general; thus, this could be useful to judge whether a given penalty can be dealt with in a scalable manner as a regularizer. The respective structured penalties from submodular functions mentioned above are in fact instances of this case. On that basis, in Section 5, we develop a procedure for problem (5) that runs at the cost of only a constant factor in its worst-case time bound of the maxflow calculation rather than the O(d)-time calculation of the straightforward implementation. In other words, we discuss whether an efficient parametric maxflow algorithm is applicable to the current problem."}, {"heading": "4 Graph-Representable Penalties", "text": "In this section, we develop sufficient conditions for determining whether the proximal problem for a given structured penalty is solvable through an efficiently-solvable class of network flow optimization. We also describe a concrete procedure to construct the corresponding network."}, {"heading": "4.1 Graph-Representable Set Functions", "text": "The currently-known best complexity of minimizing a general submodular function is O(d6 + d5 EO), where EO is the cost of evaluating a function value [42]. Although there exist practically faster algorithms, such as the minimum-norm-point algorithm [15] as well as faster algorithms for special cases (e.g., Queyranne\u2019s algorithm for symmetric submodular functions [44]), their scalability would not be practically sufficient, especially if we must solve submodular minimization several times, which is the current case. In addition,\nit is well known that a cut function (which is almost equivalent to a second order submodular function [16]) can be minimized much faster through calculation of maxflows over the corresponding network. Given a directed network N = (V,E) with nonnegative capacity c(e) on each edge e \u2208 E, a cut function \u03baN : V \u2192 R is defined as\n\u03baN (A) := \u2211 { c(e) | e \u2208 \u03b4outN (A) } (A \u2286 V ),\nwhere \u03b4outN (A) denotes the set of edges leaving A in N . If N consists of d nodes and m edges, the currently best runtime bound for the minimization is O(md) [43]. Albeit it is a better run-time bound, the empirical complexity is often much better with practical fast algorithms, e.g., [18, 9].\nHowever, the expressive power of a cut function is limited. Therefore, in order to balance between expressiveness and computational simplicity, using a higher-order function that is represented as a cut function with auxiliary nodes is often helpful. Such a function is sometimes referred to as graph-representable [26],1 and defined as follows. Let U = V \u222aW \u222a {s, t} for some finite set W with W \u2229 V = \u2205 and distinct elements s, t 6\u2208 V \u222aW , and let N\u0303 = (U, E\u0303) be a directed network with nonnegative capacity c(e) on each edge e \u2208 E\u0303. Then, define a set function F : 2V \u2192 R as\nF (A) := min Y\u2286W \u03baN\u0303 ({s} \u222aA \u222a Y ) + CF (A \u2286 V ),\nwhere CF \u2208 R is an arbitrary constant, and such F is said to be graph-representable. If W is empty, this function coincides with a cut function. The submodularity of this function is derived from the classical result of Megiddo (1974) [38] on network flow problems with multiple terminals (see, for the proof, [40])."}, {"heading": "4.2 Sufficient Conditions and Network Construction", "text": "As described in Section 5, if the corresponding set function F for norm \u2126F,p is graph-representable, then its proximal problem (5) can be efficiently solved through a parametric maxflow computation. Hereafter, we refer to such a penalty as a graph-representable penalty, which is defined as follows.\n1This class of functions is closely related to the class of energy minimization problems that can be solved by the so-called graph-cut algorithm [9, 30]. Related results are also found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].\nDefinition 2 (Graph-representable penalty). A penalty defined in Proposition 1 is said to be graph-representable if the set function F on supports is graph-representable.\nHere, we present three types of sufficient conditions for a penalty \u2126F,p from a given submodular function F (as described in Section 3.1) to be graph-representable by constructing networks representing F . The first one is mentioned as \u201ctruncations,\u201d where a function F is graph-representable by just one additional node (see Figure 1(a) and also refer [26] or [40]). The second one is closely related to [8], and the third one is derived from [16], for which we describe concrete procedures to construct networks (see Figure 1 for the construction).\nTheorem 3. A set function F with one of the following conditions is graph-representable.\n(i) F (A) = min{w(A), y} for some w \u2208 RV+ and y \u2208 R+. (ii) F is submodular and of order at most three, i.e., F (i) = 0 for i = 4, 5, . . . , d.\n(iii) F has no positive term of order at least two, i.e., F (i) \u2264 0 for i = 2, 3, . . . , d.\nRemark. It should be noted that the sum of graph-representable submodular functions is also graphrepresentable by considering the union of the corresponding networks."}, {"heading": "4.3 Examples", "text": "A submodular function F (A) = \u2211 g\u2208G min{|A\u2229g|, 1}, which gives the grouped-type regularization, is graphrepresentable since each term min{|A\u2229g|, 1} = min{eg(A), 1} is guaranteed to be so from Condition (i). The cost for constructing the corresponding network for this function is O(|G|) and the number of the additional nodes is |G|. Condition (ii) is a generalization of the condition that a cut function F (A) = \u2211 i\u2208A,j\u2208V \\A aij for a network (V,E) can be solved with maximum flows, i.e., positive weights aij for all i, j \u2208 E. Besides, a hypergraph cut function F (A) = \u2211 e\u2208E:e\u2229A 6=\u2205,e\u2229A6=\u2205 ae is also confirmed as graph-representable as follows. For each hyperedge e \u2208 E, define Fe,1(A) := ae \u00b7min{|A \u2229 e|, 1}, Fe,2(A) := \u2212ae if e \u2286 A, and Fe,2(A) := 0 otherwise. Then, F (|e|) e,2 (e) = \u2212ae and F (|A|) e,2 (A) = 0 for A 6= e. Hence, Fe,1 and Fe,2 satisfy\nConditions (i) and (iii), respectively, and it is easy to see F = \u2211 e\u2208E(Fe,1 +Fe,2). The network construction\nrequires O(\u2016E\u2016) time, where \u2016E\u2016 = \u2211 e\u2208E |e|."}, {"heading": "5 Parametric Maxflows for prox\u2126F,p(z)", "text": "We describe how the proximal problem (5) for a network representable penalty is solvable with an adaptation of the GGT-type algorithms. We first derive a parametric formulation of this problem in Subsection 5.1, and then develop the algorithm in Subsection 5.2."}, {"heading": "5.1 Parametric Formulation", "text": "We address a parametric formulation of problem (6). To this end, we first consider\nmin \u03c4\u2208B+(F )\n\u2211 i\u2208V \u03c8i(\u03c4i). (7)\nNote that the above optimization is over B+(F ) in place of P+(F ). In the following parts of this section, we suppose that F is non-decreasing (thus, B+(F ) coincides with B(F )). Although this does not necessarily hold for our case, we can show the following:\nLemma 4. Let b \u2208 RV and F be submodular, and set \u03b2 := supi\u2208V {0, F (V \\ {i})\u2212F (V )}/bi. Then, F +\u03b2b is a nondecreasing submodular function. Also, \u03c4 \u2217 is optimal to problem (7) for F if and only if \u03c4 \u2217 + \u03b2b is optimal to problem (7) for F + \u03b2b.\nThus, for F that is not non-decreasing, we can apply the algorithm developed below and recover an optimal solution to the original problem by transforming it to a non-decreasing one as in this lemma.\nFirst, we define an interval J \u2208 R as\nJ := \u22c2 i\u2208V {\u03c8\u2032i(\u03c4i) | \u03c4i \u2208 (dom\u03c8i \u2229 R+) } (= (\u2212\u221e, 0]).\nLet \u03c4 \u2217 be an optimal solution to problem (7). Denote the distinct values of \u03c8\u2032i(\u03c4 \u2217 i ) by \u03be \u2217 1 < \u00b7 \u00b7 \u00b7 < \u03be\u2217k, and let \u03be\u22170 := \u2212\u221e and \u03be\u2217k+1 := +\u221e. Let A\u2217j := { i \u2208 V | \u03c8i(\u03c4\u2217i ) \u2264 \u03be\u2217j } for j = 0, 1, . . . , k + 1. Also, let\nF\u03b1(A) := F (A)\u2212 \u2211 i\u2208A\u03c6i(\u03b1) (\u03b1 \u2208 J),\nwhere \u03c6i(\u03b1) = \u03c8 \u2032\u22121 i (\u03b1) (\u03b1 \u2208 J \\ {0}) or (|zi|/\u03bb)r (\u03b1 = 0), and \u2022\u22121 means an inverse function.\nLemma 5. Let \u03b1 \u2208 J . If \u03be\u2217j < \u03b1 < \u03be\u2217j+1, A\u2217j is a minimizer of F\u03b1. If \u03b1 = \u03be\u2217j , A\u2217j\u22121 is a minimal minimizer and A\u2217j is a maximal minimizer of F\u03b1.\nThis is obtained, in Lemma 4 of [39], by replacing the assumption on the strict convexity of \u03c8\u2032i with the monotonisity of the function in the region under consideration. As discussed in [39], this lemma implies that problem (6) can be reduced to the following parametric problem:\nmin A\u2286V\nF\u03b1(A) for all \u03b1 \u2208 J. (8)\nThat is, once we have the chain of solutions A\u22170 \u2282 \u00b7 \u00b7 \u00b7 \u2282 A\u2217k+1 to problem (8) for all \u03b1 \u2208 J , we can obtain an optimal solution to problem (7) as for j = 0, . . . , k\n\u03c4\u2217i = \u03c6i(\u03b1 \u2217 j+1) (i \u2208 A\u2217j+1 \\A\u2217j ) with \u03b1\u2217j+1 s.t. F (A\u2217j+1)\u2212 F (A\u2217j ) = \u2211 i\u2208A\u2217j+1\\A\u2217j \u03c6i(\u03b1). (9)\nThe key here is that, if function F is graph-representable, problem (8) can be solved as a parametric minimum-cut (equivalently, a parametric maxflow) problem on N\u0303 , where c(s, v) for v \u2208 V are functions of \u03b1 (since \u03c6i(\u03b1) \u2265 0 for \u03b1 \u2208 J), as will be stated in the next subsection. Once we have a solution \u03c4 \u2217 to problem (7), we can then obtain a solution to problem (5) as follows.\nCorollary 6. If \u03c4 \u2217 be an optimal solution to problem (7), then the one to problem (5) is given by\nw\u2217i = { zi \u2212 sign(zi)\u03bb(max(\u03c4\u2217i , 0))1/r if 0 \u2264 \u03c4\u2217i \u2264 (|zi|/\u03bb)r, 0 otherwise."}, {"heading": "5.2 Algorithm Description", "text": "As mentioned above, if the penalty is network representable, then problem (8) is solved as a parametric maxflow problem on network N\u0303 , where capacities c\u03b1(s, v) for v \u2208 V are c\u03b1(s, v) = (\u03c6i(\u03b1) + const.) and the others are constants for \u03b1 (note that \u03c6i(\u03b1) \u2265 0 for \u03b1 \u2208 J). Since \u03c8i is convex, those capacities satisfy the conditions of the monotone source-sink class of problems, i.e.,\n1. c(s, v) is a non-decreasing function of \u03b1 for all v \u2208 U ,\n2. c(v, t) is a non-increasing function of \u03b1 for all v \u2208 U , and\n3. c(u, v) is constant for all u, v \u2208 U \\ {s, t}.\nTherefore, for a given on-line sequence of parameter values \u03b11 < \u00b7 \u00b7 \u00b7 < \u03b1k, there exists a parametric maxflow algorithm that computes minimum cuts (A1, A1), \u00b7 \u00b7 \u00b7 , (Ak, Ak) on the network such that A1 \u2286 \u00b7 \u00b7 \u00b7 \u2286 Ak, and runs at the cost of only a constant factor in the worst-case time bound of a single maxflow computation.\nAlgorithm 1 Parametric preflow algorithm for the computation of prox\u03bb\u2126F,p(z). Input: z \u2208Rd, N\u0303 = (U,E). Output: w\u2217= prox\u03bb\u2126F,p(z). 1: Compute \u03b10 as in Eq. (10) and set \u03b1k+1 \u2190 0. Compute maximum flows f0 and fk+1, and minimum\ncuts (C0, C0) and (Ck+1, Ck+1) for \u03b10 and \u03b1k+1 such that |C0| and |Ck+1| are maximum and minimum by applying the preflow algorithm to N\u0303 , respectively. Form N \u2032 from N\u0303 by shrinking the nodes in C0 and in Ck+1 to single nodes respectively, eliminating loops, and combining multiple arcs by adding their capacities. 2: If N \u2032 has at least three vertices, let f \u20320, f \u2032k+1 be respectively the flows in N \u2032 corresponding to f0, fl+1. Then, perform Slice(N \u2032, \u03b10, \u03b1k+1, f \u20320, f \u2032k+1, C0, Ck+1). 3: Compute w\u2217 as in Corollary 6 and return w\u2217.\nProcedure Slice(N , \u03b1l, \u03b1u, fl, fu, Al, Au) 1: Find \u03b1\u0303 such that c\u03b1\u0303({s}, U \\ {s}) = c(U \\ {t}, {t}) (cf. Lemma 7). 2: Run the preflow algorithm for \u03b1\u0303 on N starting with the preflow f \u2032l formed by increasing fl on arcs (s, v)\nto saturate them and decreasing fl on arcs (v, t) to meet the capacity constraints for v \u2208 U . As an initial valid labeling, use d(v)=min{df \u2032l (v, t), df \u2032l (v, s)+(|U |\u22122)}. Find the minimal and maximal minimum cuts (C,C) and (C \u2032, C \u2032 ) for \u03b1\u0303, respectively.\n3: If C \u2032 = {t}, set \u03c4 \u2217Au\\Al\u2190F (Au)\u2212F (Al). Otherwise, run Slice(N (C \u2032), \u03b1\u0303, \u03b1u, f\u0303 , fu, C,Au). And if C 6= {s}, then run Slice(N (C), \u03b1l, \u03b1\u0303, fl, f\u0303 , Al, C \u2032 ).\nIf parametric capacities in the monotone source-sink class of problems are linear for \u03b1, all breakpoints, i.e., a value of parameter \u03b1 at which the capacity for the corresponding cut changes, can also be found at the cost of a constant factor in the worst-case time bound of a single maxflow computation using the GGT-type algorithms. However, this is generally not true for non-linear capacities because we must solve nonlinear equations to identify such a parameter value [20]. Although this is the case for our situation in general, we can find such a value in closed-form for the important cases p = 2,+\u221e due to its specific form of the problem.\nLemma 7. For network N\u0303 corresponding to a graph-representative penalty, the value of \u03b1 such that\u2211 v\u2208V c\u03b1(s, v) = \u2211 v\u2208U\\{t}c(v, t)\u2212 \u2211 v\u2208U\\V c(s, v)\nis found in close form for p = 2,+\u221e.\nThe concrete derivations of these closed-forms are described in Appendix B. For the other cases, we can at least apply some line search for finding such value of \u03b1 due to the monotonicity of \u03c6i. Thus, we can adapt the procedure of the GGT-type algorithms to find the chain of solutions A1 \u2286 \u00b7 \u00b7 \u00b7 \u2286 Ak, which results in giving an optimal solution to problem (5), as shown in Algorithm 1 (a brief review on the preflow-push algorithm used in Algorithm 1 is given in Appendix A).\nTheorem 8. Algorithm 1 is correct, and runs at the cost of a constant factor in the worst-case time bound of a single maxflow computation. For example, it runs in O(dm log(d2/m)) with dynamic trees.\nThat is, although the preflow algorithm is applied several times, the total runtime of Algorithm 1 is equivalent to that of a single application of the preflow algorithm to the original network.\nThe interval (\u03b10, \u03b1k+1) is chosen such that it covers all possible breakpoints \u03b11, . . . , \u03b1k. In other words, it suffices to select a sufficiently small \u03b10 so that for each vertex v such that (s, v) is of nonconstant capacity, c\u03b10(s, v) + \u2211 u\u2208U\\{s,t}c(u, v)<c(v, t), which is given as\n\u03b10 \u2190 \u03c8\u2032i(minv\u2208V {c(v, t)\u2212 \u2211 u\u2208V \\{s,t}c(u, v)})\u2212 1. (10)\nSimilarly, it suffices to select \u03b1k+1 sufficiently large so that for each vertex v such that (s, v) is of nonconstant capacity, c(v, t)+ \u2211 u\u2208U\\{s,t} c(v, u)<c\u03b1k+1(s, v), which is obtained as \u03b1k+1 \u2190 0.\nBy following the above results, any GGT-type algorithms can be adapted to solve the problem (8). Algorithm 1 shows an adaptation of the simplified version [2] of the original GGT algorithm."}, {"heading": "6 Related Work", "text": "Learning with structured sparsity-inducing regularization has been actively discussed in machine learning for a decade. Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6]. Generalized fused Lasso is closely related to the so-called total variation, which has often been discussed in computer vision [45]. Recently, group penalties have been applied to more complex groups, such as hierarchical penalty [57, 27] and path penalty [37]. The total variation regularization is known to be solvable with an efficient parametric maxflow algorithm [10, 19]. In addition, the proximal problem for l1/l\u221e-group penalty is calculated via parametric maxflow optimization [36]. The proposed optimization formulation includes these formulations as special cases. Bach (2010) [3] and Bach & Obozinski (2012) [41] revealed that many of the existing structured penalties are obtained as convex relaxations of submodular functions, and those proximal problems are formulated as separable convex minimization.\nThe sufficient condition in Section 4 is closely related to the class of energy minimization problems solvable by graph-cut algorithm [9, 30]. Energy minimization is a formulation of the maximum a posteriori (MAP) estimation on MRFs (see, for example, [53]). Similar results are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].\nAlgorithm 1 is a divide-and-conquer implementation of the preflow algorithm proposed by Gallo & Tarjan (1988) [18]. Bach (2010) [3] and Bach & Obozinski (2012) [41] have mentioned an application of a divide-and-conquer approach to separable convex minimization proposed by Groenevelt (1991) [21] for proximal problem (5), which takes O(d) times of the cost for submodular minimization. Algorithm 1 takes the cost for only a single run of the preflow algorithm by adapting Gallo et al. (1989) [17]\u2019s algorithm to the current problem."}, {"heading": "7 Runtime Comparisons", "text": "Here, we show empirical runtime comparisons of our algorithm with some existing ones based on different principles to see the scalabilities of the algorithms. The experiments were run on a 2.6 GHz 64-bit workstation using C++. We applied our algorithm (we refer it as \u2018PARA\u2019) to the proximal problems for the penalty from F (A) = \u2211 g min{|A \u2229 g|, 1} (p = 2,\u221e) (as a typical example of penalties described in 3.1.1) and the (generalized) fused penalty (as one described in 3.1.2). We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (\u2018DA-MNP\u2019/\u2019DA-MF\u2019) and the algorithm by [36] (\u2018MJOB\u2019)2 for the penalties from F (A) = \u2211 g min{|A \u2229 g|, 1} (MJOB is applicable only for p=\u221e), and the MNP algorithm (\u2018MNP\u2019), the algorithm by Tibshirani & Taylor (2011) [51] (\u2018TT\u2019) and the one by Liu et al. (2010) [32] (\u2018LYY\u2019)3 for the (generalized) fused penalty (LYY is applicable only to the 1d fused case).\nWe generated data as follows. First, we generated a random vector z \u2208 Rd from the uniform distribution in [\u22121, 1]d. For generalized fused penalty, we randomly generated a directed network over nodes corresponding to V using GENRMF from DIMACS Challenge.4 And for generating overlapping groups, we randomly generated d/20\u2013d/10 groups of size 30\u2013100. The graphs in Figure 2 show the empirical runtimes (in logarithm scale) for the algorithms. The plotted points are the averaged values over 10 randomly generated datasets."}, {"heading": "8 Conclusions", "text": "In this paper, we provided a comprehensive class of structured penalties for which the proximal problem can be solved via an efficiently-solvable class of parametric maxflow optimization. Then, we showed that the parametric maxflow algorithm by Gallo et al. (1989) [17] and its variants, which runs at the cost of a constant\n2We used the code modified from the one available at http://spams-devel.gforge.inria.fr/ 3We used the code available at http://www.yelab.net/software/SLEP/ 4The first DIMACS Int\u2019l Algorithm Implementation Challenge (http://dimacs.rutgers.edu/Challenges/).\nfactor in the worst-case time bound of the corresponding maxflow optimization, is applicable to solve this problem. The runtime of the proposed algorithm was empirically compared to those of the state-of-the-art ones.\nSeveral avenues would be worth investigating: First, our formulation does not include the type of sparsity by the latent group penalties, such as [25]. As mentioned in [37], the proximal problem for the penalties of Jacob et al. (2009) [25] and its generalization can be solved as a minimum-cost flow problem, which is known to be calculated as a parametric maxflow problem if the costs are quadratic and only on edges connected to source/sink [23]. It would be important to consider an unified framework connecting the current and such problems in the future work. Also, it would be interesting to address a new structured penalty satisfying the developed condition for some specific application."}, {"heading": "A Review of the Preflow-Push Algorithm", "text": "The preflow algorithm computes a maximum flow in a directed network N [18]. We first define terminology to describe the algorithm. A preflow f on N is a real-valued function on vertex pairs satisfying the capacity constant, the antisymmetry constraint, and the following relaxation of the conservation constraint\u2211\nv1\u2208U f(v1, v2) \u2265 0 for all v2 \u2208 V \\ {s, t}. (11)\nFor a given preflow, we define the excess e(v) of a vertex v to be \u2211 u\u2208U f(u, v) if v 6= s, or infinity if v = s. We call a vertex v 6= {s, t} active if e(v) > 0. A preflow is a flow if and only if Eq. (11) holds with equality for all v 6= {s, t}, i.e., e(v) = 0 for all v 6= {s, t}. A vertex pair (v, u) is a residual arc for f if (v, u) < c(v, u).\nA path of residual arcs is a residual path. A valid labeling d for a preflow f is a function from the vertices to the nonnegative integers and infinity, such that d(t) = 0, d(s) = n, and d(v) \u2264 d(u) + 1 for every residual arc (v, u). The residual distance df (v, u) from v to u is the minimum number of arcs on a residual path from v to u, or infinity if there is no such a path.\nTo implement the preflow algorithm, we use the incidence list I(v) for each vertex v. The elements of I(v) are the unordered pairs {v, u} such that (v, u) \u2208 E or (u, v) \u2208 E. The algorithm consists of repeating the following procedure until no active vertices exist. Select any active vertex v1. Let (v1, v2) be the current edge of v1. Then, apply the appropriate one of the following three cases.\nPush: If d(v1) > d(v2) and f(v1, v2) < c(v1, v2), send \u03b4 = min{e(v1), c(v1, v2)\u2212 f(v1, v2)} units of flow from v1 to v2, by increasing f(v1, v2) and e(v2) by \u03b4, and by decreasing f(v1, v2) and e(v1) by \u03b4.\nGet Next Edge: If d(v1) \u2264 d(v2) or f(v1, v2) = c(v1, v2), and (v1, v2) is not the last edge in I(v1), replace (v1,v2) as the current edge of v1 with the next in I(v1).\nRelabel: If d(v1) \u2264 d(v2) or f(v1, v2) = c(v1, v1), and (v1, v2) is the last edge in I(v1), replace d(v1) by min{(v1, v2) \u2208 I(v1), f(v1, v2) < c(v1, v2)}+1 and make the first edge in I(v1) the current edge of v1.\nWhen the algorithm terminates, f is a maximum flow. A minimum cut can be computed, after replacing d(v) by min{df (v, s) +n, df (v, t)} for each v \u2208 V , as (A,A) such that A = {v|d(v) \u2265 n}, where the sink side A is of minimum. The worst-case total time is O(dm log(d2/m)) if we use dynamic trees for the selection of active vertices."}, {"heading": "B Details of Algorithm 1", "text": "In this appendix, we describe the details of Algorithm 1 for solving the proximal problem (5). Especially, we give the closed-form solutions for finding \u03b1 described in Lemma 7 for p = 2,\u221e (i.e., r = 1, 2), which is the key to make the complexity of Algorithm 1 equivalent to the original GGT-type algorithm.\nFirst, from the definition (see, Eq. (6)), function \u03c8i(\u03c4i) is represented as\n\u03c8i(\u03c4i) =\n{ 1 2\u03bb 2\u03c4 2/r i \u2212 \u03bb\u03c4 1/r i |zi| (0 \u2264 \u03c4i \u2264 (|zi|/\u03bb) r )\n\u2212 12z 2 i ((|zi|/\u03bb) r < \u03c4i).\nNote that this function is non-increasing for \u03c4i (for \u03c4i such that 0 \u2264 \u03c4i \u2264 (|zi|/\u03bb)r, it is monotone). The derivative is given by\n\u03c8\u2032i(\u03c4i) =  \u03bb2\u03c4 1/r i \u2212\u03bb|zi| r\u03c4 1\u22121/r i (0 \u2264 \u03c4i \u2264 (|zi|/\u03bb)r)\n0 ((|zi|/\u03bb)r < \u03c4i). (12)\nThis derivative is a non-decreasing function for \u03c4i (for \u03c4i such that 0 \u2264 \u03c4i \u2264 (|zi|/\u03bb)r, it is monotone). Hence, \u03c8\u2032i has an inverse function for 0 \u2264 \u03c4i \u2264 (|zi|/\u03bb) r .\nTo give an closed-form solution for \u03b1 as in Eq. (9) and in Lemma 7, it is sufficient to describe how we can find \u03b1\u0303 satisfies for S \u2286 V \u2211\ni\u2208S\u03c6i(\u03b1\u0303) = c\u0303,\nwhere c\u0303 is some constant, which is stated in following parts for p = 2,\u221e, respectively.\nCase for p = 2 (r = 2) By substituting r = 2 into Eq. (12), we have for 0 \u2264 \u03c4i \u2264 (|zi|/\u03bb)2\n\u03c8\u2032i(\u03c4i) = \u03bb\n2\n( \u03bb\u2212 |zi|/\u03c41/2i ) .\nTherefore, \u03c4\u0303i and \u03c4\u0303j such that \u03c8 \u2032 i(\u03c4\u0303i) = \u03c8 \u2032 j(\u03c4\u0303j) satisfy\n|zi|2\u03c4\u0303j = |zj |2\u03c4\u0303i.\nThis means that, if \u03b1\u0303 satisfies \u2211 i\u2208S \u03c6i(\u03b1\u0303) = c\u0303, then we have\n\u03c6i(\u03b1\u0303) = |zi|2\u2211 j\u2208S |zj |2 c\u0303.\nThus, we can calculate such \u03b1\u0303 as\n\u03b1\u0303 = \u03c8\u2032i ( |zi|2c\u0303/ \u2211 j\u2208S |zj | 2 ) .\nCase for p = +\u221e (r = 1) By substituting r = 1 into Eq. (12), we have for 0 \u2264 \u03c4i \u2264 |zi|/\u03bb\n\u03c8\u2032i(\u03c4i) = \u03bb(\u03bb\u03c4i \u2212 |zi|).\nThus, \u03c4\u0303i and \u03c4\u0303j such that \u03c8 \u2032 i(\u03c4\u0303i) = \u03c8 \u2032 j(\u03c4\u0303j) satisfy\n|zi| \u2212 |zj | = \u03bb(\u03c4\u0303i \u2212 \u03c4\u0303j).\nThis means that, if \u03b1\u0303 satisfies \u2211 i\u2208S \u03c6i(\u03b1\u0303) = c\u0303, then we have\n\u03c6i(\u03b1\u0303) = c\u0303 d + |zi| \u2212 \u2211 j\u2208S |zj |/|S| \u03bb .\nHence, we can calculate such \u03b1\u0303 as\n\u03b1\u0303 = \u03c8\u2032i\n( c\u0303\nd + |zi| \u2212 \u2211 j\u2208S |zj |/|S| \u03bb ) ."}, {"heading": "C Proofs", "text": "Theorem 3\n(i) Let N\u0303 be the constructed network (see Figure 1-(a)) with the additional node u. Then, for each A \u2286 V , we have \u03baN\u0303 ({s}\u222aA) = w(A) and \u03baN\u0303 ({s}\u222aA\u222a{u}) = y. Hence, the constructed network indeed represents F (A) = min{w(A), y}.\n(ii) Let N\u0303 = (U = V \u222aW \u222a {s, t}, E\u0303) be the constructed network (see Figures 1-(c),(d)). We show that F (A) = minY\u2286W \u03baN\u0303 ({s} \u222a A \u222a Y ) \u2212 \u03baN\u0303 ({s}) + F (\u2205) for every A \u2286 V . It is easy to confirm that, for each A \u2286 V , the set {wB \u2208 W | B \u2286 A } attains the minimum of minY\u2286W \u03baN\u0303 ({s} \u222a A \u222a Y ). When A = \u2205, the minimum value is indeed \u03baN\u0303 ({s}). Besides, when \u2205 6= A \u2286 V , it increases by \u2211 v\u2208V max{0, F (1)(v)}\nand decreases by \u2211 v\u2208V max{0,\u2212F (1)(v)} and \u2211 A{\u2212F (|A|)(A) | A \u2286 V with |A| \u2265 2 }, which implies that\nminY\u2286W \u03baN\u0303 ({s} \u222aA \u222a Y ) = \u03baN\u0303 ({s}) + \u2211 A{F (|A|)(A) | \u2205 6= A \u2286 V } = \u03baN\u0303 ({s}) + F (A)\u2212 F (\u2205).\n(iii) For a fixed set function F satisfying Condition (iii), we construct a directed network N\u0303 = (U = V \u222aW \u222a {s, t}, E\u0303) with nonnegative capacity c : E\u0303 \u2192 R+ as follows. Then, N\u0303 coincides with the network just before Step 4 in the construction procedure in Section 4.2 (up to modular terms), and we have F (A) = minY\u2286W \u03baN\u0303 ({s} \u222aA \u222a Y ) for every A \u2286 V . First, we define W as the union of the following:\nW2 := {wA | A \u2208 ( V 2 ) },\nW+3 := {wA | A \u2208 ( V 3 ) with F (3)(A) > 0 },\nW\u22123 := {wA | A \u2208 ( V 3 ) with F (3)(A) < 0 },\nwhere each wA is an additional node adjacent to the nodes in A. Next, we define E\u0303 as the union of the following:\nE+1 := V \u00d7 {t}, E \u2212 1 := {s} \u00d7 V, E2 := {s} \u00d7W2, E21 := { (wA, v) | wA \u2208W2, v \u2208 A }, E+3 := W + 3 \u00d7 {t}, E13 := { (v, wA) | wA \u2208W + 3 , v \u2208 A }, E\u22123 := {s} \u00d7W \u2212 3 , E31 := { (wA, v) | wA \u2208W \u2212 3 , v \u2208 A }.\nLet us define a set function H : 2V \u2192 R as\nH(A) := \u2211 B{F (3)(B) | A \u2286 B \u2286 V, wB \u2208W+3 } (A \u2286 V ),\nand the capacity function c : E \u2192 R+ as, for each e \u2208 E,\nc(e) :=  max{0, F (1)({v})\u2212H({v})} (e = (v, t) \u2208 E+1 ) max{0,\u2212F (1)({v}) +H({v})} (e = (s, v) \u2208 E\u22121 ) \u2212F (2)(A)\u2212H(A) (e = (s, wA) \u2208 E2) F (3)(A) (e = (wA, t) \u2208 E+3 ) \u2212F (3)(A) (e = (s, wA) \u2208 E\u22123 ) +\u221e (e \u2208 E21 \u222a E13 \u222a E31).\nThe nonnegativity of c is guaranteed by the submodularity of F as follows: for any A = {u, v} \u2286 V with |A| = 2, we have\n0 \u2264 min B {F (B \\ {u}) + F (B \\ {v})\u2212 F (B)\u2212 F (B \\ {u, v}) | A \u2286 B \u2286 V }\n= min B { \u2212 \u2211 B\u2032 {F (|B \u2032|)(B\u2032) | A \u2286 B\u2032 \u2286 B } \u2223\u2223\u2223\u2223\u2223 A \u2286 B \u2286 V }\n= min B\n{ \u2212F (2)(A)\u2212\n\u2211 B\u2032 { F (3)(B\u2032) \u2223\u2223\u2223 A \u2286 B\u2032 \u2208 (B 3 )} \u2223\u2223\u2223\u2223\u2223 A \u2286 B \u2286 V }\n= \u2212F (2)(A)\u2212 max B\u0303\u2286V \\A \u2211 v\u2208B\u0303 F (3)(A \u222a {v}) = \u2212F (2)(A)\u2212H(A).\nWe first check the value of minY\u2286W \u03baN (Y \u222a{s}). If Y \u2229 (W2 \u222aW\u22123 ) 6= \u2205, then at least one edge in E21 \u222aE31 contributes to the cut capacity, which makes it +\u221e. Otherwise (i.e., if Y \u2286W+3 ), as no edge in E is from s to W+3 , we have \u03baN (Y \u222a {s}) \u2265 \u03baN ({s}) for any Y \u2286 W + 3 , which means that Y = \u2205 attains the minimum value. Without loss of generality, we assume F (0)(\u2205) = F (\u2205) = \u03baN ({s}) (i.e., CF = 0), Then, it suffices to show that F (A) = minY\u2286W \u03baN\u0303 (A \u222a Y \u222a {s}) for each nonempty A \u2286 V . For any B \u2286 V with wB \u2208 W2 \u222aW\u22123 , only the edge (s, wB) enters wB , and the edges (wB , v) (v \u2208 B) with c(wB , v) = +\u221e leave wB . Therefore, if B \u2286 A, we have\n\u03baN (A \u222a Y \u222a {s, wB}) = \u03baN (A \u222a Y \u222a {s})\u2212 c(s, wB) \u2264 \u03baN (A \u222a Y \u222a {s})\nfor every Y \u2286 W \\ {wB}. Moreover, for any B \u2286 V with wB \u2208 W+3 , only the edge (wB , t) leaves wB , and the edges (v, wB) (v \u2208 B) with c(v, wB) = +\u221e enter wB . Thus, if B \u2229A 6= \u2205, we have wB \u2208 Y and the edge (wB , t) contributes to the cut capacity. Thus, the minimum value is attained by Y := {wB \u2208 W2 \u222aW\u22123 |\nB \u2286 A } \u222a {wB \u2208W+3 | B \u2229A 6= \u2205 }, and we have\n\u03baN\u0303 (A \u222a Y \u222a {s})\u2212 \u03baN\u0303 ({s}) = \u2211 v\u2208A (c(v, t)\u2212 c(s, v))\u2212 \u2211\nwB\u2208W2\u222aW\u22123 : B\u2286A\nc(s, wB) + \u2211\nwB\u2208W+3 : B\u2229A6=\u2205\nc(wB , t)\n= \u2211 v\u2208A (F (1)(v)\u2212H(v)) + \u2211 wB\u2208W2 : B\u2286A (F (2)(B) +H(B)) + \u2211\nwB\u2208W\u22123 : B\u2286A\nF (3)(B) + \u2211\nwB\u2208W+3 : B\u2229A 6=\u2205\nF (3)(B)\n= \u2211\nB : \u22056=B\u2286A\nF (|B|)(B) + \u2211\nwB\u2208W+3 : B\u2229A6=\u22056=B\\A\nF (3)(B) \u2212 \u2211 v\u2208A H(v) + \u2211 wB\u2208W2 : B\u2286A H(B)\n= F (A)\u2212 F (0)(\u2205),\nwhich means \u03baN\u0303 (A\u222aY \u222a{s}) = F (A). To see the last equality, it suffices to count the contribution of F (3)(B\u2032) to the second to the last line, which is easily seen to be totally zero, for each B\u2032 \u2286 V with wB\u2032 \u2208W+3 .\nLemma 4\nThe first is shown in Lemma 2 and 3 in [40] or Proposition 2.5 in [5]. The equivalence of optimal solutions to the two problems is obvious.\nCorollary 6\nFirst, from Proposition 8.8 in [5], we obtain a solution to problem (6) as\nt\u2217i = { \u03c4\u2217i if (|zi|/\u03bb)r > \u03c4\u2217i , sign(zi)(|zi|/\u03bb)r otherwise.\n(13)\nAlthough the proposition assumes the strict convexity on separable functions, the above can be obtain since (\u2212\u03c8i)\u2032 is monotone for \u03c4i s.t. (|zi|/\u03bb)r > \u03c4\u2217i . Then, the corollary follows by solving analytically the minimization w.r.t. w in the definition of \u03c8i.\nLemma 7\nThe statement of this lemma is shown by Appendix B.\nTheorem 8\nThe correctness follows the monotone source-sink property of the current network. The runtime follows the analysis in [17] from Lemma 7."}], "references": [{"title": "Combinatorial Theory", "author": ["M. Aigner"], "venue": "Springer\u2013Verlag,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1979}, {"title": "Experimental evaluation of parametric max-flow algorithms", "author": ["M. Babenko", "J. Derryberry", "A. Goldberg", "R. Tarjan", "Y. Zhou"], "venue": "Proc. of the 6th Int\u2019l WS on Experimental Algorithms, pages 256\u2013269,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Structured sparsity-inducing norms through submodular functions", "author": ["F. Bach"], "venue": "Advances in Neural Information Processing Systems, volume 23, pages 118\u2013126.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Shaping level sets with submodular functions", "author": ["F. Bach"], "venue": "Advances in Neural Information Processing Systems, volume 24, pages 10\u201318.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with submodular functions: A convex optimization perspective", "author": ["F. Bach"], "venue": "Foundations and Trends in Machine Learning, 6(2\u20133):145\u2013373,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Structured sparsity through convex optimization", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Statistical Science, 27(4):450\u2013468,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal of Image Science, 2(1):183\u2013202,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximizing a supermodular pseudoboolean function: A polynomial algorithm for supermodular cubic functions", "author": ["A. Billionnet", "M. Minoux"], "venue": "Discrete Applied Mathematics, 12(1):1\u201311,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1985}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(9):1222\u20131239,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "On total variation minimization and surface evolution using parametric maximum flows", "author": ["A. Chambolle", "J. Darbon"], "venue": "International Journal of Computer Vision, 84:288\u2013307,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Submodular functions, matroids, and certain polyhedra", "author": ["J. Edmonds"], "venue": "Combinatorial structures and their applications, pages 69\u201387,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1970}, {"title": "Discovering sociolinguistic associations with structured sparsity", "author": ["J. Eisenstein", "N.A. Smith", "E.P. Xing"], "venue": "Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (HLT\u201911), pages 1365\u20131374,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Flows in Networks", "author": ["L.R. Ford", "D.R. Fulkerson"], "venue": "Princeton University Press,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1962}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Elsevier, 2nd edition,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "The minimum-norm-point algorithm applied to submodular function minimization and linear programming", "author": ["S. Fujishige", "T. Hayashi", "S. Isotani"], "venue": "Report RIMS-1571, Kyoto University,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Realization of set functions as cut functions of graphs and hypergraphs", "author": ["S. Fujishige", "S.B. Patkar"], "venue": "Discrete Mathematics, 226(1-3):199\u2013210,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "A fast parametric maximum flow algorithm and applications", "author": ["G. Gallo", "M.D. Grigoriadis", "R.E. Tarja"], "venue": "SIAM Journal of Computing, 18(1):30\u201355,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1989}, {"title": "A new approach to the maximum-flow problem", "author": ["A. Goldberg", "R. Tarjan"], "venue": "J. ACM, 35(4):921\u2013940,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1988}, {"title": "Parametric maximum flow algorithms for fast total variation minimization", "author": ["D. Goldfarb", "W. Yin"], "venue": "SIAM journal of Scientific Computing, 31(5):3712\u20133743,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Structural and algorithmic properties for parametric minimum cuts", "author": ["F. Granot", "S.T. McCormick", "M. Queyranne", "F. Tardella"], "venue": "Math. Prog., 135(1-2):337\u2013367,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Two algorithms for maximizing a separable concave function over a polymatroid feasible region", "author": ["H. Groenevelt"], "venue": "European Journal of Operational Research, 54:227\u2013236,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1991}, {"title": "The total variation on hypergraphs \u2013 Learning on hypergraphs revisited", "author": ["M. Hein", "S. Setzer", "L. Jost", "S.S. Rangapuram"], "venue": "Adv. in NIPS, 26:2427\u20132435,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Complexity and algorithms for nonlinear optimization problems", "author": ["D.S. Hochbaum"], "venue": "Annals of Operations Research, 153(1):257\u2013296,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "T. Zhang", "D. Metaxas"], "venue": "Journal of Machine Learning Research, 12:3371\u20133412,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Group Lasso with overlaps and graph Lasso", "author": ["L. Jacob", "G. Obozinski", "J.P. Vert"], "venue": "Proc. of the 26th Int\u2019l Conf. on Machine Learning (ICML\u201909), pages 433\u2013440,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "On fast approximate submodular minimization", "author": ["S. Jegelka", "H. Liu", "J. Bilmes"], "venue": "NIPS, 24:460\u2013468,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Proximal methods for hierarchical sparse coding", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Journal of Machine Learning Research, 12:2297\u20132334,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eqtl mapping", "author": ["S. Kim", "E.P. Xing"], "venue": "Annals of Applied Statistics, 6(3):1095\u20131117,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["P. Kohli", "L.u. Ladick\u00fd", "P.H.S. Torr"], "venue": "International Journal of Computer Vision,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "What energy functions can be minimized via graph cuts? IEEE Trans", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "on Pattern Analysis and Machine Intelligence, 26(2):147\u2013159,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Network-constrained regularization and variable selection for analysis of genomic data", "author": ["C. Li", "H. Li"], "venue": "Bioinformatics, 24(9):1175\u20131182,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "An efficient algorithm for a class of fused lasso problems", "author": ["J. Liu", "L. Yuan", "J. Ye"], "venue": "Proc. of KDD\u201910, pages 323\u2013332,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodular functions and convexity", "author": ["L. Lov\u00e1sz"], "venue": "Math. Prog.\u2013The State of the Art, pages 235\u2013257,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1983}, {"title": "Supervised group lasso with applications to microarray data analysis", "author": ["S. Ma", "X. Song", "J. Huang"], "venue": "BMC Bioinformatics, 8:60,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse modeling for image and vision processing", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "Foundations and Trends in Computer Graphics and Vision, 8(2-3):85\u2013283,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex and network flow optimization for structured sparsity", "author": ["J. Mairal", "R. Jenatton", "G. Obozinski", "F. Bach"], "venue": "Journal of Machine Learning Research, 12:2681\u20132720,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised feature selection in graphs with path coding penalties and network flows", "author": ["J. Mairal", "B. Yu"], "venue": "Journal of Machine Learning Research, 14:2449\u20132485,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal flows in networks with multiple sources and sinks", "author": ["N. Megiddo"], "venue": "Mathematical Programming, 7:97\u2013107,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1974}, {"title": "Equivalent of convex minimization problems over base polytopes", "author": ["K. Nagano", "K. Aihara"], "venue": "Japan Journal of Industrial and Applied Mathematics, 29:519\u2013534,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Structured convex optimization under submodular constraints", "author": ["K. Nagano", "Y. Kawahara"], "venue": "Proc. of the 29th Ann. Conf. on Uncertainty in Artificial Intelligence (UAI\u201913), pages 459\u2013468,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex relaxation for combinatorial penalties", "author": ["G. Obozinski", "F. Bach"], "venue": "Report HAL 00694765,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J.B. Orlin"], "venue": "Mathematicl Programming, 118:237\u2013251,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Max flows in o(nm) time, or better", "author": ["J.B. Orlin"], "venue": "Proc. of STOC\u201913, pages 765\u2013774,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Minimizing symmetric submodular functions", "author": ["M. Queyranne"], "venue": "Mathematicl Programming, 82(1):3\u201312,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L.I. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D, 60(1-4):259\u2013268,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1992}, {"title": "Toward probabilistic diagnosis and understanding of depression based on functional MRI data analysis with logistic group LASSO", "author": ["Y. Shimizu", "J. Yoshimoto", "S. Toki", "M. Takamura", "S. Yoshimura", "Y. Okamoto", "S. Yamawaki", "K. Doya"], "venue": "PLoS ONE, 10(5):e0123524,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured sparsity for audio signals", "author": ["K. Siedenburg", "M. D\u00f6rfler"], "venue": "Proc. of the 14th Int\u2019l Conf. on Digital Audio Effects (DAFx-11),, pages 23\u201326,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Pathwaysdriven sparse regression identifies pathways and genes associated with high-density lipoprotein cholesterol in two Asian cohorts", "author": ["M. Silver", "P. Chen", "R. Li", "C.-Y. Cheng", "T.-Y. Wong", "E.-S. Tai", "Y.-Y. Teo", "G. Montana"], "venue": "PLoS Genetics, 9(11):e1003939,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparsity and smoothness via the fused Lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "Journal of the Royal Statistical Society: Series B, 67(1):91\u2013108,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2005}, {"title": "The solution path of the generalized lasso", "author": ["R. Tibshirani", "J. Taylor"], "venue": "Ann. Stat., 39(3):1335\u20131371,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Wavelet shrinkage using adaptive structured sparsity constraints", "author": ["D. Tomassia", "D. Miloned", "J.D.B. Nelson"], "venue": "Signal Processing, 106:73\u201387,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning, 1(1\u20132):1\u2013305,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient generalized fused Lasso with application to the diagnosis of Alzheimer\u2019s disease", "author": ["B. Xin", "Y. Kawahara", "Y. Wang", "W. Gao"], "venue": "Proc. of AAAI\u203214, pages 2163\u20132169,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Linguistic structured sparsity in text categorization", "author": ["D. Yogatama", "N.A. Smith"], "venue": "Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL\u201914), pages 786\u2013796,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B, 68(1):49\u201367,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2006}, {"title": "The composite absolute penalties family for grouped and hierarchical variable selection", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Annals of Statistics, 37(6A):3468\u20133497,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 16, "context": "[17] and its variants, which runs, in the worst-case, at the cost of only a constant factor of a single computation of the corresponding maxflow optimization, can be adapted to solve the proximal problems for those penalties.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 80, "endOffset": 88}, {"referenceID": 24, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 80, "endOffset": 88}, {"referenceID": 48, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 119, "endOffset": 127}, {"referenceID": 49, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 119, "endOffset": 127}, {"referenceID": 33, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 319, "endOffset": 339}, {"referenceID": 30, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 319, "endOffset": 339}, {"referenceID": 24, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 319, "endOffset": 339}, {"referenceID": 27, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 319, "endOffset": 339}, {"referenceID": 47, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 319, "endOffset": 339}, {"referenceID": 52, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 357, "endOffset": 369}, {"referenceID": 34, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 357, "endOffset": 369}, {"referenceID": 45, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 357, "endOffset": 369}, {"referenceID": 11, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 399, "endOffset": 407}, {"referenceID": 53, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 399, "endOffset": 407}, {"referenceID": 46, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 430, "endOffset": 438}, {"referenceID": 50, "context": "Regularization with structured sparsity-inducing penalties, such as group Lasso [56, 25] and (generalized) fused Lasso [50, 51], has been shown to achieve high predictive performance and solutions that are easier to interpret, and has been successfully applied to a broad range of applications, including bioinfomatics [34, 31, 25, 28, 48], computer vision [54, 35, 46], natural language processing [12, 55] and signal processing [47, 52].", "startOffset": 430, "endOffset": 438}, {"referenceID": 2, "context": "Recently, it has been revealed that many of the existing structured sparsity-inducing penalties can be interpreted as convex relaxations of submodular functions [3, 41].", "startOffset": 161, "endOffset": 168}, {"referenceID": 40, "context": "Recently, it has been revealed that many of the existing structured sparsity-inducing penalties can be interpreted as convex relaxations of submodular functions [3, 41].", "startOffset": 161, "endOffset": 168}, {"referenceID": 9, "context": "For example, a class of the total variation, which is equivalent to generalized fused Lasso (GFL), is known to be solved via parametric maxflows [10, 19].", "startOffset": 145, "endOffset": 153}, {"referenceID": 18, "context": "For example, a class of the total variation, which is equivalent to generalized fused Lasso (GFL), is known to be solved via parametric maxflows [10, 19].", "startOffset": 145, "endOffset": 153}, {"referenceID": 35, "context": "(2011) [36] and Mairal & Yu (2013) [37] proposed parametric maxflow algorithms for l1/l\u221e-regularization and the path-coding, respectively.", "startOffset": 7, "endOffset": 11}, {"referenceID": 36, "context": "(2011) [36] and Mairal & Yu (2013) [37] proposed parametric maxflow algorithms for l1/l\u221e-regularization and the path-coding, respectively.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "[17] and its variants (hereafter, we call those the GGT-type algorithms) is applicable to calculate the proximal problems for penalties obtained via convex", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Note that the first one is closely related to the class of energy minimization problems that can be solved with the so-called graph-cut algorithm, which has been discussed actively in computer vision [30, 29].", "startOffset": 200, "endOffset": 208}, {"referenceID": 28, "context": "Note that the first one is closely related to the class of energy minimization problems that can be solved with the so-called graph-cut algorithm, which has been discussed actively in computer vision [30, 29].", "startOffset": 200, "endOffset": 208}, {"referenceID": 7, "context": "Similar discussions are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 38, 16].", "startOffset": 133, "endOffset": 144}, {"referenceID": 37, "context": "Similar discussions are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 38, 16].", "startOffset": 133, "endOffset": 144}, {"referenceID": 15, "context": "Similar discussions are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 38, 16].", "startOffset": 133, "endOffset": 144}, {"referenceID": 9, "context": "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.", "startOffset": 283, "endOffset": 310}, {"referenceID": 18, "context": "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.", "startOffset": 283, "endOffset": 310}, {"referenceID": 35, "context": "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.", "startOffset": 283, "endOffset": 310}, {"referenceID": 40, "context": "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.", "startOffset": 283, "endOffset": 310}, {"referenceID": 36, "context": "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.", "startOffset": 283, "endOffset": 310}, {"referenceID": 4, "context": "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.", "startOffset": 283, "endOffset": 310}, {"referenceID": 52, "context": "And as for the second one, our proposed formulation gives an unified view of the class of structured regularization that can be solved as a parametric maxflow problem, which generalizes, extends or connects several existing works that have been separately discussed to date, such as [10, 19, 36, 41, 37, 5, 54], without increasing the essential theoretical run-time bound.", "startOffset": 283, "endOffset": 310}, {"referenceID": 10, "context": "A set function F : 2 \u2192 R is called submodular if F (A) + F (B) \u2265 F (A \u2229B) + F (A \u222aB) for any A,B \u2286 V [11, 14].", "startOffset": 101, "endOffset": 109}, {"referenceID": 13, "context": "A set function F : 2 \u2192 R is called submodular if F (A) + F (B) \u2265 F (A \u2229B) + F (A \u222aB) for any A,B \u2286 V [11, 14].", "startOffset": 101, "endOffset": 109}, {"referenceID": 32, "context": ", wj1 \u2265 wj2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 wjd [33].", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "by the M\u00f6bius inversion formula (see, for example, [1]).", "startOffset": 51, "endOffset": 54}, {"referenceID": 12, "context": "The max-flow min-cut theorem of [13] states that these two quantities are equal.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "The first type of the penalty from a submodular function is defined through convex relaxation with the `p-norm [3, 41, 5].", "startOffset": 111, "endOffset": 121}, {"referenceID": 40, "context": "The first type of the penalty from a submodular function is defined through convex relaxation with the `p-norm [3, 41, 5].", "startOffset": 111, "endOffset": 121}, {"referenceID": 4, "context": "The first type of the penalty from a submodular function is defined through convex relaxation with the `p-norm [3, 41, 5].", "startOffset": 111, "endOffset": 121}, {"referenceID": 40, "context": "Proposition 1 ([41]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 40, "context": "And, if we use F (A) = \u2211 g\u2208G min{|A\u2229g|, 1} for a group of variables G, then \u03a9\u0303f,p is equivalent to the (possibly, overlapping) `1/`\u221e and non-overlapping `1/`p group regularizations or provides group sparsity similar to the overlapping `1/`p group regularization [41, 5].", "startOffset": 262, "endOffset": 269}, {"referenceID": 4, "context": "And, if we use F (A) = \u2211 g\u2208G min{|A\u2229g|, 1} for a group of variables G, then \u03a9\u0303f,p is equivalent to the (possibly, overlapping) `1/`\u221e and non-overlapping `1/`p group regularizations or provides group sparsity similar to the overlapping `1/`p group regularization [41, 5].", "startOffset": 262, "endOffset": 269}, {"referenceID": 3, "context": "This is known to make some of the components of w equal when used as a regularizer [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": ", F (A) = \u2211 i\u2208A,j\u2208V \\A aij [4, 54].", "startOffset": 27, "endOffset": 34}, {"referenceID": 52, "context": ", F (A) = \u2211 i\u2208A,j\u2208V \\A aij [4, 54].", "startOffset": 27, "endOffset": 34}, {"referenceID": 21, "context": "This can be extended to a hypergraph H = (V,E) with non-negative weight ae for each hyperedge e \u2208 E, where the Lov\u00e1sz extension of a hypergraph cut function F (A) = \u2211 e\u2208E:e\u2229A6=\u2205,e\u2229A 6=\u2205 ae gives the hypergraph regularization \u03a9hr(w) = \u2211 e\u2208E ae(maxi\u2208e wi \u2212 mini\u2208e wj) p [22].", "startOffset": 268, "endOffset": 272}, {"referenceID": 32, "context": "From the definition, the Lov\u00e1sz extension of a submodular function with F (\u2205) = 0 can be represented as a greedy solution over the submodular polyhedron [33], i.", "startOffset": 153, "endOffset": 157}, {"referenceID": 6, "context": "Since the objective of this problem is the sum of smooth and non-smooth convex functions, a major option for its optimization is the proximal gradient method, such as FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) [7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 40, "context": "Based on the above formulation, Obozinski & Bach (2012) [41] recently suggested a divide-and-conquer algorithm as an adaptation of the decomposition algorithm by Groenevelt (1991) [21] for penalties from general submodular functions (for the case of p = 2).", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "Based on the above formulation, Obozinski & Bach (2012) [41] recently suggested a divide-and-conquer algorithm as an adaptation of the decomposition algorithm by Groenevelt (1991) [21] for penalties from general submodular functions (for the case of p = 2).", "startOffset": 180, "endOffset": 184}, {"referenceID": 4, "context": "A more general version of this approach was also developed by Bach (2013) [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 41, "context": "1 Graph-Representable Set Functions The currently-known best complexity of minimizing a general submodular function is O(d + d EO), where EO is the cost of evaluating a function value [42].", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "Although there exist practically faster algorithms, such as the minimum-norm-point algorithm [15] as well as faster algorithms for special cases (e.", "startOffset": 93, "endOffset": 97}, {"referenceID": 43, "context": ", Queyranne\u2019s algorithm for symmetric submodular functions [44]), their scalability would not be practically sufficient, especially if we must solve submodular minimization several times, which is the current case.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "it is well known that a cut function (which is almost equivalent to a second order submodular function [16]) can be minimized much faster through calculation of maxflows over the corresponding network.", "startOffset": 103, "endOffset": 107}, {"referenceID": 42, "context": "If N consists of d nodes and m edges, the currently best runtime bound for the minimization is O(md) [43].", "startOffset": 101, "endOffset": 105}, {"referenceID": 17, "context": ", [18, 9].", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": ", [18, 9].", "startOffset": 2, "endOffset": 9}, {"referenceID": 25, "context": "Such a function is sometimes referred to as graph-representable [26], and defined as follows.", "startOffset": 64, "endOffset": 68}, {"referenceID": 37, "context": "The submodularity of this function is derived from the classical result of Megiddo (1974) [38] on network flow problems with multiple terminals (see, for the proof, [40]).", "startOffset": 90, "endOffset": 94}, {"referenceID": 39, "context": "The submodularity of this function is derived from the classical result of Megiddo (1974) [38] on network flow problems with multiple terminals (see, for the proof, [40]).", "startOffset": 165, "endOffset": 169}, {"referenceID": 8, "context": "1This class of functions is closely related to the class of energy minimization problems that can be solved by the so-called graph-cut algorithm [9, 30].", "startOffset": 145, "endOffset": 152}, {"referenceID": 29, "context": "1This class of functions is closely related to the class of energy minimization problems that can be solved by the so-called graph-cut algorithm [9, 30].", "startOffset": 145, "endOffset": 152}, {"referenceID": 7, "context": "Related results are also found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].", "startOffset": 134, "endOffset": 141}, {"referenceID": 15, "context": "Related results are also found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].", "startOffset": 134, "endOffset": 141}, {"referenceID": 25, "context": "The first one is mentioned as \u201ctruncations,\u201d where a function F is graph-representable by just one additional node (see Figure 1(a) and also refer [26] or [40]).", "startOffset": 147, "endOffset": 151}, {"referenceID": 39, "context": "The first one is mentioned as \u201ctruncations,\u201d where a function F is graph-representable by just one additional node (see Figure 1(a) and also refer [26] or [40]).", "startOffset": 155, "endOffset": 159}, {"referenceID": 7, "context": "The second one is closely related to [8], and the third one is derived from [16], for which we describe concrete procedures to construct networks (see Figure 1 for the construction).", "startOffset": 37, "endOffset": 40}, {"referenceID": 15, "context": "The second one is closely related to [8], and the third one is derived from [16], for which we describe concrete procedures to construct networks (see Figure 1 for the construction).", "startOffset": 76, "endOffset": 80}, {"referenceID": 38, "context": "This is obtained, in Lemma 4 of [39], by replacing the assumption on the strict convexity of \u03c8\u2032 i with the monotonisity of the function in the region under consideration.", "startOffset": 32, "endOffset": 36}, {"referenceID": 38, "context": "As discussed in [39], this lemma implies that problem (6) can be reduced to the following parametric problem:", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "However, this is generally not true for non-linear capacities because we must solve nonlinear equations to identify such a parameter value [20].", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "Algorithm 1 shows an adaptation of the simplified version [2] of the original GGT algorithm.", "startOffset": 58, "endOffset": 61}, {"referenceID": 48, "context": "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].", "startOffset": 52, "endOffset": 60}, {"referenceID": 49, "context": "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].", "startOffset": 52, "endOffset": 60}, {"referenceID": 54, "context": "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].", "startOffset": 77, "endOffset": 88}, {"referenceID": 23, "context": "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].", "startOffset": 77, "endOffset": 88}, {"referenceID": 5, "context": "Typical instances include (generalized) fused Lasso [50, 51] and group Lasso [56, 24, 6].", "startOffset": 77, "endOffset": 88}, {"referenceID": 44, "context": "Generalized fused Lasso is closely related to the so-called total variation, which has often been discussed in computer vision [45].", "startOffset": 127, "endOffset": 131}, {"referenceID": 55, "context": "Recently, group penalties have been applied to more complex groups, such as hierarchical penalty [57, 27] and path penalty [37].", "startOffset": 97, "endOffset": 105}, {"referenceID": 26, "context": "Recently, group penalties have been applied to more complex groups, such as hierarchical penalty [57, 27] and path penalty [37].", "startOffset": 97, "endOffset": 105}, {"referenceID": 36, "context": "Recently, group penalties have been applied to more complex groups, such as hierarchical penalty [57, 27] and path penalty [37].", "startOffset": 123, "endOffset": 127}, {"referenceID": 9, "context": "The total variation regularization is known to be solvable with an efficient parametric maxflow algorithm [10, 19].", "startOffset": 106, "endOffset": 114}, {"referenceID": 18, "context": "The total variation regularization is known to be solvable with an efficient parametric maxflow algorithm [10, 19].", "startOffset": 106, "endOffset": 114}, {"referenceID": 35, "context": "In addition, the proximal problem for l1/l\u221e-group penalty is calculated via parametric maxflow optimization [36].", "startOffset": 108, "endOffset": 112}, {"referenceID": 2, "context": "Bach (2010) [3] and Bach & Obozinski (2012) [41] revealed that many of the existing structured penalties are obtained as convex relaxations of submodular functions, and those proximal problems are formulated as separable convex minimization.", "startOffset": 12, "endOffset": 15}, {"referenceID": 40, "context": "Bach (2010) [3] and Bach & Obozinski (2012) [41] revealed that many of the existing structured penalties are obtained as convex relaxations of submodular functions, and those proximal problems are formulated as separable convex minimization.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "The sufficient condition in Section 4 is closely related to the class of energy minimization problems solvable by graph-cut algorithm [9, 30].", "startOffset": 134, "endOffset": 141}, {"referenceID": 29, "context": "The sufficient condition in Section 4 is closely related to the class of energy minimization problems solvable by graph-cut algorithm [9, 30].", "startOffset": 134, "endOffset": 141}, {"referenceID": 51, "context": "Energy minimization is a formulation of the maximum a posteriori (MAP) estimation on MRFs (see, for example, [53]).", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "Similar results are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].", "startOffset": 129, "endOffset": 136}, {"referenceID": 15, "context": "Similar results are found in the context of realization of a submodular function as a cut function in combinatorial optimization [8, 16].", "startOffset": 129, "endOffset": 136}, {"referenceID": 17, "context": "Algorithm 1 is a divide-and-conquer implementation of the preflow algorithm proposed by Gallo & Tarjan (1988) [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 2, "context": "Bach (2010) [3] and Bach & Obozinski (2012) [41] have mentioned an application of a divide-and-conquer approach to separable convex minimization proposed by Groenevelt (1991) [21] for proximal problem (5), which takes O(d) times of the cost for submodular minimization.", "startOffset": 12, "endOffset": 15}, {"referenceID": 40, "context": "Bach (2010) [3] and Bach & Obozinski (2012) [41] have mentioned an application of a divide-and-conquer approach to separable convex minimization proposed by Groenevelt (1991) [21] for proximal problem (5), which takes O(d) times of the cost for submodular minimization.", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "Bach (2010) [3] and Bach & Obozinski (2012) [41] have mentioned an application of a divide-and-conquer approach to separable convex minimization proposed by Groenevelt (1991) [21] for proximal problem (5), which takes O(d) times of the cost for submodular minimization.", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "(1989) [17]\u2019s algorithm to the current problem.", "startOffset": 7, "endOffset": 11}, {"referenceID": 40, "context": "We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (\u2018DA-MNP\u2019/\u2019DA-MF\u2019) and the algorithm by [36] (\u2018MJOB\u2019) for the penalties from F (A) = \u2211 g min{|A \u2229 g|, 1} (MJOB is applicable only for p=\u221e), and the MNP algorithm (\u2018MNP\u2019), the algorithm by Tibshirani & Taylor (2011) [51] (\u2018TT\u2019) and the one by Liu et al.", "startOffset": 89, "endOffset": 96}, {"referenceID": 4, "context": "We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (\u2018DA-MNP\u2019/\u2019DA-MF\u2019) and the algorithm by [36] (\u2018MJOB\u2019) for the penalties from F (A) = \u2211 g min{|A \u2229 g|, 1} (MJOB is applicable only for p=\u221e), and the MNP algorithm (\u2018MNP\u2019), the algorithm by Tibshirani & Taylor (2011) [51] (\u2018TT\u2019) and the one by Liu et al.", "startOffset": 89, "endOffset": 96}, {"referenceID": 35, "context": "We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (\u2018DA-MNP\u2019/\u2019DA-MF\u2019) and the algorithm by [36] (\u2018MJOB\u2019) for the penalties from F (A) = \u2211 g min{|A \u2229 g|, 1} (MJOB is applicable only for p=\u221e), and the MNP algorithm (\u2018MNP\u2019), the algorithm by Tibshirani & Taylor (2011) [51] (\u2018TT\u2019) and the one by Liu et al.", "startOffset": 205, "endOffset": 209}, {"referenceID": 49, "context": "We compared ours with the following algorithms; the decomposition algorithm described in [41, 5] with the minimum-norm-point (MNP) algorithm / the maxflow algorithm (\u2018DA-MNP\u2019/\u2019DA-MF\u2019) and the algorithm by [36] (\u2018MJOB\u2019) for the penalties from F (A) = \u2211 g min{|A \u2229 g|, 1} (MJOB is applicable only for p=\u221e), and the MNP algorithm (\u2018MNP\u2019), the algorithm by Tibshirani & Taylor (2011) [51] (\u2018TT\u2019) and the one by Liu et al.", "startOffset": 380, "endOffset": 384}, {"referenceID": 31, "context": "(2010) [32] (\u2018LYY\u2019) for the (generalized) fused penalty (LYY is applicable only to the 1d fused case).", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "(1989) [17] and its variants, which runs at the cost of a constant 2We used the code modified from the one available at http://spams-devel.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "Several avenues would be worth investigating: First, our formulation does not include the type of sparsity by the latent group penalties, such as [25].", "startOffset": 146, "endOffset": 150}, {"referenceID": 36, "context": "As mentioned in [37], the proximal problem for the penalties of Jacob et al.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "(2009) [25] and its generalization can be solved as a minimum-cost flow problem, which is known to be calculated as a parametric maxflow problem if the costs are quadratic and only on edges connected to source/sink [23].", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "(2009) [25] and its generalization can be solved as a minimum-cost flow problem, which is known to be calculated as a parametric maxflow problem if the costs are quadratic and only on edges connected to source/sink [23].", "startOffset": 215, "endOffset": 219}, {"referenceID": 17, "context": "The preflow algorithm computes a maximum flow in a directed network N [18].", "startOffset": 70, "endOffset": 74}, {"referenceID": 39, "context": "The first is shown in Lemma 2 and 3 in [40] or Proposition 2.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "5 in [5].", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "8 in [5], we obtain a solution to problem (6) as", "startOffset": 5, "endOffset": 8}, {"referenceID": 16, "context": "The runtime follows the analysis in [17] from Lemma 7.", "startOffset": 36, "endOffset": 40}], "year": 2015, "abstractText": "The proximal problem for structured penalties obtained via convex relaxations of submodular functions is known to be equivalent to minimizing separable convex functions over the corresponding submodular polyhedra. In this paper, we reveal a comprehensive class of structured penalties for which penalties this problem can be solved via an efficiently solvable class of parametric maxflow optimization. We then show that the parametric maxflow algorithm proposed by Gallo et al. [17] and its variants, which runs, in the worst-case, at the cost of only a constant factor of a single computation of the corresponding maxflow optimization, can be adapted to solve the proximal problems for those penalties. Several existing structured penalties satisfy these conditions; thus, regularized learning with these penalties is solvable quickly using the parametric maxflow algorithm. We also investigate the empirical runtime performance of the proposed framework.", "creator": "LaTeX with hyperref package"}}}