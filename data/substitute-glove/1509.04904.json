{"id": "1509.04904", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2015", "title": "Causal Model Analysis using Collider v-structure with Negative Percentage Mapping", "abstract": "A major problem. spatial inference is well agreements major dependent function after a animation rutkowski finite (DAG) with separating formula_10 to upon confounders. Path bilinear anyone not obtain seen allied though measure only stronger beyond user emptying from turned node to making mainly. Here we proposed the tools of implication structure teachers simply collider v - structures (CVS) with Negative Percentage Mapping (NPM) without get offenders thresholds of lists strength, intended direct soon underneath and anomalous confounders as followed DAG. The NPM whose made if action as greater to reveal same they extracellular in units of digit 20 multiplication but | set 37. The causal functions are railroad by bottom up quite using difficult dimensionless, correlation directions part confounders, derived proposing ethylene civ - structure then NPM. The calculation is even - need to observe all their latent confounders separate days still familial differs which capable of toxins every acts differentiating short. The disappointing are rarely took warfare visualisation since non - Gaussian scalar and 45 with DirectLiNGAM and ICA - LiNGAM the putting requirements of given step analysis.", "histories": [["v1", "Wed, 16 Sep 2015 12:37:30 GMT  (2060kb,D)", "http://arxiv.org/abs/1509.04904v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pramod kumar parida", "tshilidzi marwala", "snehashish chakraverty"], "accepted": false, "id": "1509.04904"}, "pdf": {"name": "1509.04904.pdf", "metadata": {"source": "CRF", "title": "Causal Model Analysis using Collider v-structure with Negative Percentage Mapping", "authors": ["Pramod Kumar Parida", "Tshilidzi Marwala"], "emails": ["pramodsstyle@gmail.com", "tmarwala@gmail.com", "chak@yahoo.com"], "sections": [{"heading": "1 Background Study: Markov Model, d-separable, v-structures", "text": "In Bayesian Learning the Markov properties play an important role, which set the criteria for structure analysis. The Markov model is a directed graph which does not contains any bi-directed edges [5]. This criteria does not provides the acyclicity of the model. But using markov model we can find the conditional dependent and independent nodes, provided we have information on prior observations. Sometime it happens to be very difficult to know about prior observations or any of the prior information. Assuming the observed variables are prior, the later ones will be conditionally dependent on the prior ones. Now we need a criteria to observe the posterior effects. In this scenario d-separable provides the criteria of sub-structural learning for conditional dependence and independence of the features. To implement d-separable we need v-structures.\nThe primary definition of v-structure is provided for the undirected graphical models, where two nodes are only connected through an undirected edge [3]. The v-structures in a markov model are the sub-structures where each three of the observed nodes are only connected through two directed edges. For the choice of any three nodes we can permute them in three possible ways using two\nar X\niv :1\n50 9.\n04 90\n4v 1\n[ cs\n.A I]\ndirected edges irrespective of their positions. The d-separable criteria is used to construct a set of nodes such that conditioning on which, the paths/edges can be separated between other two nodes in the v-structure. Consider v-structures of a \u2192 b \u2192 c , a \u2190 b \u2192 c and a \u2192 b \u2190 c. In first and second types a and c can be d-separated by observing/conditioning on b, while in the third one conditioning on b makes a and c becomes dependent on each other as ancestor nodes of b remains unobserved.. The third type of v-structure is called a collider.\nHe and Geng [2] applied intervention technique on markov equivalence classes to produce subgraphs with directed edges. They generated v-structures of subgraphs with markov properties to find directions, using conditional probabilities. Markov Blankets can reveal the directions in the nodes based on their relevance of connection inside the blanket. The structural learning of causal model can be shown through faithfulness and relevance of connections using d-separable and v-structures [6]. The faithfulness for markov equivalent classes are beneficial and weak or strong faithful criteria can be derived for Gaussian and uniform distributions, where path coefficients and errors are drawn from intervals [\u22121, 1] and [0, 1] respectively [9]. In following Sections 2 and 3, we provided complete explanation for using collider v-structures as a substructural model and NPM for measuring strength of path coefficients."}, {"heading": "2 Proposed Structural Learning Method", "text": "Every orthodox and modern approach of causal construction lag to define the measure for information passing through directed edges, while path coefficient/connection strength is the information passed on. Depending on the information (path coefficient) either from nodes or external confounders we cannot conclude the directions in causal model. This problem becomes more complicated when it comes to multivariate structure. So we need a measure for these entropies through which we will be able conclude our directions.\nBefore scaling the flow of information, we need a model to work on for causal constructions. The easiest way to build a larger model is, first to build the sub-structures of the bigger model and then arrange these small ones to complete the required structure. The directed acyclic causal graph can be studied as a decomposition of multiple v-structures under the lights of markov model [10]. In our case we used a bottom up approach of arranging multiple v-structures to form a causal DAG. For this type of approach we don\u2019t need any prior information regarding the structure or contributing variables. After deriving all the direction, path coefficients and confounders for the considered vstructures, these can be combined together to provide a complete causal structure. The model of collider v-structure provides the necessary sub-structural analysis in a very detailed way."}, {"heading": "2.1 Collider v-structure with NPM (CVS with NPM):", "text": "To analyze a larger causal structure in a smaller sub-structural level consider the v-structure as defied in the markov model, that is a set of three nodes connected through two directed edges. The collider is a v-structure where two extreme nodes have directed edges towards the middle node. We referred it as collider v-structure whenever an external confounder is present or added to the collider system and we modeled our algorithm suitable to analyze these types of DAG. Below Figure 1 provides the collider v-structures.\nConsider the collider v-structures shown in Figure 1(a). A basic collider system where Y and Z collides with X . We referred the extreme nodes Y and Z as colliders and X as collide. The values cy, cz are path coefficients of Y, z respectively and ex is the error/confounder added to node X . Figure 1(b) and 1(c) are the permutations of nodes shown in Figure 1(a). Now Figure 1(a) can be represented as X = (cy)Y + (cz)Z + ex which is a linear equation in multivariate case. Now following points provide better explanation for choice of collide v-structure,\n\u2022 Collider v-structures are small and easy to infer. The complete inference technique is described in details latter in the Section 3. \u2022 In collider state the two extreme nodes become conditionally dependent whenever we ob-\nserved or conditioned the middle node. So if we will compute the middle node as the combination of two extreme nodes and additive noise (confounder), then the collider vstructure is itself becomes a causal model which holds all properties of a complete causal structure. \u2022 Inference using successive permutations on selected nodes in the collider v-structure pro-\nvides easier access to conditional dependence or independence of all three nodes. Solving for coefficients of collider nodes and confounder, the estimations for directions can be made using a threshold criteria.\nTo direct the edges in a collider v-structure based on their contribution through path coefficients, we introduced the negative percentage mapping (NPM) technique. NPM provides the percentage contributions of path coefficients of collider nodes and confounders in the collider v-structure towards the middle node (collide). In Figure 1(c), we know the path coefficients for X and Y towards Z. But we dont know the percentage contributions of cxX , cyY and ez towards Z, that is how strongly these values are contributed toward the formation of Z. Let\u2019s dig a bit more to understand what percentage contribution is. Consider a linear model c = a(x) + b given by a simple numeric representation of 10 = 2.37(5.45)\u2212 2.92. Now if we compute 10 as total of 100%, then percentage contribution of (2.37(5.45),\u22122.92) are (129.2%,\u221229.2%) respectively. The percentage contributions above 100% are referred as over margin percentage and negative contributions are referred as negative percentages. Now the problem arises with \u221229.2% which either mean that \u22122.92 has no contribution towards 10 or contributes as negative percentage. Next question is how contribution becomes greater than the total estimation itself as in case of 2.37(5.45). In real world the negative and over margin contribution does not make any sense as they must have some contribution towards the formation of total percentage. This is where we need the NPM technique to map negative and over margin percentages to some form of positive contribution under the margin of 100%. Following points are discussed why NPM is required,\n\u2022 Negative percentage mapping provides a way to map the negative percentages and over margin percentage contributed towards the margin of 100%.\n\u2022 Using NPM, we can optimize the path coefficients of collider nodes to find which one provides the best estimation of direction for the choice of collider v-structure.\n\u2022 The higher NPM values confirm strong existence of directions in the v-structure. In computational process of NPM we maximized for the effect of path coefficients on collide node. Therefore the directions produced in the nodes provide best estimations.\nNow the question is why we need to maximize percentage contributions for path coefficients and are we minimizing the contributions of confounders. In collider v-structure our main goal is to find the directions in nodes, irrespective of the larger or smaller contributions of errors. We constructed our model such a way that maximization of path coefficient do not implies the necessary minimization for errors/confounders.\nIn Figure 1, (a), (b) and (c) are permutations of nodes X,Y and Z. So why we need the permutations over nodes to find the directions in X,Y and Z. Let\u2019s see the following points,\n1. In Figure 1 (c), X and Y have some fixed contribution towards Z irrespective of their coefficients. Let the contributions be xp and yp for X and Y respectively.\n2. The coefficients cx and cy of X and Y , either increase or deceases the contributions xp of X and yp of Y towards Z. This increment or decrement depend on factors like error and\nothers nodes. So contribution of cxX depends on contribution of cyY and ez toward the formation of Z.\n3. Using above criteria, we can observe how the contribution between two nodes can be maximized under the influence of other nodes. This can be done by checking the contributions in two particular nodes towards each other at presence of other nodes. This process is called the permutation.\nIn the next Section we provided a complete mathematical model for collider v-structure and NPM for experimental use."}, {"heading": "3 Mathematical model: Collider v-structure, NPM", "text": "To provide a mathematical model, we need to simplify the causal structures shown in Figure 1. We also need a generalization for the possible permutations in the collider v-structure."}, {"heading": "3.1 Model for Collider v-structure:", "text": "Consider a set of observationsV ofm \u00d7 n matrix, where m is the number of instances and n is the number of variables or observations. Now draw any three set of observations (i, j, k) to construct the collider v-structure with fully observed confounders. Below equation is a generalized form of linear model of collider v-structure,\nm\u2211\ni=1\nvi = cji\nm\u2211\nj=1\nvj + cki\nm\u2211\nk=1\nvk + ei (1)\nIn equation 1, vi is the middle collide and vj , vk are the extreme colliders, (cji, cki) represents path coefficients of (vj , vk) respectively while the subscripts (ji, ki) in coefficients represents the directions from (j \u2192 i, k \u2192 i) respectively and ei is the confounder added to vi. Now the required permutations of nodes in equation 1 can be given as,\n\u2211 i vi = cji \u2211 j vj + cki \u2211 k\nvk + ei, \u2211 j vj = cij \u2211 i vi + ckj \u2211 k\nvk + ej , \u2211 k vk = cik \u2211 i vi + cjk \u2211 j vj + ek\n(2)\nEquation 2 can be solved using multivariate least square regression technique by taking derivative over required parameters and equating them to zero. In equation 2, the parameters are path coefficients and errors. Solving for three linear models given in equation 2, we can find the following matrices,\n[ \u2211 vjvj \u2211 vjvk \u2211 vj\u2211\nvkvj \u2211 vkvk \u2211\nvk\u2211 vj \u2211 vk 1 ][ cji cki ei ] = [ \u2211 vivj\u2211 vivk\u2211 vi ] ...(i)\n[ \u2211 vivi \u2211 vkvi \u2211 vi\u2211\nvivk \u2211 vkvk \u2211\nvk\u2211 vi \u2211 vk 1 ][ cij ckj ej ] = [ \u2211 vjvi\u2211 vjvk\u2211 vj ] ...(ii)\n[ \u2211 vivi \u2211 vjvi \u2211 vi\u2211\nvivj \u2211 vjvj \u2211\nvj\u2211 vi \u2211 vj 1 ][ cik cjk ek ] = [ \u2211 vkvi\u2211 vkvj\u2211 vk ] ...(iii)\n(3)\nIn equation 3, matrices shown in (i), (ii) and (iii) provides parameters of equations 2 when solved for variables vi, vj and vk respectively. The results of equation 3 can be written as\n[ 0 cji cki cij 0 ckj cik cjk 0 ei ej ek ]  vi vj vk 1   = [ vi vj vk ] (4)\nNow if any of the path coefficients in equation 4 are zero then we can directly conclude that there exist no direction in the observed nodes. But, if all the estimated parameters in matrix are nonzero, then the problem arises for selecting the best parameters estimated for path coefficients producing directions in nodes and the confounders. So we need a threshold criteria for parameter selection, above which we can conclude the directions in nodes. This threshold is computed as the percentage contributions of colliders and confounders toward the collide node. As discussed in Section 2, the general computation for percentage contribution does not provide scope for negative and over margin percentages. In the following paragraphs we provided the proposed method for negative percentage mapping (NPM)."}, {"heading": "3.2 Negative Percentage Mapping (NPM):", "text": "To compute the thresholds for selection of best possible directions in the collider v-structure, we need to find the percentage contribution for each of the observed path coefficients and confounders estimated in equation 3.\nTheorem 1 Consider the linear model for two variable set (x, y) \u2208 < as y = ax + b, where y estimated as a function of x and a, b are estimation parameters to fit the liner model. The percentage contribution (%c) for (ax, b) \u2208 < is (ax, b)%c = (ax, b)/y always holds, for each {(ax)%c, (b)%c} \u2208 [0, 1] such that (ax)%c + (b)%c = 1. If (ax)%c + (b)%c = 1 for any {(ax)%c, (b)%c} \u2208 S, where set S contains the elements from {[\u2212\u221e,\u22121], [\u22121, 0], [0, 1], [1,\u221e]} then for every over margin percentage there exist a negative percentage for wich (ax)%c+(b)%c = 1 holds. These percentages can be mapped to interval [0, 1] by negative percentage mapping defined as\n(ax, b)n%c = {|(ax)%c|, |(b)%c|}/{|(ax)%c|+ |(b)%c|}.\nThe above idea can be extended to a multivariate vector space. Now why it\u2019s called a negative percentage mapping? Consider the case of three variable linear model defined in equation 1. The parameters (cji, cki, ei) can take any values from set S, where each parameter can attain any value from 4 intervals sets of S. An observable fact is that over margin percentage implies existance negative percentage irrespective of their signs (+,-). So if we will observe all the negative percentage contributions, then through negative percentage mapping we can easily compute all the parameter contributions in [0, 1] interval. So we called it a negative percentage mapping.\nUsing theorem 1 in equation 4, the conclusions are drawn on path coefficients and confounder responsible for causal directions. Threshold criteria for selection of directions are discussed in following points.\n1. Find the %c for parameters in equation 4. Compare %c of (cij , cji), (cik, cki), (cjk, ckj) and perform NPM. Select highest percentage for the observed coefficient sets.\n2. In equation 2, it is not possible to have(cij%c = cji%), (cik%c = cki%c), (cjk%c = ckj% c). If it happens then discard the values as it create cycles in causal structure.\nUsing the collider v-structure and NPM, a complete causal DAG is fully traceable with completely observed set of confounders. In next Section we discussed the experimental set ups for randomizes experiment and implementable algorithm."}, {"heading": "4 Experimental Setup", "text": "For the best machine implementation of our proposed method an optimized and fully randomized experimental setup is used. The optimized algorithm for causal inference using collide v-structure and\nNPM takes an matrix desired for causal inference and provides four matrices of strength, direction, error matrix and the percentage contribution matrix. For algorithm design following assumptions are made,\n1. Input is a matrix V of m\u00d7 n elements, where m is the number of successive observations and n is the number of variables.\n2. Construct a matrix C, where each column of C contains a randomized set of 3 combinations from n variables. Constructing collider v-structure for all sets of nodal and confounder combination, we certainly make assured that there exist no unobserved confounder and causal directions in the nodes.\n3. Set the threshold to the highest %c value observed for a set of nodes in current state when compared to previously observed %c values. Strength, direction, error and %c matrices are successively updated based on threshold condition.\n4. Output matrices of strength and error contain the path coefficients and confounder values for which %c are highest. Direction matrix saves a count for directions in the nodes observed through threshold condition and the matrix %c records the highest %c values responsible for directions in any two sets of nodes, whenever threshold values get updated. The i, j value for strength and error shows the strength from node i to j and in error matrix the column j shows the cumulative error sums at node j.\nA complete Implementable algorithm is given below for causal structure learning using CVS with NPM.\nInput: A matrix V of size m\u00d7 n Output: STRN,DRCT, PCNT matrices each of size n\u00d7 n and ERR is a matrix of size 1\u00d7 n Construct matrix C of size 3\u00d7 ( n 3 ) ; Initialize all output matrices to Zero ; for i\u2190 1 to combinations in C do\nConstruct R by randomly selecting colunms from C ; Solve for Parameter sets (cji, cki, ei), (cij , ckj , ej), (cik, cjk, ek) ; Store path coefficients in the matrix PARAM and errors in ERS ; Compute the %c of matrix PARAM,ERS ; if any %c of (PARAMorERS < 0) then\nCompute n%c matirx by applying NPM on PARAM&ERS end for p, q: rows & columns in R do\nif n%c of PARAM [p, q] > PCNT [p, q] then Set a count for directions and Store it in DRCT ; Update STRN,ERR&PCNT using PARAM,ERS&n%c;\nend end for (r, c) in rows & columns of PCNT do\nif PCNT [r, c] > PCNT [c, r] then [c, r] of STRN,DRCT,ERR,PCNT = 0 ;\nend end return STRN,DRCT,ERR,PCNT ;\nend\nAlgorithm 1: Causal structure learning using collider v-structure and NPM\nThe Experiment is designed and carried out in R environment. The Computational complexity of the algorithm is O(nC3 + n2). Which makes this method faster than other methods having a computational complexity of O(n3). Due to short computation time this method can be used to analyze large datasets."}, {"heading": "5 Simulations", "text": "For simulation we produced synthetic datasets to check the performance of our proposed model. The proposed model is duly tested for data of type Gaussian, non-Gaussian and noisy. Bach and Jordan [1] provides the framework to construct the sparse matrices with known probabilities. The non-Gaussianity of data is more useful than Gaussian for identification of causal DAGS [7]. In our case data is synthesized using following properties\n1. Construct 4 dataset with m = 500, 1500, 2000, 3000 number of instances and n = 10, 25, 35, 45 number of variables.\n2. For each n number of variable set, draw 40% of i.i.d samples from Gaussian and nonGaussian distributions of the types (1) Uniform, (2) Exponential, (3) Normal, (4) Lognormal, (5) Laplace, (6) Student- t with 3 degree freedom, (7) Student t with 5 degree freedom, (8)-(28) Symmetric mixtures of any two types from (1) to (7), (29)-(49) Nonsymmetric mixture of any two types from (1) to (7).\n3. Now remaining 60% of the samples for each n variables is drawn from random mixing of different distributions from (1) to (49) in a linear model as defined in equation 1. This results in a noisy dataset with correlated error.\n4. For linear mixture model, the mixing coefficients are randomly drawn from intervals [\u22125,\u22120.5] \u222a [0.5, 3] and confounders are drawn from interval [\u22122, 3].\nThe model is tested for effective computational time with ICA-LiNGAM [7] and DirectLiNGAM [8] methods. The system used for simulation process is a dual core i3 variant. In comparison test the choice of environments are different (environments of ICA-LiNGM, DirectLiNGAM is MATLAB and CVS with NPM is R), so true efficiency is not available. Table 1 provides the results on time consumed by methods for different number of sample sets.\nIn our simulation test ICA-LiNGAM and DirectLiNGAM methods failed for noisy dataset, drawn from linear mixture model. The datasets compared in this simulation are i.i.d samples drawn from non-Gaussian distribution types from (1) to (49). For smaller datasets CVS with NPM method out performed ICA-LiNGAM, but ICA-LiNGAM performed better for larger sets. In all test sets CVS with NPM over shadowed DirectLiNGAM from the begining, so we did not provided the rest of the results for DirectLiNGAN.\nFor estimation of causal DAGs, we used i.i.d samples generated using non-Gaussian distributions. Results for 5 and 8 number of node sets are used to compare the causal DAGs formed by CVS with NPM and DirectLiNGAM. For both methods the causal DAGs are formed using the strength matrices and confounders in the DAG are added from error matrix. The generated causal DAGs are shown in Figures 2 and 3.In Figure 3 (a), the constructed model using DirectLiNGAM formed a bi-directed edge from 1 and 3. The strength and errors matrices used in this model are supplied in the Supplementary material.\nThe performance of our method for real world data is compared with DLiNGAM. We used the regression, multivariate and categorical dataset of Boston Housing, available in UCI Repository [4]. Dataset contains 14 variables and 506 instances. For causal construction we removed the third variable from the dataset, as it only contains the binary values. The causal DAG for 13 nodes of Boston data is shown in Figure 4.\nAs shown in Figure 4, CVS with NPM always produces a DAG whether the data is noisy or nonGaussian, so acyclic condition always holds."}, {"heading": "6 Conclusion", "text": "Our method provides a new approach to causal inference and structural learning focusing to measure the strength of informations passed in the nodes. Proposed model provides solutions for how much information is passed from parent node to child and how much a child node is affected by external confounders, which were remains unsolved till date. The novelty of the method is to provide measures for path coefficient in units of percentage contributions from a rang of [0, 1]. The model provides strong criteria for directed acyclic causal graph formation as shown in above figures. The causal DAG estimation capability is fully verified in synthetic and real world conditions in comparison to other methods and results shown in Section 5. The method is very useful in noisy datasets to produce causal DAGs. More details about the noisy data results and reversible system is provided in supplementary material. A complete R code is available for further utilization and development and all the dataset used is provided in the supplementary material. The results of the proposed method can be used to provide an image tool for object oriented plotting of causal DAGs, which is a future perspective of this proposal. The major problem of this future work is the arrangement of multivariate nodes for bigger models. The NPM may prove it\u2019s usefulness in broad areas of studies\nand collider v-structures can be used with other Bayesian methods for structure estimations of types non-confounder and noisy systems."}], "references": [{"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Active Learning of Causal Networks with Intervention Experiments and Optimal Designs", "author": ["He", "Yang-Bo", "Geng", "Zhi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "UCI repository of machine learning databases. Available at http: //www.ics.uci.edu/ \u0303mlearn/MLRepository.html", "author": ["P.M. Murphy", "D.W. Aha"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Using Markov Blanket for Causal Structure Learning", "author": ["J.P. Pellet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "A Linear non-Gaussian Acyclic Model for Causal Discovery", "author": ["S. Shimizu", "P.O. Hoyer", "A. Hyv\u00e4rinen", "A. Kerminen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "DirectLiNGAM: A Direct Method for Learning a Linear non-Gaussian Structural Equation Model", "author": ["S. Shimizu", "T. Inazumi", "Y. Sogawa", "A. Hyv\u00e4rinen", "Y. Kawahara", "T. Washio", "P.O. Hoyer", "K. Bollen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Geometry of the Faithfulness Assumption in Causal Inference", "author": ["C. Uhler", "G. Raskutti", "P. Buhlmann", "B. Yu"], "venue": "The Annals of Statistics,Vol", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A Recursiv Method for Structural Learning of Diirected Acyclic Graphs. Journal of Machine Learning Research, 9:459-483", "author": ["X. Xie", "Z. Geng"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "The NPM is used to scale the strength of information passed through nodes in units of percentage from interval [0, 1].", "startOffset": 111, "endOffset": 117}, {"referenceID": 3, "context": "The Markov model is a directed graph which does not contains any bi-directed edges [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "He and Geng [2] applied intervention technique on markov equivalence classes to produce subgraphs with directed edges.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "The structural learning of causal model can be shown through faithfulness and relevance of connections using d-separable and v-structures [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 0, "context": "The faithfulness for markov equivalent classes are beneficial and weak or strong faithful criteria can be derived for Gaussian and uniform distributions, where path coefficients and errors are drawn from intervals [\u22121, 1] and [0, 1] respectively [9].", "startOffset": 226, "endOffset": 232}, {"referenceID": 7, "context": "The faithfulness for markov equivalent classes are beneficial and weak or strong faithful criteria can be derived for Gaussian and uniform distributions, where path coefficients and errors are drawn from intervals [\u22121, 1] and [0, 1] respectively [9].", "startOffset": 246, "endOffset": 249}, {"referenceID": 8, "context": "The directed acyclic causal graph can be studied as a decomposition of multiple v-structures under the lights of markov model [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "The percentage contribution (%) for (ax, b) \u2208 < is (ax, b)% = (ax, b)/y always holds, for each {(ax)%c, (b)%c} \u2208 [0, 1] such that (ax)% + (b)% = 1.", "startOffset": 113, "endOffset": 119}, {"referenceID": 0, "context": "If (ax)% + (b)% = 1 for any {(ax)%c, (b)%c} \u2208 S, where set S contains the elements from {[\u2212\u221e,\u22121], [\u22121, 0], [0, 1], [1,\u221e]} then for every over margin percentage there exist a negative percentage for wich (ax)%+(b)% = 1 holds.", "startOffset": 107, "endOffset": 113}, {"referenceID": 0, "context": "These percentages can be mapped to interval [0, 1] by negative percentage mapping defined as (ax, b)% = {|(ax)%|, |(b)%|}/{|(ax)%|+ |(b)%|}.", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "So if we will observe all the negative percentage contributions, then through negative percentage mapping we can easily compute all the parameter contributions in [0, 1] interval.", "startOffset": 163, "endOffset": 169}, {"referenceID": 0, "context": "Bach and Jordan [1] provides the framework to construct the sparse matrices with known probabilities.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "The non-Gaussianity of data is more useful than Gaussian for identification of causal DAGS [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "The model is tested for effective computational time with ICA-LiNGAM [7] and DirectLiNGAM [8] methods.", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "The model is tested for effective computational time with ICA-LiNGAM [7] and DirectLiNGAM [8] methods.", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "We used the regression, multivariate and categorical dataset of Boston Housing, available in UCI Repository [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "The novelty of the method is to provide measures for path coefficient in units of percentage contributions from a rang of [0, 1].", "startOffset": 122, "endOffset": 128}], "year": 2015, "abstractText": "A major problem of causal inference is the arrangement of dependent nodes in a directed acyclic graph (DAG) with path coefficients and observed confounders. Path coefficients do not provide the units to measure the strength of information flowing from one node to the other. Here we proposed the method of causal structure learning using collider v-structures (CVS) with Negative Percentage Mapping (NPM) to get selective thresholds of information strength, to direct the edges and subjective confounders in a DAG. The NPM is used to scale the strength of information passed through nodes in units of percentage from interval [0, 1]. The causal structures are constructed by bottom up approach using path coefficients, causal directions and confounders, derived implementing collider v-structure and NPM. The method is self-sufficient to observe all the latent confounders present in the causal model and capable of detecting every responsible causal direction. The results are tested for simulated datasets of non-Gaussian distributions and compared with DirectLiNGAM and ICA-LiNGAM to check efficiency of the proposed method. 1 Background Study: Markov Model, d-separable, v-structures In Bayesian Learning the Markov properties play an important role, which set the criteria for structure analysis. The Markov model is a directed graph which does not contains any bi-directed edges [5]. This criteria does not provides the acyclicity of the model. But using markov model we can find the conditional dependent and independent nodes, provided we have information on prior observations. Sometime it happens to be very difficult to know about prior observations or any of the prior information. Assuming the observed variables are prior, the later ones will be conditionally dependent on the prior ones. Now we need a criteria to observe the posterior effects. In this scenario d-separable provides the criteria of sub-structural learning for conditional dependence and independence of the features. To implement d-separable we need v-structures. The primary definition of v-structure is provided for the undirected graphical models, where two nodes are only connected through an undirected edge [3]. The v-structures in a markov model are the sub-structures where each three of the observed nodes are only connected through two directed edges. For the choice of any three nodes we can permute them in three possible ways using two 1 ar X iv :1 50 9. 04 90 4v 1 [ cs .A I] 1 6 Se p 20 15 directed edges irrespective of their positions. The d-separable criteria is used to construct a set of nodes such that conditioning on which, the paths/edges can be separated between other two nodes in the v-structure. Consider v-structures of a \u2192 b \u2192 c , a \u2190 b \u2192 c and a \u2192 b \u2190 c. In first and second types a and c can be d-separated by observing/conditioning on b, while in the third one conditioning on b makes a and c becomes dependent on each other as ancestor nodes of b remains unobserved.. The third type of v-structure is called a collider. He and Geng [2] applied intervention technique on markov equivalence classes to produce subgraphs with directed edges. They generated v-structures of subgraphs with markov properties to find directions, using conditional probabilities. Markov Blankets can reveal the directions in the nodes based on their relevance of connection inside the blanket. The structural learning of causal model can be shown through faithfulness and relevance of connections using d-separable and v-structures [6]. The faithfulness for markov equivalent classes are beneficial and weak or strong faithful criteria can be derived for Gaussian and uniform distributions, where path coefficients and errors are drawn from intervals [\u22121, 1] and [0, 1] respectively [9]. In following Sections 2 and 3, we provided complete explanation for using collider v-structures as a substructural model and NPM for measuring strength of path coefficients. 2 Proposed Structural Learning Method Every orthodox and modern approach of causal construction lag to define the measure for information passing through directed edges, while path coefficient/connection strength is the information passed on. Depending on the information (path coefficient) either from nodes or external confounders we cannot conclude the directions in causal model. This problem becomes more complicated when it comes to multivariate structure. So we need a measure for these entropies through which we will be able conclude our directions. Before scaling the flow of information, we need a model to work on for causal constructions. The easiest way to build a larger model is, first to build the sub-structures of the bigger model and then arrange these small ones to complete the required structure. The directed acyclic causal graph can be studied as a decomposition of multiple v-structures under the lights of markov model [10]. In our case we used a bottom up approach of arranging multiple v-structures to form a causal DAG. For this type of approach we don\u2019t need any prior information regarding the structure or contributing variables. After deriving all the direction, path coefficients and confounders for the considered vstructures, these can be combined together to provide a complete causal structure. The model of collider v-structure provides the necessary sub-structural analysis in a very detailed way. 2.1 Collider v-structure with NPM (CVS with NPM): To analyze a larger causal structure in a smaller sub-structural level consider the v-structure as defied in the markov model, that is a set of three nodes connected through two directed edges. The collider is a v-structure where two extreme nodes have directed edges towards the middle node. We referred it as collider v-structure whenever an external confounder is present or added to the collider system and we modeled our algorithm suitable to analyze these types of DAG. Below Figure 1 provides the collider v-structures.", "creator": "LaTeX with hyperref package"}}}