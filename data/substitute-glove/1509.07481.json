{"id": "1509.07481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks", "abstract": "We agenda while back - right direction as accepted macros well-defined differ spatially on specific sometimes among images, well, Gramian Angular Fields bringing Markov Transition Fields. This enabled given devices of conventional later uses vision in films skill also classification. We used Tiled Convolutional Neural Networks to anything falls - comparable incorporates from individual GAF, MTF, once GAF - MTF images early 12 benchmark every aired datasets and also fact adaptive - temporal jumps visualisation. The depending overall according sense emphasize rarely qualified with state - \u2014 - while - inspiration learning on shared types of comparison. An calculation while the various and multiplication everyone several the CNNs explains sure a considering works.", "histories": [["v1", "Thu, 24 Sep 2015 19:14:20 GMT  (2223kb,D)", "http://arxiv.org/abs/1509.07481v1", "Submit to JCSS. Preliminary versions are appeared in AAAI 2015 workshop and IJCAI 2016 [arXiv:1506.00327]"]], "COMMENTS": "Submit to JCSS. Preliminary versions are appeared in AAAI 2015 workshop and IJCAI 2016 [arXiv:1506.00327]", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhiguang wang", "tim oates"], "accepted": false, "id": "1509.07481"}, "pdf": {"name": "1509.07481.pdf", "metadata": {"source": "CRF", "title": "Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks", "authors": ["I Zhiguang Wanga", "Tim Oates"], "emails": ["zgwang813@gmail.com"], "sections": [{"heading": null, "text": "We propose an off-line approach to explicitly encode temporal patterns spatially as different types of images, namely, Gramian Angular Fields and Markov Transition Fields. This enables the use of techniques from computer vision for feature learning and classification. We used Tiled Convolutional Neural Networks to learn high-level features from individual GAF, MTF, and GAF-MTF images on 12 benchmark time series datasets and two real spatial-temporal trajectory datasets. The classification results of our approach are competitive with state-of-the-art approaches on both types of data. An analysis of the features and weights learned by the CNNs explains why the approach works.\nKeywords: Time-series, Trajectory, Classification, Gramian Angular Field, Markov Transition Field, Convolutional Neural Networks"}, {"heading": "1. Introduction", "text": "The problem of temporal data classification has attracted great interest recently, finding applications in domains as diverse as medicine, finance, entertainment, and industry. However, learning the complicated temporal correlations in complex dynamic systems is still a challenging problem. Inspired by recent successes of deep learning in computer vision, we consider the problem of en-\nIPreliminary versions of parts of this paper appear in the Twenty-Ninth AAAI workshop proceedings on Trajectory-based Behaviour Analytics.\n\u2217Corresponding author. Email address: zgwang813@gmail.com (Zhiguang Wang)\nPreprint submitted to Journal of Computer and Systems Sciences September 25, 2015\nar X\niv :1\n50 9.\n07 48\n1v 1\n[ cs\n.L G\n] 2\n4 Se\np 20\ncoding temporal information spatially as images to allow machines to \u201dvisually\u201d recognize and classify temporal data, especially time series data.\nRecognition tasks in speech and audio have been well studied. Researchers have achieved success using combinations of HMMs with acoustic models based on Gaussian Mixture models (GMMs) [1, 2]. An alternative approach is to use deep neural networks to produce posterior probabilities over HMM states. Deep learning has become increasingly popular since the introduction of effective ways to train multiple hidden layers [3] and has been proposed as a replacement for GMMs to model acoustic data in speech recognition tasks [4]. These Deep Neural Network - Hidden Markov Model hybrid systems (DNN-HMM) achieved remarkable performance in a variety of speech recognition tasks [5, 6, 7]. Such success stems from learning distributed representations via deeply layered structure and unsupervised pretraining by stacking single layer Restricted Boltzmann Machines (RBM).\nAnother deep learning architecture used in computer vision is convolutional neural networks (CNNs) [8]. CNNs exploit translational invariance within their structures by extracting features through receptive fields [9] and learn with weight sharing. CNNs are the state-of-the-art approach in various image recognition and computer vision tasks [10, 11, 12]. Since unsupervised pretraining has been shown to improve performance [13], sparse coding and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariances [14, 15].\nCNNs were proposed for speech processing because of their invariance to shifts in time and frequency [16]. Recently, CNNs have been shown to further improve hybrid model performance by applying convolution and max-pooling in the frequency domain on the TIMIT phone recognition task [17]. A heterogeneous pooling approach proved to be beneficial for training acoustic invariance [18]. Further exploration with limited weight sharing and a weighted softmax pooling layer has been proposed to optimize CNN structures for speech recognition tasks [19].\nHowever, except for audio and speech data, relatively little work has explored feature learning in the context of typical time series analysis tasks with current deep learning architectures. [20] explores supervised feature learning with CNNs to classify multi-channel time series with two datasets. They extracted subsequences with sliding windows and compared their results to Dynamic Time Warping (DTW) with a 1-Nearest-Neighbor classifier (1NN-DTW). Our motivation is to explore a novel framework to encode time series as images and thus to take advantage of the success of deep learning architectures in computer vision to learn features and identify structure in time series. Unlike speech recognition systems in which acoustic/speech data input is typically represented by concatenating Mel-frequency cepstral coefficients (MFCCs) or perceptual linear predictive coefficient (PLPs) [21], typical time series data are not likely to benefit from transformations applied to speech or acoustic data.\nIn this work, we propose two types of representations for explicitly encoding the temporal patterns in time series as images. We test our approach on twelve time series datasets produced from 2D shape, physiological surveillance, industry and other domains. Two real spatial-temporal trajectory datasets are also considered for experiments to demonstrate the performance of our approach. We applied deep Convolutional Neural Networks with a pretraining stage that exploits local orthogonality by Topographic ICA [15] to \u201cvisually\u201d inspect and classify time series. We report our classification performance both on GAF and MTF separately, and GAF-MTF which resulted from combining GAF and MTF representations into single image. By comparing our results with the current best hand-crafted representation and classification methods on both time series and trajectory data, we show that our approach in practice achieves competitive performance with the state of the art with only cursory exploration of hyperparameters. In addition to exploring the high level features learned by Tiled CNNs, we provide an in-depth analysis in terms of the duality between time series and images. This helps us to more precisely identify the reasons why our approaches work well."}, {"heading": "2. Motivation", "text": "Learning the (long) temporal correlations that are often embedded in time series remains a major challenge in time series analysis and modeling. Most realworld data has a temporal component, whether it is measurements of natural (weather, sound) or man-made (stock market, robotics) phenomena. Traditional approaches for modeling and representing time-series data fall into three categories. In time series learning problems, non-data adaptive models, such as Discrete Fourier Transformation (DFT) [23], Discrete Wavelet Transformation (DWT) [24], and Discrete Cosine Transformation (DCT) [25], compute the transformation with an algorithm that is invariant with respect to the data to capture the intrinsic temporal correlation with the different basis functions. Meanwhile, researchers explored in the model-based approaches to model time series, such as Auto-Regressive Moving Average models (ARMA) [26] and Hidden Markov Models (HMMs) [27], in which the underlying data is assumed to fit a specific type of model to explicitly function the temporal patterns. The estimated parameters can then be used as features for classification or regression. However, more complex, high-dimensional, and noisy real-world time-series data are often difficult to model because the dynamics are either too complex or unknown. Traditional methods, which contain a small number of non-linear operations, might not have the capacity to accurately model such complex systems.\nIf implicitly learning the complex temporal correlation is difficult, how about reformulating the data to explicitly or even visually encode the temporal dependency, allowing the algorithms to learn more easily? Actually, reformulating the features of time series as visual clues has raised much attention in computer science and physics. The typical examples in speech recognition tasks are that acoustic/speech data input is typically represented by MFCCs or PLPs to explicitly represent the temporal and frequency information. Recently, researchers are trying to build different network structures from time series for visual inspection or designing distance measures. Recurrence Networks were proposed to analyze the structural properties of time series from complex sys-\ntems [28, 29]. They build adjacency matrices from the predefined recurrence functions to interpret the time series as complex networks. Silva et al. extended the recurrence plot paradigm for time series classification using compression distance [30]. Another way to build a weighted adjacency matrix is extracting transition dynamics from the first order Markov matrix [31]. Although these maps demonstrate distinct topological properties among different time series, it remains unclear how these topological properties relate to the original time series since they have no exact inverse operations. One of our contributions is to propose a set of off-line algorithm to encode the complex correlations in time series into images for visual inspection and classification. The proposed encoding functions have exact/approximate inverse maps, making such transformations more interpretable."}, {"heading": "3. Encoding Methods", "text": "We first introduce our two frameworks to encode time series data as images. The first type of image is the Gramian Angular field (GAF), in which we represent time series in a polar coordinate system instead of the typical Cartesian coordinates. In the Gramian matrix, each element is actually the cosine of the summation of pairwise temporal values. Inspired by previous work on the duality between time series and complex networks [31], the main idea of the second framework, the Markov Transition Field (MTF), is to build the Markov matrix of quantile bins after discretization and encode the dynamic transition probability in a quasi-Gramian matrix."}, {"heading": "3.1. Gramian Angular Field", "text": "Given a time series X = {x1, x2, ..., xn} of n real-valued observations, we\nrescale X so that all values fall in the interval [\u22121, 1] or [0, 1] by:\nx\u0303i\u22121 = (xi\u2212max(X)+(xi\u2212min(X)) max(X)\u2212min(X) (1)\nor x\u0303i0 = xi\u2212min(X) max(X)\u2212min(X) (2)\nThus we can represent the rescaled time series X\u0303 in polar coordinates by encoding the value as the angular cosine and the time stamp as the radius with the equations below:\u03c6 = arccos (x\u0303i),\u22121 \u2264 x\u0303i \u2264 1, x\u0303i \u2208 X\u0303r = tiN , ti \u2208 N (3) ti is the time stamp and N is a constant factor to regularize the span of the polar coordinate system. This polar coordinate based representation is a novel way to understand time series. As time increases, corresponding values warp among different angular points on the spanning circles, like water rippling. The encoding map of Eq. 3 has two important properties. First, it is bijective as cos(\u03c6) is monotonic when \u03c6 \u2208 [0, \u03c0]. Given a time series, the proposed map produces one and only one result in the polar coordinate system with a unique inverse function. Second, as opposed to Cartesian coordinates, polar coordinates preserve absolute temporal relations. In Cartesian coordinates, the\narea is defined by Si,j = \u222b x(j) x(i) f(x(t))dx(t), we have Si,i+k = Sj,j+k if f(x(t)) has the same values on [i, i+ k] and [j, j + k]. However, in polar coordinates, if\nthe area is defined as S\u2032i,j = \u222b \u03c6(j) \u03c6(i) r[\u03c6(t)]2d(\u03c6(t)), then S\u2032i,i+k 6= S\u2032j,j+k. That is, the corresponding area from time stamp i to time stamp j is not only dependent on the time interval |i\u2212 j|, but also determined by the absolute value of i and j.\nAfter transforming the rescaled time series into the polar coordinate system, we can easily exploit the angular perspective by considering the trigonometric sum between each pair of points to identify the temporal correlation in different time intervals. The GAF is defined as follows:\nG =  cos(\u03c61 + \u03c61) \u00b7 \u00b7 \u00b7 cos(\u03c61 + \u03c6n) cos(\u03c62 + \u03c61) \u00b7 \u00b7 \u00b7 cos(\u03c62 + \u03c6n) ... . . . ...\ncos(\u03c6n + \u03c61) \u00b7 \u00b7 \u00b7 cos(\u03c6n + \u03c6n)\n (4)\n= X\u0303 \u2032 \u00b7 X\u0303 \u2212 \u221a I \u2212 X\u03032 \u2032 \u00b7 \u221a I \u2212 X\u03032 (5)\nI is the unit row vector [1, 1, ..., 1]. After transforming to the polar coordinate\nsystem, we take the data in a time series as a 1-D metric space. By defining the inner product < x, y >= x \u00b7 y \u2212 \u221a 1\u2212 x2 \u00b7 \u221a 1\u2212 y2, G is a Gramian matrix:\n < x\u03031, x\u03031 > \u00b7 \u00b7 \u00b7 < x\u03031, x\u0303n > < x\u03032, x\u03031 > \u00b7 \u00b7 \u00b7 < x\u03032, x\u0303n > ... . . . ...\n< x\u0303n, x\u03031 > \u00b7 \u00b7 \u00b7 < x\u0303n, x\u0303n >\n (6)\nGAF has several advantages. It provides a way to preserve temporal dependency. When time increases, the position moves from top-left to bottom-right in the Gramian matrix. The GAF contains temporal correlations, as G(i,j||i\u2212j|=k) represents the relative correlation by superposition of directions with respect to time interval k. The main diagonal Gi,i is the special case when k = 0, which contains the original value/angular information. With the main diagonal, we will approximately reconstruct the time series from the high level features learned by the deep neural network. The GAF images may be large because the size of the Gramian matrix is n\u00d7 n when the length of the raw time series is n. To reduce the size of the GAF images, we apply Piecewise Aggregate Approximation [32] to smooth the time series while keeping the overall trends. The full procedure for generating the GAF is illustrated in Figure 1.\nThrough the polar coordinate system, GAFs actually represent the mutual correlations between each pair of points/phases by the superposition of the nonlinear cosine functions. Different types of time series always have their specific patterns embedded along the time and frequency dimensions. After the feature reformulation process by GAF, most different patterns are enhanced even for visual inspection by humans (Figure 2)."}, {"heading": "3.2. Markov Transition Field", "text": "We propose a framework that is similar to [31] for encoding dynamical transition statistics. We develop that idea by representing the Markov transition probabilities sequentially to preserve information in the temporal dimension.\nGiven a time series X, we identify its Q quantile bins and assign each xi to its corresponding bin qj (j \u2208 [1, Q]). Thus we construct a Q\u00d7Q weighted adjacency matrix W by counting transitions among quantile bins in the manner of a firstorder Markov chain along the time axis. wi,j is the frequency with which a point in quantile qj is followed by a point in quantile qi. After normalization by\u2211 j wij = 1, W is the Markov transition matrix:\nV =  v11|P (xt\u2208q1|xt\u22121\u2208q1) \u00b7 \u00b7 \u00b7 v1Q|P (xt\u2208q1|xt\u22121\u2208qQ) v21|P (xt\u2208q2|xt\u22121\u2208q1) \u00b7 \u00b7 \u00b7 v2Q|P (xt\u2208q2|xt\u22121\u2208qQ) ... . . . ...\nvQ1|P (xt\u2208qQ|xt\u22121\u2208q1) \u00b7 \u00b7 \u00b7 vQQ|P (xt\u2208qQ|xt\u22121\u2208qQ)\n (7)\nIt is insensitive to the distribution of X and the temporal dependency on the time steps ti. However, getting rid of the temporal dependency results in too much information loss in the matrix W . To overcome this drawback, we\ndefine the Markov Transition Field (MTF) as follows:\nM =  vij|x1\u2208qi,x1\u2208qj \u00b7 \u00b7 \u00b7 vij|x1\u2208qi,xn\u2208qj vij|x2\u2208qi,x1\u2208qj \u00b7 \u00b7 \u00b7 vij|x2\u2208qi,xn\u2208qj ... . . . ...\nvij|xn\u2208qi,x1\u2208qj \u00b7 \u00b7 \u00b7 vij|xn\u2208qi,xn\u2208qj\n (8)\nWe build a Q\u00d7Q Markov transition matrix W by dividing the data (magnitude) into Q quantile bins. The quantile bins that contain the data at time steps i and j (temporal axis) are qi and qj (q \u2208 [1, Q]). Mij in MTF denotes the transition probability of qi \u2192 qj . That is, we spread out matrix W , which contains the transition probability on the magnitude axis, into the MTF matrix by considering temporal positions.\nBy assigning the probability from the quantile at time step i to the quantile at time step j at each pixel Mij , the MTF M actually encodes multi-step transition probabilities of the time series. Mi,j||i\u2212j|=k denotes the transition probability between the points with time interval k. For example, Mij|j\u2212i=1 illustrates the transition process along the time axis with a skip step. The main diagonal Mii, which is a special case when k = 0 captures the probability from\neach quantile to itself (the self-transition probability) at time step i. To make the image size manageable for more efficient computation, we reduce the MTF by averaging the pixels in each non-overlapping m\u00d7m patch with the blurring kernel { 1m2 }m\u00d7m. That is, we aggregate the transition probabilities in each subsequence of length m together. Figure 3 shows the procedure to encode time series to MTF.\nBy scattering the first-order transition probability into the temporally ordered matrix, MTFs encode the transition dynamics between different time lags k. We assume that different types of time series have their specific transition dynamics embedded in the temporal and frequency domains. After the feature reformulation process by MTF, most transition dynamics are extracted, which are explicitly obvious for visual inspection (Figure 4).\nImaging Time-Series \u2013 GAF-MTF MTF images\n[Wang, Oates, AAAI workshop on TrBA, 2015]"}, {"heading": "4. Tiled Convolutional Neural Networks", "text": "Tiled Convolutional Neural Networks [15] are a variation of Convolutional Neural Networks. They use tiles and multiple feature maps to learn invariant features. Tiles are parameterized by a tile size K to control the distance over which weights are shared. By producing multiple feature maps, Tiled CNNs learn overcomplete representations through unsupervised pretraining with Topographic ICA (TICA).\nA typical TICA network is actually a double-stage optimization procedure with square and square root nonlinearities in each stage, respectively. In the first stage, the weight matrix W is learned while the matrix V is hard-coded to represent the topographic structure of units. More precisely, given a sequence of inputs {xh}, the activation of each unit in the second stage is fi(x(h);W,V ) =\u221a\u2211p k=1 Vik( \u2211q j=1Wkjx (h) j ) 2. TICA learns the weight matrix W in the second stage by solving:\nminimize W\nn\u2211 h=1 p\u2211 i=1 fi(x (h);W,V )\nsubject to WWT = I\n(9)\nW \u2208 Rp\u00d7q and V \u2208 Rp\u00d7p where p is the number of hidden units in a layer\nAlgorithm 1 Unsupervised pretraining with TICA [15] Require: {x(t)}Tt=1, v, s,W, V as input Ensure: W as output\nrepeat fold = \u2211T t=1 \u2211m i=1 \u221a\u2211m k=1 Vik( \u2211n j=1Wkjx (t) j ) 2, g = f old \u2202W , f new = +\u221e, \u03b1 = 1\nwhile fnew > fold do\nWnew = W \u2212 \u03b1g Wnew = Localize(Wnew, s) Wnew = tieWeights(Wnew, k) Wnew = orthogonalizeLocalRF (Wnew) Wnew = tieWeights(Wnew, k)\nfnew = \u2211T t=1 \u2211m i=1 \u221a\u2211m k=1 Vik( \u2211n j=1Wkjx (t) j ) 2 \u03b1 = 0.5\u03b1\nend while W = Wnew\nuntil convergence\nand q is the size of the input. V is a logical matrix (Vij = 1 or 0) that encodes the topographic structure of the hidden units by a contiguous 3\u00d7 3 block. The orthogonality constraint WWT = I provides diversity among learned features.\nThe pretraining algorithm (Algorithm. 1) is based on gradient descent on the TICA objective function in Equation. 9. The inner loop is a simple implementation of backtracking linesearch. The orthogonalize localRF (Wnew) function only orthogonalizes the weights that have completely overlapping receptive fields. Weight-tying is applied by averaging each set of tied weights. The algorithm is trained by batch projected gradient descent. Other unsupervised feature learning algorithms such as RBMs and autoencoders [33] require more parameter tuning, especially during optimization. However, pretraining with TICA usually requires little tuning of optimization parameters, because the tractable objective function of TICA allows to monitor convergence easily.\nNeither GAF nor MTF images are natural images; they have no natural concepts such as \u201cedges\u201d and \u201cangles\u201d. Thus, we propose to exploit the benefits of unsupervised pretraining with TICA to learn many diverse features with local orthogonality. In [15], the authors empirically demonstrate that tiled CNNs perform well with limited labeled data because the partial weight tying requires fewer parameters and reduces the need for a large amount of labeled data. Our data from the UCR Time Series Repository [34] tends to have few instances (e.g., the \u201cyoga\u201d dataset has 300 labeled instance in the training set and 3000 unlabeled instance in the test set), so tiled CNNs are suitable for our learning task. Moreover, Tiled CNNs achieve good performance on large datasets (such as NORB and CIFAR).\nTypically, tiled CNNs are trained with two hyperparameters, the tiling size k and the number of feature maps l. In our experiments, we directly fixed\nthe network structures without tuning these hyperparameters in loops. Our experimental settings follow the default deep network structures and parameters in [15]. Tiled CNNs with such configurations are reported to achieve the best performance on the NORB image classification benchmark. Although tuning the parameters will surely enhance performance, doing so may cloud our understanding of the power of the representation. Another consideration is computational efficiency. All of the experiments on the 12 datasets could be done in one day on a laptop with an Intel i7-3630QM CPU and 8GB of memory (our experimental platform). Thus, the results in this paper are a preliminary lower bound on the potential best performance. Thoroughly exploring network structures and parameters will be addressed in future work. The structure and parameters of the tiled CNN used in this paper are illustrated in Figure 5."}, {"heading": "5. Experiments on Time Series Data", "text": "We apply Tiled CNNs to classify using GAF and MTF representation on twelve tough datasets, on which the classification error rate is above 0.1 with the state-of-the-art SAX-BoP approach [35, 22]. More detailed statistics are summarized in Table 1. The datasets are pre-split into training and testing sets for experimental comparisons. For each dataset, the table gives its name, the number of classes, the number of training and test instances, and the length of the individual time series."}, {"heading": "5.1. Experiment Settings", "text": "In our experiments, the size of the GAF image is regulated by the the number of PAA bins SGAF . Given a time series X of size n, we divide the time series into SGAF adjacent, non-overlapping windows along the time axis and extract the means of each bin. This enables us to construct the smaller GAF matrix GSGAF\u00d7SGAF . MTF requires the time series to be discretized into Q quantile bins to calculate the Q\u00d7Q Markov transition matrix, from which we construct the raw MTF image Mn\u00d7n afterwards. Before classification, we shrink the MTF image size to SMTF \u00d7 SMTF by the blurring kernel { 1m2 }m\u00d7m where m = d nSMTF e. The Tiled CNN is trained with image size {SGAF , SMTF } \u2208 {16, 24, 32, 40, 48} and quantile size Q \u2208 {8, 16, 32, 64}. At the last layer of the Tiled CNN, we use a linear soft margin SVM [36] and select C by 5-fold cross validation over {10\u22124, 10\u22123, . . . , 104} on the training set.\nFor each input of image size SGAF or SMTF and quantile size Q, we pretrain the Tiled CNN with the full unlabeled dataset (both training and test set with no labels) to learn the initial weights W through TICA.Then we train the SVM at the last layer by selecting the penalty factor C with cross validation. Finally, we classify the test set using the optimal hyperparameters {S,Q,C} with the lowest error rate on the training set. If two or more models tie, we prefer the larger S and Q because larger S helps preserve more information through the PAA procedure and larger Q encodes the dynamic transition statistics with more\ndetail. Our model selection approach provides generalization without being overly expensive computationally."}, {"heading": "5.2. Results and Discussion", "text": "We use Tiled CNNs to classify GAF and MTF representations separately on the 12 datasets. The training and test error rates are shown in Table 2. Generally, our approach is not prone to overfitting as seen by the relatively small difference between training and test set errors. One exception is the \u2019Olive Oil\u2019 dataset with the MTF approach where the test error is significantly higher.\nIn addition to the slight risk of potential overfitting, MTF has generally higher error rates than GAF. This is most likely because of uncertainty in the inverse image of MTF. Note that the encoding function from time series to GAF and MTF are both surjection. The map functions of GAF and MTF will each produce only one image with fixed S andQ for each given time seriesX. Because\nthey are both surjective mapping functions, the inverse image of the map is not fixed. As shown in a later section, we can approximately reconstruct the raw time series from GAF, but it is very hard to even roughly recover the signal from MTF. GAF has smaller uncertainty in the inverse image of its mapping function because randomness only comes from the ambiguity of cos(\u03c6) when \u03c6 \u2208 [0, 2\u03c0]. MTF, on the other hand, has a much larger inverse image space, which results in large variation when we try to recover the signals. Although MTF encodes the transition dynamics, which are important features of time series, such features seem not to be sufficient for recognition/classification tasks.\nNote that at each pixel, Gij , denotes the superstition of the directions at ti and tj , Mij is the transition probability from the quantile at ti to the quantile at tj . GAF encodes static information while MTF depicts information about dynamics. From this point of view, we consider them as two \u201corthogonal\u201d channels, like different colors in the RGB image space. Thus, we can combine GAF and MTF images of the same size (i.e. SGAF = SMTF ) to construct a double-channel image (GAF-MTF). Since GAF-MTF combines both the static and dynamic statistics embedded in raw time series, we posit that it will improve classification performance. In the next experiment, we pretrained and finetuned the Tiled CNN on the compound GAF-MTF images. Then, we report the classification error rate on test sets.\nTable 3 compares the classification error rate of our approach with previously published results of five competing methods: two state-of-the-art 1NN classifiers based on Euclidean distance and DTW, the recently proposed Fast-Shapelets based classifier [37], the classifier based on Bag-of-Patterns (BoP) [35, 22] and the most recent SAX-VSM approach [38]. Our approach outperforms 1NNEuclidean, fast-shapelets, and BoP, and is competitive with 1NN-DTW and SAX-VSM.\nIn addition, by comparing the results between Table 3 and Table 2, we verified our assumption that combined GAF-MTF images have better expressive power than the single GAF or MTF alone for classification. GAF-MTF images achieves the lower test error rate on ten datasets out of twelve (except\nfor the \u2019Adiac\u2019 and \u2019Beef\u2019 dataset ). On the \u2019Olive Oil\u2019 dataset, the training error rate is 6.67% and the test error rate is 16.67%. This demonstrates that the integration of both types of images into one compound image decreases the risk of overfitting as well as enhancing the overall classification accuracy. Thus, the intrinsic \u201dorthogonality\u201d between GAF and MTF on the same time series helps improve the classification performance with more comprehensive features. The multi-channel encoding approach is a scalable framework. The combination of multiple orthogonal channels into one images potentially improve the classification results, decreasing the risk of overfitting by a generalized ensemble framework. Meanwhile, hand-crafted feature integration potentially helps learn different informative features through deep learning architectures."}, {"heading": "5.3. Analysis of Learned Features", "text": "In contrast to the cases in which the CNN is applied in natural image recognition tasks, neither GAF nor MTF have natural interpretations of visual concepts (e.g., \u201dedges\u201d or \u201cangles\u201d). In this section, we analyze the features and weights learned through the Tiled CNNs to explain why our approach works.\nAs mentioned earlier, the mapping function from time series to GAF is surjective and the uncertainty in its inverse image comes from the ambiguity of cos(\u03c6) when \u03c6 \u2208 [0, 2\u03c0]. The main diagonal of GAF, i.e. {Gii} = {cos(2\u03c6i)} allows us to approximately reconstruct the original time series, ignoring the signs, by\ncos(\u03c6) =\n\u221a cos(2\u03c6) + 1\n2 (10)\nMTF has much larger uncertainty in its inverse image, making it hard to\nreconstruct the raw data from MTF alone. However, the diagonal {Mij||i\u2212j|=k} represents the transition probability among the quantiles in temporal order considering the time interval k. We construct the self-transition probability along the time axis from the main diagonal of MTF like we do for GAF. Although such reconstructions less accurately capture the morphology of the raw time series, they provide another perspective of how Tiled CNNs capture the transition dynamics embedded in MTF.\nFigure 6 illustrates the reconstruction results from six feature maps learned before the last SVM layer on the GAF and MTF. The Tiled CNN extracts the color patch, which is essentially an adaptive moving average that enhances several receptive fields within the nonlinear units by different trained weights. It is not a simple moving average but the synthetic integration by considering the 2D temporal dependencies among different time intervals, which is a benefit from the Gramian matrix structure that helps preserve the temporal information. By observing the rough orthogonal reconstruction from each layer of the feature maps, we can clearly observe that the CNNs can extract the multi-frequency dependencies through the convolution and pooling architecture on the GAF and MTF images. Different feature maps preserve the overall trend while addressing more details in different subphases. As shown in Figures 6(b) and 6(d), the high-leveled feature maps learned by the Tiled CNN are equivalent to a multifrequency approximator of the original curve.\nFigure 7 demonstrates the learned sparse weight matrix W with the constraint WWT = I, which makes effective use of local orthogonality. The TICA pretraining provides the built-in advantage that the function w.r.t the parameter space is not likely to be ill-conditioned as WWT = 1. As shown in Figure 7 (right), the weight matrix W is quasi-orthogonal and approaching 0 without very large magnitude. This implies that the condition number of W approaches 1 and helps the system to be well-conditioned."}, {"heading": "6. Experiments on Trajectory Data", "text": "We have demonstrated the effectiveness of GAF and MTF the benchmark time series datasets as diverse as shape, physiological surveillance and industry from the UCR time series repository. In this section we describe an application of our approaches to classify spatial-temporal trajectory data. The trajectory data is complex because patterns of movement are often driven by unperceived goals and constrained by an unknown environment.\nTo compare our results with other benchmark approaches including the seminal work from [39], we run experiments on two benchmark datasets, the animal movement dataset (Animal) and the hurricane track dataset (Hurricane) (Figure 8). Both datasets have trajectories of unequal length. For the \u201dAnimal\u201d dataset, the x and y coordinates are extracted from animal movements observed in June 1995. It is divided into three classes by species: elk, deer, and cattle, as shown in Figure 15. The numbers of trajectories (points) are 38 (7117), 30 (4333), and 34 (3540), respectively. In the \u201dHurricane\u201d dataset, the latitude and longitude are extracted from Atlantic hurricanes for the years 1950 through\n2006. The Saffir-Simpson scale classifies hurricanes into categories 1-5 by intensity. A high category number indicates high intensity. Categories 2 and 3 are chosen for two classes. The numbers of trajectories (points) are 61 (2459) and 72 (3126), respectively. Both datasets are pre-split into two parts for training (80%) and testing (20%). Figure 8 shows the overview of the trajectory data. Table 4 provides the classes, training size, testing size, minimum length and maximum length of the trajectory data."}, {"heading": "6.1. Hilbert Space Filling Curves", "text": "Spatial-temporal trajectory data is commonly multi-dimensional. We use Hilbert Space Filling Curves (SFC) to transform the trajectory into time series while preserving the spatial-temporal information.\nSpace filling has been studied by the mathematicians since the late 19th century when the first graphical representation was proposed by David Hilbert\nin 1891 [40]. Space filling curves provide a linear mapping from the multidimensional space to the 1-dimensional space. This mapping can be thought of as dividing D-dimensional space into D-dimensional hypercubes with a line passing through each hypercube. Recently, filling curve based approaches have shown to be able to preserve locality between objects in the multidimensional space in the linear space, and thus have been applied to different tasks like clustering [41], high dimensional outlier detection [42], and trajectory query [43] and classification [44]. Figure 9 (a) shows SFC examples of order {1,2,3,4,5,6}.\nBasically, the SFC of order 1 divides the square into 4 area. For the Hilbert curve with order 2, each sub-area of the curve with order 1 is further divided into 4 sub-areas. This process goes on as the order of the SFC increases. It is clear that the number of sub-areas in 2 dimensional SFC is 4order. To convert 2-dimensional data points to 1-dimensional points, each sub-area is integer numbered from 0 to 4order \u2212 1 starting from the lower left corner as 0 to the lower right corner. All other sub-areas are numbered in order of occurrence of the corresponding vertex as shown in Figure 9 (b) when order = 2. It also shows the example transformation process from a 2D trajectory to a sequence of scalars (time series). The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].\nWe map the trajectory points by the visiting order of the SFC embedded\nin the trajectory manifold space to the index sequence by the recorded times. The produced time series can be used for classification using our algorithm. This adds another hyperparameter called the SFC order, which decides the granularity of the space filling curve."}, {"heading": "6.2. Experiment Settings", "text": "The parameter settings are the same as the previous experiments on UCR datasets (Section 5). The optimal SFC order is selected together with other parameters through 5-fold cross validation from {3,4,5,6,7,8,9,10}.\nNote that both trajectory datasets have quite small sample size with varying length. When the trajectory length (as well as the time series length produced by SFC) is smaller than image size S, we uniformly duplicate each point in the time series in temporal order to stretch the sequence to length S. If the difference between the length of a time series and S is smaller than the original time series length, the interpolation strategy changes to random duplication instead of following the temporal order."}, {"heading": "6.3. Results and Discussion", "text": "Both \u2019Animal\u2019 and \u2019Hurricane\u2019 datasets have been used in previous research [39, 44] to achieve state-of-the-art classification accuracy. Traclass give two algorithms, trajectory-based (TB-only) and region-based + trajectory-based (RBTB) approaches based on features used for classification on these datastes. They carefully designed a hierarchy of features by partitioning trajectories and exploring two types of clustering. In [44], the author used SFC transformation to linearly map the trajectory data to time series and classified the sequences based on symbolic discretization with the multiple normal distribution assumption.\nAfter transforming the 2D trajectory data to time series using SFC, we generate the corresponding GAF and MTF images as shown in Figure 10. However, we found significant overfitting with CNNs even using 5-fold cross validation. This is probably because both the sample size and the time series length of the trajectory datasets are too small to avoid overfitting in neural networks.\nPrevious work has discussed overfitting during cross validation and proposed potential techniques to address this problem [45, 46]. Here, we applied a simple and straight-forward hyperparameter selection approach to reduce classifier variance. For a given set of hyperparameter {S,Q, SFCorder}, after cross validation with different C values of the linear SVM, we compute the mean and standard deviation to get the 3\u03c3 lower bound over all C by\nscore3\u03c3 = mean(Accuracy)\u2212 3\u00d7 STD(Accuracy) (11)\nBy selecting the other hyperparameters {S,Q, SFC \u2212 order} with the best statistical lower bound on the classifier performance over C, the optimal hyperparameters have lower variance while preserving lower bias. Using this hyperparameter selection approach, the classification results are reported in Table 5.\nWe perform better than the TB-Only method on both datasets and almost as good as the RB-TB method on the \u2019Hurricane\u2019 dataset. However, both RB-\nTB and NDist methods outperform ours on the \u2019Animal\u2019 dataset. As shown in Figure 8, both region and trajectory based features are useful for classification. For the \u2019Hurricane\u2019 dataset, direction based features are more useful than region based features. Direction based features are quite easy to capture using our approach as the GAF is actually calculating the pairwise direction fields on each points in the trajectory data. For the \u2019Animal\u2019 dataset, region is very important as shown in Figure 8 (a). Elk, deer and cattle are almost separable just using location as their regions are clearly located at the left, right top and right bottom, respectively. When transforming the trajectory data into time series using SFC, two close regions might be mapped to different sub-areas with different SFC indexes. When the indexes of two close regions are also near, this can be handled by CNNs with its capability to capture the small shifting-invariance features. However, CNNs are not good at discriminating similar images with large shifting from each other. Thus, when the region information is preserved by the manner of shifting the specific patterns largely in the time series produced by SFC, CNNs might have difficulty capturing the region information.\nAlthough our approach does not overtake other benchmark methods on both trajectory datasets, we provide a more general framework to encode the spatialtemporal patterns for classification tasks. Instead of complicated hand-tuned features, our approach can be applied to a variety of time series and trajectory data. When the region of the trajectory is not significantly important or the direction feature dominates, our general methods work quite well. On large datasets where the volume of time series/trajectory data is big, our deep neural\nnetwork based approach will greatly benefit from the large sample size in both feature learning and classification tasks."}, {"heading": "7. Conclusions and Future Work", "text": "This paper proposed an off-line approach to spatially encode the temporal patterns for classification using convolutional neural networks. We created a pipeline for converting trajectory and time series data into novel representations, GAF and MTF images, and extracted high-level features from these using CNNs. The features were subsequently used for classification. We demonstrated that our approach yields competitive results when compared to state-of-the-art methods by searching a relatively small parameter space. We found that GAFMTF multi-channel images are scalable to larger numbers of quasi-orthogonal features that yield more comprehensive images. Our analysis of high-level features learned from CNNs suggested Tiled CNNs work like multi-frequency moving averages that benefit from the 2D temporal dependency that is preserved by the Gramian matrix.\nImportant future work will involve applying our method to massive amounts of data and searching in a more complete parameter space to solve real world problems. We are also quite interested in how different deep learning architectures perform on the GAF and MTF images generated from large datasets. Another interesting future direction is to model time series through GAF and MTF images. We aim to apply learned time series models in regression/imputation and anomaly detection tasks. To extend our methods to the streaming data, we suppose to design the online learning approach with recurrent network structures to represent, learn and model temporal data in real-time."}, {"heading": "8. Author Biography", "text": ""}, {"heading": "8.1. Author I", "text": "Zhiguang Wang received his B.S. degree in Mathematics and Applied Mathematics from Fudan University, Shanghai, China, in 2012 and enrolled in the Ph.D. program after then. He received the first prize in Chinese National Mathematical Olympiad in Senior in 2007 and NSF travel award for IJCAI in 2015. He is currently a Ph.D. Candidate in the Department of Computer Science and Electrical Engineering at the University of Maryland Baltimore County. His research interests are in the areas of artificial intelligence, machine learning theory, deep neural networks, non-convex optimization with emphasis on mathematical modeling, time series analysis and pattern recognition in streaming data."}, {"heading": "8.2. Author II", "text": "Tim Oates is a Professor of Computer Science at the University of Maryland Baltimore County. He received B.S. degrees in Computer Science and Electrical Engineering from North Carolina State University in 1989, and M.S. and Ph.D. degrees from the University of Massachusetts Amherst in 1997 and 2000, respectively. Prior to coming to UMBC in the Fall of 2001, he spent a year as a postdoc in the Artificial Intelligence Lab at the Massachusetts Institute of Technology. In 2004 Dr. Oates won a prestigious\nNSF CAREER award. He is an author or co-author of more than 190 peer reviewed papers and is a member of the Association for Computing Machinery and the Association for the Advancement of Artificial Intelligence. His research interests include pattern discovery in time series, grammatical inference, graph mining, statistical natural language processing, robotics, and language acquisition."}], "references": [{"title": "Robust text-independent speaker identification using gaussian mixture speaker models", "author": ["D.A. Reynolds", "R.C. Rose"], "venue": "Speech and Audio Processing, 27  IEEE Transactions on 3 (1) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models", "author": ["C.J. Leggetter", "P.C. Woodland"], "venue": "Computer Speech & Language 9 (2) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation 18 (7) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Acoustic modeling using deep belief networks, Audio, Speech, and Language Processing, IEEE Transactions on", "author": ["A.-r. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "G", "author": ["G. Hinton", "L. Deng", "D. Yu"], "venue": "E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al., Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups, Signal Processing Magazine, IEEE 29 (6) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["L. Deng", "G. Hinton", "B. Kingsbury"], "venue": "in: Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "J", "author": ["L. Deng", "J. Li", "J.-T. Huang", "K. Yao", "D. Yu", "F. Seide", "M. Seltzer", "G. Zweig", "X. He"], "venue": "Williams, et al., Recent advances in deep learning for speech research at microsoft, in: Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86 (11) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Receptive fields", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "binocular interaction and functional architecture in the cat\u2019s visual cortex, The Journal of physiology 160 (1) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1962}, {"title": "Face recognition: A convolutional neural-network approach", "author": ["S. Lawrence", "C.L. Giles", "A.C. Tsoi", "A.D. Back"], "venue": "Neural Networks, IEEE Transactions on 8 (1) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional networks and applications in vision", "author": ["Y. LeCun", "K. Kavukcuoglu", "C. Farabet"], "venue": "in: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "The Journal of Machine Learning Research 11 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y.L. Cun"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Tiled convolutional neural networks", "author": ["J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "Q.V. Le", "A.Y. Ng"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition, in: Acoustics", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "G. Penn"], "venue": "Speech and Signal Processing (ICASSP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion", "author": ["L. Deng", "O. Abdel-Hamid", "D. Yu"], "venue": "in: Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "D", "author": ["O. Abdel-Hamid", "L. Deng"], "venue": "Yu, Exploring convolutional neural network structures and optimization techniques for speech recognition., in: INTER- SPEECH", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Time series classification using multi-channels deep convolutional neural networks", "author": ["Y. Zheng", "Q. Liu", "E. Chen", "Y. Ge", "J.L. Zhao"], "venue": "in: Web-Age Information Management, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Perceptual linear predictive (plp) analysis of speech", "author": ["H. Hermansky"], "venue": "the Journal of the Acoustical Society of America 87 (4) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1990}, {"title": "Exploiting representational diversity for time series classification", "author": ["T. Oates", "C.F. Mackenzie", "D.M. Stein", "L.G. Stansbury", "J. Dubose", "B. Aarabi", "P.F. Hu"], "venue": "in: Machine Learning and Applications (ICMLA), 2012 11th International Conference on, Vol. 2, IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient similarity search in sequence databases", "author": ["R. Agrawal", "C. Faloutsos", "A. Swami"], "venue": "Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1993}, {"title": "Haar wavelets for efficient similarity search of time-series: with and without time warping", "author": ["F.-P. Chan", "A.-C. Fu", "C. Yu"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 15 (3) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficiently supporting ad hoc queries in large datasets of time sequences", "author": ["F. Korn", "H.V. Jagadish", "C. Faloutsos"], "venue": "ACM SIGMOD Record 26 (2) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Distance measures for effective clustering of arima time-series", "author": ["K. Kalpakis", "D. Gada", "V. Puttagunta"], "venue": "in: Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference on, IEEE", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "A hidden markov model-based approach to sequential data clustering", "author": ["A. Panuccio", "M. Bicego", "V. Murino"], "venue": "in: Structural, Syntactic, and Statistical Pattern Recognition, Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Recurrence networksa novel paradigm for nonlinear time series analysis", "author": ["R.V. Donner", "Y. Zou", "J.F. Donges", "N. Marwan", "J. Kurths"], "venue": "New Journal of Physics 12 (3) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrence-based time series analysis by means of complex network methods", "author": ["R.V. Donner", "M. Small", "J.F. Donges", "N. Marwan", "Y. Zou", "R. Xiang", "J. Kurths"], "venue": "International Journal of Bifurcation and Chaos 21 (04) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Time series classification using compression distance of recurrence plots", "author": ["D.F. Silva", "V. Souza", "M. De", "G.E. Batista"], "venue": "in: Data Mining (ICDM), 2013 IEEE 13th International Conference on, IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Duality between time series and networks", "author": ["A.S. Campanharo", "M.I. Sirer", "R.D. Malmgren", "F.M. Ramos", "L.A.N. Amaral"], "venue": "PloS one 6 (8) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Scaling up dynamic time warping for datamining applications", "author": ["E.J. Keogh", "M.J. Pazzani"], "venue": "in: Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, ACM", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "H", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici"], "venue": "Larochelle, et al., Greedy layerwise training of deep networks, Advances in neural information processing systems 19 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Rotation-invariant similarity in time series using bag-of-patterns representation", "author": ["J. Lin", "R. Khade", "Y. Li"], "venue": "Journal of Intelligent Information Systems 39 (2) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "The Journal of Machine Learning Research 9 ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast shapelets: A scalable algorithm for discovering time series shapelets", "author": ["T. Rakthanmanon", "E. Keogh"], "venue": "in: Proceedings of the thirteenth SIAM conference on data mining (SDM), SIAM", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Sax-vsm: Interpretable time series classification using sax and vector space model", "author": ["P. Senin", "S. Malinchik"], "venue": "in: Data Mining (ICDM), 2013 IEEE 13th International Conference on, IEEE", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Traclass: trajectory classification using hierarchical region-based and trajectory-based clustering", "author": ["J.-G. Lee", "J. Han", "X. Li", "H. Gonzalez"], "venue": "Proceedings of the VLDB Endowment 1 (1) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Ueber die stetige abbildung einer line auf ein fl\u00e4chenst\u00fcck", "author": ["D. Hilbert"], "venue": "Mathematische Annalen 38 (3) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1891}, {"title": "Analysis of the clustering properties of the hilbert space-filling curve", "author": ["B. Moon", "H.V. Jagadish", "C. Faloutsos", "J.H. Saltz"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 13 (1) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2001}, {"title": "Outlier mining in large high-dimensional data sets", "author": ["F. Angiulli", "C. Pizzuti"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 17 (2) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient maintenance of continuous queries for trajectories", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann"], "venue": "GeoInformatica 12 (3) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Preventing\u201d overfitting\u201d of cross-validation data", "author": ["A.Y. Ng"], "venue": "in: ICML, Vol. 97", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1997}, {"title": "Automatic early stopping using cross validation: quantifying the criteria", "author": ["L. Prechelt"], "venue": "Neural Networks 11 (4) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Researchers have achieved success using combinations of HMMs with acoustic models based on Gaussian Mixture models (GMMs) [1, 2].", "startOffset": 122, "endOffset": 128}, {"referenceID": 1, "context": "Researchers have achieved success using combinations of HMMs with acoustic models based on Gaussian Mixture models (GMMs) [1, 2].", "startOffset": 122, "endOffset": 128}, {"referenceID": 2, "context": "Deep learning has become increasingly popular since the introduction of effective ways to train multiple hidden layers [3] and has been proposed as a replacement for GMMs to model acoustic data in speech recognition tasks [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "Deep learning has become increasingly popular since the introduction of effective ways to train multiple hidden layers [3] and has been proposed as a replacement for GMMs to model acoustic data in speech recognition tasks [4].", "startOffset": 222, "endOffset": 225}, {"referenceID": 4, "context": "These Deep Neural Network - Hidden Markov Model hybrid systems (DNN-HMM) achieved remarkable performance in a variety of speech recognition tasks [5, 6, 7].", "startOffset": 146, "endOffset": 155}, {"referenceID": 5, "context": "These Deep Neural Network - Hidden Markov Model hybrid systems (DNN-HMM) achieved remarkable performance in a variety of speech recognition tasks [5, 6, 7].", "startOffset": 146, "endOffset": 155}, {"referenceID": 6, "context": "These Deep Neural Network - Hidden Markov Model hybrid systems (DNN-HMM) achieved remarkable performance in a variety of speech recognition tasks [5, 6, 7].", "startOffset": 146, "endOffset": 155}, {"referenceID": 7, "context": "Another deep learning architecture used in computer vision is convolutional neural networks (CNNs) [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "CNNs exploit translational invariance within their structures by extracting features through receptive fields [9] and learn with weight sharing.", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "CNNs are the state-of-the-art approach in various image recognition and computer vision tasks [10, 11, 12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 10, "context": "CNNs are the state-of-the-art approach in various image recognition and computer vision tasks [10, 11, 12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 11, "context": "CNNs are the state-of-the-art approach in various image recognition and computer vision tasks [10, 11, 12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 12, "context": "Since unsupervised pretraining has been shown to improve performance [13], sparse coding and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariances [14, 15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "Since unsupervised pretraining has been shown to improve performance [13], sparse coding and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariances [14, 15].", "startOffset": 253, "endOffset": 261}, {"referenceID": 14, "context": "Since unsupervised pretraining has been shown to improve performance [13], sparse coding and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariances [14, 15].", "startOffset": 253, "endOffset": 261}, {"referenceID": 15, "context": "Recently, CNNs have been shown to further improve hybrid model performance by applying convolution and max-pooling in the frequency domain on the TIMIT phone recognition task [17].", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "A heterogeneous pooling approach proved to be beneficial for training acoustic invariance [18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "Further exploration with limited weight sharing and a weighted softmax pooling layer has been proposed to optimize CNN structures for speech recognition tasks [19].", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": "[20] explores supervised feature learning with CNNs to classify multi-channel time series with two datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Unlike speech recognition systems in which acoustic/speech data input is typically represented by concatenating Mel-frequency cepstral coefficients (MFCCs) or perceptual linear predictive coefficient (PLPs) [21], typical time series data are not likely to benefit from transformations applied to speech or acoustic data.", "startOffset": 207, "endOffset": 211}, {"referenceID": 14, "context": "We applied deep Convolutional Neural Networks with a pretraining stage that exploits local orthogonality by Topographic ICA [15] to \u201cvisually\u201d inspect and classify time series.", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "In time series learning problems, non-data adaptive models, such as Discrete Fourier Transformation (DFT) [23], Discrete Wavelet Transformation (DWT) [24], and Discrete Cosine Transformation (DCT) [25], compute the transformation with an algorithm that is invariant with respect to the data to capture the intrinsic temporal correlation with the different basis functions.", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "In time series learning problems, non-data adaptive models, such as Discrete Fourier Transformation (DFT) [23], Discrete Wavelet Transformation (DWT) [24], and Discrete Cosine Transformation (DCT) [25], compute the transformation with an algorithm that is invariant with respect to the data to capture the intrinsic temporal correlation with the different basis functions.", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "In time series learning problems, non-data adaptive models, such as Discrete Fourier Transformation (DFT) [23], Discrete Wavelet Transformation (DWT) [24], and Discrete Cosine Transformation (DCT) [25], compute the transformation with an algorithm that is invariant with respect to the data to capture the intrinsic temporal correlation with the different basis functions.", "startOffset": 197, "endOffset": 201}, {"referenceID": 24, "context": "Meanwhile, researchers explored in the model-based approaches to model time series, such as Auto-Regressive Moving Average models (ARMA) [26] and Hidden Markov Models (HMMs) [27], in which the underlying data is assumed to fit a specific type of model to explicitly function the temporal patterns.", "startOffset": 137, "endOffset": 141}, {"referenceID": 25, "context": "Meanwhile, researchers explored in the model-based approaches to model time series, such as Auto-Regressive Moving Average models (ARMA) [26] and Hidden Markov Models (HMMs) [27], in which the underlying data is assumed to fit a specific type of model to explicitly function the temporal patterns.", "startOffset": 174, "endOffset": 178}, {"referenceID": 26, "context": "tems [28, 29].", "startOffset": 5, "endOffset": 13}, {"referenceID": 27, "context": "tems [28, 29].", "startOffset": 5, "endOffset": 13}, {"referenceID": 28, "context": "extended the recurrence plot paradigm for time series classification using compression distance [30].", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "Another way to build a weighted adjacency matrix is extracting transition dynamics from the first order Markov matrix [31].", "startOffset": 118, "endOffset": 122}, {"referenceID": 29, "context": "Inspired by previous work on the duality between time series and complex networks [31], the main idea of the second framework, the Markov Transition Field (MTF), is to build the Markov matrix of quantile bins after discretization and encode the dynamic transition probability in a quasi-Gramian matrix.", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": ", xn} of n real-valued observations, we rescale X so that all values fall in the interval [\u22121, 1] or [0, 1] by:", "startOffset": 101, "endOffset": 107}, {"referenceID": 30, "context": "To reduce the size of the GAF images, we apply Piecewise Aggregate Approximation [32] to smooth the time series while keeping the overall trends.", "startOffset": 81, "endOffset": 85}, {"referenceID": 29, "context": "We propose a framework that is similar to [31] for encoding dynamical transition statistics.", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "Tiled Convolutional Neural Networks [15] are a variation of Convolutional Neural Networks.", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "Algorithm 1 Unsupervised pretraining with TICA [15] Require: {x}t=1, v, s,W, V as input Ensure: W as output repeat f = \u2211T t=1 \u2211m i=1 \u221a\u2211m k=1 Vik( \u2211n j=1Wkjx (t) j ) 2, g = f old \u2202W , f new = +\u221e, \u03b1 = 1 while f > f do W = W \u2212 \u03b1g W = Localize(W, s) W = tieWeights(W, k) W = orthogonalizeLocalRF (W) W = tieWeights(W, k) f = \u2211T t=1 \u2211m i=1 \u221a\u2211m k=1 Vik( \u2211n j=1Wkjx (t) j ) 2", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "Other unsupervised feature learning algorithms such as RBMs and autoencoders [33] require more parameter tuning, especially during optimization.", "startOffset": 77, "endOffset": 81}, {"referenceID": 14, "context": "In [15], the authors empirically demonstrate that tiled CNNs perform well with limited labeled data because the partial weight tying requires fewer parameters and reduces the need for a large amount of labeled data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "Our experimental settings follow the default deep network structures and parameters in [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "1 with the state-of-the-art SAX-BoP approach [35, 22].", "startOffset": 45, "endOffset": 53}, {"referenceID": 20, "context": "1 with the state-of-the-art SAX-BoP approach [35, 22].", "startOffset": 45, "endOffset": 53}, {"referenceID": 33, "context": "At the last layer of the Tiled CNN, we use a linear soft margin SVM [36] and select C by 5-fold cross validation over {10\u22124, 10\u22123, .", "startOffset": 68, "endOffset": 72}, {"referenceID": 34, "context": "Table 3 compares the classification error rate of our approach with previously published results of five competing methods: two state-of-the-art 1NN classifiers based on Euclidean distance and DTW, the recently proposed Fast-Shapelets based classifier [37], the classifier based on Bag-of-Patterns (BoP) [35, 22] and the most recent SAX-VSM approach [38].", "startOffset": 252, "endOffset": 256}, {"referenceID": 32, "context": "Table 3 compares the classification error rate of our approach with previously published results of five competing methods: two state-of-the-art 1NN classifiers based on Euclidean distance and DTW, the recently proposed Fast-Shapelets based classifier [37], the classifier based on Bag-of-Patterns (BoP) [35, 22] and the most recent SAX-VSM approach [38].", "startOffset": 304, "endOffset": 312}, {"referenceID": 20, "context": "Table 3 compares the classification error rate of our approach with previously published results of five competing methods: two state-of-the-art 1NN classifiers based on Euclidean distance and DTW, the recently proposed Fast-Shapelets based classifier [37], the classifier based on Bag-of-Patterns (BoP) [35, 22] and the most recent SAX-VSM approach [38].", "startOffset": 304, "endOffset": 312}, {"referenceID": 35, "context": "Table 3 compares the classification error rate of our approach with previously published results of five competing methods: two state-of-the-art 1NN classifiers based on Euclidean distance and DTW, the recently proposed Fast-Shapelets based classifier [37], the classifier based on Bag-of-Patterns (BoP) [35, 22] and the most recent SAX-VSM approach [38].", "startOffset": 350, "endOffset": 354}, {"referenceID": 36, "context": "To compare our results with other benchmark approaches including the seminal work from [39], we run experiments on two benchmark datasets, the animal movement dataset (Animal) and the hurricane track dataset (Hurricane) (Figure 8).", "startOffset": 87, "endOffset": 91}, {"referenceID": 36, "context": "Figure 8: Overview of the trajectory and the RB-TB features [39] learnt in (a).", "startOffset": 60, "endOffset": 64}, {"referenceID": 37, "context": "in 1891 [40].", "startOffset": 8, "endOffset": 12}, {"referenceID": 38, "context": "Recently, filling curve based approaches have shown to be able to preserve locality between objects in the multidimensional space in the linear space, and thus have been applied to different tasks like clustering [41], high dimensional outlier detection [42], and trajectory query [43] and classification [44].", "startOffset": 213, "endOffset": 217}, {"referenceID": 39, "context": "Recently, filling curve based approaches have shown to be able to preserve locality between objects in the multidimensional space in the linear space, and thus have been applied to different tasks like clustering [41], high dimensional outlier detection [42], and trajectory query [43] and classification [44].", "startOffset": 254, "endOffset": 258}, {"referenceID": 40, "context": "Recently, filling curve based approaches have shown to be able to preserve locality between objects in the multidimensional space in the linear space, and thus have been applied to different tasks like clustering [41], high dimensional outlier detection [42], and trajectory query [43] and classification [44].", "startOffset": 281, "endOffset": 285}, {"referenceID": 2, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 1, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 1, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 1, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 6, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 6, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 7, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 10, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 12, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 12, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 1, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 0, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 0, "context": "The final time series generated after SFC transformation is T = [0, 3, 2, 2, 2, 7, 7, 8, 11, 13, 13, 2, 1, 1].", "startOffset": 64, "endOffset": 109}, {"referenceID": 36, "context": "Both \u2019Animal\u2019 and \u2019Hurricane\u2019 datasets have been used in previous research [39, 44] to achieve state-of-the-art classification accuracy.", "startOffset": 75, "endOffset": 83}, {"referenceID": 41, "context": "Previous work has discussed overfitting during cross validation and proposed potential techniques to address this problem [45, 46].", "startOffset": 122, "endOffset": 130}, {"referenceID": 42, "context": "Previous work has discussed overfitting during cross validation and proposed potential techniques to address this problem [45, 46].", "startOffset": 122, "endOffset": 130}], "year": 2015, "abstractText": "We propose an off-line approach to explicitly encode temporal patterns spatially as different types of images, namely, Gramian Angular Fields and Markov Transition Fields. This enables the use of techniques from computer vision for feature learning and classification. We used Tiled Convolutional Neural Networks to learn high-level features from individual GAF, MTF, and GAF-MTF images on 12 benchmark time series datasets and two real spatial-temporal trajectory datasets. The classification results of our approach are competitive with state-of-the-art approaches on both types of data. An analysis of the features and weights learned by the CNNs explains why the approach works.", "creator": "LaTeX with hyperref package"}}}