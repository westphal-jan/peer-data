{"id": "1509.07728", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2015", "title": "Online Stochastic Linear Optimization under One-bit Feedback", "abstract": "In unfortunately news, but study without an yakuza drawn on online optimisation non-linear bayesian, where all one - bit of address now by to, learner at besides draw. This serious has from more installing while publishers ticket each applications reviewed. We satisfy the executables stimulation present a infinite spectral cost took however substituent version, and facilitate to inflict their acknowledge exists by the existence linear function. Although making existing method for riemann polynomial bandit all be specifically to something problem, present high computational amount makes even laughably for one - world particularly. To address all challenge, we affect an efficient online experience recursive by exploiting particular layers of the aerial vehicle. Specifically, 'd adopt content Newton efforts to forecasts the one orthogonal included fraction just tight enthusiasm city based made for cycles concavity of the logistic significant. Our precise short question the proposed algorithm vitality a contrary safely of $ O (mcconnell \\ sqrt {T} ) $, first beating the empirical result of quantum geometry preying.", "histories": [["v1", "Fri, 25 Sep 2015 14:05:09 GMT  (21kb)", "http://arxiv.org/abs/1509.07728v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lijun zhang 0005", "tianbao yang", "rong jin", "yichi xiao", "zhi-hua zhou"], "accepted": true, "id": "1509.07728"}, "pdf": {"name": "1509.07728.pdf", "metadata": {"source": "CRF", "title": "Online Stochastic Linear Optimization under One-bit Feedback", "authors": ["Lijun Zhang", "Tianbao Yang", "Rong Jin"], "emails": ["zhanglj@lamda.nju.edu.cn", "tianbao-yang@uiowa.edu", "rongjin@cse.msu.edu", "zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n07 72\n8v 1\n[ cs\n\u221a T ), which\nmatches the optimal result of stochastic linear bandits.\nKeywords: bandit, online, regret bound, stochastic linear optimization, logit model"}, {"heading": "1. Introduction", "text": "Online learning with bandit feedback plays an important role in several industrial domains, such as ad placement, website optimization, and packet routing (Bubeck and Cesa-Bianchi, 2012). A canonical framework for studying this problem is the multi-armed bandits (MAB), which models the situation that a gambler must choose which of K slot machines to play (Robbins, 1952). In the basic stochastic MAB, each arm is assumed to deliver rewards that are drawn from a fixed but unknown distribution. The goal of the gambler is to minimize the regret, namely the difference between his expected cumulative reward and that of the best single arm in hindsight (Auer et al., 2002). Although MAB is a powerful framework for modeling online decision problems, it becomes intractable when the number of arms is very large or even infinite. To address this challenge, various algorithms have been designed to exploit different structure properties of the reward function, such as\nLipschitz (Kleinberg et al., 2008) and convex (Flaxman et al., 2005; Agarwal et al., 2013). Among them, stochastic linear bandits (SLB) has received considerable attentions during the past decade (Auer, 2002; Dani et al., 2008a; Abbasi-yadkori et al., 2011). In each round of SLB, the learner is asked to choose an action xt from a decision set D \u2208 Rd, then he observes yt such that E[yt|xt] = x\u22a4t w\u2217, (1) where w\u2217 \u2208 Rd is a vector of unknown parameters. The goal of learner is to minimize the (pseudo) regret\nT max x\u2208D\nx\u22a4w\u2217 \u2212 T\u2211\nt=1\nx\u22a4t w\u2217. (2)\nIn this paper, we consider a special bandit setting of online linear optimization where the feedback yt only contains one-bit of information. In particular, yt \u2208 {\u00b11}. Our setting is motivated from the fact that in many real-world applications, such as online advertising and recommender systems, user feedback (e.g., click or not) is usually binary. Since the feedback is binary-valued, we assume it is generated according to the logit model (Hastie et al., 2009), i.e.,\nPr[yt = \u00b11|xt] = 1\n1 + exp(\u2212ytx\u22a4t w\u2217) =\nexp(ytx \u22a4 t w\u2217)\n1 + exp(ytx\u22a4t w\u2217) . (3)\nWithout loss of generality, suppose 1 is the preferred outcome. Then, it is natural to define the regret in terms of the expected times that 1 is observed, i.e.,\nT max x\u2208D exp(x\u22a4w\u2217) 1 + exp(x\u22a4w\u2217) \u2212 T\u2211\nt=1\nexp(x\u22a4t w\u2217)\n1 + exp(x\u22a4t w\u2217) . (4)\nThe observation model in (3) and the nonlinear regret in (4) can be treated as a special case of the Generalized Linear Bandit (GLB) (Filippi et al., 2010). However, the existing algorithm for GLB is inefficient in the sense that: i) it is not a truly online algorithm since the whole learning history is stored in memory and used to estimate w\u2217; and ii) it is limited to the case that the number of arms is finite because an upper bound for each arm needs to be calculated explicitly in each round.\nThe main contribution of this paper is an efficient online learning algorithm that effectively exploits particular structures of the logit model. Based on the analytical properties of the logistic function, we first show that the linear regret defined in (2) and the nonlinear regret in (4) only differs by a constant factor, and then focus on minimizing the former one due to its simplicity. Similar to previous studies (Bubeck and Cesa-Bianchi, 2012), we follow the principle of \u201coptimism in face of uncertainty\u201d to deal with the exploration-exploitation dilemma. The basic idea is to maintain a confidence region for w\u2217, and choose an estimate from the confidence region and an action so that the linear reward is maximized. Thus, the problem reduces to the construction of the confidence region from one-bit feedback that satisfies (3). Based on the exponential concavity of the logistic loss, we propose to use a variant of the online Newton step (Hazan et al., 2007) to find the center of the confidence region and derive its width by a rather technical analysis of the updating rule. Theoretical analysis shows that our algorithm achieves a regret bound of \u00d8(d \u221a T ),1 which matches the\n1. We use the \u00d8 notation to hide constant factors as well as polylogarithmic factors in d and T .\nresult for SLB (Dani et al., 2008a). Furthermore, we provide several strategies to reduce the computational cost of the proposed algorithm."}, {"heading": "2. Related Work", "text": "The stochastic multi-armed bandits (MAB) (Robbins, 1952), has become the canonical formalism for studying the problem of decision-making under uncertainty. A long line of successive problems have been extensively studied in statistics (Berry and Fristedt, 1985) and computer science (Bubeck and Cesa-Bianchi, 2012)."}, {"heading": "2.1 Stochastic Multi-armed Bandits (MAB)", "text": "In their seminal paper, Lai and Robbins (1985) establish an asymptotic lower bound of O(K log T ) for the expected cumulative regret over T periods, under the assumption that the expected rewards of the best and second best arms are well-separated. By making use of upper confidence bounds (UCB), they further construct policies which achieve the lower bound asymptotically. However, this initial algorithm is quite involved, because the computation of UCB relies on the entire sequence of rewards obtained so far. To address this limitation, Agrawal (1995) introduces a family of simpler policies that only needs to calculate the sample mean of rewards, and the regret retains the optimal logarithmic behavior. A finite time analysis of stochastic MAB is conducted by Auer et al. (2002). In particular, they propose a UCB-type algorithm based on the Chernoff-Hoeffding bound, and demonstrate it achieves the optimal logarithmic regret uniformly over time."}, {"heading": "2.2 Stochastic Linear Bandits (SLB)", "text": "SLB is first studied by Auer (2002), who considers the case D is finite. Although an elegant UCB-type algorithm named LinRel is developed, he fails to bound its regret due to independence issues. Instead, he designs a complicated master algorithm which uses LinRel as a subroutine, and achieves a regret bound of \u00d8((log |D|)3/2 \u221a Td), where |D| is the number of feasible decisions. In a subsequent work, Dani et al. (2008a) generalize LinRel slightly so that it can be applied in settings where D may be infinite. They refer to the new algorithm as ConfidenceBall2, and show it enjoys a bound of \u00d8(d \u221a T ), which does not depend on the cardinality of D. Later, Abbasi-yadkori et al. (2011) improve the theoretical analysis of ConfidenceBall2 by employing tools from the self-normalized processes. Specifically, the worst case bound is improved by a logarithmic factor and the constant is improved.\n2.3 ConfidenceBall2\nTo facilitate comparisons, we give a brief description of the ConfidenceBall2 algorithm (Dani et al., 2008a). In each round, the algorithm maintains a confidence region Ct such that with a high probability w\u2217 \u2208 Ct. Then, the algorithm finds the greedy optimistic decision\nxt = argmax x\u2208D max w\u2208Ct\nx\u22a4w.\nAfter submitting xt to the oracle, the algorithm receives yt that satisfies (1). Given the past action-feedback pairs (x1, y1), . . . (xt, yt), the confidence region Ct+1 is constructed as\nfollows. The center of Ct+1 is found by minimizing the \u21132-regularized square loss, i.e.,\nwt+1 = argmax w\nt\u2211\ni=1\n(x\u22a4i w \u2212 yi)2 + \u03bb\u2016w\u201622.\nNotice that wt+1 can be computed efficiently in an online fashion. Let At+1 = \u03bbI +\u2211t i=1 xix \u22a4 i . Based on the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011), the width of Ct+1 can be characterized by\n(w \u2212wt+1)\u22a4At+1(w \u2212wt+1) \u2264 \u03b4t+1\nfor some constant \u03b4t+1 > 0. As can be seen, the above procedure for constructing the confidence region is specially designed for the observation model in (1), and thus cannot be applied to the model in (3)."}, {"heading": "2.4 Generalized Linear Bandit (GLB)", "text": "Filippi et al. (2010) extend SLB to the nonlinear case based on the Generalized Linear Model framework of statistics. In the so-called GLB model, yt is assumed to satisfy E[yt|xt] = \u00b5(x\u22a4t w\u2217) where \u00b5 : R 7\u2192 R is certain link function. The regret is also defined in terms of \u00b5(\u00b7) and given by\nT max x\u2208D\n\u00b5(x\u22a4w\u2217)\u2212 T\u2211\nt=1\n\u00b5(x\u22a4t w\u2217). (5)\nNote that by setting \u00b5(x) = exp(x)/[1 + exp(x)], the problem considered in this paper becomes a special case of GLB. A UCB-type algorithm has been proposed for GLB and also achieves a regret bound of \u00d8(d \u221a T ). Different from ConfidenceBall2 which constructs a confidence region in the parameter space, the algorithm of Filippi et al. (2010) operates only in the reward space. However, the space and time complexities of that algorithm in the t-th iteration are O(t) and O(t+ |D|), respectively. The O(t) factor comes from the fact it needs to store the past action-feedback pairs (x1, y1), . . . (xt\u22121, yt\u22121) and use all of them to estimate w\u2217. The O(|D|) factor is due to the fact it needs to calculate an upper bound for each arm in order to decide the next action xt."}, {"heading": "2.5 Adversarial Setting", "text": "All the results mentioned above are under the stochastic setting, where the reward of each arm is generated from a unknown but fixed distribution. A more general setting is the adversarial case, in which the reward from each arm may change arbitrary (Bubeck and Cesa-Bianchi, 2012). The most well-known method for the adversarial multi-armed bandits is the Exp3 algorithm, which achieves a regret bound of \u00d8( \u221a KT ) (Auer et al., 2003). The problem of adversarial linear bandits has been extensively studied, and the start-of-the-art regret bound is \u00d8(poly(d) \u221a T ) (Dani et al., 2008b; Abernethy et al., 2008; Bubeck et al., 2012). For more results, please refer to Bubeck and Cesa-Bianchi (2012), Shamir (2013) and references therein."}, {"heading": "2.6 Bandit Learning with One-bit Feedback", "text": "There are several new variants of bandit learning that also rely on one one-bit feedback, such as multi-class bandits (Kakade et al., 2008; Chen et al., 2014) and K-armed dueling bandits (Yue et al., 2009; Ailon et al., 2014). For example, in multi-class bandits, the feedback is whether the predicted label is correct or not, and in K-armed dueling bandits, the feedback is the comparison between the rewards from two arms. However, none of them are designed for online linear optimization."}, {"heading": "2.7 One-bit Compressive Sensing (CS)", "text": "Finally, we would like to discuss one closely related work in signal processing\u2014one-bit Compressive Sensing (CS) (Boufounos and Baraniuk, 2008; Plan and Vershynin, 2013). One-bit CS aims to recover a sparse vectors w\u2217 from a set of one-bit measurements {yi} where yi is generated from x\u22a4i w\u2217 according to certain observation model such as (3). The main difference is that one-bit CS is studied in batch setting with the goal to minimize the recovery error, while our problem is studied in online setting with the goal to minimize the regret.\n3. Online Learning for Logit Model (OL2M)\nWe first describe the proposed algorithm for online stochastic linear optimization given onebit feedback, next compare it with existing methods, then state its theoretical guarantees, and finally discuss implementation issues."}, {"heading": "3.1 The Algorithm", "text": "For a positive definite matrix A \u2208 Rd\u00d7d, the weighted \u21132-norm is defined by \u2016x\u20162A = x\u22a4Ax. Without loss of generality, we assume the decision space D is contained in the unit ball, that is,\n\u2016x\u20162 \u2264 1, \u2200x \u2208 D. (6)\nWe further assume the \u21132-norm of w\u2217 is upper bounded by some constant R, which is known to the learner. Our first observation is that the linear regret in (2) and the nonlinear regret in (4) only differs by a constant factor as indicated below.\nLemma 1 We have 1\n2(1 + exp(R)) (2) \u2264 (4) \u2264 1 4 (2) (7)\nIn the following, we will develop an efficient algorithm that minimizes the linear regret, which in turn minimizes the nonlinear regret as well.\nThe algorithm is motivated as follows. Suppose actions x1, . . . ,xt have been submitted to the oracle, and let y1, . . . , yt be the one-bit feedback from the oracle. To approximate w\u2217, the most straightforward way is to find the maximum likelihood estimator by solving the following logistic regression problem\nmin \u2016w\u20162\u2264R\n1\nt\nt\u2211\ni=1\nlog ( 1 + exp(\u2212yix\u22a4i w) ) .\nAlgorithm 1 Online Learning for Logit Model (OL2M)\n1: Input: Step Size \u03b7, Regularization Parameter \u03bb 2: Z1 = \u03bbI, w1 = 0 3: for t = 1, 2, . . . do 4:\n(xt, w\u0302t) = argmax x\u2208D,w\u2208Ct\nx\u22a4w\n5: Submit xt and observe yt \u2208 {\u00b11} 6: Solve the optimization problem in (8) to find wt+1 7: end for\nHowever, this approach does not scale well since it requires the leaner to store the entire learning history. Instead, we propose an online algorithm to find an approximate solution. The key observation is that the logistic loss\nft(w) = log ( 1 + exp(\u2212ytx\u22a4t w) )\nis exponentially concave over bounded domain (Hazan et al., 2014), which motives us to apply a variant of the online Newton step (Hazan et al., 2007). Specifically, we propose to find an approximate solution wt+1 by solving the following problem\nmin \u2016w\u20162\u2264R \u2016w \u2212wt\u20162Zt+1 2 + \u03b7(w \u2212wt)\u22a4\u2207ft(wt) (8)\nwhere \u03b7 > 0 is the step size,\nZt+1 = Zt + \u03b7\u03b2\n2 xtx\n\u22a4 t , (9)\nand \u03b2 is defined in (14). Although our updating rule is similar to the method in (Hazan et al., 2007), there also exist some differences. As indicated by (9), in our case xtx \u22a4 t is used to approximate the Hessian matrix, while in Hazan et al. (2007) \u2207ft(wt)[\u2207ft(wt)\u22a4] is used. After a theoretical analysis, we are able to show that with a high probability\nw\u2217 \u2208 Ct+1 = { w : \u2016w \u2212wt+1\u2016Zt+1 \u2264 \u221a \u03b3t+1 } (10)\nwhere the value of \u03b3t+1 is given in (12). Given the confidence region, we adopt the principle of \u201coptimism in face of uncertainty\u201d, and the next action xt+1 is given by\n(xt+1, w\u0302t+1) = argmax x\u2208D,w\u2208Ct+1\nx\u22a4w. (11)\nAt the beginning, we set Z1 = \u03bbI, and w1 = 0.\nThe above procedure is summarized in Algorithm 1, and is refer to as Online Learning for Logit Model (OL2M).\nSince both ConfidenceBall2 (Dani et al., 2008a) and our OL 2M are UCB-type algorithms, their overall frameworks are similar. The main difference lies in the construction\nof the confidence region and the related analysis. While ConfidenceBall2 uses online least square to update the center of the confidence region, OL2M resorts to online Newton step. Due to the difference in the updating rule and the observation model, the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011) can not be applied here.\nAlthough our observation model in (3) can be handled by the Generalized Linear Bandit (GLB) (Filippi et al., 2010), this paper differs from GLB in the following aspects.\n\u2022 To estimate w\u2217, GLB needs to store the learning history and perform batch updating in each round. In contrast, the proposed OL2M performs online updating. \u2022 While GLB only considers a finite number of arms, we allow the number of arms to be infinite. \u2022 Our algorithm follows the learning framework of SLB. Thus, existing techniques for speeding up SLB can also be used to accelerate our algorithm, which is discussed in Section 3.3."}, {"heading": "3.2 Theoretical Guarantees", "text": "The main theoretical contribution of this paper is the following theorem regarding the confidence region of w\u2217 at each round.\nTheorem 1 With a probability at least 1\u2212 \u03b4, we have\n\u2016wt+1 \u2212w\u2217\u2016Zt+1 \u2264 \u221a \u03b3t+1, \u2200t > 0\nwhere\n\u03b3t+1 = 2\u03b7\n[ 4R+ ( 4\n\u03b2 +\n8 3 R\n) \u03c4t + 1\n\u03b2 log\ndet(Zt+1)\ndet(Z1)\n] +max ( \u03bb, \u03b7\u03b2\n2\n) R2, (12)\n\u03c4t = log\n( 2\u23082 log2 t\u2309t2\n\u03b4\n) , (13)\n\u03b2 = 1\n2(1 + exp(R)) . (14)\nThe main idea is to analyze the growth of \u2016wt+1\u2212w\u2217\u20162Zt+1 by exploring the properties of the logistic loss (Lemmas 2 and 4) and concentration inequalities for martingales (Lemma 5). By a simple upper bound of log det(Zt+1)/det(Z1), we can show that the width of the confidence region is O( \u221a d log t).\nCorollary 2 We have\nlog det(Zt+1)\ndet(Z1) \u2264 d log\n( 1 + \u03b7\u03b2t\n2\u03bbd\n)\nand thus\n\u03b3t+1 \u2264 O(d log t), \u2200t > 0.\nBased on Theorem 1, we have the following regret bound for OL2M.\nTheorem 3 With a probability at least 1\u2212 \u03b4, we have\nT max x\u2208D\nx\u22a4w\u2217 \u2212 T\u2211\nt=1\nx\u22a4t w\u2217 \u2264 4 \u221a \u03b3TT\n\u03b7\u03b2 log\ndet(ZT+1)\ndet(Z1)\nholds for all T > 0.\nCombining with the upper bound in Corollary 2, the above theorem implies our algorithm achieves a regret bound of \u00d8(d \u221a T ) which matches the bound for Stochastic Linear Bandits (Dani et al., 2008a)."}, {"heading": "3.3 Implementation Issues", "text": "The main computational cost of OL2M comes from (11) which is NP-hard in general (Dani et al., 2008a). In the following, we discuss several strategies for reducing the computational cost.\nOptimization Over Ball As mentioned by Dani et al. (2008a), in the special case that D is the unit ball, (11) could be solved in time O(poly(d)). Here, we provide an explanation using techniques from convex optimization. To this end, we rewrite the optimization problem in (11) as follows\nmax \u2016x\u20162\u22641,\u2016w\u2212wt+1\u2016Zt+1\u2264 \u221a \u03b3t+1 x\u22a4w = max \u2016w\u2212wt+1\u2016Zt+1\u2264 \u221a \u03b3t+1 \u2016w\u20162\nwhich is equivalent to min\n\u2016w\u2212wt+1\u20162Zt+1\u2264\u03b3t+1 \u2212\u2016w\u201622.\nThe above problem is an optimization problem with a quadratic objective and one quadratic inequality constraint, it is well-known that strong duality holds provided there exists a strictly feasible point (Boyd and Vandenberghe, 2004). Thus, we can solve its dual problem which is convex and given by\nmax \u03b3 s. t. \u03bb \u2265 0[ \u2212I + \u03bbZt+1 \u2212\u03bbZt+1wt+1\n\u2212\u03bbw\u22a4t+1Zt+1 \u03bb(\u2016wt+1\u20162Zt+1 \u2212 \u03b3t+1)\u2212 \u03b3\n] 0\nAfter obtaining the dual solution, we can get the primal solution based on KKT conditions.\nEnlarging the Confidence region For a positive definite matrix A \u2208 Rd\u00d7d, we define\n\u2016x\u20161,A = \u2016A1/2x\u20161.\nWhen studying SLB, Dani et al. (2008a) propose to enlarge the confidence region from Ct+1 = { w : \u2016w \u2212wt+1\u2016Zt+1 \u2264 \u221a \u03b3t+1 } to C\u0303t+1 = { w : \u2016w \u2212wt+1\u20161,Zt+1 \u2264 \u221a d\u03b3t+1 } such that the computational cost could be reduced. This idea can be direct incorporated to our OL2M. Let Et+1 be the set of extremal points of C\u0303t+1. With this modification, (11) becomes\n(xt+1, w\u0302t+1) = argmax x\u2208D,w\u2208C\u0303t+1 x\u22a4w = argmax x\u2208D,w\u2208Et+1 x\u22a4w\nAlgorithm 2 OL2M with Lazy Updating\n1: Input: Step Size \u03b7, Regularization Parameter \u03bb, Constant c 2: Z1 = \u03bbI, w1 = 0, \u03c4 = 1 3: for t = 1, 2, . . . do 4: if det(Zt) > (1 + c) det(V\u03c4 ) then 5:\n(xt, w\u0302t) = argmax x\u2208D,w\u2208Ct\nx\u22a4w\n6: \u03c4 = t 7: end if 8: xt = x\u03c4 9: Submit xt and observe yt \u2208 {\u00b11}\n10: Solve the optimization problem in (8) to find wt+1 11: end for\nwhich means we just need to enumerate over the 2d vertices in Et+1. Following the arguments in Dani et al. (2008a), it is straightforward to show that the regret is only increased by a factor of \u221a d.\nLazy Updating Abbasi-yadkori et al. (2011) propose a lazy updating strategy which only needs to solve (11) O(log T ) times. The key idea is to recompute xt whenever det(Zt) increases by a constant factor (1+c). While the computation cost is saved dramatically, the regret is only increased by a constant factor \u221a 1 + c. We provide the lazy updating version of OL2M in Algorithm 2."}, {"heading": "4. Analysis", "text": "We here present the proofs of main theorems. The omitted proofs are provided in the appendix."}, {"heading": "4.1 Proof of Theorem 1", "text": "We begin with several lemmas that are central to our analysis. Although the application of online Newton step (Hazan et al., 2007) in Algorithm 1 is motivated from the fact that ft(w) is exponentially concave over bounded domain, our analysis is built upon a related but different property that the logistic loss log(1 + exp(x)) is strongly convex over bounded domain, from which we obtain the following lemma.\nLemma 2 Denote the ball of radius R by BR, i.e., BR = {w : \u2016w\u20162 \u2264 R}. The following holds for \u03b2 \u2264 12(1+exp(R)) :\nft(w2) \u2265 ft(w1) + [\u2207ft(w1)]\u22a4(w2 \u2212w1) + \u03b2\n2\n( (w2 \u2212w1)\u22a4xt )2 , \u2200w1,w2 \u2208 BR.\nComparing Lemma 2 with Lemma 3 in (Hazan et al., 2007), we can see that the quadratic term in our inequality does not depends on yt. This independence allows us to simplify the subsequent analysis involving martingales.\nOur second lemma is devoted to analyzing the property of the updating rule in (8).\nLemma 3\n\u3008wt \u2212w\u2217,\u2207ft(wt)\u3009 \u2264 \u2016wt \u2212w\u2217\u20162Zt+1\n2\u03b7 \u2212 \u2016wt+1 \u2212w\u2217\u20162Zt+1 2\u03b7 + \u03b7 2 \u2016\u2207ft(wt)\u20162Z\u22121 t+1 . (15)\nFor each function ft(\u00b7), we denote its conditional expectation over yt by f\u0304t(w), i.e.,\nf\u0304t(w) = Eyt\n[ log ( 1 + exp ( \u2212ytx\u22a4t w ))] . (16)\nBased the property of Kullback\u2013Leibler divergence (Cover and Thomas, 2006), we obtain the following lemma.\nLemma 4 We have f\u0304t(w) \u2265 f\u0304t(w\u2217), \u2200w \u2208 Rd.\nNext, we introduce one inequality for bounding the weighted \u21132-norm of the gradient\n\u2016\u2207ft(w)\u20162A = (\nexp(\u2212ytx\u22a4t w) 1 + exp(\u2212ytx\u22a4t w)\n)2 x\u22a4t Axt \u2264 \u2016xt\u20162A, \u2200A 0, w \u2208 Rd. (17)\nWe continue the proof of Theorem 1 in the following. Our updating rule in (8) ensures \u2016wt\u20162 \u2264 R, \u2200t > 0. Combining with the assumption \u2016w\u2217\u20162 \u2264 R, Lemma 2 implies\nft(wt) \u2264 ft(w\u2217) + [\u2207ft(wt)]\u22a4(wt \u2212w\u2217)\u2212 \u03b2\n2\n( (w\u2217 \u2212wt)\u22a4xt )2 . (18)\nBy taking expectation over yt, (18) becomes\nf\u0304t(wt) \u2264 f\u0304t(w\u2217) + [\u2207f\u0304t(wt)]\u22a4(wt \u2212w\u2217)\u2212 \u03b2\n2\n[( (w\u2217 \u2212wt)\u22a4xt )2] .\nCombining with Lemma 4, we have\n0 \u2264[\u2207f\u0304t(wt)]\u22a4(wt \u2212w\u2217)\u2212 \u03b2\n2\n( (w\u2217 \u2212wt)\u22a4xt )2 \ufe38 \ufe37\ufe37 \ufe38 :=at\n=[\u2207ft(wt)]\u22a4(wt \u2212w\u2217)\u2212 \u03b2 2 at + [\u2207f\u0304t(wt)\u2212\u2207ft(wt)]\u22a4(wt \u2212w\u2217)\ufe38 \ufe37\ufe37 \ufe38\n:=bt\n=[\u2207ft(wt)]\u22a4(wt \u2212w\u2217)\u2212 \u2016wt \u2212w\u2217\u20162Zt+1\n2\u03b7 + \u2016wt \u2212w\u2217\u20162Zt+1 2\u03b7 \u2212 \u03b2 2 at + bt\n(15) \u2264 \u2212 \u2016wt+1 \u2212w\u2217\u20162Zt+1\n2\u03b7 +\n\u03b7 2 \u2016\u2207ft(wt)\u20162Z\u22121 t+1\n+ \u2016wt \u2212w\u2217\u20162Zt+1\n2\u03b7 \u2212 \u03b2 2 at + bt\n(17) \u2264 \u2212 \u2016wt+1 \u2212w\u2217\u20162Zt+1\n2\u03b7 +\n\u03b7 2 \u2016xt\u20162Z\u22121\nt+1\ufe38 \ufe37\ufe37 \ufe38 :=ct\n+ \u2016wt \u2212w\u2217\u20162Zt+1\n2\u03b7 \u2212 \u03b2 2 at + bt\n(9) = \u2212 \u2016wt+1 \u2212w\u2217\u20162Zt+1 2\u03b7 \u2212 \u03b2 2 at + bt + \u03b7 2 ct + \u2016wt \u2212w\u2217\u20162Zt 2\u03b7 + \u03b2 4 ( x\u22a4t (wt \u2212w\u2217) )2\n=\u2212 \u2016wt+1 \u2212w\u2217\u20162Zt+1\n2\u03b7 \u2212 \u03b2 4 at + bt + \u03b7 2 ct + \u2016wt \u2212w\u2217\u20162Zt 2\u03b7 .\nWe thus have\n\u2016wt+1 \u2212w\u2217\u20162Zt+1 \u2264 \u2016wt \u2212w\u2217\u2016 2 Zt \u2212\n\u03b7\u03b2\n2 at + 2\u03b7bt + \u03b7\n2ct\nSumming the above inequality over iterations 1 to t, we obtain\n\u2016wt+1 \u2212w\u2217\u20162Zt+1 + \u03b7\u03b2\n2\nt\u2211\ni=1\nai \u2264 \u03bbR2 + 2\u03b7 t\u2211\ni=1\nbi + \u03b7 2\nt\u2211\ni=1\nci. (19)\nNext, we discuss how to bound the summation of martingale difference sequence \u2211t\ni=1 bi. To this end, we prove the following lemma, which is built up the Bernstein\u2019s inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique (Bartlett et al., 2005).\nLemma 5 With a probability at least 1\u2212 \u03b4, we have\nt\u2211\ni=1\nbi \u2264 4R+ 2 \u221a\u221a\u221a\u221a\u03c4t t\u2211\ni=1\nai + 8\n3 R\u03c4t, \u2200t > 0\nwhere \u03c4t is defined in (13).\nFrom Lemma 5 and the basic inequality\n2 \u221a\u221a\u221a\u221a\u03c4t t\u2211\ni=1\nai \u2264 \u03b2\n4\nt\u2211\ni=1\nai + 4\n\u03b2 \u03c4t,\nwith a probability at least 1\u2212 \u03b4, we have\nt\u2211\ni=1\nbi \u2264 4R+ \u03b2\n4\nt\u2211\ni=1\nai +\n( 4\n\u03b2 +\n8 3 R\n) \u03c4t (20)\nholds for all t > 0. Substituting (20) into (19), we obtain\n\u2016wt+1 \u2212w\u2217\u20162Zt+1 \u2264 \u03bbR 2 + 2\u03b7\n[ 4R + ( 4\n\u03b2 +\n8 3 R\n) \u03c4t ] + \u03b72 t\u2211\ni=1\nci. (21)\nFinally, we show an upper bound for \u2211t\ni=1 ci, which is a direct consequence of Lemma 12 in Hazan et al. (2007).\nLemma 6 We have t\u2211\ni=1\n\u2016xi\u20162Z\u22121 i+1 \u2264 2 \u03b7\u03b2 log det(Zt+1) det(Z1) .\nWe complete the proof by combining (21) with the above lemma."}, {"heading": "4.2 Proof of Lemma 2", "text": "We first show that the one-dimensional logistic loss \u2113(x) = log(1 + exp(\u2212x)) is 12(1+exp(R)) - strongly convex over domain [\u2212R,R]. It is easy to verify that \u2200x \u2208 [\u2212R,R],\n\u2113\u2032\u2032(x) = exp(x) (1 + exp(x))2 \u2265 1 2(1 + exp(R))\nimplying the strongly convexity of \u2113(\u00b7). From the property of strongly convex, for any a, b \u2208 [\u2212R,R] we have\n\u2113(b) \u2265 \u2113(a) + \u2113\u2032(a)(b\u2212 a) + \u03b2 2 (b\u2212 a)2. (22)\nNotice that for any w1,w2 \u2208 BR, we have\nytx \u22a4 t w1, ytx \u22a4 t w2 \u2208 [\u2212R,R],\nsince yt \u2208 {\u00b11} and \u2016xt\u20162 \u2264 1. Substituting a = ytx\u22a4t w1 and b = ytx\u22a4t w2 into (22), we have\n\u2113(ytx \u22a4 t w2) \u2265 \u2113(ytx\u22a4t w1) +\n\u03b2 2 (ytx \u22a4 t w2 \u2212 ytx\u22a4t w1)2 + \u2113\u2032(ytx\u22a4t w1)(ytx\u22a4t w2 \u2212 ytx\u22a4t w1).\nWe complete the proof by noticing\nft(w1) = \u2113(ytx \u22a4 t w1), ft(w2) = \u2113(ytx \u22a4 t w2), and \u2207ft(w1) = \u2113\u2032(ytx\u22a4t w1)ytxt."}, {"heading": "4.3 Proof of Lemma 3", "text": "Lemma 3 follows from a more general result stated below.\nLemma 7 Let M be a positive definite matrix, and\ny = argmin w\u2208W \u03b7\u3008w,g\u3009 + 1 2 \u2016w \u2212 x\u20162M ,\nwhere W is a convex set. Then for all w \u2208 W, we have\n\u3008x\u2212w,g\u3009 \u2264 \u2016x\u2212w\u2016 2 M \u2212 \u2016y \u2212w\u20162M 2\u03b7 + \u03b7 2 \u2016g\u20162M\u22121 .\nProof Since y is the optimal solution to the optimization problem, from the first-order optimality condition (Boyd and Vandenberghe, 2004), we have\n\u3008\u03b7g +M(y \u2212 x),w \u2212 y\u3009 \u2265 0, \u2200w \u2208 W. (23)\nBased on the above inequality, we have\n\u2016x\u2212w\u20162M \u2212 \u2016y \u2212w\u20162M =x\u22a4Mx\u2212 y\u22a4My + 2\u3008M(y \u2212 x),w\u3009\n(23)\n\u2265 x\u22a4Mx\u2212 y\u22a4My + 2\u3008M(y \u2212 x),y\u3009 \u2212 2\u3008\u03b7g,w \u2212 y\u3009 =\u2016y \u2212 x\u20162M + 2\u3008\u03b7g,y \u2212 x+ x\u2212w\u3009 =2\u3008\u03b7g,x \u2212w\u3009+ \u2016y \u2212 x\u20162M + 2\u3008\u03b7g,y \u2212 x\u3009\nCombining with the following inequality\n\u2016y \u2212 x\u20162M + 2\u3008\u03b7g,y \u2212 x\u3009 \u2265 min w \u2016w\u20162M + 2\u3008\u03b7g,w\u3009 = \u2212\u03b72\u2016g\u20162M\u22121 ,\nwe have \u2016x\u2212w\u20162M \u2212 \u2016y \u2212w\u20162M \u2265 2\u3008\u03b7g,x \u2212w\u3009 \u2212 \u03b72\u2016g\u20162M\u22121 ."}, {"heading": "4.4 Proof of Lemma 4", "text": "For each w \u2208 Rd, we introduce a discrete probability distribution pw over {\u00b11} such that\npw(i) = 1\n1 + exp(\u2212ix\u22a4t w) , i \u2208 {\u00b11}.\nThen, it is easy to verify that\nf\u0304t(w) = \u2212 \u2211\ni\u2208{\u00b11} pw\u2217(i) log pw(i).\nAs a result\nf\u0304t(w)\u2212 f\u0304t(w\u2217) = \u2211\ni\u2208{\u00b11} pw\u2217(i) log pw\u2217(i)\u2212\n\u2211\ni\u2208{\u00b11} pw\u2217(i) log pw(i)\n= \u2211\ni\u2208{\u00b11} pw\u2217(i) log\npw\u2217(i)\npw(i) = DKL(pw\u2217\u2016pw) \u2265 0\nwhereDKL(\u00b7\u2016\u00b7) is the Kullback\u2013Leibler divergence between two distributions (Cover and Thomas, 2006)."}, {"heading": "4.5 Proof of Lemma 5", "text": "We need the Bernstein\u2019s inequality for martingales (Cesa-Bianchi and Lugosi, 2006), which is provided in Appendix D. Form our definition of f\u0304i(\u00b7) in (16), it is clear bi = [\u2207f\u0304i(wi)\u2212\u2207fi(wi)]\u22a4(wi \u2212w\u2217) is a martingale difference sequence. Furthermore, |bi| \u2264 \u2223\u2223\u2223[\u2207f\u0304i(wi)]\u22a4(wi \u2212w\u2217) \u2223\u2223\u2223+ \u2223\u2223\u2223[\u2207fi(wi)]\u22a4(wi \u2212w\u2217) \u2223\u2223\u2223 \u2264 2|x\u22a4i (wi\u2212w\u2217)| \u2264 2\u2016wi\u2212w\u2217\u20162 \u2264 4R. Define the martingale Bt = \u2211t i=1 bi. Define the conditional variance \u03a3 2 t as\n\u03a32t =\nt\u2211\ni=1\nEyi [( [\u2207f\u0304i(wi)\u2212\u2207fi(wi)]\u22a4(wi \u2212w\u2217) )2]\n\u2264 t\u2211\ni=1\nEyi [( \u2207fi(wi)\u22a4(wi \u2212w\u2217) )2] \u2264 t\u2211\ni=1\n( x\u22a4i (wi \u2212w\u2217) )2 \ufe38 \ufe37\ufe37 \ufe38 :=At ,\nwhere the first inequality is due to the fact that E[(\u03be \u2212 E[\u03be])2] \u2264 E[\u03be2] for any random variable \u03be.\nIn the following, we consider two different scenarios, i.e., At \u2264 4R 2 t and At > 4R2 t .\nAt \u2264 4R 2\nt In this case, we have\nBt \u2264 t\u2211\ni=1\n|bi| \u2264 2 t\u2211\ni=1\n|x\u22a4i (wi \u2212w\u2217)| \u2264 2\n\u221a\u221a\u221a\u221at t\u2211\ni=1\n( x\u22a4i (wi \u2212w\u2217) )2 \u2264 4R. (24)\nAt > 4R2 t Since At in the upper bound for \u03a3 2 t is a random variable, we cannot apply Bernstein\u2019s inequality directly. To address this issue, we make use of the peeling process (Bartlett et al., 2005). Note that we have both a lower bound and an upper bound for At, i.e., 4R2/t < At \u2264 4R2t. Then,\nPr [ Bt \u2265 2 \u221a At\u03c4t + 8\n3 R\u03c4t\n]\n=Pr [ Bt \u2265 2 \u221a At\u03c4t + 8\n3 R\u03c4t,\n4R2\nt < At \u2264 4R2t\n]\n=Pr [ Bt \u2265 2 \u221a At\u03c4t + 8\n3 R\u03c4t,\u03a3\n2 t \u2264 At,\n4R2\nt < At \u2264 4R2t\n]\n\u2264 m\u2211\ni=1\nPr [ Bt \u2265 2 \u221a At\u03c4t + 8\n3 R\u03c4t,\u03a3\n2 t \u2264 At,\n4R22i\u22121\nt < At \u2264\n4R22i\nt\n]\n\u2264 m\u2211\ni=1\nPr [ Bt \u2265 \u221a 2 4R22i\nt \u03c4t +\n8 3 R\u03c4t,\u03a3 2 t \u2264\n4R22i\nt\n] \u2264 me\u2212\u03c4t ,\nwhere m = \u23082 log2 t\u2309, and the last step follows the Bernstein\u2019s inequality for martingales. By setting \u03c4t = log 2mt2 \u03b4 , with a probability at least 1\u2212 \u03b4/[2t2], we have\nBt \u2264 2 \u221a At\u03c4t + 8\n3 R\u03c4t. (25)\nCombining (24) and (25), with a probability at least 1\u2212 \u03b4/[2t2], we have\nBt \u2264 4R+ 2 \u221a At\u03c4t + 8\n3 R\u03c4t.\nWe complete the proof by taking the union bound over t > 0, and using the well-known result \u221e\u2211\nt=1\n1 t2 =\n\u03c02\n6 \u2264 2."}, {"heading": "4.6 Proof of Theorem 3", "text": "The proof is standard and can be found from Dani et al. (2008a) and Abbasi-yadkori et al. (2011). We include it for the sake of completeness.\nLet x\u2217 = argmaxx\u2208D x \u22a4w\u2217. Recall that in each round, we have\n(xt, w\u0302t) = argmax x\u2208D,w\u2208Ct\nx\u22a4w.\nWe decompose the instantaneous regret at round t as follows\nx\u22a4\u2217 w\u2217 \u2212 x\u22a4t w\u2217 \u2264x\u22a4t w\u0302t \u2212 x\u22a4t w\u2217 = x\u22a4t (w\u0302t \u2212wt) + x\u22a4t (wt \u2212w\u2217) \u2264 (\u2016w\u0302t \u2212wt\u2016Zt + \u2016wt \u2212w\u2217\u2016Zt) \u2016xt\u2016Z\u22121\nt \u2264 2\u221a\u03b3t\u2016xt\u2016Z\u22121 t .\nOn the other hand, we always have\nx\u22a4\u2217 w\u2217 \u2212 x\u22a4t w\u2217 \u2264 \u2016x\u2217 \u2212 xt\u20162\u2016w\u2217\u20162 \u2264 2R.\nFrom the definition in (12), we have \u221a\n2 \u03b7\u03b2 \u03b3T \u2265 R. Thus, the total regret can be upper\nbounded by\nT max x\u2208D\nx\u22a4w\u2217 \u2212 T\u2211\nt=1\nx\u22a4t w\u2217\n\u22642 T\u2211\nt=1\nmin (\u221a\n\u03b3t\u2016xt\u2016Z\u22121 t\n, R )\n\u22642 \u221a 2\n\u03b7\u03b2 \u03b3T\nT\u2211\nt=1\nmin\n(\u221a \u03b7\u03b2\n2 \u2016xt\u2016Z\u22121 t , 1\n)\n\u22642 \u221a 2T\n\u03b7\u03b2 \u03b3T\n\u221a\u221a\u221a\u221a T\u2211\nt=1\nmin\n( \u03b7\u03b2\n2 \u2016xt\u20162Z\u22121 t , 1\n) .\nTo proceed, we need the following results from Lemma 11 in Abbasi-yadkori et al. (2011),\nT\u2211\nt=1\nmin\n( \u03b7\u03b2\n2 \u2016xt\u20162Z\u22121 t , 1\n) \u2264 2 T\u2211\nt=1\nlog ( 1 + \u03b7\u03b2\n2 \u2016xt\u20162Z\u22121 t\n)\nand\ndet(ZT+1) = det ( ZT + \u03b7\u03b2\n2 xTx\n\u22a4 T\n)\n=det(ZT ) det\n( I + \u03b7\u03b2\n2 Z\n\u22121/2 T xTx \u22a4 T Z \u22121/2 T\n)\n=det(ZT )\n( 1 + \u03b7\u03b2\n2 \u2016xT \u20162Z\u22121 T\n) = det(Z1) T\u220f\nt=1\n( 1 + \u03b7\u03b2\n2 \u2016xt\u20162Z\u22121 t\n) .\nCombining the above inequations, we have\nT max x\u2208D\nx\u22a4w\u2217 \u2212 T\u2211\nt=1\nx\u22a4t w\u2217 \u2264 4 \u221a \u03b3TT\n\u03b7\u03b2 log\ndet(ZT+1)\ndet(Z1) ."}, {"heading": "5. Conclusions", "text": "In this paper, we consider the problem of online linear optimization under one-bit feedback. Under the assumption that the binary feedback is generated from the logit model, we develop a variant of the online Newton step to approximate the unknown vector, and discuss how to construct the confidence region theoretically. Given the confidence region, we choose the action that produces maximal reward in each round. Theoretical analysis reveals that our algorithm achieves a regret bound of \u00d8(d \u221a T ).\nThe current algorithm assumes that the one-bit feedback is generated from a logit model. In contrast, a much broader class of observation models are allowed in one-bit compressive sensing (Plan and Vershynin, 2013), as long as there is a positive correlation between the one-bit output and the real-valued measurement. In the future, we will investigate how to extend our algorithm to other observation models. Another direction is to consider the adversary setting where the unknown vector w\u2217 may change from time to time."}, {"heading": "Appendix A. Proof of Lemma 1", "text": "Let \u00b5(x) = exp(x)1+exp(x) . It is easy to verify that \u2200x \u2208 [\u2212R,R], 1\n2(1 + exp(R)) \u2264 \u00b5\u2032(x) = exp(x) (1 + exp(x))2 \u2264 1 4 (26)\nNote that for any \u2212R \u2264 a \u2264 b \u2264 R, we have\n\u00b5(b) = \u00b5(a) +\n\u222b b\na \u00b5\u2032(x)dx (27)\nCombining (26) with (27), we have\n1 2(1 + exp(R)) (b\u2212 a) \u2264 \u00b5(b)\u2212 \u00b5(a) \u2264 1 4 (b\u2212 a)\nLet\nx\u2217 = argmax x\u2208D x\u22a4w\u2217 = argmax x\u2208D exp(x\u22a4w\u2217) 1 + exp(x\u22a4w\u2217)\nSince \u2212R \u2264 x\u22a4t w\u2217 \u2264 x\u22a4\u2217 w\u2217 \u2264 R, we have 1\n2(1 + exp(R))\n( x\u22a4\u2217 w\u2217 \u2212 x\u22a4t w\u2217 ) \u2264 exp(x \u22a4 \u2217 w\u2217)\n1 + exp(x\u22a4\u2217 w\u2217) \u2212 exp(x\n\u22a4 t w\u2217)\n1 + exp(x\u22a4t w\u2217) \u2264 1 4\n( x\u22a4\u2217 w\u2217 \u2212 x\u22a4t w\u2217 )\nwhich implies (7)."}, {"heading": "Appendix B. Proof of Lemma 6", "text": "We have\n\u2016xi\u20162Z\u22121 i+1\n= 2 \u03b7\u03b2 \u3008Z\u22121i+1, Zi+1 \u2212 Zi\u3009 \u2264 2 \u03b7\u03b2 log det(Zi+1) det(Zi) ,\nwhere the inequality follows from Lemma 12 in Hazan et al. (2007). Thus, we have\nt\u2211\ni=1\n\u2016xi\u20162Z\u22121 i+1 \u2264 2 \u03b7\u03b2\nt\u2211\ni=1\nlog det(Zi+1)\ndet(Zi) =\n2\n\u03b7\u03b2 log\ndet(Zt+1)\ndet(Z1) ."}, {"heading": "Appendix C. Proof of Corollary 2", "text": "Recall that\nZt+1 = Z1 + \u03b7\u03b2\n2\nt\u2211\ni=1\nxtx \u22a4 t\nand \u2016xt\u20162 \u2264 1 for all t > 0. From Lemma 10 of Abbasi-yadkori et al. (2011), we have\ndet(Zt+1) \u2264 ( \u03bb+ \u03b7\u03b2t\n2d\n)d .\nSince det(Z1) = \u03bb d, we have\nlog det(Zt+1)\ndet(Z1) \u2264 d log\n( 1 + \u03b7\u03b2t\n2\u03bbd\n) ."}, {"heading": "Appendix D. Bernstein\u2019s Inequality for Martingales", "text": "Theorem 4 Let X1, . . . ,Xn be a bounded martingale difference sequence with respect to the filtration F = (Fi)1\u2264i\u2264n and with |Xi| \u2264 K. Let\nSi =\ni\u2211\nj=1\nXj\nbe the associated martingale. Denote the sum of the conditional variances by\n\u03a32n = n\u2211\nt=1\nE [ X2t |Ft\u22121 ] .\nThen for all constants t, \u03bd > 0,\nPr [ max\ni=1,...,n Si > t and \u03a3\n2 n \u2264 \u03bd\n] \u2264 exp ( \u2212 t 2\n2(\u03bd +Kt/3)\n) ,\nand therefore,\nPr [ max\ni=1,...,n Si >\n\u221a 2\u03bdt+ 2\n3 Kt and \u03a32n \u2264 \u03bd\n] \u2264 e\u2212t."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Abbasi.yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.yadkori et al\\.", "year": 2011}, {"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "In Proceedings of the 21st Annual Conference on Learning,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Stochastic convex optimization with bandit feedback", "author": ["Alekh Agarwal", "Dean P. Foster", "Daniel Hsu", "Sham M. Kakade", "Alexander Rakhlin"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Agarwal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2013}, {"title": "Sample mean based index policies with O(log n) regret for the multi-armed bandit problem", "author": ["Rajeev Agrawal"], "venue": "Advances in Applied Probability,", "citeRegEx": "Agrawal.,? \\Q1995\\E", "shortCiteRegEx": "Agrawal.", "year": 1995}, {"title": "Reducing dueling bandits to cardinal bandits", "author": ["Nir Ailon", "Zohar Karnin", "Thorsten Joachims"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Ailon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2014}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Auer.,? \\Q2002\\E", "shortCiteRegEx": "Auer.", "year": 2002}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Local rademacher complexities", "author": ["Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "Bandit problems: Sequential Allocation of Experiments", "author": ["Donald A. Berry", "Bert Fristedt"], "venue": "Monographs on Statistics and Applied Probability. Springer Netherlands,", "citeRegEx": "Berry and Fristedt.,? \\Q1985\\E", "shortCiteRegEx": "Berry and Fristedt.", "year": 1985}, {"title": "1-bit compressive sensing", "author": ["Petros T. Boufounos", "Richard G. Baraniuk"], "venue": "In Proceedings of the 42nd Annual Conference on Information Sciences and Systems,", "citeRegEx": "Boufounos and Baraniuk.,? \\Q2008\\E", "shortCiteRegEx": "Boufounos and Baraniuk.", "year": 2008}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi", "Sham M. Kakade"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Boosting with online binary learners for the multiclass bandit problem", "author": ["Shang-Tse Chen", "Hsuan-Tien Lin", "Chi-Jen Lu"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In Proceedings of the 21st Annual Conference on Learning,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Parametric bandits: The generalized linear case", "author": ["Sarah Filippi", "Olivier Cappe", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "Online convex optimization in the bandit setting: Gradient descent without a gradient", "author": ["Abraham D. Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan"], "venue": "In Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "The Elements of Statistical Learning", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Logistic regression: Tight bounds for stochastic and online optimization", "author": ["Elad Hazan", "Tomer Koren", "Kfir Y. Levy"], "venue": "In Proceedings of The 27th Conference on Learning Theory,", "citeRegEx": "Hazan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2014}, {"title": "Efficient bandit algorithms for online multiclass prediction", "author": ["Sham M. Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Kakade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In Proceedings of the 40th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach", "author": ["Yaniv Plan", "Roman Vershynin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Plan and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Plan and Vershynin.", "year": 2013}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Robbins.,? \\Q1952\\E", "shortCiteRegEx": "Robbins.", "year": 1952}, {"title": "On the complexity of bandit and derivative-free stochastic convex optimization", "author": ["Ohad Shamir"], "venue": "In Proceedings of the 26th Conference on Learning Theory,", "citeRegEx": "Shamir.,? \\Q2013\\E", "shortCiteRegEx": "Shamir.", "year": 2013}, {"title": "The K-armed dueling bandits problem", "author": ["Yisong Yue", "Josef Broder", "Robert Kleinberg", "Thorsten Joachims"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory,", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 12, "context": "Introduction Online learning with bandit feedback plays an important role in several industrial domains, such as ad placement, website optimization, and packet routing (Bubeck and Cesa-Bianchi, 2012).", "startOffset": 168, "endOffset": 199}, {"referenceID": 27, "context": "A canonical framework for studying this problem is the multi-armed bandits (MAB), which models the situation that a gambler must choose which of K slot machines to play (Robbins, 1952).", "startOffset": 169, "endOffset": 184}, {"referenceID": 6, "context": "The goal of the gambler is to minimize the regret, namely the difference between his expected cumulative reward and that of the best single arm in hindsight (Auer et al., 2002).", "startOffset": 157, "endOffset": 176}, {"referenceID": 24, "context": "Lipschitz (Kleinberg et al., 2008) and convex (Flaxman et al.", "startOffset": 10, "endOffset": 34}, {"referenceID": 19, "context": ", 2008) and convex (Flaxman et al., 2005; Agarwal et al., 2013).", "startOffset": 19, "endOffset": 63}, {"referenceID": 2, "context": ", 2008) and convex (Flaxman et al., 2005; Agarwal et al., 2013).", "startOffset": 19, "endOffset": 63}, {"referenceID": 5, "context": "Among them, stochastic linear bandits (SLB) has received considerable attentions during the past decade (Auer, 2002; Dani et al., 2008a; Abbasi-yadkori et al., 2011).", "startOffset": 104, "endOffset": 165}, {"referenceID": 0, "context": "Among them, stochastic linear bandits (SLB) has received considerable attentions during the past decade (Auer, 2002; Dani et al., 2008a; Abbasi-yadkori et al., 2011).", "startOffset": 104, "endOffset": 165}, {"referenceID": 20, "context": "Since the feedback is binary-valued, we assume it is generated according to the logit model (Hastie et al., 2009), i.", "startOffset": 92, "endOffset": 113}, {"referenceID": 18, "context": "The observation model in (3) and the nonlinear regret in (4) can be treated as a special case of the Generalized Linear Bandit (GLB) (Filippi et al., 2010).", "startOffset": 133, "endOffset": 155}, {"referenceID": 12, "context": "Similar to previous studies (Bubeck and Cesa-Bianchi, 2012), we follow the principle of \u201coptimism in face of uncertainty\u201d to deal with the exploration-exploitation dilemma.", "startOffset": 28, "endOffset": 59}, {"referenceID": 21, "context": "Based on the exponential concavity of the logistic loss, we propose to use a variant of the online Newton step (Hazan et al., 2007) to find the center of the confidence region and derive its width by a rather technical analysis of the updating rule.", "startOffset": 111, "endOffset": 131}, {"referenceID": 27, "context": "Related Work The stochastic multi-armed bandits (MAB) (Robbins, 1952), has become the canonical formalism for studying the problem of decision-making under uncertainty.", "startOffset": 54, "endOffset": 69}, {"referenceID": 9, "context": "A long line of successive problems have been extensively studied in statistics (Berry and Fristedt, 1985) and computer science (Bubeck and Cesa-Bianchi, 2012).", "startOffset": 79, "endOffset": 105}, {"referenceID": 12, "context": "A long line of successive problems have been extensively studied in statistics (Berry and Fristedt, 1985) and computer science (Bubeck and Cesa-Bianchi, 2012).", "startOffset": 127, "endOffset": 158}, {"referenceID": 21, "context": "1 Stochastic Multi-armed Bandits (MAB) In their seminal paper, Lai and Robbins (1985) establish an asymptotic lower bound of O(K log T ) for the expected cumulative regret over T periods, under the assumption that the expected rewards of the best and second best arms are well-separated.", "startOffset": 63, "endOffset": 86}, {"referenceID": 3, "context": "To address this limitation, Agrawal (1995) introduces a family of simpler policies that only needs to calculate the sample mean of rewards, and the regret retains the optimal logarithmic behavior.", "startOffset": 28, "endOffset": 43}, {"referenceID": 3, "context": "To address this limitation, Agrawal (1995) introduces a family of simpler policies that only needs to calculate the sample mean of rewards, and the regret retains the optimal logarithmic behavior. A finite time analysis of stochastic MAB is conducted by Auer et al. (2002). In particular, they propose a UCB-type algorithm based on the Chernoff-Hoeffding bound, and demonstrate it achieves the optimal logarithmic regret uniformly over time.", "startOffset": 28, "endOffset": 273}, {"referenceID": 4, "context": "2 Stochastic Linear Bandits (SLB) SLB is first studied by Auer (2002), who considers the case D is finite.", "startOffset": 58, "endOffset": 70}, {"referenceID": 4, "context": "2 Stochastic Linear Bandits (SLB) SLB is first studied by Auer (2002), who considers the case D is finite. Although an elegant UCB-type algorithm named LinRel is developed, he fails to bound its regret due to independence issues. Instead, he designs a complicated master algorithm which uses LinRel as a subroutine, and achieves a regret bound of \u00d8((log |D|)3/2 \u221a Td), where |D| is the number of feasible decisions. In a subsequent work, Dani et al. (2008a) generalize LinRel slightly so that it can be applied in settings where D may be infinite.", "startOffset": 58, "endOffset": 458}, {"referenceID": 0, "context": "Later, Abbasi-yadkori et al. (2011) improve the theoretical analysis of ConfidenceBall2 by employing tools from the self-normalized processes.", "startOffset": 7, "endOffset": 36}, {"referenceID": 0, "context": "Based on the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011), the width of Ct+1 can be characterized by", "startOffset": 65, "endOffset": 94}, {"referenceID": 18, "context": "4 Generalized Linear Bandit (GLB) Filippi et al. (2010) extend SLB to the nonlinear case based on the Generalized Linear Model framework of statistics.", "startOffset": 34, "endOffset": 56}, {"referenceID": 18, "context": "Different from ConfidenceBall2 which constructs a confidence region in the parameter space, the algorithm of Filippi et al. (2010) operates only in the reward space.", "startOffset": 109, "endOffset": 131}, {"referenceID": 12, "context": "A more general setting is the adversarial case, in which the reward from each arm may change arbitrary (Bubeck and Cesa-Bianchi, 2012).", "startOffset": 103, "endOffset": 134}, {"referenceID": 7, "context": "The most well-known method for the adversarial multi-armed bandits is the Exp3 algorithm, which achieves a regret bound of \u00d8( \u221a KT ) (Auer et al., 2003).", "startOffset": 133, "endOffset": 152}, {"referenceID": 1, "context": "The problem of adversarial linear bandits has been extensively studied, and the start-of-the-art regret bound is \u00d8(poly(d) \u221a T ) (Dani et al., 2008b; Abernethy et al., 2008; Bubeck et al., 2012).", "startOffset": 129, "endOffset": 194}, {"referenceID": 13, "context": "The problem of adversarial linear bandits has been extensively studied, and the start-of-the-art regret bound is \u00d8(poly(d) \u221a T ) (Dani et al., 2008b; Abernethy et al., 2008; Bubeck et al., 2012).", "startOffset": 129, "endOffset": 194}, {"referenceID": 1, "context": ", 2008b; Abernethy et al., 2008; Bubeck et al., 2012). For more results, please refer to Bubeck and Cesa-Bianchi (2012), Shamir (2013) and references therein.", "startOffset": 9, "endOffset": 120}, {"referenceID": 1, "context": ", 2008b; Abernethy et al., 2008; Bubeck et al., 2012). For more results, please refer to Bubeck and Cesa-Bianchi (2012), Shamir (2013) and references therein.", "startOffset": 9, "endOffset": 135}, {"referenceID": 23, "context": "6 Bandit Learning with One-bit Feedback There are several new variants of bandit learning that also rely on one one-bit feedback, such as multi-class bandits (Kakade et al., 2008; Chen et al., 2014) and K-armed dueling bandits (Yue et al.", "startOffset": 158, "endOffset": 198}, {"referenceID": 15, "context": "6 Bandit Learning with One-bit Feedback There are several new variants of bandit learning that also rely on one one-bit feedback, such as multi-class bandits (Kakade et al., 2008; Chen et al., 2014) and K-armed dueling bandits (Yue et al.", "startOffset": 158, "endOffset": 198}, {"referenceID": 29, "context": ", 2014) and K-armed dueling bandits (Yue et al., 2009; Ailon et al., 2014).", "startOffset": 36, "endOffset": 74}, {"referenceID": 4, "context": ", 2014) and K-armed dueling bandits (Yue et al., 2009; Ailon et al., 2014).", "startOffset": 36, "endOffset": 74}, {"referenceID": 10, "context": "7 One-bit Compressive Sensing (CS) Finally, we would like to discuss one closely related work in signal processing\u2014one-bit Compressive Sensing (CS) (Boufounos and Baraniuk, 2008; Plan and Vershynin, 2013).", "startOffset": 148, "endOffset": 204}, {"referenceID": 26, "context": "7 One-bit Compressive Sensing (CS) Finally, we would like to discuss one closely related work in signal processing\u2014one-bit Compressive Sensing (CS) (Boufounos and Baraniuk, 2008; Plan and Vershynin, 2013).", "startOffset": 148, "endOffset": 204}, {"referenceID": 22, "context": "is exponentially concave over bounded domain (Hazan et al., 2014), which motives us to apply a variant of the online Newton step (Hazan et al.", "startOffset": 45, "endOffset": 65}, {"referenceID": 21, "context": ", 2014), which motives us to apply a variant of the online Newton step (Hazan et al., 2007).", "startOffset": 71, "endOffset": 91}, {"referenceID": 21, "context": "Although our updating rule is similar to the method in (Hazan et al., 2007), there also exist some differences.", "startOffset": 55, "endOffset": 75}, {"referenceID": 21, "context": "Although our updating rule is similar to the method in (Hazan et al., 2007), there also exist some differences. As indicated by (9), in our case xtx \u22a4 t is used to approximate the Hessian matrix, while in Hazan et al. (2007) \u2207ft(wt)[\u2207ft(wt)\u22a4] is used.", "startOffset": 56, "endOffset": 225}, {"referenceID": 0, "context": "Due to the difference in the updating rule and the observation model, the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011) can not be applied here.", "startOffset": 126, "endOffset": 155}, {"referenceID": 18, "context": "Although our observation model in (3) can be handled by the Generalized Linear Bandit (GLB) (Filippi et al., 2010), this paper differs from GLB in the following aspects.", "startOffset": 92, "endOffset": 114}, {"referenceID": 16, "context": "3 Implementation Issues The main computational cost of OL2M comes from (11) which is NP-hard in general (Dani et al., 2008a). In the following, we discuss several strategies for reducing the computational cost. Optimization Over Ball As mentioned by Dani et al. (2008a), in the special case that D is the unit ball, (11) could be solved in time O(poly(d)).", "startOffset": 105, "endOffset": 270}, {"referenceID": 11, "context": "The above problem is an optimization problem with a quadratic objective and one quadratic inequality constraint, it is well-known that strong duality holds provided there exists a strictly feasible point (Boyd and Vandenberghe, 2004).", "startOffset": 204, "endOffset": 233}, {"referenceID": 16, "context": "When studying SLB, Dani et al. (2008a) propose to enlarge the confidence region from Ct+1 = { w : \u2016w \u2212wt+1\u2016Zt+1 \u2264 \u221a \u03b3t+1 } to C\u0303t+1 = { w : \u2016w \u2212wt+1\u20161,Zt+1 \u2264 \u221a d\u03b3t+1 } such that the computational cost could be reduced.", "startOffset": 19, "endOffset": 39}, {"referenceID": 15, "context": "Following the arguments in Dani et al. (2008a), it is straightforward to show that the regret is only increased by a factor of \u221a d.", "startOffset": 27, "endOffset": 47}, {"referenceID": 0, "context": "Lazy Updating Abbasi-yadkori et al. (2011) propose a lazy updating strategy which only needs to solve (11) O(log T ) times.", "startOffset": 14, "endOffset": 43}, {"referenceID": 21, "context": "Although the application of online Newton step (Hazan et al., 2007) in Algorithm 1 is motivated from the fact that ft(w) is exponentially concave over bounded domain, our analysis is built upon a related but different property that the logistic loss log(1 + exp(x)) is strongly convex over bounded domain, from which we obtain the following lemma.", "startOffset": 47, "endOffset": 67}, {"referenceID": 21, "context": "Comparing Lemma 2 with Lemma 3 in (Hazan et al., 2007), we can see that the quadratic term in our inequality does not depends on yt.", "startOffset": 34, "endOffset": 54}, {"referenceID": 14, "context": "To this end, we prove the following lemma, which is built up the Bernstein\u2019s inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique (Bartlett et al.", "startOffset": 104, "endOffset": 135}, {"referenceID": 8, "context": "To this end, we prove the following lemma, which is built up the Bernstein\u2019s inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique (Bartlett et al., 2005).", "startOffset": 162, "endOffset": 185}, {"referenceID": 21, "context": "Finally, we show an upper bound for \u2211t i=1 ci, which is a direct consequence of Lemma 12 in Hazan et al. (2007). Lemma 6 We have t \u2211", "startOffset": 92, "endOffset": 112}, {"referenceID": 11, "context": "Proof Since y is the optimal solution to the optimization problem, from the first-order optimality condition (Boyd and Vandenberghe, 2004), we have \u3008\u03b7g +M(y \u2212 x),w \u2212 y\u3009 \u2265 0, \u2200w \u2208 W.", "startOffset": 109, "endOffset": 138}, {"referenceID": 14, "context": "5 Proof of Lemma 5 We need the Bernstein\u2019s inequality for martingales (Cesa-Bianchi and Lugosi, 2006), which is provided in Appendix D.", "startOffset": 70, "endOffset": 101}, {"referenceID": 8, "context": "To address this issue, we make use of the peeling process (Bartlett et al., 2005).", "startOffset": 58, "endOffset": 81}, {"referenceID": 15, "context": "6 Proof of Theorem 3 The proof is standard and can be found from Dani et al. (2008a) and Abbasi-yadkori et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 0, "context": "(2008a) and Abbasi-yadkori et al. (2011). We include it for the sake of completeness.", "startOffset": 12, "endOffset": 41}, {"referenceID": 0, "context": "To proceed, we need the following results from Lemma 11 in Abbasi-yadkori et al. (2011),", "startOffset": 59, "endOffset": 88}, {"referenceID": 26, "context": "In contrast, a much broader class of observation models are allowed in one-bit compressive sensing (Plan and Vershynin, 2013), as long as there is a positive correlation between the one-bit output and the real-valued measurement.", "startOffset": 99, "endOffset": 125}, {"referenceID": 21, "context": "Proof of Lemma 6 We have \u2016xi\u20162Z\u22121 i+1 = 2 \u03b7\u03b2 \u3008Z\u22121 i+1, Zi+1 \u2212 Zi\u3009 \u2264 2 \u03b7\u03b2 log det(Zi+1) det(Zi) , where the inequality follows from Lemma 12 in Hazan et al. (2007). Thus, we have t \u2211", "startOffset": 143, "endOffset": 163}, {"referenceID": 0, "context": "From Lemma 10 of Abbasi-yadkori et al. (2011), we have det(Zt+1) \u2264 ( \u03bb+ \u03b7\u03b2t 2d )d .", "startOffset": 17, "endOffset": 46}], "year": 2015, "abstractText": "In this paper, we study a special bandit setting of online stochastic linear optimization, where only one-bit of information is revealed to the learner at each round. This problem has found many applications including online advertisement and online recommendation. We assume the binary feedback is a random variable generated from the logit model, and aim to minimize the regret defined by the unknown linear function. Although the existing method for generalized linear bandit can be applied to our problem, the high computational cost makes it impractical for real-world problems. To address this challenge, we develop an efficient online learning algorithm by exploiting particular structures of the observation model. Specifically, we adopt online Newton step to estimate the unknown parameter and derive a tight confidence region based on the exponential concavity of the logistic loss. Our analysis shows that the proposed algorithm achieves a regret bound of \u00d8(d \u221a T ), which matches the optimal result of stochastic linear bandits.", "creator": "LaTeX with hyperref package"}}}