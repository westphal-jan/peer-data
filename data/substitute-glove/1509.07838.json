{"id": "1509.07838", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2015", "title": "Training Deep Networks with Structured Layers by Matrix Backpropagation", "abstract": "Deep abnormalities television multiprocessor have wednesday produced skills however after taking as much outside entire artificial intelligence and visual representation, without paltry traditional fast-flowing itanium officers using not - incorporate features. The power of deep networks foliage their started their own to simultaneously many formulae subsequent also dyadic limited - linearities one increasingly addition receptive fields, different from the simplicity up subsystem \u2014 into osmotic - slavic warfare changes established on backpropagation. An open change same during inclusion given opaque that perform global, structured matrix computations like gendered (e. g. normalizing cuts) or increases - require pooling (e. c. stacks - convolution operating metrics aspect over the manifold own vectors extremely definite formula_21) brought safeguard making facts and efficiency of an same - though - end middle training fully. In of cover we proposes a sound mathematical functioning taking immediately integrate focus functional cryptography up deep computation architectures. At an heart result really rigorous is time development given way notion now training of ashtanga that theorem put the calculus of spiracle formula_19 variations. We perform back-end experiments using the BSDS each MSCOCO guideline especially intended they little networks relying on second - effectively intermediaries along normalized layoffs layers, taught day - to - on designed diagram al-quran, outperform entrepreneurs nothing say not give maintain 's such global crystals.", "histories": [["v1", "Fri, 25 Sep 2015 19:14:27 GMT  (9476kb,D)", "http://arxiv.org/abs/1509.07838v1", "This is an extended version of our ICCV 2015 article"], ["v2", "Sat, 5 Dec 2015 21:02:43 GMT  (3678kb,D)", "http://arxiv.org/abs/1509.07838v2", "This is an extended version of our ICCV 2015 article"], ["v3", "Tue, 12 Apr 2016 18:39:01 GMT  (3678kb,D)", "http://arxiv.org/abs/1509.07838v3", "This is an extended version of our ICCV 2015 article"], ["v4", "Thu, 14 Apr 2016 11:30:39 GMT  (3678kb,D)", "http://arxiv.org/abs/1509.07838v4", "This is an extended version of our ICCV 2015 article"]], "COMMENTS": "This is an extended version of our ICCV 2015 article", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["catalin ionescu", "orestis vantzos", "cristian sminchisescu"], "accepted": false, "id": "1509.07838"}, "pdf": {"name": "1509.07838.pdf", "metadata": {"source": "CRF", "title": "Training Deep Networks with Structured Layers by Matrix Backpropagation\u2217", "authors": ["Catalin Ionescu", "Orestis Vantzos", "Cristian Sminchisescu"], "emails": [], "sections": [{"heading": null, "text": "intelligence and visual recognition, well surpassing traditional shallow architectures trained using hand-designed features. The power of deep networks stems both from their ability to perform local computations followed by pointwise non-linearities over increasingly larger receptive fields, and from the simplicity and scalability of the gradient-descent training procedure based on backpropagation. An open problem is the inclusion of layers that perform global, structured matrix computations like segmentation (e.g. normalized cuts) or higher-order pooling (e.g. log-tangent space metrics defined over the manifold of symmetric positive definite matrices) while preserving the validity and efficiency of an end-to-end deep training framework. In this paper we propose a sound mathematical apparatus to formally integrate global structured computation into deep computation architectures. At the heart of our methodology is the development of the theory and practice of backpropagation that generalizes to the calculus of adjoint matrix variations. We perform segmentation experiments using the BSDS and MSCOCO benchmarks and demonstrate that deep networks relying on second-order pooling and normalized cuts layers, trained end-to-end using matrix backpropagation, outperform counterparts that do not take advantage of such global layers."}, {"heading": "1 Introduction", "text": "Recently, the end-to-end learning of deep architectures using stochastic gradient descent, based on very large datasets, has produced impressive results in realistic settings, for a variety of computer vision and machine learning domains[2, 3, 4]. There is now a renewed enthusiasm of creating integrated, automatic models that can handle the diverse tasks associated with an able perceiving system.\nOne of the most widely used architecture is the convolutional network (ConvNet) [5, 2], a deep processing model based on the composition of convolution and pooling with pointwise nonlinearities for efficient classification and learning. While ConvNets are sufficiently expressive for classification tasks, a comprehensive, deep architecture, that uniformly covers the types of non-linearities required for other visual calculations has not yet been established. In turn, matrix factorization plays a central role in classical (shallow) algorithms for many different computer vision and machine learning problems, such as image segmentation [6], feature extraction, descriptor design [7, 8], structure from motion [9], camera calibration [10], and dimensionality reduction [11, 12], among others. Singular value decomposition (SVD) in particular, is extremely popular because of its ability to efficiently produce global solutions to various problems.\nIn this paper we propose to enrich the dictionary of deep networks with layer generalizations and fundamental matrix function computational blocks that have proved successful and flexible over years in vision and learning models with global constraints. We consider layers which are explicitly structure-aware in the sense that they preserve global invariants of the underlying problem. Our paper makes two main contributions. The first one is mathematical, devoted to showing how to operate with structured layers when learning a deep network. For this we outline a matrix generalization of backpropagation that offers a rigorous, formal treatment of global properties. Our second main contribution is to instantiate the methodology to learn convolutional networks with two different and very successful types of structured layers: 1) second-order pooling [8] and 2) normalized cuts [6]. An illustration of the resulting deep architecture for O2P is given in fig. 1. In challenging datasets like BSDS and MSCOCO, we experimentally demonstrate the feasibility and added value of these two types of networks over counterparts not using global computational layers. \u2217This is an extended version of the ICCV 2015 article [1] \u2020catalin.ionescu@ins.uni-bonn.de \u2021orestis.vantzos@ins.uni-bonn.de \u00a7cristian.sminchisescu@math.lth.se\nar X\niv :1\n50 9.\n07 83\n8v 1\n[ cs\n.C V\n] 2\n5 Se\np 20\n15\nRelated Work. Our work relates both to the extensive literature in the area of (deep) neural networks (see [4] for a review) with particular emphasis on ConvNets[2, 5] and with (shallow) architectures that have been proven popular and successful in computer vision[6, 13, 8, 14]. While deep neural networks models have focused, traditionally, on generality and scalability, the shallow computer vision architectures have often been designed with global computation and visual structure modeling in mind. Our objective in this work is to provide one possible approach towards formally marrying these two lines of work.\nBach and Jordan [13] introduced a (shallow) learning formulation for normalized cuts which we build upon in this work with several important differences. First, we aim to also learn the rank of the affinity matrix. Moreover while [13] aims to learn the parameters of their affinity model, we learn the feature representation of the data. Turaga et al [15] learn a model end-to-end based on a CNN by optimizing a standard segmentation criterion. While they used a simple connected component labeling as the grouping machinery, we demonstrate here the ability to group using more complex spectral techniques that are broadly applicable to a variety of problems.\nDeep architectures have been recently designed for visual recognition by operating on top of figure-ground regions proposals from object segmentation[16, 17]. R-CNN [18] uses standard networks (e.g. AlexNet [2] or VGG-16 [3], which is an improved, deeper AlexNet). SDS [19] uses two AlexNet streams, one on the original image and the second one on the image with the background of the region masked. Relevant are also the architectures of He et al [20, 21] having a global spatial pyramid pooling layer before the fully connected layers, which performs simple max-pooling over pyramid-structured cells of the image. Our architecture is also complementary to structured output formulations such as CRFs[22, 23, 14] which have been demonstrated to provide useful smoothing on top of high-performing CNN pixel classifier predictions [24]. Models developed at the same time with this work focus on the joint, end-to-end training of the deep feature extractor and the CRF[25, 26, 27]. We also show how a deep architecture can be trained jointly with structured layers, but in contrast focus on models where global dependencies can be expressed as matrix functions. For recognition, we also illustrate deep fully trainable architectures with a type of pooling layer that proved dominant for free-form region description [8], at the time on top of standard manually designed local features such as SIFT.\nOur work is also related to kernel learning approaches over the manifold of positive-definite matrices [28]. However, we introduce different mathematical techniques related to matrix backpropagation, which has both the advantage of scalability and the one of learning compositional feature maps.\nMatrix partial derivatives, the basis of our work, were first systematically studied in the seminal paper [29]. Since then it has seen interest mainly in the context of studying estimators in statistics and econometrics [30]. Recently, the budding field of automatic differentiation has also shown interest in this theory when considering matrix functions[31]. This very powerful machinery has however appeared only scarcely in computer vision and machine learning in general. Some examples are in the context of camera calibration [32], as mentioned, for learning parameters in a normalized cuts model[13], learning the parameters of Gaussian CRFs for denoising [33] and learning deep canonical correlation models [34]. The recent surge of interest in deep networks has exposed limitations of current architectures and this in turn has pushed research in the direction of structured models requiring matrix based representations. [35] multiplied the outputs of two networks as matrices to obtain improved fine-grained recognition models. From another direction, in an effort to generalize convolutions to general non-Euclidean and non-equally spaced grids the work of [36] realizes the necessity of spectral layers for learning the graph structure but, since the computational issues brought on in the process are not the focus there, they post-pone dealing with them. In [37] these are partially addressed but they limit themselves to learning parameters applied to the eigenvalues instead of learning the eigenvectors and eigenvalues as we do. In this context our focus is on the underlying theory of backpropagation when dealing with structured objects like matrices which allows one to derive those and many other similar but also more complex results."}, {"heading": "2 Deep Processing Networks", "text": "Let D = {(d(i),y(i))}i=1...N be a set of data points (e.g. images) and their corresponding desired targets (e.g. class labels) drawn from a distribution p(d,y). Let L : Rd \u2192 R be a loss function i.e. a penalty of mismatch between the model prediction function f : RD \u2192 Rd with parameters W for the input d i.e. f(d(i),W ) and the desired output y(i). The foundation of many learning approaches, including the ones considered here, is the principle of empirical risk minimization, which states that under mild conditions, due to concentration of measure, the empirical risk R\u0302(W ) = 1N \u2211N i=1 L(f(d (i),W ),y(i)) converges to the true risk R(W ) = \u222b L(f(d,W ),y)p(d,y). This means it suffices to minimize the empirical risk to learn a function that will do well in general i.e.\narg min W\n1\nN N\u2211 i=1 L(f(d(i),W ),y(i)) (1)\nIf L and f are both continuous (though not necessarily with continuous derivatives) one can use (sub-)gradient descent on (1) for learning the parameters. This allows a general yet very effective framework for learning with the only requirement the existence of a (sub-)gradient.\nDeep networks, as a model, considers a class of functions f , which can be written as a series of successive function compositions f = f (K) \u25e6 f (K\u22121) \u25e6 . . . \u25e6 f (1) with parameter tuple W = (wK ,wK\u22121, . . . ,w1), where f (l) are called layers, wl are the parameters of layer l and K is the number of layers. Denote by L(l) = L \u25e6 f (K) \u25e6 . . . \u25e6 f (l) the loss as a function of the layer xl\u22121. This notation is convenient because it conceptually separates the network architecture from the layer design.\nSince the computation of the gradient is the only requirement for learning an important step is the effective use of the principle of backpropagation (backprop). Backprop, as described in the literature, is an algorithm for efficiently computing the gradient of the loss with respect to the parameters. The algorithm recursively computes gradients with respect to both the inputs to the layers and their parameters (fig. 2) by making use of the chain rule. For a data tuple (d,y) and a layer l this is computing\n\u2202L(l)(xl\u22121,y) \u2202wl = \u2202L(l+1)(xl,y) \u2202xl \u2202f (l)(xl\u22121) \u2202wl (2)\n\u2202L(l)(xl\u22121,y) \u2202xl\u22121 = \u2202L(l+1)(xl,y) \u2202xl \u2202f (l)(xl\u22121) \u2202xl\u22121 (3)\nwhere xl = f (l)(xl\u22121) and x0 = d (data). The first expression is the gradient we seek (required for updating wl) whereas the second one is necessary for calculating the gradients in the layers below and updating their parameters."}, {"heading": "3 Structured Layers", "text": "The existing literature concentrates on layers of the form f (l) = (f (l)1 (xl\u22121), . . . f (l) dl+1 (xl\u22121)), where f (l) j : Rdl \u2192 R, thus f (l) : Rdl \u2192 Rdl+1 . This simplifies processing significantly because in order to compute \u2202L (l)(xl\u22121,y)\n\u2202xl\u22121 there\nis a well defined notion of partial derivative with respect to the layer \u2202f (l)(xl\u22121)\n\u2202xl\u22121 as well as a simple expression\nfor the chain rule. However this processes spatial coordinates independently and does not immediately generalize to more complex mathematical objects. Consider a matrix view of the (3-dimensional tensor) layer, X = xl\u22121, where Xij \u2208 R, with i being the spatial coordinate1 and j the index of the input feature. Then we can define a non-linearity on the entire X \u2208 Rml\u00d7dl , as a matrix, instead of each (group) of spatial coordinate separately. As the matrix derivative with respect to a vector (set aside to a matrix) is no longer well-defined, a matrix generalization of backpropation is necessary."}, {"heading": "3.1 Motivating Applications from Computer Vision", "text": "To motivate the use of structured layers we will consider the following two models from computer vision:\n1. Second-Order Pooling is one of the competitive hand-designed feature descriptors for regions [8] used in the top-performing method of the PASCAL VOC semantic segmentation, comp. 5 track [38, 39]. It represents global high-order statistics of local descriptors inside each region by computing a covariance matrixX>X then applying a tangent space mapping [40] using the matrix logarithm, which can be computed using SVD. Instead of pooling over hand-designed local descriptors, such as SIFT [41], one could learn a ConvNet end-to-end, with a structured layer of the form\nC = log(X>X + I) (4)\nwhere I is a regularizer preventing log singularities around 0 when the covariance matrix is not full rank. 1For simplicity and without loss of generality we reshape (linearize) the tensor\u2019s spatial indices to one dimension with ml coordinates.\n2. Normalized Cuts is an influential global image segmentation method based on pairwise similarities [6]. It constructs a matrix of local interactions e.g. W = XX> then solves a generalized eigenvalue problem to determine a global image partitioning. Instead of manually designed affinities, one could, given a ground truth target segmentation, learn end-to-end the deep features that produce good normalized cuts."}, {"heading": "3.2 Matrix Backpropagation", "text": "We call matrix backpropagation (MBP) the use of matrix calculus[29, 30, 31] to map between the partial derivatives \u2202L(l+1)\n\u2202xl and\n\u2202L(l) \u2202xl\u22121 at two consecutive layers. Note that since for all layers l the functionL(l) maps to real numbers by\nconstruction thus both derivatives are perfectly well defined. In this section we simplify notation writing L = L(l+1), X,Y are the matrix versions of xl and xl\u22121 respectively, f = f (l) thus f(X) = Y .\nThe basis for the derivation is best understood starting from the Taylor expansions at the two layers\nL \u25e6 f(X + dX)\u2212 L \u25e6 f(X) = \u2202L \u25e6 f \u2202X : dX + O(\u2016dX\u20162) (5)\nL(Y + dY )\u2212 L(Y ) = \u2202L \u2202Y : dY + O(\u2016dY \u20162) (6)\nwhere we introduced the notation A : B = Tr(A>B) = vec(A)> vec(B) for convenience. Thus A : B is an inner product in the Euclidean vec\u2019d matrix space.\nOur strategy of derivation, outlined below, involves two important concepts we sketch here2. A variation, which corresponds to the forward sensitivity and it allows us to easily manipulate first and higher order terms of a Taylor expansion e.g. for a function g we write dg = dg(X; dX) = g(X + dX)\u2212 g(X) = A(X) : dX +O(\u2016dX\u20162), with A(X) a matrix of the same size as X and depending on X but not on dX . The (partial) derivative is by definition the linear \u201ccoefficient\u201d of a Taylor expansion i.e. the coefficient of dX ergo \u2202g\u2202X = A(X). These two are very different objects, in particular dg is always defined if g is defined and it can have matrix inputs and map to a space of matrices (unlike the partial derivative). Also, the variation is used for convenience of the derivation and need not be implemented in practice. What we are ultimately after, for the purpose of backpropagation, is the partial derivative.\nThe important thing is to understand that when\ndY = df(X; dX) (7)\nthe expressions (5) and (6) should be equal, since they both represent the variation of L for a given perturbation dX of the variable X . Looking at the Taylor expansions, their first order terms should also match which gives us the chain rule\n\u2202L \u25e6 f \u2202X : dX = \u2202L \u2202Y : dY (8)\nThe aim is to use this identity to express the partial derivative of the left hand side as a function of the partial derivative in the right hand side. The derivation process follows 2 steps3:\n1. Derive L, the functional describing the variations of the upper layer variables with respect to the lower layer variables.\ndY = L(dX) = df(X; dX) (9)\nIn many cases, for simplicity, we will only consider the first order variations since in the second step will disregard the higher order variations anyways thus this simplification, which makes the derivations more readable, remains correct. The variation derivation involves not only the forward mapping of the layer, f (l), but also the invariants associated to its variables. IfX satisfies certain invariants, these need to be preserved to first (leading) order when computing dX . Invariants such as diagonality, symmetry, or orthogonality are explicitly enforced by our methodology.\n2See [30] for an in-depth treatment including questions about existence and uniqueness of the partial derivatives. 3The appendices contain some basic identities and material necessary for each of these steps.\n2. With the dY produced in the first step we know (8) holds thus we can use the properties of the matrix inner product A : B = Tr(A>B) to obtain partial derivatives with respect to the lower layer variables. This holds for a general variation, e.g. for a non-symmetric dX even if X itself is symmetric. To remain within a subspace like the one of symmetric, diagonal or orthogonal matrices, we can consider a projection of dX onto the space of admissible variations and then transfer the projection onto the derivative, to obtain the projected gradient. We use this technique repeatedly in our derivations.\nSince the \u201c:\u201d operator is an inner product on the space of matrices, this is equivalent to constructively producing L\u2217, a non-linear adjoint operator of L\n\u2202L(l+1)\n\u2202Y : dY =\n\u2202L(l+1)\n\u2202Y : L(dX) , L\u2217\n( \u2202L(l+1)\n\u2202Y\n) : dX \u21d2 L\u2217 ( \u2202L(l+1)\n\u2202Y\n) = \u2202L(l)\n\u2202X by the chain rule (10)"}, {"heading": "4 Spectral and Non-Linear Layers", "text": "When global matrix operations are used in deep networks, they compound with other processing layers performed along the way. Such steps are architecture specific, although calculations like spectral decomposition are widespread and play a central role in many vision and machine learning models. SVD possesses a powerful structure that allows to express complex transformations like matrix functions and algorithms in a numerically stable form. In the sequel we first show how singular value decomposition (SVD) and the symmetric eigenvalue problem (EIG) can appear as layers in a deep network. Then we show how they can be leveraged towards constructing layers that perform global calculations in deep networks.\nWe denote Asym = 12 (A > +A) and Adiag be A with all off-diagonal elements set to 0."}, {"heading": "4.1 Spectral Layers", "text": "The first computational block we detail is matrix backpropagation for SVD problems.\nProposition 1 (SVD Variations). Let X = U\u03a3V > with X \u2208 Rm,n and m > n, such that U>U = I , V >V = I and \u03a3 possessing diagonal structure. Then\nd\u03a3 = (U>dXV )diag (11)\nand dV = 2V ( K> \u25e6 (\u03a3>U>dXV )sym ) (12)\nwith\nKij =  1 \u03c32i \u2212 \u03c32j , i 6= j\n0, i = j (13)\nConsequently the partial derivatives are\n\u2202L \u2202X = U\n{ 2\u03a3 ( K> \u25e6 ( V > \u2202L\n\u2202V )) sym + ( \u2202L \u2202\u03a3 ) diag } V > (14)\nProof. Let X = U\u03a3V > by way of SVD, with X \u2208 Rm\u00d7n and m \u2265 n, \u03a3 \u2208 Rm\u00d7n diagonal and U \u2208 Rm\u00d7m, V \u2208 Rn\u00d7n orthogonal. For a given variation dX of X , we want to calculate the variations d\u03a3 and dV . Note that the variation dU is not uniquely defined and indeed not needed for our purposes so we do not treat it here to simplify the exposition. The variation d\u03a3 is diagonal, like \u03a3, whereas dU and dV satisfy (by orthogonality) the constraints U>dU + dU>U = 0 and V >dV + dV >V = 0 respectively. Taking the first variation of the SVD decomposition, we have\ndX = dU\u03a3V > + Ud\u03a3V > + U\u03a3dV > \u21d2 \u21d2U>dXV = U>dU\u03a3 + d\u03a3 + \u03a3dV >V \u21d2 \u21d2R = A\u03a3 + d\u03a3 + \u03a3B\nwith R = U>dXV and A = U>dU , B = dV >V both antisymmetric. Since d\u03a3 is diagonal whereas A\u03a3, \u03a3B have both zero diagonal, we conclude that\nd\u03a3 = (U>dXV )diag (15)\nThe off-diagonal part then satisfies\nA\u03a3 + \u03a3B = R\u2212Rdiag \u21d2 \u03a3>A\u03a3 + \u03a3>\u03a3B = \u03a3>(R\u2212Rdiag) = \u03a3>R\u0304\n\u21d2\n{ \u03c3iaij\u03c3j + \u03c3 2 i bij = \u03c3iR\u0304ij\n\u2212\u03c3jaij\u03c3i \u2212 \u03c32j bij = \u03c3jR\u0304ji (A,B antisym.)\n\u21d2 (\u03c32i \u2212 \u03c32j )bij = \u03c3iR\u0304ij + R\u0304ji\u03c3j \u21d2 bij =\n{ (\u03c32i \u2212 \u03c32j )\u22121 ( \u03c3iR\u0304ij + R\u0304ji\u03c3j ) , i 6= j\n0 , i = j (16)\nwhere \u03c3i = \u03a3ii and R\u0304 = R\u2212Rdiag . This we can write as B = K \u25e6 (\u03a3>R\u0304+ R\u0304>\u03a3) = K \u25e6 (\u03a3>R+R>\u03a3), where\nKij =  1 \u03c32i \u2212 \u03c32j , i 6= j\n0, i = j (17)\nFinally, dV = V B> \u21d2 dV = 2V ( K> \u25e6 (\u03a3>U>dXV )sym ) (18)\nNote that this satisfies the condition V >dV + dV >V = 0 by construction, and so preserves the orthogonality of V (to leading order).\nProceeding further with the second part of the matrix backprop, we can replace dV and d\u03a3 with their expressions w.r.t. dX to obtain the partial derivatives:\n\u2202L \u2202V : dV + \u2202L \u2202\u03a3 : d\u03a3 = \u2202L \u2202V : { 2V ( K> \u25e6 (\u03a3>U>dXV )sym )} + \u2202L \u2202\u03a3 : (U>dXV )diag\n=2 ( V > \u2202L\n\u2202V\n) : ( K> \u25e6 (\u03a3>U>dXV )sym ) + ( \u2202L\n\u2202\u03a3 ) diag : (U>dXV ) (by (53), (54))\n=2 ( K> \u25e6 ( V > \u2202L\n\u2202V\n)) : (\u03a3>U>dXV )sym + { U ( \u2202L\n\u2202\u03a3 ) diag V > } : dX (by (56), (53))\n=2 { U\u03a3 ( K> \u25e6 ( V > \u2202L\n\u2202V )) sym V > } : dX + { U ( \u2202L \u2202\u03a3 ) diag V > } : dX (by (55), (53))\n=U { 2\u03a3 ( K> \u25e6 ( V > \u2202L\n\u2202V )) sym + ( \u2202L \u2202\u03a3 ) diag } V > : dX\nand so, since the last expression is equal to \u2202L\n\u2202X : dX by the chain rule,\n\u2202L \u2202X = U\n{ 2\u03a3 ( K> \u25e6 V > \u2202L\n\u2202V ) sym + ( \u2202L \u2202\u03a3 ) diag } V > (19)\nProposition 2 (EIG Variations). Let X = U\u03a3U> with X \u2208 Rm,m, such that U>U = I and \u03a3 possessing diagonal structure. Then the d\u03a3 is still (11) with V = U but\ndU = 2U ( K> \u25e6 (U>dXU)sym ) (20)\nwith\nK\u0303ij =  1 \u03c3i \u2212 \u03c3j , i 6= j\n0, i = j (21)\nThe resulting partial derivatives are\n\u2202L \u2202X = U\n{ 2 ( K\u0303> \u25e6 ( U> \u2202L\n\u2202V ) sym ) + ( \u2202L \u2202\u03a3 ) diag } U> (22)\nProof. First note that (15) still holds and with the notation above we have in this case m = n, U = V . This implies\nd\u03a3 = (U>dXU)diag (23)\nFurthermore we have A = B> (A, B antisymmetric) and the off-diagonal part then satisfies A\u03a3 + \u03a3A> = R\u2212Rdiag . In a similar process with the asymmetric case, we have\nA\u03a3 + \u03a3A> = R\u2212Rdiag \u21d2 A\u03a3\u2212 \u03a3A = R\u0304\u21d2 { aij\u03c3j \u2212 aij\u03c3i = R\u0304ij , i 6= j aij = 0, i = j\n(24)\nso that A = K\u0303> \u25e6 R\u0304 with\nK\u0303ij =  1 \u03c3i \u2212 \u03c3j , i 6= j\n0, i = j (25)\nFrom this, we get then dU = 2U ( K\u0303> \u25e6 (U>dXU)sym ) (26)\nAgain, the symmetrization is so that U>dU + dU>U = 0 by construction. For the second step of the derivation we can replace dU and d\u03a3 with their expressions w.r.t. dX:\n\u2202L \u2202U : dU + \u2202L \u2202\u03a3 : d\u03a3 = \u2202L \u2202U :\n{ U ( K\u0303> \u25e6 ( 2U>dXU )) sym } + \u2202L \u2202\u03a3 : ( U>dXU ) diag\n= 2U ( K\u0303> \u25e6 ( U> \u2202L\n\u2202U ) sym ) U> : dX + U ( \u2202L \u2202\u03a3 ) diag U> : dX\nand so \u2202L\n\u2202X = U\n{ 2 ( K\u0303> \u25e6 ( U> \u2202L\n\u2202U ) sym ) + ( \u2202L \u2202\u03a3 ) diag } U> (27)\nNote that (15), (26), (25) and (27) represent the desired quantities of the proposition."}, {"heading": "4.2 Non-Linear Layers", "text": "Proposition 3 (SVD matrix function). An (analytic) matrix function of a diagonalizable matrix A = U\u03a3U> can be written as f(A) = Uf(\u03a3)U>. Since \u03a3 is diagonal this is equivalent to applying f element-wise to \u03a3\u2019s diagonal elements. Combining this idea with the SVD decomposition X = U\u03a3V >, our matrix logarithm C can be written as an element-wise function C = f(X>X + I) = V f(\u03a3>\u03a3 + I)V >.\nThen the variations are dC = 2 ( dV f(\u03a3>\u03a3 + I)V > ) sym + 2 ( V f \u2032(\u03a3>\u03a3 + I)\u03a3>d\u03a3V > ) sym\nand the partial derivatives are \u2202L\n\u2202V = 2\n( \u2202L\n\u2202C ) sym V f(\u03a3>\u03a3 + I) (28)\nand \u2202L\n\u2202\u03a3 = 2\u03a3f \u2032(\u03a3>\u03a3 + I)V >\n( \u2202L\n\u2202C ) sym V (29)\nProof. Using the fact that for a positive diagonal matrixA and a diagonal variation dA, f(A+dA) = f(A)+f \u2032dA+ O(dA2), we can write\ndC = 2 ( dV f(\u03a3>\u03a3 + I)V ) sym + 2 ( V f \u2032(\u03a3>\u03a3 + I)\u03a3>d\u03a3V > ) sym\nThe total variation dL of an expression of the form L = f(C), f : Rn\u00d7n \u2192 R, can be written then as: \u2202L \u2202C : dC = \u2202L \u2202C : { 2 ( dV f(\u03a3>\u03a3 + I)V > ) sym + 2 ( V f \u2032(\u03a3>\u03a3 + I)\u03a3>d\u03a3V > ) sym } =2 ( \u2202L\n\u2202C ) sym : (dV f(\u03a3>\u03a3 + I)V >) + 2 ( \u2202L \u2202C ) sym : (V f \u2032(\u03a3>\u03a3 + I)\u03a3>d\u03a3V >) (by (55))\n=2\n{( \u2202L\n\u2202C ) sym V f(\u03a3>\u03a3 + I) } : dV + 2 { \u03a3f \u2032(\u03a3>\u03a3 + I)V > ( \u2202L \u2202C ) sym V } : d\u03a3 (by (53))\nBy the chain rule, we must have\n\u2202L \u2202C : dC = \u2202L \u2202V : dV + \u2202L \u2202\u03a3 : d\u03a3\u21d2\n{ \u2202L \u2202V = 2 ( \u2202L \u2202C ) sym\nV f(\u03a3>\u03a3 + I) \u2202L \u2202\u03a3 = 2\u03a3f \u2032(\u03a3>\u03a3 + I)V > ( \u2202L \u2202C ) sym V (30)\nPlugging in (30) in (19), gives \u2202L\n\u2202X as a function of\n\u2202L \u2202C .\nProposition 4 (EIG matrix function). Let Z = UQU> by way of eigen-decomposition (symmetric SVD), with Z \u2208 S+(m) an m \u00d7 m real symmetric matrix. Then Q \u2208 Rm\u00d7m is diagonal (the strictly positive eigenvalues) and U \u2208 Rm\u00d7m is orthogonal (the eigenvectors). Denote with C = f(Z) = Uf(Q)U> Then the variations of C are given by\ndC = 2(dUf(Q)U>)sym + Uf \u2032(Q)dQU> (31)\nand the partial derivatives are\n\u2202L \u2202U = 2\n( \u2202L\n\u2202C ) sym Uf(Q) (32)\n\u2202L \u2202Q = f \u2032(Q)U> \u2202L \u2202C U (33)\nProof. The variation of C is\ndC = dUf(Q)U> + Udf(Q)U> + Uf(Q)dU> \u21d2 dC = 2(dUf(Q)U>)sym + Uf \u2032(Q)dQU> (34)\nWe consider again the variation of L,\n\u2202L \u2202C : dC = \u2202L \u2202C : { 2(dUf(Q)U>)sym + Uf \u2032(Q)dQU> } = f \u2032(Q)U> \u2202L\n\u2202C U : dQ+ 2\n( \u2202L\n\u2202C ) sym Uf(Q) : dU\nBy the chain rule, we must have\n\u2202L \u2202C : dC = \u2202L \u2202U : dU + \u2202L \u2202Q : dQ\u21d2  \u2202L \u2202U = 2 ( \u2202L \u2202C ) sym Uf(Q) \u2202L\n\u2202Q = f \u2032(Q)U>\n\u2202L \u2202C U\nSince Z is symmetric, these can be plugged in to (27) with X = Z and \u03a3 = Q.\nNow it is trivial to derive two versions of the O2P descriptor[8] by plugging in log and its derivative in the propositions above.\nCorollary 1 (DeepO2P). Deep O2P layers can be implemented and have the following backpropagation rules\n1. DeepO2P-SVD:\n\u2202L \u2202V = 2\n( \u2202L\n\u2202C ) sym V log(\u03a3>\u03a3 + I) and \u2202L \u2202\u03a3 = 2\u03a3(\u03a3>\u03a3 + I)\u22121V > ( \u2202L \u2202C ) sym V (35)\n2. DeepO2P-EIG: \u2202L\n\u2202U = 2\n( \u2202L\n\u2202C ) sym U log(Q) and \u2202L \u2202Q = Q\u22121 ( U> \u2202L \u2202C U ) (36)"}, {"heading": "5 Spectral Clustering Layers", "text": "In this section we focus on the discovery of global structure. An iconic example in computer vision is grouping (segmentation) i.e. discovering which pixels belong to different visual objects. A successful approach to grouping is normalized cuts. Let m be the number of pixels in the image and let V = {1, . . . ,m} be the set of indices. We are interested in producing a partition P = {P1, . . . , Pk}, where k = |P|, Pi \u2282 V , \u22c3 i Pi = V and Pj \u22c2 Pi = \u2205. This is equivalent to producing a matrix E \u2208 {0, 1}m\u00d7k such that E(i, j) = 1 if i \u2208 Pj and 0 otherwise. Let X \u2208 Rm\u00d7d be a feature matrix with descriptor of size d and let W be a similarity matrix with positive entries. For simplicity we consider W = X\u039bX>, where \u039b is a d \u00d7 d parameter matrix. Note that one can also apply global non-linearities on top using the material in the previous section. Finally, let D = [W1], where [v] is the diagonal matrix with main diagonal v. That is the diagonal elements of D are the sums of the corresponding rows of W . The normalized cuts criterion is then\nC(W,E) = Tr(E>WE(E>DE)\u22121) (37)\nFinding theE that minimizesC(W,E) is finding a partitioning that minimizes the cut energy but penalizes unbalanced solutions.\nIt is easy to show that C(W,E) = k \u2212 Tr(Y >D\u22121/2WD\u22121/2Y ), where Y is such that a) Y >Y = I and b) D1/2Y is piecewise constant with respect to E (i.e. it is equal to E times some scaling for each column). Ignoring the\nsecond condition we obtain a relaxed problem that can be solved, due to Ky Fan theorem, by an eigen-decomposition of\nM = D\u22121/2WD\u22121/2 (38)\n[13] propose to learn the parameters \u039b such that D1/2Y is piecewise constant because then, solving the relaxed problem is equivalent to the original problem. In [13] the input features were fixed thus \u039b are the only parameters to permit the alignment. This is not our case, as we place our global objective on top of convolutional network inputs. We can can therefore take leverage the network parameters in order to change X directly, thus training the bottom layers to produce a representation that is appropriate for normalized cuts.\nTo obtain a Y that is piecewise constant with respect to D1/2E we can align the span of M with that of \u2126 = D1/2EE>D1/2. This can be done by minimizing the Frobenius norm of the corresponding projectors\nJ1(W,E) = 1\n2 \u2016\u03a0M \u2212\u03a0\u2126\u20162F (39)\nWe will again obtain the partial derivatives of an objective with respect to the matrices it depends on. We first start with a lemma that simplifies the calculations for the variations because it is common to both objectives.\nLemma 1. Consider a symmetric matrixA and its orthogonal projection operator \u03a0A. If dA is a symmetric variation of A then\nd\u03a0A = 2 ( (I \u2212\u03a0A)dAA+ ) sym ) (40)\nProof 1. (We skip the subscript in \u03a0A for brevity.) Taking the variation of the basic properties of the projector \u03a02 = \u03a0 and \u03a0A = A, we have\nd\u03a0\u03a0 + \u03a0d\u03a0 = d\u03a0 (41) d\u03a0A+ \u03a0dA = dA (42)\nWe then consider the following decomposition of d\u03a0:\nd\u03a0 = \u03a0M\u03a0 + (I \u2212\u03a0)Q\u03a0 + \u03a0Q>(I \u2212\u03a0) + (I \u2212\u03a0)R(I \u2212\u03a0)\nwith M and R symmetric, so that d\u03a0 is symmetric by construction. Plugging into the equations above, we get\n2\u03a0M\u03a0 + (I \u2212\u03a0)Q\u03a0 + \u03a0Q>(I \u2212\u03a0) = d\u03a0 \u03a0MA+ (I \u2212\u03a0)QA = (I \u2212\u03a0)dA\nComparing the first equation with the decomposition of d\u03a0 above, we deduce that M = R = 0, and so\n(I \u2212\u03a0)Q\u03a0 + \u03a0Q>(I \u2212\u03a0) = d\u03a0 (I \u2212\u03a0)QA = (I \u2212\u03a0)dA\nMultiplying the second equation with A+ from the right gives (I \u2212 \u03a0)Q\u03a0 = (I \u2212 \u03a0)dAA+. Plugging this into the first equation gives the desired result.\nThe derivation relies only on basic properties of the projector with respect to itself and its matrix: \u03a02A = \u03a0A (idempotency of the projector) and \u03a0AA = A (projector leaves the original space unchanged). Note that since \u03a0A = AA\n+, there exists a spectral decomposition in training but it is hidden in A+ (where A+ is the Moore-Penrose inverse).\nProposition 5. The partial derivative of J1 with respect to W is\n\u2202J1 \u2202W = D\u22121/2 \u2202J1 \u2202M D\u22121/2 + diag\n( D\u22121\u2126 ( \u2202J1 \u2202\u2126 ) sym \u2212D\u22121M ( \u2202J1 \u2202M ) sym ) 1>\nProof. Working on the total variation of J we have\ndJ1 = \u2202J1 \u2202\u03a0M : d\u03a0M + \u2202J1 \u2202\u03a0\u2126 : d\u03a0\u2126\n= (\u03a0M \u2212\u03a0\u2126) : d\u03a0M + (\u03a0\u2126 \u2212\u03a0M ) : d\u03a0\u2126 = 2(\u03a0M \u2212\u03a0\u2126)sym : ((I \u2212\u03a0M )dMM+) + 2(\u03a0\u2126 \u2212\u03a0M )sym : ((I \u2212\u03a0\u2126)d\u2126\u2126+) (by (40)) = 2(I \u2212\u03a0M )(\u03a0M \u2212\u03a0\u2126)symM+ : dM + 2(I \u2212\u03a0\u2126)(\u03a0\u2126 \u2212\u03a0M )sym\u2126+ : d\u2126 = \u22122(I \u2212\u03a0M )\u03a0\u2126M+ : dM \u2212 2(I \u2212\u03a0\u2126)\u03a0M\u2126+ : d\u2126\nand so \u2202J\n\u2202M = \u22122(I \u2212\u03a0M )\u03a0\u2126M+ (43)\nand \u2202J\n\u2202\u2126 = \u22122(I \u2212\u03a0\u2126)\u03a0M\u2126+ (44)\nFor a diagonal matrix D under a diagonal variation dD, we can show that d(Dp) = pDp\u22121dD (by means of element-wise differentiation). For the particular D = [W1], we have dD = [dW1]. Using these, we get\nd\u2126 = 1\n2 dDD\u22121/2EE>D1/2+\n1 2 D1/2EE>D\u22121/2dD =\n( D1/2EE>D\u22121/2dD ) sym = ( \u2126D\u22121[dW1] ) sym (45)\nand\ndM = \u22121 2 dDD\u22123/2WD\u22121/2 +D\u22121/2dWD\u22121/2 \u2212 1 2 D\u22121/2WD\u22123/2dD\n= D\u22121/2dWD\u22121/2 \u2212 ( MD\u22121[dW1] ) sym\nThen\ndJ1 = \u2202J1 \u2202M : dM + \u2202J1 \u2202\u2126 : d\u2126\n= ( D\u22121/2\n\u2202J1 \u2202M\nD\u22121/2 ) : dW \u2212 ( D\u22121M\n\u2202J1 \u2202M sym\n) : [dW1] + ( D\u22121\u2126\n\u2202J1 \u2202\u2126 sym\n) : [dW1]\nthen identifying we obtain\n\u2202J1 \u2202W = D\u22121/2 \u2202J1 \u2202M D\u22121/2 + diag\n( D\u22121\u2126\n\u2202J1 \u2202\u2126 sym \u2212D\u22121M \u2202J1 \u2202M sym\n) 1>\nwhere we used the property A : [Bx] = Aii(Bijxj) = (Aiixj)Bij = ( diag(A)x> ) : B.\nA related optimization objective is\nJ2 = 1\n2 \u2016\u03a0W \u2212\u03a0\u03a8\u20162F , (46)\nwith \u03a8 = E(E>E)\u22121E>. Here we consider \u03a0W = V (V >V )\u22121V >, where V = D1/2U . We can check that this is a projector for W by noting that \u03a0W = D1/2U(U>DU)\u22121U>D1/2 and M = U\u03a3U> = D\u22121/2WD\u22121/2 (by eigen decomposition and (38)). Then indeed\n1. Idempotency of \u03a0W\n\u03a02W = D 1/2U(U>DU)\u22121U>DU(U>DU)\u22121U>D1/2 = \u03a0W\n2. \u03a0W leaves W unchanged\n\u03a0WW = \u03a0W (D 1/2MD1/2)\n= D1/2U(U>DU)\u22121(U>DU)\u03a3U>D1/2\n= D\u22121/2U\u03a3U>D\u22121/2 = W\nProposition 6. The corresponding partial derivative for J2 is\n\u2202J2 \u2202W = \u22122(I \u2212\u03a0W )\u03a0\u03a8W+ (47)\nProof. The derivation is trivial, since \u03a8 does not depend on W and a direct application of Lemma 1 gives\n\u2202J2 \u2202W = \u22122(I \u2212\u03a0W )\u03a0\u03a8W+ (48)\nIn both cases, we can propagate the partial derivatives of Ji down to \u039b and X . First we compute the first order variations dW = dX\u039bX> + Xd\u039bX> + X\u039bdX>. Then use the trace properties make the partial derivatives identifiable\ndJi = \u2202Ji \u2202W : dW = \u2202Ji \u2202W\n: ( Xd\u039bX> ) + 2\n\u2202Ji \u2202W : (dX\u039bX>)sym\n= ( X>\n\u2202Ji \u2202W X\n) : d\u039b + 2 ( \u2202Ji \u2202W sym X\u039b> ) : dX\nThus we obtain \u2202Ji \u2202\u039b = X> \u2202Ji \u2202W X (49)\nand \u2202Ji \u2202X = 2 ( \u2202Ji \u2202W ) sym X\u039b> (50)\nNote that when J = J2 then \u2202J2 \u2202\u039b = 0, since (I \u2212 \u03a0W )X = X>(I \u2212 \u03a0W ) = 0. Thus we cannot learn \u039b with our projector trick but there is no problem learning X which is much more interesting anyway.\nAn important feature of our approach is that we do not restrict the rank in training. During alignment the optimization may choose to collapse certain directions thus reducing rank. We can prove a topological lemma implying that if the Frobenius distance between the projectors (such as in the two objectives J1, J2) drops below a certain value, then the ranks of the two projectors will match. Conversely, if for some reason the ranks can not converge, the objectives are bounded away from zero. The following lemma shows that when the projectors of two matricesA and B are close enough in the \u2016\u00b7\u20162 norm, then the matrices have the same rank:\nLemma 2. Let A,B \u2208 Rm\u00d7n, and \u03a0A, \u03a0B their respective orthogonal projectors. If \u2016\u03a0A \u2212 \u03a0B\u20162 < 1 then rankA = rankB.\nProof 2. The spectral norm \u2016\u00b7\u20162 can indeed be defined as \u2016A\u20162 = sup\u2016x\u20162 6=0 \u2016Ax\u2016 \u2016x\u2016 . We assume w.l.o.g. that rankA > rankB, so that by the fundamental theorem of linear algebra there exists a vector v in the range of A (so that \u03a0Av = v), that is orthogonal to the range of B (so that \u03a0Bv = 0). We have then\n\u2016\u03a0A \u2212\u03a0B\u20162 \u2265 \u2016\u03a0Av \u2212\u03a0Bv\u2016 \u2016v\u2016 = \u2016\u03a0Av\u2016 \u2016v\u2016 = 1\nwhich is a contradiction.\nGiven that the Frobenius norm controls the spectral norm, i.e. \u2016A\u20162 \u2264 \u2016A\u2016F , (section 2.3.2 of [42]) an immediate corollary is that when\nJ2(W ) < 1\n2 \u21d2 rank(W ) = rank(EE>) (51)"}, {"heading": "6 Experiments", "text": "In this section we validate the proposed methodology by constructing models on standard datasets for region-based object classification, like Microsoft COCO [43], and for image segmentation on BSDS [44]. A matconvnet [45] implementation of our models and methods is publicly available."}, {"heading": "6.1 Segmented Object Classification on MSCOCO", "text": "For recognition we use the MSCOCO dataset [43], which provides 880k segmented training instances across 80 classes, divided into training and validation sets. The main goal is to assess our second-order pooling layer in various training settings. A secondary goal is to study the behavior of ConvNets learned from scratch on segmented training data. This has not been explored before in the context of deep learning because of the relatively small size of the datasets with associated object segmentations, such as PASCAL VOC [46].\nThe experiments in this section use the convolutional architecture component of AlexNet[2] with the global O2P layers we propose in order to obtain DeepO2P models with both classification and fully connected (FC) layers in the same topology as Alexnet. We crop and resize each object bounding box to have 200 pixels on the largest side, then pad it to the standard AlexNet input size of 227x227 with small translation jittering, to limit over-fitting. We also randomly flip the images in each mini-batch horizontally, as in standard practice. Training is performed with stochastic gradient descent with momentum. We use the same batch size (100 images) for all methods but the learning rate was optimized for each model independently. All the DeepO2P models used the same = 10\u22123 parameter value in (4).\nArchitecture and Implementation details. Implementing the spectral layers efficiently is challenging since the GPU support for SVD is still very limited and our parallelization efforts even using the latest CUDA 7.0 solver API have delivered a slower implementation than the standard CPU-based. Consequently, we use CPU implementations thus we incur a penalty for moving data back and forth to the CPU. The numerical experiments revealed that an implementation in single precision obtained a significantly less accurate gradient for learning. Therefore all computations in our proposed layers, both in the forward and backward passes, are made in double precision. In experiments we still noticed a significant accuracy penalty due to inferior precision in all the other layers (above and below the structured ones), still computed in single precision, on the GPU.\nA second formal derivation of the non-linear spectral layer based on an eigen-decomposition of Z = X>X + I instead of SVD of X is also possible. Our experiments favor the formulation using SVD. The alternative implementation, which is formally correct, exhibits numerical instability in the derivative when multiple eigenvalues have very close values, thus producing blow up in K\u0303. Such numerical issues are expected to appear under some implementations, when complex layers like the ones presented here are integrated in deep network settings. Results. The results of the recognition experiment are presented in table 1. They show that our proposed DeepO2PFC models, containing global layers, outperform standard convolutional pipelines based on AlexNet, on this problem. The bottom layers are pre-trained on ImageNet using AlexNet, and this might not provide the ideal initial input features. However, despite this potentially unfavorable initialization, our model jointly refines all parameters (both convolutional, and corresponding to global layers), jointly, end to end, using a consistent cost function.\nWe note that the fully connected layers on top of the DeepO2P layer offer good performance benefits. O2P over hand-crafted SIFT performs considerably less well than our DeepO2P models, suggesting that large potential gains can be achieved when deep features replace existing descriptors."}, {"heading": "6.2 Full-Image Segmentation on BSDS300", "text": "We use the BSDS300 dataset to validate our deep normalized cuts approach. BSDS contains 200 training images and 100 testing images and human annotations of all the relevant regions in the image. Although small by the standards of neural network learning it provides exactly the supervision we need to refine our model using global information. Note that since the supervision is pixel-wise, the number of effective datapoint constraints is much larger. We evaluate using the average and best covering metric under the Optimal Image Scale (OIS) criterion [44]. This amounts to, given a set of full image segmentations computed for an image, selecting the one that maximizes the average and best covering, respectively, compared to the pool of ground truth segmentations. Architecture and Implementation details. We use both the AlexNet[2] and the VGG-16[3] architectures to feed our global layers. All the parameters of the deep global models (including the low-level features, pretrained on ImageNet) are refined end-to-end. We use a linear affinity but we need all entries of W to be positive. Thus, we use ReLU layers to feed the segmentation ones. Initially, we just cascaded our segmentation layer to different layers in AlexNet but the resulting models were hard to learn. Our best results were obtained by adding two Conv-ReLU pairs initialized randomly before the normalized cuts layer. This results in many filters in the lower layer (256 for AlexNet and 1024 for VGG) for high capacity but few in the top layer (20 dimensions) to limit the maximal rank of W . For AlexNet we chose the last convolutional layer while for VGG we used both the first ReLU layer in block4 4 and the top layer from block 5. This gives us feeds from layers with different invariances, receptive field sizes (32 vs. 132 pixels) and coarseness (block 4 has 2\u00d7 the resolution of 5). We used an initial learning rate of 10\u22124 but 10\u00d7 larger rates for the newly initialized layers. A dropout layer between the last two layers with a rate of .25 reduces overfitting. In inference, we generate 8 segmentations by clustering[13] then connected components are split into separate segments. Results. The results in table 2 show that in all cases we obtain important performance improvements with respect to the corresponding models that perform inference directly on original AlexNet/VGG features. Training using our Matlab implementation takes 2 images/s considering 1 batch of images while testing at about 3 images/s on a standard Titan Z GPU with an 8 core E5506 CPU. In experiments we monitor both the objective and the rank of the similarity matrix. Rank reduction is usually a good indicator of performance in both training and testing. In the context of the\n4We call a block the set of layers between two pooling levels.\nrank analysis in \u00a75, we interpret these findings to mean that if the rank of the similarity is too large compared to the target, the objective is not sufficient to lead to rank reduction. However if the rank of the predicted similarity and the ground truth are initially not too far apart, then rank reduction (although not always rank matching) does occur and improves the results."}, {"heading": "7 Conclusion", "text": "Motivated by the recent success of deep network architectures, in this work we have introduced the mathematical theory and the computational blocks that support the development of more complex models with layers that perform structured, global matrix computations like segmentation or higher-order pooling. Central to our methodology is the development of the matrix backpropagation methodology which relies on the calculus of adjoint matrix variations. We provide detailed derivations, operating conditions for spectral and non-linear layers, and illustrate the methodology for normalized cuts and second-order pooling layers. Our extensive region recognition and segmentation experiments based on MSCoco and BSDS show that that deep networks relying on second-order pooling and normalized cuts layers, trained end-to-end using the introduced practice of matrix backpropagation, outperform counterparts that do not take advantage of such global layers.\nAcknowledgements. This work was partly supported by CNCS-UEFISCDI under CT-ERC-2012-1, PCE-2011-30438, JRP-RO-FR-2014-16. We thank J. Carreira for helpful discussions and Nvidia for a graphics board donation."}, {"heading": "7.1 Notation and Basic identities", "text": "In this section we present for completeness the notation and some basic linear algebra identities that are useful in the calculations associated to matrix backpropagation and its instantiation for log-covariance descriptors [48, 40] and normalized cuts segmentation [6].\nThe following notation is used in the derivations\n\u2022 The symmetric part Asym = 12 (A > +A) of a square matrix A.\n\u2022 The diagonal operator Adiag for an arbitrary matrix A \u2208 Rm\u00d7n, which is the m \u00d7 n matrix which matches A on the main diagonal and is 0 elsewhere. Using the notations diag(A) and [x] to denote the diagonal of A (taken as a vector) and the diagonal matrix with the vector x in the diagonal resp., then Adiag = [diag(A)].\n\u2022 The colon-product A : B = \u2211 i,j AijBij = Tr(A >B) for matrices A,B \u2208 Rm\u00d7n, and the associated Frobenius\nnorm \u2016A\u2016 := \u221a A : A.\n\u2022 The Hadamard (element-wise) product A \u25e6B.\nWe note the following properties of the matrix inner product \u201c:\u201d :\nA : B = A> : B> = B : A (52)\nA : (BC) = (B>A) : C = (AC>) : B (53) A : Bdiag = Adiag : B (54) A : Bsym = Asym : B (55) A : (B \u25e6 C) = (B \u25e6A) : C (56)"}], "references": [{"title": "Matrix Backpropagation for Deep Networks with Structured Layers", "author": ["C. Ionescu", "O. Vantzos", "C. Sminchisescu"], "venue": "ICCV, 2015. 1", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pp. 1097\u20131105, 2012. 1, 2, 11, 12", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, vol. abs/1409.1556, 2014. 1, 2, 12", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, vol. 61, pp. 85 \u2013 117, 2015. 1, 2", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998. 1, 2", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "PAMI, vol. 22, pp. 888\u2013905, Aug 2000. 1, 2, 4, 15", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "A combined corner and edge detector", "author": ["C. Harris", "M. Stephens"], "venue": "Alvey vision conference, vol. 15, p. 50, Manchester, UK, 1988. 1", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Semantic segmentation with second-order pooling", "author": ["J. Carreira", "R. Caseiro", "J. Batista", "C. Sminchisescu"], "venue": "ECCV, pp. 430\u2013443, Springer, 2012. 1, 2, 3, 8", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Shape and motion from image streams under orthography: a factorization method", "author": ["C. Tomasi", "T. Kanade"], "venue": "IJCV, vol. 9, no. 2, pp. 137\u2013154, 1992. 1", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Multiple view geometry in computer vision", "author": ["R. Hartley", "A. Zisserman"], "venue": "Cambridge university press,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Principal Component Analysis", "author": ["I. Jolliffe"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation, vol. 15, no. 6, pp. 1373\u20131396, 2003. 1", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning spectral clustering, with application to speech separation", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "JMLR, vol. 7, pp. 1963\u2013 2001, 2006. 2, 9, 12", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1963}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR, 2015. 2", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximin affinity learning of image segmentation", "author": ["S. Turaga", "K. Briggman", "M.N. Helmstaedter", "W. Denk", "S. Seung"], "venue": "NIPS, pp. 1865\u20131873, 2009. 2", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1865}, {"title": "CPMC: Automatic Object Segmentation Using Constrained Parametric Min-Cuts", "author": ["J. Carreira", "C. Sminchisescu"], "venue": "PAMI, 2012. 2", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Selective search for object recognition", "author": ["J.R. Uijlings", "K.E. van de Sande", "T. Gevers", "A.W. Smeulders"], "venue": "IJCV, 2013. 2", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, pp. 580\u2013587, IEEE, 2014. 2", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbelaez", "R. Girshick", "J. Malik"], "venue": "ECCV, 2014. 2", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "PAMI, 2015. 2", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. Bengio", "Y. Le Cun"], "venue": "CVPR, pp. 489\u2013494, IEEE, 1997. 2", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Conditional neural fields", "author": ["J. Peng", "L. Bo", "J. Xu"], "venue": "NIPS, pp. 1419\u20131427, 2009. 2", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Fully Convolutional Networks for Semantic Segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, 2015. 2", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H.S. Torr"], "venue": "ICCV, 2015. 2", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep structured models", "author": ["L.C. Chen", "A.G. Schwing", "A.L. Yuille", "R. Urtasun"], "venue": "ICML, 2015. 2", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "I.D. Reid", "A. v. d. Hengel"], "venue": "CoRR, vol. abs/1504.01013, 2015. 2", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernel methods on the riemannian manifold of symmetric positive definite matrices", "author": ["S. Jayasumana", "R. Hartley", "M. Salzmann", "H. Li", "M. Harandi"], "venue": "CVPR, IEEE, 2013. 2", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Symbolic matrix derivatives", "author": ["P.S. Dwyer", "M. MacPhail"], "venue": "The Annals of Mathematical Statistics, pp. 517\u2013534, 1948. 2, 4", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1948}, {"title": "Matrix differential calculus with applications in statistics and econometrics", "author": ["J.R. Magnus", "H. Neudecker"], "venue": "J. Wiley & Sons,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "Collected matrix derivative results for forward and reverse mode algorithmic differentiation", "author": ["M.B. Giles"], "venue": "Advances in Automatic Differentiation, pp. 35\u201344, Springer, 2008. 2, 4", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Estimating the jacobian of the singular value decomposition: Theory and applications", "author": ["T. Papadopoulo", "M.I. Lourakis"], "venue": "Computer Vision-ECCV 2000, pp. 554\u2013570, Springer, 2000. 2", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning gaussian conditional random fields for low-level vision", "author": ["M. Tappen", "C. Liu", "E. Adelson", "W. Freeman"], "venue": "CVPR, 2007. 2", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J. Bilmes", "K. Livescu"], "venue": "ICML, pp. 1247\u20131255, 2013. 2", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Bilinear CNN models for fine-grained visual recognition", "author": ["T. Lin", "A. RoyChowdhury", "S. Maji"], "venue": "CoRR, vol. abs/1504.07889, 2015. 2", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "CoRR, vol. abs/1312.6203, 2013. 2", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional networks on graph-structured data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "CoRR, vol. abs/1506.05163, 2015. 2", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic joint image segmentation and labeling by figure-ground composition", "author": ["A. Ion", "J. Carreira", "C. Sminchisescu"], "venue": "IJCV, vol. 107, no. 1, pp. 40\u201357, 2014. 3", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Composite statistical inference for semantic segmentation", "author": ["F. Li", "J. Carreira", "G. Lebanon", "C. Sminchisescu"], "venue": "CVPR, pp. 3302\u20133309, IEEE, 2013. 3", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Geometric means in a novel vector space structure on symmetric positivedefinite matrices", "author": ["V. Arsigny", "P. Fillard", "X. Pennec", "N. Ayache"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 29, no. 1, pp. 328\u2013347, 2007. 3, 15", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "ICCV, vol. 2, pp. 1150\u20131157, Ieee, 1999. 3", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1999}, {"title": "Matrix Computations (3rd Ed.)", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1996}, {"title": "Microsoft COCO: Common Objects in Context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollr", "C. Zitnick"], "venue": "ECCV, 2014. 11", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "PAMI, vol. 33, pp. 898\u2013916, May 2011. 11, 12", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Int. Conf. Multimedia, 2015. 11", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "The Pascal visual object classes (VOC) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, vol. 88, no. 2, pp. 303\u2013338, 2010. 11", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Spectral segmentation with multiscale graph decomposition", "author": ["T. Cour", "F. Benezit", "J. Shi"], "venue": "CVPR, 2005. 12", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Free-form region description with second-order pooling", "author": ["J. Carreira", "R. Caseiro", "J. Batista", "C. Sminchisescu"], "venue": "PAMI, 2014. 15", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Recently, the end-to-end learning of deep architectures using stochastic gradient descent, based on very large datasets, has produced impressive results in realistic settings, for a variety of computer vision and machine learning domains[2, 3, 4].", "startOffset": 237, "endOffset": 246}, {"referenceID": 2, "context": "Recently, the end-to-end learning of deep architectures using stochastic gradient descent, based on very large datasets, has produced impressive results in realistic settings, for a variety of computer vision and machine learning domains[2, 3, 4].", "startOffset": 237, "endOffset": 246}, {"referenceID": 3, "context": "Recently, the end-to-end learning of deep architectures using stochastic gradient descent, based on very large datasets, has produced impressive results in realistic settings, for a variety of computer vision and machine learning domains[2, 3, 4].", "startOffset": 237, "endOffset": 246}, {"referenceID": 4, "context": "One of the most widely used architecture is the convolutional network (ConvNet) [5, 2], a deep processing model based on the composition of convolution and pooling with pointwise nonlinearities for efficient classification and learning.", "startOffset": 80, "endOffset": 86}, {"referenceID": 1, "context": "One of the most widely used architecture is the convolutional network (ConvNet) [5, 2], a deep processing model based on the composition of convolution and pooling with pointwise nonlinearities for efficient classification and learning.", "startOffset": 80, "endOffset": 86}, {"referenceID": 5, "context": "In turn, matrix factorization plays a central role in classical (shallow) algorithms for many different computer vision and machine learning problems, such as image segmentation [6], feature extraction, descriptor design [7, 8], structure from motion [9], camera calibration [10], and dimensionality reduction [11, 12], among others.", "startOffset": 178, "endOffset": 181}, {"referenceID": 6, "context": "In turn, matrix factorization plays a central role in classical (shallow) algorithms for many different computer vision and machine learning problems, such as image segmentation [6], feature extraction, descriptor design [7, 8], structure from motion [9], camera calibration [10], and dimensionality reduction [11, 12], among others.", "startOffset": 221, "endOffset": 227}, {"referenceID": 7, "context": "In turn, matrix factorization plays a central role in classical (shallow) algorithms for many different computer vision and machine learning problems, such as image segmentation [6], feature extraction, descriptor design [7, 8], structure from motion [9], camera calibration [10], and dimensionality reduction [11, 12], among others.", "startOffset": 221, "endOffset": 227}, {"referenceID": 8, "context": "In turn, matrix factorization plays a central role in classical (shallow) algorithms for many different computer vision and machine learning problems, such as image segmentation [6], feature extraction, descriptor design [7, 8], structure from motion [9], camera calibration [10], and dimensionality reduction [11, 12], among others.", "startOffset": 251, "endOffset": 254}, {"referenceID": 9, "context": "In turn, matrix factorization plays a central role in classical (shallow) algorithms for many different computer vision and machine learning problems, such as image segmentation [6], feature extraction, descriptor design [7, 8], structure from motion [9], camera calibration [10], and dimensionality reduction [11, 12], among others.", "startOffset": 275, "endOffset": 279}, {"referenceID": 10, "context": "In turn, matrix factorization plays a central role in classical (shallow) algorithms for many different computer vision and machine learning problems, such as image segmentation [6], feature extraction, descriptor design [7, 8], structure from motion [9], camera calibration [10], and dimensionality reduction [11, 12], among others.", "startOffset": 310, "endOffset": 318}, {"referenceID": 11, "context": "In turn, matrix factorization plays a central role in classical (shallow) algorithms for many different computer vision and machine learning problems, such as image segmentation [6], feature extraction, descriptor design [7, 8], structure from motion [9], camera calibration [10], and dimensionality reduction [11, 12], among others.", "startOffset": 310, "endOffset": 318}, {"referenceID": 7, "context": "Our second main contribution is to instantiate the methodology to learn convolutional networks with two different and very successful types of structured layers: 1) second-order pooling [8] and 2) normalized cuts [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 5, "context": "Our second main contribution is to instantiate the methodology to learn convolutional networks with two different and very successful types of structured layers: 1) second-order pooling [8] and 2) normalized cuts [6].", "startOffset": 213, "endOffset": 216}, {"referenceID": 0, "context": "\u2217This is an extended version of the ICCV 2015 article [1] \u2020catalin.", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Our work relates both to the extensive literature in the area of (deep) neural networks (see [4] for a review) with particular emphasis on ConvNets[2, 5] and with (shallow) architectures that have been proven popular and successful in computer vision[6, 13, 8, 14].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "Our work relates both to the extensive literature in the area of (deep) neural networks (see [4] for a review) with particular emphasis on ConvNets[2, 5] and with (shallow) architectures that have been proven popular and successful in computer vision[6, 13, 8, 14].", "startOffset": 147, "endOffset": 153}, {"referenceID": 4, "context": "Our work relates both to the extensive literature in the area of (deep) neural networks (see [4] for a review) with particular emphasis on ConvNets[2, 5] and with (shallow) architectures that have been proven popular and successful in computer vision[6, 13, 8, 14].", "startOffset": 147, "endOffset": 153}, {"referenceID": 5, "context": "Our work relates both to the extensive literature in the area of (deep) neural networks (see [4] for a review) with particular emphasis on ConvNets[2, 5] and with (shallow) architectures that have been proven popular and successful in computer vision[6, 13, 8, 14].", "startOffset": 250, "endOffset": 264}, {"referenceID": 12, "context": "Our work relates both to the extensive literature in the area of (deep) neural networks (see [4] for a review) with particular emphasis on ConvNets[2, 5] and with (shallow) architectures that have been proven popular and successful in computer vision[6, 13, 8, 14].", "startOffset": 250, "endOffset": 264}, {"referenceID": 7, "context": "Our work relates both to the extensive literature in the area of (deep) neural networks (see [4] for a review) with particular emphasis on ConvNets[2, 5] and with (shallow) architectures that have been proven popular and successful in computer vision[6, 13, 8, 14].", "startOffset": 250, "endOffset": 264}, {"referenceID": 13, "context": "Our work relates both to the extensive literature in the area of (deep) neural networks (see [4] for a review) with particular emphasis on ConvNets[2, 5] and with (shallow) architectures that have been proven popular and successful in computer vision[6, 13, 8, 14].", "startOffset": 250, "endOffset": 264}, {"referenceID": 12, "context": "Bach and Jordan [13] introduced a (shallow) learning formulation for normalized cuts which we build upon in this work with several important differences.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "Moreover while [13] aims to learn the parameters of their affinity model, we learn the feature representation of the data.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "Turaga et al [15] learn a model end-to-end based on a CNN by optimizing a standard segmentation criterion.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "Deep architectures have been recently designed for visual recognition by operating on top of figure-ground regions proposals from object segmentation[16, 17].", "startOffset": 149, "endOffset": 157}, {"referenceID": 16, "context": "Deep architectures have been recently designed for visual recognition by operating on top of figure-ground regions proposals from object segmentation[16, 17].", "startOffset": 149, "endOffset": 157}, {"referenceID": 17, "context": "R-CNN [18] uses standard networks (e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "AlexNet [2] or VGG-16 [3], which is an improved, deeper AlexNet).", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "AlexNet [2] or VGG-16 [3], which is an improved, deeper AlexNet).", "startOffset": 22, "endOffset": 25}, {"referenceID": 18, "context": "SDS [19] uses two AlexNet streams, one on the original image and the second one on the image with the background of the region masked.", "startOffset": 4, "endOffset": 8}, {"referenceID": 19, "context": "Relevant are also the architectures of He et al [20, 21] having a global spatial pyramid pooling layer before the fully connected layers, which performs simple max-pooling over pyramid-structured cells of the image.", "startOffset": 48, "endOffset": 56}, {"referenceID": 20, "context": "Our architecture is also complementary to structured output formulations such as CRFs[22, 23, 14] which have been demonstrated to provide useful smoothing on top of high-performing CNN pixel classifier predictions [24].", "startOffset": 85, "endOffset": 97}, {"referenceID": 21, "context": "Our architecture is also complementary to structured output formulations such as CRFs[22, 23, 14] which have been demonstrated to provide useful smoothing on top of high-performing CNN pixel classifier predictions [24].", "startOffset": 85, "endOffset": 97}, {"referenceID": 13, "context": "Our architecture is also complementary to structured output formulations such as CRFs[22, 23, 14] which have been demonstrated to provide useful smoothing on top of high-performing CNN pixel classifier predictions [24].", "startOffset": 85, "endOffset": 97}, {"referenceID": 22, "context": "Our architecture is also complementary to structured output formulations such as CRFs[22, 23, 14] which have been demonstrated to provide useful smoothing on top of high-performing CNN pixel classifier predictions [24].", "startOffset": 214, "endOffset": 218}, {"referenceID": 23, "context": "Models developed at the same time with this work focus on the joint, end-to-end training of the deep feature extractor and the CRF[25, 26, 27].", "startOffset": 130, "endOffset": 142}, {"referenceID": 24, "context": "Models developed at the same time with this work focus on the joint, end-to-end training of the deep feature extractor and the CRF[25, 26, 27].", "startOffset": 130, "endOffset": 142}, {"referenceID": 25, "context": "Models developed at the same time with this work focus on the joint, end-to-end training of the deep feature extractor and the CRF[25, 26, 27].", "startOffset": 130, "endOffset": 142}, {"referenceID": 7, "context": "For recognition, we also illustrate deep fully trainable architectures with a type of pooling layer that proved dominant for free-form region description [8], at the time on top of standard manually designed local features such as SIFT.", "startOffset": 154, "endOffset": 157}, {"referenceID": 26, "context": "Our work is also related to kernel learning approaches over the manifold of positive-definite matrices [28].", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "Matrix partial derivatives, the basis of our work, were first systematically studied in the seminal paper [29].", "startOffset": 106, "endOffset": 110}, {"referenceID": 28, "context": "Since then it has seen interest mainly in the context of studying estimators in statistics and econometrics [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "Recently, the budding field of automatic differentiation has also shown interest in this theory when considering matrix functions[31].", "startOffset": 129, "endOffset": 133}, {"referenceID": 30, "context": "Some examples are in the context of camera calibration [32], as mentioned, for learning parameters in a normalized cuts model[13], learning the parameters of Gaussian CRFs for denoising [33] and learning deep canonical correlation models [34].", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "Some examples are in the context of camera calibration [32], as mentioned, for learning parameters in a normalized cuts model[13], learning the parameters of Gaussian CRFs for denoising [33] and learning deep canonical correlation models [34].", "startOffset": 125, "endOffset": 129}, {"referenceID": 31, "context": "Some examples are in the context of camera calibration [32], as mentioned, for learning parameters in a normalized cuts model[13], learning the parameters of Gaussian CRFs for denoising [33] and learning deep canonical correlation models [34].", "startOffset": 186, "endOffset": 190}, {"referenceID": 32, "context": "Some examples are in the context of camera calibration [32], as mentioned, for learning parameters in a normalized cuts model[13], learning the parameters of Gaussian CRFs for denoising [33] and learning deep canonical correlation models [34].", "startOffset": 238, "endOffset": 242}, {"referenceID": 33, "context": "[35] multiplied the outputs of two networks as matrices to obtain improved fine-grained recognition models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "From another direction, in an effort to generalize convolutions to general non-Euclidean and non-equally spaced grids the work of [36] realizes the necessity of spectral layers for learning the graph structure but, since the computational issues brought on in the process are not the focus there, they post-pone dealing with them.", "startOffset": 130, "endOffset": 134}, {"referenceID": 35, "context": "In [37] these are partially addressed but they limit themselves to learning parameters applied to the eigenvalues instead of learning the eigenvectors and eigenvalues as we do.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "Second-Order Pooling is one of the competitive hand-designed feature descriptors for regions [8] used in the top-performing method of the PASCAL VOC semantic segmentation, comp.", "startOffset": 93, "endOffset": 96}, {"referenceID": 36, "context": "5 track [38, 39].", "startOffset": 8, "endOffset": 16}, {"referenceID": 37, "context": "5 track [38, 39].", "startOffset": 8, "endOffset": 16}, {"referenceID": 38, "context": "It represents global high-order statistics of local descriptors inside each region by computing a covariance matrixX>X then applying a tangent space mapping [40] using the matrix logarithm, which can be computed using SVD.", "startOffset": 157, "endOffset": 161}, {"referenceID": 39, "context": "Instead of pooling over hand-designed local descriptors, such as SIFT [41], one could learn a ConvNet end-to-end, with a structured layer of the form C = log(X>X + I) (4) where I is a regularizer preventing log singularities around 0 when the covariance matrix is not full rank.", "startOffset": 70, "endOffset": 74}, {"referenceID": 5, "context": "Normalized Cuts is an influential global image segmentation method based on pairwise similarities [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 27, "context": "2 Matrix Backpropagation We call matrix backpropagation (MBP) the use of matrix calculus[29, 30, 31] to map between the partial derivatives \u2202L \u2202xl and \u2202L \u2202xl\u22121 at two consecutive layers.", "startOffset": 88, "endOffset": 100}, {"referenceID": 28, "context": "2 Matrix Backpropagation We call matrix backpropagation (MBP) the use of matrix calculus[29, 30, 31] to map between the partial derivatives \u2202L \u2202xl and \u2202L \u2202xl\u22121 at two consecutive layers.", "startOffset": 88, "endOffset": 100}, {"referenceID": 29, "context": "2 Matrix Backpropagation We call matrix backpropagation (MBP) the use of matrix calculus[29, 30, 31] to map between the partial derivatives \u2202L \u2202xl and \u2202L \u2202xl\u22121 at two consecutive layers.", "startOffset": 88, "endOffset": 100}, {"referenceID": 28, "context": "2See [30] for an in-depth treatment including questions about existence and uniqueness of the partial derivatives.", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "Now it is trivial to derive two versions of the O2P descriptor[8] by plugging in log and its derivative in the propositions above.", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "second condition we obtain a relaxed problem that can be solved, due to Ky Fan theorem, by an eigen-decomposition of M = D\u22121/2WD\u22121/2 (38) [13] propose to learn the parameters \u039b such that DY is piecewise constant because then, solving the relaxed problem is equivalent to the original problem.", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "In [13] the input features were fixed thus \u039b are the only parameters to permit the alignment.", "startOffset": 3, "endOffset": 7}, {"referenceID": 40, "context": "2 of [42]) an immediate corollary is that when J2(W ) < 1 2 \u21d2 rank(W ) = rank(EE>) (51)", "startOffset": 5, "endOffset": 9}, {"referenceID": 41, "context": "In this section we validate the proposed methodology by constructing models on standard datasets for region-based object classification, like Microsoft COCO [43], and for image segmentation on BSDS [44].", "startOffset": 157, "endOffset": 161}, {"referenceID": 42, "context": "In this section we validate the proposed methodology by constructing models on standard datasets for region-based object classification, like Microsoft COCO [43], and for image segmentation on BSDS [44].", "startOffset": 198, "endOffset": 202}, {"referenceID": 43, "context": "A matconvnet [45] implementation of our models and methods is publicly available.", "startOffset": 13, "endOffset": 17}, {"referenceID": 41, "context": "1 Segmented Object Classification on MSCOCO For recognition we use the MSCOCO dataset [43], which provides 880k segmented training instances across 80 classes, divided into training and validation sets.", "startOffset": 86, "endOffset": 90}, {"referenceID": 44, "context": "This has not been explored before in the context of deep learning because of the relatively small size of the datasets with associated object segmentations, such as PASCAL VOC [46].", "startOffset": 176, "endOffset": 180}, {"referenceID": 1, "context": "The experiments in this section use the convolutional architecture component of AlexNet[2] with the global O2P layers we propose in order to obtain DeepO2P models with both classification and fully connected (FC) layers in the same topology as Alexnet.", "startOffset": 87, "endOffset": 90}, {"referenceID": 45, "context": "Arch [47] AlexNet VGG Layer ReLU-5 ReLU-4 ReLU-5 Method NCuts NCuts DeepNCuts NCuts DeepNCuts NCuts DeepNCuts Results .", "startOffset": 5, "endOffset": 9}, {"referenceID": 42, "context": "Table 2: Segmentation results give best and average covering to the pool of ground truth segmentations on the BSDS300 dataset [44] (larger is better).", "startOffset": 126, "endOffset": 130}, {"referenceID": 45, "context": "We use as baselines the original normalized cuts [47] using intervening contour affinities as well as normalized cuts with affinities derived from non-finetuned deep features in different layers of AlexNet (ReLU-5 - the last local ReLU before the fully connected layers) and VGG (first layer in block 4 and the last one in block 5).", "startOffset": 49, "endOffset": 53}, {"referenceID": 42, "context": "We evaluate using the average and best covering metric under the Optimal Image Scale (OIS) criterion [44].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "We use both the AlexNet[2] and the VGG-16[3] architectures to feed our global layers.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "We use both the AlexNet[2] and the VGG-16[3] architectures to feed our global layers.", "startOffset": 41, "endOffset": 44}, {"referenceID": 12, "context": "In inference, we generate 8 segmentations by clustering[13] then connected components are split into separate segments.", "startOffset": 55, "endOffset": 59}], "year": 2017, "abstractText": "Deep neural network architectures have recently produced excellent results in a variety of areas in artificial intelligence and visual recognition, well surpassing traditional shallow architectures trained using hand-designed features. The power of deep networks stems both from their ability to perform local computations followed by pointwise non-linearities over increasingly larger receptive fields, and from the simplicity and scalability of the gradient-descent training procedure based on backpropagation. An open problem is the inclusion of layers that perform global, structured matrix computations like segmentation (e.g. normalized cuts) or higher-order pooling (e.g. log-tangent space metrics defined over the manifold of symmetric positive definite matrices) while preserving the validity and efficiency of an end-to-end deep training framework. In this paper we propose a sound mathematical apparatus to formally integrate global structured computation into deep computation architectures. At the heart of our methodology is the development of the theory and practice of backpropagation that generalizes to the calculus of adjoint matrix variations. We perform segmentation experiments using the BSDS and MSCOCO benchmarks and demonstrate that deep networks relying on second-order pooling and normalized cuts layers, trained end-to-end using matrix backpropagation, outperform counterparts that do not take advantage of such global layers.", "creator": "LaTeX with hyperref package"}}}