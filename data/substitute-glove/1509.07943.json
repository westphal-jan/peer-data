{"id": "1509.07943", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2015", "title": "Super-Resolution Off the Grid", "abstract": "Super - resolution much the that beyond recovering it quantum-mechanical of given sources typically bandlimited measurements, being may be manipulated, effect. This initial processing question arises at numerous microscope failure, ranging although sociological well geography could nmr, inside it makes hence to hoped (mixing) Fourier qualitative own an sometimes. Of particular interest is in apply heritability procedures which example maintaining to throttle, past the starting desirable determining different computational properties: because seek although make sprinkled Fourier phenomena (contiguous late unlike supplemental oscillations ); we hope to giving with (quantifiably) small number from optical; we their any clustering to through probably.", "histories": [["v1", "Sat, 26 Sep 2015 03:49:27 GMT  (45kb,D)", "http://arxiv.org/abs/1509.07943v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qingqing huang", "sham m kakade"], "accepted": true, "id": "1509.07943"}, "pdf": {"name": "1509.07943.pdf", "metadata": {"source": "CRF", "title": "Super-Resolution Off the Grid", "authors": ["Qingqing Huang", "Sham M. Kakade"], "emails": ["qqh@mit.edu", "sham@cs.washington.edu"], "sections": [{"heading": null, "text": "Suppose we have k point sources in d dimensions, where the points are separated by at least \u2206 from each other (in Euclidean distance). This work provides an algorithm with the following favorable guarantees:\n\u2022 The algorithm uses Fourier measurements, whose frequencies are bounded by O(1/\u2206) (up to log factors). Previous algorithms require a cutoff frequency which may be as large as \u2126( \u221a d/\u2206).\n\u2022 The number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d, with no dependence on the separation \u2206. In contrast, previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities.\nOur estimation procedure itself is simple: we take random bandlimited measurements (as opposed to taking an exponential number of measurements on the hyper-grid). Furthermore, our analysis and algorithm are elementary (based on concentration bounds for sampling and the singular value decomposition).\nar X\niv :1\n50 9.\n07 94\n3v 1\n[ cs\n.L G\n] 2\n6 Se\np 20"}, {"heading": "1 Introduction", "text": "We follow the standard mathematical abstraction of this problem (Candes & Fernandez-Granda [4, 3]): consider a d-dimensional signal x(t) modeled as a weighted sum of k Dirac measures in Rd:\nx(t) = k\u2211 j=1 wj\u03b4\u00b5(j) , (1)\nwhere the point sources, the \u00b5(j)\u2019s, are in Rd. Assume that the weights wj are complex valued, whose absolute values are lower and upper bounded by some positive constant. Assume that we are given k, the number of point sources1.\nDefine the measurement function f(s) : Rd \u2192 C to be the convolution of the point source x(t) with a low-pass point spread function ei\u03c0<s,t> as below:\nf(s) = \u222b t\u2208Rd ei\u03c0<t,s>x(dt) = k\u2211 j=1 wje i\u03c0<\u00b5(j),s>. (2)\nIn the noisy setting, the measurements are corrupted by uniformly bounded perturbation z:\nf\u0303(s) = f(s) + z(s), |z(s)| \u2264 z,\u2200s. (3)\nSuppose that we are only allowed to measure the signal x(t) by evaluating the measurement function f\u0303(s) at any s \u2208 Rd, and we want to recover the parameters of the point source signal, i.e., {wj , \u00b5(j) : j \u2208 [k]}. We follow the standard normalization to assume that:\n\u00b5(j) \u2208 [\u22121,+1]d, |wj | \u2208 [0, 1] \u2200j \u2208 [k].\nLet wmin = minj |wj | denote the minimal weight, and let \u2206 be the minimal separation of the point sources defined as follows:\n\u2206 = min j 6=j\u2032 \u2016\u00b5(j) \u2212 \u00b5(j\u2032)\u20162, (4)\nwhere we use the Euclidean distance between the point sources for ease of exposition2. These quantities are key parameters in our algorithm and analysis. Intuitively, the recovery problem is harder if the minimal separation \u2206 is small and the minimal weight wmin is small.\nThe first question is that, given exact measurements, namely z = 0, where and how many measurements should we take so that the original signal x(t) can be exactly recovered.\nDefinition 1.1 (Exact recovery). In the exact case, i.e. z = 0, we say that an algorithm achieves exact recovery with m measurements of the signal x(t) if, upon input of these m measurements, the algorithm returns the exact set of parameters {wj , \u00b5(j) : j \u2208 [k]}.\nMoreover, we want the algorithm to be measurement noise tolerant, in the sense that in the presence of measurement noise we can still recover good estimates of the point sources.\n1An upper bound of the number of point sources suffices. 2Our claims hold withut using the \u201cwrap around metric\u201d, as in [4, 3], due to our random sampling. Also, it is\npossible to extend these results for the `p-norm case.\nDefinition 1.2 (Stable recovery). In the noisy case, i.e., z \u2265 0, we say that an algorithm achieves stable recovery with m measurements of the signal x(t) if, upon input of these m measurements, the algorithm returns estimates {w\u0302j , \u00b5\u0302(j) : j \u2208 [k]} such that\nmin \u03c0\nmax { \u2016\u00b5\u0302(j) \u2212 \u00b5(\u03c0(j))\u20162 : j \u2208 [k] } \u2264 poly(d, k) z,\nwhere the min is over permutations \u03c0 on [k] and poly(d,k) is a polynomial function in d and k.\nBy definition, if an algorithm achieves stable recovery with m measurements, it also achieves exact recovery with these m measurements.\nThe terminology of \u201csuper-resolution\u201d is appropriate due to the following remarkable result (in the noiseless case) of Donoho [9]: suppose we want to accurately recover the point sources to an error of \u03b3, where \u03b3 \u2206. Naively, we may expect to require measurements whose frequency depends inversely on the desired the accuracy \u03b3. Donoho [9] showed that it suffices to obtain a finite number of measurements, whose frequencies are bounded by O(1/\u2206), in order to achieve exact recovery; thus resolving the point sources far more accurately than that which is naively implied by using frequencies of O(1/\u2206). Furthermore, the work of Candes & Fernandez-Granda [4, 3] showed that stable recovery, in the univariate case (d = 1), is achievable with a cutoff frequency of O(1/\u2206) using a convex program and a number of measurements whose size is polynomial in the relevant quantities."}, {"heading": "1.1 This work", "text": "We are interested in stable recovery procedures with the following desirable statistical and computational properties: we seek to use coarse (low frequency) measurements; we hope to take a (quantifiably) small number of measurements; we desire our algorithm run quickly. Informally, our main result is as follows:\nTheorem 1.3 (Informal statement of Theorem 3.2). For a fixed probability of error, the proposed algorithm achieves stable recovery with a number of measurements and with computational runtime that are both on the order of O((k log(k) + d)2). Furthermore, the algorithm makes measurements which are bounded in frequency by O(1/\u2206) (ignoring log factors).\nNotably, our algorithm and analysis directly deal with the multivariate case, with the univariate case as a special case. Importantly, the number of measurements and the computational runtime do not depend on the minimal separation of the point sources. This may be important even in certain low dimensional imaging applications where taking physical measurements are costly (indeed, super-resolution is important in settings where \u2206 is small). Furthermore, our technical contribution of how to decompose a certain tensor constructed with Fourier measurements may be of broader interest to related questions in statistics, signal processing, and machine learning."}, {"heading": "1.2 Comparison to related work", "text": "Table 1 summarizes the comparisons between our algorithm and the existing results. The multidimensional cutoff frequency we refer to in the table is the maximal coordinate-wise entry of any measurement frequency s (i.e. \u2016s\u2016\u221e). \u201cSDP\u201d refers to the semidefinite programming (SDP) based algorithms of Candes & Fernandez-Granda [3, 4]; in the univariate case, the number of measurements can be reduced by the method in Tang et. al. [23] (this is reflected in the table). \u201cMP\u201d refers to the matrix pencil type of methods, studied in [14] and [15] for the univariate case. Here, we are defining the infinity norm separation as \u2206\u221e = minj 6=j\u2032 \u2016\u00b5(j) \u2212 \u00b5(j \u2032)\u2016\u221e, which is understood\nas the wrap around distance on the unit circle. Cd \u2265 1 is a problem dependent constant (discussed below).\nObserve the following differences between our algorithm and prior work:\n1) Our minimal separation is measured under the `2-norm instead of the infinity norm, as in the SDP based algorithm. Note that \u2206\u221e depends on the coordinate system; in the worst case, it can underestimate the separation by a 1/ \u221a d factor, namely \u2206\u221e \u223c \u2206/ \u221a d.\n2) The computation complexity and number of measurements are polynomial in dimension d and the number of point sources k, and surprisingly do not depend on the minimal separation of the point sources! Intuitively, when the minimal separation between the point sources is small, the problem should be harder, this is only reflected in the sampling range and the cutoff frequency of the measurements in our algorithm.\n3) Furthermore, one could project the multivariate signal to the coordinates and solve multiple univariate problems (such as in [19, 17], which provided only exact recovery results). Naive random projections would lead to a cutoff frequency of O( \u221a d/\u2206).\nSDP approaches: The work in [3, 4, 10] formulates the recovery problem as a total-variation minimization problem; they then show the dual problem can be formulated as an SDP. They focused on the analysis of d = 1 and only explicitly extend the proofs for d = 2. For d \u2265 1, Ingham-type theorems (see [20, 12]) suggest that Cd = O( \u221a d).\nThe number of measurements can be reduced by the method in [23] for the d = 1 case, which is noted in the table. Their method uses sampling \u201coff the grid\u201d; technically, their sampling scheme is actually sampling random points from the grid, though with far fewer measurements.\nMatrix pencil approaches: The matrix pencil method, MUSIC and Prony\u2019s method are essentially the same underlying idea, executed in different ways. The original Prony\u2019s method directly attempts to find roots of a high degree polynomial, where the root stability has few guarantees. Other methods aim to robustify the algorithm.\nRecently, for the univariate matrix pencil method, Liao & Fannjiang [14] and Moitra [15] provide a stability analysis of the MUSIC algorithm. Moitra [15] studied the optimal relationship between the cutoff frequency and \u2206, showing that if the cutoff frequency is less than 1/\u2206, then stable recovery is not possible with matrix pencil method (with high probability)."}, {"heading": "1.3 Notation", "text": "Let R, C, and Z to denote real, complex, and natural numbers. For d \u2208 Z, [d] denotes the set [d] = {1, . . . , d}. For a set S, |S| denotes its cardinality. We use \u2295 to denote the direct sum of sets,\nnamely S1 \u2295 S2 = {(a+ b) : a \u2208 S1, b \u2208 S2}. Let en to denote the n-th standard basis vector in Rd, for n \u2208 [d]. Let PdR,2 = {x \u2208 Rd : \u2016x\u20162 = 1} to denote the d-sphere of radius R in the d-dimensional standard Euclidean space. Denote the condition number of a matrix X \u2208 Rm\u00d7n as cond2(X) = \u03c3max(X)/\u03c3min(X), where \u03c3max(X) and \u03c3min(X) are the maximal and minimal singular values of X. We use \u2297 to denote tensor product. Given matrices A,B,C \u2208 Cm\u00d7k, the tensor product\nV = A \u2297 B \u2297 C \u2208 Cm\u00d7m\u00d7m is equivalent to Vi1,i2,i3 = \u2211k\nn=1Ai1,nBi2,nCi3,n. Another view of tensor is that it defines a multi-linear mapping. For given dimension mA,mB,mC the mapping V (\u00b7, \u00b7, \u00b7) : Cm\u00d7mA \u00d7 Cm\u00d7mB \u00d7 Cm\u00d7mC \u2192 CmA\u00d7mB\u00d7mC is defined as:\n[V (XA, XB, Xc)]i1,i2,i3 = \u2211\nj1,j2,j3\u2208[m]\nVj1,j2,j3 [XA]j1,i1 [XB]j2,i2 [XC ]j3,i3 .\nIn particular, for a \u2208 Cm, we use V (I, I, a) to denote the projection of tensor V along the 3rd dimension. Note that if the tensor admits a decomposition V = A \u2297 B \u2297 C, it is straightforward to verify that\nV (I, I, a) = ADiag(C>a)B>.\nIt is well-known that if the factors A,B,C have full column rank then the rank k decomposition is unique up to re-scaling and common column permutation. Moreover, if the condition number of the factors are upper bounded by a positive constant, then one can compute the unique tensor decomposition V with stability guarantees (See [1] for a review. Lemma 3.5 herein provides an explicit statement.)."}, {"heading": "2 Warm-up", "text": ""}, {"heading": "2.1 1-D case: revisiting the matrix pencil method", "text": "Let us first review the matrix pencil method for the univariate case, which stability was recently rigorously analyzed in Liao & Fannjiang [14] and Moitra [15].\nA square matrix H is called a Hankel matrix if its skew-diagonals are constants, namely Hi,j = Hi\u22121,j+1. For some positive constants m \u2208 Z, sample to get the measurements f(s) evaluated at the sampling set S3 = {0, 1, . . . , 2m}, and construct two Hankel matrices H0, H1 \u2208 Cm\u00d7m:\nH0 =  f(0) f(1) . . . f(m\u2212 1) f(1) f(2) . . . f(m) ... ...\nf(m\u2212 1) f(m) . . . f(2m\u2212 1)\n , H1 =  f(1) f(2) . . . f(m) f(2) f(3) . . . f(m+ 1) ... ...\nf(m) f(m+ 1) . . . f(2m)  . (5) Define Dw \u2208 Ck\u00d7kdiag to be the diagonal matrix with the weights on the main diagonal: [Dw]j,j =\nwj . Define D\u00b5 \u2208 Ck\u00d7kdiag to be [D\u00b5]j,j = e i\u03c0\u00b5(j) .\nA matrix V is called a Vandermonde matrix if each column is a geometric progression. defined the Vandermonde matrix Vm \u2208 Cm\u00d7k as below:\nVm =  1 . . . 1 (ei\u03c0\u00b5 (1) )1 . . . (ei\u03c0\u00b5 (k) )1 ... ...\n(ei\u03c0\u00b5 (1) )m\u22121 . . . (ei\u03c0\u00b5 (k) )m\u22121\n . (6)\nThe two Hankel matrices H0 and H1 admit the following simultaneous diagonalization:\nH0 = VmDwV > m , H1 = VmDwD\u00b5V > m . (7)\nAs long as Vm is of full rank, this simultaneous diagonalization can be computed by solving the generalized eigenvalue problem, and the parameters of the point source can thus be obtained from the factor Vm and Dw.\nThe univariate matrix pencil method only needs m \u2265 k to achieve exact recovery. In the noisy case, the stability of generalized eigenvalue problem depends on the condition number of the Vandermonde matrix Vm and the minimal weight wmin.\nSince all the nodes (ei\u03c0\u00b5 (j)\n\u2019s) of this Vandermonde matrix lie on the unit circle in the complex plane, it is straightforward to see that asymptotically limm\u2192\u221e cond2(Vm) = 1. Furthermore, for m > 1/\u2206, [14, 15] showed that cond2(Vm) is upper bounded by a constant that does not depend on k and m. This bound on condition number is also implicitly discussed in [19].\nAnother way to view the matrix pencil method is that it corresponds to the low rank 3rd order tensor decomposition (see for example [1]). This view will help us generalize matrix pencil method to higher dimension d in a direct way, without projecting the signal on each coordinate and apply the univariate algorithm multiple times. For m \u2265 k, construct a 3rd order tensor F \u2208 Cm\u00d7m\u00d72 with elements of H0 and H1 defined in (5) as:\nFi,i\u2032,j = [Hj\u22121]i,i\u2032 , \u2200j \u2208 [2], i, i\u2032 \u2208 [m].\nNote that the two slices along the 3rd dimension of F are H0 and H1. Namely F (I, I, e1) = H0, and F (I, I, e2) = H1. Recall the matrix decomposition of H0 and H1 in (7). Since m \u2265 k and the \u00b5(j)\u2019s are distinct, we know that F has the unique rank k tensor decomposition:\nF = Vm \u2297 Vm \u2297 (V2Dw).\nGiven the tensor F , the basic idea of the well-known Jennrich\u2019s algorithm ([11, 13]) for finding the unique low rank tensor decomposition is to consider two random projections v1, v2 \u2208 Rm, and then with high probability the two matrices F (I, I, v1) and F (I, I, v2) admit simultaneous diagonalization. Therefore, the matrix pencil method is indeed a special case of Jennrich\u2019s algorithm by setting v1 = e1 and v2 = e2"}, {"heading": "2.2 The multivariate case: a toy example", "text": "One could naively extend the matrix pencil method to higher dimensions by using taking measurements from a hyper-grid, which is of size exponential in the dimension d. We now examine a toy problem which suggests that the high dimensional case may not be inherently more difficult than the univariate case.\nThe key ideas is that an appropriately sampled set can significantly reduce the number of measurements (as compared to using all the grid points). Tang et al [23] made a similar observation for the univariate case. They used a small random subset of measurements (actually still from the grid points) and showed that this contains enough information to recover all the measurement on the grid; the full measurements were then used for stably recovering the point sources.\nConsider the case where the dimension d \u2265 k. Assume that wj \u2019s are real valued, and for all j \u2208 [k] and n \u2208 [d], the parameters \u00b5(j)n are i.i.d. and uniformly distributed over [\u22121,+1]. This essentially corresponds to the standard (L2) incoherence conditions (for the \u00b5\n(j)\u2019s). 3 The following simple algorithm achieves stability with polynomial complexity.\n3 This setting is different from the 2-norm separation condition. To see the difference, note that the toy algorithm does not work for constant shift \u00b5(1) = \u00b5(2) + \u2206. This issue is resolved in the general algorithm, when the condition is stated in terms of 2-norm separation.\nFirst, take d3 number of measurements by evaluating f(s) in the set S3 = {s = en1 + en2 + en3 : [n1, n2, n3] \u2208 [d]\u00d7 [d]\u00d7 [d]}, noting that S3 contains only a subset of d3 points from the grid of [3]d. Then, construct a 3rd order tensor F \u2208 Cd\u00d7d\u00d7d with the measurements in the following way:\nFn1,n2,n3 = f(s) \u2223\u2223 s=en1+en2+en3 , \u2200n1, n2, n3 \u2208 [d].\nNote that the measurement f(e1+e2+e3) = \u2211k j=1wje i\u03c0(\u00b5 (j) 1 +\u00b5 (j) 2 +\u00b5 (j) 3 ) = \u2211k j=1wje i\u03c0\u00b5 (j) 1 ei\u03c0\u00b5 (j) 2 ei\u03c0\u00b5 (j) 3 . It is straightforward to verify that F has a rank-k tensor factorization F = Vd\u2297Vd\u2297(VdDw), where the factor Vd \u2208 Rd\u00d7k is given by:\nVd =  ei\u03c0\u00b5 (1) 1 . . . ei\u03c0\u00b5 (k) 1 ei\u03c0\u00b5 (1) 2 . . . ei\u03c0\u00b5 (k) 2 ... . . . ...\nei\u03c0\u00b5 (1) d . . . ei\u03c0\u00b5 (k) d\n . (8)\nUnder the distribution assumption of the point sources, the entries ei\u03c0\u00b5 (j) n are i.i.d. and uniformly distributed over the unit circle on the complex plane. Therefore almost surely the factor Vd has full column rank, and thus the tensor decomposition is unique. Moreover here wj \u2019s are real and each element of VS has unit norm, we have a rescaling constraint with the tensor decomposition, with which we can uniquely obtain the factor VS and the weights in Dw. By taking element-wise log of VS we can read off the parameters of the point sources from VS directly. Moreover, with high probability, we have that cond2(Vd) concentrates around 1, thus the simple algorithm achieves stable recovery."}, {"heading": "3 Main Results", "text": ""}, {"heading": "3.1 The algorithm", "text": "We briefly describe the steps of Algorithm 1 below:\n(Take measurements) Given positive numbers m and R, randomly draw a sampling set S ={ s(1), . . . s(m) } of m i.i.d. samples of the Gaussian distribution N (0, R2Id\u00d7d). Form the set S \u2032 = S \u222a {s(m+1) = e1, . . . , s(m+d) = ed, s(m+d+1) = 0} \u2282 Rd. Denote m\u2032 = m + d + 1. Take another independent random sample v from the unit sphere, and define v(1) = v, v(2) = 2v. Construct the 3rd order tensor F\u0303 \u2208 Cm\u2032\u00d7m\u2032\u00d73 with noise corrupted measurements f\u0303(s) evaluated at the points in S \u2032 \u2295 S \u2032 \u2295 {v(1), v(2)}, arranged in the following way:\nF\u0303n1,n2,n3 = f\u0303(s) \u2223\u2223 s=s(n1)+s(n2)+v(n3) ,\u2200n1, n2 \u2208 [m\u2032], n3 \u2208 [2]. (9)\n(Tensor decomposition) Define the characteristic matrix VS to be:\nVS =  ei\u03c0<\u00b5 (1),s(1)> . . . ei\u03c0<\u00b5 (k),s(1)> ei\u03c0<\u00b5 (1),s(2)> . . . ei\u03c0<\u00b5 (k),s(2)> ... . . . ...\nei\u03c0<\u00b5 (1),s(m)> . . . ei\u03c0<\u00b5 (k),s(m)>\n . (10)\nInput: R, m, noisy measurement function f\u0303(\u00b7). Output: Estimates {w\u0302j , \u00b5\u0302(j) : j \u2208 [k]}."}, {"heading": "1. Take measurements:", "text": "Let S = {s(1), . . . , s(m)} be m i.i.d. samples from the Gaussian distribution N (0, R2Id\u00d7d). Set s(m+n) = en for all n \u2208 [d] and s(m+n+1) = 0. Denote m\u2032 = m+ d+ 1. Take another random samples v from the unit sphere, and set v(1) = v and v(2) = 2v. Construct a tensor F\u0303 \u2208 Cm\u2032\u00d7m\u2032\u00d73: F\u0303n1,n2,n3 = f\u0303(s) \u2223\u2223 s=s(n1)+s(n2)+v(n3) .\n2. Tensor Decomposition: Set (V\u0302S\u2032 , D\u0302w) = TensorDecomp(F\u0303 ).\nFor j = 1, . . . , k, set [V\u0302S\u2032 ]j = [V\u0302S\u2032 ]j/[V\u0302S\u2032 ]m\u2032,j\n3. Read of estimates: For j = 1, . . . , k, set \u00b5\u0302(j) = Real(log([V\u0302S ][m+1:m+d,j])/(i\u03c0))."}, {"heading": "4. Set W\u0302 = arg minW\u2208Ck \u2016F\u0302 \u2212 V\u0302S\u2032 \u2297 V\u0302S\u2032 \u2297 V\u0302dDw\u2016F .", "text": "Algorithm 1: General algorithm\nand define matrix V \u2032 \u2208 Cm\u2032\u00d7k to be\nVS\u2032 =  VSVd 1, . . . , 1  , (11) where Vd \u2208 Cd\u00d7k is defined in (8). Define\nV2 =  ei\u03c0<\u00b5 (1),v(1)> . . . ei\u03c0<\u00b5 (k),v(1)> ei\u03c0<\u00b5 (1),v(2)> . . . ei\u03c0<\u00b5 (k),v(2)>\n1 . . . 1  . Note that in the exact case ( z = 0) the tensor F constructed in (9) admits a rank-k decomposition:\nF = VS\u2032 \u2297 VS\u2032 \u2297 (V2Dw), (12)\nAssume that VS\u2032 has full column rank, then this tensor decomposition is unique up to column permutation and rescaling with very high probability over the randomness of the random unit vector v. Since each element of VS\u2032 has unit norm, and we know that the last row of VS\u2032 and the last row of V2 are all ones, there exists a proper scaling so that we can uniquely recover wj \u2019s and columns of VS\u2032 up to common permutation.\nIn this paper, we adopt Jennrich\u2019s algorithm (see Algorithm 2) for tensor decomposition. Other algorithms, for example tensor power method ([1]) and recursive projection ([24]), which are possibly more stable than Jennrich\u2019s algorithm, can also be applied here.\n(Read off estimates) Let log(Vd) denote the element-wise logarithm of Vd. The estimates of the point sources are given by: [\n\u00b5(1), \u00b5(2), . . . , \u00b5(k) ] = log(Vd)\ni\u03c0 .\nInput: Tensor F\u0303 \u2208 Cm\u00d7m\u00d73, rank k. output: Factor V\u0302 \u2208 Cm\u00d7k.\n1. Compute the truncated SVD of F\u0303 (I, I, e1) = P\u0302 \u039b\u0302P\u0302 > with the k leading singular values.\n2. Set E\u0302 = F\u0303 (P\u0302 , P\u0302 , I). Set E\u03021 = E\u0302(I, I, e1) and E\u03022 = E\u0302(I, I, e2).\n3. Let the columns of U\u0302 be the eigenvectors of E\u03021E\u0302 \u22121 2 corresponding to the k eigenvalues with\nthe largest absolute value.\n4. Set V\u0302 = \u221a mP\u0302U\u0302 .\nAlgorithm 2: TensorDecomp\nRemark 3.1. In the toy example, the simple algorithm corresponds to using the sampling set S \u2032 = {e1, . . . , ed}. The conventional univariate matrix pencil method corresponds to using the sampling set S \u2032 = {0, 1, . . . ,m} and the set of measurements S \u2032 \u2295 S \u2032 \u2295 S \u2032 corresponds to the grid [m]3."}, {"heading": "3.2 Guarantees", "text": "In this section, we discuss how to pick the two parameters m and R and prove that the proposed algorithm indeed achieves stable recovery in the presence of measurement noise.\nTheorem 3.2 (Stable recovery). There exists a universal constant C such that the following holds. Fix x, \u03b4s, \u03b4v \u2208 (0, 12); pick m such that m \u2265 max { k x \u221a 8 log k\u03b4s , d } ;\nfor d = 1, pick R \u2265 \u221a 2 log(1+2/ x) \u03c0\u2206 ; for d \u2265 2, pick R \u2265 \u221a 2 log(k/ x) \u03c0\u2206 . Assume the bounded measurement noise model as in (3) and that z \u2264 \u2206\u03b4vw2min 100 \u221a dk5 ( 1\u22122 x 1+2 x )2.5 .\nWith probability at least (1 \u2212 \u03b4s) over the random sampling of S, and with probability at least (1\u2212\u03b4v) over the random projections in Algorithm 2, the proposed Algorithm 1 returns an estimation of the point source signal x\u0302(t) = \u2211k j=1 w\u0302j \u03b4\u0302\u00b5(j) with accuracy:\nmin \u03c0\nmax { \u2016\u00b5\u0302(j) \u2212 \u00b5(\u03c0(j))\u20162 : j \u2208 [k] } \u2264 C \u221a dk5\n\u2206\u03b4v wmax w2min ( 1 + 2 x 1\u2212 2 x )2.5 z,\nwhere the min is over permutations \u03c0 on [k]. Moreover, the proposed algorithm has time complexity in the order of O((m\u2032)3).\nProof. (of Theorem 3.2) The algorithm is correct if the tensor decomposition in Step 2 is unique, and achieves stable recovery if the tensor decomposition is stable. By the stability Lemma of tensor decomposition (Lemma 3.5), this is guaranteed if we can bound the condition number of VS\u2032 . It\nfollows from Lemma 3.8 that the condition number of VS\u2032 is at most \u221a 1 + \u221a k times of cond2(VS). By the main technical lemma (Lemma 3.10) we know that with the random sampling set S of size m, the condition number cond2(VS) is upper bounded by a constant. Thus we can bound the distance between VS\u2032 and the estimation V\u0302S\u2032 according to (13).\nSince we adopt Jennrich\u2019s algorithm for the low rank tensor decomposition, the overall computation complexity is roughly the complexity of SVD of a matrix of size m\u2032 \u00d7m\u2032, namely in the order of O((m\u2032)3).\nThe next lemma shows that essentially, with overwhelming probability, all the frequencies taken concentrate within the hyper-cube with cutoff frequency R\u2032 on each coordinate, where R\u2032 is comparable to R,\nLemma 3.3 (The cutoff frequency). For d > 1, with high probability, all of the 2(m\u2032)2 sampling frequencies in S \u2032\u2295S \u2032\u2295{v(1), v(2)} satisfy that \u2016s(j1) + s(j2) + v(j3)\u2016\u221e \u2264 R\u2032, \u2200j1, j2 \u2208 [m], j3 \u2208 [2], where the per-coordinate cutoff frequency is given by R\u2032 = O(R \u221a logmd).\nFor d = 1 case, the cutoff frequency R\u2032 can be made to be in the order of R\u2032 = O(1/\u2206).\nProof. For d > 1 case, with straightforward union bound over the m\u2032 = O(k2) samples each of which has d coordinates, one can show that the cutoff frequency is in the order of R \u221a log(kd),\nwhere R is in the order of\n\u221a log(k)\n\u2206 as shown in Theorem 3.2. For d = 1 case, we bound the cutoff frequency with slightly more careful analysis. Instead of Gaussian random samples, consider uniform samples from the interval [\u2212R\u2032, R\u2032]. We can modify the proof of Lemma 3.9 and show that if R\u2032 \u2265 1/(\u2206(1 + x)):\u2211\nj\u2032 6=j |Yj,j\u2032 | = \u2211 j\u2032 6=j 1 2R\u2032 \u222b \u2212R\u2032,R\u2032 ei\u03c0(\u00b5 j\u2032\u2212\u00b5(j))s = \u2211 j\u2032 6=j sin(\u03c0|\u00b5(j\u2032) \u2212 \u00b5(j)|R\u2032) \u03c0|\u00b5(j\u2032) \u2212 \u00b5(j)|R\u2032\n\u2264 k\u2211 l=1 sin(l\u03c0\u2206R\u2032) (l\u03c0\u2206R\u2032) \u2264 sin(\u03c0\u2206R \u2032)/(\u03c0\u2206R\u2032) 1\u2212 sin(\u03c0\u2206R\u2032)/(\u03c0\u2206R\u2032) \u2264 x\nwhere the second last inequality uses the inequality that sin(a+b)a+b \u2264 sin(a) a sin(b) b .\nRemark 3.4 (Failure probability). Overall, the failure probability consists of two pieces: \u03b4v for random projection of v, and \u03b4s for random sampling to ensure the bounded condition number of VS. This may be boosed to arbitrarily high probability through repetition."}, {"heading": "3.3 Key Lemmas", "text": "Stability of tensor decomposition: In this paragraph, we give a brief description and the stability guarantee of the well-known Jennrich\u2019s algorithm ([11, 13]) for low rank 3rd order tensor decomposition. We only state it for the symmetric tensors as appeared in the proposed algorithm.\nConsider a tensor F = V \u2297 V \u2297 (V2Dw) \u2208 Cm\u00d7m\u00d73 where the factor V has full column rank k. Then the decomposition is unique up to column permutation and rescaling, and Algorithm 2 finds the factors efficiently. Moreover, the eigen-decomposition is stable if the factor V is well-conditioned and the eigenvalues of FaF \u2020 b are well separated. Lemma 3.5 (Stability of Jennrich\u2019s algorithm). Consider the 3rd order tensor F = V \u2297 V \u2297 (V2Dw) \u2208 Cm\u00d7m\u00d73 of rank k \u2264 m, constructed as in Step 1 in Algorithm 1.\nGiven a tensor F\u0303 that is element-wise close to F , namely for all n1, n2, n3 \u2208 [m], \u2223\u2223F\u0303n1,n2,n3 \u2212\nFn1,n2,n3 \u2223\u2223 \u2264 z, and assume that the noise is small z \u2264 \u2206\u03b4vw2min100\u221adkwmaxcond2(V )5 . Use F\u0303 as the input to Algorithm 2. With probability at least (1 \u2212 \u03b4v) over the random projections v(1) and v(2), we can bound the distance between columns of the output V\u0302 and that of V by:\nmin \u03c0 max j\n{ \u2016V\u0302j \u2212 V\u03c0(j)\u20162 : j \u2208 [k] } \u2264 C \u221a dk2\n\u2206\u03b4v wmax w2min cond2(V ) 5 z, (13)\nwhere C is a universal constant.\nProof. (of Lemma 3.5) The proof is mostly based on the arguments in [16, 2], we still show the clean arguments here for our case.\nWe first introduce some notations for the exact case. Define D1 = diag([V2]1,:Dw) and D2 = diag([V2]2,:Dw). Recall that the symmetric matrix F1 = F (I, I, e1) = V D1V\n>. Consider its SVD F1 = P\u039bP >. Denote U = P>V \u2208 Ck\u00d7k. Define the whitened rank-k tensor\nE = F (P, P, I) = (P>V )\u2297 (P>V )\u2297 (V2Dw) = U \u2297 U \u2297 (V2Dw) \u2208 Ck\u00d7k\u00d73.\nDenote the two slices of the tensor E by E1 = E(I, I, e1) = UD1U > and E2 = E(I, I, e2) = UD2U >. Define M = E1E \u22121 2 , and its eigen decomposition is given by M = UDU \u22121, where D = D1D \u22121 2 . Note that in the exact case, D is given by:\nD = diag(ei\u03c0<\u00b5 (j),v(1)\u2212v(2)> : j \u2208 [k])\nNote that |Dj,j | = 1 for all j. Define the minimal separation of the diagonal entries in D to be:\nsep(D) = min{min j 6=j\u2032 |Dj,j \u2212Dj\u2032,j\u2032 |},\n1. We first apply perturbation bounds to show that the noise in F\u0303 propagates the estimates P\u0302 and E\u0302 in a mild way when the condition number of V is bounded by a constant.\nProof. Apply Wedin\u2019s matrix perturbation bound, we have:\n\u2016P\u0302 \u2212 P\u20162 \u2264 \u2016F\u03031 \u2212 F1\u20162 \u03c3min(F1)\n\u2264 z \u221a m\nwmin\u03c3min(V )2\nAnd then for the two slices of E\u0302 = F\u0303 (P\u0302 , P\u0302 , I), namely E\u0302i = Ei +Zi for i = 1, 2, we can bound the distance between estimates and the exact case, namely Zi = P\u0302 >F\u0303iP\u0302 \u2212 P>FiP , by:\n\u2016Zi\u2016 \u2264 8\u2016Fi\u2016\u2016P\u2016\u2016P\u0302 \u2212 P\u2016+ 4\u2016P\u20162\u2016F\u0303i \u2212 Fi\u2016 \u2264 16 wmax wmin cond2(V ) 2 z \u221a m\n2. Then, recall that M = E1E \u22121 2 = UDU \u22121. Note that\nM\u0302 = (E1 + Z1)(E2 + Z2) \u22121 = E1E \u22121 2 (I \u2212 Z2(I + E \u22121 2 Z2) \u22121E\u221212 ) + Z1E \u22121 2 .\nLet H and G denote the perturbation matrices:\nH = \u2212Z2(I + E\u221212 Z2) \u22121E\u221212 , G = Z1E \u22121 2 .\nIn the following claim, we show that given M\u0302 = E\u03021E\u0302 \u22121 2 = M(I+H)+G for some small perturbation matrixH andG, if the perturbation \u2016H\u2016 and \u2016G\u2016 are small enough and that sep(D) is large enough, the eigen decomposition M\u0302 = U\u0302D\u0302U\u0302\u22121 is close to that of M .\nClaim 3.6. If \u2016MH +G\u2016 \u2264 sep(D) 2 \u221a kcond2(U) , then the eigenvalues of M\u0302 are distinct and we can bound the columns of U\u0302 and U by:\nmin \u03c0 max j \u2016U\u0302j \u2212 U\u03c0(j)\u20162 \u2264 3\n\u03c3max(H)\u03c3max(D) + \u03c3max(G)\n\u03c3min(U)sep(D) \u2016U\u0302j\u20162\u2016Vj\u20162.\nProof. Let \u03bbj and Uj for j \u2208 [k] denote the eigenvalue and corresponding eigenvectors of M . If \u2016MH +G\u2016 \u2264 sep(D)\n2 \u221a kcond2(U) , we can bound\n\u2016M\u0302 \u2212M\u2016 = \u2016U\u22121(M + (MH +G))U \u2212D\u2016 = \u2016U\u22121(MH +G)U\u2016 \u2264 sep(D)/2 \u221a k,\nthus apply Gershgorin\u2019s disk theorem, we have |\u03bb\u0302j\u2212\u03bbj | \u2264 \u2016[U\u22121(MH+G)U ]j\u20161 \u2264 \u221a k\u2016[U\u22121(MH+ G)U ]j\u20162 \u2264 sep(D)/2. Therefore, the eigenvalues are distinct and we have\n|\u03bb\u0302j \u2212 \u03bbj\u2032 | \u2265 |\u03bbj \u2212 \u03bbj\u2032 | \u2212 |\u03bb\u0302j \u2212 \u03bbj | \u2265 1\n2 |\u03bbj \u2212 \u03bbj\u2032 | \u2265\n1 2 sep(D). (14)\nNote that {Uj\u2032} and {U\u0302j} define two sets of basis vectors, thus we can write U\u0302j = \u2211 j\u2032 cj\u2032Uj\u2032\n(with the correct permutation for columns of U\u0302j and Uj) for some coefficients \u2211 j\u2032 c 2 j\u2032 = 1. Apply first order Taylor expansion of eigenvector definition we have:\n\u03bb\u0302jU\u0302j = M\u0302U\u0302j = (M + (MH +G)) \u2211 j\u2032 cj\u2032Uj\u2032 = \u2211 j\u2032 \u03bbj\u2032cj\u2032Uj\u2032 + (MH +G)U\u0302j .\nSince we also have \u03bb\u0302jU\u0302j = \u2211 j\u2032 \u03bb\u0302jcj\u2032Uj\u2032 , we can write \u2211\nj\u2032(\u03bb\u0302j\u2212\u03bbj\u2032)cj\u2032Uj\u2032 = (MH+G)U\u0302j , and we can solve for the coefficients cj\u2032 \u2019s from the linear system as [(\u03bb\u0302j \u2212 \u03bbj\u2032)cj\u2032 : j\u2032 \u2208 [k]] = U\u22121(MH +G)U\u0302j . Finally plug in the inequality in (14) we have that for any j:\n\u2016U\u0302j \u2212 Uj\u201622 = \u2211 j\u2032 6=j c2j\u2032\u2016Uj\u2032\u201622 + (cj \u2212 1)2\u2016Uj\u201622\n\u2264 2 \u2211 j\u2032 6=j c2j\u2032\u2016Vj\u2032\u201622 \u2264 8\u2016U \u22121(MH +G)U\u0302j\u201622\nsep(D)2\n\u2264 8(\u03c3max(D)\u03c3max(H) + \u03c3max(G)) 2\n\u03c3min(U)2sep(D)2 \u2016U\u0302j\u201622\u2016Vj\u201622\n3. Note that in the above bound for \u2016U\u0302j \u2212Uj\u2016, we can bound the perturbation matrices H and G by:\n\u03c3max(H) \u2264 \u2016Z2\u2016 (1\u2212 \u03c3max(E\u221212 Z2))\u03c3min(E2) \u2264 \u2016Z2\u2016 \u03c3min(E2)\u2212 \u2016Z2\u2016 \u2264 \u2016Z2\u2016 \u03c3min(U)2\u03c3min(D2)\u2212 \u2016Z2\u2016 , \u03c3max(G) \u2264 \u03c3max(Z1)\n\u03c3min(E2) \u2264 \u2016Z2\u2016 \u03c3min(U)2\u03c3min(D2) ,\nNote that \u03c3min(D2) \u2265 wmin and \u03c3max(D) = 1 by definition. In the following claim, we apply anti-concentration bound to show that with high probability sep(D) is large.\nClaim 3.7. For any \u03b4v \u2208 (0, 1), with probability at least 1\u2212 \u03b4v, we can bound sep(D) by:\nsep(D) \u2265 \u2206\u03b4v\u221a dk2 .\nProof. Denote v = v(1) \u2212 v(2), and note that \u2016v\u2016 \u2264 \u221a 2. In the regime we concern, for any pair j 6= j\u2032, we have |ei\u03c0<\u00b5(j),v> \u2212 ei\u03c0<\u00b5(j \u2032),v>| \u2264 | < \u00b5(j) \u2212 \u00b5(j\u2032), v > |. Apply Lemma 4.3, we have that for \u03b4 \u2208 (0, 1),\nP(| < \u00b5(j) \u2212 \u00b5(j\u2032), v > | \u2264 \u2016\u00b5(j) \u2212 \u00b5(j\u2032)\u2016 \u03b4\u221a d ) \u2264 \u03b4.\nTake a union bound over all pairs of j 6= j\u2032, we have that\nP (\nfor somej 6= j\u2032, | < \u00b5(j) \u2212 \u00b5(j\u2032), v > | \u2264 \u2016\u00b5(j) \u2212 \u00b5(j\u2032)\u2016 \u03b4\u221a dk2\n) \u2264 k2 \u03b4\nk2 = \u03b4.\nRecall that \u2206 = minj 6=j\u2032 \u2016\u00b5(j) \u2212 \u00b5(j \u2032)\u2016.\n4. Recall that U = P>V . Note that since P has orthonormal columns, we have \u03c3min(U) = \u03c3min(V ) and \u2016Ui\u2016 \u2264 \u2016Vi\u2016 = \u221a m.\nFinally we apply perturbation bound to the estimates V\u0302i = P\u0302 U\u0302i and conclude with the above inequalities:\n\u2016V\u0302i \u2212 Vi\u2016 \u2264 2(\u2016P\u0302 \u2212 P\u2016\u2016Ui\u2016+ \u2016P\u2016\u2016U\u0302i \u2212 Ui\u2016) \u2264 2 ( z \u221a m\nwmin\u03c3min(V )2 + 3\n\u03c3max(H)\u03c3max(D) + \u03c3max(G)\n\u03c3min(U)sep(D) \u2016Vi\u2016\n) \u2016Vi\u2016\n\u2264 2 (\nz \u221a m\nwmin\u03c3min(V )2 + 6 \u2016Z2\u2016\u2016Vi\u2016 (\u03c3min(V )2\u03c3min(D2)\u2212 \u2016Z2\u2016)\u03c3min(V )sep(D)\n) \u2016Vi\u2016\n\u2264 C( \u221a dk2m\n\u2206\u03b4v\nwmaxcond2(V ) 2\nw2min\u03c3min(V ) 3\n)\u2016Vi\u2016 z,\nfor some universal constant C. Note that the last inequality used the assumption that z is small enough.\nCondition number of VS\u2032: The following lemma is helpful:\nLemma 3.8. Let VS\u2032 \u2208 C(m+d+1)\u00d7k be the factor as defined in (11). Recall that VS\u2032 = [VS ;Vd; 1], where Vd is defined in (8), and VS is the characteristic matrix defined in (10).\nWe can bound the condition number of VS\u2032 by cond2(VS\u2032) \u2264 \u221a 1 + \u221a kcond2(VS). (15)\nProof. (of Lemma 3.8) By definition, there exist some constants \u03bb and \u03bb\u2032 such that cond2(VS) = \u03bb\u2032/\u03bb, and for all w \u2208 Pk1,2, we have \u03bb \u2264 \u2016VSw\u2016 \u2264 \u03bb\u2032. Note that each element of the factor VS\u2032 lies on the unit circle in the complex plane, then we have:\n\u03bb2 \u2264 \u2016VSw\u201622 \u2264 \u2016VS\u2032w\u201622 \u2264 (\u03bb\u2032)2 + \u221a kd.\nWe can bound the condition number of VS\u2032 by:\ncond2(VS\u2032) \u2264\n\u221a (\u03bb\u2032)2 + \u221a kd\n\u03bb2 =\n\u221a 1 + \u221a kd\n(\u03bb\u2032)2 cond2(VS) \u2264\n\u221a 1 + \u221a kcond2(VS),\nwhere the last inequality is because that maxw \u2016VSw\u201622 \u2265 \u2016VSe1\u201622 = d, we have (\u03bb\u2032)2 \u2265 d.\nCondition number of the characteristic matrix VS: Therefore, the stability analysis of the proposed algorithm boils down to understanding the relation between the random sampling set S and the condition number of the characteristic matrix VS . This is analyzed in Lemma 3.10 (main technical lemma).\nLemma 3.9. For any fixed number x \u2208 (0, 1/2). Consider a Gaussian vector s with distribution N (0, R2Id\u00d7d), where R \u2265 \u221a 2 log(k/ x) \u03c0\u2206 for d \u2265 2, and R \u2265 \u221a 2 log(1+2/ x) \u03c0\u2206 for d = 1. Define the Hermitian random matrix Xs \u2208 Ck\u00d7kherm to be\nXs =  e\u2212i\u03c0<\u00b5 (1),s> e\u2212i\u03c0<\u00b5 (2),s> ...\ne\u2212i\u03c0<\u00b5 (k),s>\n [ ei\u03c0<\u00b5 (1),s>, ei\u03c0<\u00b5 (2),s>, . . . ei\u03c0<\u00b5 (k),s> ] . (16)\nWe can bound the spectrum of Es[Xs] by:\n(1\u2212 x)Ik\u00d7k Es[Xs] (1 + x)Ik\u00d7k. (17)\nProof. (of Lemma 3.9) Denote Y = Es[Xs]. Note that Yj,j = 1 for all diagonal entries. For d = 1 case, the point sources all lie on the interval [\u22121, 1], we can bound the summation of the off diagonal entries in the matrix Y by:\u2211\nj\u2032 6=j |Yj,j\u2032 | = Es[ei\u03c0<\u00b5\n(j\u2032)\u2212\u00b5(j),s>]\n= \u2211 j\u2032 6=j e\u2212 1 2 \u03c02\u2016\u00b5(j)\u2212\u00b5(j\u2032)\u201622R2 \u2264 2(e\u2212 1 2 (\u03c0\u2206R)2 + e\u2212 1 2 (\u03c0(2\u2206)R)2 + \u00b7 \u00b7 \u00b7+ e\u2212 1 2 (\u03c0(k/2)\u2206R)2)\n\u2264 2e\u2212 1 2 (\u03c0\u2206R)2/(1\u2212 e\u2212 1 2 (\u03c0\u2206R)2)\n\u2264 x.\nFor d \u2265 2 case, we simply bound each off-diagonal entries by:\nYj,j\u2032 = e \u2212 1 2 \u03c02\u2016\u00b5(j)\u2212\u00b5(j\u2032)\u201622R2 \u2264 e\u2212 1 2 \u03c02\u22062R2 \u2264 x/k.\nApply Lemma 4.2 (Gershgorin\u2019s Disk Theorem) and we know that all the eigenvalues of Y are bounded by 1\u00b1 x.\nLemma 3.10 (Main technical lemma). In the same setting of Lemma 3.9, Let S = {s(1), . . . , s(m)} be m independent samples of the Gaussian vector s. For m \u2265 k x \u221a 8 log k\u03b4s , with probability at least 1\u2212 \u03b4s over the random sampling, the condition number of the factor VS is bounded by:\ncond2(VS) \u2264 \u221a\n1 + 2 x 1\u2212 2 x . (18)\nProof. (of Lemma 3.10) Let {X(1), . . . , X(m)} denote the i.i.d. samples of the random matrix Xs defined in (16), with s evaluated at the i.i.d. random samples in S. Note that we have\n\u2016VSw\u201622 = w>V \u2217S VSw = w> ( 1\nm m\u2211 i=1 X(i)\n) w.\nBy definition of condition number, to show that cond2(VS) \u2264 \u221a\n1+2 x 1\u22122 x , it suffices to show that\n(1\u2212 2 x)Ik\u00d7k\n( 1\nm m\u2211 i=1 X(i)\n) (1 + 2 x)Ik\u00d7k.\nBy Lemma 3.9, the spectrum of Es[Xs] lies in (1\u2212 x, 1 + x). Here we only need to show that the spectrum of the sample mean ( 1 m \u2211m i=1X (i) )\nis close to the spectrum of the expectation Es[Xs]. Since each element of the random matrix Xs \u2208 Ck\u00d7k lies on the unit circle in the complex plane, we have X2s k2I almost surely. Therefore we can apply Lemma 4.1 (Matrix Hoeffding) to show that for m > k x \u221a 8 log k\u03b4s , with probability at least 1\u2212\u03b4s, it holds that \u2016 1 m \u2211m i=1X (i)\u2212Es[Xs]\u20162 \u2264 x."}, {"heading": "4 Discussions", "text": ""}, {"heading": "4.1 Numerical results", "text": "We empirically demonstrate the performance of the proposed super-resolution algorithm in this section.\nFirst, we look at a simple instance with dimension d = 2 and the minimal separation \u2206 = 0.05. Our perturbation analysis of the stability result limits to small noise, i.e. z is inverse polynomially small in the dimensions, and the number of measurements m needs to be polynomially large in the dimensions. However, we believe these are only the artifact of the crude analysis, instead of being intrinsic to the approach. In the following numerical example, we examine a typical instance of 8 randomly generated 2-D point sources. The minimal separation \u2206 is set to be 0.01, and the weights are uniformly distributed in [0.1, 1.1] The measurement noise level z is set to be 0.1, and we take only 2178 noisy measurements ( 1/\u22062). Figure 1 shows reasonably good recovery result.\nNext, we examine the phase transition properties implied by the main theorem.\nFigure 2 shows the dependency between the cutoff frequency and the minimal separation. For each fixed pair of the minimal separation and the cutoff frequency (\u2206, R), we randomly generate k = 8 point sources in 4-dimensional space while maintaining the same minimal separation. The weights are uniformly distributed in [0.1, 1.1]. The recovery is considered successful if the error\u2211\nj\u2208[k] \u221a \u2016\u00b5\u0302(j) \u2212 \u00b5(j)\u201622 \u2264 0.1 (on average it tolerates around 4% error per coordinate per point source). This process is repeated 50 times and the rate of success was recorded. Figure 2 plots the success rate in gray-scale, where 0 is black and 1 is white.\nWe observe that there is a sharp phase transition characterized by a linear relation between the cutoff frequency and the inverse of minimal separation, which is implied by Theorem 3.2.\nThe recovery is considered successful if the error\n\u2211\nj\u2208[k]\n\u221a\n\u2016\u00b5\u0302(j) \u2212 \u00b5(j)\u201622 \u2264 0.1. This process is\nrepeated 50 times and the rate of success was recorded.\nIn a similar setup, we examine the success rate while varying the minimal separation \u2206 and the number of measurement m.\nIn Figure 3, we observe that there is a threshold of m below which the number of measurements is too small to achieve stable recovery; when m is above the threshold, the success rate increases with the number of measurements as the algorithm becomes more stable. However, note that given the appropriately chosen cutoff frequency R, the number of measurements required does not depend on the minimal separation, and thus the computation complexity does not depend on the minimal separation neither."}, {"heading": "4.2 Connection with learning GMMs", "text": "One reason we are interested in the scaling of the algorithm with respect to the dimension d is that it naturally leads to an algorithm for learning Gaussian mixture models (GMMs).\nWe first state the problem: given a number of N i.i.d. samples coming from a random one out of k Gaussian distributions in d dimensional space, the learning problem asks to estimate the\nmeans and the covariance matrices of these Gaussian components, as well as the mixing weights. We denote the parameters by {(wj , \u00b5(j),\u03a3(j))}i\u2208[k] where the mean vectors \u00b5(j) \u2208 [\u22121,+1]d, the covariance matrices \u03a3(j) \u2208 Rd\u00d7d and the mixing weights wj \u2208 R+. Learning mixture of Gaussians is a fundamental problem in statistics and machine learning, whose study dates back to Pearson[18] in the 1900s, and later arise in numerous areas of applications.\nIn this brief discussion, we only consider the case where the components are spherical Gaussians with common covariance matrices, namely \u03a3(j) = \u03c32Id\u00d7d for all j. Moreover, we define the separation \u2206G by:\n\u2206G = minj 6=j\u2032 \u2016\u00b5(j) \u2212 \u00b5(j \u2032)\u20162 \u03c3 ,\nand we will focus on the well-separated case where \u2206G is sufficiently large. This class of wellseparated GMMs is often used in data clustering.\nBy the law of large numbers, for large d, the probability mass of a d-dimensional Gaussian distribution tightly concentrates within a thin shell with a \u221a d\u03c3 distance from the mean vector. This concentration of distance leads to a line of works of provably learning GMMs in the wellseparated case, started by the seminal work of Dasgupta[6] (spherical and identical \u03a3, \u2206G \u2265 \u2126(d1/2), complexity poly(d, k)) and followed by works of Dasgupta & Schulman [8] (spherical and identical \u03a3, d log(k), \u2206G \u2265 \u2126(d1/4), complexity poly(d, k)), Arora & Kannan [21] (general and identical \u03a3, \u2206G \u2265 \u2126(d1/4) complexity O(kd)).\nInstead of relying on the concentration of distance and use distance based clustering to learn the GMM, we observe that in the well-separated case the characteristic function of the GMM has nice properties, and one can exploit the concentration of the characteristic function to learn the parameters. Note that we do not impose any other assumption on the dimensions k and d.\nNext, we sketch the basic idea of applying the proposed super-resolution algorithm to learn well-separated GMMs, guaranteeing that N the required number of samples from the GMM, as well as the computation complexity both are in the order of poly(d, k). Since \u03c3 is a bounded scalar parameter, we can simply apply grid-search to find the best match. In the following we assume that the \u03c3 is given and focus on learning the mean vectors and the mixing weights.\nEvaluate the characteristic function of a d dimensional Gaussian mixture X, with identical and spherical covariance matrix \u03a3 = \u03c32Id\u00d7d, at s \u2208 Rd:\n\u03c6X(s) = E[ei<x,s>] = \u2211 j\u2208[k] wje \u2212 1 2 \u03c32\u2016s\u201622+i<\u00b5(j),s>.\nAlso we let \u03c6\u0302X(s) denote the empirical characteristic function evaluated at s based on N i.i.d. samples {x1, . . . xN} drawn from this GMM:\n\u03c6\u0302X(s) = 1\nN \u2211 l\u2208[N ] ei<xl,s>.\nNote that |ei<xl,s>| = 1 for all samples, thus we can apply Bernstein concentration inequality to the characteristic function and argue that |\u03c6\u0302X(s)\u2212 \u03c6X(s)| \u2264 O( 1\u221aN ) for all s.\nIn order to apply the proposed super-resolution algorithm, define\nf(s) = e 1 2 \u03c32\u03c02\u2016s\u201622\u03c6X(\u03c0s) = \u2211 j\u2208[k] wje i\u03c0<\u00b5(j),s>, and f\u0303(s) = e 1 2 \u03c02\u03c32\u2016s\u201622 \u03c6\u0302X(s).\nIn the context of learning GMM, taking measurements of f\u0303(s) corresponding to evaluating the empirical characteristic function at different s, for \u2016s\u2016\u221e \u2264 R, where R is the cutoff frequency. Note that this implies \u2016s\u201622 \u2264 dR2. Therefore, we have that with high probability the noise level z can be bounded by\nz = max \u2016s\u2016\u221e\u2264R\n|f(s)\u2212 f\u0303(s)| = O\n( e\u03c3 2dR2\n\u221a N\n) .\nIn order to achieve stable recovery of the mean vector \u00b5(j)\u2019s using the proposed algorithm, on one hand, we need the cutoff frequency R = \u2126(1/\u03c3\u2206G); on the other hand, we need the noise level z = o(1). It suffices to require \u03c3\n2dR2 = o(1), namely having large enough separation \u2206G \u2265 \u2126(d1/2). In summary, when the separation condition is satisfied, to achieve target accuracy in estimating the parameters, we need the noise level z to be upper bounded by some inverse polynomial in the dimensions, and this is equivalent to requiring the number of samples from the GMM to be lower bounded by poly(k, d).\nAlthough this algorithm does not outperform the scaling result in Dasgupta[6], it still sheds light on a different approach of learning GMMs. We leave it as future work to apply super-resolution algorithms to learn more general cases of GMMs or even learning mixtures of log-concave densities."}, {"heading": "4.3 Open problems", "text": "In a recent work, Chen & Chi [5] showed that via structured matrix completion, the sample complexity for stable recovery can be reduced to O(k log4 d). However, the computation complexity is still in the order of O(kd) as the Hankel matrix is of dimension O(kd) and a semidefinite program is used to complete the matrix. It remains an open problem to reduce the sample complexity of our\nalgorithm from O(k2) to the information theoretical bound O(k), while retaining the polynomial scaling of the computation complexity.\nRecently, Schiebinger et al [22] studied the problem of learning a mixture of shifted and re-scaled point spread functions f(s) = \u2211 j wj\u03d5(s, \u00b5 (j)). This model has the Gaussian mixture as a special case, with the point spread function being Gaussian point spread \u03d5(s, \u00b5(j)) = e\u2212(s\u2212\u00b5 (j))>\u03a3\u22121j (s\u2212\u00b5\n(j)). We have discussed the connection between super-resolution and learning GMM. Another interesting open problem is to generalize the proposed algorithm to learn mixture of broader classes of nonlinear functions."}, {"heading": "Acknowledgments", "text": "The authors thank Rong Ge and Ankur Moitra for very helpful discussions. Sham Kakade acknowledges funding from the Washington Research Foundation for innovation in Data-intensive Discovery."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "The Journal of Machine Learning Research, 15(1):2773\u20132832,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "arXiv preprint arXiv:1203.0683,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Super-resolution from noisy data", "author": ["E.J. Cand\u00e8s", "C. Fernandez-Granda"], "venue": "Journal of Fourier Analysis and Applications, 19(6):1229\u20131254,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards a mathematical theory of super-resolution", "author": ["E.J. Cand\u00e8s", "C. Fernandez-Granda"], "venue": "Communications on Pure and Applied Mathematics, 67(6):906\u2013956,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust spectral compressed sensing via structured matrix completion", "author": ["Y. Chen", "Y. Chi"], "venue": "Information Theory, IEEE Transactions on, 60(10):6576\u20136601,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning mixtures of gaussians", "author": ["S. Dasgupta"], "venue": "Foundations of Computer Science, 1999. 40th Annual Symposium on, pages 634\u2013644. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random structures and algorithms, 22(1):60\u201365,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "A two-round variant of em for gaussian mixtures", "author": ["S. Dasgupta", "L.J. Schulman"], "venue": "Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 152\u2013 159. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Superresolution via sparsity constraints", "author": ["D.L. Donoho"], "venue": "SIAM Journal on Mathematical Analysis, 23(5):1309\u20131331,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "A Convex-programming Framework for Super-resolution", "author": ["C. Fernandez-Granda"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Foundations of the parafac procedure: Models and conditions for an \u201dexplanatory\u201d multi-modal factor analysis", "author": ["R.A. Harshman"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1970}, {"title": "Fourier series in control theory", "author": ["V. Komornik", "P. Loreti"], "venue": "Springer Science & Business Media,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "A decomposition for three-way arrays", "author": ["S. Leurgans", "R. Ross", "R. Abel"], "venue": "SIAM Journal on Matrix Analysis and Applications, 14(4):1064\u20131083,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Music for single-snapshot spectral estimation: Stability and superresolution", "author": ["W. Liao", "A. Fannjiang"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "The threshold for super-resolution via extremal functions", "author": ["A. Moitra"], "venue": "arXiv preprint arXiv:1408.1681,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning nonsingular phylogenies and hidden markov models", "author": ["E. Mossel", "S. Roch"], "venue": "Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, pages 366\u2013 375. ACM,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Noise space decomposition method for twodimensional sinusoidal model", "author": ["S. Nandi", "D. Kundu", "R.K. Srivastava"], "venue": "Computational Statistics & Data Analysis, 58:147\u2013161,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Contributions to the mathematical theory of evolution", "author": ["K. Pearson"], "venue": "Philosophical Transactions of the Royal Society of London. A, pages 71\u2013110,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1894}, {"title": "Parameter estimation for nonincreasing exponential sums by pronylike methods", "author": ["D. Potts", "M. Tasche"], "venue": "Linear Algebra and its Applications, 439(4):1024\u20131039,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Controllability and stabilizability theory for linear partial differential equations: recent progress and open questions", "author": ["D.L. Russell"], "venue": "Siam Review, 20(4):639\u2013739,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1978}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["A. Sanjeev", "R. Kannan"], "venue": "Proceedings of the thirty-third annual ACM symposium on Theory of computing, pages 247\u2013257. ACM,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Superresolution without separation", "author": ["G. Schiebinger", "E. Robeva", "B. Recht"], "venue": "arXiv preprint arXiv:1506.03144,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressed sensing off the grid", "author": ["G. Tang", "B.N. Bhaskar", "P. Shah", "B. Recht"], "venue": "Information Theory, IEEE Transactions on, 59(11):7465\u20137490,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Max vs min: Independent component analysis with nearly linear sample complexity", "author": ["S.S. Vempala", "Y.F. Xiao"], "venue": "arXiv preprint arXiv:1412.2954,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "We follow the standard mathematical abstraction of this problem (Candes & Fernandez-Granda [4, 3]): consider a d-dimensional signal x(t) modeled as a weighted sum of k Dirac measures in Rd:", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "We follow the standard mathematical abstraction of this problem (Candes & Fernandez-Granda [4, 3]): consider a d-dimensional signal x(t) modeled as a weighted sum of k Dirac measures in Rd:", "startOffset": 91, "endOffset": 97}, {"referenceID": 0, "context": "We follow the standard normalization to assume that: \u03bc \u2208 [\u22121,+1], |wj | \u2208 [0, 1] \u2200j \u2208 [k].", "startOffset": 74, "endOffset": 80}, {"referenceID": 3, "context": "Our claims hold withut using the \u201cwrap around metric\u201d, as in [4, 3], due to our random sampling.", "startOffset": 61, "endOffset": 67}, {"referenceID": 2, "context": "Our claims hold withut using the \u201cwrap around metric\u201d, as in [4, 3], due to our random sampling.", "startOffset": 61, "endOffset": 67}, {"referenceID": 8, "context": "The terminology of \u201csuper-resolution\u201d is appropriate due to the following remarkable result (in the noiseless case) of Donoho [9]: suppose we want to accurately recover the point sources to an error of \u03b3, where \u03b3 \u2206.", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "Donoho [9] showed that it suffices to obtain a finite number of measurements, whose frequencies are bounded by O(1/\u2206), in order to achieve exact recovery; thus resolving the point sources far more accurately than that which is naively implied by using frequencies of O(1/\u2206).", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "Furthermore, the work of Candes & Fernandez-Granda [4, 3] showed that stable recovery, in the univariate case (d = 1), is achievable with a cutoff frequency of O(1/\u2206) using a convex program and a number of measurements whose size is polynomial in the relevant quantities.", "startOffset": 51, "endOffset": 57}, {"referenceID": 2, "context": "Furthermore, the work of Candes & Fernandez-Granda [4, 3] showed that stable recovery, in the univariate case (d = 1), is achievable with a cutoff frequency of O(1/\u2206) using a convex program and a number of measurements whose size is polynomial in the relevant quantities.", "startOffset": 51, "endOffset": 57}, {"referenceID": 2, "context": "\u201cSDP\u201d refers to the semidefinite programming (SDP) based algorithms of Candes & Fernandez-Granda [3, 4]; in the univariate case, the number of measurements can be reduced by the method in Tang et.", "startOffset": 97, "endOffset": 103}, {"referenceID": 3, "context": "\u201cSDP\u201d refers to the semidefinite programming (SDP) based algorithms of Candes & Fernandez-Granda [3, 4]; in the univariate case, the number of measurements can be reduced by the method in Tang et.", "startOffset": 97, "endOffset": 103}, {"referenceID": 22, "context": "[23] (this is reflected in the table).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "\u201cMP\u201d refers to the matrix pencil type of methods, studied in [14] and [15] for the univariate case.", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "\u201cMP\u201d refers to the matrix pencil type of methods, studied in [14] and [15] for the univariate case.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "3) Furthermore, one could project the multivariate signal to the coordinates and solve multiple univariate problems (such as in [19, 17], which provided only exact recovery results).", "startOffset": 128, "endOffset": 136}, {"referenceID": 16, "context": "3) Furthermore, one could project the multivariate signal to the coordinates and solve multiple univariate problems (such as in [19, 17], which provided only exact recovery results).", "startOffset": 128, "endOffset": 136}, {"referenceID": 2, "context": "SDP approaches: The work in [3, 4, 10] formulates the recovery problem as a total-variation minimization problem; they then show the dual problem can be formulated as an SDP.", "startOffset": 28, "endOffset": 38}, {"referenceID": 3, "context": "SDP approaches: The work in [3, 4, 10] formulates the recovery problem as a total-variation minimization problem; they then show the dual problem can be formulated as an SDP.", "startOffset": 28, "endOffset": 38}, {"referenceID": 9, "context": "SDP approaches: The work in [3, 4, 10] formulates the recovery problem as a total-variation minimization problem; they then show the dual problem can be formulated as an SDP.", "startOffset": 28, "endOffset": 38}, {"referenceID": 19, "context": "For d \u2265 1, Ingham-type theorems (see [20, 12]) suggest that Cd = O( \u221a d).", "startOffset": 37, "endOffset": 45}, {"referenceID": 11, "context": "For d \u2265 1, Ingham-type theorems (see [20, 12]) suggest that Cd = O( \u221a d).", "startOffset": 37, "endOffset": 45}, {"referenceID": 22, "context": "The number of measurements can be reduced by the method in [23] for the d = 1 case, which is noted in the table.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "Recently, for the univariate matrix pencil method, Liao & Fannjiang [14] and Moitra [15] provide a stability analysis of the MUSIC algorithm.", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "Recently, for the univariate matrix pencil method, Liao & Fannjiang [14] and Moitra [15] provide a stability analysis of the MUSIC algorithm.", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "Moitra [15] studied the optimal relationship between the cutoff frequency and \u2206, showing that if the cutoff frequency is less than 1/\u2206, then stable recovery is not possible with matrix pencil method (with high probability).", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "Moreover, if the condition number of the factors are upper bounded by a positive constant, then one can compute the unique tensor decomposition V with stability guarantees (See [1] for a review.", "startOffset": 177, "endOffset": 180}, {"referenceID": 13, "context": "1 1-D case: revisiting the matrix pencil method Let us first review the matrix pencil method for the univariate case, which stability was recently rigorously analyzed in Liao & Fannjiang [14] and Moitra [15].", "startOffset": 187, "endOffset": 191}, {"referenceID": 14, "context": "1 1-D case: revisiting the matrix pencil method Let us first review the matrix pencil method for the univariate case, which stability was recently rigorously analyzed in Liao & Fannjiang [14] and Moitra [15].", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "Furthermore, for m > 1/\u2206, [14, 15] showed that cond2(Vm) is upper bounded by a constant that does not depend on k and m.", "startOffset": 26, "endOffset": 34}, {"referenceID": 14, "context": "Furthermore, for m > 1/\u2206, [14, 15] showed that cond2(Vm) is upper bounded by a constant that does not depend on k and m.", "startOffset": 26, "endOffset": 34}, {"referenceID": 18, "context": "This bound on condition number is also implicitly discussed in [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "Another way to view the matrix pencil method is that it corresponds to the low rank 3rd order tensor decomposition (see for example [1]).", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "For m \u2265 k, construct a 3rd order tensor F \u2208 Cm\u00d7m\u00d72 with elements of H0 and H1 defined in (5) as: Fi,i\u2032,j = [Hj\u22121]i,i\u2032 , \u2200j \u2208 [2], i, i\u2032 \u2208 [m].", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "Given the tensor F , the basic idea of the well-known Jennrich\u2019s algorithm ([11, 13]) for finding the unique low rank tensor decomposition is to consider two random projections v1, v2 \u2208 Rm, and then with high probability the two matrices F (I, I, v1) and F (I, I, v2) admit simultaneous diagonalization.", "startOffset": 76, "endOffset": 84}, {"referenceID": 12, "context": "Given the tensor F , the basic idea of the well-known Jennrich\u2019s algorithm ([11, 13]) for finding the unique low rank tensor decomposition is to consider two random projections v1, v2 \u2208 Rm, and then with high probability the two matrices F (I, I, v1) and F (I, I, v2) admit simultaneous diagonalization.", "startOffset": 76, "endOffset": 84}, {"referenceID": 22, "context": "Tang et al [23] made a similar observation for the univariate case.", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "First, take d3 number of measurements by evaluating f(s) in the set S3 = {s = en1 + en2 + en3 : [n1, n2, n3] \u2208 [d]\u00d7 [d]\u00d7 [d]}, noting that S3 contains only a subset of d3 points from the grid of [3]d.", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "Construct the 3rd order tensor F\u0303 \u2208 Cm\u00d7m\u00d73 with noise corrupted measurements f\u0303(s) evaluated at the points in S \u2032 \u2295 S \u2032 \u2295 {v(1), v(2)}, arranged in the following way: F\u0303n1,n2,n3 = f\u0303(s) \u2223\u2223 s=s(n1)+s(n2)+v(n3) ,\u2200n1, n2 \u2208 [m\u2032], n3 \u2208 [2].", "startOffset": 231, "endOffset": 234}, {"referenceID": 0, "context": "Other algorithms, for example tensor power method ([1]) and recursive projection ([24]), which are possibly more stable than Jennrich\u2019s algorithm, can also be applied here.", "startOffset": 51, "endOffset": 54}, {"referenceID": 23, "context": "Other algorithms, for example tensor power method ([1]) and recursive projection ([24]), which are possibly more stable than Jennrich\u2019s algorithm, can also be applied here.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "For d > 1, with high probability, all of the 2(m\u2032)2 sampling frequencies in S \u2032\u2295S \u2032\u2295{v(1), v(2)} satisfy that \u2016s(j1) + s(j2) + v3\u2016\u221e \u2264 R\u2032, \u2200j1, j2 \u2208 [m], j3 \u2208 [2], where the per-coordinate cutoff frequency is given by R\u2032 = O(R \u221a logmd).", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "3 Key Lemmas Stability of tensor decomposition: In this paragraph, we give a brief description and the stability guarantee of the well-known Jennrich\u2019s algorithm ([11, 13]) for low rank 3rd order tensor decomposition.", "startOffset": 163, "endOffset": 171}, {"referenceID": 12, "context": "3 Key Lemmas Stability of tensor decomposition: In this paragraph, we give a brief description and the stability guarantee of the well-known Jennrich\u2019s algorithm ([11, 13]) for low rank 3rd order tensor decomposition.", "startOffset": 163, "endOffset": 171}, {"referenceID": 15, "context": "5) The proof is mostly based on the arguments in [16, 2], we still show the clean arguments here for our case.", "startOffset": 49, "endOffset": 56}, {"referenceID": 1, "context": "5) The proof is mostly based on the arguments in [16, 2], we still show the clean arguments here for our case.", "startOffset": 49, "endOffset": 56}, {"referenceID": 17, "context": "Learning mixture of Gaussians is a fundamental problem in statistics and machine learning, whose study dates back to Pearson[18] in the 1900s, and later arise in numerous areas of applications.", "startOffset": 124, "endOffset": 128}, {"referenceID": 5, "context": "This concentration of distance leads to a line of works of provably learning GMMs in the wellseparated case, started by the seminal work of Dasgupta[6] (spherical and identical \u03a3, \u2206G \u2265 \u03a9(d1/2), complexity poly(d, k)) and followed by works of Dasgupta & Schulman [8] (spherical and identical \u03a3, d log(k), \u2206G \u2265 \u03a9(d1/4), complexity poly(d, k)), Arora & Kannan [21] (general and identical \u03a3, \u2206G \u2265 \u03a9(d1/4) complexity O(kd)).", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "This concentration of distance leads to a line of works of provably learning GMMs in the wellseparated case, started by the seminal work of Dasgupta[6] (spherical and identical \u03a3, \u2206G \u2265 \u03a9(d1/2), complexity poly(d, k)) and followed by works of Dasgupta & Schulman [8] (spherical and identical \u03a3, d log(k), \u2206G \u2265 \u03a9(d1/4), complexity poly(d, k)), Arora & Kannan [21] (general and identical \u03a3, \u2206G \u2265 \u03a9(d1/4) complexity O(kd)).", "startOffset": 262, "endOffset": 265}, {"referenceID": 20, "context": "This concentration of distance leads to a line of works of provably learning GMMs in the wellseparated case, started by the seminal work of Dasgupta[6] (spherical and identical \u03a3, \u2206G \u2265 \u03a9(d1/2), complexity poly(d, k)) and followed by works of Dasgupta & Schulman [8] (spherical and identical \u03a3, d log(k), \u2206G \u2265 \u03a9(d1/4), complexity poly(d, k)), Arora & Kannan [21] (general and identical \u03a3, \u2206G \u2265 \u03a9(d1/4) complexity O(kd)).", "startOffset": 357, "endOffset": 361}, {"referenceID": 5, "context": "Although this algorithm does not outperform the scaling result in Dasgupta[6], it still sheds light on a different approach of learning GMMs.", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "3 Open problems In a recent work, Chen & Chi [5] showed that via structured matrix completion, the sample complexity for stable recovery can be reduced to O(k log d).", "startOffset": 45, "endOffset": 48}, {"referenceID": 21, "context": "Recently, Schiebinger et al [22] studied the problem of learning a mixture of shifted and re-scaled point spread functions f(s) = \u2211 j wj\u03c6(s, \u03bc (j)).", "startOffset": 28, "endOffset": 32}], "year": 2015, "abstractText": "Super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements, which may be corrupted with noise. This signal processing problem arises in numerous imaging problems, ranging from astronomy to biology to spectroscopy, where it is common to take (coarse) Fourier measurements of an object. Of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties: we seek to use coarse Fourier measurements (bounded by some cutoff frequency); we hope to take a (quantifiably) small number of measurements; we desire our algorithm to run quickly. Suppose we have k point sources in d dimensions, where the points are separated by at least \u2206 from each other (in Euclidean distance). This work provides an algorithm with the following favorable guarantees: \u2022 The algorithm uses Fourier measurements, whose frequencies are bounded by O(1/\u2206) (up to log factors). Previous algorithms require a cutoff frequency which may be as large as \u03a9( \u221a d/\u2206). \u2022 The number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d, with no dependence on the separation \u2206. In contrast, previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities. Our estimation procedure itself is simple: we take random bandlimited measurements (as opposed to taking an exponential number of measurements on the hyper-grid). Furthermore, our analysis and algorithm are elementary (based on concentration bounds for sampling and the singular value decomposition). 1 ar X iv :1 50 9. 07 94 3v 1 [ cs .L G ] 2 6 Se p 20 15", "creator": "LaTeX with hyperref package"}}}