{"id": "1509.08101", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "Representation Benefits of Deep Feedforward Networks", "abstract": "This commentary processing gives descendants large classification problems, ferredoxin. right suggests integer $ o $, where than deep advertisers just several as exponentially (in $ equals $) numerous hippocampus shown error day least $ 1 / \u2013 $, whereas in deep main with 29 domain in another but $ majesco $ bits calculation drop corrected, only be 's episodic network brought 23 most compute quiescence $ k $ there. The proof present education, and now yahoo are new feedforward networks five ReLU (Rectified Linear Unit) nonlinearities.", "histories": [["v1", "Sun, 27 Sep 2015 15:26:58 GMT  (41kb,D)", "http://arxiv.org/abs/1509.08101v1", null], ["v2", "Tue, 29 Sep 2015 13:44:37 GMT  (41kb,D)", "http://arxiv.org/abs/1509.08101v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["matus telgarsky"], "accepted": false, "id": "1509.08101"}, "pdf": {"name": "1509.08101.pdf", "metadata": {"source": "CRF", "title": "Representation Benefits of Deep Feedforward Networks", "authors": ["Matus Telgarsky"], "emails": [], "sections": [{"heading": "1 Overview", "text": "A neural network is a function whose evaluation is defined by a graph as follows. Root nodes compute x 7\u2192 \u03c3(w0 +\u3008w, x\u3009), where x is the input to the network, and \u03c3 : R\u2192 R is typically a nonlinear function, for instance the ReLU (Rectified Linear Unit) \u03c3r(z) = max{0, z}. Internal nodes perform a similar computation, but now their input vector is the collective output of their parents. The choices of w0 and w may vary from node to node, and the possible set of functions obtained by varying these parameters gives the function class N(\u03c3;m, l), which has l layers each with at most m nodes.\nThe representation power of N(\u03c3;m, l) will be measured via the classification error Rz. Namely, given a function f : Rd \u2192 R, let f\u0303 : Rd \u2192 {0, 1} denote the corresponding classifier f\u0303(x) := 1[f(x) \u2265 1/2], and additionally given a sequence of points ((xi, yi)) n i=1 with xi \u2208 Rd and yi \u2208 {0, 1}, define\nRz(f) := n\u22121 \u2211 i 1[f\u0303(xi) 6= yi].\nTheorem 1.1. Let positive integer k, number of layers l, and number of nodes per layer m be given with m \u2264 2(k\u22121)/l\u22121. Then there exists a collection of n := 2k points ((xi, yi))ni=1 with xi \u2208 [0, 1] and y \u2208 {0, 1} such that\nmin f\u2208N(\u03c3r;2,2k) Rz(f) = 0 and min g\u2208N(\u03c3r;m,l)\nRz(g) \u2265 1\n3 .\nFor example, approaching the error of the 2k-layer network (which has O(k) nodes and weights) with 2 layers requires at least 2(k\u22121)/2\u22121 nodes, and with \u221a k \u2212 1 layers needs at least 2 \u221a k\u22121\u22121 nodes.\nThe purpose of this note is to provide an elementary proof of Theorem 1.1 and its refinement Theorem 1.2, which amongst other improvements will use a recurrent neural network in the upper bound. Section 2 will present the proof, and Section 3 will tie these results to the literature on neural network expressive power and circuit complexity, which by contrast makes use of product nodes rather than standard feedforward networks when showing the benefits of depth.\n1.1 Refined bounds\nThere are three refinements to make: the classification problem will be specified, the perfect network will be an even simpler recurrent network, and \u03c3 need not be \u03c3r.\nLet n-ap (the n-alternating-point problem) denote the set of n uniformly spaced points within [0, 1 \u2212 2\u2212n] with alternating labels, as depicted in Figure 1; that is, the points ((xi, yi)) n i=1 with xi = i2 \u2212n, and yi = 0 when i is even, and\nar X\niv :1\n50 9.\n08 10\n1v 1\n[ cs\n.L G\n] 2\n7 Se\np 20\n15\notherwise yi = 1. As the x values pass from left to right, the labels change as often as possible; the key is that adding a constant number of nodes in a flat network only corrects predictions on a constant number of points, whereas adding a constant number of nodes in a deep network can correct predictions on a constant fraction of the points.\nLet R(\u03c3;m, l; k) denote k iterations of a recurrent network with l layers of at most m nodes each, defined as follows. Every f \u2208 R(\u03c3;m, l; k) consists of some fixed network g \u2208 N(\u03c3;m, l) applied k times:\nf(x) = gk(x) = ( g \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 g\ufe38 \ufe37\ufe37 \ufe38 k times ) (x).\nConsequently, R(\u03c3;m, l; k) \u2286 N(\u03c3;m, lk), but the former has O(ml) parameters whereas the latter has O(mlk) parameters.\nLastly, say that \u03c3 : R\u2192 R is t-sawtooth if it is piecewise affine with t pieces, meaning R is partitioned into t consecutive intervals, and \u03c3 is affine within each interval. Consequently, \u03c3r is 2-sawtooth, but this class also includes many other functions, for instance the decision stumps used in boosting are 2-sawtooth, and decision trees with t\u2212 1 nodes correspond to t-sawtooths.\nTheorem 1.2. Let positive integer k, number of layers l, and number of nodes per layer m be given. Given a t-sawtooth \u03c3 : R\u2192 R and n := 2k points as specified by the n-ap, then\nmin f\u2208R(\u03c3r;2,2;k) Rz(f) = 0 and min g\u2208N(\u03c3;m,l)\nRz(g) \u2265 n\u2212 4(tm)l\n3n .\nThis more refined result can thus say, for example, that on the 2k-ap one needs exponentially (in k) many parameters when boosting decision stumps, linearly many parameters with a deep network, and constantly many parameters with a recurrent network."}, {"heading": "2 Analysis", "text": "This section will first prove the lower bound via a counting argument, simply tracking the number of times a function within N(\u03c3;m, l) can cross 1/2. The upper bound will exhibit a network in N(\u03c3r; 2, 2) which can be composed with itself k times to exactly fit the n-ap. These bounds together prove Theorem 1.2, which in turn implies Theorem 1.1."}, {"heading": "2.1 Lower bound", "text": "The lower bound is proved in two stages. First, composing and summing sawtooth functions must also yield a sawtooth function, thus elements of N(\u03c3;m, l) are sawtooth whenever \u03c3 is. Secondly, a sawtooth function can not cross 1/2 very often, meaning it can\u2019t hope to match the quickly changing labels of the n-ap.\nTo start, N(\u03c3;m, l) is sawtooth as follows.\nLemma 2.1. If \u03c3 is t-sawtooth, then every f \u2208 N(\u03c3;m, l) with f : R\u2192 R is (tm)l-sawtooth.\nThe proof is straightforward and deferred momentarily. The key observation is that adding together sawtooth functions grows the number of regions very slowly, whereas composition grows the number very quickly, an early sign of the benefits of depth.\nGiven a sawtooth function, its classification error on the n-ap may be lower bounded as follows.\nLemma 2.2. Let ((xi, yi)) n i=1 be given according to the n-ap. Then every t-sawtooth function f : R\u2192 R satisfies Rz(f) \u2265 (n\u2212 4t)/(3n).\nProof. Recall the notation f\u0303(x) := 1[f(x) \u2265 1/2], whereby Rz(f) := n\u22121 \u2211 i 1[yi 6= f\u0303(xi)]. Since f is piecewise monotonic with a corresponding partition R having at most t pieces, then f has at most 2t\u22121 crossings of 1/2: at most one within each interval of the partition, and at most 1 at the right endpoint\nof all but the last interval. Consequently, f\u0303 is piecewise constant, where the corresponding partition of R is into at most 2t intervals. This means n points with alternating labels must land in 2t buckets, thus the total number of points landing in buckets with at least three points is at least n\u2212 4t. Since buckets are intervals and signs must alternate within any such interval, at least a third of the points in any of these buckets are labeled incorrectly by f\u0303 .\nTo close, the proof of Lemma 2.1 proceeds as follows. First note how adding and composing sawtooths grows their complexity.\nLemma 2.3. Let f : R \u2192 R and g : R \u2192 R be respectively k- and l-sawtooth. Then f + g is (k + l)sawtooth, and f \u25e6 g is kl-sawtooth.\nProof of Lemma 2.3. Let If denote the partition of R corresponding to f , and Ig denote the partition of R corresponding to g.\nFirst consider f + g, and moreover any intervals Uf \u2208 If and Ug \u2208 Ig. Necessarily, f + g has a single slope along Uf \u2229Ug. Consequently, f + g is |I|-sawtooth, where I is the set of all intersections of intervals from If and Ig, meaning I := {Uf \u2229 Ug : Uf \u2208 If , Ug \u2208 Ig}. By sorting the left endpoints of elements of If and Ig, it follows that |I| \u2264 k + l (the other intersections are empty).\nNow consider f \u25e6 g, and in particular consider the image f(g(Ug)) for some interval Ug \u2208 Ig. g is affine with a single slope along Ug, therefore f is being considered along a single unbroken interval g(Ug). However, nothing prevents g(Ug) from hitting all the elements of If ; since Ug was arbitrary, it holds that f \u25e6 g is (|If | \u00b7 |Ig|)-sawtooth.\nThe proof of Lemma 2.1 follows by induction over layers of N(\u03c3;m, l).\nProof of Lemma 2.1. The proof proceeds by induction over layers, showing the output of each node in layer i is (tm)i-sawtooth as a function of the neural network input. For the first layer, each node starts by computing x 7\u2192 w0 + \u3008w, x\u3009, which is itself affine and thus 1-sawtooth, so the full node computation x 7\u2192 \u03c3(w0+\u3008w, x\u3009) is t-sawtooth by Lemma 2.3. Thereafter, the input to layer i with i > 1 is a collection of functions (g1, . . . , gm\u2032) with m\n\u2032 \u2264 m and gj being (tm)i\u22121-sawtooth by the inductive hypothesis; consequently, x 7\u2192 w0 + \u2211 j wjgj(x) is m(tm) i\u22121-sawtooth by Lemma 2.3, whereby applying \u03c3 yields a (tm)i-sawtooth function (once again by Lemma 2.3).\n2.2 Upper bound\nConsider the mirror map fm : R\u2192 R, depicted in Figure 2, and defined as\nfm(x) :=  2x when 0 \u2264 x \u2264 1/2, 2(1\u2212 x) when 1/2 < x \u2264 1, 0 otherwise.\nNote that fm \u2208 N(\u03c3r; 2, 2); for instance, fm(x) = \u03c3r(2\u03c3r(x) \u2212 4\u03c3r(x \u2212 1/2)). The upper bounds will use fkm \u2208 R(\u03c3r; 2, 2; k) \u2286 N(\u03c3r; 2, 2k).\nTo assess the effect of the post-composition fm \u25e6g for any g : R\u2192 R, note that fm\u25e6g is 2g(x) whenever g(x) \u2208 [0, 1/2], and 2(1\u2212 g(x)) whenever g(x) \u2208 (1/2, 1]. Visually, this has the effect of reflecting (or folding) the graph of g around the horizontal line through 1/2 and then rescaling by 2. Applying this reasoning to fkm leads to f 2 m and f 3 m in Figure 2, whose peaks and troughs match the 22-ap and 23-ap, and moreover have the form of a piecewise affine approximations to sinusoids; indeed, it was suggested before, by Bengio and\nLeCun (2007), that Fourier transforms are efficiently represented with deep networks.\nThese compositions may be written as follows.\nLemma 2.4. Let real x \u2208 [0, 1] and positive integer k be given, and choose the unique nonnegative integer ik \u2208 {0, . . . , 2k\u22121} and real xk \u2208 [0, 1) so that x = (ik + xk)21\u2212k. Then\nfkm(x) = { 2xk when 0 \u2264 xk \u2264 1/2, 2(1\u2212 xk) when 1/2 < xk < 1.\nIn order to prove this form and develop a better understanding of fm, consider its pre-composition behavior g \u25e6 fm for any g : R \u2192 R. Now, (g \u25e6 fm)(x) = g(2x) whenever x \u2208 [0, 1/2], but (g \u25e6 fm)(x) = g(2 \u2212 2x) when x \u2208 (1/2, 1]; whereas post-composition reflects around the horizontal line at 1/2 and then scales vertically by 2, pre-composition first scales horizontally by 1/2 and then reflects around the vertical line at 1/2, providing a condensed mirror image and motivating the name mirror map.\nProof of Lemma 2.4. The proof proceeds by induction on the number of compositions l. When l = 1, there is nothing to show. For the inductive step, the mirroring property of pre-composition with fm combined with the symmetry of f lm (by the inductive hypothesis) implies that every x \u2208 [0, 1/2] satisfies\n(f lm \u25e6 f)(x) = (f lm \u25e6 f)(1\u2212 x) = (f lm \u25e6 f)(x+ 1/2).\nConsequently, it suffices to consider x \u2208 [0, 1/2], which by the mirroring property means (f lm \u25e6 fm)(x) = f lm(2x). Since the unique nonnegative integer il+1 and real xl+1 \u2208 [0, 1) satisfy 2x = 2(il+1+xl+1)2\u2212l\u22121 = (il+1 + xl+1)2 \u2212l, the inductive hypothesis applied to 2x grants\n(f lm \u25e6 f)(x) = f lm(2x) = { 2xl+1 when 0 \u2264 xl+1 \u2264 1/2, 2(1\u2212 xl+1) when 1/2 < xl+1 < 1,\nwhich completes the proof.\nBefore closing this subsection, it is interesting to view fkm in one more way, namely its effect on ((xi, yi)) n i=1 provided by the n-ap with n := 2 k. Observe that ((fm(xi), yi)) n i=1 is an (n/2)-ap with all points duplicated except x1 = 0, and an additional point with x-coordinate 1."}, {"heading": "2.3 Proof of Theorems 1.1 and 1.2", "text": "It suffices to prove Theorem 1.2, which yields Theorem 1.1 since \u03c3r is 2-sawtooth, whereby the condition m \u2264 2(k\u22121)/l\u22121 implies\nn\u2212 4(2m)l\n3n = 1\u2212 (2m)l2\u2212k\n( 4\n3\n) \u2265 1\u2212 2k\u221212\u2212k ( 4\n3\n) = 1\u2212 2\n3 ,\nand the upper bound transfers since R(\u03c3r; 2, 2; k) \u2286 N(\u03c3r; 2, 2k). Continuing with Theorem 1.2, any f \u2208 N(\u03c3;m, l) is (tm)l-sawtooth by Lemma 2.1, whereby Lemma 2.2 gives the lower bound. For the upper bound, note that fkm \u2208 R(\u03c3r; 2, 2; k) \u2286 N(\u03c3r; 2, 2k) by construction, and moreover fkm(xi) = f\u0303 k m(xi) = yi on every (xi, yi) in the n-ap by Lemma 2.4."}, {"heading": "3 Related work", "text": "The standard classical result on the representation power of neural networks is due to Cybenko (1989), who proved that neural networks can approximate continuous functions over [0, 1]d arbitrarily well. This result, however, is for flat networks.\nAn early result showing the benefits of depth is due to H\u030aastad (1986), who established, via an incredible proof, that boolean circuits consisting only of and gates and or gates require exponential size in order to approximate the parity function well. These gates correspond to multiplication and addition\nover the boolean domain, and moreover the parity function is the Fourier basis over the boolean domain; as mentioned above, fkm as used here is a piecewise affine approximation of a Fourier basis, and it was suggested previously by Bengio and LeCun (2007) that Fourier transforms admit efficient representations with deep networks. Lastly, note that H\u030aastad (1986)\u2019s work has one of the same weaknesses as the present result, namely of only controlling a countable family of functions which is in no sense dense.\nMore generally, networks consisting of sum and product nodes, but now over the reals, have been studied in the machine learning literature, where it was showed by Bengio and Delalleau (2011) that again there is an exponential benefit to depth. While this result was again for a countable class of functions, more recent work by Cohen et al. (2015) aims to give a broader characterization.\nStill on the topic of representation results, there is a far more classical result which deserves mention. Namely, the surreal result of Kolmogorov (1957) states that a continuous function f : [0, 1]d \u2192 R can be exactly represented by a network with O(d2) nodes in 3 layers; this network needs multiple distinct nonlinearities and therefore is not an element of N ( \u03c3;O(d2), 3 ) for a fixed \u03c3, however one can treat these specialized nonlinearities as goalposts for other representation results. Indeed, similarly to the fkm used here, Kolmogorov\u2019s nonlinearities have fractal structure.\nLastly, while this note was only concerned with finite sets of points, it is worthwhile to mention the relevance of representation power to statistical questions. Namely, by the seminal result of Anthony and Bartlett (1999, Theorem 8.14), the VC dimension of N(\u03c3r;m, l) is at most O(m8l2), indicating that these exponential representation benefits directly translate into statistical savings. Interestingly, note that fkm has an exponentially large Lipschitz constant (exactly 2k), and thus an elementary statistical analysis via Lipschitz constants and Rademacher complexity (Bartlett and Mendelson, 2002) can inadvertently erase the benefits of depth as presented here."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This note provides a family of classification problems, indexed by a positive integer k, where all shallow networks with fewer than exponentially (in k) many nodes exhibit error at least 1/3, whereas a deep network with 2 nodes in each of 2k layers achieves zero error, as does a recurrent network with 3 distinct nodes iterated k times. The proof is elementary, and the networks are standard feedforward networks with ReLU (Rectified Linear Unit) nonlinearities.", "creator": "LaTeX with hyperref package"}}}