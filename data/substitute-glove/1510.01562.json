{"id": "1510.01562", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "Parameterized Neural Network Language Models for Information Retrieval", "abstract": "Information Retrieval (IR) models need to move out last but issues, contexts looseness and changes warm-temperate. Vocabulary intractable notation to the invariably over incriminating relevant proof that able not are actual interfaces merely but interchangeably related comparable. Term dependencies known even the ready of make similar relationship line the words over next query with accounting the perceptions that a submit. A multitude much approaches has some policies will matter put also these two however, being something stances designed solved but. In within, brought the last their although, religion models for on differentiation business than previously used one recovery this complex preserve example component functions like emotion them replies detection. Although they present good cues to cope time both term dependencies and vocabulary disadvantage problems, thanks to however distributed representation instance text they are unit yet, such version give taken one few readily in IR, where, optimization has one tagalog new 2.2 document (small query) not consideration. This is unlike computationally unfeasible to avoid to when - wardrobe. Based in has month learned that proposed intended knowing a generic text addition that can also obsolete end a turn of submit - specific constraint, good explore without particular new neural operates models possible make \u201d this text - pwc IR tasks. Within the phrase definition IR establishing, we propose without scientific the and significant a proprietary language modern even however but a document - mechanisms language styling. Both can it used under given smoothing capabilities, but same assumed form more adapted though called document at set have previously in potential of being used as right full authorized alphabet model. We experiment with kinds wheels and analyze their results thursday TREC - 1 would 90 datasets.", "histories": [["v1", "Tue, 6 Oct 2015 13:07:31 GMT  (53kb,D)", "http://arxiv.org/abs/1510.01562v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["benjamin piwowarski", "sylvain lamprier", "nicolas despres"], "accepted": false, "id": "1510.01562"}, "pdf": {"name": "1510.01562.pdf", "metadata": {"source": "CRF", "title": "Parameterized Neural Network Language Models for Information Retrieval", "authors": ["N. Despres", "S. Lamprier", "B. Piwowarski"], "emails": ["nicolas.despres@gmail.com", "sylvain.lamprier@lip6.fr", "benjamin@bpiwowar.net"], "sections": [{"heading": "1 Introduction", "text": "To improve search effectiveness, Information Retrieval (IR) have sought for a long time to properly take into account term dependencies and tackle term mismatch issues. Both problems have been tackled by various models, ranging from empirical to more principled approaches, but no principled approach for both problems have been proposed so far. This paper proposes an approach based on recent developments of neural network language models.\nTaking into account dependent query terms (as compound words for instance) in document relevance estimations usually increases the precision of the search process. This corresponds to developing approaches for identifying term dependencies and considering spatial proximity of such identified linked terms in the documents. Among the first proposals to cope with term dependency issues, Fagan et al. [7] proposed to consider pairs of successive terms (bi-grams) in vector space models. The same principle can be found in language models such as in [22] that performs mixtures of uni- and bi-gram models. Other works have sought to combine the scores of models by taking into account different co-occurrence patterns, such as [14] which proposes a Markov random field model to capture the term dependencies of queries and documents. In each case, the problem comes down to computing accurate estimates of n-gram language models (or variants thereof), i.e. language models where the probability distribution of a term depends on a finite sequence of previous terms.\n\u2217Sorbonne Universit\u00e9s, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu 75005 Paris\nar X\niv :1\n51 0.\n01 56\n2v 1\n[ cs\n.I R\n] 6\nO ct\n2 01\n5\nOn the other hand, taking into account semantic relationships (such as synonymy) increases recall by enabling the retrieval of relevant documents that do not contain the exact query terms but use some semantically related terms. This is particularly important because of the asymmetry between documents and queries. Two main different approaches are used to cope with this problem. The first is to use (pseudo) relevance feedback to add query terms that were not present in the original query. The second is to use distributed models, like latent semantic indexing, where terms and documents are represented in a latent space, which might be probabilistic or vectorial. None of these approaches have taken term dependencies into account.\nIn this paper, we show that neural network language models, that leverage a distributed representation of words, handle naturally terms dependence, and have hence an interesting potential in IR. Neural Language Models [2] have been successfully used in many natural language processing tasks, like part-of-speech tagging [2], semantic labeling and more recently, translation. The main interest of such works is that they allow to take into account both long-range term dependencies and semantic relatedness of words, thanks to the distributed representation of words in a vector space. However, most of the works focused on building generic language model, i.e. what would be called in IR a \u201cbackground language model\u201d. Such language models cannot be used directly for ad-hoc IR tasks, since, due to their huge number of parameters, learning accurate individual models for each document of the collection using maximum likelihood techniques like for classical unigram multinomial language models appears completely intractable. Equally importantly, the learned document model would be over-specific \u2013 the hypothesis that the document language model generate the query would not be held anymore.\nAn interesting alternative was proposed by Le and Mikolov [12] who recently published a neural network language model in which they propose to represent a context (a document, or a paragraph) as a vector that modifies the language model, which avoids building costly individual models for each document to consider. Our work is based on the findings of Le and Mikolov [12]. In this work, we follow a distributed approach where the document is represented by a vector in Rn. Our contributions are the following:\n1. We generalize the model proposed in [12], defining a more powerful architecture and new ways to consider individual specificities of the documents;\n2. We apply the model for ad-hoc Information Retrieval;\n3. We perform intensive experiments on standard IR collections (TREC-1 to 8) and analyze the results.\nThe outline of the paper is as follows. We first briefly overview the many related works (Section 2), before exposing the language models (Section 3). Finally, experiments are reported (Section 4)."}, {"heading": "2 Related works", "text": "This section discuss related works by first presenting those dealing with the term dependencies and mismatch problems. We then introduce related works about Neural Network Language Models (NNLM)."}, {"heading": "2.1 Handling term dependencies", "text": "Early approaches for handling term dependencies in IR considered extensions of the bag of word representation of texts, by including bi-grams to the vocabulary. Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams). This approach has proved to be not so successful, most probably because more complex units imply sparser data [27], which in turn implies inaccurate bigram probability estimations. An alternative, where the mixture is defined within the quantum probabilistic framework, was proposed by Sordoni et al.[23]. This work proposed an elegant way to combine unigram and bigram distributions by\nleveraging this new probabilistic framework. In this paper, we investigate another principled way to model distributions over n-grams, with n not being restricted to 1 or 2.\nMore sophisticated probabilistic models have been proposed to deal with more terms, as well as different dependency constraints between terms. Region-based proximity models combine the score of several models, each dealing with a specific dependency between query terms. For example, one of the submodels could be computing a score for three query terms co-occurring close together in the document. Metzler and Croft [14] proposed a Markov Random Field approach where each clique corresponds to a set or sequence of query terms. This work was extended by Bendersky et al. [1] who considered sets of concepts (phrases or set of words) instead of set of words. In a different probabilistic framework, Blanco [3] proposed to extend BM25F, a model able to take into account different source of evidence to compute the importance of a term for a document, to take into account term proximity by defining operators on so-called virtual regions. In this work, these works are somehow orthogonal to ours since we are not interested by combining various sources of evidences, but rather by investigating whether a parametric language model can capture typical sequence of terms."}, {"heading": "2.2 Vocabulary mismatch", "text": "One of the most used techniques to deal with the problem of vocabulary mismatch is query expansion, based on pseudo-relevance feedback, whereby terms are added to the query based on a set of (pseudo) relevant documents [13]. It has been shown to improve search results in some cases, but is prone to the problem of query drift, which can be controlled using statistical theories like the portfolio theory [4]. Using pseudorelevance feedback is orthogonal to our work, and could be used as an extension to estimate a query relevance language model [11].\nGlobal analysis can be used to enrich the document representation by using co-occurrence information at the dataset level. For example, Goyal et al. [9] use a term association matrix to modify the term-document matrix and account for term relatedness. We believe that such information to be encoded in the neural language model we propose to use in this paper.\nDimensionality reduction techniques such as latent semantic models have been proposed for dealing with vocabulary mismatch issues [5]. The idea is to represent both the document and the query in a latent space, and to compute a retrieval score based on these representations. However, such models do not work well in practice because many document specific terms are discarded during the dimensionality reduction process [25]. It is thus necessary to combine scores from such latent models with scores from standard IR approaches such as BM25 to observe effectiveness improvements. This approach has been followed by [25] in vector spaces and Deveaud et al. [6] for probabilistic (LDA) models. In the latter work, a latent-based language model is used as a background language model. In this paper, we consider a combination of a document-specific neural network language model with a standard unigram multinomial one."}, {"heading": "2.3 Neural Network Language Models", "text": "The idea of using neural networks to build language models emerged in the last ten years. This area of research is included in the recent very active field of \u201crepresentation learning\u201d. Bengio et al. [2] is one of the first works that model text generation (at a word level) using neural networks. The model does so by using a state (a vector in Rn) to represent the history. This state defines a probability distribution over words, and can be updated with a new observation, hence allowing to define a language model over a text.\nSuch an idea of representation of texts in some latent space has been widely explored since then. A recent successful work is the well-known Word2Vec model [15] that proposed a simple neural network architecture to predict a term within a predefined window. This model is fast to learn, thus allowing to compute distributed representations of words over large collections, and has also been shown to implicitly encode relationships (syntactic and semantic) between words: for example, there exists a translation that transforms the representation of a singular word (e.g. \u201ccomputer\u201d) into the representation of its plural form (\u201ccomputers\u201d). Other works based on similar ideas were applied to sentiment detection [21] or automated translation [20].\nCloser to IR, the idea of representing the history as a state, as in [2], in a vectorial space has been exploited by Palangi et al. [17] who proposed to use the state obtained at the end of the document (resp. the query) as a vectorial representation of the whole document (resp. the query). The relevance score is then equal to the cosine between the query and document vectors. They trained the model over clickthrough data and observed that their model was able to better rank clicked documents above the others.\nCompared to this work, our approach does not rely on external click data, and has the advantage of conditioning the language model on the document vector, thus being less influenced by the end of the document. It is based on the idea of using parameters to modify a generative probabilistic model. This is what we call hereafter a parametrized model.\nParameterized probabilistic models have been first applied to speaker recognition. The need arise because those systems have to adapt fast enough to a new user. Few researchers have tackled this problem by designing a HMM whose probability distribution depends on contextual variables. Wilson and Bobick [26] proposed probabilistic models where the means of Gaussian distribution vary linearly as a function of the context. As the output distribution depends not only on the state but also on the context, a model may express many distributions with a limited number of additional parameters.\nThis idea has been exploited by Le and Mikolov [12], who proposed a parameterized language model which they experimented for sentiment analysis and an information related task where relationships between query snippets are encoded by distances of their representations in the considered projection space. We propose in this paper to extend this approach, by designing more sophisticated models dedicated for ad-hoc IR tasks and experimenting them on various IR corpora (TREC-1 to 8)."}, {"heading": "3 Neural Network IR Models", "text": "In this section, we first present some background on classical language models for IR. Then, we present our contribution that allows the resulting IR model to deal with term dependencies and cope with term mismatch issues using representation learning techniques. At last, we present a parametric extension that performs document-dependent transformations of the model.\nAs most of the models deal with sequences, to clarify and shorten notations, we define Xi...j as the sequence Xi, Xi+1, . . . , Xj\u22121, Xj and suppose that if i > j, the sequence is empty."}, {"heading": "3.1 Background: Language Models for IR", "text": "Language models are probabilistic generative models of text \u2013 viewed as a sequence of terms. If a text is composed of a sequence of terms t1...n, where each ti corresponds to a word in a pre-defined vocabulary, we can compute the probability of observing this sequence given the language model M as:\nP (t1...n|M) = N\u220f i=1 P (ti|t1...i\u22121,M)\nLanguage models are used in IR as a simple yet effective way to compute the relevance of a document to a query [27]. There exists different types of language models for IR, but one of the most standard is to equate the relevance of the document d with the likelihood of the language model generating the query, using the evaluated document language model Md.\nP (d relevant to q) = P (q|Md)\nwhere Md is the so-called document language model, which is the model within the family of modelsM that maximizes the probability of observing the document d composed of terms d1...N , that is:\nMd = argmax M\u2208M P (d|M) (1)\n= argmax M\u2208M N\u2211 i=1 logP (di|d1...i\u22121,M)\nAmong the different families of generative models, the n-gram multinomial family is the most used in IR, with n usually equal to 1. The multinomial model assumes the independence of terms given the n \u2212 1 previous ones in the sequence. Formally, if M is within this family, then\nP (ti|t1...i\u22121,M) = P (ti|ti\u2212n+1...i\u22121,M) \u2212 = \u03b8(ti|ti\u2212n+1...i\u22121)\nwhere \u03b8 is a conditional probability table giving the probability of observing the term ti after having observed the sequence ti\u2212n+1...i\u22121.\nFor a given document, the parameters \u03b8 that maximize equation (1) can be computed in a closed form formula:\n\u03b8(ti|ti\u2212n+1...i\u22121) = Count of (ti\u2212n+1...i) in d\nCount of (ti\u2212n+1...i\u22121\u2022) in d (2)\nwhere \u2022 correspond to any term of the vocabulary (i.e. ti\u2212n+1...i\u22121\u2022 corresponds to the sequence ti\u2212n+1, . . . , ti\u22121, u where u \u2208 W). With n = 1, we get a simple unigram model that does not consider the context of the term (note that in that case the denominator is equal to the length of the document). For instance, in a document about Boston, \u201ctrail\u201d is more likely to occur after \u201cfreedom\u201d than in other documents. This information is then important to take into account to build more accurate IR models, since a good document model about Boston would give a higher probability to \u201ctrail\u201d occurring after \u201cfreedom\u201d, and thus the corresponding documents would get a higher score for queries containing the sequence \u201cfreedom trail\u201d. Works like [22] have explored the use of language models with n > 1. In such cases, the models are able to capture term dependencies but usually at the cost of higher complexity and loss of generalization due to the sparsity of the data \u2013 longer sequences are more unlikely to occur in a document d, even for sequences that are strongly related with d in terms of content from a user perspective. With n \u2265 2, the estimated probabilities would be in most cases equal to 0.\nEven with n = 1, the estimation given by the maximum likelihood might be wrong, and discard documents just because they do not contain one query term, even if they contain several occurrences of all others. To avoid such problems, smoothing techniques are used to avoid a zero probability (and thus a score of 0) by mixing the document language model with a collection language model1 denoted MC . A collection language model Mc correspond to the language model that maximizes the probability of observing the sequences of terms contained in the document of the collection C.\nA standard smoothing method is the Jelinek-Mercer one that consists in a mixture of the document language model and the collection language model. Formally, given a smoothing coefficient \u03bb \u2208 [0, 1], the language model of the document becomes:\nP (ti|ti\u2212n+1...i\u22121, \u03bb, d, C) = (1\u2212 \u03bb)P (ti|ti\u2212n+1...i\u22121,Md) + \u03bbP (ti|ti\u2212n+1...i\u22121,MC) (3)\nEven for low values of n, the collection-based smoothing might not be effective. In the following, we propose to develop new language models, that consider more sophisticated methodologies for smoothing, able to deal with long term dependencies and vocabulary mismatch issues."}, {"heading": "3.2 Neural Language Models for IR", "text": "Distributed representations of words and documents have been known for long in IR as Latent Semantic Indexing techniques were introduced in 1999 [5]. They are useful since they overcome the sparsity problem\n1It can be the collection the document belongs to, or any external collection of documents\nwe just evoked by relying on the spatial relatedness of the embedded objects. For example, the words \u201ccat\u201d and \u201cdogs\u201d should be closer together than \u201ccat\u201d and \u201cpencil\u201d in the vector space. This has been exploited in IR to deal with vocabulary mismatch problem, but the idea of leveraging this kind of representation for language models is more recent [2]. Such language models are built using neural networks (hence their name neural network language models) and offer several advantages:\n\u2022 Compression abilities offered by representation learning techniques allow us to consider longer term dependencies (i.e., longer n-grams) than with classical approaches;\n\u2022 Geometric constraints implied by the continuous space used to represent words induce some natural smoothing on the extracted relationships, which enables better generalization abilities and avoids wellknown difficulties related to zero counts of query words (or n-grams) in the considered documents.\nIn this work, we propose to include such a distributed language model in the classical probability computations of IR query terms. Thus, rather than equation 3, we propose to consider the following probability computation:\nP (ti|ti\u2212n+1...i\u22121, \u03bb, \u03b3, d, C) = (1\u2212 \u03bb)((1\u2212 \u03b3)P (ti|Md) + \u03b3P (ti|MC)) + \u03bbPNN (ti|ti\u2212n+1...i\u22121, d, C) (4)\nwhere PNN (ti|ti\u2212n+1...i\u22121, d, C) is the probability of observing the term ti after the sequence ti\u2212n+1...i\u22121 in the document d of the collection C according to our neural network model. It corresponds to introduce term dependencies and vocabulary proximity in a classical unigram language model that is not able to capture such relationships.\nTwo version of our model are detailed hereafter:\n\u2022 A generic neural network language model defined by a background collection (section 3.2.1) ;\n\u2022 A document-specific neural network language model estimated by a background collection and the document at hand (section 3.2.2). Note that in that case, we are interested by the performance of the model when \u03bb is close to 1 (ideally, 1) since this would mean that the document-specific model is specific enough to fully represent the document.\nBoth generic and document-dependent models are represented in figure 1, where the black part corresponds to the common framework for both models and the green and blue parts respectively stand for specific layers for the generic and the document-dependent models."}, {"heading": "3.2.1 Generic Neural Model", "text": "There are two types of neural network language model, those that take into account an infinite context, and those that only take into account a limited number of previous terms. The former are based on recursive neural networks, while the latter are standard feedforward ones. In this work, we investigate the use of the feedforward networks since they are easier to train. Moreover, we expect that the document-specific model that we describe in the next section captures longer term dependencies.\nThe input of the neural network corresponds to the n \u2212 1 previous terms. For the first n \u2212 1 words of a document, we use a special \u201cpadding\u201d term. To each term ti corresponds a vector zti in the vector space Rm0 . The n\u22121 vectors are then transformed through a function \u03c6 (descr below) into a state vector s in Rmf where f is the index of the last layer. This state vector purpose is to summarize the contextual information, which is then taken as an input by the last layer (HSM in the figure) that computes a probability for each term of the vocabulary.\nIn our experiments, we considered three different functions for \u03c6, each of which being a composition of different functions. Following the neural network literature, we term each of the component functions a layer.\nModel 1 (M1): linear(m1) \u2212 tanh The first layer transforms linearly the n \u2212 1 vectors zj \u2208 Rm0 in a vector in Rm1 , that is:\nl1 = n\u22121\u2211 j=1 Ajzj + b\nwhere each Aj is a matrix of dimension m1 \u00d7 k and b is a bias vector in Rm1 .The second layer introduces a non-linearity by computing the hyperbolic tangent (tanh) of each of its inputs\n\u2200j l2j = tanh(l1j)\nIn this model, the function \u03c6 has (n\u2212 1)\u00d7m0 \u00d7m1 parameters.\nModel 2 (M2): linear(m1)\u2212tanh\u2212linear(m2)\u2212tanh The second function \u03c6 we consider is an extension of the first one where we add a linear (matrix B of dimension m2 \u00d7m1) and a non-linear layer (tanh). In this model, the function \u03c6 has (n\u2212 1)\u00d7m0 \u00d7m1 +m1 \u00d7m2 parameters.\nModel 3 (M2Max): linear(\u03bam1) \u2212max(\u03ba) \u2212 linear(m2) \u2212 tanh In the third model, we substitute to the second layer (hyperbolic tangent) of the previous model another non-linear function, a maximum pooling layer. Maximum pooling layers are useful in deep neural networks because they introduce an invariant [10], i.e. they allow to learn more easily that \u201cbig\u201d in a \u201ca big blue cone\u201d and \u201ca big cone\u201d has the same influence on the next term to appear. It is defined by a parameter \u03ba (set to 4 in our experiments)\nmax-pooling\u03ba(x)j = max{x\u03ba\u00d7(j\u22121), . . . , x\u03ba\u00d7j\u22121}\nIn this model, the function \u03c6 has (n\u2212 1)\u00d7 \u03ba\u00d7m0 \u00d7m1 +m1 \u00d7m2 parameters. Then, from a sequence of n \u2212 1 terms (t1...n\u22121), we get a summary vector \u03c6(z1...n\u22121) that is used to compute probability distributions over terms \u2013 the probability that each term in the vocabulary occurs after the sequence of n\u2212 1 terms.\nIn our model, we use a Hierarchical SoftMax (HSM), which corresponds to an efficient multivariate differentiable function that maps an input vector (given by \u03c6 ) to a vector in RV whose values sum to 1 [16]. It allows us to compute the probability HSMt(v) of a term t given an input vector v, with a complexity logarithmic with respect to the number of words in the vocabulary. The HSM is associated to a (binary) tree where leaves correspond to words. Formally, the function is defined as:\nHSMt(v) = \u220f\ns\u2208path(t)\n1\n1 + exp(bs(t)\u00d7 xs \u00b7 v) (5)\nwhere HSMt denotes the component corresponding to the word t in the distribution encoded by the HSM layer, v corresponds to the input vector given to the function, path(t) corresponds to the set of nodes leading to the leaf corresponding to word t, xs is the vector associated to the inner node s of the tree, and bs(t) is -1 (resp. 1) if the word t can be accessed through the left (resp. right) branch from node s. In our case, the tree of the HSM function is a Huffman tree Starting with trees composed of one node (the terms) associated with a weight (the number of occurrences in the collection), the algorithm combines iteratively the two trees with the lowest weights into a new tree formed by a new node with two branches leading to the two selected trees. The set of vectors xs associated to each node s correspond are parameters of the model.\nThis allows us to easily compute a distribution of conditional probabilities PNN (t|t1...n\u22121) for the next word w \u2208 \u2126 knowing the past sequence of n\u2212 1 observed words:\nPNN (t|t1...n\u22121) = HSMt(\u03c6(t1...n\u22121)) (6)\nAt last, to get an operational model that enables the computation of the probability of a given query with regards to the generic model, one has to learn the representation of words, the parameters of the HSM\nand the parameters of the functions. We denote this set of parameters by \u0398. The learning problem can then be formulated as the maximization of the probability of observing the documents d in the collection D :\n\u0398\u2217 = argmax \u0398 \u2211 d\u2208D |d|\u2211 i=0 w(di) log HSMdi(\u03c6(z d i\u2212n+1...i\u22121)) (7)\nwhere w(di) is a weight used to lower the importance of frequent terms. This was used by Mikolov [15] to ensure a better learning, and we used the same settings."}, {"heading": "3.2.2 Document-Dependent Model", "text": "The generic neural network language model presented above handles long term dependencies and semantic relatedness between words for all documents in the collection. This language model can be a good alternative to the multinomial unigram collection language model used for smoothing. However, we believe that taking into account specificities of the document at hand leads to a better language model, and hence to better retrieval results. Indeed, as explained above on an example about Boston, term relationships can be different from documents to others.\nLearning specific neural language models for all individual documents is unfeasible for the same reasons as for n-gram language models for n > 1: Learning term dependencies and semantic relatedness on the sequences contained in a single considered document is likely to lead to over-training issues due to the lack of vocabulary diversity and sequence samples. To overcome this problem, we follow the approach of [12] where the probability distribution of a generic neural network language model is modified by a relatively small set of parameters that can be reliably learned from the observation of a single document: The dimension of such a vector (100-200) is typically much smaller than the number of parameters of a multinomial distribution (size of the vocabulary). Following this approach is interesting since it allows to first learn a language model from the whole collection, and then only learn how to modify it for a specific piece of content. Using parameters to modify the behavior of a generative probabilistic model has been used in many works in signal processing, like gesture recognition [26], where the model has to be quickly adapted to a specific user: Such models benefit from a large source of information (the collection of all gestures or documents in our case) and at the same time can be made specific enough to describe an individual user or document.\nThe modification of the neural network language model is shown in figure 1 (blue/dotted part). A document-specific vector zd \u2208 Rmf is used to modify the generic language model learned on the whole\ncollection, by modifying the vector representing the context of the word to appear. The modified vector is then used to generate a probability distribution by using, as for the generic language model, a HSM.\nIn this paper, we consider two \u201cmerging\u201d operations \u03c8 : Rmf \u00d7 Rmf \u2192 Rmf that associate the state vector s given by \u03c6 and the document specific vector zd to\n\u2022 their sum, i.e. \u03c8(s, zd) = s+ zd\n\u2022 their component-wise product, i.e. \u03c8(s, zd) = s zd\nThese two functions are simple yet they can substantially modify the distribution of the language model. Taking again the example of Boston, the components of zd would bias the model to words likely to occur in such documents, thereby e.g. increasing the probability to find \u201ctrail\u201d after \u201cfreedom\u201d. This is done by making the state vector more orthogonal to vectors in the HSM that lead to \u201ctrail\u201d than to other words associated to \u201cfreedom\u201d.\nNote that, because the solution of our optimization problem is not unique, equivalent solutions (in term of the cost function) could be obtained by rotation \u2013 this would in turn have an impact on the benefit of using such modifications. Our experiments show however that there is a gain associated to using even simple functions like term-wise multiplication or addition. Future works will explore more sophisticated and appropriate transformations of the state vector.\nIn theory, this document-specific language model could be used alone (i.e. without any smoothing, since it is based on a background LM) to estimate the likelihood of generating a query. In practice, as shown in the experiments below, the model is not yet powerful enough to be used so. However, combined with a classical multinomial unigram model as proposed by equation 4, it allows to observe interesting improvements for IR ad-hoc tasks. This corresponds to a first step towards a principled solution for handling vocabulary mismatch and term dependencies."}, {"heading": "4 Experiments", "text": "We used the TREC-1 to 8 collections for experimenting our models. We used as a baseline BM25 [19] with standard parameter settings (k1 = 1.2 and b = 0.5) since it has a reasonable performance on these collections. We left out the comparison with stronger baselines since we were interested first in comparing the different models between themselves. The collection was pre-processed using the Porter stemmer, with no stop words.\nTo learn our language models, we pre-processed the collection using the same pre-processing (no stop words, Porter stemmer), but removed words occurring less than 5 times in the dataset since their learned representation would have been wrongly estimated. The vocabulary size we obtained was of 375,219 words. We used the word2vec [15] implementation2 to pre-compute the initial word representations and hierarchical SoftMax parameters. We used standard word2vec parameters (window of size 5, and used an initial learning rate of 0.5) and iterated 5 times over the corpus, thus ensuring that the initial representation of words is meaningful and easing the following learning process.\nThen, we first trained the generic language models (Section 3.2.1) that are listed in table 1 along with the number of parameters, where we distinguish the computation of the context vector (\u03a6) and the parameters\n2Code available https://code.google.com/p/word2vec/\nrepresenting words (initial representation and HSM parameters). We can see that most of the parameters are related to words.\nFollowing common practice with stochastic gradient descent, we used a steepest gradient descent to learn our terms representation, using the following decreasing learning rate:\nk = 0\n1 + k \u00d7 \u03b4\nwhere 0 = 0.1 and \u03b4 = 2e \u2212 4. Following the literature in neural language model learning, We used minibatch stochastic optimization with batches of 100 documents. After 50000 iterations, the average perplexity was around 2. The number of iterations has thus been set empirically to 50000, as it corresponded to (1) having seen (in expectation) at least 5 times each word of the collection and (2) the likelihood was not evolving anymore.\nFor each topic, we selected the top-100 documents as ranked by BM25, and learned the document parameters zd. We also used a gradient descent3 with resilient backpropagation (Rprop) [18] using an initial learning rate of 0.1, and iterated until the difference between two successive settings be under 1e\u2212 4.\nResults are reported in Figure 2 and table 2 where we compare the different IR models for MAP and GMAP metrics on all TREC datasets. In this figure LM stands for the classical unigram multinomial language model with Jelinek-Mercer smoothing (equation 3). Our 9 models are noted by the name of the model (M1, M2 or M2max following which \u03c6 function is used) followed by the operator used to transform the representation with the document vector (# for none which corresponds to the generic model, + for the addition operator and \u2217 for the component-wise multiplication). The x-axis is the value of \u03bb (in equation 3 for LM and in equation 4 for our neural models). We set \u03b3 to 0.5 in Equation 4.\nWe first observe that compared to classical Jelinek-Mercer LM, there is a slight overall improvements with our document-specific models for both metrics. Furthermore, there is greater stability with respect to the smoothing coefficient \u03bb, where the best values are for small values of \u03bb. We also note that the best improvements are for badly performing topics (since GMAP is more improved than MAP).\nComparing the different models, we see that the generic language models perform worse than their specific counterparts. In table 3, we compared the performance of the model when using a generic and a documentspecific language model (we set \u03bb to 0.01), and observed that in the vast majority of cases we improved the performance by using a document specific model.\nWhile there is a difference on each TREC dataset, there is no clear pattern between the different models M1, M2 and M2Max. It seems however that more complex models do not perform better. We hypothesize that there is more to gain when trying different merging functions \u03c8.\nWe also observe that taking \u03bb = 1 for our document-specific language models, which corresponds to only considering probabilities from our neural models without any other form of smoothing, does not degrade as much as extreme values of \u03bb for the classical multinomial unigram model LM (\u03bb = 0 for the document model only or \u03bb = 1 for the collection one). This is an encouraging result since it shows that using document specific parameters enables to well capture some specificities of the documents. It actually modifies the term representation, leading to better results than a generic language model.\nDocument length has also an influence on the latter observation: We observe that for TREC-1 to TREC-4 the drop is lesser than for TREC-5 to TREC-8. This is to be related to the average length of documents which is higher for latter TREC datasets. It shows that our document specific models are less effective for longer documents, and that more sophisticated models for modifying the context state with the document specific parameters are needed."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed new parametric neural network language models specifically designed for adhoc IR tasks. We proposed various variants for both a generic and a document specific version of the\n3We used a different technique here since we wanted a fast convergence over a small set of parameters and a single document"}, {"heading": "M1* 0.0056 0.0024 0.0105 0.0061 0.0055 0.0119 0.0148 0.0093", "text": ""}, {"heading": "M1+ 0.0044 0.0029 0.0127 0.0043 0.0046 0.0133 0.0120 0.0108", "text": ""}, {"heading": "M2* 0.0054 0.0028 0.0147 0.0035 0.0115 0.0149 0.0132 0.0167", "text": ""}, {"heading": "M2+ 0.0026 0.0020 0.0125 0.0050 0.0096 0.0154 0.0156 0.0125", "text": "model. While the generic version of the model learns term dependencies and accurate representations of words at a collection level, our document-specific language model is modified for each document, through the use of a vector of small dimension, which only contains a few hundred parameters, and thus can be learned efficiently and estimated correctly from a single document. This vector is used to modify the state vector representing the context of the word for which we want to compute the probability of occurring, by using a component-wise multiplication or an addition operator.\nWe experimented with TREC-1 to TREC-8 IR test collections, where the top 100 results retrieved by BM25 were reranked using our generic and document specific language models. The results show that using a document-specific language model improves results over a baseline classical Jelinek-Mercer language model. We have also shown that the document-specific model obtained better results than the generic one, thus validating the approach document-dependent parameterization of the term representations.\nWhile the results do not show a substantial improvement, we believe such models are interesting because, by improving the way a generic language model can be modified using a small set of parameters that represent a textual object (document, paragraph, sentence, etc.), we could finally reach a point where the documentspecific model can make a big difference. Future work will thus study different architectures of the neural language model (including recurrent to consider longer dependencies), as well as use a relevance language model [11] based on pseudo-relevance feedback \u2013 this might be more reliable since language models will be learned from documents and applied on documents (rather than queries)."}], "references": [{"title": "Modeling higher-order term dependencies in information retrieval using query hypergraphs", "author": ["M. Bendersky", "W.B. Croft"], "venue": "In SIGIR \u201912,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Extending BM25 with multiple query operators", "author": ["R. Blanco", "P. Boldi"], "venue": "In SIGIR \u201912,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Reducing the risk of query expansion via robust constrained optimization", "author": ["K. Collins-Thompson"], "venue": "In CIKM", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester", "Scott", "Dumais", "Susan T", "Furnas", "George W", "Landauer", "Thomas K", "Harshman", "Richard"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Unsupervised Latent Concept Modeling to Identify Query Facets", "author": ["R. Deveaud", "E. SanJuan", "P. Bellot"], "venue": "In OAIR\u201913,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Automatic Phrase Indexing for Document Retrieval: An Examination of Syntactic and Non-Syntactic Methods", "author": ["J.L. Fagan"], "venue": "In SIGIR\u201987,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "Dependence language model for information retrieval", "author": ["J. Gao", "J.-Y. Nie", "G. Wu", "G. Cao"], "venue": "In SIGIR\u201904,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "A novel neighborhood based document smoothing model for information", "author": ["P. Goyal", "L. Behera", "T.M. McGinnity"], "venue": "retrieval. IR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A Generative Theory of Relevance", "author": ["V. Lavrenko"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML\u201914,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "A Markov random field model for term dependencies", "author": ["D. Metzler", "W.B. Croft"], "venue": "In SIGIR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["F. Morin", "Y. Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Semantic Modelling with Long-Short-Term Memory for Information", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward"], "venue": "Retrieval. arXiv.org,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A direct adaptive method for faster backpropagation learning: the RPROP algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "In IEEE International Conference on Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "The Probabilistic Relevance Framework: BM25 and Beyond", "author": ["S.E. Robertson", "H. Zaragoza"], "venue": "Foundations and Trends in Information Retrieval", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation", "author": ["H. Schwenk"], "venue": "Proceedings of COLING 2012: Posters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "C.D. Manning", "B. Huval", "A.Y. Ng"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "A general language model for information retrieval", "author": ["F. Song", "W.B. Croft"], "venue": "In CIKM\u201999,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Modeling Term Dependencies with Quantum Language Models for IR", "author": ["A. Sordoni", "J.-Y. Nie", "Y. Bengio"], "venue": "Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Biterm language models for document retrieval", "author": ["M. Srikanth", "R.K. Srihari"], "venue": "In SIGIR\u201902,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Regularized Latent Semantic Indexing: A New Approach to Large-Scale Topic Modeling", "author": ["Q. Wang", "J. Xu", "H. Li", "N. Craswell"], "venue": "ACM TOIS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Parametric hidden Markov models for gesture recognition", "author": ["A.D. Wilson", "A.F. Bobick"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Statistical Language Models for Information Retrieval: A Critical Review", "author": ["C. Zhai"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "[7] proposed to consider pairs of successive terms (bi-grams) in vector space models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "The same principle can be found in language models such as in [22] that performs mixtures of uni- and bi-gram models.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Other works have sought to combine the scores of models by taking into account different co-occurrence patterns, such as [14] which proposes a Markov random field model to capture the term dependencies of queries and documents.", "startOffset": 121, "endOffset": 125}, {"referenceID": 1, "context": "Neural Language Models [2] have been successfully used in many natural language processing tasks, like part-of-speech tagging [2], semantic labeling and more recently, translation.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "Neural Language Models [2] have been successfully used in many natural language processing tasks, like part-of-speech tagging [2], semantic labeling and more recently, translation.", "startOffset": 126, "endOffset": 129}, {"referenceID": 11, "context": "An interesting alternative was proposed by Le and Mikolov [12] who recently published a neural network language model in which they propose to represent a context (a document, or a paragraph) as a vector that modifies the language model, which avoids building costly individual models for each document to consider.", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "Our work is based on the findings of Le and Mikolov [12].", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": "We generalize the model proposed in [12], defining a more powerful architecture and new ways to consider individual specificities of the documents;", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 36, "endOffset": 39}, {"referenceID": 21, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 135, "endOffset": 146}, {"referenceID": 23, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 135, "endOffset": 146}, {"referenceID": 7, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 135, "endOffset": 146}, {"referenceID": 7, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 331, "endOffset": 334}, {"referenceID": 26, "context": "This approach has proved to be not so successful, most probably because more complex units imply sparser data [27], which in turn implies inaccurate bigram probability estimations.", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Metzler and Croft [14] proposed a Markov Random Field approach where each clique corresponds to a set or sequence of query terms.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "[1] who considered sets of concepts (phrases or set of words) instead of set of words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In a different probabilistic framework, Blanco [3] proposed to extend BM25F, a model able to take into account different source of evidence to compute the importance of a term for a document, to take into account term proximity by defining operators on so-called virtual regions.", "startOffset": 47, "endOffset": 50}, {"referenceID": 12, "context": "2 Vocabulary mismatch One of the most used techniques to deal with the problem of vocabulary mismatch is query expansion, based on pseudo-relevance feedback, whereby terms are added to the query based on a set of (pseudo) relevant documents [13].", "startOffset": 241, "endOffset": 245}, {"referenceID": 3, "context": "It has been shown to improve search results in some cases, but is prone to the problem of query drift, which can be controlled using statistical theories like the portfolio theory [4].", "startOffset": 180, "endOffset": 183}, {"referenceID": 10, "context": "Using pseudorelevance feedback is orthogonal to our work, and could be used as an extension to estimate a query relevance language model [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 8, "context": "[9] use a term association matrix to modify the term-document matrix and account for term relatedness.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Dimensionality reduction techniques such as latent semantic models have been proposed for dealing with vocabulary mismatch issues [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 24, "context": "However, such models do not work well in practice because many document specific terms are discarded during the dimensionality reduction process [25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "This approach has been followed by [25] in vector spaces and Deveaud et al.", "startOffset": 35, "endOffset": 39}, {"referenceID": 5, "context": "[6] for probabilistic (LDA) models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] is one of the first works that model text generation (at a word level) using neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "A recent successful work is the well-known Word2Vec model [15] that proposed a simple neural network architecture to predict a term within a predefined window.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "Other works based on similar ideas were applied to sentiment detection [21] or automated translation [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "Other works based on similar ideas were applied to sentiment detection [21] or automated translation [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Closer to IR, the idea of representing the history as a state, as in [2], in a vectorial space has been exploited by Palangi et al.", "startOffset": 69, "endOffset": 72}, {"referenceID": 16, "context": "[17] who proposed to use the state obtained at the end of the document (resp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Wilson and Bobick [26] proposed probabilistic models where the means of Gaussian distribution vary linearly as a function of the context.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "This idea has been exploited by Le and Mikolov [12], who proposed a parameterized language model which they experimented for sentiment analysis and an information related task where relationships between query snippets are encoded by distances of their representations in the considered projection space.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "Language models are used in IR as a simple yet effective way to compute the relevance of a document to a query [27].", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": "Works like [22] have explored the use of language models with n > 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "Formally, given a smoothing coefficient \u03bb \u2208 [0, 1], the language model of the document becomes:", "startOffset": 44, "endOffset": 50}, {"referenceID": 4, "context": "2 Neural Language Models for IR Distributed representations of words and documents have been known for long in IR as Latent Semantic Indexing techniques were introduced in 1999 [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "This has been exploited in IR to deal with vocabulary mismatch problem, but the idea of leveraging this kind of representation for language models is more recent [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "Maximum pooling layers are useful in deep neural networks because they introduce an invariant [10], i.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "In our model, we use a Hierarchical SoftMax (HSM), which corresponds to an efficient multivariate differentiable function that maps an input vector (given by \u03c6 ) to a vector in RV whose values sum to 1 [16].", "startOffset": 202, "endOffset": 206}, {"referenceID": 14, "context": "This was used by Mikolov [15] to ensure a better learning, and we used the same settings.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "To overcome this problem, we follow the approach of [12] where the probability distribution of a generic neural network language model is modified by a relatively small set of parameters that can be reliably learned from the observation of a single document: The dimension of such a vector (100-200) is typically much smaller than the number of parameters of a multinomial distribution (size of the vocabulary).", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "Using parameters to modify the behavior of a generative probabilistic model has been used in many works in signal processing, like gesture recognition [26], where the model has to be quickly adapted to a specific user: Such models benefit from a large source of information (the collection of all gestures or documents in our case) and at the same time can be made specific enough to describe an individual user or document.", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "We used as a baseline BM25 [19] with standard parameter settings (k1 = 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "We used the word2vec [15] implementation2 to pre-compute the initial word representations and hierarchical SoftMax parameters.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "We also used a gradient descent3 with resilient backpropagation (Rprop) [18] using an initial learning rate of 0.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "Future work will thus study different architectures of the neural language model (including recurrent to consider longer dependencies), as well as use a relevance language model [11] based on pseudo-relevance feedback \u2013 this might be more reliable since language models will be learned from documents and applied on documents (rather than queries).", "startOffset": 178, "endOffset": 182}], "year": 2015, "abstractText": "Information Retrieval (IR) models need to deal with two difficult issues, vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to the difficulty of retrieving relevant documents that do not contain exact query terms but semantically related terms. Term dependencies refers to the need of considering the relationship between the words of the query when estimating the relevance of a document. A multitude of solutions has been proposed to solve each of these two problems, but no principled model solve both. In parallel, in the last few years, language models based on neural networks have been used to cope with complex natural language processing tasks like emotion and paraphrase detection. Although they present good abilities to cope with both term dependencies and vocabulary mismatch problems, thanks to the distributed representation of words they are based upon, such models could not be used readily in IR, where the estimation of one language model per document (or query) is required. This is both computationally unfeasible and prone to over-fitting. Based on a recent work that proposed to learn a generic language model that can be modified through a set of document-specific parameters, we explore use of new neural network models that are adapted to ad-hoc IR tasks. Within the language model IR framework, we propose and study the use of a generic language model as well as a document-specific language model. Both can be used as a smoothing component, but the latter is more adapted to the document at hand and has the potential of being used as a full document language model. We experiment with such models and analyze their results on TREC-1 to 8 datasets.", "creator": "LaTeX with hyperref package"}}}