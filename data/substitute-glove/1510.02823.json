{"id": "1510.02823", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Human languages order information efficiently", "abstract": "Most alphabet usually the reflects order between words be introns meaning relations. Languages correlate, however, in thing request they uses and very these sending none 206-448-8135 onto different meanings. We take the hypothesis that, strong seem discussed, human linguistic might moreover other ` tool ' bring common deflation large language use. Using Monte Carlo labs putting data from nine languages, we find say brought given drop could inefficient for technologies in present country unlike thereby whereas few locals variation vector. This impact seen biases originating next thought the organism understands phrases strongly proactively how expression domain-specific impact over earliest.", "histories": [["v1", "Fri, 9 Oct 2015 21:05:02 GMT  (1382kb,D)", "http://arxiv.org/abs/1510.02823v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel gildea", "t florian jaeger"], "accepted": false, "id": "1510.02823"}, "pdf": {"name": "1510.02823.pdf", "metadata": {"source": "CRF", "title": "Human languages order information efficiently", "authors": ["Daniel Gildea", "Florian Jaeger"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "We test the hypothesis that language change is subject to small but persistent biases that result, on average, in languages that are easier to process. Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].\nIf language change is indeed subject to biases towards languages with higher processing efficiency and if these biases are sufficiently strong, these biases should over accumulate over historical time, leading natural languages that have existed for sufficiently long to have higher than expected processing efficiency. This is the primary hypothesis we set out to test. Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85]. At\nar X\niv :1\n51 0.\n02 82\n3v 1\n[ cs\n.C L\n] 9\nthose levels of linguistic organization, studies over the last couple of years have also provided more direct correlational evidence that language change is affected by processing [82]. Miniature language learning experiments have documented similar biases during language acquisition and that these biases can accumulate over generations of learners [50].\nThe level of linguistic organization that has remained elusive with regard to this question, however, is also arguably the one that is the one that makes human languages most unique compared to all other animal communication systems: syntax \u2013or some aspects of syntax (recursion)\u2013 give human languages infinite expressivity with finite means [45, 67] and it is syntax that has been taken to be the defining property of human languages (e.g., [40]; but see [72]). Whether at least some properties of the syntactic systems of languages can be derived from the fact that languages need to be processed continues to be a heatedly debated question (for recent high impact reviews, see [21, 51, 50, 72]). One reason why this question has not been directly addressed, as we detail below, is that until very recently it has been impossible to directly test whether the syntax of natural languages tends to facilitate processing efficiency. Here we present the results of several large-scale computational simulations that address these questions. For the purpose of presentation, we group these simulations into Studies 1 and 2.\nStudy 1 tests and finds confirmed the hypothesis that natural languages have word orders that makes them easier to process than expected by chance. From this is does not follow that natural languages have optimal or even close to optimal processing efficiency. Processing efficiency is presumably just one of several factors that might bias language change (the ease of acquisition of a grammar being another constraint). Still, if biases towards efficient processing are among the most influential factors influencing language change, we would expect human languages to have word orders that are pretty close to optimal in terms of processing efficiency. This hypothesis is tested in Study 2. Taken together, Studies 1 and 2 suggest that language processing exhibits a surprisingly strong bias on language change.\nThe hypothesis we test is one that has long intrigued language researchers. The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74]. However, until relatively recently it has virtually been impossible to obtain reliable estimates of the processing efficiency of a language. Imagine one was to obtain such estimates experimentally (e.g., by obtaining estimates of the word-by-word processing times a native speaker of that language experiences while reading sentences from that language). A reliable estimate of the processing efficiency of an entire language would require reading data for a representative sample of the language. Ideally, this sample would be representative in terms of its lexical and grammatical distributions \u2013i.e., it should contain both low and high frequency words, more and less complex syntactic structures, and so on. Further reliable estimates would require that individual differences in, for instance, reading abilities are averaged out. In short, hundreds of readers would likely have to read thousands of sentences. This alone is a daunting task. In one of the\ntwo most commonly used methods to obtain word-by-word reading time estimates (selfpaced reading), it takes between .5-1 hour to obtain reading times for 100 sentences. So, to obtain data from 100 readers on, say 1000 sentences from a language \u2013which would still not be a lot of sentences\u2013, we would require about 500-1000 participant hours.\nHowever, by far the biggest challenge lies in establishing a chance-level against which to compare the processing efficiency of a language. This requires estimates of processing efficiency from a large set of randomized variants of a language (see below). This further increases the required experimental data by several orders of magnitude. Assessing the processing efficiency of a language based on human data is thus prohibitively expensive and time-consuming. The smallest study we present below would correspond to 500,000 participant hours. At New York State minimum wage (as of 12/31/2014), this approach of assessing processing efficiency would cost over 4 million US Dollars per language, for a total of 20 million Dollars for the five languages we examine. It would also arguably provide an utterly anti-conservative estimate of chance (to say the least): without extensive training on the new language variant, participants would experience massive interference from their native language, making it appear as if human languages are highly efficient simply because it is the one that participants are familiar with (it takes most learners of a language years to have approximately native-like processing speeds).\nHere, we take an alternative approach. We take advantage of advances in computational psycholinguistics, natural language processing, and the availability of large linguistic databases. Rather than to obtain estimates of processing efficiency from human readers, we automatically estimate the processing efficiency of a language from large linguistically annotated collections of text (syntactically annotated corpora). This is now possible, because psycholinguistic research has identified grammar-dependent measures of processing efficiency. Here, we focus on two properties that are known to affect wordby-word processing times: a word\u2019s Shannon information in context (i.e., its surprisal [39, 56]) and the length of the dependencies that are integrated at the word (dependency length, [28, 29]).\nWe describe and further motivate these two measures in more detail in below. For now, it suffices to say that processing difficulty (as assessed through, e.g., per-word reading times) is positively correlated with surprisal and dependency length. If a bias for processing efficiency affects the development of languages over time, it is thus expected that natural languages have lower average surprisal and shorter average dependency lengths than expected by chance.\nWe test these predictions against data from five languages: Arabic (Modern Standard), Czech, English (American), German, and Mandarin Chinese. These five languages were chosen for two reasons. First, we aimed for representative linguistic coverage. Languages often share linguistic properties simply because they are historically related or because they have co-existed in geographic proximity over long periods of time, with the ensuing language contact leading to lexical and grammatical borrowings. Here, we are interested in testing hypotheses that are assumed to apply universally across all languages. The less historically and geographically related the languages in our sample are, the more\nlikely any effect found on this sample is to generalize beyond the particular sample to any language.\nThe five languages we investigated represent three major language families (SinoTibetan, Indo-European, and Semitic) and four language subfamilies (Chinese, BaltoSlavic, Germanic, and Arabic). The five languages also differ in a variety of linguistic properties that are known to be relevant to processing difficulty. For example, three of the languages in our sample have dominant Subject-Verb-Object (SVO) order, one of them has dominant VSO order (Arabic), and one has no dominant word order (German). The languages also differ in whether they productively use morphological means to mark grammatical relations, such as using case (Arabic, Czech, German), or not (English, Mandarin). As a third and final example, the languages differ in whether and how they express certain arguments to the verb. For example, pronominal elements in subject position (e.g., I, you, he) can optionally be omitted in Mandarin, are realized as suffixes on the verb in Czech, but are more or less obligatorily realized as separate words in English and German. Any of these properties could theoretically affect the measures we assess in our studies.\nSecond, as we describe next, sufficiently large electronic corpora with the necessary linguistic annotations are now available for these languages. Corpus size is critical for our purpose. The reliability of the estimates we derive below depends on the number of words and sentences in the corpus. For example, the accuracy and reliability of the processing efficiency estimates described below increases with the number of words in a corpus. The corpora we employ in our studies are the largest available corpora for the five languages with the required linguistic annotation. The methods we use to obtain surprisal and dependency length estimates further increase robustness of estimates."}, {"heading": "2 Data", "text": "The data for all languages comes from newspaper corpora. For English, we also had access to a corpus of conversational speech data with the required annotations. An overview of the corpora is provided in Table 1.\nSpecifically, the Arabic data consists of 6776 sentences from the Penn Arabic Treebank,\nin the dependency representation of the Prague Arabic Dependency Treebank version 1 [38]. The Czech data consists of 72,703 sentences from the Prague Dependency Treebank version 1 [7], as used in the CoNLL 2006 dependency parsing evaluation [10]. The English data comes from two sources. For written data, we use the 39,832 sentences from the Wall Street Journal portion of the Penn Treebank version 3 [65]. For spoken data, we use 17,968 sentences from the Switchboard corpus of spoken English [32]. The German data consists of 45,422 sentences from the TIGER corpus [9], which primarily consists of articles from the German newspaper \u201cFrankfurter Rundschau\u201d. Finally, the Mandarin Chinese data consists of 28,289 sentences from the Penn Chinese Treebank version 6.0 [83]. This includes newswire from Xinhua News Agency, articles from Sinorama Magazine, news from the website of the Hong Kong Special Administrative Region and transcripts from various broadcast news programs.\nAll corpora consist of sentences that have been manually annotated with the syntactic structure of each sentence. The annotations specify a syntactic structure for each sentence. The annotation types differed somewhat between languages. An example, from the English corpus is shown in Figure 1.\nWe automatically converted the different syntactic annotations into a dependency representation, as shown in Figure 2. We use the dependency representation because dependency length has been shown to be an important variable affecting human language processing (see below). The dependency representation is a directed graph specifying, for each word in the sentence, the head word (or \u2018sender\u2019 [23]) that it modifies. For example, subjects and direct objects modify the main verb of the clause; determiners, adjectives, and relative pronouns typically modify nouns; prepositions can modify nouns or verbs; prepositions are modified by the object nouns; and so on. We convert trees to dependency representations using a set of rules which specify which child of each node in the tree is the head child, i.e., the main component of the phrase [62, 16]. Recursively choosing a head child for each node from the top down, we find a head word for each node in the tree. At each node in the tree, dependency relations are created indicating that the head\nword of the head child is modified by the head word of each other child. The dependency representation tends to be robust to the details of the syntactic annotation schemes used by various corpora.\nFor English, German, and Chinese, we extracted dependencies from constituent representations, converting the representation of Figure 1 to that of Figure 2. Specifically, we extract dependencies using the head-finding rules of Collins [16]. Our dependency types consist of pairs of syntactic categories, with one element representing the category of the maximal projection of the head, and one representing the category of the maximal projection of the modifier. Additionally, we include a special subject type in order to differentiate verb subjects and direct objects, by using the \u201cSBJ\u201d function tag in the Penn treebank annotation (see Figure 2).\nFor Czech and Arabic, our data was originally annotated in a dependency representation. We take advantage of relation labels provided, which included relations such subject, object, attribute, and so on. Our dependency types consist of both the relation of a word and the relation of its parent, in order to allow us to distinguish between, for example, an attribute relation in a subject noun phase and an attribute relation modfifying a verb in a relative clause."}, {"heading": "3 Estimating the Processing Efficiency of Languages", "text": "As outlined in the introduction, we focus on two measures of processing efficiency that have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].\nThe surprisal of a word is identical to its Shannon information (in bits) in context, which is defined as the logarithm (to base 2) of the inverse of its probability in context.\nI(w) = log2 1\np(w|context) (1)\n= \u2212 log2 p(w|context) (2)\nA word\u2019s surprisal (conditioned on all relevant preceding context) has been shown to be identical to the relative entropy (or Kullback-Leibler divergence) between the distribution over all possible parses prior to the word and the distribution over all possible parses after processing the word [55]. Surprisal can thus be understood as a measure of\nthe amount of syntactic belief-updating that is associated with processing the word. Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75]. For example, in a large-scale reading experiment, Smith and Levy [75] found that per-word reading times were linear in the word\u2019s surprisal. This relation held over six orders of magnitude in the probability, from almost perfectly predictable instances of words to barely predictable instances (1 \u2265 p(word|context) \u2265 .000001). Surprisal has also been found to be reflected in neural responses.\nDependency length, too, has been found to affect processing difficulty, with longer dependencies leading to longer reading times at their integration point. Consider the word left in the example in Figure 2. Two dependencies end \u2013and are thus assumed to be integrated\u2013 at the word left. One is the dependency between the verb left and its subject (I). This dependency is local. The other dependency is between the verb and its temporal modifier (when the man arrived). This dependency is non-local. Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion). There is also evidence that cross-linguistically speakers prefer shorter dependencies over longer ones when their language provides them with two ways of encoding a message (e.g., for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).\nHere, we estimate these two measures for entire languages. That is, unlike in psycholinguistic work, which has focused on the word-by-word effects of surprisal and dependency length on language processing, we are estimating surprisal and dependency length at the system level. To us, these measures are of interest because they provide an estimate of the average processing difficulty a native speaker of a language experiences while processing that language. This allows us to test whether natural languages have lower average surprisal and shorter average dependency lengths than expected by chance. An overview of the procedure is given in Figure 3.\nThere are other factors that are known to contribute to processing efficiency. For example, among the primary contributors to word-by-word processing are lexical properties. To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63]. As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85]. Here, however, we are interested in a grammatical property of languages \u2013specifically, word order\u2013 and how it affects processing efficiency. It is grammatical properties that would differ between grammatical systems, thus allowing processing preferences to affect \u2018selection\u2019 of these properties over time. The approach we present below therefore holds constant all contextinsensitive lexical properties, ruling these factors out as an explanation for hypothetical preferences for certain grammatical systems."}, {"heading": "3.1 Estimating Processing Efficiency", "text": ""}, {"heading": "3.1.1 Surprisal and Information Density", "text": "We estimate surprisal by means of a trigram model, which conditions a word\u2019s probability on the previous two words. For example, probability of the sentence in Figure 2 would be modeled as:\nP (When | \u3008s\u3009)P (the | \u3008s\u3009When)P (man |When the)P (arrived | the man) \u00b7 \u00b7 \u00b7 where \u3008s\u3009 indicates a sentence boundary.\nN-gram models of this type are widely used in speech recognition [49, 33] and machine translation [53]. N-gram models like the ones used here are also known to provide good approximations to computationally far more complex language models, such as probabilistic phrase structure grammars (see e.g., [27]). One reason for this is presumably that the local context of a word often captures many semantic phenomena through the cooccurrence of related words (e.g., read and book in the trigram read the book). Trigrams also capture local syntactic patterns, such as the requirement of accusative case after certain prepositions (e.g., to me) or subject-verb agreement (e.g., man arrives).\nN-gram models also have two properties that make them particularly appealing for the current purpose. First, estimating n-gram probabilities from corpora is far less computationally complex than estimating the same probabilities from structurally more complex models (such as probabilistic phrase structure grammars). Since, as we detail below, this modeling needs to be repeated many times for each language, computational simplicity is critical for the current study. Second, n-gram models have also been successfully used as models of human language processing [8, 25, 48]. In fact, recent studies have argued that models that primarily rely on the information captured by local context (such as the two preceding words) fair better in explaining word-by-word variation in human processing times than structurally more complex models ([25]; but see also [24, 77]). Indeed, the finding we mentioned above, that a word\u2019s probability in context is log-linearly related to the processing difficulty it causes, was based on a trigram estimate of the type employed here [75]. In short, trigram models are well-suited for the current purpose of estimating processing efficiency. One reason for this might be that human language processing preferably relies on more local information \u2013for example, because non-local information will tend to be less informative or because non-local information will be more costly or less reliably retrieved from memory (consistent with the observation that non-local dependencies are harder to process).\nIn order to obtain reliable estimates of a word\u2019s trigram probability even when the preceding two words were rarely (or never) observed in the training corpus, we smooth our trigram probabilities using the interpolated Kneser-Ney method [52, 13]. KneserNey is a technique that assigns probability to unseen n-grams according to a measure of how likely the words in the trigram are to combine with new words. Using Kneser-Ney smoothed trigram probabilities have two advantages over alternative n-gram models. First, Kneyser-Ney smoothing perform well across a wide variety of tasks and is considered one of the most effective methods of dealing with unobserved trigrams. Second, it is\nspecifically Kneser-Ney smoothed trigram estimates of surprisal that recent work found to be linearly correlated with reaction times [75]. This makes this particular approach well-suited for our purpose of estimating the average processing efficiency of a language.\nSurprisal and information density can be estimated at different levels of linguistic description. For example, in the psycholinguistic literature on sentence processing, surprisal is usually calculated per word [25, 56, 75]. However, psycholinguistic research on phonetic production has also calculated information density at the sub-lexical level (e.g., the information per sound in a word, [15, 78]). Natural languages could theoretically be efficient at one level but not the other.\nHere, we consider two estimates of information density. The first estimate is the by-word information density based on the unnormalized per-word information derived from the trigram model. This is essentially the same measure that has found to correlate linearly with word-by-word reading times in English [75].\nThe second estimate is a normalized by-character estimate of the amount of information per sound or writing unit. For this second estimate, we first counted the number of unique characters in the data base (see Data above). Specifically, we used the logarithm to base 2 of that count, thereby measuring the number of bits one would need to encode all unique characters observed in the databased for each language. For example, there were 48 unique characters (5.6 bits) in our English corpora (this includes special symbols like $) and 4394 unique characters (12.1 bits) in our Mandarin database. We then normalized the information content of each word by the number of letters in that word multiplied by the per-character bits for that language. This normalization has the advantage that it applies the same standard across different writing systems. For example, Mandarin Chinese employs a logographic writing system, so that there are no letters. For spoken language, our normalization approximates the number of phonemes in a word and its spoken duration, while also taking into account the number of distinct sounds in the language. For written language, our normalization corresponds directly to information per character, taking into account the number of distinct symbols used in the database.\nWe note that our results are not sensitive to the choice of normalization: all results were qualitatively similar without any length normalization. Furthermore, the specific normalization procedure chosen here only affects comparisons across languages (which is not of theoretical interest here), as the normalization constant does not vary within one language (see Equation 5 below, where only |wi| varies by word, whereas the percharacter bits are a constant factor)."}, {"heading": "3.1.2 Dependency Length", "text": "Our other measure of processing difficulty is dependency length. This metric can be read off the dependency trees, counting the number of words from each modifier to its head in the linear order of the sentence. For example, for the dependency structure in Figure 2, the word left is the fifth word from the word when. The length of the SBAR>S dependency between when and left is thus of length 5. The SBJ>S dependency between the words I and left, on the other hand, is of length 1. In our experiments, we compute the average length\nof all the dependencies in all sentence. In Figure 2, we have dependencies of length 1, 1, 1, 3, and 5, for an average length of 2.2.\nA number of different metrics have been proposed to measure dependency length. For example, dependency length is sometimes measured in terms of the number of intervening non-discourse given referents [29], or in terms of the syntactic complexity of intervening material. All of these measures tend to be highly correlated [76, 80]. For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]). This measure has the advantage that it is easy to calculate and achieves broad-coverage (see also [18])."}, {"heading": "3.2 Estimating Chance", "text": "To obtain a chance baseline against which to compare the processing efficiency of each language, we create 1000 variants for each language. Specifically, we obtain 1000 pseudogrammars by randomly re-ordering the dependency structures described above, while keeping the dependency relations between heads and their dependents intact. Each pseudogrammar thus describes a theoretically possible reordering of the actual human language. Critically, this variant holds constant:\n\u2022 all context-insensitive lexical properties, including all semantic and phonological factors at the level of the word\n\u2022 the number and identity of the sentences in the corpus\n\u2022 the number of words in each sentence (which is known to affect estimates of the per-word information) and in the corpus\n\u2022 the identity of the words in each sentence (including their part of speech) and in the corpus\n\u2022 the number of heads, dependents, and dependencies in each sentence and in the corpus\n\u2022 the frequency of different types of dependencies in each sentence and in the corpus\nWe then measure the average information density and dependency length of each variant of a language, allowing us to compare the information density and dependency length of the actual languages against what is expected by chance (i.e., against the distribution of information density and dependency lengths observed for the 1000 pseudogrammars derived from that language).\nFor our representation of a possible fixed order, we use weighted grammars [31]. In this representation, each dependency type (e.g., SBJ>S in Figure 2 is assigned a numeric weight \u03bbi between -1 and 1. The head itself always has weight zero. Dependencies with negative weights appear to the left of the head, and dependencies with positive weights\nto the right. For all studies reported below, these weights were held constant for each dependency type. More specifically, we held orders constant within each set of dependencies, where a set refers to all dependency types that end in the same head. One example of a dependency set are all dependencies that end in a head noun (i.e., all noun phraseinternal dependencies). Weights thus define a deterministic order over all dependents of a head, from left to right in order of their numeric weights. For example, with regard to the head of the sentence (S), a given pseudo-grammar might define the order SBJ SBAR S PP NP. The relative order for the four dependency types in the rule above (SBJ>S, SBAR>S, PP>S, and NP>S), then also implies an order of SBJ S NP for sentences in which only these two dependencies connect to S. As we show in Control Study 1, this is a conservative assumption for the calculation of chance for both information density and dependency length, i.e., it biases against the hypothesis tested here.\nAn example of a possible re-ordering of the example sentence of Figure 2 is shown in Figure 4.\nFor each pseudo-grammar specified by a set of weights \u03bb, we estimate the information density and dependency length with the following procedure:\n1. Order the training portion of our corpus according to \u03bb.\n2. Estimate a Kneser-Ney trigram language model L(\u03bb) from the training corpus.\n3. Order the test portion of our corpus according to \u03bb.\n4. (a) Compute the average per-word information, hword, and normalized per-character information, hcharacter, in the test data according to \u03bb, where N is number of word tokens in the database, wi is the ith word token in the test data, wi\u22122, wi\u22121 are the two preceding word tokens, and PL(\u03bb) is the probability according to \u03bb:\nhi(\u03bb) = log2 1\nPL(\u03bb)(wi | wi\u22122, wi\u22121) (3)\nh\u0304word(\u03bb) = 1\nN N\u2211 i=1 hi(\u03bb) (4)\nPearson \u03c1 by-character by-word\nArabic 0.50 0.41 Czech 0.25 0.47 English 0.40 0.62 German 0.15 0.43 Mandarin 0.23 0.24\nIn all experiments, we use 9/10s of the available data as training data in step 1 above, and the remaining 1/10 as test data in steps 4 and 5. This procedure takes several hours (a few minutes per random order) of computer time, as it involves building a large table of n-gram counts for each new random order considered.\nFigure 6 shows the information density and dependency length of all 1000 samples for the five languages. As indicated by the non-parametric smoother in Figure 6, information density and dependency length are positively correlated in the random pseudogrammars. Although the strength of this correlation differs across languages (see Table 2), this correlation is significant in all languages (Pearson correlation ps < 10\u22126). This means that shorter dependencies (i.e., keeping words that belong together adjacent to each other) also tend to reduce information density. This correlation makes intuitive sense. Recall that we are using a trigram language model to estimate information density. To the extent that the syntactic dependencies annotated in the corpora we employed (see Data above) capture relevant statistical dependencies between words, it is thus expected that trigram probabilities will be higher (and information density estimates lower) for word orders that keep syntactic dependencies (and thus more often within the three word window). It is, however, an interesting question for future research whether the correlation we observe here holds even when more computational more complex estimates of word probabilities are used.\nArabic Czech English\nGerman Mandarin\n0.146\n0.148\n0.150\n0.152\n0.154\n0.156\n0.430\n0.435\n0.440\n0.445\n0.450\n0.40\n0.41\n0.42\n0.380\n0.385\n0.390\n0.395\n0.164\n0.166\n0.168\n6 7 8 4.0 4.5 5.0 3.0 3.5 4.0 4.5\n3.0 3.5 4.0 3 4 5\nDependency length (in words)\nIn fo\nrm at\nio n\nd en\nsi ty\n( in\nb it\ns / c\nh ar\nac te\nr)\nLanguage Arabic\nCzech\nEnglish\nGerman\nMandarin"}, {"heading": "4 Results", "text": "We first compare the actual information density and dependency length of five languages in our sample against the pseudo-grammars derived from them. Then we present four control studies that serve to illustrate the robustness of our results."}, {"heading": "4.1 Study 1: Comparing the information density and dependency length of human languages to chance", "text": "Figure 6 shows both the actual human languages and the 1000 random samples for each of them on a plane defined by the two measures of processing efficiency considered here. Table 3 provides a numerical summary. As can be seen, the processing efficiency of actual Arabic, Czech, English, German, and Mandarin Chinese is considerably better than expected by chance. Specifically, applying a standard significance criterion of \u03b1 = .05, all five languages have lower information density than expected by chance, and all languages but Chinese have shorter dependency lengths than expected by chance.\nNext, we present three control studies that demonstrate the robustness of our findings. Since the studies we present here are computationally demanding, we limit our control studies to one of the two information density estimates. We chose to focus on the percharacter estimate, as we take it to be less reflective of properties specific to the writing systems of the language (such as what constitutes a written word).1 Additionally, this is the more conservative approach given the results in Table 3, which are stronger for by-word information density.\n1For example, whereas compounds are generally written as one word in German (e.g., Rotwein), they tend to be written as separate words in English (e.g., red wine). The per-character estimate of information density is not affected by this orthographic decision.\n\u25cf Arabic Czech English\nGerman Mandarin\n0.144\n0.147\n0.150\n0.153\n0.156\n0.435\n0.440\n0.445\n0.450\n0.38\n0.39\n0.40\n0.41\n0.42\n0.380\n0.385\n0.390\n0.395\n0.160\n0.162\n0.164\n0.166\n0.168\n3 4 5 6 7 3.0 3.5 4.0 4.5 5.0 2.5 3.0 3.5 4.0 4.5\n3.2 3.4 3.6 3.8 4.0 4.2 3.0 3.5 4.0 4.5\nDependency length (in words)\nIn fo\nrm at\nio n\nd en\nsi ty\n( in\nb it\ns / c\nh ar\nac te\nr)\nLanguage\n\u25cf\nArabic\nCzech\nEnglish\nGerman\nMandarin"}, {"heading": "4.2 Control study 1: Fixed vs. flexible constituent order", "text": "The results in Table 3 are based on pseudo-grammars that were calculated under the assumption that languages have fixed constituent orders within a dependency type. Interestingly, this assumption does approximate, but not quite match, what is observed for human languages. Table 4 provides a measure of the word order consistency of the languages in our sample.\nFor the calculation of chance for information density, the fixed-order assumption made in Study 1 is expected to be conservative, biasing against the hypothesis we are testing: On average, fixed constituent orders increased the predictability of words, thereby lowering the average information density. This should give the pseudo-grammars derived for Study 1 a distinct advantage compared to the actual human languages, which often do not have fixed constituent orders (or at least not entirely fixed orders). For example, even English, which is considered a relatively fixed order language, allows constituent order variation. Most obviously this holds for alternations, such as the choice between active and passive or heavy noun phrase shift (e.g.,he put the book on the table vs. he put on the table the book, but he put the book he had gotten from a long lost friend on the table vs. he put on the table the book he had gotten from a long lost friend). Generally, the assumption of fixed constituent orders in Study 1 should thus be conservative with regard to information density.\nHowever, for the calculation of chance for dependency length, the consequences of the assumption of fixed constituent order are less clear. It is possible that this assumption made the dependency length results anti-conservative. We therefore repeated Study 1 while allowing constituent order to vary absolutely freely. That is, rather than to use the weighted grammar approach described above in creating random pseudo-grammars, we randomly ordered all dependents for each instance of a dependency.\nTable 5 summarizes the results. For all languages in our sample, both the by-character information density and dependency length of actual human languages were better than that observed for any of the 1000 pseudo-grammars. Control Study 1 thus replicates the results of Study 1 and shows that the assumption of fixed constituent order made in Study 1 biases against our hypothesis, relatively to allowing constituent order freedom.\nWe further note that the results of Study 1 were also replicated when constituents were allowed to order freely, but the position of dependents relative to the head was held\nconstant (e.g., if all dependents occurred to the right of their head). Taken together, this suggests that the results obtained in Study 1 are robust to assumptions about constituent order freedom in the calculation of the chance baseline."}, {"heading": "4.3 Control study 2: Sensitivity to Genre and Mode", "text": "While our primary datasets are taken from newspaper text, we wanted to test whether our results were sensitive to the genre of the corpus, and in particular whether edited, written text might have different properties than spontaneous, spoken text. Unfortunately, large syntactically annotated corpora are available only for very few languages. Here we test our hypothesis against conversational speech data from English.\nRepeating Study 1 on the English Switchboard corpus of conversational speech, we again find that the processing efficiency of actual English is better than expected by chance. Actual conversational English had better per-character information density than all of 1000 random word orders, and better dependency length than all of 1000 random orders (both ps < .0001)."}, {"heading": "4.4 Control study 3: Sensitivity to Corpus Size", "text": "Finally, we tested the sensitivity of our results to the amount of text available for estimating the parameters of the Kneser-Ney trigram model. We thus repeated the analysis reported above, using a much smaller data set of 1000 sentences randomly drawn from the Wall Street Journal (i.e., about 2.5% of the original corpus). Unsurprisingly, information density estimates were higher compared to the main study (reported in Table 3)\u2013this is a direct consequence of the reduced data size: for smaller corpora, there will be more n-grams in the test data that were never observed in the training data; these n-grams are assigned low probability (and thus high information). The estimates based on a smaller corpus are also expected to be less reliable because a large porportion of the n-grams in\nthe test data will be unseen in the training data, regardless of the word order used. To quantify this effect, using actual English word order, we find that with 1000 training sentences, only 10% of bigram tokens in test data have been observed in training data, even when not predicting any single words in the test data that are unseen in training data. In contrast, with our full training set for English, the corresponding figure is 31%. Despite the low coverage of n-grams in our 1000-sentence training set, we again find that actual English has better per-character information density than all of 1000 random word orders, and better dependency length than all of 1000 random orders (both ps < .05)."}, {"heading": "4.5 Summary", "text": "We find that the five languages we investigated all have significantly higher processing efficiency than expected by chance. This holds for both measures of processing efficiency considered here. For information density, all five languages fall into the top 95th percentile or better. For dependency length, four of the five languages fall into the 95th percentile, and one language (Mandarin) falls into approximately the 75th percentile of the distribution defined by the random pseudo-grammars. Our findings held regardless of the size of the corpus and, more importantly, for both written and spoken language. Taken together, this suggests that language use \u2013specifically, pressures that originate in the incremental processing of language\u2013 shape the grammar of languages over time.\nWe note that information density and dependency length were not independent in our random samples. It is thus possible that what we have \u2013following the literature\u2013 treated as two independent measures of processing efficiency is in reality due to one underlying cause. This would not affect the conclusion that the processing efficiency of human languages is better than expected by chance. Further, it is worth noting that the correlations in Table 2 are mild. Indeed, Study 2 finds that information density and dependency length can be optimized separately."}, {"heading": "5 Study 2: Are natural languages optimal with regard to", "text": "information density and dependency length?\nNext, we tested whether an even stronger claim can be made. Specifically, we wondered whether the pressures for efficient processing are sufficiently strong to constrain language change to the subspace of possible grammars that is optimal (or very close to optimal) in terms of processing efficiency. As outlined in the introduction, many pressures of language use have been hypothesized to bias and constrain language change, thereby contributing to cross-linguistically observed properties of languages. We thus did not expect languages to be optimal in terms of processing efficiency. We begin by describing the procedure used to estimate the minimum possible information density for each language. Then we describe the procedure used to estimate the minimum possible dependency length for each language. Finally, we present a procedure that jointly\noptimizes both information density and dependency length for each language, allowing us to compare human languages against pseudo-grammars that optimally trade-off the two major contributors to processing efficiency. The results of these three calculations are presented and discussed at the end of this section."}, {"heading": "5.1 Computing pseudo-grammars with optimal information density", "text": "We begin by describing the procedure used to calculate the minimum possible information density h\u2217 for each language:\nh\u2217 = min \u03bb h(\u03bb)\nIn order to find h\u2217, we optimize one weight at a time, holding all others fixed, and iterating though the set of weights to be set. The objective function describing information density is piecewise constant, as the objective function will not change until one weight crosses some other, causing two dependents to reverse order, at which point the objective will discontinuously jump. This non-differentiability implies that methods based on gradient ascent will not apply. However, because the objective function only changes at points where one weight crosses another\u2019s value, the set of segments of weight values with different values of the objective function can be exhaustively enumerated. In fact, the only significant points are the values of other weights for dependency types which occur in the corpus attached the same head as the dependency being optimized. We build a table of interacting dependencies as a preprocessing step on the data, and then when optimizing a weight, consider the sequence of values between consecutive interacting weights. For each value in this sequence, we evaluate the objective function h on the test corpus, and choose the value yielding the minimum value of h.\nThis search procedure is similar to one previously used for finding the grammar that minimizes dependency length [31]. Our present problem, however, is considerably more computationally intensive, because when evaluating each possible value of each weight, we must examine the entire test corpus, whereas, when optimizing dependency length, one can take a shortcut in evaluation, by only considering sentences with the dependency type whose weight has been modified. In our case, since all the parameters of the n-gram language model are subject to change at each step, we must evaluate the language model on every word in the test corpus.\nThis optimization process is not guaranteed to find the global maximum, but is guaranteed to converge simply from the fact that there are a finite number of objective function values, and the objective function must increase at each step at which weights are adjusted. Running the optimization procedure from different random initializations, we find that, while final grammars are not identical, they are very close in terms of our objective function, which indicates that we are likely to be close to the global optimum. For example, in ten runs optimizing the by-word information density of English, our final values of the objective function have a variance of less than 10\u22125.\nFor our experiments, we find that the optimization procedure converges after several days of computer time. However, the procedure reaches points very close to the eventual optimum within several hours."}, {"heading": "5.2 Computing pseudo-grammars with optimal dependency length", "text": "We also optimize our pseudo-grammar in order to find the weights giving the lowest dependency length:\nd\u2217 = min \u03bb d(\u03bb)\nwhere d(\u03bb) is the average dependency length for the pseudo-grammar with weights \u03bb. The search over weights uses the same algorithm described above."}, {"heading": "5.3 Joint Optimization", "text": "There may be a trade-off between information density and dependency length. Although we found information density and dependency length to be positively correlated in the random pseudo-grammars generated for Study 1, these correlations were mild to moderate. It is therefore possible that optimization of information density trades off against optimization of dependency length (and vice versa). We therefore investigate the effect on the dependency length of optimizing for information density, and vice versa. We also experiment with a joint objective function j that combines information density and dependency length:\nj\u2217 = min \u03bb (1\u2212 \u03b1)d(\u03bb) + \u03b1h(\u03bb)\nWe then applied the same optimization algorithm described above to this joint measure of processing efficiency. The separate optimizations described in the previous two sections correspond to \u03b1s of 1 and 0, respectively. We also considered \u03b1s of .5, .6, .7, .8, and .9 (note that these weights are hard to interpret by themselves: information density and dependency length are on different scales, as shown in Table 3 above."}, {"heading": "5.4 Results", "text": "Table 6 shows results when optimizing for different objective functions in each column. Rows show the by-character information density and dependency length for each language. The left half of the table shows the information density and dependency length for the optimal pseudo-grammars derived by optimizing information density (ID), dependency length (DL) or both jointly with \u03b1 = 0.5 (ID & DL). The right half of the table shows the information density and dependency length of the actual human language, as well as the mean of the random pseudo-grammars generated for Study 1.\nOur first observation from Table 6 is that optimizing either information density or dependency length indeed comes at the expense of the other property (despite the overall positive correlations between information density and dependency length in the random\npseudo-grammars, cf. Figure 5). Further, the average information density and dependency length of all five natural languages is overall closer to the joint optimum, than to either of the separate optima, suggesting that natural languages indeed trade-off information density and dependency length.\nThis means that the jointly optimized pseudo-grammars provide the most relevant point of comparison for natural languages, since we are interested in understanding whether the grammars of natural languages are close to optimal in their overall processing efficiency. One way to further illustrate this trade-off is to look at the equi-weighted joint optimization (\u03b1 = .5). These jointly optimized grammars do not unambiguously outperform actual natural languages. For two of the five languages, Arabic, and Czech, the equiweighted jointly optimized pseudo-grammar has lower information density and dependency length. For the other three languages, however, the optimized pseudo-grammar is better on one dimension of processing efficiency, but worse on the other (e.g., for the jointly optimized pseudo-grammar of Mandarin, a slight improvement in dependency length results in a considerably worsening in information density, compared to actual Mandarin). This is visualized in Figure 8.\nThe full results of the different joint optimizations, varying the weighting parameter \u03b1, are shown in Figure 7. If language processing is just one among many equally im-\nportant factors that shape word order preferences over time, the processing efficiency of optimized grammars should far outperform that of the actual human languages. The gray lines in the Figure 7 can be thought of as representing a \u2018frontier\u2019 of optimality within the space of possible grammars for each of the languages in our sample.\nNone of the languages in our sample lies on this frontier. That is, all languages could theoretically change to have better information density and dependency length. However, we also find that the word orders of some languages (Arabic and English) have close to optimal processing efficiency. Of the five languages investigated here, only the word order of German clearly has non-optimal processing efficiency. One possible reason for such striking differences between languages might be differences in how they use other means than word order to convey the relations between words in a sentence (e.g., word-internal structure and morphosyntactic means). It is also possible that the historical development of some languages has been more strongly affected by other factors of language use (such ease of production or learnability). The current computational simulations cannot distinguish between these possibilities. The approach applied here does, however, point a way forward: as better quantitative models of these other factors become available, future simulations can investigate to what extent other aspects of language use shape the cultural evolution of language.\nWe further note that our optimization procedure held constant the headedness of each dependency type. As mentioned above, this is close to, but not identical to, what the\nhuman languages in our sample do. It is an open question how a relaxation of the the constant-headedness constraint would affect our results. Varying headedness arguably makes a language harder to learn, so that the assumption of constant headedness can be seen as holding constant what likely constitutes an additional (third) constraint that languages need to balance."}, {"heading": "6 Discussion", "text": "The present results suggest that language processing affects language change: all natural languages for which we could test the hypothesis have word orders that make them easier to process than expected by chance. Specifically, the average information density and dependency length of the natural languages in our sample is lower than would be expected if language change was not subject to a bias toward systems with high processing efficiency.\nTo the best of our knowledge, this is the first cross-linguistic broad-coverage study of the processing efficiency of natural languages. The measures of processing efficiency we have employed here are two of the best documented correlates of processing complexity. By calculating the average information density and dependency length of natural languages based on large collections of text from these languages, we were able to side step the insurmountable challenges that would be associated with a behavioral approach to this question (see Introduction). There are three directly related previous findings that we are aware of [23, 31, 26]. Gildea and Temperley [31] investigated the average dependency length of two closely related languages, English and German. Ferrer i Cancho [23] investigates Czech and Romanian, while Futrell et al. [26] study 37 languages spoken worldwide. These studies found that the languages studied had shorter average dependency length than expected by chance. Our contribution is to \u2013for the first time, to our knowledge\u2013 assess the joint effect of two of the biggest contributors to the grammatical processing efficiency of a language, information density and dependency length. As the trade-off between these factors in Study 2 shows, it is crucial to investigate the effect of multiple contributors to processing efficiency simultaneously.\nSome other recent studies complement the approach taken here. These studies have tested whether learners of miniature languages designed by experimenters prefer languages that increase processing efficiency [20, 21, 22]. In the most informative of these studies, great care is taken to rule out native language biases as the source of the observed preferences (cf. [34]). For example, [19] finds that language learners prefer languages that reduce unnecessary uncertainty about the syntactic structure of sentences. Studies like this provide evidence that processing preferences can bias the outcome of language learning and thus provide support for one causal pathway through which processing preferences could come to affect language change, thereby shaping languages over time.\nThere are several caveats that apply to our study. The most obvious perhaps is that we have only considered syntactic dependencies that are annotated in the available syntactic corpora. These dependencies constitute an impoverished subset of all the semantic and\nsyntactic dependencies that human comprehenders process when listening or reading. For example, one obvious omission in our approach is that we did not consider the internal structure of words, the complexity of which varies starkly across languages. Another simplifying assumption we have implicitly made in our studies is the focus on information density and dependency length. While these two measure of processing efficiency are arguably the best documented ones, there are other properties of grammatical systems that are known to affect processing efficiency (e.g., interference in memory due to similar words or referents, [57, 61]). As far as we can tell, neither of these simplifying assumption is likely to have biased the results in favor of the hypothesis tested here (for that to be the case, the measures of processing efficiency we have employed here would have to be systematically inversely correlated with other measures or properties of the languages under study).\nAcknowledgments The authors thank Masha Fedzechkina, Chigusa Kurumada, and Olga Nikolayeva for feedback on earlier versions of this paper. This work was partially funded by National Science Foundation award IIS-1446996 to DG and National Science Foundation CAREER award IIS-1150028 to TFJ. The views expressed here at not necessarily those of the funding agencies."}], "references": [{"title": "Avoiding attachment ambiguities : The role of constituent ordering", "author": ["Jennifer E Arnold", "Thomas Wasow", "Ash Asudeh", "Peter Alrenga"], "venue": "Journal of Memory and Language,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Heaviness vs. newness: the effects of structural complexity and discourse status on constituent ordering", "author": ["Jennifer E Arnold", "Thomas Wasow", "Anthony Losongco", "Ryan Ginstrom"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Morphological influences on the recognition of monosyllabic monomorphemic words", "author": ["R H Baayen"], "venue": "Journal of Memory and Language,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Visual Word Recognition of Single-Syllable Words", "author": ["David A Balota", "Michael J Cortese", "Susan D Sergent-Marshall", "Daniel H Spieler", "Melvin J Yap"], "venue": "Journal of experimental psychology: General,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Functionalist approaches to grammar. In Language acquisition: the state of the art, pages 173\u2013218", "author": ["Elizabeth Bates", "Brian MacWhinney"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1982}, {"title": "Competition, Variation, and Language Learning", "author": ["Elizabeth Bates", "Brian MacWhinney"], "venue": "Mechanisms of Language Acquisition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1987}, {"title": "The PDT: a 3-level annotation scenario", "author": ["A. B\u00f6hmov\u00e1", "J. Haji\u010d", "E. Haji\u010dov\u00e1", "B. Hladk\u00e1"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus", "author": ["Marisa Ferrara Boston", "John Hale", "Reinhold Kliegl", "Umesh Patil", "Shravan Vasishth"], "venue": "Journal of Eye Movement Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "The TIGER treebank", "author": ["S. Brants", "S. Dipper", "S. Hansen", "W. Lezius", "G. Smith"], "venue": "In Proc. of the 1st Workshop on Treebanks and Linguistic Theories (TLT),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi"], "venue": "In Proceedings of the Tenth Conference on Computational Natural Language Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Frequency and the Emergence of Linguistic Structure", "author": ["Joan Bybee", "Paul J Hopper"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "The effect of usage on degrees of constituency: the reduction of dont\u0301", "author": ["Joan Bybee", "Joanne Scheibman"], "venue": "in English. Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Length and Order: A Corpus Study of Korean Dative-Accusative Construction", "author": ["Hye-won Choi"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Using Information Content to Predict Phone Deletion", "author": ["Uriel Cohen Priva"], "venue": "Proceedings of the 27th West Coast Conference on Formal Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Head-driven Statistical Models for Natural Language Parsing", "author": ["Michael John Collins"], "venue": "PhD thesis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Putting the bits together: an information theoretical perspective on morphological processing", "author": ["Aleksandar Kosti\u0107", "R Harald Baayen"], "venue": "Cognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Data from eye-tracking corpora as evidence for theories of syntactic processing", "author": ["Vera Demberg", "Frank Keller"], "venue": "complexity. Cognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Communicative Efficiency, Language Learning, and Language Universals", "author": ["Maryia Fedzechkina"], "venue": "PhD thesis, University of Rochester,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Functional Biases in Language Learning: Evidence from Word Order and Case-Marking Interaction", "author": ["Maryia Fedzechkina", "T Florian Jaeger", "Elissa L Newport"], "venue": "In 33rd Annual Meeting of the Cognitive Science Society,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Language learners restructure their input to facilitate efficient communication", "author": ["Maryia Fedzechkina", "T. Florian Jaeger", "Elissa L. Newport"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Communicative biases shape structures of newly acquired languages", "author": ["Maryia Fedzechkina", "T Florian Jaeger", "Elissa L Newport"], "venue": "Proceedings of the 35th Annual Meeting of the Cognitive Science Society", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Euclidean distance between syntactically linked words", "author": ["Ramon Ferrer i Cancho"], "venue": "Physical Review E,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Sequential vs. hierarchical syntactic models of human incremental sentence processing", "author": ["Victoria Fossum", "Roger Levy"], "venue": "In Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Insensitivity of the human sentence-processing system to hierarchical structure", "author": ["Stefan L Frank", "Rens Bod"], "venue": "Psychological Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Large-scale evidence of dependency length minimization in 37 languages", "author": ["Richard Futrell", "Kyle Mahowald", "Edward Gibson"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Entropy Rate Constancy in Text", "author": ["Dmitriy Genzel", "Eugene Charniak"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Linguistic complexity: locality of syntactic dependencies", "author": ["E Gibson"], "venue": "Cognition,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "The Dependency Locality Theory: A Distance-Based Theory of Linguistic Complexity", "author": ["Edward Gibson"], "venue": "Image, language, brain: Papers from the first mind articulation symposium,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "A noisy-channel account of crosslinguistic word-order variation", "author": ["Edward Gibson", "Steven T Piantadosi", "Kimberly Brink", "Leon Bergen", "Eunice Lim", "Rebecca Saxe"], "venue": "Psychological Science,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Do grammars minimize dependency length", "author": ["Daniel Gildea", "David Temperley"], "venue": "Cognitive Science,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["J. Godfrey", "E. Holliman", "J. McDaniel"], "venue": "In IEEE ICASSP-92,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1992}, {"title": "Speech and audio signal processing: processing and perception of speech and music", "author": ["Ben Gold", "Nelson Morgan", "Dan Ellis"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Substantive learning bias or an effect of familiarity? comment", "author": ["Adele E Goldberg"], "venue": "on. Cognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Locality and Feature Specificity in OCP Effects: Evidence from Aymara, Dutch, and Javanese", "author": ["Peter Graff", "T Florian Jaeger"], "venue": "In Proceedings of the Main Session of the 45th Meeting of the Chicago Linguistic Society,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Consequences of the serial nature of linguistic input for sentenial complexity", "author": ["Daniel Grodner", "Edward Gibson"], "venue": "Cognitive science,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Form and function in linguistic variation", "author": ["Gegory R Guy"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1996}, {"title": "Be\u0161ka. Prague arabic dependency treebank: Development in data and tools", "author": ["Jan Haji\u010d", "Otakar Smr\u017e", "Petr Zem\u00e1nek", "Jan \u0160naidauf", "Emanuel"], "venue": "In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "A Probabilistic Earley Parser as a Psycholinguistic Model", "author": ["John Hale"], "venue": "Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2001}, {"title": "The faculty of language: what is it, who has it, and how did it", "author": ["Marc D Hauser", "Noam Chomsky", "W Tecumseh Fitch"], "venue": "evolve? science,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "Cross-linguistic variation and efficiency", "author": ["J.A. Hawkins"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "A Performance Theory of Order and Constituency", "author": ["John Hawkins"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1994}, {"title": "Efficiency and complexity in grammars", "author": ["John A Hawkins"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "Processing typology and why psychologists need to know about it", "author": ["John A. Hawkins"], "venue": "New Ideas in Psychology,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "Linguistic Variability and Intellectual Development", "author": ["W. von Humboldt"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1972}, {"title": "The Role of Entropy and Surprisal in Phonologization and Language Change", "author": ["Elizabeth Hume", "Fr\u00e9d\u00e9ric Mailhot"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "The cross-linguistic study of sentence production", "author": ["T F Jaeger", "E J Norcliffe"], "venue": "Language and Linguistics Compass,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Redundancy and reduction: speakers manage syntactic information density", "author": ["T Florian Jaeger"], "venue": "Cognitive Psychology,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Statistical Methods for Speech Recognition", "author": ["Frederick Jelinek"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1997}, {"title": "Cumulative cultural evolution in the laboratory: an experimental approach to the origins of structure in human language", "author": ["Simon Kirby", "Hannah Cornish", "Kenny Smith"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2008}, {"title": "Innateness and culture in the evolution of language", "author": ["Simon Kirby", "Mike Dowman", "Thomas L Griffiths"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2007}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1995}, {"title": "Statistical machine translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}, {"title": "The Phonetics/Phonology Issue in the Study of Articulatory Reduction", "author": ["K J Kohler"], "venue": "Phonetica,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1991}, {"title": "Probabilistic Models of Word Order and Syntactic Discontinuity", "author": ["Roger Levy"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2005}, {"title": "Expectation-based syntactic comprehension", "author": ["Roger Levy"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2008}, {"title": "Computational principles of working memory in sentence comprehension", "author": ["Richard L Lewis", "Shravan Vasishth", "Julie a Van Dyke"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2006}, {"title": "Explaining phonetic variation: A sketch of the H&H theory", "author": ["Bj\u00f6rn Lindblom"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1990}, {"title": "Domain Minimization in English Verb-Particle", "author": ["Barbara Lohse", "John A Hawkins", "Thomas Wasow"], "venue": "Constructions. Language,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2004}, {"title": "Recognizing Spoken Words: The Neighborhood Activation Model", "author": ["Paul A Luce", "David B Pisoni"], "venue": "Ear and Hearing,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1998}, {"title": "How language production shapes language form and comprehension", "author": ["Maryellen C MacDonald"], "venue": "Frontiers in Psychology,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "Natural Language Parsing as Statistical Pattern Recognition", "author": ["David Magerman"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1994}, {"title": "The dynamics of lexical competition during spoken word recognition", "author": ["James S Magnuson", "James A Dixon", "Michael K Tanenhaus", "Richard N Aslin"], "venue": "Cognitive science,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2007}, {"title": "Experiments on predictability of word in context and information rate in natural language", "author": ["D Yu Manin"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2006}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1993}, {"title": "Low-level predictive inference in reading: the influence of transitional probabilities on eye movements", "author": ["Scott A. McDonald", "Richard C. Shillcock"], "venue": "Vision Research,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2003}, {"title": "The evolution of syntactic communication", "author": ["Martin A Nowak", "Joshua B Plotkin", "Vincent AA Jansen"], "venue": "Nature, 404(6777):495\u2013498,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2000}, {"title": "Discussion of Bjoern Lindblom\u2019s \u2019Phonetic Invariance and the adaptive nature of speech", "author": ["John J Ohala"], "venue": "In Working Models of Human Perception", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 1988}, {"title": "Word lengths are optimized for efficient communication", "author": ["Steven T Piantadosi", "Harry Tily", "Edward Gibson"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2011}, {"title": "The communicative function of ambiguity", "author": ["Steven T Piantadosi", "Harry Tily", "Edward Gibson"], "venue": "in language. Cognition,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2012}, {"title": "Word-specific phonetics", "author": ["Janet B Pierrehumbert"], "venue": "Laboratory phonology", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2002}, {"title": "The faculty of language: what\u2019s special about it? Cognition, 95(2):201\u201336", "author": ["Steven Pinker", "Ray Jackendoff"], "venue": null, "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2005}, {"title": "Aiming at shorter dependencies: the role of agreement morphology", "author": ["Idoia Ros", "Mike Santesteban", "Kumiko Fukumura", "Itziar Laka"], "venue": "Language, Cognition, and Neuroscience,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2015}, {"title": "Language Change in Childhood and in History", "author": ["Dan I Slobin"], "venue": "Working Papers of the Language Behavior Research Laboratory,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1975}, {"title": "The effect of word predictability on reading time is logarithmic", "author": ["Nathaniel J Smith", "Roger Levy"], "venue": "Cognition,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2013}, {"title": "Szmrecs\u00e1nyi. On Operationalizing Syntactic Complexity", "author": ["M Benedikt"], "venue": "JADT", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2004}, {"title": "Hierarchic syntax improves reading time prediction", "author": ["Marten van Schijndel", "William Schuler"], "venue": "In Proceedings of the 2013 Meeting of the North American chapter of the Association for Computational Linguistics", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "How efficient is speech", "author": ["R J J H van Son", "Louis C W Pols"], "venue": "Proceedings Institute of Phonetic Sciences, University of Amsterdam,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2003}, {"title": "An activation-based model of sentence processing as skilled memory retrieval", "author": ["Shravan Vasishth", "R L Lewis"], "venue": "Cognitive science,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2005}, {"title": "Exemplar models, evolution and language change", "author": ["Andrew Wedel"], "venue": "The Linguistic Review,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2006}, {"title": "Functional Load and the Lexicon: Evidence that Syntactic Category and Frequency Relationships in Minimal Lemma Pairs Predict the Loss of Phoneme contrasts in Language Change", "author": ["Andrew Wedel", "Scott Jackson", "Abby Kaplan"], "venue": "Language and Speech,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2013}, {"title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "author": ["Nianwen Xue", "Fei Xia", "Fu-Dong Chiou", "Martha Palmer"], "venue": "Natural Language Engineering,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2005}, {"title": "Long before short\u201d preference in the production of a head-final", "author": ["Hiroko Yamashita", "Franklin Chang"], "venue": "language. Cognition,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2001}, {"title": "Human Behavior and the Principle of Least Effort", "author": ["George K. Zipf"], "venue": "Addison-Wesley, New York,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 1949}], "referenceMentions": [{"referenceID": 73, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 109, "endOffset": 117}, {"referenceID": 20, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 109, "endOffset": 117}, {"referenceID": 10, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 174, "endOffset": 186}, {"referenceID": 41, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 174, "endOffset": 186}, {"referenceID": 42, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 174, "endOffset": 186}, {"referenceID": 29, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 274, "endOffset": 286}, {"referenceID": 47, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 274, "endOffset": 286}, {"referenceID": 57, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 274, "endOffset": 286}, {"referenceID": 36, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 326, "endOffset": 342}, {"referenceID": 67, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 326, "endOffset": 342}, {"referenceID": 70, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 326, "endOffset": 342}, {"referenceID": 79, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 326, "endOffset": 342}, {"referenceID": 34, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 63, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 68, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 69, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 83, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 80, "context": "those levels of linguistic organization, studies over the last couple of years have also provided more direct correlational evidence that language change is affected by processing [82].", "startOffset": 180, "endOffset": 184}, {"referenceID": 49, "context": "Miniature language learning experiments have documented similar biases during language acquisition and that these biases can accumulate over generations of learners [50].", "startOffset": 165, "endOffset": 169}, {"referenceID": 44, "context": "The level of linguistic organization that has remained elusive with regard to this question, however, is also arguably the one that is the one that makes human languages most unique compared to all other animal communication systems: syntax \u2013or some aspects of syntax (recursion)\u2013 give human languages infinite expressivity with finite means [45, 67] and it is syntax that has been taken to be the defining property of human languages (e.", "startOffset": 342, "endOffset": 350}, {"referenceID": 66, "context": "The level of linguistic organization that has remained elusive with regard to this question, however, is also arguably the one that is the one that makes human languages most unique compared to all other animal communication systems: syntax \u2013or some aspects of syntax (recursion)\u2013 give human languages infinite expressivity with finite means [45, 67] and it is syntax that has been taken to be the defining property of human languages (e.", "startOffset": 342, "endOffset": 350}, {"referenceID": 39, "context": ", [40]; but see [72]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 71, "context": ", [40]; but see [72]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "Whether at least some properties of the syntactic systems of languages can be derived from the fact that languages need to be processed continues to be a heatedly debated question (for recent high impact reviews, see [21, 51, 50, 72]).", "startOffset": 217, "endOffset": 233}, {"referenceID": 50, "context": "Whether at least some properties of the syntactic systems of languages can be derived from the fact that languages need to be processed continues to be a heatedly debated question (for recent high impact reviews, see [21, 51, 50, 72]).", "startOffset": 217, "endOffset": 233}, {"referenceID": 49, "context": "Whether at least some properties of the syntactic systems of languages can be derived from the fact that languages need to be processed continues to be a heatedly debated question (for recent high impact reviews, see [21, 51, 50, 72]).", "startOffset": 217, "endOffset": 233}, {"referenceID": 71, "context": "Whether at least some properties of the syntactic systems of languages can be derived from the fact that languages need to be processed continues to be a heatedly debated question (for recent high impact reviews, see [21, 51, 50, 72]).", "startOffset": 217, "endOffset": 233}, {"referenceID": 53, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 57, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 67, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 45, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 83, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 10, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 4, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 5, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 41, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 73, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 38, "context": ", its surprisal [39, 56]) and the length of the dependencies that are integrated at the word (dependency length, [28, 29]).", "startOffset": 16, "endOffset": 24}, {"referenceID": 55, "context": ", its surprisal [39, 56]) and the length of the dependencies that are integrated at the word (dependency length, [28, 29]).", "startOffset": 16, "endOffset": 24}, {"referenceID": 27, "context": ", its surprisal [39, 56]) and the length of the dependencies that are integrated at the word (dependency length, [28, 29]).", "startOffset": 113, "endOffset": 121}, {"referenceID": 28, "context": ", its surprisal [39, 56]) and the length of the dependencies that are integrated at the word (dependency length, [28, 29]).", "startOffset": 113, "endOffset": 121}, {"referenceID": 37, "context": "in the dependency representation of the Prague Arabic Dependency Treebank version 1 [38].", "startOffset": 84, "endOffset": 88}, {"referenceID": 6, "context": "The Czech data consists of 72,703 sentences from the Prague Dependency Treebank version 1 [7], as used in the CoNLL 2006 dependency parsing evaluation [10].", "startOffset": 90, "endOffset": 93}, {"referenceID": 9, "context": "The Czech data consists of 72,703 sentences from the Prague Dependency Treebank version 1 [7], as used in the CoNLL 2006 dependency parsing evaluation [10].", "startOffset": 151, "endOffset": 155}, {"referenceID": 64, "context": "For written data, we use the 39,832 sentences from the Wall Street Journal portion of the Penn Treebank version 3 [65].", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "For spoken data, we use 17,968 sentences from the Switchboard corpus of spoken English [32].", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "The German data consists of 45,422 sentences from the TIGER corpus [9], which primarily consists of articles from the German newspaper \u201cFrankfurter Rundschau\u201d.", "startOffset": 67, "endOffset": 70}, {"referenceID": 81, "context": "0 [83].", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": "The dependency representation is a directed graph specifying, for each word in the sentence, the head word (or \u2018sender\u2019 [23]) that it modifies.", "startOffset": 120, "endOffset": 124}, {"referenceID": 61, "context": ", the main component of the phrase [62, 16].", "startOffset": 35, "endOffset": 43}, {"referenceID": 15, "context": ", the main component of the phrase [62, 16].", "startOffset": 35, "endOffset": 43}, {"referenceID": 15, "context": "Specifically, we extract dependencies using the head-finding rules of Collins [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 38, "context": "As outlined in the introduction, we focus on two measures of processing efficiency that have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].", "startOffset": 137, "endOffset": 145}, {"referenceID": 55, "context": "As outlined in the introduction, we focus on two measures of processing efficiency that have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].", "startOffset": 137, "endOffset": 145}, {"referenceID": 27, "context": "As outlined in the introduction, we focus on two measures of processing efficiency that have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].", "startOffset": 169, "endOffset": 177}, {"referenceID": 28, "context": "As outlined in the introduction, we focus on two measures of processing efficiency that have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].", "startOffset": 169, "endOffset": 177}, {"referenceID": 54, "context": "A word\u2019s surprisal (conditioned on all relevant preceding context) has been shown to be identical to the relative entropy (or Kullback-Leibler divergence) between the distribution over all possible parses prior to the word and the distribution over all possible parses after processing the word [55].", "startOffset": 295, "endOffset": 299}, {"referenceID": 7, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 17, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 24, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 65, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 74, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 74, "context": "For example, in a large-scale reading experiment, Smith and Levy [75] found that per-word reading times were linear in the word\u2019s surprisal.", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 101, "endOffset": 113}, {"referenceID": 28, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 101, "endOffset": 113}, {"referenceID": 35, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 101, "endOffset": 113}, {"referenceID": 56, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 126, "endOffset": 134}, {"referenceID": 78, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 126, "endOffset": 134}, {"referenceID": 72, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 28, "endOffset": 38}, {"referenceID": 0, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 28, "endOffset": 38}, {"referenceID": 58, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 28, "endOffset": 38}, {"referenceID": 82, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 62, "endOffset": 66}, {"referenceID": 40, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 100, "endOffset": 108}, {"referenceID": 46, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 100, "endOffset": 108}, {"referenceID": 2, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 3, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 7, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 16, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 17, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 59, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 62, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 11, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 14, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 34, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 63, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 68, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 69, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 83, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 48, "context": "N-gram models of this type are widely used in speech recognition [49, 33] and machine translation [53].", "startOffset": 65, "endOffset": 73}, {"referenceID": 32, "context": "N-gram models of this type are widely used in speech recognition [49, 33] and machine translation [53].", "startOffset": 65, "endOffset": 73}, {"referenceID": 52, "context": "N-gram models of this type are widely used in speech recognition [49, 33] and machine translation [53].", "startOffset": 98, "endOffset": 102}, {"referenceID": 26, "context": ", [27]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 7, "context": "Second, n-gram models have also been successfully used as models of human language processing [8, 25, 48].", "startOffset": 94, "endOffset": 105}, {"referenceID": 24, "context": "Second, n-gram models have also been successfully used as models of human language processing [8, 25, 48].", "startOffset": 94, "endOffset": 105}, {"referenceID": 47, "context": "Second, n-gram models have also been successfully used as models of human language processing [8, 25, 48].", "startOffset": 94, "endOffset": 105}, {"referenceID": 24, "context": "In fact, recent studies have argued that models that primarily rely on the information captured by local context (such as the two preceding words) fair better in explaining word-by-word variation in human processing times than structurally more complex models ([25]; but see also [24, 77]).", "startOffset": 261, "endOffset": 265}, {"referenceID": 23, "context": "In fact, recent studies have argued that models that primarily rely on the information captured by local context (such as the two preceding words) fair better in explaining word-by-word variation in human processing times than structurally more complex models ([25]; but see also [24, 77]).", "startOffset": 280, "endOffset": 288}, {"referenceID": 76, "context": "In fact, recent studies have argued that models that primarily rely on the information captured by local context (such as the two preceding words) fair better in explaining word-by-word variation in human processing times than structurally more complex models ([25]; but see also [24, 77]).", "startOffset": 280, "endOffset": 288}, {"referenceID": 74, "context": "Indeed, the finding we mentioned above, that a word\u2019s probability in context is log-linearly related to the processing difficulty it causes, was based on a trigram estimate of the type employed here [75].", "startOffset": 199, "endOffset": 203}, {"referenceID": 51, "context": "In order to obtain reliable estimates of a word\u2019s trigram probability even when the preceding two words were rarely (or never) observed in the training corpus, we smooth our trigram probabilities using the interpolated Kneser-Ney method [52, 13].", "startOffset": 237, "endOffset": 245}, {"referenceID": 12, "context": "In order to obtain reliable estimates of a word\u2019s trigram probability even when the preceding two words were rarely (or never) observed in the training corpus, we smooth our trigram probabilities using the interpolated Kneser-Ney method [52, 13].", "startOffset": 237, "endOffset": 245}, {"referenceID": 74, "context": "specifically Kneser-Ney smoothed trigram estimates of surprisal that recent work found to be linearly correlated with reaction times [75].", "startOffset": 133, "endOffset": 137}, {"referenceID": 24, "context": "For example, in the psycholinguistic literature on sentence processing, surprisal is usually calculated per word [25, 56, 75].", "startOffset": 113, "endOffset": 125}, {"referenceID": 55, "context": "For example, in the psycholinguistic literature on sentence processing, surprisal is usually calculated per word [25, 56, 75].", "startOffset": 113, "endOffset": 125}, {"referenceID": 74, "context": "For example, in the psycholinguistic literature on sentence processing, surprisal is usually calculated per word [25, 56, 75].", "startOffset": 113, "endOffset": 125}, {"referenceID": 14, "context": ", the information per sound in a word, [15, 78]).", "startOffset": 39, "endOffset": 47}, {"referenceID": 77, "context": ", the information per sound in a word, [15, 78]).", "startOffset": 39, "endOffset": 47}, {"referenceID": 74, "context": "This is essentially the same measure that has found to correlate linearly with word-by-word reading times in English [75].", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "For example, dependency length is sometimes measured in terms of the number of intervening non-discourse given referents [29], or in terms of the syntactic complexity of intervening material.", "startOffset": 121, "endOffset": 125}, {"referenceID": 75, "context": "All of these measures tend to be highly correlated [76, 80].", "startOffset": 51, "endOffset": 59}, {"referenceID": 22, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 30, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 42, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 43, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 58, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 72, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 17, "context": "This measure has the advantage that it is easy to calculate and achieves broad-coverage (see also [18]).", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "For our representation of a possible fixed order, we use weighted grammars [31].", "startOffset": 75, "endOffset": 79}, {"referenceID": 30, "context": "This search procedure is similar to one previously used for finding the grammar that minimizes dependency length [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 22, "context": "There are three directly related previous findings that we are aware of [23, 31, 26].", "startOffset": 72, "endOffset": 84}, {"referenceID": 30, "context": "There are three directly related previous findings that we are aware of [23, 31, 26].", "startOffset": 72, "endOffset": 84}, {"referenceID": 25, "context": "There are three directly related previous findings that we are aware of [23, 31, 26].", "startOffset": 72, "endOffset": 84}, {"referenceID": 30, "context": "Gildea and Temperley [31] investigated the average dependency length of two closely related languages, English and German.", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "Ferrer i Cancho [23] investigates Czech and Romanian, while Futrell et al.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "[26] study 37 languages spoken worldwide.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "These studies have tested whether learners of miniature languages designed by experimenters prefer languages that increase processing efficiency [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 20, "context": "These studies have tested whether learners of miniature languages designed by experimenters prefer languages that increase processing efficiency [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 21, "context": "These studies have tested whether learners of miniature languages designed by experimenters prefer languages that increase processing efficiency [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 33, "context": "[34]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "For example, [19] finds that language learners prefer languages that reduce unnecessary uncertainty about the syntactic structure of sentences.", "startOffset": 13, "endOffset": 17}, {"referenceID": 56, "context": ", interference in memory due to similar words or referents, [57, 61]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 60, "context": ", interference in memory due to similar words or referents, [57, 61]).", "startOffset": 60, "endOffset": 68}], "year": 2015, "abstractText": "Most languages use the relative order between words to encode meaning relations. Languages differ, however, in what orders they use and how these orders are mapped onto different meanings. We test the hypothesis that \u2013despite these differences\u2013 human languages might constitute different \u2018solutions\u2019 to common pressures of language use. Using Monte Carlo simulations over data from five languages, we find that their word orders are efficient for processing in terms of both dependency length and local lexical probability. This suggests that biases originating in how the brain understands language strongly constrain how human languages change over generations.", "creator": "LaTeX with hyperref package"}}}