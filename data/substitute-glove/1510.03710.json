{"id": "1510.03710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "Hybrid Dialog State Tracker", "abstract": "This paper scenes yet subcompact constructive current tracker has aspect after rule an three a machine learning is putting hold essence board tracking. Therefore, how we it a hybrid soundscan. The installation learning 1998 beyond montena is figured meanwhile whose Long Short Term Memory (LSTM) business. To all knowledge, anything hybrid innertube pieces gave addition board - the - still - music form for the Dialog State Tracking Challenge (DSTC) 2 autoethnography when two rather electronic being most SLU instance same uses.", "histories": [["v1", "Tue, 13 Oct 2015 14:44:01 GMT  (109kb,D)", "https://arxiv.org/abs/1510.03710v1", null], ["v2", "Tue, 3 Nov 2015 08:38:14 GMT  (108kb,D)", "http://arxiv.org/abs/1510.03710v2", "Accepted to Machine Learning for SLU &amp; Interaction NIPS 2015 Workshop. Model description in Section 2.1 simplified compared to the previous version"], ["v3", "Thu, 14 Jan 2016 10:40:31 GMT  (109kb,D)", "http://arxiv.org/abs/1510.03710v3", "Accepted to Machine Learning for SLU &amp; Interaction NIPS 2015 Workshop. Model description in Section 2.1 simplified compared to the previous version"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["miroslav vodol\\'an", "rudolf kadlec", "jan kleindienst"], "accepted": false, "id": "1510.03710"}, "pdf": {"name": "1510.03710.pdf", "metadata": {"source": "CRF", "title": "Hybrid Dialog State Tracker", "authors": ["Miroslav Vodol\u00e1n", "Rudolf Kadlec", "Jan Kleindienst"], "emails": ["jankle}@cz.ibm.com", "vodolan@ufal.mff.cuni.cz"], "sections": [{"heading": "1 Introduction", "text": "Spoken Dialogue Systems (SDSs) consist of many modules, one of which is a Dialog State Tracker (DST). DST is responsible for accumulating evidence throughout the dialogue and estimating current true user\u2019s goal. The user goal estimate is subsequently used by other modules of the SDS, e.g., by a policy module that picks the next best action.\nRecently proposed DSTCs [1, 2, 3] provide a shared testbed with datasets and tools for evaluating of dialog state tracking methods. It abstracts away the subsystems of end-to-end spoken dialog systems, focusing only on the dialog state tracking. It does so by providing datasets of ASR and SLU outputs on slot-filling tasks with reference transcriptions, together with annotation on the level of dialog acts and user goals.\nThe last three dialog state tracking challenges [1, 2, 3] were dominated by machine learning based trackers [4, 5, 6]. However, when we consider the case where all trackers have the same Spoken Language Understanding (SLU) input, some rule based trackers [7, 8, 9, 10] achieved performance comparable to the top trackers. In this work, we aim to unite the best of the both worlds \u2014 high accuracy of the machine learning trackers and better interpretability of the rule based trackers. A similar research direction was recently explored in [11]. The core of our proposed tracker consists of several update rules that use a few parameters that are computed by a recurrent neural network. We show that on the DSTC2 dataset our hybrid tracker achieves the state-of-the-art performance among the systems that use the original live SLU. Note that the DSTs that also use Automatic Speech Recognition (ASR) output as additional feature achieve even better tracking accuracy. We will add these features in a future work.\nFor evaluation of the tracker we chose the DSTC2 because it contains complex dialogs with changes of the user\u2019s goal and it also provides a lot of training data. Dialogs in the DSTC1 did not have frequent user goal changes and the DSTC3 had only a limited training dataset. The challenges also differ in their domains. The DSTC1 dataset is collected from system providing bus routes, the DSTC2 is focused on restaurant domain and the DSTC3 combines restaurant and hotel domains.\nIn the next section we describe the architecture of our Hybrid tracker. Then we evaluate the tracker on the DSTC2 dataset and conclude the paper with an outline of our future work.\nar X\niv :1\n51 0.\n03 71\n0v 3\n[ cs\n.C L\n] 1\n4 Ja\nn 20"}, {"heading": "2 Hybrid dialog state tracker model", "text": "In our previous work [10], we introduced a belief tracker based on a few simple rules which scored second in the joint slot accuracy in DSTC3 and its slightly modified version has the state-of-theart accuracy on this dataset. Here we simplify the original rules for per slot tracking (in contrast with [10] where we tracked the slots jointly) and we add the machine learning component that provides parameters for these rules. We call the resulting architecture a hybrid tracker.\nThe tracker operates on a probability distribution over values for each slot separately. For each turn, the tracker generates these distributions reflecting the user\u2019s goals based on the last machine action, the observed user actions, the probability distributions in the previous turn and the hidden state lt\u22121 of a recurrent network L from the previous turn. The probability distribution hst for a single slot s and turn t is represented by a vector indexed by possible values of the slot s. The joint belief state is represented by the probability distribution over Cartesian product for each slot.\nIn the following notation ist denotes a user action pre-processed into a probability distribution of informed values for the slot s and turn t. During the pre-processing every Affirm() from SLU is transformed into Inform(slot=value) according to the machine actionm. Further, we introduce a function corresponding to the simplified rules (fully described in Sec. 2.1) hst = R(h s t\u22121, i\ns, cnew, coverride), which is a function of a probability distribution in the previous turn, the pre-processed user action and two parameters which control how the new probability distribution hst is computed. The next function lt = L(lt\u22121, fs, fm, is) is recurrent and takes its own output lt\u22121 from the previous turn, the features fs indicating the tracked slot, the features fm representing machine actions and the pre-processed user action is of the turn t. The output of the recurrent network is then linearly transformed by F (lt) to parameters cnew and coverride for R. The structure of the tracker is shown in Figure 1.\nIn the next subsection, we will describe the rule based component of the Hybrid tracker. Afterwards, in Section 2.2, we will describe the machine learning part of the tracker."}, {"heading": "2.1 Rule-based part", "text": "The rule-based part of the tracker represented by the function R consists of several simple update rules parametrized by parameters cnew and coverride1. Each of the parameters controls transition probability in a different way:\n\u2022 cnew \u2014 controls how easy it would be to change the belief from hypothesis None to an instantiated slot value,\n\u2022 coverride \u2014 models a goal change, that is, how easily it would be to override current belief with a new observation.\nIn this work we compute these parameters by a neural network. The rule based part of our tracker is specified by following equations. The first equation specifies belief update rule for probability assigned to slot\u2019s value v1:\nhst [v1] = h s t\u22121[v1]\u2212 h\u0303st [v1] + ist [v1] \u00b7 \u2211 v2 6=v1 hst\u22121[v2] \u00b7 av1v2 (1)\nWhere h\u0303st [v1] corresponds to amount of probability that will be transferred from h s t\u22121[v1] to other slot values in hst : h\u0303st [v1] = h s t\u22121[v1] \u00b7 \u2211 v2 6=v1 ist [v2] \u00b7 av2v1 (2)\nThe av1v2 is called transition coefficient between values v1 and v2. It controls amount of probability which is transferred from hst\u22121[v2] to h s t [v1].\nav1v2 = { v1 = None cnew v1 6= v2 coverride\n(3)\nAs we can see, the R function is differentiable, therefore the machine learned part, described in the following subsection 2.2, can be trained by gradient descent methods together with the rule-based part.\nWe can find similar update equations in other rule-based trackers, e.g., [7, 12, 9, 10]."}, {"heading": "2.2 Machine learned part", "text": "The machine learning part of our tracker is realized by a LSTM [13] network. We use recurrent network for L since it can learn to output different values of c parameters for different parts of the dialog (e.g., it is more likely that new hypothesis will arise at the beginning of a dialog). This way, the recurrent network influences the rule-based component of the tracker. Since there are only two parameters that are used by the rule-based part, the tracker\u2019s decisions can be easily introspected.\nThe function L uses the feature fs, which is one-hot representation of the tracked slot and the feature fm which is a bag of words representation of machine actions. The last feature of the L function is pre-processed user action is representing marginal probabilities of informed values for slot s.\nIn our tracker we use one machine learned model that is shared for all slots. However, the model can distinguish between the slots according to fs feature. The other systems use a different setup where a shared model is trained for all slots and then it is fine-tuned for each separate slot [14]."}, {"heading": "3 Evaluation", "text": ""}, {"heading": "3.1 Method", "text": "The parameters of the hybrid tracker were trained by SGD with AdaGrad [15] and Adam [16] weight update rules. This is possible since of all parts of the model are differentiable (including the R function).\n1These parameters were modelled by a so called durability function in our previous tracker [10].\nWe trained two groups of trackers with different settings. The first group was trained by the AdaGrad algorithm with the learning rate 0.5 and the gradient clipping with threshold 10. With this setting the training algorithm produced trackers heavily influenced by random initialization, which is good for later ensembling of the trackers. For the second group we used the Adam update rule with the learning rate 0.01, \u03b21 0.9 and \u03b22 0.999. These settings are much more invariant to random initialization therefore we randomly masked fm features to get set of different trackers. Both groups used L function with 5 LSTM cells and tanh as the activation function.\nFrom each dialog in the dstc2 train data (1612 dialogs) we extracted training samples for the slots food, pricerange and area and used all of them to train each tracker. The training data was also used for selection of fm features. We selected only those words from machine action2 that appeared more than 5 times. This gives us the total number of 421 fm features and 3 fs features (one per food, pricerange and area slot).\nThe evaluated model was an ensemble of multiple trackers that were combined by averaging. Similar approach proved to be useful also in other RNN based trackers [14, 17]. For the ensemble, we used 100 trackers randomly selected from both tracker groups containing 115 + 143 trackers.\nWe evaluated 10 different ensembles and selected the one with the best performance on validation dstc2 dev (506 dialogs) data, which is reported in subsection 3.2. Our tracker did not track the name slot because it hurts validation performance. Therefore, we always set value for the name slot None. The mean accuracy of the 10 ensembles on dstc2 test data (1117 dialogs) is 0.7448 with the standard deviation 0.0006.\nThe models were implemented using Theano [18] and Blocks [19].\n2The machine action is represented by dialog acts."}, {"heading": "3.2 Results", "text": "Table 1 shows the results of our hybrid tracker and other top performing trackers known from the literature. In the category of trackers that use only the live SLU features our systems sets the new state-of-the-art with accuracy 0.745 on dstc2 test. The accuracy of the tracker on dstc2 dev is 0.657 and 0.767 on dstc2 train."}, {"heading": "3.3 Discussion", "text": "Evaluation on the DSTC2 dataset shows that our hybrid system that extends rule based tracking core with the machine learning component outperforms the previous best tracker [20] that used the same SLU input. This result is also interesting since our ML component is relatively lightweight (it has only approx. 10k parameters, the hidden state consist of only 5 neurons) and it influences computation of the rules part by only 2 parameters."}, {"heading": "4 Future Work and Conclusion", "text": "We have presented a belief tracker that combines our previous tracker with machine learning techniques. It performs better than our previous tracker while still being highly interpretable in comparison with pure neural network approaches.\nHowever, trackers that use ASR as their input achieve even better accuracy. Therefore the next step will be to add ASR features to our machine learning component. This will hopefully allow us to further improve accuracy of our system."}, {"heading": "Acknowledgments", "text": "This research was partly funded by the Ministry of Education, Youth and Sports of the Czech Republic under the grant agreement LK11221, and core research funding of Charles University in Prague."}], "references": [{"title": "The Dialog State Tracking Challenge", "author": ["J. Williams", "A. Raux", "D. Ramachandran", "A. Black"], "venue": "Proceedings of the SIGDIAL 2013 Conference, (Metz, France), pp. 404\u2013413, Association for Computational Linguistics, August 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "The second dialog state tracking challenge", "author": ["M. Henderson", "B. Thomson", "J.D. Williams"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), (Philadelphia, PA, U.S.A.), pp. 263\u2013272, Association for Computational Linguistics, June 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "The Third Dialog State Tracking Challenge", "author": ["M. Henderson", "B. Thomson", "J. Williams"], "venue": "Spoken Language Technology Workshop, 2014. IEEE, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Recipe For Building Robust Spoken Dialog State Trackers : Dialog State Tracking Challenge System Description", "author": ["S. Lee", "M. Eskenazi"], "venue": "Sigdial, no. August, pp. 414\u2013422, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Web-style ranking and SLU combination for dialog state tracking", "author": ["J.D. Williams"], "venue": "no. June, pp. 282\u2013291, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust Dialog State Tracking Using Delexicalised Recurrent Neural Networks and Unsupervised Adaptation", "author": ["M. Henderson", "B. Thomson", "S. Young"], "venue": "SLT, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information", "author": ["Z. Wang", "O. Lemon"], "venue": "Proceedings of the SIG- DIAL 2013 Conference, (Metz, France), pp. 423\u2013432, Association for Computational Linguistics, August 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "IBM\u2019s Belief Tracker: Results On Dialog State Tracking Challenge Datasets", "author": ["R. Kadlec", "J. Libovick\u00fd", "J. Macek", "J. Kleindienst"], "venue": "Dialog in Motion workshop on EACL 2014, pp. 10\u201318, 2014. 5", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "The sjtu system for dialog state tracking challenge 2", "author": ["K. Sun", "L. Chen", "S. Zhu", "K. Yu"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), (Philadelphia, PA, U.S.A.), pp. 318\u2013326, Association for Computational Linguistics, June 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowledge-based dialog state tracking", "author": ["R. Kadlec", "M. Vodolan", "J. Libovicky", "J. Macek", "J. Kleindienst"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pp. 348\u2013353, IEEE, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent Polynomial Network for Dialogue State Tracking", "author": ["K. Sun", "Q. Xie", "K. Yu"], "venue": "Dialog and Discourse, pp. 1\u201322, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparison of bayesian discriminative and generative models for dialogue state tracking", "author": ["L. \u017dilka", "D. Marek", "M. Korvas", "F. Jur\u010d\u0131\u0301\u010dek"], "venue": "Proceedings of the SIGDIAL 2013 Conference, (Metz, France), pp. 452\u2013456, Association for Computational Linguistics, August 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, pp. 1735\u20131780, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["M. Henderson", "B. Thomson", "S. Young"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), (Philadelphia, PA, U.S.A.), pp. 292\u2013299, Association for Computational Linguistics, June 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental LSTM-based Dialog State Tracker", "author": ["L. Zilka", "F. Jurcicek"], "venue": "2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements.", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Blocks and Fuel : Frameworks for deep learning", "author": ["B. van Merrienboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde-farley", "J. Chorowski", "Y. Bengio"], "venue": "pp. 1\u20135, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Web-style ranking and slu combination for dialog state tracking", "author": ["J.D. Williams"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), (Philadelphia, PA, U.S.A.), pp. 282\u2013291, Association for Computational Linguistics, June 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient Dialogue State Tracking", "author": ["K. Yu", "K. Sun", "L. Chen", "S. Zhu"], "venue": "vol. 23, no. 12, pp. 2177\u20132188, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "YARBUS : Yet Another Rule Based belief Update System", "author": ["J. Fix", "H. Frezza-buet"], "venue": "2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparative error analysis of dialog state tracking", "author": ["R. Smith"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), (Philadelphia, PA, U.S.A.), pp. 300\u2013309, Association for Computational Linguistics, June 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimizing generative dialog state tracker via cascading gradient descent", "author": ["B.-J. Lee", "W. Lim", "D. Kim", "K.-E. Kim"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), (Philadelphia, PA, U.S.A.), pp. 273\u2013281, Association for Computational Linguistics, June 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Markovian discriminative modeling for dialog state tracking", "author": ["H. Ren", "W. Xu", "Y. Yan"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), (Philadelphia, PA, U.S.A.), pp. 327\u2013331, Association for Computational Linguistics, June 2014. 6", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recently proposed DSTCs [1, 2, 3] provide a shared testbed with datasets and tools for evaluating of dialog state tracking methods.", "startOffset": 24, "endOffset": 33}, {"referenceID": 1, "context": "Recently proposed DSTCs [1, 2, 3] provide a shared testbed with datasets and tools for evaluating of dialog state tracking methods.", "startOffset": 24, "endOffset": 33}, {"referenceID": 2, "context": "Recently proposed DSTCs [1, 2, 3] provide a shared testbed with datasets and tools for evaluating of dialog state tracking methods.", "startOffset": 24, "endOffset": 33}, {"referenceID": 0, "context": "The last three dialog state tracking challenges [1, 2, 3] were dominated by machine learning based trackers [4, 5, 6].", "startOffset": 48, "endOffset": 57}, {"referenceID": 1, "context": "The last three dialog state tracking challenges [1, 2, 3] were dominated by machine learning based trackers [4, 5, 6].", "startOffset": 48, "endOffset": 57}, {"referenceID": 2, "context": "The last three dialog state tracking challenges [1, 2, 3] were dominated by machine learning based trackers [4, 5, 6].", "startOffset": 48, "endOffset": 57}, {"referenceID": 3, "context": "The last three dialog state tracking challenges [1, 2, 3] were dominated by machine learning based trackers [4, 5, 6].", "startOffset": 108, "endOffset": 117}, {"referenceID": 4, "context": "The last three dialog state tracking challenges [1, 2, 3] were dominated by machine learning based trackers [4, 5, 6].", "startOffset": 108, "endOffset": 117}, {"referenceID": 5, "context": "The last three dialog state tracking challenges [1, 2, 3] were dominated by machine learning based trackers [4, 5, 6].", "startOffset": 108, "endOffset": 117}, {"referenceID": 6, "context": "However, when we consider the case where all trackers have the same Spoken Language Understanding (SLU) input, some rule based trackers [7, 8, 9, 10] achieved performance comparable to the top trackers.", "startOffset": 136, "endOffset": 149}, {"referenceID": 7, "context": "However, when we consider the case where all trackers have the same Spoken Language Understanding (SLU) input, some rule based trackers [7, 8, 9, 10] achieved performance comparable to the top trackers.", "startOffset": 136, "endOffset": 149}, {"referenceID": 8, "context": "However, when we consider the case where all trackers have the same Spoken Language Understanding (SLU) input, some rule based trackers [7, 8, 9, 10] achieved performance comparable to the top trackers.", "startOffset": 136, "endOffset": 149}, {"referenceID": 9, "context": "However, when we consider the case where all trackers have the same Spoken Language Understanding (SLU) input, some rule based trackers [7, 8, 9, 10] achieved performance comparable to the top trackers.", "startOffset": 136, "endOffset": 149}, {"referenceID": 10, "context": "A similar research direction was recently explored in [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "In our previous work [10], we introduced a belief tracker based on a few simple rules which scored second in the joint slot accuracy in DSTC3 and its slightly modified version has the state-of-theart accuracy on this dataset.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "Here we simplify the original rules for per slot tracking (in contrast with [10] where we tracked the slots jointly) and we add the machine learning component that provides parameters for these rules.", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": ", [7, 12, 9, 10].", "startOffset": 2, "endOffset": 16}, {"referenceID": 11, "context": ", [7, 12, 9, 10].", "startOffset": 2, "endOffset": 16}, {"referenceID": 8, "context": ", [7, 12, 9, 10].", "startOffset": 2, "endOffset": 16}, {"referenceID": 9, "context": ", [7, 12, 9, 10].", "startOffset": 2, "endOffset": 16}, {"referenceID": 12, "context": "The machine learning part of our tracker is realized by a LSTM [13] network.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "The other systems use a different setup where a shared model is trained for all slots and then it is fine-tuned for each separate slot [14].", "startOffset": 135, "endOffset": 139}, {"referenceID": 14, "context": "The parameters of the hybrid tracker were trained by SGD with AdaGrad [15] and Adam [16] weight update rules.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "The parameters of the hybrid tracker were trained by SGD with AdaGrad [15] and Adam [16] weight update rules.", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "These parameters were modelled by a so called durability function in our previous tracker [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "Similar approach proved to be useful also in other RNN based trackers [14, 17].", "startOffset": 70, "endOffset": 78}, {"referenceID": 16, "context": "Similar approach proved to be useful also in other RNN based trackers [14, 17].", "startOffset": 70, "endOffset": 78}, {"referenceID": 17, "context": "The models were implemented using Theano [18] and Blocks [19].", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "The models were implemented using Theano [18] and Blocks [19].", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "466 DSTC2 stacking ensemble [2] \u221a .", "startOffset": 28, "endOffset": 31}, {"referenceID": 19, "context": "Williams [20] \u221a .", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "[14] \u221a .", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] \u221a .", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "YARBUS [22] \u221a .", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "[9] \u221a .", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Williams [20] .", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "[14] .", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "406 Our previous tracker [10] .", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "[9] .", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "433 Smith [23] .", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "[24] .", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "427 YARBUS [22] .", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "[25] .", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Evaluation on the DSTC2 dataset shows that our hybrid system that extends rule based tracking core with the machine learning component outperforms the previous best tracker [20] that used the same SLU input.", "startOffset": 173, "endOffset": 177}], "year": 2016, "abstractText": "This paper presents a hybrid dialog state tracker that combines a rule based and a machine learning based approach to belief state tracking. Therefore, we call it a hybrid tracker. The machine learning in our tracker is realized by a Long Short Term Memory (LSTM) network. To our knowledge, our hybrid tracker sets a new state-of-the-art result for the Dialog State Tracking Challenge (DSTC) 2 dataset when the system uses only live SLU as its input.", "creator": "LaTeX with hyperref package"}}}