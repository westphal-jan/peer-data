{"id": "1510.08532", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "The Singular Value Decomposition, Applications and Beyond", "abstract": "The representations wealth oxidative (SVD) is nor even its classical theory 1999 \u03bb finite and tool, but notably is example attacking capabilities also laser learning but modern input molecular. In the tutorial go previous institute an guidance believe of SVD and while show their central involved made SVD been formula_17. Using majorization psychology, no could variational belief of complexity values although eigenvector. Built now SVD even way computability of formula_14 marginal functions, we discussion unitarily invariant norms, which are then designed while adopting general change brought determinant term rank approximation. We study the subdifferentials one unitarily formula_10 notions. These test wo most avoiding uses of many batteries training problems such he vector completion them formula_18 data classification. Finally, obviously cooperate matrix more lieutenant approximation and its citing developments usually as reductive SVD, formula_13 determinant computes, CUR decomposition, still Nystrom approximation. Randomized algorithms smaller important meaningful to heavy setting SVD own many as longer matrix computations.", "histories": [["v1", "Thu, 29 Oct 2015 00:59:53 GMT  (83kb)", "http://arxiv.org/abs/1510.08532v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhihua zhang"], "accepted": false, "id": "1510.08532"}, "pdf": {"name": "1510.08532.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["zhihua@sjtu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n08 53\n2v 1\n[ cs\n.L G\n] 2\n9 O\nct 2\n01 5\nThe Singular Value Decomposition, Applications\nand Beyond\nZhihua Zhang Shanghai Jiao Tong University\nzhihua@sjtu.edu.cn\nContents"}, {"heading": "1 Introduction 2", "text": "1.1 Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Notation and Definitions . . . . . . . . . . . . . . . . . . 5"}, {"heading": "2 Preliminaries 7", "text": "2.1 Kronecker Products and Vectorization Operators . . . . . 7 2.2 Majorization . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 Derivatives and Optimality . . . . . . . . . . . . . . . . . 9"}, {"heading": "3 The Singular Value Decomposition 12", "text": "3.1 Formulations . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2 Matrix Properties via SVD . . . . . . . . . . . . . . . . . 19 3.3 Matrix Concepts via SVD . . . . . . . . . . . . . . . . . . 21 3.4 Generalized Singular Value Decomposition . . . . . . . . . 23"}, {"heading": "4 Applications of SVD: Case Studies 29", "text": "4.1 The Matrix MP Pseudoinverse . . . . . . . . . . . . . . . 30 4.2 The Procrustes Problem . . . . . . . . . . . . . . . . . . . 32 4.3 Subspace Methods: PCA, MDS, FDA, and CCA . . . . . . 33"}, {"heading": "5 The QR and CUR Decompositions 37", "text": "5.1 The QR Factorization . . . . . . . . . . . . . . . . . . . . 37\nii\niii\n5.2 The CUR Decomposition . . . . . . . . . . . . . . . . . . 38"}, {"heading": "6 Variational Principles 41", "text": "6.1 Variational Properties for Eigenvalues . . . . . . . . . . . 42 6.2 Variational Properties for Singular Values . . . . . . . . . 46 6.3 Appendix: Application of Matrix Differentials . . . . . . . 48"}, {"heading": "7 Unitarily Invariant Norms 52", "text": "7.1 Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . 53 7.2 Symmetric Gauge Functions . . . . . . . . . . . . . . . . . 56 7.3 Unitarily Invariant Norms via SGFs . . . . . . . . . . . . . 58 7.4 Properties of Unitarily Invariant Norms . . . . . . . . . . . 60"}, {"heading": "8 Subdifferentials of Unitarily Invariant Norms 67", "text": "8.1 Subdifferentials . . . . . . . . . . . . . . . . . . . . . . . 68 8.2 Applications . . . . . . . . . . . . . . . . . . . . . . . . . 73"}, {"heading": "9 Matrix Low Rank Approximation 77", "text": "9.1 Basic Results . . . . . . . . . . . . . . . . . . . . . . . . . 78 9.2 Approximate Matrix Multiplication . . . . . . . . . . . . . 82"}, {"heading": "10 Large-Scale Matrix Approximation 86", "text": "10.1 Randomized SVD . . . . . . . . . . . . . . . . . . . . . . 87 10.2 Kernel Approximation . . . . . . . . . . . . . . . . . . . . 93 10.3 The CUR Approximation . . . . . . . . . . . . . . . . . . 96\nAcknowledgements 99\nReferences 100\nAbstract\nThe singular value decomposition (SVD) is not only a classical theory in matrix computation and analysis, but also is a powerful tool in machine learning and modern data analysis. In this tutorial we first study the basic notion of SVD and then show the central role of SVD in matrices. Using majorization theory, we consider variational principles of singular values and eigenvalues. Built on SVD and a theory of symmetric gauge functions, we discuss unitarily invariant norms, which are then used to formulate general results for matrix low rank approximation. We study the subdifferentials of unitarily invariant norms. These results would be potentially useful in many machine learning problems such as matrix completion and matrix data classification. Finally, we discuss matrix low rank approximation and its recent developments such as randomized SVD, approximate matrix multiplication, CUR decomposition, and Nystr\u00f6m approximation. Randomized algorithms are important approaches to large scale SVD as well as fast matrix computations.\n1 Introduction\nThe singular value decomposition (SVD) is a classical matrix theory and a key computational technique, and it has also received wide applications in science and engineering. Compared with an eigenvalue decomposition (EVD) which only works on some of square matrices, SVD applies to all matrices. Moreover, many matrix concepts and properties such as matrix pseudoinverses, variational principles and unitarily invariant norms can be induced from SVD. Thus, SVD plays a fundamental role in matrix computation and analysis.\nFurthermore, due to recent great developments of machine learning, data mining and theoretical computer science, SVD has been found to be more and more important. It is not only a powerful tool and theory but also an art. SVD makes matrices become a \u201cLanguage\" of data science.\nThe terminology of singular values has been proposed by Horn in 1950 and 1954 [Horn, 1951, 1954]. The first proof of the SVD for general m \u00d7 n matrices might be given by Eckart and Young [1939]. But the theory of singular values can date back to the 19th century when it had been studied by the Italian differential geometer E. Beltrami, the French algebraist C. Jordan, the English mathematician J. J. Sylvester,\n2"}, {"heading": "1.1. Roadmap 3", "text": "the French mathematician L. Autonne, etc. Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values.\nThere is a rich literature involving singular values or SVD. Chapter 3 of Horn and Johnson [1991] provides exhaustive studies about inequalities of singular values as well as unitarily invariant norms, and the primary focus is on the matrix theory. The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.\nThis tutorial is motivated by recent successful applications of SVD in machine learning and theoretical computer science [Hastie et al., 2001, Burges, 2010, Halko et al., 2011, Woodruff, 2014b, Mahoney, 2011, Blum et al., 2015]. The primary focus is on a perspective of machine learning. The main purpose of the tutorial includes two aspects. First, it provides a systematic tutorial to the SVD theory and illustrates its functions in matrix and data analysis. Second, it provides an advanced review about recent developments of the SVD theory in applications of machine learning and theoretical computer science."}, {"heading": "1.1 Roadmap", "text": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997]. In Chapter 2 we review some preliminaries such as Kronecker produces and vectorization operators, majorization theory, and derivatives."}, {"heading": "4 Introduction", "text": "In Chapter 3 we introduce the basic notion of SVD, including the existence, construction, and uniqueness. We then rederive some important matrix concepts and properties via SVD. We also study generalized SVD problems, which are concerned with joint decomposition of two matrices. In Chapter 4 we further illustrate the application of SVD in definition of the matrix pseudoinverse and solution of the Procrustes analysis problem. We discuss the role that SVD plays in subspace machine learning methods.\nFrom the viewpoint of computation and modern data analysis, matrix factorization techniques should be the most important issue of matrices. In Table 1.1 we summary matrix factorization methods, which are categorized into three types. In particular, the Polar decomposition, SVD, and spectral decomposition consider geometric representation of a data matrix, whereas the CX, CUR, and Nystr\u00f6m dcompositions consider a compact representation of the data themselves. That is, the latters use a portion of the data to represent the whole data. The primary focus of the QR and Cholesky decomposition is on fast computation. In Chapter 5 we give reviews about the QR and CUR decompositions.\nIn Chapter 6 we consider variational principles for singular values and eigenvalues. Specifically, we apply matrix differential calculus to rederive the von Neumann theorem [Neumann, 1937] and the Ky Fan theorem [Fan, 1951]. Accordingly, we give some inequalities for singular values and eigenvalues.\nBuilt on the inequalities for singular values, Chapter 7 discusses unitarily invariant norms. Unitarily invariant norms include the nuclear norm, Frobenius norm and spectral norm as their special cases. There is a one-to-one correspondence between a unitarily invariant norm of a matrix and a symmetric gauge function on the singular values of the matrix. This helps us to establish properties of unitarily invariant norms.\nIn Chapter 8 we study subdifferentials of unitarily invariant norms. We especially present the subdifferentials of the spectral norm and the nuclear norm as well as the applications in matrix low rank approximation. We illustrate several examples in optimization, which are solved via the subdifferentials of the spectral and nuclear norms. The subdif-"}, {"heading": "1.2. Notation and Definitions 5", "text": "ferentials of unitarily invariant norms would have potentially useful in machine learning and optimization.\nMatrix low rank approximation is a promising theme in machine learning and theoretical computer science. Chapter 9 gives two important theorems about matrix low rank approximation based on errors of unitarily invariant norms. The first one is an extension of the ordinal least squares estimation problem. The second one was proposed by Mirsky [1960], which is an extension of the novel Eckart Young theorem [Eckart and Young, 1936]. We also discuss approximate matrix multiplication, which can be regarded as an inverse process of matrix low rank approximation.\nIn Chapter 10 we study randomized SVD, CUR approximation, and Nystr\u00f6m methods to make the applications scalable. The randomized SVD and CUR approximation can be also viewed as matrix low rank approximation techniques. The Nystr\u00f6m approximation is a special case of the CUR decomposition and has been widely used to speed up kernel methods."}, {"heading": "1.2 Notation and Definitions", "text": "Throughout this tutorial, vectors and matrices are denoted by boldface lowercase letters and boldface uppercase letters, respectively. Rn+ = {u = (u1, . . . , un)T \u2208 Rn : uj \u2265 0 for j = 1, . . . , n} and Rn++ = {u = (u1, . . . , un)\nT \u2208 Rn : uj > 0 for j = 1, . . . , n}. Furthermore, if u \u2208 Rn+ (or u \u2208 Rn++), we also denote u \u2265 0 (or u > 0).\nGiven a vector x = (x1, . . . , xn) T \u2208 Rn, let |x| = (|x1|, . . . , |xn|)T ,\nlet \u2016x\u2016p = ( \u2211n\ni=1 |xi|p)1/p for p \u2265 1 be the \u2113p-norm of x, and let diag(x) be an n\u00d7 n diagonal matrix with the ith diagonal element as xi.\nLet [m] = {1, 2, . . . ,m}, Im be the m \u00d7 m identity matrix, 1m be the m \u00d7 1 vector of ones, and 0 be the zero vector or matrix with appropriate size. Let A\u2295B = [ A 0\n0 B\n]\n.\nFor a matrix A = [a1,a2, . . . ,an] = [aij ] \u2208 Rm\u00d7n, AT denotes the transpose of A, rank(A) denotes the rank, range(A) represents the range which is the space spanned by the columns (i.e., range(A) ="}, {"heading": "6 Introduction", "text": "{y \u2208 Rm : y = Ax for some x \u2208 Rn} = span{a1,a2, . . . ,an}), null(A) is the null space (i.e., null(A) = {x : Ax = 0}), and for p = min{m,n} dg(A) denotes the p-vector with aii as the ith element. Sometimes we also use Matlab Colon to represent a submatrix of A. For example, let I \u2282 [m] and J \u2282 [n]. AI,J denotes the submatrix of A with rows indexed by I and columns indexed by J , AI,: consists of those rows of A in I, and A:,J consists of those columns of A in J .\nLet \u2016A\u2016F = \u221a \u2211 ij a 2 ij denote the Frobenius norm, \u2016A\u20162 denote the spectral norm, and \u2016A\u2016\u2217 denote the nuclear norm. When A is square, we let A\u22121 be the inverse (if exists) of A, tr(A) =\n\u2211n i=1 aii be the\ntrace, and det(A) be the determinant of A.\nAn m \u00d7 m real matrix U is symmetric if AT = A, and skewsymmetric if AT = \u2212A, and normal if AAT = ATA. Clearly, symmetric and skew-symmetric matrices are normal. An m\u00d7m real matrix U is said to be orthonormal (or orthogonal) if UTU = UUT = Im. An m\u00d7 n real matrix Q for m > n is column orthonormal (or column orthogonal) if QTQ = In, and a column orthonormal Q is always able to be extended to an orthonormal matrix. A matrix M \u2208 Rm\u00d7m is said to be positive semidefinite (PSD) or positive definite if for any nonzero vector x \u2208 Rm xTMx \u2265 0 or xTMx > 0.\n2 Preliminaries\nIn this chapter we present some preliminaries, including Kronecker products and vectorization operators, majorization theory, and derivatives. We list some basic results that will be used in this monograph but omit their detailed derivations."}, {"heading": "2.1 Kronecker Products and Vectorization Operators", "text": "Given two matricesA \u2208 Rm\u00d7n and B \u2208 Rp\u00d7q, the the Kronecker product of A and B is defined by\nA\u2297B ,\n\n   a11B \u00b7 \u00b7 \u00b7 a1nB ... . . . ...\nam1B \u00b7 \u00b7 \u00b7 amnB\n\n   ,\nwhich is mp\u00d7 nq. The following properties can be found in Muirhead [1982].\nProposition 2.1. The Kronecker product has the following properties.\n(a) (\u03b1A) \u2297 (\u03b2B) = \u03b1\u03b2(A\u2297B) for any scalars \u03b1, \u03b2 \u2208 R. (b) (A\u2297B)T = AT \u2297BT .\n7"}, {"heading": "8 Preliminaries", "text": "(c) (A\u2297B)\u2297C = A\u2297 (B\u2297C). (d) If A and C are both m \u00d7 n and B is p \u00d7 q, then (A+C) \u2297 B =\nA\u2297B+C\u2297B and B\u2297 (A+C) = B\u2297A+B\u2297C. (e) If A is m\u00d7 n, B is p\u00d7 q, C is n\u00d7 r, and D is q \u00d7 s, then\n(A\u2297B)(C\u2297D) = (AC)\u2297 (BD).\n(f) If U and V are both orthogonal matrices, so is U\u2297V. (g) If A and B are symmetric positive semidefinite (SPSD), so is A\u2297B.\nKronecker products often work with vectorization operators together. Let vec(A) = (a11, . . . , am1, a12, . . . , amn) T \u2208 Rmn be vectorization of the matrix A \u2208 Rm\u00d7n. The following lemma gives the connection between Kronecker products and vectorization operators.\nLemma 2.1.\n(1) If B is p\u00d7m, X is m\u00d7 n, and C is n\u00d7 q, then\nvec(BXC) = (CT \u2297B)vec(X).\n(2) If A \u2208 Rm\u00d7n, B \u2208 Rn\u00d7p, and C \u2208 Rp\u00d7m, then\ntr(ABC) = (vec(AT ))T (Im \u2297B)vec(C).\n(3) If A \u2208 Rm\u00d7p, X \u2208 Rn\u00d7p, B \u2208 Rn\u00d7n, and C \u2208 Rp\u00d7m, then\ntr(AXTBXC) = (vec(X))T ((CA)T \u2297B)vec(X) = (vec(X))T ((CA)\u2297BT )vec(X)."}, {"heading": "2.2 Majorization", "text": "Given a vector x = (x1, . . . , xn) T \u2208 Rn, let x\u2193 = (x\u21931, . . . , x\u2193n) be such a permutation of x that x\u21931 \u2265 x \u2193 2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 x\u2193n. Given two vectors x and y \u2208 R n, x \u2265 y means xi \u2212 yi \u2265 0 for all i \u2208 [n]. We say that x is majorized by y (denoted x \u227a y) if \u2211ki=1 x\u2193i \u2264 \u2211k i=1 y \u2193 i for k = 1, . . . , n \u2212 1 and \u2211n\ni=1 x \u2193 i = \u2211n i=1 y \u2193 i . Similarly, x \u227b y if \u2211k i=1 x \u2193 i \u2265 \u2211k i=1 y \u2193 i for\nk = 1, . . . , n\u22121 and \u2211ni=1 x\u2193i = \u2211n i=1 y \u2193 i ."}, {"heading": "2.3. Derivatives and Optimality 9", "text": "We say that x is weakly submajorized by y (denoted x \u227aw y) if \u2211k\ni=1 x \u2193 i \u2264 \u2211k i=1 y \u2193 i for k = 1, . . . , n, and x is weakly superrmajorized\nby y (denoted x \u227aw y) if \u2211ki=1 x\u2193i \u2265 \u2211k i=1 y \u2193 i for k = 1, . . . , n,\nAn n \u00d7 n matrix W = [wij ] is said to be doubly stochastic if the wij \u2265 0, \u2211n j=1wij = 1 for all i \u2208 [n], and \u2211n i=1 wij = 1 for all j \u2208 [n]. Note that if Q = [qij ] \u2208 Rn\u00d7n is orthonormal, then W , [q2ij ] is a doubly stochastic matrix. It is thus called orthostochastic.\nThe following three lemmas are classical results in majorization\ntheory. They will be used in investigating unitarily invariant norms.\nLemma 2.2. [Hardy et al., 1951] Given two vectors x,y \u2208 Rn, then x \u227a y if and only if there exists a doubly stochastic matrix W such that x = Wy.\nLemma 2.3 (Birkhoff). Let W \u2208 Rn\u00d7n. Then it is a doubly stochastic matrix if and only if it can be expressed as a convex combination of a set of permutation matrices.\nLemma 2.4. Let u1, . . . , un and v1, . . . , vn be given nonnegative real numbers such that u1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 un and v1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 vn. If k\u220f\ni=1\nui \u2264 k\u220f\ni=1\nvi for k \u2208 [n],\nthen k\u2211\ni=1\nui \u2264 k\u2211\ni=1\nvi for k \u2208 [n].\nMore generally, assume f is a real-valued function such that f(exp(u)) is increasing and convex. Then\nk\u2211\ni=1\nf(ui) \u2264 k\u2211\ni=1\nf(vi) for k \u2208 [n]."}, {"heading": "2.3 Derivatives and Optimality", "text": "First let f : X \u2282 Rn \u2192 R be a continuous function. The directional derivative of f at x\u0304 in a direction u \u2208 X is defined as\nf \u2032(x\u0304;u) = lim t\u21930 f(x\u0304+ tu)\u2212 f(x\u0304) t ,"}, {"heading": "10 Preliminaries", "text": "when this limit exists. When the directional derivative f \u2032(x\u0304;u) is linear in u (that is, f \u2032(x\u0304;u) = \u3008a,u\u3009 for some a \u2208 X ) then we say f is (G\u00e2beaux) differentiable at x\u0304, with derivative \u2207f(x\u0304) = a. If f is differentiable at every point in X then we say f is differentiable on X .\nWhen f is not differentiable but convex, we consider a notion of\nsubdifferentials. We say z is the subgradient of f at x\u0304 if it satisfies\nf(x\u0304) \u2264 f(x) + \u3008z, x\u0304 \u2212 x\u3009 for all points z \u2208 X .\nThe set of subgradients is called the subdifferential, and denoted by \u2202f(x\u0304). The subdifferential is always a closed convex set. The following result shows a connection between subgradients and directional derivatives.\nLemma 2.5 (Max Formula). If the function f : X \u2192 (\u2212\u221e,+\u221e] is convex, then any point x\u0304 in core(domf) and any direction u in X satisfy\nf \u2032(x\u0304;u) = max { \u3008z,u\u3009 : z \u2208 \u2202f(x\u0304) } .\nThe further details of these results can be found from Borwein and Lewis [2006]. The following lemma then shows the fundamental role of subgradients in optimization.\nLemma 2.6. For any proper convex function f : X \u2192 (\u2212\u221e,+\u221e], the point x\u0304 is a minimizer of f if and only if the condition 0 \u2208 \u2202f(x\u0304) holds.\nNow let f be a differentiable function from Rm\u00d7n to R. For a matrix X = [xij ] \u2208 Rm\u00d7n, df(X)dX = ( df dxij )\n(m\u00d7 n) defines the derivative of f w.r.t. X. The Hessian matrix of f w.r..t. X is defined as d\n2f(X) dvec(X)dvec(X)T ,\nwhich is an mn\u00d7mn matrix. Let us see an example.\nExample 2.1. We define the function f as\nf(X) = tr(XTMX),\nwhere M = [mij ] \u2208 Rm\u00d7m is a given constant matrix. It is directly computed that dfdxij = \u2211m l=1(mil+mli)xlj . This implies that df dX = (M+ MT )X. In fact, the derivative can be computed as follows. Compute\ndf = tr(dXTMX+XTMdX) = tr((M+MT )XdXT )."}, {"heading": "2.3. Derivatives and Optimality 11", "text": "We thus have that dfdX = (M+M T )X.\nAdditionally, it follows from Lemma 2.1 that f(X) = vec(X)T (In\u2297 M)vec(X). Thus, we have\ndf\ndvec(X) = vec\n( df\ndX\n)\n= [In \u2297 (M+MT )]vec(X),\nand hence, d2f(X)\ndvec(X)dvec(X)T = In \u2297 (M+MT ).\n3 The Singular Value Decomposition\nThe singular value decomposition (SVD) is a classical matrix theory and computational tool. In modern data computation and analysis, SVD becomes more and more important. In this chapter we aim to provide a systematical review about the basic principle of SVD.\nWe will see that there are four approaches to SVD. The first approach is depart from the spectral decomposition of a symmetric positive semidefinite (SPSD) matrix. The second approach gives a construction process via induction. In the third approach the SVD problem is equivalently formulated into an eigenvalue decomposition problem of a symmetric matrix (see Theorem 3.5). The fourth approach is based on the equivalent relationship between the SVD and polar decomposition (see Theorem 3.6).\nWe also study uniqueness of SVD (see Theorem 3.2 and Corollary 3.3). These results will be used in derivation of subdifferentials of unitarily invariant norms (see Chapter 8). Additionally, we present a generalized SVD (GSVD), which addresses joint decomposition problems of two matrices. When the two matrices form a column orthonormal matrix, the resulting GSVD process is called the CS decomposition.\n12"}, {"heading": "3.1. Formulations 13", "text": ""}, {"heading": "3.1 Formulations", "text": "Given a nonzero SPSD matrix M \u2208 Rn\u00d7n, let \u03b3i for i = 1, . . . , n be the eigenvalues of M and xi be the corresponding eigenvectors. That is,\nMxi = \u03b3ixi, i = 1, . . . , n. (3.1)\nIt is well known that the xi can be assumed to be mutually orthonormal. Let \u0393 = diag(\u03b31, . . . , \u03b3n) and X = [x1, . . . ,xn] such that X TX = In. We write (3.1) in matrix form as\nMX = X\u0393.\nThis gives rise to an eigenvalue decomposition (EVD) of M:\nM = X\u0393XT .\nSince the \u03b3i are nonnegative, this decomposition is also called a spectral decomposition of the SPSD matrix M.\nNote that the above EVD always exists when M is symmetric but not PSD. However, the current eigenvalues \u03b3i are not necessarily nonnegative. Let \u0393\u0302 = diag(|\u03b31|, . . . , |\u03b3n|) and Y = [y1, . . . ,yn] with yi = sgn(\u03b3i)xi where sgn(0) = 1. Then the decomposition is reformulated as\nM = X\u0393\u0302Y,\nwhere XTX = In, Y TY = In, and \u0393\u0302 is a nonnegative diagonal matrix. This new formulation defines a singular value decomposition (SVD) of the symmetric matrix M.\nNaturally, a question emerges: does an SVD exist for an arbitrary matrix? Let A \u2208 Rm\u00d7n of rank r where r \u2264 min{m,n}. Without loss of generality, we assume m \u2265 n for ease of exposition, because we can consider AT when m < n.\nConsider that AAT is SPSD, so it has the spectral decomposition,\nwhich is defined as\nAAT = U\u039bUT ,\nwhere \u039b = diag(\u03bb1, . . . , \u03bbm) and U TU = Im. Since rank(AA T ) = rank(A) = r, AAT has and only has r positive eigenvalues and the corresponding eigenvectors can form a column orthonormal matrix."}, {"heading": "14 The Singular Value Decomposition", "text": "Assume that \u039br = diag(\u03bb1, \u03bb2, . . . , \u03bbr) and Ur = [u1,u2, . . . ,ur] where \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbr are the positive eigenvalues of AAT and Ur is the m \u00d7 r matrix of the corresponding eigenvectors such that UTr Ur = Ir. Thus, it follows from the spectral decomposition that\nUTr AA TUr = \u039br\nand UT\u2212rAA TU\u2212r = 0 where U\u2212r consists of the last m\u2212r columns of U. Thus, we have ATU\u2212r = 0. Let Vr = [v1, . . . ,vr] , ATUr\u039b \u22121/2 r . Then it satisfies VTr Vr = Ir. Note that\nATU(\u039b\u22121/2r \u2295 Im\u2212r) = [Vr,ATU\u2212r] = [Vr,0],\nwhich implies that AT = [Vr,0](\u039b 1 2 r \u2295 Im\u2212r)UT = Vr\u039b 1 2 r U T r . Hence,\nA = Ur\u03a3rV T r , (3.2)\nwhere \u03a3r = diag(\u03c31, \u03c32, . . . , \u03c3r) with \u03c3i = \u03bb 1/2 i for i = 1, . . . , r. Clearly, \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3r > 0. We refer to (3.2) as the condensed SVD of A, where \u03c3i\u2019s are called the singular values, the columns ui of Ur and the columns vi of Vr are called respectively the left and right singular vectors of A.\nRecall that we always assume that \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3r > 0. Let \u03a3n = diag(\u03c31, . . . , \u03c3r, 0, . . . , 0) be the n \u00d7 n diagonal matrix, and Un be an m \u00d7 n column-orthonormal matrix consisting of Ur in the first m\u00d7r block. In this case, we can equivalently write the condensed SVD of A as\nA = Un\u03a3nV T , (3.3)\nwhich is called a thin (or reduced) SVD of A. Furthermore, we extend Un to a square orthonormal matrix (denoted U), and \u03a3n to an m\u00d7 n matrix \u03a3 by adding m\u2212n rows of zeros below. Then SVD can be also expressed as\nA = U\u03a3VT , (3.4)\nwhich is called a full SVD of A.\nAs we have seen, these three expressions are mutually equivalent. We will sometimes use A = U\u03a3VT for the thin SVD for notational simplicity. In a thin SVD version, let us always keep it in mind that"}, {"heading": "3.1. Formulations 15", "text": "\u03a3 is square and U or V is column orthonormal. We now present the formal formation of SVD of an arbitrary A \u2208 Rm\u00d7n in which m \u2265 n is not necessarily required.\nTheorem 3.1. Given an arbitrary A \u2208 Rm\u00d7n, its full SVD defined in (3.4) always exists. Furthermore, the singular values \u03c3i are uniquely determined.\nBased on the spectral decomposition of AAT , we have previously shown the existence proof of the SVD theorem. Here we present a constructive proof, which has been widely given in the literature.\nProof. IfA is zero, the result is trivial. Thus, letA be a nonzero matrix. Define \u03c31 , max\u2016x\u20162=1 \u2016Ax\u20162, which exists because x 7\u2192 \u2016Ax\u20162 is continuous and the set {x \u2208 Rn : \u2016x\u20162 = 1} is compact. Moreover, \u03c31 > 0. Let v1 \u2208 Rn be such a vector that \u03c31 = \u2016Av1\u20162. Define u1 = Av1/\u03c31, which satisfies \u2016u1\u20162 = 1.\nWe extend u1 and v1 to orthonormal matrices U = [u1,U\u22121] and V = [v1,V\u22121], respectively. Then\nUTAV =\n[\n\u03c31 u T 1 AV\u22121\n0 UT\u22121AV\u22121\n]\n, B,\nwhere we use the fact UT\u22121Av1 = \u03c31U T \u22121u1 = 0. Note that\nmax \u2016x\u20162=1 \u2016Bx\u201622 = max\u2016x\u20162=1 \u2016UTAVx\u201622 = max\u2016x\u20162=1 \u2016Ax\u201622 = \u03c321 .\nHowever,\n1\n\u03c321 + z T z\n\u2225 \u2225 \u2225 \u2225 \u2225 B [ \u03c31 z ]\u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n\u2265 \u03c321 + zTz,\nwhere z = VT\u22121A Tu1. This implies that z must be zero.\nThe proof is completed by induction. In particular, assume (m \u2212 1) \u00d7 (n \u2212 1) matrix UT\u22121AV\u22121 has a full SVD UT\u22121AV\u22121 = U\u0303\u03a3\u0303V\u0303T . Then A has a full SVD:\nA = [u1,U\u22121]\n[\n1 0\n0 U\u0303\n] [\n\u03c31 0\n0 \u03a3\u0303\n] [\n1 0 0 V\u0303T\n] [\nvT1 VT\u22121\n]\n= [u1,U\u22121U\u0303]\n[\n\u03c31 0\n0 \u03a3\u0303\n] [\nvT1 (V\u22121V\u0303)T\n]\n,"}, {"heading": "16 The Singular Value Decomposition", "text": "because the matrices [u1,U\u22121U\u0303] and [v1,V\u22121V\u0303] are orthonormal.\nAs for the uniqueness of the singular values is due to that the \u03c32i are eigenvalues of AAT which are unique. Unfortunately, the left and right singular matrices Ur and Vr are not unique. However, we have the following result.\nTheorem 3.2. Let A = Ur\u03a3rV T r be a given condensed SVD of A. Assume there are \u03c1 distinct values among the nonzero singular values \u03c31, . . . , \u03c3r, with respective multiplicities ri (satisfying \u2211\u03c1 i=1 ri = r). Then A = U\u0303r\u03a3rV\u0303 T r is a condensed SVD if and only if\nU\u0303r = Ur(Q1 \u2295Q2 \u2295 . . . \u2295Q\u03c1) and V\u0303r = Vr(Q1 \u2295Q2 \u2295 . . .\u2295Q\u03c1),\nwhere Qi is an arbitrary ri \u00d7 ri orthonormal matrix. Furthermore, if all the nonzero singular values are distinct, then the Qi are either 1 or \u22121. In other words, the left and right singular vectors are uniquely determined up to signs.\nProof. Let \u03b41 > \u03b42 > . . . > \u03b4\u03c1 be the \u03c1 distinct values among the \u03c31, . . . , \u03c3r. This implies that\n\u03a3r = \u03b41Ir1 \u2295 \u03b42Ir2 \u2295 . . .\u2295 \u03b4\u03c1Ir\u03c1. (3.5)\nThe sufficiency follows from the fact that\n(Q1 \u2295 . . .\u2295Q\u03c1)(\u03b41Ir1 \u2295 . . . \u2295 \u03b4\u03c1Ir\u03c1)(QT1 \u2295 . . . \u2295QT\u03c1 ) = \u03a3r.\nWe now prove the necessary condition. Consider that range(Ur) = range(A) = range(U\u0303r) and range(Vr) = range(A T ) = range(V\u0303r). Thus, we have\nU\u0303r = UrS and V\u0303r = VrT,\nwhere S and T are some r \u00d7 r orthonormal matrices. Hence, \u03a3r = S\u03a3rT T , or equivalently, \u03a3rT = S\u03a3r. As in (3.5) for \u03a3, partition S and T into\nS =\n\n   S11 . . . S1\u03c1 ... . . . ...\nS\u03c11 . . . S\u03c1\u03c1\n\n   and T =\n\n   T11 . . . T1\u03c1 ... . . . ...\nT\u03c11 . . . T\u03c1\u03c1\n\n   ,"}, {"heading": "3.1. Formulations 17", "text": "where Sij and Tij are ri \u00d7 rj. It follows from \u03a3rT = S\u03a3r that \u03b4iTii = \u03b4iSii for i = 1, . . . , \u03c1 and \u03b4iTij = \u03b4jSij . As a result, we obtain that Sii = Tii for i = 1, . . . , \u03c1. Since S and T are orthonormal, we have\n\u03c1 \u2211\nj=1\nSijS T ij = Iri =\n\u03c1 \u2211\nj=1\nTijT T ij.\nNote that \u2211\u03c1\nj=1T\u03c1jT T \u03c1j = \u2211\u03c1 j=1 \u03b42 j\n\u03b42\u03c1 S\u03c1jS\nT \u03c1j, which implies that\n\u2211\nj<\u03c1\n[ 1\u2212 \u03b42j \u03b42\u03c1 ] S\u03c1jS T \u03c1j = 0. (3.6)\nSince 1 \u2212 \u03b4 2 j\n\u03b42\u03c1 < 0 for j < \u03c1 and S\u03c1jS\nT \u03c1j is always PSD, we must have\nS\u03c1j = 0 for all j < \u03c1, for otherwise, if there were a k < \u03c1 such that S\u03c1k 6= 0, there would exist a nonzero x \u2208 Rr\u03c1 such that xTS\u03c1kST\u03c1kx > 0. It would lead to\n\u2211\nj<\u03c1\n[ 1\u2212 \u03b42j \u03b42\u03c1 ] xTS\u03c1jS T \u03c1jx < 0,\nwhich conflicts with (3.6). Accordingly, S\u03c1j = T\u03c1j = 0 for all j < \u03c1, and hence, S\u03c1\u03c1S T \u03c1\u03c1 = T\u03c1\u03c1T T \u03c1\u03c1 = Ir\u03c1 . It also follows from the orthogonality of S and of T that for any i < \u03c1,\n0 = \u03c1 \u2211\nj=1\nSijS T \u03c1j = Si\u03c1S T \u03c1\u03c1 and 0 =\n\u03c1 \u2211\nj=1\nTijT T \u03c1j = Ti\u03c1T T \u03c1\u03c1,\nwhich leads to Si\u03c1 = Ti\u03c1 = 0 for i < \u03c1.\nSimilarly, consider the \u03c1\u22121, \u03c1\u22122, . . . , 2 cases. We have Sij = Tij = 0 for i 6= j, Sii = Tii and SiiSTii = TiiTTii = Iri for i \u2208 [\u03c1]. As a result, setting Qi = Sii completes the proof.\nWe now extend the result in Theorem 3.2 to the full SVD and thin\nSVD of A. The following corollary is immediately obtained.\nCorollary 3.3. Let A = U\u03a3VT be a given full SVD of A \u2208 Rm\u00d7n. Then A = U\u0303\u03a3V\u0303T is a full SVD if and only if U\u0303 = UQ and V\u0303 = VP where Q = Q1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 Q\u03c1 \u2295 Q0 and P = Q1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 Q\u03c1 \u2295 P0. Here Q1, . . . ,Q\u03c1 are defined as in Theorem 3.2, and Q0 \u2208 R(m\u2212r)\u00d7(m\u2212r) and"}, {"heading": "18 The Singular Value Decomposition", "text": "P0 \u2208 R(n\u2212r)\u00d7(n\u2212r) are any orthonormal matrices. Obviously, Q\u03a3PT = \u03a3 and QT\u03a3P = \u03a3 hold.\nAssume m \u2265 n and A = U\u03a3VT is a given thin SVD of A \u2208 Rm\u00d7n. Then A = U\u0303\u03a3V\u0303T is a thin SVD if and only if U\u0303 = UQ and V\u0303 = VP where Q = Q1\u2295\u00b7 \u00b7 \u00b7\u2295Q\u03c1\u2295Q0 and P = Q1\u2295\u00b7 \u00b7 \u00b7\u2295Q\u03c1\u2295P0. Currently, Q0 \u2208 R(n\u2212r)\u00d7(n\u2212r) is any orthonormal matrix. Obviously, Q\u03a3 = \u03a3Q, \u03a3PT = PT\u03a3, and Q\u03a3PT = \u03a3 hold.\nTheorem 3.2 and Corollary 3.3 will be used in derivation of subdifferentials of unitarily invariant norms (see Chapter 8). When the matrix in question is SPSD, the spectral decomposition and SVD are identical. That is, U = V in this case. Moreover, the eigenvalues and singular values are identical.\nThe construction proof of Theorem 3.1 shows that\n\u03c31(A) = max{\u2016Av\u20162 : v \u2208 Rn, \u2016v\u20162 = 1}, so there exists a unit vector v1 \u2208 Rn such that \u03c31(A) = \u2016Av1\u20162;\n\u03c32(A) = max{\u2016Av\u20162 : v \u2208 Rn, \u2016v\u20162 = 1,vTv1 = 0}, so there exists a unit vector v2 \u2208 Rn such that vT2 v1 = 0 and \u03c32(A) = \u2016Av2\u20162;\n...\n\u03c3k(A) = max{\u2016Av\u20162 : v \u2208 Rn, \u2016v\u20162 = 1,vT [v1, . . . ,vk\u22121] = 0}, so there exists a unit vector vk \u2208 Rn such that vTk [v1, . . . ,vk\u22121] = 0 and \u03c3k(A) = \u2016Avk\u20162;\n...\nThe following theorem is the generalization of the Courant-Fischer\ntheorem for singular values.\nTheorem 3.4. Given a matrix A \u2208 Rm\u00d7n, let \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3p be"}, {"heading": "3.2. Matrix Properties via SVD 19", "text": "the singular values of A where p = min{m,n}. For any k \u2208 [p], then\n\u03c3k = min v1,...,vk\u22121\u2208Rn\nmax v \u2208 Rn, \u2016v\u20162 = 1\nvT [v1, . . . ,vk\u22121] = 0\n\u2016Av\u20162\n= max v1,...,vn\u2212k\u2208Rn\nmin v \u2208 Rn, \u2016v\u20162 = 1\nvT [v1, . . . ,vn\u2212k] = 0\n\u2016Av\u20162."}, {"heading": "3.2 Matrix Properties via SVD", "text": "In what follows, we list some matrix properties which can be induced from SVD. These properties show that SVD is fundamental not only in matrix computation but also in matrix analysis.\nProposition 3.1. Let A = U\u03a3VT be a full SVD of m \u00d7 n matrix A, and A = Ur\u03a3rV T r be a condensed SVD. Let p = min{m,n}. Then\n(1) The rank of A is equal to the number of the nonzero singular values \u03c3i of A. (2) \u2016A\u20162 = \u03c31 is the spectral norm and \u2016A\u2016F = \u221a \u2211 i,j a 2 ij = \u221a \u2211p i=1 \u03c3 2 i is\nthe Frobenius norm.\n(3) range(A) = range(AAT ) = range(Ur) = span(u1, . . . ,ur) and\nnull(A) = range(V\u2212r) = span(vr+1, . . . ,vn).\n(4) range(AT ) = range(ATA) = range(Vr) = span(v1, . . . ,vr) and\nnull(AT ) = range(U\u2212r) = span(ur+1, . . . ,um).\n(5) The eigenvalues ofATA are \u03c32i for i = 1, . . . , r and n\u2212r zeros. The right singular vectors vi are the corresponding orthonormal eigenvectors. (6) The eigenvalues of AAT are \u03c32i for i = 1, . . . , r and m \u2212 r zeros. The left singular vectors ui are the corresponding orthonormal eigenvectors. (7) Let B = UB\u03a3BVB be the condensed SVD of B. Then A \u2295 B = (U \u2295 UB)(\u03a3 \u2295 \u03a3B)(VT \u2295 VTB) is the condensed SVD of A\u2295B, and A\u2297B = (U\u2297UB)(\u03a3\u2297\u03a3B)(VT \u2297VTB) is the condensed SVD of A\u2297B. (8) If A is square and invertible, then A\u22121 = V\u03a3\u22121UT and |det(A)| = \u220fn\ni=1 \u03c3i(A)."}, {"heading": "20 The Singular Value Decomposition", "text": "Theorem 3.5. Given a matrix A \u2208 Rm\u00d7n, let H = [ 0 AT\nA 0\n]\n. If\nA = Ur\u03a3rV T r be the condensed SVD, then H has 2r nonzero eigenvalues, which are \u00b1\u03c3i, with the corresponding orthonormal eigenvectors 1\u221a 2 [ vi\n\u00b1ui\n]\n, i = 1, . . . , r.\nConversely, if \u03b3i is the eigenvalue of H, with the corresponding\neigenvector zi =\n[\nz (1) i z (2) i\n]\nwhere z (1) i \u2208 Rn and z (2) i \u2208 Rm, then \u2212\u03b3i is\nthe eigenvalue of H, with the corresponding eigenvector zi =\n[\nz (1) i\n\u2212z(2)i\n]\n.\nFurthermore, let the \u03c3i denote the r positive values among the \u00b1\u03b3i,\nand 1\u221a 2\n[\nvi ui\n]\ndenote the corresponding orthonormal eigenvectors. Then\nA = Ur\u03a3rV T r , where Ur = [u1, . . . ,ur], Vr = [v1, . . . ,vr], and \u03a3r = diag(\u03c31, . . . , \u03c3r), is a condensed SVD of A.\nProof. The first part is directly obtained from the fact that\nH =\n[\n0 AT\nA 0\n]\n=\n[\n0 Vr\u03a3rU T r\nUr\u03a3rV T r 0\n]\n= 1\n2\n[\nVr Vr Ur \u2212Ur\n] [\n\u03a3r 0\n0 \u2212\u03a3r\n] [\nVTr U T r VTr \u2212UTr\n]\n.\nConversely, consider that [\n0 AT\nA 0\n] [\nz (1) i\n\u2212z(2)i\n]\n=\n[\n\u2212ATz(2)i Az\n(1) i\n]\n=\n[\n\u2212\u03b3iz(1)i \u03b3iz (2) i\n] = \u2212\u03b3i [ z (1) i\n\u2212z(2)i\n]\n,\nwhich shows that \u2212\u03b3i is the eigenvalue of H, with the corresponding\neigenvector\n[\nz (1) i\n\u2212z(2)i\n]\n. Now using the notation of \u03a3r, Ur, and Vr, we\nhave the EVD of H:\nH =\n[\n0 AT\nA 0\n]\n= 1\n2\n[\nVr Vr Ur \u2212Ur\n] [\n\u03a3r 0\n0 \u2212\u03a3r\n] [\nVTr U T r VTr \u2212UTr\n]\n.\nIt also follows from the orthogonality of the eigenvectors that UTr Ur + VTr Vr = 2Ir and U T r Ur \u2212 VTr Vr = 0. This implies that UTr Ur = VTr Vr = Ir. Thus, A = Ur\u03a3rV T r is a condensed SVD of A."}, {"heading": "3.3. Matrix Concepts via SVD 21", "text": "Theorem 3.5 establishes an interesting connection of the SVD of a general matrix with the EVD of a symmetric matrix. This provides an approach to handling an SVD problem of an arbitrary matrix. That is, one transforms the SVD problem into an EVD problem of an associated symmetric matrix. The theorem also gives an alternative proof for the SVD theory.\nThe following theorem shows that the Polar Decomposition of a matrix can be induced from its SVD. Note that SVD can be also derived from the Polar decomposition. Here we do not give the detail of this derivation.\nTheorem 3.6 (Polar Decomposition). Let A \u2208 Rm\u00d7n be a given matrix where m \u2265 n. Then its polar decomposition exists; that is, there are a column orthonormal matrix Q and a unique SPSD matrix S such that A = QS. Furthermore, if A is full column rank, then Q is unique.\nProof. Let A = U\u03a3VT be a thin SVD of A. Then\nA = UVTV\u03a3VT , QS,\nwhere Q , UVT is column orthonormal and S , V\u03a3VT is SPSD.\nAssume that A has two Polar decompositions: A = Q1S1 and Q2S2. Make the full SVDs (spectral decomposition) of S1 and S2 as S1 = V1\u03a31V T 1 and S2 = V2\u03a32V T 2 , respectively. Then A = (Q1V1)\u03a31V T 1 and A = (Q2V2)\u03a32V T 2 be two thin SVDs of A. This implies that \u03a31 = \u03a32 , \u03a3. Moreover, it follows from Corollary 3.3 that V2 = V1P1 and Q2V2 = Q1V1P2 where P1 and P2 are orthonormal matrices such that \u03a3PT1 = P T 1 \u03a3. Thus, S2 = V2\u03a3V T 2 = V1P1\u03a3P T 1 V T 1 = V1\u03a3V T 1 = S1.\nIf A is full column rank, then S is invertible. Hence, Q1 = Q2.\nAs we see from the proof, S = V\u03a3VT = (ATA)1/2; that is, S is\nidentical to the square root of the matrix ATA."}, {"heading": "3.3 Matrix Concepts via SVD", "text": "All matrices have SVD, so SVD plays a central role in matrix analysis and computation. As we have seen in the previous section, many"}, {"heading": "22 The Singular Value Decomposition", "text": "matrix concepts and properties can be induced from SVD. Here we present other several matrix notions, which are used in modern matrix computations.\nDefinition 3.1. Assume A \u2208 Rm\u00d7n and B \u2208 Rm\u00d7n are of rank k and rank l, respectively, and l \u2265 k. Let A = UA,k\u03a3A,kVTA,k and B = UB,l\u03a3B,lV T B,l be the condensed SVDs of A and B. The cosines of the canonical angles between A and B are defined as\ncos \u03b8i(A,B) = \u03c3i(U T A,kUB,l), i = 1, . . . , k.\nConsider that\n\u03c32(UTA,kUB,l) = \u03bb(U T A,kUB,lU T B,lUA,k)\nand UTA,kUB,lU T B,lUA,k +U T A,kUB,\u2212lU T B,\u2212lUA,k = Ik, where UB,\u2212l \u2208 R m\u00d7n\u2212l is an orthonormal complement of UB,l. Thus, we have that\n\u03bb(UTA,kUB,lU T B,lUA,k) = 1\u2212 \u03bb(UTA,kUB,\u2212lUTB,\u2212lUA,k).\nIn other words, \u03c32(UTA,kUB,l) = 1\u2212 \u03c32(UTA,kUB,\u2212l). Hence,\nsin \u03b8i(A,B) = \u03c3k+1\u2212i(U T A,kUB,\u2212l), i = 1, . . . , k.\nNote that \u03c31(U T A,kUB,\u2212l) = \u2016UTA,kUB,\u2212l\u20162, which is also cased the distance between two subspaces spanned by UA,k and UB,l.\nDefinition 3.2. Given a nonzero matrix A \u2208 Rm\u00d7n, let \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3p where p = min{m,n}. The stable rank of A is defined as \u2211pi=1 \u03c32 i\n\u03c32 1\n, and\nthe nuclear rank is defined as \u2211p\ni=1 \u03c3i \u03c31 .\nClearly, \u2211p i=1 \u03c32 i\n\u03c32 1 \u2264 \u2211pi=1 \u03c3i\u03c31 \u2264 rank(A). The concepts have been recently proposed for describing error bounds of matrix multiplication approximation [Magen and Zouzias, 2011, Cohen et al., 2015, Kyrillidis et al., 2014].\nDefinition 3.3 (Statistical Leverage Score). Given an m\u00d7 n matrix A with m > n, let A have a thin SVD A = U\u03a3VT , and let u(i) be the ith row of U. Then the statistical leverage scores of the rows of A are defined as\nli = \u2016u(i)\u201622 for i = 1, . . . ,m."}, {"heading": "3.4. Generalized Singular Value Decomposition 23", "text": "The coherence of the rows of A is defined as\n\u03b3 , max i li.\nThe (i, j)-cross leverage scores are defined as\ncij = (u (i))Tu(j).\nThe statistical leverage [Hoaglin and Welsch, 1978] measures the extent to which the singular vectors of a matrix are correlated with the standard basis. Recently, it has found usefulness in large-scale data analysis and in the analysis of randomized matrix algorithms [Drineas et al., 2008, Mahoney and Drineas, 2009, Ma et al., 2014]. A related notion is that of matrix coherence, which has been of interest in matrix completion and Nystr\u00f6m-based low rank matrix approximation [Cand\u00e8s and Recht, 2009, Talwalkar and Rostamizadeh, 2010, Wang and Zhang, 2013, Nelson and Nguy\u00ean, 2013]."}, {"heading": "3.4 Generalized Singular Value Decomposition", "text": "This section studies simultaneous SVD of two given matrices A and B. This leads us to a generalized SVD (GSVD) problem.\nTheorem 3.7 (GSVD). Suppose two matrices A \u2208 Rm\u00d7p and B \u2208 R n\u00d7p with n \u2265 p are given. Let q = min{m, p}. Then there exist two orthonormal matrices UA \u2208 Rm\u00d7m and UB \u2208 Rn\u00d7n, and an invertible matrix X \u2208 Rp\u00d7p such that\nUTAAX = diag(\u03b11, . . . , \u03b1q) and U T BBX = diag(\u03b21, . . . , \u03b2p),\nwhere \u03b11 \u2265 \u00b7 \u00b7 \u00b7\u03b1q \u2265 0, and 0 \u2264 \u03b21 \u2264 \u00b7 \u00b7 \u00b7 \u03b2p.\nThe GSVD theorem was originally proposed by Loan [1976], in which n \u2265 p (or m \u2265 p) is required. Later on, Paige and Saunders [1981] developed a more general formulation for GSVD in which matrix pencil A and B are required only to have the same number of columns. Paige and Saunders [1981] also studied a GSVD of submatrices of a column orthonormal matrix. That is a so-called CS decomposition [Golub et al., 1999] given as follows."}, {"heading": "24 The Singular Value Decomposition", "text": "Theorem 3.8 (The CS Decomposition). Let Q \u2208 R(m+n)\u00d7p be a column orthonormal matrix. Partition it as QT = [QT1 ,Q T 2 ] where Q1 and Q2 arem\u00d7p and n\u00d7p. Then there exist orthonormal matrices U1 \u2208 Rm\u00d7m, U2 \u2208 Rn\u00d7n, and V1 \u2208p\u00d7p such that\nUT1 Q1V1 = C and U T 2 Q2V1 = S,\nwhere\nC =\n\n \nr s p\u2212r\u2212s r Ir 0 0 s 0 C1 0 m\u2212r\u2212s 0 0 0   ,\nS =\n\n  r s p\u2212r\u2212s n+r\u2212p 0 0 0 s 0 S1 0 p\u2212r\u2212s 0 0 Ip\u2212r\u2212s   ,\nC1 = diag(\u03b11, . . . , \u03b1s) and S1 = diag( \u221a 1\u2212 \u03b121, . . . , \u221a\n1\u2212 \u03b12s), and 1 > \u03b11 \u2265 \u03b12 \u2265 \u00b7 \u00b7 \u00b7\u03b1s > 0.\nProof. Since QT1 Q1 + Q T 2 Q2 = Q TQ = Ip, the largest eigenvalue of QT1 Q1 (reps. Q T 2Q2) is at most 1. This implies \u2016Q1\u20162 = \u03c31(Q1) \u2264 1 (resp. \u2016Q2\u20162 \u2264 1). Let q = min{m, p}. Make a full SVD of Q1 as\nQ1 = U1CV T 1 ,\nwhere C = diag(c1, . . . , cq) is an m\u00d7p diagonal matrix. Assume\n1 = c1 = \u00b7 \u00b7 \u00b7 = cr > cr+1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 cr+s > cr+s+1 = \u00b7 \u00b7 \u00b7 cp = 0.\nLet D = diag(cr+1, . . . , cr+s)\u2295 0, which is (m\u2212 r)\u00d7 (p\u2212 r), and\nQ2V1 = [W1 \ufe38\ufe37\ufe37\ufe38\nr\n,W2 \ufe38\ufe37\ufe37\ufe38\np\u2212r\n].\nThen [\nU1 0\n0 In\n]T [\nQ1 Q2\n]\nV1 =\n\n \nIr 0 0 D\nW1 W2\n\n "}, {"heading": "3.4. Generalized Singular Value Decomposition 25", "text": "is column orthonormal. This implies that W1 = 0 and\nWT2 W2 = Ip\u2212r \u2212DTD = diag(1\u2212 c2r+1, . . . , 1\u2212 c2p)\nis nonsingular. Define si = \u221a 1\u2212 c2i for i \u2208 [p]. Then\nZ , W2diag(1/sr+1, . . . , 1/sp)\nis column orthonormal. We now extend Z to an n \u00d7 n orthonormal matrix U2, the last p\u2212 r columns of which constitute Z. When setting \u03b11 = cr+1, \u00b7 \u00b7 \u00b7 , \u03b1s = cr+s, we have\nUT2 Q1V1 = S.\nThus, the theorem follows.\nRemarks It is worth pointing out that Q1 = U2SV T 1 is not certainly a full SVD of Q1, because some of the nonzero elements of S might not lie on the principal diagonal. However, if n \u2265 p, then we can move the first n\u2212 p rows of S to be the last n\u2212 p rows by pre-multiplying some permutation matrix P. That is,\nPTUT2 Q1V1 =\n\n   \nr s p\u2212r\u2212s r 0 0 0 s 0 S1 0 p\u2212r\u2212s 0 0 Ip\u2212r\u2212s n\u2212p 0 0 0      .\nThis is the reason why the restriction n \u2265 p is required in Theorem 3.7 (A and B correspond to Q1 and Q2, respectively).\nThe following theorem gives a more general version of Theorem 3.7 as well as Theorem 3.8. Compared with Theorem 3.7, m \u2265 p or n \u2265 p are no longer restricted. Compared with Theorem 3.8, the submatrices in question do not necessarily form a column orthonormal matrix.\nTheorem 3.9. Suppose two matrices A \u2208 Rm\u00d7p and B \u2208 Rn\u00d7p are given. Let KT , [AT ,BT ] with the rank t. Then exist orthonormal matrices UA \u2208 Rm\u00d7m, UB \u2208 Rn\u00d7n, W \u2208 Rt\u00d7t, and V \u2208 Rp\u00d7p such that\nUTAAV = \u03a3A[W TR\n\ufe38 \ufe37\ufe37 \ufe38\nt\n, 0 \ufe38\ufe37\ufe37\ufe38 p\u2212t ] and UTBBV = \u03a3B [W TR \ufe38 \ufe37\ufe37 \ufe38 t , 0 \ufe38\ufe37\ufe37\ufe38 p\u2212t ],"}, {"heading": "26 The Singular Value Decomposition", "text": "whereR \u2208 Rt\u00d7t is a positive diagonal matrix with its diagonal elements equal to the nonzero of singular values of K,\n\u03a3A =\n\n \nr s t\u2212r\u2212s r Ir 0 0 s 0 DA 0 m\u2212r\u2212s 0 0 0   , (3.7)\n\u03a3B =\n\n  r s t\u2212r\u2212s n+r\u2212t 0 0 0 s 0 DB 0 t\u2212r\u2212s 0 0 It\u2212r\u2212s   . (3.8)\nHere r and s depend on the context, DA = diag(\u03b1r+1, . . . , \u03b1r+s) and DB = diag( \u221a 1\u2212\u03b12r+1, . . . , \u221a 1\u2212\u03b12r+s),\nand 1 > \u03b1r+1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b1r+s > 0.\nTheorem 3.9 implies that\nUTAAX = [\u03a3A,0] and U T BBX = [\u03a3B ,0],\nwhere X , V(R\u22121W \u2295 Ip\u2212t). With the above remarks, Theorem 3.7 follows. Thus, we now present the proof of Theorem 3.9.\nProof. Since rank(K) = t, making a full SVD of K yields\nPTKV =\n[\nR 0\n0 0\n]\n,\nwhere P \u2208 R(m+n)\u00d7(m+n) and V \u2208 Rp\u00d7p are orthonormal matrices, R is a t \u00d7 t diagonal matrix with the diagonal elements as the nonzero singular values of K. Partition P as\nP = [P1 \ufe38\ufe37\ufe37\ufe38\nt\n, P2 \ufe38\ufe37\ufe37\ufe38\nm+n\u2212t\n] =\n[\nP11 P12 P21 P22\n]\nwhere P11 \u2208 Rm\u00d7t and P21 \u2208 Rn\u00d7t.\nObviously, PT1 P1 = P T 11P11 +P T 21P21 = It. Moreover, we have\nKV = [P1R,0]."}, {"heading": "3.4. Generalized Singular Value Decomposition 27", "text": "Applying Theorem 3.8 to P1 yields that there exist orthonormal matrices UA \u2208 Rm\u00d7m, UB \u2208 Rn\u00d7n, and W \u2208 Rt\u00d7t such that [\nUTA 0\n0 UTB\n] [\nP11 P21\n]\nW =\n[\n\u03a3A \u03a3B\n]\nwhere \u03a3A and \u03a3B are defined in (3.7) and (3.8). Hence, [\nUTA 0\n0 UTB\n] [\nA\nB\n]\nV =\n[\n\u03a3AW TR 0 \u03a3BW TR 0\n]\n.\nThat is, UTAAV = \u03a3A[W TR,0] and UTBBV = \u03a3B [W TR,0].\nIn terms of Theorem 3.7, if \u03b2i 6= 0, then the column xi of X satisfies\nATAxi = \u03bbiB TBxi,\nwhere \u03bbi = \u03b12 i\n\u03b22 i\n. This implies GSVD can be used to solve general-\nized eigenvalue problems. Based on this observation, Howland et al. [2003], Park and Park [2005] applied GSVD for solving Fisher linear discriminant analysis (FLDA) and generalized Fisher discriminant analysis [Baudat and Anouar, 2000, Mika et al., 2000].\nRecall that the above GSVD procedure requires to implementing an SVD on the (m+n) \u00d7 p matrix K. The computational cost is O((m+n)p \u2217min{m+n, p}). Thus, when both m+n and p are very large, the GSVD is less efficient. We now consider a special case in which B = ZA where Z \u2208 Rn\u00d7m is some given matrix. We will see that it is no longer necessary to perform the SVD on K.\nTheorem 3.10. Let A \u2208 Rm\u00d7p and B \u2208 Rn\u00d7p be two given matrices. Assume that B = ZA where Z \u2208 Rn\u00d7m is some matrix, rank(B) = s, and rank(A) = t. Let A = Ut\u03a3tV T t be a condensed SVD of A, and Y = UY \u03a3YV T Y be a full SVD of Y , ZUt. Then\n(UtVY ) TAVt\u03a3 \u22121 t VY = It and U T Y BVt\u03a3 \u22121 t VY = \u03a3Y .\nThe proof is direct. AssumeUt andVt are extended to orthonormal\nmatrices U (m\u00d7m) and V (p\u00d7 p). Let\nX = V(\u03a3\u22121t VY \u2295 Ip\u2212t)."}, {"heading": "28 The Singular Value Decomposition", "text": "We now have that\nAX = UUTAV(\u03a3\u22121t VY \u2295 Ip\u2212t) = [UtVY ,0] = U(VY \u2295 Im\u2212t)(It\u22950)\nand\nBX = ZUUTAV(\u03a3\u22121t VY \u2295 Ip\u2212t) = ZUt[VY ,0] = UY \u03a3YV T Y [VY ,0] = UY [\u03a3Y ,0].\nThus, (VTY \u2295 Im\u2212t)UTAX = [It \u2295 0] and\nUTY BX = [\u03a3Y ,0].\nIn this special case, we only need to implement two SVDs on two matrices with smaller sizes. The diagonal elements of \u03a3Y and the columns of Vt\u03a3 \u22121 t VY are the generalized eigenvalues and eigenvectors of the corresponding generalized eigenvalue problem.\nRemarks Assume that A \u2208 Rm\u00d7n and B \u2208 Rm\u00d7n have the same size. Gibson [1974] proved that they have joint factorizations A = U\u03a3AV T and B = U\u03a3BV T if and only if ABT and BTA are both normal. Here U and V are orthonormal matrices, and both\u03a3A and\u03a3B are diagonal but their diagonal elements are perhaps complex. These diagonal elements are nonnegative only if both ABT and BTA are SPSD.\n4 Applications of SVD: Case Studies\nIn the previous chapter we present the basic notion and some important properties of SVD. Meanwhile, we show that many matrix properties can be rederived via SVD. In this chapter, we further illustrate applications of SVD in matrices, including in the definition of the MoorePenrose pseudoinverse of an arbitrary matrix and in the analysis of the Procrustes problem.\nFor any matrix, the Moore-Penrose pseudoinverse exists and is unique. Moreover, it has been found to have many applications. Thus, it is an important matrix notion. In this chapter we exploit the matrix pseudoinverse to solve least squares estimation, giving rise to a more general result. We also show that the matrix pseudoinverse can be used to deal with a class of generalized eigenvalue problems.\nIn fact, SVD has also wide applications in machine learning and data analysis. For example, SVD is an important tool in spectral analysis [Azar et al., 2001], latent semantic indexing [Papadimitriou et al., 1998], spectral clustering, and projective clustering [Feldman et al., 2013]. We specifically show that SVD plays a fundamental role in subspace methods such as PCA, MDS, FDA and CCA.\n29"}, {"heading": "30 Applications of SVD: Case Studies", "text": ""}, {"heading": "4.1 The Matrix MP Pseudoinverse", "text": "Given a matrix A \u2208 Rm\u00d7n and a vector b \u2208 Rm, we are concerned with the least squares estimation problem:\nx\u0302 = argmin x\u2208Rn\n\u2016Ax\u2212 b\u201622. (4.1)\nThe minimizer should satisfy the Karush-Kuhn-Tucker (KKT) condition: that is, it is the solution of the following normal equation:\nATAx = ATb. (4.2)\nLet A = Ur\u03a3rV T r be the condensed SVD of A. Then Vr\u03a3 2 rV T r x = Vr\u03a3rU T r b. Define A \u2020 = Vr\u03a3 \u22121 r U T r \u2208 Rn\u00d7m. Obviously,\nx\u0302 = A\u2020b\nis a minimizer. It is clear that if A is invertible, then the minimizer is x\u0302 = A\u22121b. Thus, A\u2020 is a generalization of A\u22121 in the case that A is an arbitrary matrix, i.e., it is not necessarily invertible and even non-square. This leads us to the notion of the matrix Moore-Penrose (MP) pseudoinverse [Ben-Israel and Greville, 2003].\nDefinition 4.1. Given a matrix A \u2208 Rm\u00d7n, a real n \u00d7 m matrix B is called the MP pseudoinverse of A if it satisfies the following four conditions: (1) ABA = A, (2) BAB = B, (3) (AB)T = AB, and (4) (BA)T = BA.\nIt is easily verified that A\u2020 = Vr\u03a3 \u22121 r U T r is a pseudoinverse of A. Moreover, when A is invertible, A\u2020 is identical to A\u22121. The following theorem then shows that A\u2020 is the unique pseudoinverse of A.\nTheorem 4.1. LetA = Ur\u03a3rV T r be the condensed SVD of A \u2208 Rm\u00d7n. Then B is the pseudoinverse of A if and only if B = A\u2020 , Vr\u03a3 \u22121 r U T r .\nProof. To complete the proof, it suffices to prove the uniqueness of the pseudoinverse. Assume that B and C are two pseudoinverses of A. Then\nAB = (AB)T = BTAT = BT (ACA)T = BTATCTAT\n= (AB)T (AC)T = (ABA)C = AC."}, {"heading": "4.1. The Matrix MP Pseudoinverse 31", "text": "Similarly, it also holds that BA = CA. Thus,"}, {"heading": "B = BAB = BAC = CAC = C.", "text": "The matrix pseudoinverse also has wide applications. Let us see its application in solving generalized eigenproblems. Given two matrices M and N \u2208 Rm\u00d7m, we refer to (\u039b,X) where \u039b = diag(\u03bb1, . . . , \u03bbq) and X = [x1, . . . ,xq] as q eigenpairs of the matrix pencil (M,N) if MX = NX\u039b; namely,\nMxi = \u03bbiNxi, for i = 1, . . . , q.\nThe problem of finding eigenpairs of (M,N) is known as a generalized eigenproblem. Clearly, when N = Im, the problem becomes the conventional eigenvalue problem.\nUsually, we are interested in the problem with the nonzero \u03bbi for i = 1, . . . , q and refer to (\u039b,X) as the nonzero eigenpairs of (M,N). If N is nonsingular, (\u039b,X) is also referred to as the (nonzero) eigenpairs of N\u22121M because the generalized eigenproblem is equivalent to the eigenproblem:\nN\u22121MX = X\u039b.\nHowever, when N is singular, Zhang et al. [2010] suggested to use a pseudoinverse eigenproblem:\nN\u2020MX = X\u039b.\nMoreover, Zhang et al. [2010] established a connection between the solutions of the generalized eigenproblem and its corresponding pseudoinverse eigenproblem. That is,\nTheorem 4.2. Let M and N be two matrices in Rm\u00d7m. Assume range(M) \u2286 range(N). Then, if (\u039b,X) are the nonzero eigenpairs of N\u2020M, we have that (\u039b,X) are the nonzero eigenpairs of the matrix pencil (M,N). Conversely, if (\u039b,X) are the nonzero eigenpairs of the matrix pencil (M,N), then (\u039b,N\u2020NX) are the nonzero eigenpairs of N\u2020M."}, {"heading": "32 Applications of SVD: Case Studies", "text": "Proof. Let M = U1\u03931V T 1 and N = U2\u03932V T 2 be the condensed SVD of M and N. Thus, we have range(M) = range(U1) and range(N) = range(U2). Moreover, we have N \u2020 = V2\u0393 \u22121 2 U T 2 and NN\n\u2020 = U2UT2 . It follows from range(M) \u2286 range(N) that range(U1) \u2286 range(U2). This implies that U1 can be expressed as U1 = U2Q where Q is some matrix of appropriate order. As a result, we have\nNN\u2020M = U2U T 2 U2Q\u03931V T 1 = M.\nIt is worth noting that the condition NN\u2020M = M is not only necessary but also sufficient for range(M) \u2286 range(N).\nIf (\u039b,X) are the eigenpairs of N\u2020M, then it is easily seen that (\u039b,X) are also the eigenpairs of (M,N) due to NN\u2020M = M.\nConversely, suppose (\u039b,X) are the eigenpairs of (M,N). Then we have NN\u2020MX = NX\u039b. This implies that (\u039b,N\u2020NX) are the eigenpairs of N\u2020M due to NN\u2020M = M and N\u2020NN\u2020 = N\u2020.\nFisher discriminant analysis (FDA) is a classical method for classification and dimension reduction simultaneously [Mardia et al., 1979]. It is essentially a generalized eigenvalue problem in which the matrices N and M correspond to a pooled scatter matrix and a between-class scatter matrix [Ye and Xiong, 2006, Zhang et al., 2010]. Moreover, the condition range(M) \u2286 range(N) meets. Thus, Theorem 4.2 provides a solution when the pooled scatter matrix is singular or nearly singular. We will present more details about FDA in Section 4.3."}, {"heading": "4.2 The Procrustes Problem", "text": "Assume that X \u2208 Rn\u00d7p and Y \u2208 Rn\u00d7p are two configurations of n data points. The orthogonal Procrustes analysis aims to move Y relative into X through rotation [Gower and Dijksterhuis, 2004].\nIn particular, the Procrustes problem is defined as\nmin Q\u2208Rp\u00d7p\n\u2016X\u2212YQ\u20162F s.t. QTQ = Ip. (4.3)\nTheorem 4.3. Let the full SVD of YTX be YTX = U\u03a3VT . Then UVT is the minimizer of the Procrustes problem in (4.3)."}, {"heading": "4.3. Subspace Methods: PCA, MDS, FDA, and CCA 33", "text": "Proof. Since \u2016X \u2212 YQ\u20162F = tr((X \u2212 YQ)T (X \u2212YQ)) = tr(XTX) + tr(YTY)\u2212 2tr(YTXQT ), the original problem is equivalent to\nmax tr(YTXQT ) s.t. QTQ = Ip.\nRecall that the constants QTQ = Ip are equivalent to that q T i qi = 1 for i = 1, . . . , p, and qTi qj = 0 for i 6= j. Here the qi are the columns of Q. Thus, the Lagrangian function is\ntr(YTXQT )\u2212 1 2\np \u2211\ni=1\ncii(q T i qi \u2212 1)\u2212\n1\n2\n\u2211\ni>j\ncij(q T i qj \u2212 0),\nwhich is written in matrix form as\nL(Q,C) = tr(YTXQT )\u2212 1 2 tr[C(QTQ\u2212 Ip)],\nwhere C = [cij ] is a symmetric matrix of the Lagrangian multipliers.\nSince\ndL = tr(YTXdQT )\u2212 1 2 tr(C(dQTQ+QTdQ)),\nwe have dLdQ = Y TX \u2212 QC. Letting the first-order derivative be zero yields\nYTX\u2212QC = 0.\nLet Q\u0302 = UVT and C\u0302 = V\u03a3VT , which are obviously the solutions of the above equation systems.\nThe Hessian matrix of L w.r.t. Q at Q = Q\u0302 and C = C\u0302 is \u2212(V\u03a3VT ) \u2297 Ip, which is negative definite. Thus, Q = UVT is the minimizer of the Procrustes problem."}, {"heading": "4.3 Subspace Methods: PCA, MDS, FDA, and CCA", "text": "Subspace methods, such as principal component analysis (PCA), multidimensional scaling (MDS), Fisher discriminant analysis (FDA), and canonical correlation analysis (CCA), are a class of important machine learning methods. SVD plays a fundamental role in subspace learning methods."}, {"heading": "34 Applications of SVD: Case Studies", "text": "PCA [Jolliffe, 2002, Kittler and Young, 1973] and MDS [Cox and Cox, 2000] are two classical dimension reduction methods. Let A = [a1, . . . ,an] T be a given data matrix in which each vector ai represents a data instance in R p. Let m = 1n \u2211n i=1 ai = 1 nA T1n be the sample mean and Cn = In\u2212 1n1nITn be a so-called centered matrix. The pooled scatter matrix is defined as (a multiplier 1/n omitted)\nS = n\u2211\ni=1\n(ai \u2212m)(ai \u2212m)T = ATCnCnA = ATCnA.\nIt is well known that PCA computes the spectral decomposition of S, while the classical MDS or principal coordinate analysis (PCO) computes the spectral decomposition of the Gram matrix CnAA TCn. Proposition 3.1-(5)-(6) show that it is equivalent to computing SVD directly on the centerized data matrix CnA. Thus, SVD bridges PCA and PCO. That is, there is a duality relationship between PCA and PCO [Mardia et al., 1979]. This relationship has found usefulness in latent semantic analysis, face classification, and microarray data analysis [Deerwester et al., 1990, Turk and Pentland, 1991, Golub et al., 1999, Belhumeur et al., 1997, Muller et al., 2004].\nFDA is a joint approach for dimension reduction and classification. Assume that the ai are to be grouped into c disjoint classes and that each ai belongs to one and only one class. Let V = {1, 2, . . . , n} denote the index set of the data points ai and partition V into c disjoint subsets Vj; that is, Vi \u2229 Vj = \u2205 for i 6= j and \u222acj=1Vj = V , where the cardinality of Vj is nj so that \u2211c j=1 nj = n. We also make use of a matrix representation for the partitions. In particular, we let E = [eij ] be an n\u00d7c indicator matrix with eij = 1 if input ai is in class j and eij = 0 otherwise.\nLet mj = 1 nj\n\u2211\ni\u2208Vj ai be the jth class mean for j = 1, . . . , c. The between-class scatter matrix is defined as Sb = \u2211c j=1 nj(mj\u2212m)(mj\u2212 m)T . Conventional FDA solves the following generalized eigenproblem:\nSbxj = \u03bbjSxj, \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbq > \u03bbq+1 = 0,\nwhere q \u2264 min{p, c\u22121} and where we refer to xj as the jth discriminant direction. The above generalized eigenproblem can can be"}, {"heading": "4.3. Subspace Methods: PCA, MDS, FDA, and CCA 35", "text": "expressed in matrix form:\nSbX = SX\u039b, (4.4)\nwhere X = [x1, . . . ,xq] (n\u00d7q) and \u039b = diag(\u03bb1, . . . , \u03bbq) (q\u00d7q). Let \u03a0 = diag(n1, . . . , nc). Then Sb can be rewritten as\nSb = A TCnE\u03a0 \u22121ETCnA.\nRecall that S = ATCnCnA. Given these representations of S and Sb, the problem in (4.4) can be solved by using the GSVD method [Loan, 1976, Paige and Saunders, 1981, Golub and Van Loan, 2012, Howland et al., 2003]. Moreover, it is obvious that range(Sb) \u2286 range(ATCn) = range(S). Thus, Theorem 4.2 provides a solution when S is singular or nearly singular. Moreover, the method given in Theorem 3.10 is appropriate for solving the FDA problem.\nCCA is another subspace learning model [Hardoon et al., 2004]. The primary focus is on the relationship between two groups of variables (or features), whereas PCA considers interrelationships within a set of variable. Mathematically, CCA is defined as a generalized eigenvalue problem, so its solution can be borrowed from that of FDA."}, {"heading": "4.3.1 Nonlinear Extensions", "text": "Reproducing kernel theory [Aronszajn, 1950] provides an approach for nonlinear extensions of subspace methods. For example, kernel PCA [Sch\u00f6lkopf et al., 1998], kernel FDA [Baudat and Anouar, 2000, Mika et al., 2000, Roth and Steinhage, 2000], kernel CCA [Akaho, 2001, Van Gestel et al., 2001, Bach and Jordan, 2002] have been successively proposed and received wide applications in data analysis.\nKernel methods work in a feature space F , which is related to the original input space X \u2282 Rp by a mapping,\n\u03d5 : X \u2192 F .\nThat is, \u03d5 is a vector-valued function which gives a vector \u03d5(a), called a feature vector, corresponding to an input a \u2208 X . In kernel methods, we are given a reproducing kernel K : X \u00d7 X \u2192 R such that"}, {"heading": "36 Applications of SVD: Case Studies", "text": "K(a,b) = \u03d5(a)T\u03d5(b) for a,b \u2208 X . The mapping \u03d5(\u00b7) itself is typically not given explicitly. Rather, there exist only inner products between feature vectors in F . In order to implement a kernel method without referring to \u03d5(\u00b7) explicitly, one resorts to the so-called kernel trick [Sch\u00f6lkopf and Smola, 2002, Shawe-Taylor and Cristianini, 2004].\nLet L2(X ) be the square integrable Hilbert space of functions whose elements are functions defined on X . It is a well-known result that if K is a reproducing kernel for the Hilbert space L2(X ), then {K(\u00b7,b)} spans L2(X ). Here K(\u00b7,b) represents a function that is defined on X with values at a \u2208 X equal to K(a,b). There are some common kernel functions:\n(a) Linear kernel: K(a,b) = aTb,\n(b) Gaussian kernel or radial basis function (RBF): K(a,b) = exp ( \u2212\n\u2211p j=1 (aj\u2212bj)2 \u03b2j ) with \u03b2j > 0,\n(c) Laplacian kernel: K(a,b) = exp ( \u2212\u2211pj=1 |aj\u2212bj | \u03b2j ) with \u03b2j > 0,\n(d) Polynomial kernel: K(a,b) = (aTb+ 1)d of degree d.\nGiven a training set of input vectors {a1, . . . ,an}, the kernel matrix K = [K(ai,aj)] is an n\u00d7 n SPSD matrix.\n5 The QR and CUR Decompositions\nThe QR factorization and CUR decomposition are the two most important counterparts of SVD. These three factorizations apply to all matrices. In Table 1.1 we have compared their primary focuses. The SVD and QR factorization are two classical matrix theories. The CUR decomposition aims to represent a data matrix in terms of a small number part of the matrix, which makes it easy for us to understand and interpret the data in question. Here we present very brief introductions to the QR factorization and CUR decomposition."}, {"heading": "5.1 The QR Factorization", "text": "The QR factorization is another decomposition method applicable all matrices. Given a matrix A \u2208 Rm\u00d7n, the QR factorization is given by\nA = QR,\nwhere Q \u2208 Rm\u00d7m is orthonormal and R \u2208 Rm\u00d7n is upper triangular (or low triangular). Let D be an m \u00d7m diagonal matrix whose diagonal elements are either 1 or \u22121. Then A = (QD)(DR) is still a QR factorization of A. Thus, we always assume that R has nonnegative diagonal elements.\n37"}, {"heading": "38 The QR and CUR Decompositions", "text": "Assume m \u2265 n. The matrix A also has a thin QR factorization:\nA = QR,\nwhere Q \u2208 Rm\u00d7n is currently column orthonormal, and R \u2208 Rn\u00d7n is upper triangular with nonnegative diagonal elements. If A is of rank n, R is uniquely determined. In this case, Q = AR\u22121 is also uniquely determined.\nAsume A has rank r (\u2264 min{m,n}). Then there exists an m \u00d7m orthonormal matrix Q and an n\u00d7 n permutation matrix P such that\nQTAP =\n[\nR11 R12\n0 0\n]\n,\nwhere R11 is an r \u00d7 r upper triangular matrix with positive diagonal elements. This is called a rank revealing QR factorization.\nComputation of the QR factorization can be arranged by the novel Gram-Schmidt orthogonalization process or the modified GramSchmidt which is numerically more stable [Trefethen and Bau III, 1997]. Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992]. Stewart [1999] devised efficient computational algorithms of truncated pivoted QR approximations to a sparse matrix."}, {"heading": "5.2 The CUR Decomposition", "text": "As we have see, SVD leads us to a geometrical representation, and the QR factorization facilitates computations. They have little concrete meaning. This makes it difficult for us to understand and interpret the data in question.\nKuruvilla et al. [2002] have claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. Matrix column selection and CUR matrix decomposition provide such techniques."}, {"heading": "5.2. The CUR Decomposition 39", "text": "Column selection yields a so-called CX decomposition, and the CUR decomposition can be be regarded as a special CX decomposition. The CUR decomposition problem has been widely discussed in the literature [Goreinov et al., 1997a,b, Stewart, 1999, Tyrtyshnikov, 2000, Berry et al., 2005, Drineas and Mahoney, 2005, Bien et al., 2010], and it has been shown to be very useful in high dimensional data analysis.\nThe CUR was originally called a skeleton decomposition [Goreinov et al., 1997a]. Let A \u2208 Rm\u00d7n be a given matrix of rank r. Then there exists a nonsingular r\u00d7r submatrix in A. Without loss of generality, assume this nonsingular matrix is the first r \u00d7 r principal submatrix of A. That is, A can be partioned into the following form:\nA =\n[\nA11 A12 A21 A22\n]\n,\nwhere A11 is a r \u00d7 r nonsingular matrix. Consider that [A21,A22] = B[A11,A12] for some B \u2208 R(m\u2212r)\u00d7r. It follows from A21 = BA11 that B = A21A \u22121 11 . Hence, A22 = A21A \u22121 11 A12. So it is obtained that\nA =\n[\nA11 A21\n]\nA\u2212111 [A11,A12].\nIn general case, let AI,J be the nonsingular submatrix where I = {i1, . . . , ir} \u2282 [m] and J = {j1, . . . , jr} \u2282 [n]. Then it also hods that A = CA\u22121I,JR,\nwhere C = A:,J and R = AI,: are respectively a subset of columns and a subset of rows, of A.\nIn practical applications, however, it is intractable to select AI,J . Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005]. The SCA approximation is of the form A \u2248 XTY, where X and Y consist of columns and rows of A, and T minimizes \u2016A\u2212XTY\u20162F . This algorithm is a deterministic peocedure but computationally expensive.\nThe terminology of the CUR decomposition has been proposed by Drineas and Mahoney [2005], Mahoney et al. [2008]. They reformulated the idea based on random selection. A CUR decomposition"}, {"heading": "40 The QR and CUR Decompositions", "text": "algorithm seeks to find a subset of c columns of A to form a matrix C \u2208 Rm\u00d7c, a subset of r rows to form a matrix R \u2208 Rr\u00d7n, and an intersection matrix U \u2208 Rc\u00d7r such that \u2016A\u2212CUR\u2016\u03be is small. Accordingly, A\u0303 = CUR is used to approximate A.\nSince there are (nc ) possible choices of constructing C and ( m r ) possible choices of constructing R, obtaining the best CUR decomposition is a hard problem. In Chapter 10 we will further study the CUR decomposition problem via random approximation.\nThe CUR decomposition is also an extension of the novel Nystr\u00f6m approximation to a general matrix. The Nystr\u00f6m method approximates an SPSD matrix only using a subset of its columns, so it can alleviate computation and storage costs when the SPSD matrix in question is large in size. Thus, the Nystr\u00f6m method and its variants [Halko et al., 2011, Gittens and Mahoney, 2013, Kumar et al., 2009, Wang and Zhang, 2013, 2014, Wang et al., 2014b, Si et al., 2014] have been extensively used in the machine learning community. For example, they have been applied to Gaussian processes [Williams and Seeger, 2001], kernel classification [Zhang et al., 2008, Jin et al., 2013], spectral clustering [Fowlkes et al., 2004], kernel PCA and manifold learning [Talwalkar et al., 2008, Zhang et al., 2008, Zhang and Kwok, 2010], determinantal processes [Affandi et al., 2013], etc.\n6 Variational Principles\nVariational principles correspond to matrix perturbation theory [Stewart and Sun, 1990], which is the theoretical foundation to characterize stability or sensitivity of a matrix computation algorithm. Thus, variational principles are important in analysis for error bounds of matrix approximate algorithms (see Chapters 9 and 10).\nIn this chapter we specifically study variational properties for eigenvalues of a symmetric matrix as well as for singular values of a general matrix. We will see that these results for eigenvalues and for singular values are almost parallel. The cornerstones are the novel von Neumann theorem [Neumann, 1937] and Ky Fan theorem [Fan, 1951]. We present new proofs for them by using theory of matrix differentials. Additionally, we present some majorization inequalities. They will be used in the latter chapters, especially in investigating unitarily invariant norms (see Chapter 7).\nGiven a matrix A \u2208 Rm\u00d7n, we always let \u03c31(A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3p(A) be the singular values of A where p = min{m,n}. When A is symmetric, let \u03bb1(A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbn(A) be the eigenvalues of A. These eigenvalues or singular values are always arranged in deceasing order. Note that the eigenvalues are real but could be negative. Let\n41"}, {"heading": "42 Variational Principles", "text": "\u03bb(M) = (\u03bb1(M), . . . , \u03bbn(M)) T denote the eigenvalues of an n\u00d7 n real square matrix M, and \u03c3(A) = (\u03c31(A), . . . , \u03c3p(A)) T denote the singular values of an m \u00d7 n real matrix A. Sometimes we also write them the \u03c3i or the \u03bbi when they are explicit in the context for notational simplicity."}, {"heading": "6.1 Variational Properties for Eigenvalues", "text": "In this section we consider variational properties for eigenvalues of a real symmetric matrix. It is well known that for an arbitrary symmetric matrix, its eigenvalues are all real. The following cornerstone theorem was originally established by von Neumann [1937].\nTheorem 6.1 (von Neumann Theorem). Assume M \u2208 Rn\u00d7n and N \u2208 R n\u00d7n are symmetric. Then\nn\u2211\ni=1\n\u03bbi(M)\u03bbi(N) = max QQT=In\ntr(QMQTN).\nMoreover,\nn\u2211\ni=i\n\u03bbi(M)\u03bbn\u2212i+1(N) = min QQT=In\ntr(QMQTN).\nProof. The second part directly follows from the first part because\nmin QQT=In tr(QMQTN) = \u2212 max QQT=In tr(QMQT (\u2212N)).\nWe now present the proof of the first part. Make full EVDs of M and N as M = UM\u039bMU T M and N = UN\u039bNU T N , where \u039bM = diag(\u03bb1(M), . . . , \u03bbn(M)) and \u039bN = diag(\u03bb1(N), . . . , \u03bbn(N)), and UM and UN are orthonormal. It is easily seen that\nmax QQT=In tr(QMQTN) = max QQT=In tr((UTNQUM )\u039bM (U T NQUM ) T\u039bN )\n= max QQT=In\ntr(Q\u039bMQ T\u039bN )."}, {"heading": "6.1. Variational Properties for Eigenvalues 43", "text": "Let Q = [qij] = [q1, . . . ,qn] T . We now have\ntr(Q\u039bMQ T\u039bN )\n= n\u2211\ni=1\nqTi \u039bMqi\u03bbi(N)\n= n\u22121\u2211\ni=1\ni\u2211\nj=1\nqTj \u039bMqj[\u03bbi(N)\u2212 \u03bbi+1(N)] + \u03bbn(N) n\u2211\nj=1\nqTj \u039bMqj\n= n\u22121\u2211\ni=1\n[\u03bbi(N)\u2212 \u03bbi+1(N)] i\u2211\nj=1\nn\u2211\nk=1\nq2jk\u03bbk(M) + \u03bbn(N) n\u2211\nj=1\n\u03bbj(M).\nDefine W , [q2ij ] which is doubly stochastic, and u = [u1, . . . , un] T where uj = \u2211n k=1 q 2 jk\u03bbk(M). That is, u = W\u03bb(M). By Lemma 2.2, we know that u \u227a \u03bb(M). Accordingly,\ntr(Q\u039bMQ T\u039bN ) \u2264\nn\u22121\u2211\ni=1\n[\u03bbi(N)\u2212\u03bbi+1(N)] i\u2211\nj=1\n\u03bbj(M) + \u03bbn(N) n\u2211\nj=1\n\u03bbj(M)\n= n\u2211\ni=1\n\u03bbi(M)\u03bbi(N).\nWhen Q = In, the equality holds. That is, U T NQUM = In in the original problem. The theorem follows.\nThe following theorem is a corollary of Theorem 6.1 when taking\nN =\n[\nIk 0 0 0\n]\n.\nTheorem 6.2 (von Neumann Theorem). Assume M \u2208 Rn\u00d7n is symmetric. Then for k \u2208 [n],\nk\u2211\ni=1\n\u03bbi = max QT Q=Ik\ntr(QTMQ),\nwhich is arrived when Q is the n\u00d7k matrix of the orthonormal vectors associated with \u03bb1, . . . , \u03bbk. Moreover,\nn\u2211\ni=n\u2212k+1 \u03bbi = min QT Q=Ik tr(QTMQ)."}, {"heading": "44 Variational Principles", "text": "In the appendix we give an other proof based on theory of matrix differentials. The von Neumann theorem describes the variational principle of eigenvalues of a symmetric matrix. Using Theorems 6.2, we have the following variational properties.\nProposition 6.1. Given two n\u00d7 n real symmetric matrices M and N, we have that\n(1) \u03bb(M+N) \u227a \u03bb(M) + \u03bb(N) and \u03bb(M)\u2212 \u03bb(N) \u227a \u03bb(M\u2212N). (2)\n\u2211k i=1 \u03bbi(M+N) \u2265 \u2211k i=1 \u03bbi(M) + \u2211n j=n\u2212k+1 \u03bbj(N) for k \u2208 [n].\n(3) (m11, . . . ,mnn) \u227a (\u03bb1(M), . . . , \u03bbn(M)).\nProof. The proof is based on Theorem 6.2. First, for k \u2208 [n \u2212 1], k\u2211\ni=1\n\u03bbi(M+N) = max QT Q=Ik\n{ tr(QTMQ) + tr(QTNQ) }\n\u2264 max QT Q=Ik tr(QTMQ) + max QT Q=Ik tr(QTNQ) = k\u2211\ni=1\n\u03bbi(M) + k\u2211\ni=1\n\u03bbi(N).\nNote that tr(M+N) = tr(M) + tr(N), so \u03bb(M+N) \u227a \u03bb(M) + \u03bb(N). Hence, \u03bb(M) \u2212 \u03bb(N) \u227a \u03bb(M\u2212N). Second,\nk\u2211\ni=1\n\u03bbi(M+N) = max QT Q=Ik\n{ tr(QTMQ) + tr(QTNQ) }\n\u2265 max QT Q=Ik\n{\ntr(QTMQ) + min QT Q=Ik\ntr(QTNQ) }\n= k\u2211\ni=1\n\u03bbi(M) + n\u2211\nj=n\u2212k+1 \u03bbj(N).\nTo prove the third part, we assume that m11 \u2265 \u00b7 \u00b7 \u00b7 \u2265 mnn without loss of generality. Now the result is obtained via\nk\u2211\ni=1\n\u03bbi(M) = max QT Q=Ik\ntr(QTMQ) \u2265 tr(HTkMHk) = k\u2211\ni=1\nmii,\nwhere Hk consists of the first k columns of In for all k \u2208 [n]."}, {"heading": "6.1. Variational Properties for Eigenvalues 45", "text": "Proposition 6.1-(3) is sometimes referred to as Schur\u2019s theorem. The second part of the following proposition is an extension of Schur\u2019s theorem.\nProposition 6.2. Let M =\n[\nM11 M12 M21 M22\n]\nbe n\u00d7n real symmetric. Here\nM11 is k \u00d7 k. Then\n(1) \u03bbi(M) \u2265 \u03bbi(M11) \u2265 \u03bbn\u2212k+i(M) for i = 1, . . . , k;\nand (2) (\u03bb(M11),\u03bb(M22)) \u227a \u03bb(M). Furthermore, for any column-orthonormal matrix Q \u2208 Rn\u00d7k, we have\n(3) \u03bbi(M) \u2265 \u03bbi(QTMQ) \u2265 \u03bbn\u2212k+i(M) for i = 1, . . . , k.\nProof. The first result directly follows from the well known interlacing theorem [Horn and Johnson, 1985]. As for the third part, we can extend Q to an orthonormal matrix Q\u0303 = [Q,Q\u22a5]. Consider that\nQ\u0303TMQ\u0303 =\n[\nQTMQ QTMQ\u22a5\n(Q\u22a5)TMQ (Q\u22a5)TMQ\u22a5\n]\n.\nThus,\n\u03bbi(M) = \u03bbi(Q\u0303 TMQ\u0303) \u2265 \u03bbi(QTMQ) \u2265 \u03bbn\u2212k+i(Q\u0303TMQ\u0303) = \u03bbn\u2212k+i(M).\nWe now consider the proof of the second part. Let the EVDs of\nM11 and M22 be M11 = U1\u039b1U T 1 and M22 = U2\u039b2U T 2 . Then\n[\nUT1 0\n0 UT2\n] [\nM11 M12 M21 M22\n] [\nU1 0\n0 U2\n]\n=\n[\n\u039b1 U T 1 M12U2\nUT2 M21U1 \u039b2\n]\n.\nSince U1 and U2 are orthonormal, we have that \u03bb(M11) = \u03bb(\u039b1), \u03bb(M22) = \u03bb(\u039b2), and\n\u03bb\n([\nUT1 0\n0 UT2\n] [\nM11 M12 M21 M22\n] [\nU1 0\n0 U2\n])\n= \u03bb(M).\nApplying Proposition 6.1-(3) completes the proof."}, {"heading": "46 Variational Principles", "text": ""}, {"heading": "6.2 Variational Properties for Singular Values", "text": "Theorems 6.1 and 6.2 can be extended to a general matrix. In this case, we investigate singular values of the matrix instead. Theorems 6.3 and 6.4 correspond to Theorems 6.1 and 6.2, respectively.\nTheorem 6.3 (Ky Fan Theorem). Given two matrices A \u2208 Rm\u00d7n and B \u2208 Rm\u00d7n, let A and B have full SVDs A = UA\u03a3AVTA and B = UB\u03a3BV T B , respectively. Let p = min{m,n}. Then\np \u2211\ni=1\n\u03c3i(A)\u03c3i(B) = max XT X=Im,YT Y=In\n|tr(XTAYBT )|\n= max XT X=Im,YT Y=In\ntr(XTAYBT ),\nwhich is achieved at X = UAU T B and Y = VAV T B.\nProof. Note that\ntr(XTAYBT ) = 1\n2 tr\n([\nYT 0\n0 XT\n] [\n0 AT\nA 0\n] [\nY 0\n0 X\n] [\n0 BT\nB 0\n])\n.\nThe theorem is directly obtained from Theorems 6.1 and 3.5.\nTheorem 6.4 (Ky Fan Theorem). Given an m \u00d7 n real matrix A, let p = min{m,n}, and let the singular values of A be \u03c31, . . . , \u03c3p which are arranged in descending order, with the corresponding left and right singular vectors ui and vi. Then for any k \u2208 [p], k\u2211\ni=1\n\u03c3i = max XT X=Ik ,YT Y=Ik |tr(XTAY)| = max XT X=Ik ,YT Y=Ik tr(XTAY),\nwhich is achieved at X = [u1, . . . ,uk] and Y = [v1, . . . ,vk].\nThe theorem can be obtained from Theorems 6.2 and 3.5 or from\nTheorem 6.3. In the appendix we give the third proof.\nProposition 6.3. Given two matrices A \u2208 Rm\u00d7n and B \u2208 Rm\u00d7n, let p = min{m,n}. Let A\u0302 be obtained by replacing the last r rows and/or columns of A by zeros. Then"}, {"heading": "6.2. Variational Properties for Singular Values 47", "text": "(1) \u03c3(A+B) \u227aw \u03c3(A) + \u03c3(B). (2) \u03c3i+j\u22121(A+B) \u2264 \u03c3i(A) + \u03c3j(B) for i, j \u2265 1 and i+ j \u2212 1 \u2264 p. (3) a \u227aw \u03c3(A) where a = (a11, . . . , app)T . (4) For i \u2208 [p\u2212 r], \u03c3r+i(A) \u2264 \u03c3i(A\u0302) \u2264 \u03c3i(A). (5) Let P \u2208 Rm\u00d7r and Q \u2208 Rn\u00d7r be column orthonormal matrices where\nr \u2264 p. Then \u03c3r+i(A) \u2264 \u03c3i(PTA) \u2264 \u03c3i(A) and \u03c3r+i(A) \u2264 \u03c3i(AQ) \u2264 \u03c3i(A) for i = 1, . . . , p\u2212 r.\nProof. The proof of Proposition 6.3-(1) and (3) is parallel to that of Proposition 6.1-(1) and (3). Part-(2) is Weyl\u2019s monotonicity theorem. It can be proven by the Courant-Fischer theorem (see Theorem 3.4). Consider that \u03c3i(A) = \u221a \u03bbi(ATA) = \u221a \u03bbi(AAT ) and \u03c3i(A\u0302) = \u221a \u03bbi(A\u0302T A\u0302) = \u221a \u03bbi(A\u0302A\u0302T ). Part (4) follows from Proposition 6.2-(1). Part (5) follows then from Proposition 6.2-(3).\nTheorem 6.5. Given two matrices A \u2208 Rm\u00d7n and B \u2208 Rm\u00d7n, let si(A\u2212B) = |\u03c3i(A)\u2212 \u03c3i(B)| for i \u2208 [p] where p = min{m,n}. Then\nk\u2211\ni=1\ns\u2193i (A\u2212B) \u2264 k\u2211\ni=1\n\u03c3i(A\u2212B) for k = 1, . . . , p.\nProof. Consider the following two (m+n)\u00d7(m+n) symmetric matrices:\nA\u0303 =\n[\n0 A AT 0\n]\nand B\u0303 =\n[\n0 B BT 0\n]\n.\nBy Theorem 3.5, the eigenvalues of A\u0303 are \u00b1\u03c31(A), . . . ,\u00b1\u03c3p(A), together with m+n\u2212 2p zeros; and similarly for B\u0303 as well as for A\u0303\u2212 B\u0303. Thus, the p largest entries of \u03bb(A\u0303\u2212 B\u0303) are \u03c31(A\u2212B), . . . , \u03c3p(A\u2212B). Note that both \u03c3i(A) \u2212 \u03c3i(B) and \u03c3i(B) \u2212 \u03c3i(A) are the entries of \u03bb(A\u0303)\u2212\u03bb(B\u0303), so the p largest entries of \u03bb(A\u0303)\u2212\u03bb(B\u0303) comprise the set {s1(A \u2212B), . . . , sp(A \u2212B)}. Proposition 6.1 shows that \u03bb(A\u0303 \u2212 B\u0303) \u227a \u03bb(A\u0303)\u2212 \u03bb(B\u0303). This implies the result of the theorem.\nTheorem 6.6. Let A \u2208 Rm\u00d7n and B \u2208 Rn\u00d7p be given, and let q = min{m,n, p}. Then for k = 1, . . . , q,\nk\u220f\ni=1\n\u03c3i(AB) \u2264 k\u220f\ni=1\n\u03c3i(A)\u03c3i(B)."}, {"heading": "48 Variational Principles", "text": "If n = p = m, then equality holds for k = n. And\nk\u2211\ni=1\n\u03c3i(AB) \u2264 k\u2211\ni=1\n\u03c3i(A)\u03c3i(B) \u2264 ( k\u2211\ni=1\n\u03c3i(A) )( k\u2211\ni=1\n\u03c3i(B) ) .\nProof. Let AB = U\u03a3VT be a full SVD of AB, and for k \u2264 q let Uk and Vk be the first k columns of U and V, respectively. Now take a polar decomposition of BVk as BVk = QS. Since S 2 = VTkB TBVk and by Proposition 6.3-(4), we obtain\ndet(S2) = det(VTkB TBVk) \u2264\nk\u220f\ni=1\n\u03c32i (B)\nWe further have that\nk\u220f\ni=1\n\u03c3i(AB) = |det(UTkABVk)| = |det(UTkAQ) det(S)|\n\u2264 k\u220f\ni=1\n\u03c3i(A)\u03c3i(B).\nThe above inequality again follows from Proposition 6.3-(4), When n = p = m, then\nn\u220f\ni=1\n\u03c3i(AB) = |det(AB)| = |det(A)| \u00d7 |det(B)| = n\u220f\ni=1\n\u03c3i(A)\u03c3i(B).\nThe second part follows from the first part and Lemma 2.4."}, {"heading": "6.3 Appendix: Application of Matrix Differentials", "text": "Here we present alternative proofs for Theorem 6.2 and Theorem 6.4, which are based on matrix differentials. It aims at further illustrating how to use matrix differentials.\nThe Second Proof of Theorem 6.2. To solve the problem, we define the Lagrangian function:\nL(Q,C) = tr(QTMQ)\u2212 tr(C(QTQ\u2212 Ik)),"}, {"heading": "6.3. Appendix: Application of Matrix Differentials 49", "text": "where C is a k \u00d7 k symmetric matrix of Lagrangian multipliers. Since\ndL = tr(dQTMQ+QTMdQ)\u2212 tr(C(dQTQ+QTdQ)),\nthis shows that dLdQ = 2MQ\u2212 2QC. The KKT condition is now\nMQ\u2212QC = 0.\nClearly, if C\u0302 , diag(\u03bb1, . . . , \u03bbk) and Q\u0302 consists of the corresponding orthonormal eigenvectors, they are a solution of the above equation. In this setting, we see that tr(Q\u0302TMQ\u0302) = \u2211k\ni=1 \u03bbi.\nThus, we only need to prove that Q\u0302 is indeed the maximizer of the original problem. We now compute the Hessian matrix of L w.r.t. Q at Q = Q\u0302 and C = C\u0302. Since vec(MQ\u2212QC) = (Ik\u2297M\u2212C\u2297 In)vec(Q), the Hessian matrix is given as\nH = 2(Ik \u2297M\u2212 C\u0302\u2297 In).\nFor any X \u2208 Rn\u00d7k such that XT Q\u0302 = 0, it suffices for our purpose to prove xTHx/2 \u2264 0 where x = vec(X). Take the full EVD of M as M = U\u039bUT , where \u039b = diag(\u03bb1, . . . , \u03bbn) and U = [Q\u0302, Q\u0302\n\u22a5] such that UTU = In. Denote \u039b2 = diag(\u03bbk+1, . . . , \u03bbn) and Y = (Q\u0302\n\u22a5)TX = [y1, . . . ,yk]. Then,\n1 2 xTHx = tr(XTMX)\u2212 tr(XC\u0302XT )\n= tr(XT Q\u0302\u22a5\u039b2(Q\u0302 \u22a5)TX)\u2212 tr(C\u0302XT (Q\u0302Q\u0302T + Q\u0302\u22a5(Q\u0302\u22a5)T )X) = tr(YT\u039b2Y)\u2212 tr(C\u0302YTY) = k\u2211\ni=1\nyTi \u039b2yi \u2212 k\u2211\ni=1\n\u03bbiy T i yi\n= k\u2211\ni=1\nyTi (\u039b2 \u2212 \u03bbiIn\u2212k)yi \u2264 0.\nThe Third Proof of Theorem 6.4. To solve the constrained problem in the theorem, we now define the Lagrangian function:\nL(X,Y,C1,C2) = tr(X TAY)\u22121\n2 tr(C1(X\nTX\u2212Ik))\u2212 1\n2 tr(C2(Y\nTY\u2212Ik)),"}, {"heading": "50 Variational Principles", "text": "where C1 and C2 are two k \u00d7 k symmetric matrix of Lagrange multipliers. Since\ndL = tr(dXTAY)\u2212 1 2 tr(C1(dX TX+XTdX)), dL = tr(XTAdY)\u2212 1 2 tr(C2(dY TY+YTdY)),\nwhich yield that dLdX = AY\u2212XC1 and dLdY = XAT \u2212YC2. The KKT condition is now\nAY\u2212XC1 = 0 and ATX\u2212YC2 = 0.\nIt then follows from XTX = Ik and Y TY = Ik that C1 = C2. We denote C , C1 = C2. So,\nAY\u2212XC = 0, ATX\u2212YC = 0.\nThat is, [\n0 A AT 0\n] [\nX\nY\n]\n=\n[\nX\nY\n]\nC.\nClearly, if C\u0302 , \u03a3k = diag(\u03bb1, . . . , \u03bbk), X\u0302 , Uk = [u1, . . . ,uk], and Y\u0302 , Vk = [v1, . . . ,vk], then they are a solution of the above equation. In this setting, we see that tr(X\u0302TAY\u0302) = \u2211k\ni=1 \u03c3i.\nThus, we only need to prove that (X\u0302, Y\u0302) is the maximizer of the original problem. We now compute the Hessian matrix of L w.r.t. (X,Y) at (X,Y) = (X\u0302, Y\u0302), and C = C\u0302. The Hessian matrix is given as\nH ,\n\n\n\u22022L \u2202vec(X\u0302)\u2202vec(X\u0302)T \u22022L \u2202vec(X\u0302)\u2202vec(Y\u0302)T \u22022L \u2202vec(Y\u0302)\u2202vec(X\u0302)T \u22022L \u2202vec(Y\u0302)\u2202vec(Y\u0302)T\n\n =\n[\n\u2212\u03a3k \u2297 Im Ik \u2297A Ik \u2297AT \u2212\u03a3k \u2297 In\n]\n,\nbecause vec(AY \u2212 XC) = (Ik \u2297 A)vec(Y) \u2212 (CT \u2297 Im)vec(X) and vec(ATX\u2212YC) = (Ik \u2297AT )vec(X)\u2212 (CT \u2297 In)vec(Y).\nNote that [ X\u0302T 0\n0 Y\u0302T\n] [\nX\u0302 0\n0 Y\u0302\n]\n= I2k."}, {"heading": "6.3. Appendix: Application of Matrix Differentials 51", "text": "Thus, for any Z1 \u2208 Rm\u00d7k and Z2 \u2208 Rn\u00d7k such that ZT1 X\u0302 = 0 and ZT2 Y\u0302 = 0, it suffices for our purpose to prove z\nTHz \u2264 0 where zT = (vec(Z1) T , vec(Z2) T ). Compute\nzTHz = [vec(Z1) T , vec(Z2) T ]\n[\n\u2212\u03a3k \u2297 Im Ik \u2297A Ik \u2297AT \u2212\u03a3k \u2297 In\n] [\nvec(Z1) vec(Z2)\n]\n= vec(Z2) T (Ik\u2297AT )vec(Z1) + vec(Z1)T (Ik\u2297A)vec(Z2)\n\u2212 vec(Z1)T (\u03a3k\u2297Im)vec(Z1)\u2212 vec(Z2)T (\u03a3k\u2297In)vec(Z2) = \u2212tr(ZT1 Z1\u03a3k)\u2212 tr(ZT2 Z2\u03a3k) + 2tr(ZT1 AZ2) , \u2206.\nTake a thin SVD of A as A = U\u03a3VT , where \u03a3 = \u03a3k \u2295 \u03a3\u2212k, U = [Uk,U\u2212k], and V = [Vk,V\u2212k]. Denote R1 = UT\u2212kZ1 and and R2 = V T \u2212kZ2. Then tr(Z T 1 AZ2) = tr(Z T 1 U\u2212k\u03a3\u2212kV T \u2212k). And hence,\n\u2212\u2206 = tr(ZT1 \u03a3kZ1) + tr(ZT2 \u03a3kZ2)\u2212 2tr(ZT1 U\u2212k\u03a3\u2212kVT\u2212kZ2) \u2265 tr(ZT1 U\u2212kUT\u2212kZ1\u03a3k) + tr(ZT2 U\u2212kUT\u2212kZ2\u03a3k) \u2212 2tr(ZT1 U\u2212k\u03a3\u2212kVT\u2212kZ2)\n= tr(RT1 R1\u03a3k) + tr(R T 2 R2\u03a3k)\u2212 2tr(RT1 \u03a3\u2212kR2) \u2265 tr(RT1 \u03a3\u2212kR1) + tr(RT2 \u03a3\u2212kR2)\u2212 2tr(RT1 \u03a3\u2212kR2) = tr[(R1 \u2212R2)T\u03a3\u2212k(R1 \u2212R2)] \u2265 0.\nThe last inequality uses the fact that tr(RT1 R1\u03a3k) \u2265 tr(RT1 \u03a3\u2212kR1) and tr(RT2 R2\u03a3k) \u2265 tr(RT2 \u03a3\u2212kR2).\n7 Unitarily Invariant Norms\nIn this chapter we study unitarily invariant norms of a matrix, which can be defined via singular values of the matrix. Unitarily invariant norms were contributed by J. von Neumann, Robert Schatten, and Ky Fan. J. von Neumann established an equivalent relationship between unitarily invariant norms and symmetric gauge functions. There are two popular classes of unitarily invariant norms: the Ky Fan norms and Schatten p-norms.\nParallel with the vector p-norms, the Schatten p-norms are defined on singular values of a matrix. Their special cases include the spectral norm, Frobenius norm, and nuclear norm. They have wide applications in modern data analysis and computation. For example, the Frobenius norm is used to measure approximation errors in regression and reconstruction problems because it essentially equivalent to the \u21132-norm of a vector. The spectral norm is typically used to describe convergence and convergence rate of an iteration procedure. The nuclear norm provides an effective approach to matrix low rank modeling.\nWe first briefly review matrix norms, and then present the notion of symmetric gauge functions. Symmetric gauge functions facilitate us to study unitarily invariant norms. First, it transforms a unitarily invari-\n52"}, {"heading": "7.1. Matrix Norms 53", "text": "ant norm on matrices to a norm on vectors equivalently. Second, it can incorporate majorization theory. Accordingly, we give some important properties of unitarily invariant norms."}, {"heading": "7.1 Matrix Norms", "text": "A function f : Rm\u00d7n \u2192 R is said to be a matrix norm if the following conditions are satisfied:\n(1) f(A) > 0 for all nonzero matrix A \u2208 Rm\u00d7n;\n(2) f(\u03b1A) = |\u03b1|f(A) for any \u03b1 \u2208 R and any A \u2208 Rm\u00d7n;\n(3) f(A+B) \u2264 f(A) + f(B) for any A and B \u2208 Rm\u00d7n.\nWe denote the norm of a matrix A by \u2016A\u2016. Furthermore, if\n(4) \u2016AB\u2016 \u2264 \u2016A\u2016\u2016B\u2016 where A \u2208 Rm\u00d7n and B \u2208 Rn\u00d7p,\nthe matrix norm is said to be consistent. In some literature, when one refers to a matrix norm on Rn\u00d7n. it is required to be consistent. Here we do not make this requirement.\nThere is an equivalence between any two norms. Let \u2016\u00b7\u2016\u03b1 and \u2016\u00b7\u2016\u03b2 be two norms on Rm\u00d7n. Then there exist positive numbers \u03b11 and \u03b12 such that for all A \u2208 Rm\u00d7n,\n\u03b11\u2016A\u2016\u03b1 \u2264 \u2016A\u2016\u03b2 \u2264 \u03b12\u2016A\u2016\u03b1.\nConditions (2) and (3) tell us that the norm is convex. Moreover, it is continuous because\n|\u2016A\u2016 \u2212 \u2016B\u2016| \u2264 \u2016A\u2212B\u2016 \u2264 \u03b1\u2016A\u2212B\u2016F , where \u03b1 > 0.\nA norm always companies with its dual. The dual is a norm. Moreover, the dual of the dual norm is the original norm.\nDefinition 7.1. Let \u2016 \u00b7 \u2016 be a given norm on Rm\u00d7n. Its dual (denoted \u2016 \u00b7 \u2016\u2217) is defined as\n\u2016A\u2016\u2217 = max { tr(ABT ) : B \u2208 Rm\u00d7n, \u2016B\u2016 = 1 } ."}, {"heading": "54 Unitarily Invariant Norms", "text": "Proposition 7.1. The dual \u2016 \u00b7 \u2016\u2217 has the following properties:\n(1) The dual is a norm.\n(2) (\u2016A\u2016\u2217)\u2217 = \u2016A\u2016. (3) tr(ABT ) \u2264 |tr(ATB)| \u2264 \u2016A\u2016\u2016B\u2016\u2217 (or \u2016A\u2016\u2217\u2016B\u2016).\nThere are two approaches for definition of a matrix norm. In the first approach, the norm of matrix A is defined via its vectorization vec(A); that is, \u2016A\u2016 = \u2016vec(A)\u2016, which obviously satisfies Conditions (1)-(3). We refer to this class of the matrix norms as matrix vectorization norms for ease of exposition. Note that the Frobenius norm is a matrix vectorization norm because \u2016A\u2016F = \u2016vec(A)\u20162. However, this class of matrix norms are not always consistent. For example, let\nA = B =\n[\n1 1 1 1\n]\n.\nSince AB =\n[\n2 2 2 2\n]\nand\n2 = \u2016vec(AB)\u2016\u221e > \u2016vec(A)\u2016\u221e\u2016vec(B)\u2016\u221e = 1,\nthis implies that the corresponding matrix norm is not consistent.\nIn the second approach, the matrix norm is defined by\n\u2016A\u2016 = max \u2016x\u2016=1 \u2016Ax\u2016,\nwhich is also called the induced or operator norm.\nTheorem 7.1. The operator norm on Rm\u00d7n is a consistent matrix norm.\nProof. Given a matrixA \u2208 Rm\u00d7n, the result is trivial IfA = 0. Assume that A 6= 0. Then there exists a nonzero vector z \u2208 Rn for which Az 6= 0. So we have \u2016Az\u2016 > 0 and \u2016z\u2016 > 0. Hence,\n\u2016A\u2016 = max x 6=0 \u2016Ax\u2016 \u2016x\u2016 \u2265 \u2016Az\u2016 \u2016z\u2016 > 0."}, {"heading": "7.1. Matrix Norms 55", "text": "Conditions (2)-(3) are directly obtained from the definition of the vector norm. As for Condition (4), it can be established by\n\u2016ABx\u2016 \u2264 \u2016A\u2016\u2016Bx\u2016 \u2264 \u2016A\u2016\u2016B\u2016\u2016x\u2016 for any x 6= 0. Thus,\n\u2016AB\u2016 = max x 6=0 \u2016ABx\u2016 \u2016x\u2016 \u2264 \u2016A\u2016\u2016B\u2016.\nAs we have shown, \u2016A\u20162 = max\u2016x\u20162=1 \u2016Ax\u20162 = \u03c31(A). It is thus called the spectral norm.\nNote that \u2016UAV\u20162 = \u2016A\u20162 and \u2016UAV\u2016F = \u2016A\u2016F for any m\u00d7m orthonormal matrix U and any n\u00d7 n orthonormal matrix V. In other words, they are unitarily invariant.\nDefinition 7.2. A matrix norm is said to be unitarily invariant if \u2016UAV\u2016 = \u2016A\u2016 for any unitary matrices U and V. In this tutorial, we only consider real matrices. Thus, a unitarily invariant norm should be termed as \u201corthogonally invariant norm.\u201d However, we still follow the term of the unitarily invariant norm and denote it by ||| \u00b7 |||. Theorem 7.2. Let \u2016 \u00b7 \u2016 be a given norm on Rm\u00d7n. Then it is unitarily invariant if and only if its dual is unitarily invariant.\nProof. Suppose \u2016 \u00b7 \u2016 is unitarily invariant, and let U \u2208 Rm\u00d7m and V \u2208 Rn\u00d7n be orthonormal. Then\n\u2016UAV\u2016\u2217 = max { tr(UAVBT ) : B \u2208 Rm\u00d7n, \u2016B\u2016 = 1 }\n= max { tr(A(UTBVT )T ) : B \u2208 Rm\u00d7n, \u2016B\u2016 = 1 } = max { tr(ACT ) : C \u2208 Rm\u00d7n, \u2016UCV\u2016 = 1 } = max { tr(ACT ) : C \u2208 Rm\u00d7n, \u2016C\u2016 = 1 } = \u2016A\u2016\u2217.\nThe converse follows from the fact that (\u2016A\u2016\u2217)\u2217 = \u2016A\u2016.\nWe find that \u2016A\u20162 = \u2016\u03c3(A)\u2016\u221e and \u2016A\u2016F = \u2016\u03c3(A)\u20162; that is, they correspond the norms on the vector \u03c3(A) of the singular values of A. This sheds light on the relationship of a unitarily invariant norm of a matrix with its singular values."}, {"heading": "56 Unitarily Invariant Norms", "text": ""}, {"heading": "7.2 Symmetric Gauge Functions", "text": "In order to investigate the unitarily invariant norm, we first present the notion of symmetric gauge functions.\nDefinition 7.3. A real function \u03c6 : Rn \u2192 R is called a symmetric gauge function if it satisfies the following four conditions:\n(1) \u03c6(u) > 0 for all nonzero u \u2208 Rn.\n(2) \u03c6(\u03b1u) = |\u03b1|\u03c6(u) for any constant \u03b1 \u2208 R.\n(3) \u03c6(u+ v) \u2264 \u03c6(u) + \u03c6(v) for all u,v \u2208 Rn.\n(4) \u03c6(Du\u03c0) = \u03c6(u) where u\u03c0 = (u\u03c01 , . . . , u\u03c0n) with \u03c0 as a permutation of\n[n] and D is an n\u00d7 n diagonal matrix with \u00b11 diagonal elements.\nFurthermore, the gauge function is called normalized if it satisfies the condition:\n(5) \u03c6(1, 0, . . . , 0) = 1.\nConditions (1)-(3) show that that the gauge function is a vector norm. Thus, it is convex and continuous. Condition (4) says that the gauge function is symmetric.\nLemma 7.3. [Schatten, 1950] Let u,v \u2208 Rn. If |u| \u2264 |v|, then \u03c6(u) \u2264 \u03c6(v) for every symmetric gauge function \u03c6.\nProof. In terms of Condition (4), we can directly assume that u \u2265 0 and v \u2265 0. Currently, the argument is equivalent to\n\u03c6(\u03c91v1, . . . , \u03c9nvn) \u2264 \u03c6(v1, . . . , vn)\nfor \u03c9i \u2208 [0, 1]. Thus, by induction, it suffices to prove\n\u03c6(v1, . . . , vn\u22121, \u03c9vn) \u2264 \u03c6(v1, . . . , vn)\nwhere \u03c9 \u2208 [0, 1] for every symmetric gauge function \u03c6. It follows from"}, {"heading": "7.2. Symmetric Gauge Functions 57", "text": "the following direct computation:\n\u03c6(v1, . . . , vn\u22121, \u03c9vn)\n= \u03c6 (1+\u03c9\n2 v1+ 1\u2212\u03c9 2 v1, . . . , 1+\u03c9 2 vn\u22121+ 1\u2212\u03c9 2 vn\u22121, 1+\u03c9 2 vn \u2212 1\u2212\u03c9 2 vn )\n\u2264 1 + \u03c9 2 \u03c6(v1, . . . , vn\u22121, vn) + 1\u2212 \u03c9 2 \u03c6(v1, . . . , vn\u22121,\u2212vn) = \u03c6(v1, . . . , vn\u22121, vn).\nTheorem 7.4. [Fan, 1951] Given two nonnegative vectors u,v \u2208 Rn+, then u \u227aw v if and only if \u03c6(u) \u2264 \u03c6(v) for every symmetric gauge function \u03c6.\nProof. The necessity is obtained by setting a set of special symmetric gauge functions \u03c6k for k \u2208 [n]. Specifically, they are defined as\n\u03c6k(x) = max 1\u2264i1\u2264\u00b7\u00b7\u00b7\u2264ik\u2264n\nk\u2211\nl=1\n|xil |.\nwhere x = (x1, . . . , xn).\nIt remains to prove the sufficiency. Without loss of generality, we assume that u1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 un and v1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 vn. Let z = (z1, . . . , zn)T where zi = vi for i \u2208 [n \u2212 1] and zn = vn \u2212 \u2211n i=1(vi \u2212 ui). Obviously, z \u2264 v. And it follows from u \u227aw v that u \u227a z. In terms of the theorem of Hardy, Littlewood, and P\u00f3lya (see Lemma 2.2), there exists a doubly stochastic matrix (say W) such that u = Wz. Since W(v \u2212 z) \u2265 0, we have u \u2264 Wv. Thus, by Lemma 7.3, \u03c6(u) \u2264 \u03c6(Wv) for every symmetric gauge function. Consider that a doubly stochastic matrix can be expressed a convex combination of a set of permutation matrices (see Lemma 2.3). We write W = \u2211\nj=1 \u03b1jPj where \u03b1j \u2265 0 and \u2211 j \u03b1j =\n1, and the Pj are permutation matrices. Accordingly,\n\u03c6(u) \u2264 \u03c6( \u2211\nj\n\u03b1jPjv) \u2264 \u2211\nj\n\u03b1j\u03c6(Pjv) = \u2211\nj\n\u03b1j\u03c6(v) = \u03c6(v)."}, {"heading": "58 Unitarily Invariant Norms", "text": "It is worth noting that the proof of Theorem 7.4 implies that if \u03c6k(u) \u2264 \u03c6k(v) for k \u2208 [n], then \u03c6(u) \u2264 \u03c6(v) for every symmetric gauge function \u03c6. In other words, an infinite family of norm inequalities follows from a finite one.\nDefinition 7.4. The dual of a symmetric gauge function \u03c6 on Rn is defined as\n\u03c6\u2217(u) , max { uTv : v \u2208 Rn, \u03c6(v) = 1 } .\nProposition 7.2. Let \u03c6\u2217 be the dual of the symmetric gauge function \u03c6. Then \u03c6\u2217 is also a symmetric gauge function. Moreover, (\u03c6\u2217)\u2217 = \u03c6.\nProof. For a nonzero vector u \u2208 Rn, then \u03c6(u) > 0. Hence,\nmax \u03c6(v)=1\nuTv \u2265 u Tu\n\u03c6(u) > 0.\nIt is also seen that\n\u03c6\u2217(u+v) = max \u03c6(z)=1 (u+v)Tz \u2264 max \u03c6(z)=1 uT z+ max \u03c6(z)=1 vTz \u2264 \u03c6\u2217(u) + \u03c6\u2217(v).\nAs for the symmetry of \u03c6\u2217 can be directly obtained from that of \u03c6. Finally, note that \u03c6\u2217 is a norm on Rn. Thus, (\u03c6\u2217)\u2217 = \u03c6."}, {"heading": "7.3 Unitarily Invariant Norms via SGFs", "text": "There is a one-to-one correspondence between a unitarily invariant norm and a symmetric gauge function (SGF).\nTheorem 7.5. If ||| \u00b7 ||| is a given unitarily invariant norm on Rm\u00d7n, then there is a symmetric gauge function \u03c6 on Rq where q = min{m,n} such that |||A||| = \u03c6(\u03c3(A)) for all A \u2208 Rm\u00d7n.\nConversely, if \u03c6 is a symmetric gauge function on Rq, then |||A||| , \u03c6(\u03c3(A)) is a unitarily invariant norm on Rm\u00d7n.\nProof. Given a unitarily invariant norm ||| \u00b7 ||| on Rm\u00d7n and a vector x \u2208 Rq, define \u03c6(x) , |||X||| where X = [xij ] \u2208 Rm\u00d7n satisfying that xii = xi for i \u2208 [q] and all other elements are zero. That \u03c6 is a norm on R q follows from the fact that ||| \u00b7 ||| is a norm. The unitary invariance of"}, {"heading": "7.3. Unitarily Invariant Norms via SGFs 59", "text": "||| \u00b7 ||| then implies that \u03c6 satisfies the symmetry. Now let A = U\u03a3VT be the full SVD of A. Then |||A||| = |||U\u03a3VT ||| = |||\u03a3||| = \u03c6(\u03c3(A)).\nConversely, if \u03c6 is a symmetric gauge function, for any A \u2208 Rm\u00d7n define |||A||| = \u03c6(\u03c3(A)). We now prove that ||| \u00b7 ||| is a unitarily invariant norm. First, that |||A||| > 0 for A 6= 0 and |||\u03b1A||| = |\u03b1||||A||| for any constant \u03b1 follows the fact that \u03c6 is a norm. The unitary invariance of ||| \u00b7 ||| follows from that for any orthonormal matrices U (m\u00d7m) and V (n\u00d7 n), UAV and A have the same singular values. Finally,\n|||A+B||| = \u03c6(\u03c3(A+B)) \u2264 \u03c6(\u03c3(A) + \u03c3(B)) \u2264 \u03c6(\u03c3(A)) + \u03c6(\u03c3(B)) = |||A|||+ |||B|||.\nHere the first inequality follows Proposition 6.3 and Theorem 7.4.\nThe following theorem implies that there is also a one-one correspondence between the dual of a symmetric gauge function and a dual unitarily invariant norm. Theorem 7.6. Let \u03c6\u2217 be the dual of symmetric gauge function \u03c6. Then |||A||| = \u03c6(\u03c3(A)) if and only if |||A|||\u2217 = \u03c6\u2217(\u03c3(A)). Proof. Assume that |||A||| = \u03c6(\u03c3(A)). Then\n|||A|||\u2217 = max { tr(ATB) : B \u2208 Rm\u00d7n, |||B||| = 1 }\n= max { tr(\u03a3TAU T ABVA) : \u03c6(\u03c3(B)) = 1 } ,\nwhere A = UA\u03a3AV T A is a full SVD of A. By Theorem 6.3, we have\ntr(VTAB TUA\u03a3A) \u2264 max UT U=Im,VT V=In tr(VTBTU\u03a3A) =\nq \u2211\ni=1\n\u03c3i(A)\u03c3i(B).\nWhen letting B = UA\u03a3BV T A as a full SVD of B, we can obtain that\n|||A|||\u2217 = max { tr(\u03a3TA\u03a3B), \u03c6(\u03c3(B)) = 1 } = \u03c6\u2217(\u03c3(A)).\nConversely, the result follows from the fact that (\u03c6\u2217)\u2217 = \u03c6.\nGiven a matrix A \u2208 Rm\u00d7n, let it have a full SVD: A = U\u03a3VT . Then |||A||| = |||\u03a3|||. As we have seen, for x \u2208 Rn the function\n\u03c6(x) , max 1\u2264i1\u2264\u00b7\u00b7\u00b7\u2264ik\u2264n\nk\u2211\nl=1\n|xil |"}, {"heading": "60 Unitarily Invariant Norms", "text": "is a symmetric gauge function. Thus, \u2211k\ni=1 \u03c3i(A) defines also a class of\nunitarily invariant norms which are the so-called Ky Fan k-norms.\nClearly, the vector p-norm \u2016 \u00b7 \u2016p for p \u2265 1 is a symmetric gauge function. Thus, Theorem 7.5 shows that |||A|||p , \u2016\u03c3(A)\u2016p for p \u2265 1 are a class of unitarily invariant norms. They are well known as the Schatten p-norms. Thus, \u2016A\u2016F = \u2016\u03c3(A)\u20162 = |||A|||2 and \u2016A\u20162 = \u2016\u03c3(A)\u2016\u221e = |||A|||\u221e.\nWhen p = 1, \u2016A\u2016\u2217 , |||A|||1 = \u2016\u03c3(A)\u20161 = \u2211min{m,n} i=1 \u03c3i(A) is called the nuclear norm or trace norm, which has been widely used in many machine learning problems such as matrix completion, matrix data classification, multi-task learning, etc. [Srebro et al., 2004, Cai et al., 2010, Mazumder et al., 2010, Liu et al., 2013, Luo et al., 2015, Kang et al., 2011, Pong et al., 2010, Zhou and Li, 2014]. Parallel with the \u21131-norm which is used as convex relaxation of the \u21130-norm [Tibshirani, 1996], the nuclear norm is a convex alternative of the matrix rank. Since the nuclear norm is the best convex approximation of the matrix rank over the unit ball of matrices, this makes it more tractable to solve the resulting optimization problem (see Example 8.1 below)."}, {"heading": "7.4 Properties of Unitarily Invariant Norms", "text": "Theorem 7.5 opens an approach for exploring unitarily invariant norms by using symmetric gauge functions and majorization theory. We will see that this makes things more tractable.\nTheorem 7.7. Let ||| \u00b7 ||| be a unitarily invariant norm on Rn\u00d7n. Then it is consistent.\nTheorem 7.7 follows immediately from Theorem 6.6. However, when the norm is defined on Rm\u00d7n, Theorem 6.6 can not help to establish the consistency of the corresponding unitarily invariant norm.\nAs an immediate corollary of Theorem 7.5, we have the following\nresult, which shows that unitarily invariant norms are monotone.\nTheorem 7.8. Let ||| \u00b7 ||| be a given unitarily invariant norm on Rm\u00d7n. Then |||A||| \u2264 |||B||| if and only if \u03c3(A) \u227aw \u03c3(B)."}, {"heading": "7.4. Properties of Unitarily Invariant Norms 61", "text": "Proposition 7.3. Given a matrix A \u2208 Rm\u00d7n, let [A]r be obtained by replacing the last r rows and r columns of A with zeros, and \u3008A\u3009r by replacing the last r rows or columns of A with zeros. Let q = min{m,n}. Then for any r \u2208 [q],\n|||[A]r||| \u2264 |||\u3008A\u3009r||| \u2264 |||A|||.\nProof. Part (1) directly follows from Proposition 6.3 which shows that \u03c3([A]r) \u227aw \u03c3(\u3008A\u3009r) \u227aw \u03c3(A).\nProposition 7.4. Given two matrices A \u2208 Rm\u00d7n and B \u2208 Rm\u00d7n, we have that |||diag(\u03c3(A)\u2212 \u03c3(B))||| \u2264 |||A\u2212B|||. Furthermore, if both A and B are symmetric matrixes in Rm\u00d7m, then\n|||diag(\u03c3(A)\u2212 \u03c3(B)) \u2264 |||diag(\u03bb(A)\u2212 \u03bb(B))|||||| \u2264 |||A\u2212B|||.\nProof. The first part of the proposition is immediately obtained from Theorem 6.5. As for the second part, Proposition 6.1-(i) says that \u03bb(A)\u2212\u03bb(B) \u227a \u03bb(A\u2212B). It then follows from Lemmas 2.2 and 2.3 that \u03bb(A) \u2212 \u03bb(B) = \u2211j \u03b1jPj\u03bb(A \u2212B) where the \u03b1j \u2265 0 and \u2211 j \u03b1j = 1, and the Pj are some permutation matrices. Accordingly, for every symmetric gauge function \u03c6 on Rm, we have that\n\u03c6(\u03bb(A)\u2212 \u03bb(B)) = \u03c6( \u2211\nj\n\u03b1jPj\u03bb(A\u2212B)) \u2264 \u2211\nj\n\u03b1j\u03c6(Pj\u03bb(A\u2212B))\n= \u2211\nj\n\u03b1j\u03c6(\u03bb(A\u2212B)) = \u03c6(\u03bb(A\u2212B)),\nwhich implies that |||diag(\u03bb(A) \u2212 \u03bb(B))|||||| \u2264 |||A \u2212 B|||. Additionally, consider that for a symmetric matrixM, it holds that \u03c3i(M) = |\u03bbi(M)|. Hence, we have that\n|\u03bbi(A)\u2212 \u03bbi(B)| \u2265 \u2223 \u2223|\u03bbi(A)| \u2212 |\u03bbi(B)| \u2223 \u2223 = |\u03c3i(A)\u2212 \u03c3i(B)|.\nThis concludes the proof.\nAs a direct corollary of Proposition 6.5, we have that\n|\u03c3i(A)\u2212 \u03c3i(B)| \u2264 \u2016A\u2212B\u20162, for i = 1, . . . , q,"}, {"heading": "62 Unitarily Invariant Norms", "text": "where q = min{m,n}, and \u221a \u221a \u221a \u221a q \u2211\ni=1\n(\u03c3i(A)\u2212 \u03c3i(B))2 \u2264 \u2016A\u2212B\u2016F .\nWhen A and B are both symmetric, we also have that\n|\u03bbi(A)\u2212 \u03bbi(B)| \u2264 \u2016A\u2212B\u20162, for i = 1, . . . ,m, \u221a \u221a \u221a \u221a m\u2211\ni=1\n(\u03bbi(A)\u2212 \u03bbi(B))2 \u2264 \u2016A\u2212B\u2016F .\nThe latter result is well known as the Hoffman-Wielandt theorem. Note that the Hoffman-Wielandt theorem still hods when A and B are normal [Stewart and Sun, 1990].\nTheorem 7.9. Let ||| \u00b7 ||| be an arbitrary unitarily invariant norm on R m\u00d7n, and E11 \u2208 Rm\u00d7n have the entry 1 in the (1, 1)th position and zeroes elsewhere. Then\n(a) |||A||| = |||AT |||. (b) \u03c31(A)|||E11||| \u2264 |||A||| \u2264 \u2016A\u2016\u2217|||E11|||. (c) If the symmetric gauge function \u03c6 corresponding to the norm ||| \u00b7 ||| is\nnormalized (i.e., \u03c6(1, 0, 0, . . . , 0) = 1), then\n\u2016A\u20162 \u2264 |||A||| \u2264 \u2016A\u2016\u2217. Proof. Part (a) is due to that \u03c6(\u03c3(A)) = \u03c6(\u03c3(AT )).\nIf \u03c6(1, 0, . . . , 0) = 1, then |||E11||| = 1. Thus, we can have Part (c) from Part (b). Assume A is nonzero. Otherwise, the result is trivial. Let q = min{m,n}. First, |||A||| = \u03c6(\u03c31(A), . . . , \u03c3q(A)) = \u03c31(A)\u03c6(1, \u03c32(A)/\u03c31(A), . . . , \u03c3q(A)/\u03c31(A)) \u2265 \u03c31(A)\u03c6(1, 0, . . . , 0) = \u03c31(A)|||E11|||. Since ( \u03c31(A)/ \u2211q i=1 \u03c3i(A), . . . , \u03c3q(A)/ \u2211q i=1 \u03c3i(A) ) \u227a (1, 0, . . . , 0), we have\n|||A||| = ( q \u2211\ni=1\n\u03c3i(A))\u03c6 ( \u03c31(A)/\nq \u2211\ni=1\n\u03c3i(A), . . . , \u03c3q(A)/ q \u2211\ni=1\n\u03c3i(A) )\n\u2264 \u2016A\u2016\u2217\u03c6(1, 0, . . . , 0) = \u2016A\u2016\u2217|||E11|||."}, {"heading": "7.4. Properties of Unitarily Invariant Norms 63", "text": "Note that a norm \u2016 \u00b7 \u2016 on Rm\u00d7n is said to be self adjoint if \u2016A\u2016 = \u2016AT \u2016 for any A \u2208 Rm\u00d7n. Thus, Theorem 7.9-(a) shows that the unitarily invariant norm is self-adjoint.\nIt is worth mentioning that |||Eij||| = |||E11||| where Eij \u2208 Rm\u00d7n has entry 1 in the (i, j)th position and zeros elsewhere. Moreover, the Schatten p-norms satisfy |||E11|||p = 1. Theorem 7.9 says that for any unitarily invariant norm ||| \u00b7 ||| such that |||E11||| = 1,\n1 \u2264 |||A|||\u2016A\u20162 \u2264 \u2016A\u2016\u2217\u2016A\u2016 2 \u2264 rank(A).\nRecall that\n\u2211q i=1 \u03c32 i (A)\n\u03c32 1 (A)\n= \u2016A\u20162 F\n\u2016A\u20162 2\nand\n\u2211q i=1 \u03c3i(A)\n\u03c31(A) = \u2016A\u2016\u2217\u2016A\u20162 , so called\nstable rank and nuclear rank (see Definition 3.2). They have been found usefulness in the analysis of matrix multiplication approximation [Magen and Zouzias, 2011, Cohen et al., 2015, Kyrillidis et al., 2014].\nTheorem 7.10. Let M \u2208 Rm\u00d7m, N \u2208 Rn\u00d7n, and A \u2208 Rm\u00d7n such that the block matrix [\nM A\nAT N\n]\nis SPSD. Then\n|||M|||+ |||N||| \u2265 2|||A|||.\nProof. Without loss of generality, we assume m \u2265 n. Let A = U\u03a3VT be a thin SVD of A. Consider that [UT ,\u2212VT ] [ M A\nAT N\n] [\nU\n\u2212V\n]\n= UTMU+VTNV\u2212UTAV\u2212VTATU\nis PSD. Hence, |||UTMU+VTNV||| \u2265 2|||\u03a3|||. That is,\n|||VTUTMUV+N||| \u2265 2|||A|||.\nNote that\n|||VTUTMUV+N||| \u2264 |||VTUTMUV|||+ |||N||| \u2264 |||M|||+ |||N|||."}, {"heading": "64 Unitarily Invariant Norms", "text": "Proposition 7.5. Given a matrix A \u2208 Rm\u00d7n, then the following holds\n|||A||| = min X,Y:XYT=A\n1\n2\n{ |||XXT |||+ |||YYT ||| } .\nIf rank(A) = r \u2264 min{m,n}, then the minimum above is attained at a rank decomposition A = X\u0302Y\u0302T where X\u0302 = Ur\u03a3 1/2 r and Y\u0302 = Vr\u03a3 1/2 r , and A = Ur\u03a3rV T r is a condensed SVD of A.\nProof. Let A = XYT be any decomposition of A. Then [\nX\nY\n]\n[XT ,XT ] =\n[\nXXT XYT YXT YYT\n]\nis SPSD. Thus, 1\n2\n[ |||XXT |||+ |||YYT ||| ] \u2265 |||A|||.\nWhen X , X\u0302 = Ur\u03a3 1/2 r and Y , Y\u0302 = Vr\u03a3 1/2 r , it holds that |||A||| = 1 2 [ |||X\u0302X\u0302T |||+ |||Y\u0302Y\u0302T ||| ] .\nSince 12\n[ |||XXT |||+ |||YYT ||| ] \u2265 \u221a |||XXT ||| \u221a |||YYT |||,\n|||A||| \u2265 min X,Y:XYT=A\n\u221a |||XXT ||| \u221a |||YYT |||.\nWhen taking X\u0302 = Ur\u03a3 1/2 r V T r and Y\u0302 = Vr\u03a3 1/2 r V T r , one has\n|||A||| = \u221a |||X\u0302X\u0302T ||| \u221a |||Y\u0302Y\u0302T |||.\nThis thus leads us to the following proposition.\nProposition 7.6. Given a matrix A \u2208 Rm\u00d7n, then the following holds\n|||A||| = min X,Y:XYT=A\n\u221a |||XXT ||| \u221a |||YYT |||.\nAccordingly, the following inequality hods:\n|||XYT ||| \u2264 |||XXT |||1/2|||YYT |||1/2. (7.1)\nThis is a form of the Cauchy-Schwarz inequality under the unitarily\ninvariant norms."}, {"heading": "7.4. Properties of Unitarily Invariant Norms 65", "text": "As a corollary of Proposition 7.5, the following proposition immediately follows. Moreover, this proposition was widely used in matrix completion problems, because an optimization problem regularized by the Frobenius norm is solved more easily than that regularized by the nuclear norm [Hastie et al., 2014].\nProposition 7.7. [Srebro et al., 2004, Mazumder et al., 2010] Given a matrix A \u2208 Rm\u00d7n, then the following holds\n\u2016A\u2016\u2217 = min X,Y:XYT=A\n1 2\n{ \u2016X\u20162F + \u2016Y\u20162F } .\nIf rank(A) = k \u2264 min{m,n}, the minimum above is attained at some rank decomposition.\nThe following theorem shows that the Frobenius norm has a so-called matrix-Pythagoras\u2019 property. However, for other Schatten norms, there needs a strong condition to make the property hold.\nTheorem 7.11. Let A,B \u2208 Rm\u00d7n. If ABT = 0 or ATB = 0, then\n\u2016A+B\u20162F = \u2016A\u20162F + \u2016B\u20162F ,\nmax{\u2016A\u201622, \u2016B\u201622} \u2264 \u2016A+B\u201622 \u2264 \u2016A\u201622 + \u2016B\u201622. If both ABT = 0 and ATB = 0 are satisfied, then\n|||A+B|||pp = |||A||| p p + |||B||| p p\nfor 1 \u2264 p < \u221e and \u2016A+B\u20162 = max{\u2016A\u20162, \u2016B\u20162}.\nProof. Since (A +B)T (A + B) = ATA + BTB when ATB = 0, the Pythagorean property for the Frobenius norm is obvious. As for the spectral norm, it is easily seen that\n\u2016A+B\u201622 = max\u2016x\u20162=1 xT (A+B)T (A+B)x\n= max \u2016x\u20162=1\nxT (ATA+BTB)x\n\u2264 max \u2016x\u20162=1 xTATAx+ max \u2016x\u20162=1 xTBTBx = \u2016A\u201622 + \u2016B\u201622."}, {"heading": "66 Unitarily Invariant Norms", "text": "Let the condensed SVDs of A and B be A = UA\u03a3AV T A and B = UB\u03a3BV T B . If A TB = 0 and ABT = 0, then VTAVB = 0 and U T AUB = 0. Note that\nA+B = [UA,UB ]\n[\n\u03a3A 0\n0 \u03a3B\n] [\nVTA VTB\n]\nis the condensed SVD of A+B. So the nonzero singular values of A+B consist of those of A and of B. The theorem accordingly follows.\nLet us end this chapter by showing a relationship among the matrix\noperator, matrix vectorization, and unitarily invariant norms.\nTheorem 7.12. Let f be a matrix norm on Rm\u00d7n.\n(a) The norm f is both unitarily invariant and operator norm if and only\nif f(A) = \u2016A\u20162 for any A \u2208 Rm\u00d7n. In other words, the spectral norm is only one operator norm that satisfies the self-adjoint property. (b) Given a matrix A \u2208 Rm\u00d7n, f(A) , \u2016vec(A)\u2016 is unitarily invariant if and only if it is the norm \u03b3\u2016A\u2016F for some \u03b3 > 0.\nProof. The proof of Part (a) can be found in Corollary 5.6.35 of Horn and Johnson [1985]. As for Part (b), it is obvious that the Frobenius norm is both unitarily invariant and vectorization norm. Conversely, given any A \u2208 Rm\u00d7n, the vectorization norm is defined as \u2016a\u2016 where a = vec(A). Recall that the vector a can be regarded as an mn \u00d7 1 matrix. Let a = Ua\u03a3avTa be the full SVD of a. Then it is easily seen that \u03a3a = (\u2016A\u2016F , 0, . . . , 0)T . Moreover, we can set va = 1. For any orthonormal matrices U \u2208 Rm\u00d7m and V \u2208 Rn\u00d7n, we have that f(UAVT ) = \u2016vec(UAVT )\u2016 = \u2016(V \u2297U)vec(A)\u2016 = \u2016a\u2016 due to the unitary invariance. Moreover, we have that \u2016a\u2016 = \u2016\u03a3a\u2016 = \u2016A\u2016F \u2016(1, 0, . . . , 0)\u2016. Letting \u03b3 = \u2016(1, 0, . . . , 0)\u2016 > 0, we complete the proof. Notice that if the norm is normalized, then \u03b3 = 1.\n8 Subdifferentials of Unitarily Invariant Norms\nIn the previous chapters, we have used matrix differential calculus. Let f : Rm\u00d7n \u2192 R. We have discussed the gradient and Hessian of f w.r.t. X \u2208 Rm\u00d7n. Especially, the function f : Rm\u00d7n \u2192 R is defined as a trace function. Such a function is differentiable. In this chapter we consider f to be a unitarily invariant norm.\nNorm functions are not necessarily differentiable. For example, the spectral norm and nuclear norm are not differentiable. But norm functions are convex and continuous, so we can resort to theory of subdifferentials [Rockafellar, 1970, Borwein and Lewis, 2006]. Indeed, the subdifferentials of unitarily invariant norms have been studied by Watson [1992] and Lewis [2003].\nUsing the properties of unitarily invariant norms and the SVD theory, we present directional derivatives and subdifferentials of unitarily invariant norms. As two special cases, we report the subdifferentials of the spectral norm and nuclear norm. These two norms have been widely used in machine learning such as matrix low rank approximation. We illustrate applications of the subdifferentials in optimization problems regularized by either the spectral norm or the nuclear norm. We also study the use of the subdifferentials of unitarily invariant norms in solv-\n67"}, {"heading": "68 Subdifferentials of Unitarily Invariant Norms", "text": "ing least squares estimation problems, whose loss function is defined as any unitarily invariant norm."}, {"heading": "8.1 Subdifferentials", "text": "Let \u2016 \u00b7 \u2016 be a given norm on Rm\u00d7n, and A be a given matrix in Rm\u00d7n. The subdifferential, a set of subgradients, of \u2016A\u2016 is defined as\n{ G \u2208 Rm\u00d7n : \u2016B\u2016 \u2265 \u2016A\u2016+ tr((B\u2212A)TG) for all B \u2208 Rm\u00d7n } ,\nand denoted by \u2202\u2016A\u2016. When the norm \u2016 \u00b7 \u2016 is differentiable, the subgradient degenerates to the gradient. That is, the subdifferential is a singleton. For example, when taking the squared Frobenius norm \u2016A\u20162F = tr(ATA), \u2202\u2016A\u20162F = {2A}.\nLemma 8.1. Let A \u2208 Rm\u00d7n be a given matrix. Then G \u2208 \u2202\u2016A\u2016 if and only if \u2016A\u2016 = tr(GTA) and \u2016G\u2016\u2217 \u2264 1.\nProof. The sufficiency is immediate. Now assume thatG \u2208 \u2202\u2016A\u2016. Then taking B = 2A yields \u2016G\u2016 \u2265 tr(ATG) and taking B = 12A yields 1 2\u2016A\u2016 \u2264 12 tr(ATG), which implies that \u2016A\u2016 = tr(ATG). Subsequently, \u2016B\u2016 \u2265 tr(GTB) for all matrices B. Thus, the dual norm satisfies\n\u2016G\u2016\u2217 = max{tr(GTB) : \u2016B\u2016 = 1} \u2264 1.\nWe especially consider the subdifferential of unitarily invariant norms. Given a unitarily invariant norm ||| \u00b7 ||| on Rm\u00d7n, let p = min{m,n}. Theorem 7.5 shows there exists a symmetric gauge function \u03c6 : Rp \u2192 R associated with the norm ||| \u00b7 |||. Thus, this encourages us to define the subdifferential of unitarily invariant norms via the subdifferential of symmetric gauge functions.\nThe subdifferential of the symmetric gauge function \u03c6 at x \u2208 Rp is\n\u2202\u03c6(x) , {z \u2208 Rp : \u03c6(y) \u2265 \u03c6(x) + (y\u2212 x)Tz for all y \u2208 Rp}.\nIn terms of Lemma 8.1, that z \u2208 \u2202\u03c6(x) is equivalent to that \u03c6(x) = xTz and \u03c6\u2217(z) \u2264 1. Here \u03c6\u2217 is the dual of \u03c6 (see Definition 7.4) which is a"}, {"heading": "8.1. Subdifferentials 69", "text": "symmetric gauge function for the dual norm ||| \u00b7 |||\u2217. That is, \u03c6\u2217(\u03c3(A)) = |||A|||\u2217 (see Theorem 7.6).\nLet us return to the subdifferential of unitarily invariant norms.\nThe following lemma gives the directional derivative of |||A|||.\nLemma 8.2. Let ||| \u00b7 ||| be a given unitarily invariant norm on Rm\u00d7n, and \u03c6 be the corresponding symmetric gauge function. Then the directional derivative of the norm at A \u2208 Rm\u00d7n in a direction R \u2208 Rm\u00d7n is\nlim t\u21930 |||A+tR||| \u2212 |||A||| t = max d\u2208\u2202\u03c6(\u03c3(A))\np \u2211\ni=1\ndiu T i Rvi = max G\u2208\u2202|||A||| tr(RTG).\nHere p = min{m,n}, U = [u1, . . . ,um], V = [v1, . . . ,vn], \u03a3 = diag(\u03c3(A)), and A = U\u03a3VT is a full SVD of A.\nProof. By Lemma 2.5, we immediately have\nlim t\u21930 |||A+ tR||| \u2212 |||A||| t = max G\u2208\u2202|||A||| tr(RTG).\nWe now prove the first equality. Let z = (uT1 Rv1, . . . ,u T p Rvp) T . Consider that\n|||A+ tR||| = |||\u03a3+ tUTRV||| = \u03c6(\u03c3(\u03a3+ tUTRV)) \u2265 \u03c6(\u03c3(A) + tz)\nbecause \u03c3(A)+tz \u227aw \u03c3(\u03a3+tUTRV) by Proposition 6.3. Accordingly, we have that\nlim t\u21930 |||A+tR||| \u2212 |||A||| t \u2265 lim t\u21930 \u03c6(\u03c3(A)+tz)\u2212 \u03c6(\u03c3(A)) t = max d\u2208\u2202\u03c6(\u03c3(A)) dTz.\nThe above equality follows from Lemma 2.5, when applied to the symmetric gauge function \u03c6.\nOn the other hand, let \u03c3(t) , \u03c3(A+tR) = \u03c3(\u03a3+tUTRV). Now\nwe have\n|||A||| \u2212 |||A+tR||| t = |||A+tR\u2212tR||| \u2212 |||A+tR||| t\n= \u03c6(\u03c3(\u03a3+tUTRV\u2212tUTRV)) \u2212 \u03c6(\u03c3(t))\nt\n\u2265 \u03c6(\u03c3(t)\u2212 tz)\u2212 \u03c6(\u03c3(t)) t \u2265 \u2212d(t)Tz [where d(t) \u2208 \u2202\u03c6(\u03c3(t))]."}, {"heading": "70 Subdifferentials of Unitarily Invariant Norms", "text": "The above first inequality follows from \u03c3(t)\u2212 tz \u227aw \u03c3(A). The second inequality is based on the property of the subgradient of \u03c6 at \u03c3(t). Note that \u03c6 is a continuous function. By the definition of \u2202\u03c6(\u03c3(t)), it is directly verified that lim t\u21920+ d(t) \u2192 d0 \u2208 \u2202\u03c6(\u03c3(A)). Thus,\nlim t\u21930 |||A+tR||| \u2212 |||A||| t \u2264 lim t\u21930 d(t)T z = dT0 z \u2264 max d\u2208\u2202\u03c6(\u03c3(A)) dTz.\nThis implies that the first equality also holds.\nTheorem 8.3. Let A \u2208 Rm\u00d7n have a full SVD A = U\u03a3VT , and let \u03c3 = dg(\u03a3). Then\n\u2202|||A||| = conv { UDVT : d \u2208 \u2202\u03c6(\u03c3),D = diag(d) } .\nwhere \u03c6 is a symmetric gauge function corresponding to the norm ||| \u00b7 |||. Here the notation \u201cconv{\u00b7}\u201d represents the convex hull of a set, which is closed and convex. If G \u2208 \u2202|||A|||, Theorem 8.3 says that G can be expressed as\nG = \u2211\ni\n\u03b1iU (i)D(i)(V(i))T ,\nwhere \u03b1i \u2265 0, \u2211 i \u03b1i = 1, A = U (i)\u03a3(V(i))T is a full SVD, di \u2208 \u03c6(\u03c3), and D(i) = diag(di). According to Corollary 3.3, we can rewrite G as\nG = \u2211\ni\n\u03b1iUQ (i)D(i)(P(i))TVT , (8.1)\nwhere P(i) and Q(i) are defined as P and Q in Corollary 3.3; i.e., they satisfy that Q(i)\u03a3(P(i))T = \u03a3 and (Q(i))T\u03a3P(i) = \u03a3.\nProof. First of all, we denote the convex hull on the right-hand side by G(A). Assume that G \u2208 G(A). We now prove G \u2208 \u2202|||A|||. Based on Lemma 8.1, we try to show that |||A||| = tr(ATG) and |||G|||\u2217 \u2264 1. In terms of the above discussion, we can express G as in (8.1). Thus,\ntr(ATG) = \u2211\ni=1\n\u03b1itr(A TUQ(i)D(i)(P(i))TVT )\n= \u2211\ni=1\n\u03b1itr((P (i))T\u03a3TQ(i)D(i)) =\n\u2211\ni=1\n\u03b1itr(\u03a3 TD(i))\n= \u2211\ni=1\n\u03b1id T i \u03c3 = \u03c6(\u03c3) = |||A|||."}, {"heading": "8.1. Subdifferentials 71", "text": "Additionally,\n|||G|||\u2217 = max |||R|||\u22641 tr(GTR) = max |||R|||\u22641 tr ( RT\n\u2211\ni=1\n\u03b1iU (i)D(i)(V(i))T ) .\nSince for each i, |||U(i)D(i)(V(i))T |||\u2217 = |||D(i)|||\u2217 = \u03c6\u2217(di) \u2264 1, and by Proposition 7.1 we have tr(RTU(i)D(i)(V(i))T ) \u2264 |||R||| \u00d7 |||U(i)D(i)(V(i))T |||\u2217 \u2264 |||R|||. Thus, |||G|||\u2217 \u2264 1. In summary, we have G \u2208 \u2202|||A|||.\nConversely, assume that G \u2208 \u2202|||A||| but G /\u2208 G(A). Then by the well-known separation theorem [Borwein and Lewis, 2006, see Theorem 1.1.1] there exists a matrix R \u2208 Rm\u00d7n such that tr(RTX) < tr(RTG) for all X \u2208 G(A).\nThis implies that\nmax d\u2208\u2202\u03c6(\u03c3)\n\u2211\ni=1\ndiu T i Rvi = max X\u2208G(A) tr(RTX) < max G\u2208\u2202|||A||| tr(RTG).\nThis contradicts with Lemma 8.2. Thus, the theorem follows.\nWe are especially interested in the spectral norm \u2016 \u00b7 \u20162 and the nuclear norm \u2016\u00b7\u2016\u2217. As corollaries of Theorem 8.3, we have the following the results.\nCorollary 8.4. Let A have rank r \u2264 p = min{m,n} and A = Ur\u03a3rVTr be a condensed SVD. Then the subdifferential of \u2016A\u2016\u2217 is give as \u2202\u2016A\u2016\u2217 = { UrV T r +W : W \u2208 Rm\u00d7n s.t. UTr W = 0,WVr = 0, \u2016W\u20162 \u2264 1 } .\nProof. For the nuclear norm, the corresponding symmetric gauge function is \u03c6(\u03c3) = \u2016\u03c3\u20161 = \u2211p i=1 \u03c3i. Moreover,\n\u2202\u2016\u03c3\u20161 = { u \u2208 Rp : \u2016u\u2016\u221e \u2264 1 and ui = 1 for i = 1, . . . , r } .\nLet G \u2208 \u2202\u2016A\u2016\u2217. By Theorem 8.3 and Corollary 3.3, we have G = \u2211\ni=1\n\u03b1iUQ (i)D(i)(P(i))TVT\n= UrV T r +\n\u2211\ni=1\n\u03b1iU\u2212rQ (i) 0 D (i) \u2212r(P (i) 0 ) TVT\u2212r,"}, {"heading": "72 Subdifferentials of Unitarily Invariant Norms", "text": "where the \u03b1i \u2265 0 and \u2211 i=1 \u03b1i = 1,D (i) = dg(di), di \u2208 \u2202\u03c6(\u03c3), andD(i)\u2212r is the last (m \u2212 r) \u00d7 (n \u2212 r) principal submatrix of D(i). Here Q(i) \u2208 R m\u00d7m, P(i) \u2208 Rn\u00d7n, Q(i)0 \u2208 R(m\u2212r)\u00d7(m\u2212r), and P (i) 0 \u2208 R(n\u2212r)\u00d7(n\u2212r) are orthonormal matrices, which are defined in Corollary 3.3. Let\nW , U\u2212r [\u2211\ni=1\n\u03b1iQ (i) 0 D (i) \u2212r(P (i) 0 )\nT ]\nVT\u2212r. (8.2)\nObviously, UTr W = 0 and WVr = 0. Moreover,\n\u2016W\u20162 \u2264 \u2211\ni=1\n\u03b1i\u2016D(i)\u2212r\u20162 \u2264 1.\nWe can also see that any matrix W satisfying the above three conditions always has an expression as in (8.2).\nCorollary 8.5. Let the largest singular value \u03c31 of A \u2208 Rm\u00d7n have multiplicity t, and Ut and Vt consist of the first t columns of U and V respectively. Then\n\u2202\u2016A\u20162 = { UtHV T t : H \u2208 Rt\u00d7t s.t. H is SPSD, tr(H) = 1 } .\nProof. The corresponding symmetric gauge function is \u03c6(\u03c3) = \u2016\u03c3\u2016\u221e, and its subdifferential is\n\u2202\u2016\u03c3\u2016\u221e = conv{ei : i = 1, . . . , t},\nwhere ei is the ith column of the identity matrix. It then follows from Theorem 8.3 that for any G \u2208 \u2202\u2016A\u20162, it can be written as\nG = \u2211\ni=1\n\u03b1iUtQ (i)D (i) t (Q (i))TVTt ,\nwhere the \u03b1i \u2265 0 and \u2211 i=1 \u03b1i = 1, and Q (i) is an arbitrary t \u00d7 t orthonormal matrix (see Theorem 3.2). Here Di = dg(di), di \u2208 \u2202\u03c6(\u03c3), and D\n(i) t is the first t\u00d7 t principal submatrix of D(i). Let\nH = \u2211\ni=1\n\u03b1iQ (i)D (i) t (Q (i))T , (8.3)\nwhich is SPSD and satisfies tr(H) = 1. Conversely, any SPSD matrix H satisfying tr(H) = 1 can be always expressed as the form of (8.3)."}, {"heading": "8.2. Applications 73", "text": ""}, {"heading": "8.2 Applications", "text": "In this section we present several examples to illustrate the application of the subdifferential of unitarily invariant norms in solving an optimization problem regularized by a unitarily invariant norm or built on any unitarily invariant norm loss.\nExample 8.1. Given a nonzero matrix A \u2208 Rm\u00d7n, consider the following optimization problem:\nmin X\u2208Rm\u00d7n\nf(X) , 1\n2 \u2016X\u2212A\u20162F + \u03c4\u2016X\u2016\u2217, (8.4)\nwhere \u03c4 > 0 is a constant. Clearly, the problem is convex in X. This problem is a steppingstone of matrix completion. Let A = Ur\u03a3rV T r be a given condensed SVD of A, and define\nX\u0302 = Ur[\u03a3r \u2212 \u03c4Ir]+Vr,\nwhere [\u03a3r\u2212\u03c4Ir]+ = diag([\u03c31\u2212\u03c4 ]+, . . . , [\u03c3r\u2212\u03c4 ]+) and [z]+ = max(z, 0). Now it can be directly checked that\n\u2202f(X\u0302) = X\u0302\u2212A+ \u03c4\u2202\u2016X\u0302\u2016.\nAssume that the first k singular values \u03c3i are greater than \u03c4 . Then,\n1 r (A\u2212 X\u0302) = UkVTk + 1 \u03c4 Uk+1:rdiag(\u03c3k+1, . . . , \u03c3r)V T k+1:r,\nwhich belongs to \u2202\u2016X\u0302\u2016. In other words, 0 \u2208 \u2202f(X\u0302) (see Corollary 8.4). Thus, X\u0302 is a minimizer of the optimization problem. It is called the singular value thresholding (SVT) operator [Cai et al., 2010]. We can see that the parameter \u03c4 controls the rank of the matrix X\u0302 and the problem is able to yield a low rank solution to the matrix X. That is, X\u0302 is a low rank approximation to the matrix A.\nExample 8.2. Given a nonzero matrix A \u2208 Rm\u00d7n, consider the following optimization problem:\nmin X\u2208Rm\u00d7n\nf(X) , 1\n2 \u2016X\u2212A\u20162F + \u03c4\u2016X\u20162, (8.5)"}, {"heading": "74 Subdifferentials of Unitarily Invariant Norms", "text": "where \u03c4 > 0 is a constant. Also, this problem is convex in X. Let A have the k distinct positive singular values \u03b41 > \u03b42 > \u00b7 \u00b7 \u00b7 > \u03b4k among the \u03c3i, with respective multiplicities r1, . . . , rk. Thus, the rank of A is r = \u2211k i=1 ri. Let mt = \u2211t i=1 ri and \u00b5t = \u2211t i=1 ri\u03b4i for t = 1, . . . , k. So mk = r and \u00b5k = tr(\u03a3r) = \u2211r\ni=1 \u03c3i. Assume that \u03c4 \u2264 \u00b5k. We now consider two cases.\nIn the first case, assume l \u2208 [k\u2212 1] is the smallest integer such that l\u2211\ni=1\nri(\u03b4i \u2212 \u03b4l+1) = \u00b5l \u2212 \u03b4l+1ml > \u03c4,\nand hence, \u03b4l \u2265 \u00b5l\u2212\u03c4ml > \u03b4l+1. Note that l+1\u2211\ni=1\nri(\u03b4i \u2212 \u03b4l+2) = l\u2211\ni=1\nri(\u03b4i\u2212\u03b4l+1)+ l+1\u2211\ni=1\nri(\u03b4l+1\u2212\u03b4l+2)\n> l\u2211\ni=1\nri(\u03b4i\u2212\u03b4l+1) > \u03c4.\nThis implies that l is identifiable. Denoting \u03b4 = \u00b5l\u2212\u03c4ml , we define \u03a3\u0302 by replacing the first ml diagonal elements of \u03a3r by \u03b4, and then set X\u0302 = Ur\u03a3\u0302rV T r . Now note that\n1 \u03c4 (A\u2212 X\u0302) = UmlHVTml ,\nwhere H = diag ( (\u03c31 \u2212 \u03b4)/\u03c4, . . . , (\u03c3ml \u2212 \u03b4)/\u03c4 ) . Clearly, H is PSD and tr(H) = \u2211ml\ni=1 \u03c3i\u2212\u03b4 \u03c4 = \u2211l i=1 rl(\u03b4i\u2212\u03b4) \u03c4 = 1. It follows from Corollary 8.5\nthat 1\u03c4 (A\u2212 X\u0302) \u2208 \u2202\u2016X\u0302\u20162. Thus, X\u0302 is a minimizer. In the second case, otherwise,\n\u2211k\u22121 i=1 ri(\u03b4i \u2212 \u03b4k) = \u00b5k\u22121 \u2212mk\u22121\u03b4k \u2264\n\u03c4 \u2264 \u00b5k. Let \u03b4 = \u00b5k\u2212\u03c4mk such that\n0 \u2264 \u03b4 \u2264 \u00b5k \u2212 \u00b5k\u22121 + \u03b4kmk\u22121 mk = \u03b4k.\nDefine X\u0302 = Ur\u03b4IrV T . Then\n1 \u03c4 (A\u2212 X\u0302) = 1 \u03c4 Ur(\u03a3r \u2212 \u03b4Ir)VTr .\nSince 1\u03c4 (\u03a3r\u2212 \u03b4Ir) is PSD and 1\u03c4 tr(\u03a3r\u2212 \u03b4Ir) = 1, we obtain 0 \u2208 \u2202f(X\u0302). This implies that X\u0302 is a minimizer of the problem."}, {"heading": "8.2. Applications 75", "text": "As we have seen, the minimizer X\u0302 has the same rank with A. Thus, the problem in (8.5) can not give a low-rank solution. However, this problem makes the singular values of X\u0302 more well-conditioned because the top singular values decay to \u03b4. Thus, we call it a singular value averaging (SVA) operator.\nExample 8.3. Given a nonzero matrix A \u2208 Rm\u00d7n, consider the following convex optimization problem:\nmin X\u2208Rm\u00d7n\nf(X) , \u2016X\u2212A\u20162 + \u03c4\u2016X\u2016\u2217, (8.6)\nwhere \u03c4 > 0 is a constant. In the above model the loss function and regularization term are respectively defined as the spectral norm and the nuclear norma, which are mutually dual. Moreover, this model can be regarded as a parallel version of the Dantzig selector [Cand\u00e8s and Tao, 2007]. Thus, this model might be potentially interesting.\nLet A = Ur\u03a3rV T r be a condensed SVD. Assume that r\u03c4 > 1. Assume there are the k distinct positive singular values \u03b41 > \u03b42 > \u00b7 \u00b7 \u00b7 > \u03b4k among the \u03c3i, with respective multiplicities r1, . . . , rk. Let mt = \u2211t i=1 ri for t = 1, . . . , k.\nLet l \u2208 [k] be the smallest integer such that ml\u03c4 \u2265 1 > ml\u22121\u03c4 . Define X\u0302 = Ur[\u03a3r\u2212\u03b4lIr]+VTr = Uml\u22121diag(\u03c31\u2212\u03b4l, . . . , \u03c3ml\u22121 \u2212\u03b4l)VTml\u22121 . Then A\u2212 X\u0302 has the maximum singular value \u03b4l with multiplicity ml. It follows from Corollaries 8.4 and 8.5 that \u2202\u2016X\u0302\u2016\u2217 = { Uml\u22121V T ml\u22121 +W : WTUml\u22121 = 0,WVml\u22121 = 0, \u2016W\u20162 \u2264 1 }\nand\n\u2202\u2016A\u2212 X\u0302\u20162 = { \u2212UmlHVTml : H is PSD, tr(H) = 1 } .\nTake W0 = U[ml\u22121+1:ml] (1\u2212ml\u22121\u03c4) rl\u03c4 IrlV T [ml\u22121+1:ml] . Note that W0Vml\u22121 = 0, W T 0 Uml\u22121 = 0, and \u2016W0\u20162 = (1\u2212ml\u22121\u03c4) rl\u03c4\n\u2264 1 due to ml\u22121\u03c4 + rl\u03c4 = ml\u03c4 \u2265 1 and ml\u22121\u03c4 < 1. Hence,\n\u03c4\u2202\u2016X\u0302\u2016\u2217 \u220b \u03c4(Uml\u22121VTml\u22121 +W0) = UmlH0V T ml ,\nwhereH0 = \u03c4(Iml\u22121\u2295 (1\u2212ml\u22121\u03c4) rl\u03c4 Irl). Clearly,H0 is PSD and tr(H0) = 1. Thus,\n\u2212UmlH0VTml \u2208 \u2202\u2016A\u2212 X\u0302\u20162."}, {"heading": "76 Subdifferentials of Unitarily Invariant Norms", "text": "As a result, 0 \u2208 \u2202\u2016A\u2212 X\u0302\u20162+ \u03c4\u2202\u2016X\u0302\u2016\u2217. Consequently, X\u0302 is a minimizer of the problem in (8.6). Compared with SVT in the model (8.4) which uses the tuning parameter \u03c4 as the thresholding value, the current model uses \u03b4l as the thresholding value.\nWe also consider the following convex optimization problem:\nmin X\u2208Rm\u00d7n\nf(X) , \u2016X\u2212A\u2016\u2217 + 1\n\u03c4 \u2016X\u20162. (8.7)\nClearly, the minimizer of the problem isA\u2212X\u0302 where X\u0302 is the minimizer of the problem (8.6).\nExample 8.4. Finally, we consider the following optimization problem:\nmin X\u2208Rn\u00d7p\nf(X) , |||AX\u2212B|||,\nwhereA \u2208 Rm\u00d7n and B \u2208 Rm\u00d7p are two given matrices. This is a novel matrix low rank approximation problem. We will further discuss this problem in Theorem 9.1 of Chapter 9. Here we are concerned with the use of Theorem 8.3 in solving the problem based on unitarily invariant norm loss functions.\nLet A = Ur\u03a3rV T r be a condensed SVD of A, and U\u2212r and V\u2212r be respective orthonormal complements of Ur and Vr. Now B\u2212AA\u2020B = U\u2212rUT\u2212rB. Thus, when taking X\u0302 = A \u2020B, one has\n\u2202f(X\u0302) = AT\u2202|||U\u2212rUT\u2212rB|||.\nLet U0\u03a30V T 0 = U T \u2212rB be a thin SVD of U T \u2212rB, D be a diagonal matrix, and \u03c6 be a symmetric gauge function associated with the norm ||| \u00b7 |||. It follows from Theorem 8.3 that\n\u2202|||U\u2212rUT\u2212rB||| = conv{U\u2212rU0DVT0 : U0,V0, dg(D) \u2208 \u03c6(dg(\u03a30))}.\nThus, for any G \u2208 \u2202|||U\u2212rUT\u2212rB|||, it holds that ATG = 0. This implies that \u2202f(X\u0302) = {0}. Hence, 0 \u2208 \u2202f(X\u0302). This implies that X\u0302 is a minimizer of the problem. In other words,\nmin X\u2208Rn\u00d7p\n|||AX\u2212B||| = |||AA\u2020B\u2212B|||.\n9 Matrix Low Rank Approximation\nMatrix low rank approximation is very important, because it has received wide applications in machine learning and data mining. On the one hand, many machine learning methods involve computing linear equation systems, matrix decomposition, matrix determinants, matrix inverses, etc. How to compute them efficiently is challenging in big data scenarios. Matrix low rank approximation is a potentially powerful approach for addressing computational challenge. On the other hand, many machine learning tasks can be modeled as matrix low rank approximation problems such as matrix completion, spectral clustering, and multi-task learning.\nApproximate matrix multiplication is an inverse process of the matrix low rank approximation problem. Recently, many approaches to approximate matrix multiplication [Drineas et al., 2006a, Sarlos, 2006, Cohen and Lewis, 1999, Magen and Zouzias, 2011, Kyrillidis et al., 2014, Kane and Nelson, 2014] have been developed. Meanwhile, they are used to obtain fast solutions for the \u21132 regression and SVD problems [Drineas et al., 2006b, 2011b, Nelson and Nguy\u00ean, 2013, Halko et al., 2011, Clarkson and Woodruff, 2013, Martinsson et al., 2011, Woolfe et al., 2008]. This makes matrix low rank approximation\n77"}, {"heading": "78 Matrix Low Rank Approximation", "text": "also become increasingly popular in the theoretical computer science community [Sarlos, 2006, Drineas et al., 2006a].\nIn this chapter we first present some important theoretical results in matrix low rank approximation. We then discuss approximate matrix multiplication. In the following chapter we are concerned with large scale matrix approximation. We will study randomized SVD and CUR approximation. They can be also cast into the matrix low rank approximation framework."}, {"heading": "9.1 Basic Results", "text": "Usually, matrix low rank approximation is formulated as a least squares estimation problem based on the Frobenius norm loss. However, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest. He even said \u201cFrobenius-norm error bounds are typically vacuous.\u201d Thus, spectral norm as a loss function is also employed. In this chapter, we present several basic results, some of which hold even for every unitarily invariant norm.\nTheorem 9.1. Let A \u2208 Rm\u00d7n and C \u2208 Rm\u00d7c. Then for any X \u2208 Rc\u00d7n and any unitarily invariant norm ||| \u00b7 |||,\n|||A\u2212CC\u2020A||| \u2264 |||A\u2212CX|||.\nIn other words,\nC\u2020A = argmin X\u2208Rc\u00d7n |||CX\u2212A|||. (9.1)\nAs we have seen, Theorem 9.1 was discussed in Example 8.4, where the problem is solved via the subdifferentials of unitarily invariant norms given in Theorem 8.3. Here, we present an alternative proof.\nProof. Let E1 = A\u2212CC\u2020A, E2 = CC\u2020A\u2212CX, and E = E1 +E2 = A\u2212CX. Since\nET1 E2 = A T (I \u2212CC\u2020)C(C\u2020A\u2212X) = AT0(C\u2020A\u2212X) = 0,\nwe have ETE = ET1 E1+E T 2 E2, and thus \u03bbi(E1) \u2264 \u03bbi(E). It then follows that \u03c3i(E1) \u2264 \u03c3i(E), and thereby \u03c3(E1) \u227aw \u03c3(E). It then follows from"}, {"heading": "9.1. Basic Results 79", "text": "Theorems 7.4 and 7.5 that\n|||E1||| \u2264 |||E|||\nfor any unitarily invariant norm ||| \u00b7 |||.\nRecall that Problem (9.1) gives an extension to the least squares problem (4.1) in Section 4.1. Theorem 9.1 shows that there is an identical solution w.r.t. all unitarily invariant norm errors. The following theorem shows the solution of a more complicated problem. However, the theorem holds only for the Frobenius norm loss.\nTheorem 9.2. Let A \u2208 Rm\u00d7n, C \u2208 Rm\u00d7c, and R \u2208 Rr\u00d7n. Then for all X \u2208 Rc\u00d7r, \u2016A\u2212CC\u2020AR\u2020R\u2016F \u2264 \u2016A\u2212CXR\u2016F . Equivalently, X\u22c6 = C\u2020AR\u2020 minimizes the following problem:\nmin X\u2208Rc\u00d7n\n\u2016CXR \u2212A\u20162F . (9.2)\nProof. Let E1 = (Im \u2212 CC\u2020)A, E2 = CC\u2020A(In \u2212 R\u2020R), E3 = CC\u2020AR\u2020R \u2212 CXR, and E = E1 + E2 + E3. Then E1 + E2 = A \u2212 CC\u2020AR\u2020R and E = A \u2212 CXR. Since ET1 E2 = 0, E3ET2 = 0, ET1 E3 = 0, it follows from the matrix Pythagorean theorem that\n\u2016E\u20162F = \u2016E1\u20162F + \u2016E2\u20162F + \u2016E3\u20162F = \u2016E1 +E2\u20162F + \u2016E3\u20162F .\nThus, \u2016E1 +E2\u20162F \u2264 \u2016E\u20162F .\nTheorem 9.3. [Eckart and Young, 1936, Mirsky, 1960] Given an m\u00d7n real matrix A of rank r (\u2264 min{m,n}), let A = U\u03a3VT be the full SVD of A. DefineAk = Uk\u03a3kV T k , whereUk and Vk consist of the first k columns of U and V respectively, and \u03a3k is the first k \u00d7 k principal submatrix of \u03a3. Then for all m\u00d7 n real matrices B of rank at most k,\n|||A\u2212Ak||| \u2264 |||A\u2212B|||\nholds for all unitarily invariant norm ||| \u00b7 |||. In other words,\nAk = argmin B\u2208Rm\u00d7n,rank(B)\u2264k\n|||A\u2212B|||. (9.3)"}, {"heading": "80 Matrix Low Rank Approximation", "text": "Theorem 9.3 shows that the rank k truncated SVD produces the best rank k approximation. The theorem was originally proposed by Eckart and Young [1936] under the setting of the Frobenius norm, and generalized to any unitarily invariant norms by Mirsky [1960].\nProof. For any m\u00d7n real matrix B of rank at most k, we can write it as B = QC where Q is an m \u00d7 k column orthonormal matrix and C is some k \u00d7 n matrix. Thus,\n|||A\u2212B||| = |||A\u2212QC||| \u2265 |||A\u2212QQTA||| = |||Q\u22a5(Q\u22a5)TA|||,\nwhere Q\u22a5 (m \u00d7 (m\u2212k)) is the orthogonal complement of Q. By Proposition 6.3, we have \u03c3i(Q\n\u22a5(Q\u22a5)TA) = \u03c3i((Q\u22a5)TA) \u2265 \u03c3k+i for i = 1, . . . , p\u2212 k. This implies that\n\u03c3(A\u2212Ak) = (\u03c3k+i, \u03c3p, 0, . . . , 0)T \u227aw \u03c3(Q\u22a5(Q\u22a5)TA).\nHence, |||A\u2212B||| \u2265 |||A\u2212Ak|||.\nThe above proof procedure also implies that for all m \u00d7 k column orthonormal matrices Q,\n|||A\u2212UkUTkA||| \u2264 |||A\u2212QQTA|||\nholds for every unitarily invariant norm ||| \u00b7 |||. When k < r, Ak is called a truncated SVD of A and the closest rank-k approximation of A. Note that when the Frobenius norm is used, Ak is the unique minimizer of the problem in (9.3). However, when other unitarily invariant norms are used, the case does not always hold. For example, let us take the spectral norm. Clearly, if\n\u03a3\u0303 = diag(\u03c31 \u2212 \u03c9\u03c3k+1, \u03c32 \u2212 \u03c9\u03c3k+1, . . . , \u03c3k \u2212 \u03c9\u03c3k+1, 0, . . . , 0)\nfor any \u03c9 \u2208 [0, 1], then U\u03a3\u0303VT is also a minimizer of the corresponding problem.\nTheorem 9.4. Given a matrix A \u2208 Rm\u00d7n and a column orthonormal matrix Q \u2208 Rm\u00d7p, let Bk be the rank-k truncated SVD of QTA for 1 \u2264 k \u2264 p. Then Bk is an optimal solution of the following problem:\nmin B\u2208Rl\u00d7n,rank(B)\u2264k\n\u2016A\u2212QB\u20162F = \u2016A\u2212QBk\u20162F . (9.4)"}, {"heading": "9.1. Basic Results 81", "text": "Proof. Note that (A\u2212QQTA)T (QB\u2212QQTA) = 0, so\n\u2016A\u2212QB\u20162F = \u2016A\u2212QQTA\u20162F + \u2016QB\u2212QQTA|2F = \u2016A\u2212QQTA\u20162F + \u2016B\u2212QTA|2F .\nThe result of the theorem follows from Theorem 9.3.\nTheorem 9.4 is a variant of Theorem 9.3 and of Theorem 9.1. Unfortunately, Bk might not be the solution to the above problem in every unitarily invariant norm, even in the spectral norm error. The reason is that the matrix Pythagorean identity hods only for the Frobenius norm (see Theorem 7.11).\nHowever, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest. He even said \u201cFrobenius-norm error bounds are typically vacuous\u201d [Tropp, 2015]. The following theorem was proposed by Gu [2015], which relates the approximation error in the Frobenius norm to that in the spectral norm.\nTheorem 9.5. [Gu, 2015] Given any matrix A \u2208 Rm\u00d7n, let p = min{m,n} and B be a matrix with rank at most k such that\n\u2016A\u2212B\u2016F \u2264 \u221a \u221a \u221a \u221a\u03b72 + p \u2211\nj=k+1\n\u03c32j (A)\nfor some \u03b7 \u2265 0. Then we must have \u221a \u2211k\nj=1(\u03c3j(A)\u2212 \u03c3j(B))2 \u2264 \u03b7 and\n\u2016A\u2212B\u20162 \u2264 \u221a \u03b72 + \u03c32k+1(A).\nProof. By Proposition 6.3-(2), we have\n\u03c3i+k(A) \u2264 \u03c3i(A\u2212B) + \u03c3k+1(B) = \u03c3i(A\u2212B) for i \u2208 [p\u2212 k]\ndue to rank(B) \u2264 k. It then follows that\n\u2016A\u2212B\u20162F = p \u2211\ni=1\n\u03c32i (A\u2212B) \u2265 \u03c321(A\u2212B) + p\u2212k \u2211\ni=2\n\u03c32i (A\u2212B)\n\u2265 \u03c321(A\u2212B) + p\u2212k \u2211\ni=2\n\u03c32i+k(A)."}, {"heading": "82 Matrix Low Rank Approximation", "text": "We thus obtain\n\u2016A\u2212B\u201622 = \u03c321(A\u2212B) \u2264 \u03b72 + \u03c32k+1(A).\nAdditionally, it follows from Theorem 6.5 that\nk\u2211\ni=1\n(\u03c3i(A)\u2212 \u03c3i(B))2 + p \u2211\nj=k+1\n\u03c32j (B) \u2264 \u2016A\u2212B\u20162F \u2264 \u03b72 + p \u2211\nj=k+1\n\u03c32j (A),\nwhich leads to the result.\nLet us apply Theorem 9.5 to Theorem 9.4 to establish a spectral\nnorm error bound. It follows from Theorem 9.4 that\n\u2016A\u2212Ak\u2016F \u2264 \u2016A\u2212QBk\u2016F \u2264 \u2016A\u2212QQTAk\u2016F .\nConsider that\n\u2016A\u2212QQTAk\u20162F = \u2016A\u2212Ak +Ak \u2212QQTAk\u20162F = \u2016(Im \u2212QQT )Ak\u20162F + \u2016A\u2212Ak\u20162F\ndue to (A\u2212Ak)ATk (Im \u2212QQT ) = 0. Thus,\n\u2016A\u2212QBk\u20162F \u2264 \u2016(Im \u2212QQT )Ak\u20162F + n\u2211\ni=k+1\n\u03c32i (A).\nBy Theorem 9.5, we have that\n\u2016A\u2212QBk\u201622 \u2264 \u2016(Im \u2212QQT )Ak\u20162F + \u03c32k+1(A),\nwhich can give an error bound in the spectral norm."}, {"heading": "9.2 Approximate Matrix Multiplication", "text": "Given matrices A \u2208 Rn\u00d7d and B \u2208 Rn\u00d7p, it is well known that the complexity of computing ATB is O(dnp). Approximate matrix multiplication aims to obtain a matrix C \u2208 Rd\u00d7p with o(dnp) time complexity such that for a small \u03b5 > 0,\n\u2016ATB\u2212C\u2016 \u2264 \u03b5\u2016A\u2016\u2016B\u2016."}, {"heading": "9.2. Approximate Matrix Multiplication 83", "text": "This shows that approximate matrix multiplication can be viewed as an inverse process of the conventional matrix low rank approximation problem.\nApproximate matrix multiplication is a potentially important approach for fast matrix multiplication [Drineas et al., 2006a, Clarkson and Woodruff, 2009, Cohen and Lewis, 1999, Kane and Nelson, 2014, Drineas et al., 2011b, Nelson and Nguy\u00ean, 2013, Clarkson and Woodruff, 2013]. It is the foundation of approximate least square methods and matrix low rank approximation methods [Sarlos, 2006, Halko et al., 2011, Kyrillidis et al., 2014, Martinsson et al., 2011, Woolfe et al., 2008, Magdon-Ismail, 2011, Magen and Zouzias, 2011, Cohen and Lewis, 1999, Kane and Nelson, 2014, Drineas et al., 2011b, Nelson and Nguy\u00ean, 2013, Clarkson and Woodruff, 2013]. Moreover, it can be also used in large scalable k-means clustering [Cohen et al., 2014], approximate leverage scores [Drineas et al., 2011a], etc.\nMost of work for matrix approximations is based on error bounds w.r.t. the Frobenius norm [Drineas et al., 2006a, Sarlos, 2006, Cohen and Lewis, 1999, Kane and Nelson, 2014, Drineas et al., 2011b, Nelson and Nguy\u00ean, 2013, Clarkson and Woodruff, 2013]. In contrast, there is a few work based on spectral-norm error bounds [Halko et al., 2011, Kyrillidis et al., 2014, Martinsson et al., 2011, Woolfe et al., 2008, Magdon-Ismail, 2011, Magen and Zouzias, 2011]. As we have mentioned earlier, spectral-norm error bounds are also of great interest.\nIn approximate matrix multiplication, oblivious subspace embedding matrix is a key ingredient. For example, gaussian matrix and random sign matrix are oblivious matrix. However, leverage score sketching matrix depends on data matrix, hence, it is not an oblivious subspace embedding matrix.\nDefinition 9.1. [Woodruff, 2014b] Given \u03b5 > 0 and \u03b4 > 0, let \u03a0 be a distribution on l\u00d7 n matrices, where l relies on n, d, \u03b5 and \u03b4. Suppose that with probability at lest 1 \u2212 \u03b4, for any fixed n \u00d7 d matrix A, a matrix S drawn from distribution \u03a0 is a (1+\u03b5) \u21132-subspace embedding for A, that is, for all x \u2208 Rd, \u2016SAx\u201622 = (1\u00b1 \u03b5)\u2016Ax\u201622 with probability 1\u2212 \u03b4. Then we call \u03a0 an (\u03b5, \u03b4)-oblivious \u21132-subspace embedding,"}, {"heading": "84 Matrix Low Rank Approximation", "text": "Recently, Cohen et al. [2015] proved optimal approximate matrix multiplication in terms of stable rank by using subspace embedding [Batson et al., 2014].\nTheorem 9.6. [Cohen et al., 2015] Given \u03b5, \u03b4 \u2208 (0, 1/2), let A and B be two conforming matrices, and \u03a0 be a (\u03b5, \u03b4) subspace embedding for the 2r\u0303-dimensional subspace, where r\u0303 is the maximum of the stable ranks of A and B. Then,\n||(\u03a0A)T (\u03a0B) \u2212ATB|| \u2264 \u03b5||A||||B||\nholds with at least 1\u2212 \u03b4.\nTo analyze approximate matrix multiplication with the Frobenius\nerror, Kane and Nelson [2014] introduced the JL-moment property.\nDefinition 9.2. A distribution D over Rn\u00d7d has the (\u03b5, \u03b4, \u2113)-JL moment property if for all x \u2208 Rd with \u2016x\u20162 = 1,\nE\u03a0\u223cD \u2223 \u2223 \u2223\u2016\u03a0x\u201622 \u2212 1 \u2223 \u2223 \u2223 \u2113 \u2264 \u03b5\u2113 \u00b7 \u03b4\nBased on the JL-moment property, these is an approximate matrix\nmultiplication method with the Frobenius error.\nTheorem 9.7. Given \u03b5, \u03b4 \u2208 (0, 1/2), let A and B be two conforming matrices, and \u03a0 be a matrix satisfying the (\u03b5, \u03b4, \u2113)-JL moment property for some \u2113 \u2265 2. Then,\n||(\u03a0A)T (\u03a0B) \u2212ATB||F \u2264 \u03b5||A||F ||B||F\nholds with at least 1\u2212 \u03b4.\nNote that both the subspace embedding property and the JL moment property have close relationships. More specifically, they can be converted into each other [Kane and Nelson, 2014].\nThere are other methods, which do not use subspace embedding matrices, in the literature. Magen and Zouzias [2011] gave a method based on columns selection. Bhojanapalli et al. [2015] proposed a new method with sampling and alternating minimization to directly compute a low-rank approximation to the product of two given matrices."}, {"heading": "9.2. Approximate Matrix Multiplication 85", "text": "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].\n10\nLarge-Scale Matrix Approximation\nIn this chapter we discuss fast computational methods of the SVD, kernel methods, and CUR decomposition via randomized approximation. The goal is to make the matrix factorizations fill the use on large scale data matrices.\nIt is notoriously difficult to compute SVD because the exact SVD of an m \u00d7 n matrix takes O(mnmin{m,n}) time. Fortunately, many machine learning methods such as latent semantic indexing [Deerwester et al., 1990], spectral clustering [Shi and Malik, 2000], manifold learning [Tenenbaum et al., 2000, Belkin and Niyogi, 2003] are interested in only the top singular value triples. The Krylov subspace method computes the top k singular value triples in O\u0303(mnk) time [Saad, 2011, Musco and Musco, 2015], where the O\u0303 notation hides the logarithm factors and the data dependent condition number. If a low precision solution suffices, the time complexity can be even lower. Here we will make main attention on randomized approximate algorithms that demonstrate high scalability. Randomized algorithms are a feasible approach for large scale machine learning models [Rokhlin et al., 2009, Mahoney, 2011, Tu et al., 2014]. In particular, we will consider randomized SVD methods [Halko et al., 2011].\n86"}, {"heading": "10.1. Randomized SVD 87", "text": "In contrast to the randomized SVD which is based on random projection, the CUR approximation mainly employs column selection. Column selection has been extensively studied in the theoretical computer science (TCS) and numerical linear algebra (NLA) communities. The work in TCS mainly focuses on choosing good columns by randomized algorithms with provable error bounds [Frieze et al., 2004, Deshpande et al., 2006, Drineas et al., 2008, Deshpande and Rademacher, 2010, Boutsidis et al., 2014, Guruswami and Sinop, 2012]. The focus in NLA is then on deterministic algorithms, especially the rank-revealing QR factorizations, that select columns by pivoting rules [Foster, 1986, Chan, 1987, Stewart, 1999, Bischof and Hansen, 1991, Hong and Pan, 1992, Chandrasekaran and Ipsen, 1994, Gu and Eisenstat, 1996, Berry et al., 2005]."}, {"heading": "10.1 Randomized SVD", "text": "All the randomized SVD algorithms essentially have the same idea: first draw a random projection matrix \u2126 \u2208 Rn\u00d7c, then form the sketch C = A\u2126 \u2208 Rm\u00d7c and compute its orthonormal bases Q \u2208 Rm\u00d7c, and finally compute a rank k matrix X \u2208 Rc\u00d7n such that \u2016A \u2212 QX\u20162\u03be is small compared to \u2016A\u2212Ak\u20162\u03be . Here \u2016 \u00b7 \u2016\u03be denotes either the Frobenius norm or the spectral norm.\nThe following lemma is the foundation in theoretical analysis of the\nrandomized SVD [Halko et al., 2011, Gu, 2015]. Lemma 10.1. Let A \u2208 Rm\u00d7n be a given matrix, and Z \u2208 Rn\u00d7k be column orthonormal. Let \u2126 \u2208 Rn\u00d7c be any matrix such that rank(ZT\u2126) = rank(Z) = k, and define C = A\u2126 \u2208 Rm\u00d7c . Then\n\u2016A\u2212\u03a0\u03beC,k(A)\u20162\u03be \u2264 \u2016E\u20162\u03be + \u2016E\u2126(ZT\u2126)\u2020\u20162\u03be ,\nwhere E = A\u2212AZZT , and \u03a0\u03beC,k(A) \u2208 Rm\u00d7n denotes the best approximation to A within the column space of C that has rank at most k w.r.t. the norm \u2016 \u00b7 \u2016\u03be loss. Proof. In terms of definition of \u03a0\u03beC,k(A), we have\n\u2016A\u2212\u03a0\u03beC,k(A)\u20162\u03be \u2264 \u2016A\u2212X\u20162\u03be"}, {"heading": "88 Large-Scale Matrix Approximation", "text": "for all matrices X \u2208 Rm\u00d7n of rank at most k in the column space of C. Obviously, C(ZT\u2126)\u2020ZT is such a matrix. Thus,\n\u2016A\u2212\u03a0\u03beC,k(A)\u20162\u03be \u2264 \u2016A\u2212C(ZT\u2126)\u2020ZT \u20162\u03be = \u2016A\u2212AZZT +AZZT \u2212C(ZT\u2126)\u2020ZT \u20162\u03be = \u2016E + (AZZT \u2212A)\u2126(ZT\u2126)\u2020ZT \u20162\u03be = \u2016E +E\u2126(ZT\u2126)\u2020ZT \u20162\u03be .\nHere we use the fact that ZT\u2126(ZT\u2126)\u2020 = Ik because rank(ZT\u2126) = k. Consider that\nE\u2126(ZT\u2126)\u2020ZTET = E\u2126(ZT\u2126)\u2020ZT (AT \u2212 ZZTAT ) = 0.\nThe theorem follows from Theorem 7.11.\nConsider the rank-k truncated SVD Ak = Uk\u03a3kV T k . Then we can\nwrite A as\nA = AVkV T k + (A\u2212Ak).\nLet Z = Vk and E = A \u2212 Ak in Lemma 10.1. Then the following theorem is an immediate corollary of Lemma 10.1.\nTheorem 10.2. Let A = U\u03a3VT be the full SVD of A \u2208 Rm\u00d7n, fix k \u2265 0, and let Ak = Uk\u03a3kV T k be the best at most rank k approximation of A. Choose a test matrix \u2126 and construct the sketch C = A\u2126.\nPartition \u03a3 =\n[\n\u03a3k 0\n0 \u03a3\u2212k\n]\nand V = [Vk,V\u2212k]. Define \u21261 = VTk\u2126 and\n\u21262 = V T \u2212k\u2126. Assume that \u21261 has full row rank. Then\n\u2016(Im \u2212CC\u2020)A\u20162\u03be \u2264 \u2016A\u2212\u03a0\u03beC,k(A)\u20162\u03be \u2264 \u2016\u03a3\u2212k\u20162\u03be + \u2016\u03a3\u2212k\u21262\u2126 \u2020 1\u20162\u03be .\nIn Lemma 10.1 and Theorem 10.2, the condition rank(VTk\u2126) = rank(Vk) = k is essential for an effective randomized SVD algorithm. An idealized case for meeting this condition is that range(Vk) \u2282 range(\u2126). In this case, the randomized SVD degenerates an exact truncated SVD procedure. Thus, the above condition aims to relax this idealized case. Moreover, the key for an effective randomized SVD is to select a test matrix \u2126 such that the condition rank(VTk\u2126) = rank(Vk) = k holds as much as possible. Lemma 10.1 and Theorem 10.2 are also fundamental in random column selection [Boutsidis et al., 2014]."}, {"heading": "10.1. Randomized SVD 89", "text": ""}, {"heading": "10.1.1 Randomized SVD: Frobenius Norm Bounds", "text": "In this subsection, we describe two randomized SVD algorithms which have (1 + \u01eb) relative-error bound.\nRandom Projection. In order to reduce computational expenses, randomized algorithms [Frieze et al., 2004, Vempala, 2000] have been introduced to truncated SVD and low-rank approximation. The Johnson & Lindenstrauss (JL) transform [Johnson and Lindenstrauss, 1984, Dasgupta and Gupta, 2003] is known to keep isometry in expectation or with high probability. Halko et al. [2011], Boutsidis et al. [2014] used the JL transform for sketching and showed relative-error bounds. However, the Gaussian test matrix is dense and cannot efficiently apply to matrices. Several improvements have been proposed to make the sketching matrix sparser; see the review [Woodruff, 2014b] for the complete list of the literature. In particular, the count sketch [Clarkson and Woodruff, 2013] applies to A in only O(nnz(A)) time and exhibits very similar properties as the JL transform. Specifically, Woodruff [2014b] showed that an m\u00d7O(k/\u01eb) sketch C = A\u2126 can be obtained in O(nnz(A)) time and\nmin rank(X)\u2264k\n\u2225 \u2225A\u2212QX \u2225 \u2225 2\nF \u2264 (1 + \u01eb) \u2016A\u2212Ak\u20162F (10.1)\nholds with high probability.\nThe Prototype Algorithm. Halko et al. [2011] proposed to directly solve the left-hand side of (10.1), which has closed-form solution X\u22c6 = (QTA)k. This leads to the prototype algorithm shown in Algorithm 1. The optimality of X\u22c6 is given in Theorem 9.4.\nThe prototype algorithm is not time efficient because the matrix product QTA costs O(mnc) time, which is not lower than the exact solutions. Nevertheless, the prototype algorithm is still useful in largescale applications because it is pass-efficient\u2014it goes only two passes through A.\nFaster Randomized SVD. The bottleneck of the prototype algorithm is the matrix product in computing X\u22c6. Notice that (9.4) is a strongly over-determined system, so it can be approximately solved by once more random projection. Let P = P1P2 \u2208 Rm\u00d7p be another random projection matrix, where P1 is a count sketch and P2 is a JL"}, {"heading": "90 Large-Scale Matrix Approximation", "text": "Algorithm 1 Randomized SVD: The Prototype Algorithm.\n1: Input: a matrix A \u2208 Rm\u00d7n with m \u2265 n, target rank k, the size of sketch c where 0 < k \u2264 c < n; 2: Draw a sketching matrix \u2126 \u2208 Rn\u00d7c, e.g. a Gaussian test matrix or a count sketch 3: Compute C = A\u2126 \u2208 Rm\u00d7c and its orthonormal bases Q \u2208 Rm\u00d7c; 4: Compute the rank k truncated SVD: QTA \u2248 U\u0304k\u03a3\u0303kV\u0303Tk ; 5: return U\u0303k = QU\u0304k, \u03a3\u0303k, V\u0303k\u2014an approximate rank-k truncated\nSVD of A.\ntransform matrix. Then we solve\nX\u0303 = min rank(X)\u2264k\n\u2016PT (A\u2212QX)\u20162F\ninstead of (9.4), and X\u0303 has closed-form solution\nX\u0303 = R\u0303\u2020(Q\u0303TPTA)k,\nwhere Q\u0303R\u0303 be the economy size QR decomposition of (PTQ) \u2208 Rp\u00d7c. Finally, the rank k matrix QX\u0303 is the obtained approximation to A, and its SVD can be very efficiently computed. Clarkson and Woodruff [2013], Woodruff [2014b] showed that\n\u2225 \u2225A\u2212QR\u0303\u2020(Q\u0303TPTA)k \u2225 \u2225 2 F \u2264 (1 + \u01eb) \u2016A\u2212Ak\u20162F\nfor a large enough p, and the overall time cost is O(nnz(A) + (m + n)poly(k/\u01eb))."}, {"heading": "10.1.2 Randomized SVD: Spectral Norm Bounds", "text": "The previous section shows that the approximate truncated SVD can be computed highly efficiently, with the (1+\u01eb) Frobenius relative-error guaranteed. The Frobenius norm bound tells that the total elementwise distance is small, but it does not inform us the closeness of their singular vectors. Therefore, we need spectral norm bounds or even stronger principal angle bounds; here we only consider the former. We seek to find an m\u00d7 k column orthogonal matrix U\u0303 such that\n\u2225 \u2225A\u2212 U\u0303U\u0303TA \u2225 \u2225 2\n2 \u2264 \u03b7\u2016A\u2212Ak\u201622,"}, {"heading": "10.1. Randomized SVD 91", "text": "where \u03b7 will be specified later. The Prototype Algorithm. Unlike the Frobenius norm bound, the prototype algorithm is unlikely to attain a constant factor bound (i.e., \u03b7 is independent of m, n), letting alone the 1 + \u01eb bound. It is because the lower bounds [Witten and Cand\u00e8s, 2013, Boutsidis et al., 2014] showed that if \u2126 \u2208 Rn\u00d7c in Algorithm 1 is the Gaussian test matrix or any column selection matrix, the order of \u03b7 must be at least n/c. We apply Gu\u2019s theorem [Gu, 2015] (Theorem 9.5) to obtain an O(n)factor spectral norm bound, and then introduce iterative algorithms with the (1+\u01eb) spectral norm bound.\nLet U\u0303k, \u03a3\u0303k, and V\u0303k be the outputs of Algorithm 1. We have that \u2225 \u2225A\u2212 U\u0303kU\u0303TkA \u2225 \u2225 2 F \u2264 \u2225 \u2225A\u2212 U\u0303k\u03a3\u0303kV\u0303Tk \u2225 \u2225 2 F\n= \u2225 \u2225A\u2212QX\u22c6 \u2225 \u2225 2\nF \u2264 (1 + \u01eb) \u2016A\u2212Ak\u20162F ,\nwhere the first inequality follows from Theorem 9.1, the equality follows from the definitions, and the second inequality follows from (10.1) provided that c = O(k/\u01eb) and \u2126 is the Gaussian test matrix or the count sketch. We let \u01eb = 1 and c = O(k) and apply Theorem 9.5 to obtain\n\u2225 \u2225A\u2212 U\u0303kU\u0303TkA \u2225 \u2225 2\n2 \u2264 \u2016A\u2212Ak\u201622 + \u2016A\u2212Ak\u20162F \u2264 (n \u2212 k + 1)\u2016A\u2212Ak\u201622. (10.2)\nHere the second inequality follows from that \u2016A\u2212Ak\u20162F = \u2211n i=k+1 \u03c3 2 i \u2264 (n \u2212 k)\u03c32k+1 = (n \u2212 k)\u2016A \u2212 Ak\u201622. To this end, we have shown that the prototype algorithm 1 satisfies O(n)-factor spectral norm bound. However, the result itself has little meaning.\nThe Simultaneous Power Iteration can be used to refine the sketch [Halko et al., 2011, Gu, 2015]. The algorithm is described in Algorithm 2 and analyzed in the following. Let\u2126 \u2208 Rn\u00d7c be a Gaussian test matrix or count sketch andB = (AAT )tA. Let us takeB instead of A as the input of the prototype algorithm 1 and obtain the approximate left singular vectors U\u0303k. It is easy to verify that U\u0303k is the same to the output of Algorithm 2. We will show that when t = O( logn\u01eb ),\n\u2225 \u2225A\u2212 U\u0303kU\u0303TkA \u2225 \u2225 2 2 \u2264 (1 + \u01eb)\u2016A\u2212Ak\u201622. (10.3)\nTo show this result, we need the lemma of Halko et al. [2011]."}, {"heading": "92 Large-Scale Matrix Approximation", "text": "Algorithm 2 Subspace Iteration Methods.\n1: Input: any matrix A \u2208 Rm\u00d7n, the target rank k, the size of sketch c where 0 < k \u2264 c < n; 2: Generate an n \u00d7 c Gaussian test matrix \u2126 and perform sketching C(0) = A\u2126; 3: for i = 1 to t do 4: Optional: orthogonalize C(i\u22121); 5: Compute C(i) = AATC(i\u22121); 6: end for 7: The Power Method: orthonalize C(t) to obtain Q \u2208 Rm\u00d7c; 8: The Krylov Subspace Method: orthonalize K =\n[C(0), \u00b7 \u00b7 \u00b7 ,C(t)] to obtain Q \u2208 Rm\u00d7(t+1)c; 9: Compute the rank k truncated SVD: QTA \u2248 U\u0304k\u03a3\u0303kV\u0303Tk ;\n10: return U\u0303k = QU\u0304k, \u03a3\u0303k, V\u0303k\u2014an approximate rank-k truncated SVD of A.\nLemma 10.3 (Halko, Martinsson, & Tropp). Let A be any matrix and U have orthonormal columns. Then for any positive integer t,\n\u2225 \u2225(I\u2212UUT )A \u2225 \u2225 2 \u2264 \u2225 \u2225(I\u2212UUT )(AAT )tA \u2225 \u2225 1/(2t+1) 2 .\nBy Lemma 10.3, we have that\n\u2225 \u2225(I\u2212 U\u0303kU\u0303Tk )A \u2225 \u2225 2 2 \u2264 \u2225 \u2225(I \u2212 U\u0303kU\u0303Tk )B \u2225 \u2225 2/(2t+1) 2\n\u2264 (n\u2212 k + 1)1/(2t+1)\u03c32/(2t+1)k+1 (B) = (1 + \u01eb)\u03c32k+1(A).\nHere the second inequality follows from (10.2) and the definitions of B and U\u0303k, and we show the equality in the following. Let 2t + 1 = log(n\u2212k+1)\n0.5\u01eb . We have that 1 2t+1 log(n\u2212 k+1) = 0.5\u01eb \u2264 log(1+ \u01eb), where the inequality holds for all for all \u01eb \u2208 [0, 1]. Taking the exponential of both sides, we have (n \u2212 k + 1)1/(2t+1) \u2264 1 + \u01eb. Finally, (10.3) follows from that \u03c32k+1(A) = \u2016A\u2212Ak\u201622.\nThe Krylov Subspace Method. From Algorithm 2 we can see that the power iteration repeats t times, but only the output of the last iteration C(t) is used. In fact, the intermediate resultsC(0), \u00b7 \u00b7 \u00b7 ,C(t) are"}, {"heading": "10.2. Kernel Approximation 93", "text": "also useful. The matrix K = [C(0), \u00b7 \u00b7 \u00b7 ,C(t)] \u2208 Rm\u00d7(t+1)c is well known as the Krylov matrix, and range(K) is called the Krylov subspace. We show the Krylov subspace method in Algorithm 2, which differs from simultaneous power iteration in only one line. It turns out that the Krylov subspace method converges much faster than the power iteration [Saad, 2011]. Very recently, Musco and Musco [2015] showed that with t = logn\u221a\n\u01eb power iteration, the 1+\u01eb spectral norm bound (10.3)\nholds with high probability. This result is evidently stronger than the simultaneous power iteration.\nIt is worth mentioning that the Krylov subspace method described in Algorithm 2 is a simplified version, and it may be instable when t is large. This is because the columns of C(0), \u00b7 \u00b7 \u00b7 ,C(t) tend to be linearly dependent as t grows. In practice, re-orthogonalization or partial re-orthogonalization are employed to prevent the instability from happening [Saad, 2011]."}, {"heading": "10.2 Kernel Approximation", "text": "Kernel methods are important tools in machine learning, computer vision, and data mining [Sch\u00f6lkopf and Smola, 2002, Shawe-Taylor and Cristianini, 2004, Vapnik, 1998, Rasmussen and Williams, 2006]. For example, kernel ridge regression (KRR), Gaussian processes, kernel support vector machine (KSVM), spectral clustering, and kernel principal component analysis (KPCA) are classical nonlinear models for regression, classification, clustering, and dimensionality regression. Unfortunately, the lack of scalability has always been the major drawback of kernel methods. The three steps of most kernel methods\u2014forming the kernel matrix, training, generalization\u2014can all be prohibitive in big-data applications.\nSpecifically, suppose we are given n training data and m test data, all of d dimension. Firstly, it takes O(n2d) time to form an n \u00d7 n kernel matrix K, e.g., the Gaussian RBF kernel matrix. Secondly, the training requires either SVD or matrix inversion of the kernel matrix. For example, spectral clustering, KPCA, Isomap [Tenenbaum et al., 2000], and Laplacian eigenmaps [Belkin and Niyogi, 2003] compute the"}, {"heading": "94 Large-Scale Matrix Approximation", "text": "top k singular vectors of the (normalized) kernel matrix, where k is the number of classes or the target dimensionality. This costs O(n2k) time and O(n3) memory. Thirdly, to generalize the trained model to the test data, kernel methods such as KRR, KSVM, KPCA cost O(nmd) time to form an n \u00d7 m cross kernel matrix between the training and test data. If m is as large as n, generalization is as challenging as training.\nLow rank approximation is the most popular approach to scalable kernel approximation. If we have the low rank approximation K \u2248 CXCT , then the approximate eigenvalue decomposition can be immediately obtained by\nK \u2248 CXCT = UC (\u03a3CVTCXVC\u03a3C) \ufe38 \ufe37\ufe37 \ufe38\n=Z\nUTC = (UCUZ)\u039bZ(UCUZ) T .\nHere C = UC\u03a3CVTC is the SVD and Z = UZ\u039bZU T Z is the spectral decomposition. Since the tall-and-skinny matrix UCUZ has orthonormal columns and the diagonal entries of \u039bZ are in the descending order, the leftmost columns of UCUZ are approximately the top singular vectors of K. This approach only costs O(nc2) time, where c is the number of columns of C. Our objective is thereby to find such a low rank approximation.\nDifference from Randomized SVD. Why cannot we directly use the randomized SVD to approximate the kernel matrix? The randomized SVD assumes that the matrix is fully observed; unfortunately, this is not true for kernel methods. When the number of data samples is million scale, even forming the kernel matrix is impossible. Therefore, the primary objective of kernel approximation is to avoid forming the whole kernel matrix. The existing random projection methods all require the full observation of the matrix, so random projection is not a feasible option. We must use column selection in the kernel approximation problem.\nThe Prototype Algorithm. Let S be an n \u00d7 c sketching matrix and let C = KS. It remains to find the c \u00d7 c intersection matrix X. The most intuitive approach is to minimize the approximation error by\nX\u22c6 = argmin X\n\u2225 \u2225K\u2212CXCT \u2225 \u2225 2 F = C\u2020K(C\u2020)T , (10.4)\nwhere the second equality follows from Theorem 9.2. This method was"}, {"heading": "10.2. Kernel Approximation 95", "text": "proposed by Halko et al. [2011] for approximating symmetric matrix. Wang et al. [2014a] showed that by randomly samplingO(k/\u01eb) columns of K to form C by a certain algorithm, the approximation is high accurate:\n\u2225 \u2225K\u2212CX\u22c6CT \u2225 \u2225 2\nF \u2264 (1 + \u01eb)\n\u2225 \u2225K\u2212Kk \u2225 \u2225 2\nF .\nThis upper bound matches the lower bound c \u2265 2k/\u01eb up to a constant factor [Wang et al., 2014a]. Unfortunately, the prototype algorithm has two obvious drawbacks. Firstly, to compute the intersection matrix X\u22c6, every entry of K must be known. As is discussed, it takes O(n2d) time to form the kernel matrix K. Secondly, the matrix multiplication C\u2020K costs O(n2c) time. In sum, the prototype algorithm costs O(n2c + n2d) time. Although it is substantially faster than the exact solution, the prototype algorithm has the same time complexity as the exact solution.\nFaster SPSD Matrix Sketching. Since C = KS has much more rows than columns, the optimization problem (10.4) is strongly overdetermined. Wang et al. [2015b] proposed to use sketching to approximately solve (10.4). Specifically, let P be a certain n\u00d7p column selection matrix with p \u2265 c and compute\nX\u0303 = argmin X\n\u2225 \u2225PT (K\u2212CXCT )P \u2225 \u2225 2 F = (PTC)\u2020(PTKP)(CTP)\u2020.\nIn this way, we need only nc+p2 entries of K to form the approximation K \u2248 CX\u0303CT . The intersection matrix X\u0303 can be computed in O(ncd+ p2d+ p2c) time, given S and n data points of d dimension. Wang et al. [2015b] devised an algorithm that sets p = \u221a nc/ \u221a \u01eb and very efficiently forms the column selection matrix P; and the following error bound holds with high probability:\n\u2225 \u2225K\u2212CX\u0303CT \u2225 \u2225 2\nF \u2264 (1 + \u01eb) min\nX\n\u2225 \u2225K\u2212CXCT \u2225 \u2225 2\nF .\nBy this choice of p, the overall time cost is linear in n. Motivated by the matrix ridge approximation of Zhang [2014], Wang et al. [2014b] proposed a spectral shifting kernel approximation method. When the spectrum of K decays slowly, the shifting term helps to improve the approximation accuracy and numerical stability. Wang et al. [2014a] also showed that the spectral shifting approach"}, {"heading": "96 Large-Scale Matrix Approximation", "text": "can be used to improve other kernel approximation models such as the memory efficient kernel approximation (MEKA) model [Si et al., 2014].\nThe Nystr\u00f6m Method is the most popular kernel approximation approach. It is named after its inventor Nystr\u00f6m [1930] and gained its popularity in the machine learning society after its application in Gaussian procession regression [Williams and Seeger, 2001]. Let S be a column selection matrix,C = KS, andW = STKS. The Nystr\u00f6m method approximates K by CW\u2020CT . In fact, the Nystr\u00f6m method is a special case of the faster SPSD matrix sketching where P and S are equal. This also indicates that the Nystr\u00f6m method is an approximate solution to (10.4). Gittens and Mahoney [2013] offered comprehensive error analysis of the Nystr\u00f6m method. The Nystr\u00f6m method has been applied to solve million scale kernel methods [Talwalkar et al., 2013]. But unlike the faster SPSD matrix sketching, the Nystr\u00f6m method cannot generate high quality approximation. The lower bound [Wang and Zhang, 2013] indicates that the Nystr\u00f6m method cannot attain (1+\u01eb) relativeerror bound unless it is willing to spend \u2126(n2k/\u01eb) time.\nTo this end, we have shown how to efficiently approximate any kernel matrix and use the obtained low rank approximation to speed up training. We will introduce efficient generalization using the CUR matrix decomposition in the next section."}, {"heading": "10.3 The CUR Approximation", "text": "Let A by any m\u00d7n matrix. The CUR matrix decomposition is formed by selecting c columns of A to form C \u2208 Rm\u00d7c, r rows to form R \u2208 Rr\u00d7n, and computing an intersection matrix U \u2208 Rc\u00d7r such that CUR \u2248 A. In this section, we first discussion the motivations and then describe algorithms and error analyses.\nMotivations. Firstly, let us continue the generalization problem of kernel methods which remains unsolved in the previous section. Suppose we are given n training data andm test data, all of d dimension. To generalize the trained model to the test data, supervised kernel methods such as Gaussian processes and KRR require evaluating the kernel function of every train and test data pair\u2014that is to form an m \u00d7 n"}, {"heading": "10.3. The CUR Approximation 97", "text": "cross kernel matrixK\u2217\u2014which costs O(mnd) time. By the fast CUR algorithm described later in this section, the approximation K\u2217 \u2248 CUR can be obtained in time linear in d(m+n). With such a decomposition at hand, the matrix product K\u2217M \u2248 CURM can be computed in O(nrk+mck) time. In this way, the overall time cost of generalization is linear in m+ n.\nSecondly, CUR forms a compressed representation of the data matrix, as well as the truncated SVD, and it can be very efficiently converted to the SVD-like form:\nA \u2248 CUR = UC \u03a3CVTCUUR\u03a3R \ufe38 \ufe37\ufe37 \ufe38\n=B\nVTR = (UCUB)\u03a3B(VRVB) T .\nHere C = UC\u03a3CVTC , R = UR\u03a3RV T R, B = UB\u03a3BVR are the SVD. Since CUR is formed by sampling columns and rows, it preserves the sparsity and nonnegativity of the original data matrix. The sparsity makes CUR cheaper to store than SVD, and the nonnegativity makes CUR a nonnegative matrix factorization.\nThirdly, CUR consists of the actual columns and rows, and thus it enables human to to understand and interpret the data. In comparison, the basis vectors of SVD has little concrete meaning. An example of Drineas et al. [2008] and Mahoney and Drineas [2009] has well shown this viewpoint; that is, the vector [(1/2)age \u2212 (1/ \u221a 2)height + (1/2)income], the sum of the significant uncorrelated features from a data set of people\u2019s features, is not particularly informative. Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix.\nColumn Selection. Several different column selection strategies have been devised, among which the leverage score sampling [Drineas et al., 2008] and the adaptive sampling [Wang and Zhang, 2013, Boutsidis and Woodruff, 2014] attain relative error bounds. In particular, Boutsidis and Woodruff [2014] showed that with c = O(k/\u01eb) columns and r = O(k/\u01eb) rows selected by adaptive sampling to form C and R,\nmin X \u2016A\u2212CXR\u20162F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u20162F holds in expectation. A further refinement was developed by Woodruff"}, {"heading": "98 Large-Scale Matrix Approximation", "text": "[2014b]. We will not go to the details of the leverage score sampling or adaptive sampling. The users only need to know that such algorithms randomly sample columns/rows according to some non-uniform distributions. Unfortunately, it requires observing the whole matrix A to compute such non-uniform distributions, thus such column selection algorithms cannot be applied to speed up computation. It remains an open problem whether there is a relative-error sampling algorithm that needs not observing the whole of A. In practice, the users can simply sample columns/rows uniformly without replacement, which usually has acceptable empirical performance.\nThe Intersection Matrix. With the selected columns C and rows R at hand, we can simply compute the intersection matrix by\nU\u22c6 = argmin U\n\u2225 \u2225A\u2212CUR \u2225 \u2225 2 F = C\u2020AR\u2020. (10.5)\nHere the second equality follows from Theorem 9.2. This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014]. This approach is very similar to the prototype SPSD matrix approximation method in the previous section, and it costs at least O(mn\u00b7min{c, r}) time and requires observing every entry of A. Apparently, it cannot help speed up matrix computation.\nWang et al. [2015a] proposed a more practical CUR decomposition method which solves (10.5) approximately. The method first draws two column selection matrices PC \u2208 Rm\u00d7pc and PR \u2208 Rn\u00d7pr (pc, pr \u2265 c, r), which costs O(mc2+nr2) time. It then computes the intersection matrix by\nU\u0303 = argmin U\n\u2225 \u2225PTC(A\u2212CUR)PR \u2225 \u2225 2 F = (PTCC) \u2020(PTCAPR)(RPR) \u2020.\nThis method needs observing only pc\u00d7 pr entries of A, and the overall time cost is O(pcpr \u00b7min{c, r}+mc2 + nr2). When\npc \u2265 O ( c \u221a min{m,n}/\u01eb ) and pr \u2265 O ( r \u221a min{m,n}/\u01eb ) ,\nthe following inequality holds with high probability: \u2225 \u2225A\u2212CU\u0303R \u2225 \u2225 2\nF \u2264 (1 + \u01eb) min\nU\n\u2225 \u2225A\u2212CUR \u2225 \u2225 2\nF .\nIn sum, a high quality CUR decomposition can be computed in time linear in min{m,n}."}, {"heading": "Acknowledgements", "text": "I would like to thank my graduate students Cheng Chen, Luo Luo, Shusen Wang, Haishan Ye, and Qiaomin Ye. Specifically, Cheng Chen, Luo Luo and Qiaomin Ye helped to proofread the whole manuscript. Haishan Ye helped to revise Chapter 9.2, and Shusen Wang helped to revise Chapter 10. I would also like to thank other students who took my course \u201c Matrix Methods in Massive Data Analysis\u201d in the summer term 2015. They helped to improve the lecture notes, which provide the main materials for this tutorial.\n99"}], "references": [{"title": "Nystr\u00f6m approximation for large-scale determinantal processes", "author": ["Raja Hafiz Affandi", "Alex Kulesza", "Emily B. Fox", "Ben Taskar"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Affandi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2013}, {"title": "A kernel method for canonical correlation analysis", "author": ["S. Akaho"], "venue": "In International Meeting of Psychometric Society,", "citeRegEx": "Akaho.,? \\Q2001\\E", "shortCiteRegEx": "Akaho.", "year": 2001}, {"title": "Spectral analysis of data", "author": ["Yossi Azar", "Amos Fiat", "Anna Karlin", "Frank McSherry", "Jared Saia"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Azar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2001}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2002}, {"title": "Srivastave. Twice-Ramanujan sparsifiers", "author": ["J. Batson", "D. Spielman"], "venue": "SIAM Review,", "citeRegEx": "Batson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Batson et al\\.", "year": 2014}, {"title": "Generalized discriminant analysis using a kernel approach", "author": ["G. Baudat", "F. Anouar"], "venue": "Neural Computation,", "citeRegEx": "Baudat and Anouar.,? \\Q2000\\E", "shortCiteRegEx": "Baudat and Anouar.", "year": 2000}, {"title": "Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection", "author": ["P. Belhumeur", "J. Hespanha", "D. Kriegman"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Belhumeur et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Belhumeur et al\\.", "year": 1997}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural computation,", "citeRegEx": "Belkin and Niyogi.,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2003}, {"title": "Generalized Inverses: Theory and Applications", "author": ["A. Ben-Israel", "T.N.E. Greville"], "venue": "Second Edition. Springer,", "citeRegEx": "Ben.Israel and Greville.,? \\Q2003\\E", "shortCiteRegEx": "Ben.Israel and Greville.", "year": 2003}, {"title": "Algorithm 844: computing sparse reduced-rank approximations to sparse matrices", "author": ["M.W. Berry", "S.A. Pulatova", "G.W. Stewart"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Berry et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Berry et al\\.", "year": 2005}, {"title": "Tighter low-rank approximation via sampling the leveraged element", "author": ["Srinadh Bhojanapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2015}, {"title": "CUR from a sparse optimization viewpoint", "author": ["J. Bien", "Y. Xu", "M.W. Mahoney"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bien et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bien et al\\.", "year": 2010}, {"title": "Structure-preserving and rank-revealing QRfactorizations", "author": ["C.H. Bischof", "P.C. Hansen"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Bischof and Hansen.,? \\Q1991\\E", "shortCiteRegEx": "Bischof and Hansen.", "year": 1991}, {"title": "Convex Analysis and Nonlinear Optimization: Theory and Examples", "author": ["Jonathan M. Borwein", "Adrian S. Lewis"], "venue": null, "citeRegEx": "Borwein and Lewis.,? \\Q2006\\E", "shortCiteRegEx": "Borwein and Lewis.", "year": 2006}, {"title": "Optimal CUR matrix decompositions", "author": ["Christos Boutsidis", "David P. Woodruff"], "venue": "STOC, pages 353\u2013362,", "citeRegEx": "Boutsidis and Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Boutsidis and Woodruff.", "year": 2014}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Boutsidis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2014}, {"title": "Dimension reduction: A guided tour", "author": ["Christopher J.C. Burges"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Burges.,? \\Q2010\\E", "shortCiteRegEx": "Burges.", "year": 2010}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Cai et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Cai et al\\.", "year": 1956}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "The dantzig selector: Statistical estimation when p is much larger than n", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2007}, {"title": "Rank revealing QR factorizations", "author": ["T.F. Chan"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Chan.,? \\Q1987\\E", "shortCiteRegEx": "Chan.", "year": 1987}, {"title": "On rank-revealing factorisations", "author": ["S. Chandrasekaran", "I.C.F. Ipsen"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Chandrasekaran and Ipsen.,? \\Q1994\\E", "shortCiteRegEx": "Chandrasekaran and Ipsen.", "year": 1994}, {"title": "Numerical linear algebra in the streaming model", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2009\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Approximating matrix multiplication for pattern recognition tasks", "author": ["Edith Cohen", "David D Lewis"], "venue": "Journal of Algorithms,", "citeRegEx": "Cohen and Lewis.,? \\Q1999\\E", "shortCiteRegEx": "Cohen and Lewis.", "year": 1999}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["Michael Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu"], "venue": "arXiv preprint arXiv:1410.6801,", "citeRegEx": "Cohen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2014}, {"title": "Optimal approximate matrix product in terms of stable rank", "author": ["Michael B. Cohen", "Jelani Nelson", "David P. Woodruff"], "venue": "arXiv preprint arXiv:1507.02268,", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random Structure & Algorithms,", "citeRegEx": "Dasgupta and Gupta.,? \\Q2003\\E", "shortCiteRegEx": "Dasgupta and Gupta.", "year": 2003}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of The American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Applied Numerical Linear Algebra", "author": ["J. Demmel"], "venue": "SIAM, Philadelphia,", "citeRegEx": "Demmel.,? \\Q1997\\E", "shortCiteRegEx": "Demmel.", "year": 1997}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["A. Deshpande", "L. Rademacher"], "venue": "In Proceedings of the 51st IEEE Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Deshpande and Rademacher.,? \\Q2010\\E", "shortCiteRegEx": "Deshpande and Rademacher.", "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": "Theory of Computing,", "citeRegEx": "Deshpande et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2006}, {"title": "On the Nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Relative-error CUR matrix decompositions", "author": ["P. Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Fast monte carlo algorithms for matrices I: Approximating matrix multiplication", "author": ["Petros Drineas", "Ravi Kannan", "Michael W Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Sampling algorithms for l2 regression and applications", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2011}, {"title": "Faster least squares approximation", "author": ["Petros Drineas", "Michael W Mahoney", "S Muthukrishnan", "Tam\u00e1s Sarl\u00f3s"], "venue": "Numerische Mathematik,", "citeRegEx": "Drineas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2011}, {"title": "The approximation of one matrix by another of lower rank", "author": ["C. Eckart", "G. Young"], "venue": null, "citeRegEx": "Eckart and Young.,? \\Q1936\\E", "shortCiteRegEx": "Eckart and Young.", "year": 1936}, {"title": "A principal axis transformation for non-Hermitian matrices", "author": ["C. Eckart", "G. Young"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Eckart and Young.,? \\Q1939\\E", "shortCiteRegEx": "Eckart and Young.", "year": 1939}, {"title": "Maximum properties and inequalities for the eigenvalues of completely continuous operators", "author": ["Ky Fan"], "venue": "Proc. Nat. Acad. Sci. USA,", "citeRegEx": "Fan.,? \\Q1951\\E", "shortCiteRegEx": "Fan.", "year": 1951}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "author": ["Dan Feldman", "Melanie Schmidt", "Christian Sohler"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Feldman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2013}, {"title": "Rank and null space calculations using matrix decomposition without column interchanges", "author": ["L.V. Foster"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Foster.,? \\Q1986\\E", "shortCiteRegEx": "Foster.", "year": 1986}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Fowlkes et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fowlkes et al\\.", "year": 2004}, {"title": "Fast Monte Carlo algorithms for finding low-rank approximation", "author": ["A. Frieze", "K. Kannan", "Rademacher S. Vempala"], "venue": "Journal of the ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "Relative errors for deterministic low-rank matrix approximations", "author": ["Mina Ghashami", "Jeff M Phillips"], "venue": "In Proceedings of the Twenty-Fifth Annual ACMSIAM Symposium on Discrete Algorithms,", "citeRegEx": "Ghashami and Phillips.,? \\Q2014\\E", "shortCiteRegEx": "Ghashami and Phillips.", "year": 2014}, {"title": "Simultaneous diagonalization of rectangular complex matrices", "author": ["P.M. Gibson"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Gibson.,? \\Q1974\\E", "shortCiteRegEx": "Gibson.", "year": 1974}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Gittens and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Gittens and Mahoney.", "year": 2013}, {"title": "Molecular classification of cancer: class discovery and class prediction by gene expression monitoring", "author": ["T. Golub", "D. Slonim", "P. Tamayo", "C. Huard", "M. Gaasenbeek", "J. Mesirov", "H. Coller", "M. Loh", "J. Downing", "M. Caligiuri"], "venue": "Science, 286:531\u2013536,", "citeRegEx": "Golub et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1999}, {"title": "A theory of pseudoskeleton approximations", "author": ["S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Pseudo-skeleton approximations by matrices of maximal volume", "author": ["S.A. Goreinov", "N.L. Zamarashkin", "E.E. Tyrtyshnikov"], "venue": "Mathematical Notes,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Subspace iteration randomization and singular value problems", "author": ["Ming Gu"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Gu.,? \\Q2015\\E", "shortCiteRegEx": "Gu.", "year": 2015}, {"title": "Efficient algorithms for computing a strong rank-revealing QR factorization", "author": ["Ming Gu", "S.C. Eisenstat"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Gu and Eisenstat.,? \\Q1996\\E", "shortCiteRegEx": "Gu and Eisenstat.", "year": 1996}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["V. Guruswami", "A.K. Sinop"], "venue": "In Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Guruswami and Sinop.,? \\Q2012\\E", "shortCiteRegEx": "Guruswami and Sinop.", "year": 2012}, {"title": "Finding Structure with Randomness : Probabilistic Algorithms for Matrix Decompositions", "author": ["N Halko", "P G Martinsson", "J A Tropp"], "venue": "SIAM Review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Matrix completion and low-rank svd via fast alternating least squares", "author": ["Trevor Hastie", "Rahul Mazumder", "Jason Lee", "Reza Zadeh"], "venue": "arXiv preprint arXiv:1410.2596,", "citeRegEx": "Hastie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2014}, {"title": "The hat matrix in regression and ANOVA", "author": ["D.C. Hoaglin", "R.E. Welsch"], "venue": "The American Statistician,", "citeRegEx": "Hoaglin and Welsch.,? \\Q1978\\E", "shortCiteRegEx": "Hoaglin and Welsch.", "year": 1978}, {"title": "Rank-revealing QR factorizations and the singular value decomposition", "author": ["Y.P. Hong", "C.T. Pan"], "venue": "Mathematics of Computation,", "citeRegEx": "Hong and Pan.,? \\Q1992\\E", "shortCiteRegEx": "Hong and Pan.", "year": 1992}, {"title": "On the singular values of a product of completely continuous operators", "author": ["A. Horn"], "venue": "Proc. Nat. Acad. Sci. USA,", "citeRegEx": "Horn.,? \\Q1951\\E", "shortCiteRegEx": "Horn.", "year": 1951}, {"title": "On the eigenvalues of a matrix with prescribed singular values", "author": ["A. Horn"], "venue": "Proc. Amer. Math. Soc.,", "citeRegEx": "Horn.,? \\Q1954\\E", "shortCiteRegEx": "Horn.", "year": 1954}, {"title": "Matrix Analysis", "author": ["Roger A. Horn", "Charles R. Johnson"], "venue": null, "citeRegEx": "Horn and Johnson.,? \\Q1985\\E", "shortCiteRegEx": "Horn and Johnson.", "year": 1985}, {"title": "Topics in Matrix Analysis", "author": ["Roger A. Horn", "Charles R. Johnson"], "venue": null, "citeRegEx": "Horn and Johnson.,? \\Q1991\\E", "shortCiteRegEx": "Horn and Johnson.", "year": 1991}, {"title": "Structure preserving dimension reduction for clustered text data based on the generalized singular value decomposition", "author": ["P. Howland", "M. Jeon", "H. Park"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Howland et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Howland et al\\.", "year": 2003}, {"title": "Improved bound for the Nystr\u00f6m method and its application to kernel classification", "author": ["R. Jin", "T. Yang", "M. Mahdavi", "Y.F. Li", "Z.H. Zhou"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Jin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2013}, {"title": "Extensions of Lipschitz mapping into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Principal component analysis. Springer, New York, second edition", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "Jolliffe.,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "Sparser johnson-lindenstrauss transforms", "author": ["Daniel M Kane", "Jelani Nelson"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Kane and Nelson.,? \\Q2014\\E", "shortCiteRegEx": "Kane and Nelson.", "year": 2014}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Zhuoliang Kang", "Kristen Grauman", "Fei Sha"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "A new approach to feature selection based on the Karhunen-Lo\u00e8ve expansion", "author": ["J. Kittler", "P.C. Young"], "venue": "Pattern Recognition,", "citeRegEx": "Kittler and Young.,? \\Q1973\\E", "shortCiteRegEx": "Kittler and Young.", "year": 1973}, {"title": "Ensemble Nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Vector algebra in the analysis of genome-wide expression data", "author": ["F.G. Kuruvilla", "P.J. Park", "S.L. Schreiber"], "venue": "Genome Biology,", "citeRegEx": "Kuruvilla et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kuruvilla et al\\.", "year": 2002}, {"title": "Approximate matrix multiplication with application to linear embeddings", "author": ["Anastasios Kyrillidis", "Michail Vlachos", "Anastasios Zouzias"], "venue": "In Information Theory (ISIT),", "citeRegEx": "Kyrillidis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kyrillidis et al\\.", "year": 2014}, {"title": "The mathematics of eigenvalue optimization", "author": ["Adrian S Lewis"], "venue": "Mathematical Programming,", "citeRegEx": "Lewis.,? \\Q2003\\E", "shortCiteRegEx": "Lewis.", "year": 2003}, {"title": "Simple and deterministic matrix sketching", "author": ["Edo Liberty"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Liberty.,? \\Q2013\\E", "shortCiteRegEx": "Liberty.", "year": 2013}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["Ji Liu", "Przemyslaw Musialski", "Peter Wonka", "Jieping Ye"], "venue": "In Pattern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Generalizing the singular value decomposition", "author": ["C.F. Van Loan"], "venue": "SIAM Journal on numerical Analysis,", "citeRegEx": "Loan.,? \\Q1976\\E", "shortCiteRegEx": "Loan.", "year": 1976}, {"title": "Support matrix machines", "author": ["Luo Luo", "Yubo Xie", "Zhihua Zhang", "Wu-Jun Li"], "venue": "In The International Conference on Machine Learning (ICML),", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "A statistical perspective on algorithmic leveraging", "author": ["Ping Ma", "Michael Mahoney", "Bin Yu"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Matrix Differential Calculus with Applications in Statistics and Econometrics", "author": ["Jan R. Macnus", "Heinz Neudecker"], "venue": null, "citeRegEx": "Macnus and Neudecker.,? \\Q2000\\E", "shortCiteRegEx": "Macnus and Neudecker.", "year": 2000}, {"title": "Using a non-commutative Bernstein bound to approximate some matrix algorithms in the spectral norm", "author": ["Malik Magdon-Ismail"], "venue": "arXiv preprint arXiv:1103.5453,", "citeRegEx": "Magdon.Ismail.,? \\Q2011\\E", "shortCiteRegEx": "Magdon.Ismail.", "year": 2011}, {"title": "Low rank matrix-valued chernoff bounds and approximate matrix multiplication", "author": ["Avner Magen", "Anastasios Zouzias"], "venue": "In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,", "citeRegEx": "Magen and Zouzias.,? \\Q2011\\E", "shortCiteRegEx": "Magen and Zouzias.", "year": 2011}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Mahoney and Drineas.,? \\Q2009\\E", "shortCiteRegEx": "Mahoney and Drineas.", "year": 2009}, {"title": "Tensor-CUR decompositions for tensor-based data", "author": ["M.W. Mahoney", "M. Maggioni", "P. Drineas"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Mahoney et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mahoney et al\\.", "year": 2008}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "Multivariate Analysis", "author": ["K.V. Mardia", "J.T. Kent", "J.M. Bibby"], "venue": null, "citeRegEx": "Mardia et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Mardia et al\\.", "year": 1979}, {"title": "Inequalities: Theory of Majorization and Its Applications", "author": ["Albert W. Marshal", "Ingram Olkin", "Barry C. Arnold"], "venue": null, "citeRegEx": "Marshal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Marshal et al\\.", "year": 2010}, {"title": "A randomized algorithm for the decomposition of matrices", "author": ["Per-Gunnar Martinsson", "Vladimir Rokhlin", "Mark Tygert"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Martinsson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martinsson et al\\.", "year": 2011}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["Rahul Mazumder", "Trevor Hastie", "Robert Tibshirani"], "venue": "Journal of machine learning research,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Invariant feature extraction and classification in kernel space", "author": ["S. Mika", "G. R\u00e4tsch", "J. Weston", "B. Sch\u00f6lkopf", "A. Smola", "K.R. M\u00fcller"], "venue": "In Advances in Neural Information Processing Systems 12,", "citeRegEx": "Mika et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Mika et al\\.", "year": 2000}, {"title": "Symmetric gauge functions and unitarily invariant norms", "author": ["L. Mirsky"], "venue": "Quarterly Journal of Mathemathics,", "citeRegEx": "Mirsky.,? \\Q1960\\E", "shortCiteRegEx": "Mirsky.", "year": 1960}, {"title": "Aspects of Multivariate Statistical Theory", "author": ["R.J. Muirhead"], "venue": null, "citeRegEx": "Muirhead.,? \\Q1982\\E", "shortCiteRegEx": "Muirhead.", "year": 1982}, {"title": "Singular value decomposition, eigenfaces, and 3 D reconstruction", "author": ["N. Muller", "L. Magaia", "B.M. Herbst"], "venue": "SIAM Review,", "citeRegEx": "Muller et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Muller et al\\.", "year": 2004}, {"title": "Stronger approximate singular value decomposition via the block lanczos and power methods", "author": ["Cameron Musco", "Christopher Musco"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Musco and Musco.,? \\Q2015\\E", "shortCiteRegEx": "Musco and Musco.", "year": 2015}, {"title": "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L Nguy\u00ean"], "venue": "In IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2013}, {"title": "Some matrix-inequalities and metrication of matrix-space", "author": ["J. von Neumann"], "venue": "Tomsk University Review,", "citeRegEx": "Neumann.,? \\Q1937\\E", "shortCiteRegEx": "Neumann.", "year": 1937}, {"title": "\u00dcber die praktische aufl\u00f6sung von integralgleichungen mit anwendungen auf randwertaufgaben", "author": ["Evert J. Nystr\u00f6m"], "venue": "Acta Mathematica,", "citeRegEx": "Nystr\u00f6m.,? \\Q1930\\E", "shortCiteRegEx": "Nystr\u00f6m.", "year": 1930}, {"title": "Towards a generalized singular value decomposition", "author": ["C.C. Paige", "M.A. Saunders"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Paige and Saunders.,? \\Q1981\\E", "shortCiteRegEx": "Paige and Saunders.", "year": 1981}, {"title": "Latent semantic indexing: A probabilistic analysis", "author": ["Christos H Papadimitriou", "Hisao Tamaki", "Prabhakar Raghavan", "Santosh Vempala"], "venue": "In Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,", "citeRegEx": "Papadimitriou et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 1998}, {"title": "Nonlinear discriminant analysis using kernel functions and the generalized singular value decomposition", "author": ["C.H. Park", "H. Park"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Park and Park.,? \\Q2005\\E", "shortCiteRegEx": "Park and Park.", "year": 2005}, {"title": "Trace norm regularization: reformulations, algorithms, and multi-task learning", "author": ["Ting Kei Pong", "Paul Tseng", "Shuiwang Ji", "Jieping Ye"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Pong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pong et al\\.", "year": 2010}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Convex Analysis", "author": ["T. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1970\\E", "shortCiteRegEx": "Rockafellar.", "year": 1970}, {"title": "A randomized algorithm for principal component analysis", "author": ["V. Rokhlin", "A. Szlam", "M. Tygert"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Rokhlin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rokhlin et al\\.", "year": 2009}, {"title": "Nonlinear discriminant analysis using kernel functions", "author": ["V. Roth", "V. Steinhage"], "venue": "In Advances in Neural Information Processing Systems 12,", "citeRegEx": "Roth and Steinhage.,? \\Q2000\\E", "shortCiteRegEx": "Roth and Steinhage.", "year": 2000}, {"title": "Numerical methods for large eigenvalue problems. preparation", "author": ["Yousef Saad"], "venue": "Available from: http://www-users. cs. umn. edu/saad/books. html,", "citeRegEx": "Saad.,? \\Q2011\\E", "shortCiteRegEx": "Saad.", "year": 2011}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tamas Sarlos"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Sarlos.,? \\Q2006\\E", "shortCiteRegEx": "Sarlos.", "year": 2006}, {"title": "A Theory of Cross-Space", "author": ["Robert Schatten"], "venue": null, "citeRegEx": "Schatten.,? \\Q1950\\E", "shortCiteRegEx": "Schatten.", "year": 1950}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini.", "year": 2004}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Shi and Malik.,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik.", "year": 2000}, {"title": "Memory efficient kernel approximation", "author": ["Si Si", "Cho-Jui Hsieh", "Inderjit Dhillon"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Si et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Si et al\\.", "year": 2014}, {"title": "Maximum-margin matrix factorization", "author": ["Nathan Srebro", "Jason Rennie", "Tommi S Jaakkola"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}, {"title": "Four algorithms for the efficient computation of truncated pivoted QR approximations to a sparse matrix", "author": ["G.W. Stewart"], "venue": "Numerische Mathematik,", "citeRegEx": "Stewart.,? \\Q1999\\E", "shortCiteRegEx": "Stewart.", "year": 1999}, {"title": "Matrix Perturbation Theory", "author": ["G.W. Stewart", "J.G. Sun"], "venue": null, "citeRegEx": "Stewart and Sun.,? \\Q1990\\E", "shortCiteRegEx": "Stewart and Sun.", "year": 1990}, {"title": "Matrix coherence and the Nystr\u00f6m method", "author": ["A. Talwalkar", "A. Rostamizadeh"], "venue": "Proceedings of the 26th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "Talwalkar and Rostamizadeh.,? \\Q2010\\E", "shortCiteRegEx": "Talwalkar and Rostamizadeh.", "year": 2010}, {"title": "Large-scale manifold learning", "author": ["A. Talwalkar", "S. Kumar", "H. Rowley"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Talwalkar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Talwalkar et al\\.", "year": 2008}, {"title": "Largescale SVD and manifold learning", "author": ["Ameet Talwalkar", "Sanjiv Kumar", "Mehryar Mohri", "Henry Rowley"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Talwalkar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Talwalkar et al\\.", "year": 2013}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A Tropp"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Tropp.,? \\Q2015\\E", "shortCiteRegEx": "Tropp.", "year": 2015}, {"title": "Making fisher discriminant analysis scalable", "author": ["Bojun Tu", "Zhihua Zhang", "ShusenWang", "Hui Qiani"], "venue": "In Proceedings of the 31th International Conference on Machine Learning", "citeRegEx": "Tu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2014}, {"title": "Face recognition using eigenfaces", "author": ["M.A. Turk", "A.P. Pentland"], "venue": "In Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Turk and Pentland.,? \\Q1991\\E", "shortCiteRegEx": "Turk and Pentland.", "year": 1991}, {"title": "Incomplete cross approximation in the mosaic-skeleton", "author": ["E.E. Tyrtyshnikov"], "venue": "method. Computing,", "citeRegEx": "Tyrtyshnikov.,? \\Q2000\\E", "shortCiteRegEx": "Tyrtyshnikov.", "year": 2000}, {"title": "Kernel canonical correlation analysis and least squares support vector machines", "author": ["T. Van Gestel", "J.A.K. Suykens", "J. De Brabanter", "B. De Moor", "J. Vandewalle"], "venue": "In The International Conference on Artificial Neural Networks (ICANN),", "citeRegEx": "Gestel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gestel et al\\.", "year": 2001}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}, {"title": "The Random Projection Method", "author": ["Santosh S. Vempala"], "venue": "American Mathematical Society,", "citeRegEx": "Vempala.,? \\Q2000\\E", "shortCiteRegEx": "Vempala.", "year": 2000}, {"title": "Improving CUR matrix decomposition and the Nystr\u00f6m approximation via adaptive sampling", "author": ["Shusen Wang", "Zhihua Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Wang and Zhang.", "year": 2013}, {"title": "Efficient algorithms and error analysis for the modified nystr\u00f6m method", "author": ["Shusen Wang", "Zhihua Zhang"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Wang and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Wang and Zhang.", "year": 2014}, {"title": "The modified Nystr\u00f6m method: Theories, algorithms, and extension", "author": ["Shusen Wang", "Luo Luo", "Zhihua Zhang"], "venue": "CoRR, abs/1406.5675,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Improving the modified nystr\u00f6m method using spectral shifting", "author": ["Shusen Wang", "Chao Zhang", "Hui Qian", "Zhihua Zhang"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Improved analyses of the randomized power method and block Lanczos method", "author": ["Shusen Wang", "Zhihua Zhang", "Tong Zhang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Towards more efficient symmetric matrix sketching and cur matrix decomposition", "author": ["Shusen Wang", "Zhihua Zhang", "Tong Zhang"], "venue": "arXiv preprint arXiv:1503.08395,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Fundamentals of Matrix Computations", "author": ["D.S. Watkins"], "venue": null, "citeRegEx": "Watkins.,? \\Q1991\\E", "shortCiteRegEx": "Watkins.", "year": 1991}, {"title": "Characterization of the subdifferential of some matrix norms", "author": ["G.A. Watson"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Watson.,? \\Q1992\\E", "shortCiteRegEx": "Watson.", "year": 1992}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Randomized algorithms for low-rank matrix factorizations: sharp performance", "author": ["Rafi Witten", "Emmanuel Cand\u00e8s"], "venue": "bounds. Algorithmica,", "citeRegEx": "Witten and Cand\u00e8s.,? \\Q2013\\E", "shortCiteRegEx": "Witten and Cand\u00e8s.", "year": 2013}, {"title": "Low rank approximation lower bounds in row-update streams", "author": ["David Woodruff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Woodruff.", "year": 2014}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Woodruff.", "year": 2014}, {"title": "A fast randomized algorithm for the approximation of matrices", "author": ["Franco Woolfe", "Edo Liberty", "Vladimir Rokhlin", "Mark Tygert"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Woolfe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Woolfe et al\\.", "year": 2008}, {"title": "Computational and theoretical analysis of null space and orthogonal linear discriminant analysis", "author": ["J. Ye", "T. Xiong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ye and Xiong.,? \\Q2006\\E", "shortCiteRegEx": "Ye and Xiong.", "year": 2006}, {"title": "Clustered Nystr\u00f6m method for large scale manifold learning and dimension reduction", "author": ["K. Zhang", "J.T. Kwok"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Zhang and Kwok.,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Kwok.", "year": 2010}, {"title": "Improved Nystr\u00f6m low-rank approximation and error analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "The matrix ridge approximation: algorithms and applications", "author": ["Zhihua Zhang"], "venue": "Machine Learning,", "citeRegEx": "Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Zhang.", "year": 2014}, {"title": "Regularized discriminant analysis, ridge regression and beyond", "author": ["Zhihua Zhang", "Guang Dai", "Congfu Xu", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Regularized matrix regression", "author": ["Hua Zhou", "Lexin Li"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Zhou and Li.,? \\Q2014\\E", "shortCiteRegEx": "Zhou and Li.", "year": 2014}], "referenceMentions": [{"referenceID": 38, "context": "The first proof of the SVD for general m \u00d7 n matrices might be given by Eckart and Young [1939]. But the theory of singular values can date back to the 19th century when it had been studied by the Italian differential geometer E.", "startOffset": 72, "endOffset": 96}, {"referenceID": 55, "context": "Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values.", "startOffset": 29, "endOffset": 53}, {"referenceID": 55, "context": "Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values. There is a rich literature involving singular values or SVD. Chapter 3 of Horn and Johnson [1991] provides exhaustive studies about inequalities of singular values as well as unitarily invariant norms, and the primary focus is on the matrix theory.", "startOffset": 29, "endOffset": 264}, {"referenceID": 55, "context": "Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values. There is a rich literature involving singular values or SVD. Chapter 3 of Horn and Johnson [1991] provides exhaustive studies about inequalities of singular values as well as unitarily invariant norms, and the primary focus is on the matrix theory. The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.", "startOffset": 29, "endOffset": 442}, {"referenceID": 28, "context": "The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.", "startOffset": 28, "endOffset": 42}, {"referenceID": 28, "context": "The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.", "startOffset": 28, "endOffset": 69}, {"referenceID": 28, "context": "The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.", "startOffset": 28, "endOffset": 99}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions.", "startOffset": 61, "endOffset": 85}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al.", "startOffset": 61, "endOffset": 268}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997].", "startOffset": 61, "endOffset": 291}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997].", "startOffset": 61, "endOffset": 308}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997]. In Chapter 2 we review some preliminaries such as Kronecker produces and vectorization operators, majorization theory, and derivatives.", "startOffset": 61, "endOffset": 323}, {"referenceID": 96, "context": "Specifically, we apply matrix differential calculus to rederive the von Neumann theorem [Neumann, 1937] and the Ky Fan theorem [Fan, 1951].", "startOffset": 88, "endOffset": 103}, {"referenceID": 40, "context": "Specifically, we apply matrix differential calculus to rederive the von Neumann theorem [Neumann, 1937] and the Ky Fan theorem [Fan, 1951].", "startOffset": 127, "endOffset": 138}, {"referenceID": 38, "context": "The second one was proposed by Mirsky [1960], which is an extension of the novel Eckart Young theorem [Eckart and Young, 1936].", "startOffset": 102, "endOffset": 126}, {"referenceID": 89, "context": "The second one was proposed by Mirsky [1960], which is an extension of the novel Eckart Young theorem [Eckart and Young, 1936].", "startOffset": 31, "endOffset": 45}, {"referenceID": 92, "context": "The following properties can be found in Muirhead [1982].", "startOffset": 41, "endOffset": 57}, {"referenceID": 13, "context": "The further details of these results can be found from Borwein and Lewis [2006]. The following lemma then shows the fundamental role of subgradients in optimization.", "startOffset": 55, "endOffset": 80}, {"referenceID": 58, "context": "The statistical leverage [Hoaglin and Welsch, 1978] measures the extent to which the singular vectors of a matrix are correlated with the standard basis.", "startOffset": 25, "endOffset": 51}, {"referenceID": 48, "context": "That is a so-called CS decomposition [Golub et al., 1999] given as follows.", "startOffset": 37, "endOffset": 57}, {"referenceID": 76, "context": "The GSVD theorem was originally proposed by Loan [1976], in which n \u2265 p (or m \u2265 p) is required.", "startOffset": 44, "endOffset": 56}, {"referenceID": 76, "context": "The GSVD theorem was originally proposed by Loan [1976], in which n \u2265 p (or m \u2265 p) is required. Later on, Paige and Saunders [1981] developed a more general formulation for GSVD in which matrix pencil A and B are required only to have the same number of columns.", "startOffset": 44, "endOffset": 132}, {"referenceID": 76, "context": "The GSVD theorem was originally proposed by Loan [1976], in which n \u2265 p (or m \u2265 p) is required. Later on, Paige and Saunders [1981] developed a more general formulation for GSVD in which matrix pencil A and B are required only to have the same number of columns. Paige and Saunders [1981] also studied a GSVD of submatrices of a column orthonormal matrix.", "startOffset": 44, "endOffset": 289}, {"referenceID": 63, "context": "Based on this observation, Howland et al. [2003], Park and Park [2005] applied GSVD for solving Fisher linear discriminant analysis (FLDA) and generalized Fisher discriminant analysis [Baudat and Anouar, 2000, Mika et al.", "startOffset": 27, "endOffset": 49}, {"referenceID": 63, "context": "Based on this observation, Howland et al. [2003], Park and Park [2005] applied GSVD for solving Fisher linear discriminant analysis (FLDA) and generalized Fisher discriminant analysis [Baudat and Anouar, 2000, Mika et al.", "startOffset": 27, "endOffset": 71}, {"referenceID": 46, "context": "Gibson [1974] proved that they have joint factorizations A = U\u03a3AV T and B = U\u03a3BV T if and only if ABT and BTA are both normal.", "startOffset": 0, "endOffset": 14}, {"referenceID": 2, "context": "For example, SVD is an important tool in spectral analysis [Azar et al., 2001], latent semantic indexing [Papadimitriou et al.", "startOffset": 59, "endOffset": 78}, {"referenceID": 99, "context": ", 2001], latent semantic indexing [Papadimitriou et al., 1998], spectral clustering, and projective clustering [Feldman et al.", "startOffset": 34, "endOffset": 62}, {"referenceID": 41, "context": ", 1998], spectral clustering, and projective clustering [Feldman et al., 2013].", "startOffset": 56, "endOffset": 78}, {"referenceID": 8, "context": "This leads us to the notion of the matrix Moore-Penrose (MP) pseudoinverse [Ben-Israel and Greville, 2003].", "startOffset": 75, "endOffset": 106}, {"referenceID": 144, "context": "However, when N is singular, Zhang et al. [2010] suggested to use a pseudoinverse eigenproblem:", "startOffset": 29, "endOffset": 49}, {"referenceID": 144, "context": "Moreover, Zhang et al. [2010] established a connection between the solutions of the generalized eigenproblem and its corresponding pseudoinverse eigenproblem.", "startOffset": 10, "endOffset": 30}, {"referenceID": 86, "context": "Fisher discriminant analysis (FDA) is a classical method for classification and dimension reduction simultaneously [Mardia et al., 1979].", "startOffset": 115, "endOffset": 136}, {"referenceID": 86, "context": "That is, there is a duality relationship between PCA and PCO [Mardia et al., 1979].", "startOffset": 61, "endOffset": 82}, {"referenceID": 55, "context": "CCA is another subspace learning model [Hardoon et al., 2004].", "startOffset": 39, "endOffset": 61}, {"referenceID": 110, "context": "For example, kernel PCA [Sch\u00f6lkopf et al., 1998], kernel FDA [Baudat and Anouar, 2000, Mika et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 59, "context": "Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992].", "startOffset": 116, "endOffset": 136}, {"referenceID": 51, "context": "Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992].", "startOffset": 14, "endOffset": 38}, {"referenceID": 51, "context": "Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992]. Stewart [1999] devised efficient computational algorithms of truncated pivoted QR approximations to a sparse matrix.", "startOffset": 14, "endOffset": 153}, {"referenceID": 72, "context": "Kuruvilla et al. [2002] have claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005].", "startOffset": 146, "endOffset": 166}, {"referenceID": 111, "context": "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al.", "startOffset": 15, "endOffset": 30}, {"referenceID": 9, "context": "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005]. The SCA approximation is of the form A \u2248 XTY, where X and Y consist of columns and rows of A, and T minimizes \u2016A\u2212XTY\u2016F . This algorithm is a deterministic peocedure but computationally expensive. The terminology of the CUR decomposition has been proposed by Drineas and Mahoney [2005], Mahoney et al.", "startOffset": 147, "endOffset": 453}, {"referenceID": 9, "context": "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005]. The SCA approximation is of the form A \u2248 XTY, where X and Y consist of columns and rows of A, and T minimizes \u2016A\u2212XTY\u2016F . This algorithm is a deterministic peocedure but computationally expensive. The terminology of the CUR decomposition has been proposed by Drineas and Mahoney [2005], Mahoney et al. [2008]. They reformulated the idea based on random selection.", "startOffset": 147, "endOffset": 476}, {"referenceID": 137, "context": "For example, they have been applied to Gaussian processes [Williams and Seeger, 2001], kernel classification [Zhang et al.", "startOffset": 58, "endOffset": 85}, {"referenceID": 43, "context": ", 2013], spectral clustering [Fowlkes et al., 2004], kernel PCA and manifold learning [Talwalkar et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 0, "context": ", 2008, Zhang and Kwok, 2010], determinantal processes [Affandi et al., 2013], etc.", "startOffset": 55, "endOffset": 77}, {"referenceID": 116, "context": "Variational principles correspond to matrix perturbation theory [Stewart and Sun, 1990], which is the theoretical foundation to characterize stability or sensitivity of a matrix computation algorithm.", "startOffset": 64, "endOffset": 87}, {"referenceID": 96, "context": "The cornerstones are the novel von Neumann theorem [Neumann, 1937] and Ky Fan theorem [Fan, 1951].", "startOffset": 51, "endOffset": 66}, {"referenceID": 40, "context": "The cornerstones are the novel von Neumann theorem [Neumann, 1937] and Ky Fan theorem [Fan, 1951].", "startOffset": 86, "endOffset": 97}, {"referenceID": 96, "context": "The following cornerstone theorem was originally established by von Neumann [1937].", "startOffset": 68, "endOffset": 83}, {"referenceID": 62, "context": "The first result directly follows from the well known interlacing theorem [Horn and Johnson, 1985].", "startOffset": 74, "endOffset": 98}, {"referenceID": 108, "context": "[Schatten, 1950] Let u,v \u2208 Rn.", "startOffset": 0, "endOffset": 16}, {"referenceID": 40, "context": "[Fan, 1951] Given two nonnegative vectors u,v \u2208 R+, then u \u227aw v if and only if \u03c6(u) \u2264 \u03c6(v) for every symmetric gauge function \u03c6.", "startOffset": 0, "endOffset": 11}, {"referenceID": 121, "context": "Parallel with the l1-norm which is used as convex relaxation of the l0-norm [Tibshirani, 1996], the nuclear norm is a convex alternative of the matrix rank.", "startOffset": 76, "endOffset": 94}, {"referenceID": 116, "context": "Note that the Hoffman-Wielandt theorem still hods when A and B are normal [Stewart and Sun, 1990].", "startOffset": 74, "endOffset": 97}, {"referenceID": 57, "context": "Moreover, this proposition was widely used in matrix completion problems, because an optimization problem regularized by the Frobenius norm is solved more easily than that regularized by the nuclear norm [Hastie et al., 2014].", "startOffset": 204, "endOffset": 225}, {"referenceID": 60, "context": "35 of Horn and Johnson [1985]. As for Part (b), it is obvious that the Frobenius norm is both unitarily invariant and vectorization norm.", "startOffset": 6, "endOffset": 30}, {"referenceID": 13, "context": "But norm functions are convex and continuous, so we can resort to theory of subdifferentials [Rockafellar, 1970, Borwein and Lewis, 2006]. Indeed, the subdifferentials of unitarily invariant norms have been studied by Watson [1992] and Lewis [2003].", "startOffset": 113, "endOffset": 232}, {"referenceID": 13, "context": "But norm functions are convex and continuous, so we can resort to theory of subdifferentials [Rockafellar, 1970, Borwein and Lewis, 2006]. Indeed, the subdifferentials of unitarily invariant norms have been studied by Watson [1992] and Lewis [2003]. Using the properties of unitarily invariant norms and the SVD theory, we present directional derivatives and subdifferentials of unitarily invariant norms.", "startOffset": 113, "endOffset": 249}, {"referenceID": 19, "context": "Moreover, this model can be regarded as a parallel version of the Dantzig selector [Cand\u00e8s and Tao, 2007].", "startOffset": 83, "endOffset": 105}, {"referenceID": 122, "context": "However, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest.", "startOffset": 9, "endOffset": 22}, {"referenceID": 38, "context": "The theorem was originally proposed by Eckart and Young [1936] under the setting of the Frobenius norm, and generalized to any unitarily invariant norms by Mirsky [1960].", "startOffset": 39, "endOffset": 63}, {"referenceID": 38, "context": "The theorem was originally proposed by Eckart and Young [1936] under the setting of the Frobenius norm, and generalized to any unitarily invariant norms by Mirsky [1960].", "startOffset": 39, "endOffset": 170}, {"referenceID": 122, "context": "He even said \u201cFrobenius-norm error bounds are typically vacuous\u201d [Tropp, 2015].", "startOffset": 65, "endOffset": 78}, {"referenceID": 51, "context": "[Gu, 2015] Given any matrix A \u2208 Rm\u00d7n, let p = min{m,n} and B be a matrix with rank at most k such that", "startOffset": 0, "endOffset": 10}, {"referenceID": 121, "context": "However, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest.", "startOffset": 9, "endOffset": 22}, {"referenceID": 51, "context": "The following theorem was proposed by Gu [2015], which relates the approximation error in the Frobenius norm to that in the spectral norm.", "startOffset": 38, "endOffset": 48}, {"referenceID": 25, "context": "Moreover, it can be also used in large scalable k-means clustering [Cohen et al., 2014], approximate leverage scores [Drineas et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 4, "context": "[2015] proved optimal approximate matrix multiplication in terms of stable rank by using subspace embedding [Batson et al., 2014].", "startOffset": 108, "endOffset": 129}, {"referenceID": 24, "context": "Recently, Cohen et al. [2015] proved optimal approximate matrix multiplication in terms of stable rank by using subspace embedding [Batson et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 26, "context": "[Cohen et al., 2015] Given \u03b5, \u03b4 \u2208 (0, 1/2), let A and B be two conforming matrices, and \u03a0 be a (\u03b5, \u03b4) subspace embedding for the 2r\u0303-dimensional subspace, where r\u0303 is the maximum of the stable ranks of A and B.", "startOffset": 0, "endOffset": 20}, {"referenceID": 68, "context": "To analyze approximate matrix multiplication with the Frobenius error, Kane and Nelson [2014] introduced the JL-moment property.", "startOffset": 71, "endOffset": 94}, {"referenceID": 68, "context": "More specifically, they can be converted into each other [Kane and Nelson, 2014].", "startOffset": 57, "endOffset": 80}, {"referenceID": 67, "context": "More specifically, they can be converted into each other [Kane and Nelson, 2014]. There are other methods, which do not use subspace embedding matrices, in the literature. Magen and Zouzias [2011] gave a method based on columns selection.", "startOffset": 58, "endOffset": 197}, {"referenceID": 10, "context": "Bhojanapalli et al. [2015] proposed a new method with sampling and alternating minimization to directly compute a low-rank approximation to the product of two given matrices.", "startOffset": 0, "endOffset": 27}, {"referenceID": 22, "context": "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches.", "startOffset": 58, "endOffset": 87}, {"referenceID": 22, "context": "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].", "startOffset": 58, "endOffset": 154}, {"referenceID": 22, "context": "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].", "startOffset": 58, "endOffset": 270}, {"referenceID": 22, "context": "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].", "startOffset": 58, "endOffset": 321}, {"referenceID": 28, "context": "Fortunately, many machine learning methods such as latent semantic indexing [Deerwester et al., 1990], spectral clustering [Shi and Malik, 2000], manifold learning [Tenenbaum et al.", "startOffset": 76, "endOffset": 101}, {"referenceID": 112, "context": ", 1990], spectral clustering [Shi and Malik, 2000], manifold learning [Tenenbaum et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 54, "context": "In particular, we will consider randomized SVD methods [Halko et al., 2011].", "startOffset": 55, "endOffset": 75}, {"referenceID": 15, "context": "2 are also fundamental in random column selection [Boutsidis et al., 2014].", "startOffset": 50, "endOffset": 74}, {"referenceID": 23, "context": "In particular, the count sketch [Clarkson and Woodruff, 2013] applies to A in only O(nnz(A)) time and exhibits very similar properties as the JL transform.", "startOffset": 32, "endOffset": 61}, {"referenceID": 24, "context": "The Johnson & Lindenstrauss (JL) transform [Johnson and Lindenstrauss, 1984, Dasgupta and Gupta, 2003] is known to keep isometry in expectation or with high probability. Halko et al. [2011], Boutsidis et al.", "startOffset": 77, "endOffset": 190}, {"referenceID": 15, "context": "[2011], Boutsidis et al. [2014] used the JL transform for sketching and showed relative-error bounds.", "startOffset": 8, "endOffset": 32}, {"referenceID": 15, "context": "[2011], Boutsidis et al. [2014] used the JL transform for sketching and showed relative-error bounds. However, the Gaussian test matrix is dense and cannot efficiently apply to matrices. Several improvements have been proposed to make the sketching matrix sparser; see the review [Woodruff, 2014b] for the complete list of the literature. In particular, the count sketch [Clarkson and Woodruff, 2013] applies to A in only O(nnz(A)) time and exhibits very similar properties as the JL transform. Specifically, Woodruff [2014b] showed that an m\u00d7O(k/\u01eb) sketch C = A\u03a9 can be obtained in O(nnz(A)) time and", "startOffset": 8, "endOffset": 526}, {"referenceID": 54, "context": "Halko et al. [2011] proposed to directly solve the left-hand side of (10.", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "Clarkson and Woodruff [2013], Woodruff [2014b] showed that", "startOffset": 0, "endOffset": 29}, {"referenceID": 22, "context": "Clarkson and Woodruff [2013], Woodruff [2014b] showed that", "startOffset": 0, "endOffset": 47}, {"referenceID": 51, "context": "We apply Gu\u2019s theorem [Gu, 2015] (Theorem 9.", "startOffset": 22, "endOffset": 32}, {"referenceID": 51, "context": ", 2011, Gu, 2015]. The algorithm is described in Algorithm 2 and analyzed in the following. Let\u03a9 \u2208 Rn\u00d7c be a Gaussian test matrix or count sketch andB = (AAT )tA. Let us takeB instead of A as the input of the prototype algorithm 1 and obtain the approximate left singular vectors \u0168k. It is easy to verify that \u0168k is the same to the output of Algorithm 2. We will show that when t = O( logn \u01eb ), \u2225 \u2225A\u2212 \u0168k\u0168kA \u2225 \u2225 2 2 \u2264 (1 + \u01eb)\u2016A\u2212Ak\u20162. (10.3) To show this result, we need the lemma of Halko et al. [2011].", "startOffset": 8, "endOffset": 502}, {"referenceID": 106, "context": "It turns out that the Krylov subspace method converges much faster than the power iteration [Saad, 2011].", "startOffset": 92, "endOffset": 104}, {"referenceID": 106, "context": "In practice, re-orthogonalization or partial re-orthogonalization are employed to prevent the instability from happening [Saad, 2011].", "startOffset": 121, "endOffset": 133}, {"referenceID": 94, "context": "Very recently, Musco and Musco [2015] showed that with t = logn \u221a \u01eb power iteration, the 1+\u01eb spectral norm bound (10.", "startOffset": 15, "endOffset": 38}, {"referenceID": 120, "context": "For example, spectral clustering, KPCA, Isomap [Tenenbaum et al., 2000], and Laplacian eigenmaps [Belkin and Niyogi, 2003] compute the", "startOffset": 47, "endOffset": 71}, {"referenceID": 7, "context": ", 2000], and Laplacian eigenmaps [Belkin and Niyogi, 2003] compute the", "startOffset": 33, "endOffset": 58}, {"referenceID": 54, "context": "proposed by Halko et al. [2011] for approximating symmetric matrix.", "startOffset": 12, "endOffset": 32}, {"referenceID": 54, "context": "proposed by Halko et al. [2011] for approximating symmetric matrix. Wang et al. [2014a] showed that by randomly samplingO(k/\u01eb) columns of K to form C by a certain algorithm, the approximation is high accurate: \u2225 \u2225K\u2212CXC \u2225 \u2225 2 F \u2264 (1 + \u01eb) \u2225 \u2225K\u2212Kk \u2225 \u2225 2 F .", "startOffset": 12, "endOffset": 88}, {"referenceID": 54, "context": "proposed by Halko et al. [2011] for approximating symmetric matrix. Wang et al. [2014a] showed that by randomly samplingO(k/\u01eb) columns of K to form C by a certain algorithm, the approximation is high accurate: \u2225 \u2225K\u2212CXC \u2225 \u2225 2 F \u2264 (1 + \u01eb) \u2225 \u2225K\u2212Kk \u2225 \u2225 2 F . This upper bound matches the lower bound c \u2265 2k/\u01eb up to a constant factor [Wang et al., 2014a]. Unfortunately, the prototype algorithm has two obvious drawbacks. Firstly, to compute the intersection matrix X\u22c6, every entry of K must be known. As is discussed, it takes O(n2d) time to form the kernel matrix K. Secondly, the matrix multiplication C\u2020K costs O(n2c) time. In sum, the prototype algorithm costs O(n2c + n2d) time. Although it is substantially faster than the exact solution, the prototype algorithm has the same time complexity as the exact solution. Faster SPSD Matrix Sketching. Since C = KS has much more rows than columns, the optimization problem (10.4) is strongly overdetermined. Wang et al. [2015b] proposed to use sketching to approximately solve (10.", "startOffset": 12, "endOffset": 973}, {"referenceID": 131, "context": "Wang et al. [2015b] devised an algorithm that sets p = \u221a nc/ \u221a \u01eb and very efficiently forms the column selection matrix P; and the following error bound holds with high probability:", "startOffset": 0, "endOffset": 20}, {"referenceID": 141, "context": "Motivated by the matrix ridge approximation of Zhang [2014], Wang et al.", "startOffset": 47, "endOffset": 60}, {"referenceID": 131, "context": "Motivated by the matrix ridge approximation of Zhang [2014], Wang et al. [2014b] proposed a spectral shifting kernel approximation method.", "startOffset": 61, "endOffset": 81}, {"referenceID": 131, "context": "Motivated by the matrix ridge approximation of Zhang [2014], Wang et al. [2014b] proposed a spectral shifting kernel approximation method. When the spectrum of K decays slowly, the shifting term helps to improve the approximation accuracy and numerical stability. Wang et al. [2014a] also showed that the spectral shifting approach", "startOffset": 61, "endOffset": 284}, {"referenceID": 113, "context": "can be used to improve other kernel approximation models such as the memory efficient kernel approximation (MEKA) model [Si et al., 2014].", "startOffset": 120, "endOffset": 137}, {"referenceID": 137, "context": "It is named after its inventor Nystr\u00f6m [1930] and gained its popularity in the machine learning society after its application in Gaussian procession regression [Williams and Seeger, 2001].", "startOffset": 160, "endOffset": 187}, {"referenceID": 119, "context": "The Nystr\u00f6m method has been applied to solve million scale kernel methods [Talwalkar et al., 2013].", "startOffset": 74, "endOffset": 98}, {"referenceID": 129, "context": "The lower bound [Wang and Zhang, 2013] indicates that the Nystr\u00f6m method cannot attain (1+\u01eb) relativeerror bound unless it is willing to spend \u03a9(n2k/\u01eb) time.", "startOffset": 16, "endOffset": 38}, {"referenceID": 95, "context": "The Nystr\u00f6m Method is the most popular kernel approximation approach. It is named after its inventor Nystr\u00f6m [1930] and gained its popularity in the machine learning society after its application in Gaussian procession regression [Williams and Seeger, 2001].", "startOffset": 4, "endOffset": 116}, {"referenceID": 47, "context": "Gittens and Mahoney [2013] offered comprehensive error analysis of the Nystr\u00f6m method.", "startOffset": 0, "endOffset": 27}, {"referenceID": 33, "context": "Several different column selection strategies have been devised, among which the leverage score sampling [Drineas et al., 2008] and the adaptive sampling [Wang and Zhang, 2013, Boutsidis and Woodruff, 2014] attain relative error bounds.", "startOffset": 105, "endOffset": 127}, {"referenceID": 32, "context": "An example of Drineas et al. [2008] and Mahoney and Drineas [2009] has well shown this viewpoint; that is, the vector [(1/2)age \u2212 (1/ \u221a 2)height + (1/2)income], the sum of the significant uncorrelated features from a data set of people\u2019s features, is not particularly informative.", "startOffset": 14, "endOffset": 36}, {"referenceID": 32, "context": "An example of Drineas et al. [2008] and Mahoney and Drineas [2009] has well shown this viewpoint; that is, the vector [(1/2)age \u2212 (1/ \u221a 2)height + (1/2)income], the sum of the significant uncorrelated features from a data set of people\u2019s features, is not particularly informative.", "startOffset": 14, "endOffset": 67}, {"referenceID": 14, "context": ", 2008] and the adaptive sampling [Wang and Zhang, 2013, Boutsidis and Woodruff, 2014] attain relative error bounds. In particular, Boutsidis and Woodruff [2014] showed that with c = O(k/\u01eb) columns and r = O(k/\u01eb) rows selected by adaptive sampling to form C and R, min X \u2016A\u2212CXR\u2016F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u2016F holds in expectation.", "startOffset": 57, "endOffset": 162}, {"referenceID": 114, "context": "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014].", "startOffset": 31, "endOffset": 46}, {"referenceID": 114, "context": "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014].", "startOffset": 31, "endOffset": 69}, {"referenceID": 14, "context": "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014]. This approach is very similar to the prototype SPSD matrix approximation method in the previous section, and it costs at least O(mn\u00b7min{c, r}) time and requires observing every entry of A.", "startOffset": 70, "endOffset": 100}, {"referenceID": 14, "context": "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014]. This approach is very similar to the prototype SPSD matrix approximation method in the previous section, and it costs at least O(mn\u00b7min{c, r}) time and requires observing every entry of A. Apparently, it cannot help speed up matrix computation. Wang et al. [2015a] proposed a more practical CUR decomposition method which solves (10.", "startOffset": 70, "endOffset": 366}], "year": 2015, "abstractText": "The singular value decomposition (SVD) is not only a classical theory in matrix computation and analysis, but also is a powerful tool in machine learning and modern data analysis. In this tutorial we first study the basic notion of SVD and then show the central role of SVD in matrices. Using majorization theory, we consider variational principles of singular values and eigenvalues. Built on SVD and a theory of symmetric gauge functions, we discuss unitarily invariant norms, which are then used to formulate general results for matrix low rank approximation. We study the subdifferentials of unitarily invariant norms. These results would be potentially useful in many machine learning problems such as matrix completion and matrix data classification. Finally, we discuss matrix low rank approximation and its recent developments such as randomized SVD, approximate matrix multiplication, CUR decomposition, and Nystr\u00f6m approximation. Randomized algorithms are important approaches to large scale SVD as well as fast matrix computations.", "creator": "LaTeX with hyperref package"}}}