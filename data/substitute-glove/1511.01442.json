{"id": "1511.01442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2015", "title": "Low-Rank Approximation of Weighted Tree Automata", "abstract": "We particularly each tools giving anticipate weighted roots algebraic (WTA ), a even formamide that subsumes algorithm context - home grammars (PCFGs) through latent - impedance PCFGs. Our method relies take gives description any algorithm many the reflect Hankel matrix principle turned created WTA. Our located discourse caused example an efficient compute bringing graphics part SVD whose time infinite Hankel formula_14 implicitly considered instead a WTA. We provide though observations a the approximation reason induced when given disorganization, and lot evaluate our method on real - world data proximity which cbs2 treebank. We entertainment been the model vitality lower circumspection while previous methods for PCFG generalisation, included may 's unfortunately say environment despite same same key of from 3000gt.", "histories": [["v1", "Wed, 4 Nov 2015 19:17:18 GMT  (401kb,D)", "http://arxiv.org/abs/1511.01442v1", "Submitted to AISTATS 2016"], ["v2", "Thu, 24 Dec 2015 08:39:49 GMT  (431kb,D)", "http://arxiv.org/abs/1511.01442v2", "To appear in AISTATS 2016"]], "COMMENTS": "Submitted to AISTATS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.FL", "authors": ["guillaume rabusseau", "borja balle", "shay b cohen"], "accepted": false, "id": "1511.01442"}, "pdf": {"name": "1511.01442.pdf", "metadata": {"source": "CRF", "title": "Weighted Tree Automata Approximation by Singular Value Truncation", "authors": ["Guillaume Rabusseau", "Borja Balle", "Shay B. Cohen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Probabilistic context-free grammars (PCFG) provide a powerful statistical formalism for modeling important phenomena occurring in natural language. In fact, learning and parsing algorithms for PCFG are now standard tools in natural language processing pipelines. Most of these algorithms can be naturally extended to the superclass of weighted context-free grammars (WCFG), and closely related models like weighted tree automata (WTA) and latent probabilistic context-free grammars (LPCFG). The complexity of these algorithms depends on the size of the grammar/automaton, typically controlled by the number of rules/states. Being able to control this complexity is essential in operations like parsing, which is typically executed every time the model is used to make a prediction. In this paper we present an algorithm that given a WTA with n states and a target number of states n\u0302 < n, returns a WTA with n\u0302 states that is a good approximation of the original automaton. This can be interpreted as a lowrank approximation method for WTA through the direct connection between number of states of a WTA and the rank of its associated Hankel matrix. This opens the door to reducing the complexity of algorithms working with WTA at the price of incurring a small, controlled amount of error in the output of such algorithms.\nOur techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al., 2010; Cohen et al., 2014), and subsequent investigations into low-rank spectral learning for predictive state representations (Kulesza et al., 2014, 2015) and approximate minimization of weighted automata (Balle et al., 2015). In spectral learning algorithms, data is used to reconstruct a finite block of a Hankel matrix and an SVD of such matrix then reveals a low-dimensional\nar X\niv :1\n51 1.\n01 44\n2v 1\n[ cs\n.L G\n] 4\nN ov\n2 01\n5\nspace where a linear regression recovers the parameters of the model. In contrast, our approach computes the SVD of the infinite Hankel matrix associated with a WTA. Our main result is an efficient algorithm for computing this singular value decomposition by operating directly on the WTA representation of the Hankel matrix; that is, without the need to explicitly represent this infinite matrix at any point. Section 2 presents the main ideas underlying our approach. Add a comment to this line An efficient algorithmic implementation of these ideas is discussed in Section 3, and a theoretical analysis of the approximation error induced by our minimization method is given in Section 4. Proofs of all results stated in the paper can be found in appendix.\nThe idea of speeding up parsing with (L)PCFG by approximating the original model with a smaller one was recently studied in (Cohen and Collins, 2012; Cohen et al., 2013a), where a tensor decomposition technique was used in order to obtain the minimized model. We compare that approach to ours in the experiments presented in Section 5, where both techniques are used to compute approximations to a grammar learned from a corpus of real linguistic data. It was observed in (Cohen and Collins, 2012; Cohen et al., 2013a) that a side-effect of reducing the size of a grammar learned from data was a slight improvement in parsing performance. The number of parameters in the approximate models is smaller, and as such, generalization improves. We show in our experimental section that our minimization algorithms have the same effect in certain parsing scenarios. In addition, our approach yields models which give lower perplexity on an unseen set of sentences, and provides a better approximation to the original model in terms of `2 distance. It is important to remark that in contrast with the tensor decompositions in (Cohen and Collins, 2012; Cohen et al., 2013a) which are susceptible to local optima problems, our approach resembles a power-method approach to SVD, which yields efficient globally convergent algorithms. Overall, we observe in our experiments that this renders a more stable minimization method than the one using tensor decompositions."}, {"heading": "1.1 Notation", "text": "For an integer n, we write [n] = {1, . . . , n}. We use lower case bold letters (or symbols) for vectors (e.g. v \u2208 Rd1), upper case bold letters for matrices (e.g. M \u2208 Rd1\u00d7d2) and bold calligraphic letters for third order tensors (e.g. T \u2208 Rd1\u00d7d2\u00d7d3). Unless explicitly stated, vectors are by default column vectors. The identity matrix will be written as I. Given i1 \u2208 [d1], i2 \u2208 [d2], i3 \u2208 [d3] we use v(i1), M(i1, i2), and T (i1, i2, i3) to denote the corresponding entries. The ith row (resp. column) of a matrix M will be noted M(i, :) (resp. M(:, i)). This notation is extended to slices across the three modes of a tensor in the straightforward way. If v \u2208 Rd1 and v\u2032 \u2208 Rd2 , we use v \u2297 v\u2032 \u2208 Rd1\u00b7d2 to denote the Kronecker product between vectors, and its straightforward extension to matrices and tensors. Given a matrix M \u2208 Rd1\u00d7d2 we use vec(M) \u2208 Rd1\u00b7d2 to denote the column vector obtained by concatenating the columns of M. Given a tensor T \u2208 Rd1\u00d7d2\u00d7d3 and matrices Mi \u2208 Rdi\u00d7d \u2032 i for i \u2208 [3], we define a tensor T (M1,M2,M3) \u2208 Rd \u2032 1\u00d7d \u2032 2\u00d7d \u2032 3 whose entries are given by\nT (M1,M2,M3)(i1, i2, i3) = \u2211\nj1,j2,j3\nT (j1, j2, j3)M1(j1, i1)M2(j2, i2)M3(j3, i3) .\nThis operation corresponds to contracting T with Mi across the ith mode of the tensor for each i."}, {"heading": "2 Approximate Minimization of WTA and SVD of Hankel Matrices", "text": "In this section we present the first contribution of the paper. Namely, the existence of a canonical form for weighted tree automata inducing the singular value decomposition of the infinite Hankel matrix associated with the automaton. We start by recalling several definitions and well-known facts about WTA that will be used in the rest of the paper. Then we proceed to establish the existence of the canonical form, which we call the singular value tree automaton. Finally we indicate how removing the states in this canonical form that correspond to the smallest singular values of the Hankel matrix leads to an effective procedure for model reduction in WTA."}, {"heading": "2.1 Weighted Tree Automata", "text": "Let \u03a3 be a finite alphabet. We use \u03a3? to denote the set of all finite strings with symbols in \u03a3 with \u03bb denoting the empty string. We write |x| to denote the length of a string x \u2208 \u03a3?. The number of occurences of a symbol \u03c3 \u2208 \u03a3 in a string x \u2208 \u03a3? is denoted by |x|\u03c3.\nThe set of all rooted full binary trees with leafs in \u03a3 is the smallest set T\u03a3 such that \u03a3 \u2282 T\u03a3 and (t1, t2) \u2208 T\u03a3 for any t1, t2 \u2208 T\u03a3. We shall just write T when the alphabet \u03a3 is clear from the context. The size of a tree t \u2208 T is denoted by size(t) and defined recursively by size(\u03c3) = 0 for \u03c3 \u2208 \u03a3, and size((t1, t2)) = size(t1) + size(t2) + 1; that is, the number of internal nodes in the tree. The depth of a tree t \u2208 T is denoted by depth(t) and defined recursively by depth(\u03c3) = 0 for \u03c3 \u2208 \u03a3, and depth((t1, t2)) = max{depth(t1), depth(t2)} + 1; that is, the distance from the root of the tree to the farthest leaf. The yield of a tree t \u2208 T is a string \u3008t\u3009 \u2208 \u03a3\u2217 defined as the left-to-right concatenation of the symbols in the leafs of t, and can be recursively defined by \u3008\u03c3\u3009 = \u03c3, and \u3008(t1, t2)\u3009 = \u3008t1\u3009 \u00b7 \u3008t2\u3009. The total number of nodes (internal plus leafs) of a tree t is denoted by |t| and satisfies |t| = size(t) + |\u3008t\u3009|.\nLet \u03a3\u2032 = \u03a3\u222a{\u2217}, where \u2217 is a symbol not in \u03a3. The set of rooted full binary context trees is the set C\u03a3 = {c \u2208 T\u03a3\u2032 | |\u3008c\u3009|\u2217 = 1}; that is, a context c \u2208 C\u03a3 is a tree in T\u03a3\u2032 in which the symbol \u2217 occurs exactly in one leaf. Note that because given a context c = (t1, t2) \u2208 C\u03a3 with t1, t2 \u2208 T\u03a3\u2032 the symbol \u2217 can only appear in one of the t1 and t2, we must actually have c = (c\u2032, t) or c = (t, c\u2032) with c\u2032 \u2208 C\u03a3 and t \u2208 T\u03a3. The drop of a context c \u2208 C is the distance between the root and the leaf labeled with \u2217 in c, which can be defined recursively as drop(\u2217) = 0, drop((c, t)) = drop((t, c)) = drop(c) + 1.\nWe usually think as the leaf with the symbol \u2217 in a context as a placeholder where the root of another tree or another context can be inserted. Accordingly, given t \u2208 T and c \u2208 C, we can define c[t] \u2208 T as the tree obtained by replacing the occurence of \u2217 in c with t. Similarly, given c, c\u2032 \u2208 C we can obtain a new context tree c[c\u2032] by replacing the occurence of \u2217 in c with c\u2032. See Figure 1 for some illustrative examples.\nA weighted tree automaton (WTA) over \u03a3 is a tuple A = \u3008\u03b1,T , {\u03c9\u03c3}\u03c3\u2208\u03a3\u3009, where \u03b1 \u2208 Rn is the vector of initial weights, T \u2208 Rn\u00d7n\u00d7n is the tensor of transition weights, and \u03c9\u03c3 \u2208 Rn is the vector of terminal weights associated with \u03c3 \u2208 \u03a3. The dimension n is the number of states of the automaton, which we shall sometimes denote by |A|. A WTAA = \u3008\u03b1,T , {\u03c9\u03c3}\u3009 computes a function fA : T\u03a3 \u2192 R assigning to each tree t \u2208 T the number computed as fA(t) = \u03b1>\u03c9A(t), where \u03c9A(t) \u2208 Rn is obtained recursively as \u03c9A(\u03c3) = \u03c9\u03c3, and \u03c9A((t1, t2)) = T (I,\u03c9A(t1),\u03c9A(t2)) \u2014 note the matching of dimensions in this last expression since contracting a third order tensor with a matrix in the first mode and\nvectors in the second and third mode yields a vector. In many cases we shall just write \u03c9(t) when the automaton A is clear from the context. While WTA are usually defined over arbitrary ranked trees, only considering binary trees does not lead to any loss of generality since WTA on ranked trees are equivalent to WTA on binary trees (see (Bailly et al., 2010) for references). Additionally, one could consider binary trees where each internal node is labelled, which leads to the definition of WTA with multiple transition tensors. Our results can be extended to this case without much effort, but we state them just for WTA with only one transition tensor to keep the notation manageable.\nAn important observation is that there exist more than one WTA computing the same function \u2014 actually there exist infinitely many. An important construction along these lines is the conjugate of a WTA A with n states by an invertible matrix Q \u2208 Rn\u00d7n. If A = \u3008\u03b1,T , {\u03c9\u03c3}\u3009, its conjugate by Q is AQ = \u3008Q>\u03b1,T (Q\u2212>,Q,Q), {Q\u22121\u03c9\u03c3}\u3009, where Q\u2212> denotes the inverse of Q>. To show that fA = fAQ one applies an induction argument on depth(t) to show that for every t \u2208 T one has \u03c9AQ(t) = Q\u22121\u03c9A(t). The claim is obvious for trees of zero depth \u03c3 \u2208 \u03a3, and for t = (t1, t2) one has\n\u03c9AQ((t1, t2)) = (T (Q\u2212>,Q,Q))(I,\u03c9AQ(t1),\u03c9AQ(t2)) = (T (Q\u2212>,Q,Q))(I,Q\u22121\u03c9A(t1), Q\u22121\u03c9A(t2)) = T (Q\u2212>,\u03c9A(t1),\u03c9A(t2)) = Q\u22121T (I,\u03c9A(t1),\u03c9A(t2)) ,\nwhere we just used some simple rules of tensor algebra. An arbitrary function f : T \u2192 R is called rational if there exists a WTA A such that f = fA. The number of states of the smallest such WTA is the rank of f \u2014 we shall set rank(f) = \u221e if f is not rational. A WTA A with fA = f and |A| = rank(f) is called minimal. Given any f : T \u2192 R we define its Hankel matrix as the infinite matrix Hf \u2208 RC\u00d7T with rows indexed by contexts, columns indexed by trees, and whose entries are given by Hf (c, t) = f(c[t]). Note that given a tree t\u2032 \u2208 T there are exactly |t\u2032| different ways of splitting t\u2032 = c[t] with c \u2208 C and t \u2208 T. This implies that Hf is a highly redundant representation for f , and it turns out that this redundancy is the key to proving the following fundamental result about rational tree functions.\nTheorem 1 ((Bozapalidis and Louscou-Bozapalidou, 1983)). For any f : T \u2192 R we have rank(f) = rank(Hf )."}, {"heading": "2.2 Rank Factorizations of Hankel Matrices", "text": "The theorem above can be rephrased as saying that the rank of Hf is finite if and only if f is rational. When the rank of Hf is indeed finite \u2014 say rank(Hf ) = n \u2014 one can\nfind two rank n matrices P \u2208 RC\u00d7n, S \u2208 Rn\u00d7T such that Hf = PS. In this case we say that P and S give a rank factorization of Hf . We shall now refine Theorem 1 by showing that when f is rational, the set of all possible rank factorizations of Hf is in direct correpondance with the set of minimal WTA computing f .\nThe first step is to show that any minimal WTA A = \u3008\u03b1,T , {\u03c9\u03c3}\u3009 computing f induces a rank factorization Hf = PASA. We build SA \u2208 Rn\u00d7T by setting the column corresponding to a tree t to SA(:, t) = \u03c9A(t). In order to define PA we need to introduce a new mapping \u039eA : C\u2192 Rn\u00d7n assigning a matrix to every context as follows: \u039eA(\u2217) = I, \u039eA((c, t)) = T (I,\u039eA(c),\u03c9A(t)), and \u039eA((t, c)) = T (I,\u03c9A(t),\u039eA(c)). If we now define \u03b1A : C\u2192 Rn as \u03b1A(c)> = \u03b1>\u039eA(c), we can set the row of PA corresponding to c to be PA(c, :) = \u03b1A(c)>. With these definitions one can easily show by induction on drop(c) that \u039eA(c)\u03c9A(t) = \u03c9A(c[t]) for any c \u2208 C and t \u2208 T. Then it is immediate to check that Hf = PASA:\nn\u2211 i=1 PA(c, i)SA(i, t) = \u03b1A(c)>\u03c9A(t) = \u03b1>\u039eA(c)\u03c9A(t)\n= \u03b1>\u03c9A(c[t]) = fA(c[t]) = Hf (c, t) . (1)\nAs before, we shall sometimes just write \u039e(c) and \u03b1(c) when A is clear from the context. We can now state the main result of this section, which generalizes similar results in (Balle et al., 2015, 2014) for weighted automata on strings.\nTheorem 2. Let f : T\u2192 R be rational. If Hf = PS is a rank factorization, then there exists a minimal WTA A computing f such that PA = P and SA = S.\nProof. See Appendix A."}, {"heading": "2.3 Approximate Minimization with the Singular Value Tree Automaton", "text": "Equation (1) can be interpreted as saying that given a fixed factorization Hf = PASA, the value fA(c[t]) is given by the inner product \u2211 i\u03b1A(c)(i)\u03c9A(t)(i). Thus, \u03b1A(c)(i) and \u03c9A(t)(i) quantify the influence of state i in the computation of fA(c[t]), and by extension one can use \u2016PA(:, i)\u2016 and \u2016SA(i, :)\u2016 to measure the overall influence of state i in fA. Since our goal is to approximate a given WTA by a smaller WTA obtained by removing some states in the original one, we shall proceed by removing those states with overall less influence on the computation of f . But because there are infinitely many WTA computing f , we need to first fix a particular representation for f before we can remove the less influential states. In particular, we seek a representation where each state is decoupled as much as possible from each other state, and where there is a clear ranking of states in terms of overall influence. It turns out all this can be achieved by a canonical form for WTA we call the singular value tree automaton, which provides an implicit representation for the SVD of Hf . We now show conditions for the existence of such canonical form, and in the next section we develop an algorithm for computing the it efficiently.\nSuppose f : T\u2192 R is a rank n rational function such that its Hankel matrix admits a reduced singular value decomposition Hf = UDV>. Then we have that P = UD1/2 and S = D1/2V> is a rank decomposition for Hf , and by Theorem 2 there exists some minimal WTA A with fA = f , PA = UD1/2 and SA = D1/2V>. We call such an A a\nsingular value tree automaton (SVTA) for f . However, these are not defined for every rational function f , because the fact that columns of U and V must be unitary vectors (i.e. U>U = V>V = I) imposes some restrictions on which infinite Hankel matrices Hf admit an SVD \u2014 this phenomenon is related to the distinction between compact and non-compact operators in functional analysis. Our next theorem gives a sufficient condition for the existence of an SVD of Hf .\nWe say that a function f : T \u2192 R is strongly convergent if the series \u2211 t\u2208T |t||f(t)| converges. To see the intuitive meaning of this condition, assume that f is a probability distribution over trees in T. In this case, strong convergence is equivalent to saying that the expected size of trees generated from the distribution f is finite. It turns out strong convergence of f is a sufficient condition to guarantee the existence of an SVD for Hf .\nTheorem 3. If f : T\u03a3 \u2192 R is rational and strongly convergent, then Hf admits a singular value decomposition.\nProof. See Appendix B.\nTogether, Theorems 2 and 3 imply that every rational strongly convergent f : T\u2192 R can be represented by an SVTA A. If rank(f) = n, then A has n states and for every i \u2208 [n] the ith state contributes to Hf by generating the ith left and right singular vectors weighted by \u221asi, where si = D(i, i) is the ith singular value. Thus, if we desire to obtain a good approximation f\u0302 to f with n\u0302 states, we can take the WTA A\u0302 obtained by removing the last n \u2212 n\u0302 states from A, which corresponds to removing from f the contribution of the smallest singular values of Hf . We call such A\u0302 an SVTA truncation. Given an SVTA A = \u3008\u03b1,T , {\u03c9\u03c3}\u3009 and \u03a0 = [I | 0] \u2208 Rn\u0302\u00d7n, the SVTA truncation to n\u0302 states can be written as\nA\u0302 = \u3008\u03a0\u03b1,T (\u03a0>,\u03a0>,\u03a0>), {\u03a0\u03c9\u03c3}\u3009 .\nTheoretical guarantees on the error induced by the SVTA truncation method are presented in Section 4 ."}, {"heading": "3 Computing the Singular Value WTA", "text": "Previous section shows that in order to compute an approximation to a strongly convergent rational function f : T\u2192 R one can proceed by truncating its SVTA. However, the only obvious way to obtain such SVTA is by computing the SVD of the infinite matrix Hf . In this section we show that if we are given an arbitrary minimal WTA A for f , then we can transform A into the corresponding SVTA efficiently.1 In other words, given a representation of Hf as a WTA, we can compute its SVD without the need to operate on infinite matrices. The key observation is to reduce the computation of the SVD of Hf to the computation of spectral properties of the Gram matrices GC = P>P and GT = SS> associated with the rank factorization Hf = PS induced by some minimal WTA computing f . In the case of weighted automata on strings, (Balle et al., 2015) recently showed a polynomial time algorithm for computing the Gram matrices of a string Hankel matrix by solving a system of linear equations. Unfortunately, extending their approach to the tree case requires obtaining a closed-form solution to a system of quadratic equations, which in general does not exist. Thus, we shall resort to a different\n1If the WTA given to the algorithm is not minimal, a pre-processing step can be used to minimize the input using the algorithm from (Kiefer et al., 2015).\nalgorithmic technique and show that GC and GT can be obtained as fixed points of a certain non-linear operator. This yields the iterative algorithm presented in Algorithm 2 which converges exponentially fast as shown in Theorem 5. The overall procedure to transform a WTA into the corresponding SVTA is presented in Algorithm 1.\nWe start with a simple linear algebra result showing exactly how to relate the eigendecompositions of GC and GT with the SVD of Hf .\nLemma 1. Let f : T\u2192 R be a rational function such that its Hankel matrix Hf admits an SVD. Suppose Hf = PS is a rank factorization. Then the following hold:\n1. GC = P>P and GT = SS> are finite symmetric positive definite matrices with eigendecompositions GC = VCDCV>C and GT = VTDTV>T .\n2. If M = D1/2C V>C VTD 1/2 T has SVD M = U\u0303DV\u0303>, then Hf = UDV> is an SVD,\nwhere U = PVCD\u22121/2C U\u0303, and V> = V\u0303>D \u22121/2 T V>TS.\nProof. The proof follows along the same lines as that of (Balle et al., 2015, Lemma 7).\nPutting together Lemma 1 and the proof of Theorem 2 we see that given a minimal WTA computing a strongly convergent rational function, Algorithm 1 below will compute the corresponding SVTA. Note the algorithm depends on a procedure for computing the Gram matrices GT and GC. In the remaining of this section we present one of our main results: a linearly convergent iterative algorithm for computing these matrices.\nAlgorithm 1 ComputeSVTA Input: A strongly convergent minimal WTA A Output: The corresponding SVTA\nGC,GT \u2190 GramMatrices(A) Let GT = VTDTV>T and GC = VCDCV>C be the eigendecompositions of GT and GC Let M = D1/2C V>C VTD 1/2 T and let M = UDV\n> be the singular value decomposition of M Let Q = VCD\u22121/2C UD1/2 return AQ\nLet A = \u3008\u03b1,T , {\u03c9\u03c3}\u3009 be a strongly convergent WTA of dimension n computing a function f . We now show how the Gram matrix GT can be approximated using a simple iterative scheme. Let A\u2297 = \u3008\u03b1\u2297,T \u2297, {\u03c9\u2297\u03c3 }\u3009 where \u03b1\u2297 = \u03b1\u2297\u03b1, T \u2297 = T \u2297T \u2208 Rn2\u00d7n2\u00d7n2 and \u03c9\u2297\u03c3 = \u03c9\u03c3 \u2297 \u03c9\u03c3 for all \u03c3 \u2208 \u03a3. It is shown in (Berstel and Reutenauer, 1982) that A\u2297 computes the function fA\u2297(t) = f(t)2. Note we have GT = SS> =\u2211 t\u2208T\u03c9(t)\u03c9(t)>, hence s , vec(GT) = \u2211 t\u2208T\u03c9\n\u2297(t) since \u03c9\u2297(t) = vec(\u03c9(t)\u03c9(t)>). Thus, computing the Gram matrix GT boils down to computing the vector s. The following theorem shows that this can be done by repeated applications of a non-linear operator until convergence to a fixed point.\nTheorem 4. Let F : Rn2 \u2192 Rn2 be the mapping defined by F (v) = T \u2297(I,v,v) +\u2211 \u03c3\u2208\u03a3\u03c9 \u2297 \u03c3 . Then the following hold:\n(i) s is a fixed-point of F ; i.e. F (s) = s.\n(ii) 0 is in the basin of attraction of s; i.e. limk\u2192\u221e F k(0) = s.\n(iii) The iteration defined by s0 = 0 and sk+1 = F (sk) converges linearly to s; i.e. there exists 0 < \u03c1 < 1 such that \u2016sk \u2212 s\u20162 \u2264 O(\u03c1k).\nProof. See Appendix C.\nThough we could derive a similar iterative algorithm for computing GC, it turns out that knowledge of s = vec(GT) provides an alternative, more efficient procedure for obtaining GC. Like before, we have GC = P>P = \u2211 c\u2208C\u03b1(c)\u03b1(c)> and \u03b1\u2297(c) =\n\u03b1(c) \u2297 \u03b1(c) for all c \u2208 C, hence q , vec(GC) = \u2211 c\u2208C\u03b1\n\u2297(c). By defining the matrix E = T \u2297(I, s, I) + T \u2297(I, I, s) which only depends on T and s, we can use the expression \u03b1\u2297 >(c) = \u03b1\u2297>\u039eA\u2297(c) to see that:\nq> = \u2211 c\u2208C (\u03b1\u2297)>\u039eA\u2297(c) = (\u03b1\u2297)> \u2211 k\u22650 Ek = (\u03b1\u2297)>(I\u2212E)\u22121 ,\nwhere we used the facts Ek = \u2211 c\u2208Ck \u039eA\u2297(c) and \u03c1(E) < 1 shown in the proof of Theorem 4. Algorithm 2 summarizes the overall approximation procedure for the Gram matrices, which can be done to an arbitrary precision. There, reshape(\u00b7, n\u00d7n) is an operation that takes an n2-dimensional vector and returns the n\u00d7n matrix whose first column contains the first n entries in the vector and so on. Theoretical guarantees on the convergence rate of this algorithm are given in the following theorem.\nTheorem 5. There exists 0 < \u03c1 < 1 such that after k iterations in Algorithm 2, the approximations G\u0302C and G\u0302T satisfy \u2016GC \u2212 G\u0302C\u2016F \u2264 O(\u03c1k) and \u2016GT \u2212 G\u0302T\u2016F \u2264 O(\u03c1k).\nProof. See Appendix D.\nAlgorithm 2 GramMatrices Input: A strongly convergent minimal WTA A = \u3008\u03b1,T , {\u03c9\u03c3}\u3009 Output: Gram matrices G\u0302C ' \u2211 c\u2208C\u03b1A(c)\u03b1A(c)> and G\u0302T ' \u2211 t\u2208T\u03c9A(t)\u03c9A(t)>\nLet T \u2297 = T \u2297 T \u2208 Rn2\u00d7n2\u00d7n2 , and let \u03c9\u2297\u03c3 = \u03c9\u03c3 \u2297 \u03c9\u03c3 \u2208 Rn 2 for all \u03c3 \u2208 \u03a3. Let I be the n2 \u00d7 n2 identity matrix and let s = 0 \u2208 Rn2 repeat s\u2190 T \u2297(I, s, s) + \u2211 \u03c3\u2208\u03a3\u03c9 \u2297 \u03c3 until convergence q = (\u03b1\u2297\u03b1)> ( I\u2212 T \u2297(I, I, s)\u2212 T \u2297(I, s, I)\n)\u22121 G\u0302T = reshape(s, n\u00d7 n) G\u0302C = reshape(q, n\u00d7 n) return G\u0302C, G\u0302T"}, {"heading": "4 Approximation Error of an SVTA Truncation", "text": "In this section, we analyze the approximation error induced by the truncation of an SVTA. We recall that given a SVTA A = \u3008\u03b1,T , {\u03c9\u03c3}\u3009, its truncation to n\u0302 states is the automaton\nA\u0302 = \u3008\u03a0\u03b1,T (\u03a0>,\u03a0>,\u03a0>), {\u03a0\u03c9\u03c3}\u3009\nwhere \u03a0 = [I | 0] \u2208 Rn\u0302\u00d7n is the projection matrix which removes the states associated with the n\u2212 n\u0302 smallest singular values of the Hankel matrix.\nIntuitively, the states associated with the smaller singular values are the ones with the less influence on the Hankel matrix, thus they should also be the states having the less effect on the computation of the SVTA. The following theorem support this intuition by showing a fundamental relation between the singular values of the Hankel matrix of a rational function f and the parameters of the SVTA computing it.\nTheorem 6. Let A = \u3008\u03b1,T , {\u03c9\u03c3}\u03c3\u2208\u03a3\u3009 be a SVTA with n states realizing a function f and let s1 \u2265 s2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 sn be the singular values of the Hankel matrix Hf . Then, for any t \u2208 T and i, j, k \u2208 [n] the following hold:\n\u2022 |\u03c9(t)i| \u2264 \u221a si , \u2022 |\u03b1i| \u2264 \u221a si , and\n\u2022 |T (i, j, k)| \u2264 min{ \u221a si\u221a sj \u221a sk , \u221a sj\u221a si \u221a sk , \u221a sk\u221a si \u221a sj }.\nProof. See Appendix E.\nTwo important properties of SVTAs follow from this proposition. First, the fact that |\u03c9(t)i| \u2264 \u221a si implies that the weights associated with states corresponding to small singular values are small. Second, this proposition gives us some intuition on how the states of an SVTA interact with each other. To see this, let M = T (\u03b1, I, I) and remark that for a tree t = (t1, t2) \u2208 T we have f(t) = \u03c9(t1)>M\u03c9(t2). Using the previous theorem one can show that\n|M(i, j)| \u2264 n \u221a\nmin{si, sj} max{si, sj} ,\nwhich tells us that two states corresponding to singular values far away from each other have very little interaction in the computations of the automata.\nTheorem 6 is key to proving the following theorem, which is the main result of this section. It shows how the approximation error induced by the truncation of an SVTA is impacted by the magnitudes of the singular values associated with the removed states.\nTheorem 7. Let A = \u3008\u03b1,T , {\u03c9\u03c3}\u03c3\u2208\u03a3\u3009 be a SVTA with n states realizing a function f and let s1 \u2265 s2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 sn be the singular values of the Hankel matrix Hf . Let f\u0302 be the function computed by the SVTA truncation of A to n\u0302 states. The following holds for any \u03b5 > 0:\n\u2022 For any tree t \u2208 T of size M , if M < log\n( 1\nsn\u0302+1\n) + log (\u03b5)\n2 logn then |f(t)\u2212 f\u0302(t)| < \u03b5.\n\u2022 Furthermore, if M < log\n( 1\nsn\u0302+1\n) + log(\u03b5)\nlog(4|\u03a3|n2) \u2212 1 then \u2211\nt:size(t)<M |f(t)\u2212 f\u0302(t)| < \u03b5.\nProof. See Appendix F.\nSince sn\u0302+1 > sn\u0302+2 > \u00b7 \u00b7 \u00b7 > sn, this theorem shows that the smaller the singular values associated with the removed states are, the better will be the approximation. As a direct consequence, the error introduced by the truncation grows with the number of states removed. The dependence on the size of the trees comes from the propagation of the error during the contractions of the tensor T\u0302 of the truncated SVTA.\nThe decay of singular values can be very slow in the worst case, but in practice is not unusual to observe an exponential decay on the tail. For example, this is shown to be the case for the SVTA we compute in Section 5. Assuming such an exponential decay of the form si = C\u03b8i for some 0 < \u03b8 < 1, the second bound above on the size of the trees for which \u2211 size(t)<M |f(t)\u2212 f\u0302(t)| < \u03b5 specializes to\n(n\u0302+ 1) log(1/\u03b8) + log(\u03b5) + log(C) log(4|\u03a3|n2) .\nIt is interesting to observe that the dependence of this bound on the number of total/removed states is O(n\u0302/ log(n))."}, {"heading": "5 Experiments", "text": "In this section, we assess the performance of our method on a model arising from realworld data, by using a PCFG learned from a text corpus as our initial model. Before presenting our experimental setup and results, we recall the standard mapping between WCFG and WTA."}, {"heading": "5.1 Converting WCFG to WTA", "text": "A weighted context-free grammar (WCFG) in Chomsky normal form is a tuple G = \u3008N ,\u03a3,R,weight\u3009 where N is the finite set of nonterminal symbols, \u03a3 is the finite set of words, with \u03a3 \u2229 N = \u2205, R is a set of rules having the form (a\u2192 bc), (a\u2192 x) or (\u2192 a) for a, b, c \u2208 N , x \u2208 \u03a3, and weight : R \u2192 R is the weight function which is extended to the set of all possible rules by letting weight(\u03b4) = 0 for all rules \u03b4 6\u2208 R.\nA WCFG G assigns a weight to each derivation tree \u03c4 of the grammar given by weight(\u03c4) = \u220f \u03b4\u2208Rw(\u03b4)]\u03b4(\u03c4) (where ]\u03b4(\u03c4) is the number of times the rule \u03b4 appears in\n\u03c4), and it computes a function fG : \u03a3+ \u2192 R defined by fG(w) = \u2211 \u03c4\u2208T (w) weight(\u03c4) for any w \u2208 \u03a3+, where T (w) is the set of trees deriving the word w. Given a WCFG G, we can build a WTA that assigns to each binary tree t \u2208 T\u03a3 the sum of the weights of all derivation trees of G having the same topology as t. Let G = \u3008N ,\u03a3,R, w\u3009 be a WCFG in normal form with N = [n]. Let A = \u3008\u03b1,T , {\u03c9\u03c3}\u03c3\u2208\u03a3\u3009 be the WTA with n states defined by \u03b1(i) = weight(\u2192 i) for all i \u2208 [n], T (i, j, k) = weight(i \u2192 jk) for all i, j, k \u2208 [n], and \u03c9\u03c3(i) = weight(i \u2192 \u03c3) for all i \u2208 [n], \u03c3 \u2208 \u03a3. Then for all w \u2208 \u03a3+ we have fG(w) = \u2211 t\u2208T\u03a3:\u3008t\u3009=w fA(t) . It is important to note that in this conversion the number of states in A corresponds to the number of non-terminals in G. A similar construction can be used to convert any WTA to a WCFG where each state in the WTA is mapped to a non-terminal in the WCFG."}, {"heading": "5.2 Experimental Setup and Results", "text": "In our experiments, we used the annotated corpus of german newspaper texts NEGRA (Skut et al., 1997). We use a standard setup, in which the first 18,602 sentences are used as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set Stest. All trees are binarized as described in (Cohen et al., 2013b). We extract a binary grammar in Chomsky normal form from the data, and then estimate its probabilities using maximum likelihood. The resulting PCFG has n = 211 nonterminals. We compare our method against the ones described in (Cohen et al.,\n2013a), who use tensor decomposition algorithms (Chi and Kolda, 2012) to decompose the tensors of an underlying PCFG.2\nWe used three evaluation measures: `2 distance (between the functions of type T\u03a3 \u2192 R computed by the original WTA and the one computed by its approximation), perplexity on a test set, and parsing accuracy on a test set (comparing the tree topology of parses using the bracketing F-measure). Because the number of states on a WTA and the CP-rank of tensor decomposition method are not directly comparable, we plotted the results using the number of parameters needed to specify the model on the horizontal axis. This number is equal to n\u03023 for a WTA with n\u0302 states, and it is equal to 3Rn when the tensor T is approximated with a tensor of CP-rank R (note in both cases these are the number of parameters needed to specify the tensor occurring in the model). The `2 distance between the original function f and its minimization f\u0302 , \u2016f \u2212 f\u0302\u201622 =\u2211 t\u2208T(f(t) \u2212 f\u0302(t))2, can be approximated to an arbitrary precision using the Gram matrices of the corresponding WTA (which follows from observing that (f \u2212 f\u0302)2 is rational). The perplexity of f\u0302 is defined by 2\u2212Htest , where Htest = \u2211 t\u2208Stest f(t) log2 f\u0302(t) and both f and f\u0302 have been normalized to sum to one over the test set. The results are plotted in Figure 2, where an horizontal dotted line represents the performance of the original model. We see that our method outperforms the tensor decomposition methods both in terms of `2 distance and perplexity. We also remark that our method obtains very smooth curves, which comes from the fact that it does not suffer from local optima problems like the tensor decomposition methods.\nFor parsing we use minimum Bayes risk decoding, maximizing the sum of the marginals for the nonterminals in the grammar, essentially choosing the best tree topology given a string (Goodman, 1996). The results for various length of sentences are shown in Figure 3, where we see that our method does not perform as well as the tensor decomposition methods in terms of parsing accuracy on long sentences. In this figure, we also plotted the results for a slight modification of our method (SVTA\u2217) that is able to achieve competitive performances. The SVTA\u2217 method gives more importance to long sentences in the minimization process. This is done by finding the highest constant \u03b3 > 0 such that the function f\u03b3 : t 7\u2192 \u03b3size(t)f(t) is still strongly convergent. This function is then approximated by a low-rank WTA computing f\u0302\u03b3 , and we let f\u0302 : t 7\u2192 \u03b3\u2212size(t)f\u0302\u03b3(t) (which is rational). In our experiment, we used \u03b3 = 2.4. While the SVTA\u2217 method improved the parsing accuracy, it had no significant repercussion on the `2 and per-\n2We use two tensor decomposition algorithms from the tensor Matlab toolbox: pqnr, which makes use of projected quasi-Newton and mu, which uses a multiplicative update. See http://www.sandia. gov/\u02dctgkolda/TensorToolbox/index-2.6.html.\nplexity measures. We believe that the parsing accuracy of our method could be further improved. Seeking techniques that combines the benefits of SVTA and previous works is a promising direction."}, {"heading": "6 Conclusion", "text": "We described a technique for approximate minimization of WTA, yielding a model smaller than the original one which retains good approximation properties. Our main algorithm relies on a singular value decomposition of an infinite Hankel matrix induced by the WTA. We provided theoretical guarantees on the error induced by our minimization method. Our experiments with real-world parsing data show that the minimized WTA, depending on the number of singular values used, approximates well the original WTA on three measures: perplexity, bracketing accuracy and `2 distance of the tree weights. Our work has connections with spectral learning techniques for WTA, and exhibits similar properties as those algorithms; e.g. absence of local optima. In future work we plan to investigate the applications of our approach to the design and analysis of improved spectral learning algorithms for WTA."}, {"heading": "A Proof of Theorem 2", "text": "Theorem. Let f : T \u2192 R be rational. If Hf = PS is a rank factorization, then there exists a minimal WTA A computing f such that PA = P and SA = S.\nProof. Let n = rank(f). Let B be an arbitrary minimal WTA computing f . Suppose B induces the rank factorization Hf = P\u2032S\u2032. Since the columns of both P and P\u2032 are basis for the column-span of Hf , there must exists a change of basis Q \u2208 Rn\u00d7n between P and P\u2032. That is, Q is an invertible matrix such that P\u2032Q = P. Furthermore, since P\u2032S\u2032 = Hf = PS = P\u2032QS and P\u2032 has full column rank, we must have S\u2032 = QS, or equivalently, Q\u22121S\u2032 = S. Thus, we let A = BQ, which immediately verifies fA = fB = f . It remains to be shown that A induces the rank factorization Hf = PS. Note that when proving the equivalence fA = fB we already showed \u03c9A(t) = Q\u22121\u03c9B(t), which means we have SA = Q\u22121S\u2032 = S. To show PA = P\u2032Q we need to show that for any c \u2208 C we have \u03b1A(c)> = \u03b1B(c)>Q. This will immediately follow if we show that \u039eA(c) = Q\u22121\u039eB(c)Q. If we proceed by induction on drop(c), we see the case c = \u2217 is immediate, and for c = (c\u2032, t) we get\n\u039eA((c\u2032, t)) = (T (Q\u2212>,Q,Q))(I,\u039eA(c\u2032),\u03c9A(t)) = (T (Q\u2212>,Q,Q))(I,Q\u22121\u039eB(c\u2032)Q,Q\u22121\u03c9B(t)) = T (Q\u2212>,\u039eB(c\u2032)Q,\u03c9B(t)) = Q\u22121T (I,\u039eB(c\u2032),\u03c9B(t))Q .\nApplying the same argument mutatis mutandis for c = (t, c\u2032) completes the proof."}, {"heading": "B Proof of Theorem 3", "text": "Theorem. If f : T\u03a3 \u2192 R is rational and strongly convergent, then Hf admits a singular value decomposition.\nProof. The result will follow if we show that Hf is the matrix of a compact operator on a Hilbert space (Conway, 1990). The main obstruction to this approach is that the rows and columns of Hf are indexed by different objects (trees vs. contexts). Thus, we will need to see Hf as an operator on a larger space that contains both these objects.\nRecall we have T\u03a3 \u2282 T\u03a3\u2032 and C\u03a3 \u2282 T\u03a3\u2032 . Given two functions g, g\u2032 : T\u03a3\u2032 \u2192 R we define their inner product to be \u3008g, g\u2032\u3009 = \u2211 t\u2032\u2208T\u03a3\u2032 g(t \u2032)g\u2032(t\u2032). Let \u2016g\u2016 = \u221a | \u3008g, g\u3009 | be the induced norm and let T be the space of all functions g : T\u03a3\u2032 \u2192 R such that \u2016g\u2016 <\u221e. Note that T with a Hilbert space, and that since T\u03a3\u2032 is countable, it actually is a separable Hilbert space isomorphic to `2, the spaces of infinite square summable sequences. Given set X \u2282 T\u03a3\u2032 we define T(X) = {g \u2208 T | g(t\u2032) = 0, t\u2032 \u2208 T\u03a3\u2032 \\ X}.\nNow let Cf : T\u2192 T be the linear operator on T given by\n(Cfg)(t\u2032) = {\u2211 t\u2208T\u03a3 f(t \u2032[t])g(t) if t\u2032 \u2208 C\u03a3\n0 if t\u2032 /\u2208 C\u03a3 .\nNow note that by construction we have T(T\u03a3) \u2286 Ker(Cf ) and Im(Cf ) \u2286 T(C\u03a3). Hence, a simple calculation shows that given the decompositions Cf : T(T\u03a3)\u22a5 \u2295 T(T\u03a3) \u2192 T(C\u03a3)\u2295 T(C\u03a3)\u22a5, the matrix of Cf is\nCf = [\nHf 0 0 0\n] .\nThus, if Cf is a compact operator, then Hf admits an SVD. Since Hf has finite rank, we only need to show that Cf is a bounded operator.\nGiven c \u2208 C\u03a3 we define fc \u2208 T(T\u03a3) given by fc(t) = f(c[t]) for t \u2208 T\u03a3. Now let g \u2208 T with \u2016g\u2016 = 1 and recall Cf is bounded if \u2016Cfg\u2016 < \u221e for every g \u2208 T with \u2016g\u2016 = 1. Indeed, because f is strongly convergent we have:\n\u2016Cfg\u20162 = \u2211 t\u2032\u2208T\u03a3\u2032 (Cfg)(t\u2032)2\n= \u2211 c\u2208C\u03a3 (Cfg)(c)2 = \u2211 c\u2208C\u03a3 \u2211 t\u2208T\u03a3 f(c[t])g(t) 2\n= \u2211 c\u2208C\u03a3 \u3008fc, g\u30092\n\u2264 \u2016g\u20162 \u2211 c\u2208C\u03a3 \u2016fc\u20162\n= \u2211 c\u2208C\u03a3 \u2211 t\u2032\u2208T\u03a3\u2032 fc(t\u2032)2\n= \u2211 c\u2208C\u03a3 \u2211 t\u2208T\u03a3 f(c[t])2\n= \u2211 t\u2208T\u03a3 |t|f(t)2\n\u2264 sup t\u2208T\u03a3 |f(t)| \u00b7 \u2211 t\u2208T\u03a3 |t||f(t)| <\u221e ,\nwhere we used the Cauchy\u2013Schwarz inequality, and the fact that supt\u2208T\u03a3 |f(t)| is bounded when f is strongly convergent."}, {"heading": "C Proof of Theorem 4", "text": "Theorem. Let F : Rn2 \u2192 Rn2 be the mapping defined by F (v) = T \u2297(I,v,v) +\u2211 \u03c3\u2208\u03a3\u03c9 \u2297 \u03c3 . Then the following hold:\n(i) s is a fixed-point of F ; i.e. F (s) = s.\n(ii) 0 is in the basin of attraction of s; i.e. limk\u2192\u221e F k(0) = s.\n(iii) The iteration defined by s0 = 0 and sk+1 = F (sk) converges linearly to s; i.e. there exists 0 < \u03c1 < 1 such that \u2016sk \u2212 s\u20162 \u2264 O(\u03c1k).\nProof. (i) We have T \u2297(I, s, s) = \u2211 t,t\u2032\u2208T T \u2297(I,\u03c9\u2297(t),\u03c9\u2297(t\u2032)) = \u2211 t,t\u2032\u2208T\u03c9\n\u2297((t, t\u2032)) =\u2211 t\u2208T\u22651 \u03c9\n\u2297(t) where T\u22651 is the set of trees of depth at least one. Hence F (s) =\u2211 t\u2208T\u22651 \u03c9 \u2297(t) + \u2211 \u03c3\u2208\u03a3\u03c9 \u2297 \u03c3 = s.\n(ii) Let T\u2264k denote the set of all trees with depth at most k. We prove by induction on k that F k(0) = \u2211 t\u2208T\u2264k \u03c9 \u2297(t), which implies that limk\u2192\u221e F k(0) = s. This is\nstraightforward for k = 0. Assuming it is true for all naturals up to k \u2212 1, we have\nF k(0) = T \u2297(I, F k\u22121(0), F k\u22121(0)) + \u2211 \u03c3\u2208\u03a3 \u03c9\u2297\u03c3\n= \u2211\nt,t\u2032\u2208T\u2264k\u22121 T \u2297(I,\u03c9\u2297(t),\u03c9\u2297(t\u2032)) + \u2211 \u03c3\u2208\u03a3 \u03c9\u2297\u03c3\n= \u2211\nt,t\u2032\u2208T\u2264k\u22121 \u03c9\u2297((t, t\u2032)) + \u2211 \u03c3\u2208\u03a3 \u03c9\u2297\u03c3\n= \u2211 t\u2208T\u2264k \u03c9\u2297(t) .\n(iii) Let E be the Jacobian of F around s, we show that the spectral radius \u03c1(E) of E is less than one, which implies the result by Ostrowski\u2019s theorem (see (Ortega, 1990, Theorem 8.1.7)).\nSince A is minimal, there exists trees t1, \u00b7 \u00b7 \u00b7 , tn \u2208 T and contexts c1, \u00b7 \u00b7 \u00b7 , cn \u2208 C such that both {\u03c9(ti)}i\u2208[n] and {\u03b1(ci)}i\u2208[n] are sets of linear independent vectors in Rn (Bailly et al., 2010). Therefore, the sets {\u03c9(ti)\u2297\u03c9(tj)}i,j\u2208[n] and {\u03b1(ci)\u2297\u03b1(cj)}i,j\u2208[n] are sets of linear independent vectors in Rn2 . Let v \u2208 Rn2 be an eigenvector of E with eigenvalue \u03bb 6= 0, and let v = \u2211 i,j\u2208[n] \u03b2i,j(\u03c9(ti) \u2297 \u03c9(tj)) be its expression in terms of the basis given by {\u03c9(ti)\u2297 \u03c9(tj)}. For any vector u \u2208 {\u03b1(ci)\u2297\u03b1(cj)} we have\nlim k\u2192\u221e u>Ekv \u2264 lim k\u2192\u221e |u>Ekv| \u2264 \u2211 i,j\u2208[n] |\u03b2i,j | lim k\u2192\u221e |u>Ek(\u03c9(ti)\u2297 \u03c9(tj))| = 0 ,\nwhere we used Lemma 2 in the last step. Since this is true for any vector u in the basis {\u03b1(ci) \u2297 \u03b1(cj)}, we have limk\u2192\u221eEkv = limk\u2192\u221e |\u03bb|kv = 0, hence |\u03bb| < 1. This reasoning holds for any eigenvalue of E, hence \u03c1(E) < 1.\nLemma 2. Let A = \u3008\u03b1,T , {\u03c9\u03c3}\u3009 be a minimal WTA of dimension n computing the strongly convergent function f , and let E \u2208 Rn2\u00d7n2 be the Jacobian around s =\u2211 t\u2208T\u03c9(t) \u2297 \u03c9(t) of the mapping F : v \u2192 T \u2297(I,v,v) + \u2211 \u03c3\u2208\u03a3\u03c9 \u2297 \u03c3 . Then for any c1, c2 \u2208 C and any t1, t2 \u2208 T we have limk\u2192\u221e |(\u03b1(c1)\u2297\u03b1(c2))>Ek(\u03c9(t1)\u2297 \u03c9(t2))| = 0.\nProof. Let \u039e\u2297 : C\u2192 Rn2\u00d7n2 be the context mapping associated with the WTA A\u2297; i.e. \u039e\u2297 = \u039eA\u2297 . We start by proving by induction on drop(c) that \u039e\u2297(c) = \u039e(c)\u2297\u039e(c) for all c \u2208 C. Let Cd denote the set of contexts c \u2208 C with drop(c) = d. The statement is trivial for c \u2208 C0. Assume the statement is true for all naturals up to d \u2212 1 and let c = (t, c\u2032) \u2208 Cd for some t \u2208 T and c\u2032 \u2208 Cd\u22121. Then using our inductive hypothesis we have that\n\u039e\u2297(c) = T \u2297(In2 ,\u03c9(t)\u2297 \u03c9(t),\u039e(c\u2032)\u2297\u039e(c\u2032)) = T (In,\u03c9(t),\u039e(c\u2032))\u2297 T (In,\u03c9(t),\u039e(c\u2032)) = \u039e(c)\u2297\u039e(c) .\nThe case c = (c\u2032, t) follows from an identical argument. Next we use the multi-linearity of F to expand F (s+h) for a vector h \u2208 Rn2 . Keeping the terms that are linear in h we obtain that E = T \u2297(I, s, I) + T \u2297(I, I, s). It follows that E = \u2211 c\u2208C1 \u039e\u2297(c), and it can be shown by induction on k that Ek = \u2211 c\u2208Ck \u039e\u2297(c).\nWriting dc = min(drop(c1),drop(c2)) and dt = min(depth(t1), depth(t2)), we can see that\u2223\u2223\u2223(\u03b1(c1)\u2297\u03b1(c2))>Ek(\u03c9(t1)\u2297 \u03c9(t2))\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 c\u2208Ck (\u03b1(c1)\u2297\u03b1(c2))>\u039e\u2297(c)(\u03c9(t1)\u2297 \u03c9(t2)) \u2223\u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 c\u2208Ck (\u03b1(c1)>\u039e(c)\u03c9(t1)) \u00b7 (\u03b1(c2)>\u039e(c)\u03c9(t2)) \u2223\u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 c\u2208Ck f(c1[c[t1]])f(c2[c[t2]]) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264\n\u2211 c\u2208Ck |f(c1[c[t1]])| \u2211 c\u2208Ck |f(c2[c[t2]])|  \u2264\n \u2211 t\u2208T\u2265dc+dt+k |t||f(t)| 2 , which tends to 0 with k \u2192\u221e since f is strongly convergent. To prove the last inequality, check that any tree of the form t\u2032 = c[c\u2032[t]] satisfies depth(t\u2032) \u2265 drop(c) + drop(c\u2032) + depth(t), and that for fixed c \u2208 C and t, t\u2032 \u2208 T we have |{c\u2032 \u2208 C : c[c\u2032[t]] = t\u2032}| \u2264 |t\u2032| (indeed, a factorization t\u2032 = c[c\u2032[t]] is fixed once the root of t is chosen in t\u2032, which can be done in at most |t\u2032| different ways)."}, {"heading": "D Proof of Theorem 5", "text": "Theorem. There exists 0 < \u03c1 < 1 such that after k iterations in Algorithm 2, the approximations G\u0302C and G\u0302T satisfy \u2016GC \u2212 G\u0302C\u2016F \u2264 O(\u03c1k) and \u2016GT \u2212 G\u0302T\u2016F \u2264 O(\u03c1k).\nProof. The result for the Gram matrix GT directly follows from Theorem 4. We now show how the error in the approximation of GT = reshape(s, n\u00d7 n) affects the approximation of q = (\u03b1\u2297)>(I \u2212 E)\u22121 = vec(GC). Let s\u0302 \u2208 Rn be such that \u2016s \u2212 s\u0302\u2016 \u2264 \u03b5, let E\u0302 = T \u2297(I, s\u0302, I) + T \u2297(I, I, s\u0302) and let q = (\u03b1\u2297)>(I\u2212 E\u0302)\u22121. We first bound the distance between E and E\u0302. We have\n\u2016E\u2212 E\u0302\u2016F = \u2016T \u2297(I, s\u2212 s\u0302, I) + T \u2297(I, I, s\u2212 s\u0302)\u2016F \u2264 2\u2016T \u2297\u2016F \u2016s\u2212 s\u0302\u2016 = O(\u03b5) ,\nwhere we used the bounds \u2016T (I, I,v)\u2016F \u2264 \u2016T \u2016F \u2016v\u2016 and \u2016T (I,v, I)\u2016F \u2264 \u2016T \u2016F \u2016v\u2016. Let \u03b4 = \u2016E\u2212 E\u0302\u2016 and let \u03c3 be the smallest nonzero eigenvalue of the matrix I\u2212 E. It follows from (El Ghaoui, 2002, Equation (7.2)) that if \u03b4 < \u03c3 then \u2016(I\u2212 E)\u22121 \u2212 (I\u2212 E\u0302)\u22121\u2016 \u2264 \u03b4/(\u03c3(\u03c3 \u2212 \u03b4)). Since \u03b4 = O(\u03b5) from our previous bound, the condition \u03b4 \u2264 \u03c3/2 will be eventually satisfied as \u03b5\u2192 0, in which case we can conclude that\n\u2016GC \u2212 G\u0302C\u2016F = \u2016q \u2212 q\u0302\u2016 \u2264 \u2016(I\u2212E)\u22121 \u2212 (I\u2212 E\u0302)\u22121\u2016\u2016\u03b1\u2297\u2016\n\u2264 2\u03b4 \u03c32 \u2016\u03b1\u2297\u2016 = O(\u03b5) ."}, {"heading": "E Proof of Theorem 6", "text": "Let A = \u3008\u03b1,T , {\u03c9\u03c3}\u03c3\u2208\u03a3\u3009 be a SVTA with n states realizing a function f and let s1 \u2265 s2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 sn be the singular values of the Hankel matrix Hf .\nTheorem 6 relies on the following lemma, which explores the consequences that the fixed-point equations used to compute GT and GC have for an SVTA.\nLemma 3. For all i \u2208 [n], the following hold:\n1. si = \u2211 \u03c3\u2208\u03a3\u03c9\u03c3(i)2 + \u2211n j,k=1 T (i, j, k)2sjsk ,\n2. si = \u03b1(j)2 + \u2211n j,k=1(T (j, i, k)2 + T (j, k, i)2)sjsk .\nProof. Let GT and GC be the Gram matrices associated with the rank factorization of Hf . Since A is a SVTA we have GT = GC = D where D = diag(s1, \u00b7 \u00b7 \u00b7 , sn) is a diagonal matrix with the Hankel singular values on the diagonal. The first equality then follows from the following fixed point characterization of GT:\nGT = \u2211 t\u2208T \u03c9(t)\u03c9(t)>\n= \u2211 \u03c3\u2208\u03a3 \u03c9\u03c3\u03c9 > \u03c3\n+ \u2211\nt1,t2\u2208T T (I,\u03c9(t1),\u03c9(t2))T (I,\u03c9(t1),\u03c9(t2))>\n= \u2211 \u03c3\u2208\u03a3 \u03c9\u03c3\u03c9 > \u03c3 + T(1)(GT \u2297GT)T>(1) ,\n(where T(i) denotes the matricization of the tensor T along the ith mode). The second equality follows from the following fixed point characterization of GC:\nGC = \u2211 c\u2208C \u03b1(c)\u03b1(c)>\n= \u03b1\u03b1> + \u2211\nc\u2208C,t\u2208T T (\u03b1(c),\u03c9(t), I)T (\u03b1(c),\u03c9(t), I)>\n+ \u2211\nc\u2208C,t\u2208T T (\u03b1(c), I,\u03c9(t))T (\u03b1(c), I,\u03c9(t))>\n= \u03b1\u03b1>\n+ T(2)(GC \u2297GT)T>(2) + T(3)(GC \u2297GT)T>(3) .\nTheorem. For any t \u2208 T, c \u2208 C and i, j, k \u2208 [n] the following hold:\n\u2022 |\u03c9(t)i| \u2264 \u221a si , \u2022 |\u03b1(c)i| \u2264 \u221a si , and\n\u2022 |T (i, j, k)| \u2264 min{ \u221a si\u221a sj \u221a sk , \u221a sj\u221a si \u221a sk , \u221a sk\u221a si \u221a sj }.\nProof. The third point is a direct consequence of the previous Lemma. For the first point, let UDV> be the SVD of Hf . Since A is a SVTA we have\n\u03c9(t)2i = (D1/2V>)2i,t = siV(t, i)2\nand since the rows of V are orthonormal we have V(t, i)2 \u2264 1. The inequality for contexts is proved similarly by reasoning on the rows of UD1/2."}, {"heading": "F Proof of Theorem 7", "text": "To prove Theorem 7, we will show how the computation of a WTA on a give tree t can be seen as an inner product between two tensors, one which is a function of the topology of the tree, and one which is a function of the labeling of its leafs (Proposition 1). We will then show a fundamental relation between the components of the first tensor and the singular values of the Hankel matrix when the WTA is in SVTA normal form (Proposition 2); this proposition will allow us to show Lemma 4 that bounds the difference between components of this first tensor for the original SVTA and its truncation. We will finally use this lemma to bound the absolute error introduced by the truncation of an SVTA (Propositions 3 and 4).\nWe first introduce another kind of contexts than the one introduced in Section 2, where every leaf of a binary tree is labeled by the special symbol \u2217 (which still acts as a place holder). Let B be the set of binary trees on the one-letter alphabet {\u2217}. We will call a tree b \u2208 B a multicontext. For any integer M \u2265 1 we let\nBM = {b \u2208 B : |\u3008b\u3009| = M}\nbe the subset of multicontexts with M leaves (equivalently, BM is the subset of multicontexts of size M \u2212 1). Given a word w = w1 \u00b7 \u00b7 \u00b7wM \u2208 \u03a3\u2217 and a multicontext b \u2208 BM , we denote by b[w1, \u00b7 \u00b7 \u00b7 , wM ] \u2208 T\u03a3 the tree obtained by replacing the ith occurrence of \u2217 in b by wi for i \u2208 [M ]. Let b \u2208 BM , for any integer m \u2208 [M ] we denote by bJmK \u2208 BM+1 the multicontext obtained by replacing the mth occurence of \u2217 in b by the tree (\u2217, \u2217). Let M > 1, it is easy to check that for any b\u2032 \u2208 BM , there exist b \u2208 BM\u22121 and m \u2208 [M \u2212 1] satisfying b\u2032 = bJmK. See Figure 4 for some illustrative examples.\nWe now show how the computation of a WTA on a given tree with M leaves can be seen as an inner product between two Mth order tensors: the first one depends on the topology of the tree, while the second one depends on the labeling of its leaves.\nLet A = \u3008\u03b1,T , {\u03c9\u03c3}\u03c3\u2208\u03a3\u3009 be a WTA with n states computing a function f . Given a multicontext b \u2208 BM , we denote by \u03b2A(b) \u2208 \u2297M i=1 Rn the Mth order tensor inductively defined by \u03b2A(\u2217) = \u03b1 and\n\u03b2A(bJmK)i1\u00b7\u00b7\u00b7iM = n\u2211 k=1 \u03b2A(b)i1\u00b7\u00b7\u00b7im\u22121kim+2\u00b7\u00b7\u00b7iMT kimim+1\nfor any b \u2208 BM\u22121, m \u2208 [M \u22121] and i1, \u00b7 \u00b7 \u00b7 , iM \u2208 [n] (i.e. \u03b2A(bJmK) is the contraction of \u03b2A(b) along the mth mode and T along the first mode). Given a word w = w1 \u00b7 \u00b7 \u00b7wM \u2208 \u03a3\u2217, we let \u03c8A(w) \u2208 \u2297M i=1 Rn be the Mth order tensor defined by\n\u03c8A(w)i1\u00b7\u00b7\u00b7iM = \u03c9(w1)i1\u03c9(w2)i2 \u00b7 \u00b7 \u00b7\u03c9(wM )iM = M\u220f m=1 \u03c9(wm)im\nfor i1, \u00b7 \u00b7 \u00b7 , iM \u2208 [n] (i.e. \u03c8A(w) is the tensor product of the \u03c9(wi)\u2019s). We will simply write \u03b2 and \u03c8 when the automaton is clear from context.\nProposition 1. For any multicontext b \u2208 BM and any word w = w1 \u00b7 \u00b7 \u00b7wM \u2208 \u03a3\u2217 we have\nf(b[w1, \u00b7 \u00b7 \u00b7 , wM ]) = \u3008\u03b2(b),\u03c8(w)\u3009 ,\nwhere the inner product between two M th order tensors U and V is defined by \u3008U ,V\u3009 =\u2211 i1\u00b7\u00b7\u00b7iM U(i1, \u00b7 \u00b7 \u00b7 , iM )V(i1, \u00b7 \u00b7 \u00b7 , iM ).\nSketch of proof. Let b \u2208 BM and w = w1 \u00b7 \u00b7 \u00b7wM \u2208 \u03a3\u2217. Let b1 \u2208 BM\u22121 and m \u2208 [M \u22121] be such that b = b1JmK. In order to lighten the notations and without loss of generality we assume that m = 1. One can check that\n\u3008\u03b2(b),\u03c8(w)\u3009 =\u03b2(b)(\u03c9w1 , \u00b7 \u00b7 \u00b7 ,\u03c9wM ) =\u03b2(b1J1K)(\u03c9w1 , \u00b7 \u00b7 \u00b7 ,\u03c9wM ) =\u03b2(b1)(\u03c9((w1, w2)),\u03c9w3 , \u00b7 \u00b7 \u00b7 ,\u03c9wM ) .\nThe same reasoning can now be applied to b1. Assume for example that b1 = b2J1K for some b2 \u2208 BM\u22122, we would have\n\u3008\u03b2(b),\u03c8(w)\u3009 = \u03b2(b1)(\u03c9((w1, w2)),\u03c9w3 , \u00b7 \u00b7 \u00b7 ,\u03c9wM ) = \u03b2(b2J1K)(\u03c9((w1, w2)),\u03c9w3 , \u00b7 \u00b7 \u00b7 ,\u03c9wM ) = \u03b2(b2)(\u03c9(((w1, w2), w3)),\u03c9w4 , \u00b7 \u00b7 \u00b7 ,\u03c9wM ) .\nBy applying the same argument again and again we will eventually obtain\n\u3008\u03b2(b),\u03c8(w)\u3009 = \u03b2(bM\u22121)(\u03c9(b[w1, \u00b7 \u00b7 \u00b7 , wM ])) = \u03b2(\u2217)(\u03c9(b[w1, \u00b7 \u00b7 \u00b7 , wM ])) = \u03b1>\u03c9(b[w1, \u00b7 \u00b7 \u00b7 , wM ]) = f(b[w1, \u00b7 \u00b7 \u00b7 , wM ]) .\nSuppose now that A = \u3008\u03b1,T , {\u03c9\u03c3}\u03c3\u2208\u03a3\u3009 is an SVTA with n states for f and let s1 \u2265 s2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 sn be the singular values of the Hankel matrix Hf . The following proposition shows a relation \u2014 similar to the one presented in Theorem 6 \u2014 between the components of the tensor \u03b2(b) (for any multicontext b) and the singular values of the Hankel matrix.\nProposition 2. If A = \u3008\u03b1,T , {\u03c9\u03c3}\u03c3\u2208\u03a3\u3009 is an SVTA, then for any b \u2208 BM and any i1, \u00b7 \u00b7 \u00b7 , iM \u2208 [n] the following holds:\n|\u03b2(b)i1\u00b7\u00b7\u00b7iM | \u2264 nM\u22121 min p\u2208[M ] {sip} M\u220f m=1 1 \u221a sim .\nProof. We proceed by induction on M . If M = 1 we have b = \u2217 and\n|\u03b2(\u2217)i| = |\u03b1i| \u2264 \u221a si = si\u221a si .\nSuppose the result holds for multicontexts in BM\u22121 and let b\u2032 \u2208 BM . Let m \u2208 [M ] and b \u2208 BM\u22121 be such that b\u2032 = bJmK. Without loss of generality and to lighten the notations we assume that m = 1. Start by writing:\n|\u03b2(b\u2032)i1\u00b7\u00b7\u00b7iM | = |\u03b2(bJ1K)i1\u00b7\u00b7\u00b7iM | = \u2223\u2223\u2223\u2223\u2223 n\u2211 k=1 \u03b2A(b)ki3\u00b7\u00b7\u00b7iMT ki1i2 \u2223\u2223\u2223\u2223\u2223 \u2264 n\u2211 k=1 |\u03b2A(b)ki3\u00b7\u00b7\u00b7iMT ki1i2 |\nRemarking that the third inequality in Theorem 6 can be rewritten as |T ijk| \u2264 min{si,sj ,sk}\u221asi\u221asj\u221ask , we have for any k \u2208 [n]:\n|\u03b2A(b)ki3\u00b7\u00b7\u00b7iMT ki1i2 | \u2264n M\u22122 min{sk, si3 , \u00b7 \u00b7 \u00b7 , siM } 1 \u221a sk M\u220f m=3 1 \u221a sim min{sk, si1 , si2}\u221a sk \u221a si1 \u221a si2\n=nM\u22122 1 sk M\u220f m=1 1 \u221a sim min{sk, si3 , \u00b7 \u00b7 \u00b7 , siM }min{sk, si1 , si2}\n\u2264nM\u22122 min p\u2208[M ] {sip} M\u220f m=1 1 \u221a sim ,\nwhere we used that\nmin{sk, si3 , \u00b7 \u00b7 \u00b7 , siM }min{sk, si1 , si2} \u2264 sk min{si1 , \u00b7 \u00b7 \u00b7 , siM }\nSumming over k yields the desired bound.\nLet f\u0302 be the function computed by the SVTA truncation of A to n\u0302 states. Let \u03a0 \u2208 Rn\u00d7n be the diagonal matrix defined by \u03a0(i, i) = 1 if i \u2264 n\u0302 and 0 otherwise. It is easy to check that the WTA A\u0302 = \u3008\u03b1\u0302, T\u0302 , \u03c9\u0302\u03c3\u3009, where \u03b1\u0302 = \u03a0\u03b1, T\u0302 = T (I,\u03a0,\u03a0) and \u03c9\u0302\u03c3 = \u03c9\u03c3, computes the function f\u0302 . We let \u03c9\u0302(t) = \u03c9A\u0302(t) for any tree t and similarly for \u03b1\u0302(c), \u03c8\u0302(w) and \u03b2\u0302(c).\nWe can now prove the following Lemma that bounds the absolute difference between the components of the tensors \u03b2(b) and \u03b2\u0302(b) for a given multicontext b.\nLemma 4. For any b \u2208 BM and any i1, \u00b7 \u00b7 \u00b7 iM \u2208 [n] we have\n|(\u03b2(b)\u2212 \u03b2\u0302(b))i1\u00b7\u00b7\u00b7iM | \u2264 sn\u0302+1 nM\u22121 M\u220f m=1 1 \u221a sim .\nProof. It is easy to check that when there exists at least one m \u2208 [M ] such that im > n\u0302, we have \u03b2\u0302(b)i1\u00b7\u00b7\u00b7iM = 0, hence\n|(\u03b2(b)\u2212 \u03b2\u0302(b))i1\u00b7\u00b7\u00b7iM | = |\u03b2(b)i1\u00b7\u00b7\u00b7iM |\nand the result directly follows from Proposition 2. Suppose i1, \u00b7 \u00b7 \u00b7 , iM \u2208 [n\u0302], we proceed by induction on M . If M = 1 then b = \u2217, thus\n|\u03b2(\u2217)i \u2212 \u03b2\u0302(\u2217)i| = |\u03b1i \u2212 \u03b1\u0302i| = 0\nfor all i \u2208 [n\u0302]. Suppose the result holds for multicontexts in BM\u22121 and let b\u2032 \u2208 BM . Let b \u2208 BM\u22121 and m \u2208 [M \u2212 1] be such that b\u2032 = bJmK. To lighten the notations we assume without loss of generality that m = 1. We have\n|(\u03b2(b\u2032)\u2212 \u03b2\u0302(b\u2032))i1\u00b7\u00b7\u00b7iM | =|(\u03b2(bJ1K)\u2212 \u03b2\u0302(bJ1K))i1\u00b7\u00b7\u00b7iM | (2)\n\u2264 n\u0302\u2211 k=1 |T ki1i2 | |(\u03b2(b)\u2212 \u03b2\u0302(b))ki3\u00b7\u00b7\u00b7iM | (3)\n+ n\u2211\nk=n\u0302+1 |T ki1i2 | |\u03b2(b)ki3\u00b7\u00b7\u00b7iM | (4)\n\u2264 n\u0302\u2211 k=1 \u221a sk\u221a si1si2 \u00b7 sn\u0302+1 n M\u22122 \u221a sk \u221a si3 \u00b7 \u00b7 \u00b7 \u221a siM\n(5)\n+ n\u2211\nk=n\u0302+1\n\u221a sk\u221a\nsi1si2 \u00b7 min{sk, si3 , \u00b7 \u00b7 \u00b7 , siM } n M\u22122 \u221a sk \u221a si3 \u00b7 \u00b7 \u00b7 \u221a siM\n(6)\n\u2264 sn\u0302+1 nM\u22121 M\u220f m=1 1 \u221a sim . (7)\nTo decompose (2) in (3) and (4) we used the fact that T ki1i2 = T\u0302 ki1i2 whenever k \u2264 n\u0302 and \u03b2\u0302(b)ki3\u00b7\u00b7\u00b7iM = 0 whenever k > n\u0302. We bounded (3) by (5) using the induction hypothesis, while we used Proposition 2 to bound (4) by (6).\nProposition 3. Let t \u2208 T be a tree of size M , then\n|f(t)\u2212 f\u0302(t)| \u2264 n2M\u22121sn\u0302+1 .\nProof. Let t \u2208 T be a tree of size M \u2212 1, then there exists a (unique) b \u2208 BM and a (unique) word w = w1 \u00b7 \u00b7 \u00b7wM \u2208 \u03a3\u2217 such that t = b[w1, \u00b7 \u00b7 \u00b7 , wM ]. Since \u03c9\u03c3 = \u03c9\u0302\u03c3 for all \u03c3 \u2208 \u03a3, we have \u03c8(x) = \u03c8\u0302(x) for all x \u2208 \u03a3\u2217. Furthermore, since \u03c9\u03c3(i)2 \u2264 si for all i \u2208 [n], we have\n|\u03c8(w)i1\u00b7\u00b7\u00b7iM | \u2264 M\u220f m=1 \u221a sim .\nIt follows that |f(t)\u2212 f\u0302(t)| = \u2223\u2223\u2223\u3008\u03b2(b),\u03c8(w)\u3009 \u2212 \u3008\u03b2\u0302(b), \u03c8\u0302(w)\u3009\u2223\u2223\u2223\n= \u2223\u2223\u2223\u3008\u03b2(b)\u2212 \u03b2\u0302(b),\u03c8(w)\u3009\u2223\u2223\u2223 \u2264\nn\u2211 i1=1 \u00b7 \u00b7 \u00b7 n\u2211 iM=1 |(\u03b2(b)\u2212 \u03b2\u0302(b))i1\u00b7\u00b7\u00b7iM | |\u03c8(w)i1\u00b7\u00b7\u00b7iM |\n\u2264 n\u2211\ni1=1 \u00b7 \u00b7 \u00b7 n\u2211 iM=1 sn\u0302+1 n M\u22121 M\u220f m=1 1 \u221a sim \u00b7 M\u220f m=1 \u221a sim\n=n2M\u22121sn\u0302+1\nProposition 4. Let S = |\u03a3| be the size of the alphabet. For any integer M we have\n\u2211 t\u2208T:\nsize(t)<M\n|f(t)\u2212 f\u0302(t)| \u2264 (4Sn 2)M+1 \u2212 1\n(4Sn2)\u2212 1 sn\u0302+1 .\nProof. For any integer m there are less than 4m binary trees with m internal nodes (which is a bound on the m-th Catalan number) and each one of these trees has m+ 1 leaves, thus Sm+1 possible labelling of the leaves. Using the previous proposition we get\n\u2211 t\u2208T:\nsize(t)<M\n|f(t)\u2212 f\u0302(t)| = M\u22121\u2211 m=0 \u2211 t\u2208T:\nsize(t)=m\n|f(t)\u2212 f\u0302(t)|\n\u2264 M\u22121\u2211 m=0 4mSm+1 \u00b7 n2(m\u22121)sn\u0302+1\n\u2264 M\u2211 m=1 (4Sn2)msn\u0302+1 \u2264 (4Sn 2)M+1 \u2212 1\n(4Sn2)\u2212 1 sn\u0302+1.\nTheorem. Let A = \u3008\u03b1,T , {\u03c9\u03c3}\u03c3\u2208\u03a3\u3009 be a SVTA with n states realizing a function f and let s1 \u2265 s2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 sn be the singular values of the Hankel matrix Hf . Let f\u0302 be the function computed by the SVTA truncation of A to n\u0302 states.\nLet S = |\u03a3| be the size of the alphabet, let M be an integer and let \u03b5 > 0.\n\u2022 For any tree t \u2208 T of size M , if M < log\n( 1\nsn\u0302+1\n) + log (\u03b5)\n2 logn then |f(t)\u2212 f\u0302(t)| < \u03b5.\n\u2022 If M < log\n( 1\nsn\u0302+1\n) + log(\u03b5)\nlog(4Sn2) \u2212 1 then \u2211\nt:size(t)<M |f(t)\u2212 f\u0302(t)| < \u03b5.\nProof. For the first bound, it is easy to check that if\nM < log\n( 1\nsn\u0302+1\n) + log (\u03b5)\n2 logn\nthen n2Msn\u0302+1 < \u03b5 and the result follows from Proposition 3. For the second one, if\nM < log\n( 1\nsn\u0302+1\n) + log(\u03b5)\nlog(4Sn2) \u2212 1\nthen (4Sn 2)M+1\u22121\n(4Sn2)\u22121 sn\u0302+1 < \u03b5 and the result follows from Proposition 4."}], "references": [{"title": "Grammatical inference as a principal component analysis problem", "author": ["R. Bailly", "F. Denis", "L. Ralaivola"], "venue": "In Proceedings of ICML", "citeRegEx": "Bailly et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bailly et al\\.", "year": 2009}, {"title": "A spectral approach for probabilistic grammatical inference on trees", "author": ["R. Bailly", "A. Habrard", "F. Denis"], "venue": "In Proceedings of ALT", "citeRegEx": "Bailly et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bailly et al\\.", "year": 2010}, {"title": "Spectral learning of weighted automata: A forward-backward perspective", "author": ["B. Balle", "X. Carreras", "F. Luque", "A. Quattoni"], "venue": "Machine Learning", "citeRegEx": "Balle et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2014}, {"title": "A canonical form for weighted automata and applications to approximate minimization", "author": ["B. Balle", "P. Panangaden", "D. Precup"], "venue": "In Proceedings of LICS", "citeRegEx": "Balle et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2015}, {"title": "Recognizable formal power series on trees", "author": ["J. Berstel", "C. Reutenauer"], "venue": "Theoretical Computer Science", "citeRegEx": "Berstel and Reutenauer,? \\Q1982\\E", "shortCiteRegEx": "Berstel and Reutenauer", "year": 1982}, {"title": "Closing the learning planning loop with predictive state representations", "author": ["B. Boots", "S. Siddiqi", "G. Gordon"], "venue": "International Journal of Robotics Research", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "The rank of a formal tree power series", "author": ["S. Bozapalidis", "O. Louscou-Bozapalidou"], "venue": "Theoretical Computer Science", "citeRegEx": "Bozapalidis and Louscou.Bozapalidou,? \\Q1983\\E", "shortCiteRegEx": "Bozapalidis and Louscou.Bozapalidou", "year": 1983}, {"title": "On tensors, sparsity, and nonnegative factorizations", "author": ["E.C. Chi", "T.G. Kolda"], "venue": "SIAM Journal on Matrix Analysis and Applications", "citeRegEx": "Chi and Kolda,? \\Q2012\\E", "shortCiteRegEx": "Chi and Kolda", "year": 2012}, {"title": "Tensor decomposition for fast parsing with latentvariable PCFGs", "author": ["S.B. Cohen", "M. Collins"], "venue": "In Proceedings of NIPS", "citeRegEx": "Cohen and Collins,? \\Q2012\\E", "shortCiteRegEx": "Cohen and Collins", "year": 2012}, {"title": "Approximate PCFG parsing using tensor decomposition", "author": ["S.B. Cohen", "G. Satta", "M. Collins"], "venue": "In Proceedings of NAACL", "citeRegEx": "Cohen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2013}, {"title": "Experiments with spectral learning of latent-variable PCFGs", "author": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"], "venue": "In Proceedings of NAACL", "citeRegEx": "Cohen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2013}, {"title": "Spectral learning of latent-variable PCFGs: Algorithms and sample complexity", "author": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Cohen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2014}, {"title": "A course in functional analysis", "author": ["J.B. Conway"], "venue": null, "citeRegEx": "Conway,? \\Q1990\\E", "shortCiteRegEx": "Conway", "year": 1990}, {"title": "Inversion error, condition number, and approximate inverses of uncertain matrices. Linear algebra and its applications", "author": ["L. El Ghaoui"], "venue": null, "citeRegEx": "Ghaoui,? \\Q2002\\E", "shortCiteRegEx": "Ghaoui", "year": 2002}, {"title": "Parsing algorithms and metrics", "author": ["J. Goodman"], "venue": "In Proceedings of ACL", "citeRegEx": "Goodman,? \\Q1996\\E", "shortCiteRegEx": "Goodman", "year": 1996}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "Journal of Computer and System Sciences", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Minimisation of Multiplicity Tree Automata", "author": ["S. Kiefer", "I. Marusic", "J. Worrell"], "venue": null, "citeRegEx": "Kiefer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiefer et al\\.", "year": 2015}, {"title": "Low-rank spectral learning with weighted loss functions", "author": ["A. Kulesza", "N. Jiang", "S. Singh"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Kulesza et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2015}, {"title": "Low-Rank Spectral Learning", "author": ["A. Kulesza", "N.R. Rao", "S. Singh"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Kulesza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2014}, {"title": "Numerical analysis: a second course. Siam", "author": ["J.M. Ortega"], "venue": null, "citeRegEx": "Ortega,? \\Q1990\\E", "shortCiteRegEx": "Ortega", "year": 1990}, {"title": "An annotation scheme for free word order languages", "author": ["W. Skut", "B. Krenn", "T. Brants", "H. Uszkoreit"], "venue": "In Conference on Applied Natural Language Processing", "citeRegEx": "Skut et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Skut et al\\.", "year": 1997}, {"title": "T \u2016F \u2016v\u2016 and \u2016T (I,v, I)\u2016F \u2264 \u2016T \u2016F \u2016v\u2016. Let \u03b4 = \u2016E\u2212 \u00ca\u2016 and let \u03c3 be the smallest nonzero eigenvalue of the matrix I\u2212 E. It follows from (El Ghaoui, 2002, Equation (7.2)) that if \u03b4 < \u03c3 then \u2016(I\u2212 E)\u22121 \u2212 (I\u2212 \u00ca)\u22121\u2016 \u2264 \u03b4/(\u03c3(\u03c3 \u2212 \u03b4))", "author": ["v)\u2016F"], "venue": "Since \u03b4 = O(\u03b5)", "citeRegEx": "I and \u2264,? \\Q2002\\E", "shortCiteRegEx": "I and \u2264", "year": 2002}], "referenceMentions": [{"referenceID": 15, "context": "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.", "startOffset": 128, "endOffset": 207}, {"referenceID": 0, "context": "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.", "startOffset": 128, "endOffset": 207}, {"referenceID": 5, "context": "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.", "startOffset": 128, "endOffset": 207}, {"referenceID": 2, "context": "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.", "startOffset": 128, "endOffset": 207}, {"referenceID": 1, "context": ", 2014) and trees (Bailly et al., 2010; Cohen et al., 2014), and subsequent investigations into low-rank spectral learning for predictive state representations (Kulesza et al.", "startOffset": 18, "endOffset": 59}, {"referenceID": 11, "context": ", 2014) and trees (Bailly et al., 2010; Cohen et al., 2014), and subsequent investigations into low-rank spectral learning for predictive state representations (Kulesza et al.", "startOffset": 18, "endOffset": 59}, {"referenceID": 3, "context": ", 2014, 2015) and approximate minimization of weighted automata (Balle et al., 2015).", "startOffset": 64, "endOffset": 84}, {"referenceID": 8, "context": "The idea of speeding up parsing with (L)PCFG by approximating the original model with a smaller one was recently studied in (Cohen and Collins, 2012; Cohen et al., 2013a), where a tensor decomposition technique was used in order to obtain the minimized model.", "startOffset": 124, "endOffset": 170}, {"referenceID": 8, "context": "It was observed in (Cohen and Collins, 2012; Cohen et al., 2013a) that a side-effect of reducing the size of a grammar learned from data was a slight improvement in parsing performance.", "startOffset": 19, "endOffset": 65}, {"referenceID": 8, "context": "It is important to remark that in contrast with the tensor decompositions in (Cohen and Collins, 2012; Cohen et al., 2013a) which are susceptible to local optima problems, our approach resembles a power-method approach to SVD, which yields efficient globally convergent algorithms.", "startOffset": 77, "endOffset": 123}, {"referenceID": 1, "context": "While WTA are usually defined over arbitrary ranked trees, only considering binary trees does not lead to any loss of generality since WTA on ranked trees are equivalent to WTA on binary trees (see (Bailly et al., 2010) for references).", "startOffset": 198, "endOffset": 219}, {"referenceID": 6, "context": "Theorem 1 ((Bozapalidis and Louscou-Bozapalidou, 1983)).", "startOffset": 11, "endOffset": 54}, {"referenceID": 3, "context": "In the case of weighted automata on strings, (Balle et al., 2015) recently showed a polynomial time algorithm for computing the Gram matrices of a string Hankel matrix by solving a system of linear equations.", "startOffset": 45, "endOffset": 65}, {"referenceID": 16, "context": "Thus, we shall resort to a different 1If the WTA given to the algorithm is not minimal, a pre-processing step can be used to minimize the input using the algorithm from (Kiefer et al., 2015).", "startOffset": 169, "endOffset": 190}, {"referenceID": 4, "context": "It is shown in (Berstel and Reutenauer, 1982) that A\u2297 computes the function fA\u2297(t) = f(t)2.", "startOffset": 15, "endOffset": 45}, {"referenceID": 20, "context": "2 Experimental Setup and Results In our experiments, we used the annotated corpus of german newspaper texts NEGRA (Skut et al., 1997).", "startOffset": 114, "endOffset": 133}, {"referenceID": 7, "context": "2013a), who use tensor decomposition algorithms (Chi and Kolda, 2012) to decompose the tensors of an underlying PCFG.", "startOffset": 48, "endOffset": 69}, {"referenceID": 14, "context": "For parsing we use minimum Bayes risk decoding, maximizing the sum of the marginals for the nonterminals in the grammar, essentially choosing the best tree topology given a string (Goodman, 1996).", "startOffset": 180, "endOffset": 195}], "year": 2017, "abstractText": "We describe a technique to minimize weighted tree automata (WTA), a powerful formalisms that subsumes probabilistic context-free grammars (PCFGs) and latent-variable PCFGs. Our method relies on a singular value decomposition of the underlying Hankel matrix defined by the WTA. Our main theoretical result is an efficient algorithm for computing the SVD of an infinite Hankel matrix implicitly represented as a WTA. We provide an analysis of the approximation error induced by the minimization, and we evaluate our method on real-world data originating in newswire treebank. We show that the model achieves lower perplexity than previous methods for PCFG minimization, and also is much more stable due to the absence of local optima.", "creator": "TeX"}}}