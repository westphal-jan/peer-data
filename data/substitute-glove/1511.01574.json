{"id": "1511.01574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2015", "title": "Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model", "abstract": "We follow Sparse Non - factors Matrix (SNM) hebrew styling estimation addition preorder surprising that met - it comparison.", "histories": [["v1", "Thu, 5 Nov 2015 01:45:29 GMT  (13kb)", "https://arxiv.org/abs/1511.01574v1", null], ["v2", "Mon, 22 Feb 2016 19:15:19 GMT  (14kb)", "http://arxiv.org/abs/1511.01574v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ciprian chelba", "fernando pereira"], "accepted": false, "id": "1511.01574"}, "pdf": {"name": "1511.01574.pdf", "metadata": {"source": "CRF", "title": "Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model", "authors": ["Ciprian Chelba"], "emails": ["ciprianchelba@google.com", "pereira@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n01 57\n4v 2\n[ cs\n.C L\n] 2\n2 Fe\nWe describe Sparse Non-negative Matrix (SNM) language model estimation using multinomial loss on held-out data. Being able to train on held-out data is important in practical situations where the training data is usually mismatched from the held-out/test data. It is also less constrained than the previous training algorithm using leave-one-out on training data: it allows the use of richer meta-features in the adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing which would be difficult to deal with correctly in leave-one-out training.\nIn experiments on the one billion words language modeling benchmark [3], we are able to slightly improve on previous results reported in [11]-[11a] which uses a different loss function, and employs leave-one-out training on a subset of the main training set. Surprisingly, an adjustment model with meta-features that discard all lexical information can perform as well as lexicalized meta-features. We find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model.\nIn a real-life scenario where the training data is a mix of data sources that are imbalanced in size, and of different degrees of relevance to the held-out and test data, taking into account the data source for a given skip-/n-gram feature and combining them for best performance on held-out/test data improves over skip-/n-gram SNM models trained on pooled data by about 8% in the SMT setup, or as much as 15% in the ASR/IME setup.\nThe ability to mix various data sources based on how relevant they are to a mismatched held-out set is probably the most attractive feature of the new estimation method for SNM LM."}, {"heading": "1 Introduction", "text": "A statistical language model estimates probability values P (W ) for strings of words W in a vocabulary V whose size is in the tens, hundreds of thousands and sometimes even millions. Typically the string W is broken into sentences, or other segments such as utterances in automatic speech recognition, which are often assumed to be conditionally independent; we will assume that W is such a segment, or sentence.\nSince the parameter space of P (wk|w1, w2, . . . , wk\u22121) is too large, the language model is forced to put the context Wk\u22121 = w1, w2, . . . , wk\u22121 into an equivalence class determined by a function \u03a6(Wk\u22121). As a result,\nP (W ) \u223c=\nn\u220f\nk=1\nP (wk|\u03a6(Wk\u22121)) (1)\nResearch in language modeling consists of finding appropriate equivalence classifiers \u03a6 and methods to estimate P (wk|\u03a6(Wk\u22121)). Once the form \u03a6(Wk\u22121) is specified, only the problem of estimating P (wk|\u03a6(Wk\u22121)) from training data remains.\nPerplexity as a Measure of Language Model Quality\nA statistical language model can be evaluated by how well it predicts a string of symbols Wt\u2014commonly referred to as test data\u2014generated by the source to be modeled.\nA commonly used quality measure for a given model M is related to the entropy of the underlying source and was introduced under the name of perplexity (PPL):\nPPL(M) = exp\n(\n\u2212 1\nN\nN\u2211\nk=1\nln [PM (wk|Wk\u22121)]\n)\n(2)\nFor an excellent discussion on the use of perplexity in statistical language modeling, as well as various estimates for the entropy of English the reader is referred to [8], Section 8.4, pages 141-142 and the additional reading suggested in Section 8.5 of the same book."}, {"heading": "2 Notation and Modeling Assumptions", "text": "We denote with e an event in the training/development/test data corresponding to each prediction (wk|\u03a6(Wk\u22121)) in Eq. (1); each event consists of:\n\u2022 a set of features F(e) = {f1, . . . , fk, . . . , fF (e)} \u2282 F , where F denotes the set of features in the model, collected on the training data: F = \u222ae\u2208T F(e); \u2022 a predicted (target) word w = t(e) from the LM vocabulary V; we denote with V = |V| the size of the vocabulary.\nThe set of features F(e) is obtained by applying the equivalence classification function \u03a6(Wk\u22121) to the context of the prediction. The most successful model so far has been the n-gram model, extracting all n-gram features of length 0, . . . , n \u2212 1 from the Wk\u22121 context1, respectively.\n2.1 Skip-n-gram Language Modeling\nA simple variant on the n-gram model is the skip-n-gram model; a skip-n-gram feature extracted from the context Wk\u22121 is characterized by the tuple (r, s, a) where:\n\u2022 r denotes number of remote context words\n\u2022 s denotes the number of skipped words\n\u2022 a denotes the number of adjacent context words\nrelative to the target word wk being predicted. For example, in the sentence, <S> The quick brown fox jumps over the lazy dog </S> a (1, 2, 3) skip-gram feature for the target word dog is: [brown skip-2 over the lazy]\nTo control the size of F(e) it is recommended to limit the skip length s and also either (r + a) or both r and s; not setting any such upper bounds will result in events containing a set of skip-gram features whose total representation size is quintic in the length of the sentence.\nWe configure the skip-n-gram feature extractor to produce all features f , defined by the equivalence class \u03a6(Wk\u22121), that meet constraints on the minimum and maximum values for:\n\u2022 the number of context words used r + a;\n\u2022 the number of remote words r;\n1The empty feature is considered to have length 0, it is present in every event e, and it produces the unigram distribution on the language model vocabulary.\n\u2022 the number of adjacent words a;\n\u2022 the skip length s.\nWe also allow the option of not including the exact value of s in the feature representation; this may help with smoothing by sharing counts for various skip features. Tied skip-n-gram features will look like: [curiosity skip-* the cat]\nSample feature extraction configuration files for a 5-gram and a skip-10-gram SNM LM are presented in Appendix A and B, respectively. A simple extension that leverages context beyond the current sentence, as well as other categorical features such as geo-location is presented and evaluated in [4].\nIn order to build a good probability estimate for the target word wk in a context Wk\u22121, or an event e in our notation, we need a way of combining an arbitrary number of features which do not fall into a simple hierarchy like regular n-gram features. The following section describes a simple, yet novel approach for combining such predictors in a way that is computationally easy, scales up gracefully to large amounts of data and as it turns out is also very effective from a modeling point of view."}, {"heading": "3 Multinomial Loss for the Sparse Non-negative Matrix Language Model", "text": "The sparse non-negative matrix (SNM) language model (LM) [11]-[11a] assigns probability to a word by applying the equivalence classification function \u03a6(W ) to the context of the prediction, as explained in the previous section, and then using a matrix M, where Mfw is indexed by feature f \u2208 F and word w \u2208 V . We further assume that the model is parameterized as a slight variation on conditional relative frequencies for words w given features f , denoted as c(w|f):\nP (w|\u03a6(W )) \u221d \u2211\nf\u2208\u03a6(W )\nc(w|f) \u00b7 exp(A(f,w;\u03b8)) \ufe38 \ufe37\ufe37 \ufe38\nMfw\n(3)\nThe adjustment function A(f,w;\u03b8) is a real-valued function whose task is to estimate the relative importance of each input feature f for the prediction of the given target word w. It is computed by a linear model on meta-features h extracted from each link (f,w) and associated feature f :\nA(f,w;\u03b8) = \u2211\nk\n\u03b8khk(f,w) (4)\nThe meta-features are either strings identifying the feature type, feature, link etc., or bucketed feature and link counts. We also allow all possible conjunctions of elementary meta-features, and estimate a weight \u03b8k for each (elementary or conjoined) metafeature hk. In order to control the model size we use the hashing technique in [6],[13]. The meta-feature extraction is explained in more detail in Section 3.2, and associated Appendix C.\nAssuming we have a sparse matrix M of adjusted relative frequencies, the probability of an event e = (w|\u03a6(Wk\u22121)) predicting word w in context \u03a6(Wk\u22121) is computed as follows:\nP (e) = yt(e)/y(e) yt(e) = \u2211\nf\u2208F\n\u2211\nw\u2208V\n1f (e) \u00b7 1w(e)Mfw\ny(e) = \u2211\nf\u2208F\n1f (e)Mf\u2217\nwhere Mf\u2217 ensures that the model is properly normalized over the LM vocabulary:\nMf\u2217 = \u2211\nw\u2208V\nMfw\nand the indicator functions 1f (e) and 1w(e) select a given feature, and target word in the event e, respectively:\n1f (e) =\n{\n1, f \u2208 F(e)\n0, o/w\n1w(e) =\n{\n1, w = t(e)\n0, o/w\nWith this notation and using the shorthand Afw = A(f,w;\u03b8), the derivative of the log-probability for event e with respect to the adjustment function Afw for a given link (f,w) is:\n\u2202 log P (e)\n\u2202Afw =\n\u2202 log yt(e)\n\u2202Afw \u2212\n\u2202 log y(e)\n\u2202Afw\n= 1\nyt(e)\n\u2202yt(e) \u2202Afw \u2212 1 y(e) \u2202y(e) \u2202Afw\n= 1f (e)Mfw\n[ 1w(e)\nyt(e) \u2212\n1\ny(e)\n]\n(5)\nmaking use of the fact that \u2202Mfw \u2202Afw = \u2202c(w|f)exp(Afw) \u2202Afw = c(w|f)exp(Afw) = Mfw\nPropagating the gradient \u2202 logP (e) \u2202Afw\nto the \u03b8 parameters of the adjustment function A(f,w;\u03b8) is done using mini-batch estimation for the reasons detailed in Section 3.1:\n\u2202 log P (e)\n\u2202\u03b8k =\n\u2211\n(f,w):hk\u2208meta\u2212features(f,w)\n\u2202 log P (e)\n\u2202Afw\n\u03b8k,B+1 \u2190 \u03b8k,B \u2212 \u03b7 \u2211\ne\u2208B\n\u2202 logP (e)\n\u2202\u03b8k (6)\nRather than using a single fixed learning rate \u03b7, we use AdaGrad [5] which uses a separate adaptive learning rate \u03b7k,B for each weight \u03b8k,B:\n\u03b7k,B = \u03b3 \u221a\n\u22060 + \u2211B\nb=1\n[ \u2211\ne\u2208b \u2202 logP (e)\n\u2202\u03b8k\n]2 (7)\nwhere B is the current batch index, \u03b3 is a constant scaling factor for all learning rates and \u22060 is an initial accumulator constant. Basing the learning rate on historical information tempers the effect of frequently occurring features which keeps the weights small and as such acts as a form of regularization."}, {"heading": "3.1 Implementation Notes", "text": "From a computational point of view, the two main issues with a straightforward gradient descent parameter update (either on-line or batch) are:\n1. the second term on the right-hand-side (RHS) of Eq. (5) is an update that needs to be propagated to all words in the vocabulary, irrespective of whether they occur on a given training event or not;\n2. keeping the model normalized after a Mfw parameter update means recomputing all normalization coefficients Mf\u2217,\u2200f \u2208 F .\nFor mini-/batch updates, the model renormalization is done at the end of each training epoch/iteration, and it is no longer a problem. To side-step the first issue, we notice that mini-/batch updates would allow us to accumulate the \u03b1f (B) = \u2211 e\u2208B 1f (e) 1 y(e)\nacross the entire mini-/batch B, and adjust the cumulative gradient at the end of the mini-/batch, in effect computing:\n\u2211\ne\u2208B\n\u2202 logP (e)\n\u2202Afw =\n\u2211\ne\u2208B\nMfw 1\nyt(e) 1f (e) \u00b7 1w(e)\u2212Mfw \u00b7 \u03b1f (B) (8)\n\u03b1f (B) = \u2211\ne\u2208B\n1f (e) 1\ny(e)\nIn summary, we use two maps to compute the gradient updates over a mini-/batch: one keyed by (f,w) pairs, and one keyed by f . The first map accumulates the first term on the RHS of Eq. (8), and is updated once for each link (f,w) occurring in a training event e. The second map accumulates the \u03b1f (B) values, and is again updated only for each of the features f encountered on a given event in the mini-/batch. At the end of the mini-/batch we update the entries in the first map acc. to Eq. (8) such that they store the cumulative gradient; these are then used to update the \u03b8 parameters of the adjustment function according to Eq. (6).\nThe model Mfw and the normalization coefficients Mf\u2217 are stored in maps keyed by (f,w), and f , respectively. The [(f,w)\u21d2Mfw] map is initialized with relative frequencies c(w|f) computed from the training data; on disk they are stored in an SSTable [2] keyed by (f,w), with f and w represented as plain strings. For training the adjustment model we only need the rows of the M matrix that are encountered on development data (i.e., the training data for the adjustment model). A MapReduce [7] with two inputs extracts and intersects the features encountered on development data with the features collected on the main training data\u2014where the relative frequencies c(w|f) were also computed. The output is a significantly smaller matrix M that is loaded in RAM and used to train the adjustment model."}, {"heading": "3.2 Meta-features extraction", "text": "The process of breaking down the original features into meta-features and recombining them, allows similar features, i.e. features that are different only in some of their base components, to share weights, thus improving generalization.\nGiven an event the quick brown fox, the 4-gram feature for the prediction of the target fox would be broken down into the following elementary meta-features:\n\u2022 feature identity, e.g. [the quick brown]\n\u2022 feature type, e.g. 3-gram\n\u2022 feature count Cf\u2217 \u2022 target identity, e.g. fox\n\u2022 feature-target count Cfw\nElementary meta-features of different types are then joined with others to form more complex meta-features, as described best by the pseudo-code in Appendix C; note that the seemingly absent feature-target identity is represented by the conjunction of the feature identity and the target identity.\nAs count meta-features of the same order of magnitude carry similar information, we group them so they can share weights. We do this by bucketing the count meta-features according to their (floored) log2 value. Since this effectively puts the lowest count values, of which there are many, into a different bucket, we optionally introduce a second (ceiled) bucket to assure smoother transitions. Both buckets are then weighted according to the log2 fraction lost by the corresponding rounding operation.\nTo control memory usage, we employ a feature hashing technique [6],[13] where we store the meta-feature weights in a flat hash table of predefined size; strings are fingerprinted, counts are hashed and the resulting integer mapped to an index k in \u03b8 by taking its value modulo the pre-defined size(\u03b8). We do not prevent collisions, which has the potentially undesirable effect of tying together the weights of different meta-features. However, when this happens the most frequent meta-feature will dominate the final value after training, which essentially boils down to a form of pruning. Because of this the model performance does not strongly depend on the size of the hash table."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Experiments on the One Billion Words Language Modeling Benchmark", "text": "Our first experimental setup used the One Billion Word Benchmark corpus2 made available by [3]. For completeness, here is a short description of the corpus, containing only monolingual English data:\n\u2022 Total number of training tokens is about 0.8 billion\n\u2022 The vocabulary provided consists of 793471 words including sentence boundary markers <S>, </S>, and was constructed by discarding all words with count below 3\n\u2022 Words outside of the vocabulary were mapped to an <UNK> token, also part of the vocabulary\n\u2022 Sentence order was randomized\n\u2022 The test data consisted of 159658 words (without counting the sentence beginning marker <S> which is never predicted by the language model)\n\u2022 The out-of-vocabulary (OOV) rate on the test set was 0.28%.\nThe foremost concern when using held-out data for estimating the adjustment model is the limited amount of data available in a practical setup, so we used a small development set consisting of 33 thousand words.\nWe conducted experiments using two feature extraction configurations identical to those used in [11]: 5-gram and skip-10-gram, see Appendix A and B. The AdaGrad parameters in Eq. (7) are set to: \u03b3 = 0.1, \u22060 = 1.0, and the mini-batch size is 2048 samples. We also experimented with various adjustment model sizes (200M, 20M, and 200k hashed parameters), non-lexicalized meta-features, and feature-only meta-features, see Appendix C. The results are presented in Tables 1-2.\nA first conclusion is that we can indeed get away with very small amounts of development data. This is excellent news, because usually people do not have lots of development data to tune parameters on, see SMT experiments presented in the next section. Using meta-features computed only from the feature component of a link does lead to a fairly significant increase in PPL: 5% rel for the 5-gram config, and 10% rel for the skip-10-gram config.\nSurprisingly, when using the 5-gram config, discarding the lexicalized meta-features consistently does a tiny bit better than the lexicalized model; for the skip-10-gram config the un-lexicalized model performs essentially as well as the lexicalized model. The number of parameters in the model is very small in this case (on the order of a thousand) so the model no longer overtrains after the first iteration as was the case when using link lexicalized meta-features; meta-feature hashing is not necessary either.\nIn summary, training and evaluating in exactly the same training/test setup as the one in [11] we find that:\n1. 5-gram config: using multinomial loss training on 33 thousand words of development data, 200K or larger adjustment model, and un-lexicalized meta-features trained over 5 epochs produces 5-gram SNM PPL of 69.6, which is just a bit better than the 5-gram SNM PPL of 70.8 reported in [11], Table 1, and very close to the Kneser-Ney PPL of 67.6.\n2. skip-10-gram config: using multinomial loss training on 33 thousand words of development data, 20M or larger adjustment model, and un-lexicalized meta-features trained over 5 epochs produced skip-10-gram SNM PPL of 50.9, again just a bit better than both the skip-10-gram SNM PPL of 52.9 and the RNN-LM PPL of 51.3 reported in [11], Table 3, respectively."}, {"heading": "4.2 Experiments on 10B Words of Burmese Data in Statistical Machine Translation Language Modeling Setup", "text": "In a separate set of experiments on Burmese data provided by the statistical machine translation (SMT) team, the held-out data (66 thousand words) and the test data (22 thousand words) is mismatched to the training data consisting of 11 billion words mostly crawled from the web (and labelled as \u201cweb\u201d) along with 176 million words (labelled as \u201ctarget\u201d) originating from\n2http://www.statmt.org/lm-benchmark\nparallel data used for training the channel model. The vocabulary size is 785261 words including sentence boundary markers; the out-of-vocabulary rate on both held-out and test set is 0.6%.\nTo quantify statistically the mismatch between training and held-out/test data, we trained both Katz and interpolated KneserNey 5-gram models on the pooled training data; the Kneser-Ney LM has PPL of 611 and 615 on the held-out and test data, respectively; the Katz LM is severely more mismatched, with PPL of 4153 and 4132, respectively3 . Because of the mismatch between the training and the held-out/test data, the PPL of the un-adjusted SNM 5-gram LM is significantly lower than that of the SNM adjusted using leave-one-out [11] on a subset of the shuffled training set: 710 versus 1285.\nThe full set of results in this experimental setup are presented in Tables 4-5. When using the multinomial adjustment model training on held-out data things fall in place, and the adjusted SNM 5-gram has lower PPL than the unadjusted one: 347 vs 710; the former is significantly lower than the 1285 value produced by leave-oneout training; the skip-5-gram SNM model (a trimmed down version of the skip-10-gram in Appendix B) has PPL of 328, improving only modestly over the 5-gram SNM result\u2014perhaps due to the mismatch between training and development/test data.\nWe also note that the lexicalized adjustment model works significantly better then either the feature-only or the un-lexicalized one, in contrast to the behavior on the one billion words benchmark.\nAs an extension we experimented with SNM training that takes into account the data source for a given skip-/n-gram feature, and combines them best on held-out/test data by taking into account the identity of the data source as well. This is the reality of most practical scenarios for training language models. We refer to such features as corpus-tagged features: in training we augment each feature with a tag describing the training corpus it originates from, in this case web and target, respectively; on held-out and test data the event extractor augments each feature with each of the corpus tags in training. The adjustment function is then trained to assign a weight for each such corpus-tagged feature. Corpus tagging the features and letting the adjustment model do the combination reduced PPL by about 8% relative over the model trained on pooled data in both 5-gram and skip-5-gram configurations."}, {"heading": "4.3 Experiments on 35B Words of Italian Data in Language Modeling Setup for Automatic Speech Recognition", "text": "We have experimented with SNM LMs in the LM training setup for Italian as used on the automatic speech recognition (ASR) project. The same LM is used for two distinct types of ASR requests: voice-search queries (VS) and Android Input Method (IME, speech input on the soft keyboard). As a result we use two separate test sets to evaluate the LM performance, one for each VS and IME, respectively.\nThe held-out data used for training the adjustment model is a mix of VS and IME transcribed utterances, consisting of 36 thousand words split 30/70% between VS/IME, respectively. The adjustment model used 20 million parameters trained using mini-batch AdaGrad (2048 samples batch size) in one epoch.\nThe training data consists of a total of 35 billion words from various sources, of varying size and degree of relevance to either of the test sets:\n\u2022 google.com (111 Gbytes) and maps.google.com (48 Gbytes) query stream\n\u2022 high quality web crawl (5 Gbytes)\n\u2022 automatically transcribed utterances filtered by ASR confidence for both VS and IME (4.1 and 0.5 Gbytes, respectively)\n\u2022 manually transcribed utterances for both VS and IME (0.3 and 0.5 Gbytes, repectively)\n\u2022 voice actions training data (0.1 Gbytes)\n3The cummulative hit-ratios on test data at orders 5 through 1 were 0.2/0.3/0.6/0.9/1.0 for the KN model, and 0.1/0.3/0.6/0.9/1.0 for the Katz model, which may explain the large gap in performance between KN and Katz: the diversity counts used by KN 80% of the time are more robust to mismatched training/test conditions than the relative frequencies used by Katz.\nAs a baseline for the SNM we built Katz and interpolated Kneser-Ney 5-gram models by pooling all the training data. We then built a 5-gram SNM LM, as well as corpus-tagged SNM 5-gram where each n-gram is tagged with the identity of the corpus it occurs in (one of seven tags). Skip-grams were added to either of the SNM models. The results are presented in Table 3; the vocabulary used to train all language models being compared consisted of 4 million words.\nA first observation is that the SNM 5-gram LM outperforms both Katz and Kneser-Ney LMs significantly on both test sets. We attribute this to the ability of the adjustment model to optimize the combination of various n-gram contexts such that they maximize the likelihood of the held-out data; no such information is available to either of the Katz/Kneser-Ney models.\nAugmenting the SNM 5-gram with corpus-tags benefits mostly the IME performance; we attribute this to the fact that the vast majority of the training data is closer to the VS test set, and clearly separating the training sources (in particular the ones meant for the IME component of the LM such as web crawl and IME transcriptions) allows the adjustment model to optimize better for that subset of the held-out data. Skip-grams offer relatively modest improvements over either SNM 5-gram models."}, {"heading": "5 Conclusions and Future Work", "text": "The main conclusion is that training the adjustment model on held-out data using multinomial loss introduces many advantages while matching the previous results reported in [11]: as observed in [12], Section 2, using a binary probability model is expected to yield the same model as a multinomial probability model. Correcting the deficiency in [11] induced by using a Poisson model for each binary random variable does not seem to make a difference in the quality of the estimated model.\nBeing able to train on held-out data is very important in practical situations where the training data is usually mismatched from the held-out/test data. It is also less constrained than the previous training algorithm using leave-one-out on training data: it allows the use of richer meta-features in the adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing which would be difficult to deal with correctly in leave-one-out training. Taking into account the data source for a given skip-/n-gram feature, and combining them for best performance on held-out/test data improves over SNM models trained on pooled data by about 8% in the SMT setup, or as much as 15% in the ASR/IME setup.\nWe find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model. Surprisingly, using meta-features that discard all lexical information can sometimes perform as well as lexicalized meta-features, as demonstrated by the results on the One Billion Words Benchmark corpus.\nGiven the properties of the SNM n-gram LM explored so far:\n\u2022 ability to mix various data sources based on how relevant they are to a given held-out set, thus providing an alternative to Bayesian mixing algorithms such as [1],\n\u2022 excellent pruning properties relative to entropy pruning of Katz and Kneser-Ney models [10],\n\u2022 conversion to standard ARPA back-off format [10],\n\u2022 effortless incorporation of richer features such as skip-n-grams and geo-tags [4],\nwe believe SNM could provide the estimation back-bone for a fully fledged LM training pipeline used in a real-life setup. A comparison of SNM against maximum entropy modeling at feature extraction parity is also long due."}, {"heading": "6 Acknowledgments", "text": "Thanks go to Yoram Singer for clarifying the correct mini-batch variant of AdaGrad, Noam Shazeer for assistance on understanding his implementation of the adjustment function estimation, Diamantino Caseiro for code reviews, Kunal Talwar, Amir Globerson and Diamantino Caseiro for useful discussions, and Anton Andryeyev for providing the SMT training/held-out/test data sets. Last, but not least, we are thankful to our former summer intern Joris Pelemans for suggestions while preparing the final version of the paper.\nA Appendix: 5-gram Feature Extraction Configuration\n/ / Sample c o n f i g g e n e r a t i n g a s t r a i g h t 5\u2212gram language model . n g r a m e x t r a c t o r {\nmin n : 0 max n : 4\n}\nB Appendix: skip-10-gram Feature Extraction Configuration\n/ / Sample c o n f i g g e n e r a t i n g a s t r a i g h t s k i p \u221210\u2212gram language model . n g r a m e x t r a c t o r {\nmin n : 0 max n : 9\n} s k i p n g r a m e x t r a c t o r {\nm a x c o n t e x t w o r d s : 4 min remote words : 1 max remote words : 1 m i n s k i p l e n g t h : 1 m a x s k i p l e n g t h : 10 t i e s k i p l e n g t h : t ru e\n} s k i p n g r a m e x t r a c t o r {\nm a x c o n t e x t w o r d s : 5 m i n s k i p l e n g t h : 1 m a x s k i p l e n g t h : 1 t i e s k i p l e n g t h : f a l s e\n}"}, {"heading": "C Appendix: Meta-features Extraction Pseudo-code", "text": "// Metafeatures are represented as tuples (hash value, weight). // Concat(metafeatures, end pos, mf new) concatenates mf new // with all the existing metafeatures up to end pos. function COMPUTEMETAFEATURES(FeatureTargetPair pair)\n// feature-related metafeatures metafeatures\u2190 (Fingerprint(pair.feature.id()), 1.0) metafeatures\u2190 (Fingerprint(pair.feature.type()), 1.0) ln count = log(pair.feature.count()) / log(2) bucket1 = floor(ln count) bucket2 = ceil(ln count) weight1 = bucket2 - ln count weight2 = ln count - bucket1 metafeatures\u2190 (Hash(bucket1), weight1) metafeatures\u2190 (Hash(bucket2), weight2)\n// target-related metafeatures Concat(metafeatures, metafeatures.size(), (Fingerprint(pair.target.id()), 1.0))\n// feature-target-related metafeatures ln count = log(pair.count()) / log(2) bucket1 = floor(ln count) bucket2 = ceil(ln count) weight1 = bucket2 - ln count weight2 = ln count - bucket1 Concat(metafeatures, metafeatures.size(), (Hash(bucket1), weight1)) Concat(metafeatures, metafeatures.size(), (Hash(bucket2), weight2))\nreturn metafeatures"}], "references": [{"title": "Bayesian Language Model Interpolation for Mobile Speech Input", "author": ["Cyril Allauzen", "Michael Riley"], "venue": "Proceedings of Interspeech, 1429\u20131432, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Bigtable: A distributed storage system for structured data", "author": ["Chang"], "venue": "ACM Transactions on Computer Systems, vol. 26, pp. 1\u201326, num. 2, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Ciprian Chelba", "Tom\u00e1\u0161 Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "Proceedings of Interspeech, 2635\u20132639, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse Non-negative Matrix Language Modeling for Geo-annotated Query Session Data", "author": ["Ciprian Chelba", "Noam Shazeer"], "venue": "ASRU, to appear, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research, 12, 2121\u20132159, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Small statistical models by random feature mixing", "author": ["Kuzman Ganchev", "Mark Dredze"], "venue": "Proceedings of the ACL-2008 Workshop on Mobile Language Processing, Association for Computational Linguistics, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "MapReduce: Simplified data processing on large clusters", "author": ["Sanjay Ghemawat", "Jeff Dean"], "venue": "Proceedings of OSDI, 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Statistical Methods for Speech Recognition", "author": ["Frederick Jelinek"], "venue": "1997. MIT Press, Cambridge, MA, USA.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Strategies for training large scale neural network language models", "author": ["Tom\u00e1\u0161 Mikolov", "Anoop Deoras", "Daniel Povey", "Luk\u00e1s Burget", "Jan Cernock\u00fd"], "venue": "Proceedings of ASRU, 196\u2013201, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Pruning Sparse Non-negative Matrix N-gram Language Models", "author": ["Joris Pelemans", "Noam M. Shazeer", "Ciprian Chelba"], "venue": "Proceedings of Interspeech, 1433\u20131437, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse Non-negative Matrix Language Modeling For Skip-grams", "author": ["Noam Shazeer", "Joris Pelemans", "Ciprian Chelba. \u201cSkip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation", "\u201d CoRR", "abs/1412.1454", "2014. [Online]. Available: http://arxiv.org/abs/1412.1454. [11a] Noam Shazeer", "Joris Pelemans", "Ciprian Chelba"], "venue": "Proceedings of Interspeech, 1428\u20131432, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient Subsampling for Training Complex Language Models", "author": ["Puyang Xu", "A. Gunawardana", "S. Khudanpur"], "venue": "Proceedings of EMNLP, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature Hashing for Large Scale Multitask Learning", "author": ["Weinberger"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ACM, pp. 1113-1120, 2009. 14", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "In experiments on the one billion words language modeling benchmark [3], we are able to slightly improve on previous results reported in [11]-[11a] which uses a different loss function, and employs leave-one-out training on a subset of the main training set.", "startOffset": 68, "endOffset": 71}, {"referenceID": 10, "context": "In experiments on the one billion words language modeling benchmark [3], we are able to slightly improve on previous results reported in [11]-[11a] which uses a different loss function, and employs leave-one-out training on a subset of the main training set.", "startOffset": 137, "endOffset": 141}, {"referenceID": 7, "context": "For an excellent discussion on the use of perplexity in statistical language modeling, as well as various estimates for the entropy of English the reader is referred to [8], Section 8.", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "A simple extension that leverages context beyond the current sentence, as well as other categorical features such as geo-location is presented and evaluated in [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 10, "context": "The sparse non-negative matrix (SNM) language model (LM) [11]-[11a] assigns probability to a word by applying the equivalence classification function \u03a6(W ) to the context of the prediction, as explained in the previous section, and then using a matrix M, where Mfw is indexed by feature f \u2208 F and word w \u2208 V .", "startOffset": 57, "endOffset": 61}, {"referenceID": 5, "context": "In order to control the model size we use the hashing technique in [6],[13].", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "In order to control the model size we use the hashing technique in [6],[13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 4, "context": "Rather than using a single fixed learning rate \u03b7, we use AdaGrad [5] which uses a separate adaptive learning rate \u03b7k,B for each weight \u03b8k,B: \u03b7k,B = \u03b3 \u221a \u22060 + \u2211B b=1 [ \u2211 e\u2208b \u2202 logP (e) \u2202\u03b8k ]2 (7)", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "The [(f,w)\u21d2Mfw] map is initialized with relative frequencies c(w|f) computed from the training data; on disk they are stored in an SSTable [2] keyed by (f,w), with f and w represented as plain strings.", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "A MapReduce [7] with two inputs extracts and intersects the features encountered on development data with the features collected on the main training data\u2014where the relative frequencies c(w|f) were also computed.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "To control memory usage, we employ a feature hashing technique [6],[13] where we store the meta-feature weights in a flat hash table of predefined size; strings are fingerprinted, counts are hashed and the resulting integer mapped to an index k in \u03b8 by taking its value modulo the pre-defined size(\u03b8).", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "To control memory usage, we employ a feature hashing technique [6],[13] where we store the meta-feature weights in a flat hash table of predefined size; strings are fingerprinted, counts are hashed and the resulting integer mapped to an index k in \u03b8 by taking its value modulo the pre-defined size(\u03b8).", "startOffset": 67, "endOffset": 71}, {"referenceID": 2, "context": "1 Experiments on the One Billion Words Language Modeling Benchmark Our first experimental setup used the One Billion Word Benchmark corpus2 made available by [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "We conducted experiments using two feature extraction configurations identical to those used in [11]: 5-gram and skip-10-gram, see Appendix A and B.", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "In summary, training and evaluating in exactly the same training/test setup as the one in [11] we find that:", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "8 reported in [11], Table 1, and very close to the Kneser-Ney PPL of 67.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "3 reported in [11], Table 3, respectively.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "Because of the mismatch between the training and the held-out/test data, the PPL of the un-adjusted SNM 5-gram LM is significantly lower than that of the SNM adjusted using leave-one-out [11] on a subset of the shuffled training set: 710 versus 1285.", "startOffset": 187, "endOffset": 191}, {"referenceID": 10, "context": "The main conclusion is that training the adjustment model on held-out data using multinomial loss introduces many advantages while matching the previous results reported in [11]: as observed in [12], Section 2, using a binary probability model is expected to yield the same model as a multinomial probability model.", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "The main conclusion is that training the adjustment model on held-out data using multinomial loss introduces many advantages while matching the previous results reported in [11]: as observed in [12], Section 2, using a binary probability model is expected to yield the same model as a multinomial probability model.", "startOffset": 194, "endOffset": 198}, {"referenceID": 10, "context": "Correcting the deficiency in [11] induced by using a Poisson model for each binary random variable does not seem to make a difference in the quality of the estimated model.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "\u2022 ability to mix various data sources based on how relevant they are to a given held-out set, thus providing an alternative to Bayesian mixing algorithms such as [1],", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "\u2022 excellent pruning properties relative to entropy pruning of Katz and Kneser-Ney models [10],", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "\u2022 conversion to standard ARPA back-off format [10],", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "\u2022 effortless incorporation of richer features such as skip-n-grams and geo-tags [4],", "startOffset": 80, "endOffset": 83}], "year": 2016, "abstractText": "We describe Sparse Non-negative Matrix (SNM) language model estimation using multinomial loss on held-out data. Being able to train on held-out data is important in practical situations where the training data is usually mismatched from the held-out/test data. It is also less constrained than the previous training algorithm using leave-one-out on training data: it allows the use of richer meta-features in the adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing which would be difficult to deal with correctly in leave-one-out training. In experiments on the one billion words language modeling benchmark [3], we are able to slightly improve on previous results reported in [11]-[11a] which uses a different loss function, and employs leave-one-out training on a subset of the main training set. Surprisingly, an adjustment model with meta-features that discard all lexical information can perform as well as lexicalized meta-features. We find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model. In a real-life scenario where the training data is a mix of data sources that are imbalanced in size, and of different degrees of relevance to the held-out and test data, taking into account the data source for a given skip-/n-gram feature and combining them for best performance on held-out/test data improves over skip-/n-gram SNM models trained on pooled data by about 8% in the SMT setup, or as much as 15% in the ASR/IME setup. The ability to mix various data sources based on how relevant they are to a mismatched held-out set is probably the most attractive feature of the new estimation method for SNM LM.", "creator": "LaTeX with hyperref package"}}}