{"id": "1511.04707", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "Deep Linear Discriminant Analysis", "abstract": "We propose Deep Linear Discriminant Analysis (DeepLDA) instead grandmother formula_5 separable latent domains in for move - already - beyond boutique. Classic LDA topical signature entire needs serves blankness and is common for dimensionality tax for there classification problems. The new idea of this paper is come turn LDA taken straight similar 's apart cognition technology. This can be turn as a non - probability renewal result invitational LDA. Instead among optimal now likelihood particular an labels for are specimen, definitely seek direct objective object come pushes there network before raw feature lifecycle which: (a) have due variance except given same one way (x) addition distributions only follow math. Our element itself characteristics from the force LDA eigenvalue concerned already not direct to ambulance on stochastic fluid descent had time - vibrations. For placement go only doing approach on their particularly firmer metadata (MNIST, CIFAR - 72 three STL - 7 ). DeepLDA produces competing results place much for visualisation and stand a recently independent seen form art on STL - 10 with a combination complete skill of 81. 2006%.", "histories": [["v1", "Sun, 15 Nov 2015 14:33:26 GMT  (2261kb,D)", "http://arxiv.org/abs/1511.04707v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Tue, 17 Nov 2015 08:05:10 GMT  (2093kb,D)", "http://arxiv.org/abs/1511.04707v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Sat, 21 Nov 2015 17:59:18 GMT  (2084kb,D)", "http://arxiv.org/abs/1511.04707v3", "Under review as a conference paper at ICLR 2016"], ["v4", "Mon, 28 Dec 2015 09:52:47 GMT  (2107kb,D)", "http://arxiv.org/abs/1511.04707v4", "Under review as a conference paper at ICLR 2016"], ["v5", "Wed, 17 Feb 2016 08:32:47 GMT  (2114kb,D)", "http://arxiv.org/abs/1511.04707v5", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["matthias dorfer", "rainer kelz", "gerhard widmer"], "accepted": true, "id": "1511.04707"}, "pdf": {"name": "1511.04707.pdf", "metadata": {"source": "CRF", "title": "DEEP LINEAR DISCRIMINANT ANALYSIS", "authors": ["Matthias Dorfer", "Rainer Kelz", "Gerhard Widmer"], "emails": ["gerhard.widmer}@jku.at"], "sections": [{"heading": null, "text": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on all three datasets and sets a new state of the art on STL-10 with a test set accuracy of 81.4%."}, {"heading": "1 INTRODUCTION", "text": "Linear Discriminant Analysis (LDA) is a method from multivariate statistics which seeks to find a linear projection of high-dimensional observations into a lower-dimensional space (Fisher, 1936). When its preconditions are fulfilled, LDA allows to define optimal linear decision boundaries in the resulting latent space. The aim of this paper is to exploit the beneficial properties of classic LDA (low intra class variability, hight inter-class variability, optimal decision boundaries) by reformulating its objective to learn linearly separable representations based on a deep neural network (DNN).\nRecently, methods related to LDA achieved great success in combination with deep neural networks. Andrew et al. published a deep version of Canonical Correlation Analysis (DCCA) (Andrew et al., 2013). In their evaluations DCCA is used to produce correlated representation of multi-modal input data of simultaneously recorded acoustic and articulatory speech data. Clevert et al. propose Rectified Factor Networks (RFNs) which are a neural network interpretation of classic factor analysis (Clevert et al., 2015). RFNs are used for unsupervised pre-training and help to improve classification performance on four different benchmark datasets. Chan et al. (2015) propose PCANet as well as an LDA based variation. PCANet can be seen as a simple unsupervised convolutional deep learning approach. Their method proceeds with cascaded Principal Component Analysis (PCA), binary hashing and block histogram computation. However, one bottleneck of their approach is its limitation to very shallow architectures (two stages) (Chan et al., 2015).\nStuhlsatz et. al. already picked up the idea of combining LDA with a neural networks and proposed a generalized version of LDA (Stuhlsatz et al., 2012). Their approach starts with pre-training a stack of restricted Boltzmann machines. In a second step, the pre-trained model is fine-tuned with respect to a linear discriminant criterion. LDA has the disadvantage that it overemphasises large distances at the cost of confusing neighbouring classes. In (Stuhlsatz et al., 2012) this problem is tackled by a heuristic weighting scheme for computing the within-class scatter matrix required for LDA optimization.\n\u2217http://www.cp.jku.at/\nar X\niv :1\n51 1.\n04 70\n7v 1\n[ cs\n.L G\n] 1\n5 N\nov 2\n01 5"}, {"heading": "1.1 MAIN IDEA OF THIS PAPER", "text": "The approaches mentioned so far, all have in common that they are based on well established methods from multivariate statistics. Inspired by their work we propose an end-to-end deep DNN version of LDA - namely Deep Linear Discriminant Analysis (DeepLDA).\nDeep learning has become the state of the art in automatic feature learning and replaced existing approaches based on hand engineered feature in many fields such as object recognition (Krizhevsky et al., 2012). DeepLDA is motivated by the fact that when the preconditions of LDA are met, it is capable of finding linear combinations of the input features which allow for optimal linear decision boundaries. In general LDA takes features as input. The intuition of our method is to use LDA as an objective on top of a powerful feature learning algorithm. Instead of maximizing the likelihood of target labels for individual samples, we propose an LDA eigenvalue-based objective function that pushes the network to produce discriminative feature distributions. The parameters are optimized by back-propagating the error of an LDA-based objective through the entire network. We tackle the feature learning problem by focusing on directions in the latent space with smallest discriminative power. This replaces the weighting scheme of (Stuhlsatz et al., 2012) and allows to operate on the original formulation of LDA. We further expect that DeepLDA will produce linearly separable hidden representations with similar discriminative power in all directions of the latent space. Such representations should also be related with a high classification potential of the respective networks. The experimental classification results reported below will confirm that positive effect on classification accuracy, and two additional experiments (Section 5) will give us some first qualitative confirmation that the learned representations show the expected properties.\nThe reminder of the paper is structured as follows. In Section 2 we provide a general formulation of a DNN. Based on this formulation we introduce DeepLDA, a non-linear extension to classic LDA in Section 3. In Section 4 we experimentally evaluate our approach on three benchmark datasets and set a new state of the art on STL-10. Section 5 provides a deeper insight into the structure of DeepLDA\u2019s internal represenations. In Section 6 we conclude the paper."}, {"heading": "2 DEEP NEURAL NETWORKS", "text": "As the proposed model is built on top of a DNN we briefly describe the training paradigm of a network used for classification problems such as object recognition.\nA neural network with P hidden layers is represented as a non-linear function f(\u0398) with model parameters \u0398 = {\u03981, ...,\u0398P }. In the supervised setting we are additionally given a set of N train samples x1, ...xN along with corresponding classification targets t1, ...tN \u2208 {1, ..., C}. We further assume that the network output pi = (pi,1, ..., pi,C) = f(xi,\u0398) is normalized by the softmaxfunction to obtain class (pseudo-)probabilities. The network is then optimized using Stochastic Gradient Descent (SGD) with the goal of finding an optimal model parametrization \u0398 with respect to a certain loss function li(\u0398) = l(f(xi,\u0398), ti).\n\u0398 = arg min \u0398\n1\nN N\u2211 i=1 li(\u0398) (1)\nFor multi-class classification problems Categorical-Cross-Entropy (CCE) is a commonly used optimization target and formulated for observation xi as follows\nli(\u0398) = \u2212 C\u2211\nj=1\nyi,j log(pi,j) (2)\nwhere yi,j is 1 if observation xi belongs to class ti (j = ti) and 0 otherwise. In particular, the CCE tries to maximize the likelihood of the target class ti for each of the individual training examples xi under the model with parameters \u0398. Figure 1a shows a sketch of this general network architecture.\nWe would like to emphasize that objectives such as CCE do not impose any direct constraints \u2013 such as linear separability \u2013 on the latent space representation."}, {"heading": "3 DEEP LINEAR DISCRIMINANT ANALYSIS (DEEPLDA)", "text": "In this section we first provide a general introduction to LDA. Based on this introduction we propose DeepLDA, which optimizes an LDA-based optimization target in an end-to-end DNN fashion. Finally we describe how DeepLDA is used to predict class probabilities of unseen test samples."}, {"heading": "3.1 LINEAR DISCRIMINANT ANALYSIS", "text": "Let x1, ...,xN = X \u2208 RN\u00d7d denote a set of N samples belonging to C different classes c = 1, ..., C. The input representation X can either be hand engineered features, or hidden space representations H produced by a DNN (Andrew et al., 2013). LDA seeks to find a linear projection A \u2208 Rl\u00d7d into a lower l-dimensional subspace L where l = C \u2212 1. The resulting linear combinations of features xiAT are maximally separated in this space (Fisher, 1936). The LDA objective to find projection matrix A is formulated as:\narg max A |ASbAT | |ASwAT |\n(3)\nwhere Sb is the between scatter matrix and defined via the total scatter matrix St and within scatter matrix Sw as Sb = St \u2212 Sw. Sw is defined as the mean of the C individual class covariance matrices Sc (Equation (4) and (5)). X\u0304c = Xc \u2212mc are the mean-centered observations of class c with per-class mean vector mc (X\u0304 is defined analogously for the entire population X). The total scatter matrix St is the covariance matrix over the entire population of observations X.\nSc = 1\nNc \u2212 1 X\u0304Tc X\u0304c (4)\nSw = 1\nC \u2211 c Sc (5)\nSt = 1\nN \u2212 1 X\u0304T X\u0304 (6)\nThe linear combinations that maximize the objective in Equation (3), maximize the ratio of betweenand within-class scatter. This means in particular that a set of projected observations of the same class show low variance, where observations of different classes have high variance in the resulting space L. To find the optimum solution for Equation (3) one has to solve the general eigenvalue problem Sbe = eSwv. The projection matrix A is the set of eigenvectors e associated with this problem. In the following sections we will cast LDA as an objective function for DNN."}, {"heading": "3.2 DEEPLDA MODEL CONFIGURATION", "text": "Figure 1b shows a schematic sketch of DeepLDA. Instead of sample-wise optimization of the CCE loss on the predicted class probabilities (see Section 2) we put a LDA-layer on top of the DNN. This means in particular that we do not penalize the misclassification of individual samples. Instead we try to produce features that show a low intra-class and high inter-class variability. We address this maximization problem by a modified version of the general LDA eigenvalue problem proposed in the following section. In contrast to CCE, DeepLDA optimization operates on the properties of the distribution parameters on the hidden representation produced by the neural net. As eigenvalue optimization is tied to its corresponding eigenvectors (a linear projection matrix) DeepLDA can be also interpreted as a special case of a dense layer."}, {"heading": "3.3 MODIFIED DEEPLDA OPTIMIZATION TARGET", "text": "Based on Section 3.1 we reformulate the LDA objective to be suitable for a combination with deep learning. As already discussed by Stuhlsatz et al. (2012) and Lu et al. (2005) the estimation of Sw overemphasises high eigenvalues whereas small eigenvalues are estimated as too low. To weaken this effect Friedman (1989) proposed to regularize the within scatter matrix by adding a multiple of the identity matrix Sw + \u03bbI. Adding the identity matrix has the second advantage of stabilizing small eigenvalues. The resulting eigenvalue problem is then formulated as\nSbei = vi(Sw + \u03bbI)ei (7)\nwhere e = e1, ..., eC\u22121 are the resulting eigenvectors and v = v1, ...vC\u22121 the corresponding eigenvalues. Once the problem is solved each eigenvalue vi quantifies the discriminative variance in direction of the corresponding eigenvector ei. If one would like to combine this objective with a DNN the optimization target would be the maximization of the individual eigenvalues. In particular by maximizing the individual eigenvalues we should also maximize the discriminative power of the neural net. In our original experiments we started to formulate the objective as:\narg max \u0398\n1\nC \u2212 1 C\u22121\u2211 i=1 vi (8)\nOne problem we discovered with the objective in Equation (8) that the net focuses to solve trivial solutions e.g. maximize only the largest eigenvalue as this produces the highest reward. In terms of classification this means that it maximizes the distance of classes that are already separated at the expense of \u2013 potentially non-separated \u2013 neighbouring classes. This was already discussed by (Stuhlsatz et al., 2012) and tackled by a weighted computation of the between covariance matrix Sb.\nWe propose a different solution to this problem and address it by focusing our optimization on the smallest of all C \u2212 1 available eigenvalues. In particular we consider only the k eigenvalues that do not exceed a certain threshold for variance maximization:\narg max \u0398\n1\nk k\u2211 i=1 vi with {v1, ..., vk} = {vj |vj < min vj + } (9)\nThe intuition behind this formulation is to learn a net parametrization that pushes as much discriminative variance as possible into all of the C \u2212 1 available feature dimensions. We would like to underline that this formulation allows to train DeepLDA networks with backpropagation in end-to-end fashion. Our models are optimized with the Nersterov momentum version of mini-batch SGD. Related methods already showed that mini-batch learning on distribution parameters (in this case covariance matrices) is feasible if the batch-size is sufficiently large to be representative for the entire population (Andrew et al., 2013)."}, {"heading": "3.4 CLASSIFICATION BY DEEPLDA", "text": "This section describes how the most likely class label is assigned to an unseen test samples xt once the network is trained and parametrized. In a first step we compute the topmost hidden representation H on the entire training set X. On this hidden representation we compute the LDA as described\nin Section 3.1 and 3.3 producing the corresponding eigenvectors e = {ei}. The vector of class probabilities for a new test sample xt is then computed as follows:\np\u2032c = 1\n1 + eht(H\u0304ce)e T\u2212 12diag(H\u0304ceT )\n(10)\nwhere ht is the hidden representation of test sample xt and H\u0304c = (h\u0304T1 , ..., h\u0304 T C) is a matrix containing the per-class mean hidden representations over the entire train set as rows. In a last step the probabilities are normalized to sum up to one pc = p\u2032c/ \u2211 p\u2032i. Finally we assign the class label with highest probability as arg maxi pi."}, {"heading": "4 EXPERIMENTS", "text": "In this section we present an experimental evaluation of DeepLDA on three benchmark data sets \u2013 namely MNIST, CIFAR-10 and STL-10 (see Figure 2 for some sample images). We compare the results of DeepLDA with the CCE based optimization target as well as the present state of the art of the respective datasets. Further we provide details on the network architectures, hyper parameters and respective training/optimization approaches used in our experiments."}, {"heading": "4.1 EXPERIMENTAL SETUP", "text": "The general structure of the networks is similar for all of the three datasets and identical for CIFAR10 and STL-10. The architecture follows the VGG model with sequences of 3 \u00d7 3 convolutions (Simonyan & Zisserman, 2014). Instead of a dense classification layer we use global average pooling on the feature maps of the last convolution layer (Lin et al., 2013). We further apply batch normalization (Ioffe & Szegedy, 2015) after each convolutional layer which (1) helped to increase convergence speed and (2) improved the performance of all our models. Batch normalization has a positive effect on both CCE as well as DeepLDA-based optimization. In Table 1 we outline the structure of our models in detail. All networks are trained using SGD with Nesterov momentum. The initial learning rate is set to 0.1 and the momentum is fixed at 0.9 for all our models. The learning rate is than halved every 25 epochs for CIFAR-10 and STL-10 and every 10 epochs for MNIST. For further regularization we add weight decay with a weighting of 0.0001 on all trainable parameters of the models. The between-class covariance matrix regularization weight \u03bb (see Section 3.3) is set to 0.001 and the -offset for DeepLDA to 1.\nOne hyper-parameter that varies between the datasets is the batch size used for training DeepLDA. Although a large batch size is desired to get stable covariance estimates it is limited by the amount of memory available on the GPU. The mini-batches for DeepLDA where for MNIST: 1000, for CIFAR-10: 1000 and for STL-10: 200. For CCE training we used a batch size of 128 for all three models. The models are trained on an NVIDIA Tesla K40 with 12GB of GPU memory."}, {"heading": "4.2 EXPERIMENTAL RESULTS", "text": "We describe the benchmark datasets as well as the pre-processing and data augmentation used for training. We present our results and relate them to the present state of the art for the respective\ndataset. As DeepLDA is supposed to produce a linearly separable feature space, we also report the results of a linear Support Vector Machine trained on the latent space of DeepLDA (tagged with LinSVM). The results of our network architecture trained with CCE are marked as OurNetCCE."}, {"heading": "4.2.1 MNIST", "text": "The MNIST dataset consists of 28 \u00d7 28 gray scale images of handwritten digits ranging from 0 to 9. The dataset is structured into 50000 train samples, 10000 validation samples and 10000 test samples. For training we did not apply any pre-processing nor data augmentation. We present results for two different scenarios. In scenario MNIST-50k we train on the 50000 train samples and use the validation set to pick the parametrization which produces the best results on the validation set. In scenario MNIST-60k we train the model for the same number of epochs as in MNIST-50k but also use the validation set for training. Finally we report the accuracy of the model on the test set after the last training epoch. This approach was also applied in (Lin et al., 2013) which produce state of the art results on the dataset.\nTable 2 summarizes all results on the MNIST dataset. DeepLDA produces competitive results \u2013 having a test set error of 0.29% \u2013 although no data augmentation is used. In the approach described in (Graham, 2014) the train set is extended with translations of up to two pixels. We also observe that a linear SVM trained on the learned representation produces comparable results on the test set. It is also interesting that early stopping with best-model-selection (MNIST-50k) performs better than training on MNIST-60k even though 10000 more training examples are available."}, {"heading": "4.2.2 CIFAR-10", "text": "The CIFAR-10 dataset consists of tiny 32 \u00d7 32 natural RGB images containing samples of 10 different classes. The dataset is structured into 50000 train samples and 10000 test samples. We pre-processed the dataset using global contrast normalization and ZCA whitening as proposed by Goodfellow et al. (2013). During training we only apply random left-right flips on the images \u2013 no additional data augmentation is used. As training approach we follow the procedure described for the MNIST dataset above to be able to make use of the entire 50000 train images.\nTable 3 summarizes our results and relates them to the present state of the art. Our models both OurNetCCE and DeepLDA both produce state of the art results on the dataset when no data augmentation is used. Although DeepLDA performs slightly worse than CCE it is capable of producing competitive results on CIFAR-10."}, {"heading": "4.2.3 STL-10", "text": "Like CIFAR-10, the STL-10 data set contains natural RGB images of 10 different object categories. However, with 96 \u00d7 96 pixels the size of the images is larger, and the training set is considerably smaller, with only 4000 images. For validation and testing 1000 and 8000 images are available. In addition STL-10 contains 100000 unlabelled images but we do not make use of this additional data at this point. As with MNIST-50k we train our models on the 4000 train images and use the validation set to pick the best performing parametrization.\nOur results on STL-10 are presented in Table 4. Zhao et al. (2015) achieved with their joint training approach a test set accuracy of 74.80% using both unlabelled and labelled data. Our model architecture trained with CCE produces an accuracy of 78.39% which is already an improvement of 3.6 percentage points. DeepLDA further improves the results and sets a new state of the art of 81.46% test set accuracy."}, {"heading": "5 INVESTIGATONS ON DEEPLDA AND DISCUSSIONS", "text": "In this section we provide deeper insights into the representations learned by DeepLDA. We experimentally investigate the eigenvalue structure of representations learned by DeepLDA as well as its relation to the classification potential of the respective networks."}, {"heading": "5.1 DOES IMAGE SIZE AFFECT DEEPLDA?", "text": "DeepLDA shows best performance on the STL-10 dataset where it outperforms CCE by 3 percentage points. The major difference between STL-10 and CIFAR-10 \u2013 apart from the number of train images \u2013 is the size of the contained images (see Figure 2 to get an impression of the size relations).\nTo get a deeper insight into the influence of this parameter we run the following additional experiment: (1) we create a downscaled version of the STL-10 dataset with the same image dimensions as CIFAR-10 (32\u00d7 32). (2) We repeat the experiment described in Section 4.2.3 on the downscaled 32\u00d7 32 dataset. The results are presented in Figure 3, as curves showing the evolution of train and validation accuracy during training. As expected, downscaling reduces the performance of both CCE\nand DeepLDA. We further observe that DeepLDA performs best when trained on larger images and has a disadvantage on the small images. However, a closer look at the results on CIFAR-10 (CCE: 7.10% error, DeepLDA: 7.29% error, see Table 3) suggests that this effect is compensated when the training set size is sufficiently large. As a reminder: CIFAR-10 contains 50000 train images in contrast to STL-10 with only 4000 samples."}, {"heading": "5.2 EIGENVALUE STRUCTURE OF DEEPLDA REPRESENTATIONS", "text": "DeepLDA optimization does not focus on maximizing the target class likelihood of individual samples. As proposed in Section 3 we encourage the net to learn feature representations with discriminative distribution parameters (within and between class scatter). We achieve this by exploiting the eigenvalue structure of the general LDA eigenvalue problem and use it as a deep learning objective. Figure 4a shows the evolution of train and test set accuracy of STL-10 along with the mean value of all eigenvalues in the respective training epoch. We observe the expected natural correlation between the magnitude of explained \u201ddiscriminative\u201d variance and the classification potential of the resulting representation. In Figure 4b we show how the individual eigenvalues increase during training. Note that in Epoch 0 almost all eigenvalues (1-7) start at a value of 0. This emphasizes the importance of the design of our objective function (compare Equation (9)) which allows to draw discriminability into the lower dimensions of the eigen-space. In Figure 4c we additionally compare the eigenvalue structure of the latent representation produced by DeepLDA with CCE based training. Again results show that DeepLDA helps to distribute the discriminative variance more equally over the available dimensions. To give the reader an additional intuition on the learned representations we visualize the latent space of STL-10 in our supplemental materials on the final page of this paper."}, {"heading": "6 CONCLUSION", "text": "We have presented DeepLDA, a deep neural network interpretation of LDA. DeepLDA learns linearly separable latent representations in an end-to-end fashion by maximizing the eigenvalues of the general LDA eigenvalue problem. Our modified version of the LDA optimization target pushes the network to distribute discriminative variance in all dimensions of the latent feature space. Experimental results show that representations learned with DeepLDA are discriminative and have a positive effect on classification accuracy. Or models achieve competitive results on MNIST and CIFAR-10 and set a new state of the art on STL-10 with a test set classification accuracy of 81.4%. The results and further investigations suggest that DeepLDA performs best, when applied to reasonable sized images (in the present case 96\u00d796 pixel). Finally, we see DeepLDA as a specific instance of a general fruitful strategy: exploit well-understood machine learning or classification models such as LDA with certain desirable properties, and use deep networks to learn representations that provide optimal conditions for these models."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Sepp Hochreiter for helpful discussions. We would also like to thank all developers of Theano and Lasagne for providing such great deep learning frameworks. The project is granted by the Software Competence Center Hagenberg. The Tesla K40 used for this research was donated by the NVIDIA Corporation."}, {"heading": "APPENDIX: DEEPLDA LATENT REPRESENTATION", "text": "The following two figures compare the latent space representations on the STL-10 dataset. We show n-to-n scatter plots of the latent features on the first 1000 test set samples. Figure 5 shows the test set samples after projection into the C \u2212 1 dimensional DeepLDA subspace. The plot suggest that DeepLDA makes use of all available feature dimensions. An interesting observation is that many of the internal representations are orthogonal to each other (which is an implication of LDA). This of course favours linear decision boundaries. For a direct comparison we show in Figure 6 the internal representation of the topmost layer of a network trained with CCE (after Global-Average-Pooling)."}], "references": [{"title": "Deep canonical correlation analysis", "author": ["Andrew", "Galen", "Arora", "Raman", "Bilmes", "Jeff", "Livescu", "Karen"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Pcanet: A simple deep learning baseline for image classification", "author": ["Chan", "Tsung-Han", "Jia", "Kui", "Gao", "Shenghua", "Lu", "Jiwen", "Zeng", "Zinan", "Ma", "Yi"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Rectified factor networks", "author": ["Clevert", "Djork-Arn\u00e9", "Unterthiner", "Thomas", "Mayr", "Andreas", "Ramsauer", "Hubert", "Hochreiter", "Sepp"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["Dosovitskiy", "Alexey", "Springenberg", "Jost Tobias", "Riedmiller", "Martin", "Brox", "Thomas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2014}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["Fisher", "Ronald A"], "venue": "Annals of eugenics,", "citeRegEx": "Fisher and A.,? \\Q1936\\E", "shortCiteRegEx": "Fisher and A.", "year": 1936}, {"title": "Regularized discriminant analysis", "author": ["Friedman", "Jerome H"], "venue": "Journal of the American statistical association,", "citeRegEx": "Friedman and H.,? \\Q1989\\E", "shortCiteRegEx": "Friedman and H.", "year": 1989}, {"title": "Spatially-sparse convolutional neural networks", "author": ["Graham", "Benjamin"], "venue": "CoRR, abs/1409.6070,", "citeRegEx": "Graham and Benjamin.,? \\Q2014\\E", "shortCiteRegEx": "Graham and Benjamin.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition", "author": ["Lu", "Juwei", "Plataniotis", "Konstantinos N", "Venetsanopoulos", "Anastasios N"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Lu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2005}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Feature extraction with deep neural networks by a generalized discriminant analysis", "author": ["Stuhlsatz", "Andre", "Lippel", "Jens", "Zielke", "Thomas"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Stuhlsatz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stuhlsatz et al\\.", "year": 2012}, {"title": "Multi-task bayesian optimization", "author": ["Swersky", "Kevin", "Snoek", "Jasper", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems, pp. 2004\u20132012,", "citeRegEx": "Swersky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2013}, {"title": "Stacked what-where autoencoders", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "Goroshin", "Ross", "Lecun", "Yann"], "venue": "arXiv preprint arXiv:1506.02351,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "published a deep version of Canonical Correlation Analysis (DCCA) (Andrew et al., 2013).", "startOffset": 66, "endOffset": 87}, {"referenceID": 2, "context": "propose Rectified Factor Networks (RFNs) which are a neural network interpretation of classic factor analysis (Clevert et al., 2015).", "startOffset": 110, "endOffset": 132}, {"referenceID": 1, "context": "However, one bottleneck of their approach is its limitation to very shallow architectures (two stages) (Chan et al., 2015).", "startOffset": 103, "endOffset": 122}, {"referenceID": 11, "context": "already picked up the idea of combining LDA with a neural networks and proposed a generalized version of LDA (Stuhlsatz et al., 2012).", "startOffset": 109, "endOffset": 133}, {"referenceID": 11, "context": "In (Stuhlsatz et al., 2012) this problem is tackled by a heuristic weighting scheme for computing the within-class scatter matrix required for LDA optimization.", "startOffset": 3, "endOffset": 27}, {"referenceID": 0, "context": "Andrew et al. published a deep version of Canonical Correlation Analysis (DCCA) (Andrew et al., 2013). In their evaluations DCCA is used to produce correlated representation of multi-modal input data of simultaneously recorded acoustic and articulatory speech data. Clevert et al. propose Rectified Factor Networks (RFNs) which are a neural network interpretation of classic factor analysis (Clevert et al., 2015). RFNs are used for unsupervised pre-training and help to improve classification performance on four different benchmark datasets. Chan et al. (2015) propose PCANet as well as an LDA based variation.", "startOffset": 0, "endOffset": 563}, {"referenceID": 8, "context": "Deep learning has become the state of the art in automatic feature learning and replaced existing approaches based on hand engineered feature in many fields such as object recognition (Krizhevsky et al., 2012).", "startOffset": 184, "endOffset": 209}, {"referenceID": 11, "context": "This replaces the weighting scheme of (Stuhlsatz et al., 2012) and allows to operate on the original formulation of LDA.", "startOffset": 38, "endOffset": 62}, {"referenceID": 0, "context": "The input representation X can either be hand engineered features, or hidden space representations H produced by a DNN (Andrew et al., 2013).", "startOffset": 119, "endOffset": 140}, {"referenceID": 10, "context": "As already discussed by Stuhlsatz et al. (2012) and Lu et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 9, "context": "(2012) and Lu et al. (2005) the estimation of Sw overemphasises high eigenvalues whereas small eigenvalues are estimated as too low.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "(2012) and Lu et al. (2005) the estimation of Sw overemphasises high eigenvalues whereas small eigenvalues are estimated as too low. To weaken this effect Friedman (1989) proposed to regularize the within scatter matrix by adding a multiple of the identity matrix Sw + \u03bbI.", "startOffset": 11, "endOffset": 171}, {"referenceID": 11, "context": "This was already discussed by (Stuhlsatz et al., 2012) and tackled by a weighted computation of the between covariance matrix Sb.", "startOffset": 30, "endOffset": 54}, {"referenceID": 0, "context": "Related methods already showed that mini-batch learning on distribution parameters (in this case covariance matrices) is feasible if the batch-size is sufficiently large to be representative for the entire population (Andrew et al., 2013).", "startOffset": 217, "endOffset": 238}, {"referenceID": 13, "context": "Zhao et al. (2015) achieved with their joint training approach a test set accuracy of 74.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "Multi-Task Bayesian Optimization (Swersky et al. (2013)) 70.", "startOffset": 34, "endOffset": 56}, {"referenceID": 3, "context": "10% Exemplar Convnets (Dosovitskiy et al. (2014)) 72.", "startOffset": 23, "endOffset": 49}, {"referenceID": 3, "context": "10% Exemplar Convnets (Dosovitskiy et al. (2014)) 72.80% Stacked What-Where Auto-encoders (Zhao et al. (2015)) 74.", "startOffset": 23, "endOffset": 110}], "year": 2017, "abstractText": "We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns linearly separable latent representations in an end-to-end fashion. Classic LDA extracts features which preserve class separability and is used for dimensionality reduction for many classification problems. The central idea of this paper is to put LDA on top of a deep neural network. This can be seen as a non-linear extension of classic LDA. Instead of maximizing the likelihood of target labels for individual samples, we propose an objective function that pushes the network to produce feature distributions which: (a) have low variance within the same class and (b) high variance between different classes. Our objective is derived from the general LDA eigenvalue problem and still allows to train with stochastic gradient descent and back-propagation. For evaluation we test our approach on three different benchmark datasets (MNIST, CIFAR-10 and STL-10). DeepLDA produces competitive results on all three datasets and sets a new state of the art on STL-10 with a test set accuracy of 81.4%.", "creator": "LaTeX with hyperref package"}}}