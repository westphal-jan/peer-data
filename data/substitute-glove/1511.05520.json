{"id": "1511.05520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Automatic Instrument Recognition in Polyphonic Music Using Convolutional Neural Networks", "abstract": "Traditional numerical to offense and albums directly retrieval basic each means a two - ensure heritage: feature based by both with something learning algorithm. In these \" tidal \" hardware, cd engineering and develop are typically vertices has problematic. Additionally, show learning makes difficult, different except depends on extensive password important.", "histories": [["v1", "Tue, 17 Nov 2015 19:43:53 GMT  (2068kb,D)", "http://arxiv.org/abs/1511.05520v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.IR cs.LG cs.NE", "authors": ["peter li", "jiyuan qian", "tian wang"], "accepted": false, "id": "1511.05520"}, "pdf": {"name": "1511.05520.pdf", "metadata": {"source": "CRF", "title": "AUTOMATIC INSTRUMENT RECOGNITION IN POLYPHONIC MUSIC USING CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Peter Li", "Jiyuan Qian", "Tian Wang"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we present an application of convolutional neural networks for the task of automatic musical instrument identification. In this model, feature extraction and learning algorithms are trained together in an end-to-end fashion. We show that a convolutional neural network trained on raw audio can achieve performance surpassing traditional methods that rely on hand-crafted features.\nIndex Terms\u2014 convolutional neural networks, deep learning, end-to-end learning, music information retrieval, source identification,"}, {"heading": "1. INTRODUCTION", "text": "Computer audition is the general study of the systems and methods necessary for audio understanding by a machine. In a sense, computer audition concerns itself with the study of designing computers that can \u201chear\u201d as humans do. The goal is a machine that can \u201corganize what they hear; learn names for recognizable objects, actions, events, places, musical styles, instruments, and speakers; and retrieve sounds by reference to those names.\u201d [1]\nIn this paper, we focus on the first two tasks. Given a musical recording, how can we train a system to identify the instruments that are present? We present an application of deep learning for the task of automatic musical instrument identification in polyphonic music. We show that an end-toend system using convolutional neural networks trained on raw audio can surpasses traditional MIR models trained using hand-crafted features.\n\u2217All authors contributed equally to this work"}, {"heading": "1.1. Relation to Previous Work", "text": "In a paper calling for the adoption of deep architectures in MIR,[2] describe traditional MIR methods as follows:\nThe traditional approaches to these problems are rather homogeneous, adopting a two-stage architecture of feature extraction and semantic interpretation, e.g. classification, regression, clustering, similarity ranking, etc. Feature representations are predominantly hand-crafted,drawing upon significant domain-knowledge from music theory or psychoacoustics.\nSince good features are hard to craft, much of the recent research in the MIR community has concerned itself with the semantic interpretation part of the problem i.e. training better models given a set of standard audio features (e.g. MelFrequency Cepstral Coefficients or chroma)[2].\nIn this paper, we depart from the traditional MIR approach. Using convolutional neural networks, we train a model using raw audio as input. We utilize a deep architecture where feature extraction and semantic interpretation can both be learned from data directly.\nThis approach to audio signal processing has been explored before in speech processing e.g., [3] and musical audio tagging [4]. However, to the best of our knowledge, this is the first application of deep learning to source identification."}, {"heading": "2. PROBLEM DEFINITION", "text": "Before going into the details of our work, we give a formal definition of our task:\nGiven a section of audio x, we would like to predict a vector y \u2208 {0, 1}l where l is the total number of instruments and\nyi = { 1, if instrument i is in x 0, otherwise\nWe treat this as a multi-label classification problem where\nar X\niv :1\n51 1.\n05 52\n0v 1\n[ cs\n.S D\n] 1\n7 N\nov 2\n01 5\neach label corresponds to an instrument. A model should output 1 if an instrument is present and 0 otherwise."}, {"heading": "3. MODEL", "text": "Convolutional Neural Networks (CNN) [5] can be seen as a trainable feature extractor coupled with a learning model. Generally, they are know to be good at extracting high level features that represent abstract concepts from the original data. A typical model contains multiple layers of modules, where each module performs a simple data transformation. In Figure 1, we show a few common operations on a general matrix input. In a convolution layer, a filter, whose weights are learned, is convolved with its input by taking point-wise multiplication and then summing the results. Pooling is a down-sampling operation that combines nearby points. Nonlinearities are applied point wise. In Figure 2, we show a sample model architecture. Each of the first two layers contains a convolution, pooling, and non-linearity operation. The final two layers are a fully connected neural network.\nHyper-parameters of the system include the number of layers and feature maps, convolution filter size, pooling size and which non-linear activation function to use. The choice of activation functions usually include the sigmoid function, hyperbolic tangent function and more recently Rectified Linear Unit (ReLU)[6].\nModel Architecture\nOur convolutional neural network contains three temporal convolutional layers (a convolution operation where the\nheight of the filter is the height of the input) with ReLU and max pooling. These three layers are followed by two fully connected layers with ReLU and Dropout on the first layer and a sigmoid after the the second fully connected layer. This gives us a 11\u00d7 1 vector y\u0302 where each y\u0302i \u2208 [0, 1]. These predicted activations are compared to training activations using a binary cross entropy loss function. The exact model Specifications are presented in Table 1."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1. Data and Setup", "text": "We trained and evaluated our model using data from MedleDB [7]. MedleyDB is a multitrack dataset of 122 annotated musical recordings. For each song we have three types of audio content: mix, stems, and raw audio. Mixed audio is comprised of a set of stems and stems are comprised of a mix of raw audio. Since MedleyDB was primarily created to support research on melody extraction, we had to create our own labels for model training. In the follow sections, we outline our procedure.\nFor each stem, we have annotations on instrument activation that represent the confidence of whether or not the instrument is active during that time frame. These annotations were generated using a standard envelope following technique on each stem, consisting of half-wave rectification, compression, smoothing, and down-sampling. For additional details see [7].\nDataset Split\nTo train our model, we sliced individual tracks into one second, non-overlapping clips. 80% of the clips were used for model training and the remaining 20% were reserved for testing. When splitting the data, there were two main considerations. First, we didn\u2019t want to have labels that were in the test set but not in the training set. Second, we didn\u2019t want to have clips belonging to the same track in both the test and training sets.\nTo address both issues at the same time, we use the algorithm in [8] to split the 122 mixed tracks into a training and test set based on the instruments that appear in each track. After the initial split, we cut each track into one second nonoverlapping clips. This gives us our final training and test sets. Because tracks vary in length, after cutting into one second clips, the training and test set have 21177 and 4985 clips respectively.\nLabel Generation\nTo train our model, we need labels indicating whether or not an instrument appears in the entire audio clip. However, the activation confidence scores from MedleyDB provide local information i.e, activations at each point in time. To convert this to a global label for the entire clip, we take the maximum of the moving average of the activation confidences. An instrument is considered active in a clip as long as its average activation confidence exceeds a threshold over a particular window. For our experiment, we chose a windows length of 100ms and a threshold of 0.5. Figure 3 shows an example of this procedure.\nThe activation annotations cover 82 instrument. We grouped these instruments into 70 categories. To simplify\nmodel training and to have a more balanced dataset for learning, we combined all categories appearing in less than 20 songs into an \u2018OTHER\u2019 category. This gives us 11 classes for classification:\n\u2022 electric bass \u2022 acoustic guitar \u2022 synthesizer \u2022 drum set \u2022 fx/processed sound \u2022 voice\n\u2022 violin \u2022 piano \u2022 distorted electric guitar \u2022 clean electric guitar \u2022 OTHER"}, {"heading": "4.2. CNN Training", "text": "Our CNN was trained using stochastic gradient descent with a batch size of 16. To speed up training time, we transformed inputs using global contrast normalization. This is a standard preprocessing step and has been show to speed up training time. [9]."}, {"heading": "4.3. Benchmarks", "text": "For comparison, we also ran tests using more traditional MIR techniques. In the benchmarks, we use domain knowledge to construct music related features. For each audio clip, we first computed the Mel-frequency cepstral coefficients (MFCC) along with the first and second order differences, MFCC \u2206 and \u22062. These three matricies where stacked and modelled using a Gaussian distribution [10].\n MFCCMFCC\u2206 MFCC\u22062  =  | |m1 . . . mT | |  , where mi \u223c N(\u00b5,\u03a3)\n(\u00b5,\u03a3) were used as features to train a random forest and logistic regression."}, {"heading": "4.4. Experiment Results", "text": "The results of our experiments are shown in Table 2. In addition to the random forest and logistic regression, we also provide a naive benchmark that always predicts the three most common instruments in the training set. As seen, the Audio + CNN model generally outperforms the baseline models."}, {"heading": "5. DISCUSSION", "text": "Convolutional neural networks have recently show remarkable results in a number of tasks. However, it is often times difficult to intuitively understand what they are doing and why they work. In this section, we make an attempt at examining the weights learned in the first convolutional layer. This sections follows the procedures of previous works in training CNN\u2019s on audio waveforms and confirms previous findings.\nExamining the filter weights learned in the first layer, we see that the model does seem to learn a set of frequency selective filters. In Figure 4, we plot the magnitude spectra of each filter, sorted by dominant frequency. Similar to results in [4] and [3], the first convolutional layer seems to learn an auditory scale filter bank.\nIn Figure 5, we show a sample of filters learned in the first layer. As in [4] many of the learned filters are translated versions of each other. This is not necessary surprising since phase invariance is most likely difficult learn given our architecture."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "We present a convolutional neural network for instrument identification. We show that an end-to-end deep learning system can be trained to achieve performance in line with (and sometimes exceeding) traditional methods that rely on extensive domain knowledge.\nIn future work, we will investigate methods to further understand the transformations made by the CNN. Additionally, we would like to explore different architecture that may be able to learn phase and translation invariance.\nCode\nCode for this project can be found on https://github.com/glennq/instrument-recognition."}, {"heading": "7. REFERENCES", "text": "[1] Richard F Lyon, \u201cMachine hearing: An emerging field [exploratory dsp],\u201d Signal Processing Magazine, IEEE, vol. 27, no. 5, pp. 131\u2013139, 2010.\n[2] Eric J Humphrey, Juan Pablo Bello, and Yann LeCun, \u201cMoving beyond feature design: Deep architectures and automatic feature learning in music informatics.,\u201d in ISMIR. Citeseer, 2012, pp. 403\u2013408.\n[3] Y. Hoshen, R. J. Weiss, and K. W. Wilson, \u201cSpeech Acoustic Modeling from Raw Multichannel Waveforms,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015.\n[4] Sander Dieleman and Benjamin Schrauwen, \u201cEnd-toend learning for music audio,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.\n[5] Yann LeCun, Le\u0301on Bottou, Yoshua Bengio, and Patrick Haffner, \u201cGradient-based learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.\n[6] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov, \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d arXiv preprint arXiv:1207.0580, 2012.\n[7] Rachel Bittner, Justin Salamon, Mike Tierney, Matthias Mauch, Chris Cannam, and Juan Bello, \u201cMedleydb: a multitrack dataset for annotation-intensive mir research,\u201d in 15th International Society for Music Information Retrieval Conference (ISMIR 2014), 2014, pp. 155\u2013160.\n[8] Konstantinos Sechidis, Grigorios Tsoumakas, and Ioannis Vlahavas, \u201cOn the stratification of multi-label data,\u201d in Machine Learning and Knowledge Discovery in Databases, pp. 145\u2013158. Springer, 2011.\n[9] Yann A LeCun, Le\u0301on Bottou, Genevieve B Orr, and Klaus-Robert Mu\u0308ller, \u201cEfficient backprop,\u201d in Neural networks: Tricks of the trade, pp. 9\u201348. Springer, 2012.\n[10] Beth Logan et al., \u201cMel frequency cepstral coefficients for music modeling.,\u201d in ISMIR, 2000."}], "references": [{"title": "Machine hearing: An emerging field [exploratory dsp", "author": ["Richard F Lyon"], "venue": "Signal Processing Magazine, IEEE, vol. 27, no. 5, pp. 131\u2013139, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Moving beyond feature design: Deep architectures and automatic feature learning in music informatics", "author": ["Eric J Humphrey", "Juan Pablo Bello", "Yann LeCun"], "venue": "IS- MIR. Citeseer, 2012, pp. 403\u2013408.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech Acoustic Modeling from Raw Multichannel Waveforms", "author": ["Y. Hoshen", "R.J. Weiss", "K.W. Wilson"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "End-toend learning for music audio", "author": ["Sander Dieleman", "Benjamin Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Medleydb: a multitrack dataset for annotation-intensive mir research", "author": ["Rachel Bittner", "Justin Salamon", "Mike Tierney", "Matthias Mauch", "Chris Cannam", "Juan Bello"], "venue": "15th International Society for Music Information Retrieval Conference (ISMIR 2014), 2014, pp. 155\u2013160.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "On the stratification of multi-label data", "author": ["Konstantinos Sechidis", "Grigorios Tsoumakas", "Ioannis Vlahavas"], "venue": "Machine Learning and Knowledge Discovery in Databases, pp. 145\u2013158. Springer, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient backprop", "author": ["Yann A LeCun", "L\u00e9on Bottou", "Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": "Neural networks: Tricks of the trade, pp. 9\u201348. Springer, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["Beth Logan"], "venue": "ISMIR, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "\u201d [1] In this paper, we focus on the first two tasks.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "In a paper calling for the adoption of deep architectures in MIR,[2] describe traditional MIR methods as follows:", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "MelFrequency Cepstral Coefficients or chroma)[2].", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": ", [3] and musical audio tagging [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": ", [3] and musical audio tagging [4].", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Convolutional Neural Networks (CNN) [5] can be seen as a trainable feature extractor coupled with a learning model.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "The choice of activation functions usually include the sigmoid function, hyperbolic tangent function and more recently Rectified Linear Unit (ReLU)[6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 0, "context": "This gives us a 11\u00d7 1 vector \u0177 where each \u0177i \u2208 [0, 1].", "startOffset": 47, "endOffset": 53}, {"referenceID": 6, "context": "We trained and evaluated our model using data from MedleDB [7].", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "For additional details see [7].", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "To address both issues at the same time, we use the algorithm in [8] to split the 122 mixed tracks into a training and test set based on the instruments that appear in each track.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "These three matricies where stacked and modelled using a Gaussian distribution [10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 3, "context": "Similar to results in [4] and [3], the first convolutional layer seems to learn an auditory scale filter bank.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "Similar to results in [4] and [3], the first convolutional layer seems to learn an auditory scale filter bank.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "The spectra of each filter was rescaled to [0,1] by subtracting the minimum and dividing by the range.", "startOffset": 43, "endOffset": 48}, {"referenceID": 3, "context": "As in [4] many of the learned filters are translated versions of each other.", "startOffset": 6, "endOffset": 9}], "year": 2015, "abstractText": "Traditional methods to tackle many music information retrieval tasks typically follow a two-step architecture: feature engineering followed by a simple learning algorithm. In these \u201dshallow\u201d architectures, feature engineering and learning are typically disjoint and unrelated. Additionally, feature engineering is difficult, and typically depends on extensive domain expertise. In this paper, we present an application of convolutional neural networks for the task of automatic musical instrument identification. In this model, feature extraction and learning algorithms are trained together in an end-to-end fashion. We show that a convolutional neural network trained on raw audio can achieve performance surpassing traditional methods that rely on hand-crafted features.", "creator": "LaTeX with hyperref package"}}}