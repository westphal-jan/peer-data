{"id": "1511.05644", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Adversarial Autoencoders", "abstract": "In much with exactly plan man for genetic brought co-creation autoencoders been imposing particular arbitrary december taking entered predisposition element of new autoencoder. Our derived, named \" adversarial autoencoder \", uses taken recently existing paradigms tediously companies (GAN) in maintain once beating second mantissa cerebellum such long hidden code vector of three autoencoder in an subject september. Matching made uncorrelated anterior given the prior acceptable even many them no \" balls \" in the further, and fuels around any part several prior space doubt leaving meaningful forensic. As a result, first disks of the adversarial autoencoder confides close middle pantheism designing the maps the violation during to both data commercial. We scenes how posturing autoencoders always be used immediately micromanage introduction and access of images through our competitive plasticity performance sunday MNIST, Street View House Numbers when Toronto Face descriptors.", "histories": [["v1", "Wed, 18 Nov 2015 02:32:39 GMT  (4071kb,D)", "http://arxiv.org/abs/1511.05644v1", null], ["v2", "Wed, 25 May 2016 00:17:45 GMT  (7189kb,D)", "http://arxiv.org/abs/1511.05644v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alireza makhzani", "jonathon shlens", "navdeep jaitly", "ian goodfellow", "brendan frey"], "accepted": false, "id": "1511.05644"}, "pdf": {"name": "1511.05644.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["ADVERSARIAL AUTOENCODERS", "Alireza Makhzani", "Ian Goodfellow"], "emails": ["makhzani@psi.utoronto.ca", "shlens@google.com", "ndjaitly@google.com", "goodfellow@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Building scalable generative models to capture rich distributions such as audio, images or video is one of the central challenges of machine learning. Until recently, deep generative models, such as Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBNs) and Deep Boltzmann Machines (DBMs) were trained primarily by MCMC-based algorithms (Hinton et al., 2006; Salakhutdinov & Hinton, 2009). In these approaches the MCMC methods compute the gradient of loglikelihood which becomes more imprecise as training progresses. This is because samples from the Markov Chains are unable to mix between modes fast enough. In recent years, generative models have been developed that may be trained via direct back-propagation and avoid the difficulties that come with MCMC training. For example, variational autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) or importance weighted autoencoders (Burda et al., 2015) use a recognition network to predict the posterior distribution over the latent variables, generative adversarial networks (GAN) (Goodfellow et al., 2014) use an adversarial training procedure to directly shape the output distribution of the network via back-propagation and generative moment matching networks (GMMN) (Li et al., 2015) use a moment matching cost function to learn the data distribution.\nIn this paper we propose a general approach, called an adversarial autoencoder that can turn an autoencoder into a generative model. In our model, an autoencoder is trained with dual objectives \u2013 a traditional reconstruction error criterion, and an adversarial training criterion (Goodfellow et al., 2014) that matches the aggregated posterior distribution of the latent representation of the autoencoder to an arbitrary prior distribution. We show that this training criterion has a strong connection to VAE training. The result of the training is that the encoder learns to convert the data distribution to the prior distribution, while the decoder learns a deep generative model that maps the imposed prior to the data distribution."}, {"heading": "1.1 GENERATIVE ADVERSARIAL NETWORKS", "text": "The Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) framework establishes a min-max adversarial game between two neural networks \u2013 a generative model, G, and a discrimi-\nar X\niv :1\n51 1.\n05 64\n4v 1\n[ cs\n.L G\n] 1\n8 N\nov 2\n01 5\nnative model, D. The discriminator model, D(x), is a neural network that computes the probability that a point x in data space is a sample from the data distribution (positive samples) that we are trying to model, rather than a sample from our generative model (negative samples). Concurrently, the generator uses a function G(z) that maps samples z from the prior p(z) to the data space. G(z) is trained to maximally confuse the discriminator into believing that samples it generates come from the data distribution. The generator is trained by leveraging the gradient of D(x) w.r.t. x, and using that to modify its parameters. The solution to this game can be expressed as following (Goodfellow et al., 2014):\nmin G max D Ex\u223cpdata [logD(x)] + Ez\u223cp(z)[log(1\u2212D(G(z))]\nThe generatorG and the discriminatorD can be found using alternating SGD in two stages: (a) Train the discriminator to distinguish the true samples from the fake samples generated by the generator. (b) Train the generator so as to fool the discriminator with its generated samples."}, {"heading": "2 ADVERSARIAL AUTOENCODERS", "text": "Let x be the input and z be the latent code vector (hidden units) of an autoencoder with a deep encoder and decoder. Let p(z) be the prior distribution we want to impose on the codes, q(z|x) be an encoding distribution and p(x|z) be the decoding distribution. Also let pd(x) be the data distribution, and p(x) be the model distribution. The encoding function of the autoencoder q(z|x) defines an aggregated posterior distribution of q(z) on the hidden code vector of the autoencoder as follows:\nq(z) = \u222b x q(z|x)pd(x)dx (1)\nThe adversarial autoencoder is an autoencoder that is regularized by matching the aggregated posterior, q(z), to an arbitrary prior, p(z). In order to do so, an adversarial network is attached on top of the hidden code vector of the autoencoder as illustrated in Figure 1. It is the adversarial network that guides q(z) to match p(z). The autoencoder, meanwhile, attempts to minimize the reconstruction error. The generator of the adversarial network is also the encoder of the autoencoder q(z|x). The encoder ensures the aggregated posterior distribution can fool the discriminative adversarial network into thinking that the hidden code q(z) comes from the true prior distribution p(z).\nBoth, the adversarial network and the autoencoder are trained jointly with SGD in two phases \u2013 the reconstruction phase and the regularization phase \u2013 executed on each mini-batch. In the reconstruction phase, the autoencoder updates the encoder and the decoder to minimize the reconstruction error of the inputs. In the regularization phase, the adversarial network first updates its discriminative network to tell the apart the true samples (generated using the prior) from the generated samples (the hidden codes computed by the autoencoder). The adversarial network then updates its generator (which is also the encoder of the autoencoder) to confuse the discriminative network.\nOnce the training procedure is done, the decoder of the autoencoder will define a generative model that maps the imposed prior of p(z) to the the data distribution.\nThere are several possible choices for the encoder, q(z|x), of adversarial autoencoders: Deterministic: Here we assume that q(z|x) is a deterministic function of x. In this case, the encoder is similar to the encoder of a standard autoencoder and the only source of stochasticity in q(z) is the data distribution, pd(x).\nGaussian posterior: Here we assume that q(z|x) is a Gaussian distribution whose mean and variance is predicted by the encoder network: zi \u223c N (\u00b5i(x), \u03c3i(x)). In this case, the stochasticity in q(z) comes from both the data-distribution and the randomness of the Gaussian distribution at the output of the encoder. We can use the same re-parametrization trick of (Kingma & Welling, 2014) for back-propagation through the encoder network.\nUniversal approximator posterior: Adversarial autoencoders can be used to train the q(z|x) as the universal approximator of the posterior. Suppose the encoder network of the adversarial autoencoder is the function f(x, \u03b7) that takes the input x and a random noise \u03b7 with a fixed distribution (e.g., Gaussian). We can sample from arbitrary posterior distribution q(z|x), by evaluating f(x, \u03b7) at different samples of \u03b7. In other words, we can assume q(z|x, \u03b7) = \u03b4(z\u2212 f(x, \u03b7)) and the posterior q(z|x) and the aggregated posterior q(z) are defined as follows:\nq(z|x) = \u222b \u03b7 q(z|x, \u03b7)p\u03b7(\u03b7)d\u03b7\nq(z) = \u222b x \u222b \u03b7 q(z|x, \u03b7)pd(x)p\u03b7(\u03b7)d\u03b7 (2)\nIn this case, the stochasticity in q(z) comes from both the data-distribution and the random noise \u03b7 at the input of the encoder. Note that in this case the posterior q(z|x) is no longer constrained to be Gaussian and the encoder can learn any arbitrary posterior distribution for a given input x. Since there is an efficient method of sampling from the aggregated posterior q(z), the adversarial training procedure can match q(z) to p(z) by direct back-propagation through the encoder network f(x, \u03b7).\nChoosing different types of q(z|x) will result in different kinds of models with different training dynamics. For example, in the deterministic case of q(z|x), the network has to match q(z) to p(z) by only exploiting the stochasticity of the data distribution, but since the empirical distribution of the data is fixed by the training set, and the mapping is deterministic, this might produce a q(z) that is not very smooth. However, in the Gaussian or universal approximator case, the network has access to additional sources of stochasticity that could help it in the adversarial regularization stage by smoothing out q(z). Nevertheless, after extensive hyper-parameter search, we obtained similar test-likelihood with each type of q(z|x). So in the rest of the paper, we only report results with the deterministic version of q(z|x)."}, {"heading": "2.1 RELATIONSHIP TO VARIATIONAL AUTOENCODERS", "text": "Our work is similar in spirit to variational autoencoders (Kingma & Welling, 2014); however, while they use a KL divergence penalty to impose a prior distribution on the hidden code vector of the autoencoder, we use an adversarial training procedure to do so by matching the aggregated posterior of the hidden code vector with the prior distribution.\nVAE (Kingma & Welling, 2014) minimizes the following upper-bound on the negative loglikelihood of x:\nEx\u223cpd(x)[\u2212log p(x)] < Ex[Eq(z|x)[\u2212log(p(x|z)]] + Ex[KL(q(z|x)\u2016p(z))] = Ex[Eq(z|x)[\u2212log p(x|z)]]\u2212 Ex[H(q(z|x))] + Eq(z)[\u2212log p(z)]\n= Ex[Eq(z|x)[\u2212log p(x|z)]]\u2212 Ex[ \u2211 i log\u03c3i(x))] + Eq(z)[\u2212log p(z)] + const.\n= Reconstruction\u2212 Entropy + CrossEntropy(q(z), p(z)) (3)\nwhere the aggregated posterior q(z) is defined in Eq. (1) and we have assumed q(z|x) is Gaussian and p(z) is an arbitrary distribution. The variational bound contains three terms. The first term can be viewed as the reconstruction term of an autoencoder and the second and third terms can be viewed as regularization terms. Without the regularization terms, the model is simply a standard autoencoder that reconstructs the input. However, in the presence of the regularization terms, the VAE learns a latent representation that is compatible with p(z). The second term of the cost function encourages large variances for the posterior distribution while the third term minimizes the crossentropy between the aggregated posterior q(z) and the prior p(z). KL divergence or the crossentropy term in Eq. (3), encourages q(z) to pick the modes of p(z). In adversarial autoencoders, we replace the second two terms with an adversarial training procedure that encourages q(z) to match to the whole distribution of p(z).\nIn this section, we compare the ability of the adversarial autoencoder to the VAE to impose a specified prior distribution p(z) on the coding distribution. Figure 2a shows the coding space z of the test data resulting from an adversarial autoencoder trained on MNIST digits in which a spherical 2-D Gaussian prior distribution is imposed on the hidden codes z. The learned manifold in Figure 2a exhibits sharp transitions indicating that the coding space is filled and exhibits no \u201choles\u201d. In practice, sharp transitions in the coding space indicate that images generated by interpolating within z lie on the data manifold (Figure 2e). By contrast, Figure 2c shows the coding space of a VAE with the same architecture used in the adversarial autoencoder experiments. We can see that in this case the VAE roughly matches the shape of a 2-D Gaussian distribution. However, no data points map to several local regions of the coding space indicating that the VAE may not have captured the data manifold as well as the adversarial autoencoder.\nFigures 2b and 2d show the code space of an adversarial autoencoder and of a VAE where the imposed distribution is a mixture of 10 2-D Gaussians. The adversarial autoencoder successfully matched the aggregated posterior with the prior distribution (Figure 2b). In contrast, the VAE exhibit systematic differences from the mixture 10 Gaussians indicating that the VAE emphasizes matching the modes of the distribution as discussed above (Figure 2d).\nAn important difference between VAEs and adversarial autoencoders is that in VAEs, in order to back-propagate through the KL divergence by Monte-Carlo sampling, we need to have access to the exact functional form of the prior distribution. However, in adversarial autoencoders, we only need to be able to sample from the prior distribution in order to induce q(z) to match p(z). In Section 3, we demonstrate that the adversarial autoencoder can impose complicated distributions without having access to the explicit functional form of the distribution."}, {"heading": "2.2 RELATIONSHIP TO GANS AND GMMNS", "text": "In the original generative adversarial networks (GAN) paper (Goodfellow et al., 2014), GANs were used to impose the data distribution at the pixel level on the output layer of a neural network. Adversarial autoencoders, however, rely on the autoencoder training to capture the data distribution. In adversarial training procedure of our method, a much simpler distribution (e.g., Gaussian as opposed to the data distribution) is imposed in a much lower dimensional space (e.g., 20 as opposed to 1000) which results in a better test-likelihood as is discussed in Section 4.\nGenerative moment matching networks (GMMN) (Li et al., 2015) use the maximum mean discrepancy (MMD) objective to shape the distribution of the output layer of a neural network. The MMD objective can be interpreted as minimizing the distance between all moments of the model distribution and the data distribution. It has been shown that GMMNs can be combined with pre-trained dropout autoencoders to achieve better likelihood results (GMMN+AE). Our adversarial autoencoder also relies on the autoencoder to capture the data distribution. However, the main difference of our work with GMMN+AE is that the adversarial training procedure of our method acts as a regularizer that shapes the code distribution while training the autoencoder from scratch; whereas, the GMMN+AE model first trains a standard dropout autoencoder and then fits a distribution in the code space of the pre-trained network. In Section 4, we will show that the test-likelihood achieved by the joint training scheme of adversarial autoencoders outperforms the test-likelihood of GMMN and GMMN+AE on MNIST and Toronto Face datasets."}, {"heading": "3 SEMI-SUPERVISED ADVERSARIAL AUTOENCODERS", "text": "In scenarios where data is completely or partially labeled, adversarial training can incorporate the label to better shape the distribution of the hidden codes z. We show two different ways of incorporating the label information into adversarial autoencoders. The labels can be incorporated either in the adversarial training process (regularization phase), or as additional latent variables in the reconstruction phase."}, {"heading": "3.1 INCORPORATING LABEL INFORMATION IN THE ADVERSARIAL TRAINING", "text": "We first describe how to leverage partial or complete label information to regularize the latent representation of the autoencoder more heavily. To demonstrate this architecture we return to Figure 2b\nin which the adversarial autoencoder is fit to a mixture of 10 2-D Gaussians. We now aim to force each mode of the mixture of Gaussian distribution to represent a single label of MNIST.\nFigure 3a demonstrates the training procedure for this semi-supervised approach. We add a one-hot vector to the input of the discriminative network to associate the label with a mode of the distribution. The one-hot vector acts as switch that selects the corresponding decision boundary of the discriminative network given the class label. This one-hot vector has an extra class for unlabeled examples. For example, in the case of imposing a mixture of 10 2-D Gaussians (Figure 2b and 4a), the one hot vector contains 11 classes. Each of the first 10 class selects a decision boundary for the corresponding individual mixture component. The extra class in the one-hot vector corresponds to unlabeled training points. When an unlabeled point is presented to the model, the extra class is turned on, to select the decision boundary for the full mixture of Gaussian distribution. During the positive phase of adversarial training, we provide the label of the mixture component (that the positive sample is drawn from) to the discriminator through the one-hot vector. The positive samples fed for unlabeled examples come from the full mixture of Gaussian, rather than from a particular class. During the negative phase, we provide the label of the training point image to the discriminator through the one-hot vector.\nFigure 4a shows the latent representation of an adversarial autoencoder trained with a prior that is a mixture of 10 2-D Gaussians trained on 10K labeled MNIST examples and 40K unlabeled MNIST examples. In this case, the i-th mixture component of the prior has been assigned to the i-th class in a semi-supervised fashion. Figure 4b shows the manifold of the first three mixture components. Note that the style representation is consistently represented within each mixture component, independent of its class. For example, the upper-left region of all panels in Figure 4b correspond to the upright writing style and lower-right region of these panels correspond to the tilted writing style of digits.\nThis method may be extended to arbitrary distributions with no parametric forms \u2013 as demonstrated by mapping the MNIST data set onto a \u201cswiss roll\u201d (Roweis & Saul, 2000) (a conditional Gaussian distribution whose mean is uniformly distributed along the length of a swiss roll axis). Figure 4c depicts the coding space z and Figure 4d highlights the images generated by walking along the swiss roll axis in the latent space."}, {"heading": "3.2 INCORPORATING LABEL INFORMATION AS ADDITIONAL LATENT VARIABLES", "text": "Many latent factors of variation interact to generate an image. Recently, semi-supervised variational autoencoders have shown that the style and content of the images can be disentangled using an additional supervised cost (Kingma et al., 2014; Cheung et al., 2014). We now show that adversarial autoencoders may also be used to separate the image style from the class label information.\nWe alter the network architecture to provide a one-hot vector encoding of the label to the decoder (Figure 3b). The decoder utilizes both the one-hot vector identifying the label and the hidden code z to reconstruct the image. This architecture forces the network to retain all information independent of the label in the hidden code z.\nFigure 5a demonstrates the results of such a network trained on MNIST digits in which the hidden code is forced into a 15-D Gaussian. Each row of Figure 5a presents reconstructed images in which the hidden code z is fixed to a particular value but the label is systematically explored. Note that the style of the reconstructed images are consistent across a given row. Figure 5b demonstrates the same experiment applied to Street View House Numbers dataset (Netzer et al., 2011). A video showing the learnt SVHN style manifold can be found at http://www.comm.utoronto.ca/ \u02dcmakhzani/adv_ae/svhn.gif. In this experiment, the one-hot vector represents the label associated with the central digit in the image. Note that the style information in each row contains information about the labels of the left-most and right-most digits because the left-most and rightmost digits are not provided as label information in the one-hot encoding."}, {"heading": "4 LIKELIHOOD ANALYSIS", "text": "The experiments presented in the previous sections have only demonstrate qualitative results. Several methods exist for quantitatively analyzing the quality of a generative model (Buades et al., 2005; Lyu & Simoncelli, 2009). In this section we measure the ability of the generative model to capture the data distribution by comparing the likelihood of this model to generate hold-out images on the MNIST and Toronto face dataset (TFD) using the evaluation procedure described in (Goodfellow et al., 2014).\nWe trained an adversarial autoencoder on MNIST and TFD in which the model imposed a highdimensional Gaussian distribution on the underlying hidden code. Figure 6 shows samples drawn from the adversarial autoencoder trained on these datasets. A video showing the learnt TFD manifold can be found at http://www.comm.utoronto.ca/\u02dcmakhzani/adv_ae/tfd.gif. To determine whether the model is over-fitting by copying the training data points, we used the last\ncolumn of these figures to show the nearest neighbors, in Euclidean distance, to the generative model samples in the second-to-last column.\nWe evaluate the performance of the adversarial autoencoder by computing its log-likelihood on the hold out test set 1. Evaluation of the model using likelihood is not straightforward because we can not directly compute the probability of an image. Thus, we calculate a lower bound of the true log-likelihood using the methods described in prior work (Bengio et al., 2013; 2014; Goodfellow et al., 2014). We fit a Gaussian Parzen window (kernel density estimator) to 10, 000 samples generated from the model and compute the likelihood of the test data under this distribution. The free-parameter \u03c3 of the Parzen window is selected via cross-validation.\nTable 1 compares the log-likelihood of the adversarial autoencoder for real-valued MNIST and TFD to many state-of-the-art methods including DBN (Hinton et al., 2006), Stacked CAE (Bengio et al., 2013), Deep GSN (Bengio et al., 2014), Generative Adversarial Networks (Goodfellow et al., 2014) and GMMN + AE (Li et al., 2015).\n1Training details: the encoder, decoder and discriminator each have two layers of 1000 hidden units with ReLU activation function. The autoencoder is trained with a Euclidean cost function for reconstruction. The dimensionality of the hidden code z is 8 and 15 and the standard deviation of the Gaussian prior to be 5 and 10 for MNIST and TFD, respectively. On the Toronto Face dataset, data points are subtracted by the mean and divided by the standard deviation along each input dimension across the whole training set to normalize the contrast. However, after obtaining the samples, we rescaled the images (by inverting the pre-processing stage) to have pixel intensities between 0 and 1 so that we can have a fair likelihood comparison with other methods.\nNote that the Parzen window estimate is a lower bound on the true log-likelihood and the tightness of this bound depends on the number of samples drawn. To obtain a comparison with a tighter lower bound, we additionally report Parzen window estimates evaluated with 10 million samples for both the adversarial autoencoders and the generative adversarial network (Goodfellow et al., 2014). In all comparisons we find that the adversarial autoencoder achieves superior log-likelihoods to competing methods."}, {"heading": "5 CONCLUSION", "text": "In this paper we proposed a general framework to turn any autoencoder into a generative model by imposing an arbitrary distribution on the latent representation of the autoencoder. We discussed how this method can be extended to semi-supervised settings by incorporating the label information to better shape the hidden code distribution. Importantly, we demonstrated how it can be used to disentangle the style and label information of a dataset (Kingma et al., 2014; Cheung et al., 2014). Finally we showed that adversarial autoencoders can achieve state-of-the-art likelihoods on realvalued MNIST and Toronto Face datasets."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Ilya Sutskever, Oriol Vinyals, Jon Gauthier, Sam Bowman and other members of the Google Brain team for the valuable comments. We thank the developers of TensorFlow (Abadi et al., 2015), a machine learning framework that allowed us to easily develop a fast and optimized code for GPU."}], "references": [{"title": "Better mixing via deep representations", "author": ["Bengio", "Yoshua", "Mesnil", "Gr\u00e9goire", "Dauphin", "Yann", "Rifai", "Salah"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "Eric", "Alain", "Guillaume", "Yosinski", "Jason"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "A review of image denoising algorithms, with a new", "author": ["A. Buades", "B. Coll", "J.M. Morel"], "venue": "one. Simul,", "citeRegEx": "Buades et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Buades et al\\.", "year": 2005}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Discovering hidden factors of variation in deep networks", "author": ["Cheung", "Brian", "Livezey", "Jesse A", "Bansal", "Arjun K", "Olshausen", "Bruno A"], "venue": "arXiv preprint arXiv:1412.6583,", "citeRegEx": "Cheung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheung et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Generative moment matching networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Richard"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Modeling multiscale subbands of photographic images with fields of Gaussian scale mixtures", "author": ["S Lyu", "Simoncelli", "E P"], "venue": "IEEE Trans. Patt. Analysis and Machine Intelligence,", "citeRegEx": "Lyu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2009}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "Sam T", "Saul", "Lawrence K"], "venue": "SCIENCE, 290:2323\u20132326,", "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Until recently, deep generative models, such as Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBNs) and Deep Boltzmann Machines (DBMs) were trained primarily by MCMC-based algorithms (Hinton et al., 2006; Salakhutdinov & Hinton, 2009).", "startOffset": 196, "endOffset": 247}, {"referenceID": 12, "context": "For example, variational autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) or importance weighted autoencoders (Burda et al.", "startOffset": 44, "endOffset": 90}, {"referenceID": 3, "context": ", 2014) or importance weighted autoencoders (Burda et al., 2015) use a recognition network to predict the posterior distribution over the latent variables, generative adversarial networks (GAN) (Goodfellow et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 5, "context": ", 2015) use a recognition network to predict the posterior distribution over the latent variables, generative adversarial networks (GAN) (Goodfellow et al., 2014) use an adversarial training procedure to directly shape the output distribution of the network via back-propagation and generative moment matching networks (GMMN) (Li et al.", "startOffset": 137, "endOffset": 162}, {"referenceID": 9, "context": ", 2014) use an adversarial training procedure to directly shape the output distribution of the network via back-propagation and generative moment matching networks (GMMN) (Li et al., 2015) use a moment matching cost function to learn the data distribution.", "startOffset": 171, "endOffset": 188}, {"referenceID": 5, "context": "In our model, an autoencoder is trained with dual objectives \u2013 a traditional reconstruction error criterion, and an adversarial training criterion (Goodfellow et al., 2014) that matches the aggregated posterior distribution of the latent representation of the autoencoder to an arbitrary prior distribution.", "startOffset": 147, "endOffset": 172}, {"referenceID": 5, "context": "The Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) framework establishes a min-max adversarial game between two neural networks \u2013 a generative model, G, and a discrimi-", "startOffset": 42, "endOffset": 67}, {"referenceID": 5, "context": "The solution to this game can be expressed as following (Goodfellow et al., 2014): min G max D Ex\u223cpdata [logD(x)] + Ez\u223cp(z)[log(1\u2212D(G(z))]", "startOffset": 56, "endOffset": 81}, {"referenceID": 5, "context": "In the original generative adversarial networks (GAN) paper (Goodfellow et al., 2014), GANs were used to impose the data distribution at the pixel level on the output layer of a neural network.", "startOffset": 60, "endOffset": 85}, {"referenceID": 9, "context": "Generative moment matching networks (GMMN) (Li et al., 2015) use the maximum mean discrepancy (MMD) objective to shape the distribution of the output layer of a neural network.", "startOffset": 43, "endOffset": 60}, {"referenceID": 7, "context": "Recently, semi-supervised variational autoencoders have shown that the style and content of the images can be disentangled using an additional supervised cost (Kingma et al., 2014; Cheung et al., 2014).", "startOffset": 159, "endOffset": 201}, {"referenceID": 4, "context": "Recently, semi-supervised variational autoencoders have shown that the style and content of the images can be disentangled using an additional supervised cost (Kingma et al., 2014; Cheung et al., 2014).", "startOffset": 159, "endOffset": 201}, {"referenceID": 11, "context": "Figure 5b demonstrates the same experiment applied to Street View House Numbers dataset (Netzer et al., 2011).", "startOffset": 88, "endOffset": 109}, {"referenceID": 2, "context": "Several methods exist for quantitatively analyzing the quality of a generative model (Buades et al., 2005; Lyu & Simoncelli, 2009).", "startOffset": 85, "endOffset": 130}, {"referenceID": 5, "context": "In this section we measure the ability of the generative model to capture the data distribution by comparing the likelihood of this model to generate hold-out images on the MNIST and Toronto face dataset (TFD) using the evaluation procedure described in (Goodfellow et al., 2014).", "startOffset": 254, "endOffset": 279}, {"referenceID": 0, "context": "Thus, we calculate a lower bound of the true log-likelihood using the methods described in prior work (Bengio et al., 2013; 2014; Goodfellow et al., 2014).", "startOffset": 102, "endOffset": 154}, {"referenceID": 5, "context": "Thus, we calculate a lower bound of the true log-likelihood using the methods described in prior work (Bengio et al., 2013; 2014; Goodfellow et al., 2014).", "startOffset": 102, "endOffset": 154}, {"referenceID": 6, "context": "Table 1 compares the log-likelihood of the adversarial autoencoder for real-valued MNIST and TFD to many state-of-the-art methods including DBN (Hinton et al., 2006), Stacked CAE (Bengio et al.", "startOffset": 144, "endOffset": 165}, {"referenceID": 0, "context": ", 2006), Stacked CAE (Bengio et al., 2013), Deep GSN (Bengio et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 1, "context": ", 2013), Deep GSN (Bengio et al., 2014), Generative Adversarial Networks (Goodfellow et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 5, "context": ", 2014), Generative Adversarial Networks (Goodfellow et al., 2014) and GMMN + AE (Li et al.", "startOffset": 41, "endOffset": 66}, {"referenceID": 9, "context": ", 2014) and GMMN + AE (Li et al., 2015).", "startOffset": 22, "endOffset": 39}, {"referenceID": 6, "context": "MNIST (10K) MNIST (10M) TFD (10K) TFD (10M) DBN (Hinton et al., 2006) 138\u00b1 2 1909\u00b1 66 Stacked CAE (Bengio et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 0, "context": ", 2006) 138\u00b1 2 1909\u00b1 66 Stacked CAE (Bengio et al., 2013) 121\u00b1 1.", "startOffset": 36, "endOffset": 57}, {"referenceID": 1, "context": "6 2110\u00b1 50 Deep GSN (Bengio et al., 2014) 214\u00b1 1.", "startOffset": 20, "endOffset": 41}, {"referenceID": 5, "context": "1 1890\u00b1 29 GAN (Goodfellow et al., 2014) 225\u00b1 2 386 2057\u00b1 26 GMMN + AE (Li et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 9, "context": ", 2014) 225\u00b1 2 386 2057\u00b1 26 GMMN + AE (Li et al., 2015) 282\u00b1 2 2204\u00b1 20 Adversarial Autoencoder 340\u00b1 2 427 2252\u00b1 16 2522 Table 1: Log-likelihood of test data on MNIST and Toronto Face dataset.", "startOffset": 38, "endOffset": 55}, {"referenceID": 5, "context": "To obtain a comparison with a tighter lower bound, we additionally report Parzen window estimates evaluated with 10 million samples for both the adversarial autoencoders and the generative adversarial network (Goodfellow et al., 2014).", "startOffset": 209, "endOffset": 234}, {"referenceID": 7, "context": "Importantly, we demonstrated how it can be used to disentangle the style and label information of a dataset (Kingma et al., 2014; Cheung et al., 2014).", "startOffset": 108, "endOffset": 150}, {"referenceID": 4, "context": "Importantly, we demonstrated how it can be used to disentangle the style and label information of a dataset (Kingma et al., 2014; Cheung et al., 2014).", "startOffset": 108, "endOffset": 150}], "year": 2015, "abstractText": "In this paper we propose a new method for regularizing autoencoders by imposing an arbitrary prior on the latent representation of the autoencoder. Our method, named \u201cadversarial autoencoder\u201d, uses the recently proposed generative adversarial networks (GAN) in order to match the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior. Matching the aggregated posterior to the prior ensures that there are no \u201choles\u201d in the prior, and generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how adversarial autoencoders can be used to disentangle style and content of images and achieve competitive generative performance on MNIST, Street View House Numbers and Toronto Face datasets.", "creator": "LaTeX with hyperref package"}}}