{"id": "1511.06038", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Neural Variational Inference for Text Processing", "abstract": "Recent advances in neural asymptotic inference have spawned new renaissance the deep latent spacing models. In actually paper 're introduce a preferred variational deviation supports including generative and initial models from references. While promote variational methods derive fact analytic deterministic all since conflicts gnu/linux much latent polynomial, hold we construct same inference network conditioned where set graphs script user will more the formulation reduced. We validate time balanced on came very different excerpts nonlinear additionally, mmf contents modelling but supervised even answering. Our neural variational explicit basic concepts taken periodic oscillations document representation with has inside - of - words generative newest many phenomenal under ratio reported idiosyncracies taken 15 standard test phyllostachys. The susceptibility read selection redesigned employs a computation relation layer within response attention facilitate to extract seen semantics years a explain and wrong pairs. On seven question answering benchmarks not theory highest this eight writings benchmarks.", "histories": [["v1", "Thu, 19 Nov 2015 01:23:28 GMT  (424kb,D)", "http://arxiv.org/abs/1511.06038v1", "ICLR 2016 submission"], ["v2", "Mon, 30 Nov 2015 14:35:48 GMT  (632kb,D)", "http://arxiv.org/abs/1511.06038v2", "ICLR 2016 submission"], ["v3", "Thu, 7 Jan 2016 19:49:17 GMT  (801kb,D)", "http://arxiv.org/abs/1511.06038v3", "ICLR 2016 submission"], ["v4", "Sat, 4 Jun 2016 06:41:58 GMT  (913kb,D)", "http://arxiv.org/abs/1511.06038v4", "ICML 2016"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["yishu miao", "lei yu", "phil blunsom"], "accepted": true, "id": "1511.06038"}, "pdf": {"name": "1511.06038.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["TEXT PROCESSING", "Yishu Miao", "Lei Yu", "Phil Blunsom"], "emails": ["yishu.miao@cs.ox.ac.uk", "lei.yu@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "Probabilistic generative models underpin many successful applications within the field of natural language processing (NLP). Their popularity stems form their ability to use unlabelled data effectively, to incorporate abundant linguistic features, and to learn interpretable dependencies among data. However these successes are tempered by the fact that as the structure of such generative models becomes deeper and more complex, true Bayesian inference becomes intractable due to the high dimensional integrals required. Markov chain Monte Carlo (MCMC) (Neal, 1993; Andrieu et al., 2003) and variational inference (Jordan et al., 1999; Attias, 2000; Beal, 2003) are the standard approaches for approximating these integrals. However the computational cost of the former results in impractical training for the large and deep neural networks which are now fashionable, and the latter is conventionally confined due to the underestimation of posterior variance. The lack of effective and efficient inference methods hinders our ability to create highly expressive models of text, especially in the situation where the model is non-conjugate.\nThis paper introduces a neural variational framework for generative models of text, inspired by the variational auto-encoder (Rezende et al., 2014; Kingma & Welling, 2013). The principle idea is to build an inference network, implemented by a deep neural network conditioned on text, to approximate the intractable distributions over the latent variables. Instead of providing an analytic approximation, as in traditional variational Bayes, neural variational inference learns to model the posterior probability, thus endowing the model with strong generalisation abilities. Due to the flexibility of deep neural networks, the inference network is capable of learning complicated non-linear distributions and processing structured inputs such as word sequences. Inference networks can be designed as, but not restricted to, multilayer perceptrons (MLP), convolutional neural networks, and recurrent neural networks (RNN), approaches which are rarely used in conventional generative models. By using the reparameterisation method (Rezende et al., 2014; Kingma & Welling, 2013), the inference network is trained through back-propagating unbiased and low variance gradients w.r.t. the latent variables. Within this framework, we propose a Neural Variational Document Model (NVDM) for document modelling and a Neural Answer Selection Model (NASM) for question answering, a task that selects the sentences that correctly answer a factoid question from a set of candidate sentences.\nar X\niv :1\n51 1.\n06 03\n8v 1\n[ cs\n.C L\n] 1\n9 N\nov 2\n01 5\nThe NVDM (Figure 1) is an unsupervised generative model of text which aims to extract a continuous semantic latent variable for each document. This model can be interpreted as a variational auto-encoder: an MLP encoder (inference network) compresses the bag-of-words document representation into a continuous latent distribution, and a softmax decoder (generative model) reconstructs the document by generating the words independently. A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al., 2013; Mnih & Gregor, 2014). Our experiments demonstrate that our neural document model achieves state of the art perplexities on the 20 Newsgroups and Reuters datasets.\nThe NASM (Figure 2) is a supervised conditional model which imbues LSTMs (Hochreiter & Schmidhuber, 1997) with a latent stochastic attention mechanism to model the semantics of question-answer pairs and predict their relatedness. The attention model is designed to focus on the phrases of an answer that are strongly connected to the question semantics and is modelled by a latent distribution. This mechanism allows the model to deal with the ambiguity inherent in the task and learns pair-specific representations that are more effective at predicting answer matches, rather than independent embeddings of question and answer sentences. Bayesian inference provides a natural safeguard against overfitting, especially as the training sets available for this task are small. The experiments show that the LSTM with a latent stochastic attention mechanism learns an effective attention model and outperforms both previously published results, and our own strong non-stochastic attention baselines.\nIn summary, we demonstrate the effectiveness of neural variational inference for text processing on two diverse tasks. These models are simple, expressive and can be trained efficiently with the highly scalable stochastic gradient variational Bayes (SGVB) algorithm (Rezende et al., 2014; Kingma & Welling, 2013). Our neural variational framework is suitable for both unsupervised and supervised learning tasks, and can be generalised to incorporate any type of neural networks."}, {"heading": "2 NEURAL VARIATIONAL INFERENCE FRAMEWORK", "text": "Latent variable modelling is popular in many NLP problems, but it is non-trivial to carry out effective and efficient inference for models with complex and deep structure. In this section we introduce a generic neural variational inference framework that we apply to both the unsupervised NVDM and supervised NASM in the follow sections.\nWe focus on the general scenario in which a latent variable h is introduced into a deep neural network. We designate the observed parent and child nodes of h as z and x respectively. We form the variational lower bound L to optimise the joint distribution:\nlog p\u03b8(z,x) = log\n\u222b q(h)\nq(h) p\u03b8(x|h)p\u03b8(h|z)p(z)dh (1)\n> Eq(h)[log p\u03b8(x|h)p\u03b8(h|z)p(z)\u2212 log q(h)] = L (2) where \u03b8 parameterises the generative distributions p\u03b8(x|h) and p\u03b8(h|z). To have a tight lower bound, the variational distribution q(h) should approach the true posterior p(h|z,x). The three steps to construct the deep neural inference network q\u03c6(h|z,x), which is a parameterised diagonal Gaussian distribution N (h|\u00b5(z,x),diag(\u03c32(z,x))), are:\n1. Construct vector representations of the observed variables: u = fz(z), v = fx(x).\n2. Assemble a joint representation: \u03c0 = g(u,v).\n3. Parameterise the variational distribution over the latent variable: \u00b5 = l1(\u03c0), log\u03c3 = l2(\u03c0).\nf(\u00b7) can be any type of deep neural network that is suitable for the observed data; g(\u00b7) is an MLP that concatenates the vector representations of the conditioning variables; l(\u00b7) is a linear transformation which outputs the parameters of the Gaussian distribution. By sampling from the variational distribution, h \u223c q\u03c6(h|z,x), we are able to carry out stochastic back-propagation to optimise the lowerbound (Eq.2).\nq (h\u2223X) (Inference Network)\nX\np(X\u2223h)\nh\nX\nFigure 1: NVDM for document modelling.\na\nq\ny\nz q\nc(a,h)\nz a\nh\np(h\u2223q) p( y\u2223zq , z a)\n\u03b1\nFigure 2: NASM for question answer selection.\nDuring training, the model parameters \u03b8 together with the inference network parameters \u03c6 are updated by stochastic back-propagation based on the samples h drawn from q\u03c6(h|z,x). For the gradients w.r.t. \u03b8, we have the form:\n\u2207\u03b8L ' 1L \u2211L l=1\u2207\u03b8 log p\u03b8(x|h (l))p\u03b8(h (l)|z) (3)\nFor the gradients w.r.t. \u03c6 we reparameterise h = \u00b5 + \u03c3 \u00b7 and sample (l) \u223c N (0, I) (Rezende et al., 2014; Kingma & Welling, 2013). The update of \u03c6 can be carried out by back-propagating the gradients w.r.t. \u00b5 and \u03c3:\n\u2207\u00b5L ' 1L \u2211L l=1\u2207h[log p\u03b8(x|h (l))p\u03b8(h (l)|z)\u2212 log q\u03c6(h(l)|z,x)] (4)\n\u2207\u03c3L ' 12L \u2211L l=1 (l)\u2207h(l) [log p\u03b8(x|h (l))p\u03b8(h (l)|z)\u2212 log q\u03c6(h(l)|z,x)] (5)\nIt is worth mentioning that unsupervised learning is a special case of the variational autoencoder where h has no parent node z. In that case h is directly drawn from the prior p(h) instead of the conditional distribution p\u03b8(h|z). Here we only discuss the scenario where the latent variables are continuous and the parameterised diagonal Gaussian distribution is employed as the variational distribution. However the framework is also suitable for discrete units, and the only modification needed is to replace the Gaussian with a multinomial parameterised by the outputs of a softmax function. Though the reparameterisation trick for continuous variables is not applicable in this case, a policy gradient approach (Mnih & Gregor, 2014) can help to alleviate the problem of high variance during stochastic estimation."}, {"heading": "3 NEURAL VARIATIONAL DOCUMENT MODEL", "text": "The Neural Variational Document Model (Figure 1) is a simple instance of unsupervised learning where a continuous hidden variable h \u2208 RK , which generates all the words in a document independently, is introduced to represent its semantic content. Let X \u2208 R|V | be the bag-of-words representation of a document and xi \u2208 R|V | be the one-hot representation of the word at position i. As an unsupervised generative model, we could interpret NVDM as a variational autoencoder: an MLP encoder p(h|X) compresses document representations into continuous hidden vectors (X \u2192 h); a softmax decoder p(X|h) = \u220f i p(xi|h) reconstructs the documents by independently generating the words (h\u2192 {xi}). To maximise the document log-likelihood log \u2211 h p(X|h)p(h), we derive the lowerbound:\nL = Eq\u03c6(h|X) [\u2211N i=1 log p\u03b8(xi|h) ] \u2212DKL[q\u03c6(h|X)\u2016p(h)] (6)\nwhereN is the number of words in the document and p(h) is a Gaussian prior for h. The conditional probability over words p\u03b8(xi|h) (decoder) is modelled by multinomial logistic regression and shared across documents:\np\u03b8(xi|h) = exp{\u2212E(xi;h, \u03b8))}\u2211|V | j=1 exp{\u2212E(xj ;h, \u03b8)}\n(7)\nE(xi;h, \u03b8) = \u2212hTRxi \u2212 bxi (8)\nwhereR \u2208 RK\u00d7|V | learns the semantic word embeddings and bxi represents the bias term. As there is no supervision information for the latent semantics, h, the posterior approximation q\u03c6(h|X) is only conditioned on the current document X . The inference network q\u03c6(h|X) = N (h|\u00b5(X), diag(\u03c32(X))) is modelled as:\n\u03c0 = g(fMLPX (X)) (9) \u00b5 = l1(\u03c0), log\u03c3 = l2(\u03c0) (10)\nFor each document X , the neural network generates its own parameters \u00b5 and \u03c3 that parameterise the latent distribution over document semantics h. Based on the samples h \u223c q\u03c6(h|X), the lower bound (Eq. 6) can be optimised by stochastic back-propagation.\nSince p(h) is a standard Gaussian prior, DKL[q\u03c6(h|X)\u2016p(h)] is a Gaussian KL-Divergence which can be computed analytically to further lower the variance of the gradients. Following the neural variational framework, the parameters of inference network \u03c6 and the model parameters \u03b8 are updated by SGVB. Appendix B.1 provides the formal details of the neural network structure."}, {"heading": "4 NEURAL ANSWER SELECTION MODEL", "text": "Answer sentence selection is a question answering paradigm where a model must identify the correct sentences answering a factual question from a set of candidate sentences. Assume a question q is associated with a set of answer sentences {a1,a2, ...,an}, together with their judgements {y1,y2, ...,yn}, where ym = 1 if the answer am is correct and ym = 0 otherwise. This is a classification task where we treat each training data point as a triple (q,a,y) while predicting y for the unlabelled question-answer pair (q,a).\nThe Neural Answer Selection Model (Figure 2) is a supervised model that learns the question and answer representations and predicts their relatedness. It employs two LSTMs to embed raw question inputs q and answer inputs a. We use sq(j) and sa(i) to represent the outputs of the two LSTMs. Conventionally, the last state outputs sq(|q|) and sa(|a|), as the independent question and answer representations, can be used for relatedness prediction. In NASM, however, we aim to learn pair-specific representations by a latent attention mechanism, which are more effective for the pair relatedness prediction.\nThe aim of the attention model is to focus on the words in the answer sentence that are prominent for predicting the answer match to the current question. Instead of using a deterministic question vector, such as sq(|q|), NASM employs a latent distribution p\u03b8(h|q) to model the question semantics, which is a parameterised diagonal Gaussian N (h|\u00b5(q),diag(\u03c32(q))). It extracts a context vector c(a,h) by iteratively attending to the answer tokens based on the stochastic vector h \u223c p\u03b8(h|q). In doing so the model is able to adapt to the ambiguity inherent in questions and obtain salient information through attention. Compared to its deterministic counterpart, the stochastic units incorporated into NASM allow multi-modality attention distributions. Further, by marginalising over the latent variables, NASM is more robust to overfitting, which is important for small question answering training sets.\nIn this model, the conditional distribution p\u03b8(h|q) is modelled as:\n\u03c0\u03b8 = g\u03b8(f LSTM q (q)) = g\u03b8(sq(|q|)) (11)\n\u00b5\u03b8 = l1(\u03c0\u03b8), log\u03c3\u03b8 = l2(\u03c0\u03b8) (12)\nFor each question q, the neural network generates its own parameters \u00b5 and \u03c3 that parameterise the latent distribution over question semantics h. The attention model is defined as:\n\u03b1(i) \u221d exp(W T\u03b1 tanh(W hh+W ssa(i))) (13) c(a,h) = \u2211 i sa(i)\u03b1(i) (14) za(a,h) = tanh (W ac(a,h) +W nsa(|a|)) (15)\nwhere \u03b1(i) is the normalised attention score at answer token i, and the context vector c(a,h) is the weighted sum of all the state outputs sa(i). We adopt zq(q), za(a,h) as the question and answer representations for predicting their relatedness y. zq(q) is a deterministic vector that is equal to\nsq(|q|), while za(a,h) is a combination of the sequence output sa(|a|) and the context vector c(a,h) (Eq.15). For the prediction of pair relatedness y, we model the conditional probability distribution p\u03b8(y|zq, za) by sigmoid function:\np\u03b8(y = 1|zq, za) = \u03c3 ( zTq Mza + b ) (16)\nTo maximise the log-likelihood log p(y|q,a) we use the variational lower bound: log p(y|q,a) = log \u2211\nh p\u03b8(y|zq(q), za(a,h))p\u03b8(h|q) (17)\n> Eq\u03c6(h)[log p\u03b8(y|zq(q), za(a,h))]\u2212DKL(q\u03c6(h)||p\u03b8(h|q)) = L (18) Following the neural variational inference framework, we construct a deep neural network as the inference network q\u03c6(h|q,a,y) = N (h|\u00b5\u03c6(q,a,y),diag(\u03c32\u03c6(q,a,y))):\n\u03c0\u03c6 = g\u03c6(f LSTM q (q), f LSTM a (a), fy(y)) = g\u03c6(sq(|q|), sa(|a|), sy) (19)\n\u00b5\u03c6 = l3(\u03c0\u03c6), log\u03c3\u03c6 = l4(\u03c0\u03c6) (20)\nwhere q and a are modelled by LSTMs1, and the relatedness label y is modelled by a simple linear transformation into the vector sy . According to the joint representation \u03c0\u03c6, we then generate the parameters \u00b5\u03c6 and \u03c3\u03c6, which parameterise the latent distribution over the question semantics h. To emphasise, though both p\u03b8(h|q) and q\u03c6(h|q,a,y) are modelled as parameterised Gaussian distributions, q\u03c6(h|q,a,y) as an approximation only functions during inference to compute the stochastic gradients, while p\u03b8(h|q) as the generative distribution is used in prediction for the question-answer relatedness y.\nBased on the samples h \u223c q\u03c6(h|q,a,y), we use SGVB to optimise the lowerbound (Eq.18). The model parameters \u03b8 and the inference network parameters \u03c6 are updated jointly using their stochastic gradients. In this case, similar to the NVDM, the Gaussian KL divergence DKL[q\u03c6(h|q,a,y))\u2016p\u03b8(h|q)] can be analytically computed during training process. More details about the network structure can be found in Appendix B.2."}, {"heading": "5 EXPERIMENTS", "text": ""}, {"heading": "5.1 DATASET & SETUP FOR DOCUMENT MODELLING", "text": "We experiment with NVDM on two standard news corpora: the 20NewsGroups2 and the Reuters RCV1-v23 datasets. The 20NewsGroups dataset is a collection of newsgroup documents, consisting of 11,314 training and 7,531 test articles. The RCV1-v2 dataset is a large collection from Reuters newswire stories with 794,414 training and 10,000 test cases. We apply the standard preprocessing procedure as Hinton & Salakhutdinov (2009); Mnih & Gregor (2014) and set the vocabulary size of the 20NewsGroups and RCV1-v2 datasets as 2,000 and 10,000 respectively.\nIn order to make a direct comparison with the prior work we follow the same setup as Hinton & Salakhutdinov (2009), Larochelle & Lauly (2012), Srivastava et al. (2013), and Mnih & Gregor (2014). We train NVDM models with 50 and 200 dimensional document representations respectively. For the construction of the inference network, we use an MLP (in Eq. 9) with 2 layers and 500 dimension rectifier linear units, which converts document representations into embeddings. During training we carry out stochastic estimation by taking one sample for computing the expectation of the stochastic gradients, while in prediction we use 20 samples for predicting document perplexity. The model is trained by Adam (Kingma & Ba, 2015) and we alternately optimise the generative model and the inference network by fixing the parameters of one while updating the parameters of the other."}, {"heading": "5.2 EXPERIMENTAL RESULTS ON DOCUMENT MODELLING", "text": "Table 1a presents the results of document modelling on the test datasets. The first column lists the baseline models, and the second column shows the dimension of latent variables used in the\n1In this case, the LSTMs for q and a are shared by the inference network and the generative model, but there is no restriction to use different LSTMs in the two neural networks.\n2http://qwone.com/ jason/20Newsgroups 3http://trec.nist.gov/data/reuters/reuters.html\nexperiments. The final two columns present the perplexity achieved by each topic model on the 20NewsGroups and RCV1-v2 datasets. In document modelling, perplexity is computed by exp(\u2212 1D \u2211Nd n 1 Nd\nlog p(Xd)), where D is the number of documents, Nd represents the length of the dth document and log p(X) = log \u222b p(X|h)p(h)dh is the log probability of the words in the document. Since log p(X) is intractable in the NVDM, we use the variational lower bound (which is an upper bound on perplexity) to compute the perplexity as in Mnih & Gregor (2014).\nWhile all the baseline models listed in Table 1a apply discrete latent variables, here NVDM employs a continuous stochastic document representation. The experiment results indicate that our NVDM achieves the best performance on both two datasets. For the experiments on RCV1-v2 dataset, the NVDM with latent variable of 50 dimension performs even better than the fDARN with 200 dimension. It demonstrates that our document model with continuous latent variables has higher expressiveness and better generalisation ability.\nIn addition to the experiments on perplexity, we also qualitatively evaluate the semantic information learned by NVDM on the 20NewsGroups dataset with latent variables of 50 dimension. We assume each dimension in the latent space represents a topic that corresponds to a specific semantic meaning. Table 1b presents 5 randomly selected topics with 10 words that have the strongest positive connection with the topic. Based on the words in each column, we can deduce their corresponding topics as: Space, Religion, Encryption, Sport and Policy. Table 2 compares the 5 nearest words selected according to the semantic vector learned from NVDM and docNADE. Although the model does not impose independent interpretability on the latent representation dimensions, we still see\n1 10 20 50 100 Samples\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nS ta\nn d a rd\nD e v ia\nti o n\n1e 3 1.28\n0.77\n0.46 0.39\n0.32\nFigure 3: The standard deviations of MAP scores computed by running 10 NASM models on WikiQA with different numbers of samples.\nthat the NVDM learns locally interpretable structure. In addition to the listed results, Appendix A presents a t-sne visualisation (Van der Maaten & Hinton, 2008) of the document representations."}, {"heading": "5.3 DATASET & SETUP FOR ANSWER SENTENCE SELECTION", "text": "We experiment on two answer selection datasets, the QASent and the WikiQA datasets. QASent (Wang et al., 2007) was created from the TREC QA track, and the WikiQA (Yang et al., 2015) is constructed from Wikipedia. Due to the different approaches for constructing the datasets4, the WikiQA dataset is less noisy and less biased towards lexical overlap. Table 3 summarises the statistics of the two datasets.\nIn order to investigate the effectiveness of our NASM model we also implemented two strong baseline models \u2014 a vanilla LSTM model (LSTM) and an LSTM model with a deterministic attention mechanism (LSTM+Att). For the former, the last state outputs from the question and answer LSTM models are passed directly to the QA matching function (Eq.16). For the latter, we add an attention model that uses the last state output of the question LSTM as the question semantics to extract the context vector (Eq.14). LSTM+Att is the deterministic counterpart of NASM. Following previous work, for each of our models we also add a lexical overlap feature by combining a co-occurrence word count feature with the probability generated from the neural model. We adopt MAP and MRR as the evaluation metrics for this task.\nTo facilitate direct comparison with previous work we follow the same experimental setup as Yu et al. (2014) and Severyn (2015). The word embeddings (K = 50) are obtained by running the word2vec tool (Mikolov et al., 2013) on the English Wikipedia dump and the AQUAINT5 corpus. We use LSTMs with 3 layers and 50 hidden units, and apply 40% dropout after the embedding layer. For the construction of the inference network, we use an MLP (in Eq. 11) with 2 layers and tanh units of 50 dimension, and an MLP (in Eq. 19) with 2 layers and tanh units of 150 dimension for the joint representation. In training we carry out stochastic estimation by taking one sample for computing the gradients, while in prediction, we use 20 samples to calculate the expectation of the lower bound. Figure 3 presents the standard deviation of NASM\u2019s MAP scores while using different numbers of samples. The models are trained using Adam, with hyperparameters selected by optimising the MAP score on the development set. Considering the trade-off between computational cost and variance, we chose 20 samples for prediction in all the experiments."}, {"heading": "5.4 EXPERIMENTAL RESULTS ON ANSWER SENTENCE SELECTION", "text": "Table 4 compares the results of our models with current state-of-the-art models on both answer selection datasets. As shown in Table 4a, on the QASent dataset, our vanilla LSTM model outperforms the deep CNN 6 model by approximately 7% on MAP and 6% on MRR. The LSTM+Att performs\n4Yang et al. (2015) provide detailed explanation of the differences between the two datasets. 5https://catalog.ldc.upenn.edu/LDC2002T31 6As stated in (Yih et al., 2013) that the evaluation scripts used by previous work are noisy \u2014 4 out of 72 questions in the test set are treated answered incorrectly. This makes the MAP and MRR scores \u223c 4% lower than the true scores. Severyn (2015) and Wang & Ittycheriah (2015), however, use a cleaned-up version of\nslightly better than the vanilla LSTM model, and our NASM improves the results further. Since the QASent dataset is biased towards lexical overlapping features, after combining with a co-occurrence word count feature, our best model NASM outperforms all the previous models, including both neural network based models and classifiers with a set of hand-crafted features (e.g. LCLR). Similarly, on the WikiQA dataset, all of our models outperform the previous distributional models by a large margin. By including a word count feature, our models improve further and achieve the state of the art. Notably, on both datasets, our two LSTM-based models have set strong baselines and NASM works even better, which demonstrates the effectiveness of introducing stochastic variables to model question semantics in this answer sentence selection task.\nthe evaluation scripts. In order to make our results directly comparable with previous work, we use the noisy evaluation scripts; and scale Severyn\u2019s and Wang\u2019s results by re-evaluating their outputs with the noisy scripts.\nIn Figure 4, we compare the effectiveness of the latent attention mechanism and its deterministic counterpart by visualising the attention scores on the answer sentences. For most of the sentences that are not answering the question, neither of the two attention models can attend to reasonable words that are beneficial for predicting relatedness. But for the correct answer sentences, such as the ones in Figure 4, both attentive models are able to capture crucial information by attending to different parts of the sentence based on the question semantics. Interestingly, compared to the deterministic counterpart, our NASM assigns higher attention scores on the prominent words that are relevant to the question, which forms a more peaked distribution and in turn helps the model achieve better performance."}, {"heading": "6 RELATED WORK", "text": "Training an inference network to approximate the variational distribution was first proposed in the context of Helmholtz machines (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996), but applications of these directed generative models come up against the problem of establishing low variance gradient estimators. Recent advances in neural variational inference (Rezende et al., 2014; Kingma & Welling, 2013; Mnih & Gregor, 2014) mitigate this problem by reparameterising the continuous random variables via differentiable transformation or using control variates. Instantiations of this idea, the Deep Recurrent Attentive Writer (DRAW) (Gregor et al., 2015) and the semi-supervised generative learning framework (Kingma et al., 2014), have demonstrated strong performance on image generation and classification respectively.\nAnother class of neural generative models make use of the autoregressive assumption to model highdimensional input distributions (Larochelle & Murray, 2011; Uria et al., 2014; Germain et al., 2015). Deep AutoRegressive Networks (DARN) (Gregor et al., 2013) integrate this idea with variational inference. Applications of these models on document modelling achieve significant improvements on generating documents, compared to conventional probabilistic topic models (Hofmann, 1999; Blei et al., 2003; Teh et al., 2006) and also the RBMs (Hinton & Salakhutdinov, 2009; Srivastava et al., 2013). Compared to these models that use binary semantic vectors, our NVDM employs dense continuous document representations which are both expressive and easy to train. The semantic word vector model (Maas et al., 2011) also employs a continuous semantic vector to generate words, but the model is trained by MAP inference which does not permit the calculation of the posterior distribution.\nPrior work on question answering relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Only very recently have researchers started to apply deep learning to question answering. Relevant work includes mapping factoid questions with answer triples in the knowledge base by projecting them into a shared vector space using convolutional neural networks (Bordes et al., 2014b;a; Yih et al., 2014). Recently, the attentionbased learning models (Bahdanau et al., 2014) are applied to QA, where long-term memories act as dynamic knowledge bases (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015) or the attentive network helps read and comprehend (Hermann et al., 2015).\nOur NASM effectively combines the strengths of LSTMs with attention mechanism and stochastic units, which equips our model with stronger capability to infer the correctness of an answer to a given question. Stochastic Feedforward Neural Networks (SFNN) (Tang & Salakhutdinov, 2013) applied similar idea of combining hybrid deterministic and stochastic units on expression classification. However, the inference is carried out by Monte Carlo EM algorithm with the reliance on importance sampling, which is less efficient and lack of scalability."}, {"heading": "7 CONCLUSION", "text": "This paper introduces a deep neural variational inference framework for generative models of text. To demonstrate the effectiveness of this framework, we experimented on two diverse tasks, document modelling and question answer selection tasks, where in both cases our models achieve state of the art performance. Apart from the promising results, our model also has the advantages of (1) simple, expressive, and efficient when training with the SGVB algorithm; (2) suitable for both unsupervised and supervised learning tasks; (3) capable of generalising to incorporate any type of neural network."}, {"heading": "A T-SNE VISUALISATION OF DOCUMENT REPRESENTATIONS", "text": ""}, {"heading": "B DETAILS OF THE DEEP NEURAL NETWORK STRUCTURES", "text": "B.1 NEURAL VARIATIONAL DOCUMENT MODEL\n(1) Inference Network q\u03c6(h|X):\n\u03bb = ReLU(W 1X + b1) (21) \u03c0 = ReLU(W 2\u03bb + b2) (22)\n\u00b5 =W 3\u03c0 + b3 (23) log\u03c3 =W 4\u03c0 + b4 (24)\nh \u223c N (\u00b5(X),diag(\u03c32(X))) (25)\n(2) Generative Model p\u03b8(X|h):\nei = exp(\u2212hTRxi + bxi) (26) p\u03b8(xi|h) = ei\u2211|V |\nj ej (27) p\u03b8(X|h) = \u220fN i p\u03b8(xi|h) (28)\n(3) KL Divergence DKL[q\u03c6(h|X)||p(h)]:\nDKL = 1 2 (K \u2212 \u2016\u00b5\u2016 2 \u2212 \u2016\u03c3\u20162 + log |diag(\u03c32)|) (29)\nThe variational lowerbound to be optimised:\nL = Eq\u03c6(h|X) [ N\u2211 i=1 log p\u03b8(xi|h) ] \u2212DKL[q\u03c6(h|X)||p(h)] (30)\n\u2248 L\u2211 l=1 N\u2211 i=1 log p\u03b8(xi|h(l))\u2212 1 2 (K \u2212 \u2016\u00b5\u20162 \u2212 \u2016\u03c3\u20162 + log |diag(\u03c32)|) (31)\nB.2 NEURAL ANSWER SELECTION MODEL\n(1) Inference Network q\u03c6(h|q,a,y):\nsq(|q|) = f LSTMq (q) (32) sa(|a|) = f LSTMa (a) (33) sy =W 5y + b5 (34)\n\u03b3 = sq(|q|)||sa(|a|)||sy (35) \u03bb\u03c6 = tanh(W 6\u03b3 + b6) (36) \u03c0\u03c6 = tanh(W 7\u03bb\u03c6 + b7) (37) \u00b5\u03c6 =W 8\u03c0\u03c6 + b8 (38)\nlog\u03c3\u03c6 =W 9\u03c0\u03c6 + b9 (39) h \u223c N (\u00b5\u03c6(q,a,y),diag(\u03c32\u03c6(q,a,y))) (40)\n(2) Generative Model\np\u03b8(h|q):\n\u03bb\u03b8 = tanh(W 1sq(|q|) + b1) (41) \u03c0\u03b8 = tanh(W 2\u03bb\u03b8 + b2) (42) \u00b5\u03b8 =W 3\u03c0\u03b8 + b3 (43)\nlog\u03c3\u03b8 =W 4\u03c0\u03b8 + b4 (44)\np\u03b8(y|q,a,h):\ne(i) =W T\u03b1 tanh(W hh+W ssa(i)) (45)\n\u03b1(i) = e(i)\u2211 j e(j)\n(46) c(a,h) = \u2211 i sa(i)\u03b1(i) (47)\nza(a,h) = tanh(W ac(a,h) +W nsa(|a|)) (48) zq(q) = sq(|q|) (49)\np\u03b8(y = 1|q,a,h) = \u03c3(zTqMza + b) (50)\n(3) KL Divergence DKL[q\u03c6(h|q,a,y)||p\u03b8(h|q)]:\nDKL = 1\n2 (K + log |diag(\u03c32\u03c6)| \u2212 log |diag(\u03c32\u03b8)| \u2212 Tr(diag(\u03c32\u03c6) diag \u22121(\u03c32\u03b8)) \u2212(\u00b5\u03c6 \u2212 \u00b5\u03b8)T diag \u22121(\u03c32\u03b8)(\u00b5\u03c6 \u2212 \u00b5\u03b8)) (51)\nThe variational lowerbound to be optimised:\nL = Eq\u03c6(h|q,a,y)[log p\u03b8(y|q,a,h)]\u2212DKL[q\u03c6(h|q,a,y)||p\u03b8(h|q)] (52)\n\u2248 L\u2211 l=1 y log \u03c3(zTqMz (l) a + b) + (1\u2212 y) log(1\u2212 \u03c3(zTqMz(l)a + b))\n\u22121 2 (K + log |diag(\u03c32\u03c6)| \u2212 log |diag(\u03c32\u03b8)| \u2212 tr(diag(\u03c32\u03c6) diag \u22121(\u03c32\u03b8)) \u2212(\u00b5\u03c6 \u2212 \u00b5\u03b8)T diag \u22121(\u03c32\u03b8)(\u00b5\u03c6 \u2212 \u00b5\u03b8)) (53)"}], "references": [{"title": "An introduction to mcmc for machine learning", "author": ["Andrieu", "Christophe", "De Freitas", "Nando", "Doucet", "Arnaud", "Jordan", "Michael I"], "venue": "Machine learning,", "citeRegEx": "Andrieu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2003}, {"title": "A variational bayesian framework for graphical models", "author": ["Attias", "Hagai"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Attias and Hagai.,? \\Q2000\\E", "shortCiteRegEx": "Attias and Hagai.", "year": 2000}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Variational algorithms for approximate Bayesian inference", "author": ["Beal", "Matthew James"], "venue": "University of London,", "citeRegEx": "Beal and James.,? \\Q2003\\E", "shortCiteRegEx": "Beal and James.", "year": 2003}, {"title": "Question answering with subgraph embeddings", "author": ["Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Open question answering with weakly supervised embedding models", "author": ["Bordes", "Antoine", "Weston", "Jason", "Usunier", "Nicolas"], "venue": "Proceedings of ECML,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Varieties of helmholtz machine", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E"], "venue": "Neural Networks,", "citeRegEx": "Dayan et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1996}, {"title": "Made: Masked autoencoder for distribution estimation", "author": ["Germain", "Mathieu", "Gregor", "Karol", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "arXiv preprint arXiv:1502.03509,", "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "Deep autoregressive networks", "author": ["Gregor", "Karol", "Mnih", "Andriy", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1310.8499,", "citeRegEx": "Gregor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisk\u00fd", "Tom\u00e1s", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Replicated softmax: an undirected topic model", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan"], "venue": "Proceedings of NIPS,", "citeRegEx": "Hinton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2009}, {"title": "Autoencoders, minimum description length, and helmholtz free energy", "author": ["Hinton", "Geoffrey E", "Zemel", "Richard S"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Hinton et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1994}, {"title": "wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M. The"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Probabilistic latent semantic indexing", "author": ["Hofmann", "Thomas"], "venue": "In Proceedings ACM SIGIR,", "citeRegEx": "Hofmann and Thomas.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann and Thomas.", "year": 1999}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "Proceedings of NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Su", "Jonathan", "Bradbury", "James", "English", "Robert", "Pierce", "Brian", "Ondruska", "Peter", "Gulrajani", "Ishaan", "Socher", "Richard"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "A neural autoregressive topic model", "author": ["Larochelle", "Hugo", "Lauly", "Stanislas"], "venue": "Proceedings of NIPS,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "Proceedings of ICML 2014,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas", "Andrew L", "Daly", "Raymond E", "Pham", "Peter T", "Huang", "Dan", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "Proceedings of ACL,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Gregory S", "Dean", "Jeffrey"], "venue": "Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Probabilistic inference using markov chain monte carlo methods", "author": ["Neal", "Radford M"], "venue": null, "citeRegEx": "Neal and M.,? \\Q1993\\E", "shortCiteRegEx": "Neal and M.", "year": 1993}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "Proceedings of ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Disi-university of trento modelling input texts: from tree kernels to deep learning", "author": ["Severyn", "Aliaksei"], "venue": null, "citeRegEx": "Severyn and Aliaksei.,? \\Q2015\\E", "shortCiteRegEx": "Severyn and Aliaksei.", "year": 2015}, {"title": "Modeling Documents with Deep Boltzmann Machines", "author": ["Srivastava", "Nitish", "RR Salakhutdinov", "Hinton", "Geoffrey"], "venue": "Proceedings of UAI,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Learning stochastic feedforward neural networks", "author": ["Tang", "Yichuan", "Salakhutdinov", "Ruslan R"], "venue": "Proceedings NIPS,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "Hierarchical dirichlet processes", "author": ["Teh", "Yee Whye", "Jordan", "Michael I", "Beal", "Matthew J", "Blei", "David M"], "venue": "Journal of the American Statistical Asociation,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "A deep and tractable density estimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": null, "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "What is the jeopardy model? a quasisynchronous grammar for qa", "author": ["Wang", "Mengqiu", "Smith", "Noah A", "Mitamura", "Teruko"], "venue": "Proceedings of EMNLP-CoNLL,", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Faq-based question answering via word alignment", "author": ["Wang", "Zhiguo", "Ittycheriah", "Abraham"], "venue": "arXiv preprint arXiv:1507.02628,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yang", "Yi", "Yih", "Wen-tau", "Meek", "Christopher"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih", "Wen-tau", "Chang", "Ming-Wei", "Meek", "Christopher", "Pastusiak", "Andrzej"], "venue": "Proceedings of ACL,", "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih", "Wen-tau", "He", "Xiaodong", "Meek", "Christopher"], "venue": "Proceedings of ACL,", "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Deep Learning for Answer Sentence Selection", "author": ["Yu", "Lei", "Hermann", "Karl Moritz", "Blunsom", "Phil", "Pulman", "Stephen"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Markov chain Monte Carlo (MCMC) (Neal, 1993; Andrieu et al., 2003) and variational inference (Jordan et al.", "startOffset": 32, "endOffset": 66}, {"referenceID": 16, "context": ", 2003) and variational inference (Jordan et al., 1999; Attias, 2000; Beal, 2003) are the standard approaches for approximating these integrals.", "startOffset": 34, "endOffset": 81}, {"referenceID": 28, "context": "This paper introduces a neural variational framework for generative models of text, inspired by the variational auto-encoder (Rezende et al., 2014; Kingma & Welling, 2013).", "startOffset": 125, "endOffset": 171}, {"referenceID": 28, "context": "By using the reparameterisation method (Rezende et al., 2014; Kingma & Welling, 2013), the inference network is trained through back-propagating unbiased and low variance gradients w.", "startOffset": 39, "endOffset": 85}, {"referenceID": 30, "context": "A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al., 2013; Mnih & Gregor, 2014).", "startOffset": 164, "endOffset": 266}, {"referenceID": 28, "context": "These models are simple, expressive and can be trained efficiently with the highly scalable stochastic gradient variational Bayes (SGVB) algorithm (Rezende et al., 2014; Kingma & Welling, 2013).", "startOffset": 147, "endOffset": 193}, {"referenceID": 28, "context": "\u03c6 we reparameterise h = \u03bc + \u03c3 \u00b7 and sample (l) \u223c N (0, I) (Rezende et al., 2014; Kingma & Welling, 2013).", "startOffset": 58, "endOffset": 104}, {"referenceID": 30, "context": "In order to make a direct comparison with the prior work we follow the same setup as Hinton & Salakhutdinov (2009), Larochelle & Lauly (2012), Srivastava et al. (2013), and Mnih & Gregor (2014).", "startOffset": 143, "endOffset": 168}, {"referenceID": 30, "context": "In order to make a direct comparison with the prior work we follow the same setup as Hinton & Salakhutdinov (2009), Larochelle & Lauly (2012), Srivastava et al. (2013), and Mnih & Gregor (2014). We train NVDM models with 50 and 200 dimensional document representations respectively.", "startOffset": 143, "endOffset": 194}, {"referenceID": 36, "context": "QASent (Wang et al., 2007) was created from the TREC QA track, and the WikiQA (Yang et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 38, "context": ", 2007) was created from the TREC QA track, and the WikiQA (Yang et al., 2015) is constructed from Wikipedia.", "startOffset": 59, "endOffset": 78}, {"referenceID": 25, "context": "The word embeddings (K = 50) are obtained by running the word2vec tool (Mikolov et al., 2013) on the English Wikipedia dump and the AQUAINT5 corpus.", "startOffset": 71, "endOffset": 93}, {"referenceID": 35, "context": "QASent (Wang et al., 2007) was created from the TREC QA track, and the WikiQA (Yang et al., 2015) is constructed from Wikipedia. Due to the different approaches for constructing the datasets4, the WikiQA dataset is less noisy and less biased towards lexical overlap. Table 3 summarises the statistics of the two datasets. In order to investigate the effectiveness of our NASM model we also implemented two strong baseline models \u2014 a vanilla LSTM model (LSTM) and an LSTM model with a deterministic attention mechanism (LSTM+Att). For the former, the last state outputs from the question and answer LSTM models are passed directly to the QA matching function (Eq.16). For the latter, we add an attention model that uses the last state output of the question LSTM as the question semantics to extract the context vector (Eq.14). LSTM+Att is the deterministic counterpart of NASM. Following previous work, for each of our models we also add a lexical overlap feature by combining a co-occurrence word count feature with the probability generated from the neural model. We adopt MAP and MRR as the evaluation metrics for this task. To facilitate direct comparison with previous work we follow the same experimental setup as Yu et al. (2014) and Severyn (2015).", "startOffset": 8, "endOffset": 1237}, {"referenceID": 35, "context": "QASent (Wang et al., 2007) was created from the TREC QA track, and the WikiQA (Yang et al., 2015) is constructed from Wikipedia. Due to the different approaches for constructing the datasets4, the WikiQA dataset is less noisy and less biased towards lexical overlap. Table 3 summarises the statistics of the two datasets. In order to investigate the effectiveness of our NASM model we also implemented two strong baseline models \u2014 a vanilla LSTM model (LSTM) and an LSTM model with a deterministic attention mechanism (LSTM+Att). For the former, the last state outputs from the question and answer LSTM models are passed directly to the QA matching function (Eq.16). For the latter, we add an attention model that uses the last state output of the question LSTM as the question semantics to extract the context vector (Eq.14). LSTM+Att is the deterministic counterpart of NASM. Following previous work, for each of our models we also add a lexical overlap feature by combining a co-occurrence word count feature with the probability generated from the neural model. We adopt MAP and MRR as the evaluation metrics for this task. To facilitate direct comparison with previous work we follow the same experimental setup as Yu et al. (2014) and Severyn (2015). The word embeddings (K = 50) are obtained by running the word2vec tool (Mikolov et al.", "startOffset": 8, "endOffset": 1256}, {"referenceID": 39, "context": "edu/LDC2002T31 As stated in (Yih et al., 2013) that the evaluation scripts used by previous work are noisy \u2014 4 out of 72 questions in the test set are treated answered incorrectly.", "startOffset": 28, "endOffset": 46}, {"referenceID": 38, "context": "The LSTM+Att performs Yang et al. (2015) provide detailed explanation of the differences between the two datasets.", "startOffset": 22, "endOffset": 41}, {"referenceID": 38, "context": "The LSTM+Att performs Yang et al. (2015) provide detailed explanation of the differences between the two datasets. https://catalog.ldc.upenn.edu/LDC2002T31 As stated in (Yih et al., 2013) that the evaluation scripts used by previous work are noisy \u2014 4 out of 72 questions in the test set are treated answered incorrectly. This makes the MAP and MRR scores \u223c 4% lower than the true scores. Severyn (2015) and Wang & Ittycheriah (2015), however, use a cleaned-up version of", "startOffset": 22, "endOffset": 404}, {"referenceID": 38, "context": "The LSTM+Att performs Yang et al. (2015) provide detailed explanation of the differences between the two datasets. https://catalog.ldc.upenn.edu/LDC2002T31 As stated in (Yih et al., 2013) that the evaluation scripts used by previous work are noisy \u2014 4 out of 72 questions in the test set are treated answered incorrectly. This makes the MAP and MRR scores \u223c 4% lower than the true scores. Severyn (2015) and Wang & Ittycheriah (2015), however, use a cleaned-up version of", "startOffset": 22, "endOffset": 434}, {"referenceID": 41, "context": "Bigram-CNN is the simple convolutional model reported in (Yu et al., 2014).", "startOffset": 57, "endOffset": 74}, {"referenceID": 13, "context": "Training an inference network to approximate the variational distribution was first proposed in the context of Helmholtz machines (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996), but applications of these directed generative models come up against the problem of establishing low variance gradient estimators.", "startOffset": 130, "endOffset": 195}, {"referenceID": 28, "context": "Recent advances in neural variational inference (Rezende et al., 2014; Kingma & Welling, 2013; Mnih & Gregor, 2014) mitigate this problem by reparameterising the continuous random variables via differentiable transformation or using control variates.", "startOffset": 48, "endOffset": 115}, {"referenceID": 9, "context": "Instantiations of this idea, the Deep Recurrent Attentive Writer (DRAW) (Gregor et al., 2015) and the semi-supervised generative learning framework (Kingma et al.", "startOffset": 72, "endOffset": 93}, {"referenceID": 19, "context": ", 2015) and the semi-supervised generative learning framework (Kingma et al., 2014), have demonstrated strong performance on image generation and classification respectively.", "startOffset": 62, "endOffset": 83}, {"referenceID": 34, "context": "Another class of neural generative models make use of the autoregressive assumption to model highdimensional input distributions (Larochelle & Murray, 2011; Uria et al., 2014; Germain et al., 2015).", "startOffset": 129, "endOffset": 197}, {"referenceID": 7, "context": "Another class of neural generative models make use of the autoregressive assumption to model highdimensional input distributions (Larochelle & Murray, 2011; Uria et al., 2014; Germain et al., 2015).", "startOffset": 129, "endOffset": 197}, {"referenceID": 8, "context": "Deep AutoRegressive Networks (DARN) (Gregor et al., 2013) integrate this idea with variational inference.", "startOffset": 36, "endOffset": 57}, {"referenceID": 33, "context": "Applications of these models on document modelling achieve significant improvements on generating documents, compared to conventional probabilistic topic models (Hofmann, 1999; Blei et al., 2003; Teh et al., 2006) and also the RBMs (Hinton & Salakhutdinov, 2009; Srivastava et al.", "startOffset": 161, "endOffset": 213}, {"referenceID": 30, "context": ", 2006) and also the RBMs (Hinton & Salakhutdinov, 2009; Srivastava et al., 2013).", "startOffset": 26, "endOffset": 81}, {"referenceID": 24, "context": "The semantic word vector model (Maas et al., 2011) also employs a continuous semantic vector to generate words, but the model is trained by MAP inference which does not permit the calculation of the posterior distribution.", "startOffset": 31, "endOffset": 50}, {"referenceID": 40, "context": "Relevant work includes mapping factoid questions with answer triples in the knowledge base by projecting them into a shared vector space using convolutional neural networks (Bordes et al., 2014b;a; Yih et al., 2014).", "startOffset": 173, "endOffset": 215}, {"referenceID": 2, "context": "Recently, the attentionbased learning models (Bahdanau et al., 2014) are applied to QA, where long-term memories act as dynamic knowledge bases (Weston et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 31, "context": ", 2014) are applied to QA, where long-term memories act as dynamic knowledge bases (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015) or the attentive network helps read and comprehend (Hermann et al.", "startOffset": 83, "endOffset": 149}, {"referenceID": 20, "context": ", 2014) are applied to QA, where long-term memories act as dynamic knowledge bases (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015) or the attentive network helps read and comprehend (Hermann et al.", "startOffset": 83, "endOffset": 149}, {"referenceID": 10, "context": ", 2015) or the attentive network helps read and comprehend (Hermann et al., 2015).", "startOffset": 59, "endOffset": 81}], "year": 2017, "abstractText": "Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.", "creator": "LaTeX with hyperref package"}}}