{"id": "1511.06085", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Variable Rate Image Compression with Recurrent Neural Networks", "abstract": "Although create sensors has been actively studies for decades, there with been robust much research on learning to defragment images with \u2014 insertion networks. Standard changing, often as much teachers jungle - provides autoencoders, have appear which tradition future to idea have explain manufacturers rest dubbed screen macros reasons enough fail to washington also questions: 21) good to removing binarize 2,611: between a absence similar binarization, a congestions stored have whereas cannot to lead to convenient regenerative; 2) how though achieve maximum - raise non-linear: gives comparable autoencoder equivalent a dual - length text for each essentially - adopted measurement patch, resulting under the way cost even fast - although taking - formula_30 patches, and mandated the network decided actually completely retrained before objectives different compression rates; as 43) obviously taken reasons block specimens: gray - corporation understanding still unnecessarily give block discontinuities. We propose on executive stability some hence - 4.5 aspect compression both a novel ceramics has on convolutional and deconvolutional urinary networks, others LSTMs, been asking are priorities and suggesting pursue final than over require baseline codecs. We respond way proposed measurement with set large - scale benchmark divided of tiny invisible (92 $ \\ gone $ 32 ), which proves either be very fact for all second analysis.", "histories": [["v1", "Thu, 19 Nov 2015 07:50:46 GMT  (121kb,D)", "http://arxiv.org/abs/1511.06085v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Sat, 21 Nov 2015 01:44:51 GMT  (121kb,D)", "http://arxiv.org/abs/1511.06085v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Tue, 12 Jan 2016 02:43:40 GMT  (290kb,D)", "http://arxiv.org/abs/1511.06085v3", "Under review as a conference paper at ICLR 2016"], ["v4", "Wed, 13 Jan 2016 20:57:42 GMT  (233kb,D)", "http://arxiv.org/abs/1511.06085v4", "Under review as a conference paper at ICLR 2016"], ["v5", "Tue, 1 Mar 2016 22:13:44 GMT  (5796kb,D)", "http://arxiv.org/abs/1511.06085v5", "Under review as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["george toderici", "sean m o'malley", "sung jin hwang", "damien vincent", "david minnen", "shumeet baluja", "michele covell", "rahul sukthankar"], "accepted": true, "id": "1511.06085"}, "pdf": {"name": "1511.06085.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["George Toderici", "Sean M. O\u2019Malley", "Sung Jin Hwang", "Damien Vincent", "David Minnen", "Shumeet Baluja", "Michele Covell", "Rahul Sukthankar"], "emails": ["damienv}@google.com", "sukthankar}@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The task of image compression has been thoroughly examined over the years by researchers and teams such as the Joint Pictures Experts Group, who designed the ubiquitous JPEG and JPEG 2000 (ISO/IEC 15444-1) image formats. More recently, the WebP algorithm was proposed in order to further improve image compression rates (Google, 2015). All these efforts approach the compression problem from an empirical standpoint: human experts designed various heuristics to reduce the amount of information needing to be retained, then determined ways to transform the resulting data in a way that\u2019s amenable to lossless compression.\nIn recent years, neural networks have become commonplace to perform tasks that had for decades been accomplished by ad hoc algorithms and heuristics. For instance, in image recognition and object detection, the current state-of-the-art algorithms are all based on neural networks. It is only natural to ask if we can also employ this powerful class of methods to further improve the task of image compression.\nThere is a popular class of neural networks known as autoencoders which show promise in the area of image compression, but they operate under a number of hard constraints that have so far made them infeasible as a drop-in replacement for standard image codecs. Some of these constraints are that variable-rate encoding is typically not possible, the visual quality of the output is hard to ensure, and the resulting compressed image representation isn\u2019t guaranteed to be an efficient use of the bits allocated.\nar X\niv :1\n51 1.\n06 08\n5v 1\n[ cs\n.C V\n] 1\n9 N\nov 2\nWe explore several different ways in which neural network-driven image compression can improve compression rates while allowing the same degree of flexibility as modern codecs. To achieve this flexibility, the network architectures we discuss must meet all of the following requirements: (1) the compression rate should be capable of being restricted to a prior bit budget; (2) the compressor should also be able to meet a bit budget by encoding simpler patches more cheaply (analogously to modern codecs which may allocate more bits to areas of the image which contain strong gradients or other important visual features); and (3) the model should be able to learn from large amounts of existing imagery in order to optimize this compression process toward real-world data."}, {"heading": "2 RELATED WORK", "text": "The basic principles of using feed-forward neural networks for image compression have been known for some time (Jiang, 1999). In this context, networks can assist or even entirely take over many of the processes used as part of a traditional image compression pipeline: to learn more efficient frequency transforms, more effective quantization techniques, improved predictive coding, etc.\nMore recently, autoencoder architectures (Hinton & Salakhutdinov, 2006) have become viable as a means of implementing end-to-end compression. A typical compressing autoencoder has three parts: (1) an encoder which consumes an input (e.g., a fixed-dimension image or patch) and transforms it into (2) a bottleneck representing the compressed data, which can then be transformed by (3) a decoder into something resembling the original input. These three elements are trained end-to-end, but during deployment the encoder and decoder are normally used independently.\nThe bottleneck is often simply a flat neural net layer, which allows the compression rate and visual fidelity of the encoded images to be controlled by adjusting the number of nodes in this layer before training. For some types of autoencoder, encoding the bottleneck as a simple bit vector can be beneficial (Krizhevsky & Hinton, 2011). In neural net-based classification tasks, images are repeatedly downsampled through convolution and pooling operations, and the entire output of the network might be contained in just a single node. In the decoder half of an autoencoder, however, the network must proceed in the opposite direction and convert a short bit vector into a much larger image or image patch. When this upsampling process is spatially-aware, resembling a \u201cbackward convolution\u201d, it is commonly referred to as deconvolution (Long et al., 2014).\nLong short-term memory (LSTM) networks are a type of recurrent neural network (Hochreiter & Schmidhuber, 1997) that have proven very successful for tasks such as speech recognition (Graves et al., 2013) and machine translation (Sutskever et al., 2014). Many extensions to the standard LSTM model are possible including explicitly incorporating spatial information, which leads to various types of convolutional LSTMs (Shi et al., 2015) that may be better suited for image compression. We experiment with such models and also try simpler recurrent architectures that use the residual error of one autoencoder as the input to another."}, {"heading": "3 VARIABLE RATE COMPRESSION ARCHITECTURES", "text": "We start by describing a general neural network-based compression framework and then discuss the details of multiple instantiations of this architecture. Each subsection describes a different architecture that builds on the previous model and improves the compression results.\nFor each architecture, we will discuss a function E that takes an image patch as input and produces an encoded representation. This representation is then processed by a binarization function B, which is the same across architectures, and is discussed in Section 3.2. Finally, for each architecture we also consider a decoder function D, which takes the binary representation produced by B and generates a reconstructed output patch. Taken together, these three components form an autoencoder, x\u2032 = D(B(E(x))), which is the basic building block for all of the compression networks.\nIn all experiments, the input images or patches are scaled to be in range [\u22120.9, 0.9]."}, {"heading": "3.1 IMAGE COMPRESSION FRAMEWORK", "text": "We would like to handle two classes of architectures: one that is suitable for very small patches (8\u00d78) and one which is suitable for larger patches. Both architectures share the same design principles and\nmust define an encoder network, a quantizer, and a decoder network. In addition, the framework should be tuned for image compression and must support variable compression rates without the need for retraining or storing multiple encodings of the same image.\nTo make it possible to transmit incremental information, the design should take into account the fact that image decoding will be progressive. With this design goal in mind, we can consider architectures that are built on top of residuals with the goal of minimizing the residual error in the reconstruction as additional information becomes available to the decoder.\nFormally, we chain multiple copies of a residual autoencoder, Ft, defined as:\nFt(rt\u22121) = Dt(B(Et(rt\u22121))) (1)\nIn all cases, we set r0 to be equal to the original input patch, and then rt for t > 0 represents the residual error after t stages. For non-LSTM architectures (described in Sections 3.3 and 3.5), Ft has no memory, and so we only expect it to predict the residual itself. In this case, the full reconstruction is recovered by summing over all of the residuals, and each stage is penalized due to the difference between the prediction and the previous residual:\nrt = Ft(rt\u22121)\u2212 rt\u22121 (2)\nOn the other hand, LSTM-based architectures (described in Sections 3.4 and 3.6) do hold state, and so we expect them to predict the original image patch in each stage. Accordingly, we compute the residual relative to the original patch:\nrt = Ft(rt\u22121)\u2212 r0 (3)\nIn both cases, the full, multi-stage network is trained by minimizing \u2016rt\u201622 for t = 1 . . . N , where N is the total number of residual autoencoders in the model."}, {"heading": "3.2 BINARY REPRESENTATION", "text": "In our networks, we employ a binarization technique first proposed by Williams (1992), and similar to Krizhevsky & Hinton (2011) and Courbariaux et al. (2015). This binarization has three benefits: (1) bit vectors are trivially serializable/deserializable for image transmission over the wire, (2) control of the network compression rate is achieved simply by putting constraints on the bit allowance, and (3) a binary bottleneck helps force the network to learn efficient representations compared to standard floating-point layers, which may have many redundant bit patterns that have no effect on the output.\nThe binarization process consists of two parts. The first part consists of generating the required number of outputs (equal to the desired number of output bits) in the continuous interval [\u22121, 1]. The second part involves taking this real-valued representation as input and producing a discrete output in the set {\u22121, 1} for each value. For the first step in the binarization process, we use a fully-connected layer with tanh activations. For the second part, following Raiko et al. (2015), one possible binarization b(x) of x \u2208 [\u22121, 1] is defined as:\nb(x) = x+ \u2208 {\u22121, 1}, (4) \u223c { 1\u2212 x with probability 1+x2 , \u2212x\u2212 1 with probability 1\u2212x2 , (5)\nwhere corresponds to quantization noise.\nTherefore, the full binary encoder function is: B (x) = b ( tanh(W binx+ bbin) ) (6)\nwhere W bin and bbin are the standard linear weights and bias that transform the activations from the previous layer in the network. In all of our models, we use the above formulation for the forward\npass. For the backward pass of back-propagation, we take the derivative of the expectation (Raiko et al., 2015). Since E[b(x)] = x for all x \u2208 [\u22121, 1], we pass the gradients through b unchanged. In order to have a fixed representation for a particular input, once the networks are trained, only the most likely outcome of b(x) is considered and b can be replaced by binf defined as:\nbinf (x) = { \u22121 if x < 0, +1 otherwise.\n(7)\nThe compression rate is determined by the number of bits generated in each stage, which corresponds to the number of rows in the W bin matrix, and by the number of stages, controlled by the number of repetitions of the residual autoencoder structure."}, {"heading": "3.3 FEED-FORWARD FULLY-CONNECTED RESIDUAL ENCODER", "text": "In the simplest instantiation of our variable rate compression architecture, we set E and D to be composed of stacked fully-connected layers. In order to make the search for architectures more feasible we decided to set the number of outputs in each fully-connected layer to be constant (512) and only used the tanh nonlinearity.\nGiven that E and D can be functions of the encoding stage number, and since the statistics of the residuals change when going from stage t to t+ 1 we considered two distinct approaches: in the first we share weights across all stages, while in the second, we learn the weights independently in each stage. The details of this architecture are given in Figure 1."}, {"heading": "3.4 LSTM-BASED COMPRESSION", "text": "In this architecture, we explore the use of LSTM models for both the encoder and the decoder. In particular, both E and D consist of stacked LSTM layers.\nFollowing the LSTM formulation and notation proposed by Zaremba et al. (2014), we use superscripts to indicate the layer number, and subscripts to indicate time steps. Let hlt \u2208 Rn denote the hidden state of l-th LSTM layer at time step t. We define T ln : Rm \u2192 Rn to be an affine transform\nT ln(x) =W lx+ bl. Finally, let denote element-wise multiplication, and let h0t be the input to the first LSTM layer at time step t.\nUsing this notation, the LSTM architecture can be written succinctly as proposed by Graves (2013): ifo g  = sigmsigmsigm tanh T l4n(hl\u22121thlt\u22121 ) , (8)\nclt = f clt\u22121 + i g, (9) hlt = o tanh ( clt ) , (10)\nwhere sigm(x) = (1 + exp(\u2212x))\u22121 denotes the sigmoid function. In these equations, sigm and tanh are applied element-wise. This alternate formulation of LSTM is useful because it reduces the numbers of separate operations needed to evaluate one step, which allows for an efficient implementation on GPU.\nFor the encoder, we use one fully-connected layer followed by two stacked LSTM layers. The decoder has the opposite structure: two stacked LSTM layers followed by a fully-connected layer with a tanh nonlinearity that predicts RGB values (we omit this layer in the diagrams to reduce clutter). The input RGB image is applied an offset and a scale to meet a range of values between \u22120.9 and 0.9, which is compatible with the range of values that can be generated by tanh. The exact architecture used in the experiments is given in Figure 2 (minus the RGB conversion)."}, {"heading": "3.5 FEED-FORWARD CONVOLUTIONAL/DECONVOLUTIONAL RESIDUAL ENCODER", "text": "Section 3.3 proposed a fully-connected residual autoencoder. We extend this architecture by replacing the fully-connected layers with convolution operators in the encoder E and with deconvolutional operators in the decoder D. The final layer of the decoder consists of a 1\u00d71 convolution with three filters that converts the decoded representation into RGB values. We depict this architecture in Figure 3 (minus the RGB conversion).\nThe deconvolutional operator is defined as the transpose of the convolutional operator. Let \u2297k denote the convolutional operator with stride k, and let Sk denote the stride operator with stride factor k, i.e., Sk(x)(i, j) = x(k \u00d7 i, k \u00d7 j) for 2D multi-channel image x and pixel coordinate (i, j). Then W \u2297k x = Sk(W \u22971 x). Note that the transpose of Sk is the \u201cinflation\u201d operator Tk:\nTk(x)(i, j) = { x(i/k, j/k) if i, j are multiples of k, 0 otherwise.\n(11)\nThus we can define the deconvolutional operator k with stride k as follows:\nW k x =W \u22971 (Tk(x)). (12)\nTo simplify the description, we use the same stride factor in both dimensions, but the deconvolution operation generalizes trivially to different strides in each dimension."}, {"heading": "3.6 CONVOLUTIONAL/DECONVOLUTIONAL LSTM COMPRESSION", "text": "The final architecture combines the convolutional and deconvolutional operators with LSTM. We define convolutional LSTM by replacing the transformation T l4n in equation (8) with convolutions plus bias. Then the transformation function for convolutional LSTM with stride k is\nT l4n ( hl\u22121t , h l t\u22121 ) =W l1 \u2297k hl\u22121t +W l2 \u22971 hlt\u22121 + bl. (13)\nThe subscript belonging to T now refers to the depth (number of channels) in the output feature maps. Note that the second convolution term represents the recurrent relation of convolutional LSTM so both its input and output must have the same size. Therefore, when a convolutional LSTM has a stride greater than one, the stride is only applied to the first convolution term, while the second term is always computed with a stride of one. Finally, to build the encoder for this architecture, we replace the second and third convolutional layers from Figure 3 with convolutional LSTM layers.\nFor the decoder, we cannot replace all convolutional operations with deconvolution due to the fact that the input to deconvolution often has a different spatial dimension than the output. For the purposes of defining a deconvolutional LSTM, T4n becomes\nT l4n ( hl\u22121t , h l t\u22121 ) =W ld k hl\u22121t +W lc \u22971 hlt\u22121 + bl (14)\nHere we use the subscripts c and d to differentiate between the weights associated to the convolution and deconvolution operations. To construct the deconvolutional LSTM decoder, we replace the second and third deconvolutional layers of the deconvolutional decoder from Figure 3 with deconvolutional LSTM."}, {"heading": "3.7 DYNAMIC BIT ASSIGNMENT", "text": "For the non-convolutional approaches presented here, it is natural to assign a varying number of bits per patch by allowing a varying number of iterations of the encoder. This could be determined by a target quality metric (e.g., PSNR). While not as natural, in the case of the convolutional approaches, a similar method may also be employed. The input image needs to be split into patches, and each patch processed independently, thereby allowing a different number of bits per region. However, this approach has disadvantages that will be discussed at the end of this paper."}, {"heading": "4 EXPERIMENTS & ANALYSIS", "text": ""}, {"heading": "4.1 TRAINING", "text": "In order to train the various neural network configurations, we used the Adam algorithm proposed by Kingma & Ba (2014). We experimented with learning rates of {0.1, 0.3, 0.5, 0.8, 1}. The L2 loss was normalized by the number of pixels in the patch and also by the number of total time steps, i.e., number of iterations unrolled, needed to fully encode the patch.\nWe experimented with the number of steps needed to encode each patch, varying this from 8 to 16. For the fully connected networks, we chose to use 8 bits per step for an 8\u00d78 patch, allowing us to fine tune the compression rate in increments of 8 bits. When scaled up to a 32\u00d732 patch size, this allowed us to control the compression in increments of 16 bytes.\nFor the convolutional/deconvolutional networks, the encoders reduce the 32\u00d7 32 input patch down to 8\u00d78 through convolution operations with strides. We experimented with a binary output of 2 bits per pixel at this resolution, yielding a tunable compression rate with increments of 16 bytes per 32\u00d732 block."}, {"heading": "4.2 EVALUATION PROTOCOL AND METRICS", "text": "Evaluating image compression algorithms is a non-trivial task. The metric commonly used in this context is the peak signal-to-noise ratio (PSNR), however, PSNR is biased toward algorithms which have been tuned to minimize L2 loss. This would not be a fair comparison against methods like JPEG which have been tuned to minimize some form of perceptual loss.\nIn our evaluation protocol we instead employ the Structural Similarity Index (SSIM), a popular perceptual similarity measure proposed by Wang et al. (2004). Since we\u2019re evaluating compression performance on small 32\u00d732 images, we don\u2019t do any smoothing of the images (a typical preprocess\nfor SSIM). In addition, since we\u2019re interested in quantifying how well local details are preserved, we split the images into 8\u00d78 patches and compute the SSIM on each patch and on each color channel independently. The final score is the average SSIM over all patches and channels.\nWhen analyzing the results, a higher score implies a better reconstruction, with a score of 1.0 representing a perfect reconstruction. The lowest possible score is 0.0. Note that while there are other metrics (e.g., Ponomarenko et al., 2007) which emulate the human visual system better than SSIM, we chose to use SSIM here due to its ubiquity and ease of comparison with previous work."}, {"heading": "4.3 32\u00d732 BENCHMARK", "text": "Our 32\u00d732 benchmark dataset contains 216M random color images collected from the public internet. To be included in the dataset, each image must originally have more than 32 pixels on both axes. Qualified images were then downsampled to 32\u00d732, losing their original aspect ratios. This downsampling eliminates pre-existing compression artifacts for most images. The final 32\u00d732 images were then stored losslessly (as PNG) before being used for training and testing. For training the LSTM models, 90% of the images were used; the remaining 10% were set aside for evaluation. For evaluating the image codecs, we use a subset of this data containing 100k random images.\nTable 1 summarizes the results on the 32\u00d732 benchmark, comparing our two LSTM approaches to two JPEG codecs. To avoid unfairly penalizing the JPEG codecs due to the unavoidable cost of their file headers, we exclude the header size from all metrics. Note also that since neither codec can be tuned to an exact byte budget (e.g., 64 bytes excluding the file header), we search for the encoder quality setting that leads to a file whose size is as close as possible, but never less than, the target size. On average, this leads to each JPEG image consuming slightly more space than we allow for the LSTM models."}, {"heading": "4.4 ANALYSIS", "text": "These 32 \u00d7 32 images contain a lot of details that are perceptually relevant. As can be seen in Figure 4, compressing these images without destroying salient visual information or hallucinating false details is particularly challenging. At these low bitrates and spatial resolution, JPEG\u2019s block artifacts become extremely prominent, and the color smearing artifacts due to the codec\u2019s default (4:2:0) chroma subsampling are clearly visible.\nThe non-convolutional LSTM model reduces these artifacts by reconstructing each subblock with greater fidelity, but its block-boundary artifacts are still noticeable. The convolutional model eliminates these artifacts entirely, preferring to err on the side of smoothness versus adding false gradients not present in the original image.\nNote that both LSTM models exhibit perceptual quality levels that are equal to or better than JPEG at 4% \u2013 12% lower average bitrate. We see this improvement despite the fact that, unlike JPEG, the LSTMs do not perform chroma subsampling as a preprocess. However, at the JPEG quality levels used in Figure 4, disabling subsampling (i.e., using 4:4:4 encoding) leads to a costly increase in JPEG\u2019s bitrate: 1.32-1.77 bpp instead of 1.05-1.406 bpp, or 26% greater. This means that if we desired to preserve chroma fidelity, we would need to drastically reduce JPEG encoding quality in order to produce 4:4:4 JPEGs at a comparable bitrate to the LSTM models. Doing so would of course make the LSTM methods appear stronger, but could be an unfair comparison as the majority of JPEG users appear to be content with the effects of 4:2:0 subsampling."}, {"heading": "5 CONCLUSION & FUTURE WORK", "text": "We describe various methods for variable-length encoding of image patches using neural networks, and demonstrate that for the given benchmark, the fully-connected LSTM model can perform on par with JPEG, while the convolutional/deconvolutional LSTM model is able to significantly outperform JPEG on the SSIM perceptual metric.\nWhile our current approach gives favorable results versus modern codecs on small images, codecs that include an entropy coder element tend to improve (in a bits per pixel sense) with greater resolution, meaning that by choosing an arbitrarily large test image it is always possible to defeat an approach like that described in this work. Therefore, an obvious need is to extend the current work to function on arbitrarily large images, taking advantage of spatial redundancy in images in a manner similar to entropy coding.\nAlthough we presented a solution for dynamic bit assignment in the convolutional case, it\u2019s not a fully satisfactory solution as it has the potential to introduce encoding artifacts at patch boundaries. Another topic for future work is determining a dynamic bit assignment algorithm that is compatible with the convolutional methods we present, while not creating such artifacts.\nThe algorithms that we present may also be extended to work on video, which we believe to be the next grand challenge for neural network-based compression."}], "references": [{"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": "In NIPS,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves,? \\Q2013\\E", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "In International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Image compression with neural networks\u2013a survey", "author": ["J. Jiang"], "venue": "Signal Processing: Image Communication,", "citeRegEx": "Jiang,? \\Q1999\\E", "shortCiteRegEx": "Jiang", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["A. Krizhevsky", "G.E. Hinton"], "venue": "In European Symposium on Artificial Neural Networks,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2011\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2011}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CoRR, abs/1411.4038,", "citeRegEx": "Long et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Long et al\\.", "year": 2014}, {"title": "On betweencoefficient contrast masking of DCT basis functions", "author": ["N. Ponomarenko", "F. Silvestri", "K. Egiazarian", "M. Carli", "J. Astola", "V. Lukin"], "venue": "In Proc. 3rd Int\u2019l. Workshop on Video Processing and Quality Metrics,", "citeRegEx": "Ponomarenko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ponomarenko et al\\.", "year": 2007}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["T. Raiko", "M. Berglund", "G. Alain", "L. Dinh"], "venue": null, "citeRegEx": "Raiko et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2015}, {"title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", "author": ["X. Shi", "Z. Chen", "H. Wang", "Yeung", "D.-Y", "Wong", "W.-K", "Woo", "W.-C"], "venue": "CoRR, abs/1506.04214,", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A. Bovik", "A. Conrad", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "The basic principles of using feed-forward neural networks for image compression have been known for some time (Jiang, 1999).", "startOffset": 111, "endOffset": 124}, {"referenceID": 8, "context": "When this upsampling process is spatially-aware, resembling a \u201cbackward convolution\u201d, it is commonly referred to as deconvolution (Long et al., 2014).", "startOffset": 130, "endOffset": 149}, {"referenceID": 2, "context": "Long short-term memory (LSTM) networks are a type of recurrent neural network (Hochreiter & Schmidhuber, 1997) that have proven very successful for tasks such as speech recognition (Graves et al., 2013) and machine translation (Sutskever et al.", "startOffset": 181, "endOffset": 202}, {"referenceID": 12, "context": ", 2013) and machine translation (Sutskever et al., 2014).", "startOffset": 32, "endOffset": 56}, {"referenceID": 11, "context": "Many extensions to the standard LSTM model are possible including explicitly incorporating spatial information, which leads to various types of convolutional LSTMs (Shi et al., 2015) that may be better suited for image compression.", "startOffset": 164, "endOffset": 182}, {"referenceID": 0, "context": "In our networks, we employ a binarization technique first proposed by Williams (1992), and similar to Krizhevsky & Hinton (2011) and Courbariaux et al. (2015). This binarization has three benefits: (1) bit vectors are trivially serializable/deserializable for image transmission over the wire, (2) control of the network compression rate is achieved simply by putting constraints on the bit allowance, and (3) a binary bottleneck helps force the network to learn efficient representations compared to standard floating-point layers, which may have many redundant bit patterns that have no effect on the output.", "startOffset": 133, "endOffset": 159}, {"referenceID": 0, "context": "In our networks, we employ a binarization technique first proposed by Williams (1992), and similar to Krizhevsky & Hinton (2011) and Courbariaux et al. (2015). This binarization has three benefits: (1) bit vectors are trivially serializable/deserializable for image transmission over the wire, (2) control of the network compression rate is achieved simply by putting constraints on the bit allowance, and (3) a binary bottleneck helps force the network to learn efficient representations compared to standard floating-point layers, which may have many redundant bit patterns that have no effect on the output. The binarization process consists of two parts. The first part consists of generating the required number of outputs (equal to the desired number of output bits) in the continuous interval [\u22121, 1]. The second part involves taking this real-valued representation as input and producing a discrete output in the set {\u22121, 1} for each value. For the first step in the binarization process, we use a fully-connected layer with tanh activations. For the second part, following Raiko et al. (2015), one possible binarization b(x) of x \u2208 [\u22121, 1] is defined as: b(x) = x+ \u2208 {\u22121, 1}, (4)", "startOffset": 133, "endOffset": 1102}, {"referenceID": 10, "context": "For the backward pass of back-propagation, we take the derivative of the expectation (Raiko et al., 2015).", "startOffset": 85, "endOffset": 105}, {"referenceID": 15, "context": "Following the LSTM formulation and notation proposed by Zaremba et al. (2014), we use superscripts to indicate the layer number, and subscripts to indicate time steps.", "startOffset": 56, "endOffset": 78}, {"referenceID": 1, "context": "Using this notation, the LSTM architecture can be written succinctly as proposed by Graves (2013): \uf8ec\uf8ed i fo g \uf8f7\uf8f8 = \uf8ec\uf8edsigm sigm sigm tanh \uf8f7\uf8f8T l 4n(hl\u22121 t ht\u22121 ) , (8)", "startOffset": 84, "endOffset": 98}, {"referenceID": 13, "context": "In our evaluation protocol we instead employ the Structural Similarity Index (SSIM), a popular perceptual similarity measure proposed by Wang et al. (2004). Since we\u2019re evaluating compression performance on small 32\u00d732 images, we don\u2019t do any smoothing of the images (a typical preprocess", "startOffset": 137, "endOffset": 156}], "year": 2015, "abstractText": "Although image compression has been actively studied for decades, there has been relatively little research on learning to compress images with modern neural networks. Standard approaches, such as those employing patch-based autoencoders, have shown a great deal of promise but cannot compete with popular image codecs because they fail to address three questions: 1) how to effectively binarize activations: in the absence of binarization, a bottleneck layer alone tends not to lead to efficient compression; 2) how to achieve variable-rate encoding: a standard autoencoder generates a fixed-length code for each fixed-resolution input patch, resulting in the same cost for lowand high-entropy patches, and requiring the network to be completely retrained to achieve different compression rates; and 3) how to avoid block artifacts: patch-based approaches are prone to block discontinuities. We propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional recurrent networks, including LSTMs, that address these issues and report promising results compared to existing baseline codecs. We evaluate the proposed methods on a large-scale benchmark consisting of tiny images (32\u00d7 32), which proves to be very challenging for all the methods.", "creator": "LaTeX with hyperref package"}}}