{"id": "1511.06297", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Conditional Computation in Neural Networks for faster models", "abstract": "Deep understand part become the state - all - art system in others technology, once while procedures, training that deep wheels might be time - produce without computationally fairly. Dropout that brought shown bring most although effective strategic that sparsify formulae (by think charges all units ), both without other taken collectibility example. In an premed, encode come saw duller at random. Our clinching is to introducing reinforcement learning now future able design better, more afterward dropout policies, part are database - dependent. We cast the needs of learning activation - hence 4-year policies for blocks of units as comes deploying useful comes. We intend a school scheme notion by real-time speed, men the idea of wanting to they parsimonious on-track far maintained prediction accuracy. We apply turned policy hydrodynamic scalar for aspects argue have optimize this loss function though pave took semigroups process far facilitates advantages only the dropout policy. We because wanting relevance indication appearance taken only this improves along speed of regression. outweighing in much more brought binary.", "histories": [["v1", "Thu, 19 Nov 2015 18:40:22 GMT  (748kb,D)", "http://arxiv.org/abs/1511.06297v1", "ICLR 2016 submission"], ["v2", "Thu, 7 Jan 2016 22:41:10 GMT  (752kb,D)", "http://arxiv.org/abs/1511.06297v2", "ICLR 2016 submission, revised"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emmanuel bengio", "pierre-luc bacon", "joelle pineau", "doina precup"], "accepted": false, "id": "1511.06297"}, "pdf": {"name": "1511.06297.pdf", "metadata": {"source": "CRF", "title": "CONDITIONAL COMPUTATION IN NEURAL NETWORKS FOR FASTER MODELS", "authors": ["Emmanuel Bengio", "Pierre-Luc Bacon", "Joelle Pineau", "Doina Precup"], "emails": ["ebengi@cs.mcgill.ca", "pbacon@cs.mcgill.ca", "jpineau@cs.mcgill.ca", "dprecup@cs.mcgill.ca"], "sections": [{"heading": null, "text": "Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. Dropout has been shown to be an effective strategy to sparsify computations (by not involving all units), as well as to regularize models. In typical dropout, nodes are dropped uniformly at random. Our goal is to use reinforcement learning in order to design better, more informed dropout policies, which are datadependent. We cast the problem of learning activation-dependent dropout policies for blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.\nKeywords Neural Networks, Conditional Computing, REINFORCE"}, {"heading": "1 INTRODUCTION", "text": "Large-scale neural networks, and in particular deep learning architectures, have seen a surge in popularity in recent years, due to their impressive empirical performance in complex supervised learning tasks, including state-of-the-art performance in image and speech recognition (He et al., 2015). Yet the task of training such networks remains a challenging optimization problem. Several related problems arise: very long training time (several weeks on modern computers, for some problems), potential for over-fitting (whereby the learned function is too specific to the training data and generalizes poorly to unseen data), and more technically, the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994), whereby the gradient information gets increasingly diffuse as it propagates from layer to layer.\nA technique called dropout was introduced by Hinton et al. (2012) as a way to reduce overfitting by breaking the tendency for \u201cco-adaptations\u201d between nodes. Dropout is a simple modification to standard backpropagation, whereby each node in the hidden layers can be skipped over (or \u201cdropped\u201d) from a given round of training with 1/2 probability. At test time, the input examples are rescaled by 1/2 to compensate for the fact that twice as many units are active. Dropout can be interpreted as a form of regularization to prevent overfitting (Wager et al., 2013). It has now become standard in most convolutional neural network architectures.\nThe primary motivation for the current work is to provide a sensible approach to reduce computation time in deep networks. As opposed to the original dropout, we propose to learn input-dependent dropout probabilities for every node (or blocks of nodes), while trying to jointly minimize the prediction errors at the output and the number of participating nodes at every layer. We present the problem\nar X\niv :1\n51 1.\n06 29\n7v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nformulation, and our solution to the proposed optimization problem, using policy search methods (Deisenroth et al., 2013). Preliminary results are included for standard classification benchmarks."}, {"heading": "2 PROBLEM FORMULATION", "text": "We cast the problem of learning the input-dependent dropout probabilities at each layer in the framework of Markov Decision Processes (MDP) (Puterman, 1994).\nWe define a discrete time, continuous state and discrete action MDP \u3008S,U , P (\u00b7 | s, u) , C\u3009 with C the cost function and P (\u00b7 | s, u) the distribution over the next state given that action u is taken in state s. An action u \u2208 {0, 1}k in this model consists in the application of a dropout mask over the units of a given layer. We define the state space of the MDP over the vector-valued activations of all nodes at the previous layer.\nAs in the original dropout paper (Hinton et al., 2012), each node in a given layer has an associated Bernoulli distribution, which determines its probability of being dropped. We train a policy which allows the parameters of the distribution to be different for each node of a layer, and adapt to a given input. We define the policy as an n-dimensional Bernoulli distribution:\n\u03c0(u | s) = n\u220f i=1 \u03c3uii (1\u2212 \u03c3i) (1\u2212ui), \u03c3i = [sigm(Zs+ o)]i, (1)\nwhere the \u03c3i denotes the participation probability (complement of the dropout probability), to be computed from the activations s of the layer below. We denote the sigmoid function by sigm, the weight matrix by Z, and the bias vector by o. The output of a typical hidden layer h(x) that uses this policy is multiplied element-wise with the mask u sampled from the probabilities \u03c3, and becomes (h(x)\u2297 u)."}, {"heading": "3 LEARNING SIGMOID-BERNOULLI POLICIES", "text": "We use the likelihood-ratio method (Williams, 1992) to learn the parameters of the sigmoidBernoulli policies. Since the nature of the observation space changes at each decision step, we learn L disjoint policies (one for each layer l of the deep network). As a consequence, the summation in the policy gradient disappears and becomes:\n\u2207\u03b8lL(\u03b8) = E { C(x)\u2207\u03b8l log \u03c0(u(l) | s(l)) } (2)\nsince \u03b8l only appears in the l-th decision stage and the gradient is zero otherwise.\nEstimating (2) from samples requires propagating through many instances at a time, which we achieve through mini-batches of size mb . This approach has the advantage of making optimal use of the fast matrix-matrix capabilities of modern hardware. Under the mini-batch setting, s(l) becomes a matrix and \u03c0(\u00b7 | \u00b7) a vector of dimension mb . Taking the gradient of the parameters with respect to the log action probabilities can then be seen as forming a Jacobian. We can thus re-write the empirical average in matrix form:\n\u2207\u03b8lL(\u03b8) \u2248 1\nmb mb\u2211 i=1 C(xi)\u2207\u03b8l log \u03c0(u (l) i | s (l) i ) = 1 mb c>\u2207\u03b8l log \u03c0(U(l) |S(l)) (3)\nwhere C(xi) is the total cost for input xi and mb is the number of examples in the mini-batch. The term c> denotes the row vector containing the total costs for every example in the mini-batch."}, {"heading": "3.1 FAST VECTOR-JACOBIAN MULTIPLICATION", "text": "While Eqn (3) suggests that the Jacobian might have to be formed explicitly, Pearlmutter (1994) showed that computing a differential derivative suffices to compute left or right vector-Jacobian (or Hessian) multiplication. The same trick has also recently been revived with the class of socalled \u201cHessian-free\u201d (Martens, 2010) methods for artificial neural networks. Using the notation of Pearlmutter (1994), we writeR\u03b8l {\u00b7} = c>\u2207\u03b8l for the differential operator.\n\u2207\u03b8lL(\u03b8) \u2248 1\nmb R\u03b8l\n{ log \u03c0(U(l) |S(l)) } (4)"}, {"heading": "3.2 SPARSITY AND VARIANCE REGULARIZATIONS", "text": "In order to favour dropout policies with sparse actions, we add two penalty terms Lb and Le that depend on some target sparsity rate \u03c4 . The first term pushes the policy distribution \u03c0 to activate each unit with probability \u03c4 in expectation over the data. The second term pushes the policy distribution to have the desired sparsity of activations for each example. Thus, for a low \u03c4 , a valid configuration would be to learn a few high probability activations for some part of the data and low probability activations for the rest of the data, which results in having activation probability \u03c4 in expectation.\nLb = n\u2211 j \u2016E{\u03c3j} \u2212 \u03c4\u20162 Le = E{\u2016( 1 n n\u2211 j \u03c3j)\u2212 \u03c4\u20162} (5)\nSince we are in a minibatch setting, these expectations can be approximated over the minibatch: Lb \u2248 n\u2211 j \u2016 1 mb mb\u2211 i (\u03c3ij)\u2212 \u03c4\u20162 Le \u2248 1 mb mb\u2211 i \u2016( 1 n n\u2211 j \u03c3ij)\u2212 \u03c4\u20162 (6)\nWe finally add a third term, Lv , in order to favour the aforementioned configurations, where units only have a high probability of activation for certain examples, and low for the rest. We aim to maximize the variances of activations of each unit, across the data. This encourages units\u2019 activations to be varied, and while similar in spirit to the Lb term, this term explicitly discourages learning a uniform distribution.\nLv = \u2212 n\u2211 j var Ei{\u03c3ij}\n\u2248 \u2212 n\u2211 j 1 mb mb\u2211 i\n( \u03c3ij \u2212 ( 1\nmb mb\u2211 i \u03c3ij\n))2 (7)"}, {"heading": "3.3 ALGORITHM", "text": "We interleave the learning of the network parameters and the learning of the policy parameters. We first update the network and policy parameters to minimize the following regularized loss function via backpropagation (Rumelhart et al., 1988):\nL = \u2212 logP (Y |X, \u03b8) + \u03bbs(Lb + Le) + \u03bbv(Lv) + \u03bbL2\u2016\u03b8\u20162\nwhere \u03bbs can be understood as a trade-off parameter between prediction accuracy and parsimony of computation (obtained through sparse node activation), and \u03bbv as a trade-off parameter between a stochastic policy and a more input dependent saturated policy. We then minimize the cost function C with a REINFORCE-style approach to update the policy parameters (Williams, 1992):\nC = \u2212 logP (Y |X, \u03b8) As previously mentioned, we use minibatch stochastic gradient descent as well as minibatch policy gradient updates."}, {"heading": "3.4 BLOCK POLICY DROPOUT", "text": "To achieve computational gain, instead of dropping single units in hidden layers, we drop contiguous (equally-sized) groups of units together (independently for each example in the minibatch), thus reducing the action space as well as the number of probabilities to compute and sample. As such, there are two potential speedups. First, the policy is much smaller and faster to compute. Second, it offers a computational advantage in the computation of the hidden layer themselves, since we are now performing a matrix multiplication of the following form:\nC = ((A\u2297Ma)B)\u2297Mc where Ma and Mc are binary mask matrices that resemble this (here there are 3 blocks of size 2): 0 0 1 1 0 01 1 0 0 0 0...\n0 0 1 1 1 1  This allows us to quickly perform matrix multiplication by only considering the non-zero output elements in C as well as the non-zero elements in A\u2297Ma."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 MODEL IMPLEMENTATION", "text": "The proposed model was implemented within Theano (Bergstra et al., 2010), a standard library for deep learning and neural networks. In addition to using optimizations offered by Theano, we also implemented specialized matrix multiplication code for the operation exposed in section 3.4. A straightforward and fairly naive CPU implementation of this operation yielded speedups of up to 5-10x, while an equally naive GPU implementation yielded speedups of up to 2-4x, both for sparsity rates of under 20% and acceptable matrix and block sizes.1\nWe otherwise use fairly standard methods for our neural network. The weight matrices are initialized using the heuristic of Glorot & Bengio (2010). We use a constant learning rate throughout minibatch SGD. We also use early stopping (Bishop, 2006) to avoid overfitting."}, {"heading": "4.2 MODEL EVALUATION", "text": "We first evaluate the performance of our model on the MNIST digit dataset. We use a single hidden layer of 16 blocks of 16 units (256 units total), with a target sparsity rate of \u03c4 = 6.25% = 1/16, learning rates of 10\u22123 for the neural network and 5 \u00d7 10\u22125 for the policy, \u03bbv = \u03bbs = 200 and \u03bbL2 = 0.005. Under these conditions, a test error of around 2.3% was achieved. A normal neural network with the same number of hidden units achieves a test error of around 1.9%, while a normal neural network with a similar amount of computation (multiply-adds) being made (32 hidden units) achieves a test error of around 2.8%.\n1Implementations used in this paper are available at http://github.com/bengioe/condnet/\nLooking at the activation of the policy (1c), we see that it tends towards what was hypothesized in section 3.2, i.e. where examples activate most units with low probability and some units with high probability. We can also observe that the policy is input-dependent in figures 1a and 1b, since we see different activation patterns for inputs of class \u20190\u2019 and inputs of class \u20191\u2019. Since the computation performed in our model is sparse, one could hope that it achieves this performance with less computation time, yet we consistently observe that models that deal with MNIST are too small to allow our specialized (3.4) sparse implementation to make a substantial difference. We include this result to highlight conditions under which it is less desirable to use our model.\nNext, we consider the performance of our model on the CIFAR-10 (Krizhevsky & Hinton, 2009) image dataset. A brief hyperparameter search was made, and a few of the best models are shown in figure 2. These results show that it is possible to achieve similar performance with our model (denoted condnet) as with a normal neural network (denoted NN), yet using sensibly reduced computation time. A few things are worth noting; we can set \u03c4 to be lower than 1 over the number of blocks, since the model learns a policy that is actually not as sparse as \u03c4 , mostly because REINFORCE pulls the policy towards higher probabilities on average. For example our best performing model has a target of 1/16 but learns policies that average an 18% sparsity rate (we used \u03bbv = \u03bbb = 20, except for the first layer \u03bbv = 40, we used \u03bbL2 = 0.01, and the learning rates were 0.001 for the neural net, 10\u22125 and 5 \u00d7 10\u22124 for the first and second policy layers respectively). The neural networks without conditional dropout are trained with L2 regularization as well as regular dropout (Hinton et al., 2012). We also train networks with the same architecture as our models, using block dropout, but with a uniform policy (as in original dropout) instead of a learned conditional one. This model (denoted bdNN) does not perform as well as our model, showing that the dropout noise by itself is not sufficient, and that learning a policy is required to fully take benefit of this architecture. Finally we tested our model on the Street View House Numbers (SVHN) (Netzer et al., 2011) dataset, which also yielded encouraging results (figure 3). As we restrain the capacity of the models (by increasing sparsity or decreasing number of units), condnets retain acceptable performance with low run times, while plain neural networks suffer highly (their performance dramatically decreases with lower run times). The best condnet model has a test error of 7.3%, and runs a validation epoch in 10s (14s without speed optimization), while the best standard neural network model has a test error of 9.1%, and runs in 16s. Note that the variance in the SVHN results (figure 3) is due to the mostly random hyperparameter exploration, where block size, number of blocks, \u03c4 , \u03bbv , \u03bbb, as well of learning rates are randomly picked. The normal neural network results were obtained by varying the number of hidden units of a 2-hidden-layer model."}, {"heading": "4.3 EFFECTS OF REGULARIZATION", "text": "The added regularization proposed in section 3.2 seems to play an important role in our ability to train the conditional model. When using only the prediction score, we observed that the algorithm tried to compensate by recruiting more units and saturating their participation probability, or even\nfailed by dismissing very early what were probably considered bad units. In practice, the variance regularization term Lv only slightly affects the prediction accuracy and learned policies of models, but we have observed that it significantly speeds up the training process, probably by encouraging policies to become less uniform earlier in the learning process. This can be seen in figure 5b, where we train a model with different values of \u03bbv . When \u03bbv is increased, the first few epochs have a much lower error rate.\nIt is possible to tune some hyperparameters to affect the point at which the trade-off between computation speed and performance lies, thus one could push the error downwards at the expense of also more computation time. This is suggested by figure 5a, which shows the effect of one such hyperparameter (\u03bbb) on both running times and performance for the CIFAR dataset. Here it seems that \u03bb \u223c [300, 400] offers the best trade-off, yet other values could be selected, depending on the specific requirements of an application."}, {"heading": "5 RELATED WORK", "text": "Ba & Frey (2013) proposed a learning algorithm called standout for computing an input-dependent dropout distribution at every node. As opposed to our layer-wise method, standout computes a oneshot dropout mask over the entire network, conditioned on the input to the network. Additionally, masks are unit-wise, while our approach uses masks that span blocks of units. Bengio et al. (2013) introduced Stochastic Times Smooth neurons as gaters for conditional computation within a deep neural network. STS neurons are highly non-linear and non-differentiable functions learned using estimators of the gradient obtained through REINFORCE (Williams, 1992). They allow a sparse\nbinary gater to be computed as a function of the input, thus reducing total computations in the (then sparse) activation of hidden layers.\nStollenga et al. (2014) recently proposed to learn a sequential decision process over the filters of a convolutional neural network (CNN). As in our work, a direct policy search method was chosen to find the parameters of a control policy. Their problem formulation differs from ours mainly in the notion of decision \u201cstage\u201d. In their model, an input is first fed through a network, the activations are computed during forward propagation then they are served to the next decision stage. The goal of the policy is to select relevant filters from the previous stage so as to improve the decision accuracy on the current example. They also use a gradient-free evolutionary algorithm, in contrast to our policy search method.\nThe Deep Sequential Neural Network (DSNN) model of Denoyer & Gallinari (2014) is possibly closest to our approach. The control process is carried over the layers of the network and uses the output of the previous layer to compute actions. The REINFORCE algorithm is used to train the policy with the reward/cost function being defined as the loss at the output in the base network. DSNN considers the general problem of choosing between between different type of mappings (weights) in a composition of functions. However, they test their model on datasets in which different modes are proeminent, making it easy for a policy to distinguish between them.\nAnother point of comparison for our work are attention models (Mnih et al., 2014; Gregor et al., 2015; Xu et al., 2015). These models typically learn a policy, or a form of policy, that allows them to selectively attend to parts of their input sequentially, in a visual 2D environnement. Both attention and our approach aim to reduce computation times. While attention aims to perform dense computations on subsets of the inputs, our approach aims to be more general, since the policy focuses on subsets of the whole computation (it is in a sense more distributed). It should also be possible to combine these approaches, since one acts on the input space and the other acts on the representation space, altough the resulting policies would be much more complex, and not necessarily easily trainable."}, {"heading": "6 CONCLUSION", "text": "This paper presents a method for tackling the problem of conditional computation in deep networks by using reinforcement learning. We propose a type of parameterized block-dropout policy that maps the activations of a layer to a Bernoulli mask. The reinforcement signal accounts for the loss function of the network in its prediction task, while the policy network itself is regularized to account\nfor the desire to have sparse computations. The REINFORCE algorithm is used to train policies to optimize this cost. Our experiments show that it is possible to train such models at the same levels of accuracy as their standard counterparts. Additionally, it seems possible to execute these similarly accurate models faster due to their sparsity. Furthermore, the model has a few simple parameters that allow to control the trade-off between accuracy and running time.\nThe use of REINFORCE could be replaced by a more efficient policy search algorithm, and also, perhaps, one in which rewards (or costs) as described above are replaced by a more sequential variant. The more direct use of computation time as a cost may prove beneficial. In general, we consider conditional computation to be an area in which reinforcement learning could be very useful, and deserves further study.\nAll the running times reported in the Experiments section are for a CPU, running on a single core. The motivation for this is to explore deployment of large neural networks on cheap, low-power, single core CPUs such as phones, while retaining high model capacity and expressiveness. While the results presented here show that our model for conditional computation can achieve speedups in this context, it is worth also investigating adaptation of these sparse computation models in multicore/GPU architectures; this is the subject of ongoing work."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors gratefully acknowledge financial support for this work by the Samsung Advanced Institute of Technology (SAIT), the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Fonds de recherche du Que\u0301bec - Nature et Technologies (FQRNT)."}], "references": [{"title": "Adaptive dropout for training deep neural networks", "author": ["Ba", "Jimmy", "Frey", "Brendan"], "venue": "K.Q. (eds.), Advances in Neural Information Processing Systems", "citeRegEx": "Ba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Nets, pp", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "A survey on policy search for robotics", "author": ["Deisenroth", "Marc Peter", "Neumann", "Gerhard", "Peters", "Jan"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "Deisenroth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Deep sequential neural network", "author": ["Denoyer", "Ludovic", "Gallinari", "Patrick"], "venue": "CoRR, abs/1410.0510,", "citeRegEx": "Denoyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denoyer et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S. Hochreiter"], "venue": "Diploma thesis, T.U. Mu\u0308nich,", "citeRegEx": "Hochreiter,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter", "year": 1991}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex", "kavukcuoglu", "koray"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Fast exact multiplication by the hessian", "author": ["Pearlmutter", "Barak A"], "venue": "Neural Comput.,", "citeRegEx": "Pearlmutter and A.,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter and A.", "year": 1994}, {"title": "Learning representations by back-propagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["Stollenga", "Marijn F", "Masci", "Jonathan", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Stollenga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stollenga et al\\.", "year": 2014}, {"title": "Dropout training as adaptive regularization", "author": ["Wager", "Stefan", "Wang", "Sida", "Liang", "Percy S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine Learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Large-scale neural networks, and in particular deep learning architectures, have seen a surge in popularity in recent years, due to their impressive empirical performance in complex supervised learning tasks, including state-of-the-art performance in image and speech recognition (He et al., 2015).", "startOffset": 280, "endOffset": 297}, {"referenceID": 11, "context": "Several related problems arise: very long training time (several weeks on modern computers, for some problems), potential for over-fitting (whereby the learned function is too specific to the training data and generalizes poorly to unseen data), and more technically, the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994), whereby the gradient information gets increasingly diffuse as it propagates from layer to layer.", "startOffset": 299, "endOffset": 338}, {"referenceID": 1, "context": "Several related problems arise: very long training time (several weeks on modern computers, for some problems), potential for over-fitting (whereby the learned function is too specific to the training data and generalizes poorly to unseen data), and more technically, the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994), whereby the gradient information gets increasingly diffuse as it propagates from layer to layer.", "startOffset": 299, "endOffset": 338}, {"referenceID": 19, "context": "Dropout can be interpreted as a form of regularization to prevent overfitting (Wager et al., 2013).", "startOffset": 78, "endOffset": 98}, {"referenceID": 1, "context": "Several related problems arise: very long training time (several weeks on modern computers, for some problems), potential for over-fitting (whereby the learned function is too specific to the training data and generalizes poorly to unseen data), and more technically, the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994), whereby the gradient information gets increasingly diffuse as it propagates from layer to layer. A technique called dropout was introduced by Hinton et al. (2012) as a way to reduce overfitting by breaking the tendency for \u201cco-adaptations\u201d between nodes.", "startOffset": 318, "endOffset": 503}, {"referenceID": 5, "context": "formulation, and our solution to the proposed optimization problem, using policy search methods (Deisenroth et al., 2013).", "startOffset": 96, "endOffset": 121}, {"referenceID": 10, "context": "As in the original dropout paper (Hinton et al., 2012), each node in a given layer has an associated Bernoulli distribution, which determines its probability of being dropped.", "startOffset": 33, "endOffset": 54}, {"referenceID": 17, "context": "We first update the network and policy parameters to minimize the following regularized loss function via backpropagation (Rumelhart et al., 1988): L = \u2212 logP (Y |X, \u03b8) + \u03bbs(Lb + Le) + \u03bbv(Lv) + \u03bbL2\u2016\u03b8\u2016 where \u03bbs can be understood as a trade-off parameter between prediction accuracy and parsimony of computation (obtained through sparse node activation), and \u03bbv as a trade-off parameter between a stochastic policy and a more input dependent saturated policy.", "startOffset": 122, "endOffset": 146}, {"referenceID": 3, "context": "The proposed model was implemented within Theano (Bergstra et al., 2010), a standard library for deep learning and neural networks.", "startOffset": 49, "endOffset": 72}, {"referenceID": 3, "context": "The proposed model was implemented within Theano (Bergstra et al., 2010), a standard library for deep learning and neural networks. In addition to using optimizations offered by Theano, we also implemented specialized matrix multiplication code for the operation exposed in section 3.4. A straightforward and fairly naive CPU implementation of this operation yielded speedups of up to 5-10x, while an equally naive GPU implementation yielded speedups of up to 2-4x, both for sparsity rates of under 20% and acceptable matrix and block sizes.1 We otherwise use fairly standard methods for our neural network. The weight matrices are initialized using the heuristic of Glorot & Bengio (2010). We use a constant learning rate throughout minibatch SGD.", "startOffset": 50, "endOffset": 690}, {"referenceID": 10, "context": "The neural networks without conditional dropout are trained with L2 regularization as well as regular dropout (Hinton et al., 2012).", "startOffset": 110, "endOffset": 131}, {"referenceID": 15, "context": "Finally we tested our model on the Street View House Numbers (SVHN) (Netzer et al., 2011) dataset, which also yielded encouraging results (figure 3).", "startOffset": 68, "endOffset": 89}, {"referenceID": 1, "context": "Bengio et al. (2013) introduced Stochastic Times Smooth neurons as gaters for conditional computation within a deep neural network.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Another point of comparison for our work are attention models (Mnih et al., 2014; Gregor et al., 2015; Xu et al., 2015).", "startOffset": 62, "endOffset": 119}, {"referenceID": 8, "context": "Another point of comparison for our work are attention models (Mnih et al., 2014; Gregor et al., 2015; Xu et al., 2015).", "startOffset": 62, "endOffset": 119}, {"referenceID": 21, "context": "Another point of comparison for our work are attention models (Mnih et al., 2014; Gregor et al., 2015; Xu et al., 2015).", "startOffset": 62, "endOffset": 119}, {"referenceID": 16, "context": "Stollenga et al. (2014) recently proposed to learn a sequential decision process over the filters of a convolutional neural network (CNN).", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "Stollenga et al. (2014) recently proposed to learn a sequential decision process over the filters of a convolutional neural network (CNN). As in our work, a direct policy search method was chosen to find the parameters of a control policy. Their problem formulation differs from ours mainly in the notion of decision \u201cstage\u201d. In their model, an input is first fed through a network, the activations are computed during forward propagation then they are served to the next decision stage. The goal of the policy is to select relevant filters from the previous stage so as to improve the decision accuracy on the current example. They also use a gradient-free evolutionary algorithm, in contrast to our policy search method. The Deep Sequential Neural Network (DSNN) model of Denoyer & Gallinari (2014) is possibly closest to our approach.", "startOffset": 0, "endOffset": 801}], "year": 2017, "abstractText": "Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. Dropout has been shown to be an effective strategy to sparsify computations (by not involving all units), as well as to regularize models. In typical dropout, nodes are dropped uniformly at random. Our goal is to use reinforcement learning in order to design better, more informed dropout policies, which are datadependent. We cast the problem of learning activation-dependent dropout policies for blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.", "creator": "LaTeX with hyperref package"}}}