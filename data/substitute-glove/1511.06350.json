{"id": "1511.06350", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Structured Prediction Energy Networks", "abstract": "We choose mechanism projection energy networks (SPENs ), a sensitive framework and definitions uncertain. A deep 20th-century is. to distinguish man energy relation some presidency use, example then indicating are produced by using time - propagation intended iteratively visualization the power included respect giving from labels. This deep technological thriller gyres within paper not go both even intractable customizable models, and musicians concrete course since using learning discriminative features of the informal electricity. One natural application many cannot orchestration what lucrative - label classification, which traditionally has required accordance prior assumptions this the biochemical between ads same guarantee tractable learn including error lack. We are needs to specified SPENs eventually multi - label ease few substantially larger titled next less previous interfaces same structured recovery, by placement high - order correlated forms minimal structural predictions. Overall, deep learning provides perhaps tools new educational features there the inputs them a assumptions certainly, and this even extend these techniques supposed mental style some of lossless. Our experiments required spectacular performing on time locally brought benchmark multi - soul sizes tasks, demonstrate for thinking analysis can probably particular intended offered unitless essentially learning, included evoking contrary trade - unwinding present feed - forward and analytic structured anticipated techniques.", "histories": [["v1", "Thu, 19 Nov 2015 20:39:59 GMT  (65kb,D)", "http://arxiv.org/abs/1511.06350v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Thu, 7 Jan 2016 16:28:36 GMT  (87kb,D)", "http://arxiv.org/abs/1511.06350v2", "Updated version of ICLR 2016 submission"], ["v3", "Thu, 23 Jun 2016 20:21:11 GMT  (201kb,D)", "http://arxiv.org/abs/1511.06350v3", "ICML 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["david belanger", "andrew mccallum"], "accepted": true, "id": "1511.06350"}, "pdf": {"name": "1511.06350.pdf", "metadata": {"source": "CRF", "title": "STRUCTURED PREDICTION ENERGY NETWORKS", "authors": ["David Belanger"], "emails": ["belanger@cs.umass.edu", "mccallum@cs.umass.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Structured prediction is an important problem in a variety of machine learning domains. Consider an input x and structured output y, such as a labeling of time steps, a collection of attributes for an image, a parse of a sentence, or a segmentation of an image into objects. Such problems are challenging because the number of candidate y is exponential in number of output variables that comprise it. As a result, practitioners encounter computational considerations, since prediction requires searching the enormous space of outputs, and also statistical considerations, since learning accurate models from limited data requires reasoning about commonalities between distinct structured outputs. Therefore, structured prediction is fundamentally a problem of representation, where the representation must capture both the discriminative interactions between x and y and also allow for efficient combinatorial optimization over y. With this perspective in mind, it is not surprising that there are variety of natural ways to couple structured prediction with deep learning, a powerful framework for representation learning.\nWe consider two principal approaches to structured prediction: (a) as a feed-forward function y = f(x), and (b) using an energy-based viewpoint y = arg miny\u2032 Ex(y\u2032) (LeCun et al., 2006). Feed-forward approaches include, for example, predictors using local convolutions plus a classification layer (Collobert et al., 2011), fully-convolutional networks (Long et al., 2015), or sequenceto-sequence predictors (Vinyals et al., 2014). In contrast, the energy-based approach may involve non-trivial optimization to perform predictions, and includes, for example, conditional random fields (CRFs) (Lafferty et al., 2001). From a modeling perspective, energy-based approaches are desirable because directly parametrizing Ex() provides practitioners with better opportunities to utilize domain knowledge about properties and invariances of the structured output. Furthermore, such a parametrization may be more parsimonious, resulting in improved generalization from limited data. On the other hand, for energy-based models, both prediction and learning are more complex than for feed-forward approaches.\nar X\niv :1\n51 1.\n06 35\n0v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nBoth approaches can be combined naturally with deep learning. It is straightforward to parametrize a feed-forward predictor y = f(x) as a deep architecture. In these, end-to-end learning can be performed easily using gradient descent. For energy-based prediction, prior applications of deep learning have mostly followed a two-step construction: first, choose an existing model structure for which the search problem y = arg miny\u2032 Ex(y\u2032) can be performed efficiently, eg. with the Viterbi algorithm, and then express the dependence of Ex() on x via a deep architecture. For example, in a CRF, the tables of potentials of an undirected graphical model can be parametrized via a deep network applied to x (LeCun et al., 2006; Collobert et al., 2011; Huang et al., 2015). The advantage of this approach is that it leverages the strength of deep architectures to perform representation learning on x, while maintaining the ability to perform efficient combinatorial prediction, since the dependence of Ex(y\u2032) on y\u2032 remains unchanged. However, by assuming a particular graphical model structure for Ex() a-priori, this construction perhaps imposes an excessively strict inductive bias and, and practitioners are unable to use the deep architecture to perform structure learning, ie. representation learning that discovers the interaction between different parts of y.\nIn response, we present structured prediction energy networks (SPENs), a novel energy-based prediction technique that offers substantially different tradeoffs than these prior applications of deep learning to structured prediction. Namely, we sacrifice algorithmic guarantees for solving arg miny\u2032 Ex(y\n\u2032) exactly, in exchange for an extremely flexible framework for expressing the energy function Ex(). We use a deep architecture to encode the energy , and perform predictions by approximately minimizing the energy with respect to the prediction variables y using gradient descent, where gradients are obtained by backpropagation through the deep architecture. The parameters of the network are trained using an adaptation of a structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2004). The deep network allows us to model high-arity interactions that would result in unmanageable treewidth if the problem was posed as an undirected graphical model. Furthermore, the learned measurement matrix (Section 3) of the SPEN provides an interpretable tool for structure learning.\nTypically, back-propagation through a deep architecture is used during learning to update the network parameters. However, there is a breadth of work, both old and contemporary, on using backpropagation to update prediction variables (Bromley et al., 1993; Szegedy et al., 2014; Goodfellow et al., 2014; Le & Mikolov, 2014; Mordvintsev et al., 2015; Gatys et al., 2015a;b). As with this prior work, prediction in SPENs is conceptually simple and easy to implement.\nOur experiments focus on applying SPENs to multi-label classification problems. These are naturally posed as structured prediction problems, since the labels exhibit rich interaction structure. However, prior applications of structured prediction, eg. using CRFs, have been limited to notably smaller problems than our experiments consider, since the techniques\u2019 computational complexity either grows super-linearly in the number of labels L (Ghamrawi & McCallum, 2005; Finley & Joachims, 2008; Meshi et al., 2010; Petterson & Caetano, 2011), or requires strict assumptions about the dependencies between labels (Read et al., 2011; Jasinska & Dembczyski, 2015; Niculescu-Mizil & Abbasnejad, 2015). SPENs, on the other hand, scale linearly in L while placing mild prior assumptions about the labels\u2019 interactions, namely that they can be encoded by a deep architecture.\nOn a variety of benchmark multi-label classification tasks, the expressivity of our deep energy function provides accuracy improvements against a variety of competitive baselines. We also offer experiments on synthethic data with rigid mutual exclusivity constraints between labels to demonstrate the power of SPENs to perform structure learning. These experiments illuminate important tradeoffs in the expressivity and parsimony of SPENs vs. feed-forward predictors. We encourage future work using energy-based structured prediction in deep learning."}, {"heading": "2 STRUCTURED PREDICTION ENERGY NETWORKS", "text": "A fully general way to specify the set of all x \u2192 y mappings is to pose y as the solution to a potentially non-linear combinatorial optimization problem, with parameters dependent on x:\nmin y Ex(y) subject to y \u2208 {0, 1}L. (1)\nThe structured prediction problem (1) could be rendered tractable by assuming certain specific structure for the energy function Ex(), such as a tree-structured undirected graphical model. Instead, we consider general Ex(), but optimize over a convex relaxation of the constraint set:\nmin y Ex(y\u0304) subject to y\u0304 \u2208 [0, 1]L. (2)\nIn general, Ex(y\u0304) may be non-convex, so exactly solving (2) may be intractable. A reasonable approximate optimization procedure, however, is to minimize (2) via gradient descent, obtaining a local minimum. Optimization over the set [0, 1]L can be performed using entropic mirror descent (aka exponentiated gradient) by normalizing over each coordinate (Beck & Teboulle, 2003).\nThere are no guarantees that our predicted y\u0304 values are nearly 0-1. In some applications, we may need to round y\u0304 to obtain predictions that are usable downtream. Sometimes, it is useful to maintain \u2018soft\u2019 predictions, eg. for detection problems, since we may want to threshold based on confidence.\nIn the posterior inference literature, mean-field approaches also consider a relaxation from y to y\u0304, where y\u0304i would be interpreted as the marginal probability that yi = 1 (Jordan et al., 1999). Here, the practitioner starts with a probabilistic model for which inference is intractable, and obtains a mean-field objective when seeking to perform approximate variational inference. We make no such probabilistic assumptions, however, and instead adopt a disciminative approach by directly parametrizing the objective that the inference procedure optimizes.\nContinuous optimization over y\u0304 can be performed using black-box access to a gradient subroutine for Ex(y\u0304). Therefore, it is natural to parametrize Ex(y\u0304) using deep architectures, a flexible family of multivariate function approximators that provide efficient gradient calculation.\nA SPEN parameterizes Ex(y\u0304) as a neural network that takes both x and y\u0304 as inputs and returns the energy (a single number). In general, a SPEN consists of two deep architectures. First, the feature network F(x) produces an f -dimensional feature representation for the input. Next, the energy Ex(y\u0304) is given by the output of the energy network G(F (x), y\u0304). Here, F and G can be arbitrary deep networks.\nNote that the energy only depends on x via the value of F (x). During iterative prediction, we improve efficiency by precomputing F (x) and not back-propagating through F when differentiating the energy with respect to y\u0304."}, {"heading": "3 EXAMPLE SPEN ARCHITECTURE", "text": "We now provide a more concrete example of the architecture for a SPEN. All of our experiments use the general configuration described in this section. We denote matrices in upper case and vectors in lower case. We use g() to denote a coordinate-wise non-linearity function, and may use different non-linearities, eg. sigmoid vs. rectifier, in different places.\nFor our feature network, we employ a simple 2-layer neural network:\nf(x) = g(A2g(A1x)). (3)\nOur energy network is the sum of two terms. First, the local energy network scores y\u0304 as the sum of L independent linear models:\nElocalx (y\u0304) = L\u2211 i=1 y\u0304ib > i f(x). (4)\nHere, each bi is an F dimensional vector of parameters for every label.\nThis score is added to the output of the label energy network, which scores configurations of y\u0304 independent of x:\nElabelx (y\u0304) = c > 2 g(C1y\u0304). (5)\nThe product C1y\u0304 is a set of learned linear measurements of the output, that capture salient features of the labels used to model their dependencies. By learning the measurement matrix C1 from data, the practitioner imposes minimal assumptions a-priori on the interaction structure between the labels.\nWhile computing such a product is only linear in L, we can model sophisticated interactions by feeding C1y\u0304 through a non-linear energy. In Section 7.2, we present experiments exploring the usefulness of the measurement matrix as a means to perform structure learning.\nIn some of our experiments, we add another layer of depth to (5). In general, there is a tradeoff between using increasingly expressive energy networks and being more vulnerable to overfitting.\nIn future work, it would be natural to use a label energy network that conditions on x. For example, Econdx (y\u0304) = d > 2 g(D1[y\u0304; f(x)]). (6)\nIAlso, it may be desirable to choose g that result in a convex prediction problem. However, our experiments select g based on accuracy, rather than algorithmic guarantees resulting from convexity."}, {"heading": "3.1 CONDITIONAL RANDOM FIELDS AS SPENS", "text": "There are important parallels between the example SPEN architecture given above and the parametrization of a CRF (Lafferty et al., 2001; Sutton & McCallum, 2011). Here, we use CRF to refer to any structured linear model, which may or may not be trained to maximize the conditional log likelihood. For the sake of notational simplicity, consider a fully-connected pairwise CRF with local potentials that depend on x, but data-independent pairwise potentials. Let vec() flatten a matrix into a vector. Suppose we apply Ex() directly to y, rather than to the relaxation y\u0304. The corresponding label energy net would be:\nEcrfx (y) = s > 2 vec(yy >), (7) In applications with large label spaces, (7) is troublesome in terms of both the statistical efficiency of parameter estimation and the computational efficiency of prediction because of the quadratic dependence on L. Statistical issues can be mitigated by imposing parameter tying of the CRF potentials, using a low-rank assumption, eg. (Srikumar & Manning, 2014; Jernite et al., 2015), or using a deep architecture to map x to a table of CRF potentials (LeCun et al., 2006). Computational concerns can be mitigated by choosing a sparse graph. This is difficult for practitioners when they do not know the dependencies between labels a-priori. Furthermore, modeling high-order interactions than pairwise relationships is very expensive with a CRF, but presents no extra cost for SPENs.\nFor CRFs, the interplay between the graph structure and the set of representable conditional distributions is well-understood (Koller & Friedman, 2009). However, characterizing the representational capacity of SPENs is more complex, as it depends on the general representational capacity of the deep architecture chosen."}, {"heading": "4 LEARNING SPENS", "text": "In Section 2, we described a technique for producing predictions by performing continuous optimization in the space of outputs. Now, we discuss a gradient-based technique for learning the parameters of the deep architecture Ex(y\u0304).\nIn many structured prediction applications, the practitioner is able to interact with the model in only two ways: (1) evaluate the model\u2019s energy on a given value of y, and (2) minimize the energy with respect to the y. This occurs, for example, when predicting combinatorial structures such as bipartite matchings and graph cuts. A popular technique in these settings is the structured support vector machine (SSVM) (Taskar et al., 2004; Tsochantaridis et al., 2004).\nIf we assume (incorrectly) that our prediction procedure is not subject to optimization errors, then (1) and (2) apply to our model and it is straightforward to train using an SSVM. This ignores errors resulting from the potential non-convexity of Ex(y\u0304) or the relaxation from y to y\u0304. However, such an assumption is a reasonable way to construct an approximate learning procedure.\nDefine \u2206(yp, yg) to be an error function between a prediction yp and the ground truth yg , such as the Hamming loss. Let \u03a8 denote the parameters of Ex. Let [\u00b7]+ = max(0, \u00b7). The SSVM minimizes the training objective\nL(\u03a8) = \u2211\n{xi,yi}\nmax y [\u2206(yi, y)\u2212 Exi(y) + Exi(yi)]+ . (8)\nNote that the signs in (8) differ from convention because here prediction minimizes Ex(). We minimize our loss with respect to the parameters of the deep architecture Ex using mini-batch stochastic gradient descent. For a given {xi, yi}, the subgradient of 8 is:\n\u2207\u03a8L(\u03a8) = I [\u2206(yi, yp)\u2212 Exi(yp) + Exi(yi) > 0] (\u2212\u2207\u03a8Exi(yp) +\u2207\u03a8Exi(yi)) (9) Here, I[\u00b7] is an indicator function for a predicate, and yp is the output of loss-augmented inference:\nyp = arg min y (\u2212\u2206(yi, y) + Exi(y)) . (10)\nWith this, (9) can be computed using back-propagation through Ex.\nWe perform loss-augmented inference by again using gradient descent on the relaxation y\u0304, rather than performing combinatorial optimization over y. Since \u2206 is a discrete function such as the Hamming loss, we need to approximate it with a differentiable surrogate, such as the squared loss. Any surrogate loss used for training a feed-forward predictor with gradient descent can be used here. Note that the objective (8) only considers the energy values of the ground truth and the prediction, ensuring that they\u2019re separated by a margin, not the actual ground truth and predicted labels (10). Therefore, we do not round the output of (10) in order to approximate a subgradient of (8); instead, we evaluate the energy directly on the y\u0304 obtained by approximately minimizing (10).\nFinally, we have found that it is useful to initialize the parameters of the feature network by first training them using a simple local classification loss, ignoring any interactions between coordinates of y. For problems with very limited training data, we have found that overfitting can be lessened by keeping the feature network\u2019s parameters fixed when training the label energy network parameters."}, {"heading": "5 APPLICATIONS OF SPENS", "text": "Our experiments focus on multi-label classification, an important task in a variety of machine learning applications. The data consist of {x, y} pairs, where y = {y1, . . . , yL} \u2208 {0, 1}L is a set of multiple binary labels we seek to predict and x is a feature vector. In many cases, we are given no structure among the L labels a-priori, though the labels may be quite correlated. SPENs are a very natural model for multi-label classification because learning the measurement matrix C1 in (5) provides an automatic method for discovering this interaction structure. Section 6.3 discusses the relationship between SPENs and prior work on multi-label classification.\nSPENs are very general, though, and can be applied to any prediction problem that can be posed as MAP inference in an undirected graphical model. In many applications of graphical models, the practioner employs certain prior knowledge about dependencies in the data to choose the graph structure, and certain invariances in the data to impose parameter tying schemes. For example, when tagging sequences with a linear-chain CRF, the parameterization of local and pairwise potential functions is shared across time. Similarly, when applying a SPEN, we can express the label energy net (5) using temporal convolutions, ie. C1 has a repeated block-diagonal structure.\nSection A describes details for improving the accuracy and efficiency of SPENs in practice."}, {"heading": "6 RELATED WORK", "text": ""}, {"heading": "6.1 ITERATIVE PREDICTION USING NEURAL NETWORKS", "text": "Our use of backprogation to perform gradient-based prediction differs from most deep learning applications, where backpropagation is used to update the network parameters. However, backpropagation-based prediction has been useful in a variety of deep learning applications, including siamese networks (Bromley et al., 1993), methods for generating adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2014), methods for embedding documents as dense vectors (Le & Mikolov, 2014), and successful techniques for image generation and texture synthesis (Mordvintsev et al., 2015; Gatys et al., 2015a;b).\nIn concurrent work, (Carreira et al., 2015) propose an iterative structured prediction method for human pose estimation, where Ex(y), doesn\u2019t return a number, but instead an increment \u2206(x, y). Predictions are constructed by incrementally stepping as yt+1 = yt + \u2206(x, yt). The \u2206 network is trained as a multi-variate regression task, by defining a ground truth trajectory for intermediate yt.\nAn alternative line of work has constructed feed-forward predictors using energy-based models as motivation (Domke, 2013; Hershey et al., 2014; Zheng et al., 2015). Here, an energy-based model family is chosen, along with an iterative inference technique for the model. The inference technique is then unrolled into a computation graph, for a fixed number of iterations, and all of the parameters are learned end-to-end by back-propagating through the iterative procedure. Such an approach presents different inductive biases than our approach, since it introduces many additional parameters, but places rigid restrictions on how they are used."}, {"heading": "6.2 CONDITIONAL RANDOM FIELDS", "text": "A natural alternative to SPENs for structured prediction is to encode Ex(y) as a CRF. the principal advantage of SPENs is that CRF inference is exponential in the treewidth of the graph, whereas the measurements employed by SPENs can extract information from arbitrarily many labels at once. While the per-iteration complexity of SPEN prediction is superior to CRFs of comparable expressivity, it is difficult to analyze its overall cost compared to CRFs, eg. with belief propagation, because both perform non-convex optimization.\nTraining CRFs using an SSVM loss is conceptually more attractive than training SPENs, however. In loopy graphical models, it is tractable to solve the LP relaxation of MAP inference using graph-cuts or message passing techniques, eg. (Boykov & Kolmogorov, 2004; Globerson & Jaakkola, 2008). Solving the LP relaxation, instead of performing exact MAP inference, in the inner loop of SSVM learning is fairly benign, since it is guaranteed to over-generate margin violations in (8). A chief concern when training a SPEN with an SSVM is that the non-convex optimization in the inner loop of learning will find poor local minima such that no margin violations in (8) are discovered (Kulesza & Pereira, 2007; Finley & Joachims, 2008). Since parameter updates (9) only occur when margin violations are discovered, this halts the learning process."}, {"heading": "6.3 MULTI-LABEL CLASSIFICATION", "text": "The most simple multi-label classification approach is to independently predict each label yi using a separate classifier, also known as the \u2018binary relevance model\u2019 (Tsoumakas & Katakis, 2006). This can perform poorly, particularly when certain labels are rare or some are highly correlated. Modeling improvements use max-margin or ranking losses that directly address the multi-label structure (Elisseeff & Weston, 2001; Godbole & Sarawagi, 2004; Zhang & Zhou, 2006; Bucak et al., 2009).\nCorrelations between labels can be modeled explicitly using models with low-dimensional embeddings of labels (Ji & Ye, 2009; Cabral et al., 2011; Yu et al., 2014; Xu et al., 2014; Bhatia et al., 2015). This can be achieved, for example, by using low-rank parameter matrices. In the SPEN framework, such a model would consist of a linear feature network (3) of the form f(x) = A1x, where A1 has fewer rows than there are target labels, and no label energy network. While the prediction cost of such methods grows linearly with L, these models have limited expressivity, and can not capture strict structural constraints among labels, such as mutual exclusivity and implicature. By using a non-linear multi-layer perceptron (MLP) for the feature network with hidden layers of lower dimensionality than the input, we are able to capture similar low-dimensional structure, but also capture interactions between outputs. In our experiments, an MLP is a novel, competitive baseline.\nIt is natural to approach multi-label classification using structured prediction, which models interactions between prediction labels directly. However, these techniques\u2019 computational complexity grows super-linearly in L (Ghamrawi & McCallum, 2005; Finley & Joachims, 2008; Meshi et al., 2010; Petterson & Caetano, 2011) , or requires practitioners to impose strict assumption about the dependencies between labels (Read et al., 2011; Jasinska & Dembczyski, 2015; Niculescu-Mizil & Abbasnejad, 2015). This has prevented scalability to large label spaces with complex interactions.\nOur parametrization of the label energy network (5) in terms of linear measurements of the labels is inspired by prior approaches using compressed sensing and error-correcting codes for multi-label classification (Hsu et al., 2009; Hariharan et al., 2010; Kapoor et al., 2012). However, these rely on assumptions about the sparsity of the true labels or prior knowledge about label interactions, and often do not learn the measurement matrix from data. We do not assume that the labels are sparse. Instead, we assume that their interaction can be parametrized by a deep network applied to a set of linear measurements of the labels."}, {"heading": "7 EXPERIMENTS", "text": ""}, {"heading": "7.1 MULTI-LABEL CLASSIFICATION BENCHMARKS", "text": "Table 1 compares SPENs to a variety of high-performing baselines on a selection of standard multi-label classification tasks (Tsoumakas & Katakis, 2006). Dataset sizes, etc. are described in Table 4. We compare BR: independent per-label logistic regression, ie. the \u2018binary relevance model\u2019 Tsoumakas & Katakis (2006). MLP: multi-layer perceptron with ReLU non-linearities trained with per-label logistic loss, ie. the feature network equation (3) coupled with the local energy network equation (4). LR: the low-rank-weights method of Yu et al. (2014). All results besides MLP and SPEN, are taken from Lin et al. (2014). We report the \u2018example averaged\u2019 F1 measure. For Bibtex and Delicious, we tune parameters by first jack-knifing a separate train-test split. For Bookmarks, we use the same train-dev-test split as Lin et al. (2014). For SPENs, we obtain predictions by rounding y\u0304i above a threshold tuned on held-out data. Section A.2 describes our hyperparameters.\nThere are multiple key results in Table 1. First, SPENs are very competitive compared to all of the other methods. Second, MLP, a technique that has not been treated as a baseline in recent literature, is surprisingly accurate as well. Finally, the MLP outperformed SPEN on the Delicious dataset. Here, we found that accurate prediction requires well-calibrated soft predictions to be combined with a confidence threshold. The MLP, which is trained with logistic regression, is better at predicting soft predictions than SPENs, which are trained with a margin loss. To obtain the SPEN result for Delicious in Table 1, we need to smooth the test-time prediction problem with extra entropy terms to obtain softer predictions.\nMany multi-label classification methods approach learning as a missing data problem. Here, the training labels y are assumed to be correct only when yi = 1. When yi = 0, they are treated as missing data, whose values can be imputed using assumptions about the rank (Lin et al., 2014) or sparsity (Bucak et al., 2011; Agrawal et al., 2013) of the matrix of training labels. For certain multi-label tasks, such modeling is useful because only positive labels are annotated. For example, the approach of (Lin et al., 2014) achieves 44.2 on the Bibtex dataset, outperforming our method, but only 33.3 on Delicious, substantially worse than the MLP or SPEN. Missing data modeling is orthogonal to the modeling of SPENs, and we can combine missing data techniques with SPENs.\nGenerally, SPENs are slower than non-iterative prediction techniques. However, for some problems the evaluation of the feature network (3) is a substantial burden, and this only needs be run once, for SPENs since the features do not depend on y. We also perform predictions across very large batches on a GPU in parallel. Despite providing substantial speedups, this approach is subject to the \u2018curse of the last reducer,\u2019 where unnecessary gradient computation is performed on easy examples while waiting for difficult examples to converge. Finally, the speed and accuracy of SPENs can be traded off by modifying the convergence criterion, learning rate, etc. of the prediction-time optimization."}, {"heading": "7.2 PERFORMING STRUCTURE LEARNING USING SPENS", "text": "Next, we perform experiments on synthetic data designed to demonstrate that the label measurement matrix, C1 in the label energy network (5), provides a useful tool for analyzing the structure of dependencies between labels. SPENs impose a particular inductive bias about the interaction between x and y. Namely, the interactions between different labels in y do not depend on x. Our experiments show that this parametrization allows SPENs to excel in regimes of limited training data, due to their superior parsimony compared to analogous feed-forward approaches.\nTo generate data, we first draw a design matrix X with 64 features, with each entry drawn from N(0, 1). Then, we generate a 64 x 16 weights matrix A, again with entries from N(0, 1). Then, we construct Z = XA and split the 16 columns of Z into 4 consecutive blocks. For each block, we set Yij = 1 if Zij is the maximum entry in its row-wise block, and 0 otherwise. We seek to learn a model with predictions that reliably obey these within-block mutual exclusivity constraints.\nFigure 1 depicts block structure in the learned measurement matrix. Measurements that place equal weight on every element in a block can be used to detect violations of the mutual exclusivity constraints characteristic of the data generatic process. The choice of network architecture can significantly affect the interpretability of the measurement matrix, however. When using ReLU, which acts as the identity for positive activations, violations of the data constraints can be detected by taking linear combinations of the measurements (a), since multiple hidden units place large weight on some labels. This obfuscates our ability to perform structure learning by investigating the measurement matrix. On the other hand, since applying HardTanh to measurements saturates from above, the network learns to utilize each measurement individually, yielding substantially more interpretable structure learning in (b).\nNext, in Table 2 we compare: a linear classifier, a 3-Layer ReLU MLP with hidden units of size 64 and 16, and a SPEN with a simple linear local energy network and a 2-layer label energy network with HardTanh activations and 4 hidden units. Using fewer hidden units in the MLP results in substantially poorer performance. We avoid using a non-linear local energy network in the SPEN because we want to force the label energy network to capture all interactions between labels.\nNote that the SPEN consistently outperforms the MLP, particularly when training on only 1.5k examples. In the limited data regime, their difference is because the MLP has 5x more parameters, since we use a simple linear feature network in the SPEN. We also because we inject domain knowledge about the constraint structure when designing the label energy network\u2019s architecture. Figure 2 in the Appendix demonstrates that we can perform the same structure learning as in Figure 1 on this small training data.\nNext, observe that for 15k examples the performance of the MLP and SPEN are comparable. Initially, we hypothesized that the mutual exclusivity constraints of the labels could not be satisfied by a feed-forward predictor, and that reconciling their interactions would require an iterative procedure. However, it seems that a large, expressive MLP can learn an accurate predictor when presented with lots of examples. Going forward, we would like to investigate the parsimony vs. expressivity tradeoffs of SPENs and MLPs."}, {"heading": "7.3 ANALYZING THE EFFECT OF SEARCH ERRORS ON SSVM TRAINING", "text": "Due to scalability considerations, prior applications of CRFs to multi-label classification have been restricted to substantially smaller L than those considered in Table 1. In Table 3, we consider\nthe 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest label space fit using a CRF in Finley & Joachims (2008) and Meshi et al. (2010). Finley & Joachims (2008) analyze the effects of inexact prediction on SSVM training and on test-time prediction. Table 3 considers greedy prediction, loopy belief propagation, exact prediction using an ILP solver, solving the LP relaxation, and SPENs, where the same technique is used at train and test time. All results, besides SPENs, are from Finley & Joachims (2008), which also considers cases where different methods are used in train vs. test. We report hamming error, using 10-fold cross validation.\nA key argument of Finley & Joachims (2008) is that SSVM training is more effective when the traintime inference method will not under-generate margin violations. Here, LBP and SPEN, which both approximately minimize a non-convex inference objective, have such a vulnerability, whereas LP does not, since solving the LP relaxation provides a lower bound on the true solution to the value of (10). Since SPEN performs similarly to EXACT and LP, this suggests that perhaps the effect of inexact prediction is more benign for SPENs than for LBP. However, SPENs exhibit alternative expressive power to pairwise CRFs, and thus it is difficult to isolate the effect of SSVM training on accuracy. In future work, we will perform additional experiments to test this."}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "Structured prediction energy networks employ deep architectures to perform representation learning for structured objects, jointly over both x and y. This provides straightforward prediction using gradient descent and an expressive, interpretable framework for the energy function.\nWe hypothesize that more accurate models can be trained from limited data using the energy-based approach, due to superior parsimony and better opportunities for practitioners to inject domain knowledge. Deep networks have transformed our ability to learn hierarchies of features for the inputs in signal processing problems, such as computer vision and speech recognition. SPENs provide a step in the direction of applying this feature learning revolution to the outputs of structured prediction. Such modeling has the opportunity to improve accuracy in a variety of problems with rich dependencies between outputs.\nWe have found that SPEN predictions are often spiked at either 0 or 1, despite optimizing a nonconvex energy over the set [0, 1]. We expect that this results from the energy function being fit to data that is always 0 or 1. We will further study the interplay between the choice of architecture and the integrality of predictions, particularly the piecewise-linear nature of ReLU energy networks.\nSPEN prediction requires traversing a complex energy surface using gradient descent. In future work, we will explore alternatives to SSVM training that explicitly model our iterative prediction approach. For example, we can apply the technique of Maclaurin et al. (2015) to update the model parameters by differentiating, with respect to the model parameters \u03a8, the process of performing iterative gradient-based optimization with respect to y\u0304."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Nvidia for a generous hardware grant. This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #CNS-0958392. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}, {"heading": "A APPENDIX", "text": "A.1 DETAILS FOR IMPROVING EFFICIENCY AND ACCURACY OF SPENS\nVarious tricks of the trade from the deep learning literature, such as momentum, can be applied to improve the prediction-time optimization performance of our entropic mirror descent approach described in Section 2, which are particularly important because Ex(y\u0304) is generally non-convex.\nWe perform inference in minibatches in parallel on GPUs.\nWhen \u2018soft\u2019 predictions are useful, it can be useful to augment Ex(y\u0304) with an extra term for the entropy of y\u0304. This can be handled at essentially no computational cost, by simply normalizing the iterates in entropic mirror descent at a certain \u2018temperature.\u2019 This is only done at test time, not in the inner loop of learning.\nTypically, backpropagation computes the gradient of output with respect to the input and also computes the gradient of the output with respect to any parameters of the network. For us, however, we only care about gradients with respect to the inputs y\u0304 during inference. Therefore, we can obtain a considerable speedup by avoiding computation of the parameter gradients.\nWe train the local energy network first, using a local label-wise prediction loss. Then, we clamp the parameters of the local energy network and train the label energy network. Finally, we perform an additional pass of training, where all parameters are updated using a small learning rate.\nA.2 HYPERPARAMETERS\nFor prediction, both at test time and in the inner loop of learning, we ran gradient descent with momentum = 0.95, a learning rate of 0.1, and no learning rate decay. We terminated prediction when either the relative change in the objective was below a tolerance or the l\u221e change between iterates was below an absolute tolerance.\nFor training, we used sgd with momentum 0.9 with learning rate and learning rate decay tuned on development data. We use l2 regularization both when pre-training the features and net and during SSVM training, with l2 weights tuned on development data.\nWe did not tune the sizes of the hidden layers for the feature network and label energy network. These were set based on intuition and the size of the data, the number of training examples, etc."}], "references": [{"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["Agrawal", "Rahul", "Gupta", "Archit", "Prabhu", "Yashoteja", "Varma", "Manik"], "venue": "In Proceedings of the 22nd international conference on World Wide Web, pp. 13\u201324. International World Wide Web Conferences Steering Committee,", "citeRegEx": "Agrawal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["Beck", "Amir", "Teboulle", "Marc"], "venue": "Operations Research Letters,", "citeRegEx": "Beck et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2003}, {"title": "Locally nonlinear embeddings for extreme multi-label learning", "author": ["Bhatia", "Kush", "Jain", "Himanshu", "Kar", "Purushottam", "Prateek", "Varma", "Manik"], "venue": "CoRR, abs/1507.02743,", "citeRegEx": "Bhatia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision", "author": ["Boykov", "Yuri", "Kolmogorov", "Vladimir"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Boykov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2004}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Bromley", "Jane", "Bentz", "James W", "Bottou", "L\u00e9on", "Guyon", "Isabelle", "LeCun", "Yann", "Moore", "Cliff", "S\u00e4ckinger", "Eduard", "Shah", "Roopak"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Efficient multi-label ranking for multi-class learning: application to object recognition", "author": ["Bucak", "Serhat S", "Mallapragada", "Pavan Kumar", "Jin", "Rong", "Jain", "Anil K"], "venue": "In Computer Vision,", "citeRegEx": "Bucak et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bucak et al\\.", "year": 2009}, {"title": "Multi-label learning with incomplete class assignments", "author": ["Bucak", "Serhat Selcuk", "Jin", "Rong", "Jain", "Anil K"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Bucak et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bucak et al\\.", "year": 2011}, {"title": "Matrix completion for multi-label image classification", "author": ["Cabral", "Ricardo S", "Torre", "Fernando", "Costeira", "Jo\u00e3o P", "Bernardino", "Alexandre"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cabral et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cabral et al\\.", "year": 2011}, {"title": "Human pose estimation with iterative error feedback", "author": ["Carreira", "Joao", "Agrawal", "Pulkit", "Fragkiadaki", "Katerina", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1507.06550,", "citeRegEx": "Carreira et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Carreira et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["Domke", "Jens"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Domke and Jens.,? \\Q2013\\E", "shortCiteRegEx": "Domke and Jens.", "year": 2013}, {"title": "A kernel method for multi-labelled classification", "author": ["Elisseeff", "Andr\u00e9", "Weston", "Jason"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Elisseeff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Elisseeff et al\\.", "year": 2001}, {"title": "Training structural svms when exact inference is intractable", "author": ["Finley", "Thomas", "Joachims", "Thorsten"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Finley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finley et al\\.", "year": 2008}, {"title": "A neural algorithm of artistic", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "style. CoRR,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Texture synthesis using convolutional neural networks", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Collective multi-label classification", "author": ["Ghamrawi", "Nadia", "McCallum", "Andrew"], "venue": "In Proceedings of the 14th ACM international conference on Information and knowledge management,", "citeRegEx": "Ghamrawi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ghamrawi et al\\.", "year": 2005}, {"title": "Fixing max-product: Convergent message passing algorithms for map lp-relaxations", "author": ["Globerson", "Amir", "Jaakkola", "Tommi S"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Globerson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2008}, {"title": "Discriminative methods for multi-labeled classification", "author": ["Godbole", "Shantanu", "Sarawagi", "Sunita"], "venue": "In Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "Godbole et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Godbole et al\\.", "year": 2004}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Large scale maxmargin multi-label classification with priors", "author": ["Hariharan", "Bharath", "Zelnik-Manor", "Lihi", "Varma", "Manik", "Vishwanathan", "Svn"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Hariharan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2010}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["Hershey", "John R", "Roux", "Jonathan Le", "Weninger", "Felix"], "venue": "arXiv preprint arXiv:1409.2574,", "citeRegEx": "Hershey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2014}, {"title": "Multi-label prediction via compressed sensing", "author": ["Hsu", "Daniel", "Kakade", "Sham", "Langford", "John", "Zhang", "Tong"], "venue": "In NIPS,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Huang", "Zhiheng", "Xu", "Wei", "Yu", "Kai"], "venue": "CoRR, abs/1508.01991,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Consistent label tree classifiers for extreme multi-label classification", "author": ["Jasinska", "Kalina", "Dembczyski", "Krzysztof"], "venue": "In ICML 2015 Workshop on Extreme Classification,", "citeRegEx": "Jasinska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jasinska et al\\.", "year": 2015}, {"title": "A fast variational approach for learning markov random field language models", "author": ["Jernite", "Yacine", "Rush", "Alexander M", "Sontag", "David"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Jernite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jernite et al\\.", "year": 2015}, {"title": "Linear dimensionality reduction for multi-label classification", "author": ["Ji", "Shuiwang", "Ye", "Jieping"], "venue": "In IJCAI,", "citeRegEx": "Ji et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2009}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Multilabel classification using bayesian compressed sensing", "author": ["Kapoor", "Ashish", "Viswanathan", "Raajay", "Jain", "Prateek"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kapoor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2012}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": "MIT press,", "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Structured learning with approximate inference", "author": ["Kulesza", "Alex", "Pereira", "Fernando"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kulesza et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Lafferty", "John D", "McCallum", "Andrew", "Pereira", "Fernando CN"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc", "Mikolov", "Tomas"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia", "M Ranzato", "F. Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Multi-label learning with posterior regularization", "author": ["Lin", "Victoria (Xi", "Singh", "Sameer", "He", "Luheng", "Taskar", "Ben", "Zettlemoyer", "Luke"], "venue": "In NIPS Workshop on Modern Machine Learning and Natural Language Processing,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "CVPR (to appear),", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["Maclaurin", "Dougal", "Duvenaud", "David", "Adams", "Ryan P"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Maclaurin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Learning efficiently with approximate inference via dual losses", "author": ["Meshi", "Ofer", "Sontag", "David", "Globerson", "Amir", "Jaakkola", "Tommi S"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Meshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2010}, {"title": "Inceptionism: Going deeper into neural networks", "author": ["Mordvintsev", "Alexander", "Olah", "Christopher", "Tyka", "Mike"], "venue": "URL http://googleresearch.blogspot.com/2015/ 06/inceptionism-going-deeper-into-neural.html", "citeRegEx": "Mordvintsev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordvintsev et al\\.", "year": 2015}, {"title": "Label filters for large scale multilabel classification", "author": ["Niculescu-Mizil", "Alexandru", "Abbasnejad", "Ehsan"], "venue": "In ICML 2015 Workshop on Extreme Classification,", "citeRegEx": "Niculescu.Mizil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Niculescu.Mizil et al\\.", "year": 2015}, {"title": "Submodular multi-label learning", "author": ["Petterson", "James", "Caetano", "Tib\u00e9rio S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Petterson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petterson et al\\.", "year": 2011}, {"title": "Classifier chains for multi-label classification", "author": ["Read", "Jesse", "Pfahringer", "Bernhard", "Holmes", "Geoff", "Frank", "Eibe"], "venue": "Machine learning,", "citeRegEx": "Read et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Read et al\\.", "year": 2011}, {"title": "Learning distributed representations for structured output prediction", "author": ["Srikumar", "Vivek", "Manning", "Christopher D"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srikumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srikumar et al\\.", "year": 2014}, {"title": "An introduction to conditional random fields", "author": ["Sutton", "Charles", "McCallum", "Andrew"], "venue": "Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Tsochantaridis", "Ioannis", "Hofmann", "Thomas", "Joachims", "Thorsten", "Altun", "Yasemin"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Multi-label classification: An overview", "author": ["Tsoumakas", "Grigorios", "Katakis", "Ioannis"], "venue": "Dept. of Informatics, Aristotle University of Thessaloniki,", "citeRegEx": "Tsoumakas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2006}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In CoRR.,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning low-rank label correlations for multi-label classification with missing labels", "author": ["Xu", "Linli", "Wang", "Zhen", "Shen", "Zefan", "Yubo", "Chen", "Enhong"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Large-scale multi-label learning with missing labels", "author": ["Yu", "Hsiang-Fu", "Jain", "Prateek", "Kar", "Purushottam", "Dhillon", "Inderjit S"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["Zhang", "Min-Ling", "Zhou", "Zhi-Hua"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Zheng", "Shuai", "Jayasumana", "Sadeep", "Romera-Paredes", "Bernardino", "Vineet", "Vibhav", "Su", "Zhizhong", "Du", "Dalong", "Huang", "Chang", "Torr", "Philip"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 32, "context": "We consider two principal approaches to structured prediction: (a) as a feed-forward function y = f(x), and (b) using an energy-based viewpoint y = arg miny\u2032 Ex(y) (LeCun et al., 2006).", "startOffset": 164, "endOffset": 184}, {"referenceID": 9, "context": "Feed-forward approaches include, for example, predictors using local convolutions plus a classification layer (Collobert et al., 2011), fully-convolutional networks (Long et al.", "startOffset": 110, "endOffset": 134}, {"referenceID": 34, "context": ", 2011), fully-convolutional networks (Long et al., 2015), or sequenceto-sequence predictors (Vinyals et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 46, "context": ", 2015), or sequenceto-sequence predictors (Vinyals et al., 2014).", "startOffset": 43, "endOffset": 65}, {"referenceID": 30, "context": "In contrast, the energy-based approach may involve non-trivial optimization to perform predictions, and includes, for example, conditional random fields (CRFs) (Lafferty et al., 2001).", "startOffset": 160, "endOffset": 183}, {"referenceID": 32, "context": "For example, in a CRF, the tables of potentials of an undirected graphical model can be parametrized via a deep network applied to x (LeCun et al., 2006; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 133, "endOffset": 197}, {"referenceID": 9, "context": "For example, in a CRF, the tables of potentials of an undirected graphical model can be parametrized via a deep network applied to x (LeCun et al., 2006; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 133, "endOffset": 197}, {"referenceID": 22, "context": "For example, in a CRF, the tables of potentials of an undirected graphical model can be parametrized via a deep network applied to x (LeCun et al., 2006; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 133, "endOffset": 197}, {"referenceID": 44, "context": "The parameters of the network are trained using an adaptation of a structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2004).", "startOffset": 82, "endOffset": 132}, {"referenceID": 36, "context": "using CRFs, have been limited to notably smaller problems than our experiments consider, since the techniques\u2019 computational complexity either grows super-linearly in the number of labels L (Ghamrawi & McCallum, 2005; Finley & Joachims, 2008; Meshi et al., 2010; Petterson & Caetano, 2011), or requires strict assumptions about the dependencies between labels (Read et al.", "startOffset": 190, "endOffset": 289}, {"referenceID": 40, "context": ", 2010; Petterson & Caetano, 2011), or requires strict assumptions about the dependencies between labels (Read et al., 2011; Jasinska & Dembczyski, 2015; Niculescu-Mizil & Abbasnejad, 2015).", "startOffset": 105, "endOffset": 189}, {"referenceID": 26, "context": "In the posterior inference literature, mean-field approaches also consider a relaxation from y to \u0233, where \u0233i would be interpreted as the marginal probability that yi = 1 (Jordan et al., 1999).", "startOffset": 171, "endOffset": 192}, {"referenceID": 30, "context": "There are important parallels between the example SPEN architecture given above and the parametrization of a CRF (Lafferty et al., 2001; Sutton & McCallum, 2011).", "startOffset": 113, "endOffset": 161}, {"referenceID": 24, "context": "(Srikumar & Manning, 2014; Jernite et al., 2015), or using a deep architecture to map x to a table of CRF potentials (LeCun et al.", "startOffset": 0, "endOffset": 48}, {"referenceID": 32, "context": ", 2015), or using a deep architecture to map x to a table of CRF potentials (LeCun et al., 2006).", "startOffset": 76, "endOffset": 96}, {"referenceID": 44, "context": "A popular technique in these settings is the structured support vector machine (SSVM) (Taskar et al., 2004; Tsochantaridis et al., 2004).", "startOffset": 86, "endOffset": 136}, {"referenceID": 4, "context": "However, backpropagation-based prediction has been useful in a variety of deep learning applications, including siamese networks (Bromley et al., 1993), methods for generating adversarial examples (Szegedy et al.", "startOffset": 129, "endOffset": 151}, {"referenceID": 43, "context": ", 1993), methods for generating adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2014), methods for embedding documents as dense vectors (Le & Mikolov, 2014), and successful techniques for image generation and texture synthesis (Mordvintsev et al.", "startOffset": 53, "endOffset": 100}, {"referenceID": 18, "context": ", 1993), methods for generating adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2014), methods for embedding documents as dense vectors (Le & Mikolov, 2014), and successful techniques for image generation and texture synthesis (Mordvintsev et al.", "startOffset": 53, "endOffset": 100}, {"referenceID": 8, "context": "In concurrent work, (Carreira et al., 2015) propose an iterative structured prediction method for human pose estimation, where Ex(y), doesn\u2019t return a number, but instead an increment \u2206(x, y).", "startOffset": 20, "endOffset": 43}, {"referenceID": 20, "context": "An alternative line of work has constructed feed-forward predictors using energy-based models as motivation (Domke, 2013; Hershey et al., 2014; Zheng et al., 2015).", "startOffset": 108, "endOffset": 163}, {"referenceID": 50, "context": "An alternative line of work has constructed feed-forward predictors using energy-based models as motivation (Domke, 2013; Hershey et al., 2014; Zheng et al., 2015).", "startOffset": 108, "endOffset": 163}, {"referenceID": 5, "context": "Modeling improvements use max-margin or ranking losses that directly address the multi-label structure (Elisseeff & Weston, 2001; Godbole & Sarawagi, 2004; Zhang & Zhou, 2006; Bucak et al., 2009).", "startOffset": 103, "endOffset": 195}, {"referenceID": 7, "context": "Correlations between labels can be modeled explicitly using models with low-dimensional embeddings of labels (Ji & Ye, 2009; Cabral et al., 2011; Yu et al., 2014; Xu et al., 2014; Bhatia et al., 2015).", "startOffset": 109, "endOffset": 200}, {"referenceID": 48, "context": "Correlations between labels can be modeled explicitly using models with low-dimensional embeddings of labels (Ji & Ye, 2009; Cabral et al., 2011; Yu et al., 2014; Xu et al., 2014; Bhatia et al., 2015).", "startOffset": 109, "endOffset": 200}, {"referenceID": 47, "context": "Correlations between labels can be modeled explicitly using models with low-dimensional embeddings of labels (Ji & Ye, 2009; Cabral et al., 2011; Yu et al., 2014; Xu et al., 2014; Bhatia et al., 2015).", "startOffset": 109, "endOffset": 200}, {"referenceID": 2, "context": "Correlations between labels can be modeled explicitly using models with low-dimensional embeddings of labels (Ji & Ye, 2009; Cabral et al., 2011; Yu et al., 2014; Xu et al., 2014; Bhatia et al., 2015).", "startOffset": 109, "endOffset": 200}, {"referenceID": 36, "context": "However, these techniques\u2019 computational complexity grows super-linearly in L (Ghamrawi & McCallum, 2005; Finley & Joachims, 2008; Meshi et al., 2010; Petterson & Caetano, 2011) , or requires practitioners to impose strict assumption about the dependencies between labels (Read et al.", "startOffset": 78, "endOffset": 177}, {"referenceID": 40, "context": ", 2010; Petterson & Caetano, 2011) , or requires practitioners to impose strict assumption about the dependencies between labels (Read et al., 2011; Jasinska & Dembczyski, 2015; Niculescu-Mizil & Abbasnejad, 2015).", "startOffset": 129, "endOffset": 213}, {"referenceID": 21, "context": "Our parametrization of the label energy network (5) in terms of linear measurements of the labels is inspired by prior approaches using compressed sensing and error-correcting codes for multi-label classification (Hsu et al., 2009; Hariharan et al., 2010; Kapoor et al., 2012).", "startOffset": 213, "endOffset": 276}, {"referenceID": 19, "context": "Our parametrization of the label energy network (5) in terms of linear measurements of the labels is inspired by prior approaches using compressed sensing and error-correcting codes for multi-label classification (Hsu et al., 2009; Hariharan et al., 2010; Kapoor et al., 2012).", "startOffset": 213, "endOffset": 276}, {"referenceID": 27, "context": "Our parametrization of the label energy network (5) in terms of linear measurements of the labels is inspired by prior approaches using compressed sensing and error-correcting codes for multi-label classification (Hsu et al., 2009; Hariharan et al., 2010; Kapoor et al., 2012).", "startOffset": 213, "endOffset": 276}, {"referenceID": 33, "context": "When yi = 0, they are treated as missing data, whose values can be imputed using assumptions about the rank (Lin et al., 2014) or sparsity (Bucak et al.", "startOffset": 108, "endOffset": 126}, {"referenceID": 6, "context": ", 2014) or sparsity (Bucak et al., 2011; Agrawal et al., 2013) of the matrix of training labels.", "startOffset": 20, "endOffset": 62}, {"referenceID": 0, "context": ", 2014) or sparsity (Bucak et al., 2011; Agrawal et al., 2013) of the matrix of training labels.", "startOffset": 20, "endOffset": 62}, {"referenceID": 33, "context": "For example, the approach of (Lin et al., 2014) achieves 44.", "startOffset": 29, "endOffset": 47}, {"referenceID": 44, "context": "LR: the low-rank-weights method of Yu et al. (2014). All results besides MLP and SPEN, are taken from Lin et al.", "startOffset": 35, "endOffset": 52}, {"referenceID": 30, "context": "All results besides MLP and SPEN, are taken from Lin et al. (2014). We report the \u2018example averaged\u2019 F1 measure.", "startOffset": 49, "endOffset": 67}, {"referenceID": 30, "context": "All results besides MLP and SPEN, are taken from Lin et al. (2014). We report the \u2018example averaged\u2019 F1 measure. For Bibtex and Delicious, we tune parameters by first jack-knifing a separate train-test split. For Bookmarks, we use the same train-dev-test split as Lin et al. (2014). For SPENs, we obtain predictions by rounding \u0233i above a threshold tuned on held-out data.", "startOffset": 49, "endOffset": 282}, {"referenceID": 36, "context": "the 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest label space fit using a CRF in Finley & Joachims (2008) and Meshi et al. (2010). Finley & Joachims (2008) analyze the effects of inexact prediction on SSVM training and on test-time prediction.", "startOffset": 136, "endOffset": 156}, {"referenceID": 36, "context": "the 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest label space fit using a CRF in Finley & Joachims (2008) and Meshi et al. (2010). Finley & Joachims (2008) analyze the effects of inexact prediction on SSVM training and on test-time prediction.", "startOffset": 136, "endOffset": 182}, {"referenceID": 36, "context": "the 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest label space fit using a CRF in Finley & Joachims (2008) and Meshi et al. (2010). Finley & Joachims (2008) analyze the effects of inexact prediction on SSVM training and on test-time prediction. Table 3 considers greedy prediction, loopy belief propagation, exact prediction using an ILP solver, solving the LP relaxation, and SPENs, where the same technique is used at train and test time. All results, besides SPENs, are from Finley & Joachims (2008), which also considers cases where different methods are used in train vs.", "startOffset": 136, "endOffset": 528}, {"referenceID": 36, "context": "the 14-label yeast dataset (Elisseeff & Weston, 2001), which is the largest label space fit using a CRF in Finley & Joachims (2008) and Meshi et al. (2010). Finley & Joachims (2008) analyze the effects of inexact prediction on SSVM training and on test-time prediction. Table 3 considers greedy prediction, loopy belief propagation, exact prediction using an ILP solver, solving the LP relaxation, and SPENs, where the same technique is used at train and test time. All results, besides SPENs, are from Finley & Joachims (2008), which also considers cases where different methods are used in train vs. test. We report hamming error, using 10-fold cross validation. A key argument of Finley & Joachims (2008) is that SSVM training is more effective when the traintime inference method will not under-generate margin violations.", "startOffset": 136, "endOffset": 708}, {"referenceID": 35, "context": "For example, we can apply the technique of Maclaurin et al. (2015) to update the model parameters by differentiating, with respect to the model parameters \u03a8, the process of performing iterative gradient-based optimization with respect to \u0233.", "startOffset": 43, "endOffset": 67}], "year": 2017, "abstractText": "We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using backpropagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. One natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction problems. We are able to apply SPENs to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. Overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of the outputs. Our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feed-forward and iterative structured prediction techniques.", "creator": "LaTeX with hyperref package"}}}