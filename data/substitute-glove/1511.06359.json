{"id": "1511.06359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning and Applications to Inverse Problems", "abstract": "Features focus later normally competence, though using enzyme guide model, once been heavily reputations under object processing even computer reflection. However, polymerase definitions learners indirect NP - getting normally interface taking selling besides steps. Recently, sparsifying explore learning same interest because entire use optimal and market - important solution. In could work, i opportunities a techniques should learning of Flipping and Rotation Invariant Sparsifying Transform, slogan FRIST, it even must common comparing adding contain modulations with notably geometrical precise. The amendments folding learners finite involves simple close - own solutions. Preliminary conducted show also usefulness of semantic shade discrete initially FRIST out image compatibility, slave-trade but inpainting with promising performances.", "histories": [["v1", "Thu, 19 Nov 2015 20:55:49 GMT  (2990kb)", "https://arxiv.org/abs/1511.06359v1", "11 pages (including the references and supplementary material), under review as conference paper at ICLR 2016"], ["v2", "Fri, 8 Jan 2016 06:43:38 GMT  (3720kb,D)", "http://arxiv.org/abs/1511.06359v2", "11 pages (including the references and supplementary material), under review as conference paper at ICLR 2016"], ["v3", "Tue, 17 May 2016 03:54:47 GMT  (2220kb,D)", "http://arxiv.org/abs/1511.06359v3", "25 pages (including the references and supplementary material)"], ["v4", "Mon, 16 Oct 2017 02:42:20 GMT  (1806kb,D)", "http://arxiv.org/abs/1511.06359v4", "Published in Inverse Problems"]], "COMMENTS": "11 pages (including the references and supplementary material), under review as conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["bihan wen", "saiprasad ravishankar", "yoram bresler"], "accepted": false, "id": "1511.06359"}, "pdf": {"name": "1511.06359.pdf", "metadata": {"source": "CRF", "title": "FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning and Applications", "authors": ["Bihan Wen", "Saiprasad Ravishankar", "Yoram Bresler"], "emails": ["bwen3@illinois.edu,", "ravisha@umich.edu,", "ybresler@illinois.edu"], "sections": [{"heading": null, "text": "dictionary model, have been heavily exploited in signal processing and computer vision. However, synthesis dictionary learning typically involves NP-hard sparse coding and expensive learning steps. Recently, sparsifying transform learning received interest for its cheap computation and its optimal updates in the alternating algorithms. In this work, we develop a methodology for learning Flipping and Rotation Invariant Sparsifying Transforms, dubbed FRIST, to better represent natural images that contain textures with various geometrical directions. The proposed alternating FRIST learning algorithm involves efficient optimal updates. We provide a convergence guarantee, and demonstrate the empirical convergence behavior of the proposed FRIST learning approach. Preliminary experiments show the promising performance of FRIST learning for sparse image representation, segmentation, denoising, robust inpainting, and compressed sensing-based magnetic resonance image reconstruction.\nKeywords: Sparsifying transform learning, Dictionary learning, Machine learning, Image denoising, Inpainting, Magnetic resonance imaging, Compressed sensing, Machine learning"}, {"heading": "1. Introduction", "text": ""}, {"heading": "1.1. Sparse Modeling", "text": "Sparse representation of natural signals in a certain transform domain or dictionary has been widely exploited. Various sparse signal models, such as the synthesis model [6, 12] and the transform model [27] have been studied. The popular synthesis model suggests that a signal y \u2208 Rn can be sparsely represented as y = Dx + \u03b7, where D \u2208 Rn\u00d7m is a synthesis dictionary, x \u2208 Rm is a sparse code, and \u03b7 is a small approximation error in the signal domain. Synthesis dictionary learning methods [13, 5] that adapt the dictionary\nar X\niv :1\n51 1.\n06 35\n9v 4\n[ cs\n.L G\nbased on training data typically involve a synthesis sparse coding step which is often NPhard [10], so that approximate solutions [23, 21, 8] are widely used. Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4]. For example, the well-known K-SVD method [5] generalizes the K-means clustering process to a dictionary learning algorithm, and alternates between updating the sparse codes of training signals (sparse coding step) and the dictionary (dictionary or codebook update step). K-SVD updates both the dictionary atoms (columns) and the non-zero entries in the sparse codes (with fixed support) in the dictionary update step using singular value decompositions (SVD). However, the dictionary learning algorithms such as K-SVD are usually computationally expensive for large-scale problems. Moreover, methods such as KSVD lack convergence guarantees, and can get easily caught in local minima, or saddle points [38].\nThe alternative transform model suggests that the signal y is approximately\nsparsifiable using a transform W \u2208 Rm\u00d7n, i.e., Wy = x + e, with x \u2208 Rm sparse in some sense and e a small approximation error in the transform domain (rather than in the signal domain). It is well-known that natural images are sparsifiable by analytical transforms such as the discrete cosine transform (DCT), or wavelet transform [20]. Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26]. Alternating minimization algorithms for learning SST have been proposed with cheap and optimal updates [35].\nSince SST learning is restricted to one adaptive square transform for all the data,\nthe diverse patches of natural images may not be sufficiently sparsified in the SST model. Recent work focused on learning a union of unstructured sparsifying transforms [44, 43], dubbed OCTOBOS (for OverComplete TransfOrm with BlOck coSparsity constraint \u2013 cf. [44]), to sparsify images with diverse contents, features and textures. Given a signal y \u2208 Rn and a union of transforms {Wk}Kk=1, where each Wk \u2208 Rm\u00d7n, the OCTOBOS model selects the best matching transform for y as the one providing the minimum transform-domain modeling error. The OCTOBOS sparse coding problem is the following:\n(P0) min 1\u2264k\u2264K min xk \u2225\u2225Wk y \u2212 xk\u2225\u222522 s.t. \u2225\u2225xk\u2225\u22250 \u2264 s \u2200 k, where xk denotes the sparse representation for y in the transform Wk, the `0 \u201cnorm\u201d counts the number of non-zeros in a vector, and s is a given sparsity level. However, learning such an unstructured OCTOBOS model [44] (that has many free parameters) especially from noisy or limited data could suffer from overfitting to noise/artifacts, thereby degrading performance in various applications and inverse problem settings.\nInstead, in this paper, we consider the use of transformation symmetries to constrain\nthe multiple learned transforms, thus reducing the number of free parameters and avoiding overfitting. While previous works exploited transformation symmetries in\nsynthesis model sparse coding [45], and applied rotational operators with analytical transforms [53], the usefulness of the rotational invariance property in learning adaptive sparse signal models has not been explored. Here, we propose a Flipping and Rotation Invariant Sparsifying Transform (FRIST) learning scheme, and show that it can provide better sparse representations by capturing the \u201coptimal\u201d orientations of patches in natural images. As such, it serves as an effective regularizer for image recovery in various inverse problems. Preliminary experiments in this paper show the usefulness of adaptive sparse representation by FRIST for sparse image representation, segmentation, denoising, robust inpainting, and compressed sensing-based magnetic resonance image (MRI) reconstruction with promising performances."}, {"heading": "1.2. Highlights and Organization", "text": "We summarize some important features and contributions of this work as follows:\n\u2022 We propose a FRIST model that exploits the flipping and rotation invariance property of natural images, i.e., image patches typically contain edges and\nfeatures at different orientations, and hence a (single) common transform could be learned for (optimally) flipped or rotated versions of patches. Compared to a general overcomplete synthesis dictionary model or OCTOBOS, FRIST is much more constrained (with the constraints reflecting commonly observed image properties), which proves beneficial in various applications and inverse problem settings involving limited or highly corrupted measurements. \u2022 We propose a novel problem formulation and an efficient algorithm for learning FRIST. All steps of our alternating minimization algorithm involve simple optimal\nupdates. We also provide a convergence analysis of the proposed FRIST learning algorithm. \u2022 We propose various adaptive FRIST-based inverse problem formulations along with algorithms for these problems, including for image denoising, inpainting,\nand compressed sensing-based image reconstruction. The proposed FRIST-based algorithms outperform the previous adaptive sparse modeling methods.\nThe adaptive FRIST algorithms in this work involve clustering the image patches.\nSuch clustering naturally arises in the proposed alternating minimization algorithms. There are other prior works such as the Structured Sparse Model Selection (SSMS) [50, 51] approach that also involve clustering, but the sub-dictionary in each cluster in SSMS is obtained by conventional Principal Component Analysis (PCA). Though both SSMS and FRIST impose more structure to reduce the degrees of freedom in the underlying model, the proposed FRIST learning involves a variational formulation that automatically enables joint sparse coding, clustering (based on flipping and rotations), and (a single, small parent) transform learning from training signals. Moreover, our FRIST learning algorithm comes with convergence guarantees.\nWe organize the rest of the paper as follows. Section 2 introduces the FRIST\nmodel and the learning formulation. In Section 3, we present efficient FRIST learning\nalgorithms to solve the proposed learning problem along with convergence analysis. Section 4 describes various applications based on FRIST learning, including image denoising, inpainting and compressed sensing-based (MR) image reconstruction. Section 5 provides experimental results demonstrating the promise of FRIST learning, including for image segmentation, sparse representation, denoising, inpainting, and MRI."}, {"heading": "2. FRIST Model and Its Learning Formulation", "text": ""}, {"heading": "2.1. FRIST Model", "text": "The learning of the sparsifying transform model [34] has been proposed recently. Extending this approach, we propose a FRIST model that first applies a flipping and rotation (FR) operator \u03a6 \u2208 Rn\u00d7n to a signal y \u2208 Rn, and models \u03a6y as approximately sparsifiable by some sparsifying transform W \u2208 Rm\u00d7n, i.e., W\u03a6y = x+ e, with x \u2208 Rm sparse in some sense, and e is a small deviation term. A finite set of flipping and rotation operators {\u03a6k}Kk=1 is considered, and the sparse coding problem in the FRIST model is as follows:\n(P1) min 1\u2264k\u2264K min xk \u2225\u2225W \u03a6k y \u2212 xk\u2225\u222522 s.t. \u2225\u2225xk\u2225\u22250 \u2264 s \u2200 k. Thus, xk denotes the sparse code of \u03a6k y in the transform W domain, with maximum sparsity s. Equivalently, the optimal x\u0302k\u0302 (achieving the minimum over all k) is called the optimal sparse code in the FRIST domain. We further decompose the FR matrix as \u03a6k , Gq F , where F can be either an identity matrix, or (for 2D signals) a left-toright flipping permutation matrix. Though there are various methods of formulating the rotation operator G with arbitrary angles [16, 15], rotating image patches by an angle \u03b8 that is not a multiple of 90\u25e6 requires interpolation, and may result in misalignment with the pixel grid. Here, we adopt the matrix Gq , G(\u03b8q) that permutes the pixels in an image patch approximating rotation by angle \u03b8q without interpolation. Constructions of such {Gq} have been proposed before [24, 28, 53]. With such implementation, the number of possible permutations {Gq} denoted by Q\u0303, is finite and grows linearly with the signal dimension n. Accounting for the flipping operation, the total number of possible FR operators is K\u0303 = 2Q\u0303. In practice, one can select a subset {\u03a6k}Kk=1, containing K < K\u0303 of FR candidates, from which the optimal \u03a6\u0302 = \u03a6k\u0302 is chosen in (P1). For each \u03a6k, the optimal sparse code x\u0302k in Problem (P1) can be solved exactly as x\u0302k = Hs(W\u03a6ky), where Hs(\u00b7) is the projector onto the s-`0 ball [32], i.e., Hs(b) zeros out all but the s elements of largest magnitude in b \u2208 Rm. The optimal FR operator \u03a6k\u0302 is selected to provide the smallest sparsification (modeling) error \u2016W \u03a6k y \u2212Hs(W\u03a6ky)\u201622 over k in Problem (P1).\nThe FRIST model can be interpreted as a structured union-of-transforms model,\nor a structured OCTOBOS model [44], i.e., compared to OCTOBOS, FRIST is much more constrained, with fewer free parameters. In particular, the OCTOBOS model involves a collection (or union) of sparsifying transforms {Wk}Kk=1 such that for each\ncandidate signal, there is a transform in the collection that is best matched (or that provides the lowest sparsification error) to the signal. The FRIST model involves a collection of transforms {W\u03a6k}Kk=1 (as in Problem (P1)) that are related to each other by rotation and flip operators (and involving a single parent transform W ). The transforms in the collection all share a common transform W . We call the shared common transform W the parent transform, and each generated Wk = W\u03a6k is called a child transform. Clearly, the collection of transforms in FRIST is more constrained than in the OCTOBOS model. The constraints that are imposed by FRIST are devised to reflect commonly observed properties of natural image patches, i.e., image patches tend to have edges and features at various orientations, and optimally rotating (or flipping) each patch would allow it to be well-sparsified by a common sparsifying transform W (as in (P1)). This property turns out to be useful in inverse problems such as denoising and inpainting, preventing the overfitting of the model in the presence of limited or highly corrupted data or measurements.\nProblem (P1) is similar to the OCTOBOS sparse coding problem [44], where each\nWk = W\u03a6k corresponds to a block of OCTOBOS. Similar to the clustering procedure in OCTOBOS, Problem (P1) matches a signal y to a particular child transform Wk with its directional FR operator \u03a6k. Thus, FRIST is potentially capable of automatically clustering a collection of signals (e.g., image patches), but according to their geometric orientations. When the parent transform W is unitary, FRIST is also equivalent to an overcomplete synthesis dictionary with block sparsity [52], with W Tk denoting the kth block of the equivalent overcomplete dictionary."}, {"heading": "2.2. FRIST Learning Formulation", "text": "Generally, the parent transform W can be overcomplete [33, 44, 26]. In this work, we restrict ourselves to learning FRIST with a square parent transform W (i.e., m = n), which leads to a highly efficient learning algorithm with optimal updates. Note that the FRIST model is still overcomplete, even with a square parent W , because of the additional FR operators. Given the training data Y \u2208 Rn\u00d7N , we formulate the FRIST learning problem as follows:\n(P2) min W,{Xi},{Ck} K\u2211 k=1 \u2211 i\u2208Ck \u2016W\u03a6kYi \u2212Xi\u201622 + \u03bbQ(W )\ns.t. \u2016Xi\u20160 \u2264 s \u2200 i, {Ck} \u2208 \u0393\nwhere {Xi} represent the FRIST-domain sparse codes of the corresponding columns {Yi} of Y , and X \u2208 Rn\u00d7N with columns Xi denotes the transform sparse code matrix of Y . The {Ck}Kk=1 indicate a clustering of the signals {Yi} N i=1 such that Cj contains the indices of signals in the jth cluster (corresponding to the child transform W\u03a6j), and each signal Yi is associated exactly with one FR operator \u03a6k. The set \u0393 is the set of all possible partitions (into K subsets) of the set of integers {1, 2, ..., N}, which enforces all of the Ck\u2019s to be disjoint [44].\nProblem (P2) is to minimize the FRIST learning objective that includes the modeling or sparsification error \u2211K\nk=1 \u2211 i\u2208Ck \u2016W\u03a6kYi \u2212Xi\u2016 2 2 for Y as well as the\nregularizer Q(W ) = \u2212 log |detW | + \u2016W\u20162F to prevent trivial solutions [34]. Here, the negative log-determinant penalty \u2212 log |detW | enforces full rank on W , and the \u2016W\u20162F penalty helps remove a \u2018scale ambiguity\u2019 in the solution. The regularizer Q(W ) fully controls the condition number and scaling of the learned parent transform [34]. The regularizer weight \u03bb is chosen as \u03bb = \u03bb0 \u2016Y \u20162F with \u03bb0 > 0, in order to scale with the first term in (P2). Previous works [34] showed that the condition number and spectral norm of the optimal parent transform W\u0302 approach 1 and 1/ \u221a\n2 respectively, as \u03bb0 \u2192\u221e in (P2). Problem (P2) imposes an `0 sparsity constraint \u2016Xi\u20160 \u2264 s on the sparse code of each signal or image patch. One can also impose an overall (or aggregate) sparsity constraint on the entire sparse code matrix X to allow variable sparsity levels across the signals (see Section 4.3). Alternatively, a sparsity penalty method can be used, instead of imposing sparsity constraints, which also leads to efficient algorithms (see Section 4.2). We will demonstrate the use of various sparsity methods in Section 4.\nNote that in the overcomplete synthesis dictionary model, sparse coding with an\n`0 \u201cnorm\u201d constraint is NP-hard in general, and convex `1 relaxations of the synthesis sparse coding problem have been popular, and solving such an `1 (relaxed) problem is known to provide the sparsest solution under certain conditions on the dictionary. On the other hand, in the sparsifying transform model (including in the FRIST model), the sparse coding problem can be solved exactly and cheaply by thresholding operations, irrespective of whether an `0 penalty or constraint (resulting in hard thresholding-type solution) or an `1 penalty (resulting in soft thresholding solution) is used. Thus there is not a computational benefit for employing the `1 norm in case of the transform model. More importantly, in practice, we have observed that transform learning with `0 sparsity leads to better performance in applications compared to `1 norm-based learning."}, {"heading": "3. FRIST Learning Algorithm and Convergence Analysis", "text": ""}, {"heading": "3.1. FRIST Learning Algorithm", "text": "We propose an efficient algorithm for solving (P2), which alternates between a sparse coding and clustering step, and a transform update step.\nSparse Coding and Clustering. Given the training matrix Y , and a fixed parent\ntransform W , we solve the following Problem (P3) for the sparse codes and clusters:\n(P3) min {Ck},{Xi} K\u2211 k=1 \u2211 i\u2208Ck \u2016W\u03a6kYi \u2212Xi\u201622 s.t. \u2016Xi\u20160 \u2264 s \u2200 i, {Ck} \u2208 \u0393.\nThe modeling error \u2016W\u03a6kYi \u2212Xi\u201622 serves as the clustering measure corresponding to signal Yi, where the best sparse code corresponding to FR permutation \u03a6k \u2021\n\u2021 The FR operator is \u03a6k = GqF , where both Gq and F are permutation matrices. Therefore the\nis Xi = Hs(W\u03a6kYi). Problem (P3) is clearly equivalent to finding the \u201coptimal\u201d FR permutation \u03a6k\u0302i independently for each data vector Yi by solving the following optimization problems:\nmin 1\u2264k\u2264K\n\u2016W\u03a6kYi \u2212Hs(W\u03a6kYi)\u201622 \u2200 i (1)\nwhere the minimization over k for each Yi determines the optimal \u03a6k\u0302i , or the cluster Ck\u0302i to which Yi belongs. The corresponding optimal sparse code for Yi in (P3) is thus X\u0302i = Hs(W\u03a6k\u0302iYi). Given the sparse code \u00a7, one can also easily recover a least squares estimate of each signal as Y\u0302i = \u03a6\nT k\u0302i W\u22121X\u0302i. Since the \u03a6k\u2019s are permutation matrices,\napplying and computing \u03a6Tk (which is also a permutation matrix) is cheap. Transform Update Step. Here, we solve for W in (P2) with fixed {Ck} and {Xi}, which leads to the following problem:\n(P4) min W \u2225\u2225\u2225WY\u0303 \u2212X\u2225\u2225\u22252 F + \u03bbQ(W )\nwhere Y\u0303 = [ \u03a6k\u03021Y1 | \u03a6k\u03022Y2 | ... | \u03a6k\u0302NYN ] contains signals after applying their optimal (as determined in the preceding sparse coding and clustering step) FR operations, and the columns of X are the corresponding sparse codes Xi\u2019s. Problem (P4) has a simple solution involving a singular value decomposition (SVD), which is similar to the transform update step in SST [35]. We first decompose the positive-definite matrix Y\u0303 Y\u0303 T + \u03bbIn = UU T (e.g., using Cholesky decomposition). Then, denoting the full singular value decomposition (SVD) of the matrix U\u22121Y\u0303 XT = S\u03a3V T , where S,\u03a3, V \u2208 Rn\u00d7n, an optimal transform W\u0302 in (P4) is\nW\u0302 = 0.5V ( \u03a3 + (\u03a32 + 2\u03bbIn) 1 2 ) STU\u22121 (2)\nwhere (\u00b7) 12 above denotes the positive definite square root, and In is the n\u00d7 n identity. Initialization Insensitivity and Cluster Elimination. Unlike the previously proposed OCTOBOS learning algorithm [44], which requires initialization of the clusters using heuristic methods such as K-means, the FRIST learning algorithm only needs initialization of the parent transform W . In Section 5.1, numerical results demonstrate the fast convergence of the proposed FRIST learning algorithm, which can be insensitive to the parent transform initialization. In practice, we apply a heuristic cluster elimination strategy in the FRIST learning algorithm, to select the desired K FR operators. In the first iteration, all possible FR operators \u03a6k\u2019s [24, 53] (i.e., all possible child transforms Wk\u2019s) are considered for sparse coding and clustering. After each iteration, the learning algorithm eliminates half of the operators with smallest cluster sizes, until the number of selected FR operators drops to K, which only takes a few\ncomposite operator \u03a6k is a permutation matrix. \u00a7 The stored sparse code includes the value of X\u0302i, as well as the membership index k\u0302i which adds just log2K bits to the code storage.\niterations. For the rest of the iterations, the algorithm only considers the selected K \u03a6k\u2019s in the sparse coding and clustering steps.\nComputational Cost Analysis. The sparse coding and clustering step computes\nthe optimal sparse codes and clusters, with O(Kn2N) cost. In the transform update step, we compute the optimal solution for the square parent transform in (P4). The cost of computing this solution scales as O(n2N) (dominated by matrix-matrix products), assuming N n, which is cheaper than the sparse coding and clustering step. Thus, the overall computational cost per iteration of FRIST learning using the proposed alternating algorithm scales as O(Kn2N), which is typically lower than the cost per iteration of the overcomplete K-SVD learning algorithm [5], with the number of dictionary atoms m = Kn, and synthesis sparsity s \u221d n. FRIST learning also converges quickly in practice as illustrated later in Section 5.1. The computational costs periteration of SST, OCTOBOS, FRIST, and KSVD learning are summarized in Table 1."}, {"heading": "3.2. Convergence Analysis", "text": "We analyze the convergence behavior of the proposed FRIST learning algorithm for (P2), assuming that every step in the algorithms (such as SVD) is computed exactly.\nNotation. Problem (P2) is formulated with sparsity constraints (for each Xi),\nwhich is equivalent to an unconstrained formulation with sparsity barrier penalties \u03c6(Xi) (which equals +\u221e when the constraint is violated, and is zero otherwise). Thus, the objective function of Problem (P2) can be rewritten as\nf (W,X,\u039b) = K\u2211 k=1 \u2211 i\u2208Ck { \u2016W\u03a6kYi \u2212Xi\u201622 + \u03c6(Xi) } + \u03bbQ(W ) (3)\nwhere \u039b \u2208 R1\u00d7N is the vector whose ith element \u039bi \u2208 {1, .., K} denotes the cluster label corresponding to the signal Yi, i.e., C\u039bi . We use {W t, X t,\u039bt} to denote the output in each iteration (consisting of the sparse coding and clustering, and transform update steps) t, generated by the proposed FRIST learning algorithm.\nMain Results. Since FRIST can be interpreted as a structured OCTOBOS,\nthe convergence results for the FRIST learning algorithm take a form similar to those obtained for the OCTOBOS learning algorithm [44] in our recent work. The convergence\nresult for the FRIST learning algorithm for (P2), is summarized in the following theorem and corollaries.\nTheorem 1 For each initialization (W 0, X0,\u039b0), the following results hold:\n(i) The objective sequence {f t = f(W t, X t,\u039bt)} in the FRIST learning algorithm is monotone decreasing, and converges to a finite value, f \u2217 = f \u2217(W 0, X0,\u039b0).\n(ii) The iterate sequence {W t, X t,\u039bt} is bounded, with all of its accumulation points equivalent, i.e., achieving the exact same value f \u2217.\n(iii) Every accumulation point (W,X,\u039b) of the iterate sequence satisfies the following\npartial global optimality conditions:\n(X,\u039b) \u2208 arg min X\u0303,\u039b\u0303\nf ( W, X\u0303, \u039b\u0303 ) (4)\nW \u2208 arg min W\u0303\nf ( W\u0303 ,X,\u039b ) (5)\n(iv) For each accumulation point (W,X,\u039b), there exists = (W ) > 0 such that\nf (W + dW,X + \u2206X,\u039b) \u2265 f (W,X,\u039b) = f \u2217, (6)\nwhich holds for all dW \u2208 Rn\u00d7n satisfying \u2016dW\u2016F \u2264 , and all \u2206X \u2208 Rn\u00d7N satisfying \u2016\u2206X\u2016\u221e < mink mini\u2208Ck {\u03c8s(W\u03a6kYi) : \u2016W\u03a6kYi\u20160 > s}. Here, we define the infinity norm of matrix \u2206X as \u2016\u2206X\u2016\u221e , maxi,j |\u2206Xi,j|, and the operator \u03c8s(\u00b7) returns the sth largest magnitude in a vector.\nConclusion (iv) provides a partial local optimality condition for each accumulation\npoint with respect to (W,X), where the local perturbation dW in Equation (6) is sufficiently small, and \u2206X is specified by a finite region, which is determined by a scalar \u03ba that limits the amplitudes of entries in \u2206X (i.e., \u2016\u2206X\u2016\u221e < \u03ba). Here \u03ba = mink \u03bak, and each \u03bak = mini\u2208Ck {\u03c8s(W\u03a6kYi) : \u2016W\u03a6kYi\u20160 > s} is computed by (i) choosing the vectors with sparsity > s from {W\u03a6kYi} where i \u2208 Ck, (ii) selecting the sth largest magnitude in each of those vectors, and (iii) returning the smallest of those values. If there are no vectors with sparsity of W\u03a6kYi greater than s in a class k, then that \u03bak is set as \u221e.\nCorollary 1 For a particular initial (W 0, X0,\u039b0), the iterate sequence in the FRIST learning algorithm converges to an equivalence class of accumulation points, which are also partial minimizers satisfying (4), (5), and (6).\nCorollary 2 The iterate sequence {W t, X t,\u039bt} in the FRIST learning algorithm is globally convergent (i.e., it converges from any initialization) to the set of partial minimizers of the objective f (W,X,\u039b).\nFor reasons of space, we only provide an outline of proofs. The conclusion (i) in\nTheorem 1 is obvious, as the proposed alternating algorithm solves the sub-problem in each step exactly. The proof of Conclusion (ii) follows the same arguments as in the proofs in Lemma 3 and Lemma 5 in [44]. In Conclusion (iii), Condition (4) can be proved using the arguments for Lemma 7 from [44], while Condition (5) can be proved with the arguments for Lemma 6 from [35]. The last conclusion in Theorem 1 can be shown using similar arguments as from the proof of Lemma 9 in [35].\nTheorem 1 and Corollaries 1 and 2 establish that with any initialization\n(W 0, X0,\u039b0), the iterate sequence {W t, X t,\u039bt} generated by the FRIST learning algorithm converges to an equivalence class (corresponding to a common objective value f \u2217 \u2013 that may depend on initialization) of partial minimizers of the objective. No assumptions are made about the initialization to establish these results. We leave the investigation of stronger convergence results (e.g., convergence to global minima) with additional assumptions including on the algorithm initialization or using a probabilistic analysis framework, to future work."}, {"heading": "4. Applications", "text": "Natural, or biomedical images typically contain a variety of directional features and edges. Thus the FRIST model is particularly appealing for applications in image processing and inverse problems. In this section, we consider three such applications, namely image denoising, image inpainting, and blind compressed sensing (BCS)-based magnetic resonance imaging (MRI)."}, {"heading": "4.1. Image Denoising", "text": "Image denoising is one of the most fundamental inverse problems in image processing. The goal is to reconstruct a 2D image represented as a vector y \u2208 RP , from its measurement z = y+h, corrupted by a noise vector h. Various denoising algorithms have been proposed recently, with state-of-the-art performance [49, 9]. Similar to previous dictionary and transform learning based image denoising methods [11, 44], we propose the following patch-based image denoising formulation using FRIST learning:\n(P5) min W,{yi,xi,Ck} K\u2211 k=1 \u2211 i\u2208Ck { \u2016W\u03a6kyi \u2212 xi\u201622 + \u03c4 \u2016Ri z \u2212 yi\u2016 2 2 } + \u03bbQ(W )\ns.t. \u2016xi\u20160 \u2264 si \u2200 i, {Ck} \u2208 \u0393\nwhere Ri \u2208 Rn\u00d7P denotes the patch extraction operator, i.e., Riz \u2208 Rn represents the ith overlapping patch of the corrupted image z as a vector. We assume N overlapping patches in total. The data fidelity term \u03c4 \u2016Ri z \u2212 yi\u201622 measures the discrepancy between the observed patch Riz and the (unknown) noiseless patch yi, and uses a weight \u03c4 = \u03c40/\u03c3 that is inversely proportional to the given noise standard deviation \u03c3 [11, 35], and \u03c40 > 0. The vector xi \u2208 Rn represents the sparse code of yi in the FRIST domain, with an a\npriori unknown sparsity level si. We follow the previous SST-based and OCTOBOSbased denoising methods [32, 44], and impose a sparsity constraint on each yi.\nWe propose a simple iterative denoising algorithm based on (P5). Each iteration\ninvolves the following steps: (i) sparse coding and clustering, (ii) sparsity level update, and (iii) transform update. Once the iterations complete, we have a denoised image reconstruction step. We initialize the {yi} in (P5) using the noisy image patches {Riz}. Step (i) is the same as described in Section 3.1. We then update the sparsity levels si for all i, similar to the SST learning-based denoising algorithm [32]. With fixed W and clusters {Ck}, we observe the solution for yi (i \u2208 Ck) in (P5) in the least squares sense,\nyi = \u03a6 T k\n[\u221a \u03c4 I\nW\n]\u2020 [ \u221a \u03c4 vi\nHsi(Wvi)\n] = G1vi +G2Hsi(Wvi) (7)\nwhere G1 and G2 are appropriate matrices in the above decomposition, and vi , \u03a6kRi z are the rotated/flipped noisy patches, which can be pre-computed in each iteration. We choose the optimal si to be the smallest integer that makes the reconstructed yi in (7) satisfy the error condition \u2016Riz \u2212 yi\u201622 \u2264 nC2\u03c32, where C is a constant parameter [32]. Once the sparsity level update (step (ii)) is completed, we proceed to the transform update based on the method in Section 3.1. The algorithm alternates between steps (i)-(iii) for a fixed number of iterations, and eventually the denoised image patches {yi} are obtained using (7). Each pixel in the reconstructed patch is projected onto the underlying intensity range (image pixel is typically stored as 8-bit integer, which corresponds to the intensity range [0, 255]). The denoised image is reconstructed by averaging the overlapping denoised patches at their respective image locations.\nFor improved denoising, the algorithm for (P5) is repeated for several passes by\nreplacing z with the most recent denoised image estimate in each pass. The noise standard deviation \u03c3 decreases gradually in each such pass, and is found (tuned) empirically [44]."}, {"heading": "4.2. Image Inpainting", "text": "The goal of image inpainting is to recover missing pixels in an image. The given image measurement, with missing pixel intensities set to zero, is denoted as z = \u039ey+ \u03b5, where \u03b5 is the additive noise on the available pixels, and \u039e \u2208 RP\u00d7P is a diagonal binary matrix with zeros at locations corresponding to missing pixels. We propose the following patchbased image inpainting formulation using FRIST learning:\n(P6) min W,{yi,xi,Ck} K\u2211 k=1 \u2211 i\u2208Ck { \u2016W\u03a6kyi \u2212 xi\u201622 + \u03c4 2 \u2016xi\u20160 + \u03b3 \u2016Piyi \u2212 zi\u2016 2 2 } + \u03bbQ(W )\nwhere zi = Riz and yi = Riy. The diagonal binary matrix Pi \u2208 Rn\u00d7n captures the available (non-missing) pixels in zi. The sparsity penalty \u03c4\n2 \u2016xi\u20160 is used, which leads to an efficient algorithm. The fidelity term \u03b3 \u2016Piyi \u2212 zi\u201622 for the ith patch has the\ncoefficient \u03b3 that is chosen inversely proportional to the noise standard deviation \u03c3 (in the measured pixels). The parameter \u03c4 is chosen proportional to the number of pixels that are missing in z.\nOur proposed iterative algorithm for solving (P6) involves the following steps: (i)\nsparse coding and clustering, and (ii) transform update. Once the iterations complete, we have a (iii) patch reconstruction step. The sparse coding problem with a sparsity penalty has a closed-form solution [37], and thus Step (i) is equivalent to solving the following clustering problem:\nmin 1\u2264k\u2264K\n\u2016W\u03a6kyi \u2212 T\u03c4 (W\u03a6kyi)\u201622 + \u03c4 2 \u2016T\u03c4 (W\u03a6kyi)\u20160 \u2200 i (8)\nwhere the hard thresholding operator T\u03c4 (\u00b7) is defined as\n(T\u03c4 (b))j = { 0 , |bj| < \u03c4 bj , |bj| \u2265 \u03c4\n(9)\nwhere the vector b \u2208 Rn, and the subscript j indexes its entries. The optimal sparse codes are obtained by hard thresholding yi in the optimal transform domain. Step (ii) is similar to that in the denoising algorithm in Section 4.1. In the following, we discuss Step (iii) by considering two cases.\nIdeal image inpainting without noise. In the ideal case when the noise \u03b5 is\nabsent, i.e., \u03c3 = 0, the coefficient of the fidelity term \u03b3 \u2192 \u221e. Thus the fidelity term can be replaced with hard constraints Pi yi = zi \u2200 i. In the noiseless reconstruction step, with fixed {xi, Ck} and W , we first reconstruct each image patch yi by solving the following linearly constrained least squares problem:\nmin yi \u2016W\u03a6kiyi \u2212 xi\u2016 2 2 s.t. Pi yi = zi . (10)\nWe define zi = Piyi , yi \u2212 ei, where ei = (In \u2212 Pi)yi. \u2126i = supp(\u03a6kidiag(In \u2212 Pi)), which is complementary to supp(\u03a6kidiag(Pi)), where \u201cdiag\u201d denotes the diagonal of a matrix extracted as a vector. Since the constraint leads to the relationship yi = zi + ei with zi given, we solve the equivalent minimization problem over ei as follows:\nmin ei \u2016W\u03a6kiei \u2212 (xi \u2212W\u03a6ki zi)\u2016 2 2 s.t. supp(\u03a6kiei) = \u2126i . (11)\nHere, we define W\u2126i to be the submatrix of W formed by columns indexed in \u2126i, and (\u03a6kiei)\u2126i to be the vector containing the entries at locations \u2126i of \u03a6kiei. Thus, W\u03a6kei = W\u2126i(\u03a6kiei)\u2126i , and we define \u03be i , \u03a6kiei. The reconstruction problem is then re-written as the following unconstrained problem:\nmin \u03bei\u2126i \u2225\u2225W\u2126i\u03bei\u2126i \u2212 (xi \u2212W\u03a6k zi)\u2225\u222522 \u2200 i . (12) The above least squares problem has a simple solution given as \u03be\u0302i\u2126i = W \u2020 \u2126i\n(xi\u2212W\u03a6k zi). Accordingly, we can calculate e\u0302i = \u03a6\nT ki \u03be\u0302i, and thus the reconstructed patches y\u0302i = e\u0302i+zi.\nRobust image inpainting. We now consider the case of noisy z, and propose\na robust inpainting algorithm (i.e., for the aforementioned Step (iii)). This is useful because real image measurements are inevitably corrupted with noise [19]. The robust reconstruction step for each patch is to solve the following problem:\nmin yi \u2016W\u03a6kiyi \u2212 xi\u2016 2 2 + \u03b3 \u2016Piyi \u2212 zi\u2016 2 2 (13)\nLet z\u0303i , \u03a6kizi, ui , \u03a6kiyi, and P\u0303i , \u03a6kiPi\u03a6 T ki , where \u03a6ki is a permutation matrix. The rotated solution u\u0302i in optimization problem (13) is equivalent to\nu\u0302i = arg min ui\n\u2016Wui \u2212 xi\u201622 + \u03b3 \u2225\u2225\u2225P\u0303iui \u2212 z\u0303i\u2225\u2225\u22252\n2 (14)\nwhich has a least squares solution u\u0302i = (W TW + \u03b3P\u0303i) \u22121(W Txi + \u03b3P\u0303iz\u0303i). As the matrix inversion (W TW+\u03b3P\u0303i) \u22121 is expensive with a cost of O(n3) for each patch reconstruction, we apply the Woodbury Matrix Identity and rewrite the solution to (14) as\nu\u0302i = [B \u2212BT\u03a5i( 1\n\u03b3 Iqi + \u03a8i)\n\u22121B\u03a5i ](W Txi + \u03b3P\u0303iz\u0303i) (15)\nwhere B , (W TW )\u22121 can be pre-computed, and the support of z\u0303i is denoted as \u03a5i , supp(diag(P\u0303i)). The scalar qi = |\u03a5i| counts the number of measured pixels in zi. Here, B\u03a5i is the submatrix of B formed by \u03a5i-indexed rows, while \u03a8i is the submatrix of B\u03a5i formed by \u03a5i-indexed columns. Thus, the matrix inversion ( 1 \u03b3 Iqi +\u03a8i) \u22121 has cost of O((qi)3) and the other matrix-matrix products in (15) have cost O(n(qi)2), compared to computing (W TW + \u03b3P\u0303i) \u22121 with cost of O(n3) for the reconstruction of each patch. For an inpainting problem with most pixels missing (qi n), this represents significant savings. On the other hand, with few pixels missing (qi \u2248 n), a similar procedure can be used with I \u2212 Pi replacing Pi. Once u\u0302i is computed, the patch in (13) is recovered as y\u0302i = \u03a6\nT ki u\u0302i.\nSimilar to Section 4.1, each pixel in the reconstructed patch is projected onto the\n(underlying) intensity range (e.g., [0, 255] for image pixel stored using 8-bit integer). Eventually, we output the inpainted image by averaging the reconstructed patches at their respective image locations. We perform multiple passes in the inpainting algorithm for (P6) for improved inpainting. In each pass, we initialize {yi} using patches extracted from the most recent inpainted image. By doing so, we indirectly reinforce the dependency between overlapping patches in each pass."}, {"heading": "4.3. BCS-based MRI", "text": "Compressed Sensing (CS) exploits sparsity and enables accurate MRI reconstruction from limited k-space or Fourier measurements [40, 36, 17]. However, CS-based MRI may suffer from artifacts at high undersampling factors, when using non-adaptive or analytical sparsifying transforms [31]. Recent works [36] proposed Blind Compressed Sensing (BCS)-based MR image reconstruction methods using learned signal models,\nand achieved superior reconstruction results. The terminology blind compressed sensing (or image model-blind compressed sensing) is used because the dictionary or sparsifying transform for the underlying image patches is assumed unknown a priori, and is learned simultaneously with the image from the undersampled (compressive) measurements themselves. MR image patches typically contain various oriented features [53], which have recently been shown to be well sparsifiable by directional wavelets [28]. As an alternative to approaches involving directional analytical transforms, here, we propose an adaptive FRIST-based approach that can adapt a parent transform W while clustering image patches simultaneously based on their geometric orientations. This leads to more accurate modeling of MR image features.\nSimilar to the previous TL-MRI work [36], we propose a BCS-based MR image\nreconstruction scheme using the (adaptive) FRIST model, dubbed FRIST-MRI. For computational efficiency, we restrict the parent W to be a unitary transform in the following. The FRIST-blind image recovery problem with a sparsity constraint is formulated as\n(P7) min W,y,{xi,Ck} \u00b5 \u2016Fuy \u2212 z\u201622 + K\u2211 k=1 \u2211 i\u2208Ck \u2016W\u03a6kRiy \u2212 xi\u201622\ns.t. WHW = I, \u2016X\u20160 \u2264 s, \u2016y\u20162 \u2264 L, {Ck} \u2208 \u0393 ,\nwhere WHW = I is the unitary constraint, y \u2208 CP is the MR image to be reconstructed, and z \u2208 CM denotes the measurements with the sensing matrix Fu \u2208 CM\u00d7P , which is the undersampled (single coil, Cartesian) Fourier encoding matrix. Here M P , so Problem (P7) is aimed to reconstruct an MR image y from highly undersampled measurements z. The constraint \u2016y\u20162 \u2264 L with L > 0, represents prior knowledge of the signal energy/range. The sparsity term \u2016X\u20160 counts the number of non-zeros in the entire sparse code matrix X, whose columns are the sparse codes {xi}. This sparsity constraint enables variable sparsity levels for individual patches [36].\nWe use a block coordinate descent approach [36] to solve Problem (P7). The\nproposed algorithm alternates between (i) sparse coding and clustering, (ii) parent transform update, and (iii) MR image update. We initialize this algorithm, dubbed FRIST-MRI, with an image (estimate for y) such as the zero-filled Fourier reconstruction FHu z. Step (i) solves Problem (P7) for {xi, Ck} with fixed W and y as\nmin {xi,Ck} K\u2211 k=1 \u2211 i\u2208Ck \u2016W\u03a6kRiy \u2212 xi\u201622 s.t. \u2016X\u20160 \u2264 s, {Ck} \u2208 \u0393. (16)\nThe exact solution to Problem (16) requires calculating the sparsification error (objective) for each possible clustering. The cost for this scales as O(Pn2KP ) for P patches \u2016, which is computationally infeasible. Instead, we present an approximate\n\u2016 The number of patches is P when we use a patch overlap stride of 1 and include patches at image boundaries by allowing them to \u2018wrap-around\u2019 on the opposite side of the image [35].\nsolution here, which we observed to work well in our experiments. In this method, we first compute the total sparsification error SEk, associated with each \u03a6k, by solving the following problem:\nSEk = P\u2211 i=1 SEik , min{\u03b2ki } P\u2211 i=1 \u2225\u2225W\u03a6kRiy \u2212 \u03b2ki \u2225\u222522 s.t. \u2225\u2225Bk\u2225\u22250 \u2264 s (17) where the columns of Bk are { \u03b2ki } . The optimal Bk above is obtained by thresholding\nthe matrix with columns { W\u03a6kRiy }P i=1 and retaining the s largest magnitude elements. The clusters {Ck} in (16) are approximately computed by assigning i \u2208 Ck\u0302i where k\u0302i = arg min\nk SEik. Once the clusters are computed, the corresponding sparse\ncodes X\u0302 in (16) (for fixed clusters) are easily found by thresholding the matrix[ W\u03a6k\u03021R1y | ... | W\u03a6k\u0302PRPy ] and retaining the s largest magnitude elements [36].\nStep (ii) updates the parent transform W with the unitary constraint (and with\nother variables fixed). The optimal solution, which is similar to previous work [35], is computed as follows. First, we calculate the full SVD AXH = S\u0303\u03a3\u0303V\u0303 H , where the columns of A are {\u03a6kRiy}Pi=1. The optimal unitary parent transform is then W\u0302 = V\u0303 S\u0303H . Step (iii) solves for y with fixed W and {xi, Ck} as\nmin y K\u2211 k=1 \u2211 i\u2208Ck \u2016W\u03a6kRiy \u2212 xi\u201622 + \u00b5 \u2016Fuy \u2212 z\u2016 2 2 s.t. \u2016y\u20162 \u2264 L. (18)\nAs Problem (18) is a least squares problem with an `2 constraint, it can be solved exactly using the Lagrange multiplier method [14]. Thus (18) is equivalent to\nmin y K\u2211 k=1 \u2211 i\u2208Ck \u2016W\u03a6kRiy \u2212 xi\u201622 + \u00b5 \u2016Fuy \u2212 z\u2016 2 2 + \u03c1(\u2016y\u2016 2 2 \u2212 L) (19)\nwhere \u03c1 \u2265 0 is the optimally chosen Lagrange multiplier. An alternative approach to solving Problem (18) is by employing the iterative projected gradient method. However, because of the specific structure of the matrices (e.g., partial Fourier sensing matrix in MRI) involved, (18) can be solved much more easily and efficiently with the Lagrange multiplier method as discussed next.\nSimilar to the previous TL-MRI work [36], the normal equation for Problem (19)\n(for known multiplier \u03c1) can be simplified as follows, where F denotes the full Fourier encoding matrix assumed normalized (FHF = I):\n(FEFH + \u00b5FFHu FuF H + \u03c1I)Fy = F K\u2211 k=1 \u2211 i\u2208Ck RTi \u03a6 H k W Hxi + \u00b5FF H u z (20)\nwhere E , \u2211K\nk=1 \u2211 i\u2208Ck R T i \u03a6 H k W HW\u03a6kRi = \u2211P i=1R T i Ri. When the patch stride is 1\nand all wrap-around patches are included, E = nI, with I the P \u00d7 P identity. Since\nFEFH , \u00b5FFHu FuF H [36], and \u03c1I are all diagonal matrices, the matrix pre-multiplying Fy in (20) is diagonal and invertible. Hence, (20) can be solved cheaply. Importantly, using a unitary constraint for W leads to an efficient update in (20). In particular, the matrix E is not easily diagonalizable when W is not unitary. The problem of finding the optimal Lagrange multiplier reduces to solving a simple scalar equation (see for example, equation (3.17) in [36]) that can be solved using Newton\u2019s method. Thus, the approach based on the Lagrange multiplier method is much simpler compared to an iterative projected gradient scheme to estimate the (typically large) vector-valued image in Problem (18)."}, {"heading": "5. Experiments", "text": "We present numerical convergence results for the FRIST learning algorithm along with image segmentation examples, as well as some preliminary results demonstrating the promise of FRIST learning in applications including image sparse representation, denoising, robust inpainting, and MRI reconstruction. We work with 8 \u00d7 8 nonoverlapping patches for the study of convergence and sparse representation, 8 \u00d7 8 overlapping patches for image segmentation, denoising, and robust inpainting, and 6\u00d76 overlapping patches (including patches at image boundaries that \u2018wrap around\u2019 on the opposite side of the image) for the MRI experiments. Figure.1 lists the testing images that are used in the image denoising and inpainting experiments."}, {"heading": "5.1. Empirical convergence results", "text": "We first illustrate the convergence behavior of FRIST learning. We randomly extract 104 non-overlapping patches from the 44 images in the USC-SIPI database [2] (the color images are converted to gray-scale images), and learn a FRIST model, with a 64 \u00d7 64 parent transform W , from the randomly selected patches using fixed sparsity level s = 10 per patch. We set K = 2, and \u03bb0 = 3.1\u00d710\u22123 for visualization simplicity. In the experiment, we initialize the learning algorithm with different square 64\u00d764 parent transforms W \u2019s, including the (i) Karhunen-Loe\u0300ve Transform (KLT), (ii) 2D DCT, (iii) random matrix with i.i.d. Gaussian entries (zero mean and standard deviation 0.2), and (iv) identity matrix."}, {"heading": "1 200 400 600 800", "text": "Figures 2(a) and 2(d) illustrate that the objective function and sparsification error\nin (P2) converge to similar values from various initializations of W , indicating that the algorithm is reasonably insensitive or robust to initializations in practice. Figures 2(b) and 2(c) show the changes in cluster sizes over iterations for the 2D DCT and KLT initializations. The final values of the corresponding cluster sizes are also similar (although, not necessarily identical) for various initializations. Figure 2(e) and 2(f) display the learned FRIST parent W \u2019s with DCT and random matrix initializations. They are non-identical, and capture features that sparsify the image patches equally well. Thus we consider such learned transforms to be essentially equivalent as they achieve similar objective values and sparsification errors for the training data and are similarly conditioned (The learned parent W \u2019s with the DCT and random matrix initializations have condition numbers 1.04 and 1.06, respectively).\nThe numerical results demonstrate that our FRIST learning algorithm is reasonably\nrobust, or insensitive to initialization. Good initialization for the parent transform W , such as the DCT, leads to faster convergence during learning. Thus, we initialize the parent transform W using the 2D DCT in the rest of the experiments."}, {"heading": "5.2. Image Segmentation and Clustering Behavior", "text": "The FRIST learning algorithm is capable of clustering image patches according to their orientations. In this subsection, we illustrate the FRIST clustering behavior by image segmentation experiments. We consider the images Wave (512 \u00d7 512) and Field (512 \u00d7 512) shown in Fig. 3(a) and Fig. 4(a) as inputs. Both images contains directional textures, and we aim to cluster the pixels of the images into one of four classes, which represent different orientations or flips. For each input image, we convert it into gray-scale, extract the overlapping mean-subtracted patches, and learn a FRIST while clustering the patches using the algorithm in Section 3.1. As overlapping patches are used, each pixel in the image belongs to several overlapping patches. We cluster a pixel into a particular class by majority voting among the patches that contain it.\nWe set s = 10, and K = 4 in the clustering experiments. Figure 3 and Figure 4\nillustrate the segmentation results of images Wave and Field, respectively. Figure 3(b) and Figure 4(b) illustrate the pixel memberships with four different colors (blue, red, green, and black, for classes 1 to 4, respectively). Figures 3(c)-(f) and Figures 4(c)-(f) each visualize the image pixels clustered into a specific class in gray-scale, and the pixels that are not clustered into that class are shown in black. Each class captures edges at specific orientations.\nThe parent transform W and its children transforms Wk\u2019s in the learned FRIST\nfor the Wave image are visualized in Fig. 5 with the rows of each Wk displayed as 8\u00d7 8\npatches. We observe that each child transform contains distinct directional features that were adaptively learned to sparsify edges with specific orientations better. The parent W turns out to be identical to the child transform shown in Fig. 4(e), implying that the corresponding FR operator is the identity matrix.\nThe preliminary image segmentation results here demonstrate some potential for\nthe FRIST scheme for directional classification or segmentation. More importantly, we wish to illustrate why FRIST can provide improvements over SST or OCTOBOS in various inverse problems. As natural images usually contain a variety of directional features and edges, FRIST is capable of grouping those patches with similar orientations/flips, and thus provides better sparsification in each cluster using directional children transforms, even while learning only a single small parent transform W (which could be learned even in cases of very limited or corrupted data)."}, {"heading": "5.3. Sparse Image Representation", "text": "Most of the popular image compression methods make use of analytical sparsifying transforms. In particular, the commonly used JPEG uses the 2D DCT to sparsify image patches. Data-driven adaptation of dictionaries using the K-SVD scheme has also been shown to be beneficial for image compression, compared to fixed analytical transforms [7]. In this section, we show that the proposed FRIST learning scheme provides improved sparse representations of images compared to related adaptive sparse modeling methods. While we focus here on a simple study of the sparse representation abilities of adaptive FRIST, the investigation of a complete adaptive image compression framework based on FRIST and its comparison to benchmarks is left for future work. We learn a FRIST, with a 64 \u00d7 64 parent transform W , from the 104 randomly selected patches (from USC-SIPI images) used in Section 5.1. We set K = 32, s = 10 and \u03bb0 = 3.1\u00d710\u22123. We compare the learned FRIST with other popular adaptive sparse signals models. In particular, we train a 64\u00d764 SST [35], a 128\u00d764 OCTOBOS [44], as well as a 64\u00d7 64 square (synthesis) dictionary and a 64\u00d7 128 overcomplete dictionary using KSVD [11], using the same training patches and sparsity level as for FRIST.\nWith the learned models, we represent each image from the USC-SIPI database as\nwell as some other standard images. Each image is represented compactly by storing its sparse representation including (i) non-zero coefficient values in the sparse codes of the 8 \u00d7 8 non-overlapping patches, (ii) locations of the non-zeros (plus the cluster membership if necessary) in the sparse codes for each patch and (iii) the adaptive sparse signal model (e.g., the dictionary or transform matrix \u2013 this would typically involve negligible overhead). For each method, the patch sparsity (or equivalently, the number of non-zero coefficients per patch) is set to s = 10 (same as during training). The adaptive SST, square KSVD, and adaptive FRIST methods store only a 64 \u00d7 64 square matrix in (iii) above, whereas the overcomplete KSVD and OCTOBOS methods store a 128\u00d7 64 matrix. The images (i.e., their non-overlapping patches) are reconstructed from their sparse representations in a least squares sense, and the reconstruction quality for each image is evaluated using the Peak-Signal-to-Noise Ratio (PSNR), expressed in decibels (dB). We use the average of the PSNR values over all 44 USC-SIPI images as the indicator of the quality of sparse representation of the USC-SIPI database.\nTable 2 lists the sparse representation reconstruction results for the USC-SIPI\ndatabase and the images Cameraman (256\u00d7 256) and House (256\u00d7 256). We observe that the learned FRIST model provides the best reconstruction quality compared to other adaptive sparse signal models or the analytical 2D DCT, for both the USCSIPI images and the external images. Compared to unstructured overcomplete models such as KSVD and OCTOBOS, the proposed FRIST provides improved PSNRs, while achieving potentially fewer bits for sparse representation \u00b6. Additionally, dictionary learning based representation requires synthesis sparse coding, which is more expensive compared to the cheap and exact sparse coding in the transform model-based methods [34]. As mentioned before, the investigation of an image compression system based on learned FRIST models, and its analysis as well as quantitative comparison to other compression benchmarks is left for future work."}, {"heading": "5.4. Image Denoising", "text": "We present denoising results using our FRIST-based framework in Section 4.1. We simulate i.i.d. Gaussian noise at 4 different noise levels (\u03c3 = 5, 10, 15, 20) for seven standard images in Fig. 1. Denoising results obtained by our proposed algorithm in Section 4.1 are compared with those obtained by the adaptive overcomplete K-SVD denoising scheme [11], adaptive SST denoising scheme [35] and the adaptive OCTOBOS denoising scheme [44]. We also compare to the denoising result using the SST method, but with fixed 2D DCT (i.e., no learning).\n\u00b6 Assuming for simplicity that L bits are used to describe each non-zero coefficient value in each model, the total number of bits for storing the sparse code (non-zero locations, non-zero coefficient values, cluster membership) of a patch is 6s + Ls + 5 in 64 \u00d7 64 FRIST (K = 32), and 7s + Ls in 64\u00d7 128 KSVD. For the setting s = 10, FRIST requires 5 fewer bits per patch compared to KSVD.\nadaptive SST and OCTOBOS denoising methods, we follow the same parameter settings as used in the previous works [35, 44]. The same parameter settings as for the SST method is used for the DCT-based denoising algorithm. A 64 \u00d7 256 learned synthesis dictionary is used in the synthesis K-SVD denoising method and for the OCTOBOS denoising scheme, we use a corresponding 256 \u00d7 64 learned OCTOBOS. For the KSVD, adaptive SST, and adaptive OCTOBOS denoising methods, we used the publicly available implementations [1, 3] in this experiment.\nTable 3 lists the denoised image PSNR values for the various methods for the\nseven tested images at several noise levels. The proposed FRIST scheme provides consistently better PSNRs compared to the other fixed or adaptive sparse modeling methods including DCT, SST, K-SVD, and OCTOBOS. The average denoising PSNR improvements provided by adaptive FRIST over DCT, adaptive SST, K-SVD, and adaptive OCTOBOS are 0.48 dB, 0.26 dB, 0.47 dB, and 0.04 dB respectively, and the standard deviation in these improvements was 0.26 dB, 0.11 dB, 0.22 dB, and 0.03 dB, respectively.\nPerformance vs. Noise Level. Table 4 lists the noisy PSNR and denoising\nPSNR values averaged over 7 testing images, using the competing methods relative to those using FRIST. It is clear for larger \u03c3 values, the proposed FRIST provides larger average denoising PSNR improvements, compared to the competing methods. Because the FRIST model is more constrained, thus more robust to measurement corruption (e.g., additive noise), comparing to other sparse models. Sections 5.5 and 5.6 will provide more evidence demonstrating the robustness of the proposed FRIST method for other types of measurement corruptions, including missing pixel in image inpainting and Fourier-domain undersampling in MRI.\nPerformance vs. Number of Clusters. In applications such as image denoising,\nwhen OCTOBOS or FRIST are learned from limited noisy patches, OCTOBOS with\nmany more degrees of freedom is more likely to overfit the data and learn noisy features, which can degrade the denoising performance. Figure 6 provides an empirical illustration of this behavior, and plots the denoising PSNRs for Peppers as a function of the number of child transforms or number of clusters K for \u03c3 = 5 and \u03c3 = 15. In both cases, the denoising PSNRs of the OCTOBOS and FRIST schemes increase with K initially. However, beyond an optimal value of K, the OCTOBOS denoising scheme suffers from overfitting the noise. Thus the OCTOBOS performance in Fig. 6 quickly degrades as the number of transforms (in the collection/union) or clusters to be learned from a set of noisy image patches is increased [44]. In contrast, the structured FRIST-based denoising scheme (involving much fewer degrees of freedom) is more robust or resilient to noise. As K increases, adaptive FRIST denoising provides continually monotonically increasing denoising PSNR in Fig. 6. For example, while the FRIST PSNR achieves a peak value for K = 128, the PSNR for adaptive OCTOBOS denoising is significantly lower at such a large K.\nAlthough we focused our comparisons here on related adaptive sparse modeling\nmethods, a very recent work [42] shows that combining transform learning based denoising with non-local similarity models leads to better denoising performance, and outperforms the state-of-the-art BM3D denoising method [9]. A further extension of the work in [42] to include FRIST learning is of interest and could potentially provide even greater advantages, but we leave this detailed investigation to future work."}, {"heading": "5.5. Image Inpainting", "text": "We present preliminary results for our adaptive FRIST-based inpainting framework (based on (P6)). We randomly remove 80% and 90% of the pixels of the entire images in Fig. 5, and simulate i.i.d. additive Gaussian noise for the sampled pixels with \u03c3 = 0, 5, 10, and 15. We set K = 64, n = 64, and apply the proposed adaptive FRIST inpainting algorithm to reconstruct the images from the corrupted and noisy measurements. For comparison, we replace the adaptive FRIST in the proposed inpainting algorithm with\nthe fixed 2D DCT, adaptive SST [35], and adaptive OCTOBOS [44] respectively, and evaluate the inpainting performance these alternatives. The image inpainting results obtained by the FRIST based methods are also compared with those obtained by the cubic interpolation [48, 41] and patch smoothing [30] methods. We used the Matlab function \u201cgriddata\u201d to implement the cubic interpolation, and use the publicly available implementation of the patch smoothing method. For the DCT, SST, OCTOBOS, and FRIST based methods, we initialize the image patches using the Cubic Interpolation method in noiseless cases, and using the Patch Smoothing method in noisy cases.\nTable 5 lists the image inpainting PSNR results, averaged over the images shown\nin Fig. 1, for various fractions of sampled pixels and noise levels. The proposed adaptive FRIST inpainting scheme provides better PSNRs compared to the other inpainting methods based on interpolation, transform-domain sparsity, and spatial similarity. The average inpainting PSNR improvements achieved by FRIST over DCT, SST, and OCTOBOS are 0.56 dB, 0.28 dB, and 0.11 dB respectively, and the standard deviations in these improvements were 0.39 dB, 0.16 dB, and 0.05 dB respectively. Importantly, adaptive FRIST provides larger improvements over the other competing methods including the learned OCTOBOS, at higher noise levels. Figure 7 provides\nan illustration of the inpainting results, with regional zoom-in for visual comparisons. We observe that the cubic interpolation produces blur in various locations. The FRIST result is much improved, and also shows fewer artifacts compared to the patch smoothing [30] and adaptive SST results. Table 5 shows that the cubic Interpolation method is extremely sensitive to noise, whereas the FRIST based method is the most robust. These results indicate the benefits of adapting the highly constrained yet overcomplete FRIST data model."}, {"heading": "5.6. MRI Reconstruction", "text": "We present preliminary MRI reconstruction results using the proposed FRIST-MRI algorithm. The three complex-valued images and the corresponding k-space sampling masks used in this section are shown in Fig. 8, Fig. 9(a), and Fig. 9(b) +. We retrospectively undersample the k-space of the reference images using the displayed sampling masks. We set K = 32, the sparsity level s = 0.05 \u00d7 nP , and the other parameters were set similarly as for TL-MRI in [36]. We used a higher sparsity level s = 0.085\u00d7nN for reconstructing Image 3, which worked well. To speed up convergence, lower sparsity levels are used in the initial iterations [36]. We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36]. The parameter settings for these methods are as mentioned in [36]. We separately tuned the sparsity parameter for TL-MRI [36] for reconstructing Image 3 \u2217. The reconstruction PSNRs (computed for image magnitudes) for various approaches are compared in Table 6.\nFirst, the proposed FRIST-MRI algorithm provides significant improvements over\n+ The testing image data in this section were used and included in previous works [36, 53] with the data sources. \u2217 We observed improved reconstruction PSNR compared to the result obtained using the sparsity settings in [36].\nthe naive Zero-filling reconstruction (the initialization of the algorithm) with 6.4 dB better PSNR on average, as well as 4.2 dB better PSNR (on average) over the nonadaptive Sparse MRI reconstructions. Compared to recently proposed popular MRI reconstruction methods, the FRIST-MRI algorithm demonstrates reasonably better performance for each testing case, with an average PSNR improvement of 0.8 dB, 0.5 dB, and 0.3 dB over the non-local patch similarity-based PANO method, the partially adaptive PBDWS method, and the adaptive dictionary-based DL-MRI method.\nThe proposed FRIST-MRI reconstruction quality is 0.2 dB better than TL-MRI\non average. As we followed a reconstruction framework and parameters similar to those used by TL-MRI [36], the quality improvement obtained with FRIST-MRI is solely because the learned FRIST can serve as a better regularizer for MR image reconstruction compared to the single adaptive square transform in TL-MRI. Figure 9 visualizes the reconstructions and reconstruction errors (magnitude of the difference between the magnitudes of the reconstructed and reference images) for FRIST-MRI and TL-MRI. The FRIST-MRI reconstruction error map clearly shows fewer artifacts, especially along the boundaries of the circles, compared to TL-MRI."}, {"heading": "6. Conclusion", "text": "In this paper, we presented a novel framework for learning flipping and rotation invariant sparsifying transforms. These transforms correspond to a structured union-oftransforms, and are dubbed FRIST. The collection of transforms in FRIST are related to an underlying (or generating) parent transform by flipping and rotation operations. Our algorithm for FRIST learning is highly efficient, and involves optimal updates, with convergence guarantees. We demonstrated the ability of FRIST learning for extracting directional features in images. In practice, FRIST learning is insensitive to initialization, and performs better than several prior adaptive sparse modeling methods in various applications including sparse image representation, image denoising, image inpainting, and blind compressed sensing MRI reconstruction."}], "references": [{"title": "Sparse and redundant modeling of image content using an imagesignature-dictionary", "author": ["M. Aharon", "M. Elad"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. on Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM Review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Compression of facial images using the k-svd algorithm", "author": ["O. Bryt", "M. Elad"], "venue": "Journal of Visual Communication and Image Representation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Atomic decomposition by basis pursuit", "author": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Image denoising by sparse 3D transformdomain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Adaptive greedy approximations", "author": ["G. Davis", "S. Mallat", "M. Avellaneda"], "venue": "Journal of Constructive Approximation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Analysis versus synthesis in signal priors", "author": ["M. Elad", "P. Milanfar", "R. Rubinstein"], "venue": "Inverse Problems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Hakon-Husoy. Method of optimal directions for frame design", "author": ["K. Engan", "S. Aase", "J.H"], "venue": "In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Blind compressed sensing", "author": ["S. Gleichman", "Y.C. Eldar"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "PCA-SIFT: A more distinctive representation for local image descriptors", "author": ["Y. Ke", "R. Sukthankar"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "In IEEE International Conference on Computer vision (ICCV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Sparse MRI: The application of compressed sensing for rapid MR imaging", "author": ["M. Lustig", "D. Donoho", "J. Pauly"], "venue": "Magnetic resonance in medicine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Sparse representation for color image restoration", "author": ["J. Mairal", "M. Elad", "G. Sapiro"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "A Wavelet Tour of Signal Processing", "author": ["S. Mallat"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S.G. Mallat", "Zhifeng Zhang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1993}, {"title": "Magnetic resonance image reconstruction using trained geometric directions in 2d redundant wavelets domain and non-convex optimization", "author": ["B. Ning", "X. Qu", "D. Guo", "C. Hu", "Z. Chen"], "venue": "Magnetic resonance imaging,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition", "author": ["Y. Pati", "R. Rezaiifar", "P. Krishnaprasad"], "venue": "In Asilomar Conf. on Signals, Systems and Comput.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Bandelet image approximation and compression", "author": ["E.L. Pennec", "S. Mallat"], "venue": "Multiscale Modeling & Simulation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Adaptive sparsifying transforms for iterative tomographic reconstruction", "author": ["L. Pfister", "Y. Bresler"], "venue": "In International Conference on Image Formation in X-Ray Computed Tomography,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Learning sparsifying filter banks", "author": ["L. Pfister", "Y. Bresler"], "venue": "In Proc. SPIE Wavelets & Sparsity XVI,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Hadamard transform image coding", "author": ["W.K. Pratt", "J. Kane", "H.C. Andrews"], "venue": "Proc. IEEE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1969}, {"title": "Undersampled MRI reconstruction with patch-based directional wavelets", "author": ["X. Qu", "D. Guo", "B. Ning", "Y. Hou", "Y. Lin", "S. Cai", "Z. Chen"], "venue": "Magnetic resonance imaging,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Magnetic resonance image reconstruction from undersampled measurements using a patch-based nonlocal operator", "author": ["X. Qu", "Y. Hou", "F. Lam", "D. Guo", "J. Zhong", "Z. Chen"], "venue": "Medical image analysis,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Image processing using smooth ordering of its patches", "author": ["I. Ram", "M. Elad", "I. Cohen"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "MR image reconstruction from highly undersampled k-space data by dictionary learning", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Learning doubly sparse transforms for images", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Learning overcomplete sparsifying transforms for signal processing.  FRIST Learning and Applications", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Learning sparsifying transforms", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "`0 sparsifying transform learning with efficient optimal updates and convergence guarantees", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Efficient blind compressed sensing using sparsifying transforms with convergence guarantees and application to magnetic resonance imaging", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Online sparsifying transform learning - part i: Algorithms", "author": ["S. Ravishankar", "B. Wen", "Y. Bresler"], "venue": "IEEE Journal of Selected Topics in Signal Process.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A.M. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Recursive least squares dictionary learning algorithm", "author": ["K. Skretting", "K. Engan"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Highly undersampled magnetic resonance image reconstruction via homotopic-minimization", "author": ["J. Trzasko", "A. Manduca"], "venue": "IEEE Transactions on Medical imaging,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Contouring: a guide to the analysis and display of spatial data", "author": ["D. Watson"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "When sparsity meets low-rankness: Transform learning with non-local low-rank constraint for image restoration", "author": ["B. Wen", "Y. Li", "Y. Bresler"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2017}, {"title": "Learning overcomplete sparsifying transforms with block cosparsity", "author": ["B. Wen", "S. Ravishankar", "Y. Bresler"], "venue": "In IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Structured overcomplete sparsifying transform learning with convergence guarantees and applications", "author": ["B. Wen", "S. Ravishankar", "Y. Bresler"], "venue": "Int. J. Computer Vision,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Sparse coding with invariance constraints", "author": ["H. Wersing", "J. Eggert", "E. K\u00f6rner"], "venue": "In Artificial Neural Networks and Neural Information Processing,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2003}, {"title": "Inverting modified matrices", "author": ["M. Woodbury"], "venue": "Memorandum report,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1950}, {"title": "Dictionary learning for sparse approximations with the majorization method", "author": ["M. Yaghoobi", "T. Blumensath", "M. Davies"], "venue": "IEEE Transaction on Signal Processing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Finite element structural analysis, volume 2", "author": ["T. Yang"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1986}, {"title": "DCT image denoising: a simple and effective image denoising algorithm", "author": ["G. Yu", "G. Sapiro"], "venue": "Image Processing On Line,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "Image modeling and enhancement via structured sparse model selection", "author": ["G. Yu", "G. Sapiro", "S. Mallat"], "venue": "In 2010 IEEE International Conference on Image Processing,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2010}, {"title": "Solving inverse problems with piecewise linear estimators: From gaussian mixture models to structured sparsity", "author": ["G. Yu", "G. Sapiro", "S. Mallat"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Dictionary optimization for block-sparse representations", "author": ["L. Zelnik-Manor", "K. Rosenblum", "Y. Eldar"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}, {"title": "Fast multi-class dictionaries learning with geometrical directions in MRI reconstruction", "author": ["Z. Zhan", "J. Cai", "D. Guo", "Y. Liu", "Z. Chen", "X. Qu"], "venue": "arXiv preprint arXiv:1503.02945,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Various sparse signal models, such as the synthesis model [6, 12] and the transform model [27] have been studied.", "startOffset": 58, "endOffset": 65}, {"referenceID": 8, "context": "Various sparse signal models, such as the synthesis model [6, 12] and the transform model [27] have been studied.", "startOffset": 58, "endOffset": 65}, {"referenceID": 23, "context": "Various sparse signal models, such as the synthesis model [6, 12] and the transform model [27] have been studied.", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Synthesis dictionary learning methods [13, 5] that adapt the dictionary ar X iv :1 51 1.", "startOffset": 38, "endOffset": 45}, {"referenceID": 1, "context": "Synthesis dictionary learning methods [13, 5] that adapt the dictionary ar X iv :1 51 1.", "startOffset": 38, "endOffset": 45}, {"referenceID": 6, "context": "based on training data typically involve a synthesis sparse coding step which is often NPhard [10], so that approximate solutions [23, 21, 8] are widely used.", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "based on training data typically involve a synthesis sparse coding step which is often NPhard [10], so that approximate solutions [23, 21, 8] are widely used.", "startOffset": 130, "endOffset": 141}, {"referenceID": 17, "context": "based on training data typically involve a synthesis sparse coding step which is often NPhard [10], so that approximate solutions [23, 21, 8] are widely used.", "startOffset": 130, "endOffset": 141}, {"referenceID": 4, "context": "based on training data typically involve a synthesis sparse coding step which is often NPhard [10], so that approximate solutions [23, 21, 8] are widely used.", "startOffset": 130, "endOffset": 141}, {"referenceID": 9, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 39, "endOffset": 55}, {"referenceID": 43, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 39, "endOffset": 55}, {"referenceID": 35, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 39, "endOffset": 55}, {"referenceID": 14, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 39, "endOffset": 55}, {"referenceID": 7, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 175, "endOffset": 186}, {"referenceID": 15, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 175, "endOffset": 186}, {"referenceID": 0, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 175, "endOffset": 186}, {"referenceID": 1, "context": "For example, the well-known K-SVD method [5] generalizes the K-means clustering process to a dictionary learning algorithm, and alternates between updating the sparse codes of training signals (sparse coding step) and the dictionary (dictionary or codebook update step).", "startOffset": 41, "endOffset": 44}, {"referenceID": 34, "context": "Moreover, methods such as KSVD lack convergence guarantees, and can get easily caught in local minima, or saddle points [38].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "It is well-known that natural images are sparsifiable by analytical transforms such as the discrete cosine transform (DCT), or wavelet transform [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 30, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 80, "endOffset": 84}, {"referenceID": 30, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 232, "endOffset": 248}, {"referenceID": 32, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 232, "endOffset": 248}, {"referenceID": 21, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 232, "endOffset": 248}, {"referenceID": 22, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 232, "endOffset": 248}, {"referenceID": 31, "context": "Alternating minimization algorithms for learning SST have been proposed with cheap and optimal updates [35].", "startOffset": 103, "endOffset": 107}, {"referenceID": 40, "context": "Recent work focused on learning a union of unstructured sparsifying transforms [44, 43], dubbed OCTOBOS (for OverComplete TransfOrm with BlOck coSparsity constraint \u2013 cf.", "startOffset": 79, "endOffset": 87}, {"referenceID": 39, "context": "Recent work focused on learning a union of unstructured sparsifying transforms [44, 43], dubbed OCTOBOS (for OverComplete TransfOrm with BlOck coSparsity constraint \u2013 cf.", "startOffset": 79, "endOffset": 87}, {"referenceID": 40, "context": "[44]), to sparsify images with diverse contents, features and textures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "However, learning such an unstructured OCTOBOS model [44] (that has many free parameters) especially from noisy or limited data could suffer from overfitting to noise/artifacts, thereby degrading performance in various applications and inverse problem settings.", "startOffset": 53, "endOffset": 57}, {"referenceID": 41, "context": "synthesis model sparse coding [45], and applied rotational operators with analytical transforms [53], the usefulness of the rotational invariance property in learning adaptive sparse signal models has not been explored.", "startOffset": 30, "endOffset": 34}, {"referenceID": 49, "context": "synthesis model sparse coding [45], and applied rotational operators with analytical transforms [53], the usefulness of the rotational invariance property in learning adaptive sparse signal models has not been explored.", "startOffset": 96, "endOffset": 100}, {"referenceID": 46, "context": "There are other prior works such as the Structured Sparse Model Selection (SSMS) [50, 51] approach that also involve clustering, but the sub-dictionary in each cluster in SSMS is obtained by conventional Principal Component Analysis (PCA).", "startOffset": 81, "endOffset": 89}, {"referenceID": 47, "context": "There are other prior works such as the Structured Sparse Model Selection (SSMS) [50, 51] approach that also involve clustering, but the sub-dictionary in each cluster in SSMS is obtained by conventional Principal Component Analysis (PCA).", "startOffset": 81, "endOffset": 89}, {"referenceID": 30, "context": "The learning of the sparsifying transform model [34] has been proposed recently.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "Though there are various methods of formulating the rotation operator G with arbitrary angles [16, 15], rotating image patches by an angle \u03b8 that is not a multiple of 90\u25e6 requires interpolation, and may result in misalignment with the pixel grid.", "startOffset": 94, "endOffset": 102}, {"referenceID": 11, "context": "Though there are various methods of formulating the rotation operator G with arbitrary angles [16, 15], rotating image patches by an angle \u03b8 that is not a multiple of 90\u25e6 requires interpolation, and may result in misalignment with the pixel grid.", "startOffset": 94, "endOffset": 102}, {"referenceID": 20, "context": "Constructions of such {Gq} have been proposed before [24, 28, 53].", "startOffset": 53, "endOffset": 65}, {"referenceID": 24, "context": "Constructions of such {Gq} have been proposed before [24, 28, 53].", "startOffset": 53, "endOffset": 65}, {"referenceID": 49, "context": "Constructions of such {Gq} have been proposed before [24, 28, 53].", "startOffset": 53, "endOffset": 65}, {"referenceID": 28, "context": "For each \u03a6k, the optimal sparse code x\u0302 in Problem (P1) can be solved exactly as x\u0302 = Hs(W\u03a6ky), where Hs(\u00b7) is the projector onto the s-`0 ball [32], i.", "startOffset": 144, "endOffset": 148}, {"referenceID": 40, "context": "The FRIST model can be interpreted as a structured union-of-transforms model, or a structured OCTOBOS model [44], i.", "startOffset": 108, "endOffset": 112}, {"referenceID": 40, "context": "Problem (P1) is similar to the OCTOBOS sparse coding problem [44], where each Wk = W\u03a6k corresponds to a block of OCTOBOS.", "startOffset": 61, "endOffset": 65}, {"referenceID": 48, "context": "When the parent transform W is unitary, FRIST is also equivalent to an overcomplete synthesis dictionary with block sparsity [52], with W T k denoting the kth block of the equivalent overcomplete dictionary.", "startOffset": 125, "endOffset": 129}, {"referenceID": 29, "context": "Generally, the parent transform W can be overcomplete [33, 44, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 40, "context": "Generally, the parent transform W can be overcomplete [33, 44, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 22, "context": "Generally, the parent transform W can be overcomplete [33, 44, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 40, "context": ", N}, which enforces all of the Ck\u2019s to be disjoint [44].", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "Problem (P2) is to minimize the FRIST learning objective that includes the modeling or sparsification error \u2211K k=1 \u2211 i\u2208Ck \u2016W\u03a6kYi \u2212Xi\u2016 2 2 for Y as well as the regularizer Q(W ) = \u2212 log |detW | + \u2016W\u20162F to prevent trivial solutions [34].", "startOffset": 230, "endOffset": 234}, {"referenceID": 30, "context": "The regularizer Q(W ) fully controls the condition number and scaling of the learned parent transform [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "Previous works [34] showed that the condition number and spectral norm of the optimal parent transform \u0174 approach 1 and 1/ \u221a 2 respectively, as \u03bb0 \u2192\u221e in (P2).", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "Problem (P4) has a simple solution involving a singular value decomposition (SVD), which is similar to the transform update step in SST [35].", "startOffset": 136, "endOffset": 140}, {"referenceID": 40, "context": "Unlike the previously proposed OCTOBOS learning algorithm [44], which requires initialization of the clusters using heuristic methods such as K-means, the FRIST learning algorithm only needs initialization of the parent transform W .", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "In the first iteration, all possible FR operators \u03a6k\u2019s [24, 53] (i.", "startOffset": 55, "endOffset": 63}, {"referenceID": 49, "context": "In the first iteration, all possible FR operators \u03a6k\u2019s [24, 53] (i.", "startOffset": 55, "endOffset": 63}, {"referenceID": 1, "context": "Thus, the overall computational cost per iteration of FRIST learning using the proposed alternating algorithm scales as O(KnN), which is typically lower than the cost per iteration of the overcomplete K-SVD learning algorithm [5], with the number of dictionary atoms m = Kn, and synthesis sparsity s \u221d n.", "startOffset": 226, "endOffset": 229}, {"referenceID": 40, "context": "Since FRIST can be interpreted as a structured OCTOBOS, the convergence results for the FRIST learning algorithm take a form similar to those obtained for the OCTOBOS learning algorithm [44] in our recent work.", "startOffset": 186, "endOffset": 190}, {"referenceID": 40, "context": "The proof of Conclusion (ii) follows the same arguments as in the proofs in Lemma 3 and Lemma 5 in [44].", "startOffset": 99, "endOffset": 103}, {"referenceID": 40, "context": "In Conclusion (iii), Condition (4) can be proved using the arguments for Lemma 7 from [44], while Condition (5) can be proved with the arguments for Lemma 6 from [35].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "In Conclusion (iii), Condition (4) can be proved using the arguments for Lemma 7 from [44], while Condition (5) can be proved with the arguments for Lemma 6 from [35].", "startOffset": 162, "endOffset": 166}, {"referenceID": 31, "context": "The last conclusion in Theorem 1 can be shown using similar arguments as from the proof of Lemma 9 in [35].", "startOffset": 102, "endOffset": 106}, {"referenceID": 45, "context": "Various denoising algorithms have been proposed recently, with state-of-the-art performance [49, 9].", "startOffset": 92, "endOffset": 99}, {"referenceID": 5, "context": "Various denoising algorithms have been proposed recently, with state-of-the-art performance [49, 9].", "startOffset": 92, "endOffset": 99}, {"referenceID": 7, "context": "Similar to previous dictionary and transform learning based image denoising methods [11, 44], we propose the following patch-based image denoising formulation using FRIST learning:", "startOffset": 84, "endOffset": 92}, {"referenceID": 40, "context": "Similar to previous dictionary and transform learning based image denoising methods [11, 44], we propose the following patch-based image denoising formulation using FRIST learning:", "startOffset": 84, "endOffset": 92}, {"referenceID": 7, "context": "The data fidelity term \u03c4 \u2016Ri z \u2212 yi\u201622 measures the discrepancy between the observed patch Riz and the (unknown) noiseless patch yi, and uses a weight \u03c4 = \u03c40/\u03c3 that is inversely proportional to the given noise standard deviation \u03c3 [11, 35], and \u03c40 > 0.", "startOffset": 231, "endOffset": 239}, {"referenceID": 31, "context": "The data fidelity term \u03c4 \u2016Ri z \u2212 yi\u201622 measures the discrepancy between the observed patch Riz and the (unknown) noiseless patch yi, and uses a weight \u03c4 = \u03c40/\u03c3 that is inversely proportional to the given noise standard deviation \u03c3 [11, 35], and \u03c40 > 0.", "startOffset": 231, "endOffset": 239}, {"referenceID": 28, "context": "We follow the previous SST-based and OCTOBOSbased denoising methods [32, 44], and impose a sparsity constraint on each yi.", "startOffset": 68, "endOffset": 76}, {"referenceID": 40, "context": "We follow the previous SST-based and OCTOBOSbased denoising methods [32, 44], and impose a sparsity constraint on each yi.", "startOffset": 68, "endOffset": 76}, {"referenceID": 28, "context": "We then update the sparsity levels si for all i, similar to the SST learning-based denoising algorithm [32].", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "We choose the optimal si to be the smallest integer that makes the reconstructed yi in (7) satisfy the error condition \u2016Riz \u2212 yi\u201622 \u2264 nC\u03c3, where C is a constant parameter [32].", "startOffset": 171, "endOffset": 175}, {"referenceID": 40, "context": "The noise standard deviation \u03c3 decreases gradually in each such pass, and is found (tuned) empirically [44].", "startOffset": 103, "endOffset": 107}, {"referenceID": 33, "context": "The sparse coding problem with a sparsity penalty has a closed-form solution [37], and thus Step (i) is equivalent to solving the following clustering problem:", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "This is useful because real image measurements are inevitably corrupted with noise [19].", "startOffset": 83, "endOffset": 87}, {"referenceID": 36, "context": "Compressed Sensing (CS) exploits sparsity and enables accurate MRI reconstruction from limited k-space or Fourier measurements [40, 36, 17].", "startOffset": 127, "endOffset": 139}, {"referenceID": 32, "context": "Compressed Sensing (CS) exploits sparsity and enables accurate MRI reconstruction from limited k-space or Fourier measurements [40, 36, 17].", "startOffset": 127, "endOffset": 139}, {"referenceID": 13, "context": "Compressed Sensing (CS) exploits sparsity and enables accurate MRI reconstruction from limited k-space or Fourier measurements [40, 36, 17].", "startOffset": 127, "endOffset": 139}, {"referenceID": 27, "context": "However, CS-based MRI may suffer from artifacts at high undersampling factors, when using non-adaptive or analytical sparsifying transforms [31].", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "Recent works [36] proposed Blind Compressed Sensing (BCS)-based MR image reconstruction methods using learned signal models,", "startOffset": 13, "endOffset": 17}, {"referenceID": 49, "context": "MR image patches typically contain various oriented features [53], which have recently been shown to be well sparsifiable by directional wavelets [28].", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "MR image patches typically contain various oriented features [53], which have recently been shown to be well sparsifiable by directional wavelets [28].", "startOffset": 146, "endOffset": 150}, {"referenceID": 32, "context": "Similar to the previous TL-MRI work [36], we propose a BCS-based MR image reconstruction scheme using the (adaptive) FRIST model, dubbed FRIST-MRI.", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "This sparsity constraint enables variable sparsity levels for individual patches [36].", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "We use a block coordinate descent approach [36] to solve Problem (P7).", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "Instead, we present an approximate \u2016 The number of patches is P when we use a patch overlap stride of 1 and include patches at image boundaries by allowing them to \u2018wrap-around\u2019 on the opposite side of the image [35].", "startOffset": 212, "endOffset": 216}, {"referenceID": 32, "context": "| W\u03a6k\u0302PRPy ] and retaining the s largest magnitude elements [36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "The optimal solution, which is similar to previous work [35], is computed as follows.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "As Problem (18) is a least squares problem with an `2 constraint, it can be solved exactly using the Lagrange multiplier method [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 32, "context": "Similar to the previous TL-MRI work [36], the normal equation for Problem (19) (for known multiplier \u03c1) can be simplified as follows, where F denotes the full Fourier encoding matrix assumed normalized (FF = I):", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "FEF , \u03bcFF u FuF H [36], and \u03c1I are all diagonal matrices, the matrix pre-multiplying Fy in (20) is diagonal and invertible.", "startOffset": 18, "endOffset": 22}, {"referenceID": 32, "context": "17) in [36]) that can be solved using Newton\u2019s method.", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "Data-driven adaptation of dictionaries using the K-SVD scheme has also been shown to be beneficial for image compression, compared to fixed analytical transforms [7].", "startOffset": 162, "endOffset": 165}, {"referenceID": 31, "context": "In particular, we train a 64\u00d764 SST [35], a 128\u00d764 OCTOBOS [44], as well as a 64\u00d7 64 square (synthesis) dictionary and a 64\u00d7 128 overcomplete dictionary using KSVD [11], using the same training patches and sparsity level as for FRIST.", "startOffset": 36, "endOffset": 40}, {"referenceID": 40, "context": "In particular, we train a 64\u00d764 SST [35], a 128\u00d764 OCTOBOS [44], as well as a 64\u00d7 64 square (synthesis) dictionary and a 64\u00d7 128 overcomplete dictionary using KSVD [11], using the same training patches and sparsity level as for FRIST.", "startOffset": 59, "endOffset": 63}, {"referenceID": 7, "context": "In particular, we train a 64\u00d764 SST [35], a 128\u00d764 OCTOBOS [44], as well as a 64\u00d7 64 square (synthesis) dictionary and a 64\u00d7 128 overcomplete dictionary using KSVD [11], using the same training patches and sparsity level as for FRIST.", "startOffset": 164, "endOffset": 168}, {"referenceID": 30, "context": "Additionally, dictionary learning based representation requires synthesis sparse coding, which is more expensive compared to the cheap and exact sparse coding in the transform model-based methods [34].", "startOffset": 196, "endOffset": 200}, {"referenceID": 7, "context": "1 are compared with those obtained by the adaptive overcomplete K-SVD denoising scheme [11], adaptive SST denoising scheme [35] and the adaptive OCTOBOS denoising scheme [44].", "startOffset": 87, "endOffset": 91}, {"referenceID": 31, "context": "1 are compared with those obtained by the adaptive overcomplete K-SVD denoising scheme [11], adaptive SST denoising scheme [35] and the adaptive OCTOBOS denoising scheme [44].", "startOffset": 123, "endOffset": 127}, {"referenceID": 40, "context": "1 are compared with those obtained by the adaptive overcomplete K-SVD denoising scheme [11], adaptive SST denoising scheme [35] and the adaptive OCTOBOS denoising scheme [44].", "startOffset": 170, "endOffset": 174}, {"referenceID": 31, "context": "Image \u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] FRIST PSNR", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "Image \u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] FRIST PSNR", "startOffset": 33, "endOffset": 37}, {"referenceID": 40, "context": "Image \u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] FRIST PSNR", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "\u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] PSNR", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "\u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] PSNR", "startOffset": 27, "endOffset": 31}, {"referenceID": 40, "context": "\u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] PSNR", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "For the adaptive SST and OCTOBOS denoising methods, we follow the same parameter settings as used in the previous works [35, 44].", "startOffset": 120, "endOffset": 128}, {"referenceID": 40, "context": "For the adaptive SST and OCTOBOS denoising methods, we follow the same parameter settings as used in the previous works [35, 44].", "startOffset": 120, "endOffset": 128}, {"referenceID": 40, "context": "6 quickly degrades as the number of transforms (in the collection/union) or clusters to be learned from a set of noisy image patches is increased [44].", "startOffset": 146, "endOffset": 150}, {"referenceID": 38, "context": "Although we focused our comparisons here on related adaptive sparse modeling methods, a very recent work [42] shows that combining transform learning based denoising with non-local similarity models leads to better denoising performance, and outperforms the state-of-the-art BM3D denoising method [9].", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": "Although we focused our comparisons here on related adaptive sparse modeling methods, a very recent work [42] shows that combining transform learning based denoising with non-local similarity models leads to better denoising performance, and outperforms the state-of-the-art BM3D denoising method [9].", "startOffset": 297, "endOffset": 300}, {"referenceID": 38, "context": "A further extension of the work in [42] to include FRIST learning is of interest and could potentially provide even greater advantages, but we leave this detailed investigation to future work.", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "the fixed 2D DCT, adaptive SST [35], and adaptive OCTOBOS [44] respectively, and evaluate the inpainting performance these alternatives.", "startOffset": 31, "endOffset": 35}, {"referenceID": 40, "context": "the fixed 2D DCT, adaptive SST [35], and adaptive OCTOBOS [44] respectively, and evaluate the inpainting performance these alternatives.", "startOffset": 58, "endOffset": 62}, {"referenceID": 44, "context": "The image inpainting results obtained by the FRIST based methods are also compared with those obtained by the cubic interpolation [48, 41] and patch smoothing [30] methods.", "startOffset": 130, "endOffset": 138}, {"referenceID": 37, "context": "The image inpainting results obtained by the FRIST based methods are also compared with those obtained by the cubic interpolation [48, 41] and patch smoothing [30] methods.", "startOffset": 130, "endOffset": 138}, {"referenceID": 26, "context": "The image inpainting results obtained by the FRIST based methods are also compared with those obtained by the cubic interpolation [48, 41] and patch smoothing [30] methods.", "startOffset": 159, "endOffset": 163}, {"referenceID": 26, "context": "The FRIST result is much improved, and also shows fewer artifacts compared to the patch smoothing [30] and adaptive SST results.", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "05 \u00d7 nP , and the other parameters were set similarly as for TL-MRI in [36].", "startOffset": 71, "endOffset": 75}, {"referenceID": 32, "context": "To speed up convergence, lower sparsity levels are used in the initial iterations [36].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 158, "endOffset": 162}, {"referenceID": 18, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 170, "endOffset": 174}, {"referenceID": 25, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 181, "endOffset": 185}, {"referenceID": 32, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 198, "endOffset": 202}, {"referenceID": 32, "context": "The parameter settings for these methods are as mentioned in [36].", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "We separately tuned the sparsity parameter for TL-MRI [36] for reconstructing Image 3 \u2217.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "+ The testing image data in this section were used and included in previous works [36, 53] with the data sources.", "startOffset": 82, "endOffset": 90}, {"referenceID": 49, "context": "+ The testing image data in this section were used and included in previous works [36, 53] with the data sources.", "startOffset": 82, "endOffset": 90}, {"referenceID": 32, "context": "\u2217 We observed improved reconstruction PSNR compared to the result obtained using the sparsity settings in [36].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 80, "endOffset": 84}, {"referenceID": 27, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 116, "endOffset": 120}, {"referenceID": 32, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 129, "endOffset": 133}, {"referenceID": 32, "context": "As we followed a reconstruction framework and parameters similar to those used by TL-MRI [36], the quality improvement obtained with FRIST-MRI is solely because the learned FRIST can serve as a better regularizer for MR image reconstruction compared to the single adaptive square transform in TL-MRI.", "startOffset": 89, "endOffset": 93}], "year": 2017, "abstractText": "Features based on sparse representation, especially using the synthesis dictionary model, have been heavily exploited in signal processing and computer vision. However, synthesis dictionary learning typically involves NP-hard sparse coding and expensive learning steps. Recently, sparsifying transform learning received interest for its cheap computation and its optimal updates in the alternating algorithms. In this work, we develop a methodology for learning Flipping and Rotation Invariant Sparsifying Transforms, dubbed FRIST, to better represent natural images that contain textures with various geometrical directions. The proposed alternating FRIST learning algorithm involves efficient optimal updates. We provide a convergence guarantee, and demonstrate the empirical convergence behavior of the proposed FRIST learning approach. Preliminary experiments show the promising performance of FRIST learning for sparse image representation, segmentation, denoising, robust inpainting, and compressed sensing-based magnetic resonance image reconstruction.", "creator": "LaTeX with hyperref package"}}}