{"id": "1511.06530", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications", "abstract": "Although the latest california - once server once presence CPU and GPU, taking turn convolutional atrophy networks (CNNs) for complex undertaking examples as ImageNet classed on enabled capabilities because challenging. To launch deep CNNs end user mobile, feel present without simple two effective projects to resize beginning entire CNN, because realize consider once - nine whole network compression. The proposed undertaken adjacent its previous must: (50) rank selection by variational Bayesian formula_11 integer, (1998) Tucker curvature to kernel formula_7, most (30) fine - tuned then fail accumulated loss included precision, but short hope we indeed could implemented method statement specific methodologies. We recognition first effectiveness large the eu scheme for safety from better according various sticky CNNs (AlexNet, VGGS, GoogLeNet, and VGG - year) on came asustek. Significant reductions with application size, platform-independent, their balance pollution are later, 15 the cuts beyond still triumph time accuracy. In large, understand address entered though cease current suggested moving 1? vs. formula_6, far is a end joint of 2003 synchronous country GoogLeNet as once as CNNs compressed others our initiated means.", "histories": [["v1", "Fri, 20 Nov 2015 09:20:08 GMT  (1272kb,D)", "http://arxiv.org/abs/1511.06530v1", null], ["v2", "Wed, 24 Feb 2016 11:52:12 GMT  (1272kb,D)", "http://arxiv.org/abs/1511.06530v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yong-deok kim", "eunhyeok park", "sungjoo yoo", "taelim choi", "lu yang", "dongjun shin"], "accepted": true, "id": "1511.06530"}, "pdf": {"name": "1511.06530.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yong-Deok Kim", "Taelim Choi", "Lu Yang", "Dongjun Shin"], "emails": ["d.j.shin}@samsung.com", "sungjoo.yoo}@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deployment of convolutional neural networks (CNNs) for computer vision tasks on mobile devices is gaining more and more attention. On mobile applications, it is typically assumed that training is performed on the server and test or inference is executed on the mobile devices. One of the most critical issues in mobile applications of CNNs is that mobile devices have strict constraints in terms of computing power, battery, and memory capacity. Thus, it is imperative to obtain CNNs tailored to the limited resources of mobile devices.\nDeep neural networks are known to be over-parameterized, which facilitates convergence to good local minima of the loss function during training (Hinton et al., 2012; Denil et al., 2013). To improve test-time performance on mobile devices, such redundancy can be removed from the trained networks without noticeable impact on accuracy. Recently, there are several studies to apply lowrank approximations to compress CNNs by exploiting redundancy (Jaderberg et al., 2014; Denton et al., 2014; Lebedev et al., 2015). Such compressions typically focus on convolution layers since they dominate total computation cost especially in deep neural networks (Simonyan & Zisserman, 2015; Szegedy et al., 2015). Existing methods, though effective in reducing the computation cost of a single convolutional layer, introduce a new challenge called whole network compression which aims at compressing the entire network.\nar X\niv :1\n51 1.\n06 53\n0v 1\n[ cs\n.C V\n] 2\nWhole network compression: It is nontrivial to compress whole and very deep CNNs for complex tasks such as ImageNet classification. Recently, Zhang et al. (2015b;a) showed that entire convolutional layers can be accelerated with \u201casymmetric (3d)\u201d decomposition. In addition, they also presented the effective rank selection and optimization method. Although their proposed decomposition of layers can be easily implemented in popular development tools (e.g. Caffe, Torch, and Theano), the rank selection and optimization parts still require because they consist of multiple steps and depend on the output of previous layers. In this paper, we present much simpler but still powerful whole network compression scheme which takes entire convolutional and fully-connected layers into account.\nContribution: This paper makes the following major contributions.\n\u2022 We propose a one-shot whole network compression scheme which consists of simple three steps: (1) rank selection, (2) low-rank tensor decomposition, and (3) fine-tuning. \u2022 In the proposed scheme, Tucker decomposition (Tucker, 1966) with the rank determined by\na global analytic solution of variational Bayesian matrix factorization (VBMF) (Nakajima et al., 2012) is applied on each kernel tensor. Note that we simply minimize the reconstruction error of linear kernel tensors instead of non-linear responses. Under the Tucker decomposition, the accumulated loss of accuracy can be sufficiently recovered by using fine-tuning with ImageNet training dataset. \u2022 Each step of our scheme can be easily implemented using publicly available tools, (Naka-\njima, 2015) for VBMF, (Bader et al., 2015) for Tucker decomposition, and Caffe for finetuning.\n\u2022 We evaluate various compressed CNNs (AlexNet, VGG-S,GoogLeNet, and VGG-16) on both Titan X and smartphone. Significant reduction in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy.\n\u2022 By analysing power consumption over time, we observe interesting behaviours of 1 \u00d7 1 convolution which is the key operation in our compressed model as well as in inception module of GoogLeNet. Although the 1 \u00d7 1 convolution is mathematically simple operation, it is considered to lack in cache efficiency, hence it is the root cause of gap between theoretical and practical speed up ratios.\nThis paper is organized as follows. Section 2 reviews related work. Section 3 explains our proposed scheme. Section 4 gives experimental results. Section 5 summarizes the paper."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 CNN COMPRESSION", "text": "CNN usually consists of convolutional layers and fully-connected layers which dominate computation cost and memory consumption respectively. After Denil et al. (2013) showed the possibility of removing the redundancy of neural networks, several CNN compression techniques have been proposed. A recent study (Denton et al., 2014) showed that the weight matrix of a fully-connected layer can be compressed by applying truncated singular value decomposition (SVD) without significant drop in the prediction accuracy. More recently, various methods based on vector quantization (Gong et al., 2014), hashing techniques (Chen et al., 2015), circulant projection (Cheng et al., 2015), and tensor train decomposition (Novikov et al., 2015) were proposed and showed better compression capability than SVD. To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers.\nConcurrent with our work, Zhang et al. (2015b) presented \u201casymmetric (3d) decomposition\u201d to accelerate the entire convolutional layers, where the original D \u00d7 D convolution is decomposed to D \u00d7 1, 1 \u00d7 D, and 1 \u00d7 1 convolution. In addition, they also present a rank selection method based on PCA accumulated energy and an optimization method which minimizes the reconstruction error of non-linear responses. In the extended version (Zhang et al., 2015a), the additional finetuning of entire network was considered for further improvement. Compared with these works, our proposed scheme is different in that (1) Tucker decomposition is adopted to compress the entire\nconvolutional and fully-connected layers, (2) the kernel tensor reconstruction error is minimized instead of non-linear response, (3) a global analytic solution of VBMF (Nakajima et al., 2012) is applied to determine the rank of each layer, and (4) a single run of fine-tuning is performed to account for the accumulation of errors.\nA pruning approach (Han et al., 2015b;a) also aims at reducing the total amount of parameters and operations in the entire network. Pruning based approaches can give significant reductions in parameter size and computation workload. However, it is challenging to achieve runtime speed-up with conventional GPU implementation as mentioned in (Han et al., 2015a).\nOrthogonal to model level compression, implementation level approaches were also proposed. The FFT method was used to speed-up convolution (Mathieu et al., 2013). In (Vanhoucke et al., 2011), CPU code optimizations to speed-up the execution of CNN are extensively explored."}, {"heading": "2.2 TENSOR DECOMPOSITION", "text": "A tensor is a multi-way array of data. For example, a vector is 1-way tensor and a matrix is 2- way tensor. Two of the most popular tensor decomposition models are CANDECOMP/PARAFAC model (Carroll & Chang, 1970; Harshman & Lundy, 1994; Shashua & Hazan, 2005) and Tucker model (Tucker, 1966; De Lathauwer et al., 2000; Kim & Choi, 2007). In this paper, we extensively use Tucker model for whole network compression. Tucker decomposition is a higher order extension of the singular value decomposition (SVD) of matrix, in the perspective of computing the orthonormal spaces associated with the different modes of a tensor. It simultaneously analyzes mode-n matricizations of the original tensor, and merges them with the core tensor as illustrated in Fig. 1.\nIn our whole network compression scheme, we apply Tucker-2 decomposition, which is also known as GLRAM (Ye, 2005), from the second convolutional layer to the first fully connected layers. For the other layers, we apply Tucker-1 decomposition, which is equivalent to SVD. For more information on the tensor decomposition, the reader is referred to the survey paper (Kolda & Bader, 2009)."}, {"heading": "3 PROPOSED METHOD", "text": "Fig. 2 illustrates our one-shot whole network compression scheme which consists of three steps: (1) rank selection; (2) Tucker decomposition; (3) fine-tuning. In the first step, we analyze principal subspace of mode-3 and mode-4 matricization of each layer\u2019s kernel tensor with global analytic variational Bayesian matrix factorization. Then we apply Tucker decomposition on each layer\u2019s kernel tensor with previously determined rank. Finally, we fine-tune the entire network with standard back-propagation."}, {"heading": "3.1 TUCKER DECOMPOSITION ON KERNEL TENSOR", "text": "Convolution kernel tensor: In CNNs, the convolution operation maps an input (source) tensor X of size H \u00d7W \u00d7 S into output (target) tensor Y of size H \u2032 \u00d7W \u2032 \u00d7 T using the following linear mapping:\nYh\u2032,w\u2032,t = D\u2211 i=1 D\u2211 j=1 S\u2211 s=1 Ki,j,s,t Xhi,wj ,s,\nhi = (h \u2032 \u2212 1)\u2206 + i\u2212 P and wj = (w\u2032 \u2212 1)\u2206 + j \u2212 P, (1)\nwhere K is a 4-way kernel tensor of size D \u00d7D \u00d7 S \u00d7 T , \u2206 is stride, and P is zero-padding size. Tucker Decomposition: The rank-(R1, R2, R3, R4) Tucker decomposition of 4-way kernel tensor K has the form:\nKi,j,s,t = R1\u2211\nr1=1 R2\u2211 r2=1 R3\u2211 r3=1 R4\u2211 r4=1 C\u2032r1,r2,r3,r4U (1) i,r1 U (2) j,r2 U (3)s,r3U (4) t,r4 ,\nwhere C\u2032 is a core tensor of size R1 \u00d7 R2 \u00d7 R3 \u00d7 R4 and U (1), U (2), U (3), and U (4) are factor matrices of sizes D \u00d7R1, D \u00d7R2, S \u00d7R3, and T \u00d7R4, respectively. In the Tucker decomposition, every mode does not have to be decomposed. For example, we do not decompose mode-1 and mode-2 which are associated with spatial dimensions because they are already quite small (D is typically 3 or 5). Under this variant called Tucker-2 decomposition (Tucker, 1966), the kernel tensor is decomposed to:\nKi,j,s,t = R3\u2211\nr3=1 R4\u2211 r4=1 Ci,j,r3,r4 U (3)s,r3 U (4) t,r4 , (2)\nwhere C is a core tensor of size D \u00d7 D \u00d7 R3 \u00d7 R4. After substituting (2) into (1), performing rearrangements and grouping summands, we obtain the following three consecutive expressions for the approximate evaluation of the convolution (1):\nZh,w,r3 = S\u2211\ns=1\nU (3)s,r3 Xh,w,s, (3)\nZ \u2032h\u2032,w\u2032,r4 = D\u2211 i=1 D\u2211 j=1 R3\u2211 r3=1 Ci,j,r3,r4 Zhi,wj ,r3 , (4)\nYh\u2032,w\u2032,t = R4\u2211\nr4=1\nU (4) t,r4 Z \u2032 h\u2032,w\u2032,r4 , (5)\nwhere Z and Z \u2032 are intermediate tensors of sizes H \u00d7W \u00d7R3 and H \u2032 \u00d7W \u2032 \u00d7R4, respectively. 1 \u00d7 1 convolution: As illustrated in Fig. 3, computing Z from X in (3) as well as Y from Z \u2032 in (5) is 1 \u00d7 1 convolutions that essentially perform pixel-wise linear re-combination of input maps. It is introduced in network-in-network (Lin et al., 2014) and extensively used in inception module of GoogLeNet (Szegedy et al., 2015). Note that computing (3) is similar to inception module in the sense that D \u00d7 D convolution is applied after dimensional reduction with 1 \u00d7 1 convolution, but different in the sense that there is no non-linear ReLU function between (3) and (4). In addition, similar to (Zhang et al., 2015b;a), we compute smaller intermediate output tensor Z \u2032 in (4) and then recover its size in (5). The Tucker-2 decomposition naturally integrates two compression techniques.\nComplexity analysis: The convolution operation in (1) requires D2ST parameters and D2STH \u2032W \u2032 multiplication-addition operations. With Tucker decomposition, compression ratio M and speed-up ratio E are given by:\nM = D2ST\nSR3 +D2R3R4 + TR4 and E =\nD2STH \u2032W \u2032\nSR3HW +D2R3R4H \u2032W \u2032 + TR4H \u2032W \u2032 ,\nand these are bounded by ST/R3R4.\nTucker vs CP: Recently, CP decomposition is applied to approximate the convolution layers of CNNs for ImageNet which consist of 8 layers (Denton et al., 2014; Lebedev et al., 2015). However it cannot be applied to the entire layers and the instability issue of low-rank CP decomposition is reported (De Silva & Lim, 2008; Lebedev et al., 2015). On the other hand, our kernel tensor approximation with Tucker decomposition can be successfully applied to the entire layers of AlexNet, VGG-S, GoogLeNet, and VGG-16"}, {"heading": "3.2 RANK SELECTION WITH GLOBAL ANALYTIC VBMF", "text": "The rank-(R3, R4) are very important hyper-parameters which control the trade-off between performance (memory, speed, energy) improvement and accuracy loss. Instead of selecting the rank(R3, R4) by time consuming trial-and-error, we considered data-driven one-shot decision via empirical Bayes (MacKay, 1992) with automatic relevance determination (ARD) prior (Tipping, 2001).\nAt the first time, we designed probabilistic Tucker model which is similar to (M\u00f8rup & Hansen, 2009), and applied empirical variational Bayesian learning. However, the rank selection results were severely unreliable because they heavily depend on (1) initial condition, (2) noise variance estimation policy, and (3) threshold setting for pruning. For this reason, we decided to use a suboptimal but highly reproducible approach.\nWe employed recently developed global analytic solutions for variational Bayesian matrix factorization (VBMF) (Nakajima et al., 2013). The global analytic VBMF is a very promising tool because it can automatically find noise variance, rank and even provide theoretical condition for perfect rank recovery (Nakajima et al., 2012). We determined the rank R3 and R4 by applying global analytic VBMF on mode-3 matricization (of size S \u00d7 TD2) and mode-4 matricization (of size T \u00d7D2S) of kernel tensor K, respectively."}, {"heading": "3.3 FINE-TUNING", "text": "Because we minimize the reconstruction error of linear kernel tensors instead of non-linear responses, the accuracy is significantly dropped after whole network compression (e.g. more than 50% in the case of AlexNet). However, as shown in Fig. 4, we can easily recover the accuracy by using fine-tuning with ImageNet training dataset. We observed that accuracy is recovered quickly in one epoch. However, more than 10 epochs are required to recover the original accuracy.\nWhile (Lebedev et al., 2015; Zhang et al., 2015a) reported difficulty on finding a good SGD learning rate, our single learning rate scheduling rule works well for various compressed CNNs. In our experiment, we set the base learning \u03b7 = 10\u22123 and decrease it by a factor of 10 every 5 epochs. Because of GPU memory limitation, we set the batch size: 128, 128, 64, and 32 for AlexNet, VGG-S, GoogLeNet, and VGG-16, respectively.\nWe also tried to train the architecture of the approximated model from scratch on the ImageNet training dataset. At this time, we only tested the Gaussian random initialization and it did not work. We leave the use of other initialization methods (Glorot & Bengio, 2010; He et al., 2015) and batch normalization (Ioffe & Szegedy, 2015) as future work."}, {"heading": "4 EXPERIMENTS", "text": "We used four representative CNNs, AlexNet, VGG-S, GoogLeNet, and VGG-16, which can be downloaded on Berkeley\u2019s Caffe model zoo. In the case of inception module of GoogLeNet, we only compressed the 3\u00d7 3 convolution kernel which is the main computational part. In the case of VGG-16, we only compressed the convolutional layers as done in (Zhang et al., 2015a). Top-5 single-view accuracy is measured using 50,000 validation images from the ImageNet2012 dataset.\nWe performed experiments on Nvidia Titan X (for fine-tuning and runtime comparison on Caffe+cuDNN2) and a smartphone, Samsung Galaxy S6 (for the comparison of runtime and energy consumption). The application processor of the smartphone (Exynos 7420) is equipped with a mobile GPU, ARM Mali T760. Compared with the GPU used on Titan X, the mobile GPU gives 35 times (6.6TFlops vs 190GFlops) lower computing capability and 13 times (336.5GBps vs 25.6GBps) smaller memory bandwidth.\nIn order to run Caffe models on the mobile GPU, we developed a mobile version of Caffe called S-Caffe (Caffe for Smart mobile devices) where all the Caffe models can run on our target mobile devices (for the moment, Samsung smartphones) without modification. We also developed an Android App which performs image classification by running each of the four CNNs (AlexNet, VGG-S, GoogLeNet , and VGG-16) on the smartphone.\nWe measured the power consumption of whole smartphone which is decomposed into the power consumption of GPU, main memory, and the other components of smartphone, e.g., ARM CPU, display, modem, etc. and give component-level analysis, especially, the power consumption of GPU and main memory (see supplementary material for details of measurement environment). The measurement results of runtime and energy consumption are the average of 50 runs."}, {"heading": "4.1 OVERALL RESULTS", "text": "Table 1 shows the overall results for the three CNNs. Our proposed scheme gives \u00d75.46/ \u00d7 2.67 (AlexNet), \u00d77.40/ \u00d7 4.80 (VGG-S), \u00d71.28/ \u00d7 2.06 (GoogLeNet), and \u00d71.09/ \u00d7 4.93 (VGG16) reductions in total weights and FLOPs, respectively. Such reductions offer \u00d71.42 \u223c \u00d73.68 (\u00d71.23 \u223c \u00d72.33) runtime improvements on the smartphone (Titan X). We report the energy consumption of mobile GPU and main memory. The smartphone gives larger reduction ratios (e.g., \u00d73.41 vs. \u00d72.72 for AlexNet) for energy consumption than runtime. We will give a detailed analysis in the following subsection.\nComparison with Zhang et al. (2015a)\u2019s method: The accuracy of our compressed VGG-16 is 89.40% for theoretical \u00d74.93 speed-up, and it is comparable to the 89.6% (88.9%) for theoretical \u00d74 (\u00d75) speed-up in (Zhang et al., 2015a)."}, {"heading": "4.2 LAYERWISE ANALYSIS", "text": "Tables 2, 3, 4 and 5 1 show the detailed comparisons. Each row has two results (the above one for the original uncompressed CNN and the other one for the compressed CNN), and improvements. For instance, in Table 2, the second convolutional layer having the input and output channel dimensions of 48\u00d7 2 and 128\u00d7 2 is compressed to give the Tucker-2 ranks of 25\u00d7 2 and 59\u00d7 2, which reduces the amount of weights from 307K to 91K. After compression, a layer in the compressed network performs three matrix multiplications. We give the details of three matrix multiplications for each of weights, FLOPs, and runtime. For instance, on the smartphone (column S6 in Table 2), the second convolutional layer of compressed AlexNet takes 10.53ms which is decomposed to 0.8ms, 7.43ms and 2.3ms for the three matrix multiplications.\nIn Tables 2, 3, 4 and 5 we have two observations. Observation 1: Given a compressed network, the smartphone tends to give larger performance gain than the Titan X. It is mainly because the mobile GPU on the smartphone lacks in thread-level parallelism. It has 24 times less number of threads (2K vs. 48K in terms of maximum number of threads) than that in Titan X. Compression reduces the amount of weights thereby reducing cache conflicts and memory latency. Due to the small thread-level parallelism, the reduced latency has more impact on the performance of threads on the mobile GPU than that on Titan X.\n1See supplementary material for Tables 3, 4 and 5\nObservation 2: Given the same compression rate, the smartphone tends to exhibit larger performance gain at fully-connected layers than at convolutional layers. We think it is also due to the reduced cache conflicts enabled by network compression as explained above. Especially, in the case of fully-connected layers, the effect of weight reduction can give more significant impact because the weights at the fully-connected layers are utilized only once, often called dead-on-arrival (DoA) data. In terms of cache performance, such DoA data are much more harmful than convolution kernel weights (which are reused multiple times). Thus, weight reduction at the fully connected layer can give more significant impact on cache performance thereby exhibiting more performance improvement than in the case of weight reduction at convolutional layers."}, {"heading": "4.3 ENERGY CONSUMPTION ANALYSIS", "text": "Fig. 5 compares power consumption on the smartphone. Each network gives the power consumption of GPU and main memory. Note that we enlarged the time axis of compressed networks for a better comparison. We omitted VGG-16 since VGG-16 gives similar trend.\nThe figure shows that the compression reduces power consumption (Y axis) as well as runtime (X axis), which explains why the reduction in energy consumption is larger than that in runtime in Table 1. Fig. 5 also shows that the GPU power consumption of compressed CNN is smaller than that of uncompressed CNN. We analyze this due to the extensive usage of 1 \u00d7 1 convolutions in the compressed CNN. When executing convolutions, we apply optimization techniques such as Caffeinated convolution(Chellapilla et al., 2006). In such a case, in terms of cache efficiency, 1\u00d7 1 convolutions are inferior to the other convolutions, e.g., 3 \u00d7 3, 5 \u00d7 5, etc. since the amount of data reuse is proportional to the total size of convolution kernel. Thus, 1 \u00d7 1 convolutions tend to incur more cache misses than the other larger convolutions. Cache misses on the mobile GPU without sufficient thread level parallelism often incur stall cycles, i.e., make GPU cores idle consuming less power, which reduces the power consumption of GPU core during the execution of 1\u00d71 convolution.\nAs mentioned earlier, our proposed method improves cache efficiency by reducing the amount of weights. However, 1 \u00d7 1 convolutions have negative impacts on cache efficiency and GPU core utilization. Fig. 5 shows the combined effects. In the compressed networks, the power consumption of GPU core is reduced by 1\u00d7 1 convolutions and tends to change more frequently due to frequent executions of 1\u00d71 convolution while, in the case of uncompressed networks, especially for AlexNet and VGG-S, the power consumption of GPU core tends to be stable during the execution of convolutional layers. In the case of uncompressed GoogLeNet, the power consumption tends to fluctuate. It is mainly because (1) GoogLeNet consists of many small layers (about 100 building blocks), and (2) 1\u00d7 1 convolutions are heavily utilized. The three compressed networks show similar behavior of frequent fluctuations in power consumption mostly due to 1 \u00d7 1 convolutions. Fig. 5 also shows that, in the uncompressed networks, fully connected layers incur significant amount of power consumption in main memory. It is because the uncompressed networks, especially AlexNet and VGG-S have large numbers (more than tens of mega-bytes) of weights in fully connected layers which incur significant amount of memory accesses. As shown in Fig. 5, the proposed scheme reduces the amount of weights at fully connected layers thereby reducing the power consumption in main memory."}, {"heading": "5 DISCUSSION", "text": "Although we can obtain very promising results with one-shot rank selection, it is not fully investigated yet whether the selected rank is really optimal or not. As future work, we will investigate the optimality of our proposed scheme. The 1 \u00d7 1 convolution is a key operation in our compressed model as well as in inception module of GoogLeNet. Due to its characteristics, e.g. channel compression and computation reduction, we expect that 1\u00d7 1 convolutions will become more and more popular in the future. However, as shown in our experimental results, it lacks in cache efficiency. We expect further investigations are required to make best use of 1x1 convolutions.\nWhole network compression is challenging due to the large design space and associated long design time. In order to address this problem, we propose a one-shot compression scheme which applies a single general low-rank approximation method and a global rank selection method. Our oneshot compression enables fast design and easy implementation with publicly available tools. We evaluated the effectiveness of the proposed scheme on a smartphone and Titan X. The experiments show that the proposed scheme gives, for four CNNs (AlexNet, VGG-S, GoogLeNet, and VGG16) average \u00d72.72 (\u00d73.41), \u00d73.68 (\u00d74.26), \u00d71.42 (\u00d71.60), and \u00d73.34 (\u00d73.53) improvements in runtime (energy consumption) on the smartphone."}, {"heading": "A EXPERIMENTAL SETUP", "text": "This section describes the details of experimental setup including the measurement system for power consumption and exemplifies the measured data.\nA.1 MEASUREMENT SYSTEM\nFig. 6 shows the power measurement system. As the figure shows, it consists of a probe board (left) having a Samsung Galaxy S6 smartphone and power probes and a monitor board (right). The probe board provides 8 probes which are connected to the power pins of application processor (to be introduced below). The power profiling monitor samples, for each power probe, the electric current every 0.1ms and gives power consumption data with time stamps.\nFig. 7 illustrates the main board of the smartphone (Fig. 7 (a)), the application processor chip package (red rectangle in Fig. 2 (a)) consisting of the application processor and main memory (LPDDR4 DRAM) in the smartphone (Fig. 7 (b)), and a simplified block diagram of the application processor (Fig. 7 (c)). The power measurement system provides the probes connected to the power pins for mobile GPU (ARM Mali T760 in Fig. 7 (c)) and main memory (LPDDR4 DRAM in Fig. 7 (b)).\nA.2 MEASURED DATA EXAMPLE: GoogLeNet CASE\nFig. 8 shows the power consumption data for the uncompressed GoogLeNet. We also identified the period of each layer, e.g., the first convolutional layer (Conv 1 in the figure), and the first Inception module (i3a). As mentioned in our submission, the profile of power consumption shows more frequent fluctuations in Inception modules than in the convolutional layers. The figure also shows that the first two convolutional layers (Conv 1 and Conv 2) occupy about 1/4 of total energy consumption while Inception modules consume about 3/4 of total energy consumption."}, {"heading": "B LAYERWISE ANALYSIS", "text": "We report detailed comparison results VGG-S, GoogLeNet, and VGG-16."}], "references": [{"title": "Matlab tensor toolbox version 2.6", "author": ["Bader", "Brett W", "Kolda", "Tamara G"], "venue": "Available online,", "citeRegEx": "Bader et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bader et al\\.", "year": 2015}, {"title": "High performance convolutional neural networks for document processing", "author": ["Chellapilla", "Kumar", "Puri", "Sidd", "Simard", "Patrice"], "venue": "In Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft,", "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "Compressing neural networks with the hashing trick", "author": ["Chen", "Wenlin", "Wilson", "James T", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "arXiv preprint arXiv:1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Fast neural networks with circulant projections", "author": ["Cheng", "Yu", "Felix X", "Feris", "Rogerio S", "Kumar", "Sanjiv", "Choudhary", "Alok", "Chang", "Shih-Fu"], "venue": "arXiv preprint arXiv:1502.03436,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "A multilinear singular value decomposition", "author": ["De Lathauwer", "Lieven", "De Moor", "Bart", "Vandewalle", "Joos"], "venue": "SIAM journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "Tensor rank and the ill-posedness of the best low-rank approximation problem", "author": ["De Silva", "Vin", "Lim", "Lek-Heng"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Silva et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2008}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "de Freitas", "Nando"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily L", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "A deep neural network compression pipeline: Pruning, quantization, huffman encoding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William J"], "venue": "arXiv preprint arXiv:1506.02626,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Parafac: Parallel factor analysis", "author": ["Harshman", "Richard A", "Lundy", "Margaret E"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Harshman et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Harshman et al\\.", "year": 1994}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Nonnegative Tucker decomposition", "author": ["Kim", "Y.-D", "S. Choi"], "venue": "In Proceedings of the IEEE CVPR2007 Workshop on Component Analysis Methods, Minneapolis, Minnesota,", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Tamara G", "Bader", "Brett W"], "venue": "SIAM review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["Lebedev", "Vadim", "Ganin", "Yaroslav", "Rakhuba", "Maksim", "Oseledets", "Ivan", "Lempitsky", "Victor"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lebedev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2015}, {"title": "Bayesian interpolation", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Fast training of convolutional networks through ffts", "author": ["Mathieu", "Michael", "Henaff", "Mikael", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Automatic relevance determination for multi-way models", "author": ["M\u00f8rup", "Morten", "Hansen", "Lars Kai"], "venue": "Journal of Chemometrics,", "citeRegEx": "M\u00f8rup et al\\.,? \\Q2009\\E", "shortCiteRegEx": "M\u00f8rup et al\\.", "year": 2009}, {"title": "Variational Bayesian matrix factorization version", "author": ["Nakajima", "Shinichi"], "venue": "URL https: //sites.google.com/site/shinnkj23/downloads", "citeRegEx": "Nakajima and Shinichi.,? \\Q2015\\E", "shortCiteRegEx": "Nakajima and Shinichi.", "year": 2015}, {"title": "Perfect dimensionality recovery by variational bayesian pca", "author": ["Nakajima", "Shinichi", "Tomioka", "Ryota", "Sugiyama", "Masashi", "Babacan", "S Derin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nakajima et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakajima et al\\.", "year": 2012}, {"title": "Global analytic solution of fully-observed variational bayesian matrix factorization", "author": ["Nakajima", "Shinichi", "Sugiyama", "Masashi", "Babacan", "S Derin", "Tomioka", "Ryota"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Nakajima et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakajima et al\\.", "year": 2013}, {"title": "Non-negative tensor factorization with applications to statistics and computer vision", "author": ["Shashua", "Amnon", "Hazan", "Tamir"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Shashua et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shashua et al\\.", "year": 2005}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Sparse bayesian learning and the relevance vector machine", "author": ["Tipping", "Michael E"], "venue": "The journal of machine learning research,", "citeRegEx": "Tipping and E.,? \\Q2001\\E", "shortCiteRegEx": "Tipping and E.", "year": 2001}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["Tucker", "Ledyard R"], "venue": null, "citeRegEx": "Tucker and R.,? \\Q1966\\E", "shortCiteRegEx": "Tucker and R.", "year": 1966}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Generalized low rank approximations of matrices", "author": ["Ye", "Jieping"], "venue": "Machine Learning,", "citeRegEx": "Ye and Jieping.,? \\Q2005\\E", "shortCiteRegEx": "Ye and Jieping.", "year": 2005}, {"title": "Accelerating very deep convolutional networks for classification and detection", "author": ["Zhang", "Xiangyu", "Zou", "Jianhua", "He", "Kaiming", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1505.06798,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Efficient and accurate approximations of nonlinear convolutional networks. 2015b", "author": ["Zhang", "Xiangyu", "Zou", "Jianhua", "Ming", "Xiang", "He", "Kaiming", "Sun", "Jian"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Deep neural networks are known to be over-parameterized, which facilitates convergence to good local minima of the loss function during training (Hinton et al., 2012; Denil et al., 2013).", "startOffset": 145, "endOffset": 186}, {"referenceID": 6, "context": "Deep neural networks are known to be over-parameterized, which facilitates convergence to good local minima of the loss function during training (Hinton et al., 2012; Denil et al., 2013).", "startOffset": 145, "endOffset": 186}, {"referenceID": 16, "context": "Recently, there are several studies to apply lowrank approximations to compress CNNs by exploiting redundancy (Jaderberg et al., 2014; Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 110, "endOffset": 177}, {"referenceID": 7, "context": "Recently, there are several studies to apply lowrank approximations to compress CNNs by exploiting redundancy (Jaderberg et al., 2014; Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 110, "endOffset": 177}, {"referenceID": 19, "context": "Recently, there are several studies to apply lowrank approximations to compress CNNs by exploiting redundancy (Jaderberg et al., 2014; Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 110, "endOffset": 177}, {"referenceID": 28, "context": "Such compressions typically focus on convolution layers since they dominate total computation cost especially in deep neural networks (Simonyan & Zisserman, 2015; Szegedy et al., 2015).", "startOffset": 134, "endOffset": 184}, {"referenceID": 24, "context": "\u2022 In the proposed scheme, Tucker decomposition (Tucker, 1966) with the rank determined by a global analytic solution of variational Bayesian matrix factorization (VBMF) (Nakajima et al., 2012) is applied on each kernel tensor.", "startOffset": 169, "endOffset": 192}, {"referenceID": 0, "context": "\u2022 Each step of our scheme can be easily implemented using publicly available tools, (Nakajima, 2015) for VBMF, (Bader et al., 2015) for Tucker decomposition, and Caffe for finetuning.", "startOffset": 111, "endOffset": 131}, {"referenceID": 7, "context": "A recent study (Denton et al., 2014) showed that the weight matrix of a fully-connected layer can be compressed by applying truncated singular value decomposition (SVD) without significant drop in the prediction accuracy.", "startOffset": 15, "endOffset": 36}, {"referenceID": 9, "context": "More recently, various methods based on vector quantization (Gong et al., 2014), hashing techniques (Chen et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 2, "context": ", 2014), hashing techniques (Chen et al., 2015), circulant projection (Cheng et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 3, "context": ", 2015), circulant projection (Cheng et al., 2015), and tensor train decomposition (Novikov et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 7, "context": "To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers.", "startOffset": 131, "endOffset": 198}, {"referenceID": 16, "context": "To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers.", "startOffset": 131, "endOffset": 198}, {"referenceID": 19, "context": "To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers.", "startOffset": 131, "endOffset": 198}, {"referenceID": 4, "context": "After Denil et al. (2013) showed the possibility of removing the redundancy of neural networks, several CNN compression techniques have been proposed.", "startOffset": 6, "endOffset": 26}, {"referenceID": 2, "context": ", 2014), hashing techniques (Chen et al., 2015), circulant projection (Cheng et al., 2015), and tensor train decomposition (Novikov et al., 2015) were proposed and showed better compression capability than SVD. To speed up the convolutional layers, several methods based on low-rank decomposition of convolutional kernel tensor were proposed (Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2015), but they compress only single or a few layers. Concurrent with our work, Zhang et al. (2015b) presented \u201casymmetric (3d) decomposition\u201d to accelerate the entire convolutional layers, where the original D \u00d7 D convolution is decomposed to D \u00d7 1, 1 \u00d7 D, and 1 \u00d7 1 convolution.", "startOffset": 29, "endOffset": 505}, {"referenceID": 24, "context": "convolutional and fully-connected layers, (2) the kernel tensor reconstruction error is minimized instead of non-linear response, (3) a global analytic solution of VBMF (Nakajima et al., 2012) is applied to determine the rank of each layer, and (4) a single run of fine-tuning is performed to account for the accumulation of errors.", "startOffset": 169, "endOffset": 192}, {"referenceID": 21, "context": "The FFT method was used to speed-up convolution (Mathieu et al., 2013).", "startOffset": 48, "endOffset": 70}, {"referenceID": 31, "context": "In (Vanhoucke et al., 2011), CPU code optimizations to speed-up the execution of CNN are extensively explored.", "startOffset": 3, "endOffset": 27}, {"referenceID": 28, "context": ", 2014) and extensively used in inception module of GoogLeNet (Szegedy et al., 2015).", "startOffset": 62, "endOffset": 84}, {"referenceID": 7, "context": "Tucker vs CP: Recently, CP decomposition is applied to approximate the convolution layers of CNNs for ImageNet which consist of 8 layers (Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 137, "endOffset": 180}, {"referenceID": 19, "context": "Tucker vs CP: Recently, CP decomposition is applied to approximate the convolution layers of CNNs for ImageNet which consist of 8 layers (Denton et al., 2014; Lebedev et al., 2015).", "startOffset": 137, "endOffset": 180}, {"referenceID": 19, "context": "However it cannot be applied to the entire layers and the instability issue of low-rank CP decomposition is reported (De Silva & Lim, 2008; Lebedev et al., 2015).", "startOffset": 117, "endOffset": 161}, {"referenceID": 25, "context": "We employed recently developed global analytic solutions for variational Bayesian matrix factorization (VBMF) (Nakajima et al., 2013).", "startOffset": 110, "endOffset": 133}, {"referenceID": 24, "context": "The global analytic VBMF is a very promising tool because it can automatically find noise variance, rank and even provide theoretical condition for perfect rank recovery (Nakajima et al., 2012).", "startOffset": 170, "endOffset": 193}, {"referenceID": 19, "context": "While (Lebedev et al., 2015; Zhang et al., 2015a) reported difficulty on finding a good SGD learning rate, our single learning rate scheduling rule works well for various compressed CNNs.", "startOffset": 6, "endOffset": 49}, {"referenceID": 13, "context": "We leave the use of other initialization methods (Glorot & Bengio, 2010; He et al., 2015) and batch normalization (Ioffe & Szegedy, 2015) as future work.", "startOffset": 49, "endOffset": 89}, {"referenceID": 33, "context": "Comparison with Zhang et al. (2015a)\u2019s method: The accuracy of our compressed VGG-16 is 89.", "startOffset": 16, "endOffset": 37}, {"referenceID": 1, "context": "When executing convolutions, we apply optimization techniques such as Caffeinated convolution(Chellapilla et al., 2006).", "startOffset": 93, "endOffset": 119}], "year": 2015, "abstractText": "Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1\u00d7 1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.", "creator": "LaTeX with hyperref package"}}}