{"id": "1511.06855", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Unsupervised learning of object semantic parts from internal states of CNNs by population encoding", "abstract": "In now letters, better make a furthermore for understanding created monitor stylized. Convolutional Neural Networks (CNNs) employed down e.g.. We hypothesize that first service is primarily across include epithelial physiological own apply new simple clustering technique set salt this possible, full we asked \\ emph {population encoding }. The population encoding technique so into made bucketful it gives merely - CNN month multiple layers latter the network several watching the implicit presence same mid - level precise most semantics distributed in full neurotransmitters opinions. Our qualitative computer-based show that most self-referential can milk time - longer personal waterproof put but graphically limits than the leggings this produce high single - filter fire-arms. Moreover, know comprehensive correlation laboratory using the true key past paraphrases of still PASCAL3D + autoethnography truthfully the visualizing turned professionalism with superiority of population encoding over entire - filter detectors, while came preparation the object - main detection. We also recording say week processes main obviously uncover. exemplified u.s.-china followed been shoreline fluid using the parts confirm by 60 encoding clumps. Finally, also same the impressions gained although beyond addition, tell giving to various include directions which continue whereby far may have each but understanding of the CNN ' this issue subjective.", "histories": [["v1", "Sat, 21 Nov 2015 09:02:21 GMT  (4295kb,D)", "http://arxiv.org/abs/1511.06855v1", "This is the submission for ICLR 2016"], ["v2", "Thu, 7 Jan 2016 22:10:52 GMT  (5168kb,D)", "http://arxiv.org/abs/1511.06855v2", "This is the submission for ICLR 2016"], ["v3", "Sat, 12 Nov 2016 13:37:07 GMT  (8201kb,D)", "http://arxiv.org/abs/1511.06855v3", null]], "COMMENTS": "This is the submission for ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jianyu wang", "zhishuai zhang", "cihang xie", "vittal premachandran", "alan yuille"], "accepted": false, "id": "1511.06855"}, "pdf": {"name": "1511.06855.pdf", "metadata": {"source": "CRF", "title": "DISCOVERING INTERNAL REPRESENTATIONS FROM OBJECT-CNNS USING POPULATION ENCODING", "authors": ["Jianyu Wang", "Zhishuai Zhang", "Vittal Premachandran", "Alan Yuille"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep Neural Networks have been greatly successful at learning powerful internal representations of the data. The strength of these internal representations have been endorsed by the state-of-theart performance that a specific type of Deep Networks, namely, Convolutional Neural Networks (CNNs) have helped achieve on various real-world machine learning problems. CNNs have been especially successful at many difficult computer vision tasks such as image classification, as showcased by Krizhevsky et al. (2012) and Simonyan & Zisserman (2015), at object detection as shown by Girshick et al. (2014), and, at semantic segmentation as demonstrated by Long et al. (2015) and Chen et al. (2015), among other tasks. Deep Networks have enabled end-to-end learning from raw data due to a handful of well-performing recipes such as convolutions, Dropout, and Rectified Linear Units (ReLUs). In spite of the fact that we have developed these robust engineering techniques to facilitate end-to-end learning of deep networks, leading to better performance on various applications, our understanding of why CNNs work so well and how CNNs represent visual patterns is still at an early stage. One of the important questions that remains unanswered is how does a CNN represent data internally? When it sees an object, does it understand the concept of parts of an object? If so, does it understand the rules of compositionality of these parts? Such questions are extremely important to ask since the answers to these questions will allow us to build more powerful AI systems that can reason about their tasks.\nThere have been several works on trying to understand how CNNs work. The thrust of these works have been in visualizing the internal neuronal activations independently as done by Zeiler & Fergus (2014), Yosinski et al. (2015) and Zhou et al. (2015). The idea that neurons can independently represent concepts is an old idea from Neuroscience. Barlow (1972) advocates extreme localist theories in which each concept is represented by a single neuronal unit. Such hypothetical neurons that respond to a complex but specific concept or object were called the \u201cgrandmother cells\u201d. These neurons fire when they see a specific entity, (e.g. one\u2019s grandmother) and do not fire when they see\nar X\niv :1\n51 1.\n06 85\n5v 1\n[ cs\n.L G\n] 2\n1 N\nov 2\n01 5\nother concepts. Moreover, such grandmother cells were supposed to fire independent of the input modality. The lack of an answer to the \u201ctremendous connectivity puzzle\u201d that will bind information from disparate sources raises questions about the specificity of individual neurons, thereby indicating that the concept of a \u201cgrandmother cell\u201d could be a biological impossibility.\nOn the other hand, a competing argument to the \u201cgrandmother cell\u201d lies in the argument of distributed representation. The distributed representation argument claims that concepts are encoded by a distributed pattern of activities spread across multiple neurons (Georgopoulos et al. (1986)). One advantage of representing concepts by a distributed representation is that it leads to automatic generalization. As mentioned by Hinton (1986), when the weights in the network are updated to incorporate new concepts, the changes affect the knowledge associated with other concepts that are represented by similar neuronal activity patterns. Other advantages of the distributed representation lies in the fact that it is more robust to rely on the activity of a population of neurons to encode information than relying on a single perishable neuronal cell. A more detailed debate between these two types of representations can be found in Rumelhart et al. (1986) and McClelland et al. (1987).\nIn this work, we lean towards the distributed representation argument and propose a simple clustering method to extract the information encoded by a population of neurons. The motivation for our work (in addition to the arguments mentioned above in favor of distributed representations) arises from the fact that multiple neurons (filters)1 act on the same part of the image (or intermediate layer features) and many of their responses are non-zero. To extract the information from the responses of a population of filters, we make use of clustering. Clustering is an extremely popular technique in computer vision that is used to group patterns into meaningful concepts. We construct a dictionary of CNN activations, in each layer of the CNN, by clustering the responses of a population of filters present in the respective layer, and call this encoding as population encoding. We will explain the population encoding strategy in more detail in Section 3.\nFrom our experiments and visualizations, we show that population encoding can automatically extract mid-level semantic part concepts (e.g. car tires, aeroplane wing tips, license plates, etc.). Our visualizations show that the data points (image patches) assigned to a particular cluster largely correspond to a part/sub-part of the input object. In addition, our visualizations also show that image patches extracted from population encoding are more consistent with each other than the patches extracted by looking at the top responses of single neurons independently. We also find that the dictionaries learnt from different layers of the CNN encode different levels of object part semantics.\nWe then move beyond mere visualizations and show the power of population encoding, over single neuron encodings, by performing exhaustive quantitative evaluations. Using the key point annotations from Pascal3D+ dataset, we evaluate each cluster as the key point detector, and select the best cluster for each key point. We adopt the precision/recall evaluation and calculate the average precision (AP) as is commonly used in object detection literature. The dictionary elements (clusters) that we obtain from population encoding achieve reasonably good average precision when evaluated as object part detectors. And the results are much better than those obtained using single neuron encodings. Finally, we explore the compositional relations between two neighboring layers of CNN.\nWe organize the rest of the paper as follows. In Section 2 we point to other works that aim to understand the workings of a CNN. The details of population encoding are presented in Section 3. We compare the population encoding and single filter encoding by visualization in Section 4. We describe a stringent quantitative evaluation strategy in Section 5 and present the results in Section 6. In Section 7 we present some preliminary experiments that showcase the compositionality of higher level parts using the sub-parts learnt at lower levels. Finally, we conclude the paper in Section 8 by pointing to future research directions."}, {"heading": "2 RELATED WORK", "text": "There have been several works on trying to understand CNNs. The thrust of these works has been towards obtaining an understanding through visualizations. Erhan et al. (2009) visualized the models by finding images that maximized the neuron activities using gradient descent in the image space. Zeiler & Fergus (2014) proposed DeConvNet-based visualization strategy that mapped intermediate features activations back to the image space using the Deconvolution Networks of Zeiler et al.\n1We use the terms neurons and filters interchangeably in this paper.\n(2011). Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images. They also release a toolbox that helps visualize what any individual neuron at any intermediate layer likes to see. Mahendran & Vedaldi (2015) showed that incorporating strong natural image priors to the optimization procedure produces images that look more like real images. Moreover, Mahendran & Vedaldi (2015)\u2019s optimization procedure differs from others in that they do not aim to produce an image that maximizes the activation of an individual neuron, but aim to produce natural-looking images whose bank of filter responses in a particular layer are similar to the bank of filter responses from a target image. Although they use a population of filter responses, their only aim is in producing better visualizations. We propose a method to extract the knowledge that is distributed over a population of neurons.\nThe following works probe the CNN using other interesting approaches. Szegedy et al. (2013) brought out the brittleness in CNNs by showing that one can change the CNN\u2019s prediction by adding an imperceptible amount of adversarial noise to the input image. Moreover, they suggested that it might be the space of the features, and not the individual units, that contains the semantic information in the higher layers of the network. Similarly, Nguyen et al. (2015) generated images using evolutionary algorithms that are unrecognizable to humans but can easily fool CNNs.\nWhile the concept of distributed representations has existed for a long time, the exploration of its existence in CNNs has been fairly limited. One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging.\nDiscovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)). More recently, Li et al. (2015) used CNN features with association rule algorithm to discover mid-level visual elements, and evaluated them on object/scene classification task. Their method uses a CNN only as a feature extractor without exploring how the intermediate layer CNN filters capture the mid-level visual elements. Xiao et al. (2015) and Simon & Rodner (2015) used the activations from the intermediate layers of the CNN to find object parts, which they then use in the task of fine-grained object classification. This method was based on the assumption that a single filter from an intermediate layer can detect parts. Zhou et al. (2015), looked at the hierarchy between object and scenes and claimed that object detectors emerge from training CNNs on scenes. Their \u201cobject detectors\u201d are grandmother cell-like filters for which they provide quantitative evaluations by asking workers on Amazon Mechanical Turk (AMT) to evaluate the performance of the detector. In comparison, we show that mid-level object part structures can be extracted using population encoding representation at intermediate layers of a CNN, in a purely unsupervised manner. Moreover, we evaluate these mid-level object part structures by testing on key point annotations of object parts without human labor."}, {"heading": "3 POPULATION ENCODING", "text": "In this section, we describe how one can extract information that is distributed over a population of filters. To this end, we develop a simple technique and name it population encoding. In our experiments, we restrict ourselves to object-CNNs and show that object parts can be discovered from the entrails of an object-CNN. Our method is generic enough to be used for other scenarios as well (e.g. scenes). We begin by extracting features produced by all the filters from a particular intermediate\nlayer in the CNN using object images of a particular category. We then cluster these features using K-means++ (Arthur & Vassilvitskii (2007)) to obtain a dictionary of mid-level elements. These dictionary elements, when visualized, correspond to semantic part structures of the objects."}, {"heading": "3.1 DICTIONARY LEARNING BY CLUSTERING", "text": "For an image x \u2208 RW\u00d7H\u00d73 with spatial resolution W \u00d7H , an intermediate layer, `, would produce a response, f` \u2208 RW`\u00d7H`\u00d7N` , where W` \u00d7H` is the spatial resolution of the intermediate layer `\u2019s response and N` is the number of filters in that layer. We randomly sample a number of population responses, pi,j` \u2208 RN` , over the spatial grid (i, j) of the layer\u2019s response, f`. Each p i,j ` corresponds to a patch from the original image with the theoretical receptive field, which can be computed for any intermediate layer `. We extract the population responses from all images in the training dataset.\nOwing to the hypothesis that similar patches in the images produce similar feature responses, we aim to identify those similar image patches by grouping feature responses that are similar to each other. In our simple strategy, we cluster the population responses, pi,j` , using the K-means++ algorithm to group together similar population responses. Each population vector, pi,j` , corresponds to a patch in the original image. We use the theoretical receptive field to crop out patches from the original image in order to visualize the consistency among the patches assigned to a cluster. Figure 1 shows a visualization of some example mid-level visual concepts/patterns. We can see that these patches correspond to meaningful mid-level semantic parts of the parent object. The patches are randomly extracted from the top patches assigned to the respective cluster center.\nSelecting the number of clusters for clustering the population responses is a hard task. Therefore, we first set K (the number of clusters) to the number of filters in the layer ` and use a greedy algorithm to merge the clusters. The merging requires us to rank the clusters based on their level of importance. Good clusters are those that are tight (small intra-cluster distance) and well-separated from other clusters (large inter-cluster distance). The \u201cgoodness\u201d of a cluster, k, can be computed using the Davies-Bouldin index,\nDB(k) = max m6=k \u03c3k + \u03c3m d(ck, cm) , (1)\nwhere ck and cm denote the centers of clusters k andm respectively, and d(ck, cm) is the Euclidean distance between the two cluster centers. \u03c3k and \u03c3k denote the average distance of all data points in clusters k and m to their respective cluster centers, i.e.,\n\u03c3k = 1\nnk \u2211 p\u2208Ck d(p, ck), (2)\nwhere Ck is the set of data points that are assigned to cluster k, and nk is the size of Ck. A good cluster is the one which is compact, i.e. small \u03c3k, and is well-separated from other clusters i.e, large d(ck, cm)\u2200m 6= k. Thus, the smaller the DB(k) the better the cluster. To discourage the selection of trivial clusters with a single data point (or a small number of data points) assigned to it, we also consider the number of points assigned to a cluster,\nCNT(k) = nk\u2211 m nm\n(3)\nwhile ranking it. The final score that we assign to each cluster is a weighted sum of the \u201cgoodness\u201d measure and the normalized count,\nScore(k) = \u03bbCNT(k)\u2212 DB(k). (4)\nAfter having a rank of all initial clusters, we adopt a greedy method to merge similar clusters. The algorithm starts from the highest-ranked cluster, which we call the target cluster. Among all the data points in this target cluster, we obtain a radius, r, which encloses 95% of the points assigned to the target cluster. If there exists any other cluster lower in the rank with more than 50% of its data points whose distances to the center of the target cluster are smaller than r, we merge that cluster into the target cluster. We repeat this procedure until all the initial clusters have been iterated through.\nWe learn the mid-level part dictionaries for layers pool1-pool5 using the VGG-16 network. In the next section, we provide some visualizations to show the various mid-level concepts that our population encoding strategy is able to learn. We provide detailed quantitative results in Section 6."}, {"heading": "4 VISUALIZATION", "text": "In Figure 1,we show a visualization of some example mid-level concepts learnt from the pool3-pool5 layer for different parent object categories. Each row corresponds to a particular layer and we show three typical clusters for each layer with four example patches. Notice that population encoding is able to extract the representative mid-level parts of the parent object category.\nWe now provide visual comparisons of the concepts learnt by population encoding and compare it with single-filter grandmother cell-like representation. Figure 2 provides example visualizations from pool3-pool5. We do not visualize the pool1 and pool2 clusters since their receptive fields are too small. The top two rows in each subfigure shows patches assigned to two clusters from population encoding and the bottom two rows show the patches producing the highest activations in two individual filters. Also, in each subfigure, the first six patches are the \u201cbest\u201d patches (closest to the cluster center or with the highest activation for a single filter) and the final six patches are randomly sampled from a pool of patches assigned to the respective cluster/filter. A general trend to notice is that the patches assigned to clusters appear more compact (look at the minimal variation in the appearance patterns of the randomly sampled patches), as opposed to the patches that produce high activations for single filters. These clusters/filters achieve high AP value when tested as key point detectors by our quantitative evaluation, which will be shown in later section.\nAs for the patches from pool5 layer\u2019s cluster, they look more similar within the effective receptive field (red bounding box) than looking at the entire patch, which is very large. We obtain the effective receptive field for both clusters and single filters by applying the deconv operation in Zeiler & Fergus (2014), and then threshing the deconv responses. The effective receptive field captures the region of pixels which are most responsible for the cluster or single filter response. Similar idea is in Zhou et al. (2015) using a different method. We can easily see that the effective receptive field of pool5 is much smaller than the corresponding theoretical receptive field size."}, {"heading": "5 QUANTITATIVE EVALUATION STRATEGY", "text": "The visualizations in the previous sections show that clusters of population responses can capture the mid-level semantics of objects. We move beyond mere visualizations and provide rigorous quantitative evaluations, which show that the clusters can be used as detectors of mid-level object parts. In this section we explain the evaluation strategy and we showcase the results in the next section.\nSince the clusters correspond to representative object parts, we aim to test their ability to act as object part detectors. We make use of the precision/recall evaluation method and calculate the average precision (AP) of each cluster over multiple detection thresholds. We test each cluster against each key point and assign the one with the best AP as the detector of that key point. We use a simple nearest neighbor approach for computing the distance between cluster centers and test patches in the feature domain of the corresponding layer. We use this distance as the detection score for the corresponding cluster. The decision to adopt the nearest neighbor strategy and not to use sophisticated detection algorithms was a conscious decision since we want to extract the raw power of these internal representations. Perhaps, using fancier detection schemes might lead to better performance in terms of key point detection, but those strategies are left for future explorations.\nIf the detected patch center is close to the key point position within a threshold, we consider it as a true positive. In order to suppress duplicate detections, we use non-maximal suppression (NMS). From the visualizations in the previous section, the fourth layer clusters largely capture the semantic level of key point annotations in the dataset that we use (PASCAL3D+). Moreover, the theoretical receptive field of pool4 allows for fair spread of the patches on the objects corresponding to the density of the key point annotations in the dataset. Therefore we decide to use the pool4 dictionaries to obtain the quantitative evaluation results presented in the following section."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": "Dataset: Evaluating mid-level parts requires us to use a dataset with part-level annotations. Therefore, we use annotations from PASCAL3D+ dataset (Xiang et al. (2014)) for our evaluation. This dataset augments PASCAL VOC2012 images with the images from the ImageNet dataset and annotates the objects with 2D key point positions for 12 rigid object categories (e.g. cars, aeroplanes, bicycles, etc.). In this work, we experiment on six object categories: car, aeroplane, bicycle, mo-\ntorbike, bus, and train. For each category, we split the ImageNet images into two halves and use one half for learning the dictionaries and the other half for testing. Since we are trying to learn the mid-level representations of object parts, we crop the objects using the bounding box annotations and use only the cropped-out regions of the images.\nThe key point annotations from Pascal3D+ are side-dependent, e.g., the left-front tires and the rightfront tires are labeled with different labels. However, the receptive field of a CNN\u2019s intermediate layer is not big enough to make such fine-grained distinctions. Therefore, we merge the key point annotations into a coarser level of granularity. The merging is done on the following occasions. a) We merge the left/right fine-scale distinctions (e.g. left/right headlight) into a single class (e.g. headlight). b) We group visually similar key points into a single class (e.g. the four wheels of a car). And, c) we discard key points that rarely occur in the images.\nBaselines: We provide two baselines for comparison. i) We test for the presence of grandmother cell-like neurons and evaluate the detection performance of each single filter independently, while trying to detect the mid-level key points. We use the neuron responses as the detection scores when computing AP values. We report results for the best performing neurons. ii) To provide an upper-bound on the detection capacity of unsupervised part-learning techniques, we report results obtained by using strong supervision. We extract the population responses for patches centered at the key point annotation (and randomly sampled patches for a separate background class) and train a multi-class linear SVM to discriminate the different key point patches.\nImplementation Details: In the experiments, we use the Caffe toolbox (Jia et al. (2014)) and VGG16 network Simonyan & Zisserman (2015) pre-trained on ImageNet objects as our object-CNN. We build separate dictionaries for different object classes. For each image, we crop the object using the bounding box annotation and resize the short side to 224 pixels. We normalize each population response (feature vector) using L2-normalization for both dictionary clustering and detection, since we find that L2-normalization gives much better results. We do not use L2-normalization for single neuron evaluation. We analyse the effect of normalization on the supplementary material.\nQuantitative Results: While evaluating the dictionaries on the key point annotations, it is important to select the dictionary layer carefully. Based on our visualizations, we find that the pool4 dictionaries best capture the level of semantic annotations in the PASCAL3D+ dataset. Therefore, we primarily report the results using the dictionaries learnt from pool4. In Section 6.1 we perform some diagnostic experiments where we report results from other layers.\nTable 1 reports the various average precision (AP) values that we obtain. For each key point from a particular object category, we show the AP achieved by the best dictionary element (cluster) and sin-\ngle neuron. \u2018single filter\u2019 refers to the baseline we obtain when testing single filters as detectors and \u2018cluster\u2019 refers to the use of dictionary elements as detectors. \u2018cluster-merge\u2019 refers to case where we use the merged set of clusters, while \u2018cluster-full\u2019 refers to the unmerged case. Finally,\u2018strong\u2019 refers to the baseline where we use strong supervision to train linear SVMs to act as key point detectors. In Figure 3 we plot the PR curves for some key points of all the six object categories.\nThe general trend that we observe from the numbers in the table is that the population encoding is a stronger way to encode the mid-level representations in comparison to single neurons. Merged cluster performs worse than full cluster, since merged clusters might not be as pure and representative as the unmerged clusters. However, merged cluster provides a much more compact representation due to much smaller dictionary size. Also we can see that the clusters obtained in an unsupervised way by population encoding achieve reasonably good AP values for most categories. The only exception is Aeroplane, which is very difficult due to large elevation and viewpoint variations. All methods have low AP values on aeroplanes."}, {"heading": "6.1 DIAGNOSTIC EXPERIMENTS", "text": "In this subsection we present results from some diagnostic experiments that we conduct to enable a better understanding of the clusters obtained from population encoding. Currently, our diagnostic experiments are performed only on cars. Experiments on other object categories is left as future work.\nEffect of Intermediate Layer: In Table 1, we present the results using the dictionary elements learnt from the pool4 layer. We select the pool4 dictionaries since they corresponded well to the semantic level of the annotated key points in the PASCAL3D+ dataset. In Table 2, we present the results obtained using the dictionaries learnt from pool3 and pool5 layers. Looking at the numbers from pool3, it is clear that its dictionaries are not able to capture semantic level of the part annotations. This is because the receptive field of pool3 is too small to capture the entire part. Surprisingly, pool5 gives similar results as pool4 layer. Upon careful examination, we see that pool5 does better than pool4 when the key points correspond to \u201cbig\u201d parts (e.g. windshield) and worse than pool4 when the key points correspond to smaller parts (e.g. wheels), the size of which is more appropriate for pool4. This shows that different layers of the CNN are capturing different levels and scales of object parts. It is interesting to see how pool5 layer dictionaries perform on other object categories, and we leave this as future work.\n1 2 3 4 5 6 7 mAP pool3 .80 .28 .13 .22 .26 .22 .16 .29 pool5 .83 .61 .27 .55 .29 .45 .29 .47\nTable 2: AP values obtained using dictionaries learnt from two other layers on the Car key points. Please refer to the Table 1\u2019s caption for key point number-name mapping.\nMethod 1 2 3 4 5 6 7 mAP single-neuron .96 .65 .61 .59 .59 .38 .27 .58 cluster-merge .99 .75 0.63 .61 .70 .49 .39 .65 cluster-full .99 .75 0.61 0.73 .70 .54 .39 .67 strong .99 .87 .82 .88 .84 .62 .41 .78\nTable 3: AP values after controlling for viewpoint on cars. Please refer to Table 1\u2019s caption for key point number-name mapping.\nViewpoint Control: While visualizing the clusters, we find that many dictionary elements are viewpoint specific. For example, front-view wheels and the side-view wheels were in separate clusters. Therefore, we test the performance of the clusters by controlling for viewpoint. We divide the test set into five viewpoint bins: front, front-side, side, rear-side, and rear. Then, for each viewpoint we run the same evaluation procedure as described above. Table 3 shows the best viewpoint result for each key point. The numbers clearly shows a significant jump in detection capabilities of both single filters and our clusters (population encoding still outperforms single-filter detectors). These results point to a lack of viewpoint invariance in the internal representation of the CNNs."}, {"heading": "7 FURTHER STUDIES", "text": ""}, {"heading": "7.1 TIGHT CLUSTER WITH LOW AVERAGE PRECISION", "text": "When examining the clusters with low AP value, we find that many of them are tight clusters. This means that these clusters capture visual patterns very well but they do not correspond to semantic key points. Figure 4 illustrates this face. On the left is an image of a car. The two bounding boxes are detections of two different clusters, which are visualized on the right with six patches. We can see that the two clusters capture horizontal structures and textures respectively, but they do not match the key point annotations in the dataset and thus have low AP values. This is due to the limitation of the current key point annotations. In order to fully evaluate the clusters of different layers, we should have key point annotations with high degree of coverage and at different semantic levels."}, {"heading": "7.2 COMPOSITIONAL RELATION BETWEEN ADJACENT LAYERS", "text": "Having obtained the mid-level parts from the population encoding method, we can further discover the compositional relations between adjacent layers (e.g., pool3 and pool4). If a cluster from a higher layer fires (gets detected), we can calculate which clusters in the lower layer also fire within the higher layer cluster\u2019s receptive field. In other words, we want to compute the conditional probability of lower layer clusters firing spatially nearby given the firing of some higher layer cluster. Figure 5 shows two examples of such compositional relations between pool4 and pool3. We can see that the selected top pool3 clusters capture parts of the patterns that the pool4 cluster represents. Note that we can apply this compositional analysis to any adjacent layer pairs."}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "In this paper, we hypothesized that the visual information is distributed over multiple CNN filters and proposed a simple clustering technique namely, population encoding, to extract it. We demonstrated the superiority of population encoding strategy over single-filter encoding both qualitatively and quantitatively. Our experiments provide new insights into the internal representations of CNNs, and pave way for many interesting future directions of exploration.\nWhile this work focused on object-CNNs, testing whether the findings in this work carry over to scene-CNNs would be interesting. Also, since our diagnostic experiments showed signs of a lack of viewpoint invariance in the internal representations, further studies in understanding the invariance properties of deep networks will help identify the CNN\u2019s strengths and weaknesses. Moreover, we saw that the performance of the clusters varies across different layers indicating that different layers of the CNN capture different levels of semantic parts. Exploring the interplay between neighboring layers should lead to more interesting findings. Finally, with the ability to extract parts in an unsupervised manner using population encoding, future works can develop techniques to parse new objects automatically.\nRed$\nGreen$\nFigure 4: Two tight clusters which do not correspond to key points well.\n0 50 100 150 200 250 300 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nconditional probability\nP ro\nb\nlower layer clusters 0 50 100 150 200 250 300\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nconditional probability\nP ro\nb\nlower layer clusters\nFigure 5: This figure provides two examples of the compositional relations between a pool4 cluster and many pool3 clusters. For each example, Top Left: conditional probability, Bottom Left: four patches from a pool4 cluster, Right: top-4 pool3 clusters that occurs most frequently within the considered pool4 cluster\u2019s receptive field. Each row corresponds to a cluster and for each cluster we show four example patches."}, {"heading": "A. L2-NORMALIZATION", "text": "In this subsection, we make a comparison between using L2-normalization as a preprocessing step and without L2-normalization. From Table 4, we can see that L2-normalization is critical for clusters from population encoding strategy. We then hypothesize that it is the population response direction that captures visual information, instead of the population response magnitude. For the single neuron case, we have also tried similar L2-normalization across filters for each spatial position, and used the normalized response as detection score. But we find that L2-normalization in single neuron case makes little difference (slightly worse) in terms of the AP values."}, {"heading": "B. RESULTS ON PASCAL VOC", "text": "In this section, we provide the evaluation results of both the population encoding and single filter on cars and aeroplanes from Pascal VOC dataset. Since we only consider non-occluded bounding box objects, there is limited number of non-occluded objects in Pascal for cars and aeroplanes. So we use all the Pascal data for test, while using the dictionaries trained on imagenet for the corresponding category. We use key point annotations from Bourdev et al. (2010) since they provide much richer key points. The AP values are shown in Table 5 and PR curves are shown in Figure 6. We can see that methods using population encoding are better than single neuron encoding, while the strong supervision method always achieves the best AP value."}], "references": [{"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["Agrawal", "Pulkit", "Girshick", "Ross B", "Malik", "Jitendra"], "venue": "In ECCV,", "citeRegEx": "Agrawal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2014}, {"title": "k-means++: The advantages of careful seeding", "author": ["Arthur", "David", "Vassilvitskii", "Sergei"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Arthur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2007}, {"title": "Single units and sensation: A neuron doctrine for perceptual psychology? Perception, 1:371\u2013394", "author": ["H Barlow"], "venue": null, "citeRegEx": "Barlow,? \\Q1972\\E", "shortCiteRegEx": "Barlow", "year": 1972}, {"title": "Detecting people using mutually consistent poselet activations", "author": ["Bourdev", "Lubomir D", "Maji", "Subhransu", "Brox", "Thomas", "Malik", "Jitendra"], "venue": "In ECCV, pp", "citeRegEx": "Bourdev et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bourdev et al\\.", "year": 2010}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Chen", "Liang-Chieh", "Papandreou", "George", "Kokkinos", "Iasonas", "Murphy", "Kevin", "Yuille", "Alan L"], "venue": "In ICLR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "In Technical Report", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Neuronal population coding of movement", "author": ["Georgopoulos", "Apostolos P", "Schwartz", "Andrew B", "Kettner", "Ronald E"], "venue": "direction. Science,", "citeRegEx": "Georgopoulos et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Georgopoulos et al\\.", "year": 1986}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross B", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In CVPR, pp", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Learning distributed representations of concepts", "author": ["Hinton", "Geoffrey E"], "venue": "In Proceedings of the Eights Annual Conference of the Cognitive Science Society,", "citeRegEx": "Hinton and E.,? \\Q1986\\E", "shortCiteRegEx": "Hinton and E.", "year": 1986}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Blocks that shout: Distinctive parts for scene classification", "author": ["Juneja", "Mamta", "Vedaldi", "Andrea", "CV Jawahar", "Zisserman", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Juneja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Juneja et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Mid-level deep pattern mining", "author": ["Li", "Yao", "Liu", "Lingqiao", "Shen", "Chunhua", "van den Hengel", "Anton"], "venue": "In CVPR, pp", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In CVPR,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Understanding deep image representations by inverting them", "author": ["Mahendran", "Aravindh", "Vedaldi", "Andrea"], "venue": "In CVPR, pp", "citeRegEx": "Mahendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran et al\\.", "year": 2015}, {"title": "Parallel distributed processing: Psychological and biological models, volume 2", "author": ["McClelland", "James L", "Rumelhart", "David E", "Group", "PDP Research"], "venue": null, "citeRegEx": "McClelland et al\\.,? \\Q1987\\E", "shortCiteRegEx": "McClelland et al\\.", "year": 1987}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Nguyen", "Anh Mai", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "In CVPR, pp", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, volume 1", "author": ["Rumelhart", "David E", "McClelland", "James L", "Group", "PDP Research"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Neural activation constellations: Unsupervised part model discovery with convolutional networks", "author": ["Simon", "Marcel", "Rodner", "Erik"], "venue": "CoRR, abs/1504.08289,", "citeRegEx": "Simon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simon et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In ICLR Workshop,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Unsupervised discovery of mid-level discriminative patches", "author": ["Singh", "Saurabh", "Gupta", "Abhinav", "Efros", "Alexei"], "venue": "In ECCV,", "citeRegEx": "Singh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2012}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian J", "Fergus", "Rob"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Beyond pascal: A benchmark for 3d object detection in the wild", "author": ["Xiang", "Yu", "Mottaghi", "Roozbeh", "Savarese", "Silvio"], "venue": "In WACV, pp", "citeRegEx": "Xiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2014}, {"title": "The application of two-level attention models in deep convolutional neural network for fine-grained image classification", "author": ["Xiao", "Tianjun", "Xu", "Yichong", "Yang", "Kuiyuan", "Zhang", "Jiaxing", "Peng", "Yuxin", "Zheng"], "venue": "In CVPR, pp", "citeRegEx": "Xiao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "Understanding neural networks through deep visualization", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Nguyen", "Anh Mai", "Fuchs", "Thomas", "Lipson", "Hod"], "venue": "In ICML Deep Learning Workshop,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In ECCV, pp", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Zeiler", "Matthew D", "Taylor", "Graham W", "Fergus", "Rob"], "venue": "In ICCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}, {"title": "Object detectors emerge in deep scene cnns", "author": ["Zhou", "Bolei", "Khosla", "Aditya", "Lapedriza", "\u00c0gata", "Oliva", "Aude", "Torralba", "Antonio"], "venue": "In ICLR,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "CNNs have been especially successful at many difficult computer vision tasks such as image classification, as showcased by Krizhevsky et al. (2012) and Simonyan & Zisserman (2015), at object detection as shown by Girshick et al.", "startOffset": 123, "endOffset": 148}, {"referenceID": 8, "context": "CNNs have been especially successful at many difficult computer vision tasks such as image classification, as showcased by Krizhevsky et al. (2012) and Simonyan & Zisserman (2015), at object detection as shown by Girshick et al.", "startOffset": 123, "endOffset": 180}, {"referenceID": 5, "context": "(2012) and Simonyan & Zisserman (2015), at object detection as shown by Girshick et al. (2014), and, at semantic segmentation as demonstrated by Long et al.", "startOffset": 72, "endOffset": 95}, {"referenceID": 5, "context": "(2012) and Simonyan & Zisserman (2015), at object detection as shown by Girshick et al. (2014), and, at semantic segmentation as demonstrated by Long et al. (2015) and Chen et al.", "startOffset": 72, "endOffset": 164}, {"referenceID": 3, "context": "(2015) and Chen et al. (2015), among other tasks.", "startOffset": 11, "endOffset": 30}, {"referenceID": 3, "context": "(2015) and Chen et al. (2015), among other tasks. Deep Networks have enabled end-to-end learning from raw data due to a handful of well-performing recipes such as convolutions, Dropout, and Rectified Linear Units (ReLUs). In spite of the fact that we have developed these robust engineering techniques to facilitate end-to-end learning of deep networks, leading to better performance on various applications, our understanding of why CNNs work so well and how CNNs represent visual patterns is still at an early stage. One of the important questions that remains unanswered is how does a CNN represent data internally? When it sees an object, does it understand the concept of parts of an object? If so, does it understand the rules of compositionality of these parts? Such questions are extremely important to ask since the answers to these questions will allow us to build more powerful AI systems that can reason about their tasks. There have been several works on trying to understand how CNNs work. The thrust of these works have been in visualizing the internal neuronal activations independently as done by Zeiler & Fergus (2014), Yosinski et al.", "startOffset": 11, "endOffset": 1137}, {"referenceID": 3, "context": "(2015) and Chen et al. (2015), among other tasks. Deep Networks have enabled end-to-end learning from raw data due to a handful of well-performing recipes such as convolutions, Dropout, and Rectified Linear Units (ReLUs). In spite of the fact that we have developed these robust engineering techniques to facilitate end-to-end learning of deep networks, leading to better performance on various applications, our understanding of why CNNs work so well and how CNNs represent visual patterns is still at an early stage. One of the important questions that remains unanswered is how does a CNN represent data internally? When it sees an object, does it understand the concept of parts of an object? If so, does it understand the rules of compositionality of these parts? Such questions are extremely important to ask since the answers to these questions will allow us to build more powerful AI systems that can reason about their tasks. There have been several works on trying to understand how CNNs work. The thrust of these works have been in visualizing the internal neuronal activations independently as done by Zeiler & Fergus (2014), Yosinski et al. (2015) and Zhou et al.", "startOffset": 11, "endOffset": 1161}, {"referenceID": 3, "context": "(2015) and Chen et al. (2015), among other tasks. Deep Networks have enabled end-to-end learning from raw data due to a handful of well-performing recipes such as convolutions, Dropout, and Rectified Linear Units (ReLUs). In spite of the fact that we have developed these robust engineering techniques to facilitate end-to-end learning of deep networks, leading to better performance on various applications, our understanding of why CNNs work so well and how CNNs represent visual patterns is still at an early stage. One of the important questions that remains unanswered is how does a CNN represent data internally? When it sees an object, does it understand the concept of parts of an object? If so, does it understand the rules of compositionality of these parts? Such questions are extremely important to ask since the answers to these questions will allow us to build more powerful AI systems that can reason about their tasks. There have been several works on trying to understand how CNNs work. The thrust of these works have been in visualizing the internal neuronal activations independently as done by Zeiler & Fergus (2014), Yosinski et al. (2015) and Zhou et al. (2015). The idea that neurons can independently represent concepts is an old idea from Neuroscience.", "startOffset": 11, "endOffset": 1184}, {"referenceID": 2, "context": "Barlow (1972) advocates extreme localist theories in which each concept is represented by a single neuronal unit.", "startOffset": 0, "endOffset": 14}, {"referenceID": 6, "context": "The distributed representation argument claims that concepts are encoded by a distributed pattern of activities spread across multiple neurons (Georgopoulos et al. (1986)).", "startOffset": 144, "endOffset": 171}, {"referenceID": 6, "context": "The distributed representation argument claims that concepts are encoded by a distributed pattern of activities spread across multiple neurons (Georgopoulos et al. (1986)). One advantage of representing concepts by a distributed representation is that it leads to automatic generalization. As mentioned by Hinton (1986), when the weights in the network are updated to incorporate new concepts, the changes affect the knowledge associated with other concepts that are represented by similar neuronal activity patterns.", "startOffset": 144, "endOffset": 320}, {"referenceID": 6, "context": "The distributed representation argument claims that concepts are encoded by a distributed pattern of activities spread across multiple neurons (Georgopoulos et al. (1986)). One advantage of representing concepts by a distributed representation is that it leads to automatic generalization. As mentioned by Hinton (1986), when the weights in the network are updated to incorporate new concepts, the changes affect the knowledge associated with other concepts that are represented by similar neuronal activity patterns. Other advantages of the distributed representation lies in the fact that it is more robust to rely on the activity of a population of neurons to encode information than relying on a single perishable neuronal cell. A more detailed debate between these two types of representations can be found in Rumelhart et al. (1986) and McClelland et al.", "startOffset": 144, "endOffset": 839}, {"referenceID": 6, "context": "The distributed representation argument claims that concepts are encoded by a distributed pattern of activities spread across multiple neurons (Georgopoulos et al. (1986)). One advantage of representing concepts by a distributed representation is that it leads to automatic generalization. As mentioned by Hinton (1986), when the weights in the network are updated to incorporate new concepts, the changes affect the knowledge associated with other concepts that are represented by similar neuronal activity patterns. Other advantages of the distributed representation lies in the fact that it is more robust to rely on the activity of a population of neurons to encode information than relying on a single perishable neuronal cell. A more detailed debate between these two types of representations can be found in Rumelhart et al. (1986) and McClelland et al. (1987). In this work, we lean towards the distributed representation argument and propose a simple clustering method to extract the information encoded by a population of neurons.", "startOffset": 144, "endOffset": 868}, {"referenceID": 5, "context": "Erhan et al. (2009) visualized the models by finding images that maximized the neuron activities using gradient descent in the image space.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Erhan et al. (2009) visualized the models by finding images that maximized the neuron activities using gradient descent in the image space. Zeiler & Fergus (2014) proposed DeConvNet-based visualization strategy that mapped intermediate features activations back to the image space using the Deconvolution Networks of Zeiler et al.", "startOffset": 0, "endOffset": 163}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN.", "startOffset": 11, "endOffset": 55}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images.", "startOffset": 11, "endOffset": 323}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images. They also release a toolbox that helps visualize what any individual neuron at any intermediate layer likes to see. Mahendran & Vedaldi (2015) showed that incorporating strong natural image priors to the optimization procedure produces images that look more like real images.", "startOffset": 11, "endOffset": 562}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images. They also release a toolbox that helps visualize what any individual neuron at any intermediate layer likes to see. Mahendran & Vedaldi (2015) showed that incorporating strong natural image priors to the optimization procedure produces images that look more like real images. Moreover, Mahendran & Vedaldi (2015)\u2019s optimization procedure differs from others in that they do not aim to produce an image that maximizes the activation of an individual neuron, but aim to produce natural-looking images whose bank of filter responses in a particular layer are similar to the bank of filter responses from a target image.", "startOffset": 11, "endOffset": 732}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images. They also release a toolbox that helps visualize what any individual neuron at any intermediate layer likes to see. Mahendran & Vedaldi (2015) showed that incorporating strong natural image priors to the optimization procedure produces images that look more like real images. Moreover, Mahendran & Vedaldi (2015)\u2019s optimization procedure differs from others in that they do not aim to produce an image that maximizes the activation of an individual neuron, but aim to produce natural-looking images whose bank of filter responses in a particular layer are similar to the bank of filter responses from a target image. Although they use a population of filter responses, their only aim is in producing better visualizations. We propose a method to extract the knowledge that is distributed over a population of neurons. The following works probe the CNN using other interesting approaches. Szegedy et al. (2013) brought out the brittleness in CNNs by showing that one can change the CNN\u2019s prediction by adding an imperceptible amount of adversarial noise to the input image.", "startOffset": 11, "endOffset": 1329}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images. They also release a toolbox that helps visualize what any individual neuron at any intermediate layer likes to see. Mahendran & Vedaldi (2015) showed that incorporating strong natural image priors to the optimization procedure produces images that look more like real images. Moreover, Mahendran & Vedaldi (2015)\u2019s optimization procedure differs from others in that they do not aim to produce an image that maximizes the activation of an individual neuron, but aim to produce natural-looking images whose bank of filter responses in a particular layer are similar to the bank of filter responses from a target image. Although they use a population of filter responses, their only aim is in producing better visualizations. We propose a method to extract the knowledge that is distributed over a population of neurons. The following works probe the CNN using other interesting approaches. Szegedy et al. (2013) brought out the brittleness in CNNs by showing that one can change the CNN\u2019s prediction by adding an imperceptible amount of adversarial noise to the input image. Moreover, they suggested that it might be the space of the features, and not the individual units, that contains the semantic information in the higher layers of the network. Similarly, Nguyen et al. (2015) generated images using evolutionary algorithms that are unrecognizable to humans but can easily fool CNNs.", "startOffset": 11, "endOffset": 1699}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors.", "startOffset": 115, "endOffset": 137}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion.", "startOffset": 115, "endOffset": 767}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging.", "startOffset": 115, "endOffset": 1038}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al.", "startOffset": 115, "endOffset": 1348}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)).", "startOffset": 115, "endOffset": 1373}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)). More recently, Li et al. (2015) used CNN features with association rule algorithm to discover mid-level visual elements, and evaluated them on object/scene classification task.", "startOffset": 115, "endOffset": 1407}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)). More recently, Li et al. (2015) used CNN features with association rule algorithm to discover mid-level visual elements, and evaluated them on object/scene classification task. Their method uses a CNN only as a feature extractor without exploring how the intermediate layer CNN filters capture the mid-level visual elements. Xiao et al. (2015) and Simon & Rodner (2015) used the activations from the intermediate layers of the CNN to find object parts, which they then use in the task of fine-grained object classification.", "startOffset": 115, "endOffset": 1719}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)). More recently, Li et al. (2015) used CNN features with association rule algorithm to discover mid-level visual elements, and evaluated them on object/scene classification task. Their method uses a CNN only as a feature extractor without exploring how the intermediate layer CNN filters capture the mid-level visual elements. Xiao et al. (2015) and Simon & Rodner (2015) used the activations from the intermediate layers of the CNN to find object parts, which they then use in the task of fine-grained object classification.", "startOffset": 115, "endOffset": 1745}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)). More recently, Li et al. (2015) used CNN features with association rule algorithm to discover mid-level visual elements, and evaluated them on object/scene classification task. Their method uses a CNN only as a feature extractor without exploring how the intermediate layer CNN filters capture the mid-level visual elements. Xiao et al. (2015) and Simon & Rodner (2015) used the activations from the intermediate layers of the CNN to find object parts, which they then use in the task of fine-grained object classification. This method was based on the assumption that a single filter from an intermediate layer can detect parts. Zhou et al. (2015), looked at the hierarchy between object and scenes and claimed that object detectors emerge from training CNNs on scenes.", "startOffset": 115, "endOffset": 2024}, {"referenceID": 28, "context": "Similar idea is in Zhou et al. (2015) using a different method.", "startOffset": 19, "endOffset": 38}, {"referenceID": 23, "context": "Therefore, we use annotations from PASCAL3D+ dataset (Xiang et al. (2014)) for our evaluation.", "startOffset": 54, "endOffset": 74}, {"referenceID": 9, "context": "Implementation Details: In the experiments, we use the Caffe toolbox (Jia et al. (2014)) and VGG16 network Simonyan & Zisserman (2015) pre-trained on ImageNet objects as our object-CNN.", "startOffset": 70, "endOffset": 88}, {"referenceID": 9, "context": "Implementation Details: In the experiments, we use the Caffe toolbox (Jia et al. (2014)) and VGG16 network Simonyan & Zisserman (2015) pre-trained on ImageNet objects as our object-CNN.", "startOffset": 70, "endOffset": 135}], "year": 2016, "abstractText": "In this paper, we provide a method for understanding the internal representations of Convolutional Neural Networks (CNNs) trained on objects. We hypothesize that the information is distributed across multiple neuronal responses and propose a simple clustering technique to extract this information, which we call population encoding. The population encoding technique looks into the entrails of an object-CNN at multiple layers of the network and shows the implicit presence of mid-level object part semantics distributed in the neuronal responses. Our qualitative visualizations show that population encoding can extract mid-level image patches that are visually tighter than the patches that produce high single-filter activations. Moreover, our comprehensive quantitative experiments using the object key point annotations from the PASCAL3D+ dataset corroborate the visualizations by demonstrating the superiority of population encoding over single-filter detectors, in the task of object-part detection. We also perform some preliminary experiments where we uncover the compositional relations between the adjacent layers using the parts detected by population encoding clusters. Finally, based on the insights gained from this work, we point to various new directions which will enable us to have a better understanding of the CNN\u2019s internal representations.", "creator": "LaTeX with hyperref package"}}}