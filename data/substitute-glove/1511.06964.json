{"id": "1511.06964", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "Online Semi-Supervised Learning with Deep Hybrid Boltzmann Machines and Denoising Autoencoders", "abstract": "Two novel cold hybrid architectures, the Deep Hybrid Boltzmann Machine has new Deep Hybrid Denoising Auto - encoder, except proposed new concerns semi - supervised hands-on dealing. The comparable soup have that model relevant logarithm wednesday different lowering significant luminescence to efficiency achieve methodology less up discriminative work. Theoretical commonalities of top-down and development learning given carry all formal. We need the first mercedes to place exists of signals - spawning in work progress reality -. classroom. The proposed networked feature needs performance 100 with a fictions - ads, prices - out interconnects network.", "histories": [["v1", "Sun, 22 Nov 2015 04:53:43 GMT  (107kb,D)", "https://arxiv.org/abs/1511.06964v1", null], ["v2", "Tue, 24 Nov 2015 22:12:51 GMT  (79kb,D)", "http://arxiv.org/abs/1511.06964v2", null], ["v3", "Fri, 27 Nov 2015 05:06:36 GMT  (79kb,D)", "http://arxiv.org/abs/1511.06964v3", null], ["v4", "Sun, 13 Dec 2015 07:24:34 GMT  (79kb,D)", "http://arxiv.org/abs/1511.06964v4", null], ["v5", "Thu, 7 Jan 2016 19:44:42 GMT  (134kb,D)", "http://arxiv.org/abs/1511.06964v5", null], ["v6", "Fri, 8 Jan 2016 06:19:07 GMT  (160kb,D)", "http://arxiv.org/abs/1511.06964v6", null], ["v7", "Mon, 18 Jan 2016 18:06:01 GMT  (160kb,D)", "http://arxiv.org/abs/1511.06964v7", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexander g ororbia ii", "c lee giles", "david reitter"], "accepted": false, "id": "1511.06964"}, "pdf": {"name": "1511.06964.pdf", "metadata": {"source": "CRF", "title": "ONLINE SEMI-SUPERVISED LEARNING WITH DEEP HYBRID BOLTZMANN MACHINES AND DENOISING AUTOENCODERS", "authors": ["Alexander G. Ororbia"], "emails": ["ago109@psu.edu", "giles@psu.edu", "reitter@psu.edu"], "sections": [{"heading": null, "text": "Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine and the Deep Hybrid Denoising Auto-encoder, are proposed for handling semisupervised learning problems. The models combine experts that model relevant distributions at different levels of abstraction to improve overall predictive performance on discriminative tasks. Theoretical motivations and algorithms for joint learning for each are presented. We apply the new models to the domain of datastreams in work towards life-long learning. The proposed architectures show improved performance compared to a pseudo-labeled, drop-out rectifier network."}, {"heading": "1 INTRODUCTION", "text": "Unsupervised pre-training can help construct an architecture composed of many layers of feature detectors (Erhan et al., 2010). Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007).\nHowever, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013). These architectures often feature many hyper-parameters that affect generalization performance, quickly creating a challenging tuning problem for human users. Furthermore, though efficient, the generative models used for pre-training learnt greedily carry the potential disadvantage of not providing \u201cglobal coordination between the different levels\u201d (Bengio, 2014), the sub-optimality of which was empirically shown in Arnold & Ollivier (2012). This issue was further discussed as the problem of \u201cshifting representations\u201d in Ororbia II et al. (2015b), where upper layers of a multi-level model are updated using immature latent representations from layers below potentially leading to unstable learning behavior or worsened generalization performance. While greedily built models can be later tuned jointly to construct architectures such as the Deep Boltzmann Machine (DBM, Salakhutdinov & Hinton (2009)) or via the Wake-Sleep algorithm (Hinton et al., 1995) (and improved variants thereof (Bornschein & Bengio)) the original training difficulty remains.\nOne way to exploit the power of representation learning without the difficulties of pre-training is to instead solve the hybrid learning problem: force a model to balance multiple supervised and unsupervised learning objectives in a principled manner. Many recent examples demonstrate the power and flexibility of this approach (Larochelle & Bengio, 2008; Ranzato & Szummer, 2008; Socher et al., 2011; Larochelle et al., 2012; Ororbia II et al., 2015b,a). The Stacked Boltzmann Expert Network (SBEN) and its autoencoder variant, the Hybrid Stacked Denoising Autoencoder (HSDA, Ororbia II et al. (2015b)) were proposed as semi-supervised deep architectures that combined the expressiveness afforded by a multi-layer composition of non-linearities with a more practical approach to model construction.\nar X\niv :1\n51 1.\n06 96\n4v 7\n[ cs\n.L G\n] 1\n8 Ja\nn 20\nThough promising, the previous approaches for learning deep hybrid architectures still suffer from some key issues: 1) parameters are learnt in a layer-wise fashion, which means that these models are susceptible to the \u201cshifting representations\u201d problem, and 2) the predictive potential of hybrid architectures\u2019 multiple layers has previously involved a naive form of vertical aggregation, whereas a more principled unified approach to layer-wise aggregation could lead to further performance improvements. In this paper, we propose two novel architectures and a general learning framework to directly address these problems while still providing an effective means of semi-supervised learning.\nThe problem of semi-supervised online learning is modeled after a scientific inquiry: how do babies learn? They start from representational beginnings that could range from blank slate to specific computational constraints. They learn from a stream of observations by perceiving the environment and (generatively) by trying to interact with it. While there are plenty of observations, the majority is unlabeled; the feedback they receive on their interpretations is uninformative and has been claimed to be too poor to facilitate learning without extensive priors (Chomsky, 1980). It seems obvious that online semi-supervised learning is a key task to achieving artificial general intelligence."}, {"heading": "2 THE MULTI-LEVEL SEMI-SUPERVISED HYPOTHESIS", "text": "Our motivation for developing hybrid models comes from the semi-supervised learning (prior) hypothesis of Rifai et al. (2011a), where learning aspects of p(x) improves the model\u2019s conditional p(y|x). The hope is that so long as there is some relationship between p(x) and p(y|x), a learner may be able to make use of information afforded by cheaply obtained unlabeled samples in tandem with expensive labeled ones.\nOrorbia II et al. (2015b,a) showed that a hybrid neural architecture, L layers deep, could combine this hypothesis with the expressiveness afforded by depth. Each layer-wise expert (or building block model) could be used to compute p(y|hl) for l = [0, L] and vertically aggregated to yield improved predictive performance. The Hybrid Stacked Denoising Autoencoders (HSDA) model, a stack of single-layer MLP coupled with single-layer auto-associators, directly embodies the multi-level view of the semi-supervised learning prior hypothesis, or rather, what we call the \u201cweak multi-level semisupervised learning hypothesis\u201d 1. According to this hypothesis, for an architecture designed to learn L levels of abstraction of data, learning something about the marginal p(hl) along with p(y|hl) for l = [0, L], will improve predictive performance on p(y|x). The Deep Hybrid Denoising Autoencoder, our proposed joint version of the HSDA presented in Section 3.2, also embodies the \u201cweak multi-level semi-supervised learning hypothesis\u201d.\nAn alternative model is the Stacked Boltzmann Experts Network (SBEN) model, a stack of hybrid restricted Boltzmann machines, where instead each layer-wise expert attempts to model a joint distribution at level l, p(y,hl). In the SBEN, aggregating the resulting p(y|hl) for l = [0, L], can ultimately improve predictive performance on p(y|x). This we call the \u201cstrong multi-level semisupervised learning hypothesis\u201d 2. The DHBM, our proposed joint version of the SBEN presented in Section 3.1, likewise embodies this hypothesis.\nSome experimental evidence has been provided to support these two variant hypotheses. In the work of Ororbia II et al. (2015b), a 3-layer SBEN outperformed a semi-supervised deep rectifier network (among other base-lines) by nearly 14% in image classification tasks. In Ororbia II et al. (2015a) a 3-layer SBEN (trained via the bottom-up-top-down algorithm) was shown to outperform a semi-supervised rectifier network and the original SBEN of Ororbia II et al. (2015b) (and other competitive text classifiers) consistently in text categorization experiments by as much as 36%. The works of Calandra et al. (2012); Zhou et al. (2012), which are special cases of the HSDA, are also further experimental evidence that support the weak multi-level semi-supervised hypothesis.\n1\u201cWeak\u201d refers to the possibility that p(x) may be loosely related to p(y|x), if at all. Learning p(x) may or may not help with prediction.\n2\u201cStrong\u201d refers to the fact we know p(y|x) is related to p(y,x) (i.e., p(y|x) = p(y,x)/p(x)). Thus learning the joint will yield information relevant to the conditional."}, {"heading": "3 DEEP HYBRID MODEL ARCHITECTURES", "text": "Below, we present the design of two candidates for building unified, deep hybrid architectures, namely, the Deep Hybrid Boltzmann Machine (DHBM) and the Deep Hybrid Denoising Autoencoder (DHDA)."}, {"heading": "3.1 THE DEEP HYBRID BOLTZMANN MACHINE", "text": "Like the Deep Boltzmann Machine (DBM) is to the Deep Belief Network (DBN), the DHBM can be viewed as a more sophisticated version of an SBEN. The primary advantage of a DHBM over the DBM, much like that of the SBEN over the DBN, is that the hybrid architecture is learnt with the ultimate intent of performing classification. This entails tracking model performance via classification error or some discriminative loss and avoids the need for expensive, non-trivial methods as annealed importance sampling (AIS, Neal (2001)) to estimate partition functions for approximate objectives (Hinton, 2002).\nInstead of a single model, another way to view a DHBM is simply as a composition of tightly integrated hybrid restricted Boltzmann machines (HRBM\u2019s). An SBEN is essentially a stack of HRBM\u2019s that each models p(y,hl) at their own respective level l of abstraction in the overall architecture. To exploit predictive power at each level of abstraction, one applies an averaging step of all layer-wise predictors to compute the SBEN\u2019s p(y|x)ensemble at inference time. The initial version of this model was trained using a greedy, bottom-up approach, where each layer learned to predict independently of the outputs of other layers (but was conditioned on the latent representation of the layer below). Ororbia II et al. (2015a) introduced a degree of joint learning of SBEN parameters through the Bottom-Up-Top-Down algorithm to improve performance, however, its bottom-up generative gradient step was still layer-wise in nature.\nUsing the comparative view of the SBEN, one creates a DHBM by taking a stack of HRBM experts and couples their predictors together, which means the overall model\u2019s prediction is immediately dependent on what all layers have learnt with respect to their level of abstraction. Furthermore, one makes the connections between layer-wise experts fully bi-directional, meaning that in order compute the state of any latent variable layer in the model (except for the input and top-most latent layers), one needs to incorporate activations from both the layer immediately below and the layer immediately above. A simple depiction of the first 3 layers of such a model is depicted in Figure 1. As a result, we have built a plausible machine that jointly leverages its multiple levels of abstraction to model the joint distribution of labeled data as opposed to a simple stack of joint distribution models that characterize the SBEN.\nWith the above picture in mind, we may explicitly define the 3-layer DHBM (or 3-DHBM), though as shown in Figure 1 this definition extends to an L-layer model. With pattern vector input x = (x1, \u00b7 \u00b7 \u00b7 , xD) and its corresponding target variable y \u2208 {1, \u00b7 \u00b7 \u00b7 , C}, utilizing two\nsets of latent variables h1 = (h11, \u00b7 \u00b7 \u00b7 , h1H1) and h 2 = (h21, \u00b7 \u00b7 \u00b7 , h2H2) and model parameters \u0398m = (W1,U1,W2,U2)3, the energy of a DHBM is:\nE(y,x,h1,h2) = \u2212h1>W1x\u2212 h1>U1ey \u2212 h2>W2h1 \u2212 h2>U2ey . (1)\nwhere we note that ey = (1i=y)Ci=1 is the one-hot vector encoding of y. The probability that the 3-DHBM assigns to the 4-tuple (y,x,h1,h2) is:\np(y,x,\u0398) = 1\nZ \u2211 h e(E(y,x,h 1,h2)) (2)\nwhere Z is the partition function meant to ensure a valid probability distribution (calculated by summing over all possible model configurations).\nNoting the introduction of top-down calculations, the visible and latent states of the 3-DHBM may be computed via the following implementable equations:\np(h1|y, x,h2) = \u220f j p(h1j |y, x,h2), with p(h1j = 1|y, x) = \u03c6(U1jy + \u2211 i W 1jixi + \u2211 k W 2kjh 2 k) (3)\np(h2|y,h1) = \u220f k p(h2k|y,h1), with p(h2k = 1|h1) = \u03c6(U2ky + \u2211 j W 2kjh 1 j ) (4)\np(x|h1) = \u220f i p(xi|h1), with p(xi = 1|h) = \u03c6( \u2211 j W 1jih 1 j ) (5)\np(y|h1,h2) = e \u2211 j U 1 jyh 1 j+ \u2211 j U 2 jyh 2 j\u2211\ny? e \u2211 j U 1 jy? h1j+ \u2211 j U 2 jy? h2j (6)\nwhere the activation \u03c6(v) = 1/(1 + e\u2212v), or the logistic sigmoid, and y is used to access a particular class filter from U l. In the interest of adapting the model to different types of input, such as continuous-valued variables, \u03c6(v) itself can be switched to alternative functions such as the rectified linear unit. One may use this set of equations as the fixed-point formulas for running mean-field inference in the deep architecture to obtain \u00b5 = {\u00b51, \u00b52}. More importantly, one may notice the dependency between these conditionals and as a result one may use a bottom-up pass approximation with weight doubling to initialize the mean-field like that of Salakhutdinov & Hinton (2009). One must then run several additional steps of mean-field, cycling through Equations 3, 4, 5, and 6, to get the model\u2019s reconstruction of the input and target (or model prediction).\nTo speed up both training and prediction time (since the goal is to make use of a single bottomup pass), we propose augmenting the DHBM architecture with a co-model, or separate auxiliary network, which was previously utilized to infer the states of latent variables in the DBM for a single bottom-up pass (Salakhutdinov & Larochelle, 2010). The recognition network, or MLP serving the role of function approximation, can be effectively fused with the deep architecture of interest and trained via a gradient descent procedure. Underlying the co-training of a separate recognition network is the expectation that the target model\u2019s mean-field parameters will not change much after only a single step of learning, which was also empirically shown in Salakhutdinov & Larochelle (2010) to work in practically training a DBM. The same principle we claim holds for a deep hybrid architecture, such as the DHBM, trained in a similar fashion.\nThe recognition network, the weights of which are initialized to those of the DHBM at the start of training, is specifically tasked with computing a fully factorized, approximate posterior distribution as shown below:\n3Note that we omit hidden and visible bias terms for simplicity.\nQrec(h|v; v) = H1\u220f j=1 H2\u220f k=1 qrec(h1j )q rec(h2k) (7)\nwhere the probability qrec(hli = 1) = v l i for layers l = 1, 2 and \u03c5 = {v1,v2}. Running the recognition network, with parameters \u0398rec = (R1, R2) (again, omitting bias terms for simplicity), is straightforward, as indicated by the equations below that constitute its feedforward operation:\nv1j = \u03c6( D\u2211 i=1 2R1ijvi) (8) v 2 j = \u03c6( H1\u2211 j=1 R2jkv 1 j ) (9)\nwhere the inference network\u2019s weights are doubled at each layer (except the top layer) to compensate for missing top-down feedback. Note that after initialization, the weights of the inference network are no longer shared with the original model. The inference network can be used to reasonably guess the values of the fixed-point mean-field to compute the values for (h1,h2) and then Equations 3, 4 5, and 6 may be run for a subsequent single mean-field step. More importantly, in our hybrid model definition, during prediction time, the architecture may directly generate the appropriate prediction for y by using the trained recognition network to infer the latent states of the DHBM.\nThe recognition network is trained according to the following objective:\nKL(QMF (h|v;\u00b5)||Qrec(h|v; \u03c5)) = \u2212 \u2211 i \u00b5ilogvi \u2212 \u2211 i (1\u2212 \u00b5i)log(1\u2212 \u03c5i) + Const (10)\nwhich is the minimization of the Kullback-Leibler (KL) divergence betweenQMF (h|v;\u00b5), the posterior of the DBM mean-field, and Qrec(h|v; v), the factorial posterior of the recognition network."}, {"heading": "3.2 THE DEEP HYBRID DENOISING AUTOENCODER", "text": "Following a similar path as the previous section but starting from the HSDA base architecture, we also propose the autoencoder variant of the DHBM, the DHDA. This also borrows the same underlying model structures of the DHBM, including the bi-directional connections needed for incorporating top-down and bottom-up influence. However, instead of learning via a Boltzmann-based approach, we shall learn a stochastic encoding and decoding process through multiple layers jointly. The DHDA may alternatively viewed as a stack of tightly integrated hybrid denoising autoencoder (HdA) building blocks with coupled predictors. An HdA is a single hidden-layer MLP that shares its input-to-hidden weights with an encoder-decoder model whose own weights are tied (i.e., decoding weights are equal to the transpose of the encoding weights).\nA 3-layer version of the joint model (also generalizable to L layers and defined by the same parameter-set as the DHBM) is specified by the following set of encoding (f1\u03b8 (y, x\u0302, h\u03022), f 2 \u03b8 (h\u0302\n1)}) and decoding (g\u03b8(h\u03021)) functions:\nh1 = f1\u03b8 (x\u0302, h\u0302 2) = \u03c6(W 1x\u0302 + (W 2)T h\u03022) (11)\nh2 = f2\u03b8 (h\u03021) = \u03c6(W 2h\u03021) (12) x\u0304 = g\u03b8(h\u03021) = \u03c6((W 1)T h\u03021) (13)\nwhere {W 1,W 2} are weight matrices connecting input x to h1 and h1 to h2 respectively (the superscript T denotes a matrix transpose operation). The output function o\u03b8(y|h1,h2) needed to generate predictions is calculated via Equation 6 much like the DHBM model. Like the HSDA, the DHDA uses a stochastic mapping function h\u0302lt \u223c qD(h\u0302lt|h) to corrupt input vectors (i.e., randomly masking entries under a given probability) 4. Note that the DHDA requires less matrix operations than the DHBM, since it exemplifies the \u201cweak multi-level semi-supervised hypothesis\u201d, giving it the advantage of a speed-up compared to the DHBM.\n4Note that h0 = x. While we opted for a simple denoising-based model, one could modify the building blocks of the DHDA to any alternative, such as one that makes uses of a contractive penalty (Rifai et al., 2011b).\nTo calculate layer-wise activation values for the DHDA, one uses the recognition network to obtain initial guesses for {h1,h2} and then cycles through Equations 11, 12, 13, and Equation 6 much like the mean-field procedure of the DHBM.\nTo solidify the comparison between layer-wise and joint models, Figure 3.2 shows the DHDA and DHBM architectures and their predecessor non-joint variants."}, {"heading": "3.3 JOINT PARAMETER LEARNING METHODS", "text": "Below, we present the general learning framework for jointly training parameters of a deep hybrid architecture, such as the DHBM and the DHDA described earlier. Note that since all parameters are jointly modified, the problem of shifting representations is no longer a concern\u2013all layers of the hybrid architecture are globally coordinated during learning. Under this framework, one may employ a variety of estimators for calculating parameter gradients. In particular we shall describe the two used in this study, namely, stochastic maximum likelihood (for the DHBM) and back-propagation of errors (for the DHDA). Before proceeding, however, we shall first define the hybrid objective functions that both architectures attempt to minimize.\nThe DHDA is designed to minimize the following hybrid loss function:\nL(Dlab,Dunlab) = \u03b1(\u2212 |Dlab|\u2211 t=1 log p(y|xt)+LCE(Dlab))+\u03b2(\u2212 |Dunlab|\u2211 t=1 log p(y\u2032|xt)+LCE(Dunlab))\n(14)\nwhere y\u2032 is a psuedo-label generated for an unlabeled sample given the current DHDA model and Dlab is the set of labeled samples and Dunlab is the set of unlabeled samples. \u03b1 and \u03b2 are coefficient handles to explicitly control the effects that supervised and unsupervised gradients have on the model\u2019s learning procedure, respectively. The loss function LCE is reconstruction cross entropy, defined as\nLCE(D) = \u2212 |D|\u2211 t=1 xt log zt + (1\u2212 xt) log(1\u2212 zt). (15)\nThe DHBM, on the other hand, minimizes the following hybrid objective:\nL(Dlab,Dunlab) = \u2212\u03b1 |Dlab|\u2211 t=1 log p(yt, xt)\u2212 \u03b2 |Dunlab|\u2211 t=1 log p(xt).5 (16)\nEach hybrid loss corresponds to an appropriate variant of the multi-level semi-supervised prior hypothesis described in Section 2. The DHDA\u2019s loss (Equation 14) embodies the weak variant since it couples a conditional model learning objective with that of a reconstruction model (that deals with the data input directly without labels). The DHBM\u2019s loss (Equation 16) maps to the strong variant since it directly attempts to model joint densities to yield a useful conditional one."}, {"heading": "3.3.1 THE JOINT HYBRID LEARNING FRAMEWORK", "text": "The general learning framework can be decomposed into 3 key steps: 1) gather mean-field statistics via approximate variational inference, 2) compute gradients to adjust the recognition network if one was used to initialize the mean-field, and 3) compute the gradients for the joint hybrid model using a relevant, architecture-specific algorithm. This procedure works for either labeled or unlabeled samples, however, in the latter case, we either make use of the model\u2019s current estimate of class probabilities or a pseudo-label (Lee, 2013) to create a proxy label. The full, general procedure is depicted in Algorithm 1.\nIn offline learning settings, we note that one could, like in Salakhutdinov & Larochelle (2010), make use of greedy, layer-wise pre-training to initialize a DHBM or DHDA very much in the same way learning a DBN can be used to initialize a DBM. In fact, learning an N-SBEN could be a precursor to learning a DHBM and a learning an N-HSDA a precursor to a DHDA, noting that after this first training phase, one would simply tie together the disparate predictor arms of this initial hybrid architecture to formulate the joint predictor needed to calculate Equation 6 for either possible joint model. This form of pre-training would be simpler to monitor, as, like the final target models, accuracy or discriminative loss can be used as a tracking objective as in Ororbia II et al. (2015b,a).\nWhat distinguishes the DHBM from the DHDA, aside from architectural considerations, is contained in the CALCPARAMGRADIENTS routine. The details of each we will briefly describe next and fully explicate in Appendices A and B."}, {"heading": "3.3.2 MEAN-FIELD CONTRASTIVE DIVERGENCE & BACK-PROPAGATION", "text": "One simple estimator for calculating the gradients of a hybrid architecture only makes use of two sets of multi-level statistics obtained from the recognition network and running the mean-field equations for a single step. This results in a set of \u201cpositive phase\u201d statistics, which result from the data vectors clamped at the input level of the model, and \u201cnegative phase\u201d statistics, which result from a single step of the model\u2019s free-running mode. One may then use these two sets of statistics to calculate parameter gradients to move the hybrid model towards more desirable optima. The explicit procedure is presented in Appendix A."}, {"heading": "3.3.3 STOCHASTIC APPROXIMATION PROCEDURE (SAP)", "text": "Another estimator of the gradients of a hybrid architecture could exploit better mixing rates (i.e., minimum number of steps before the Markov chain\u2019s distribution is close to its stationary distribution with respect to total variation distance) afforded by approximate maximum likelihood learning. The key idea behind this procedure is to maintain a set of multiple persistent Gibbs sampling chains\n5We note that the DHBM\u2019s hybrid loss function could be augmented with a pair of direct discriminative gradients that additionally minimize \u2212 \u2211|D| t=1 log p(y|xt) for both Dlab and Dunlab using the ensemble backpropagation algorithm proposed in Ororbia II et al. (2015a). This could further improve performance but we found not necessary in this paper\u2019s experiments.\nAlgorithm 1 The general learning framework for performing a single parameter update to an L-layer hybrid architecture, where L is the desired number of latent variable layers.\nInput: 1) Labeled (y,x) and unlabeled (u) samples or mini-batches, 2) learning rate \u03bb, hyperparameters \u03b2, numSteps (i.e., # of mean field steps) and specialized hyper-parameters \u039e, and 3) initial model parameters \u0398m = {\u0398m1 ,\u0398m2 , ...,\u0398mN} and recognition model parameters \u0398rec = {\u0398rec1 ,\u0398rec2 , ...,\u0398recL } function UPDATEMODEL((y,x), (u), \u03bb, \u03b2, numSteps, \u039e, \u0398m, \u0398rec)\nUse recognition model \u0398rec to calculate \u03c5 of approximate factorial posteriors Qreclab for (y,x) and Qrecunlab for (u) (Eqs. 8, 9)\nUse Qrecunlab to generate a proxy label y\u0302 for (u) via Eq. 6 Set \u00b5 = \u03c5 and run mean-field updates (Eqs. 3, 4, 5, 6 or 11, 12, 13, 6) for numSteps to acquire mean-field approximate posteriors QMFlab and Q MF unlab, and mean-field labels y\u0302lab and y\u0302unlab Adjust recognition model parameters via one step of gradient descent for both (y,x) and (u), weighting the gradients accordingly: \u0398rec \u2190 \u0398rec \u2212 \u03bb(5reclab + \u03b25recunlab), where 5reclab and 5recunlab are calculated via back-propagation (with KL-divergence objective) 5mlab \u2190 CALCPARAMGRADIENTS((y,x), y\u0302lab, Qreclab , QMFlab ,\u0398m,\u039e) 5munlab \u2190 CALCPARAMGRADIENTS((y\u0302,u), y\u0302unlab, Qrecunlab, QMFunlab,\u0398m,\u039e) \u0398m \u2190 \u0398m + \u03bb(5mlab + \u03b25munlab) . Final update to model parameters\nin the background from which we may sample during each update of the hybrid architecture. The details of this procedure are described in Appendix B."}, {"heading": "4 RELATED WORK", "text": "There have been a vast array of approaches to semi-supervised learning that deviate from the original pre-training breakthrough. Some leverage auxiliary models to encourage learning of discriminative information earlier at various stages of the learning process (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014). Others adopt a manifold perspective to learning and attempt to learn a representation that is robust to small variations in the input (and thus generalize better to unseen, unlabeled samples) either through a penalty on the model\u2019s Jacobian (Rifai et al., 2011a) or through a special regularizer (Weston et al., 2012).\nA simpler alternative is through Entropy Regularization, where a standard architecture, such as a drop-out rectifier network, is used in a self-training scheme, where the model\u2019s own generated proxy labels for unlabeled samples are then used in a weighted secondary gradient (Lee, 2013). Other hybrid learning approaches follow similar ideas of (Lasserre et al., 2006), including the training schemes proposed in Ororbia II et al. (2015b,a), which built on the initial ideas of Larochelle & Bengio (2008); Larochelle et al. (2012), and of which the schemes of Calandra et al. (2012); Zhou et al. (2012) were shown to be special cases. The hybrid learning framework for DHBM and DHDA models described in this paper follow in the spirit of the compound learning objectives described in those studies. However, they do differ slight from their predecessors, the SBEN and the HSDA, in that they do not always make use of an additional pure discriminative gradient (such as in the case of the DHBM).\nThe DHBM shares some similarites to that of the \u201cstitched-together\u201d DBM of Salakhutdinov & Hinton (2009) and the MP-DBM (Goodfellow et al., 2013), which was originally proposed as another way to circumvent the greedy pre-training of DBM\u2019s using a back-propagation-based approach on an unfolded inference graph. The key advantage of the MP-DBM is that it is capable of working on a variety of variational inference tasks beyond classification (i.e., input completion, classification with missing inputs, etc.). We note that combining our hybrid learning framework with a more advanced Boltzmann architecture like the MP-DBM could yield even more powerful semi-supervised models. The deep generative models proposed Kingma et al. (2014) also share similarity to our work especially in their use of a recognition network to make inference of latent states in such models fast and tractable.\nThe generality of our hybrid learning framework, we argue, extends far beyond only learning DHBM\u2019s and DHDA\u2019s, as indicated by the general presentation of Algorithm 3.3.1. In fact, any\nmodel that is capable of learning either discriminatively or generatively may be employed, such as Sum-Product Networks (Poon & Domingos, 2011) or recently proposed back-propagation-free models like the Difference-Target Propagation network (Lee et al., 2015).\nFurthermore, general enhancements to gradient descent learning (i.e., momemtum, regularization, etc.) may also be incorporated into our procedure. For simplicity, we only chose to incorporate a drop-out scheme Hinton et al. (2012) to our learning procedure since our comparative base-line (the psuedo-labeled MLP) also employed one. During learning, all this entails is applying random binary masks to the hidden layer statistics calculated from the recognition network and once again to calculated mean-field statistics (which will set various hidden units to 0 under a given probability). For the DHDA, the same binary masks used during calculation of layer-wise statistics are applied to the error deltas computed during back-propagation (as in Hinton et al. (2012)). Prediction in both the DHBM and DHDA simply requires multiplying layer-wise statistics by the probability used in drop-out learning."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "5.1 ONLINE LEARNING RESULTS", "text": "We make use of the Massive Online Analysis framework (MOA, Bifet et al. (2010)) to simulate the process of online learning, where the learner does not have access to a static, finite data-set of i.i.d. labeled samples but rather is presented with mini-batches of samples (we chose N = 20 samples at time) generated from a continuous non-stationary flow of non-i.i.d data. We extended the evolving processes to the semi-supervised setting by enforcing that generated mini-batches are mixed in the sense that, on average, only 10% of the samples were labeled at any time. To learn effectively,\nthe learner must make use of these labeled samples as best as it can while also self-training from unlabeled samples.\nSpecifically, the LED and Waveform streams were leveraged, in particular the versions that featured customizable feature concept drift and injection of feature noise to make learning more difficult over time. The LED stream task entails predicting 1 of 10 possible digits on an LED panel given 24 noisy features (percentage of noise applied was set to 10%, and 4 features in the space experienced concept drift). The Waveform task involves predicting 1 of 3 waveform types given 40 noisy real-valued attributes (these features were additionally normalized to lie in the range [0, 1], and 10 features in this space experienced concept drift). For both data-streams, we investigate model performance over a 1,000,000 iteration sweep.\nWe compare 4 models: 1) the rectifier network trained via pseudo-labeled back-propagation of Lee (2013), 2) a sigmoidal DHBM trained via MF-CD, 3) a sigmoidal DHBM trained via SAP (using 10 fantasy particles), and 4) a sigmoidal DHDA trained via MF-BP with corruption probability p = 0.15. For both streams, all models consisted of complete-representation architectures (the LED stream used 24\u221224\u221224\u221224\u221224\u221210 and the Waveform stream used 40\u221240\u221240\u221240\u221240\u22123). All models used a drop-out probability of p = 0.5, a learning rate \u03bb = 0.051, \u03b1 = 1.0, and \u03b2 = 0.1 (we note that adaptive hyper-parameter schedules would be highly suitable to this learning setting and will be investigated in future work).\nThe results we report are 10-trial averages of each model\u2019s following the prequential method for evaluating statistical learning models for streaming scenarios. Specifically, we make use of prequential error with a weighted forgetting factor \u03b1err = 0.995 (following the discussion of admissible values in Gama et al. (2012)). This error metric is calculated (at time i) via the following equation:\nP\u03b1(i) =\n\u2211i k=1 \u03b1\ni\u2212kL(yk, y\u0302k)\u2211i k=1 \u03b1 i\u2212k , with 0 \u03b1 \u2264 1. (17)\n(Gama et al., 2012) showed that metrics with forgetting mechanisms were more appropriate for evolving data stream settings (as compared to simple predictive sequential error). In particular, the one we use in this work is memoryless and thus advantageous to use in streaming settings over other alternatives such as sliding window prequential error. For each model, we show an error curve for the conditions when, on average, only 10% of samples of a mini-batch at given time-step are labeled (Figure 5.1).\nFor both the LED and Waveform data-stream experiments, our proposed hybrid architectures consistently outperform the pseudo-labeled MLP. However, we note that it is the DHBM MF that performs the best (and not the DHBM SAP), maintaining a consistently lower error in the face of both evolving data-streams (with the DHDA also performing reasonably well). We speculate that the reason the SAP estimator does not help improve (and in fact, appears to hurt) performance is tied to the evolutionary nature of the input distribution itself. SAP uses additional fantasy particles to better explore the DHBM\u2019s fantasy distribution and generally exhibit improved results over 1-step Contrastive Divergence in static data-set settings (where multiple epochs are possible). However, since our input changes with time, it becomes more challenging to manipulate the model\u2019s fantasy distribution to match the input distribution. The simpler mean-field CD approach, where the negative phase is simply 1 step away from the positive data-driven one, may simply facilitate an easier learning problem and thus allow for better adaptation of model parameters to handle changes in the input distribution.\nNonetheless, while all models ultimately experience fluctuations in error as the distributions change, we see that the DHBM MF is even able to begin recovering in the Waveform experiment (roughly at around 60,000 iterations). We observe that in the long-run, the MLP model ultimately performs quite poorly. While the MLP is also semi-supervised, one reason behind its failure may be simply that its design is to make use of information useful for learning a discriminative mapping (or rather a conditional model p(y|x)). This is the case for both of its gradients (supervised and unsupervised). We argue that the key to our models\u2019 success is that they attempt to directly learn information about the input distribution itself in addition to conditional information. The information afforded from including a generative perspective of the data seems to better equip hybrid architectures for input distributions that change with time than those without.\nAdditionally, it is likely that at certain time steps mini-batches presented to a learner are entirely unlabeled (as in our setting). It is here that semi-supervised models that also attempt to model the input distribution directly have an advantage over those that do not. In this case, the pseudolabeled MLP, which relies on the strength of its supervised gradient, is more likely to succumb to the problem most often associated with self-training schemes: reinforcing incorrect predictions."}, {"heading": "5.2 FINITE DATA-SET RESULTS", "text": "We evaluate our proposed architectures on the MNIST data-set to gauge their viability for offline semi-supervised learning. The details of the experimental set-up can be found in Appendix C.1. In Table 1 6, we observe that the 3-DHBM is competitive with several state-of-the-art models, notably the EMBEDNN and DROPNN+PL. However, while the DHBM is unable to outperform the DROPNN+PL+DAE, we note that that state-of-the-art method uses pre-training in order to obtain its final performance boost. Further work could combine our hybrid architectures with the discriminatively-tracked pre-training approach proposed in Section 3.3.1. Alternatively, the DHBM could benefit from an additional weighted discriminative gradient like that used in the Top-DownBottom-Up algorithm of Ororbia II et al. (2015a).\nWe also note that our DHDA model does not perform so well on this benchmark, only beating out the architectures trained on the supervised subset. This poorer performance is somewhat surprising. However, we attribute this to too coarse a search for hyper-parameters (i.e., DHDA is particularly sensitive to \u03bb and \u03b2 and is affected by the corruption probability and type). Its performance could also be improved by employing an SAP-like framework similar to that of the DHBM.\nIn an additional (5-trial) experiment using the 20 NewsGroup text data-set, the 3-DHDA (39.45\u00b10.1 % test error) outperformed the DROPNN+PL (44.39\u00b1 0.4 % test error) and 3-DHBM (44.67\u00b1 0.6 % test error). The details of the experimental set-up can be found in Appendix C.2."}, {"heading": "6 CONCLUSIONS", "text": "We have presented two novel deep hybrid architectures where parameters are learned jointly. These unified hybrid models, unlike their predecessors, circumvent the problem of shifting representations since the different layers of these models are optimized from a global perspective. Furthermore, prediction takes advantage of classification information found at all abstraction levels of the model\n6For purely supervised models, NN stands for a single-hidden layer neural network, SVM stands for support sector machine, CNN stands for convolutional neural network. For semi-supervised models, TSVM is a transductive support vector machine, EMBEDNN is deep neural network trained with an unsupervised embedding criterion, CAE is a contractive auto-encoder, MTC is the Manifold Tangent Classifier, and DROPNN stands for a drop-out deep neural network (where PL means pseudo-labeled and DAE means with stacked denoising autoencoder pre-training.\nwithout resorting to a vertical aggregation of disjoint, layer-wise experts (like that of the SBEN and HSDA),\nExperiments show that our proposed unified hybrid architectures are well-suited to tackling more difficult, online data-stream settings. We also observe that our architectures compare to some stateof-the-art semi-supervised learning methods on finite data-sets, even though we note that the online and offline tasks are substantially different. More importantly, the unified hybrid learning framework described in this paper can be used beyond learning DHBM\u2019s and DHDA\u2019s. Rather, we argue that it is applicable to any multi-level neural architecture that can compute discriminative and generative gradients for parameters jointly. This implies that future work could train new hybrid variants of the models."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Hugo Larochelle for useful conversations that helped inform this work. The shortcomings of this paper, however, are ours and ours alone. This work was supported by NSF grant 1528409 to DR."}, {"heading": "A MEAN-FIELD CONTRASTIVE DIVERGENCE & BACK-PROPAGATION DETAILS", "text": "When gradients for a DHBM are estimated this way, one is effectively combining approximate mean-field variational inference with a multi-level Contrastive Divergence approximation. This process, when it was introduced for learning unsupervised generative models like the DBM, is known Mean-Field Contrastive Divergence (MF-CD, Welling & Hinton (2002)). However, training a DHBM with MF-CD is a bit simpler than for a DBM, since like the SBEN, the DHBM hybrid architecture allows for immediate classification and thus facilitates tracking of learning progress through a discriminative objective (such as the negative log loss). The parameter gradients for each layer-wise expert are computed via the Contrastive Divergence approximation, without sampling the probability vectors obtained in the block Gibbs-sampling procedure. The explicit procedure for estimating this gradient is given in Algorithm 2.\nIn contrast, when this set of statistics is applied to a DHDA, one combines the mean-field variational inference procedure with a multi-level back-propagation of errors procedure. Since each layer of the DHDA is a hybrid Denoising autoencoder (hDA), one may leverage the view in Ororbia II et al. (2015b) that such a building block is fusion of an encoder-decoder model with a single hiddenlayer MLP. The resulting component model may be trained via an unsupervised and supervised generative back-propagation procedure using a differentiable loss function such as reconstruction cross-entropy (or a quadratic loss). The steps for calculating these gradient estimators using the mean-field statistics is shown in Algorithm 3.\nAlgorithm 3 The estimator for calculating gradients using back-propagation of errors for the DHDA. Note that \u201c\u00b7\u201d indicates a Hadamard product, \u03be is an error signal vector, the prime superscript indicates a derivative (i.e., \u03c6\u2032(v) means first derivative of activation function \u03c6(v)). The symbol \u0398 : W denotes an access to element W contained in \u0398.\nInput: 1) (y,x) mini-batch of N samples, 2) Qrec approximate factorial posterior for (y,x), 3) QMF mean-field factorial posterior for (y,x), 3) initial model parameters \u0398m = {\u0398m1 ,\u0398m2 , ...,\u0398mL }, and 4) specialized hyper-parameters \u039e. function CALCPARAMGRADIENTS((y,x), y\u0302, Qrec, QMF ,\u0398m,\u039e)\nl\u2190 1, \u03beout \u2190 softmax\u2032(y\u0302) \u00b7 \u2212(y/y\u0302) . Calculate derivative of negative log-loss cost while l \u2264 L do\n// Gather positive phase statistics at l h+l \u2190 Qrecl if l > 1 then\nv+l \u2190 Qrecl else\nv+l \u2190 x // Gather negative phase statistics at l (v\u2212l ,h \u2212 l )\u2190 QMFl z\u0302 v\u2212l l \u2190 v \u2212 l , z\u0302 h\u2212l l \u2190 h \u2212 l . Get linear pre-activations for v \u2212 l & h \u2212 l \u03bereconl \u2190 DERIVRECONLOSS(v \u2212 l ,v + l ), \u03be recon l \u2190 \u03bereconl \u00b7 \u03c6\u2032(z\u0302 v\u2212l l ) . Derivative of\nreconstruction loss at l. \u03behidl \u2190 (\u0398ml : W )\u03bereconl . Propagate error signal back to hiddens \u03behidl \u2190 \u03behidl \u00b7 \u03c6\u2032(z\u0302 h\u2212l l ) . Compute error derivatives with respect to hiddens\n5Wl \u2190 (\u03behidl v \u2212 l ) + (\u03be recon l h \u2212 l ) . Compute 1st part of gradient for W \u03behidl \u2190 (\u0398ml : U)\u03beout . Propagate output error signal back to hiddens 5Wl \u21905Wl + (\u03behidl v \u2212 l ) . Compute 2nd part of gradient for W 5Ul \u2190 h \u2212 l (\u03be\nout)T . Compute gradient for U 5ml \u2190 (5Wl ,5Ul ) return5m .5m = (5m1 ,5m1 , ...,5mL )"}, {"heading": "B STOCHASTIC APPROXIMATION PROCEDURE DETAILS", "text": "The Stochastic Approximation Procedure specifically requires maintaining a set of persistent Markov Chains (randomly initialized), or set of M fantasy particles Xt = {xt,1, ...,xt,M}, from which we calculate an average over. From an implementation perspective, each time we make a call to update the hybrid model\u2019s parameters, we sample a new state xt+1 given xt via a transition operator T\u0398t(xt+1 \u2190 xt), of which, like the DBM in Salakhutdinov & Larochelle (2010), we use Gibbs sampling. Maintaining a set of persistent Gibbs chains facilitates better mixing during the MCMC learning procedure and better exploration of the model\u2019s energy landscape. More importantly, as we take a gradient step to obtain \u0398t+1 using a point estimate of the model\u2019s intractable expectation at sample xt+1, we obtain a better estimate for the gradient of the final hybrid model.\nConstructing an SAP for the unified learning framework defined by Algorithm 3.3.1 entails no further work beyond implementing a multi-level block Gibbs sampler for each of the M fantasy particles used for learning. The explicit procedure is shown in Algorithm 4."}, {"heading": "C EXPERIMENTAL SET-UP DETAILS", "text": "C.1 MNIST\nTo evaluate the viability of our proposed hybrid architectures, we investigate their performance on the well-known MNIST benchmark. However, since our models are designed for the semisupervised setting, we make use of a similar experimental setting to Lee (2013), which entails only using a small subset of the original 60,000 training sample set as a labeled training set and with the\nAlgorithm 4 The estimator for calculating gradients using Stochastic Maximum Likelihood for the DHBM (and possibly the DHDA).\nInput: 1) (y,x) mini-batch of N samples, 2) Qrec approximate factorial posterior for (y,x), 3) QMF mean-field factorial posterior for (y,x), 3) initial model parameters \u0398m = {\u0398m1 ,\u0398m2 , ...,\u0398mL }, and 4) specialized hyper-parameters \u039e. function CALCPARAMGRADIENTS((y,x), y\u0302, Qrec, QMF ,\u0398m,\u039e)\nl\u2190 1 while l \u2264 L do\n// Gather positive phase statistics at l h+l \u2190 Qrecl if l > 1 then\nv+l \u2190 Qrecl else\nv+l \u2190 x // Gather fantasy particle samples at l for each particle m = 1 to M do\nSample (v\u0303t+1,ml , h\u0303 t+1,m l ) given (v\u0303 t,m l , h\u0303 t,m l ) via block Gibbs sampling.\n// Calculate parameter gradients at l via persistent Contrastive Divergence 5Wl \u2190 (< h + l (v + l ) T >N \u2212 < h\u0303t+1,ml (v\u0303 t+1,m l ) T >M ) 5Ul \u2190 (h + l (< ey)\nT >N \u2212 < h\u0303t+1,ml (ey\u0302)T >M ) 5ml \u2190 (5Wl ,5Ul )\nreturn5m .5m = (5m1 ,5m1 , ...,5mL )\nrest treated as purely unlabeled data points. We separate out 1000 unique samples for validation (i.e., to perform the necessary model selection for finite data-set settings). We ensure that there is an equal or fairly representative portion of each class variable in the training and validation subsets. The MNIST data-set contains 28 x 28 images with pixel feature gray-scale feature values in the range of [0, 255] of which we normalized to the range of [0, 1]. We use these normalized real-valued raw features as direct input to our models.\nModel selection was performed using a coarse grid search, where the hyper-parameters \u03bb, in the range [0.05, 0.11], and \u03b2 in the range [0.35, 0.9], were key values to explore that affected generalization performance the most. We employed an annealing schedule for \u03b2f , following the formula:\n\u03b2(t) =  0, if t < T1 t\u2212T1 T2\u2212T1 \u03b2f , if T1 < t < T2 \u03b2f , if T2 < t\n(18)\nwhere T denotes a \u201clabeled epoch\u201d or full pass through the labeled subset. We set T1 = 3 and T2 = 300 for our experiments.\nFor the stochastic approximation procedure used to learn the DHBM, we made use of M = 10 fantasy particles, and for the MF-BP used to train the DHDA, we used a corruption probability of p = 0.2. In calculating gradient estimates, we used mini-batches of 10 samples each iteration and did not decay the learning rate for any model. Model architectures were kept to complete representations of the 3 latent layer form: 784 \u2212 784 \u2212 784 \u2212 784 \u2212 10. For both the DHDA and the DHBM, we employed a Drop-Out scheme (Hinton et al., 2012), with probability of p = 0.5 for randomly masking out latent variables to better protect against overfitting of the data. We only applied dropout to the latent variables of the model, though note dropping out input variables may also improve performance yet further on the MNIST data-set. Models were trained for a full 6 epochs through the entire training set (where a full epoch includes the set of all labeled and unlabeled samples).\nC.2 20 NEWSGROUPS SET-UP\nWe also investigate the performance of our model\u2019s on the 20-NewsGroup text classification dataset. We opted to use the time-split version, which contains a training set with approximately 10000\ndocument samples and a separate test set 8000 document samples. In regards to pre-processing of the text, we removed stop-words, applied basic stemming, and removed numerics. To create the final low-level representation of the data, we used only the 2000 most frequently occurring terms after pre-processing and create a binary occurrence vector for each document. There are 20 class targets, each a different topic of discussion of the text.\nAll models in this experiment were chosen to use 2 hidden layers totaling 1200 variables. We compare a 2-DHBM and 2-DHDA against 2 layer rectifier drop-out network trained via pseudolabeled back-propagation. For the rectifier network, we use an architecture of 2000\u2212600\u2212600\u221220. For the 2-DHBM, we use logistic sigmoid activation functions and the same architecture as the rectifier network but do not use drop-out (as we found it worsened model performance on this dataset). For the 2-DHDA, we use an drop-out architecture of 2000 \u2212 650 \u2212 550 \u2212 20 with rectifier activation functions (and thus use a quadratic cost as the objective function for the mean-field backpropagation sub-routine) with denoising corruption set to p = 0.2. All models had their learning rate searched in the interval [0.01, 0.1] and used an annealing schedule for \u03b2 with T1 = 3 and T2 = 600, where values for \u03b2 were searched in [0.35, 0.9]."}], "references": [{"title": "Layer-wise learning of deep generative models", "author": ["Arnold", "Ludovic", "Ollivier", "Yann"], "venue": null, "citeRegEx": "Arnold et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arnold et al\\.", "year": 2012}, {"title": "How auto-encoders could provide credit assignment in deep networks via target propagation", "author": ["Bengio", "Yoshua"], "venue": null, "citeRegEx": "Bengio and Yoshua.,? \\Q2014\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2014}, {"title": "Greedy Layer-wise Training of Deep Networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "MOA: Massive online analysis, a framework for stream classification and clustering", "author": ["Bifet", "Albert", "Holmes", "Geoffrey", "Pfahringer", "Bernhard", "Kranen", "Philipp", "Kremer", "Hardy", "Jansen", "Timm", "Seidl", "Thomas"], "venue": null, "citeRegEx": "Bifet et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bifet et al\\.", "year": 2010}, {"title": "Learning Deep Belief Networks from Non-stationary Streams", "author": ["Calandra", "Roberto", "Raiko", "Tapani", "Deisenroth", "Marc Peter", "Pouzols", "Federico Montesino"], "venue": "Artificial Neural Networks and Machine Learning \u2013 ICANN 2012,", "citeRegEx": "Calandra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Calandra et al\\.", "year": 2012}, {"title": "Rules and representations", "author": ["Chomsky", "Noam"], "venue": "Behavioral and brain sciences,", "citeRegEx": "Chomsky and Noam.,? \\Q1980\\E", "shortCiteRegEx": "Chomsky and Noam.", "year": 1980}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "On evaluating stream learning algorithms", "author": ["Gama", "Jo\u00e3o", "Sebasti\u00e3o", "Raquel", "Rodrigues", "Pedro Pereira"], "venue": "Machine Learning,", "citeRegEx": "Gama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gama et al\\.", "year": 2012}, {"title": "Multi-prediction deep boltzmann machines", "author": ["Goodfellow", "Ian", "Mirza", "Mehdi", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Training Products of Experts by Minimizing Contrastive Divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M. The"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Classification using Discriminative Restricted Boltzmann Machines", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Learning Algorithms for the Classification Restricted Boltzmann Machine", "author": ["Larochelle", "Hugo", "Mandel", "Michael", "Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "Principled Hybrids of Generative and Discriminative Models", "author": ["Lasserre", "Julia A", "Bishop", "Christopher M", "Minka", "Thomas P"], "venue": "In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1,", "citeRegEx": "Lasserre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lasserre et al\\.", "year": 2006}, {"title": "Pseudo-label: The Simple and Efficient Semi-supervised Learning Method for Deep Neural Networks", "author": ["Lee", "Dong-Hyun"], "venue": "In Workshop on Challenges in Representation Learning,", "citeRegEx": "Lee and Dong.Hyun.,? \\Q2013\\E", "shortCiteRegEx": "Lee and Dong.Hyun.", "year": 2013}, {"title": "Difference target propagation", "author": ["Lee", "Dong-Hyun", "Zhang", "Saizheng", "Fischer", "Asja", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Annealed importance sampling", "author": ["Neal", "Radford M"], "venue": null, "citeRegEx": "Neal and M.,? \\Q2001\\E", "shortCiteRegEx": "Neal and M.", "year": 2001}, {"title": "Learning a deep hybrid model for semisupervised text classification", "author": ["Ororbia II", "Alexander G", "Giles", "C. Lee", "Reitter", "David"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "II et al\\.,? \\Q2015\\E", "shortCiteRegEx": "II et al\\.", "year": 2015}, {"title": "Online learning of deep hybrid architectures for semi-supervised categorization. In Machine Learning and Knowledge Discovery in Databases (Proceedings", "author": ["Ororbia II", "Alexander G", "Reitter", "David", "Wu", "Jian", "Giles", "C. Lee"], "venue": "ECML PKDD 2015),", "citeRegEx": "II et al\\.,? \\Q2015\\E", "shortCiteRegEx": "II et al\\.", "year": 2015}, {"title": "Sum-product networks: A new deep architecture", "author": ["Poon", "Hoifung", "Domingos", "Pedro"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "Poon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2011}, {"title": "Semi-supervised learning of compact document representations with deep networks", "author": ["Ranzato", "Marc\u2019 Aurelio", "Szummer", "Martin"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Ranzato et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2008}, {"title": "The manifold tangent classifier", "author": ["Rifai", "Salah", "Dauphin", "Yann N", "Vincent", "Pascal", "Bengio", "Yoshua", "Muller", "Xavier"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Efficient learning of deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Larochelle", "Hugo"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2010}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher", "Richard", "Pennington", "Jeffrey", "Huang", "Eric H", "Ng", "Andrew Y", "Manning", "Christopher D"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "A new learning algorithm for mean field boltzmann machines", "author": ["Welling", "Max", "Hinton", "Geoffrey E"], "venue": "Artificial Neural Networks \u2014 ICANN 2002,", "citeRegEx": "Welling et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2002}, {"title": "Deep learning via semisupervised embedding", "author": ["Weston", "Jason", "Ratle", "Fr\u00e9d\u00e9ric", "Mobahi", "Hossein", "Collobert", "Ronan"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Supervised Deep Learning with Auxiliary Networks", "author": ["Zhang", "Junbo", "Tian", "Guangjian", "Mu", "Yadong", "Fan", "Wei"], "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Unsupervised pre-training can help construct an architecture composed of many layers of feature detectors (Erhan et al., 2010).", "startOffset": 106, "endOffset": 126}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007).", "startOffset": 292, "endOffset": 313}, {"referenceID": 14, "context": "However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013).", "startOffset": 209, "endOffset": 259}, {"referenceID": 8, "context": "However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013).", "startOffset": 209, "endOffset": 259}, {"referenceID": 10, "context": "While greedily built models can be later tuned jointly to construct architectures such as the Deep Boltzmann Machine (DBM, Salakhutdinov & Hinton (2009)) or via the Wake-Sleep algorithm (Hinton et al., 1995) (and improved variants thereof (Bornschein & Bengio)) the original training difficulty remains.", "startOffset": 186, "endOffset": 207}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007). However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013). These architectures often feature many hyper-parameters that affect generalization performance, quickly creating a challenging tuning problem for human users. Furthermore, though efficient, the generative models used for pre-training learnt greedily carry the potential disadvantage of not providing \u201cglobal coordination between the different levels\u201d (Bengio, 2014), the sub-optimality of which was empirically shown in Arnold & Ollivier (2012). This issue was further discussed as the problem of \u201cshifting representations\u201d in Ororbia II et al.", "startOffset": 293, "endOffset": 1021}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007). However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013). These architectures often feature many hyper-parameters that affect generalization performance, quickly creating a challenging tuning problem for human users. Furthermore, though efficient, the generative models used for pre-training learnt greedily carry the potential disadvantage of not providing \u201cglobal coordination between the different levels\u201d (Bengio, 2014), the sub-optimality of which was empirically shown in Arnold & Ollivier (2012). This issue was further discussed as the problem of \u201cshifting representations\u201d in Ororbia II et al. (2015b), where upper layers of a multi-level model are updated using immature latent representations from layers below potentially leading to unstable learning behavior or worsened generalization performance.", "startOffset": 293, "endOffset": 1129}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007). However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013). These architectures often feature many hyper-parameters that affect generalization performance, quickly creating a challenging tuning problem for human users. Furthermore, though efficient, the generative models used for pre-training learnt greedily carry the potential disadvantage of not providing \u201cglobal coordination between the different levels\u201d (Bengio, 2014), the sub-optimality of which was empirically shown in Arnold & Ollivier (2012). This issue was further discussed as the problem of \u201cshifting representations\u201d in Ororbia II et al. (2015b), where upper layers of a multi-level model are updated using immature latent representations from layers below potentially leading to unstable learning behavior or worsened generalization performance. While greedily built models can be later tuned jointly to construct architectures such as the Deep Boltzmann Machine (DBM, Salakhutdinov & Hinton (2009)) or via the Wake-Sleep algorithm (Hinton et al.", "startOffset": 293, "endOffset": 1483}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007). However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013). These architectures often feature many hyper-parameters that affect generalization performance, quickly creating a challenging tuning problem for human users. Furthermore, though efficient, the generative models used for pre-training learnt greedily carry the potential disadvantage of not providing \u201cglobal coordination between the different levels\u201d (Bengio, 2014), the sub-optimality of which was empirically shown in Arnold & Ollivier (2012). This issue was further discussed as the problem of \u201cshifting representations\u201d in Ororbia II et al. (2015b), where upper layers of a multi-level model are updated using immature latent representations from layers below potentially leading to unstable learning behavior or worsened generalization performance. While greedily built models can be later tuned jointly to construct architectures such as the Deep Boltzmann Machine (DBM, Salakhutdinov & Hinton (2009)) or via the Wake-Sleep algorithm (Hinton et al., 1995) (and improved variants thereof (Bornschein & Bengio)) the original training difficulty remains. One way to exploit the power of representation learning without the difficulties of pre-training is to instead solve the hybrid learning problem: force a model to balance multiple supervised and unsupervised learning objectives in a principled manner. Many recent examples demonstrate the power and flexibility of this approach (Larochelle & Bengio, 2008; Ranzato & Szummer, 2008; Socher et al., 2011; Larochelle et al., 2012; Ororbia II et al., 2015b,a). The Stacked Boltzmann Expert Network (SBEN) and its autoencoder variant, the Hybrid Stacked Denoising Autoencoder (HSDA, Ororbia II et al. (2015b)) were proposed as semi-supervised deep architectures that combined the expressiveness afforded by a multi-layer composition of non-linearities with a more practical approach to model construction.", "startOffset": 293, "endOffset": 2237}, {"referenceID": 21, "context": "Our motivation for developing hybrid models comes from the semi-supervised learning (prior) hypothesis of Rifai et al. (2011a), where learning aspects of p(x) improves the model\u2019s conditional p(y|x).", "startOffset": 106, "endOffset": 127}, {"referenceID": 18, "context": "In the work of Ororbia II et al. (2015b), a 3-layer SBEN outperformed a semi-supervised deep rectifier network (among other base-lines) by nearly 14% in image classification tasks.", "startOffset": 23, "endOffset": 41}, {"referenceID": 18, "context": "In the work of Ororbia II et al. (2015b), a 3-layer SBEN outperformed a semi-supervised deep rectifier network (among other base-lines) by nearly 14% in image classification tasks. In Ororbia II et al. (2015a) a 3-layer SBEN (trained via the bottom-up-top-down algorithm) was shown to outperform a semi-supervised rectifier network and the original SBEN of Ororbia II et al.", "startOffset": 23, "endOffset": 210}, {"referenceID": 18, "context": "In the work of Ororbia II et al. (2015b), a 3-layer SBEN outperformed a semi-supervised deep rectifier network (among other base-lines) by nearly 14% in image classification tasks. In Ororbia II et al. (2015a) a 3-layer SBEN (trained via the bottom-up-top-down algorithm) was shown to outperform a semi-supervised rectifier network and the original SBEN of Ororbia II et al. (2015b) (and other competitive text classifiers) consistently in text categorization experiments by as much as 36%.", "startOffset": 23, "endOffset": 383}, {"referenceID": 4, "context": "The works of Calandra et al. (2012); Zhou et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 4, "context": "The works of Calandra et al. (2012); Zhou et al. (2012), which are special cases of the HSDA, are also further experimental evidence that support the weak multi-level semi-supervised hypothesis.", "startOffset": 13, "endOffset": 56}, {"referenceID": 19, "context": "Ororbia II et al. (2015a) introduced a degree of joint learning of SBEN parameters through the Bottom-Up-Top-Down algorithm to improve performance, however, its bottom-up generative gradient step was still layer-wise in nature.", "startOffset": 8, "endOffset": 26}, {"referenceID": 19, "context": "We note that the DHBM\u2019s hybrid loss function could be augmented with a pair of direct discriminative gradients that additionally minimize \u2212 \u2211|D| t=1 log p(y|xt) for both Dlab and Dunlab using the ensemble backpropagation algorithm proposed in Ororbia II et al. (2015a). This could further improve performance but we found not necessary in this paper\u2019s experiments.", "startOffset": 251, "endOffset": 269}, {"referenceID": 2, "context": "Some leverage auxiliary models to encourage learning of discriminative information earlier at various stages of the learning process (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014).", "startOffset": 133, "endOffset": 192}, {"referenceID": 30, "context": "Some leverage auxiliary models to encourage learning of discriminative information earlier at various stages of the learning process (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014).", "startOffset": 133, "endOffset": 192}, {"referenceID": 29, "context": ", 2011a) or through a special regularizer (Weston et al., 2012).", "startOffset": 42, "endOffset": 63}, {"referenceID": 15, "context": "Other hybrid learning approaches follow similar ideas of (Lasserre et al., 2006), including the training schemes proposed in Ororbia II et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 12, "context": "Other hybrid learning approaches follow similar ideas of (Lasserre et al., 2006), including the training schemes proposed in Ororbia II et al. (2015b,a), which built on the initial ideas of Larochelle & Bengio (2008); Larochelle et al.", "startOffset": 58, "endOffset": 217}, {"referenceID": 12, "context": "(2015b,a), which built on the initial ideas of Larochelle & Bengio (2008); Larochelle et al. (2012), and of which the schemes of Calandra et al.", "startOffset": 75, "endOffset": 100}, {"referenceID": 4, "context": "(2012), and of which the schemes of Calandra et al. (2012); Zhou et al.", "startOffset": 36, "endOffset": 59}, {"referenceID": 4, "context": "(2012), and of which the schemes of Calandra et al. (2012); Zhou et al. (2012) were shown to be special cases.", "startOffset": 36, "endOffset": 79}, {"referenceID": 8, "context": "The DHBM shares some similarites to that of the \u201cstitched-together\u201d DBM of Salakhutdinov & Hinton (2009) and the MP-DBM (Goodfellow et al., 2013), which was originally proposed as another way to circumvent the greedy pre-training of DBM\u2019s using a back-propagation-based approach on an unfolded inference graph.", "startOffset": 120, "endOffset": 145}, {"referenceID": 8, "context": "The DHBM shares some similarites to that of the \u201cstitched-together\u201d DBM of Salakhutdinov & Hinton (2009) and the MP-DBM (Goodfellow et al., 2013), which was originally proposed as another way to circumvent the greedy pre-training of DBM\u2019s using a back-propagation-based approach on an unfolded inference graph. The key advantage of the MP-DBM is that it is capable of working on a variety of variational inference tasks beyond classification (i.e., input completion, classification with missing inputs, etc.). We note that combining our hybrid learning framework with a more advanced Boltzmann architecture like the MP-DBM could yield even more powerful semi-supervised models. The deep generative models proposed Kingma et al. (2014) also share similarity to our work especially in their use of a recognition network to make inference of latent states in such models fast and tractable.", "startOffset": 121, "endOffset": 735}, {"referenceID": 17, "context": "model that is capable of learning either discriminatively or generatively may be employed, such as Sum-Product Networks (Poon & Domingos, 2011) or recently proposed back-propagation-free models like the Difference-Target Propagation network (Lee et al., 2015).", "startOffset": 241, "endOffset": 259}, {"referenceID": 10, "context": "For simplicity, we only chose to incorporate a drop-out scheme Hinton et al. (2012) to our learning procedure since our comparative base-line (the psuedo-labeled MLP) also employed one.", "startOffset": 63, "endOffset": 84}, {"referenceID": 10, "context": "For simplicity, we only chose to incorporate a drop-out scheme Hinton et al. (2012) to our learning procedure since our comparative base-line (the psuedo-labeled MLP) also employed one. During learning, all this entails is applying random binary masks to the hidden layer statistics calculated from the recognition network and once again to calculated mean-field statistics (which will set various hidden units to 0 under a given probability). For the DHDA, the same binary masks used during calculation of layer-wise statistics are applied to the error deltas computed during back-propagation (as in Hinton et al. (2012)).", "startOffset": 63, "endOffset": 622}, {"referenceID": 3, "context": "We make use of the Massive Online Analysis framework (MOA, Bifet et al. (2010)) to simulate the process of online learning, where the learner does not have access to a static, finite data-set of i.", "startOffset": 59, "endOffset": 79}, {"referenceID": 7, "context": "995 (following the discussion of admissible values in Gama et al. (2012)).", "startOffset": 54, "endOffset": 73}, {"referenceID": 7, "context": "(Gama et al., 2012) showed that metrics with forgetting mechanisms were more appropriate for evolving data stream settings (as compared to simple predictive sequential error).", "startOffset": 0, "endOffset": 19}, {"referenceID": 29, "context": "81 EMBEDNN (Weston et al., 2012) 16.", "startOffset": 11, "endOffset": 32}, {"referenceID": 19, "context": "Alternatively, the DHBM could benefit from an additional weighted discriminative gradient like that used in the Top-DownBottom-Up algorithm of Ororbia II et al. (2015a). We also note that our DHDA model does not perform so well on this benchmark, only beating out the architectures trained on the supervised subset.", "startOffset": 151, "endOffset": 169}], "year": 2016, "abstractText": "Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine and the Deep Hybrid Denoising Auto-encoder, are proposed for handling semisupervised learning problems. The models combine experts that model relevant distributions at different levels of abstraction to improve overall predictive performance on discriminative tasks. Theoretical motivations and algorithms for joint learning for each are presented. We apply the new models to the domain of datastreams in work towards life-long learning. The proposed architectures show improved performance compared to a pseudo-labeled, drop-out rectifier network.", "creator": "LaTeX with hyperref package"}}}