{"id": "1511.06995", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "Non-Sentential Utterances in Dialogue: Experiments in Classification and Interpretation", "abstract": "Non - diegetic insinuations (NSUs) because utterances but impression on complete sentential possibly enough giving as can some explainable from created willingness understood, with as \" OK \", \" several? \", \" so at again apartment \". The interpretation some non - brechtian utterances is means important lack place biomedical criminology which they moreover piece typical reflection both dialogue from know exist functionally necessity - regulated. The description but NSUs another time reconstruction and retrieving the setting grammatical available half they form and soon dialogue context. The first six thus this co-authored be creative to made NSU characteristics working. Our work builds own Fern \\ ' andez cie. baghdadi. (year) which fact called series of mechanical - learning discoveries saw three classification example NSUs. We return way approach short though combination of new music there team - demobilization science study. The metaphysical impact details took its anthropology fox though surprisingly what certain efficiency week long state - between - been - art phylogenetic performance. The consecutive, yet organization, cannot where could able ordinarily from purpose semantic lacks from usually NSUs close once comparison of several follow fact. Fern \\ ' andez (1977) formalizes this task it current although \" bush rules \" small came top mainly the Type Theory with Records (TTR ). Our important even focused saw the reimplementation of though resolution impose one Fern \\ ' andez (october) over a discrete numbers included along meetings governor. The quantum-mechanical not analyser Lison (dec.) is have nimble first one progress because, similarly although own guidelines developed continued Ginzburg (1970) and Fern \\ ' andez (2006 ), it merely the metadata far nov. conventions on the formula_12 of followed dialogue of to capture that engineering of before interrupted. However, soon posits rules know addition encode probabilistic distinction, thereby offers a 80-run account of unresolved in although NSU resolution rather.", "histories": [["v1", "Sun, 22 Nov 2015 11:28:26 GMT  (460kb,D)", "http://arxiv.org/abs/1511.06995v1", "Master thesis, 98 pages, ISBN: 9788887096057"]], "COMMENTS": "Master thesis, 98 pages, ISBN: 9788887096057", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["paolo dragone"], "accepted": false, "id": "1511.06995"}, "pdf": {"name": "1511.06995.pdf", "metadata": {"source": "CRF", "title": "Non-Sentential Utterances in Dialogue: Experiments in Classification and Interpretation", "authors": ["Paolo Dragone", "Pierre Lison"], "emails": ["dragone.paolo@gmail.com"], "sections": [{"heading": null, "text": "Non-Sentential Utterances in Dialogue: Experiments in Classification and Interpretation\nDepartment of Computer, Control and Management Engineering Master of Science in Engineering in Computer Science\nCandidate Paolo Dragone 1370640\nThesis Advisor Prof. Roberto Navigli\nCo-Advisor Dr. Pierre Lison (University of Oslo)\nAcademic Year 2014/2015\nar X\niv :1\n51 1.\n06 99\n5v 1\n[ cs\n.C L\n] 2\n2 N\nov 2\n01 5\nNon-Sentential Utterances in Dialogue: Experiments in Classification and Interpretation Master thesis. Sapienza \u2013 University of Rome\n\u00a9 2015 Paolo Dragone. All rights reserved\nThis thesis has been typeset by LATEX and the Sapthesis class.\nVersion: October 14, 2015 Author\u2019s email: dragone.paolo@gmail.com"}, {"heading": "Abstract", "text": "Non-sentential utterances (NSUs) are utterances that lack a complete sentential form but whose meaning can be inferred from the dialogue context, such as \u201cOK\u201d, \u201cwhere?\u201d, \u201cprobably at his apartment\u201d. The interpretation of non-sentential utterances is an important problem in computational linguistics since they constitute a frequent phenomena in dialogue and they are intrinsically context-dependent. The interpretation of NSUs is the task of retrieving their full semantic content from their form and the dialogue context.\nNSUs also come in a wide variety of forms and functions and classifying them in the right category is a prerequisite to their interpretation. The first half of this thesis is devoted to the NSU classification task. Our work builds upon Ferna\u0301ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance.\nThe consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Ferna\u0301ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012). We argue that logic-based formalisms, such as TTR, have a number of shortcomings when dealing with conversational data, which often include partially observable knowledge and nondeterministic phenomena. An alternative to address these issues is to rely on probabilistic modeling of the dialogue context. Our work is focused on the reimplementation of the resolution rules from Ferna\u0301ndez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism (Lison, 2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Ferna\u0301ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation. However, the probabilistic rules can also encode probabilistic knowledge, thereby providing a principled account of ambiguities in the NSU resolution process. In the second part of this thesis, we present our proof-of-concept framework for NSU resolution using probabilistic rules.\nv"}, {"heading": "Acknowledgments", "text": "The past two years have been utterly intense, but beautiful nonetheless. In all my wandering around the Earth, first in Melbourne then in Oslo, I have learned an incredible amount of things and met as many interesting people. I have studied hard, explored the world, suffered homesickness and experienced many wonderful moments. It has been an exiting time and now, at the finishing line, I am walking towards the end with a smile on my face. For those opportunities I am grateful to my alma mater Sapienza which, despite all the difficulties, gave me also many positive memories.\nThis thesis is the product of the period I spent at University of Oslo under the supervision of Dr. Pierre Lison. Pierre has not only been a great supervisor but he has also been my mentor. He has taught me an uncountable number of things. He introduced me to the research on dialogue and proposed the topic of non-sentential utterances in the first place. My collaboration with Pierre has also led to my very first scientific publication at the SemDial 2015 in Go\u0308teborg. I can safely say that writing this thesis would not have been possible without his guidance. For these and many other reasons I want to thank him and wish him all the best. I also want to thank all the Language Technology Group at University of Oslo for their hospitality and for the many (or perhaps too few) happy days we spent together.\nA thankful note to all my friends too, who support me everyday of my life, even (and especially) when I am not around. In particular, a big thank you to all my friends and colleagues from the course in Engineering in Computer Science with whom I walked side by side for all these years. However, a special thanks goes to my estimate colleague Alessandro Ronca who has practically been my moral supporter as well as my personal reviewer.\nLast but not least, I want to express my gratitude to my parents and all my family who have always encouraged me to go beyond my own limits (even if sometimes I went perhaps too far away) and never stopped believing in me.\nRome, 22th of October 2015\nPaolo Dragone\nvii"}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 1", "text": "1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3"}, {"heading": "2 Background 5", "text": "2.1 Non-Sentential Utterances . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.1.1 A taxonomy of NSUs . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.1.2 The NSU corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.1.3 Interpretation of NSUs . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.2 A formal model of dialogue . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.2.1 Type Theory with Records . . . . . . . . . . . . . . . . . . . . . . . 11 2.2.2 The dialogue context . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2.3 Update rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.3 Probabilistic modeling of dialogue . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3.1 Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.3.2 Probabilistic rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20"}, {"heading": "3 Classification of Non-Sentential Utterances 21", "text": "3.1 The data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3.2 Machine learning algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.2.1 Classification: Decision Trees . . . . . . . . . . . . . . . . . . . . . . 23 3.2.2 Classification: Support Vector Machines . . . . . . . . . . . . . . . . 24 3.2.3 Optimization: Coordinate ascent . . . . . . . . . . . . . . . . . . . . 24\n3.3 The baseline feature set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.4 Feature engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.5 Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.5.1 Unlabeled data extraction . . . . . . . . . . . . . . . . . . . . . . . . 32 3.5.2 Semi-supervised learning techniques . . . . . . . . . . . . . . . . . . 33\n3.6 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n3.6.1 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.6.2 Empirical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\nix"}, {"heading": "4 Resolution of Non-Sentential Utterances 47", "text": "4.1 The resolution task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 4.2 Theoretical foundation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.2.1 Partial Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4.2.2 Propositional lexemes . . . . . . . . . . . . . . . . . . . . . . . . . . 50 4.2.3 Focus Establishing Constituents . . . . . . . . . . . . . . . . . . . . 50 4.2.4 Understanding and acceptance . . . . . . . . . . . . . . . . . . . . . 51 4.2.5 Sluicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n4.3 Dialogue context design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n4.3.1 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4.3.2 Dialogue acts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.3.3 Variables of the dialogue context . . . . . . . . . . . . . . . . . . . . 53\n4.4 NSU resolution rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.4.1 Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 4.4.2 Affirmative Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 4.4.3 Rejections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 4.4.4 Propositional Modifiers . . . . . . . . . . . . . . . . . . . . . . . . . 58 4.4.5 Check Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 4.4.6 Short Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 4.4.7 Sluices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 4.4.8 Clarification Ellipsis . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n4.5 Implementation and use case example . . . . . . . . . . . . . . . . . . . . . 64\n4.5.1 Dialogue system architecture . . . . . . . . . . . . . . . . . . . . . . 65 4.5.2 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71"}, {"heading": "5 Conclusion 73", "text": "5.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 5.2 Future developments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\nAppendices 79"}, {"heading": "A Context update rules 79", "text": "x"}, {"heading": "List of acronyms", "text": "AL Active Learning\nBNC British National Corpus\nDGB Dialogue Gameboard\nFEC Focus-Establishing Constituent\nMaxQUD Maximal QUD element\nNLG Natural Language Generation\nNLU Natural Language Understanding\nNSU Non-Sentential Utterance\nParPar Partial Parallelism\nQUD Question Under Discussion\nSMO Sequential Minimal Optimization\nSVM Support Vector Machine\nTSVM Transductive Support Vector Machine\nTTR Type Theory with Records\nxi\nChapter 1"}, {"heading": "Introduction", "text": "In dialogue, utterances do not always take the form of complete sentences. Utterances may sometimes lack some constituents \u2013 subject, verb or complements \u2013 because they can be understood from the previous utterances or other contextual information. These fragmentary utterances are often called non-sentential utterances (NSUs). The following are some examples from the British National Corpus:\n(1.1) a: How do you actually feel about that?\nb: Not too happy.\n[BNC: JK8 168\u2013169]1\n(1.2) a: They wouldn\u2019t do it, no.\nb: Why?\n[BNC: H5H 202\u2013203]\n(1.3) a: So will the tape last for the whole two hours?\nb: Yes, apparently.\n[BNC: J9A 76\u201377]\n(1.4) a: Right disk number four?\nb: Three.\n[BNC: HDH 377\u2013378]\nWe can understand without effort the meaning of the NSUs in the short dialogues above, even though they do not have the form of full sentences. We can easily make sense of them by extrapolating their meaning from the surrounding context, which for the above examples is given by the preceding utterance. Other possible contextual factors that affect the intended meaning of the NSUs are, for instance, the history of the dialogue, the shared environment of the conversational participants, their common knowledge and so on. From a computational linguistic perspective, making sense of this kind of utterances is a difficult problem because it involves the formalization of a robust theory of dialogue context.\n1This notation indicates the file name and the line numbers of the portion of dialogues in the British National Corpus.\n1\nMoreover, NSUs are a large variety of phenomena that need to be treated in different ways. Ferna\u0301ndez and Ginzburg (2002) identify 15 different types of NSUs. One of the problems that must be addressed to make sense of NSUs is determining their type. One possible way is to classify NSUs using machine learning, as previously experimented by Ferna\u0301ndez et al. (2007).\nTo interpret a given NSU, one also has to resolve its meaning i.e. construct an highlevel semantic representation of the NSU by extracting the relevant information from the dialogue context. To select the right resolution procedure for the given NSU, one needs first to determine its type. That is why the two task are connected. However, they can still be formalized and employed independently."}, {"heading": "1.1 Motivation", "text": "Non-sentential utterances are interesting in many ways. First of all, they are very frequent in dialogue. According to Ferna\u0301ndez and Ginzburg (2002) and related works, the frequency of NSUs in the dialogue transcripts of the British National Corpus is about 10% of the total number of utterances. However, this number may vary greatly if one takes into account a larger variety of phenomena or different dialogue domains e.g. Schlangen (2003) estimates the frequency of NSUs to be 20% of the total number of utterances.\nDespite their ubiquity, the semantic content of NSUs is often difficult to extract automatically. Non-sentential utterances are indeed intrinsically dependent on the dialogue context. It is impossible to make sense of them without accessing to the surrounding context. Their high context-dependency makes their interpretation a difficult problem from both a theoretical and computational point of view.\nNSUs form a wide range of linguistic phenomena that need to be considered in the formulation of a theory of dialogue context. Only few previous works tackled this problem directly and the majority of them take place in theoretical semantics of dialogue without addressing the possible applications. This means that the interpretation of NSUs is still an understudied problem, making them possibly an even more interesting subject."}, {"heading": "1.2 Contribution", "text": "Our work follows two parallel paths. On one hand we address the problem of the classification of NSUs by extending the work of Ferna\u0301ndez et al. (2007). On the other hand we propose a novel approach to the resolution of NSUs using probabilistic rules (Lison, 2015).\nThe classification task is needed to select the resolution procedure but it is nonetheless an independent problem and it can arise in many different situations. Our contribution to this problem is a small but significant improvement over the accuracy of the previous works as well as the exploration of one way to tackle the scarcity of labeled data.\n2\nOur work on the resolution of NSUs takes inspiration from Ferna\u0301ndez (2006) and Ginzburg (2012) which provide the theoretical background for our study. Their framework is however purely logic-based therefore it can have some drawbacks in dealing with raw conversational data which often contains hidden or partially observable variables. To this end a probabilistic account of the dialogue state is preferable. In our work we implemented a new approach to NSU resolution based on the probabilistic rules formalism of Lison (2015). Probabilistic rules are similar, in some way, to the rules formalized by Ginzburg (2012), as both express updates on the dialogue state given a set of conditions. However, probabilistic rules can also take into account probabilistic knowledge, thereby making them more suited to deal with the uncertainty often associated with conversational data. Our work does not aim to provide a full theory of NSU resolution but rather be a proof-of-concept for the resolution of NSUs via the probabilistic rules formalism. Nevertheless we detail a large set of NSU resolution rules based on the probabilistic rules formalism and provide an actual implementation of a dialogue system for NSU resolution using the OpenDial toolkit (Lison and Kennington, 2015), which can be the baseline reference for future developments.\nOur work for this thesis has produced the following publications:\n\u2022 Paolo Dragone and Pierre Lison. Non-Sentential Utterances in Dialogue: Experiments in classification and interpretation. In: Proceedings of the 19th workshop\non the Semantics and Pragmatics of Dialogue, SEMDIAL 2015 \u2013 goDIAL, p. 170. Go\u0308teborg, 2015.\n\u2022 Paolo Dragone and Pierre Lison. An Active Learning Approach to the Classification of Non-Sentential Utterances. In: Proceedings of the second Italian Conference on\nComputational Linguistics, CLiC-IT 2015, in press. Trento, 2015."}, {"heading": "1.3 Outline", "text": "Chapter 2\nThis chapter discusses the background knowledge needed for the development of the following chapters. In particular the chapter describes the concept of non-sentential utterance and the task of interpretation of NSUs with an emphasis on the previous works. Secondly the chapter contains an overview on the formal representation of the dialogue context from the theory of Ginzburg (2012). We discuss briefly the Type Theory with Records, the semantic representation of utterances and the update rules on the dialogue context. Finally, we introduce the probabilistic approach to the definition of the dialogue context from Lison (2014). We discuss the basics of Bayesian Networks (the dialogue context representation) and the probabilistic rules formalism.\n3\nChapter 3\nThis chapter describes the task of the classification of non-sentential utterances. It provides details on our approach, starting from the replication of the work from Ferna\u0301ndez et al. (2007) which we use as baseline. We then discuss the extended feature set we used and the semi-supervised learning techniques we employed in our experiments. Lastly we discuss the empirical results we obtained.\nChapter 4\nThis chapter describes the problem of resolving non-sentential utterances and our approach to address it through probabilistic rules. First we formalize the NSU resolution task and describe the theoretical notions needed to address it. We then describe our dialogue context design as a Bayesian network and our formulation for the resolution rules as probabilistic rules. In the end we describe our implementation and an extended example of its application to a real-world scenario.\nChapter 5\nThis is the conclusive chapter of this thesis which summarizes the work and describes possible future works.\n4\nChapter 2"}, {"heading": "Background", "text": ""}, {"heading": "2.1 Non-Sentential Utterances", "text": "From a linguistic perspective, Non-Sentential Utterances \u2013 also known as fragments \u2013 has been historically an umbrella term for many elliptical phenomena that often take place in dialogue. In order to give a definition of Non-Sentential Utterances ourselves, we shall start by quoting the definition given by Ferna\u0301ndez (2006):\n\u201cIn a broad sense, non-sentential utterances are utterances that do not have the form of a full sentence according to most traditional grammars, but that nevertheless convey a complete sentential meaning, usually a proposition or a question.\u201d\nThis is indeed a very general definition, whereas a perhaps simpler approach is taken by Ginzburg (2012) which defines NSUs as \u201cutterances without an overt predicate\u201d. The minimal clausal structure of a sentence in English (as in many other languages) is composed of at least a noun phrase and a verb phrase. However, in dialogue the clausal structure is often truncated in favor of shorter sentences that can be understood by inferring their meaning from the surrounding context. We are interested in those utterances that, despite the lack of a complete clausal structure, convey a well-defined meaning given the dialogue context.\nThe context of an NSU can comprise any variable in the dialogue context but it usually suffice to consider only the antecedent of the NSU. The \u201cantecedent\u201d of an NSU is the utterance in the dialogue history that can be used to infer its underspecified semantic content. For instance, the NSU in (2.1) can be interpreted as \u201cPaul went to his apartment\u201d by extracting its semantic content from the antecedent. Generally, it is possible to understand the meaning of an NSU by looking at its antecedent.\n(2.1) a: Where did Paul go?\nb: To his apartment.\n5\nIt is often the case that an NSU and its antecedent present a certain grade of parallelism. Usually the meaning of an NSU is associated to a certain aspect of the antecedent. As described in Ginzburg (2012), the parallelism between an NSU and its antecedent can be of syntactic, semantic or phonological nature. The NSU in (2.1) presents syntactic parallelism \u2013 the use of \u201chis\u201d is syntactically constrained by the fact that Paul is a male individual \u2013 as well as semantic \u2013 the content of an NSU is a location as constrained by the where interrogative. This parallelism is one of the properties of NSUs that can be exploited in their interpretation (more details in Chapter 4). Even though it is often the case, the antecedent of an NSU is not always the preceding utterance, especially in multi-party dialogues."}, {"heading": "2.1.1 A taxonomy of NSUs", "text": "As we briefly mentioned in Chapter 1, non-sentential utterances come in a large variety of forms. We can categorize NSUs on the basis of their form and their intended meaning. For instance NSUs can be affirmative or negative answers to polar questions, requests for clarification or corrections.\nIn order to classify the NSUs, we use a taxonomy defined by Ferna\u0301ndez and Ginzburg (2002). This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Ferna\u0301ndez (2006) then refined by Ginzburg (2012).\nOther taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Ferna\u0301ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Ferna\u0301ndez et al. (2007) and it is also used in the theory of Ginzburg (2012), which is our reference for the resolution part of our investigation. A detailed comparison of this taxonomy and other ones is given by Ferna\u0301ndez (2006), which also details the corpus study on the BNC that led to the definition of this taxonomy.\nFollows a brief description of all the classes with some examples. Ferna\u0301ndez (2006) provides more details about the rationale of each class."}, {"heading": "Plain Acknowledgment", "text": "Acknowledgments are used to signal understanding or acceptance of the preceding utterance, usually using words or sounds like yeah, right, mhm.\n(2.2) a: I shall be getting a copy of this tape.\nb: Right.\n[BNC: J42 71\u201372]\n6"}, {"heading": "Repeated Acknowledgment", "text": "This is another type of acknowledgement that make use of repetition or reformulation of some constituent of the antecedent to show understanding.\n(2.3) a: Oh so if you press enter it\u2019ll come down one line.\nb: Enter.\n[BNC: G4K 102\u2013103]"}, {"heading": "Clarification Ellipsis", "text": "These are NSUs that are used to request a clarification of some aspect of the antecedent that was not fully understood.\n(2.4) a: I would try F ten.\nb: Just press F ten?\n[BNC: G4K 72\u201373]"}, {"heading": "Check Question", "text": "Check Questions are used to request an explicit feedback of understanding or acceptance, usually uttered by the same speaker as the antecedent.\n7\n(2.5) a: So (pause) I\u2019m allowed to record you.\nOkay?\nb: Yes.\n[BNC: KSR 5\u20136]"}, {"heading": "Sluice", "text": "Sluices are used for requesting additional information related to or underspecified into the antecedent.\n(2.6) a: They wouldn\u2019t do it, no.\nb: Why?\n[BNC: H5H 202\u2013203]"}, {"heading": "Filler", "text": "These are fragments used to complete a previous unfinished utterance.\n(2.7) a: [...] would include satellites like erm\nb: Northallerton.\n[BNC: H5D 78\u201379]"}, {"heading": "Short Answer", "text": "The NSUs that are typically answers to wh-questions.\n(2.8) a: What\u2019s plus three times plus three?\nb: Nine.\n[BNC: J91 172\u2013173]"}, {"heading": "Plain Affirmative Answer and Plain Rejection", "text": "A type of NSUs used to answer polar questions using yes-words and no-words.\n(2.9) a: Have you settled in?\nb: Yes, thank you.\n[BNC: JSN 36\u201337]\n(2.10) a: (pause) Right, are we ready?\nb: No, not yet.\n[BNC: JK8 137\u2013138]\n8"}, {"heading": "Repeated Affirmative Answer", "text": "NSUs used to give an affirmative answer by repeating or reformulating part of the query.\n(2.11) a: You were the first blind person to be employed in the County Council?\nb: In the County Council, yes.\n[BNC: HDM 19\u201320]"}, {"heading": "Helpful Rejection", "text": "Helpful Rejections are used to correct some piece of information from the antecedent.\n(2.12) a: Right disk number four?\nb: Three.\n[BNC: H61 10\u201311]"}, {"heading": "Propositional and Factual Modifiers", "text": "Used to add modal or attitudinal information to the previous utterance. They are usually expressed (respectively) by modal adverbs and exclamatory factual (or factive) adjectives.\n(2.13) a: Oh you could hear it?\nb: Occasionally yeah.\n[BNC: J8D 14\u201315]\n(2.14) a: You\u2019d be there six o\u2019clock gone mate.\nb: Wonderful.\n[BNC: J40 164\u2013165]"}, {"heading": "Bare Modifier Phrase", "text": "Modifiers that behave like non-sentential adjunct modifying a contextual utterance.\n(2.15) a: [...] then across from there to there.\nb: From side to side.\n[BNC: HDH 377\u2013378]"}, {"heading": "Conjunct", "text": "A Conjunct is a modifier that extends a previous utterance through a conjunction.\n(2.16) a: I\u2019ll write a letter to Chris\nb: And other people.\n[BNC: G4K 19\u201320]\n9"}, {"heading": "NSU Class Total %", "text": ""}, {"heading": "2.1.2 The NSU corpus", "text": "The taxonomy presented in the previous section is the result of a corpus study on a portion of the dialogue transcripts in the British National Corpus, first started by Ferna\u0301ndez and Ginzburg (2002), then refined by Ferna\u0301ndez (2006). The dialogue transcripts used in the corpus study contain both two-party and multi-party conversations. The transcripts cover a wide variety of dialogue domains including free conversation, interviews, seminars and more. Ferna\u0301ndez (2006) also describes the annotation procedure and a reliability test. The reliability test was carried out on a subset of the annotated instances comparing the manual annotation of three annotators. The test showed a good agreement between the annotators with a kappa-score of 0.76. From this test it is also clear that humans can reliably distinguish between the NSU classes in the taxonomy. Ferna\u0301ndez (2006) provides more details about the complete analysis of the corpus.\nIn total about 14 000 sentences from 54 files were examined by the annotators, resulting in a corpus of 1 299 NSUs, about 9% of the total of the sentences examined. Of the extracted NSUs, 1 283 were successfully categorized according to the defined taxonomy making up a coverage of 98.9%. Table 2.2 shows the distribution of the classes in the corpus.\nThe annotated instances were also tagged with a reference to the antecedent of the NSU. About 87.5% of annotated NSUs have their immediately preceding utterance as antecedent. Ferna\u0301ndez (2006) describes a study of the distance between NSUs and their antecedents, with a comparison between two-party and multi-party dialogues.\n10"}, {"heading": "2.1.3 Interpretation of NSUs", "text": "Due to their incomplete form, non-sentential utterances do not have an exact meaning by themselves. They need to be \u201cinterpreted\u201d i.e. their intended meaning must be inferred from the dialogue context. One way to interpret NSUs is developed by Ferna\u0301ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs. The first step for the interpretation of an NSU is its classification i.e. finding its class according to the taxonomy described in Section 2.1.1. As demonstrated in Ferna\u0301ndez et al. (2007), we can infer the class of an NSU using machine learning, i.e. we can train a classifier on the corpus detailed in Section 2.1.2 and use it to classify unseen NSU instances. The type of an NSU is used to determine the right resolution procedure to use. The resolution of an NSU is the task of recovering the full clausal meaning from their incomplete form on the basis of contextual information. Ferna\u0301ndez (2006) describes a resolution procedure in terms of rules that, given some preconditions on the antecedent and other elements of the dialogue states, builds the semantic representation of the NSU. This approach to the resolution of NSUs has been the basis of several implementations of dialogue systems handling the resolution of NSUs such as Ginzburg et al. (2007) and Purver (2006).\nExtending the interpretation problem to raw conversational data we need also a way to \u201cdetect\u201d an NSU i.e. decide whether an utterance should be considered as an NSU in the first place. Since this is not our direct concern, we employ in our experiments a simple set of heuristics to distinguish between NSU and non-NSU utterances (see Section 3.5.1)."}, {"heading": "2.2 A formal model of dialogue", "text": "As theoretical base of our work we rely on the theory of dialogue context brought up by Ginzburg (2012), which presents a grammatical framework expressly developed for dialogue. The claim of Ginzburg (2012) is that the rules that encode the dynamics of the dialogue have to be built into the grammar itself. The grammatical framework is formulated using Type Theory with Records (Cooper, 2005). Type Theory with Records (TTR) is a logical formalism developed to cope with semantics of natural language. TTR is used to build a semantic ontology of abstract entities and events as well as to formalize the dialogue gameboard i.e. a formal representation of the dialogue context and its rules. The evolution of the conversation is formalized by means of update rules on the dialogue context. Ginzburg (2012) also accounts for NSUs and provides a set of dedicated rules."}, {"heading": "2.2.1 Type Theory with Records", "text": "We will now briefly introduce the basic notions of the Type Theory with Records (TTR), with just enough detail needed by to understand the following sections, referring to Ginzburg (2012) for a complete description.\n11\nIn TTR, objects can be of different types. The statement x : T is a typing judgment, indicating that the object x is of type T . If x is of type T , x is said to be a witness of T . Types can either be basic (atomic) such as IND1 or complex i.e. dependent on other objects or types such as drive(x, y). Types also include constructs such as lists, sets and so on. Other useful constructs are records and record types. A record contains a set of assignments between labels and values whereas a record type contains a set of judgments between labels and types:\nr :  l1 = v1 l2 = v2 . . .\nln = vn\n \u03c1 :  l1 : T1 l2 : T2 . . .\nln : Tn  The record r is of record type \u03c1 if and only if v1 :T1 \u2227 v2 :T2 \u2227 . . . \u2227 vn :Tn. Typing judgment can be used to indicate the record r being of record type \u03c1 as r : \u03c1.\nTTR also provides function types of the form T1 \u2192 T2 which maps records of type T1 to records of type T2. Functional application is indicated as (x : T1).T2."}, {"heading": "Utterance representation", "text": "At the basis of the grammatical framework of Ginzburg (2012) lies the notion of proposition. Propositions are entities used to represent facts, events and situations as well as to characterize the communicative process. In TTR propositions are records of the type:\nProp = sit : Record sit-type : RecType \nA simple example of proposition may be the following:\nPaul drives a car. sit = r1 sit-type =  x : IND p1 : named(x,Paul) y : IND p2 : car(y)\nc : drive(x, y)\n \n1The type IND stands for a generic \u201cindividual\u201d.\n12\nOn the other hand, questions are represented as propositional abstracts i.e. functions from the question domain \u2206q to propositions. Following the definition of Ferna\u0301ndez (2006):\nQuestion = \u2206q \u2192 Prop\nThe question domain \u2206q is a record type containing the wh-restrictors of the question q. 2 The wh-restrictors are record types that characterize the necessary information needed to resolve a wh-question e.g. for a where interrogative the answer must be a place instead for a when interrogative it must be a time. Clearly, the right wh-restrictor depends on the wh-interrogative used. Consider the following example of a wh-question:\nWho drives?r : x : IND\nrest : person(x)\n . sit = r1\nsit-type = [ c : drive(r.x)\n] \nHere the question domain of the who interrogative is an individual x that is a person.\nPolar questions, i.e. bare yes/no-questions, are represented as propositional abstract as the wh-questions, with the difference that their question domain is an empty record type. An example of polar question:\nDoes Paul drive?\n( r : [ ]) .  sit = r1 sit-type =  x : IND p1 : named(x,Paul)\nc : drive(x)\n \nA special type of propositions are used to represent the content of conversational moves which need to take into a account the relation that stands between the speaker, the addressee and the content of the move. Those are called illocutionary propositions (of type IllocProp) and the relation that they contain is called illocutionary relation3. Illocutionary relations indicates the function of a proposition, such as \u201cAssert\u201d, \u201cAsk\u201d, \u201cGreet\u201d. For a proposition p, the illocutionary proposition that holds p as its content can be indicated as R(spkr, addr, p), where R is the illocutionary relation, spkr and addr refer respectively to the speaker and the addressee4. Examples of illocutionary propositions are:\nAssert(spkr : IND, addr : IND, p : Prop)\nAsk(spkr : IND, addr : IND, q : Question)\n2Ginzburg (2012) extends this field to be a list of record types to take into account situations with multiple question domains.\n3Also called illocutionary act or dialogue act. 4For brevity only the semantic content of the illocutionary proposition is shown here.\n13"}, {"heading": "2.2.2 The dialogue context", "text": "In Ginzburg (2012), the dialogue context \u2013 also known as the Dialogue Gameboard (DGB) \u2013 is a formal representation that describes the current state of the dialogue. It includes a wide range of variables needed to handle different aspects of the dialogue. However, we concentrated on the most basic ones:\n\u2022 Facts, a set of known facts;\n\u2022 LatestMove, the latest move made in the dialogue;\n\u2022 QUD, a partially ordered set of questions under discussion.\nThe DGB can be represented in TTR as a record in the following way: Facts : Set(Prop) LatestMove : IllocProp\nQUD : poset(Question)  The elements in the DGB represent the common ground of the conversation, shared between all the participants. In this representation we abstracted away several details that would be included in the actual DGB presented by Ginzburg (2012) such as the fields to track who is holding the turn, the current time and so on. We now detail the basic variables of the DGB."}, {"heading": "Facts", "text": "Facts is a set of known facts, shared by all the conversational participants. The elements of Facts are propositions, which are assumed to be sufficient to encode the knowledge of the participants within the context of the dialogue. The Facts encode all the records that are accepted by all participants, i.e. facts that will not raise issues in the future development of the conversation. A complementary problem that we marginally address is the understanding \u2013 or grounding \u2013 of a sentence. Ginzburg (2012) develops a comprehensive theory of grounding but we do not include it in our work."}, {"heading": "LatestMove", "text": "Dialogue utterances are made of coherent responses to the preceding utterances, that is why it is important to keep track of the history of the dialogue. In a two-party dialogue it is usually the case that the current utterance is a response to previous one, instead in a multi-party dialogue can be useful to keep track of a larger window of the dialogue history. Ginzburg (2012) keeps track of the history of the dialogue within the variable Moves while a reference to the latest (illocutionary) proposition is recorded in the field LatestMove.\n14"}, {"heading": "QUD", "text": "QUD is a set of questions under discussion. In a general sense, a \u201cquestion under discussion\u201d represents an issue being raised in the conversation which drives the future discussion. Despite the name, QUDs may arise from both questions and propositions.\nGinzburg (2012) defines QUD as a partially ordered set (poset). Its ordering determines the priority of the issues to be resolved. Of particular importance is the first element in the set according to the defined ordering which is taken as the topic of discussion of the subsequent utterances until it is resolved. Such element is referred to as MaxQUD.\nThe formalization of the ordering is a rather complex matter in a generic theory of context that needs to account for the beliefs of the participants and it is especially problematic when dealing with multi-party dialogues. The usage of QUD is of particular importance in our case because the MaxQUD is used as the antecedent in the interpretation of NSUs."}, {"heading": "2.2.3 Update rules", "text": "The dynamics of the DGB are defined by a set of update rules \u2013 also called conversational rules \u2013 which are applied on the DGB throughout the course of the conversation. Update rules are formalized as a set of effects on the parameters of the DGB given that certain preconditions hold. An update rule can be represented in the following way:pre : [ . . .\n] effects : [ . . . ] \nwhere both pre and effects are subsets of the parameters of the DGB and they respectively represent the necessary conditions for the application of the rule and the values of the involved variables right after the application of the rule.\nGinzburg (2012) defines all sorts of rules needed to handle a great variety of conversational protocols. Rules that are particularly interesting with respect to our work are those that handle queries and assertions as well as the ones that describe the dynamics of QUD and Facts.\nThe following rule describes how QUD is incremented when a question is posed: pre : q : Question LatestMove = Ask(spkr, addr, q) : IllocProp  effects : [ qud = \u3008 q, pre.qud \u3009 : poset(Question) ] \n15\nAs argued above, issues are also raised by assertions, as realized by the following rule: pre : p : Prop LatestMove = Assert(spkr, addr, p) : IllocProp  effects : [ qud = \u3008 p?, pre.qud \u3009 : poset(Question) ] \nThe act of answering to a question is nothing else than asserting a proposition that resolves such a question. As a consequence the other speaker can either raise another issue related to the previous one or accept the fact that the issue has been resolved. The acceptance move is realized in the following way: pre :  p : Prop LatestMove = Assert(spkr, addr, p) : IllocProp qud = \u3008p?, pre.qud \u3009 : poset(Question)  effects :  spkr = pre.addr : Ind addr = pre.spkr : Ind\nLatestMove = Accept(spkr, addr, p) : IllocProp\n  The speaker can also query the addressee with a Check move in order to ask for an explicit acknowledgment (Confirm) to a question-resolving assertion5. Acceptance and confirmation lead to an update of Facts and to a \u201cdowndate\u201d of the QUD i.e. the removal of the resolved questions in QUD: pre :  p : Prop LatestMove = Accept(spkr, addr, p) \u2228 Confirm(spkr, addr, p) : IllocProp qud = \u3008p?, pre.qud \u3009 : poset(Question)  effects :\nfacts = pre.facts \u222a { p } : Set(Prop) qud = NonResolve(pre.qud, facts) : poset(Question) \n\nWhile QUD represents the unresolved issues that have been introduced in the dialogue, Facts contains all the issues that have been resolved instead. That is why their update rules are closely related. The function NonResolve in the above rule checks for any resolved issues by the just updated facts and leave the unresolved ones into QUD.\n5The rules for the Check and Confirm moves are omitted for brevity.\n16"}, {"heading": "2.3 Probabilistic modeling of dialogue", "text": "In the previous section we detailed a logic-based model of dialogue from Ginzburg (2012). Another possible approach to dialogue modeling relies on probabilistic models to encode the variables and the dynamics of the dialogue context. Arguably this approach can be considered more robust to the intrinsic randomness present in dialogue. This is partially the reason why we explored this strategy as well as other advantages that will be discussed in Chapter 4.\nWe based our work on the probabilistic rules formalism developed by Lison (2012). This formalism is particularly suited for our purpose because of their commonalities with the update rules described in Section 2.2.3. The probabilistic rules formalism is based on the representation of the dialogue state as a Bayesian network. In this section we briefly describe how Bayesian networks are structured, then we detail the probabilistic rules formalism that we employ in Chapter 4 to model the resolution of the NSUs."}, {"heading": "2.3.1 Bayesian Networks", "text": "Bayesian networks are probabilistic graphical models6 representing a set of random variables (nodes) and their conditional dependency relations (edges). A Bayesian network is a directed acyclic graph i.e. a direct graph that does not contain cycles (two random variables cannot be mutually dependent). Given the random variables X1, . . . , Xn in a Bayesian network, we are interested in the joint probability distribution P (X1, . . . , Xn) of those variables. In general, the size of the joint distribution is exponential in the number n of variables therefore it is difficult to estimate when n grows. In the case of Bayesian networks we can exploit the conditional independence to reduce the complexity of the joint distribution. Given three random variables X, Y and Z, X and Y are said to be conditionally independent given Z if and only if (for all combinations of values) P (X,Y |Z) = P (X|Z)P (Y |Z). We can define for a variable Xi in X1, . . . , Xn the set parents(Xi) such that if there is a direct edge from Xj to Xi then Xj \u2208 parents(Xi). Given a topological ordering7 of the variables (nodes) of the Bayesian network, a variable Xi is conditionally independent from all its predecessor that are not in parents(Xi) therefore the joint probability distribution can be defined as follows:\nP (X1, . . . , Xn) = n\u220f i=1 P (Xi|parents(Xi))\nFor each variable Xi, P (Xi|parents(Xi)) is the conditional probability distribution (CPD) of Xi. The CPDs together with the directed graph fully determine the joint distribution of the Bayesian network.\n6A type of probabilistic models represented by graphs. 7A topological ordering is an ordering of the nodes such that for every two nodes u and v connected by a directed edge from u to v, u appears before v in such ordering. A topological ordering can only be defined on directed acyclic graphs.\n17\nThe network can be used for inference by querying the distribution of a subset of variables, usually given some evidence. Given a subset of variables Q \u2282 X and an assignment of values e of the evidence variables, the query is the posterior distribution P (Q|e). To compute the posterior distribution one needs an inference algorithm. Such algorithm can be exact \u2013 such as the variable elimination algorithm (Zhang and Poole, 1996) \u2013 or approximate \u2013 such as the loopy belief propagation algorithm (Murphy et al., 1999).\nThe distributions of the single variables can be learned from observed data using maximum likelihood estimation or Bayesian learning."}, {"heading": "2.3.2 Probabilistic rules", "text": "The probabilistic rules formalism is a domain-independent dialogue modeling framework. Probabilistic rules are expressed as if . . . then . . . else . . . constructs mapping logical conditions on the state variables to effects encoded by either probability distributions or utility functions. The former are called probability rules while the latter are utility rules. While we make use of both types of rules in our work, here we concentrate only on the probability rules which are the ones used for the resolution of the NSUs.\nLet c1, . . . , cn be a sequence of logical conditions and P (E1), . . . , P (En) a sequence of categorical probability distributions8 over mutually exclusive effects. A probability rule r is defined as follows:\nr :\n\u2200x if c1 then P (E1 = e1,1) = p1,1 . . .\nP (E1 = e1,m1) = p1,m1\nelse if c2 then P (E2 = e2,1) = p2,1 . . .\nP (E2 = e2,m2) = p2,m2\nelse P (En = en,1) = pn,1 . . .\nP (En = en,mn) = pn,mn\n8A categorical distribution is a probability distribution of an event having a finite set of outcomes with defined probability.\n18\nThe random variable Ei encodes a range of possible effects ei,1, . . . , ei,mi , each one with a corresponding probability pi,j . The conditions and effects of a rule may include underspecified variables, denoted with x, which are universally quantified on the top of the rule. The effects are duplicated for every possible assignments (grounding) of the underspecified variables.\nEach pair of condition and probability distribution over the effects \u3008ci, P (Ei)\u3009 is a branch bri of the rule. Overall, the rule is a sequence of branches br1, . . . , brn. The rule is \u201cexecuted\u201d by running sequentially through the branches. Only the first condition satisfied triggers the corresponding probabilistic effect, the subsequent branches are ignored (as in programming languages).\nThe dialogue state is represented as a Bayesian network containing a set of nodes (random variables). At each state update, rules are instantiated as nodes in the network. For each rule, the input edges of the node come from the condition variables whereas the output edges go towards the effect variables. The probability distribution of the rule is extracted by executing it. The probability distribution of the effect variables are then retrieved by probabilistic inference. Lison (2014) details the rules and update procedure.\nThe probabilistic rules are useful in at least three ways:\n\u2022 They are expressly designed for dialogue modeling. They combine the expressivity of both probabilistic inference and first order logic. This is an advantage in dialogue\nmodeling where one has to describe objects that relate to each other in the dialogue domain and, at the same time, handle uncertain knowledge of the state variables.\n\u2022 They can cope with the scarcity of training data of most dialogue domains by exploiting the internal structure of the dialogue models. By using logical formulae\nto encode the conditions for a possible outcome, it is possible to group the values of the variables into partitions, reducing the number of parameters needed to infer the outcome distribution and therefore the amount of data needed to learn the distribution.\n\u2022 The state update is handled with probabilistic inference therefore they can operate under uncertain settings which is often needed in dialogue modeling where variables\nare best represented as belief states, continuously updated by observed evidence.\nThe probabilistic rules formalism has also been implemented into a framework called OpenDial (Lison and Kennington, 2015). OpenDial is a Java toolkit for developing spoken dialogue systems using the probabilistic rules formalism. Using an XML-based language it is possible to define in OpenDial the probabilistic rules to handle the evolution of the dialogue state in a domain-independent way. OpenDial can either work on existent transcripts or as an interactive user interface. OpenDial can also learn parameters from small amounts of data using either supervised or reinforcement learning.\n19"}, {"heading": "2.4 Summary", "text": "In this chapter we discussed the background knowledge needed for describing our work on non-sentential utterances. We first described the notion of non-sentential utterances and the problem of interpreting them. We showed how those utterances can be categorized with a taxonomy from Ferna\u0301ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Ferna\u0301ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task. Ferna\u0301ndez (2006) describes a set of NSU resolution rules rooted in a TTR representation of the dialogue context. Section 2.2 briefly described the TTR notions we employed as well as the dialogue context theory based on TTR from Ginzburg (2012).\nAt last we described the probabilistic modeling of dialogue from Lison (2014) based on the probabilistic rules formalism. As we mentioned in Chapter 1 this formalism is the framework for our formulation of the NSU resolution rules on the basis of the one developed by Ferna\u0301ndez (2006). We described in Section 2.3 the basic notion of Bayesian networks which is the representation of the dialogue state employed by the probabilistic rules formalism. Finally, in Section 2.3.2 we explained the probabilistic rules formalism itself and its advantages.\n20\nChapter 3"}, {"heading": "Classification of Non-Sentential", "text": ""}, {"heading": "Utterances", "text": "Non-sentential utterances are pervasive dialogue phenomena. Any dialogue processing application (e.g. parsing or machine translation of dialogues, or interactive dialogue systems) has to take into account the presence of NSUs and deal with them. As described in Section 2.1, the NSUs come in a great variety of forms that must be treated differently from one another. To this end, the most basic (and perhaps useful) task is classifying them. In our work we employ the taxonomy and the corpus described in Sections 2.1.1 and 2.1.2. As demonstrated by Ferna\u0301ndez et al. (2007), we can use machine learning techniques to automatically classify a given NSU, using the annotated corpus as training data. Ferna\u0301ndez et al. (2007) is our main theoretical reference and (to our knowledge) the state-of-the-art in performance for the task of classification of NSUs. We first replicated the approach of the aforementioned work and used it as a benchmark for our experiments. Secondly, we tried to improve the classification performances, starting from an expansion of the feature set, then employing semi-supervised learning to address the scarcity of labeled data."}, {"heading": "3.1 The data", "text": "The corpus from Ferna\u0301ndez et al. (2007) contains 1 283 annotated NSU instances, each one identified by the name of the containing BNC file and their sentence number, a sequential number to uniquely identify a sentence in a dialogue transcript. The instances are also tagged with the sentence number of their antecedent which makes up the context for the classification. The raw utterances can be retrieved from the BNC using this information.\nFor the classification task, we make the same simplifying restriction on the corpus made by Ferna\u0301ndez et al. (2007), that is to consider only the NSUs whose antecedent is their preceding sentence. This assumption facilitates the feature extraction procedure without reducing significantly the size of the dataset (about 12% of the total). The resulting distribution of the NSUs after the restriction is showed in Table 3.1.\n21"}, {"heading": "NSU class Total", "text": "As one can see from Table 3.1, the distribution of the instances is quite skewed, largely in favor of some classes than others. Moreover very frequent classes are usually the easiest to classify, leaving the most difficult ones with few instances as examples for the classifiers. Although the scarcity of the training material and the imbalance of the classes are the two major problems for the classification task, we propose a set of methods to address them, as described in the following sections."}, {"heading": "The British National Corpus", "text": "The British National Corpus (Burnard, 2000) \u2013 BNC for short \u2013 is a collection of spoken and written material, containing about 100 million words of (British) English texts from a large variety of sources. Among the others, it contains a vast selection of dialogue transcripts covering a wide range of domains. Each dialogue transcript in the BNC is contained in an XML file along with many details about the dialogue settings. The dialogues are structured following the CLAWS tagging system (Garside, 1993) which segmented the utterances both at word and sentence level. The word units contains both the raw text, the corresponding lemma (headword) and the POS-tag according to the C5 tagset (Leech et al., 1994). Each sentence is identified by an unique ID number within the file. Sentences can also contain information about the pauses and the unclarities. The sentences are sorted in their order of appearance and include additional information about temporal alignment in case of overlapping.\n22"}, {"heading": "3.2 Machine learning algorithms", "text": "We employ two different supervised learning algorithms: decision trees and support vector machines. The former are used mainly as a comparison with Ferna\u0301ndez et al. (2007) which employ this algorithm as well. For parameters tuning we implemented a coordinate ascent algorithm. As a framework for our experiments we rely on the Weka toolkit (Hall et al., 2009), a Java library containing the implementation of many machine learning algorithms as well as a general-purpose machine learning API."}, {"heading": "3.2.1 Classification: Decision Trees", "text": "We employ the C4.5 aglorithm (Quinlan, 1993) for decision tree learning. Weka contains an implementation of this algorithm called J48. The goal of decision tree learning is to create a predictive model from the training data. The construction of the decision tree is performed by splitting the training set into subsets according to the values of an attribute. This process is then repeated recursively on each subset. The construction algorithm is usually an informed search using some kind of heuristics to drive the choice of the splitting attribute. In the case of C4.5 the metric used for the attribute choice is the expected information gain. The information gain is based on the concept of entropy.\nIn information theory, the entropy (Shannon, 1948) is the expected value of information carried by a message (or an event in general). It is also a measure of the \u201cunpredictability\u201d of an event. The more unpredictable an event is, the more information it provides when it occurs. Formally, the entropy of a random variable X is\nH(X) = \u2212 \u2211 i P (xi) logP (xi)\nwhere P (xi) is the probability of the i-th value of the variable X. A derived notion is the conditional entropy of a random variable Y knowing the value of another variable X:\nH(Y |X) = \u2211 i P (xi)H(Y |X=xi)\n= \u2211 i P (xi) \u2211 j P (yj |xi) logP (yj |xi)\nwhere xi are the values of the variable X and yj are the value of the variable Y .\nFor the decision tree construction, the information gain of an attribute A is the reduction of the entropy of the class C gained by knowing the value of A:\nIG(A,C) = H(C)\u2212H(C|A)\nThe attribute with the highest information gain is used as splitting attribute.\n23"}, {"heading": "3.2.2 Classification: Support Vector Machines", "text": "The Support Vector Machines (SVMs) (Boser et al., 1992) is one of the most studied and reliable family of learning algorithms. An SVM is a binary classifier that uses a representation of the instances as points in a m-dimensional space, where m is the number of attributes. Assuming that the instances of the two classes are linearly separable1, the goal of SVMs is to find an hyperplane that separates the classes with the maximum margin. The task of finding the best hyperplane that separates the classes is defined as an optimization problem. SVMs can also be formulated to have \u201csoft margins\u201d i.e. allowing some points of a class to lay in the opposite side of the hyperplane in order to find a better solution. The SVM algorithm we use regularizes the model through a single parameter C.\nThe SVMs can also be used with non-linear (i.e. non linearly separable) data using the so called kernel method. A kernel function maps the points from the input space into an highdimentional space where they might be linearly separable. A popular kernel function is the (Gaussian) Radial Basis Function (RBF) which maps the input space into an infinitedimensional space. Its popularity is partially due to the simplicity of its model which involves only one parameter \u03b3.\nEven though SVMs are defined as binary classifiers, they can be extended to a multiclass scenario by e.g. training multiple binary classifiers using a one-vs-all or a one-vs-one classification strategy (Duan and Keerthi, 2005).\nThe Weka toolkit contains an implementation of SVMs that uses the Sequential Minimal Optimization (SMO) algorithm (Platt et al., 1999). In all our experiments we use the SMO algorithm with an RBF kernel."}, {"heading": "3.2.3 Optimization: Coordinate ascent", "text": "The parameter tuning of all our experiments is carried out automatically through a simple coordinate ascent2 optimization algorithm. Coordinate ascent is based on the idea of maximizing a multivariable function f(X) along one direction at a time, as apposed to e.g. gradient descent which follows the direction given by the gradient of the function. Our implementation detects the ascent direction by lookup of the function value. The Algorithm 1 contains a procedure to maximize a function f along the direction k while Algorithm 2 performs the coordinate ascent. The step-size values decay at a rate given by the coefficient \u03b1. The minimum step-sizes determine the stopping conditions for the maximize function, instead the coordinateAscent algorithm stops as soon as the found values do not change between two iterations therefore the maximum is found. The latter algorithm can be easily modified to account for a stopping condition given by a maximum number of iterations.\n1An hyperplane can be drawn in the space such that the instances of one class are all in one side of the hyperplane and the instances of the other class are in the other side.\n2Also known as coordinate descent which is the minimization counterpart (distinguished only by changing the sign of the function).\n24\nWe implemented this algorithm ourselves because, for technical reasons, it was easier than rely on a third-party API. Its simplicity is one of its advantages but it is more prone to be stuck on local maximums than more sophisticated techniques such as gradient ascent.\nFor all our experiments we use the described algorithm to find the parameters that yield the maximum accuracy of the classifiers (using 10-fold cross-validation). For the SMO algorithm we optimize the parameters C and \u03b3, whereas for the J48 algorithm we optimize the parameters C (confidence threshold for pruning) and M (minimum number of instances per leaf).\nAlgorithm 1: maximize(f, k,X, \u03b4k,mk)\nInput: Function f to be maximized; index k of the parameter to maximize; vector X of the current parameter values; initial step-size value \u03b4k for the k-th parameter; minimum step-size value mk for the k-th parameter Output: The value of the k-th parameter that maximizes the function f along the corresponding direction 1 ymax \u2190 f(X); 2 while |\u03b4k| \u2265 mk do 3 X\u0302\u2190 X; 4 X\u0302[k]\u2190 \u03b4k \u00d7 X\u0302[k]; 5 y\u0302 \u2190 f(X\u0302); 6 if y\u0302 > ymax then 7 ymax \u2190 y\u0302; 8 X[k]\u2190 X\u0302[k]; 9 else\n10 \u03b4k \u2190 \u2212\u03b4k; 11 end 12 \u03b4k \u2190 \u03b1\u00d7 \u03b4k; 13 end 14 return X[k];\nAlgorithm 2: coordinateAscent(f, n,X,\u2206,m)\nInput: Function f to be maximized; number n of parameters of the function; vector X of initial parameter values; vector \u2206 of initial step-size values; vector m of minimum step-sizes; Output: The vector X that maximizes the function f 1 initialize Xlast to random values (different from X); 2 while X 6= Xlast do 3 Xlast \u2190 X; 4 for k \u2208 1 . . . n do 5 \u03b4k \u2190\u2206[k]; 6 mk \u2190m[k]; 7 X[k]\u2190 maximize(f , k,X, \u03b4k,mk); 8 end\n9 end 10 return X;\n25"}, {"heading": "3.3 The baseline feature set", "text": "Our baseline is set to be the replicated approach of the classification experiments carried out by Ferna\u0301ndez et al. (2007), which is our reference work for our study. It contains two experiments, one with a restricted set of classes (leaving out Acknowledgments and Check Questions) and a second taking into account all classes. We are interested in the latter although the former is useful to understand the problem and to analyze the results of our classifier. The aforementioned paper also contains an analysis of the results and the feature contribution which proved useful in the replication of the experiments. For our baseline we use only the features they describe. The feature set is composed of 9 features exploiting a series of syntactic and lexical properties of the NSUs and their antecedents. The features can be categorized as: NSU features, Antecedent features, Similarity features. Table 3.2 contains an overview of the feature set."}, {"heading": "NSU features", "text": "Different NSU classes are often distinguished by their form. The following is a group of features exploiting their syntactic and lexical properties.\nnsu cont Denotes the \u201ccontent\u201d of the NSU i.e. whether it is a question or a proposition. This is useful to distinguish between question denoting classes, such as Clarification Ellipsis and Sluices, and the rest.\nwh nsu Denotes whether the NSU contains a wh-word, namely: what, which, who, where, when, how. This can help for instance to distinguish instances of Sluices and Clarification Ellipsis knowing that the former are wh-questions while the latter are not.\naff neg Denotes the presence of a yes-word, a no-word or an ack-word in the NSU. Yes-words are for instance: yes, yep, aye; no-words are for instance: no, not, nay ; ack-words are: right, aha, mhm. This is particularly needed to distinguish between Affirmative Answers, Rejections and Acknowledgments.\nlex Indicates the presence of lexical items at the beginning of the NSU. This feature is intended to indicate the presence of modifiers. A modal adverb (e.g. absolutely, clearly, probably) at the beginning of the utterance usually denotes a Propositional Modifier. The same applies for Factual Modifiers, which are usually denoted by factual adjectives (e.g. good, amazing, terrible, brilliant) and Conjuncts which are usually denoted by conjunctions. Bare Modifier Phrases are a wider class of NSUs which do not have a precise lexical conformation but they are usually started by lexical patterns containing a Prepositional Phrase (PP) or an Adverbial Phrase (AdvP).\n26"}, {"heading": "Antecedent features", "text": "As for the NSUs, antecedents also show different syntactic and lexical properties that can be used as features for the classification task. This is a group of features exploiting those properties.\nant mood As defined by Rodr\u0301\u0131guez and Schlangen (2004), this feature was though to distinguish between declarative and non-declarative antecedent sentences. This feature is useful to indicate the presence of an answer NSU, if the antecedent is a question, or a modifier, if the antecedent is not a question.\nwh ant As the corresponding NSU feature, this indicates the presence of a wh-word in the antecedent. Usually Short Answers are answers to wh-questions while Affirmative Answers and Rejections are are answers to polar questions i.e. yes/no-questions without a wh-interrogative.\nfinished This feature encodes a truncated antecedent sentence as well as the presence of uncertainties at the end of it. Truncated sentences lack a closing full stop, question mark or exclamation mark. Uncertainties are given by the presence of pauses or unclear words or else a last word being \u201cnon-closing\u201d, e.g. conjunctions or articles.\n27"}, {"heading": "Similarity features", "text": "As discussed in Section 2.1, some classes show some kind of parallelism between the NSU and its antecedent. The parallelism of certain classes can be partially captured by similarity measures. The following is a group of features encoding the similarity at the word and POS-level between the NSUs and their antecedents.\nrepeat This feature counts the content words that the NSU and the antecedent have in common (a maximum value of 3 is taken as a simplification). A value greater than 0 is usually a sign of Repeated Acknowledgment or Repeated Affirmative Answers.\nparallel This feature encodes whether there is a common sequence of POS tags between the NSU and the antecedent and denotes its length. This feature can help classify Repeated Acknowledgments, Repeated Affirmative Answers and Helpful Rejections."}, {"heading": "3.4 Feature engineering", "text": "The first and most straightforward method we use to address the classification problem is to find more features to describe the NSU instances. We present here the combination of features that we employ as our final approach. The extended feature set is composed of all the baseline features plus 23 new linguistic features, summing up to a total of 32 features. Our features can be clustered into five groups: POS-level features, Phrase-level features, Dependency features, Turn-taking features and Similarity features. Table 3.3 shows an overview of the additional features we use in the extended feature set."}, {"heading": "POS-level features", "text": "Shallow syntactic properties of the NSUs that make use of the pieces of information already present in the BNC such as POS tags and other markers.\npos {1,2,3,4} A feature for each one of the first four POS-tags in the NSU. If an NSU is shorter than four words the value None is assigned to each missing POS tag. Many NSU classes share (shallow) syntactic patterns among their instances, especially at the beginning of the NSU phrase. Those features aim to capture those patterns in a shallow way through the POS tags.\nending punct A feature to encode the final punctuation mark of the antecedent if any.\nhas pause Marks the presence of a pause in the antecedent.\nhas unclear Marks the presence of an unclear passage in the antecedent.\n28\n29"}, {"heading": "Phrase-level features", "text": "Occurrence of certain syntactic structures in the NSU and the antecedent. These features were extracted through the use of the Stanford PCFG parser (Klein and Manning, 2003) on the utterances. Refer to Marcus et al. (1993) for more information about the tag set used for the English grammar.\nant {sq,sbarq,sinv} Those features indicate the presence of the syntactic tags SQ, SBARQ and SINV in the antecedent. Those tags indicate a question formulated in various ways even when there is no explicit question mark at the end. Useful to recognize e.g. Short Answers.\nnsu first clause Marks the first clause-level tag (S, SQ, SBAR, . . . ) in the NSU.\nnsu first phrase Marks the first phrase-level tag (NP, VP, ADJP, . . . ) in the NSU.\nnsu first word Marks the first word-level tag (NN, RB, UH, . . . ) in the NSU.\nneg correct Presence of a negation word (no, nope, . . . ), followed by a comma and a correction. For instance:\n(3.1) a: Or, or were they different in your childhood?\nb: No, always the same.\n[BNC: HDH 158\u2013159]\nThis pattern is useful to describe some of the Helpful Rejections such as (3.1)."}, {"heading": "Dependency features", "text": "Presence of certain dependency patterns in the antecedent. These features were extracted through the use of the Stanford Dependency Parser (Chen and Manning, 2014) on the utterances. For more details about the dependency relations tag set please refer to De Marneffe et al. (2014).\nant neg Signals the presence of a neg dependency relation in the antecedent. The neg dependency arises from an adverbial negation in the sentence (not, don\u2019t, never, . . . ). This feature helps to capture situations such as the following:\n(3.2) a: You\u2019re not getting any funny fits from that at all, June?\nb: Er no.\n[BNC: H4P 36\u201337]\n30\nSince the question in the antecedent is negative, the NSU in (3.2) is actually an Affirmative Answer, even though it contains a negative word. This feature, in combination with the aff neg feature, addresses this situation.\nwh inter Whether the antecedent contains a wh-interrogative fragment such as the one in the following example:\n(3.3) a: And you know what the voltage is\nb: Yeah, two forty.\n[BNC: GYR 174\u2013175]\nThe feature looks for a dobj dependency with a wh-word then for an nsubj dependency with the dependent element of the previous dependency, for instance in (3.3) we have dobj(is-7, what-4) and nsubj(is-7, voltage-6). This features tries to mitigate the absence of a question as antecedent for Short Answers such as (3.3)."}, {"heading": "Turn-taking features", "text": "Features indicating certain patterns in the turn-taking of the dialogue.\nsame who Denotes whether the NSU and the antecedent were uttered by the same speaker. Sometimes dialogues do not provide the speaker information so an additional value unk is added for this cases. This feature is particularly important to capture Check Questions which are almost always uttered by the same speaker."}, {"heading": "Similarity features", "text": "Additional numeric features and similarity measures between the NSU and its antecedent.\nrepeat last This measures the number of words in common between the NSU and the last portion of the antecedent. Often happens that Repeated Affirmative Answers and Repeated Acknowledgments contain the last words in the antecedent.\nabs len The total number of words in the NSU.\ncont len The number of content-words in the NSU.\nlocal all A feature that denotes the local alignment at the character-level between the NSU and the antecedent, computed using the Smith\u2013Waterman algorithm (Smith and Waterman, 1981).\n31\nlcs A feature to express the longest common subsequence at the word-level between the NSU and its antecedent, computed using a modified version of the Needleman\u2013Wunsch algorithm (Needleman and Wunsch, 1970), tailored to account for words instead of characters.\nlcs pos The longest common subsequence at the POS-level between the NSU and its antecedent, computed with the same algorithm of above but using the list of POS tags instead of the list of words."}, {"heading": "3.5 Semi-Supervised Learning", "text": "The scarcity of labeled data is probably the major problem to face in this classification task. Even though the quality of the data is good enough, it is still difficult for a classifier to learn patterns out of 20 instances or less for some classes (see Table 3.1). However, a large amount of unlabeled data is available in the BNC. There are many classification tasks, such as ours, in which it is hard or costly to label a large amount of instances while instead it is relatively cheap to extract unlabeled ones. The empirical question is whether the use of unlabeled data is useful to improve the classification performances. Semi-Supervised Learning techniques deal with this issue. They exploit the combination of a small amount of labeled data and a large amount of unlabeled data to try improve the classification accuracy. Even though it is still a young research field, semi-supervised learning has already found many fields of application (Liang, 2005; Bergsma, 2010)."}, {"heading": "3.5.1 Unlabeled data extraction", "text": "With the use of some heuristics it is possible to extract NSU instances of good quality from the BNC. We use a set of rules to determine whether an utterance in a dialogue transcript of the BNC is a probable NSU. The following is a list of such rules:\n\u2022 The number of words in the NSU must be less than a given threshold;\n\u2022 The number of characters in the NSU must be higher than a given threshold;\n\u2022 The NSU must not contain only pauses, unclear passages and punctuation;\n\u2022 The NSU must not contain a greeting (e.g. hi, hello, good night);\n\u2022 The NSU must not contain a verb in any form.\nAn accuracy test was run over the corpus of NSUs: of the 1 123 NSUs examined, 1033 where detected correctly by this set of rules, for an accuracy of 0.92. The main flaws of the rules were mostly overlong NSUs, such as (3.4), and presence of verbs, such as (3.5).\n32\n(3.4) a: Was it a coal fire?\nb: Coal fire and er scrubbed the cabin out like that, soda water and\nsoft soap.3\n[BNC: H5G 151\u2013152]\n(3.5) a: [. . . ] the resistance the same the current goes up.\nb: Current goes up.4\n[BNC: GYR 112\u2013113]\nThe detection of NSUs using the rules above is not the only problem to face. Perhaps more challenging is the selection of an antecedent for the NSU. As pointed out in Section 2.1, the antecedent of an NSU is not always the preceding utterance. Nevertheless, as proved in the corpus study of Ferna\u0301ndez (2006), the percentage of the utterances whose antecedent is not the preceding utterance is rather low. Another result of the aforementioned work is that the case in which the antecedent is an utterance at distance greater than one is far more probable in a multi-party dialogue context. In light of the above considerations, we restrict the instances we extract to only those from two-party dialogues and we always consider the preceding utterance as the antecedent of an NSU. While there has been some previous work towards using machine learning techniques for the detection of the antecedent of NSUs in multi-party dialogue (Schlangen, 2005), we consider sufficient the amount of unlabeled data we can extract following the previous rule.\nIn order to maximize the quality of the unlabeled data that we extract we also enforce some rules over the antecedent utterance:\n\u2022 The number of words in the antecedent must be greater than the number of words in the NSU;\n\u2022 The antecedent must have a complete clausal form i.e. at least a verb phrase and a noun phrase.\nUsing the whole set of heuristics we extracted in total 3 198 new unlabeled NSU instances from the BNC (checked not to be already in the corpus)."}, {"heading": "3.5.2 Semi-supervised learning techniques", "text": "As previously mentioned, semi-supervised learning techniques are used when labeled data is scarce and unlabeled data is abundant. Every techniques tries to integrate the information yield by the unlabeled instances inside a learning model based on the available labeled data. In this section we give a brief and high-level description of the semi-supervised learning techniques that we have employed, namely: Self Training, Transductive SVM and Active Learning.\n3A Repeated Affirmative Answer, but the additional content after the conjunction makes the NSU much longer. It is still a valid NSU since it does not have a full clausal structure.\n4The NSU is a Repeated Acknowledgment. Repeating the words in the antecedent, it introduces a verb. It is still considered an NSU according to the definition of Ferna\u0301ndez (2006).\n33"}, {"heading": "Self Training", "text": "The simplest way to exploit unlabeled data is to automatically predict some unlabeled instances through a classifier built from the available labeled data then add them to the training data for the next step. This is an iterative process, at each step one or more newly labeled instances are added to the training set then the classifier is retrained and more unlabeled instances are predicted.\nVarious strategies can be used at each step:\n\u2022 Add one or a few (random) instances at the time;\n\u2022 Add a few most confident instances;\n\u2022 Add all the first time, correct the wrong predictions the next times.\nThe last strategy as well as other variants can be cast as an Expectation-Maximization problem, especially when using a probabilistic learning model."}, {"heading": "Transductive SVM", "text": "As already described in Section 3.2.2, Support Vector Machines are one of the most studied and reliable family of classification algorithms. Transductive SVM (TSVM) is a variant of the standard SVM algorithm which exploits unlabeled data to help adjust the SVM model. The basic assumption under TSVM is that unlabeled instances from different classes are separated with large margin. Therefore, similarly to the standard SVM, TSVM tries to find the hyperplane that maximizes the unlabeled data margin i.e. considering unlabeled points as labeled ones. To decide whether an unlabeled point should be considered of one class or the other, clustering techniques are used e.g. k-nearest neighbors (the class of the majority of the neighbors or some other variant).\nWe will not go into mathematical details so we recommend the interested reader to Vapnik (1998), Collobert et al. (2006)."}, {"heading": "Active Learning", "text": "Annotating data is often a very expensive procedure, mostly because one needs to annotate a lot of instances in order to be able to reliably classify unseen ones. An idea to ease this problem is to let the learning algorithm choose which instance could be the most informative (i.e. the most difficult to predict) then annotate it manually. This technique has the advantage of reducing the cost of manual annotation of the instances by making informed guesses over the instances to label and discarding the redundant ones.\nThis kind of techniques is typically employed to cope with the scarcity of labeled data. In our case, the lack of sufficient training data is especially problematic due to the strong class imbalance between the NSU classes.\n34\nThe Active Learning (AL) scheme, which is a special case of semi-supervised learning, trains the model over the available labeled data then queries the user for the label of one (or few more) instances then retrain the model and so on until convergence criteria are met, e.g. the wanted number of new instances is reached.\nThere can be different query strategies, some of them are:\n\u2022 Uncertainty Sampling: queries the least confident instance (according to the probability of the prediction). A variant of that uses entropy to determine the most\ninformative instance.\n\u2022 Query-by-committee: uses many different classifiers to predict unlabeled data then formulates the most informative query as the instance about which they most dis-\nagree.\n\u2022 Expected Model Change: selects the instance that would impart the greatest change to the model, according to a decision-theoretic approach.\n\u2022 Expected Error Reduction: Another decision-theoretic approach that aims to minimize the risk, that is the expected future error. The instances are selected on the\nbasis of how much the model generalization error is likely to be reduced. A variant of this approach considers only the output variance of the model.\nThe particular active learning algorithm we employed in our experiments is a pool-based method5 with uncertainty sampling (Lewis and Catlett, 1994). The sampling relies on entropy as measure of uncertainty. Given a particular (unlabeled) instance with a vector of feature values f , we use the existing classifier to predict the class C of the instance, and derive the probability distribution P (C = ci|f) for each possible output class ci. We can then determine the corresponding entropy of the class C:\nH(C) = \u2212 \u2211 i P (C=ci|f) logP (C=ci|f)\nAs seen in section 3.2.1, entropy indicates the \u201cunpredictability\u201d of a random variable and also how much information it carries. The higher the entropy of the class of an instance the more information we gain by knowing it. The algorithm we employ (Algorithm 3) selects the instances with highest entropy as the most informative ones. As argued in Settles (2010), entropy sampling is especially useful when there are more than two classes, as in our setting. In practice, we applied the JCLAL active learning library6 to extract and annotate 100 new instances of NSUs, which were subsequently added to the existing training data.\n5That involves drawing labeled instances from a \u201cpool\u201d that remains the same over the iterations, as opposed of stream-based ones in which sampling is done over a stream of data.\n6cf. https://sourceforge.net/projects/jclal.\n35\nAlgorithm 3: entropySampling(\u0393,U, k)\nInput: The classifier \u0393; the unlabeled data U; the sample size k. Output: The k instances with highest entropy. 1 H\u2190 vector of the same size of U; 2 for i \u2208 1..|U| do 3 u\u2190 U[i]; 4 Pu \u2190 classProbDist(\u0393, u); 5 Hu \u2190 \u2212 \u2211 p\u2208Pu p log p; 6 H[i]\u2190 Hu; 7 end 8 U\u2190 sort(U,H); // Sort U according to H (descending) 9 return firstK(U, k);"}, {"heading": "3.6 Evaluation", "text": "In this section we discuss the evaluation of our experiments and their empirical results. We first discuss the evaluation metrics for the classification task we employed, then we present the evaluation results on each setting."}, {"heading": "3.6.1 Metrics", "text": "Given the dataset with a total of N instances, the metrics are based on the amount of true positives (TP ), true negatives (TN), false positive (FP ) and false negatives (FN)."}, {"heading": "Accuracy", "text": "The ratio of the correctly classified instances over the total\nAcc =\n\u2211 c\u2208C TPc + TNc\nN\nwhere C is the set of the classes and TPc and TNc are respectively the true positives and the true negatives of the class c \u2208 C."}, {"heading": "Precision", "text": "The ratio between the true positives and the total instances classified as positives. In a context with multiple classes (more than two) such as ours, the precision must be calculated per class, where the positive instances are the ones classified with the current class whereas the negative instances are the ones classified otherwise. The per class precision is calculated as follows:\nPrecc = TPc\nTPc + FPc\nTo have a summary value for all the classes we can compute the weighted average precision:\nPrecavg =\n\u2211 c\u2208C Nc \u00b7 Precc\nN\n36"}, {"heading": "Recall", "text": "The recall is the ratio between the true positives and the total instances that are actually positives. As for the precision, we can calculate the per class recall:\nRecc = TPc\nTPc + FNc\nAnd the weighted average recall:\nRecavg =\n\u2211 c\u2208C Nc \u00b7Recc\nN\nF1-score\nThe F1-score is the harmonic mean of precision and recall. As for the other two measures, we compute the per class F1-score:\nF1,c = 2 \u00b7 Precc \u00b7Recc Precc +Recc\n= 2 \u00b7 TPc\n2 \u00b7 TPc + FPc + FNc\nThen the weighted average F1-score:\nF1,avg =\n\u2211 c\u2208C Nc F1,c\nN\nStudent\u2019s t-test\nEmpirical results alone can not assess whether a classifier performs better than another. To assess that the performances of one classifier being higher than a second one is not due to the randomness associated with the data manipulation but to a statistically significant difference between the classifiers one needs to prove with high confidence that the null hypothesis is false. The null hypothesis is a statement that is assumed to be true until evidence indicates otherwise. When comparing two learning systems, the null hypothesis states that there is no difference between the performances of the two learning systems. To prove that a classifier performs better than another we need to disprove the null hypothesis with a high degree of confidence. For this purpose we employ a Student\u2019s ttest, a widespread method to compare two sets of data. The t-test can be used to find the probability p of the performance values of the two classifiers being drawn from the same mean.\nTo run the t-test, we compare the differences \u03b4i among the performance values of the two classifiers over the n independent samples. We first compute the mean of the differences:\n\u03b4\u0304 = 1\nn n\u2211 i=1 \u03b4i\n37\nThen we compute the t-statistic:\nt = \u03b4\u0304\u221a\n1 n(n\u22121) \u2211n i=1(\u03b4i \u2212 \u03b4\u0304)2\nFrom the t-statistic we can derive the p-value from a Student\u2019s t-distribution with n \u2212 1 degree of freedom. A small p-value means that it is unlikely that the samples show such a t-statistic by chance therefore we can assess that the difference in performance between the two classifiers is statistically significant.\nIn our case we use a paired t-test on the accuracy values of the 10-fold cross-validation over the dataset (thus n = 10). By convention, an acceptable p-value is p \u2264 0.05. For our experiments we rely on the t.test function from the R project, a framework for statistical computing (R Core Team, 2015)."}, {"heading": "3.6.2 Empirical results", "text": ""}, {"heading": "Baseline", "text": "As in Ferna\u0301ndez et al. (2007), we evaluate our system in a 10-fold cross-validation fashion. Weka\u2019s J48 algorithm was used as a comparing classifier. Thanks to the analysis of the resulting trees, we managed to imitate quite closely the behavior of their system as well as reaching a very close performance overall. Even though we use the same feature set and the same algorithm the performance parameters turn out to be slightly lower than the ones claimed in Ferna\u0301ndez et al. (2007). That might be for a variety of reasons, for instance the way feature were extracted or how the parameters were tuned. Nevertheless the overall performance is matched as well as many of the patterns in the scores. Table 3.4 shows the comparison between the performance parameters of the reference classification (Ferna\u0301ndez et al., 2007) and the values of the same parameters achieved by our implementation."}, {"heading": "Self-training and TSVM", "text": "Both those two techniques did not perform particularly well, sometimes even worsening the classification accuracy. Self-training was implemented and tested in many variants but none were successful. One possible explanation is that the labeled data added at each step to the training data is always biased by the labeled data available in the initial training set. This may lead to adding redundant data that is not actually useful to improve the classification performances. On the other hand, TSVM has been unsuccessful mostly due to computational performances of the implementation and other technical difficulties. It was impracticable to run it on a large amount of unlabeled data so we managed to test it only on few unlabeled instances and therefore no improvement was shown.\n38"}, {"heading": "Active Learning", "text": "Our active learning experiment was carried out using the JCLAL library. For the active learning process we divided the dataset into three parts: training set (50%), development set (25%) and test set (25%). At each iteration, the JCLAL library builds a classifier on the training set and evaluates it over the development set. The same classifier is then used to select an instance from the unlabeled data, as described in Section 3.5.2. The user is then asked to annotate the selected instance. The process iterates in this manner until the stopping criteria is met, that is when the goal of 100 newly annotated instances is reached. Table 3.5 shows the distribution of the instances annotated with Active Learning. From Table 3.5 we can see that the AL algorithm using the entropy measure prefers the instances that belongs to the classes that are most difficult to classify and, in particular, the ones that are ambiguous, such as Clarification Ellipsis and Sluices. This process has been performed once with the extended feature set and the SMO classifier. Secondly, it has been simulated (i.e. using the data obtained in the previous run) using the baseline feature set instead. The Figures 3.1, 3.2, 3.3, 3.4 show the learning curves7, respectively for the accuracy, precision, recall and F1-score, of both the extended feature set and the baseline feature set.8 All the performance measures are clearly improving as new instances become available, for both the extended feature set and the baseline one.\n7The graph showing how the performances change as the new labeled data extracted with Active Learning are inserted in the training set.\n8Notice that the images are scaled on the y-axis to make the change visible.\n39"}, {"heading": "NSU Class Instances", "text": "In the end, the test set has been used to evaluate the overall performances of the various settings. Table 3.6 and Table 3.7 show the results of the experiments respectively over the development set and the test set. The results on the test set show that the inclusion of the active learning data is only beneficial when combined with the extended feature set.\nWe also performed an evaluation of the various settings using 10-fold cross-validation over the full dataset. The evaluation results based on the active learning procedure (AL) refer to the performance of the system after the inclusion of all newly annotated instances. The novel data was added to the training set of each fold.\nWe compare the results of the various settings using the J48 algorithm (Table 3.8) and SMO algorithm (Table 3.9). The use of active learning was successful and, in the end, the use of the SMO classifier with the extended feature set and the inclusion of the AL instances constitutes our final approach.\nThe results show a significant improvement of the classification performance between the baseline and the final approach. Using a paired t-test with a 95% confidence interval between the baseline and the final results (as detailed in Section 3.6.1), the improvement in classification accuracy is statistically significant with a p-value of 6.9\u00d7 10\u22123.\nThe SVM algorithm does not perform particularly well with the baseline feature set but scales better than the J48 classifier after the inclusion of the additional features. Overall, the results demonstrate that the classification can be improved using a modest amount of additional training data combined with an extended feature set. However, we can observe from Table 3.10 that some NSU classes remain difficult to classify even with the insertion of additional training data. For instance, Helpful Rejections are still the most difficult classes to classify, even with the addition of 21 new instances. One of the problems with\n40\nHelpful Rejections is that they are connected to their antecedents mainly at the semantic level. Consider the following example of Helpful Rejection that is hard to classify:\n(3.6) a: There was one which you said Ernest Morris was born in 1950.\nb: Fifteen. [BNC: J9A 372\u2013373]\nIt is clear that, for the Helpful Rejection in (3.6), morpho-syntactic and lexical features, such as the ones we employ, are of little use in classifying this utterance. Most of the connection is at the semantic level therefore we would need to use features that exploit semantic patterns. At the same time, the use of this type of features would add several layers of complexity at the feature extraction process. Other examples of difficult classes are the Repeated Affirmative Answers and Repeated Acknowledgments. They are highly ambiguous because they can be misclassified between each other, with their respective non-repeated classes and sometimes with other NSU classes. An example of ambiguous Repeated Acknowledgment can be the following:\n(3.7) a: Selected period.\nb: Selected period, right, Andrew?9\n[BNC: JK8 114\u2013115]\nThe instance in (3.7) contains also a question therefore it is often misclassified with other question denoting NSU classes. It is clear that handling these type of NSU requires to perform a deeper semantic analysis of the connection with their antecedents then design appropriate semantic features. The extraction of additional labeled data is also especially important for both the feature engineering and the learning process of the classifiers. This two approaches may be the starting points of any future work on this task.\n9In the dialogue, the speaker B is asking the same question to many people in turns.\n41\n42\n43\n44"}, {"heading": "3.7 Summary", "text": "This chapter presented the task of classifying non-sentential utterances and our approach to address this problem. This task is formulated as a machine learning problem and we follow and extend the work of Ferna\u0301ndez et al. (2007). We use their corpus as a goldstandard and a replica of their approach as a baseline. The data, the machine learning algorithm used and the feature set of the baseline were discussed respectively in Section 3.1, 3.2, 3.3. The two main problems we faced in our work have been the scarcity of the labeled data and the imbalance in the distribution of the classes. To address these problems we extended the baseline approach in two ways: using a larger feature set (detailed in Section 3.4) and employing semi-supervised learning techniques to exploit the abundance of unlabeled data. We described in Section 3.5 the semi-supervised learning techniques that we employed, namely: Self Training, Transductive SVM and Active Learning. Section 3.6 shows the empirical results we got from our experiments. While the extended feature set alone did not make an improvement on the performances of the classifiers, its use in combination with Active Learning made a modest but significant difference.\n45\nChapter 4"}, {"heading": "Resolution of Non-Sentential", "text": ""}, {"heading": "Utterances", "text": "As introduced in Chapter 2, the resolution of an NSU is the task of reconstructing its meaning from the dialogue context. Ferna\u0301ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous.\nFor this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Ferna\u0301ndez (2006). We reinterpreted the variables in the dialogue state as random variables and straightforwardly \u201cconverted\u201d the resolution rules into probabilistic rules.\nIn the next sections we explain how we represented the variables in the dialogue state and how we translated the rules from Ferna\u0301ndez (2006) into probabilistic rules. First we describe the theoretical aspects from Ginzburg (2012) we employ in our approach. We present then the design of our dialogue context and the rules to resolve the NSUs.\nWe surely take a much simpler approach than Ginzburg (2012) in the modeling of the dialogue state, abstracting intentionally from many details that would add complexity to the modeling. Indeed there are a number of issues that arise in the resolution of NSUs\n47\nthat need to be treated with proper lexical and semantic resources that we did not include. However, in the end our goal for this work is not to formulate a complete theory of NSU resolution but rather to provide a proof-of-concept implementation for the resolution of the NSUs in the dialogue context with the probabilistic rules formalism.\nThis framework has also been implemented and tested with the OpenDial toolkit. We developed a dialogue system able to update the dialogue state probabilistically with update rules similar to the ones from Ginzburg (2012). The system is also able to resolve toy examples in an interactive way. A detailed example of the behavior of the system is given in Section 4.5. More high-level details about the rules for the state update (complementary to the rules for the NSU resolution) can be found in the Appendix A1."}, {"heading": "4.1 The resolution task", "text": "The resolution of an NSU is the task of extracting its meaning from the dialogue context. More precisely, let ua and nsua represent respectively the word word sequence making up the NSU and its type according to the taxonomy presented in Section 2.1.1. We also assume MaxQUD to be a high-level semantic representation of the antecedent, as mentioned in Section 2.2.2. Through a resolution procedure, we want to extract aa i.e. the high-level semantic representation of the NSU. The right resolution procedure is selected on the basis of the type of the NSU. In our case the value of nsua is retrieved using the classifier developed in Chapter 3 which takes as input the raw NSU and the antecedent. Figure 4.1 shows a schema of the task just defined. Indeed this is the simplest way to define the task. The resolution procedure may also be dependent of other variables in the dialogue state such as the Facts. In principle, the resolution task is defined independently from the actual semantic representation of the utterances. It is also defined independently from the rules used to update the variables in the dialogue state such as QUD and Facts. In practice, define a set of rules that are generic enough to handle every possible case and behave independently from the state update rules is a difficult task and still an open research problem.\n1For more technical details about the implementation and examples of interaction visit: https://github.com/paolodragone\n48"}, {"heading": "4.2 Theoretical foundation", "text": "As previously stated, we rely on Ferna\u0301ndez (2006) and Ginzburg (2012) for the theoretical notions needed to represent the dialogue state and to develop the NSU resolution rules. In Section 2.2 we detailed the basic concepts of TTR, the utterance representation and the update rules for the dialogue state. In this section we describe the notions needed for the resolution of the NSUs. In particular we describe how we can exploit the parallelism between the NSU and its antecedent that we mentioned in Section 2.1. We discuss here the concepts that Ginzburg (2012) defines to address the resolution of NSUs then we will describe how we adapt those concepts to our needs in the next section."}, {"heading": "4.2.1 Partial Parallelism", "text": "Instances of NSU classes such as Acknowledgment and Affirmative Answers are related to their antecedent as a whole, that is to understand their meaning one as to consider not a specific aspect of the antecedent but the entire sentence. On the other hand, there are NSU classes, such as Short Answers and Sluices, that show a more fine grained parallelism between their instances and their antecedents i.e. they may refer in particular to certain aspects of the antecedent. In the theory of Ginzburg (2012), this concept is named Partial Parallelism2. Partial Parallelism is one way to categorize NSU classes according to the relation with their antecedents. NSU classes are categorized as +/-ParPar in order to find the right way to treat them. An NSU class categorized as +ParPar involves the access to one or more sub-utterances from its antecedent. On the contrary, -ParPar NSU classes do not need to know the internal structure of their antecedents to be resolved. Table 4.1 shows how NSU classes are categorized in this way.\n2Ferna\u0301ndez (2006) previously addressed this concept as Sentential Antecedent (SA).\n49"}, {"heading": "4.2.2 Propositional lexemes", "text": "-ParPar NSU classes are realized (mainly) by propositional lexemes, i.e. words that can stand alone and form a proposition with full contextual meaning. Among those classes there are the Plain Affirmative Answers, Plain Rejections and Propositional Modifiers which respectively are realized by the words yes, no and adverbials such as probably and possibly.\nThose classes of NSU arise from polar questions such as (4.1).\n(4.1) a: Will you go to the party on Saturday?\nb: Yes. / No. / Probably.\nThe semantic content of these stand-alone lexemes can be modeled as a function R of the content of the antecedent polar question.\nFor Plain Affirmative Answers, R is the Identity (Id) relation, i.e. the function that returns the argument itself. This means that the positive answer \u201cyes\u201d to a polar question is equivalent to the assertion of a proposition with the same content as the polar question.\nFor Plain Rejections, R is the relation Neg. Neg indicates the negation of a proposition p although it is sensitive to the polarity of p, meaning that, when p is positive, Neg(p) is the negation of p (denoted with p\u0304) whereas, when p is negative, Neg(p) is p itself. This rule is needed to account for the asymmetry in the meaning of negative answers to negative questions. A negative answer to a negative question does not equate a positive one, as exemplified in (4.2) (rephrased from Ginzburg (2012)).\n(4.2) a: Did Paul not leave?\nb: No. (= Paul did not leave.)\nFor Propositional Modifiers, R is a relation PropRel which applies different modalities on the basis of the lexical meaning of the word used as modifier, e.g. \u201cprobably\u201d would have a different modality than \u201cclearly\u201d."}, {"heading": "4.2.3 Focus Establishing Constituents", "text": "To account for the partial parallelism between NSUs and their antecedents stemming out from the instances of the classes of the +ParPar group we need to keep track of the focal sub-utterances of the antecedents i.e. of the elements of QUD. For this reason we employ the notion of focus establishing constituents (FEC) from the theory of Ginzburg (2012)3. The FECs are relevant constituents in the elements of QUD that may be used to resolve NSUs. Consider the following example:\n(4.3) a: A friend is coming to the party.\nb: Who?\n3The concept was previously formalized by Ferna\u0301ndez (2006) as topical constituents.\n50\nThe noun phrase \u201cA friend\u201d in the first sentence of (4.3) is the one which the following Sluice is referring to. Roughly the Sluice can be resolved in such a manner: \u201cWho is your friend that is coming to the party?\u201d. It is clear that the aforementioned sub-utterance has to be contextually available to allow the resolution of the subsequent Sluice. In this we follow Ginzburg (2012), who defines a set of rules to follow to make FECs contextually available. In particular we are interested in the following ones:\n\u2022 The FEC associated with a wh-interrogative is the wh-phrase4 itself:\n(4.4) a: Who is organizing the party?\nb: Paul.\n\u2022 The FEC associated with a polar interrogative or declarative utterance can be any (quantified) noun phrases:\n(4.5) a: A friend is organizing a party and many people are coming.\nb: Who?\n\u2022 The FEC associated with a clarification request is the sub-utterance that has to be clarified i.e. any sub-utterance in the antecedent:\n(4.6) a: Is Paul organizing a party?\nb: Paul? / Organizing? / A party?"}, {"heading": "4.2.4 Understanding and acceptance", "text": "The classes of Plain Acknowledgments and Check Questions are used to handle understanding and acceptance in the conversation. Plain Acknowledgments are used to send a direct feedback of understanding or acceptance of the previous utterances. Understanding involves grasping successfully the content of an utterance while acceptance is a sign of shared belief which therefore updates the Facts with the accepted utterance and removes the corresponding issue from the QUD. As argued in Ferna\u0301ndez (2006), understanding does not always imply acceptance, and Plain Acknowledgments are ambiguous in this distinction. Despite this difference, we assume that Plain Acknowledgments are used to show acceptance, therefore the use of a Plain Acknowledgments also downdates the QUD. On the other hand, understanding is assumed to be shown by any utterance that is not a Clarification Ellipsis.\nCheck Questions are used in conversation to request an explicit feedback about the understanding/acceptance of the previous utterance.\n4As in Ginzburg (2012) we consider only unary wh-interrogatives. Refer to Ferna\u0301ndez (2006) for an account of utterances with multiple wh-interrogatives\n51"}, {"heading": "4.2.5 Sluicing", "text": "Sluices can take a wide range of meanings depending on the particular situation. To formalize the meaning of Sluices, Ferna\u0301ndez et al. (2007) distinguish four types of Sluices that convey different meanings: Direct Sluices, Reprise Sluices, Clarification Sluices, Whanaphor.\nThe aforementioned paper describes a machine learning experiment to automatically classify Sluices according to these types. Ginzburg (2012) describes several different treatments for every group of Sluices.\nIn our work we do not distinguish between those type of Sluices but we confine ourselves for simplicity to direct Sluices only. Direct Sluices, such as the one in (4.7), are used to query the other speaker for additional information about some aspect of the antecedent.\n(4.7) a: Can I have some toast please?\nb: Which sort?\n[BNC: KCH 104\u2013105]"}, {"heading": "4.3 Dialogue context design", "text": "As mentioned before, the dialogue context is represented as a Bayesian network containing a set of random variables representing the current information state. The values of those random variables can represent virtually anything, from the raw utterances to their semantic representation. The variables in the dialogue context are inspired by Ginzburg (2012). In order to make the transition from the rules of Ferna\u0301ndez (2006) to probabilistic rules as direct as possible, we mimic the basic dynamic of the DGB detailed in Section 2.2. For our semantics we do not employ TTR because it would add unnecessary complexity to our formalization. In this section we first describe the semantics we adopt and then we discuss the random variables that compose the dialogue context."}, {"heading": "4.3.1 Semantics", "text": "The semantic content of the utterance is represented by logical predicates, individuals and variables. Predicates are labeled as words or camel-case phrases and can present zero or more arguments. Individuals are labeled with uppercase abbreviations such as IND for generic individuals or E for events. Variables are labeled with an uppercase X. Both individuals and variables are uniquely identified by a numeric subscript.\nPredicates represent the high-level semantic meaning of the constituents of the utterances. Intuitively, predicates without variables as argument can represent propositions such as (4.8). As discussed in Section 2.2.1, polar questions and wh-questions can be seen as functions from/to record types. Polar questions take as argument the empty record type.\n52\nFollowing this schema, in our formalism polar questions are denoted by predicates with no variables, whereas wh-questions are denoted by predicates containing one or more variables, as exemplified by (4.9).\n(4.8) Paul is a friend of yours. friend(addr,Paul)\n(4.9) Is Paul a friend of yours? friend(addr,Paul)\nWho is your friend? friend(addr,X1)\nRetrieving the semantic representation from the raw utterances is a Natural Language Understanding (NLU) task, a completely different task with respect to the resolution of NSUs. We do not attempt to generate predicates from raw utterances instead we make use of simple handcrafted predicates in our examples, abstracting the necessity of NLU to retrieve the meaning of all the utterances that are not NSUs. We try to keep the problem of NSU resolution generic separating it as much as possible from the NLU task."}, {"heading": "4.3.2 Dialogue acts", "text": "As seen in Section 2.2.1, to represent the \u201cpurpose\u201d of an utterance, we need to use an illocutionary relation, also known as dialogue act. The set of dialogue acts we employ in our dialogue context is a small subset of the ones defined by Ginzburg (2012):\n\u2022 Assert, denoting the act of asserting a proposition;\n\u2022 Ask, denoting the act of posing a question;\n\u2022 Ground, denoting the act of understanding what being previously said;\n\u2022 Accept, denoting the act of accepting what being previously said.\nAssertions are applied to propositions and they are implicitly considered truthful unless they violate some predicates in the Facts. Asking a query is the act of posing questions and they are piled up in the QUD until they are resolved by an answer. The act of answering to a query corresponds, in the case of a wh-interrogative, to finding the valid arguments to the variables of the question. In case of a polar question, the answer is derived simply by its truth status, denoted by the presence of the same predicate in the Facts. In our formalization we use \u201cGround\u201d to represent the act understanding. Acceptance is the act of resolving an issue, which involves updating Facts and downdating QUD."}, {"heading": "4.3.3 Variables of the dialogue context", "text": "For our formalization, as in TTR, we assume the availability of various data structures such as variables, lists, sets and complex types. The probabilistic rules formalism provides those structures out of the box as possible values for the random variables. Random variables are denoted with the notation \u201cvar1\u201d. Array element accesses are indicated with the square brackets notation, such as \u201carray[0]\u201d. Sets are denoted with the classical mathematical\n53\nnotation \u201c{e1, . . . , en}\u201d. Complex type accesses are denoted with the dot notation, such as \u201ccomplexVar.var1\u201d. The classical operations on sets are available such as the union and the intersection. Array concatenation is denoted with the + symbol. Now we describe the variables used in our formalization of the dialogue context.\nua, ub, aa, ab\nAs a convention, raw utterances and dialogue acts are indicated respectively with the letters u and a and a subscript denotes the speaker. We record in separate variables only the last utterance and dialogue act of each speaker.\nnsua\nA random variable that contains the distribution over the NSU classes returned by the classifier for the latest recorded utterance. It uses max-qud to refer to the antecedent therefore the probabilistic inference framework takes care of finding the most probable antecedent for the current NSU. Besides the values of the NSU classes, a distinct value NoNsu is used to account for input utterances that are not NSUs. To determine whether an utterance is an NSU or not we used the same detection rules explained in Section 3.5.1.\nnew-fec\nThe set of FECs introduced by the NLU of the last recorded utterance. It is also a buffer variable used in the NSU resolution to encode the focal constituents of the newly resolved NSU. It is used also to hold FECs of the utterance that is being inserted in the qud.\nfacts\nA set of predicates representing the common knowledge of the users. The predicates in facts contain only individuals as arguments (i.e. no variables) and they are implicitly considered truthful.\nqud\nAs defined in Section 2.2, the QUD is a partially ordered set containing the issues currently under discussion. Its ordering determines the \u201cpriority\u201d of the issues to be resolved. Here instead qud is represented as a vector and the max-qud variable denotes the index of the MaxQUD element (see below). Each element in qud has a number of sub-fields:\n\u2022 utt: The raw utterance associated to the current question under discussion;\n\u2022 q: The semantic representation of the utterance;\n\u2022 fec: An array of topical sub-utterances used in the resolution of the NSUs.\nThe qud is incremented by adding elements in the tail (growing numbers) and decremented in a random-access fashion, usually by removing the MaxQUD element (which could not be the last element) after its resolution. We denote with qudsize the size of qud.\n54\nmax-qud\nDespite being represented as the maximal element of QUD in Ginzburg (2012), here max-qud actually denotes the index of such element which therefore is retrieved in this way: qud[max-qud]. In Ginzburg (2012), MaxQUD is given from the partial ordering imposed on QUD. This ordering is often similar but not limited to the behavior of a stack. At our disposal we have the full power of probabilistic modeling which enables us to encode max-qud as a random variable with a prior that gives more probability to the highest element in qud. The function used to give this prior to max-qud is\nP (max-qud= i) = ei\u2212qudsize\nwhere i < qudsize is an index in qud. In this way, the prior most probable MaxQUD is the last element inserted in the QUD but the probability can be modified by other contextual elements by probabilistic inference on the dialogue state."}, {"heading": "4.4 NSU resolution rules", "text": "Here we present the probabilistic rules that handle the resolution of NSUs. For each rule we also present an example of usage. Since they are a (almost) direct translation of the deterministic rules from Ferna\u0301ndez (2006), most of them have deterministic effects (i.e. a single effect with probability 1). Nonetheless the updates are handled probabilistically by the probabilistic rules framework through probabilistic inference over the Bayesian network representing the dialogue state. We show an example of probabilistic update in Section 4.4.1, which is valid for every other resolution rule."}, {"heading": "4.4.1 Acknowledgments", "text": "The only requirement for Acknowledgment resolution is to have at least one issue under discussion to be accepted. As explained in Section 4.2.4, we assume that an explicit Acknowledgment is a sign of acceptance of the latest issue under discussion. For Repeated Acknowledgments, Ferna\u0301ndez (2006) requires to have co-referentiality between the repeated constituent in the NSU and the relative constituent in the FEC of MaxQUD. We decided to drop this requirement assuming that the co-reference is always present when the classifier assigns the class RepAck to the current NSU. This assumption does not affect the system given that the effect on the state variables is the same for both Acks and RepAcks. The rule for Acknowledgments is the following5:\nack :\nif ((nsua=Ack \u2228 nsua=RepAck) \u2227 max-qud>0) then{ P (aa \u2190 Accept()) = 1\n5The symbol \u2190 indicates the assignment of the right-hand side value to the left-hand side variable.\n55\nConsider the following example.\n(4.10) b: I am going to the party.\na: OK.\nThe dialogue context of (4.10) is:\nmax-qud = 1\nqud[max-qud].q = goingToParty(IND1)\nnsua = Ack\nAfter the application of the rule:\naa = Accept()\nNotice that this may be an oversimplification since often the values of the variables in the dialogue state are not determined with full probability but rather the variables encode a probability distribution over a set of values. For instance, it is often the case that the classifier will retrieve the type of the NSU in a probability distribution with one value with large probability and other few values with smaller probability scores. In this case we could have a situation resembling the following:\nnsua =  Ack with probability 0.75 AffAns with probability 0.2\nCheckQu with probability 0.05\nThe case above would result in the following distribution of aa 6:\naa = Accept() with probability 0.75None with probability 0.25 Since the dialogue state is a Bayesian network, the update rules will return a distribution of values that is dependent on both the distribution assigned by the rule (in this case only one value with full probability) and the distributions of the variables the rule depend on. These considerations can be of course extended to all the other classes so in the following sections we will only point out the most relevant use cases."}, {"heading": "4.4.2 Affirmative Answers", "text": "The context for an Affirmative Answer contains a polar question q(y) as MaxQUD. As for the Acknowledgments, we drop the requirement of co-referentiability between the repeated constituent of the RepAffAns and the same constituent in the FECs of the MaxQUD element.\n6The actual distribution would not necessarily assign None as alternative value because other rules may be triggered by the other values of nsua.\n56\nAn Affirmative Answer to a polar question corresponds to asserting the same semantic content (predicate) of the question. The following is the rule to handle Affirmative Answers7,8:\naffAns :\n\u2200 q,y if ((nsua=AffAns \u2228 nsua=RepAffAns) \u2227 qud[max-qud].q=q(y)) thenP  aa \u2190 Assert(q(y)), new-fec\u2190 qud[max-qud].fec\n = 1 An example of application of the affAns rule can be:\n(4.11) b: Are you going to the party?\na: Yes.\nThe context of (4.11) is the following:\nmax-qud = 1\nqud[max-qud].q = goingToParty(IND2)\nqud[max-qud].fec = {}\nnsua = AffAns\nAfter the application of the rule:\naa = Assert(goingToParty(IND2))\nnew-fec = {}"}, {"heading": "4.4.3 Rejections", "text": "As for the Affirmative Answers, the context of Rejections is a polar question q(y), but, as explained in Section 4.2.2, we need to distinguish the cases in which q is positive or negative. We will define the following function Neg indicating the negation of a proposition p (or equivalently a question):\nNeg(p) = p\u0304 if p is positivep if p is negative where p\u0304 is the negative of p. As an extension of the above notation, we indicate a proposition that is explicitly negative as p\u0304.\n7As a convention, quantified variables and quantified individuals in the rule definitions are indicated respectively as x and y. Vectors of variables or individuals are indicated respectively as x and y.\n8As in this case, a probabilistic effect might contain several assignments. Hereafter, for readability, we write the sequence of assignments in a vertical notation.\n57\nRejections are handled by the following rule:\nreject :\n\u2200 q,y if (nsua=Reject \u2227 qud[max-qud].q=q(y)) thenP  aa \u2190 Assert(Neg(q)(y)), new-fec\u2190 qud[max-qud].fec\n = 1 else if (nsua=Reject \u2227 qud[max-qud].q= q\u0304(y)) thenP  aa \u2190 Assert(q\u0304(y)), new-fec\u2190 qud[max-qud].fec\n = 1 The following is an example for the above rule:\n(4.12) b: Are you going to the party?\na: No.\nThe context of (4.12) is the following:\nmax-qud = 1\nqud[max-qud].q = goingToParty(IND2)\nqud[max-qud].fec = {}\nnsua = Reject\nAfter the application of the rule:\naa = Assert(Neg(goingToParty)(IND2))\nnew-fec = {}"}, {"heading": "4.4.4 Propositional Modifiers", "text": "As Affirmative Answers and Rejections, Propositional Modifiers are triggered by polar questions. As seen in Section 4.2.2, their resolution corresponds to asserting the predicate of the polar question modified by a certain modality given by the lexical meaning of the NSU itself.\nWe define the function PropRelM (p) that modifies the meaning of a proposition p (or equivalently a question) with the modality M . The modality is given by the lexical meaning of the word used in the NSU, here indicated for simplicity as the word itself (contained in the variable ua).\n58\nThe rule for Propositional Modifiers states:\npropMod :\n\u2200 q,y if (nsua=PropMod \u2227 qud[max-qud].q=q(y)) thenP  aa \u2190 Assert(PropRelua(q)(y)), new-fec\u2190 qud[max-qud].fec\n = 1 Here is an example of application of the above rule:\n(4.13) b: Are you going to the party?\na: Probably.\nThe dialogue state of (4.13) before the application of the rule is the following:\nmax-qud = 1\nqud[max-qud].q = goingToParty(IND2)\nqud[max-qud].fec = {}\nnsua = PropMod\nAfter the application of the rule we would have:\naa = Assert(PropRelprobably(goingToParty)(IND2))\nnew-fec = {}\nConversely to Affirmative Answers and Rejections, the Propositional Modifiers need to take into account the lexical meaning of the modifier used to update the dialogue state accordingly. This requires a set of lexicalized update rules to properly react to each possible modality of the modified proposition. However, these rules will only take place at the level of action selection and context update therefore it is still possible to resolve this kind of NSUs in a general way, as previously explained in Section 4.2.2.\nAn example of lexicalized rule for updating the context in the presence of a modified proposition can be the following.\nfactsIncrementPropRel :\n\u2200 p,y if (ab=Accept(PropRelprobably(p)(y))) then{ P (facts\u2190 facts \u222a {p(y)} \u222a new-fec) = 0.75 else if (ab=Accept(PropRelunlikely(p)(y))) then{ P (facts\u2190 facts \u222a {p(y)} \u222a new-fec) = 0.25\n59\nThis rule handles the update of the facts variable when the addressee decides to accept a proposition modified by a \u201cprobably\u201d relation or by an \u201cunlikely\u201d relation. The latter is realized by updating facts with a high probability while the former updates facts with a low probability.\nWhile the above rule has handcrafted probabilities, they can in principle be learned from actual data. Of course this would be only possible given a corpus containing NSU instances annotated with the dialogue acts and state updates at each step. When an instance of Propositional Modifier is encountered, the probabilities of the effects are updated according to the relative state update move."}, {"heading": "4.4.5 Check Questions", "text": "As defined in Section 4.2.4, Check Questions are used to ask for understanding/acceptance of the latest issue being raised. In practice this means asking the latest asserted proposition as a polar question. The following is the rule to handle this type of NSUs:\ncheckQu :\n\u2200 p,y if (nsua=CheckQu \u2227 qud[max-qud].q=p(y)) thenP  aa \u2190 Ask(p(y)), new-fec\u2190 qud[max-qud].fec\n = 1 An example of application of the previous rule is the following:\n(4.14) a: I am going to the party.\na: OK?\nThe dialogue context of (4.14) is:\nmax-qud = 1\nqud[max-qud].q = goingToParty(IND1)\nqud[max-qud].fec = {}\nnsua = CheckQu\nAfter the application of the rule:\naa = Ask(goingToParty(IND1))\nnew-fec = {}\n60"}, {"heading": "4.4.6 Short Answers", "text": "The antecedent of Short Answers is assumed to be a wh-question. As stated previously, in this work we limit ourselves to unary wh-interrogatives i.e. questions with only one unknown variable x. Short answers are resolved by applying them to the MaxQUD whinterrogative then asserting the resulting proposition. The application of the Short Answer is done by substituting every occurrences of the variable x with ua (or equivalently a highlevel representation of it). The following is the rule for Short Answers:\nshortAns :\n\u2200 q, x,y, pi,yi if (nsua=ShortAns \u2227 qud[max-qud].q=q(x,y) \u2227\n{p1(x,y1), . . . , pn(x,yn)}\u2286qud[max-qud].fec) then  aa \u2190 Assert(q(ua,y)), new-fec\u2190 {p1(ua,y1), . . . , pn(ua,yn)}  = 1 An example of use of this rule is:\n(4.15) b: Who is your friend organizing the party?\na: Paul.\nTo make the example more appropriate we added a constituent that will be resolved by the rule together with the dialogue act. The context of (4.15) is:\nmax-qud = 1\nqud[max-qud].q = organizingTheParty(X1)\nqud[max-qud].fec = {friend(IND2,X1)}\nnsua = ShortAns\nAfter the application of the rule:\naa = Assert(organizingTheParty(Paul))\nnew-fec = {friend(IND2,Paul)}"}, {"heading": "4.4.7 Sluices", "text": "As argued in Section 4.2.5, we limit ourselves to the treatment of direct Sluices. Even for this type of Sluices only, the resolution rules are many since they have to account for the lexical meaning of each wh-word. Furthermore, the meaning of wh-words can be modified in many ways, e.g. \u201chow many\u201d, \u201chow long\u201d, \u201cwho else\u201d, \u201cwhat about\u201d. This would require an extensive treatment for this kind of NSUs that we do not attempt to elaborate.\n61\nNevertheless we will show some rules to treat simple direct Sluices like the ones in (4.16) and (4.17).\n(4.16) b: A friend is coming to the party.\na: Who?\n(4.17) b: Paul is throwing a party.\na: When?\nThe context of the Sluices is a MaxQUD with at least one variable (raised by either a wh-question or a proposition with some undefined reference). The Sluice is used to request some kind of information regarding one of the FECs of the antecedent. The requested information as well as the context generated by the resolution depend on the lexical meaning of the wh-word. For instance, the following rule treats the Sluice \u201cwho?\u201d.\nsluicewho :\n\u2200 q, x,y, pi,yi if (nsua=Sluice \u2227 \u201cwho\u201d\u2208ua \u2227 qud[max-qud].q=q(x,y) \u2227\n{p1(x,y1), . . . , pn(x,yn)}\u2286qud[max-qud].fec) thenP  aa \u2190 Ask(named(x, x\u0302)), new-fec\u2190 {p1(x,y1), . . . , pn(x,yn)} \u222a {person(x)}  = 1 where x\u0302 is a newly created variable. Such a Sluice asks about the identity (here simplified by the name) of a person which is referred to in the antecedent.\nWe can use as example the transcript (4.16). The context before the application of the rule is:\nmax-qud = 1\nqud[max-qud].q = comingToParty(X1)\nqud[max-qud].fec = {friend(IND2,X1)}\nnsua = Sluice\nAfter the application of the rule we have:\naa = Ask(named(X1))\nnew-fec = {friend(IND2,X1),person(X1)}\nAn interesting result of this rule in combination with the probabilistic inference employed in OpenDial is how the ambiguity in the FECs is handled. As argued in Ginzburg (2012), the antecedent of a Sluice can contain more than one potential FEC as exemplified by the following transcript.\n(4.18) b: A friend of mine is organizing a party for his girlfriend.\na: Who? (= Who is your friend? / Who is his girlfriend?)\n62\nThe resolution of this kind of ambiguities is automatically handled in a probabilistic fashion. In (4.18) the representation of the antecedent (MaxQUD) would be the following:\nqud[max-qud].q = organizingPartyFor(X1,X2) qud[max-qud].fec = {friend(IND1,X1), girlfriend(X2,X1)}\nIn this case, the above rule would be applied to both variables X1 and X2, resulting in two possible assignments of aa and new-fec with 0.5 probability:\naa = Ask(named(X1,X3)) with probability 0.5Ask(named(X2,X3)) with probability 0.5 new-fec =\n{friend(IND1,X1),person(X1)} with probability 0.5{girlfriend(X2,X1),person(X2)} with probability 0.5 Without any prior, the probabilistic inference over the dialogue state would assign equal probability to each possible assignment of aa. A more sophisticated approach may use some notion of saliency as a prior to adjust the probabilities of each focal constituent. For example one could adjust the probability according to whether the constituent is a subject or an object in the antecedent. In the previous example the friend would have had more probability mass (e.g. 0.8) and the girlfriend less probability mass (e.g. 0.2). It is also possible that the prior saliency could depend on other variables such as the Facts or other contextual factors. Moreover, the parameters of the saliency function could also be learned from data."}, {"heading": "4.4.8 Clarification Ellipsis", "text": "Clarification Ellipsis are a kind of clarification requests. To resolve clarification requests and their elliptical variants, Ginzburg (2012) includes a general theory of grounding and clarification requests. This theory would add a non-trivial amount of complexity to our formalization so we shall assume in our formalization that the latest utterance always grounded unless a clarification request comes afterwards. Therefore we resolve Clarification Ellipsis on the MaxQUD element without adding any other structure to the dialogue state. We also consider the Clarification Ellipsis to have only a clausal confirmation reading, leaving aside their other possible readings which would require a more elaborate approach (more details in Ginzburg (2012)). The clausal confirmation reading can be exemplified by (4.19).\n(4.19) a: Is Paul coming to the party?\nb: Paul? (= Are you asking if Paul is coming to the party?)\nThe clausal confirmation reading can be interpreted as asking a polar question about the constituent brought about by the Clarification Ellipsis.\n63\nCEconf :\n\u2200 p, x,y\nif (nsua=CE \u2227 ua=\u201cx?\u201d\u2227\n(qud[max-qud].q=p(x,y) \u2228 p(x,y)\u2208qud[max-qud].fec)) then{ P ( aa \u2190 Ask(p(x,y)) ) = 1\nAs an example we can show the application of the rule on (4.19). The context is:\nmax-qud = 1\nqud[max-qud].q = comingToParty(IND1)\nqud[max-qud].fec = {named(IND1,Paul)}\nnsua = CE\nThe result of the application of the rule is:\naa = Ask(named(IND1,Paul))\nnew-fec = {}"}, {"heading": "4.5 Implementation and use case example", "text": "In this section we exemplify some usages of the rules on a real-world conversation. It shows some example of behavior of the rules over a selected transcript from the Communicator dataset. However, we want to point out that this section is not intended to give an empirical evaluation of the rules which is very far from being trivial since such an evaluation would require (at least) the availability of a fully annotated dataset of transcripts with the dialogue acts and the context updates at each step.\nThe Communicator (Walker et al., 2001) dataset is a set of transcripts of interactions between a dialogue system and human testers. The Communicator dataset contains transcripts of conversations for booking flight tickets. The interactions are mainly \u201cmachinedriven\u201d meaning that the system drives the conversation, it asks questions and the user only answer to those questions. In this scenario it is possible to find many answers NSUs such as Short Answers, Affirmative Answers and Rejections. To test the resolution rules over this transcript we integrated the rules within a dialogue system developed with the OpenDial toolkit.\nNext we will briefly talk about the architecture of our dialogue system and then we elaborate the step-by-step description of the example of interaction with the system using the chosen transcript from the Communicator dataset.\n64"}, {"heading": "4.5.1 Dialogue system architecture", "text": "As defined in Lison (2014), \u201ca spoken dialogue system is a computational agent that can converse with humans through everyday spoken language\u201d. These systems have a complex structure formed of many different parts, however they are usually formed by the following major components:\n\u2022 Natural language understanding (NLU), maps the textual utterances into a high-level semantic representation;\n\u2022 Dialogue management, updates the dialogue state and plans the actions to perform;\n\u2022 Natural language generation (NLG), generates the linguistic realization of the planned actions or dialogue acts;\nThe resolution of NSUs is closely related to the NLU task and to the dialogue management in presence of such utterances. For its proper operation, our implementation indeed includes also shallow NLU and NLG modules as well as a very simple action selection procedure. Figure 4.2 shows the work-flow of the system. The system takes as input a user utterance uu. The NLU module generates the semantic representation of the utterance au, the NSU resolution takes place at this stage and involves the recovery of the right semantic form for the incomplete utterance using the information available in the dialogue context. The classifier, right before the resolution, generates the content of the variable nsuu. The action selection module decides what to do with the user utterance, producing a semantic representation of the action to perform am (or dialogue act of the sentence to be uttered). The NLG module transforms am into its linguistic form um. Throughout the process the context is updated by rules triggered by au, am and um.\n65"}, {"heading": "4.5.2 Example", "text": "In this section we present an extended example of the usage of some of the rules on the example transcript (4.20) from the Communicator dataset. The transcript (4.20) encodes a conversation between a dialogue system and a user with the purpose of booking a ticket for a flight.\n(4.20) Communicator 1693:04\n1. m: [. . . ] What are your travel plans?\n2. u: I\u2019d like to travel from Columbus Ohio to Phoenix Arizona on\nThursday October 5th.\n3. m: What time do you want to leave Columbus?\n4. u: Before 6 P.M.\n5. m: Do you have a preferred airline?\n6. u: No.\n7. m: I have you going from Columbus Ohio to Phoenix Arizona on\nOctober 5th. Is that correct?\n8. u: Yes.\n9. m: Will you return to Columbus from Phoenix?\n10. u: No.\n11. m: Is Phoenix your final destination?\n12. u: Yes.\n13. m: Hold on while I check availability.\nAs previously remarked, this type of conversations is driven by the system which asks the questions to fill the pieces of information it needs to properly retrieve the available flight options. The user answers to the questions posed by system, often using NSUs such as Short Answers, Affirmative Answers and Rejections. It is clear that this example covers only few of the available NSU classes but still it suffice at explaining the overall behavior of our implementation. More elaborate examples would have been complicated to understand and to read therefore we address the interested reader to the actual implementation9 to have a more detailed look at the behavior of the system.\nWe want to stress that the predicates we use are simple and handcrafted. A much more sophisticated NLU module would be required to automatize the process of extracting those predicates from the sentences which is a completely different problem from the one we addressed in this work.\nThe following is a step-by-step explanation of the systems behavior through the updates of the dialogue state. The process is intrinsically probabilistic, for every move there are many updates with a certain probability but, for readability, we avoid to show each possible probabilistic update and we show the most probable one instead.\n9cf. https://github.com/paolodragone\n66\n1. The system greets and asks for the the travel plans of the user, updating the QUD\nwith the new question:\num = [. . . ] What are your travel plans?\nam = Ask(travelPlans(x1, x2, x3))\nqud[1].q = travelPlans(x1, x2, x3)\nmax-qud = 1\n2. The user asserts its travel plans resolving the current issue under discussion:\nuu = I\u2019d like to travel from Columbus Ohio to Phoenix Arizona on\nThursday October 5th.\nau = Assert(travelPlans(C1,C2,D1)) new-fec = {city(C1,Columbus), city(C2,Phoenix),date(D1, 05-10)}\nThe system then accepts the assertion and resolves the issue:\nfacts = {travelPlans(C1,C2,D1),\ncity(C1,Columbus), city(C2,Phoenix),date(D1, 05-10)}\nmax-qud = 0\n3. The system asks for time of departure, a new issue arises:\nuu = What time do you want to leave Columbus?\nau = Ask(departTime(x1))\nqud[1].q = departTime(x1)\nmax-qud = 1\n4. The user resolves the issue with a Short Answer. The meaning of this NSU is inferred\nfrom the MaxQUD: the departure time must be before the given hour. The Short Answer resolution rule gives the following result:\nuu = Before 6 P.M.\nnsuu = ShortAns\nau = Assert(departTime(T1)) new-fec = {before(T1,T2), time(T2, 18:00)}\nThe system again acknowledges the answer of the user inserting his assertion in the set of Facts and downdates the QUD with the resolved issue:\n67\nfacts = {travelPlans(C1,C2,D1),\ncity(C1,Columbus), city(C2,Phoenix), date(D1, 05-10), departTime(T1), before(T1,T2), time(T2, 18:00)}\nmax-qud = 0\n5. The system asks again a question about the preferred airline of the user. This time\nit is a polar question (notice the absence of a variable in the question predicate).\num = Do you have a preferred airline?\nam = Ask(havePreferredAirline(user))\nqud[1].q = havePreferredAirline(user)\nmax-qud = 1\n6. The user gives a negative answer to the previous question using an NSU. Again the\nmeaning is inferred from the MaxQUD: the user does not have a preferred airline. From Section 4.4.3 we recall that the resolution rule in this case applies the Neg function to the predicate to indicate its negative form.\nuu = No.\nnsuu = Reject\nau = Assert(Neg(havePreferredAirline)(user))\nAgain the answer of the user is inserted in the Facts:\nfacts = {travelPlans(C1,C2,D1),\ncity(C1,Columbus), city(C2,Phoenix), date(D1, 05-10),\ndepartTime(T1), before(T1,T2), time(T2, 18:00), Neg(havePreferredAirline)(user)}\nmax-qud = 0\n7. The system summarizes the pieces of information gained so far and queries the user\nfor their correctness. A check question like this can be represented as asking the first proposition as a polar question in the following way:\num = I have you going . . . Is that correct?\nam = Ask(travelPlans(C1,C2,D1))\nqud[1].q = travelPlans(C1,C2,D1) qud[1].fec = {city(C1,Columbus), city(C2,Phoenix), date(D1, 05-10)}\nmax-qud = 1\n68\n8. The user confirms the information stored by the system with an Affirmative Answer.\nWe recall that an Affirmative Answer is equivalent to stating the polar question in MaxQUD as it is:\nuu = Yes.\nnsuu = AffAns\nau = Assert(travelPlans(C1,C2,D1)) new-fec = {city(C1,Columbus), city(C2,Phoenix),date(D1, 05-10)}\nThe system acknowledges the answer by downdating the QUD but leaving Facts as it is since no additional information was included.\nmax-qud = 0\n9. The system asks a new question about a possible return flight.\num = Will you return to Columbus from Phoenix?\nam = Ask(return(C2,C1))\nqud[1].q = return(C2,C1) qud[1].fec = {city(C1,Columbus), city(C2,Phoenix)}\nmax-qud = 1\n10. The user says that he will not return from its destination.\nuu = No.\nau = Assert(Neg(return)(C2,C1)) facts = {travelPlans(C1,C2,D1),\ncity(C1,Columbus), city(C2,Phoenix), date(D1, 05-10),\ndepartTime(T1), before(T1,T2), time(T2, 18:00),\nNeg(havePreferredAirline)(user),\nNeg(return)(C2,C1)}\nmax-qud = 0\n11. The system asks one last question about a its final destination.\num = Is Phoenix your final destination?\nam = Ask(finalDest(C2))\nqud[1].q = finalDest(C2) qud[1].fec = {city(C2,Phoenix)}\nmax-qud = 1\n69\n12. The user confirms that Phoenix is his last destination.\nuu = Yes.\nau = Assert(lastDest(C2)) facts = {travelPlans(C1,C2,D1),\ncity(C1,Columbus), city(C2,Phoenix), date(D1, 05-10),\ndepartTime(T1), before(T1,T2), time(T2, 18:00),\nNeg(havePreferredAirline)(user),\nNeg(willReturn)(user),\nlastDest(C2)}\nmax-qud = 0\n13. After gathering up all the information needed, contained in the Facts, the system\nstarts checking the availability of the flights for the user.\nA possible continuation would be that the system finds some alternative flights to show to the user and ask him which one he or she prefers. The understanding in the dialogues in not always perfect though. It often happens that the system asks for repetition or even that the user resets or interrupts the conversation. This makes the dialogue transcripts from the Communicator dataset very unpredictable and a good starting poin to evaluate the benefits and limitations of probabilistic approaches to NSU resolution.\nThis example gives enough detail to understand the basic behavior of the system in this particular dialogue domain. It is of course limited in many ways and extensions could be sought in different directions. Testing the system on a different domain covering other types of NSUs should be perhaps the first way to check the validity of the rules. However, a complete evaluation would require the manual annotation of several dialogue transcripts, which is not an easy task.\nBesides, the behavior of the rules rely on a fixed semantics that is highly domain-dependent. To make the system scalable to different domains one needs to integrate other grammatical and lexical resources to make the semantics as generic as possible. It is clear though that a complete domain independent framework would be difficult to achieve.\nOur system is of course still a prototype of a complete dialogue system for NSU interpretation. There are many improvement needed to achieve a working system. For instance the inclusion of rules to handle the NSU classes not covered by our work. A more general theory of grounding is also needed to properly account for the clarification requests. Extend every other assumption we made to simplify the development is another important goal for possible future works. Furthermore, to enhance the capabilities of the system, the integration of other natural language understanding modules is needed as well. Anaphora Resolution (Mitkov, 2014) and Named Entity Recognition (Nadeau and Sekine, 2007) are just a few of the problems concerning the correct interpretation of NSUs.\n70"}, {"heading": "4.6 Summary", "text": "In this chapter we presented how to resolve the semantic meaning of NSUs. The resolution is a task that aims at extracting the meaning of an NSU given the dialogue context. To do so, we relied on the previous work of Ferna\u0301ndez (2006) which presented a series of rules to resolve the meaning of the NSUs from a TTR-encoded dialogue context. As previously argued throughout this thesis, the use of a purely logic-based formalism, such as TTR, has some disadvantages in dealing with partially observable inputs and stochastic events when compared to a probabilistic approach. We showed how to reformulate the rules from Ferna\u0301ndez (2006) using the probabilistic rules formalism (Lison, 2014) in order to include a probabilistic account of the dialogue state. We made use of a portion of the dialogue context theory from Ginzburg (2012) to encode the basic elements of the dialogue state needed for the resolution of the NSUs (see Section 2.2). We presented in Section 4.4 the probabilistic rules for the resolution of the NSUs. In Section 4.5 we also described a stepby-step example of the usage of the rules. The framework presented in this chapter has also been implemented and tested with OpenDial (Lison and Kennington, 2015).\n71\nChapter 5"}, {"heading": "Conclusion", "text": "This chapter concludes the present thesis by summarizing the contributions of our work. The chapter also points out some of the possible developments that can be pursued in future works."}, {"heading": "5.1 Contributions", "text": "In this thesis we described our research work regarding non-sentential utterances. NSUs are utterances that do not have a complete sentential form but convey a full meaning. However, they require to be \u201cinterpreted\u201d i.e. their meaning must be extracted from the context of the dialogue. Our experiments concerned two separate aspects of the interpretation of the NSUs, namely:\n\u2022 The classification of NSUs given their context;\n\u2022 The resolution of the semantic content of the NSUs from the dialogue context;\nIn Chapter 2 we presented the background knowledge needed to the development of our work. We discussed in Section 2.1 the concept of non-sentential utterance, referring to Ferna\u0301ndez (2006) as our theoretical basis. From the aforementioned work we employ the same taxonomy and corpus of NSUs. We then explain our methodology for the interpretation of NSUs, also based on the theory from Ferna\u0301ndez (2006). To interpret an NSU, we first classify it according to the aforementioned taxonomy using machine learning then we \u201cresolve\u201d its meaning from the dialogue context through a resolution procedure dependent on its type.\nFerna\u0301ndez (2006) develops a set of resolution procedures based on Type Theory with Records (Cooper, 2004; Ginzburg, 2012). In Section 2.2 we briefly describe the aspects of TTR and the theory of dialogue context from Ginzburg (2012) that we needed in our work.\n73\nIn this thesis we argue that a purely logical framework such as TTR may have disadvantages in dealing with the uncertain nature of the NSUs. In our view a proper alternative is a probabilistic approach to the resolution of NSUs. To this end we employ the probabilistic rules formalism and the theory from Lison (2014) as probabilistic representation of the dialogue state and the NSU resolution procedures. We detail the basic aspects of the theory from Lison (2014) in Section 2.3. In Lison (2014) the dialogue state is represented as a Bayesian network and its dynamics are described by probabilistic rules. Probabilistic rules are if . . . then . . . else . . . constructs that map logical conditions to probabilistic effects.\nThe focus of Chapter 3 is on the classification of NSUs, which is the task of inferring the type of a given NSU from its context. The context of a NSU is formed by its \u201cantecedent\u201d, the preceding utterance that holds its hidden meaning. Our work on the classification of NSUs is based on Ferna\u0301ndez et al. (2007). We replicated their approach and set it as our baseline, as explained in Section 3.3. In Section 3.4 we describe the new features we use to extend the baseline feature set. The extended feature set alone was not enough to achieve an improvement of the classification performances. The major problems in this respect were the scarcity of labeled data and the class imbalance. To address those problems we employed semi-supervised learning techniques that we detail in Section 3.5. Our experiments show that the combination of the extended feature set and new training instances labeled with Active Learning led to a significant improvement of the classification accuracy. Nevertheless we argue that further analysis and testing need a larger amount of labeled data to be carried out properly. In Dragone and Lison (2015a) we present our findings in the classification of NSUs using Active Learning.\nIn Chapter 4 we detail the resolution of NSUs. The NSU resolution is the task of extracting the meaning of a given NSU from the dialogue context. We explain the process that we employ for the NSU resolution in Section 4.1. In Section 4.2 we describe the theoretical concepts we need from Ginzburg (2012). The description of our theory starts in Section 4.3 where we explained the design of the dialogue context. To model our dialogue context we take inspiration from Ginzburg (2012), however we reinterpret its constructs as random variables. The random variables in the dialogue state interact with each other through the probabilistic rules. The resolution rules that we developed are explained in Section 4.4. Finally, in Section 4.5, we show a detailed example of the use of the resolution rules over a transcript from the Communicator dataset (Walker et al., 2001).\nOur approach to the resolution of NSUs is intended to be a proof-of-concept for this task. We showed how we could reuse many concepts from the theory of Ferna\u0301ndez (2006) and Ginzburg (2012) and \u201ctranslate\u201d the resolution rules based on TTR into probabilistic rules. The works on classification and interpretation carried out in this thesis were also presented in Dragone and Lison (2015b).\n74"}, {"heading": "5.2 Future developments", "text": "In this section we list a series of ideas for possible future works that come out directly from our findings and from the assumptions we made."}, {"heading": "Improve the NSU classification performances", "text": "In our work on the classification of NSUs we experimented many different approaches in seeking an improvement of the classification accuracy. However, there are many other paths that we did not explore or aspects in our approaches that can be improved. Here we discuss a few of the possible extension of our work.\nThere are classes of NSU that are intrinsically difficult to predict such as Helpful Rejections and other pairs of classes that are difficult to discriminate such as Repeated Acknowledgments and Repeated Affirmative Answers. A common issue in trying to predict those classes is that the parallelism with their antecedent is almost entirely at the semantic level. This requires deeper understanding of the phenomena and the use of features that exploit semantic relations in the NSU instances. We did not use any semantic feature since it would have added a non-trivial amount of complexity to our feature extraction algorithms. The deeper understanding of \u201cdifficult\u201d classes and the use of such features may be a good starting point to any feature work on this topic.\nUsing additional features does not avoid the problem of class imbalance in the dataset. Many techniques could be experimented to try to mitigate this issue. An example may be an over-sampling technique such as SMOTE 1 (Chawla et al., 2002). The aforementioned work shows that the combination of SMOTE and majority class under-sampling leads to better classification performances on certain domains.\nPerhaps the most difficult issue to overcome is the scarcity of labeled data. Our work shows that additional training data is indeed useful to improve the classification performances but we still lack enough data to run proper evaluations. We did not use the instances labeled with Active Learning as test data. Additional data for the gold standard should be composed of high-quality, manually annotated instances extracted within a corpus study that closely follows the original one from Ferna\u0301ndez and Ginzburg (2002).\nIncorporate additional elliptical phenomena\nThe corpus study from Ferna\u0301ndez and Ginzburg (2002) is focused on data extracted from the British National Corpus therefore confined to only certain kind of dialogue domains. As argued by Raghu et al. (2015), there are many elliptical phenomena that do not fit well in the taxonomy from Ferna\u0301ndez and Ginzburg (2002). An interesting follow up work on non-sentential utterances might try to find new elliptical phenomena in different dialogue\n1Synthetic Minority Over-sampling Technique.\n75\ndomains and try to extend the taxonomy. Another point that may be considered is that some classes include large variety of forms and functions such as Short Answers and Helpful Rejections. A possible development could be to increase the granularity of such classes to try to capture more subtle differences."}, {"heading": "Extend our NSU resolution approach", "text": "Our study on the resolution of NSUs pioneers a rule-based approach that involves a probabilistic dynamics of the dialogue state. There are still many issues to address:\n\u2022 Increase the coverage of the rules to all the classes that were not covered by our work: Factual Modifiers, Helpful Rejections and so on.\n\u2022 Develop a proper mechanism of rule adaptation in the presence of lexical modifiers e.g. for Sluices such as \u201cFor how long?\u201d.\n\u2022 Include grammatical and lexical resources to extract more complex meanings. A simple Short Answer that would not be covered by our rules is:\n(5.1) a: Who is coming tomorrow?\nb: Nobody.\n\u2022 Properly evaluate the rules on testing data from different dialogue domains.\nThe last point, perhaps the most important one, would require the development of a corpus of dialogue transcripts annotated with each semantic move and state update. In turn this would require to develop a generic representation of the semantic content of the utterances which is a non-trivial task by itself. A possibility could be to use TTR as semantic representation and reformulate the rules accordingly. Using TTR as basic semantic formalism it could be an interesting challenge to develop probabilistic rules to address a larger set of linguistic phenomena besides NSUs."}, {"heading": "Combine different NSU resolution approaches", "text": "For our work on the NSU resolution we develop a rule-based approach based on a probabilistic representation of the dialogue state. It bares similarities with statistical approaches for the resolution such as Raghu et al. (2015). Their work is concentrated on follow-up NSU questions such as:\n(5.2) a: How much for this model?\nb: . . .\na: For this other one?\nTheir approach is based on the combination of keywords from the follow-up question and the original one. From the combination of keywords they build possible meaningful \u201ccompletions\u201d of the NSU e.g. a completion for the NSU at the third line in (5.2) would\n76\nbe \u201cHow much for this other model?\u201d. After generating each possible completions they rank them according to some score and pick the best one.\nDifferently from ours, their approach does not use a high-level semantic representation of the utterances. It would be interesting to try to combine their statistical approach to our probabilistic rule-based one.\nCompare our approach with other existing systems\nAnother useful comparison, and perhaps integration, should be made with the systems originally developed on the theory of Ferna\u0301ndez (2006), namely SHARDS (Ginzburg et al., 2007) and one of its extensions CLARIE (Purver, 2006). The former is a system for ellipsis resolution that can handle Short Answer, Sluices and Affirmative Answers. The latter is a dialogue system developed to deal with clarification requests and, among them, Clarification Ellipsis, implementing the theory of Ginzburg and Cooper (2004) on top of the GoDiS dialogue system (Larsson et al., 2000). Both are based on the HPSG2 framework from Ginzburg and Sag (2000), which is substantially different from our current design. Our framework lacks a grammar and other lexical resources that are indeed needed to build a functional system. It would be interesting to further develop our approach taking advantage from aspects of those systems and perhaps even integrate them into our architecture based on probabilistic rules.\n2Head-driven Phrase Structure Grammar.\n77"}, {"heading": "Appendix A", "text": ""}, {"heading": "Context update rules", "text": "In this appendix we provide an overview of the probabilistic rules used for updating the context that have been implemented in the dialogue system for the testing of the resolution rules. As described in Section 4.5, the dialogue system that we implemented is focused on conversation of the human-machine kind therefore we will be using the notations am and um to refer respectively to the dialogue act performed by the system and the corresponding raw utterance. Following the aforementioned interaction model, the dialogue context is only representing the pieces of information known by the system. The context update rules are needed in order to make the dialogue context evolve along with the user acts and relative system reactions. In particular we need the rules for updating the QUD and the Facts variables. These rules are inspired by, but not limited to, Ginzburg (2012). Section 2.2 gives the background knowledge for the rules from Ginzburg (2012).\nThe rules shown in this appendix are not meant to give an extensive look on the system architecture but rather an high-level insight on the behavior of the system. These rules may differ from the actual implementation due to technicalities but still they fit for the purpose of the explanation. We will not talk about other modules of the system that we used in the implementation (i.e. NLU, NLG and action selection) because they have been implemented merely for toy examples of interaction on simple domains and they are not directly concerning the interpretation of NSUs. Follows a description of each context update rule."}, {"heading": "QUD increment", "text": "NSUs are mostly reactionary utterances to previously raised issues. We aim at using the NSU resolution rules to interpret the content of the user NSUs. For this reason we concentrate on \u201cmachine-driven\u201d conversations such as the one used in Section 4.5. We assume that in this type of dialogues issues are raised only by the system while the user limits to answer. In this scenario is common to find NSUs uttered by the users, as it often happens in the Communicator dataset.\n79\nGiven this setting, we update the QUD only when the system raises a new issue and we downdate it only when the system accepts a user assertion which resolves the maximal element. We also take into account that asking a question and asserting a proposition may have different probabilities to update the QUD. In the rules below we encode the asking with full probability and the asserting with a probability of 0.75, although, as remarked many times throughout this thesis, those probabilities may actually be estimated on real data.\nqud-increment :\n\u2200 q,x if (am=Ask(q(x))) thenP  qud[qud.size + 1].q\u2190 q(x), qud[qud.size + 1].utt\u2190 um,\nqud.size\u2190 qud.size + 1  = 1 else if (am=Assert(q(x))) thenP  qud[qud.size + 1].q\u2190 q(x), qud[qud.size + 1].utt\u2190 um,\nqud.size\u2190 qud.size + 1  = 0.75 The rule below handles the update of the FEC of the newly added QUD element. The rule adds to the FEC of the new element of QUD only the new-fec predicates sharing at least a variable with the proposition predicate.\nfec-update :\n\u2200 p,x, p\u2032,x\u2032, x if ((am=Ask(p(x)) \u2228 am=Assert(p(x))) \u2227 p\u2032(x\u2032)\u2208new-fec \u2227 x\u2208x \u2227 x\u2208x\u2032) then{ P ( qud[qud.size + 1].fec\u2190 qud[qud.size + 1].fec \u222a {p\u2032(x\u2032)} ) = 1"}, {"heading": "QUD downdate", "text": "The QUD is downdated when the system accepts a proposition asserted by the user. The system responds with an Accept act which will remove the MaxQUD from the QUD.\nqud-downdate :\nif (am=Accept(p)) then P  qud[max-qud].q\u2190 None, qud[max-qud].utt\u2190 None, qud[max-qud].fec\u2190 None,\nqud.size\u2190 qud.size\u2212 1\n = 1\n80"}, {"heading": "Max-qud update", "text": "As soon as the QUD array is updated, the MaxQUD is updated too. As explained in Section 4.3.3, the max-qud variable is defined as the index of the MaxQUD inside the QUD array and its stack-like behavior is determined by an exponentially decreasing probability with maximum on the last inserted element.\nmax-qud-update :\n\u2200i if (i > 0 \u2227 i \u2264 qud.size \u2227 qud[i].q 6= None) then{ P (max-qud\u2190 i) = ei\u2212qud.size"}, {"heading": "Facts increment", "text": "As mentioned above, the dialogue context encodes the knowledge of the system and so are the Facts. The Facts variable contains only predicates accepted by the system. In the rule below we incorporate the ones presented in Section 4.4.4 for Propositional Modifiers and the ones for handling Rejections and Affirmative Answers. Again we point out that while the probabilities are handcrafted for simplicity here, they can be actually learned from data.\nfacts-increment :\n\u2200 p,y if (am=Accept(PropRelprobably(p)(y))) then{ P (facts\u2190 facts \u222a {p(y)} \u222a new-fec) = 0.75 else if (am=Accept(PropRelunlikely(p)(y))) then{ P (facts\u2190 facts \u222a {p(y)} \u222a new-fec) = 0.25\n. . . else if (am=Accept(Neg(p)(y))) then{ P (facts\u2190 facts \u222a {Neg(p)(y)} \u222a new-fec) = 1 else if (am=Accept(p(y))) then{ P (facts\u2190 facts \u222a {p(y)} \u222a new-fec) = 1\n81"}], "references": [{"title": "Large-scale semi-supervised learning for natural language processing", "author": ["S. Bergsma"], "venue": "University of Alberta. Boser, B. E., I. M. Guyon, and V. N. Vapnik (1992). \u201cA training algorithm for optimal margin classifiers\u201d. In: Proceedings of the fifth annual workshop on Computational learning theory. ACM, pp. 144\u2013152.", "citeRegEx": "Bergsma,? 2010", "shortCiteRegEx": "Bergsma", "year": 2010}, {"title": "Reference guide for the British National Corpus (world edition)", "author": ["L. Burnard"], "venue": "Chawla, N. V. et al. (2002). \u201cSMOTE: synthetic minority over-sampling technique\u201d. In: Journal of artificial intelligence research, pp. 321\u2013357. Chen, D. and C. D. Manning (2014). \u201cA fast and accurate dependency parser using neural networks\u201d. In: Proceedings of the 2014 Conference on Empirical Methods in Natural", "citeRegEx": "Burnard,? 2000", "shortCiteRegEx": "Burnard", "year": 2000}, {"title": "Large scale transductive SVMs", "author": ["R Collobert"], "venue": "Language Processing (EMNLP)", "citeRegEx": "Collobert,? \\Q2006\\E", "shortCiteRegEx": "Collobert", "year": 2006}, {"title": "Records and record types in semantic theory", "author": ["R. Cooper"], "venue": "In: Journal of Logic and Computation 15.2, pp. 99\u2013112. De Marneffe, M.-C. et al. (2014). \u201cUniversal Stanford Dependencies: A cross-linguistic typology\u201d. In: Proceedings of LREC, pp. 4585\u20134592. Dragone, P. and P. Lison (2015a). \u201cAn Active Learning Approach to the Classification of", "citeRegEx": "Cooper,? 2005", "shortCiteRegEx": "Cooper", "year": 2005}, {"title": "Non-Sentential Utterances", "author": ["P. Dragone", "P. Lison"], "venue": "Proceedings of the second Italian Conference on Computational Linguistics. Trento, Italy,", "citeRegEx": "Dragone and Lison,? \\Q2015\\E", "shortCiteRegEx": "Dragone and Lison", "year": 2015}, {"title": "Which is the best multiclass SVM method? An empirical study", "author": ["Duan", "K.-B.", "S.S. Keerthi"], "venue": "In: Multiple Classifier Systems, pp. 278\u2013285.", "citeRegEx": "Duan et al\\.,? 2005", "shortCiteRegEx": "Duan et al\\.", "year": 2005}, {"title": "Non-sentential utterances in dialogue: A corpus", "author": ["R. Fern\u00e1ndez", "J. Ginzburg"], "venue": null, "citeRegEx": "Fern\u00e1ndez and Ginzburg,? \\Q2002\\E", "shortCiteRegEx": "Fern\u00e1ndez and Ginzburg", "year": 2002}, {"title": "Non-sentential utterances in dialogue: Classification, resolution", "author": ["R.R. Fern\u00e1ndez"], "venue": null, "citeRegEx": "Fern\u00e1ndez,? \\Q2006\\E", "shortCiteRegEx": "Fern\u00e1ndez", "year": 2006}, {"title": "The large-scale production of syntactically analysed corpora", "author": ["R. Garside"], "venue": "In:", "citeRegEx": "Garside,? 1993", "shortCiteRegEx": "Garside", "year": 1993}, {"title": "The interactive stance", "author": ["J. Ginzburg"], "venue": "Oxford University Press.", "citeRegEx": "Ginzburg,? 2012", "shortCiteRegEx": "Ginzburg", "year": 2012}, {"title": "Clarification, ellipsis, and the nature of contextual", "author": ["J. Ginzburg", "R. Cooper"], "venue": null, "citeRegEx": "Ginzburg and Cooper,? \\Q2004\\E", "shortCiteRegEx": "Ginzburg and Cooper", "year": 2004}, {"title": "Interrogative investigations", "author": ["J. Ginzburg", "I. Sag"], "venue": "Stanford: CSLI publications.", "citeRegEx": "Ginzburg and Sag,? 2000", "shortCiteRegEx": "Ginzburg and Sag", "year": 2000}, {"title": "SHARDS: Fragment resolution in dialogue", "author": ["J Ginzburg"], "venue": "In: Computing", "citeRegEx": "Ginzburg,? 2007", "shortCiteRegEx": "Ginzburg", "year": 2007}, {"title": "The WEKA data mining software: an update", "author": ["M Hall"], "venue": "In: ACM SIGKDD", "citeRegEx": "Hall,? 2009", "shortCiteRegEx": "Hall", "year": 2009}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "In: Proceedings of", "citeRegEx": "Klein and Manning,? 2003", "shortCiteRegEx": "Klein and Manning", "year": 2003}, {"title": "GoDiS: an accommodating dialogue system", "author": ["S Larsson"], "venue": "In: Proceedings of", "citeRegEx": "Larsson,? 2000", "shortCiteRegEx": "Larsson", "year": 2000}, {"title": "CLAWS4: the tagging of the British Na", "author": ["G. Leech", "R. Garside", "M. Bryant"], "venue": null, "citeRegEx": "Leech et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Leech et al\\.", "year": 1994}, {"title": "Heterogeneous uncertainty sampling for supervised", "author": ["D.D. Lewis", "J. Catlett"], "venue": null, "citeRegEx": "Lewis and Catlett,? \\Q1994\\E", "shortCiteRegEx": "Lewis and Catlett", "year": 1994}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "PhD thesis. Mas-", "citeRegEx": "Liang,? 2005", "shortCiteRegEx": "Liang", "year": 2005}, {"title": "A hybrid approach to dialogue management based on", "author": ["P. Lison"], "venue": null, "citeRegEx": "Lison,? \\Q2015\\E", "shortCiteRegEx": "Lison", "year": 2015}, {"title": "Declarative Design of Spoken Dialogue Systems with Probabilistic", "author": ["P. Lison"], "venue": null, "citeRegEx": "Lison,? \\Q2012\\E", "shortCiteRegEx": "Lison", "year": 2012}, {"title": "Structured Probabilistic Modelling for Dialogue Management", "author": ["P. Lison"], "venue": "PhD", "citeRegEx": "Lison,? 2014", "shortCiteRegEx": "Lison", "year": 2014}, {"title": "Developing Spoken Dialogue Systems with the Open", "author": ["P. Lison", "C. Kennington"], "venue": null, "citeRegEx": "Lison and Kennington,? \\Q2015\\E", "shortCiteRegEx": "Lison and Kennington", "year": 2015}, {"title": "Anaphora resolution", "author": ["R. Mitkov"], "venue": "Routledge.", "citeRegEx": "Mitkov,? 2014", "shortCiteRegEx": "Mitkov", "year": 2014}, {"title": "A survey of named entity recognition and classification", "author": ["D. Nadeau", "S. Sekine"], "venue": null, "citeRegEx": "Nadeau and Sekine,? \\Q2007\\E", "shortCiteRegEx": "Nadeau and Sekine", "year": 2007}, {"title": "A general method applicable to the search", "author": ["S.B. Needleman", "C.D. Wunsch"], "venue": null, "citeRegEx": "Needleman and Wunsch,? \\Q1970\\E", "shortCiteRegEx": "Needleman and Wunsch", "year": 1970}, {"title": "Fast training of support vector machines using sequential minimal", "author": ["J Platt"], "venue": null, "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "CLARIE: Handling Clarification Requests in Dialogue System", "author": ["M. Purver"], "venue": "In:", "citeRegEx": "Purver,? 2006", "shortCiteRegEx": "Purver", "year": 2006}, {"title": "R: A Language and Environment for Statistical Computing", "author": ["R Core Team"], "venue": "R Foun-", "citeRegEx": "Team,? 2015", "shortCiteRegEx": "Team", "year": 2015}, {"title": "A Statistical Approach for Non-Sentential Utterance Resolution", "author": ["D Raghu"], "venue": null, "citeRegEx": "Raghu,? \\Q2015\\E", "shortCiteRegEx": "Raghu", "year": 2015}, {"title": "Form, intonation and function of clarification", "author": ["K.J. Rod\u0155\u0131guez", "D. Schlangen"], "venue": null, "citeRegEx": "Rod\u0155\u0131guez and Schlangen,? \\Q2004\\E", "shortCiteRegEx": "Rod\u0155\u0131guez and Schlangen", "year": 2004}, {"title": "A coherence-based approach to the interpretation of non-sentential", "author": ["D. Schlangen"], "venue": null, "citeRegEx": "Schlangen,? \\Q2003\\E", "shortCiteRegEx": "Schlangen", "year": 2003}, {"title": "Towards finding and fixing fragments: Using ML to identify nonsentential utterances and their antecedents in multi-party dialogue", "author": ["D. Schlangen"], "venue": "In: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pp. 247\u2013254. Settles, B. (2010). \u201cActive learning literature survey\u201d. In: University of Wisconsin, Madi-", "citeRegEx": "Schlangen,? 2005", "shortCiteRegEx": "Schlangen", "year": 2005}, {"title": "A mathematical theory of communication", "author": ["C. Shannon"], "venue": "Bell System Technical Journal,", "citeRegEx": "Shannon,? \\Q1948\\E", "shortCiteRegEx": "Shannon", "year": 1948}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley-Interscience. Walker, M. et al. (2001). \u201cDARPA Communicator dialog travel planning systems: The June 2000 data collection\u201d. In: EUROSPEECH, pp. 1371\u20131374. Zhang, N. L. and D. Poole (1996). \u201cExploiting causal independence in Bayesian network inference\u201d. In: Journal of Artificial Intelligence Research, pp. 301\u2013328.", "citeRegEx": "Vapnik,? 1998", "shortCiteRegEx": "Vapnik", "year": 1998}], "referenceMentions": [{"referenceID": 9, "context": "Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012).", "startOffset": 175, "endOffset": 191}, {"referenceID": 21, "context": "The probabilistic rules formalism (Lison, 2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs.", "startOffset": 21, "endOffset": 45}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012).", "startOffset": 21, "endOffset": 542}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012). We argue that logic-based formalisms, such as TTR, have a number of shortcomings when dealing with conversational data, which often include partially observable knowledge and nondeterministic phenomena. An alternative to address these issues is to rely on probabilistic modeling of the dialogue context. Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state.", "startOffset": 21, "endOffset": 1112}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012). We argue that logic-based formalisms, such as TTR, have a number of shortcomings when dealing with conversational data, which often include partially observable knowledge and nondeterministic phenomena. An alternative to address these issues is to rely on probabilistic modeling of the dialogue context. Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism (Lison, 2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation.", "startOffset": 21, "endOffset": 1314}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012). We argue that logic-based formalisms, such as TTR, have a number of shortcomings when dealing with conversational data, which often include partially observable knowledge and nondeterministic phenomena. An alternative to address these issues is to rely on probabilistic modeling of the dialogue context. Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism (Lison, 2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation.", "startOffset": 21, "endOffset": 1335}, {"referenceID": 6, "context": "Fern\u00e1ndez and Ginzburg (2002) identify 15 different types of NSUs.", "startOffset": 0, "endOffset": 30}, {"referenceID": 6, "context": "Fern\u00e1ndez and Ginzburg (2002) identify 15 different types of NSUs. One of the problems that must be addressed to make sense of NSUs is determining their type. One possible way is to classify NSUs using machine learning, as previously experimented by Fern\u00e1ndez et al. (2007). To interpret a given NSU, one also has to resolve its meaning i.", "startOffset": 0, "endOffset": 274}, {"referenceID": 6, "context": "According to Fern\u00e1ndez and Ginzburg (2002) and related works, the frequency of NSUs in the dialogue transcripts of the British National Corpus is about 10% of the total number of utterances.", "startOffset": 13, "endOffset": 43}, {"referenceID": 6, "context": "According to Fern\u00e1ndez and Ginzburg (2002) and related works, the frequency of NSUs in the dialogue transcripts of the British National Corpus is about 10% of the total number of utterances. However, this number may vary greatly if one takes into account a larger variety of phenomena or different dialogue domains e.g. Schlangen (2003) estimates the frequency of NSUs to be 20% of the total number of utterances.", "startOffset": 13, "endOffset": 337}, {"referenceID": 19, "context": "On the other hand we propose a novel approach to the resolution of NSUs using probabilistic rules (Lison, 2015).", "startOffset": 98, "endOffset": 111}, {"referenceID": 7, "context": "On one hand we address the problem of the classification of NSUs by extending the work of Fern\u00e1ndez et al. (2007). On the other hand we propose a novel approach to the resolution of NSUs using probabilistic rules (Lison, 2015).", "startOffset": 90, "endOffset": 114}, {"referenceID": 22, "context": "Nevertheless we detail a large set of NSU resolution rules based on the probabilistic rules formalism and provide an actual implementation of a dialogue system for NSU resolution using the OpenDial toolkit (Lison and Kennington, 2015), which can be the baseline reference for future developments.", "startOffset": 206, "endOffset": 234}, {"referenceID": 7, "context": "Our work on the resolution of NSUs takes inspiration from Fern\u00e1ndez (2006) and Ginzburg (2012) which provide the theoretical background for our study.", "startOffset": 58, "endOffset": 75}, {"referenceID": 7, "context": "Our work on the resolution of NSUs takes inspiration from Fern\u00e1ndez (2006) and Ginzburg (2012) which provide the theoretical background for our study.", "startOffset": 58, "endOffset": 95}, {"referenceID": 7, "context": "Our work on the resolution of NSUs takes inspiration from Fern\u00e1ndez (2006) and Ginzburg (2012) which provide the theoretical background for our study. Their framework is however purely logic-based therefore it can have some drawbacks in dealing with raw conversational data which often contains hidden or partially observable variables. To this end a probabilistic account of the dialogue state is preferable. In our work we implemented a new approach to NSU resolution based on the probabilistic rules formalism of Lison (2015). Probabilistic rules are similar, in some way, to the rules formalized by Ginzburg (2012), as both express updates on the dialogue state given a set of conditions.", "startOffset": 58, "endOffset": 529}, {"referenceID": 7, "context": "Our work on the resolution of NSUs takes inspiration from Fern\u00e1ndez (2006) and Ginzburg (2012) which provide the theoretical background for our study. Their framework is however purely logic-based therefore it can have some drawbacks in dealing with raw conversational data which often contains hidden or partially observable variables. To this end a probabilistic account of the dialogue state is preferable. In our work we implemented a new approach to NSU resolution based on the probabilistic rules formalism of Lison (2015). Probabilistic rules are similar, in some way, to the rules formalized by Ginzburg (2012), as both express updates on the dialogue state given a set of conditions.", "startOffset": 58, "endOffset": 619}, {"referenceID": 9, "context": "Secondly the chapter contains an overview on the formal representation of the dialogue context from the theory of Ginzburg (2012). We discuss briefly the Type Theory with Records, the semantic representation of utterances and the update rules on the dialogue context.", "startOffset": 114, "endOffset": 130}, {"referenceID": 9, "context": "Secondly the chapter contains an overview on the formal representation of the dialogue context from the theory of Ginzburg (2012). We discuss briefly the Type Theory with Records, the semantic representation of utterances and the update rules on the dialogue context. Finally, we introduce the probabilistic approach to the definition of the dialogue context from Lison (2014). We discuss the basics of Bayesian Networks (the dialogue context representation) and the probabilistic rules formalism.", "startOffset": 114, "endOffset": 377}, {"referenceID": 7, "context": "It provides details on our approach, starting from the replication of the work from Fern\u00e1ndez et al. (2007) which we use as baseline.", "startOffset": 84, "endOffset": 108}, {"referenceID": 7, "context": "In order to give a definition of Non-Sentential Utterances ourselves, we shall start by quoting the definition given by Fern\u00e1ndez (2006):", "startOffset": 120, "endOffset": 137}, {"referenceID": 9, "context": "This is indeed a very general definition, whereas a perhaps simpler approach is taken by Ginzburg (2012) which defines NSUs as \u201cutterances without an overt predicate\u201d.", "startOffset": 89, "endOffset": 105}, {"referenceID": 9, "context": "As described in Ginzburg (2012), the parallelism between an NSU and its antecedent can be of syntactic, semantic or phonological nature.", "startOffset": 16, "endOffset": 32}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000).", "startOffset": 107, "endOffset": 122}, {"referenceID": 5, "context": "In order to classify the NSUs, we use a taxonomy defined by Fern\u00e1ndez and Ginzburg (2002). This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000).", "startOffset": 60, "endOffset": 90}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012).", "startOffset": 108, "endOffset": 268}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.", "startOffset": 108, "endOffset": 300}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al.", "startOffset": 108, "endOffset": 385}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al.", "startOffset": 108, "endOffset": 446}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al. (2007) and it is also used in the theory of Ginzburg (2012), which is our reference for the resolution part of our investigation.", "startOffset": 108, "endOffset": 542}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al. (2007) and it is also used in the theory of Ginzburg (2012), which is our reference for the resolution part of our investigation.", "startOffset": 108, "endOffset": 595}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al. (2007) and it is also used in the theory of Ginzburg (2012), which is our reference for the resolution part of our investigation. A detailed comparison of this taxonomy and other ones is given by Fern\u00e1ndez (2006), which also details the corpus study on the BNC that led to the definition of this taxonomy.", "startOffset": 108, "endOffset": 748}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al. (2007) and it is also used in the theory of Ginzburg (2012), which is our reference for the resolution part of our investigation. A detailed comparison of this taxonomy and other ones is given by Fern\u00e1ndez (2006), which also details the corpus study on the BNC that led to the definition of this taxonomy. Follows a brief description of all the classes with some examples. Fern\u00e1ndez (2006) provides more details about the rationale of each class.", "startOffset": 108, "endOffset": 925}, {"referenceID": 6, "context": "The taxonomy presented in the previous section is the result of a corpus study on a portion of the dialogue transcripts in the British National Corpus, first started by Fern\u00e1ndez and Ginzburg (2002), then refined by Fern\u00e1ndez (2006).", "startOffset": 169, "endOffset": 199}, {"referenceID": 6, "context": "The taxonomy presented in the previous section is the result of a corpus study on a portion of the dialogue transcripts in the British National Corpus, first started by Fern\u00e1ndez and Ginzburg (2002), then refined by Fern\u00e1ndez (2006). The dialogue transcripts used in the corpus study contain both two-party and multi-party conversations.", "startOffset": 169, "endOffset": 233}, {"referenceID": 6, "context": "The taxonomy presented in the previous section is the result of a corpus study on a portion of the dialogue transcripts in the British National Corpus, first started by Fern\u00e1ndez and Ginzburg (2002), then refined by Fern\u00e1ndez (2006). The dialogue transcripts used in the corpus study contain both two-party and multi-party conversations. The transcripts cover a wide variety of dialogue domains including free conversation, interviews, seminars and more. Fern\u00e1ndez (2006) also describes the annotation procedure and a reliability test.", "startOffset": 169, "endOffset": 472}, {"referenceID": 6, "context": "The taxonomy presented in the previous section is the result of a corpus study on a portion of the dialogue transcripts in the British National Corpus, first started by Fern\u00e1ndez and Ginzburg (2002), then refined by Fern\u00e1ndez (2006). The dialogue transcripts used in the corpus study contain both two-party and multi-party conversations. The transcripts cover a wide variety of dialogue domains including free conversation, interviews, seminars and more. Fern\u00e1ndez (2006) also describes the annotation procedure and a reliability test. The reliability test was carried out on a subset of the annotated instances comparing the manual annotation of three annotators. The test showed a good agreement between the annotators with a kappa-score of 0.76. From this test it is also clear that humans can reliably distinguish between the NSU classes in the taxonomy. Fern\u00e1ndez (2006) provides more details about the complete analysis of the corpus.", "startOffset": 169, "endOffset": 876}, {"referenceID": 7, "context": "Fern\u00e1ndez (2006) describes a study of the distance between NSUs and their antecedents, with a comparison between two-party and multi-party dialogues.", "startOffset": 0, "endOffset": 17}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs.", "startOffset": 42, "endOffset": 59}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs.", "startOffset": 42, "endOffset": 94}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs. The first step for the interpretation of an NSU is its classification i.e. finding its class according to the taxonomy described in Section 2.1.1. As demonstrated in Fern\u00e1ndez et al. (2007), we can infer the class of an NSU using machine learning, i.", "startOffset": 42, "endOffset": 385}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs. The first step for the interpretation of an NSU is its classification i.e. finding its class according to the taxonomy described in Section 2.1.1. As demonstrated in Fern\u00e1ndez et al. (2007), we can infer the class of an NSU using machine learning, i.e. we can train a classifier on the corpus detailed in Section 2.1.2 and use it to classify unseen NSU instances. The type of an NSU is used to determine the right resolution procedure to use. The resolution of an NSU is the task of recovering the full clausal meaning from their incomplete form on the basis of contextual information. Fern\u00e1ndez (2006) describes a resolution procedure in terms of rules that, given some preconditions on the antecedent and other elements of the dialogue states, builds the semantic representation of the NSU.", "startOffset": 42, "endOffset": 798}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs. The first step for the interpretation of an NSU is its classification i.e. finding its class according to the taxonomy described in Section 2.1.1. As demonstrated in Fern\u00e1ndez et al. (2007), we can infer the class of an NSU using machine learning, i.e. we can train a classifier on the corpus detailed in Section 2.1.2 and use it to classify unseen NSU instances. The type of an NSU is used to determine the right resolution procedure to use. The resolution of an NSU is the task of recovering the full clausal meaning from their incomplete form on the basis of contextual information. Fern\u00e1ndez (2006) describes a resolution procedure in terms of rules that, given some preconditions on the antecedent and other elements of the dialogue states, builds the semantic representation of the NSU. This approach to the resolution of NSUs has been the basis of several implementations of dialogue systems handling the resolution of NSUs such as Ginzburg et al. (2007) and Purver (2006).", "startOffset": 42, "endOffset": 1157}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs. The first step for the interpretation of an NSU is its classification i.e. finding its class according to the taxonomy described in Section 2.1.1. As demonstrated in Fern\u00e1ndez et al. (2007), we can infer the class of an NSU using machine learning, i.e. we can train a classifier on the corpus detailed in Section 2.1.2 and use it to classify unseen NSU instances. The type of an NSU is used to determine the right resolution procedure to use. The resolution of an NSU is the task of recovering the full clausal meaning from their incomplete form on the basis of contextual information. Fern\u00e1ndez (2006) describes a resolution procedure in terms of rules that, given some preconditions on the antecedent and other elements of the dialogue states, builds the semantic representation of the NSU. This approach to the resolution of NSUs has been the basis of several implementations of dialogue systems handling the resolution of NSUs such as Ginzburg et al. (2007) and Purver (2006). Extending the interpretation problem to raw conversational data we need also a way to \u201cdetect\u201d an NSU i.", "startOffset": 42, "endOffset": 1175}, {"referenceID": 3, "context": "The grammatical framework is formulated using Type Theory with Records (Cooper, 2005).", "startOffset": 71, "endOffset": 85}, {"referenceID": 8, "context": "As theoretical base of our work we rely on the theory of dialogue context brought up by Ginzburg (2012), which presents a grammatical framework expressly developed for dialogue.", "startOffset": 88, "endOffset": 104}, {"referenceID": 8, "context": "As theoretical base of our work we rely on the theory of dialogue context brought up by Ginzburg (2012), which presents a grammatical framework expressly developed for dialogue. The claim of Ginzburg (2012) is that the rules that encode the dynamics of the dialogue have to be built into the grammar itself.", "startOffset": 88, "endOffset": 207}, {"referenceID": 3, "context": "The grammatical framework is formulated using Type Theory with Records (Cooper, 2005). Type Theory with Records (TTR) is a logical formalism developed to cope with semantics of natural language. TTR is used to build a semantic ontology of abstract entities and events as well as to formalize the dialogue gameboard i.e. a formal representation of the dialogue context and its rules. The evolution of the conversation is formalized by means of update rules on the dialogue context. Ginzburg (2012) also accounts for NSUs and provides a set of dedicated rules.", "startOffset": 72, "endOffset": 497}, {"referenceID": 9, "context": "We will now briefly introduce the basic notions of the Type Theory with Records (TTR), with just enough detail needed by to understand the following sections, referring to Ginzburg (2012) for a complete description.", "startOffset": 172, "endOffset": 188}, {"referenceID": 9, "context": "At the basis of the grammatical framework of Ginzburg (2012) lies the notion of proposition.", "startOffset": 45, "endOffset": 61}, {"referenceID": 7, "context": "Following the definition of Fern\u00e1ndez (2006): Question = \u2206q \u2192 Prop The question domain \u2206q is a record type containing the wh-restrictors of the question q.", "startOffset": 28, "endOffset": 45}, {"referenceID": 9, "context": "In Ginzburg (2012), the dialogue context \u2013 also known as the Dialogue Gameboard (DGB) \u2013 is a formal representation that describes the current state of the dialogue.", "startOffset": 3, "endOffset": 19}, {"referenceID": 9, "context": "In this representation we abstracted away several details that would be included in the actual DGB presented by Ginzburg (2012) such as the fields to track who is holding the turn, the current time and so on.", "startOffset": 112, "endOffset": 128}, {"referenceID": 9, "context": "Ginzburg (2012) develops a comprehensive theory of grounding but we do not include it in our work.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "Ginzburg (2012) keeps track of the history of the dialogue within the variable Moves while a reference to the latest (illocutionary) proposition is recorded in the field LatestMove.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "In the previous section we detailed a logic-based model of dialogue from Ginzburg (2012). Another possible approach to dialogue modeling relies on probabilistic models to encode the variables and the dynamics of the dialogue context.", "startOffset": 73, "endOffset": 89}, {"referenceID": 9, "context": "In the previous section we detailed a logic-based model of dialogue from Ginzburg (2012). Another possible approach to dialogue modeling relies on probabilistic models to encode the variables and the dynamics of the dialogue context. Arguably this approach can be considered more robust to the intrinsic randomness present in dialogue. This is partially the reason why we explored this strategy as well as other advantages that will be discussed in Chapter 4. We based our work on the probabilistic rules formalism developed by Lison (2012). This formalism is particularly suited for our purpose because of their commonalities with the update rules described in Section 2.", "startOffset": 73, "endOffset": 541}, {"referenceID": 19, "context": "Lison (2014) details the rules and update procedure.", "startOffset": 0, "endOffset": 13}, {"referenceID": 22, "context": "The probabilistic rules formalism has also been implemented into a framework called OpenDial (Lison and Kennington, 2015).", "startOffset": 93, "endOffset": 121}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context.", "startOffset": 71, "endOffset": 101}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task.", "startOffset": 71, "endOffset": 471}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task. Fern\u00e1ndez (2006) describes a set of NSU resolution rules rooted in a TTR representation of the dialogue context.", "startOffset": 71, "endOffset": 551}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task. Fern\u00e1ndez (2006) describes a set of NSU resolution rules rooted in a TTR representation of the dialogue context. Section 2.2 briefly described the TTR notions we employed as well as the dialogue context theory based on TTR from Ginzburg (2012). At last we described the probabilistic modeling of dialogue from Lison (2014) based on the probabilistic rules formalism.", "startOffset": 71, "endOffset": 778}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task. Fern\u00e1ndez (2006) describes a set of NSU resolution rules rooted in a TTR representation of the dialogue context. Section 2.2 briefly described the TTR notions we employed as well as the dialogue context theory based on TTR from Ginzburg (2012). At last we described the probabilistic modeling of dialogue from Lison (2014) based on the probabilistic rules formalism.", "startOffset": 71, "endOffset": 857}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task. Fern\u00e1ndez (2006) describes a set of NSU resolution rules rooted in a TTR representation of the dialogue context. Section 2.2 briefly described the TTR notions we employed as well as the dialogue context theory based on TTR from Ginzburg (2012). At last we described the probabilistic modeling of dialogue from Lison (2014) based on the probabilistic rules formalism. As we mentioned in Chapter 1 this formalism is the framework for our formulation of the NSU resolution rules on the basis of the one developed by Fern\u00e1ndez (2006). We described in Section 2.", "startOffset": 71, "endOffset": 1064}, {"referenceID": 7, "context": "As demonstrated by Fern\u00e1ndez et al. (2007), we can use machine learning techniques to automatically classify a given NSU, using the annotated corpus as training data.", "startOffset": 19, "endOffset": 43}, {"referenceID": 7, "context": "As demonstrated by Fern\u00e1ndez et al. (2007), we can use machine learning techniques to automatically classify a given NSU, using the annotated corpus as training data. Fern\u00e1ndez et al. (2007) is our main theoretical reference and (to our knowledge) the state-of-the-art in performance for the task of classification of NSUs.", "startOffset": 19, "endOffset": 191}, {"referenceID": 7, "context": "The corpus from Fern\u00e1ndez et al. (2007) contains 1 283 annotated NSU instances, each one identified by the name of the containing BNC file and their sentence number, a sequential number to uniquely identify a sentence in a dialogue transcript.", "startOffset": 16, "endOffset": 40}, {"referenceID": 7, "context": "The corpus from Fern\u00e1ndez et al. (2007) contains 1 283 annotated NSU instances, each one identified by the name of the containing BNC file and their sentence number, a sequential number to uniquely identify a sentence in a dialogue transcript. The instances are also tagged with the sentence number of their antecedent which makes up the context for the classification. The raw utterances can be retrieved from the BNC using this information. For the classification task, we make the same simplifying restriction on the corpus made by Fern\u00e1ndez et al. (2007), that is to consider only the NSUs whose antecedent is their preceding sentence.", "startOffset": 16, "endOffset": 559}, {"referenceID": 1, "context": "The British National Corpus (Burnard, 2000) \u2013 BNC for short \u2013 is a collection of spoken and written material, containing about 100 million words of (British) English texts from a large variety of sources.", "startOffset": 28, "endOffset": 43}, {"referenceID": 8, "context": "The dialogues are structured following the CLAWS tagging system (Garside, 1993) which segmented the utterances both at word and sentence level.", "startOffset": 64, "endOffset": 79}, {"referenceID": 16, "context": "The word units contains both the raw text, the corresponding lemma (headword) and the POS-tag according to the C5 tagset (Leech et al., 1994).", "startOffset": 121, "endOffset": 141}, {"referenceID": 7, "context": "The former are used mainly as a comparison with Fern\u00e1ndez et al. (2007) which employ this algorithm as well.", "startOffset": 48, "endOffset": 72}, {"referenceID": 33, "context": "In information theory, the entropy (Shannon, 1948) is the expected value of information carried by a message (or an event in general).", "startOffset": 35, "endOffset": 50}, {"referenceID": 7, "context": "Our baseline is set to be the replicated approach of the classification experiments carried out by Fern\u00e1ndez et al. (2007), which is our reference work for our study.", "startOffset": 99, "endOffset": 123}, {"referenceID": 30, "context": "ant mood As defined by Rod\u0155\u0131guez and Schlangen (2004), this feature was though to distinguish between declarative and non-declarative antecedent sentences.", "startOffset": 23, "endOffset": 54}, {"referenceID": 14, "context": "These features were extracted through the use of the Stanford PCFG parser (Klein and Manning, 2003) on the utterances.", "startOffset": 74, "endOffset": 99}, {"referenceID": 14, "context": "These features were extracted through the use of the Stanford PCFG parser (Klein and Manning, 2003) on the utterances. Refer to Marcus et al. (1993) for more information about the tag set used for the English grammar.", "startOffset": 75, "endOffset": 149}, {"referenceID": 25, "context": "lcs A feature to express the longest common subsequence at the word-level between the NSU and its antecedent, computed using a modified version of the Needleman\u2013Wunsch algorithm (Needleman and Wunsch, 1970), tailored to account for words instead of characters.", "startOffset": 178, "endOffset": 206}, {"referenceID": 18, "context": "Even though it is still a young research field, semi-supervised learning has already found many fields of application (Liang, 2005; Bergsma, 2010).", "startOffset": 118, "endOffset": 146}, {"referenceID": 0, "context": "Even though it is still a young research field, semi-supervised learning has already found many fields of application (Liang, 2005; Bergsma, 2010).", "startOffset": 118, "endOffset": 146}, {"referenceID": 32, "context": "While there has been some previous work towards using machine learning techniques for the detection of the antecedent of NSUs in multi-party dialogue (Schlangen, 2005), we consider sufficient the amount of unlabeled data we can extract following the previous rule.", "startOffset": 150, "endOffset": 167}, {"referenceID": 7, "context": "Nevertheless, as proved in the corpus study of Fern\u00e1ndez (2006), the percentage of the utterances whose antecedent is not the preceding utterance is rather low.", "startOffset": 47, "endOffset": 64}, {"referenceID": 7, "context": "It is still considered an NSU according to the definition of Fern\u00e1ndez (2006).", "startOffset": 61, "endOffset": 78}, {"referenceID": 33, "context": "We will not go into mathematical details so we recommend the interested reader to Vapnik (1998), Collobert et al.", "startOffset": 82, "endOffset": 96}, {"referenceID": 2, "context": "We will not go into mathematical details so we recommend the interested reader to Vapnik (1998), Collobert et al. (2006).", "startOffset": 97, "endOffset": 121}, {"referenceID": 17, "context": "The particular active learning algorithm we employed in our experiments is a pool-based method5 with uncertainty sampling (Lewis and Catlett, 1994).", "startOffset": 122, "endOffset": 147}, {"referenceID": 7, "context": "As in Fern\u00e1ndez et al. (2007), we evaluate our system in a 10-fold cross-validation fashion.", "startOffset": 6, "endOffset": 30}, {"referenceID": 7, "context": "As in Fern\u00e1ndez et al. (2007), we evaluate our system in a 10-fold cross-validation fashion. Weka\u2019s J48 algorithm was used as a comparing classifier. Thanks to the analysis of the resulting trees, we managed to imitate quite closely the behavior of their system as well as reaching a very close performance overall. Even though we use the same feature set and the same algorithm the performance parameters turn out to be slightly lower than the ones claimed in Fern\u00e1ndez et al. (2007). That might be for a variety of reasons, for instance the way feature were extracted or how the parameters were tuned.", "startOffset": 6, "endOffset": 485}, {"referenceID": 7, "context": "4: Performances comparison between Fern\u00e1ndez et al. (2007) and our replica.", "startOffset": 35, "endOffset": 59}, {"referenceID": 7, "context": "This task is formulated as a machine learning problem and we follow and extend the work of Fern\u00e1ndez et al. (2007). We use their corpus as a goldstandard and a replica of their approach as a baseline.", "startOffset": 91, "endOffset": 115}, {"referenceID": 6, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012).", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012).", "startOffset": 98, "endOffset": 112}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge.", "startOffset": 98, "endOffset": 154}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous.", "startOffset": 98, "endOffset": 685}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.", "startOffset": 98, "endOffset": 938}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.", "startOffset": 98, "endOffset": 1131}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006).", "startOffset": 98, "endOffset": 1316}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006). We reinterpreted the variables in the dialogue state as random variables and straightforwardly \u201cconverted\u201d the resolution rules into probabilistic rules.", "startOffset": 98, "endOffset": 1337}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006). We reinterpreted the variables in the dialogue state as random variables and straightforwardly \u201cconverted\u201d the resolution rules into probabilistic rules. In the next sections we explain how we represented the variables in the dialogue state and how we translated the rules from Fern\u00e1ndez (2006) into probabilistic rules.", "startOffset": 98, "endOffset": 1633}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006). We reinterpreted the variables in the dialogue state as random variables and straightforwardly \u201cconverted\u201d the resolution rules into probabilistic rules. In the next sections we explain how we represented the variables in the dialogue state and how we translated the rules from Fern\u00e1ndez (2006) into probabilistic rules. First we describe the theoretical aspects from Ginzburg (2012) we employ in our approach.", "startOffset": 98, "endOffset": 1722}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006). We reinterpreted the variables in the dialogue state as random variables and straightforwardly \u201cconverted\u201d the resolution rules into probabilistic rules. In the next sections we explain how we represented the variables in the dialogue state and how we translated the rules from Fern\u00e1ndez (2006) into probabilistic rules. First we describe the theoretical aspects from Ginzburg (2012) we employ in our approach. We present then the design of our dialogue context and the rules to resolve the NSUs. We surely take a much simpler approach than Ginzburg (2012) in the modeling of the dialogue state, abstracting intentionally from many details that would add complexity to the modeling.", "startOffset": 98, "endOffset": 1895}, {"referenceID": 9, "context": "We developed a dialogue system able to update the dialogue state probabilistically with update rules similar to the ones from Ginzburg (2012). The system is also able to resolve toy examples in an interactive way.", "startOffset": 126, "endOffset": 142}, {"referenceID": 7, "context": "As previously stated, we rely on Fern\u00e1ndez (2006) and Ginzburg (2012) for the theoretical notions needed to represent the dialogue state and to develop the NSU resolution rules.", "startOffset": 33, "endOffset": 50}, {"referenceID": 7, "context": "As previously stated, we rely on Fern\u00e1ndez (2006) and Ginzburg (2012) for the theoretical notions needed to represent the dialogue state and to develop the NSU resolution rules.", "startOffset": 33, "endOffset": 70}, {"referenceID": 7, "context": "As previously stated, we rely on Fern\u00e1ndez (2006) and Ginzburg (2012) for the theoretical notions needed to represent the dialogue state and to develop the NSU resolution rules. In Section 2.2 we detailed the basic concepts of TTR, the utterance representation and the update rules for the dialogue state. In this section we describe the notions needed for the resolution of the NSUs. In particular we describe how we can exploit the parallelism between the NSU and its antecedent that we mentioned in Section 2.1. We discuss here the concepts that Ginzburg (2012) defines to address the resolution of NSUs then we will describe how we adapt those concepts to our needs in the next section.", "startOffset": 33, "endOffset": 565}, {"referenceID": 9, "context": "In the theory of Ginzburg (2012), this concept is named Partial Parallelism2.", "startOffset": 17, "endOffset": 33}, {"referenceID": 9, "context": "2) (rephrased from Ginzburg (2012)).", "startOffset": 19, "endOffset": 35}, {"referenceID": 8, "context": "For this reason we employ the notion of focus establishing constituents (FEC) from the theory of Ginzburg (2012)3.", "startOffset": 97, "endOffset": 113}, {"referenceID": 7, "context": "b: Who? The concept was previously formalized by Fern\u00e1ndez (2006) as topical constituents.", "startOffset": 49, "endOffset": 66}, {"referenceID": 9, "context": "In this we follow Ginzburg (2012), who defines a set of rules to follow to make FECs contextually available.", "startOffset": 18, "endOffset": 34}, {"referenceID": 7, "context": "As argued in Fern\u00e1ndez (2006), understanding does not always imply acceptance, and Plain Acknowledgments are ambiguous in this distinction.", "startOffset": 13, "endOffset": 30}, {"referenceID": 8, "context": "As in Ginzburg (2012) we consider only unary wh-interrogatives.", "startOffset": 6, "endOffset": 22}, {"referenceID": 7, "context": "Refer to Fern\u00e1ndez (2006) for an account of utterances with multiple wh-interrogatives", "startOffset": 9, "endOffset": 26}, {"referenceID": 7, "context": "To formalize the meaning of Sluices, Fern\u00e1ndez et al. (2007) distinguish four types of Sluices that convey different meanings: Direct Sluices, Reprise Sluices, Clarification Sluices, Whanaphor.", "startOffset": 37, "endOffset": 61}, {"referenceID": 7, "context": "To formalize the meaning of Sluices, Fern\u00e1ndez et al. (2007) distinguish four types of Sluices that convey different meanings: Direct Sluices, Reprise Sluices, Clarification Sluices, Whanaphor. The aforementioned paper describes a machine learning experiment to automatically classify Sluices according to these types. Ginzburg (2012) describes several different treatments for every group of Sluices.", "startOffset": 37, "endOffset": 335}, {"referenceID": 8, "context": "The variables in the dialogue context are inspired by Ginzburg (2012). In order to make the transition from the rules of Fern\u00e1ndez (2006) to probabilistic rules as direct as possible, we mimic the basic dynamic of the DGB detailed in Section 2.", "startOffset": 54, "endOffset": 70}, {"referenceID": 7, "context": "In order to make the transition from the rules of Fern\u00e1ndez (2006) to probabilistic rules as direct as possible, we mimic the basic dynamic of the DGB detailed in Section 2.", "startOffset": 50, "endOffset": 67}, {"referenceID": 9, "context": "The set of dialogue acts we employ in our dialogue context is a small subset of the ones defined by Ginzburg (2012): \u2022 Assert, denoting the act of asserting a proposition; \u2022 Ask, denoting the act of posing a question; \u2022 Ground, denoting the act of understanding what being previously said; \u2022 Accept, denoting the act of accepting what being previously said.", "startOffset": 100, "endOffset": 116}, {"referenceID": 9, "context": "Despite being represented as the maximal element of QUD in Ginzburg (2012), here max-qud actually denotes the index of such element which therefore is retrieved in this way: qud[max-qud].", "startOffset": 59, "endOffset": 75}, {"referenceID": 9, "context": "Despite being represented as the maximal element of QUD in Ginzburg (2012), here max-qud actually denotes the index of such element which therefore is retrieved in this way: qud[max-qud]. In Ginzburg (2012), MaxQUD is given from the partial ordering imposed on QUD.", "startOffset": 59, "endOffset": 207}, {"referenceID": 7, "context": "Since they are a (almost) direct translation of the deterministic rules from Fern\u00e1ndez (2006), most of them have deterministic effects (i.", "startOffset": 77, "endOffset": 94}, {"referenceID": 7, "context": "For Repeated Acknowledgments, Fern\u00e1ndez (2006) requires to have co-referentiality between the repeated constituent in the NSU and the relative constituent in the FEC of MaxQUD.", "startOffset": 30, "endOffset": 47}, {"referenceID": 9, "context": "As argued in Ginzburg (2012), the antecedent of a Sluice can contain more than one potential FEC as exemplified by the following transcript.", "startOffset": 13, "endOffset": 29}, {"referenceID": 9, "context": "To resolve clarification requests and their elliptical variants, Ginzburg (2012) includes a general theory of grounding and clarification requests.", "startOffset": 65, "endOffset": 81}, {"referenceID": 9, "context": "To resolve clarification requests and their elliptical variants, Ginzburg (2012) includes a general theory of grounding and clarification requests. This theory would add a non-trivial amount of complexity to our formalization so we shall assume in our formalization that the latest utterance always grounded unless a clarification request comes afterwards. Therefore we resolve Clarification Ellipsis on the MaxQUD element without adding any other structure to the dialogue state. We also consider the Clarification Ellipsis to have only a clausal confirmation reading, leaving aside their other possible readings which would require a more elaborate approach (more details in Ginzburg (2012)).", "startOffset": 65, "endOffset": 693}, {"referenceID": 19, "context": "As defined in Lison (2014), \u201ca spoken dialogue system is a computational agent that can converse with humans through everyday spoken language\u201d.", "startOffset": 14, "endOffset": 27}, {"referenceID": 23, "context": "Anaphora Resolution (Mitkov, 2014) and Named Entity Recognition (Nadeau and Sekine, 2007) are just a few of the problems concerning the correct interpretation of NSUs.", "startOffset": 20, "endOffset": 34}, {"referenceID": 24, "context": "Anaphora Resolution (Mitkov, 2014) and Named Entity Recognition (Nadeau and Sekine, 2007) are just a few of the problems concerning the correct interpretation of NSUs.", "startOffset": 64, "endOffset": 89}, {"referenceID": 21, "context": "We showed how to reformulate the rules from Fern\u00e1ndez (2006) using the probabilistic rules formalism (Lison, 2014) in order to include a probabilistic account of the dialogue state.", "startOffset": 101, "endOffset": 114}, {"referenceID": 22, "context": "The framework presented in this chapter has also been implemented and tested with OpenDial (Lison and Kennington, 2015).", "startOffset": 91, "endOffset": 119}, {"referenceID": 7, "context": "To do so, we relied on the previous work of Fern\u00e1ndez (2006) which presented a series of rules to resolve the meaning of the NSUs from a TTR-encoded dialogue context.", "startOffset": 44, "endOffset": 61}, {"referenceID": 7, "context": "To do so, we relied on the previous work of Fern\u00e1ndez (2006) which presented a series of rules to resolve the meaning of the NSUs from a TTR-encoded dialogue context. As previously argued throughout this thesis, the use of a purely logic-based formalism, such as TTR, has some disadvantages in dealing with partially observable inputs and stochastic events when compared to a probabilistic approach. We showed how to reformulate the rules from Fern\u00e1ndez (2006) using the probabilistic rules formalism (Lison, 2014) in order to include a probabilistic account of the dialogue state.", "startOffset": 44, "endOffset": 461}, {"referenceID": 7, "context": "To do so, we relied on the previous work of Fern\u00e1ndez (2006) which presented a series of rules to resolve the meaning of the NSUs from a TTR-encoded dialogue context. As previously argued throughout this thesis, the use of a purely logic-based formalism, such as TTR, has some disadvantages in dealing with partially observable inputs and stochastic events when compared to a probabilistic approach. We showed how to reformulate the rules from Fern\u00e1ndez (2006) using the probabilistic rules formalism (Lison, 2014) in order to include a probabilistic account of the dialogue state. We made use of a portion of the dialogue context theory from Ginzburg (2012) to encode the basic elements of the dialogue state needed for the resolution of the NSUs (see Section 2.", "startOffset": 44, "endOffset": 659}, {"referenceID": 7, "context": "1 the concept of non-sentential utterance, referring to Fern\u00e1ndez (2006) as our theoretical basis.", "startOffset": 56, "endOffset": 73}, {"referenceID": 7, "context": "1 the concept of non-sentential utterance, referring to Fern\u00e1ndez (2006) as our theoretical basis. From the aforementioned work we employ the same taxonomy and corpus of NSUs. We then explain our methodology for the interpretation of NSUs, also based on the theory from Fern\u00e1ndez (2006). To interpret an NSU, we first classify it according to the aforementioned taxonomy using machine learning then we \u201cresolve\u201d its meaning from the dialogue context through a resolution procedure dependent on its type.", "startOffset": 56, "endOffset": 287}, {"referenceID": 9, "context": "Fern\u00e1ndez (2006) develops a set of resolution procedures based on Type Theory with Records (Cooper, 2004; Ginzburg, 2012).", "startOffset": 91, "endOffset": 121}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) develops a set of resolution procedures based on Type Theory with Records (Cooper, 2004; Ginzburg, 2012). In Section 2.2 we briefly describe the aspects of TTR and the theory of dialogue context from Ginzburg (2012) that we needed in our work.", "startOffset": 92, "endOffset": 233}, {"referenceID": 19, "context": "To this end we employ the probabilistic rules formalism and the theory from Lison (2014) as probabilistic representation of the dialogue state and the NSU resolution procedures.", "startOffset": 76, "endOffset": 89}, {"referenceID": 19, "context": "To this end we employ the probabilistic rules formalism and the theory from Lison (2014) as probabilistic representation of the dialogue state and the NSU resolution procedures. We detail the basic aspects of the theory from Lison (2014) in Section 2.", "startOffset": 76, "endOffset": 238}, {"referenceID": 19, "context": "To this end we employ the probabilistic rules formalism and the theory from Lison (2014) as probabilistic representation of the dialogue state and the NSU resolution procedures. We detail the basic aspects of the theory from Lison (2014) in Section 2.3. In Lison (2014) the dialogue state is represented as a Bayesian network and its dynamics are described by probabilistic rules.", "startOffset": 76, "endOffset": 270}, {"referenceID": 6, "context": "Our work on the classification of NSUs is based on Fern\u00e1ndez et al. (2007). We replicated their approach and set it as our baseline, as explained in Section 3.", "startOffset": 51, "endOffset": 75}, {"referenceID": 4, "context": "In Dragone and Lison (2015a) we present our findings in the classification of NSUs using Active Learning.", "startOffset": 3, "endOffset": 29}, {"referenceID": 9, "context": "2 we describe the theoretical concepts we need from Ginzburg (2012). The description of our theory starts in Section 4.", "startOffset": 52, "endOffset": 68}, {"referenceID": 9, "context": "2 we describe the theoretical concepts we need from Ginzburg (2012). The description of our theory starts in Section 4.3 where we explained the design of the dialogue context. To model our dialogue context we take inspiration from Ginzburg (2012), however we reinterpret its constructs as random variables.", "startOffset": 52, "endOffset": 247}, {"referenceID": 6, "context": "We showed how we could reuse many concepts from the theory of Fern\u00e1ndez (2006) and Ginzburg (2012) and \u201ctranslate\u201d the resolution rules based on TTR into probabilistic rules.", "startOffset": 62, "endOffset": 79}, {"referenceID": 6, "context": "We showed how we could reuse many concepts from the theory of Fern\u00e1ndez (2006) and Ginzburg (2012) and \u201ctranslate\u201d the resolution rules based on TTR into probabilistic rules.", "startOffset": 62, "endOffset": 99}, {"referenceID": 4, "context": "The works on classification and interpretation carried out in this thesis were also presented in Dragone and Lison (2015b).", "startOffset": 97, "endOffset": 123}, {"referenceID": 6, "context": "Additional data for the gold standard should be composed of high-quality, manually annotated instances extracted within a corpus study that closely follows the original one from Fern\u00e1ndez and Ginzburg (2002).", "startOffset": 178, "endOffset": 208}, {"referenceID": 6, "context": "The corpus study from Fern\u00e1ndez and Ginzburg (2002) is focused on data extracted from the British National Corpus therefore confined to only certain kind of dialogue domains.", "startOffset": 22, "endOffset": 52}, {"referenceID": 6, "context": "The corpus study from Fern\u00e1ndez and Ginzburg (2002) is focused on data extracted from the British National Corpus therefore confined to only certain kind of dialogue domains. As argued by Raghu et al. (2015), there are many elliptical phenomena that do not fit well in the taxonomy from Fern\u00e1ndez and Ginzburg (2002).", "startOffset": 22, "endOffset": 208}, {"referenceID": 6, "context": "The corpus study from Fern\u00e1ndez and Ginzburg (2002) is focused on data extracted from the British National Corpus therefore confined to only certain kind of dialogue domains. As argued by Raghu et al. (2015), there are many elliptical phenomena that do not fit well in the taxonomy from Fern\u00e1ndez and Ginzburg (2002). An interesting follow up work on non-sentential utterances might try to find new elliptical phenomena in different dialogue Synthetic Minority Over-sampling Technique.", "startOffset": 22, "endOffset": 317}, {"referenceID": 29, "context": "It bares similarities with statistical approaches for the resolution such as Raghu et al. (2015). Their work is concentrated on follow-up NSU questions such as: (5.", "startOffset": 77, "endOffset": 97}, {"referenceID": 27, "context": ", 2007) and one of its extensions CLARIE (Purver, 2006).", "startOffset": 41, "endOffset": 55}, {"referenceID": 6, "context": "Another useful comparison, and perhaps integration, should be made with the systems originally developed on the theory of Fern\u00e1ndez (2006), namely SHARDS (Ginzburg et al.", "startOffset": 122, "endOffset": 139}, {"referenceID": 3, "context": "The latter is a dialogue system developed to deal with clarification requests and, among them, Clarification Ellipsis, implementing the theory of Ginzburg and Cooper (2004) on top of the GoDiS dialogue system (Larsson et al.", "startOffset": 159, "endOffset": 173}, {"referenceID": 3, "context": "The latter is a dialogue system developed to deal with clarification requests and, among them, Clarification Ellipsis, implementing the theory of Ginzburg and Cooper (2004) on top of the GoDiS dialogue system (Larsson et al., 2000). Both are based on the HPSG2 framework from Ginzburg and Sag (2000), which is substantially different from our current design.", "startOffset": 159, "endOffset": 300}, {"referenceID": 9, "context": "These rules are inspired by, but not limited to, Ginzburg (2012). Section 2.", "startOffset": 49, "endOffset": 65}, {"referenceID": 9, "context": "These rules are inspired by, but not limited to, Ginzburg (2012). Section 2.2 gives the background knowledge for the rules from Ginzburg (2012).", "startOffset": 49, "endOffset": 144}], "year": 2015, "abstractText": "Non-sentential utterances (NSUs) are utterances that lack a complete sentential form but whose meaning can be inferred from the dialogue context, such as \u201cOK\u201d, \u201cwhere?\u201d, \u201cprobably at his apartment\u201d. The interpretation of non-sentential utterances is an important problem in computational linguistics since they constitute a frequent phenomena in dialogue and they are intrinsically context-dependent. The interpretation of NSUs is the task of retrieving their full semantic content from their form and the dialogue context. NSUs also come in a wide variety of forms and functions and classifying them in the right category is a prerequisite to their interpretation. The first half of this thesis is devoted to the NSU classification task. Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012). We argue that logic-based formalisms, such as TTR, have a number of shortcomings when dealing with conversational data, which often include partially observable knowledge and nondeterministic phenomena. An alternative to address these issues is to rely on probabilistic modeling of the dialogue context. Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism (Lison, 2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation. However, the probabilistic rules can also encode probabilistic knowledge, thereby providing a principled account of ambiguities in the NSU resolution process. In the second part of this thesis, we present our proof-of-concept framework for NSU resolution using probabilistic rules.", "creator": "LaTeX with hyperref package"}}}