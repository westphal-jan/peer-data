{"id": "1511.07373", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "What is the plausibility of probability?(revised 2003, 2015)", "abstract": "We present that scrutinize a either resulted to extent reasoning, namely that for furthermore topicality sphere of Cox ' / type whatever more extremely embedded last setting minimal ordered field. This, early a complicated metaphysics spite, will be reportedly would imply that more rational measurement one n't although uncertainty hold take companies from setting of extended probability distributions, where beginning probability is standard formula_18 since having infinitesimals.", "histories": [["v1", "Mon, 23 Nov 2015 19:24:17 GMT  (29kb)", "http://arxiv.org/abs/1511.07373v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["stefan arnborg", "gunnar sj\\\"odin"], "accepted": false, "id": "1511.07373"}, "pdf": {"name": "1511.07373.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["stefan@nada.kth.se", "sjodin@sics.se"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n07 37\n3v 1\n[ cs\n.A I]\n2 3\nN ov\nThis claim must be supported by some argumentation of non-mathematical type, however, since pure mathematics does not tell us anything about the world. We propose one such argumentation, and relate it to results from the literature of uncertainty and statistics.\nIn an added retrospective section we discuss some developments in the area regarding countable additivity, partially ordered domains and robustness, and philosophical stances on the Cox/Jaynes approach since 2003. We also show that the most general partially ordered plausibility calculus embeddable in a ring can be represented as a set of extended probability distributions or, in algebraic terms, is a subdirect sum of ordered fields. In other words, the robust Bayesian approach is universal. This result is exemplified by relating Dempster-Shafer\u2019s evidence theory to robust Bayesian analysis.\nKeywords. belief, plausibility, probability, Bayesianism"}, {"heading": "1 Introduction", "text": "We consider plausibility spaces of the type defined by Cox[13], with auxiliary functions F , S and G used for computing plausibilities of conjunctions, complements and disjunctions, respectively. The domain of plausibility values with the auxiliary functions and some or-\ndering relation \u2264 is called a plausibility space1, where a \u2264 b means that an event with plausibility value a is equally or less plausible than one with the value b. A system of conditional propositions or events with plausibility values is a plausibility model 2. Standard probability is one possible plausibility space, where the domain is the real numbers in [0, 1] and the functions F and G are multiplication and addition, respectively. Extended probability is standard probability extended with infinitesimal probabilities.\nWe will motivate a definition, prove two theorems and justify a claim. Definition 1 below of proper ordered plausibility spaces and models lists a number of properties we assume for the domain of plausibility values and the combination functions F and G used to find plausibilities of conjunctions and disjunctions when we have the plausibilities of their operands. These properties are similar to those assumed by Acze\u0301l and Cox. In particular, F and G satisfy the algebraic laws of the operators \u00d7 and + of a ring and have certain monotonicity properties.\nTheorem 1 says that a proper ordered plausibility space can be embedded in an ordered field where the field operators\u00d7 and + are extensions of F andG. An ordered field is a ring where certain monotonicity and solvability properties hold, the best known examples being the fields of reals R and rationals Q . But there are more complex ordered fields that may be relevant and which we will discuss.\nIn order to motivate Definition 1, we investigate several assumptions, some old and some new, which state desirable properties of a plausibility space. We argue for the acceptance of a number of these assumptions, which we call Sufficiency, Monotonicity, Propositional limit, Refinability and Closedness.\n1We use the term space instead of measure or algebra in order to avoid confusion with terminology in related work.\n2Model is one of several possible terms; it suggests the analoguous use of models in logic.\nTheorem 2 says that a plausibility space satisfying the above assumptions is a proper ordered plausibility space as defined in Definition 1. Our claim is:\nClaim 1 Theorems 1 and 2 imply that the most general rational way to deal with uncertainty, under the assumptions made, is the Extended Bayes\u2019 method, where uncertainty is completely described with an extended probability distribution. With the additional Robustness assumption, uncertainty is completely described with a set of such distributions.\nThere are many results of the type above in the 20th century literature on probability and uncertainty. Nevertheless, there is no consensus among researchers in uncertainty management that Claim 1 is even approximately valid. The Claim is of obvious interest as a foundational issue. But interest is not confined to the ivory tower: Designers of future complex systems that need new types of humanagent and agent-agent interactions struggle with this question, and fielded solutions have already unraveled difficult compatibility problems, e.g., when advanced system components using different, sometimes incompatible, unknown or irrational ways to describe uncertainty are put together to form systems of systems. If Claim 1 above can be accepted, this task becomes easier, since questions can center around requirements on useful probability models rather than comparing disparate schemes of uncertainty management.\nThe auxiliary argumentation required to support our claims is similar to inference principles in statistics, like the Sufficiency, Likelihood and Conditionality principles[39]. Such principles have only a pragmatic validity, meaning that their acceptability may be questioned in new applications different from those conceived when they were first accepted. We will state a number of assumptions made in derivations of this kind, and they are indicated in bold face. Those that will eventually be retained in this study are marked with an asterisk (*).\nIn section 2 we review the basic features of plausibility spaces and state a number of common assumptions used to analyze them. In section 3 we review the two basic methods used to derive probability as canonical uncertainty measure, coherence arguments and Cox consistency based argument. In section 4 we discuss a number of fundamental principles that are also required but which are often hastily glossed over: these principles have the common feature that they say that inferences possible should not depend in an arbitrary manner on how our problem is embedded in a larger context. This discussion leads to Definition 1. The consequences of Theorem 1 are discussed in section 5. In section 6 we discuss extended probability.\nTheorems are proved in the appendix. The scientific contribution claimed is the concept of embeddability of the auxiliary functions in an ordered field, and its proofs. These are somewhat more complex than we hoped, mainly because the function G is a partial function on the domain of plausibility values.\nThere are two papers that analyze a similar question and lead to different but related and compatible conclusions, namely Kraft et al[27] and Hardy[22]."}, {"heading": "2 Symbols and Plausibilities.", "text": "The methods of definition and induction are attributed, by Aristotle, to Socrates[46], and were developed further by Aristotle and the Stoic philosophers. Although present English translations of Aristotle contain words like symbol, utility and probability, it is clear that Aristotle was not very interested in mathematics and does not give quantitative models of decision problems. So his terms do not reflect a detailed and precise understanding of modern Bayesian decision theory. In particular, the modern concept of probability was certainly not known at the time. Nevertheless, Aristotle is the first to give a surviving qualitative version of the Bayesian decision making principle in his Nichomachean ethics[24]: \u201c Find out what you think is a good life, and consider the probabilities of your possible actions to achieve this. Then follow the course of action which with highest probabibility results in a good life.\u201d\nTwo thousand years later, Thomas Bayes was the first to apply the newly invented probability calculus to an inference problem. In his posthumous essay he computes the posterior probability for the probability of heads in tossing a coin, with respect to observed outcomes and under the assumption that tosses are independent and identically distributed, and that the prior probability of getting a head is uniformly distributed between 0 and 1. Somewhat later, Laplace gave a more satisfactory analysis of the Bayesian method.\nOur modern plausibility models assume a set of events, or statements, or possible world sets A, B, C, etc., which in an application will correspond to conditions existing in the world or in the minds of humans and agents. This step is completely standard. Many theories of plausibility go on to define conditional event plausibilities, which in many cases are written using the probability inspired notation A|B - the plausibility of A given that we know B to be true3. This is the model in which foundational stud-\n3A|B is thus the plausibility, not the conditional event. It is sometimes written pl(A|B)\nies are usually made, and which we use here.\nThere are essentially two different methods that were used to analyse possible plausibility spaces for consistency. The more common is based on a gambling analogy: plausibilities are assumed analog to betting odds, and a situation where a gambler can pick bets on a set of related events to hedge completely the risk of loosing money is called incoherent. The plausibility assignment leading to such a situation is considered incoherent, and under various assumptions several arguments exist that end up in concluding that probability or a set of probability distributions is the only coherent plausibility space [41, 28, 15, 45]. We will use here the alternative approach devised by Cox[13] which avoids the betting scenario and instead analyses how plausibilities of combined events can be defined, and what properties the plausibility combination functions must have in order to avoid that an events plausibility will depend on which of several possible derivations is used.\nCox[13] assumes that the domain of plausibility values is an interval of real values, which without loss of generality can be assumed to be [0, 1], with 0 for falsity and 1 for truth. He furthermore finds that two functions F and S must exist such that A \u2227 B|C = F (A|B \u2227 C,B|C) and \u00acA|C = S(A|C). With a number of \u2013 sometimes implicit \u2013 regularity assumptions, he shows that the domain of plausibility values can be rescaled so that F is tranformed to multiplication, and S to the function S such that S(x) = 1 \u2212 x. In other words, if his assumptions are accepted, plausibility models must be equivalent to probability models. We can state an assumption made implicitly by Cox here (the function G was introduced by Acze\u0301l[1] in a related investigation):\nSufficiency assumption*: The plausibility value of a proposition is a sufficient characterization of the statement with respect to uncertainty and propositional connectives, i. e., there are functions F and S, and a partial function G such that A \u2227 B|C = F (A|BC,B|C), A|C = S(A|C) and A \u2228 B|C = G(A|C,B \u2212A|C).\nThere has been some concern with alternative interpretations of symbols used. In rough and fuzzy set theory[37, 51], the meanings of symbols are taken to be ambiguous, in the sense that even with full information we cannot definitely say, e.g., whether or not A obtains or if x is a member of set X . The ambiguity problem has in many articles been claimed to be an obstacle to probabilistic uncertainty management[49]. An alternative way is to see ambiguity as an aspect that should go into the structure of a probabilistic model. So instead of introducing A as an objective\nfuzzy set, we can introduce the judgments made by various agents and decision makers as A|Ci for the judgment made e.g., by agent i based on the information and background factors available to agent i. This makes fuzziness and plausibility orthogonal concepts, and both can be present in an application. Those who focus on impreciseness assume that this is the most prominent characteristic of their applications, while those focusing on plausibility do not think so.\nWhich is the domainD of plausibility values? Fine[17, Ch 1] lists a number of theories, which correspond to different order structures of the domain of plausibility values. In short, the domain can be discrete (with true/false as the case equivalent to propositional logic), ordered, continuous or partially ordered. All applications of plausibility in real systems aim at decision making, and for this reason we must eventually be able to say that one event is more plausible than another one. For this reason it is essential to have some order structure in the domain, and we rule out completely unordered domains like the complex numbers. This means that the relation \u2019more plausible than\u2019 is transitive. This is not necessarily the case in practical decision making but because of psychological effects like framing, unstable preferences or computational limitations of the brain. Our study aims at prescriptive theories so this problem will be ignored: the domain of plausibility values is always (at least) partially ordered and usually linearly ordered.\nThere are several uncertainty management methods that make use of partially ordered domains of plausibility values, typically in the form of intervals of real numbers.\nRobustness assumption*: A plausibility space with a partial order relation on the domain of plausibility values, describing the relation \u2019more plausible than\u2019, is definable using indexed sets of plausibility spaces each having a linearly ordered domain of plausibility values. In such a space, an event e1 is more plausible than e2 if and only if it is more plausible in each constituent indexed space.\nWe have not yet a compelling argument for the robustness assumption4 \u2013 existing proposals have rather large gaps. A possible approach is to develop a general theory of partially ordered plausibility spaces. Until this has been done, we assume robustness and consequently analyze models with a linearly ordered domain D of plausibility values.\nMonotonicity assumption*: The domain of plausibility is ordered, S is decreasing5, and F and G are\n4see however the restrospective section 8.2 5Decreasing is a stronger condition than nonincreasing. The assumption could also have been called\nincreasing in each argument (in the case of F if the other argument is non-\u22a5).\nBy considering the limiting case of propositional logic we also find a number of constraints based on the requirement that reasoning with entirely false or true events should follow the rules of propositional logic. We omit a number of rules which will follow from the algebraic rules of auxiliary functions F and G that we will soon adopt:\nPropositional limit assumption*: G(x,\u22a5) = x, F (\u22a5, x) = \u22a5, F (x,\u22a4) = x, and S(\u22a5) = \u22a4.\nAn immediate consequence of Monotonicity and Propositional limit is F (x, y) \u2264 min(x, y) and G(x, y) \u2265 max(x, y)."}, {"heading": "3 Coherence, or Dutch Book avoidance", "text": "We have now stated the common assumptions of most existing theories of plausibility[17, 15, 13, 41]. But more is required before probability appears as inevitable in a form we recognize. What is required is some means to derive constraints on sets of plausibilities of different conditional statements. Without such constraints we can easily define plausibility spaces that are not equivalent to probability. There are basically two ways to proceed. One followed by de Finetti is to construct a gambling situation where different plausibilities are tied together in composite bets offered by a bookmaker according to his beliefs. A bookmaker who offers a set of bets among which a gambler can choose a combination which gives positive payoff in every situation seems to have made his bets from a globally incoherent set of beliefs. Several papers show, with important differences in detail, that every coherent belief set is equivalent to a probability model[15, 13, 41, 29]. The coherence concept can also be applied to some domains not totally but partially ordered, and Goodman, Ngyuen and Rogers[19] show that some plausibilty measures not coherent under the total order assumption, among others the standard version of DS-theory, are coherent under the latter assumption.\nAnother way to connect different plausibilities is via propositional logic connectives, and this gives us some constraints on the auxiliary functions F , S and G used in Cox\u2019s approach. As an example, the rule of associativity for conjunction enforces F to be associative for certain arguments, namely, from\nstrict monotonicity. Despite the subtle distinction, the weaker assumption would lead to completely different conclusions[14] since non-strict monotonicity does not entail cancellation laws.\nABC|D = F (AB|CD,C|D) = F (A|BCD,BC|D) and x = A|BCD, y = B|CD and z = C|D, we can derive\nF (x, F (y, z)) = F (F (x, y), z). (1)\nIn other words, under some circumstances associativity is inherited by F from the associativity of conjunction. If we want to show that every good plausibility space must be rescalable to probability, we must prove that F and G satisfy certain algebraic laws that are satisfied by \u00b7 and + and invariant under rescaling - the laws of associativity, commutativity and distributivity satisfied by the operators of a ring. As an example, a model is discussed by Halpern[20] that has only a few events. Thus the domain of plausibility values is finite and associativity and differentiability of F does not seem inevitable. In order to claim that the auxiliary function must be associative, we must either claim that no sensible person would deny it, which is difficult, or find some type of argument that goes outside the finite example. In [13], it is plainly assumed (as pointed out in [47]) that the auxiliary function F has certain regularity properties:\nAssociativity and differentiability assumption: The function F is defined on [0, 1]2, and is associative and twice continously differentiable; the auxiliary function S is continuously differentiable and S(S(x)) = x\nThe differentiability part is implicit \u2013 as was common when Cox wrote his paper \u2013 but is a standard assumption made to justify switching the order of differentiation which occurs in the derivations of [13], and also in [25]."}, {"heading": "4 Embeddability, Denseness,", "text": "Refinability.\nThe argumentation reviewed in section 3 must be complemented by some arguments that bind together plausibilities of events that are not connected by propositional identities. In Cox\u2019s work, this is effected by regularity assumptions: the auxiliary functions F and S are assumed to be defined on an interval of real numbers, and obey the associativity and differentiability assumption on this interval. Implicit in this assumption seems to be the idea that the functions F and S are universal, and every plausibility problem should be solvable with the same functions F and S. In the criticisms of Cox\u2019s work, this assumption has either been assumed not to exist[20, 21] or been ignored[36]. The statistics based derivations of probability as universal uncertainty measure often mention similar assumptions, but surprisingly often\nthese are somewhat glossed over, the main exceptions being [9, 17, 41, 49]. We make a sketchy review of some of the assumptions that have been proposed. In the prevision-based analyses, the first methods were based on assuming that an arbitrarily fine partitioning of the events space exists; this allows one to deduce the existence of a probability measure based on a relation \u2264 on events with the meaning A \u2264 B if A is equally or less plausible than B.\nUniform partition assumption[15]: For every event B, and every number n, there is a partition of B into n equally plausible events.\nAlmost uniform partition assumption[41, Ch 3]: For every event B, and every number n, there is a partition of B into n events, such that for r = 1, . . . , n\u22121, the union of any r elements of the partition is less plausible than the union of r + 1 members.\nStandard events and precise measurements assumptions[9, Ch. 2.3, Ax. 4,5]: For every conditional event there exists a standard event with the same plausibility, and for every number r \u2208 [0, 1] there is a standard event with probability r.\nEmbeddability assumption[49]: Inference should not depend on how the events participating in calculations are embedded in a larger event plausibility system.\nAn alternative assumption used to derive Cox\u2019s result is given by Paris[36]:\nDenseness assumption[36]: For all real values x, y, z and each \u01eb > 0, there are events A, B, C such that |(A|BCD)\u2212 x|, |(B|CD)\u2212 y| and |(C|D)\u2212 z| are all less than \u01eb.\nThis assumption might be more motivated than Cox\u2019s original assumptions, although it does not seem to be weaker. In response to [20], we developed very weak assumptions binding the different parts of a system of events together[4, 5].\nRefinability assumption*[4]: In a plausibility model with a conditional event of plausibility p, it must be possible to introduce a new subcase B of a non-false event A with plausibility value p given to B|A. This means that it should also be possible to define a new subcase of an event and get a new model which is equivalent to the original one as long as no reference is made to the new event. If two new subcases B and B\u2032 of an event A are defined in this way, they can be specified to be information independent, i.e., B|B\u2032A = B|A, B\u2032|BA = B\u2032|A. For two plausibility values x, y such that x < S(y), it should be possible to define two new subcases C, C\u2032 of any non-false event A such that x = C|A, y = C\u2032|A and\nC \u2227 C\u2032|A = \u22a5.\nThese augmentations of the model are called refinements. A refinement just introduces an already existing plausibility value in \u2019another part\u2019 of the model. This means that there is no assumption of a \u2019dense domain\u2019 or that a given new plausibility value can be introduced. The motivation for refinability is that the same cognitive or other process that resulted in a particular plausibility value for one event can always be mirrored in another part of the model to produce the same plausibility value for another event. If this process introduces an inconsistency, then we feel that there is a basic shortcoming of the model, and we should not accept it. We claim that this assumption is weaker than those referenced above in this section, and also more motivated. This is the weakest condition we have found that ensures that the auxiliary functions F , G and S have enough algebraic properties \u2013 associativity, symmetry(i.e., commutativity for corresponding binary operators) and distributivity properties of + and \u00b7 in a ring \u2013 to ensure that the main theorem follows. Somewhat surprisingly, Cox\u2019s result on rescalability of the plausibility space to standard probability follows from refinability and the assumptions in section 2 for models that have a finite number of plausibility values and an ordered plausibility space[4]. Even more surprisingly, the same result cannot be proved for infinite plausibility spaces (non-provability follows from a counter-example[5]). Two responses to this are possible: either more assumptions are required, or the result would not be right. We choose the second alternative. The reason is that we want to extend probability to extended probability, where infinitesimal probability values are allowed. This idea goes back to Adams[2] and has been found to give a very basic uncertainty management that seems to incorporate many uncertainty calculi as special cases (several are analyzed using extended but not necessarily Bayesian probability in [7]). The combination of robust and extended probability has been analyzed by Wilson[50]. Our motivation for accepting extended probability is the existence of infinite plausibility models that are refinable and embedded in the real numbers, but whose plausibility spaces are not rescalable to standard probability[5].\nIn order to derive extended probability as a canonical plausibility space, we must assume that models can be refined to the limit:\nClosedness assumption*[5]: The functions F , S and G have the following additional properties:\nF : D \u00d7D \u2192 D, S : D \u2192 D, E = {(x, y) \u2208 D \u00d7D : x \u2264 S(y)} and G : E \u2192 D.\nIn other words, the functions F and S are total while G(x, y) is defined when x \u2264 S(y). This requirement is similar (but not equivalent) to Kolmogorov\u2019s insistence that a probability space is defined on a \u03c3algebra. Refinability would imply that F and G must be associative and symmetric, and also that F must distribute overG on the domain of plausibilities (since G is a partial function, we must moderate its laws by only requiring that both sides of its associativity and symmetry equations are equal when one of them has a defined value). These algebraic properties will simply be inherited from the corresponding properties of \u2227 and \u2228. We can now see that Definition 1 of a proper ordered plausibility space is relevant in the sense that we must work with such spaces if we accept the general framework of section 2, and if we also accept the refinability and closedness assumptions. If we accept the Robustness assumption we can concentrate on ordered spaces.\nDefinition 1 A proper plausibility space is a seven-tuple (D,F,G, S,\u2264,\u22a5,\u22a4), where D is a partially ordered domain with smallest value \u22a5 and largest value \u22a4, F : D \u00d7 D \u2192 D, S : D \u2192 D, E = {(x, y) \u2208 D \u00d7 D : x < S(y)} and G : E \u2192 D. Moreover, F and G are symmetric and associative, F distributes over G, F and G are increasing in their arguments and S is decreasing. Additionally, G(x,\u22a5) = x, F (\u22a5, x) = \u22a5, F (x,\u22a4) = x, S(S(x)) = x, and S(\u22a5) = \u22a4. A proper ordered plausibility space is a proper plausibility space where \u2264 is a total ordering.\nTheorem 1 A proper ordered plausibility space can be uniquely embedded in a minimal ordered field where multiplication and addition are extensions of F and G, respectively.\nProof. See appendix.\nTheorem 2 Assume that a plausibility model and its plausibility space satisfies the assumptions of Sufficiency, Monotonicity, Propositional limit, Refinability and Closedness. Then the plausibility space is a proper plausibility space.\nProof. See appendix.\nWe can now see that Theorem 2 leads us to concentrate on proper plausibility spaces, and that with the Robustness assumption Claim 1 should follow from Theorem 1."}, {"heading": "5 Common sense assumptions entail extended robust probability.", "text": "The best known ordered fields are Q and R. However, there are ordered fields that contain more than the real numbers. One example is the field of rational functions R(\u01eb) in one variable \u01eb. These functions can be added to and multiplied with each other, and if they are ranked by the lexicographical ordering of their values and derivatives of order 1, 2, ... at \u01eb = 0, we get an ordered field. This field contains all real numbers (as constant functions) but does not admit lowest upper and greatest lower bounds, and thus cannot be embedded in the field of reals. The variable \u01eb can be regarded as an infinitesimal, positive but smaller than every positive real number. There are many ordered fields that are superfields of the reals, and there is even a unique maximal ordered field No, described by Conway[12]. This field consists of reals, infinite numbers and infinitesimals. There is an infinitesimal element (a non-zero element smaller than every real number) for every infinite ordinal and transfinite number, namely its inverse. The field No has an extremely complex structure and cannot be easily represented in a computer. The concepts of infinitesimals and infinite numbers in non-standard analysis[40] are closely related to extended probability. However, non-standard analysis has a set-modeltheoretic basis whereas our basis is classical algebra, and it is not completely obvious that the two concepts are exactly identical.\nThe above leads us to a belief structure where belief is an extended probability value. Many arguments have been forwarded for belief to be modeled not by an ordered set, but a partially ordered set like a set of intervals. Because of the subtle properties of belief, it is difficult to resolve what the right thing is in a convincing way. The preceding analysis started out with the assumption that belief values are totally ordered, and cannot get to other conclusions. However, assume that the possible plausibility values form a partially ordered set, and accept the Robustness assumption of section 2. This partially ordered set can then be regarded as a set of different contexts (e.g., different unfused expert assessments) of linearly ordered plausibility models. For each linearly ordered domain belonging to an indexed member , Theorem 1 is applicable, and thus a plausibility model can be fully characterized by a set of extended probability distributions, one for each index. One way to fuse such sets of distributions is to form a weighted average of the distributions, but it is not clear which principles such fusion should be based on."}, {"heading": "6 Pragmatics", "text": "If we design a methodology where users are permitted to define epistemic states as sets of extended probability distributions we will soon find us in a situation where these users are encouraged to define things they cannot possibly understand. Extended Robust Bayes allows us to construct extremely complex descriptions of systems, where instead of points we must work with polytopes in high-dimensional or even infinitedimensional spaces. Even simple things cannot easily be done with such models[3]. The most interesting aspect of the above analysis is that it indicates a possible maximal generality in uncertainty management, but in every particular application this generality must probably be pruned. Is the full generality really required? There are strong claims in the literature that it is not. There are also strong claims that conventional Bayesianism, where only one probability distribution is used and where probabilities are standard, is inadequate. So, a pragmatic approach would be to investigate claims made, and see how reasonable they are. In this investigation there seems no definite need to distinguish human from machine decision making \u2013 most requirements we find are obviously applicable to human and machine alike, particularly when we want machines to interact with humans in terms of beliefs and intentions."}, {"heading": "6.1 Extended Probability", "text": "Is extended probability needed or is standard probability adequate? Extended probability is not completely unavoidable, in the sense that every finite extended probability model is equivalent to a standard probability model[4]. But in some cases extended probability seems useful as a pragmatic simplification of a modeling problem and obtaining natural problem descriptions. It seems clear that there is a phase in cognitive assessments where qualitative and order-of-magnitude reasoning is done[26], followed by a quantitative phase where more quantitative reasoning occurs. In AI reasoning research, many non-probabilistic methods have been proposed, and it seems as if many of them can be described in terms of extended probability[7, 8]. Non-standard analysis approaches have been advocated as an alternative to measure-theoretic ones in stochastic processes. Although no immediate reaction followed from the user communities, this is still a promising direction[34]. A number of assumptions have been proposed in the literature for arriving at the inevitability of standard probability and thus the exclusion of extended probability:\nReal valued assumption: Assume (with, e.g, Cox) that plausibility is real valued.\nArchimedean assumption: Fine[17] assumes that for every non-zero probability e, with ne = G((n \u2212 1)e, e) and 1\u00b7e = e, there is anN such thatNe > S(e). This assumption is introduced to step from comparative to standard probability.\nSeparability assumption: Arnborg and Sjo\u0308din[5] define a separable model as one in which, for every x < y and c, there are n, m such that xn < cm < yn, with xn = F (x, xn\u22121) and x1 = x. This is introduced as a weaker assumption than continuity of the auxiliary function F introduced in [1].\nWith the hindsight given by Theorem 1, these assumptions appear only as alternative ways to say that we do not accept infinitesimal probabilities. None of these assumptions are very compelling except possibly the first, if plausibility values are used in an application strictly using expected utility decision making. In this case plausibilities separated by an infinitesimal amount cannot be distinguished and can be considered equivalent."}, {"heading": "7 Summary and Conclusions", "text": "Many of the objections to Bayesian methods come from the sometimes very complex analytical models preferred by theoretical statisticians, and many practitioners have seen alternative methods like neural networks as a way to by-pass statistics. Unfortunately (or fortunately) this is to a large extent an illusion. There is no principled reason why models used could not be based on other types of models like neural networks, linguistic coding or case based reasoning, and indeed efforts have been made to view these techniques as special types of Bayesian models[23, 38]. As detailed above, there is no simple way around the normative claims of the Robust Extended Bayes\u2019 method. So if one uses an alternative method, it will sooner or later have to be evaluated against the standard scale of rationality. Bayesian methods contain a large amount of freedom in the sense that there are no \u2019correct\u2019 models or model sets on which to base conclusions about real world or inner world phenomena, but models must be chosen and tested against requirements of applications. This makes the Robust Extended Bayesian method itself more or less impossible to falsify - but so are other very basic methods like arithmetic.\nIs it now possible to answer the question in the title of this paper? Even a subjectivistic probabilist would hesitate, but mainly because the question is a little ambiguous. Our answer is that extended robust\nprobability is completely plausible as a universal uncertainty or belief measure until an example is given where some of the starred assumptions above can be demonstrated dubious."}, {"heading": "Acknowledgments", "text": "The reviewers of previous versions of this contribution have influenced it significantly, by pointing out questionable statements (some now deleted, others elaborated) and several relevant related papers in many disciplines.\nSA is indebted to David Draper for discussions of the importance of and approach to including countable additivity in the Cox/Jaynes framework, which also made me finalize this unpublished manuscript and add a retrospective section below (after the bibliography)."}, {"heading": "8 Retrospect", "text": "Various versions of this manuscript were accidentally indexed by search engines and are cited by other papers. This is the last 2003 version, with very few changes made in 2015: typos removed and a few too short arguments elaborated. Nothing reflecting developments since 2003 has been added, except in this retrospective section. Several aspects of possible polishing of Cox/Jaynes\u2019s development have been published, for a survey see the introductory sections\nof [48]. Apparently, little effort has been spent on analysing extended probability and partially ordered plausibility domains, but there are a few results to review. I will only mention those results easily discussed in our algebraic framework. Some papers argue that we have made above quite a number of assumptions compared to other papers in the same area. But this is, on closer reading, only an effect of our need to be explicit about assumptions and to discuss several alternative assumption sets. Counting assumptions as axioms is a bit misleading since we actually want compelling axioms, not minimum number of them. The shortest axiom set would be our Claim above..."}, {"heading": "8.1 Countable additivity", "text": "In the paper by Terenin and Draper[48], a new aspect is taken up, namely countable additivity of probabilities which has several equivalent definitions, the one coupled to its name is that for all events C and all sets of mutually disjoint events {Ei}, we must have \u03a3iP (Ei|C) = P ( \u22c2 i Ei|C). Yosida and Hewitt, however, define countable additivity as the condition that for every event C and sequence (Ai) such that Ai+1 \u2282 Ai and \u22c2 i Ai = \u2205, we also have limi P (Ai|C) = 0, and they claim it trivially equivalent to the former definition. The latter seems more understandable than the first, and could possibly serve as a \u2019compelling assumption\u2019 in the Cox/Jaynes framework, as is approximately the case in [48].\nAlthough one normally, in applications where the question arises, just assumes or postulates that this assumption is fulfilled (as among others Kolmogorov did), it actually does not have to be true for infinite sets of events (but it must be true when {Ei} is a finite set, a property called finite additivity), this being a consequence of somewhat counter-intuitive measuretheoretic considerations. A simple example of a probability distribution violating countable additivity is obtained by starting with a uniform distribution over integers 1 to N and letting N go to \u221e. Then every probability of an integer goes to 0 but all the sums are 1. The limit of the sums is thus 1 while the sum of the limits is 0. One common way to handle this is to claim that the limit, an improper prior, is not a probability distribution, despite the fact that unnormalized improper priors are sometimes (actually, quite often) used \u2019as if\u2019 they can be normalized. If the likelihood is enough concentrated, the posterior obtained by applying Bayes rule can be normalized and thus gives an inference, although it is clear that there are some problems with this approach. An improper prior, however, contains more significant information than the normalized finitely additive probability distribution, as can be seen in that the standard improper priors\n(uniform, Jeffrey\u2019s, etc) are identical as probability distributions: they all have density zero (except, for Jeffrey\u2019s prior, at zero). However, in the framework of extended probability they are proper distributions with infinitesimal densities. The question whether probabilities in infinite spaces should be countably or just finitely additive has been debated a lot, while in most applications one just assumes, with Kolmogorov, countable additivity. The Cox/Jaynes framework is explicitly based on what Jaynes refers to as common sense, but the question of countable/finite additivity has not previously (before Terenin and Draper) been analysed within this framework. Both Cox and Jaynes most likely regarded this as a non-issue, and for non-parametric Bayesian inference it is clear that current practice is based on the assumption of countably additive probability. Considering the current situation, we have here a case were researchers have not quite agreed about what common sense requires. From an engineering perspective the most difficult to swallow property of spaces not fulfilling countable additivity is known as non-conglomerability (de Finetti[15]). Non-conglomerability can be defined in terms of plausibilities: For an infinite partition {Ei} of E and an event C, it is not necessarily the case that infi C|Ei \u2264 C|E \u2264 supiC|Ei. Although my own reaction to this, both immediately and after considerable reflecting over it, is that non-conglomerability is absurd and must be rejected, the analyses of consequences of non-conglomerability are sometimes just mentioning that the concept invalidates some familiar inference methods. For example, it is not permitted to conclude that C|E is a weighted average of the C|Ei for an infinite partition {Ei} of E, in other words among the C|Ei we will not necessarily find both numbers not less than and numbers not greater than C|E. Is my reaction to this a sound reaction or just unwillingness to change habits?\nIf non-conglomerability is rejected and conglomerability is accepted as a common sense assumption, we can only end up with a system of plausibilities that can be rescaled to a system of probabilities that has to be conglomerable. Although it is relatively easy to see that a countably additive probability space is conglomerable, the reverse is also true but not so easy to see: a conglomerable probability space is countably additive. This was proved in [42], so conglomerability entails countable additivity, at least for real valued probability systems. This is an alternative axiom to that presented in [48], maybe also more compelling. What happens in extended probability has to be investigated, since the existing derivation relies in many places on standard probability, where, for example, least upper bounds exist."}, {"heading": "8.2 Partially ordered domains and robustness", "text": "Concerning partially ordered plausibility domains, little progress has been made. However, Jo\u0308rg Zimmermann in his thesis [52] shows that a partially ordered plausibility space is embeddable in a partially ordered ring, and shows, assuming that this ring has a greatest ordered subfield called backbone (and satisfies additional technical constraints), that any plausibility value r can be uniquely decomposed into s + a \u00b7 t, where s and t are from the backbone and a is interactive, which means that a is incomparable to elements between 0 and 1. He also shows that DS-theory is interpretable in this way and clearly robust probability also fits the model. In robust probability the plausibility domain is a vector (or indexed set) of probabilities and the backbone elements have all components equal. The domain is a ring, not a field, since it has zero divisors. The interpretation of the decomposition here is that s is an estimator of the probability (actually the min probability), t is a measure of the \u2019uncertainty of the probability\u2019 (maybe s + t/2 is a better estimator of the probability), and a gives a kind of \u2019profile\u2019 characterizing the deviation of the uncertainty from the estimated probability. This is quite nice, but the assignment of the estimate to every plausibility in the model does not itself give a coherent probability assignment. It is also not clear that the mentioned subfield always exists and thus a general understanding of partially ordered plausibility has not yet been obtained, in particular we do not know what weight the robustness assumption carries. Zimmermann, in his ring theorem, clearly makes an assumption slightly stronger than we have done, in the axiom And2. This entails the cancellability law for + of a ring, and we did (probably) not make enough assumptions to get cancellability. His assumption is clever and plausible, how compelling it is I cannot yet quite see. We could prove cancellability for G only for totally ordered plausibility domains. Without some type of cancellability assumption it is difficult to characterize the possible plausibility domains, and in particular to develop the algebraic approach. Even with the assumption it is difficult to get a real grip on what the most general such domains mean in practical terms. Ring structures are quite flexible. There is a fair number of in-depth treatments of partially ordered rings, the more recent motivated by its relevance to real algebraic geometry, but as far as I know they have not yet been seriously applied to the philosophical study of plausibility. It seems as if Zimmermann\u2019s assumption (And2) is a good starting point, and by his ring theorem the plausibility domain is then a partially ordered ring. He calls rings suitable for plausibility calculus c-rings. Now, the c-\nring does not have to contain a field (the backbone) under assumptions made, since the ring of integers is a counterexample, and powers (repeated products) of this ring are others. But if there is a (non-integer) rational number (a rational number is a ring element f such that the n-fold sum of f :s equals m for some integers n and m) in the c-ring it must also contain lots of rational numbers since the ring is closed under addition, subtraction and multiplication and this subring of rationals seems a good approximation to a field, the backbone. Moreover, any ordered field can be added to any partially ordered ring, creating a cring with a backbone from a c-ring without one. So the decomposition theorem of Zimmermann holds for all c-rings, even if some c-rings may not have interesting decompositions. A second question raised by the referenced thesis is the structure of c-rings. Our robustness assumption can also be formulated as a conjecture:\nConjecture: Every c-ring is embeddable in a product of ordered fields.\nConsidering the intense studies made of rings and fields, one would expect the literature to contain the verdict on this conjecture. And it seems to do, but the standard algebraic operating procedures makes it more appropriate to say that every c-ring is a subdirect sum of fields:"}, {"heading": "8.2.1 Representing rings as subdirect sums", "text": "The problem of representing a complicated ring as a product of simpler rings has been thoroughly studied in [10] and [32, 31, 33]. A product ring P = S \u00d7 T of two rings S and T has a carrier that is the Cartesian product of those of the factors S and T , and the operations are defined component-wise, thus, for example, if S contains s1 and s2 and T contains t1 and t2, then P contains (si, tj) for i = 1, 2 and j = 1, 2. Moreover, (s1, t1) \u00b7 (s2, t2) = (s1 \u00b7 t1, s2 \u00b7 t2) and (s1, t1) + (s2, t2) = (s1 + t1, s2 + t2). Two observations: for some, probably historical, reason, our main tool is actually a product but is called subdirect sum. Another important thing to note is that the product construction always introduces zero divisors, so the product of two integral domains is never itself an integral domain: (s, 0) \u00b7 (0, t) = (0, 0) regardless of s and t, and (0, 0) is the zero element of the product P since it is the unit for +. Therefore, a cancellation law for \u00b7 never holds in a product or subdirect sum. By a subdirect sum S of rings {Si}i\u2208I (where the index set I is not necessarily finite or countable) we mean a ring isomorphic to a subring P of \u220f i\u2208I Si, and such that the projection of P on component i is exactly Si (i.e., the homomorphism is onto). Moreover, we require the representation to be non-trivial, i.e., none\nof the subrings Si is allowed to be isomorphic to P . If such a non-trivial representation exists, the ring is called reducible. If no such representation exists, the ring is irreducible. Note that an irreducible ring is not the same as a reduced ring; the former can not be split into a subdirect sum whereas a reduced ring is a ring that has no nilpotent elements.\nTheorem: [10] Every reducible ring is a subdirect sum of irreducible rings.\nWhat makes this relatively simple theorem interesting is the connection to fields :\nTheorem[33, Theorem 3.14] Every subdirectly irreducible ring with no nilpotent elements is a field.\nThis is as close as we can get using the literature on rings to deriving the robustness assumption (In [32, Theorem 4], the corresponding statement was that every reduced (in the sense of absence of nilpotent elements) and reducible ring is a subdirect sum of integral domains, a slightly weaker result). The question now is: is there some good argument to reject factors in the subdirect sum with nilpotent elements, which would recover the robustness conjecture? Yes, there is. If a factor in a subdirect sum P has a nilpotent element then P also has a nilpotent element and there is a nilpotent element in the plausibility domain: If Si has the nilpotent element a, then the product has the nilpotent element (0, . . . , 0, a, 0, . . . , 0) with only one non-zero component. If there is a nilpotent plausibility value then there is also a 6= 0 such that a2 = 0. Use the refinement assumption to define two new subevents B1 and B2 of A such that B1|A = B2|A = a and such that they are information independent: B2|B1A = a. Now B1 and B2 are both possible (a 6= 0) in the context of A, but their combination in the context A has plausibility a2 = 0 and is impossible. This contradicts the assumption that the events are information independent: since B2 is possible in the context A and information independent of B1, it should also be possible in the context AB1. We conclude that the ring is reduced, i.e., lacks nilpotent elements. Since the decomposition of a reduced ring will result in a subdirect sum of fields, and these will be partially ordered by assumption, we can use the fact([43]) that a partially ordered field can always be totally ordered and thus is isomorphic to one of the fields between Q and No, to recover the robustness conjecture.\nWe have just informally proved:\nRobustness Theorem: A plausibility calculus embeddable in a partially ordered ring is, under the refinability assumption, equivalent to a subring of a power NoX , for some set X . The domain of this calculus\nis the set of maps from X to subfields of No and the operations \u00b7 and + are defined component-wise using the corresponding field operations.\nThe question remains of course how this set of probability assignments should be interpreted. Since the domain is a direct sum of ordered fields, there is only one partial order to choose, where a \u2265 b if and only if ai \u2265 bi for all i \u2208 X .\nThe assumption is that event A is more plausible than B if each distribution in the index set says that A is more plausible than B. Our assumptions do not let us conclude that the lower and upper envelopes have the specific meaning of lower and upper limits of an unknown probability distribution, although this interpretation is done in robust Bayesian inference. However, the domain values having all components equal represent obviously precise probabilities (as well as the backbone of [52]) and an event B with imprecise probability (not all components equal) is more plausible than an event A with precise plausibility if every component of B is more plausible than A, and thus an upper and by symmetry lower plausibility is indicated for the maximum and minimum components plausibilities. The robust Bayesian interpretation of domain values is thus justified, and Zimmermann\u2019s decomposition p = s+ a \u00b7 t contains indeed the lower and upper probabilities of p, namely s and s+t. There is a small complication in that sets of extended probabilities are sets of hyperreal numbers which do not necessarily have tight lower and upper bounds. The standard way to define infemum and supremum of such sets is to use the standard part of the numbers, although we have not investigated the appropriateness of this method in this particular application."}, {"heading": "8.2.2 Robustness examples", "text": "Since the robust Bayesian representation seems universal one can ask why there are other uncertainty management schemes than robust Bayesian analysis, or rather how these fare with respect to the Robustness theorem. The robustness theorem says that a problem where uncertainty is present should be modeled with a set of conditional probability assignments, and that these have the interpretation of possible probability distributions. This seems in agreement with how robust Bayesian analysis is performed and interpreted. Zimmerman considers also other uncertainty management schemes, and observes that the system of lower probabilities violates his ring theorem. This is because lower and upper probabilities are insufficient for recovering a set of distributions: in the lower and upper envelopes of a set of distributions the latter are mixed up and there is no way to recover the set of distributions from which the upper\nand lower probabilities were generated. This has the consequence that the calculus cannot be embedded in a ring and there is no useful definition of conditional plausibility. This appears as a weakness or at least inconvenience of the method.\nA third method, probably the most serious competitor besides Bayes, is Dempster-Shafer evidence theory, DS-theory[16, 44]. Here, an assessment is described as a \u2019body of evidence\u2019 or mass assignment, and this is a random set over the frame of discernment \u2126 which in turn is a partition of the universe of discourse. A generative model of the outcome given a body of evidence is that an outcome of the random set is generated and then one member of the resulting set is chosen arbitrarily. If the random set has non-zero probability only for singleton sets there is no arbitrary choice and we have a precise Bayesian probability model. Zimmerman observes that a body of evidence can be translated to a set of distributions and this is a good beginning. However, the DS-theory also has the peculiarity that bodies of evidence that are independent (of which apparently no precise characterization has been given) should be combined with random set intersection (conditioned on being nonempty), the Dempster\u2019s rule. Dempster\u2019s rule is different from the robust Bayesian combination rule. A very useful example of the difference was presented by Gelman[18]: it involves a match between a boxer and a wrestler, the outcome of which is entirely uncertain, and a coin flip which is a standard random event with probability 0.5 for each outcome. Here the events are: B: boxer wins; B: wrestler wins; C: heads up; C: tails up. The frame of discernment is \u2126 = {BC,BC,BC,BC}. The total uncertainty of B is expressed with the vacuous mass assignment [m1({BC,BC,BC,BC}) = 1], a random set whose outcome is always \u2126. The information on the coin flip is expressed by the body of evidence [m2({BC,BC}) = 0.5;m2{BC,BC}) = 0.5)], a random set whose outcome is {BC,BC} or {BC,BC}, each with probability 0.5. Gelman now goes on and adds the evidence that an impartial and trustworthy person reports that the two events had identical outcome (either BC or BC happened) This can be encoded as the body of evidence [m3({BC,BC}) = 1]. Combining, using Depster\u2019s rule, the bodies of evidence m1, m2 and m3, gives the random set [m({BC}) = 0.5;m({BC}) = 0.5), in other words the outcome is that of the standard coin flip. On the other hand, if the bodies of evidence are translated to sets of distributions and combined using Laplace\u2019s combination (component-wise multiplication followed by normalization), the outcome is completely uncertain between BC and BC, in other words [mr({BC,BC}) = 1], the random set outcome is always {BC,BC} . There is a significant practi-\ncal difference between these two results: In the first case any bet on the outcome at better than even odds is known to be advantageous, in the second case no bets or odds are known to be advantageous. And it is not difficult to see, by considering probabilities for B close to 0 and 1, that in this simple example the robust Bayesian method is the appropriate one. Gelman is dissatisfied with both results, but the robust Bayesian method actually gives the right result in this simple example, the difficulty seems to be in the problem formulation. Dempster\u2019s rule gives no imprecision when an imprecise body of evidence is combined with a precise one (which, as mentioned, is characterized by being a singleton random set). This is a trivial consequence of the random set intersection in Dempster\u2019s rule: the intersection of a singleton random set with any random set is itself a singleton or empty.\nIt is possible to bring in DS-theory under the robust Bayesian methodology, but only by modelling the random set operations directly \u2013 they are after all entirely kosher probability models. This would entail introduction of new symbols for the subsets of \u2126. These symbols are new and do not correspond to unions or intersections but to the random sets involved, so for example the symbol E = {BC} gives rise to the assignments E|m1 = 0, E|m2 = 0, E|m3 = 0 and E|m = 0.5, whereas the correct answer based on [mr({BC,BC}) = 1] would have E|mr = 0 but F |mr = 1 where F = {BC,BC}. This gives a precise Bayesian model where only the last step, translating the final random set to a convex set of probabilities, uses the robust Bayesian framework. The problem is thus not that DS-theory does not fit into the Cox/Jaynes framework, but that the implied probability model sometimes (like in Gelman\u2019s case) gives wrong answers for simple problems, as has been pointed out in [6, 11]. The discussion of the justification of Dempster\u2019s rule in [16] seems fair, but the problem is that the implied probability model is only known to be appropriate for problems where imprecision can be attributed to private frames of reference."}, {"heading": "8.3 Philosophy", "text": "A number of philosophical papers on the justifications of the line of investigations based on Cox\u2019s and de Finetti\u2019s work have also appeared. An article by John D Norton[35] groups assumptions in several groups, gives counterarguments to some of them, and suggests that each group is checked and a subset is chosen according to the needs of each specific application. This seems a reasonable idea, but it requires a lot of effort to find out what these subsets actually mean in terms of the ensuing uncertainty calculi, and my experience is that finding the specific needs in an ap-\nplication area is difficult, since as Norton points out the game is really to find convincing arguments for an already made decision. In practice it seems that, currently, a number of methods are developed in separate communities and one of their major tasks (given that the claim to universality is too difficult as they usually know by now) is to find the applications that confirm the method. The conclusion of Norton is quite plausible while his specific examples in the argumentation are sometimes difficult to follow and agree with, like the lack of distinction between descriptive (Cialdini, Ellsberg, Kahneman, Tversky, Klein) and prescriptive studies. As an example, the non-associativity of the \u2019more plausible than\u2019 relation occurs only in descriptive studies (reflecting framing, cognitive barriers, unstable preferences or approximate computations) and there is no real argument against it in a prescriptive system. When and if a difference is found it seems that three lines of investigation are possible: (i) explaining the difference, for example by the wiring of the brain, the social context of the decision making, computational or cognitive limitations, or a real problem with the prescription; (ii) explaining what is wrong in practical decision making and trying to fix it by adjusting the education system; (iii) finding what is wrong with the prescriptive theories and adapting them to the real needs or finding a new prescriptive theory of some generality if not universality. It seems to me that accepting (iii) as the only appropriate way is not so convincing. The development of Bayesian analysis is however an example of the third option: from being harshly criticised most of the time before 1980, it has been tuned by an enormous effort by Bayesians addressing both fundamental and computational questions as well as developing sophisticated model families suitable for areas such as health sciences, bioinformatics, language translation, language and species evolution, robotics and vision, web analytics, signals and systems and finance, in several of which it has supplanted earlier methodologies. There is by now a toolbox that, by its diversity and the versatility of supporting computer codes, makes the Bayesian framework rather hard to replace. It is also not the case that spectacular developments have been achieved all over the field and certainly some Bayesian efforts have faded away, but enough remains to make a convincing case for robust Bayesian analysis as the starting effort in an area, and in case the application area is resisting, see what modifications of the method can be made before looking for completely different alternatives. Such modifications have been made, and I find it difficult to see many of them as contrived or exotic.\nIn summary, it seems that despite progress the last decade, there is more to find out about plausibility\ncalculi."}, {"heading": "A Proof of Theorem 1", "text": "Recall the definition of an ordered field: it is a structure R = (D,<, \u00b7,+, 1, 0), where D is a domain ordered by <, \u00b7 and + are total functions from D2 to D, satisfying the properties of associativity and symmetry, and where \u00b7 distributes over +. Moreover, it has an additive inverse, i.e.,, the equation a + x = b can be solved for x, and the equation a \u00b7 x = b can be solved for x if a 6= 0. The element 1 of D is a unit for \u00b7 and 0 is a unit for +. The function + is increasing and \u00b7 is increasing for arguments larger (by <) than 0. Fields are special cases of rings and integral domains. An ordered integral domain satisfies the rules of a field except that we do not require solvability for x of a \u00b7 x = b.\nWe will show how an ordered plausibility space is embedded in an ordered field. We have thus the space defined by the ordered domain D with smallest and largest elements now called 0 and 1, respectively, and they will always be embedded as 0 and 1 of the field, respectively. Because of the significant number of algebraic formulas used here, we also use infix notation + and \u00b7 for G and F , as well as for their extensions.\nThe function S : D \u2192 D maps its argument x to a solution - the only one because of the monotonicity assumption - y of the equation x+y = 1. The function \u00b7 is defined on D\u00d7D, and + on {(x, y) : (x, y) \u2208 D\u00d7 D\u2227x \u2264 S(y)}. We now extend D while extending the definitions of \u00b7 and +. We know that \u00b7 is associative and symmetric, and that + is symmetric. If (a+ b)+ c and a + (b + c) are both defined, they are equal. Likewise, if a+b is defined, c \u00b7 (a+b) = c \u00b7a+c \u00b7b, and if a+ b is defined and c < b, then a+ c is also defined.\nThe strict monotonicity assumption leads to cancellation properties:\nLemma 1 If a+ b = a+ c then b = c, if a \u00b7 b = a \u00b7 c and a 6= 0 then b = c, if a+ b \u2264 a+ c then b \u2264 c, and if a \u00b7 b \u2264 a \u00b7 c and a 6= 0 then b \u2264 c.\nIf there are no elements in D except 0 and 1, the embedding is trivial, so we assume an element e with 0 < e < 1 in D. We do the embedding in three steps, using the standard technique of defining an extension as the quotient of a set of pairs by an equivalence relation, and indicating which element of the extension that corresponds to each element of the original domain. We use the notation [a]\u223c for the equivalence class of \u223c containing a. It is easy, in each embedding step, to define the functions \u00b7 and + on the extensions and verify that they are indeed functions and\nextensions, that their laws are preserved, as well as to verify that no two elements of the old domain become equivalent in the new domain. Details of this verification are shown here, in the form of a series of Lemmas, whose proofs are sometimes terse and sometimes omitted.\nLemma 2 If a1+a2 is defined and a1 \u2265 b1 and a2 \u2265 b2, then b1 + b2 is defined.\nProof: Obviously, b1 \u2264 a1 \u2264 S(a2) \u2264 S(b2).\nLemma 3 If e 6= 0, 1 and f = min(e, S(e)), then f \u00b7 a+ f \u00b7 b is defined.\nProof: Assume (no loss of generality because of symmetry of +) that a \u2264 b.\n1. f = e \u2264 S(e): In this case, f \u00b7a \u2264 f \u00b7 b \u2264 S(f) \u00b7 b, and f \u00b7 b + S(f) \u00b7 b is defined, thus by Lemma 1 f \u00b7 a+ f \u00b7 b is also defined.\n2. e > S(e) = f : Since e \u00b7 b+ S(e) \u00b7 b is defined and e \u00b7 b \u2265 f \u00b7 a, thus by Lemma 2 f \u00b7 a+ f \u00b7 b is also defined.\nLemma 4 For every sequence (ai) n 1 there is a nonzero cn depending only on n such that cn \u00b7 a1 + cn \u00b7 a2 . . . cn \u00b7 an is defined.\nProof: For any non-trivial plausibility value e, choose c = min(e, S(e)) and cn = c\n\u2308logn\u2309. Use Lemma 3 inductively on half sequences.\nThe first embedding step introduces non-negative rationals and is similar to the standard quotient construction for integral domains. Let D+ = D \u2212 {\u22a5} and D(1) = (D \u00d7 D+)/ \u223c, where, for a, c \u2208 D and b, d \u2208 D+, (a, b) \u223c (c, d) iff a \u00b7 d = b \u00b7 c. This makes \u223c an equivalence relation. Use notation [a, b] for [(a, b)]\u223c. An element d \u2208 D is identified with [d, 1] \u2208 D(1).\nDefine <, \u00b7 and + as total functions onD(1) by [a, b] < [c, d] iff a \u00b7 d < c \u00b7 b, [a, b] \u00b7 [c, d] = [a \u00b7 c, b \u00b7 d], and [a, b]+ [c, d] = [e \u00b7a \u00b7d+ e \u00b7 c \u00b7 b, e \u00b7 b \u00b7d], for some e such that the expressions are defined (see Lemma A). The rational number 2/3 is identified with [x+x, x+x+x] for some 0 < x < c3, and the other non-negative rationals are similarly defined. In this extension the rules for a field are satisfied, except that we have not yet an additive inverse or negative values:\nLemma 5 The relation \u223c is an equivalence relation and if [d, 1] = [d\u2032, 1], then d = d\u2032.\nProof: The relation \u223c is obviously reflexive and symmetric. It is also associative, since if (a, b) \u223c (c, d)\nand (c, d) \u223c (e, f) with b, d, f 6= 0 then a \u00b7d = b \u00b7c and c \u00b7 f = d \u00b7 e, hence c 6= 0, a \u00b7 f \u00b7 (c \u00b7 d) = e \u00b7 b \u00b7 (c \u00b7 d) and by the cancellation Lemma, (a, b) \u223c (e, f).\nLemma 6 The relation < is a total order on D(1).\nProof: Omitted.\nLemma 7 The relations \u00b7 and + are total functions on D(1) \u00d7D(1).\nProof: Consider +: If [a, b]+[c, d] = [f, g] and [a\u2032, b\u2032]+ [c, d] = [f \u2032, g\u2032] with [f, g] = [f \u2032, g\u2032], then [f, g] = [e \u00b7 a \u00b7 d+e \u00b7c \u00b7b, e \u00b7b \u00b7d] and [f \u2032, g\u2032] = [e \u00b7a\u2032 \u00b7d+e \u00b7c \u00b7b\u2032, e \u00b7b\u2032 \u00b7d], The equivalence condition [f, g] = [f \u2032, g\u2032] translates to (e\u00b7a\u00b7d+e\u00b7c\u00b7b)\u00b7e\u00b7b\u2032 \u00b7d = (e\u00b7a\u2032 \u00b7d+e\u00b7c\u00b7b\u2032)\u00b7e\u00b7b\u00b7d, in other words (by cancellation) (a\u00b7d+c\u00b7b)\u00b7b\u2032 = (a\u2032\u00b7d+c\u00b7b\u2032)\u00b7b so [a, b] = [a\u2032, b\u2032]. In other words the type of + is indeed D(1) \u00d7D(1) \u2192 D(1). The other cases are similar but easier.\nLemma 8 The functions \u00b7 and + are associative and symmetric, and \u00b7 distributes over + on D(1) \u00d7D(1).\nProof: Consider associativity for +: If ([a, b]+[c, d])+ [f, g] = [h, i] and [a, b] + ([c, d] + [f, g]) = [h\u2032, i\u2032], then [h, i] = [e\u00b7(e\u00b7a\u00b7d+e\u00b7c\u00b7b)\u00b7g+e\u00b7f \u00b7e\u00b7b\u00b7d, e\u00b7e\u00b7b\u00b7d\u00b7g] and [h\u2032, i\u2032] = [e \u00b7(e \u00b7c \u00b7g+e \u00b7d \u00b7f) \u00b7b+e \u00b7a \u00b7d \u00b7g \u00b7e, e \u00b7b \u00b7e \u00b7d \u00b7g]. These two are equal. The other cases are similar.\nOur next embedding step introduces subtraction and negative values: Let D(2) = (D(1)\u00d7(D(1))/ \u2248, where, for a, b, c, d \u2208 D(1), (a, b) \u2248 (c, d) iff a+d = b+ c. Use notation [[a, b]] for [(a, b)]\u2248. An element d \u2208 D(1) is identified with [[d, 0]] \u2208 D(2). Define <, \u00b7 and + in this extension by [[a, b]] < [[c, d]] iff a + d < c + b, [[a, b]] \u00b7 [[c, d]] = [[a \u00b7 c+ b \u00b7 d, a \u00b7 d+ b \u00b7 c]], and [[a, b]] + [[c, d]]) = [[a+ c, b+ d]].\nLemma 9 The relation \u2248 is an equivalence relation and if [[d, 0]] = [[d\u2032, 0]], then d = d\u2032.\nLemma 10 The relation < is a total order on D(2) and extends < on D(1).\nLemma 11 The relations \u00b7 and + are total functions on D(2) \u00d7D(2) and extend \u00b7 and + on D(1) \u00d7D(1).\nLemma 12 The functions \u00b7 and + are associative, symmetric and \u00b7 distributes over + on D(2) \u00d7D(2).\nLemma 13 The structure R = (D(2), <, \u00b7,+, 1, 0) is an ordered integral domain.\nLemma 14 ([30, Ch V.2, Theorem 6]) Every ordered integral domain can be embedded in an ordered field\nThe structure R can thus be embedded in an ordered field. This field embeds the plausibility space. This finishes the proof of Theorem 1."}, {"heading": "B Proof of Theorem 2", "text": "Let the plausibility space be (D, \u00b7,+, S,\u2264,\u22a5,\u22a4). Most of the properties of a proper ordered plausibility space are immediate consequences of our stated assumptions. The non-trivial part is to show that the stated laws for F , G and S must hold in a consistently refinable model. This follows from a sequence of Lemmas, each showing how an algebraic law follows from the corresponding law of propositional logic. We only state them and prove some of them:\nLemma 15 For all x \u2208 D, S(S(x)) = x.\nProof: If x = A|B, then x = A|B = A|B = S(S(x)).\nLemma 16 For all x, y, z \u2208 D, (x \u00b7y) \u00b7z) = x \u00b7(y \u00b7z)).\nProof: If we have worked out a model where a\u00b7(b\u00b7c) 6= (a \u00b7 b) \u00b7 c for some plausibilities a, b and c that occur in the model, then we take an arbitrary statement S (not false) and refine with A\u2032, B\u2032 and C\u2032: A\u2032 \u2192 B\u2032, B\u2032 \u2192 C\u2032 and C\u2032 \u2192 S, A\u2032|B\u2032 = a, B\u2032|C\u2032 = b and C\u2032|S = c. Now the value A\u2032B\u2032C\u2032|S can be computed in two ways giving different results, as (A\u2032B\u2032)C\u2032|S = F (A\u2032B\u2032|SC\u2032, C\u2032|S) = F (F (A\u2032|B\u2032, B\u2032|C\u2032), c) = (a \u00b7 b) \u00b7 c and as A\u2032(B\u2032C\u2032)|S = F (A\u2032|SB\u2032C\u2032, B\u2032C\u2032|S) = F (A\u2032|B\u2032, F (B\u2032|C\u2032, C\u2032|S)) = a \u00b7 (b \u00b7 c).\nLemma 17 For all x, y \u2208 D, x \u00b7 y = y \u00b7 x.\nProof: If x = A|D and y = B|E, introduce by refinement B\u2032 such that D \u2192 B\u2032, B\u2032|D = y and B\u2032 is independent of A. Now AB\u2032|D = x \u00b7 y and AB\u2032|D = B\u2032A|D = y \u00b7 x, so x \u00b7 y = y \u00b7 x.\nLemma 18 For all x, y \u2208 D, x + y = y + x, in the sense that if one side is defined, then the other side is defined and equal.\nProof: If x = A|D and y = B|E and x \u2264 S(y), introduce by refinement B\u2032 such that D \u2192 B\u2032, B\u2032|D = y and AB = \u22a5. Now A \u2228B|D = x+ y and A \u2228B|D = B \u2228A|D = y + x, so x+ y = y + x.\nLemma 19 For all x, y, z \u2208 D, (x + y) + z = x + (y + z), in the sense that if one side is defined, then the other side is defined and equal.\nProof: By refinement and associativity of \u2228.\nLemma 20 For all x, y, z \u2208 D, x\u00b7z+y \u00b7z = (y+x)\u00b7z, in the sense that if x + y is defined, then both sides are defined and equal.\nProof: Assume x, y and z are non-trivial plausibilities of the model and x \u2264 S(y), because otherwise the\nLemma is obvious. Introduce for non-\u22a5 D by refinement A \u2192 D, B \u2192 D and C \u2192 D such that A\u2227B = \u22a5 and A and B are independent of C, and x = A|D, y = B|D and z = C|D. Now, (A\u2228B)\u2227C|D = (x+y)\u00b7z and (A\u2228B)\u2227C|D = (A\u2227C)\u2228(B\u2227C)|D = x \u00b7z+y \u00b7z and the Lemma follows."}], "references": [{"title": "Lectures on Functional Equations and their Applications", "author": ["J. Acz\u00e9l"], "venue": "Academic Press,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1966}, {"title": "Probability and the logic of conditionals", "author": ["E. Adams"], "venue": "J. Hintikka and P. Suppes, editors, Aspects of Inductive Logic, pages 265\u2013316. North Holland, Amsterdam,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1966}, {"title": "Learning in prevision space", "author": ["S. Arnborg"], "venue": "Gert de Cooman, Fabio G. Cozman, Serafin Moral, and Peter Walley, editors, Proceedings of the First International Symposium on Imprecise Probabilities and their Applications, pages 8\u201314. Gent University,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Bayes rules in finite models", "author": ["S. Arnborg", "G. Sj\u00f6din"], "venue": "Proc. European Conference on Artificial Intelligence, pages 571\u2013575, Berlin,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "On the foundations of Bayesianism", "author": ["S. Arnborg", "G. Sj\u00f6din"], "venue": "Ali Mohammad-Djarafi, editor, Bayesian Inference and Maximum Entropy Methods in Science and Engineering, 20th International Workshop, Gif-sur-Yvette, 2000, pages 61\u201371. American Institute of Physics,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Robust Bayesianism: Relation to evidence theory", "author": ["Stefan Arnborg"], "venue": "Journal of Advances in Information Fusion,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Nonmonotonic reasoning, conditional objects and possibility theory", "author": ["S. Benferhat", "D. Dubois", "H. Prade"], "venue": "Artificial Intelligence, 92:259\u2013 276,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Belief functions and default reasoning", "author": ["S. Benferhat", "A. Saffiotti", "P. Smets"], "venue": "Artificial Intelligence, 122:1\u201369,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Subdirect unions in universal algebra", "author": ["G. Birkhoff"], "venue": "Bull. Amer. Math. Soc., 50,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1944}, {"title": "A case of combination of evidence in the Dempster-Shafer theory inconsistent with evaluation of probabilities", "author": ["Andrzej K. Brodzik", "Robert H. Enders"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On Numbers and Games", "author": ["J.H. Conway"], "venue": "Academic Press,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1976}, {"title": "Probability, frequency, and reasonable expectation", "author": ["R.T. Cox"], "venue": "Am. Jour. Phys., 14:1\u201313,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1946}, {"title": "Identical foundation of probability theory and fuzzy set theory", "author": ["D. de Brucq", "O. Colot", "A.Sombo"], "venue": "FUSION", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Upper and lower probabilities induced by a multi-valued mapping", "author": ["A.P. Dempster"], "venue": "Annals of Mathematical Statistics, 38:325\u2013339,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1967}, {"title": "Theories of Probability", "author": ["T.L. Fine"], "venue": "Academic Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1973}, {"title": "The Boxer, the Wrestler, and the Coin Flip: A paradox of robust Bayesian inference and belief functions", "author": ["Andrew Gelman"], "venue": "The American Statistician,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "On the scoring approach to admissibility of uncertainty measures in expert systems", "author": ["I. Goodman", "H.T. Nguyen", "G. Rogers"], "venue": "J. Math. Analysis Appl., 159:550\u2013594,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1991}, {"title": "A counterexample to theorems of Cox and Fine", "author": ["J. Halpern"], "venue": "Journal of AI research, 10:67\u201385,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Cox\u2019s theorem revisited", "author": ["J. Halpern"], "venue": "Journal of AI research, 11:429\u2013435,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Scaled boolean algebras", "author": ["M. Hardy"], "venue": "Advances in Applied Mathematics, 29:243\u2013292,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Toward a probabilistic formalization of case-based inference", "author": ["E. H\u00fcllermeier"], "venue": "D. Thomas, editor, Proceedings of the 16th International Joint  Conference on Artificial Intelligence (IJCAI-99- Vol1), pages 248\u2013253, S.F., July 31\u2013August 6", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Ethics", "author": ["D.S. Hutchinson"], "venue": "P. Barnes, editor, Aristotle, Cambridge,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Probability Theory: The Logic of Science", "author": ["E.T. Jaynes"], "venue": "Cambridge University Press,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Recognition-primed decisions", "author": ["G.A. Klein"], "venue": "W. Rouse, editor, Advances in Man-Machine systems research, pages 47\u201392, Greenwich,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1989}, {"title": "Intuitive probability on finite sets", "author": ["C.H. Kraft", "J.W. Pratt", "A. Seidenberg"], "venue": "Annals of Mathematical Statistics, 30:408\u2013419,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1959}, {"title": "Bayesian Statistics: a Review", "author": ["D.V. Lindley"], "venue": "SIAM,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1971}, {"title": "Scoring rules and the inevitability of probability (with discussion)", "author": ["D.V. Lindley"], "venue": "Internat. Stat. Rev, 50:1\u201326,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1982}, {"title": "Algebra", "author": ["S. MacLane", "G. Birkhoff"], "venue": "The MacMillan Company, New York,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1967}, {"title": "Subdirect sums of rings", "author": ["Neal H. McCoy"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1947}, {"title": "Subrings of direct sums", "author": ["N.H. McCoy"], "venue": "Amer. J. Math., 60,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1938}, {"title": "Theory of Rings", "author": ["N.H. McCoy"], "venue": "McMillan Company,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1964}, {"title": "Radically Elementary Probability Theory", "author": ["E. Nelson"], "venue": "Princeton University Press,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1987}, {"title": "Probability disassembled", "author": ["John D. Norton"], "venue": "British Journal for the Philosophy of Science,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "The Uncertain Reasoner\u2019s Companion", "author": ["J.B. Paris"], "venue": "Cambridge University Press,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1994}, {"title": "Rough Sets", "author": ["Z. Pawlak"], "venue": "Kluwer, Dordrecht,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1992}, {"title": "Pattern Recognition and Neural Networks", "author": ["B. Ripley"], "venue": "Cambridge University Press,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1996}, {"title": "Conditioning, likelihood and coherence: A review of some foundational concepts", "author": ["J. Robins", "L. Wasserman"], "venue": "J. American Statistical Ass., 95:1340\u20131345,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "Introduction to Model Theory and to the Metamathematics of Algebra", "author": ["A. Robinson"], "venue": "North- Holland,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1963}, {"title": "Foundations of Statistics", "author": ["L.J. Savage"], "venue": "John Wiley & Sons, New York,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1954}, {"title": "Probability disassembled", "author": ["Schervish", "Seidenfeld", "Kadane"], "venue": "Z Wahrscheinlichkeitstheorie,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1984}, {"title": "Convex extensions of partially ordered rings", "author": ["Niels Schwartz"], "venue": "In Ge\u0301ome\u0301trie alge\u0301brique et analytique re\u0301elle,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "A mathematical theory of evidence", "author": ["G. Shafer"], "venue": "Princeton University Press,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1976}, {"title": "Probability Theory \u2013 it\u2019s only a Game", "author": ["G Shafer", "V. Vovk"], "venue": "MIT Press,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2001}, {"title": "Logic", "author": ["R. Smith"], "venue": "P. Barnes, editor, Aristotle, Cambridge,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1995}, {"title": "The vulnerability of the transferable belief model to Dutch books", "author": ["P. Snow"], "venue": "Artificial Intelligence, 105:345\u2013354,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1998}, {"title": "Rigorizing and Extending the Cox-Jaynes Derivation of Probability: Implications for Statistical Practice", "author": ["Alexander Terenin", "David Draper"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Measures of uncertainty in expert systems", "author": ["P. Walley"], "venue": "Artificial Intelligence, 83:1\u201358,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1996}, {"title": "A logic of extended probability", "author": ["N. Wilson"], "venue": "Gert de Cooman, Fabio G. Cozman, Serafin Moral, and Peter Walley, editors, Proceedings of the First International Symposium on Imprecise Probabilities and their Applications, pages 397\u2013 404. Gent University,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1999}, {"title": "The roles of fuzzy logic and soft computing in the conception, design and deployment of intelligent systems", "author": ["L.A. Zadeh"], "venue": "Lecture Notes in Computer Science, 1198:183\u2013210,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 11, "context": "We consider plausibility spaces of the type defined by Cox[13], with auxiliary functions F , S and G used for computing plausibilities of conjunctions, complements and disjunctions, respectively.", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "Standard probability is one possible plausibility space, where the domain is the real numbers in [0, 1] and the functions F and G are multiplication and addition, respectively.", "startOffset": 97, "endOffset": 103}, {"referenceID": 36, "context": "The auxiliary argumentation required to support our claims is similar to inference principles in statistics, like the Sufficiency, Likelihood and Conditionality principles[39].", "startOffset": 171, "endOffset": 175}, {"referenceID": 24, "context": "There are two papers that analyze a similar question and lead to different but related and compatible conclusions, namely Kraft et al[27] and Hardy[22].", "startOffset": 133, "endOffset": 137}, {"referenceID": 19, "context": "There are two papers that analyze a similar question and lead to different but related and compatible conclusions, namely Kraft et al[27] and Hardy[22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 43, "context": "The methods of definition and induction are attributed, by Aristotle, to Socrates[46], and were developed further by Aristotle and the Stoic philosophers.", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "Nevertheless, Aristotle is the first to give a surviving qualitative version of the Bayesian decision making principle in his Nichomachean ethics[24]: \u201c Find out what you think is a good life, and consider the probabilities of your possible actions to achieve this.", "startOffset": 145, "endOffset": 149}, {"referenceID": 38, "context": "The plausibility assignment leading to such a situation is considered incoherent, and under various assumptions several arguments exist that end up in concluding that probability or a set of probability distributions is the only coherent plausibility space [41, 28, 15, 45].", "startOffset": 257, "endOffset": 273}, {"referenceID": 25, "context": "The plausibility assignment leading to such a situation is considered incoherent, and under various assumptions several arguments exist that end up in concluding that probability or a set of probability distributions is the only coherent plausibility space [41, 28, 15, 45].", "startOffset": 257, "endOffset": 273}, {"referenceID": 42, "context": "The plausibility assignment leading to such a situation is considered incoherent, and under various assumptions several arguments exist that end up in concluding that probability or a set of probability distributions is the only coherent plausibility space [41, 28, 15, 45].", "startOffset": 257, "endOffset": 273}, {"referenceID": 11, "context": "We will use here the alternative approach devised by Cox[13] which avoids the betting scenario and instead analyses how plausibilities of combined events can be defined, and what properties the plausibility combination functions must have in order to avoid that an events plausibility will depend on which of several possible derivations is used.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "Cox[13] assumes that the domain of plausibility values is an interval of real values, which without loss of generality can be assumed to be [0, 1], with 0 for falsity and 1 for truth.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Cox[13] assumes that the domain of plausibility values is an interval of real values, which without loss of generality can be assumed to be [0, 1], with 0 for falsity and 1 for truth.", "startOffset": 140, "endOffset": 146}, {"referenceID": 0, "context": "We can state an assumption made implicitly by Cox here (the function G was introduced by Acz\u00e9l[1] in a related investigation):", "startOffset": 94, "endOffset": 97}, {"referenceID": 34, "context": "In rough and fuzzy set theory[37, 51], the meanings of symbols are taken to be ambiguous, in the sense that even with full information we cannot definitely say, e.", "startOffset": 29, "endOffset": 37}, {"referenceID": 48, "context": "In rough and fuzzy set theory[37, 51], the meanings of symbols are taken to be ambiguous, in the sense that even with full information we cannot definitely say, e.", "startOffset": 29, "endOffset": 37}, {"referenceID": 46, "context": "The ambiguity problem has in many articles been claimed to be an obstacle to probabilistic uncertainty management[49].", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "We have now stated the common assumptions of most existing theories of plausibility[17, 15, 13, 41].", "startOffset": 83, "endOffset": 99}, {"referenceID": 11, "context": "We have now stated the common assumptions of most existing theories of plausibility[17, 15, 13, 41].", "startOffset": 83, "endOffset": 99}, {"referenceID": 38, "context": "We have now stated the common assumptions of most existing theories of plausibility[17, 15, 13, 41].", "startOffset": 83, "endOffset": 99}, {"referenceID": 11, "context": "Several papers show, with important differences in detail, that every coherent belief set is equivalent to a probability model[15, 13, 41, 29].", "startOffset": 126, "endOffset": 142}, {"referenceID": 38, "context": "Several papers show, with important differences in detail, that every coherent belief set is equivalent to a probability model[15, 13, 41, 29].", "startOffset": 126, "endOffset": 142}, {"referenceID": 26, "context": "Several papers show, with important differences in detail, that every coherent belief set is equivalent to a probability model[15, 13, 41, 29].", "startOffset": 126, "endOffset": 142}, {"referenceID": 16, "context": "The coherence concept can also be applied to some domains not totally but partially ordered, and Goodman, Ngyuen and Rogers[19] show that some plausibilty measures not coherent under the total order assumption, among others the standard version of DS-theory, are coherent under the latter assumption.", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "Despite the subtle distinction, the weaker assumption would lead to completely different conclusions[14] since non-strict monotonicity does not entail cancellation laws.", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "As an example, a model is discussed by Halpern[20] that has only a few events.", "startOffset": 46, "endOffset": 50}, {"referenceID": 11, "context": "In [13], it is plainly assumed (as pointed out in [47]) that the auxiliary function F has certain regularity properties:", "startOffset": 3, "endOffset": 7}, {"referenceID": 44, "context": "In [13], it is plainly assumed (as pointed out in [47]) that the auxiliary function F has certain regularity properties:", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "Associativity and differentiability assumption: The function F is defined on [0, 1], and is associative and twice continously differentiable; the auxiliary function S is continuously differentiable and S(S(x)) = x", "startOffset": 77, "endOffset": 83}, {"referenceID": 11, "context": "The differentiability part is implicit \u2013 as was common when Cox wrote his paper \u2013 but is a standard assumption made to justify switching the order of differentiation which occurs in the derivations of [13], and also in [25].", "startOffset": 201, "endOffset": 205}, {"referenceID": 22, "context": "The differentiability part is implicit \u2013 as was common when Cox wrote his paper \u2013 but is a standard assumption made to justify switching the order of differentiation which occurs in the derivations of [13], and also in [25].", "startOffset": 219, "endOffset": 223}, {"referenceID": 17, "context": "In the criticisms of Cox\u2019s work, this assumption has either been assumed not to exist[20, 21] or been ignored[36].", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": "In the criticisms of Cox\u2019s work, this assumption has either been assumed not to exist[20, 21] or been ignored[36].", "startOffset": 85, "endOffset": 93}, {"referenceID": 33, "context": "In the criticisms of Cox\u2019s work, this assumption has either been assumed not to exist[20, 21] or been ignored[36].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "these are somewhat glossed over, the main exceptions being [9, 17, 41, 49].", "startOffset": 59, "endOffset": 74}, {"referenceID": 38, "context": "these are somewhat glossed over, the main exceptions being [9, 17, 41, 49].", "startOffset": 59, "endOffset": 74}, {"referenceID": 46, "context": "these are somewhat glossed over, the main exceptions being [9, 17, 41, 49].", "startOffset": 59, "endOffset": 74}, {"referenceID": 0, "context": "4,5]: For every conditional event there exists a standard event with the same plausibility, and for every number r \u2208 [0, 1] there is a standard event with probability r.", "startOffset": 117, "endOffset": 123}, {"referenceID": 46, "context": "Embeddability assumption[49]: Inference should not depend on how the events participating in calculations are embedded in a larger event plausibility system.", "startOffset": 24, "endOffset": 28}, {"referenceID": 33, "context": "An alternative assumption used to derive Cox\u2019s result is given by Paris[36]:", "startOffset": 71, "endOffset": 75}, {"referenceID": 33, "context": "Denseness assumption[36]: For all real values x, y, z and each \u01eb > 0, there are events A, B, C such that |(A|BCD)\u2212 x|, |(B|CD)\u2212 y| and |(C|D)\u2212 z| are all less than \u01eb.", "startOffset": 20, "endOffset": 24}, {"referenceID": 17, "context": "In response to [20], we developed very weak assumptions binding the different parts of a system of events together[4, 5].", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "In response to [20], we developed very weak assumptions binding the different parts of a system of events together[4, 5].", "startOffset": 114, "endOffset": 120}, {"referenceID": 4, "context": "In response to [20], we developed very weak assumptions binding the different parts of a system of events together[4, 5].", "startOffset": 114, "endOffset": 120}, {"referenceID": 3, "context": "Refinability assumption*[4]: In a plausibility model with a conditional event of plausibility p, it must be possible to introduce a new subcase B of a non-false event A with plausibility value p given to B|A.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "Somewhat surprisingly, Cox\u2019s result on rescalability of the plausibility space to standard probability follows from refinability and the assumptions in section 2 for models that have a finite number of plausibility values and an ordered plausibility space[4].", "startOffset": 255, "endOffset": 258}, {"referenceID": 4, "context": "Even more surprisingly, the same result cannot be proved for infinite plausibility spaces (non-provability follows from a counter-example[5]).", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "This idea goes back to Adams[2] and has been found to give a very basic uncertainty management that seems to incorporate many uncertainty calculi as special cases (several are analyzed using extended but not necessarily Bayesian probability in [7]).", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "This idea goes back to Adams[2] and has been found to give a very basic uncertainty management that seems to incorporate many uncertainty calculi as special cases (several are analyzed using extended but not necessarily Bayesian probability in [7]).", "startOffset": 244, "endOffset": 247}, {"referenceID": 47, "context": "The combination of robust and extended probability has been analyzed by Wilson[50].", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "Our motivation for accepting extended probability is the existence of infinite plausibility models that are refinable and embedded in the real numbers, but whose plausibility spaces are not rescalable to standard probability[5].", "startOffset": 224, "endOffset": 227}, {"referenceID": 4, "context": "Closedness assumption*[5]: The functions F , S and G have the following additional properties:", "startOffset": 22, "endOffset": 25}, {"referenceID": 10, "context": "There are many ordered fields that are superfields of the reals, and there is even a unique maximal ordered field No, described by Conway[12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 37, "context": "The concepts of infinitesimals and infinite numbers in non-standard analysis[40] are closely related to extended probability.", "startOffset": 76, "endOffset": 80}, {"referenceID": 2, "context": "Even simple things cannot easily be done with such models[3].", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "Is extended probability needed or is standard probability adequate? Extended probability is not completely unavoidable, in the sense that every finite extended probability model is equivalent to a standard probability model[4].", "startOffset": 223, "endOffset": 226}, {"referenceID": 23, "context": "It seems clear that there is a phase in cognitive assessments where qualitative and order-of-magnitude reasoning is done[26], followed by a quantitative phase where more quantitative reasoning occurs.", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "In AI reasoning research, many non-probabilistic methods have been proposed, and it seems as if many of them can be described in terms of extended probability[7, 8].", "startOffset": 158, "endOffset": 164}, {"referenceID": 7, "context": "In AI reasoning research, many non-probabilistic methods have been proposed, and it seems as if many of them can be described in terms of extended probability[7, 8].", "startOffset": 158, "endOffset": 164}, {"referenceID": 31, "context": "Although no immediate reaction followed from the user communities, this is still a promising direction[34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Archimedean assumption: Fine[17] assumes that for every non-zero probability e, with ne = G((n \u2212 1)e, e) and 1\u00b7e = e, there is anN such thatNe > S(e).", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "Separability assumption: Arnborg and Sj\u00f6din[5] define a separable model as one in which, for every x < y and c, there are n, m such that x < c < y, with x = F (x, x) and x = x.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "This is introduced as a weaker assumption than continuity of the auxiliary function F introduced in [1].", "startOffset": 100, "endOffset": 103}, {"referenceID": 20, "context": "There is no principled reason why models used could not be based on other types of models like neural networks, linguistic coding or case based reasoning, and indeed efforts have been made to view these techniques as special types of Bayesian models[23, 38].", "startOffset": 249, "endOffset": 257}, {"referenceID": 35, "context": "There is no principled reason why models used could not be based on other types of models like neural networks, linguistic coding or case based reasoning, and indeed efforts have been made to view these techniques as special types of Bayesian models[23, 38].", "startOffset": 249, "endOffset": 257}], "year": 2015, "abstractText": "We present and examine a result related to uncertainty reasoning, namely that a certain plausibility space of Cox\u2019s type can be uniquely embedded in a minimal ordered field. This, although a purely mathematical result, can be claimed to imply that every rational method to reason with uncertainty must be based on sets of extended probability distributions, where extended probability is standard probability extended with infinitesimals. This claim must be supported by some argumentation of non-mathematical type, however, since pure mathematics does not tell us anything about the world. We propose one such argumentation, and relate it to results from the literature of uncertainty and statistics. In an added retrospective section we discuss some developments in the area regarding countable additivity, partially ordered domains and robustness, and philosophical stances on the Cox/Jaynes approach since 2003. We also show that the most general partially ordered plausibility calculus embeddable in a ring can be represented as a set of extended probability distributions or, in algebraic terms, is a subdirect sum of ordered fields. In other words, the robust Bayesian approach is universal. This result is exemplified by relating Dempster-Shafer\u2019s evidence theory to robust Bayesian analysis.", "creator": "LaTeX with hyperref package"}}}