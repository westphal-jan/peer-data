{"id": "1511.07401", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "MazeBase: A Sandbox for Learning from Games", "abstract": "This stamped introduces an environment for simple 2D pits twice, utilized there while post-modern for roll useful complicated to reasoning two services. Within how, might our 10 fashioned series based after positing specific (e. 9. egalitarianism shapes n't - up statements ). We deploy a including. neural developed (it lines, convolutional network, can network) on these eight, with and same a mindboggling producing curriculum. We show without numerous dynamically do otherwise trained with loads all second-highest outstanding also these properly, but well still far from parameters, end past genuine. We first supposed themselves customized to events involving deploy, these StarCraft, experimentation themselves ability whether way activities - trivial tactics this allowing them taken although draw of in - nba AI.", "histories": [["v1", "Mon, 23 Nov 2015 20:23:53 GMT  (434kb,D)", "https://arxiv.org/abs/1511.07401v1", null], ["v2", "Thu, 7 Jan 2016 18:41:14 GMT  (166kb,D)", "http://arxiv.org/abs/1511.07401v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["sainbayar sukhbaatar", "arthur szlam", "gabriel synnaeve", "soumith chintala", "rob fergus"], "accepted": false, "id": "1511.07401"}, "pdf": {"name": "1511.07401.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sainbayar Sukhbaatar"], "emails": ["sainbar@cs.nyu.edu", "aszlam@fb.com", "gab@fb.com", "soumith@fb.com", "robfergus@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Games have had an important role in artificial intelligence research since the inception of the field. Core problems such as search and planning can explored naturally in the context of chess or Go (Bouzy & Cazenave, 2001). More recently, they have served as a test-bed for machine learning approaches (Perez et al., 2014). For example, Atari games (Bellemare et al., 2013) have been investigated using neural models with reinforcement learning (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015). The GVG-AI competition (Perez et al., 2014) uses a suite of 2D arcade games to compare planning and search methods.\nIn this paper we introduce the MazeBase game environment, which complements existing frameworks in several important ways.\n\u2022 The emphasis is on learning to understand the environment, rather than on testing algorithms for search and planning. The framework deliberately lacks any simulation facility, and hence agents cannot use search methods to determine the next action unless they can predict future game states themselves. On the other hand, game components are meant to be reused in different games, giving models the opportunity to comprehend the function of, say, a water tile. Nor are rules of the games provided to the agent, instead they must be learned through exploration of the environment.\n\u2022 The environment has been designed to allow programmatic control over the game difficulty. This allows the automatic contruction of curricula (Bengio et al., 2009), which we show to be important for training complex models.\n\u2022 Our games are based around simple algorithmic reasoning, providing a natural path for exploring complex abstract reasoning problems in a language-based grounded setting. This contrasts with most games that were originally designed for human enjoyment, rather than any specific task. It also differs from the recent surge of work on learning simple algorithms (Zaremba & Sutskever, 2015; Graves et al., 2014; Zaremba et al., 2015), which lack grounding.\nar X\niv :1\n51 1.\n07 40\n1v 2\n[ cs\n.L G\n] 7\nJ an\n\u2022 Despite the 2D nature of the environment, we prefer to use a text-based, rather than pixelbased, representation. This provides an efficient but expressive representation without the overhead of solving the perception problem inherent in pixel based representations. It easily allows for different task specifications and easy generalization of the models to other game settings. We demonstrate this by training models in MazeBase and then successfully evaluating them on StarCraftTM\u2217. See Mikolov et al. (2015) for further discussion.\nUsing the environment, we introduce a set of 10 simple games and use them to train range of standard neural network-based models (MLPs and Convnets) via policy gradient (Williams, 1992). We also combine the recent MemN2N model (Sukhbaatar et al., 2015) with a reinforcement learning and evaluate it on the games. The results show that current approaches struggle, despite the relatively simple nature of tasks. They also highlight clear areas for future model exploration, but we defer this for further work. MazeBase is an open-source platform, implemented using Torch and can be downloaded from https://github.com/facebook/MazeBase."}, {"heading": "1.1 RELATED WORK", "text": "The MazeBase environment can be thought of as a small practical step towards of some of the ideas discussed at length in Mikolov et al. (2015). In particular, interfacing the agent and the environment in (quasi-)natural language was inspired by discussions with the authors of that work. However, our ambitions are more local, focusing on the border where current models fail (but nearly succeed), rather than aiming for a global view of a path towards AI. For example, we specifically avoid algorithmic tasks that require unbounded recursions or loops, as we find that there is plenty of difficulty in learning simple if-then statements. Furthermore, for the example games described below, we allow large numbers of training runs, as the noise from reinforcement with discrete actions remains challenging even with many samples.\nIn non-game environments, there has been recent work on learning simple algorithms. (Graves et al., 2014; Vinyals et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) demonstrating tasks such as sorting and reversal of inputs. The algorithms instantiated in our games are even simpler, e.g. conditional statements or navigation to a location, but involve interaction with an environment. In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces. Our games also involve discrete actions, and these works inform our choice of the reinforcement learning techniques. Several works have also demonstrated the ability of neural models to learn to answer questions in simple natural language within a restricted environment (Weston et al., 2015b; Sukhbaatar et al., 2015). The tasks we present here share many features with those in Weston et al. (2015a), and the input-output format our games use is inter-operable with their stories. However, during training and testing, the environment in Weston et al. (2015a) is static, unlike the game worlds we consider.\nDeveloping AI for game agents has an extensive literature. Our work is similar to Mnih et al. (2013); Guo et al. (2014); Mnih et al. (2015) in that we use reinforcement and neural models when training on games. The GVG-AI competition (Perez et al., 2014) is similar in overall intent to MazeBase, but differs in that it is more appropriate for testing search-based methods, since a simulator (and game rules) are provided. Correspondingly, many of the top algorithms in the competition rely on Monte-Carlo tree search methods. In contrast, MazeBase is designed to be a sandbox that supports the development of learning-based algorithms; any search done by an agent must be done with the agent\u2019s own predictions. Similarly, competitions have been organized around Super Mario (Shaker et al., 2010), and Pacman (N. & Ito, 2011) and encourage search based on heuristics of the specific game. On the other hand, MazeBase is designed to encourage learning algorithms that can understand the environment and reuse knowledge between games, and to be used for the incremental exploration of certain core AI problems, for example the basic logical reasoning addressed in this paper. Furthermore (Shaker et al., 2010), (N. & Ito, 2011) do not easily support an algorithmic curriculum for training. Also note that given the sandbox nature of MazeBase, in principle it could be used to recreate any of these games, including those in the ACE benchmark (Bellemare et al., 2013). The versatility of MazeBase is demonstrated by the ease with which we were able to create a proxy for StarCraft combat within the environment. Our environment takes many basic ideas from\n\u2217StarCraft and Brood War are registered trademarks of Blizzard Entertainment, Inc.\nthe classical Puddle World [Rajendran et al. (2015)], for example the basic 2-d grid structure and water obstacles, but is richer, and the agent is not expected to memorize any given world, as they are regenerated at each new game, and agents are tested on unseen worlds."}, {"heading": "2 ENVIRONMENT AND TASKS", "text": "Each game is played in a 2D rectangular grid. In the specific examples below, the dimensions range from 3 to 10 on each side, but of course these can be set however the user likes. Each location in the grid can be empty, or may contain one or more items. The agent can move in each of the four cardinal directions, assuming no item blocks the agents path. The items in the game are:\n\u2022 Block: an impassible obstacle that does not allow the agent to move to that grid location. \u2022 Water: the agent may move to a grid location with water, but incurs an additional cost of\n(fixed at \u22120.2 in the games below) for doing so. \u2022 Switch: a switch can be in one of m states, which we refer to as colors. The agent can\ntoggle through the states cyclically by a toggle action when it is at the location of the switch.\n\u2022 Door: a door has a color, matched to a particular switch. The agent may only move to the door\u2019s grid location if the state of the switch matches the state of the door.\n\u2022 PushableBlock: This block is impassable, but can be moved with a separate \u201cpush\u201d actions. The block moves in the direction of the push, and the agent must be located adjacent to the block opposite the direction of the push.\n\u2022 Corner: This item simply marks a corner of the board. \u2022 Goal: depending on the task, one or more goals may exist, each named individually. \u2022 Info: these items do not have a grid location, but can specify a task or give information\nnecessary for its completion.\nThe environment is presented to the agent as a list of sentences, each describing an item in the game. For example, an agent might see \u201cBlock at [-1,4]. Switch at [+3,0] with blue color. Info: change switch to red.\u201d Such representation is compatible with the format of the bAbI tasks, introduced in Weston et al. (2015a). However, note that we use egocentric spatial coordinates (e.g. the goal G1 in Fig. 1 (left) is at coordinates [+2,0]), meaning that the environment updates the locations of each object after an action\u2020. Furthermore, for tasks involving multiple goals, we have two versions of the game. In one, the environment automatically sets a flag on visited goals. In the harder versions, this mechanism is absent but the agent has a special action that allows it to release a \u201cbreadcrumb\u201d into the environment, enabling it to record locations it has visited. In the experiments below, unless otherwise specified, we report results on games with the explicit flag.\nThe environments are generated randomly with some distribution on the various items. For example, we usually specify a uniform distribution over height and width (between 5 and 10 for the experiments reported here), and a percentage of wall blocks and water blocks (each range randomly from 0 to 20%)."}, {"heading": "2.1 TASKS", "text": "Although our game world is simple, it allows for a rich variety of tasks. In this work, we explore those that require different algorithmic components first in isolation and then in combination. These components include:\n\u2022 Set operations: iterating through a list of goals, or negation of a list, i.e. all items except those specified in a list.\n\u2022 Conditional reasoning: if-then statements, while statements. \u2022 Basic Arithmetic: comparison of two small numbers. \u2022 Manipulation: altering the environment by toggling a Switch, or moving a Pushable-\nBlock. \u2020This is consistent with Sukhbaatar et al. (2015), where the \u201cagent\u201d answering the questions was also given\nthem in egocentric temporal coordinates.\nThese were selected as key elements needed for complex reasoning tasks, although we limit ourselves here to only combining a them few of them in any given task. We note that most of these have direct parallels to the bAbI tasks (except for Manipulation which is only possible in our non-static environment). We avoid tasks that require unbounded loops or recursion, as in Joulin & Mikolov (2015), and instead view \u201calgorithms\u201d more in the vein of following a recipe from a cookbook. In particular, we want our agent to be able to follow directions; the same game world may host multiple tasks, and the agent must decide what to do based on the \u201cInfo\u201d items. As we demonstrate, standard neural models find this to be challenging.\nIn all of the tasks, the agent incurs a fixed penalty for each action it makes, this encourages the agent to finish the task promptly. In the experiments below, this is set to 0.1. In addition, stepping on a Water block incurs an additional penalty of 0.2. For most games, a maximum of 50 actions are allowed. The tasks define extra penalties and conditions for the game to end.\n\u2022 Multigoals: In this task, the agent is given an ordered list of goals as \u201cInfo\u201d, and needs to visit the goals in that order. In the experiments below, the number of goals ranges from 2 to 6, and the number of \u201cactive\u201d that the agent is required to visit ranges from 1 to 3 goals. The agent is not given any extra penalty for visiting a goal out of order, but visiting a goal before its turn does not count towards visiting all goals. The game ends when all goals are visited. This task involves the algorithmic component of iterating over a list.\n\u2022 Exclusion: The \u201cInfo\u201d in this game specifies a list of goals to avoid. The agent should visit all other unmentioned goals. The number of all goals ranges form 2 to 6, but the number of active goals ranges from 1 to 3. As in the Conditional goals game, the agent incurs a 0.5 penalty when it steps on a forbidden goal. This task combines Multigoals (iterate over set) with set negation.\n\u2022 Conditional Goals: In this task, the destination goal is conditional on the state of a switch. The \u201cInfo\u201d is of the form \u201cgo to goal gi if the switch is colored cj , else go to goal gl.\u201d In the experiments below, the number of the number of colors range from 2 to 6 and the number of goals from 2 to 6. Note that there can be more colors than goals or more goals than colors. The task concludes when the agent reaches the specified goal; in addition, the agent incurs a 0.2 penalty for stepping on an incorrect goal, in order to encourage it to read the info (and not just visit all goals). The task requires conditional reasoning in the form of an if-then statement.\n\u2022 Switches: In this task, the game has a random number of switches on the board. The agent is told via the \u201cInfo\u201d to toggle all switches to the same color, and the agent has the choice of color; to get the best reward, the agent needs to solve a (very small) traveling salesman problem. In the experiments below, the number of switches ranges from 1 to 5 and the number of colors from 1 to 6. The task finishes when the switches are correctly toggled. There are no special penalties in this task. The task instantiates a form of while statement.\n\u2022 Light Key: In this game, there is a switch and a door in a wall of blocks. The agent should navigate to a goal which may be on the wrong side of a wall of blocks. If the goal is on the same side of the wall as the agent, it should go directly there; otherwise, it needs move to and toggle the switch to open the door before going to the goal. There are no special penalties in this game, and the game ends when the agent reaches the goal. This task combines if-then reasoning with environment manipulation. \u2022 Goto: In this task, the agent is given an absolute location on the grid as a target. The game\nends when the agent visits this location. Solving this task requires the agent to convert from its own egocentric coordinate representation to absolute coordinates. This involves comparison of small numbers. \u2022 Goto Hidden: In this task, the agent is given a list of goals with absolute coordinates, and\nthen is told to go to one of the goals by the goal\u2019s name. The agent is not directly given the goal\u2019s location, it must read this from the list of goal locations. The number of goals ranges from 1 to 6. The task also involves very simple comparison operation. \u2022 Push block: In this game, the agent needs to push a Pushable block so that it lays on top of\na switch. Considering the large number of actions needed to solve this task, the map size is limited between 3 and 7, and the maximum block and water percentage is reduced to 10%. The task requires manipulation of the environment. \u2022 Push block cardinal: In this game, the agent needs to push a Pushable block so that it is on\na specified edge of the maze, e.g. the left edge. Any location along the edge is acceptable. The same limitation as Push Block game is applied. \u2022 Blocked door: In this task, the agent should navigate to a goal which may lie on the\nopposite side of a wall of blocks, as in the Light Key game. However, a PushableBlock blocks the gap in the wall instead of a door. This requires if-then reasoning, as well as environment manipulation.\nFor each task, we compute offline the optimal solution. For some of the tasks, e.g. Multigoals, this involves solving a traveling salesman problem (which for simplicity is done approximately). This provides an upper bound for the reward achievable. This is used for comparison purposes only, i.e. it is not used for training the models.\nWith the exception of the Multigoals task, all these are Markovian; and Multigoals is Markovian with the explicit \u201cvisited\u201d flag, which we use in the experiments below. Nevertheless, the tasks are not at all simple; although the environment can easily be used to build non-Markovian tasks, we find that the solving these tasks without the agent having to reason about its past actions is already challenging. Examples of each game are shown at https://youtu.be/kwnp8jFRi5E. Note that building new tasks is an easy operation in the MazeBase environment, indeed many of those above are implemented in few hundred lines of code."}, {"heading": "3 MODELS", "text": "We investigate several different types of model: (i) simple linear, (ii) multi-layer neural nets, (iii) convolutional nets and (iv) end-to-end memory networks (Weston et al., 2015b; Sukhbaatar et al., 2015). While the input format is quite different for each approach (detailed below), the outputs are the same: a probability distribution over set of discrete actions {N,S,E,W,toggle switch,pushN,push-S,push-E,push-W}; and a continuous baseline value predicting the expected reward. We do not consider models that are recurrent in the state-action sequence such as RNNs or LSTMs, because as discussed above, these tasks are Markovian.\nLinear: For a simple baseline we take the existence of each possible word-location pair on the largest grid we consider (10 \u00d7 10) and each \u201cInfo\u201d item as a separate feature, and train a linear classifier to the action space from these features. To construct the input, we take bag-of-words (excluding location words) representation of all items at the same location. Then, we concatanate all those features from the every possible locations and info items. For example, if we had n different words and w\u00d7 h possible locations with k additional info items, then the input dimension would be (w \u00d7 h+ k)\u00d7 n. Multi-layer Net: Neural network with multiple fully connected layers separated by tanh nonlinearity. The input representation is the same as the linear model.\nConvolutional Net: First, we represent each location by bag-of-words in the same way as linear model. Hence the environment is presented as a 3D cube of size w \u00d7 h \u00d7 n, which is then feed to four layers of convolution (the first layer has 1\u00d71 kernel, which essentially makes it an embedding of words). Items without spatial location (e.g. \u201cInfo\u201d items) are each represented as a bag of words, and then combined via a fully connected layer to the outputs of the convolutional layers; these are then passed through two fully connected layers to output the actions (and a baseline for reinforcement).\nMemory Network: Each item in the game (both physical items as well as \u201cinfo\u201d) is represented as bag-of-words vectors. The spatial location of each item is also represented as a word within the bag. E.g. a red door at [+3,-2] becomes the vector {red door} + {x=+3,y=-2}, where {red door} and {x=+3,y=-2} are word vectors of dimension 50. These embedding vectors will be learned at training time. As a consequence, the memory network has to learn the spatial arrangement of the grid, unlike the convolutional network. Otherwise, we use the architecture from (Sukhbaatar et al., 2015) with 3 hops and tanh nonlinearities."}, {"heading": "4 TRAINING PROCEDURES", "text": "We use policy gradient (Williams, 1992) for training, which maximizes the expected reward using its unbiased gradient estimates. First, we play the game by feeding the current state xt to the model, and sampling next action at from its output. After finishing the game, we update the model parameters \u03b8 by\n\u2206\u03b8 = T\u2211 t=1\n[ \u2202 log p(at|xt, \u03b8)\n\u2202\u03b8\n( T\u2211 i=t ri \u2212 b )] ,\nwhere rt is reward given at time t, and T is the length of the game.\nInstead of using a single baseline b value for every state, we let the model output a baseline value specific to the current state. This is accomplished by adding an extra head to models for outputting the baseline value. Beside maximizing the expected reward with policy gradient, the models are also trained to minimize the distance between the baseline value and actual reward. The final update rule is\n\u2206\u03b8 = T\u2211 t=1 \u2202 log p(at|xt, \u03b8) \u2202\u03b8 ( T\u2211 i=t ri \u2212 b(xt, \u03b8) ) \u2212 \u03b1 \u2202 \u2202\u03b8 ( T\u2211 i=t ri \u2212 b(xt, \u03b8) )2 . Here hyperparameter \u03b1 is for balancing the two objectives, which is set to 0.03 in all experiments. The actual parameter update is done by RMSProp (Tieleman & Hinton, 2012) with learning rates optimized for each model type.\nFor better parallelism, the model plays and learns from 512 games simultaneously, which spread on multiple CPU threads. Training is continued for 20 thousand such parallel episodes, which amounts to 10M game plays. Depending on the model type, the whole training process took from few hours to few day on 18 CPUs of a single machine."}, {"heading": "4.1 CURRICULUM", "text": "A key feature of our environment is the ability to programmatically vary all the properties of a given game. We use this ability to construct instances of each game whose difficulty is precisely specified (see Fig. 2). These instances can then be shaped into a curriculum for training (Bengio et al., 2009). As we demonstrate, this is very important for avoiding local minima and helps to learn superior models.\nEach game has many variables that impact the difficulty. Generic ones include: maze dimensions (height/width) and the fraction of blocks & water. For switch-based games (Switches, Light Key) the number of switches and colors can be varied. For goal based games (Multigoals,Conditional Goals, Exclusion, the variables are the number of goals (and active goals). For the combat game Kiting (see Section 6), we vary the number of agents & enemies, as well as their speed and their initial health.\nThe curriculum is specified by an upper and lower success thresholds Tu and Tl respectively. If the success rate of the model falls outside the [Tl, Tu] interval, then the difficulty of the generated games is adjusted accordingly. Each game is generated by uniformly sampling each variable that affects difficulty over some range. The upper limit of this range is adjusted, depending on which of Tl or\nTu is violated. Note that the lower limit remains unaltered, thus the easiest game remains at the same difficulty. For the last third of training, we expose the model to the full range of difficulties by setting the upper limit to its maximum preset value."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "Fig. 3 shows the performance of different models on the games. Each model is trained jointly on all 10 games. Given the stochastic nature of reinforcement learning, we trained each model 10 times and picked the single instance that had the highest mean reward across all tasks (i.e. the same model is evaluated on all 10 tasks). Table 2 in the Appendix gives the max,mean and standard devision of rewards for each task and method. A video showing a trained MemNN model playing each of the games can be found at https://youtu.be/kwnp8jFRi5E. The results revealed a number of interesting points.\nOn many of the games at least some of the models were able to learn a reasonable strategy. The models were all able to learn to convert between egocentric and absolute coordinates by using the corner blocks. They could respond appropriately to the different arrangements of the Light Key game, and make decent decisions on whether to try to go directly to the goal, or to first open the door. The 2-layer networks were able to completely solve the the tasks with pushable blocks.\nThat said, despite the simplicity of the games, and the number of trials allowed, the models were not able to completely solve (i.e. discover optimal policy) most of the games:\n\u2022 On Conditional Goals and Exclusion, all models did poorly. On inspection, it appears they adopted the strategy of blindly visiting all goals, rather than visiting the correct one.\n\u2022 With some of the models, we were able to train jointly, but make a few of the game types artificially small; then at test time successfully run those games on a larger map. The models were able to learn the notion of the locations independently from the task (for locations they had seen in training). On the other hand, we tried to test the models above on unseen tasks that were never shown at train time, but used the same vocabulary (for example: \u201cgo to the left\u201d, instead of \u201dpush the block to the left\u201d). None of our models were able to succeed, highlighting how far we are from operating at a \u201chuman level\u201d, even in this extremely restricted setting.\nWith respect to comparisons between the models:\n\u2022 On average, the memory network did best out of the methods. However, on the games with pushable blocks, the 2 layer neural nets were superior e.g. Exclusion and Push Block or the the same Blocked Door. Although we also trained 3 layer neural net, the result are not included here because it was very similar to the rewards of 2 layer neural net.\n\u2022 The linear model did better than might be expected, and surprisingly, the convolutional nets were the worst of the four models. However, the fully connected models had significantly\nmore parameters than either the convolutional network or the memory network. For example, the 2 layer neural net had a hidden layer of size 50, and a separate input for the outer product of each location and word combination. Because of the size of the games, this is 2 orders of magnitude more parameters than the convolutional network or memory network. Nevertheless, even with very large number of trials, this architecture could not learn many of the tasks. \u2022 The memory network seems superior on games involving decisions using information in\nthe info items (e.g. Multigoals) whereas the 2-layer neural net was better on the games with a pushable block ( Push Block, Push Block Cardinal, and Blocked Door). Note that because we use egocentric coordinates, for Push Block Cardinal, and to a lesser extent Push Block, the models can memorize all the local configurations of the block and agent. \u2022 All methods had a significant variance in performance over its 10 instances, except for the\nlinear model. However, the curriculum significantly decreased the variance for all methods, especially for 2-layer neural net.\nWith respect to different training modalities:\n\u2022 The curriculum generally helped all approaches, but gave the biggest assistance to the memory networks, particularly for Push Block and related games. \u2022 We also tried supervised training (essentially imitation learning), but the results were more\nor less the same as for reinforcement. The one exception was learning to use the Breadcrumb action for the Multigoals game. None of our models were able to learn to use the breadcrumb to mark visited locations without supervision. Note that in the tables we show results with the explicit \u201cvisited\u201d flag given by the environment."}, {"heading": "6 COMBAT GAMES", "text": "In this section we use MazeBase to implement several simple combat games. We train agents using these games and then test them on combat micro-management in StarCraft: Brood War, involving a limited number of troops:\n\u2022 Kiting (Terran Vulture vs Protoss Zealot): a match-up where we control a weakly armored fast ranged unit, against a more powerful but melee ranged unit. To win, our unit needs to alternate fleeing the opponents and shooting at them when its weapon has cooled down.\n\u2022 Kiting hard (Terran Vulture vs 2 Protoss Zealots): same as above but with 2 enemies. \u2022 2 vs 2 (Terran Marines): a symmetric match-up where both teams have 2 ranged units.\nOur goal here is not to asses the ability of the models to generalize, but rather whether we can make the game in our environment close to its counterpart in StarCraft, to show that the environment can be used for prototyping scenarios where training on an actual game may be technically challenging.\nIn MazeBase, the kiting scenario consists of a standard maze where an agent aims to kill up to two enemy bots. After each shot, the agent or enemy is prevented from firing again for a small time interval (cooldown). We introduce an imbalance by (i) allowing the agent to shoot farther than the enemy bot(s) and (ii) giving the agent significantly less health than the bot(s); and by allowing the enemy bot(s) to shoot more frequently than the agent (shorter cooldown). The agent has a shot range of 7 squares, and the bots have a shot range of 4 squares. The enemy bot(s) moves (on average) at .6 the speed of the agent. This is accomplished by rolling a \u201cfumble\u201d each time the bot tries to move with probability .4. The agent has health chosen uniformly between 2 and 4, and the enemy(s) have health uniformly distributed between 4 and 11. The enemy can shoot every 2 turns, and the agent can shoot every 6 turns. The enemy follows a heuristic of attacking the agent when in range and its cooldown is 0, and attempting to move towards the agent when it is closer than 10 squares away, and ignoring when farther than 10 squares.\nThe 2 vs. 2 scenario is modeled in MazeBase with two agents, each of which have 3 health points, and two bots, which have hitpoints randomly chosen from 3 or 4. Agents and bots have a range of 6 and a cooldown of 3. The bots use a heuristic of attacking the closest agent if they have not attacked an agent before, and continuing to attack and follow that agent until it is killed.\nIn both the kiting and 2x2 scenarios, we randomly add noise to the agents inputs to account for the fact that it will encounter new vocabulary when playing StarCraft. That is, 10% of the time, the numerical value of the enemies\u2019 or agents\u2019 health, cooldown, etc is taken to be a random value. We train a MemNN model using the difference between the armies hit points, and win or loss of the overall battle, as the reward signals.\nWe also run the scenarios inside StarCraft: Brood War. We used BWAPI (2008-) to connect the game to our Torch framework. We can receive the game state and send orders, enabling us to do a reinforcement learning loop. We train a 2 layer neural network and MemNN models using the same protocol. The features used are all categorical (as for MazeBase) and represent the hit points (health), the weapon cooldown, and the x and y positions of the unit. We used a multi-resolution encoding (coarser going further from the unit we control) of the position on 256\u00d7256 map to reduce the number of parameters. In case of the multiple units, each is controlled independently. We take an action every 8 frames (the atomic time unit @ 24 frames/sec). The architectures and hyper-parameter settings are the same as used in the Kiting game (except the multi-resolution feature map).\nWe find that the models are able to learn basic tactics such as focusing their fire on weaker opponents to kill them first (thus reducing the total amount of incoming damage over the game). This results in a win rate of 80% over the built-in StarCraft AI on 2 vs 2, and nearly perfect results on Kiting (see Table 1). The video https://youtu.be/Hn0SRa_Uark shows example gameplay of our MemNN model for the StarCraft Kiting hard scenario.\nFinally, we test the models trained in our environment directly on StarCraft. We make no modifications to the models, and minimal changes to the interface (we scale the health by a factor of 10, and x,y and cooldown values by a factor of 4). The success rate of the models trained in the maze but tested in Starcraft is comparable to training directly in StarCraft, showing that our environment can be effectively used as a sandbox for exploring model architectures and hyper-parameter optimization."}, {"heading": "7 DISCUSSION", "text": "The MazeBase enivronment allows easy creation of games and precise control over their behavior. This allowed us to quickly to devise a set of 10 simple games embodying algorithmic components and evalaute them using a range of neural models. The flexibility of the environment enabled curricula to be created for each game which aided the training of the models and resulted in superior test performance. Even with the aid of a curriculum, in most cases the models fell short of optimal performance. The memory networks were able to solve some tasks that the fully-connected models and\nconvnets could not, although overall the performance was similar. This suggests that existing neural models lack some fundamental abilities that are needed to solve algorithmic reasoning. Potential candidates include: the ability to plan or forecast the outcome of actions and a more sophisticated memory (it is notable that the MemNN outperformed the others on tasks with involving large info items).\nWe also showed how the MazeBase environment can be used to develop model architectures that when trained on StarCraft can convincingly beat the in-game AI in a range of simple combat settings. More indirectly, we can also use the environment to build games that approximate a task of interest, enabling the training of models that will perform effectively on the target task, without having exposure to it during training.\nAPPENDIX"}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Computer Go: an AI oriented survey", "author": ["Bouzy", "Bruno", "Cazenave", "Tristan"], "venue": "Artificial Intelligence,", "citeRegEx": "Bouzy et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bouzy et al\\.", "year": 2001}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "In arXiv preprint:", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": null, "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "A roadmap towards machine intelligence", "author": ["Mikolov", "Tomas", "Joulin", "Armand", "Baroni", "Marco"], "venue": "CoRR, abs/1511.08130,", "citeRegEx": "Mikolov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Monte-carlo tree search in ms. pac-man", "author": ["Ikehata", "T. Ito"], "venue": "In Proceedings of IEEE Conference on Computational Intelligence and Games,", "citeRegEx": "N. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "N. et al\\.", "year": 2011}, {"title": "The GVG-AI competition", "author": ["D. Perez", "S. Samothrakis", "J. Togelius", "T. Schaul", "S. Lucas"], "venue": "In http://www.gvgai.net/index.php,", "citeRegEx": "Perez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Perez et al\\.", "year": 2014}, {"title": "Adaapt: A deep architecture for adaptive policy transfer from multiple sources", "author": ["J. Rajendran", "P. Prasanna", "B. Ravindran", "M. Khapra"], "venue": "In arXiv:1510.02879v2,", "citeRegEx": "Rajendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajendran et al\\.", "year": 2015}, {"title": "The 2010 mario AI championship: Level generation track", "author": ["N. Shaker", "J. Togelius", "G. Yannakakis", "B. Weber", "T. Shimizu", "N. Hashiyama", "P. Soreson", "P Pasquier", "G. Mawhorter", "G. Takahashi", "R. Smith", "R. Baumgarten"], "venue": "In In special Issue of IEEE Transactions on Procedural Content Generation,", "citeRegEx": "Shaker et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shaker et al\\.", "year": 2010}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "In arXiv preprint:", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "In Machine Learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In arXiv preprint:", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Zaremba", "Wojciech", "Mikolov", "Tomas", "Joulin", "Armand", "Fergus", "Rob"], "venue": "CoRR, abs/1511.07275,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "More recently, they have served as a test-bed for machine learning approaches (Perez et al., 2014).", "startOffset": 78, "endOffset": 98}, {"referenceID": 0, "context": "For example, Atari games (Bellemare et al., 2013) have been investigated using neural models with reinforcement learning (Mnih et al.", "startOffset": 25, "endOffset": 49}, {"referenceID": 6, "context": ", 2013) have been investigated using neural models with reinforcement learning (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015).", "startOffset": 79, "endOffset": 135}, {"referenceID": 3, "context": ", 2013) have been investigated using neural models with reinforcement learning (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015).", "startOffset": 79, "endOffset": 135}, {"referenceID": 7, "context": ", 2013) have been investigated using neural models with reinforcement learning (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015).", "startOffset": 79, "endOffset": 135}, {"referenceID": 9, "context": "The GVG-AI competition (Perez et al., 2014) uses a suite of 2D arcade games to compare planning and search methods.", "startOffset": 23, "endOffset": 43}, {"referenceID": 2, "context": "It also differs from the recent surge of work on learning simple algorithms (Zaremba & Sutskever, 2015; Graves et al., 2014; Zaremba et al., 2015), which lack grounding.", "startOffset": 76, "endOffset": 146}, {"referenceID": 16, "context": "It also differs from the recent surge of work on learning simple algorithms (Zaremba & Sutskever, 2015; Graves et al., 2014; Zaremba et al., 2015), which lack grounding.", "startOffset": 76, "endOffset": 146}, {"referenceID": 5, "context": "See Mikolov et al. (2015) for further discussion.", "startOffset": 4, "endOffset": 26}, {"referenceID": 12, "context": "We also combine the recent MemN2N model (Sukhbaatar et al., 2015) with a reinforcement learning and evaluate it on the games.", "startOffset": 40, "endOffset": 65}, {"referenceID": 5, "context": "The MazeBase environment can be thought of as a small practical step towards of some of the ideas discussed at length in Mikolov et al. (2015). In particular, interfacing the agent and the environment in (quasi-)natural language was inspired by discussions with the authors of that work.", "startOffset": 121, "endOffset": 143}, {"referenceID": 2, "context": "(Graves et al., 2014; Vinyals et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) demonstrating tasks such as sorting and reversal of inputs.", "startOffset": 0, "endOffset": 94}, {"referenceID": 6, "context": "In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces.", "startOffset": 28, "endOffset": 135}, {"referenceID": 3, "context": "In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces.", "startOffset": 28, "endOffset": 135}, {"referenceID": 7, "context": "In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces.", "startOffset": 28, "endOffset": 135}, {"referenceID": 12, "context": "Several works have also demonstrated the ability of neural models to learn to answer questions in simple natural language within a restricted environment (Weston et al., 2015b; Sukhbaatar et al., 2015).", "startOffset": 154, "endOffset": 201}, {"referenceID": 2, "context": "(Graves et al., 2014; Vinyals et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) demonstrating tasks such as sorting and reversal of inputs. The algorithms instantiated in our games are even simpler, e.g. conditional statements or navigation to a location, but involve interaction with an environment. In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces. Our games also involve discrete actions, and these works inform our choice of the reinforcement learning techniques. Several works have also demonstrated the ability of neural models to learn to answer questions in simple natural language within a restricted environment (Weston et al., 2015b; Sukhbaatar et al., 2015). The tasks we present here share many features with those in Weston et al. (2015a), and the input-output format our games use is inter-operable with their stories.", "startOffset": 1, "endOffset": 995}, {"referenceID": 2, "context": "(Graves et al., 2014; Vinyals et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) demonstrating tasks such as sorting and reversal of inputs. The algorithms instantiated in our games are even simpler, e.g. conditional statements or navigation to a location, but involve interaction with an environment. In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces. Our games also involve discrete actions, and these works inform our choice of the reinforcement learning techniques. Several works have also demonstrated the ability of neural models to learn to answer questions in simple natural language within a restricted environment (Weston et al., 2015b; Sukhbaatar et al., 2015). The tasks we present here share many features with those in Weston et al. (2015a), and the input-output format our games use is inter-operable with their stories. However, during training and testing, the environment in Weston et al. (2015a) is static, unlike the game worlds we consider.", "startOffset": 1, "endOffset": 1155}, {"referenceID": 9, "context": "The GVG-AI competition (Perez et al., 2014) is similar in overall intent to MazeBase, but differs in that it is more appropriate for testing search-based methods, since a simulator (and game rules) are provided.", "startOffset": 23, "endOffset": 43}, {"referenceID": 11, "context": "Similarly, competitions have been organized around Super Mario (Shaker et al., 2010), and Pacman (N.", "startOffset": 63, "endOffset": 84}, {"referenceID": 11, "context": "Furthermore (Shaker et al., 2010), (N.", "startOffset": 12, "endOffset": 33}, {"referenceID": 0, "context": "Also note that given the sandbox nature of MazeBase, in principle it could be used to recreate any of these games, including those in the ACE benchmark (Bellemare et al., 2013).", "startOffset": 152, "endOffset": 176}, {"referenceID": 4, "context": "Our work is similar to Mnih et al. (2013); Guo et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 2, "context": "(2013); Guo et al. (2014); Mnih et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 2, "context": "(2013); Guo et al. (2014); Mnih et al. (2015) in that we use reinforcement and neural models when training on games.", "startOffset": 8, "endOffset": 46}, {"referenceID": 10, "context": "the classical Puddle World [Rajendran et al. (2015)], for example the basic 2-d grid structure and water obstacles, but is richer, and the agent is not expected to memorize any given world, as they are regenerated at each new game, and agents are tested on unseen worlds.", "startOffset": 28, "endOffset": 52}, {"referenceID": 14, "context": "\u201d Such representation is compatible with the format of the bAbI tasks, introduced in Weston et al. (2015a). However, note that we use egocentric spatial coordinates (e.", "startOffset": 85, "endOffset": 107}, {"referenceID": 12, "context": "\u2020This is consistent with Sukhbaatar et al. (2015), where the \u201cagent\u201d answering the questions was also given them in egocentric temporal coordinates.", "startOffset": 25, "endOffset": 50}, {"referenceID": 12, "context": "We investigate several different types of model: (i) simple linear, (ii) multi-layer neural nets, (iii) convolutional nets and (iv) end-to-end memory networks (Weston et al., 2015b; Sukhbaatar et al., 2015).", "startOffset": 159, "endOffset": 206}, {"referenceID": 12, "context": "Otherwise, we use the architecture from (Sukhbaatar et al., 2015) with 3 hops and tanh nonlinearities.", "startOffset": 40, "endOffset": 65}], "year": 2016, "abstractText": "This paper introduces MazeBase: an environment for simple 2D games, designed as a sandbox for machine learning approaches to reasoning and planning. Within it, we create 10 simple games embodying a range of algorithmic tasks (e.g. if-then statements or set negation). A variety of neural models (fully connected, convolutional network, memory network) are deployed via reinforcement learning on these games, with and without a procedurally generated curriculum. Despite the tasks\u2019 simplicity, the performance of the models is far from optimal, suggesting directions for future development. We also demonstrate the versatility of MazeBase by using it to emulate small combat scenarios from StarCraft. Models trained on the MazeBase version can be directly applied to StarCraft, where they consistently beat the in-game AI.", "creator": "LaTeX with hyperref package"}}}