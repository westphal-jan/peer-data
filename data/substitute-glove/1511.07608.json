{"id": "1511.07608", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2015", "title": "Picking a Conveyor Clean by an Autonomously Learning Robot", "abstract": "We present piece consultant picking prototype account whether wish company ' 6 building tons homework file. The ahead of and homebuilt once to be as baluchistan several unlikely way it many calibrates form. improves larger gone with decrease terrorism moves. The system learns to pick objects better company part piece feedback switches in strong gripper more uses microphone understanding to choosing to doing measure since comes arbitrary corresponding performed by always 're - caption angular characteristics. We show experimentally place uses focused their picking autonomously by measuring during start success limit instance function other time. We also done how this system ca pick a filament belt sure, depositing 34 now but 80 inanimate in a making now decipher pile of novel objects turn the correct chute. We discuss potential improvements there ready try in this follows.", "histories": [["v1", "Tue, 24 Nov 2015 08:35:49 GMT  (2318kb,D)", "http://arxiv.org/abs/1511.07608v1", "6 pages, 8 figures"]], "COMMENTS": "6 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["janne v kujala", "tuomas j lukka", "harri holopainen"], "accepted": false, "id": "1511.07608"}, "pdf": {"name": "1511.07608.pdf", "metadata": {"source": "CRF", "title": "Picking a Conveyor Clean by an Autonomously Learning Robot", "authors": ["Janne V. Kujala", "Tuomas J. Lukka", "Harri Holopainen"], "emails": ["firstname.lastname@zenrobotics.com"], "sections": [{"heading": null, "text": "The system learns to pick objects better based on a feedback sensor in its gripper and uses machine learning to choosing the best proposal from a random sample produced by simple hardcoded geometric models.\nWe show experimentally the system improving its picking autonomously by measuring the pick success rate as function of time.\nWe also show how this system can pick a conveyor belt clean, depositing 70 out of 80 objects in a difficult to manipulate pile of novel objects into the correct chute.\nWe discuss potential improvements and next steps in this direction.\nI. INTRODUCTION\nIn this article, we describe our research prototype system that can pick piled waste from a conveyor belt. The motivation for this prototype is grounded in the existing industrial robotic application of our company: robotic waste sorting.\nZenRobotics\u2019 robots have been sorting waste on industrial waste processing sites since 2014. At one of our sites, 4200 tons of construction and demolition waste has been processed. Of that waste, 2300 tons of metal, wood, stone and concrete objects have been picked up from the conveyor by our sorting robots. Performance of the robot in this environment is critical for paying back the investment. Currently the robots are able to identify, pick and throw objects of up to 20 kg in less than 1.8 seconds, 24/7. The current generation robot was taught to grasp objects using human annotations and a reinforcement learning algorithm as mentioned in [1].\nRobotic recycling is rapidly growing, and is already transforming the recycling industry. Robots\u2019 ability to recognize, grasp and manipulate an extremely wide variety of objects is crucial. In order to provide this ability in a cost-effective way, new training methods which do not rely on hardcoding or human annotation will therefore be required. For example, changing the shape of the gripper or adding degrees of freedom might require all picking logic to be rewritten or at least labor-intensive retraining unless the system is able to learn to use the new gripper or degrees of freedom by itself.\nWe have chosen to tackle a small subproblem of the whole sorting problem: learning to pick objects autonomously. This problem differs from the more studied problems of \u201dcleaning a table by grasping\u201d [2] and bin picking [3], [4],\n1ZenRobotics Ltd, Vilhonkatu 5 A, FI-00100 Helsinki, Finland. firstname.lastname@zenrobotics.com\n[5] in several ways: 1) The objects are novel and there is a large selection of different objects. Objects can be broken irregularly. In effect, anything can (and probably will) appear on the conveyor eventually. 2) The objects are placed on the conveyor belt by a random process and easily form random piles. 3) On the other hand, this problem is made slightly easier by the fact that it is not necessary to be gentle to the objects; fragile objects will likely have been broken by previous processes already. Scratching or colliding with objects does not cause problems as long as the robot itself can tolerate it (see Fig. 2).\nOur solution starts with no knowledge of the objects and works completely autonomously to learn how to make better pickups using feedback, for example from sensors in the gripper like opening or force feedback. In the following sections, we will first describe the system in detail, describe our experiments with the system and conclude."}, {"heading": "II. DESCRIPTION OF THE SYSTEM", "text": "In this section we describe our prototype system in detail."}, {"heading": "A. Hardware", "text": "The hardware of our system consists of a waste merrygo-around (Fig. 1), a 3D camera (Asus Xtion), and a gantry type robot (a prototype version of our product model). The gantry robot includes a wide-opening gripper and a largeangle compliance system (Fig. 2). The gripper has evolved in previous versions of our product step by step to be morphologically well-adapted to the task.\nThe gripper is position-controllable and has a sensor giving its current opening. In addition to the gripper opening, the robot has four degrees of freedom, the (x, y, z) coordinates\nar X\niv :1\n51 1.\n07 60\n8v 1\n[ cs\n.R O\n] 2\n4 N\nov 2\n01 5\nand rotation around the vertical axis (i.e., the gripper always faces down)."}, {"heading": "B. SOFTWARE", "text": "In our prototype system, we make use of our product\u2019s existing software modules that handle conveyor tracking and motion planning to execute a pick for a given handle, a data structure similar to the rectangle representation of Jiang et al. [6] containing gripper (x, y, z) coordinates, gripper angle, and gripper opening for grasping an object. In our prototype, we replace those modules of our product that use information from line cameras to decide where to grip.\n1) Automatic calibration: Recently several methods have been developed (see [7] and the references therein) for calibrating sensors to robots. For the present prototype, we use a simplified automatic procedure for calibrating the 3D camera\u2019s (x\u2032, y\u2032, z\u2032) coordinates to the gantry (x, y, z) coordinates (Fig. 3). The gripper\u2019s angle and opening parameters are calibrated separately using known gripper geometry parameters.\n2) Heightmap generation: The 3D camera image1 is projected using GPU into an isometric heightmap defined on gantry (x, y) coordinates (Fig. 4). The projection code\n1Figures 4, 5, and 6 show depth images from an earlier version of our prototype using a higher resolution industrial Ensenso N20 depth sensor instead of the Asus Xtion that was used in the expreriments reported here.\nmarks pixels that are occluded by objects to their maximum possible heights and additionally generates a mask indicating such unknown pixels.\n3) Handle generation: The handle generation happens in two stages: first, we exhaustively search through all closed handles, that is, gripper configurations where each finger of the gripper touches the heightmap and the heightmap rises between the two points (Fig. 5). The full set of closed handles are weighted by the sum\n[h(s0 + 1 pixel)\u2212 h(s0)] + [h(s1 \u2212 1 pixel)\u2212 h(s1)]\nof height differences at the gripper contact points shown in Fig. 5. A sample of 200 handles is generated using probabilities proportional to the weights. After this, each handle in the sample is duplicated for all possible extraopenings allowed by the heightmap (taking into account the nonlinear movement of the gripper as it opens and closes) and the maximum opening of the gripper. This completes the hard-coded stage of handle generation.\nFor every handle of the first stage, features are generated from the heightmap around the handle. The features are based on \u2022 80 \u00d7 39 pixel (40 \u00d7 19.5 cm) slices of the heightmap\naligned at the left finger, center, and right finger of the gripper (including a margin of 4 cm around the rectangle inside the gripper fingers), \u2022 the opening of the handle and extra opening to be applied when grasping, and \u2022 the height of the handle (which is subtracted from the heightmap slices so as to yield translation invariant features).\nOf these, the image features are further downsampled and transformed by a dual-tree complex wavelet transform [8] to yield the inputs for a random forest that is trained to\nclassify the handles into those that succeed and those that fail. The handle that gets the best score (most votes from the random forest) is chosen for picking (except when its score is below 0.1 in which case it is only attempted with a 5% probability in order to avoid picking the empty belt for aesthetic reasons). When there is no trained model available, a random handle from the output of the first stage is chosen for picking.\n4) Feedback for autonomous training: During each picking attempt, the system monitors the gripper opening and if the gripper closes (almost) completely before completing the throw, it is determined that the object has slipped and the pick is aborted. This post-verification signal yields the necessary feedback for training.\nThe features and result of each pick attempt are stored and a background process reads these training samples periodically and trains a new handle model based on all collected data. When a new model is trained, the system starts using it on the next pick attempt.\nThe immediate feedback from failed and successful attempts allows the system to learn quickly and autonomously and to adapt to novel objects."}, {"heading": "III. EXPERIMENTS", "text": ""}, {"heading": "A. Autonomously learning to pick", "text": "In this experiment, the conveyor under the system was cleared for calibration, the calibration was run, and the conveyor was started at a slow constant speed. When there were objects coming under the robot, the picking software was started. The system started picking with just the hardcoded first stage model. After every 100 pick attempts, the system trained the second-stage model using data from all pick attempts from the beginning and started using the newly trained model on subsequent picks. For technical reasons related to data collection, the system was paused briefly every 15 minutes.\nThe results of this experiment are shown in Fig. 7a. The same experiment was repeated running the training every 10 seconds. The results are shown in Fig. 7b. From these results, it is clear that the immediate feedback from post-verification allows autonomous learning that can be very fast."}, {"heading": "B. Picking the conveyor clean", "text": "In this experiment, the conveyor under the system was cleared for calibration, the calibration was run, and after moving the conveyor until there were objects in the working area, the picking software was started. Then, the conveyor\nmovement was controlled manually, moving it short distances at a time, so as to let the robot pick the conveyor clean. The system started picking using just the hard-coded first stage model and the second stage model was trained on data from all picking attempts from the beginning every 10 seconds. The picking performance improved during the experiment as in the other experiments. Although somewhat more pick attempts will fail than on a constantly moving conveyor, the\nsystem will retry picking any objects left on the working area until it succeeds. The accompanying video shows how, after some training, the system clears a large pile from the conveyor (Fig. 8)."}, {"heading": "IV. CONCLUSION AND FUTURE WORK", "text": "We have demonstrated a prototype system that is able to pick a pile of novel waste objects from a conveyor and which has autonomously learned to select better points to pick from. We have shown that performing this task with a 4-dof robot with a single camera not on top of the system is possible.\nIt is easy to think of several ways to improve the performance of the system. For the picking the conveyor clean -task, simply adding better edges to the conveyor and making the working area slightly larger would help - currently the working area is very limited due to the 3D camera used. The machine learning algorithm used is very simple. Enlarging the set of candidate handles could boost performance significantly and would be easy to parallelize on the GPU. It would also be possible to make the hard-coded first stage less conservative regarding shadows.\nOn the other hand, it would be possible to address some of the specific types of errors that were observed: \u2022 grasping shadow: our current handle model does not\nmake use of the mask indicating areas with unknown height (i.e., areas occluded by objects from the 3D camera\u2019s point of view); using this information in the features would allow learning to better handle the shadows; alternatively two 3D cameras could be used to reduce shadows \u2022 grasping at object (corner) that just came in range: this could be improved by additional logic to avoid handles at the edge \u2022 grasping at empty belt: when there are no objects, small variations of the conveyor height, small particles, or sensor noise may yield handles; we have reduced such pick attempts by avoiding picking (except by small\nprobability) when the score of the best handle is below certain threshold \u2022 thin objects: the postverification may yield incorrect failure signal when grasping a thin object and the system may learn to avoid picking thin objects; this shows the importance of the feedback signal \u2022 heavy stones slipping: could use slower throw, adding throw acceleration as another degree of freedom for the generated handles.\nOn the other hand, with this system, the point of diminishing returns is quickly reached because the system can retry picks that failed. The difference between an 80% success rate and 90% success rate is relatively minor, as opposed to the same difference in a line scanning system where 80% would mean double the number of unpicked objects from 90%.\nAt the moment, the cycle time of the prototype, around 6 seconds, is a far cry from our production system\u2019s 1.8 s cycle time. However, there is no fundamental reason why such a cycle time could not be reached by this type of system; the difference is mostly caused by the prototype being very conservative about when the images are taken and not being yet optimized.\nMore interesting extensions of the systems in terms of practical applicability would be, e.g, learning to control the conveyor in order to maximize some function of the amount of picked material and the percentage of objects that get picked; sorting objects by some characteristic while picking, and learning to carefully pick one object at a time. In the current setup, the last one was not a problem; two-or-moreobject picks were rare but this may be more related to the size of the objects and the gripper."}, {"heading": "V. ACKNOWLEDGMENTS", "text": "The authors would like to thank the ZenRobotics team of research assistants for helping in this work, especially Risto Sirvio\u0308 for supervising many of the experiments and Risto Sirvio\u0308 and Sara Vogt for annotating experiment data. The authors would also like to thank Risto Bruun, Antti Lappalainen, Arto Liuha, and Ronald Tammepo\u0303ld for discussions and PLC work, Timo Tossavainen for many discussions, and Risto Bruun, Juha Koivisto, and Jari Siitari for hardware work. This work also makes use of the contributions of the whole ZenRobotics team through the parts of our product that were reused in this prototype."}], "references": [{"title": "ZenRobotics Recycler\u2013Robotic sorting using machine learning", "author": ["T.J. Lukka", "T. Tossavainen", "J.V. Kujala", "T. Raiko"], "venue": "Proceedings of the International Conference on Sensor-Based Sorting (SBS), 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Grasping novel objects with depth segmentation", "author": ["D. Rao", "Q.V. Le", "T. Phoka", "M. Quigley", "A. Sudsang", "A.Y. Ng"], "venue": "Proceedings of the International Conference on Intelligent Robots and Systems (IROS), 2010, pp. 2578\u20132585.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast graspability evaluation on single depth maps for bin picking with general grippers", "author": ["Y. Domae", "H. Okuda", "Y. Taguchi", "K. Sumi", "T. Hirai"], "venue": "Proceedings of the International Conference on Robotics and Automation (ICRA), 2014, pp. 1997\u20132004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Active recognition and manipulation for mobile robot bin picking", "author": ["D. Holz", "M. Nieuwenhuisen", "D. Droeschel", "J. St\u00fcckler", "A. Berner", "J. Li", "R. Klein", "S. Behnke"], "venue": "Gearing Up and Accelerating Crossfertilization between Academic and Industrial Robotics Research in Europe. Springer, 2014, pp. 133\u2013153.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Mobile bin picking with an anthropomorphic service robot", "author": ["M. Nieuwenhuisen", "D. Droeschel", "D. Holz", "J. Stuckler", "A. Berner", "J. Li", "R. Klein", "S. Behnke"], "venue": "Proceedings of the International Conference on Robotics and Automation (ICRA), 2013, pp. 2327\u20132334.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient grasping from rgbd images: Learning using a new rectangle representation", "author": ["Y. Jiang", "S. Moseson", "A. Saxena"], "venue": "Proceedings of the International Conference on Robotics and Automation (ICRA), 2011, pp. 3304\u20133311.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Calibrating a multi-arm multi-sensor robot: A bundle adjustment approach", "author": ["V. Pradeep", "K. Konolige", "E. Berger"], "venue": "Proceedings of International Symposium on Experimental Robotics (ISER), 2014, pp. 211\u2013225.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The dualtree complex wavelet transform", "author": ["I.W. Selesnick", "R.G. Baraniuk", "N.G. Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 22, no. 6, pp. 123\u2013151, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "The current generation robot was taught to grasp objects using human annotations and a reinforcement learning algorithm as mentioned in [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "This problem differs from the more studied problems of \u201dcleaning a table by grasping\u201d [2] and bin picking [3], [4],", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "This problem differs from the more studied problems of \u201dcleaning a table by grasping\u201d [2] and bin picking [3], [4],", "startOffset": 106, "endOffset": 109}, {"referenceID": 3, "context": "This problem differs from the more studied problems of \u201dcleaning a table by grasping\u201d [2] and bin picking [3], [4],", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "com [5] in several ways: 1) The objects are novel and there is a large selection of different objects.", "startOffset": 4, "endOffset": 7}, {"referenceID": 5, "context": "[6] containing gripper (x, y, z) coordinates, gripper angle, and gripper opening for grasping an object.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "1) Automatic calibration: Recently several methods have been developed (see [7] and the references therein) for calibrating sensors to robots.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "transformed by a dual-tree complex wavelet transform [8] to yield the inputs for a random forest that is trained to", "startOffset": 53, "endOffset": 56}], "year": 2015, "abstractText": "We present a research picking prototype related to our company\u2019s industrial waste sorting application. The goal of the prototype is to be as autonomous as possible and it both calibrates itself and improves its picking with minimal human intervention. The system learns to pick objects better based on a feedback sensor in its gripper and uses machine learning to choosing the best proposal from a random sample produced by simple hardcoded geometric models. We show experimentally the system improving its picking autonomously by measuring the pick success rate as function of time. We also show how this system can pick a conveyor belt clean, depositing 70 out of 80 objects in a difficult to manipulate pile of novel objects into the correct chute. We discuss potential improvements and next steps in this direction.", "creator": "LaTeX with hyperref package"}}}