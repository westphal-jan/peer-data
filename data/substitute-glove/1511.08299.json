{"id": "1511.08299", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "Hierarchical classification of e-commerce related social media", "abstract": "In the magazines, let allowing to quantify netizens same root these among with Amazon users induces analytic uses came back of twitter into enables node ID labels, a much larger will of tweets without contained, most end back related Amazon weekly. Examining app data presents unlike governance took might followed samples should short (under 138 characters) others often multiple misspellings or lower-level would are generalization brought a human to eludes while might for man lab to parse. A variety of interface by authorization expansion techniques are implemented in critical make move achieve information retrieval to modest success.", "histories": [["v1", "Thu, 26 Nov 2015 06:57:06 GMT  (463kb,D)", "http://arxiv.org/abs/1511.08299v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.CL cs.IR cs.LG", "authors": ["matthew long", "aditya jami", "ashutosh saxena"], "accepted": false, "id": "1511.08299"}, "pdf": {"name": "1511.08299.pdf", "metadata": {"source": "CRF", "title": "Hierarchical classification of e-commerce related social media", "authors": ["Matthew Long", "Aditya Jami", "Ashutosh Saxena"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Internet users post information regarding a topic on a number of different websites, but companies and organizations typically only train their classification algorithms using only the information posted on their own platform. Obviously, data from competitors is often difficult to acquire, but in cases where it is freely available, cross-platform analysis can only benefit a model as data from other sources can be used only if it improves performance. In order for this data to be valuable, it has to be correctly classified by what it refers to.\nThe goal of this project is to to find a likely product category within the root categories of the Amazon browse node hierarchy for a given tweet. Twitter data consisted of a training dataset with 58,000 tweets labeled with Amazon browse node IDs, and a much larger set of 15,000,000 unlabeled tweets that can be used for augmentation. The Amazon data consisted of 1,900,000 reviews for products labeled by their browse node ID. All of the datasets originally were in JSON format and contained metadata as well as text content for each review or tweet. To obtain root nodes for tweets, a browse node ID tree was created so that a simple parent traversal could identify a root category. The Amazon product hierarchy is more of a directed graph in that it children categories can have multiple parents. In these\ncases, the parent is chosen randomly. 28 root categories were identified from the browse nodes within the labeled dataset, but the distribution was heavily skewed, with 47,000 tweets in the books root category, and 10 or fewer in 5 categories. Furthermore, over half of the tweets were re-tweets, which have the same text content as the original tweet, providing no additional information to a classifier while inflating accuracy misleadingly. Once re-tweets and tweets from categories with fewer than 5 tweets were removed, the labeled corpus contained 23,910 tweets from 24 root categories."}, {"heading": "2 Method", "text": ""}, {"heading": "2.1 Preprocessing", "text": "As the data consists of text strings, a bag-of-words model was used to represent the tweets. To reduce feature size and trim unhelpful data, all the tweets were converted to lower case and stripped of all punctuation except hashtags. Additionally, URLs and stop words from both a list within the Natural Language Toolkit and a list we developed specifically for Twitter were removed and words were stemmed with the WordNet Lemmatizer [1][5]. With 5-fold cross validation, corresponding to an 80/20 training/testing split, the unprocessed tweets had 48,000 unique words, which got truncated to 22,213 words after pre-processing. Text was then transformed to a sparse matrix representation of TF-IDF features in order to be acceptable for downstream estimators. This weighting scheme was chosen because it weights against words that show up frequently across all documents and thus implicitly reflects the importance of a word in a document [2][7]. TF-IDF refers to the term-frequency\nar X\niv :1\n51 1.\n08 29\n9v 1\n[ cs\n.S I]\n2 6\nN ov\n2 01\n5\nmultiplied by the inverse document frequency and is calculated as,\ntfij = f\n\u2211 i fi j\nidfi = log N dfi"}, {"heading": "2.2 Baseline Models", "text": "To evaluate the impact of our tests, we compared different learning algorithms performance when trained on the preprocessed dataset with all features. To ensure that there were both training and testing examples for each category a stratified 5-fold cross-validation was used to split up the dataset into training and testing sets. The metrics associated with each classifier indicate the unweighted mean of the metrics for each category. We choose to evaluate model quality in this fashion because of the imbalanced nature of the labeled dataset. The vectorization of the corpus and the training of the models were done using the Scikit-Learn package[6].\nClass weights provide a way of correcting for imbalance by weighting for or against certain classes but would be difficult to tune for each technique we will explore[9]. For this reason, an unweighted linear SVM will be used as the baseline against which to measure the effectiveness of our approach, although class weights will be used for final model. The evaluation metric for these comparisons will be the F1-score, as it combines precision and recall into a single number.\nF1 = 2 \u00b7 precision \u00b7 recall precision+ recall"}, {"heading": "2.3 Feature Selection", "text": "Features were ranked according to their Anova F-values and models were evaluated when trained on the top n percent of features[8]. We trained models for unigram features and unigram and bigram features.\nIt is clear from Figure 2, that precision and recall in the test set stabilize after using around 20% of the features in both the unigram and unigram+bigram cases. As the F1-score for both of these cases were roughly similar, and the absolute number of features for a given percentage is much lower for only unigram features, we decided to use 25% of the unigram features for our models."}, {"heading": "2.4 Expansion", "text": "As tweets are shorter than typical documents, expanding them seems reasonable as it improves the vocabulary of the model[3]. In order to improve classification accuracy, we considered query expansion, in which terms are added\nto testing tweets, and document expansion, in which terms are added to training tweets. Both topics are areas of research in Information Retrieval (IR), although query expansion is the more promising, and thus more studied field[4]."}, {"heading": "2.4.1 Document", "text": "Tweets from the training set were expanded based upon hashtags contained within them and the root category they belonged to. To perform hashtag expansion a thesaurus was built up of the most frequent words in tweets containing a given hashtag using the large unlabeled Twitter dataset. n randomly selected words from the top 2n words from each hashtag were then added to each tweet containing that hashtag. No words from the stop lists would be added, nor would the hashtag word. For root category expansion, one thesaurus was built using for each category using the words in the training set portion of the labeled tweets and another was built for the reviews in the Amazon set. When building the thesaurus for root category expansion using Twitter, the top words for each category were chosen with a TF-IDF weighting scheme, however, because the corpus the thesaurus was built upon was much smaller allowing the process to be computationally feasible."}, {"heading": "2.4.2 Query", "text": "As the hashtag thesaurus was built from an external dataset, hashtag expansion could be used on tweets from the testing set portion of the labeled tweets as well. An identical process to document hashtag expansion was used."}, {"heading": "3 Results", "text": ""}, {"heading": "3.1 Expansion", "text": "Tweet expansion saw mixed results in category classification. Hashtag expansion on both the training and testing set marginally improved performance, while hashtag expansion on each set exclusively worsened\nperformance. Amazon node expansion achieved similar results as the base case model, while Twitter node expansion significantly decreased performance. Figure 3 details the results expansion for various expansion lengths."}, {"heading": "3.2 Overall", "text": "In the final model, we used both hashtag document and query expansion and also added class weights to the linear SVM classifier. The class weighting scheme that was added was primarily directed at reducing the effects of the imbalance toward the books category so a weight of 0.1 was applied to that category, while other categories weighted by 1[9]. Additionally, the C parameter of the SVM estimator was tuned using the GridSearch function of Scikit-Learn and a value of 5 was selected. Table 4 shows the results of our final model."}, {"heading": "4 Discussion", "text": "The model achieved an average F1-score across all categories of 0.61 with average precision of 81% and average recall of 54%. Categories with more tweets tended to be classified more accurately than tweets with few samples to draw upon. This makes intuitive sense as the vocabulary of the samples in the small\ncategories is limited so there are high odds that the testing samples do not contain the same words as in the training samples. This is representative of the fact that the bound on generalization error decreases as the sample size increases, so naturally larger categories are capable of better testing accuracy. Figure 5 demonstrates this rough trend. Query expansion is typically regarded to be more effective than document expansion and the only thing we expanded in the test set were hashtags[4]. Many tweets do not contain any hashtags, so the effects of query expansion was only received by a fraction of the test set. It is clear that using external datasets (ie. Amazon, unlabeled twitter) to augment the labeled twitter set do not decrease performance. Less clear, however, is whether these dataset can be better leveraged to significantly improve performance."}, {"heading": "5 Future Work", "text": "The next step to take would be to build up a thesaurus on individual words from both Amazon and unlabeled Twitter data in order to expand testing and training tweets on a per word basis. Building these thesauruses will be space intensive because for each word the frequency of all the other words it has appeared with in a tweet or review has to be stored. This step holds promise as it could be used for both query and document expansion and could be used upon all tweets. With a full word thesaurus, selective expansion could also be explored, where only certain categories are expanded. There are existing thesauruses that can be downloaded such as WordNet, but the frequent use of abbreviations and slang on Twitter makes building a thesaurus from a corpus of tweets potentially more\nbeneficial[5]. Another step that would provide immediate benefits is building a larger corpus for under-represented categories. An alternative to hand labeling additional tweets would be to make use of semi-supervised learning techniques that can leverage the large unlabeled dataset to improve performance."}], "references": [{"title": "Natural Language Processing with Python, OReilly", "author": ["S. Bird", "E. Loper", "E Klein"], "venue": "Media Inc,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Schtze"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Document Expansion Based on WordNet for Robust IR", "author": ["E. Agirre", "X. Arregi", "A. Otegi"], "venue": "Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Document Expansion versus Query Expansion for Ad-hoc Retrieval", "author": ["B. Billerbeck", "J. Zobel"], "venue": "Proceedings of the Tenth Australasian Document Computing Symposium,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Term-weighting approaches in automatic text retrieval,Information", "author": ["G. Salton", "C. Buckley"], "venue": "Processing and Management,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1988}, {"title": "An overview of classification algorithms for imbalanced datasets, International", "author": ["V. Ganganwar"], "venue": "Journal of Emerging Technology and Advanced Engineering,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Additionally, URLs and stop words from both a list within the Natural Language Toolkit and a list we developed specifically for Twitter were removed and words were stemmed with the WordNet Lemmatizer [1][5].", "startOffset": 200, "endOffset": 203}, {"referenceID": 1, "context": "This weighting scheme was chosen because it weights against words that show up frequently across all documents and thus implicitly reflects the importance of a word in a document [2][7].", "startOffset": 179, "endOffset": 182}, {"referenceID": 4, "context": "This weighting scheme was chosen because it weights against words that show up frequently across all documents and thus implicitly reflects the importance of a word in a document [2][7].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "Class weights provide a way of correcting for imbalance by weighting for or against certain classes but would be difficult to tune for each technique we will explore[9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "As tweets are shorter than typical documents, expanding them seems reasonable as it improves the vocabulary of the model[3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Both topics are areas of research in Information Retrieval (IR), although query expansion is the more promising, and thus more studied field[4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "1 was applied to that category, while other categories weighted by 1[9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "Query expansion is typically regarded to be more effective than document expansion and the only thing we expanded in the test set were hashtags[4].", "startOffset": 143, "endOffset": 146}], "year": 2015, "abstractText": "In this paper, we attempt to classify tweets into root categories of the Amazon browse node hierarchy using a set of tweets with browse node ID labels, a much larger set of tweets without labels, and a set of Amazon reviews. Examining twitter data presents unique challenges in that the samples are short (under 140 characters) and often contain misspellings or abbreviations that are trivial for a human to decipher but difficult for a computer to parse. A variety of query and document expansion techniques are implemented in an effort to improve information retrieval to modest success.", "creator": "LaTeX with hyperref package"}}}