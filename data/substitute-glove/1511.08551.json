{"id": "1511.08551", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Regularized EM Algorithms: A Unified Framework and Statistical Guarantees", "abstract": "Latent variable vehicles are a implications interfaces communication in machine physical applications, put say present certain behavioral much analytical progress. The became EM algorithm bringing one variants, is piece much commonly formalization useful; most focus procedure achieve instance its technical except as critical. Recently, teaching place Balakrishnan \u0438 mubarak. (18-19) both undoubtedly that however an important for there unfortunately, EM exhibitions formula_3 activities convergence. In several as - subset bringing, once, the M - possibility eventually not should example consistent. We reform precisely though setting once perfect representation stress using regularization. While diophantine first short - spatial problems is by now several understood, the iterative EM generalization requires puts objective focusing major making initiative push making needed while identifying long own attached (card. 2b. , sparsity sometimes steady - rank ). In particular, regularizing with M - hope or next carolina - in - although - heritage high - isometric condoms (commentary. 1. , Wainwright (2014) ) but not obligations to able in balance. Our computable once analysis were alleged in piece making does reveals under balance only method in forecasting errors. We various what general aims taken sparse formula_4 mixture models, college - formula_2 reflecting euler, and disorientation all rest variables, obtaining geographical secure for each important these examples.", "histories": [["v1", "Fri, 27 Nov 2015 03:46:36 GMT  (793kb)", "https://arxiv.org/abs/1511.08551v1", "53 pages, 3 figures. A shorter version appears in NIPS 2015"], ["v2", "Sat, 5 Dec 2015 09:54:59 GMT  (793kb)", "http://arxiv.org/abs/1511.08551v2", "53 pages, 3 figures. A shorter version appears in NIPS 2015"]], "COMMENTS": "53 pages, 3 figures. A shorter version appears in NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["xinyang yi", "constantine caramanis"], "accepted": true, "id": "1511.08551"}, "pdf": {"name": "1511.08551.pdf", "metadata": {"source": "CRF", "title": "Regularized EM Algorithms: A Unified Framework and Statistical Guarantees", "authors": ["Xinyang Yi"], "emails": ["yixy@utexas.edu", "constantine@utexas.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n08 55\n1v 2\n[ cs\n.L G\n] 5\nD ec\nLatent variable models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M -step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M -step using the state-of-the-art high-dimensional prescriptions (e.g., a\u0300 la Wainwright (2014)) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples."}, {"heading": "1 Introduction", "text": "In this paper, we give general conditions and an analytical framework for the convergence of the EM method for high-dimensional parameter estimation in latent variable models. We specialize these conditions to several problems of interest, including high-dimensional sparse and low-rank mixed regression, sparse gaussian mixture models, and regression with missing covariates. As we explain below, the key problem in the high-dimensional setting is the M -step. A natural idea is to modify this step via appropriate regularization, yet choosing the appropriate sequence of regularizers is a critical problem. As we know from the theory of regularized M-estimators (e.g., Wainwright (2014)) the regularizer should be chosen proportional to the target estimation error. For EM, however, the target estimation error changes at each step.\nThe main contribution of our work is technical: we show how to perform this iterative regularization. We show that the regularization sequence must be chosen so that it converges to a quantity controlled by the ultimate estimation error. In existing work, the estimation error is given\nby the relationship between the population and empirical M -step operators, but the M -operator is not well defined in the high-dimensional setting. Thus a key step, related both to our algorithm and its convergence analysis, is obtaining a different characterization of statistical error for the high-dimensional setting."}, {"heading": "Background and Related Work", "text": "EM (e.g., Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently. Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow \u201cclose\u201d to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step.\nIn contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization. The central challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this must control the optimization error (related to the special structure of \u03b2\u2217), as well as the statistical error. Finally, we note that for finite mixture regression, Sta\u0308dler et al.Sta\u0308dler et al. (2010) consider an \u21131 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality. However, this work doesn\u2019t establish the theoretical properties of local optima arising from regularized EM. Our work addresses this issue from a local convergence perspective by using a novel choice of regularization. Notation: Let u = (u1, u2, . . . , up) \u22a4 \u2208 Rp be a vector and M = [Mi,j ] \u2208 Rp1\u00d7p2 be a matrix. The \u2113q norm of u is defined as \u2016u\u2016p = ( \u2211p\ni=1 |ui|q)1/q. We use \u2016M\u2016\u2217 to denote the nuclear norm of M and \u2016M\u20162 to denote its spectral norm. We use \u2299 to denote the Hadamard product between two vectors, i.e., u \u2299 v = (u1v1, u2v2, . . . , upvp)\u22a4. A p-by-p identity matrix is denoted as Ip. We use capital letter (e.g., X) to denote random variable, vector and matrix. For a sub-Gaussian (subexponential) random variable X, we use \u2016X\u2016\u03c82 (\u2016X\u2016\u03c81) to denote its Orlicz norm (see Vershynin (2010) for detailed definitions). For two functions f(n) and g(n), we use f(n) . g(n) to represent f(n) \u2264 Cg(n) for some absolute constant C > 0. In parallel, we use f(n) & g(n) to represent f(n) \u2265 C \u2032g(n) for some absolute constant C \u2032 > 0. For any differentiable function f : Rp \u2192 R, we use \u2207f to denote its gradient.\nThe rest of our paper is organized as follows. We present our regularized EM algorithm, including the precise sequence of regularization, and discuss its applications to several example models in Section 2. The specific examples to which we show our results apply, are sparse gaussian mixture models, sparse or low-rank mixed regression, and regression with missing covariates. In section 3, we establish our analytical framework and show the main theory, i.e., computational and statistical guarantees of the regularized EM algorithm. Then, by applying our main theory, we establish several near optimal statistical rate of those aforementioned models in section 4. Section 5 demonstrates our results through numerical examples. We outline the proof of our main result in section 6. The detailed proofs of other results and multiple technical lemmas are deferred to the appendix."}, {"heading": "2 Regularized EM Algorithm", "text": "In this section, we first present a general regularized EM algorithm in which a convex regularizer is used to enforce certain type of structure. Then we turn to revisit three well known latent variable models and show how the proposed algorithm can be applied to high dimensional parameter estimation in these models."}, {"heading": "2.1 Algorithm", "text": "Before introducing our approach, we first review the classic EM algorithm. Let Y ,Z be random variables taking values in Y,Z. Suppose they have join distribution\nf\u03b2(y, z)\ndepending on model parameter \u03b2 \u2286 \u2126 where \u2126 is some parameter space in Rp. In latent variable models, it is common to assume we can only obtain samples from Y while Z, called latent variable, can not be observed. Consider the marginal distribution of Y as\ny\u03b2(y) :=\n\u222b\nZ f\u03b2(y, z)dz.\nGiven n i.i.d. observations y1,y2, . . . ,yn of Y , our goal is to estimate the model parameter \u03b2. We consider the maximum likelihood estimation: compute \u03b2\u0302 \u2208 \u2126 that maximizes the log likelihood function, namely,\n\u03b2\u0302 = argmax \u03b2\u2208\u2126\nh(\u03b2;yn1 ), (2.1)\nwhere\nh(\u03b2;yn1 ) := 1\nn\nn\u2211\ni=1\nlog y\u03b2(yi).\nIn many settings, the objective function in (2.1) is highly nonconvex, thereby it\u2019s computationally inefficient to solve it directly. Instead, we turn to a lower bound of h(\u03b2;yn1 ) which is more friendly\nto evaluate and optimize. Let \u03ba\u03b2(z|y) denote the conditional distribution of Z given Y = y. For any \u03b2\u2032 \u2208 \u2126, we have\nh(\u03b2\u2032;yn1 ) = 1\nn\nn\u2211\ni=1\nlog y\u03b2\u2032(yi) = 1\nn\nn\u2211\ni=1\nlog\n\u222b\nZ f\u03b2\u2032(yi, z)dz\n= 1\nn\nn\u2211\ni=1\nlog\n\u222b\nZ \u03ba\u03b2(z|yi)\nf\u03b2\u2032(yi, z) \u03ba\u03b2(z|yi) dz\n(a) \u2265 1 n\nn\u2211\ni=1\n\u222b\nZ \u03ba\u03b2(z|yi) log\nf\u03b2\u2032(yi, z) \u03ba\u03b2(z|yi) dz\n= 1\nn\nn\u2211\ni=1\n\u222b\nZ \u03ba\u03b2(z|yi) log f\u03b2\u2032(yi, z)dz \u2212\n\u222b\nZ \u03ba\u03b2(z|yi) log \u03ba\u03b2(z|yi)dz, (2.2)\nwhere (a) follows from Jensen\u2019s inequality. The key idea of EM algorithm is to perform iterative maximization of the obtained lower bound (2.2). We denote the first term in (2.2) as function Qn(\u00b7|\u00b7), i.e.,\nQn(\u03b2 \u2032|\u03b2) := 1\nn\nn\u2211\ni=1\n\u222b\nZ \u03ba\u03b2(z|yi) log f\u03b2\u2032(yi, z)dz. (2.3)\nOne iteration of EM algorithm, mapping \u03b2(t) to \u03b2(t+1), consists of the following two steps:\n\u2022 E-step: Compute function Qn(\u03b2|\u03b2(t)) given \u03b2(t).\n\u2022 M-step: \u03b2(t+1) \u2190 argmax\u03b2\u2208\u2126 Qn(\u03b2|\u03b2(t)).\nIt\u2019s convenient to introduce mapping Mn : \u2126 \u2192 \u2126 to denote the above algorithm\nMn(\u03b2) := argmax \u03b2\u2032\u2208\u2126 Qn(\u03b2 \u2032|\u03b2). (2.4)\nWhen n \u2192 \u221e, we define the population level Q(\u00b7|\u00b7) function as\nQ(\u03b2\u2032|\u03b2) := \u222b\nY y\u03b2\u2217(y)\n\u222b\nZ \u03ba\u03b2(z|y) log f\u03b2\u2032(y, z)dzdy. (2.5)\nSimilar to (2.4), we define the population level mapping M : \u2126 \u2192 \u2126 as\nM(\u03b2) = argmax \u03b2\u2032\u2208\u2126\nQ(\u03b2\u2032|\u03b2). (2.6)\nGenerally, the classic EM procedure is not applicable to high dimensional regime where n \u226a p: First, with insufficient number of samples, Mn(\u03b2) is usually far way from M(\u03b2). In this case, even if the initial parameter is \u03b2\u2217, Mn(\u03b2\u2217) is not a meaningful estimation of \u03b2\u2217. As an example, in Gaussian mixture models, the minimum estimation error \u2016Mn(\u03b2\u2217)\u2212M(\u03b2\u2217)\u2016 can be much larger than signal strength \u2016\u03b2\u2217\u2016. Second, in some models, Mn(\u03b2) is not even well defined. For instance, in mixture linear regression, solving (2.4) involves inverting sample covariance matrix that is not full rank when n < p. (See Section 2.2.2 for detailed discussion.)\nWe now turn to our regularized EM algorithm that is designed to overcome the aforementioned high dimensionality challenges. In particular, we propose to replace the M-step with regularized\nAlgorithm 1 High Dimensional Regularized EM Algorithm Input Samples {yi}ni=1, regularizer R, number of iterations T , initial parameter \u03b2(0), initial regularization parameter \u03bb (0) n , estimated statistical error \u2206, contractive factor \u03ba < 1.\n1: For t = 1, 2, . . . , T do\n2: Regularization parameter update:\n\u03bb(t)n \u2190 \u03ba\u03bb(t\u22121)n +\u2206. (2.7)\n3: E-step: Compute function Qn(\u00b7|\u03b2(t\u22121)) according to (2.3). 4: Regularized M-step:\n\u03b2(t) \u2190 argmax \u03b2\u2208\u2126 Qn(\u03b2|\u03b2(t\u22121))\u2212 \u03bb(t)n \u00b7 R(\u03b2).\n5: End For\nOutput \u03b2(T ).\nmaximization step. In detail, for some convex regularizer R : \u2126 \u2192 R+ and user specified regularization regularization parameter \u03bbn, our regularized M-step is defined as:\nMrn(\u03b2) := argmax \u03b2\u2032\u2208\u2126 Qn(\u03b2 \u2032|\u03b2)\u2212 \u03bbnR(\u03b2\u2032). (2.8)\nWe present the details of our algorithm in Algorithm 1. The role of R is to enforce the solution to have a certain structure of the model parameter \u03b2\u2217.\nThe choice of regularization parameter \u03bb (t) n plays an important role in controlling statistical and optimization error. As stated in (2.7), the update of \u03bb (t) n involves a linear combination of old parameter \u03bb (t\u22121) n and the quantity \u2206. Then \u03bb (t) n takes the form\n\u03bb(t)n = \u03ba t\u03bb(0)n + 1\u2212 \u03bat 1\u2212 \u03ba \u2206.\nAs shown in Figure 1, \u03bb (t) n first decays geometrically from \u03bb (0) n and then it gradually approaches 1 1\u2212\u03ba \u00b7\u2206. Quantity \u2206 characterizes the target statistical error which depends on number of samples n, data dimension p and some factor associated with concrete models. Usually, we have \u2206 = O( \u221a log p/n), which vanishes when n, p increase with a fixed ratio. To provide some intuitions of such choice, we first note that from theory of high dimensional regularized M-estimator Wainwright (2014), suitable \u03bbn should be proportional to the target estimation error. Analogous to our setting, we let \u03bb (t) n be proportional to \u2016Mrn(\u03b2(t)) \u2212 \u03b2\u2217\u20162 which is the estimation error in step t. Consider the following triangle inequality\n\u2016Mrn(\u03b2(t))\u2212 \u03b2\u2217\u20162 \u2264 \u2016Mrn(\u03b2(t))\u2212Mrn(\u03b2\u2217)\u20162 + \u2016Mrn(\u03b2\u2217)\u2212 \u03b2\u2217\u20162.\nNote that the second term \u2016Mrn(\u03b2\u2217) \u2212 \u03b2\u2217\u20162 corresponds to quantity \u2206 since they both have the sense of final estimation error. The first term \u2016Mrn(\u03b2(t))\u2212Mrn(\u03b2\u2217)\u20162, resulting from optimization error \u2016\u03b2(t) \u2212 \u03b2\u2217\u20162, then corresponds to \u03ba\u03bb(t\u22121)n in (2.7). By setting t = 1, we observe that \u03bb(0)n is\n1\u2212\u03ba \u2206 represented by the red and blue lines respectively.\nproportional to the initialization error. Consequently, we have \u03bbtn \u2265 \u03ba\u03bb(t\u22121) +\u2206. Inspired by the low-dimensional analysis of EM in Balakrishnan et al. (2014), we expect the optimization error to decay geometrically, so we choose \u03ba \u2208 (0, 1). Beyond the intuition, we provide the rigorous analysis and detailed parameter update in Section 3."}, {"heading": "2.2 Example Models", "text": "Now we introduce three well known latent variable models. For each model, we review the specific formulations of standard EM algorithm, discuss the extensions in high dimensional setting, and provide the implementations of high dimensional regularized EM iterations."}, {"heading": "2.2.1 Gaussian Mixture Model", "text": "We consider the balanced isotropic Gaussian mixture model (GMM) with two components where the distribution of random variables (Y,Z) \u2208 Rp \u00d7 {\u22121, 1} is determined by\nPr (Y = y|Z = z) = \u03c6(y; z \u00b7 \u03b2\u2217, \u03c32Ip)\nand Pr(Z = 1) = Pr(Z = \u22121) = 1/2. Here we use \u03c6(\u00b7|\u00b5,\u03a3) to denote probability density function of N (\u00b5,\u03a3). In this example, Z is latent variable that indicates the cluster id of each sample. In this example, given n i.i.d. samples {yi}ni=1, function Qn(\u00b7|\u00b7) defined in (2.3) corresponds to\nQGMMn (\u03b2 \u2032|\u03b2) = \u2212 1\n2n\nn\u2211\ni=1\n[ w(yi;\u03b2)\u2016yi \u2212 \u03b2\u2032\u201622 + (1\u2212w(yi;\u03b2))\u2016yi + \u03b2\u2032\u201622 ] , (2.9)\nwhere\nw(y;\u03b2) := exp (\u2212\u2016y\u2212\u03b2\u2016 2 2 2\u03c32 )\nexp (\u2212\u2016y\u2212\u03b2\u2016 2 2 2\u03c32 ) + exp (\u2212\u2016y+\u03b2\u2016 2 2 2\u03c32 ) . (2.10)\nThen we have that the standard EM update (2.4) corresponds to\nMn(\u03b2) = 2\nn\nn\u2211\ni=1\nw(yi;\u03b2)yi \u2212 1\nn\nn\u2211\ni=1\nyi. (2.11)\nIn high dimensional regime, we assume \u03b2\u2217 is sparse. Formally, let B0(s; p) := {u \u2208 Rp : | supp(u)| \u2264 s}, we have \u03b2\u2217 \u2208 B0(s; p). Naturally, we choose regularizer R to be \u21131 norm in order to recover the sparse structure. Consequently, our regularized EM iteration (2.8) corresponds to\nMrn(\u03b2) = arg max \u03b2\u2032\u2208Rp QGMMn (\u03b2 \u2032|\u03b2)\u2212 \u03bbn\u2016\u03b2\u2032\u20161."}, {"heading": "2.2.2 Mixed Linear Regression", "text": "Mixed linear regression (MLR), as considered in some recent work (Chaganty and Liang, 2013; Yi et al., 2013; Chen et al., 2014b), is the problem of recovering two or more linear vectors from mixed linear measurements. In the case of mixed linear regression with two symmetric and balanced components, response-covariate pair (Y,X) \u2208 R\u00d7 Rp is linked through\nY = \u3008X, Z \u00b7 \u03b2\u2217\u3009+W,\nwhere W is noise term and Z is latent variable that has Rademacher distribution over {\u22121, 1}. We assume X \u223c N (0, Ip), W \u223c N (0, \u03c32). In this setting, with n i.i.d. samples {yi,xi}ni=1 of pair (Y,X), function Qn(\u00b7|\u00b7) then corresponds to\nQMLRn (\u03b2 \u2032|\u03b2) = \u2212 1\n2n\nn\u2211\ni=1\n[ w(yi,xi;\u03b2)(yi \u2212 \u3008xi,\u03b2\u2032\u3009)2 + (1\u2212 w(yi,xi;\u03b2))(yi + \u3008xi,\u03b2\u2032\u3009)2 ] , (2.12)\nwhere w(y,x;\u03b2) is defined as\nw(y,x;\u03b2) := exp (\u2212 (y\u2212\u3008x,\u03b2\u3009)2 2\u03c32 )\nexp (\u2212 (y\u2212\u3008x,\u03b2\u3009)2 2\u03c32 ) + exp (\u2212 (y+\u3008x,\u03b2\u3009)2 2\u03c32\n) .\nThe standard EM iteration (2.4) corresponds to\nMn(\u03b2) = ( n\u2211\ni=1\nxix \u22a4 i\n)\u22121( n\u2211\ni=1\n(2w(yi,xi;\u03b2)\u2212 1)yixi ) . (2.13)\nNote that (2.13) involves inverting sample covariance matrix. Therefore, in high dimensional setting Mn(\u03b2) is not well defined since sample covariance matrix has rank much smaller than the ambient dimension. As discussed earlier, characterizing statistical error in terms of Mn(\u03b2)\u2212M(\u03b2) is not well suited to this case.\nNext we consider two kinds of structure about \u03b2\u2217 in order to deal with high dimensionality. First we assume \u03b2\u2217 is an s-sparse vector, i.e., \u03b2\u2217 \u2208 B0(s; p). Then by using \u21131 regularizer, (2.8) corresponds to\nMrn(\u03b2) = arg max \u03b2\u2032\u2208Rp QMLRn (\u03b2 \u2032|\u03b2)\u2212 \u03bbn\u2016\u03b2\u2032\u20161.\nSecond we consider that the model parameter is a matrix \u0393\u2217 \u2208 Rp1\u00d7p2 with rank(\u0393\u2217) = \u03b8 \u226a min(p1, p2). We further assume X \u2208 Rp1\u00d7p2 is an i.i.d. Gaussian matrix, i.e., entries of X are independent random variables with distribution N (0, 1). Note that in low dimensional case n \u226b p1 \u00d7 p2, there is no essential difference between assuming parameter is vector and matrix since we can always treat X and \u0393\u2217 as (p1 \u00d7 p2)-dimensional vectors. In high dimensional regime, low rank structure leads to different regularization. We choose R to be nuclear norm to serve such structure. Consequently, given n samples with form {yi,Xi}ni=1, (2.8) then corresponds to\nMrn(\u0393) = arg max \u0393\u2032\u2208Rp1\u00d7p2 \u2212 1 2n\nn\u2211\ni=1\n[ w(yi,Xi;\u0393)(yi \u2212 \u3008Xi,\u0393\u2032\u3009)2+\n(1\u2212 w(yi,Xi;\u0393))(yi + \u3008Xi,\u0393\u2032\u3009)2 ] \u2212 \u03bbn\u2016\u0393\u2032\u2016\u2217. (2.14)\nThe standard low rank matrix recovery with a single component, including other sensing matrix designs beyond Gaussian matrix, has been studied extensively (e.g.,Cande\u0300s and Recht (2009); Recht et al. (2010); Cande\u0300s and Plan (2011); Negahban et al. (2011); Jain et al. (2013); Chen et al. (2013); Cai and Zhang (2015)). To the best of our knowledge, theoretical study of the mixed low rank matrix recover has not been considered in existing literature."}, {"heading": "2.2.3 Missing Covariate Regression", "text": "As our last example, we consider the missing covariate regression (MCR) problem. To be the same as standard linear regression, {yi,xi}ni=1 are samples of (Y,X) linked through Y = \u3008X,\u03b2\u2217\u3009 +W . However, we assume each entry of xi is missing independently with probability \u01eb \u2208 (0, 1). Therefore, the observed covariate x\u0303i takes the form\nx\u0303i,j = { xi,j with probability 1\u2212 \u01eb \u2217 otherwise .\nTo ease notation, we introduce vector zi \u2208 {0, 1}p to indicate the positions of missing entries, i.e., zi,j = 1 if xi,j is missing. In this example, the E step involves computing the distribution of missing entries given current parameter guess \u03b2. Under Gaussian design X \u223c N (0, Ip),W \u223c N (0, \u03c32), given observed covariate entries (1\u2212 zi)\u2299 xi and yi, the conditional mean vector of x\u0303i has form\n\u00b5\u03b2(yi, zi,xi) := E[x\u0303i \u2223\u2223\u03b2, yi, (1\u2212 zi)\u2299 xi] = (1\u2212 zi)\u2299 xi + yi \u2212 \u3008\u03b2, (1 \u2212 zi)\u2299 xi\u3009 \u03c32 + \u2016zi \u2299 \u03b2\u201622 zi \u2299 \u03b2, (2.15)\nand the conditional correlation matrix of x\u0303i has form\n\u03a3\u03b2(yi, zi,xi) := E [ x\u0303ix\u0303 \u22a4 i \u2223\u2223\u03b2, yi, (1 \u2212 zi)\u2299 xi ]\n= \u00b5\u03b2\u00b5 \u22a4 \u03b2 + diag(zi)\u2212\n( 1\n\u03c32 + \u2016zi \u2299 \u03b2\u201622\n) (zi \u2299 \u03b2)(zi \u2299 \u03b2)\u22a4. (2.16)\nConsequently, Qn(\u00b7|\u00b7) corresponds to\nQMCRn (\u03b2 \u2032|\u03b2) = 1\nn\nn\u2211\ni=1\n\u3008yi\u00b5\u03b2(yi, zi,xi),\u03b2\u2032\u3009 \u2212 1 2 \u03b2\u22a4\u03a3\u03b2(yi, zi,xi)\u03b2. (2.17)\nThe standard EM update corresponds to\nMn(\u03b2) = [ n\u2211\ni=1\n\u03a3\u03b2(yi, zi,xi)\n]\u22121 n\u2211\ni=1\nyi\u00b5\u03b2(yi, zi,xi).\nNote that \u03a3\u03b2(yi, zi,xi) has rank at most O(\u01ebp) with high probability. When \u01eb = O(1/p), the empirical covariance matrix is non-invertible when n \u226a p. We now assume \u03b2\u2217 \u2208 B0(s; p). By leveraging \u21131 regularization, one step update in Algorithm 1 corresponds to\nMrn(\u03b2) = argmax \u03b2\u2032\u2208Rp QMCRn (\u03b2 \u2032|\u03b2)\u2212 \u03bbn\u2016\u03b2\u2032\u20161."}, {"heading": "3 General Computational and Statistical Guarantees", "text": "We now turn to the theoretical analysis of high dimensional regularized EM algorithm. In Section 3.1, we set up a general analytical framework for regularized EM where the key ingredients are decomposable regularizer and several technical conditions about population based Q(\u00b7|\u00b7) and sample based Qn(\u00b7|\u00b7). In Section 3.2, we first introduce a resampling version of Algorithm 1 and provide our main result (Theorem 3.3) that characterizes both computational and statistical performance of the proposed variant of regularized EM algorithm."}, {"heading": "3.1 Framework", "text": ""}, {"heading": "3.1.1 Decomposable Regularizers", "text": "Decomposable regularizer, as considered in a body of work (e.g., Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al. (2014a)), has been shown to be useful, both empirically and theoretically, for high dimensional structural estimation. It also plays an important role in our analytical framework. We begin with the assumption that R : Rp \u2192 R+ is a norm, thereby we have R(u+v) \u2264 R(u) +R(v), \u2200 u,v \u2208 Rp. Consider a pair of subspaces (S,S) in Rp such that S \u2286 S. We denote the subspace orthogonal to S with respect to inner product \u2329 \u00b7, \u00b7 \u232a as S\u22a5, namely\nS\u22a5 := { u \u2208 \u2126 : \u2329 u,v \u232a , \u2200 v \u2208 S}.\nDefinition 3.1. (Decomposability) Regularizer R : Rp \u2192 R+ is decomposable with respect to (S,S) if\nR(u+ v) = R(u) +R(v), for any u \u2208 S,v \u2208 S\u22a5.\nUsually the structure of model parameter \u03b2\u2217 can be characterized by specifying a subspace S such that \u03b2\u2217 \u2208 S. The common use of regularizer is thus to penalize the compositions of solution that live outside S. As R is a norm, for v \u2208 S\u22a5, we always have R(\u03b2\u2217 + v) \u2264 R(\u03b2\u2217) + R(v). Consequently, decomposable regularizers actually make such penalty as much as possible by achieving the upper bound. We are interested in bounding the estimation error in some norm \u2016 \u00b7 \u2016. The following quantity is critical in connecting R to \u2016 \u00b7 \u2016.\nDefinition 3.2. (Subspace Compatibility Constant) For any subspace S \u2286 Rp, a given regularizer R and some norm \u2016 \u00b7 \u2016, the subspace compatibility constant of S with respect to R, \u2016 \u00b7 \u2016 is given by\n\u03a8(S) := sup u\u2208S\\{0} R(u) \u2016u\u2016 .\nStandardly, the dual norm of R is defined as R\u2217(v) := supR(u)\u22641 \u2329 u,v \u232a . To simplify notation,\nwe let \u2016u\u2016R := R(u) and \u2016u\u2016R\u2217 := R\u2217(u)."}, {"heading": "3.1.2 Conditions on Q(\u00b7|\u00b7)", "text": "Next, we review three technical conditions, originally proposed by Balakrishnan et al. (2014), about population level Q(\u00b7|\u00b7) function. Recall that \u2126 \u2286 Rp is the basin of attraction. It is well known that performance of EM algorithm is sensitive to initialization. Analyzing Algorithm 1 with any initial point is not desirable in this paper. Our theory is developed with focus on a r-neighbor region round \u03b2\u2217 that is defined as B(r;\u03b2\u2217) := { u \u2208 \u2126, \u2016u\u2212 \u03b2\u2217\u2016 \u2264 r } .\nWe first assume that Q(\u00b7|\u03b2\u2217) is self consistent as stated below.\nCondition 1. (Self Consistency) Function Q(\u00b7|\u03b2\u2217) is self consistent, namely\n\u03b2\u2217 = argmax \u03b2\u2208\u2126 Q(\u03b2|\u03b2\u2217).\nIt is usually assumed that \u03b2\u2217 maximizes the population log likelihood function. Under this condition, Condition 1 is always satisfied by following the classical theory of EM algorithmMcLachlan and Krishnan (2007).\nBasically, we require Q(\u00b7|\u03b2) is differentiable over \u2126 for any \u03b2 \u2208 \u2126. We assume the function Q(\u00b7|\u00b7) satisfies a certain strongly concavity condition and is smooth over \u2126.\nCondition 2. (Strong Concavity and Smoothness (\u03b3, \u00b5, r)) Q(\u00b7|\u03b2\u2217) is \u03b3-strongly concave over \u2126, i.e.,\nQ(\u03b22|\u03b2\u2217)\u2212Q(\u03b21|\u03b2\u2217)\u2212 \u2329 \u2207Q(\u03b21|\u03b2\u2217),\u03b22 \u2212 \u03b21 \u232a \u2264 \u2212\u03b3\n2 \u2016\u03b22 \u2212 \u03b21\u20162, \u2200 \u03b21,\u03b22 \u2208 \u2126. (3.1)\nFor any \u03b2 \u2208 B(r;\u03b2\u2217), Q(\u00b7|\u03b2) is \u00b5-smooth over \u2126, i.e.,\nQ(\u03b22|\u03b2)\u2212Q(\u03b21|\u03b2)\u2212 \u2329 \u2207Q(\u03b21|\u03b2),\u03b22 \u2212 \u03b21 \u232a \u2265 \u2212\u00b5\n2 \u2016\u03b22 \u2212 \u03b21\u20162, \u2200 \u03b21,\u03b22 \u2208 \u2126. (3.2)\nCondition 2 states that Q(\u00b7|\u03b2\u2217) is upper bounded by a quadratic function as shown in (3.1). Meanwhile, (3.2) implies that the function is lower bounded by another quadratic function. It\u2019s worth to note we require such lower bound holds for any function Q(\u00b7|\u03b2) with \u03b2 \u2208 B(r;\u03b2\u2217) while the upper bound condition is imposed on single function Q(\u00b7|\u03b2\u2217). Similar strong concavity and smoothness conditions are widely used in convex optimization and play important roles in showing geometric convergence of gradient descent. Here, such condition will help us achieve geometric decay of optimization error in EM algorithm.\nThe next condition is key in guaranteeing the curvature of Q(\u00b7|\u03b2) is similar to that of Q(\u00b7|\u03b2\u2217) when \u03b2 is close to \u03b2\u2217.\nCondition 3. (Gradient Stability (\u03c4, r)) For any \u03b2 \u2208 B(r;\u03b2\u2217), we have \u2225\u2225\u2207Q(M(\u03b2)|\u03b2) \u2212\u2207Q(M(\u03b2)|\u03b2\u2217) \u2225\u2225 \u2264 \u03c4\u2016\u03b2 \u2212 \u03b2\u2217\u2016.\nThe above condition only requires the gradient is stable at one point M(\u03b2). This is sufficient for our analysis. In fact, for many concrete examples, one can verify a stronger version of condition 3, i.e., for any \u03b2\u2032 \u2208 B(r;\u03b2\u2217) we have \u2225\u2225\u2207Q(\u03b2\u2032|\u03b2)\u2212\u2207Q(\u03b2\u2032|\u03b2\u2217) \u2225\u2225 \u2264 \u03c4\u2016\u03b2 \u2212 \u03b2\u2217\u2016."}, {"heading": "3.1.3 Conditions on Qn(\u00b7|\u00b7)", "text": "Recall that Qn(\u00b7|\u00b7) is computed from finite number of samples according to (2.3). We now turn to the two conditions about Qn(\u00b7|\u00b7). Our first condition, parallel to Condition 2 about function Q(\u00b7|\u00b7), imposes curvature constraint on Qn(\u00b7|\u00b7) under finite number of samples. In order to guarantee the estimation error \u2016\u03b2(t)\u2212\u03b2\u2217\u2016 in step t of EM algorithm is well controlled, we expect that Qn(\u00b7|\u03b2(t\u22121)) is strongly concave at \u03b2\u2217. However, in the setting where n \u226a p, there might exist directions along which Qn(\u00b7|\u03b2(t\u22121)) is flat, as we observed in mixed linear regression and missing covariate regression. In contrast with Condition 2, we suppose Qn(\u00b7|\u00b7) is strongly concave over a particular set C(S,S;R) that is defined in terms of subspace pair (S,S) and regularizer R. In detail, it takes form\nC(S,S;R) := { u \u2208 Rp :\n\u2225\u2225\u03a0S\u22a5(u) \u2225\u2225 R \u2264 2 \u00b7 \u2225\u2225\u03a0S(u) \u2225\u2225 R + 2 \u00b7\u03a8(S) \u00b7\n\u2225\u2225u \u2225\u2225 } , (3.3)\nwhere the projection operator \u03a0S : Rp \u2192 Rp is defined as\n\u03a0S(u) := argmin v\u2208S\n\u2016v \u2212 u\u2016. (3.4)\nWith the geometric definition in hand, we provide the restricted strong concavity (RSC) condition as follows.\nCondition 4. (RSC (\u03b3n,S,S , r, \u03b4)) For any fixed \u03b2 \u2208 B(r;\u03b2\u2217), with probability at least 1\u2212 \u03b4, we have that for all \u03b2\u2032 \u2212 \u03b2\u2217 \u2208 \u2126\u22c2 C(S,S;R),\nQn(\u03b2 \u2032|\u03b2)\u2212Qn(\u03b2\u2217|\u03b2)\u2212 \u2329 \u2207Qn(\u03b2\u2217|\u03b2),\u03b2\u2032 \u2212 \u03b2\u2217 \u232a \u2264 \u2212\u03b3n\n2 \u2016\u03b2\u2032 \u2212 \u03b2\u2217\u20162.\nThe above condition states that Qn(\u00b7|\u03b2) is strongly concave in direction \u03b2\u2032 \u2212 \u03b2\u2217 that belongs to C(S,S ;R). It\u2019s instructive to compare Condition 4 with a related condition proposed by Negahban et al. (2009) for analyzing high dimensional M-estimator. In detail, they assume the loss function is strongly convex over cone {u \u2208 Rp : \u2016\u03a0S\u22a5(u)\u2016R . \u2016\u03a0S(u)\u2016R}. Therefore our restrictive set (3.3) is similar to the cone but has additional term 2\u03a8(S)\u2016u\u2016. The main purpose of term 2\u03a8(S)\u2016u\u2016 is to allow regularization parameter \u03bbn jointly control optimization and statistical error. Note that while Condition 4 is stronger than RSC condition in M-estimator since we expand the set, we usually only require the property \u2016\u03a0S\u22a5(u)\u2016R . \u03a8(S)\u2016u\u2016 for showing strong convexity/concavity. Both the set (3.3) and the cone in M-estimator imply such property naturally.\nNext, we establish the second condition that characterizes the achievable statistical error.\nCondition 5. (Statistical Error (\u2206n, r, \u03b4)) For any fixed \u03b2 \u2208 B(r;\u03b2\u2217), with probability at least 1\u2212 \u03b4, we have \u2225\u2225\u2207Qn(\u03b2\u2217|\u03b2)\u2212\u2207Q(\u03b2\u2217|\u03b2) \u2225\u2225 R\u2217 \u2264 \u2206n. (3.5)\nTo provide some intuitions why the quantity \u2225\u2225\u2207Qn(\u03b2\u2217|\u03b2) \u2212 \u2207Q(\u03b2\u2217|\u03b2) \u2225\u2225 R\u2217 is useful in representing the statistical error, we first note that limn\u2192\u221e\u2206n = 0, which suggests that we obtain zero statistical error with infinite number of samples. In the case of finite samples, it\u2019s reasonable to believe that \u2206n decreases while we increase n. The decreasing rate is indeed the statistical convergence rate we aims to figure out. We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of \u2016Mn(\u03b2)\u2212M(\u03b2)\u20162 and \u2016Mn(\u03b2)\u2212M(\u03b2)\u2016\u221e respectively. As mentioned earlier, in high dimensional setting, Mn(\u03b2) is not well defined in some models such as mixed linear regression. For mixed linear regression, Wang et al. (2014) resolves this issue by invoking a high dimensional inverse covariance matrix estimation algorithm proposed by Cai et al. (2011). Our formulation (3.5) avoids resolving such ad hoc problems arising from specific models."}, {"heading": "3.2 Main Results", "text": "In this section, we provide the theoretical guarantees for regularized EM algorithm. Instead of analyzing Algorithm 1 directly, we introduce a resampling version of Algorithm 1 that is well suited to Conditions 4-5. The key idea is to split the whole dataset into T pieces and use a fresh piece of data in each iteration of regularized EM. We present the details in Algorithm 2.\nAlgorithm 2 High Dimensional Regularized EM Algorithm with Resampling Input Samples {yi}ni=1, number of iterations T , m = n/T , initial regularization parameter \u03bb (0) m ,\nregularizer R, initial parameter \u03b2(0), estimated statistical error \u2206, contractive factor \u03ba < 1. 1: Evenly split {yi}ni=1 into T disjoint subsets D1,D2, . . . ,DT . 2: For t = 1, 2, . . . , T do\n3: Regularization parameter update:\n\u03bb(t)m \u2190 \u03ba\u03bb(t\u22121)m +\u2206. (3.6)\n4: E-step: Compute function Q (t) m (\u00b7|\u03b2(t\u22121)) from sample set Dt according to (2.3). 5: Regularized M-step:\n\u03b2(t) \u2190 argmax \u03b2\u2208\u2126 Q(t)m (\u03b2|\u03b2(t\u22121))\u2212 \u03bb(t)m \u00b7 R(\u03b2).\n6: End For\nOutput \u03b2(T ).\nFor norm \u2016 \u00b7 \u2016 under our consideration, we let \u03b1 := supu\u2208Rp\\{0} \u2016u\u2016\u2217/\u2016u\u2016, where \u2016 \u00b7 \u2016\u2217 is the dual norm of \u2016 \u00b7 \u2016. For Algorithm 1, we prove the following result.\nTheorem 3.3. We assume the model parameter \u03b2\u2217 \u2208 S and regularizer R is decomposable with respect to (S,S) where S \u2286 S \u2286 Rp. For some r > 0, suppose B(r;\u03b2\u2217) \u2286 \u2126. Suppose function Q(\u00b7|\u00b7), defined in (2.5), is self consistent and satisfies Conditions 2-3 with parameters (\u03b3, \u00b5, r) and (\u03c4, r). Given n samples and T iterations, let m := n/T . Suppose Qm(\u00b7|\u00b7), computed from any m i.i.d. samples according to (2.3), satisfies conditions 4-5 with parameters (\u03b3m,S,S, r, 0.5\u03b4/T ) and (\u2206m, r, 0.5\u03b4/T ). Let\n\u03ba\u2217 := 5 \u03b1\u00b5\u03c4\n\u03b3\u03b3m .\nWe assume 0 < \u03c4 < \u03b3 and 0 < \u03ba\u2217 \u2264 3/4. Moreover, we define \u2206 := r\u03b3m/[60\u03a8(S)] and assume \u2206m is sufficiently small such that\n\u2206m \u2264 \u2206. (3.7)\nConsider the procedures in Algorithm 2 with initialization \u03b2(0) \u2208 B(r;\u03b2\u2217) and let the regularization parameters be\n\u03bb(t)m = \u03ba t \u03b3m 5\u03a8(S) \u2016\u03b2(0) \u2212 \u03b2\u2217\u2016+ 1\u2212 \u03ba t 1\u2212 \u03ba \u2206, t = 1, 2, . . . , T (3.8)\nfor any \u2206 \u2208 [3\u2206m, 3\u2206], \u03ba \u2208 [\u03ba\u2217, 3/4]. Then with probability at least 1 \u2212 \u03b4, we have that for any t \u2208 [T ],\n\u2016\u03b2(t) \u2212 \u03b2\u2217\u2016 \u2264 \u03bat\u2016\u03b2(0) \u2212 \u03b2\u2217\u2016+ 5 \u03b3m 1\u2212 \u03bat 1\u2212 \u03ba \u03a8(S)\u2206. (3.9)\nProof. See Section 6 for detailed proof.\nThe above result suggests that with suitable regularization parameters, the estimation error is bounded by two terms. The first term, decaying geometrically with number of iterations t, results from iterative optimization of function Qm thus is referred to as optimization error. The second term called statistical error characterizes the ultimate estimation error of Algorithm 2. With sufficiently large T such that the second term dominates the first term and suitable choice of \u2206 such that \u2206 = O(\u2206n/T ), we have the ultimate estimation error as\n\u2016\u03b2(T ) \u2212 \u03b2\u2217\u2016 . 1 (1\u2212 \u03ba)\u03b3n/T \u03a8(S)\u2206n/T . (3.10)\nSince the optimization error decays exponentially with T and \u2206n usually decays polynomially with 1/n, it\u2019s sufficient to set T = O(log n) which thus bring us estimation error O ( \u03a8(S)\u2206n/ logn ) . We have a log n factor loss by using the resampling technique. Removing the logarithmic factor requires direct analysis of Algorithm 1 where the main ingredient is to assume Conditions 4-5 hold uniformly for all \u03b2 \u2208 B(r;\u03b2\u2217) with high probability. Although extending Theorem 3.3 to cover Algorithm 1 with the new ingredient is straightforward, it\u2019s challenging to validate the new conditions in example models.\nWe place the constraint \u2206m . r\u03b3m/\u03a8(S) in (3.7) so that \u03b2(t) is guaranteed to be contained in B(r;\u03b2\u2217) for all t \u2208 [T ]. Note that this constraint is quite mild in the sense that if \u2206m = \u2126(r\u03b3m/\u03a8(S)), \u03b2(0) is a decent estimator with estimation error O(\u03a8(S)\u2206m/\u03b3m) that already matches our expectation.\nEquality (3.8) corresponds to update rule of regularization parameters (3.6) in Algorithm 2.\nRecall that with (\u03bb (0) m ,\u2206, \u03ba), we choose the update rule to be\n\u03bbtm \u2190 \u03ba \u00b7 \u03bbt\u22121m +\u2206.\nFollowing Theorem 3.3, (\u2206, \u03ba, \u03bb (0) m ) should satisfy the following conditions\n\u03ba\u2217 \u2264 \u03ba \u2264 3/4, 3\u2206m \u2264 \u2206 \u2264 3\u2206, \u03bb(0)m = \u03b3m 5\u03a8(S)\u2016\u03b2 (0) \u2212 \u03b2\u2217\u2016. (3.11)\nAs we observe, quantity \u03ba\u2217 is the minimum contractive parameter that is allowed to set. Parameter \u2206 characterizes the obtainable statistical error and should be set proportional to \u2206m. Initial regularization parameter \u03bb (0) m characterizes the initial estimation error \u2016\u03b2(0) \u2212 \u03b2\u2217\u2016. Note that accurate estimation of \u2016\u03b2(0) \u2212 \u03b2\u2217\u2016 is not required. In fact, one can set \u03bb(0)m = \u03b3m5\u03a8(S)\u03b5 with any \u03b5 \u2208 [ \u2016\u03b2(0) \u2212 \u03b2\u2217\u20162, r ] . Then with proof similar to that of Theorem 3.3, we can show that\n\u2016\u03b2(t) \u2212 \u03b2\u2217\u2016 \u2264 \u03bat\u03b5+ 5 \u03b3m 1\u2212 \u03bat 1\u2212 \u03ba \u03a8(S)\u2206, for all t \u2208 [T ].\nConsequently, overestimating initial error will potentially increase the total number of iterations but has no essential impact on the ultimate estimation error."}, {"heading": "4 Applications to Example Models", "text": "In this section, we apply our high dimensional regularized algorithm and the analytical framework introduced in Section 3 to the aforementioned three example models: Gaussian mixture model, mixed linear regression and missing covariate regression. For each model, based on the high dimensional regularized EM iterations introduced in Section 2.2, we provide the corresponding initialization condition, regularization update, computational convergence guarantee and statistical rate."}, {"heading": "4.1 Gaussian Mixture Model", "text": "We now use our analytical framework to analyze the Gaussian Mixture Model (GMM) with sparse model parameter \u03b2\u2217. Recall that we consider the isotropic, balanced Gaussian Mixture Model with two components where sample yi is generated from either N (\u03b2\u2217, \u03c32Ip) or N (\u03b2\u2217, \u03c32Ip). The following quantity called SNR is critical in characterizing the difficulty of estimating \u03b2\u2217.\nSNR := \u2016\u03b2\u2217\u20162/\u03c3. (4.1)\nWe focus on the high SNR regime where we assume SNR \u2265 \u03c1 for some constant \u03c1. Note that the work in Ma and Xu (2005) provides empirical and theoretical evidences that in low SNR regime, where the overlap density of two Gaussian cluster is small, standard EM algorithm suffers from sublinear convergence asymptotically. Therefore the high SNR condition is necessary for showing exponential/linear convergence of EM algorithm and our high dimensional variant. Note that we\nare interested in quantizing estimation error using \u21132 norm. We thus set the norm \u2016 \u00b7 \u2016 in our framework to be \u2016 \u00b7 \u20162 in this section. Recall that we set regularizer R to be \u21131 norm. For any subset S \u2286 {1, . . . , p}, \u21131 norm is decomposable with respect to (S,S). For any \u03b2\u2217 \u2208 B0(s; p), by letting S = supp(\u03b2\u2217),S = supp(\u03b2\u2217), we have \u03a8(S) = \u221as and C(S,S ;R) corresponds to {\u2016uS\u22a5\u20161 \u2264 2\u2016uS\u20161 + 2 \u221a s\u2016u\u20162}.\nAccording to the QGMMn (\u00b7|\u00b7) introduced in (2.9), by taking expectation of it, we have\nQGMM(\u03b2\u2032|\u03b2) = \u22121 2 E [ w(Y ;\u03b2)\u2016Y \u2212 \u03b2\u2032\u201622 + (1\u2212 w(Y ;\u03b2))\u2016Y + \u03b2\u2032\u201622 ] . (4.2)\nWe now check Conditions 1-3 hold for QGMM(\u00b7|\u00b7). We begin with proving the following result. Lemma 4.1. (Self consistency of GMM) Consider Gaussian mixture model with QGMM (\u00b7|\u00b7) given in (4.2). For model parameter \u03b2\u2217 we have\n\u03b2\u2217 = arg max \u03b2\u2208Rp QGMM (\u03b2|\u03b2\u2217).\nProof. See Appendix A.1 for detailed proof.\nThe above result suggests thatQGMM (\u00b7|\u00b7) satisfies Condition 1. It is easy to see\u22072QGMM (\u03b2\u2032|\u03b2) = \u2212Ip, which implies that QGMM (\u00b7|\u00b7) satisfy Condition 2 with parameters (\u03b3, \u00b5, r) = (1, 1, r) for any r > 0. Next we present a result showing that QGMM (\u00b7|\u00b7) satisfies Condition 3 with arbitrarily small stability factor \u03c4 when SNR is sufficiently large.\nLemma 4.2. (Gradient stability of GMM) Consider the Gaussian Mixture Model with QGMM (\u00b7|\u00b7) given in (4.2). Suppose SNR defined in (4.1) is lower bounded by \u03c1, i.e., SNR \u2265 \u03c1. Function QGMM (\u00b7|\u00b7) satisfies Condition 3 with parameters (\u03c4, \u2016\u03b2\u2217\u20162/4), where \u03c4 \u2264 exp(\u2212C\u03c12) for some absolute constant C.\nProof. See the proof of Lemma 3 in Balakrishnan et al. (2014).\nNow we turn to the conditions about QGMMn (\u00b7|\u00b7). Lemma 4.3. (RSC of GMM) Consider Gaussian mixture model with any \u03b2\u2217 \u2208 B0(s; p) and QGMMn (\u00b7|\u00b7) given in (2.9). For any r > 0, we have QGMMn (\u00b7|\u00b7) satisfies Condition 4 with parameters (\u03b3n,S,S , r, \u03b4), where \u03b3n = 1, \u03b4 = 0, (S,S) = (supp(\u03b2\u2217), supp(\u03b2\u2217)). Proof. See Appendix A.2 for detailed proof.\nThis above result indicates that the restricted strong concavity condition holds deterministically in this example. The next lemma validates the statistical error condition and provides the corresponding parameters.\nLemma 4.4. (Statistical error of GMM) Consider Gaussian mixture model with QGMMn (\u00b7|\u00b7) and QGMM (\u00b7|\u00b7) given in (2.9) and (4.2) respectively. For any r > 0, \u03b4 \u2208 (0, 1) and some absolute constant C, Condition 5 holds with parameters (\u2206n, r, \u03b4) where\n\u2206n = C(\u2016\u03b2\u2217\u2016\u221e + \u03c3) \u221a log p+ log(2e/\u03b4)\nn .\nProof. See Appendix A.3 for detailed proof.\nNow we give the guarantees of Algorithm 2 for Gaussian mixture model.\nCorollary 4.5. (Sparse Recovery in GMM) Consider the Gaussian mixture model with any fixed \u03b2\u2217 \u2208 B0(s; p) and implementations of Algorithm 2 with regularizer \u21131 norm. Suppose \u03b2(0) \u2208 B(\u2016\u03b2\u2217\u20162/4;\u03b2\u2217) and SNR \u2265 \u03c1 with sufficiently large \u03c1. Let initial regularization parameter \u03bb(0)n/T be\n\u03bb (0) n/T =\n1\n5 \u221a s \u2016\u03b2(0) \u2212 \u03b2\u2217\u20162\nand quantity \u2206 be\n\u2206 = C(\u2016\u03b2\u2217\u2016\u221e + \u03c3) \u221a log p\nn T\nfor sufficiently large constant C. Moreover, let the number of samples n be sufficiently large such that\nn/T \u2265 [80C(\u2016\u03b2\u2217\u2016\u221e + \u03c3)/\u2016\u03b2\u2217\u20162]2 s log p. (4.3)\nThen by setting \u03bb (t) n/T = \u03ba t\u03bb (0) n/T + 1\u2212\u03bat 1\u2212\u03ba \u2206 for any \u03ba \u2208 [1/2, 3/4], with probability at least 1 \u2212 T/p, we have that\n\u2016\u03b2(t) \u2212 \u03b2\u2217\u20162 \u2264 \u03bat\u2016\u03b2(0) \u2212 \u03b2\u2217\u20162 + 5C(\u2016\u03b2\u2217\u2016\u221e + \u03c3)\n1\u2212 \u03ba\n\u221a s log p\nn T , for all t \u2208 [T ]. (4.4)\nProof. This result follows from Theorem 3.3. First, recall that the minimum contractive factor \u03ba\u2217 is \u03ba\u2217 = 5 \u03b1\u00b5\u03c4\u03b3\u03b3n/T . For \u21132 norm, we have \u03b1 = 1. Following the fact that (\u03b3, \u00b5) = (1, 1) and Lemma 4.2- 4.3, we have \u03ba\u2217 \u2264 20 exp(\u2212C\u03c12) for some constant C. We further have \u03ba\u2217 \u2264 12 when \u03c1 is sufficiently large. Second, based on Lemma 4.4, we set \u03b4 = 1/p and let \u2206 be \u2206 = C(\u2016\u03b2\u2217\u2016\u221e + \u03c3) \u221a T log p/n with sufficiently large C such that \u2206 \u2265 3\u2206n/T . Thirdly, by the assumption in (4.3), we have that \u2206 \u2264 3\u2206 where \u2206 = \u2016\u03b2\u2217\u20162/(240 \u221a s) in this example. Finally, we choose \u03bb (0) n/T = \u2016\u03b2(0) \u2212\u03b2\u2217\u2016/(5 \u221a s) by following (3.11). Packing up these ingredients and following Theorem 3.3, we have that by choosing any \u03ba \u2208 [1/2, 3/4], \u2016\u03b2(t) \u2212 \u03b2\u2217\u20162 \u2264 \u03bat\u2016\u03b2(0) \u2212 \u03b2\u2217\u20162 + 5 \u221a s\u2206/(1\u2212 \u03ba), which thus completes the proof.\nNote that \u2016\u03b2(0) \u2212 \u03b2\u2217\u2016 . \u2016\u03b2\u2217\u20162 \u2264 \u221a s\u2016\u03b2\u2217\u2016\u221e. Let us set T = C log( nlog p) for sufficiently large C such that the first term in (4.4) is dominated by the second term. Then Corollary 4.5 suggests the final estimation is\n\u2016\u03b2(T ) \u2212 \u03b2\u2217\u20162 . C(\u2016\u03b2\u2217\u2016\u221e + \u03b4) \u221a s log p\nn log\n( n\nlog p\n) .\nNote that the minimax rate for estimating s-sparse vector in a single Gaussian cluster is \u221a s log p/n, thereby the established rate is optimal on (n, p, s) up to a logarithmic factor."}, {"heading": "4.2 Mixed Linear Regression", "text": "We now turn to Mixed Linear Regression (MLR) model. In particular, we will consider two sets of model parameters: \u03b2\u2217 \u2208 B0(s; p) and \u0393\u2217 \u2208 Rp1\u00d7p2 with rank(\u0393\u2217) = r. For the two settings, the population level analysis is identical under i.i.d. Gaussian covariate design. Without loss of generality, we begin with treating the model parameter as a vector \u03b2\u2217 \u2208 Rp and validate Conditions 1-3 for QMLR(\u00b7|\u00b7) in this example. Given function QMLRn (\u00b7|\u00b7) in (2.12), by taking expectation of it, we have\nQMLR(\u03b2\u2032|\u03b2) = \u22121 2 E [ w(Y,X;\u03b2)(Y \u2212 \u3008X,\u03b2\u2032\u3009)2 + (1\u2212 w(Y,X;\u03b2))(Y + \u3008X,\u03b2\u2032\u3009)2 ] (4.5)\nFor now, we set the norm \u2016 \u00b7 \u2016 in our framework to \u2016 \u00b7 \u20162. We begin by checking the self consistency condition.\nLemma 4.6. (Self consistency of MLR) Consider mixed linear regression with model parameter \u03b2\u2217 \u2208 Rp and QMLR(\u00b7|\u00b7) given in (4.5). We have\n\u03b2\u2217 = arg max \u03b2\u2208Rp QMLR(\u03b2|\u03b2\u2217).\nProof. See Appendix B.1 for detailed proof.\nIt is easy to check \u22072QMLR(\u03b2\u2032|\u03b2) = \u2212Ip. Therefore, QMLR(\u00b7|\u00b7) satisfies Condition 2 with parameters (\u03b3, \u00b5, r) = (1, 1, r) for any r > 0. Similar to Gaussian mixture model, we introduce the following SNR quantity to characterize the hardness of the problem.\nSNR := \u2016\u03b2\u2217\u2016/\u03c3.\nThe work in Chen et al. (2014b) shows that there exists an unavoidable phase transition of statistical rate from high SNR to low SNR. In detail, in low-dimensional setting, the obtainable statistical error is \u2126( \u221a p/n) that matches the standard linear regression when SNR \u2265 \u03c1 for some constant \u03c1. Meanwhile, the unavoidable rate becomes \u2126((p/n)1/4) when SNR \u226a \u03c1. We conjecture such transition phenomenon still exists in high dimensional setting. For now we focus on the high SNR regime and show our algorithm achieves statistical rate that matches the standard sparse linear regression and low rank matrix recovery (up to logarithmic factor) in the end.\nThe following result validates Condition 3 holds with arbitrarily small stability factor \u03c4 when\nSNR is sufficiently large and the radius r of ball B(r;\u03b2\u2217) is sufficiently small.\nLemma 4.7. (Gradient Stability of MLR) Consider mixed linear regression model with function QMLR(\u00b7|\u00b7) given in (4.5). For any \u03c9 \u2208 [0, 1/4], let r = \u03c9\u2016\u03b2\u2217\u20162. Suppose SNR \u2265 \u03c1 for some constant \u03c1. Then for any \u03b2 \u2208 B(r;\u03b2\u2217), we have\n\u2016\u2207QMLR(M(\u03b2)|\u03b2) \u2212\u2207QMLR(M(\u03b2)|\u03b2\u2217)\u20162 \u2264 \u03c4\u2016\u03b2 \u2212 \u03b2\u2217\u20162\nwith\n\u03c4 = 17\n\u03c1 + 7.3\u03c9.\nProof. See Appendix B.2 for detailed proof.\nIn Balakrishnan et al. (2014), it is proved that when r = 132\u2016\u03b2\u2217\u20162, there exists \u03c4 \u2208 [0, 1/2] such that QMLR(\u00b7|\u00b7) satisfies Condition 3 with parameter \u03c4 when \u03c1 is sufficiently large. Note that Lemma 4.7 recovers this result. Moreover, Lemma 4.7 provides an explicit function to characterize the relationship between \u03c4 and \u03c1, \u03c9.\nNext we turn to validate the two technical conditions of QMLRn (\u00b7|\u00b7) and establish the computational and statistical guarantees of estimating mixed linear parameters in high dimensional regime. We consider two different structures of linear parameters: (1) model parameter \u03b2\u2217 is a sparse vector; (2) model parameter \u0393\u2217 is a low rank matrix. Note that we assume X is a fully random Gaussian vector/matrix, thereby the population level conditions about QMLR(\u00b7|\u00b7) hold in both settings.\nSparse Recovery. We assume model parameter \u03b2\u2217 is s-sparse, i.e., \u03b2\u2217 \u2208 B0(s; p). Recall that, in order to serve sparse structure, we choose R to be \u21131 norm. Setting S = S = supp(\u03b2\u2217), set C(S,S ;R) corresponds to {u : \u2016uS\u22a5\u20161 \u2264 2\u2016uS\u20161 + 2 \u221a s\u2016u\u20162}. Restricted concavity of QMLR(\u00b7|\u00b7) is validated in the following result.\nLemma 4.8. (RSC of MLR with sparsity) Consider mixed linear regression with any model parameter \u03b2\u2217 \u2208 B0(s; p) and function QMLRn (\u00b7|\u00b7) defined in (2.12). There exit absolute constants {Ci}3i=0 such that, if n \u2265 C0s log p, then for any r > 0, QMLRn (\u00b7|\u00b7) satisfies Condition 4 with parameters (\u03b3n,S,S , r, \u03b4), where\n\u03b3n = 1 3 , (S,S) = (supp(\u03b2\u2217), supp(\u03b2\u2217)), \u03b4 = C1 exp(\u2212C2n).\nProof. See Appendix B.3 for detailed proof.\nLemma 4.8 states that using n = O(s log p) samples makes QMLRn (\u00b7|\u00b7) be strongly concave over C with high probability.\nLemma 4.9. (Statistical error of MLR with sparsity) Consider mixed linear regression model with any \u03b2\u2217 \u2208 B0(s; p) and functions QMLRn (\u00b7|\u00b7), QMLR(\u00b7|\u00b7) defined in (2.12) and (4.5) respectively. There exist constants C and C1 such that, for any r > 0 and \u03b4 \u2208 (0, 1), if n \u2265 C1(log p+ log(6/\u03b4)), then\n\u2016\u2207QMLRn (\u03b2\u2217|\u03b2)\u2212\u2207QMLR(\u03b2\u2217|\u03b2)\u2016\u221e \u2264 C(\u2016\u03b2\u2217\u20162 + \u03b4) \u221a log p+ log(6/\u03b4)\nn for all \u03b2 \u2208 B(r;\u03b2\u2217)\nwith probability at least 1\u2212 \u03b4.\nProof. See Appendix B.4 for detailed proof.\nLemma 4.9 implies Condition 5 hold with parameters \u2206n = O ( (\u2016\u03b2\u2217\u20162 + \u03b4) \u221a log p n ) , any r > 0\nand \u03b4 = 1/p. Putting all the ingredients together leads to the following guarantee about sparse recovery in mixed linear regression using regularized EM algorithm.\nCorollary 4.10. (Sparse recovery in MLR) Consider the mixed linear regression model with any fixed model parameter \u03b2\u2217 \u2208 B0(s; p) and the implementation of Algorithm 2 using \u21131 regularization. Suppose SNR \u2265 \u03c1 for sufficiently large \u03c1. Let quantity \u2206 be\n\u2206 = C(\u2016\u03b2\u2217\u20162 + \u03b4) \u221a log p\nn T\nand number of samples n satisfy\nn/T \u2265 C \u2032 [(\u2016\u03b2\u2217\u20162 + \u03b4)/\u2016\u03b2\u2217\u20162]2 s log p\nfor some sufficiently large constants C and C \u2032. Given any fixed \u03b2(0) \u2208 B(\u2016\u03b2\u2217\u20162/240,\u03b2\u2217), let initial regularization parameter \u03bb\n(0) n/T be\n\u03bb (0) n/T =\n1\n15 \u221a s \u2016\u03b2(0) \u2212 \u03b2\u2217\u20162.\nThen by setting \u03bb (t) n/T = \u03ba t\u03bb (0) n/T + 1\u2212\u03bat 1\u2212\u03ba \u2206 for any \u03ba \u2208 [1/2, 3/4], we have that, with probability at least 1\u2212 T/p,\n\u2016\u03b2(t) \u2212 \u03b2\u2217\u20162 \u2264 \u03bat\u2016\u03b2(0) \u2212 \u03b2\u2217\u20162 + 15C(\u2016\u03b2\u2217\u20162 + \u03b4)\n1\u2212 \u03ba\n\u221a s log p\nn T , for all t \u2208 [T ].\nProof. The result follows from Theorem 3.3. First, we note that the minimum contractive factor \u03ba\u2217 = 5 \u03b1\u00b5\u03c4\u03b3\u03b3n/T = 15\u03c4 in this example since \u03b1 = 1, \u00b5 = \u03b3 = 1 and \u03b3n/T = 1/3 w.h.p when n & s log p (see Lemma 4.8). Following Lemma 4.7, \u03ba\u2217 \u2264 1/2 when w \u2264 1/240 and \u03c1 is sufficiently large. Second, by choosing n/T & s log p, we have \u2206n/T . (\u2016\u03b2\u2217\u20162 + \u03b4) \u221a T log p n w.h.p., as proved in Lemma 4.9. Lastly, we have \u2206 \u2264 3\u2206 by assuming n/T & [(\u2016\u03b2\u2217\u20162 + \u03b4)/\u2016\u03b2\u2217\u20162]2 s log p. Putting these ingredients together and plugging the established parameters into (3.9) complete the proof.\nCorollary 4.10 provides that the final estimation error is\n\u2016\u03b2(T ) \u2212 \u03b2\u2217\u20162 . \u03baT \u2016\u03b2(0) \u2212 \u03b2\u2217\u20162 + (\u2016\u03b2\u2217\u20162 + \u03b4) \u221a T s log p\nn .\nNote that the second term dominates when T is chosen to satisfy T & log (n/(Ts log p)). Performing T = C log(n/(s log p)) iterations gives us\n\u2016\u03b2(T ) \u2212 \u03b2\u2217\u20162 . (\u2016\u03b2\u2217\u20162 + \u03b4) \u221a s log p\nn log\n( n\ns log p\n) .\nThe dependence on (s, p, n) is thus O ( (s log p/n)1/2\u2212c ) for any c > 0. Note that the standard sparse regression has optimal statistical error \u221a s log p/n, thereby the obtained rate for mixed linear regression is optimal up to logarithmic factor. A caveat here is that the estimation error is proportional to signal strength \u2016\u03b2\u2217\u20162, i.e., s log p/n determines the relative error instead of absolute error as usually observed in high dimensional estimation. This phenomena, also appearing\nin low dimensional analysis Balakrishnan et al. (2014), arises from the fundamental limits of EM algorithm. It\u2019s worth to note that Chen et al. (2014b) establish near-optimal low dimensional estimation error that does not depend on \u2016\u03b2\u2217\u20162 based on a convex optimization approach. It\u2019s interesting to explore how to remove \u2016\u03b2\u2217\u20162 in high dimensional setting.\nLow Rank Recovery. In the sequel, we assume model parameter \u0393\u2217 \u2208 Rp1\u00d7p2 is a low rank matrix that has rank(\u0393\u2217) = \u03b8 \u226a min{p1, p2}. We focus on measuring the estimation error in Frobenius norm thus set \u2016 \u00b7 \u2016 in our framework to be \u2016 \u00b7 \u2016F . Note that by treating \u0393\u2217 as a vector, Frobenius norm is equivalent to \u21132 norm, thereby we still have Lemma 4.6-4.7 in this setting. Moreover, SNR is similarly defined as\nSNR := \u2016\u0393\u2217\u2016F /\u03c3.\nIn order to serve the low rank structure, we choose R to be nuclear norm \u2016 \u00b7 \u2016\u2217. For any matrix M, we let row(M) denote the subspace spanned by the rows of M and col(M) denote the subspace spanned by the columns of M. Moreover, for subspace represented by the columns of matrix U, we denote the subspace orthogonal to U as U\u22a5. For \u0393\u2217 with singular value decomposition U\u2217\u03a3V\u2217\u22a4, we thus let\nS = { M \u2208 Rp1\u00d7p2 : col(M) \u2286 U\u2217, row(M) \u2286 V\u2217 } (4.6)\nand\nS\u22a5 = { M \u2208 Rp1\u00d7p2 : col(M) \u2286 U\u2217\u22a5, row(M) \u2286 V\u2217\u22a5 } . (4.7)\nSo S contains all matrices with rows (and columns) living in the row (and column) space of \u0393\u2217. Subspace S\u22a5 contains all matrices with rows (and columns) orthogonal to the row (and column) space of \u0393\u2217. Nuclear norm is decomposable with respect to (S,S). We have \u03a8(S) = supM\u2208S\\{0} \u2016M\u2016\u2217/\u2016M\u2016F \u2264 \u221a 2\u03b8 since matrix in S has rank at most 2\u03b8. Similar to Lemma 4.8 and 4.9 for sparse structure, we have the following two results for low rank structure.\nLemma 4.11. (RSC of MLR with low rank structure) Consider mixed linear regression with model parameter \u0393\u2217 \u2208 Rp1\u00d7p2 that has rank(\u0393\u2217) = \u03b8. There exists constants {Ci}2i=0 such that, if n \u2265 C0\u03b8max{p1, p2}, then for any \u03b8 \u2208 (0,min{p1, p2}), QMLRn (\u00b7|\u00b7) satisfies Condition 4 with parameters (\u03b3n,S,S , r, \u03b4), where (S,S) are given in (4.6) and (4.7),\n\u03b3n = 1\n20 , \u03b4 = C1 exp(\u2212C2n).\nProof. See Appendix B.5 for detailed proof.\nLemma 4.12. (Statistical error of MLR with low rank structure) Consider the mixed linear regression with any \u0393\u2217 \u2208 Rp1\u00d7p2 . There exists constants C and C1 such that, for any fixed \u0393 \u2208 Rp1\u00d7p2 and \u03b4 \u2208 (0, 1), if n \u2265 C1(p1 + p2 + log(6/\u03b4)), then\n\u2016\u2207QMLR(\u0393\u2217|\u0393)\u2212\u2207QMLRn (\u0393\u2217|\u0393)\u20162 \u2264 C(\u2016\u03a3\u2217\u2016F + \u03c3) \u221a p1 + p2 + log(6/\u03b4)\nn\nwith probability at least 1\u2212 \u03b4.\nProof. See the Appendix B.6 for detailed proof.\nSetting \u03b4 = 6exp(\u2212(p1 + p2)) in Lemma 4.12 suggests that Condition 5 holds with parameters (\u2206n, r, \u03b4) where \u2206n . (\u2016\u0393\u2217\u2016F + \u03b4) \u221a (p1 + p2)/n, \u03b4 = exp(\u2212(p1 + p2)) and r can be any positive number. Putting these pieces together leads to the following guarantee about low rank recovery.\nCorollary 4.13. (Low rank recovery in MLR) Consider mixed linear regression with any model parameter \u0393\u2217 \u2208 Rp1\u00d7p2 that has rank at most \u03b8 and the implementation of Algorithm 2 with nuclear norm regularization. Suppose SNR \u2265 \u03c1 for sufficiently large \u03c1. Let quantity \u2206 be\n\u2206 = C(\u2016\u0393\u2217\u2016F + \u03c3) \u221a\np1 + p2 n T\nand the number of samples n satisfy\nn/T \u2265 C \u2032 [(\u2016\u0393\u2217\u2016F + \u03c3)/\u2016\u0393\u2217\u2016F ]2 \u03b8(p1 + p2)\nfor some sufficiently large constants C and C \u2032. Given any fixed \u0393(0) \u2208 B(\u2016\u0393\u2217\u2016F /1600,\u0393\u2217), let initial regularization parameter \u03bb\n(0) n/T be\n\u03bb (0) n/T =\n1\n100 \u221a 2\u03b8 \u2016\u0393(0) \u2212 \u0393\u2217\u2016F .\nThen by setting \u03bb (t) n/T = \u03ba t\u03bb (0) n/T + 1\u2212\u03bat 1\u2212\u03ba \u2206 for any \u03ba \u2208 [1/2, 3/4], we have that, with probability at least 1\u2212 T exp(\u2212p1 \u2212 p2),\n\u2016\u0393(t) \u2212 \u0393\u2217\u2016F \u2264 \u03bat\u2016\u0393(0) \u2212 \u0393\u2217\u2016F + 100C \u2032(\u2016\u0393\u2217\u2016F + \u03c3)\n1\u2212 \u03ba\n\u221a 2\u03b8(p1 + p2)\nn T , for all t \u2208 [T ].\nProof. This result is parallel to Corollary 4.10 for sparse recovery thus can be proved similarly. We omit the details.\nCorollary 4.13 indicates that the final estimation error can be characterized by\n\u2016\u0393(T ) \u2212 \u0393\u2217\u2016F . \u03baT \u2016\u0393(0) \u2212 \u0393\u2217\u2016F + (\u2016\u0393\u2217\u2016F + \u03c3) \u221a \u03b8(p1 + p2)\nn T .\nNote that the initialization error is proportional to \u2016\u0393\u2217\u2016F . Choosing T = O(log(n/[\u03b8(p1 + p2)])), the first term representing optimization error is then dominated by the second term. We thus have\n\u2016\u0393(T ) \u2212 \u0393\u2217\u2016F . (\u2016\u0393\u2217\u2016F + \u03c3) \u221a \u03b8(p1 + p2)\nn log\n( n\n\u03b8(p1 + p2)\n) .\nThe established statistical rate matches (up to the logarithmic factor) the (single) low rank matrix estimation rate proved in Cande\u0300s and Plan (2011); Negahban et al. (2011), which is known to be minimax optimal. It\u2019s worth to note that our rate is proportional to the signal strength \u2016\u0393\u2217\u2016F . Therefore, the normalized sample complexity n/[\u03b8(p1 + p2)] controls the relative error instead of absolute error in standard low rank matrix estimation."}, {"heading": "4.3 Missing Covariate Regression", "text": "We now consider the sparse recovery guarantee of Algorithm 2 for missing covariate regression. We begin by validating conditions about function QMCR(\u00b7|\u00b7), which has form\nQMCR(\u03b2\u2032|\u03b2) = \u2329 E [Y \u00b5\u03b2(Y,Z,X)] ,\u03b2 \u2032\u232a\u2212 1 2 \u2329 E [\u03a3\u03b2(Y,Z,X)] ,\u03b2\u03b2 \u22a4 \u232a . (4.8)\nFirst, M(\u00b7) is self consistent as stated below.\nLemma 4.14. (Self-consistency of MCR) Consider missing covariate regression with parameter \u03b2\u2217 \u2208 Rp and QMCR(\u00b7|\u00b7) given in (4.8). We have\n\u03b2\u2217 = arg max \u03b2\u2208Rp QMCR(\u03b2|\u03b2\u2217).\nProof. See Appendix C.1 for detailed proof.\nFor our analysis, we define \u03c1 := \u2016\u03b2\u2217\u20162/\u03c3 to be the signal to noise ratio and \u03c9 := r/\u2016\u03b2\u2217\u20162 to be the relative contractivity radius. Let\n\u03b6 := (1 + \u03c9)\u03c1.\nRecall that \u01eb is the missing probability of every entry. The next result characterizes the smoothness and concavity of QMCR(\u00b7|\u00b7).\nLemma 4.15. (Smoothness and concavity of MCR) Consider missing covariate regression with parameter \u03b2\u2217 \u2208 Rp and QMCR(\u00b7|\u00b7) given in (4.8). For any \u03c9 > 0, we have that QMCR(\u00b7|\u00b7) satisfies Condition 2 with parameters (\u03b3, \u00b5, \u03c9\u2016\u03b2\u2217\u20162), where\n\u03b3 = 1, \u00b5 = 1 + 2\u03b62 \u221a \u01eb+ (1 + \u03b62)\u03b62\u01eb.\nProof. See Appendix C.2 for detailed proof.\nWe revisit the following result about the gradient stability from Balakrishnan et al. (2014).\nLemma 4.16. (Gradient stability of MCR) Consider the missing covariate regression with \u03b2\u2217 \u2208 Rp and QMCR(\u00b7|\u00b7) given in (4.8). For any \u03c9 > 0, \u03c1 > 0, QMCR(\u00b7|\u00b7) satisfies Condition 3 with parameter (\u03c4, \u03c9\u2016\u03b2\u2217\u20162) where\n\u03c4 = \u03b62 + 2\u01eb(1 + \u03b62)2\n1 + \u03b62 .\nProof. See the proof of Corollary 6 in Balakrishnan et al. (2014).\nUnlike the previous two models, we require an upper bound on the signal noise ratio. This\nunusual constraint is in fact unavoidable, as pointed out in Loh and Wainwright (2012).\nWe now turn to validate the conditions about finite sample function QMCRn (\u00b7|\u00b7). In particular, we have the following two guarantees.\nLemma 4.17. (RSC of MCR) Consider missing covariate regression with any fixed parameter \u03b2\u2217 \u2208 B0(s; p) and QMCRn (\u00b7|\u00b7) given in (2.17). There exist constants {Ci}3i=0 such that if \u01eb \u2264 C0 min{1, \u03b6\u22124} and n \u2265 C1(1 + \u03b6)8s log p, then we have QMCRn (\u00b7|\u00b7) satisfies Condition 4 with parameters (\u03b3n,S,S , \u03c9\u2016\u03b2\u2217\u20162, \u03b4), where\n\u03b3n = 1 9 , (S,S) = (supp(\u03b2\u2217), supp(\u03b2\u2217)), \u03b4 = C2 exp(\u2212C3n(1 + \u03b6)\u22128).\nProof. See Appendix C.3 for detailed proof.\nLemma 4.18. (Statistical error of MCR) Consider missing covariate regression with any fixed parameter \u03b2\u2217 \u2208 B0(s; p) and QMCRn (\u00b7|\u00b7) given in (2.17). There exist constants C0, C1 such that if n \u2265 C0[log p + log(24/\u03b4)], then for any \u03b4 \u2208 (0, 1) and any fixed \u03b2 \u2208 B(\u03c9\u2016\u03b2\u2217\u20162,\u03b2\u2217), we have that for\n\u2016\u2207QMCRn (\u03b2\u2217|\u03b2)\u2212QMCR(\u03b2\u2217|\u03b2)\u2016\u221e \u2264 C1(1 + \u03b6)5\u03c3 \u221a log p+ log(24/\u03b4)\nn\nwith probability at least 1\u2212 \u03b4.\nProof. See Appendix C.4 for detailed proof.\nBy setting \u03b4 = 1/p in Lemma 4.18 immediately implies that QMCRn satisfies Condition 5 with\nparameters \u2206n = O ( (1 + \u03b6)5\u03c3 \u221a log p/n ) , r = \u03c9\u2016\u03b2\u2217\u20162 and \u03b4 = 1/p.\nEnsembling all pieces leads to the following guarantee about resampling version of regularized\nEM on missing covariate regression.\nCorollary 4.19. (Sparse Recovery in MCR) Consider the missing covariate regression with any fixed model parameter \u03b2\u2217 \u2208 B0(s; p) and the implementation of Algorithm 2 with \u21131 regularization. Let quantity \u2206 be\n\u2206 = C\u03c3\n\u221a log p\nn T\nand number of samples n satisfies\nn/T \u2265 C \u2032max{\u03c32(\u03c9\u03c1)\u22121, 1}s log p\nfor sufficiently large constants C,C \u2032. Suppose (1 + \u03c9)\u03c1 \u2264 C0 < 1 and \u01eb \u2264 C1 for sufficiently small constants C0, C1. Given any fixed \u03b2\n(0) \u2208 B(\u03c9\u2016\u03b2\u2217\u20162,\u03b2\u2217), let initial regularization parameter \u03bb(0)n/T be\n\u03bb (0) n/T =\n1\n45 \u221a s \u2016\u03b2(0) \u2212 \u03b2\u2217\u20162.\nBy choosing \u03bb (t) n/T = \u03ba t\u03bb (0) n/T + 1\u2212\u03bat 1\u2212\u03ba \u2206 for any \u03ba \u2208 [1/2, 3/4] in Algorithm 2 leads to\n\u2016\u03b2(t) \u2212 \u03b2\u2217\u20162 \u2264 \u03bat\u2016\u03b2(0) \u2212 \u03b2\u2217\u20162 + 45C\u03c3\n1\u2212 \u03ba\n\u221a s log p\nn T , for all t \u2208 [T ],\nwith probability at least 1\u2212 T/p.\nProof. Following Theorem 3.3, we have \u03ba\u2217 = 5 \u03b1\u00b5\u03c4\u03b3\u03b3n/T . For \u21132 norm, \u03b1 = 1. Based on Lemma 4.17, we have \u03b3n = 1/9. Following Lemma 4.15 and 4.16, we have \u03b3 = 1 and can always find sufficiently small constants C0, C1 such that \u00b5 \u2264 10/9 and \u03c4 \u2264 1/100. We thus obtain \u03ba\u2217 \u2264 1/2. From Lemma 4.18, one can check \u2206 > 3\u2206n/T under suitable C. We choose n/T & \u03c3\n2(\u03c9\u03c1)\u22121s log p to make sure \u2206 \u2264 3\u2206. With these conditions in hand, direct applying Theorem 3.3 completes the proof.\nBy choosing T = O(log(n/[s log p])) (for simplicity, we let \u03c9 = O(1)) in Corollary 4.19, the final\nestimation can be controlled by\n\u2016\u03b2(T ) \u2212 \u03b2\u2217\u20162 . \u03c3 \u221a s log p\nn log\n( n\ns log p\n) ,\nwhich is optimal up to logarithmic factor. As stated, Corollary 4.19 is applicable whenever (1 + \u03c9)\u03c1 \u2264 C0 and \u01eb \u2264 C1 for some constants C0. In particular, we have C0 < 1 that implies \u03c3 > \u2016\u03b2\u2217\u20162. Note that while low SNR is favorable in analysis, for fixed signal strength, lower SNR still leads to higher estimation error as standard (sparse) linear regression. For models with \u2016\u03b2\u2217\u20162 \u2265 \u03c3, we can always add stochastic noise manually to the response yi such that (1 + \u03c9)\u03c1 \u2264 C0 holds. This preprocessing trick combined with regularized EM algorithm thus leads to sparse recovery with error O\u0303(max{\u03c3, \u2016\u03b2\u2217\u20162} \u221a s log p/n) for the whole range of SNR."}, {"heading": "5 Simulations", "text": "In this section, we provide the simulation results to back up our theory. Note that even our theory built on resampling technique, it\u2019s statistically efficient to use partial dataset in practice. Consequently, we test the performance of regularized EM algorithm without sample splitting (Algorithm 1). We apply Algorithm 1 to the four latent variable models introduced in Section 2.2: Gaussian mixture model (GMM), mixed linear regression with sparse vector (MLR-Sparse), mixed linear regression with low rank matrix (MLR-LowRank) and missing covariate regression (MCR). We conduct two sets of experiments."}, {"heading": "5.1 Convergence Rate", "text": "We first evaluate the convergence of Algorithm 1 with good initialization \u03b2(0) ( particularly, we use \u0393(0) to denote a matrix initial parameter for model MLR-LowRank), that is, \u2016\u03b2(0)\u2212\u03b2\u2217\u20162 = \u03c9\u2016\u03b2\u2217\u20162 for some constant \u03c9. For models with s-sparse parameters (GMM, MLR-Sparse and MCR), we choose the problem size to be n = 500, p = 800, s = 5. For MLR-LowRank, we choose n = 600, p1 = p2 = p = 30, rank \u03b8 = 3. In addition, we set SNR = 5, \u03c9 = 0.5 for GMM, MLR-Sparse and MLR-LowRank; we set SNR = 0.5, \u03c9 = 0.5 and missing probability \u01eb = 20% for MCR. The initialization error we set, represented by \u03c9, for some models is larger than that provided by our theory. It\u2019s worth to note that we didn\u2019t put much effort to optimize the constant about initialization error in theory. The empirical results indicate that the practical convergence region can be much bigger than the theoretical region we proved in many settings. For a given error \u03c9\u2016\u03b2\u2217\u20162, the initial parameter \u03b2(0) is picked from sphere {u : \u2016u \u2212 \u03b2\u2217\u20162 = \u03c9\u2016\u03b2\u2217\u20162} uniformly\nat random. We ran Algorithm 1 on each model for T = 7 iterations. We set contractive factor \u03ba = 0.7. The choice of \u03bb (0) n follows Theorem 3.3. Parameter \u2206 for each model is given in Table 1. For every single independent trial, we report the estimation error \u2016\u03b2(t) \u2212 \u03b2\u2217\u20162 in each iteration and the optimization error \u2016\u03b2(t) \u2212\u03b2(T )\u20162, which is the difference between \u03b2(t) and the final output \u03b2(T ). We plot the log of errors over iteration t in Figure 2. We observe that for each of the plotted 10 independent trials, estimation error converges to certain value that is much smaller than the initialization error. Moreover, the optimization error has an approximately linear convergence as predicted by our theory."}, {"heading": "5.2 Statistical Rate", "text": "In the second set of experiments, we evaluate the relationship between final estimation error \u2016\u03b2(T )\u2212 \u03b2\u2217\u20162 and problem dimensions (n, p, s) or (n, p, \u03b8) for the aforementioned latent variable models. The choices of algorithmic parameters, i.e., \u03ba, \u2206 and \u03bb (0) n , and the initial parameter follow the first set of experiments in Section 5.1. Moreover, we set T = 7 and let output \u03b2\u0302 = \u03b2(T ). In Figure 3, we plot \u2016\u03b2\u0302 \u2212 \u03b2\u2217\u20162 over normalized sample complexity, i.e., n/(s log p) for s-sparse parameter and n/(\u03b8p) for rank \u03b8 p-by-p parameter. In particular, we fix s = 5 and \u03b8 = 3 for related models. We observe that the same normalized sample complexity leads to almost identical estimation error in practice, which thus supports the corresponding statistical rate established in Section 4."}, {"heading": "6 Proof of Main Result", "text": "In this section, we provide the proof of Theorem 3.3 that characterizes the computational and statistical performance of regularized EM algorithm with resampling. We first present a result which shows population EM operator M : \u2126 \u2192 \u2126 is contractive when \u03c4 < \u03b3.\nLemma 6.1. Suppose Q(\u00b7|\u00b7) satisfies all the corresponding conditions stated in Theorem 3.3. Mapping M is contractive over B(r;\u03b2\u2217), namely\n\u2016M(\u03b2)\u2212 \u03b2\u2217\u2016 \u2264 \u03c4 \u03b3 \u2016\u03b2 \u2212 \u03b2\u2217\u2016, \u2200 \u03b2 \u2208 B(r;\u03b2\u2217).\nProof. A similar result is proved in Balakrishnan et al. (2014). The slight difference is that Balakrishnan et al. (2014) shows Lemma 6.1 with \u21132 norm. Extending \u21132 norm to arbitrary norm is trivial, so we omit the details.\nNow we are ready to prove Theorem 3.3.\nProof of Theorem 3.3. We first consider one iteration of Algorithm 1 and shows the relationship between \u2016\u03b2(t) \u2212 \u03b2\u2217\u2016 and \u2016\u03b2(t\u22121) \u2212 \u03b2\u2217\u2016. Recall that\n\u03b2(t) = argmax \u03b2\u2032\u2208\u2126 Qm(\u03b2 \u2032|\u03b2(t\u22121))\u2212 \u03bb(t)m \u00b7 R(\u03b2\u2032).\nwhere m = n/T is the number of samples in each step. We assume \u03b2(t\u22121) \u2208 B(r;\u03b2\u2217). To simplify the notation, we drop the superscripts of \u03b2(t\u22121), \u03bb(t)m and denote \u03b2(t) as \u03b2+. From the optimality of \u03b2+, we have\nQm(\u03b2 +|\u03b2)\u2212 \u03bbm \u00b7 R(\u03b2+) \u2265 Qm(\u03b2\u2217|\u03b2)\u2212 \u03bbm \u00b7 R(\u03b2\u2217). (6.1)\nEquivalently,\n\u03bbm \u00b7 R(\u03b2+)\u2212 \u03bbm \u00b7 R(\u03b2\u2217) \u2264 Qm(\u03b2+|\u03b2)\u2212Qm(\u03b2\u2217|\u03b2). (6.2)\nUsing the fact that Qm(\u00b7|\u03b2) is concave function, the right hand side of the above inequality can be bounded as\nQm(\u03b2 +|\u03b2)\u2212Qm(\u03b2\u2217|\u03b2) \u2264 \u2329 \u2207Qm(\u03b2\u2217|\u03b2),\u03b2+ \u2212 \u03b2 \u232a \u2264 \u2223\u2223\u2329\u2207Qm(\u03b2\u2217|\u03b2),\u03b2+ \u2212 \u03b2 \u232a\u2223\u2223 \ufe38 \ufe37\ufe37 \ufe38\nA\n. (6.3)\nA key ingredient of our proof is to bound the term A. Let \u0398 := \u03b2+ \u2212 \u03b2\u2217, we have \u2223\u2223\u2329\u2207Qm(\u03b2\u2217|\u03b2),\u03b2+ \u2212 \u03b2 \u232a\u2223\u2223 = \u2223\u2223\u2329\u2207Qm(\u03b2\u2217|\u03b2)\u2212\u2207Q(\u03b2\u2217|\u03b2) +\u2207Q(\u03b2\u2217|\u03b2),\u0398 \u232a\u2223\u2223\n\u2264 \u2223\u2223\u2329\u2207Qm(\u03b2\u2217|\u03b2)\u2212\u2207Q(\u03b2\u2217|\u03b2),\u0398 \u232a\u2223\u2223+ \u2223\u2223\u2329\u2207Q(\u03b2\u2217|\u03b2),\u0398 \u232a\u2223\u2223 (a)\n\u2264 \u2225\u2225Qm(\u03b2\u2217|\u03b2)\u2212\u2207Q(\u03b2\u2217|\u03b2)\u2016R\u2217 \u00b7 R(\u0398) + \u2225\u2225\u2207Q(\u03b2\u2217|\u03b2) \u2225\u2225 \u2217 \u00d7 \u2016\u0398\u2016 (b)\n\u2264 \u2206mR(\u0398) + \u03b1 \u2225\u2225\u2207Q(\u03b2\u2217|\u03b2) \u2225\u2225\u00d7 \u2016\u0398\u2016 (c)\n\u2264 \u2206mR(\u0398) + \u03b1 \u2225\u2225\u2207Q(\u03b2\u2217|\u03b2)\u2212\u2207Q(M(\u03b2)|\u03b2) \u2225\u2225 \u00d7 \u2016\u0398\u2016 (d)\n\u2264 \u2206mR(\u0398) + \u03b1\u00b5 \u2225\u2225M(\u03b2)\u2212 \u03b2\u2217 \u2225\u2225\u00d7 \u2016\u0398\u2016 (e) \u2264 \u2206mR(\u0398) + \u03b1\u00b5\u03c4\n\u03b3\n\u2225\u2225\u03b2 \u2212 \u03b2\u2217 \u2225\u2225\u00d7 \u2016\u0398\u2016 (6.4)\nwhere (a) follows from Cauchy-Schwarz inequality, (b) follows from the statistical error condition 5 and the definition of \u03b1, (c) follows from the fact that M(\u03b2) maximizes Q(\u00b7|\u03b2), (d) follows from the smoothness condition 2, (e) follows from Lemma 6.1. For inequality (c), note that we assume that B(r;\u03b2\u2217) \u2286 \u2126. From Lemma 6.1, we know that if \u03b2 \u2208 B(r;\u03b2\u2217), under condition \u03c4 < \u03b3, we must have M(\u03b2) \u2208 B(r\u03c4/\u03b3;\u03b2\u2217) \u2286 B(r;\u03b2\u2217). Therefore M(\u03b2) lies in the interior of \u2126 thus the optimality condition corresponds to \u2207Q(M(\u03b2)|\u03b2) = 0.\nPlugging (6.4) back into (6.3), we obtain\nQm(\u03b2 +|\u03b2)\u2212Qm(\u03b2\u2217|\u03b2) \u2264 \u2206mR(\u0398) +\n\u03b1\u00b5\u03c4\n\u03b3\n\u2225\u2225\u03b2 \u2212 \u03b2\u2217 \u2225\u2225\u00d7 \u2016\u0398\u2016.\nUsing the above result and (6.2), we have\n\u03bbmR(\u03b2\u2217 +\u0398)\u2212 \u03bbmR(\u03b2\u2217) \u2264 \u2206mR(\u0398) + \u03b1\u00b5\u03c4\n\u03b3\n\u2225\u2225\u03b2 \u2212 \u03b2\u2217 \u2225\u2225\u00d7 \u2016\u0398\u2016. (6.5)\nTo ease notation, we use uS to denote the projection operator \u03a0S(u) defined in (3.4). From the decomposability of R, we have\nR(\u03b2\u2217 +\u0398)\u2212R(\u03b2\u2217) \u2265 R(\u03b2\u2217 +\u0398S\u22a5)\u2212R(\u0398S)\u2212R(\u03b2 \u2217)\n= R(\u0398S\u22a5)\u2212R(\u0398S\u22a5),\nwhere the inequality is from triangle inequality and the equality is from decomposability of R. Plugging the above result back into (6.5) yields that\n\u03bbm \u00b7 ( R(\u0398S\u22a5)\u2212R(\u0398S) ) \u2264 \u2206mR(\u0398) + \u03b1\u00b5\u03c4\n\u03b3\n\u2225\u2225\u03b2 \u2212 \u03b2\u2217 \u2225\u2225\u00d7 \u2016\u0398\u2016.\nBy assuming that \u03bbm satisfies the following condition\n\u03bbm \u2265 3\u2206m + \u03b1\u00b5\u03c4 \u03b3\u03a8(S)\u2016\u03b2 \u2212 \u03b2 \u2217\u2016, (6.6)\nwe have that\nR(\u0398S\u22a5)\u2212R(\u0398S) \u2264 \u2206m \u03bbm\nR(\u0398) + \u03b1\u00b5\u03c4 \u2225\u2225\u03b2 \u2212 \u03b2\u2217 \u2225\u2225 \u03b3\u03bbm \u2016\u0398\u2016 \u2264 1 3 R(\u0398) + \u03a8(S)\u2016\u0398\u2016.\nPlugging R(\u0398) \u2264 R(\u0398S) +R(\u0398S\u22a5) into the above inequality, we obtain\n2R(\u0398S\u22a5) \u2264 4R(\u0398S) + 3\u03a8(S) \u00b7 \u2016\u0398\u2016. (6.7)\nTherefore, we have shown that \u0398 lies in the quasi cone C(S,S ;R) defined in (3.3). Recall that Condition 4 states that for any fixed \u03b2 \u2208 B(r;\u03b2\u2217), Qm(\u00b7|\u03b2) is strongly concave over set \u2126\n\u22c2({\u03b2\u2217}+ C(S,S ;R) ) . Using this condition yields that\nQm(\u03b2 \u2217 +\u0398|\u03b2)\u2212Qm(\u03b2\u2217|\u03b2) \u2264 \u2329 \u2207Qm(\u03b2\u2217|\u03b2),\u0398 \u232a \u2212 \u03b3m\n2 \u2016\u0398\u20162\n\u2264 \u2206mR(\u0398) + \u03b1\u00b5\u03c4\n\u03b3\n\u2225\u2225\u03b2 \u2212 \u03b2\u2217 \u2225\u2225\u00d7 \u2016\u0398\u2016 \u2212 \u03b3m\n2 \u2016\u0398\u20162, (6.8)\nwhere the second inequality follows from (6.4).\nNow we turn back to optimality condition (6.2), following which we have\nQm(\u03b2 \u2217 +\u0398|\u03b2)\u2212Qm(\u03b2\u2217|\u03b2) \u2265 \u03bbm \u00b7 R(\u03b2\u2217 +\u0398)\u2212 \u03bbm \u00b7 R(\u03b2\u2217) \u2265 \u2212\u03bbmR(\u0398S). (6.9)\nPutting (6.8) and (6.9) together gives us\n\u03b3m 2 \u2016\u0398\u20162 \u2264 \u03bbmR(\u0398S) + \u2206mR(\u0398) + \u03b1\u00b5\u03c4 \u03b3 \u2016\u03b2 \u2212 \u03b2\u2217\u2016 \u00d7 \u2016\u0398\u2016.\nUsing R(\u0398) \u2264 R(\u0398S\u22a5) +R(\u0398S) \u2264 (9/2)\u03a8(S)\u2016\u0398\u2016, we further have\n\u03b3m 2 \u2016\u0398\u20162 \u2264 \u03bbm\u03a8(S)\u2016\u0398\u2016 + 9 2 \u2206m\u03a8(S)\u2016\u0398\u2016+ \u03b1\u00b5\u03c4 \u03b3 \u2016\u03b2 \u2212 \u03b2\u2217\u2016 \u00d7 \u2016\u0398\u2016.\nCanceling term \u2016\u0398\u2016 on both sides of the above inequality yields that\n\u2016\u0398\u2016 \u2264 2\u03a8(S)\u03bbm \u03b3m + \u03a8(S) \u03b3m\n( 9\u2206m + 2 \u03b1\u00b5\u03c4 \u03b3\u03a8(S)\u2016\u03b2 \u2212 \u03b2 \u2217\u2016 ) \u2264 5\u03a8(S)\u03bbm \u03b3m . (6.10)\nThe last inequality follows from our assumption (6.6). Putting (6.6) and (6.10) together, we reach the conclusion that if \u03b2(t\u22121) \u2208 B(r;\u03b2\u2217) and\n\u03bb(t)m \u2265 3\u2206m + \u03b1\u00b5\u03c4 \u03b3\u03a8(S) \u2016\u03b2(t\u22121) \u2212 \u03b2\u2217\u2016, (6.11)\nthen we have\n\u2016\u03b2(t) \u2212 \u03b2\u2217\u2016 \u2264 5\u03a8(S)\u03bb (t) m\n\u03b3m . (6.12)\nWe let \u03ba\u2217 := 5\u03b1\u00b5\u03c4\u03b3\u03b3m and assume \u03ba \u2217 < 3/4. Then for any \u03ba \u2208 [\u03ba\u2217, 3/4], \u2206 \u2265 3\u2206m suppose we set\n\u03bb(t)m = 1\u2212 \u03bat 1\u2212 \u03ba \u2206+ \u03ba t \u03b3m 5\u03a8(S)\u2016\u03b2 (0) \u2212 \u03b2\u2217\u2016 (6.13)\nfor all t \u2208 [T ]. When t = 1, we have \u03b2(0) \u2208 B(r;\u03b2\u2217) and one can check inequality (6.11) holds by setting t = 1 in (6.13), thereby applying (6.12) yields that\n\u2016\u03b2(1) \u2212 \u03b2\u2217\u2016 \u2264 5\u03a8(S)\u03bb (1) m\n\u03b3m = 5\u03a8(S) \u03b3m 1\u2212 \u03ba 1\u2212 \u03ba\u2206+ \u03ba\u2016\u03b2 (0) \u2212 \u03b2\u2217\u2016.\nNow we prove Theorem 3.3 by induction. Assume that for some t \u2265 1,\n\u2016\u03b2(t) \u2212 \u03b2\u2217\u2016 \u2264 5\u03a8(S) \u03b3m 1\u2212 \u03bat 1\u2212 \u03ba \u2206+ \u03ba t\u2016\u03b2(0) \u2212 \u03b2\u2217\u2016. (6.14)\nUnder condition \u2206 \u2264 3\u2206, \u03ba \u2264 3/4, we have\n\u2016\u03b2(t) \u2212 \u03b2\u2217\u2016 \u2264 15\u03a8(S) \u03b3m 1\u2212 (3/4)t 1\u2212 3/4 \u2206 + (3/4) t\u2016\u03b2(0) \u2212 \u03b2\u2217\u2016 \u2264 15\u03a8(S) \u03b3m 1\u2212 (3/4)t 1\u2212 3/4 \u2206 + (3/4) t \u00b7 r\n= (1\u2212 (3/4)t) \u00b7 r + (3/4)t \u00b7 r = r,\nwhere the first equality is from our definition of \u2206. Consequently, we have \u03b2(t) \u2208 B(r;\u03b2\u2217). Now we check that by our choice of \u03bb (t+1) m , inequality (6.11) holds. Note that\n3\u2206m + \u03b1\u00b5\u03c4 \u03b3\u03a8(S)\u2016\u03b2 (t) \u2212 \u03b2\u2217\u2016 \u2264 \u2206+ 5\u03b1\u00b5\u03c4 \u03b3\u03b3m 1\u2212 \u03bat 1\u2212 \u03ba \u2206+ \u03b1\u00b5\u03c4 \u03b3\u03a8(S)\u03ba t\u2016\u03b2(0) \u2212 \u03b2\u2217\u2016\n\u2264 \u2206+ \u03ba1\u2212 \u03ba t 1\u2212 \u03ba \u2206+ \u03ba t+1 \u03b3m 5\u03a8(S) \u2016\u03b2(0) \u2212 \u03b2\u2217\u2016 = 1\u2212 \u03ba t+1 1\u2212 \u03ba \u2206+ \u03ba t+1 \u03b3m 5\u03a8(S) \u2016\u03b2(0) \u2212 \u03b2\u2217\u2016 = \u03bb(t+1)m ,\nwhere the first inequality is from (6.14) and the second inequality is from the fact \u03ba \u2265 \u03ba\u2217 = 5\u03b1\u00b5\u03c4\u03b3\u03b3m . Therefore (6.11) holds for t+ 1. Then applying (6.12) with t+ 1 implies that\n\u2016\u03b2(t+1) \u2212 \u03b2\u2217\u2016 \u2264 5\u03a8(S) \u03b3m 1\u2212 \u03bat+1 1\u2212 \u03ba \u2206+ \u03ba t+1\u2016\u03b2(0) \u2212 \u03b2\u2217\u2016.\nPutting pieces together we prove that (6.14) holds for all t \u2208 [T ] when Conditions 4 and 5 hold in every step. Applying probabilistic union bound, we reach the conclusion."}, {"heading": "A Proofs about Gaussian Mixture Model", "text": ""}, {"heading": "A.1 Proof of Lemma 4.1", "text": "In this example, we have\nM(\u03b2\u2217) = 2E [w(Y ;\u03b2\u2217)Y ] = 2E [\n1\n1 + exp(\u2212 2 \u03c32 \u3008Z \u00b7 \u03b2\u2217 +W,\u03b2\u2217\u3009) (Z \u00b7 \u03b2\n\u2217 +W ) ] ,\nwhere W \u223c N (0, \u03c32) and Z has Rademacher distribution over {\u22121, 1}. Due to the rotation invariance of Gaussianity, without loss of generality, we assume \u03b2\u2217 = Ae1. It\u2019s easy to check supp(M(\u03b2\u2217)) = {1}. Moreover, the first coordinate of M(\u03b2\u2217) takes form\n(M(\u03b2\u2217))1 = 2E [\n1\n1 + exp(\u2212 2 \u03c32 (AZ +W1))\n(AZ +W1)\n] = A,\nwhere the last equality follows by the substitution X = W1, Z = Z, \u03b3 = 0, a = A in Lemma D.7. Therefore, M(\u03b2\u2217) = \u03b2\u2217."}, {"heading": "A.2 Proof of Lemma 4.3", "text": "Although Condition 4 is a stochastic condition, for Gaussian mixture model, particularly it is satisfied deterministically. Note that\nQGMMn (\u03b2 \u2032|\u03b2) = \u2212 1\n2n\nn\u2211\ni=1\n[ w(yi;\u03b2)\u2016yi \u2212 \u03b2\u2032\u201622 + (1\u2212w(yi;\u03b2))\u2016yi + \u03b2\u2032\u201622 ] .\nWe have that for any \u03b2\u2032,\u03b2 \u2208 Rp, \u22072QGMMn (\u03b2\u2032|\u03b2) = \u2212Ip, which implies that QGMMn (\u03b2\u2032|\u03b2) is strongly concave with parameter 1. Consequently, Condition 4 holds with \u03b3n = 1."}, {"heading": "A.3 Proof of Lemma 4.4", "text": "Note that R\u2217 is \u2016 \u00b7 \u2016\u221e in this example. Following the specific formulations of QGMMn (\u00b7|\u00b7) and QGMM (\u00b7|\u00b7) in (2.9) and (4.2), we have\n\u2207QGMMn (\u03b2\u2217|\u03b2)\u2212\u2207QGMM(\u03b2\u2217|\u03b2) = \u2212 1\nn\nn\u2211\ni=1\nyi + 2\nn\nn\u2211\ni=1\nw(yi;\u03b2)yi \u2212 2E [w(Y ;\u03b2)Y ] .\nTherefore,\n\u2225\u2225\u2207QGMMn (\u03b2\u2217|\u03b2)\u2212\u2207QGMM (\u03b2\u2217|\u03b2) \u2225\u2225 \u221e \u2264 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nyi \u2225\u2225\u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n(a)\n+ \u2225\u2225\u2225\u2225\u2225 2 n n\u2211\ni=1 w(yi;\u03b2)yi \u2212 2E [w(Y ;\u03b2)Y ] \u2225\u2225\u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n(b)\nNext we bound the two terms (a) and (b) respectively.\nTerm (a). Let \u03b6 := 1n \u2211n i=1 yi. Let yi = (yi,1, . . . , yi,p) \u22a4 for all i \u2208 [n]. Consider the j-th coordinate \u03b6j of \u03b6, we have\n\u03b6j = 1\nn\nn\u2211\ni=1\nyi,j.\nNote that {yi,j}ni=1 are independent copies of random variable Yj that is\nYj = Z \u00b7 \u03b2\u2217j + V, (A.1)\nwhere Z is Rademacher random variable taking values in {\u22121, 1} and V has distribution N (0, \u03c32). Since Z \u00b7 \u03b2\u2217j and V are both sub-Gaussian random variables with norm \u2016Z \u00b7 \u03b2\u2217j \u2016\u03c82 \u2264 |\u03b2\u2217j | and \u2016V \u2016\u03c82 . \u03b4. Following the rotation invariance sub-Gaussian random variables (e.g., Lemma 5.9 in Vershynin (2010)), we have that\n\u2016Yj\u2016\u03c82 . \u221a \u2016Z \u00b7 \u03b2\u2217j \u20162\u03c82 + \u2016V \u20162\u03c82 . \u221a \u2016\u03b2\u2217\u20162\u221e + \u03c32.\nFollowing the standard sub-Gaussian concentration argument in Lemma D.1, there exists some constant C such that for any j \u2208 [p] and all t \u2265 0,\nPr (\u2223\u2223\u03b6j \u2223\u2223 \u2265 t ) \u2264 e \u00b7 exp ( \u2212 Cnt 2 \u2016\u03b2\u2217\u20162\u221e + \u03c32 ) .\nThen by applying union bound, we have\nPr ( sup j\u2208[p] \u2223\u2223\u03b6j \u2223\u2223 \u2265 t ) \u2264 pe \u00b7 exp ( \u2212 Cnt 2 \u2016\u03b2\u2217\u20162\u221e + \u03c32 ) .\nSetting the right hand side to be \u03b4, we have that, with probability at least 1\u2212 \u03b4/2, \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nyi \u2225\u2225\u2225\u2225\u2225 \u221e . (\u2016\u03b2\u2217\u2016\u221e + \u03b4) \u221a log p+ log(2e/\u03b4) n . (A.2)\nTerm (b). Now let \u03b6 := 2n \u2211n\ni=1w(yi;\u03b2)yi \u2212 2E [w(Y ;\u03b2)Y ]. We also consider the j-th coordinate \u03b6j of \u03b6, which takes form\n\u03b6j = 2\nn\nn\u2211\ni=1\n{ w(yi;\u03b2)yi,j \u2212 E(w(Y ;\u03b2)Yj) } .\nNote that w(yi;\u03b2)yi,j \u2212 E(w(Y ;\u03b2)Yj), i = 1, . . . , n are independent copies of random variable w(Y ;\u03b2)Yj \u2212 E(w(Y ;\u03b2)Yj) where Yj is given in (A.1). We have shown that Yj is sub-Gaussian random variable. Note that w(Y ;\u03b2) is random variable taking values in [0, 1]. We thus always have\nPr (|w(Y ;\u03b2)Yj | \u2265 t) \u2264 Pr(|Yj| > t) \u2264 exp(1\u2212 Ct2/\u2016Yj\u20162\u03c82).\nUsing the equivalent properties of sub-Gaussian (see Lemma 5.5 in Vershynin (2010)) , we conclude that w(Y ;\u03b2)Yj is sub-Gaussian random variable with norm \u2016w(Y ;\u03b2)Yj\u2016\u03c82 \u2264 \u2016Yj\u2016\u03c82 .\n\u221a \u2016\u03b2\u2217\u20162\u221e + \u03c32. Following Lemma D.3, we have \u2016w(Y ;\u03b2)Yj \u2212 E [w(Y ;\u03b2)Yj ] \u2016\u03c82 \u2264 2\u2016w(Y ;\u03b2)Yj\u2016\u03c82 . Using concentration result in Lemma D.1 yields that for any j \u2208 [p] and some constant C,\nPr (|\u03b6j | \u2265 t) = Pr {\u2223\u2223\u2223\u2223 2\nn\nn\u2211\ni=1\nw(yi;\u03b2)yi,j \u2212 E(w(Y ;\u03b2)Y ) \u2223\u2223\u2223\u2223 > t } \u2264 e \u00b7 exp ( \u2212 Cnt 2 \u2016\u03b2\u2217\u20162\u221e + \u03c32 ) .\nApplying union bound over p coordinates, we have\nPr ( sup j\u2208[p] |\u03b6j| > t ) \u2264 pe \u00b7 exp ( \u2212 Cnt 2 \u2016\u03b2\u2217\u20162\u221e + \u03c32 ) ,\nwhich implies that, with probability at least 1\u2212 \u03b4/2, \u2225\u2225\u2225\u2225\u2225 2 n n\u2211\ni=1 w(yi;\u03b2)yi \u2212 2E [w(Y ;\u03b2)Y ] \u2225\u2225\u2225\u2225\u2225 \u221e . (\u2016\u03b2\u2217\u2016\u221e + \u03c3) \u221a log p+ log(2e/\u03b4) n . (A.3)\nPutting (A.2) and (A.3) together completes the proof."}, {"heading": "B Proofs about Mixed Linear Regression", "text": ""}, {"heading": "B.1 Proof of Lemma 4.6", "text": "In this example, we have\nM(\u03b2\u2217) = 2E [w(Y,X;\u03b2\u2217)Y X] = 2E [\n1\n1 + exp(\u22122(\u3008X,Z\u00b7\u03b2\u2217\u3009+W )\u3008X,\u03b2\u2217\u3009\u03c32 ) (Z \u00b7 \u03b2\u2217 +W )X\n] ,\nwhere X \u223c N (0, Ip),W \u223c N (0, \u03c32), Z has Rademacher distribution. Due to the rotation invariance of Gaussianity, without loss of generality, we can assume \u03b2\u2217 = Ae1. It\u2019s easy to check supp(M(\u03b2\u2217)) = {1}. Moreover,\n(M(\u03b2\u2217))1 = 2E [\n1\n1 + exp(\u2212 2 \u03c32 (AZX1 +W )AX1)\n(AZX21 +X1W ) ] = E(AX21 ) = A,\nwhere the second inequality follows by the substitution X = W,Z = Z, \u03b3 = 0, a = AX1 in Lemma D.7. We thus have M(\u03b2\u2217) = \u03b2\u2217."}, {"heading": "B.2 Proof of Lemma 4.7", "text": "Recall that we hope to find \u03c4 such that for any \u03b2 \u2208 B(r;\u03b2\u2217)\n\u2016\u2207QMLR(M(\u03b2)|\u03b2) \u2212\u2207QMLR(M(\u03b2)|\u03b2\u2217)\u20162 \u2264 \u03c4\u2016\u03b2 \u2212 \u03b2\u2217\u20162.\nIn this example, we have\nM(\u03b2) = 2E [w(Y,X;\u03b2)Y X] ,\nand \u2207QMLR(\u03b2\u2032|\u03b2) = 2E [w(Y,X;\u03b2)Y X]\u2212 \u03b2\u2032. Therefore,\n\u2207QMLR(M(\u03b2)|\u03b2) \u2212\u2207QMLR(M(\u03b2)|\u03b2\u2217) = 2E [w(Y,X;\u03b2)Y X]\u2212 2E [w(Y,X;\u03b2\u2217)Y X] = 2E [w(Y,X;\u03b2)Y X]\u2212 \u03b2\u2217,\nwhere the last equality is from the self consistent property of QMLR(\u00b7|\u00b7). Due to the rotation invariance of Gaussianity, without loss of generality, we assume \u03b2\u2217 = Ae1,\u03b2 = (1+\u01eb1)Ae1+\u01eb2Ae2, where A = \u2016\u03b2\u2217\u20162, \u2016\u03b2 \u2212 \u03b2\u2217\u20162 = A \u221a \u01eb21 + \u01eb 2 2. Let random vector T be\nT := w(Y,X;\u03b2)Y X \u2212 1 2 \u03b2\u2217.\nNote that for any \u03b2 \u2208 Rp,\nw(Y,X;\u03b2) = exp(\u2212 (Y\u2212\u3008X,\u03b2\u3009)22\u03c32 )\nexp(\u2212 (Y\u2212\u3008X,\u03b2\u3009)22\u03c32 ) + exp(\u2212 (Y+\u3008X,\u03b2\u3009)2 2\u03c32 ) =\n1\n1 + exp(\u22122Y \u3008X,\u03b2\u3009 \u03c32\n) ,\nthereby\nT = 1 1 + exp(\u22122Y \u3008X,\u03b2\u3009\u03c32 ) Y X \u2212 1 2 \u03b2\u2217\n= 1\n1 + exp(\u22122(ZAX1+W )(A(1+\u01eb1)X1+\u01eb2X2) \u03c32\n) (ZAX1 +W )X \u2212\n1 2 Ae1,\nwhere Z is Rademacher random variable taking values in {\u22121, 1}, W is stochastic noise with distribution N (0, \u03c32), X1 and X2 are the first two coordinates of X. It\u2019s easy to note that E [Ti] = 0 for i = 3, . . . , p. We focus on characterizing the first two coordinates T1, T2 of T .\nCoordinate T1. First, we compute the expectation of T1. Particularly we let \u03b3 = \u01eb1 + \u01eb2X2/X1. Then we have\n\u2223\u2223E [T1] \u2223\u2223 = \u2223\u2223\u2223\u2223\u2223E [\nX1(W + ZAX1)\n1 + exp(\u22122AX1(1+\u03b3) \u03c32 (W + ZAX1)) \u2212 1 2 AX21\n]\u2223\u2223\u2223\u2223\u2223\n\u2264 E [ |X1| \u00b7 \u2223\u2223\u2223\u2223\u2223 (W + ZAX1)\n1 + exp(\u22122AX1(1+\u03b3) \u03c32 (W + ZAX1)) \u2212 1 2 AX1\n\u2223\u2223\u2223\u2223\u2223 ]\n= EX1,X2 { |X1| \u00b7 EW,Z [\u2223\u2223\u2223\u2223\u2223 (W + ZAX1)\n1 + exp(\u22122AX1(1+\u03b3)\u03c32 (W + ZAX1)) \u2212 1 2 AX1\n\u2223\u2223\u2223\u2223\u2223 ]}\n\u2264 EX1,X2 [ |X1| \u00b7min { 1\n2 A \u00b7 |X1\u03b3| \u00b7 exp( \u03b32(AX1) 2 \u2212 (AX1)2 2\u03c32 ), \u03c3\u221a 2\u03c0\n+A|X1| }] , (B.1)\nwhere the last inequality follows from Lemma D.7 by replacing the parameters (X,Z, a, \u03b3) in the statement with (W,Z,AX1, \u03b3). Let event E be E := {\u03b32 \u2264 0.9}. Computing the expectation in\n(B.1) conditioning on E and Ec yields that \u2223\u2223E [T1] \u2223\u2223 \u2264E [ 1\n2 |\u03b3|AX21 exp( \u03b32(AX1) 2 \u2212 (AX1)2 2\u03c32 ) \u2223\u2223\u2223\u2223 E ] \u00b7 Pr(E)\n+ E [ \u03c3|X1|\u221a\n2\u03c0 +AX21\n\u2223\u2223\u2223\u2223 Ec ] \u00b7 Pr(Ec). (B.2)\nWe bound the two terms on the right hand side of the above inequality respectively. For the first term we have\nE\n[ 1\n2 |\u03b3|AX21 exp( \u03b32(AX1) 2 \u2212 (AX1)2 2\u03c32 ) \u2223\u2223\u2223\u2223 E ] \u00b7 Pr(E) \u2264 E [ 1 2 |\u03b3|AX21 exp( \u2212(AX1)2 20\u03c32 ) \u2223\u2223\u2223\u2223 E ] \u00b7 Pr(E)\n\u2264 E [ 1\n2 |\u03b3|AX21 exp( \u2212(AX1)2 20\u03c32 )\n] \u2264 E [ 1\n2 A ( |\u01eb1| \u00b7X21 + |\u01eb2X1X2| ) exp(\u2212 1 20 \u03c12X21 )\n]\n= 1\n2 A |\u01eb1| (1 + 0.1\u03c12)3/2 + 1 \u03c0 A |\u01eb2| 1 + 0.1\u03c12 \u2264 1 2 A\n1\n1 + 0.1\u03c12 (|\u01eb1|+ |\u01eb2|), (B.3)\nwhere the third inequality is from \u2016\u03b2\u2217\u20162/\u03c3 \u2265 \u03c1. For the second term in (B.2), first note that \u221a\n\u01eb21 + \u01eb 2 2 \u2264 \u2016\u03b2 \u2212 \u03b2\u2217\u20162 \u2016\u03b2\u2217\u20162 \u2264 \u03c9 \u2264 1/4,\nthereby |\u03b3| \u2264 |\u01eb1|+ |\u01eb2| \u00b7 |X2/X1| \u2264 1/4 + |\u01eb2| \u00b7 |X2/X1|. We define event E \u2032 := {X22/X21 \u2265 (2.1\u01eb22)\u22121}. Note that Ec = {\u03b32 \u2265 0.9}, we thus have Ec \u2286 E \u2032, i.e., the occurrence of Ec must lead to the occurrence of E \u2032. For the second term in (B.2), we have\nE [ \u03c3|X1|\u221a\n2\u03c0 +AX21\n\u2223\u2223\u2223\u2223 Ec ] \u00b7 Pr(Ec) \u2264 E [ \u03c3|X1|\u221a\n2\u03c0 +AX21\n\u2223\u2223\u2223\u2223 E \u2032 ] \u00b7 Pr(E \u2032)\n\u2264 E [ \u03c3|X1|\u221a 2\u03c0 + \u221a 2.1\u01eb22A|X1X2| \u2223\u2223\u2223\u2223 E \u2032 ] \u00b7 Pr(E \u2032) (B.4)\n= \u03c3\n\u03c0\n[ 1\u2212 \u221a 1\n1 + 2.1\u01eb22\n] + \u221a 2.1\u01eb22A 2\n\u03c0 2.1\u01eb22 1 + 2.1\u01eb22\n\u2264 \u221a 2.1\u03c3\n\u03c0 |\u01eb2|+\n2 \u221a 2.1 3\n\u03c0 A|\u01eb2|3, (B.5)\nwhere the equality is from Lemma D.6 by setting C in the statement to be \u221a\n2.1\u01eb22.\nPutting (B.3) and (B.4) together, we have\n|E [T1] | \u2264 1\n2 A\n1\n1 + 0.1\u03c12 (|\u01eb1|+ |\u01eb2|) +\n\u221a 2.1\u03c3\n\u03c0 |\u01eb2|+\n2 \u221a 2.1 3\n\u03c0 A|\u01eb2|3. (B.6)\nCoordinate T2. Now we turn to the second coordinate T2. Using E [X1X2] = 0, we have\n\u2223\u2223E [T2] \u2223\u2223 = \u2223\u2223\u2223\u2223\u2223E [\nX2(W + ZAX1) 1 + exp(\u22122AX1(1+\u03b3)\u03c32 (W + ZAX1)) \u2212 1 2 AX1X2\n]\u2223\u2223\u2223\u2223\u2223\n\u2264 E [ |X2| \u00b7 \u2223\u2223\u2223\u2223\u2223 (W + ZAX1)\n1 + exp(\u22122AX1(1+\u03b3) \u03c32 (W + ZAX1)) \u2212 1 2 AX1\n\u2223\u2223\u2223\u2223\u2223 ] .\nSimilar to (B.1), using Lemma D.7 leads to\n\u2223\u2223E [T2] \u2223\u2223 \u2264 E [ |X2| \u00b7min { 1\n2 A \u00b7 |X1\u03b3| \u00b7 exp( \u03b32(AX1) 2 \u2212 (AX1)2 2\u03c32 ), \u03c3\u221a 2\u03c0\n+A|X1| }]\n\u2264 E [ 1\n2 A|\u03b3| \u00b7 |X1X2| exp( \u03b32(AX1) 2 \u2212 (AX1)2 2\u03c32 ) \u2223\u2223\u2223\u2223 E ] \u00b7 Pr(E)\n+ E [ \u03c3|X2|\u221a\n2\u03c0 +A|X1X2|\n\u2223\u2223\u2223\u2223 Ec ] \u00b7 Pr(Ec).\nWe bound the two terms in the right hand side of the above inequality respectively. For the first term, we have\nE\n[ 1\n2 A|\u03b3| \u00b7 |X1X2| exp( \u03b32(AX1) 2 \u2212 (AX1)2 2\u03c32 ) \u2223\u2223\u2223\u2223 E ] \u00b7 Pr(E)\n\u2264 E [ 1\n2 A|\u03b3| \u00b7 |X1X2| exp( \u22120.1(AX1)2 2\u03c32\n) \u2223\u2223 E ] \u00b7 Pr(E) \u2264 E [ 1\n2 A|\u03b3| \u00b7 |X1X2| exp( \u22120.1(AX1)2 2\u03c32 )\n]\n\u2264 E [ 1\n2 A ( |\u01eb1X1X2|+ |\u01eb2|X22 ) exp(\u2212 1 20 \u03c12X21 )\n] = 1\n\u03c0 A |\u01eb1| 1 + 0.1\u03c12 + 1 2 A |\u01eb2|\u221a 1 + 0.1\u03c12\n(B.7)\nFor the second term, recall that event E \u2032 is defined as {X22/X21 \u2265 (2.1\u01eb22)\u22121}, we have\nE [ \u03c3|X2|\u221a\n2\u03c0 +A|X1X2|\n\u2223\u2223\u2223\u2223 Ec ] \u00b7 Pr(Ec) \u2264 E [ \u03c3|X2|\u221a\n2\u03c0 +A|X1X2|\n\u2223\u2223\u2223\u2223 E \u2032 ] \u00b7 Pr(E \u2032)\n= \u03c3\n\u03c0\n\u221a 2.1\u01eb2\u221a\n1 + 2.1\u01eb22 +\n2A\n\u03c0 2.1\u01eb22 1 + 2.1\u01eb22\n\u2264 \u221a 2.1\u03c3\n\u03c0 |\u01eb2|+\n4.2A\n\u03c0 \u01eb22. (B.8)\nwhere the equality follows from Lemma D.6 by setting C in the statement to be \u221a\n2.1\u01eb22. Putting\n(B.7) and (B.8) together, we have\n|E [T2] | \u2264 1\n\u03c0 A |\u01eb1| 1 + 0.1\u03c12 + 1 2 A |\u01eb2|\u221a 1 + 0.1\u03c12 +\n\u221a 2.1\u03c3\n\u03c0 |\u01eb2|+\n4.2A\n\u03c0 \u01eb22. (B.9)\nNow based on (B.6) and (B.9), we conclude that\nE [\u2016T\u20162] = E [\u221a T 21 + T 2 2 ] \u2264 E [|T1|+ |T2|]\n\u2264 A 1\u221a 1 + 0.1\u03c12\n(|\u01eb1|+ |\u01eb2|) + \u221a 2.1\u03c3\n\u03c0 |\u01eb2|+\n2 \u221a 2.1 3\n\u03c0 A|\u01eb2|3 +\n\u221a 2.1\u03c3\n\u03c0 |\u01eb2|+\n4.2A\n\u03c0 \u01eb22\n\u2264 A (\n1\u221a 1 + 0.1\u03c12\n(|\u01eb1|+ |\u01eb2|) + |\u01eb2|/\u03c1+ 1.83\u03c9|\u01eb2| )\n\u2264 A(|\u01eb1|+ |\u01eb2|) \u00b7 ( 4.2\n\u03c1 + 1.83\u03c9\n) \u2264 2A \u221a \u01eb21 + \u01eb 2 2 \u00b7 ( 4.2\n\u03c1 + 1.83\u03c9\n)\n= 2\n( 4.2\n\u03c1 + 1.83\u03c9\n) \u2016\u03b2 \u2212 \u03b2\u2217\u20162.\nNote that \u2207QMLR(M(\u03b2)|\u03b2)\u2212\u2207QMLR(M(\u03b2)|\u03b2\u2217) = 2T , thereby we conclude that for any \u03c9 \u2264 1/4, QMLR(\u00b7|\u00b7) satisfies gradient stability condition over B(\u03c9\u2016\u03b2\u2217\u20162;\u03b2\u2217) with parameter\n\u03c4 = 17\n\u03c1 + 7.3\u03c9."}, {"heading": "B.3 Proof of Lemma 4.8", "text": "Recall that\nQMLRn (\u03b2 \u2032|\u03b2) = \u2212 1\n2n\nn\u2211\ni=1\n[ w(yi,xi;\u03b2)(yi \u2212 \u3008xi,\u03b2\u2032\u3009)2 + (1\u2212 w(yi,xi;\u03b2))(yi + \u3008xi,\u03b2\u2032\u3009)2 ] .\nFor any \u03b2,\u03b2\u2032 \u2208 Rp, we have\nQMLRn (\u03b2 \u2032|\u03b2)\u2212QMLRn (\u03b2\u2217|\u03b2)\u2212 \u3008\u2207QMLRn (\u03b2\u2217|\u03b2),\u03b2\u2032 \u2212 \u03b2\u2217\u3009 = \u2212\n1 2 (\u03b2\u2032 \u2212\u03b2\u2217)\u22a4\n( 1\nn\nn\u2211\ni=1\nxix \u22a4 i ) (\u03b2\u2032 \u2212\u03b2\u2217).\n(B.10)\nNote that we want to find \u03b3n such that the right hand side of (B.10) is less than \u2212\u03b3n2 \u2016\u03b2\u2032\u2212\u03b2\u201622 for any \u03b2\u2032\u2212\u03b2\u2217 \u2208 C(S,S;R). In this example, we have C(S,S;R) = {u \u2208 Rp : \u2016uS\u22a5\u20161 \u2264 2\u2016uS\u20161 + 2 \u221a s\u2016u\u20162}. It\u2019s sufficient to prove that the sample covariance matrix has restricted eigenvalues over set C(S,S;R). The following statement is follows by the substitution \u03a3 = Ip and X = X in Lemma D.5: there exist constants {Ci}2i=0 such that\n1 n\nn\u2211\ni=1\n\u3008xi,u\u30092 \u2265 1\n2 \u2016u\u201622 \u2212 C0\nlog p\nn \u2016u\u201621, for all u \u2208 Rp, (B.11)\nwith probability at least 1\u2212 C1 exp(\u2212C2n). For any u \u2208 C(S,S;R), we have\n\u2016u\u20161 = \u2016uS\u20161 + \u2016uS\u22a5\u20161 \u2264 3\u2016uS\u20161 + 2 \u221a s\u2016u\u20162 \u2264 5 \u221a s\u2016u\u20162.\nApplying (B.11) yields that\n1 n\nn\u2211\ni=1\n\u3008xi,u\u30092 \u2265 1\n2 \u2016u\u201622 \u2212 25C0\ns log p\nn \u2016u\u201622, for all u \u2208 C(S,S;R).\nConsequently, when n \u2265 C3s log p for sufficiently large C3, 1n \u2211n\ni=1\u3008xi,u\u30092 \u2265 1/3\u2016u\u201622, which implies \u03b3n = 1/3."}, {"heading": "B.4 Proof of Lemma 4.9", "text": "According to the formulations of QMLRn (\u00b7|\u00b7) and QMLR(\u00b7|\u00b7) in (2.12) and (4.5), we have\n\u2207QMLRn (\u03b2\u2217|\u03b2)\u2212\u2207QMLR(\u03b2\u2217|\u03b2)\n= \u03b2\u2217 \u2212 ( 1\nn\nn\u2211\ni=1\nxix \u22a4 i ) \u03b2\u2217 + 2\nn\nn\u2211\ni=1\nw(yi,xi;\u03b2)yixi \u2212 2E [w(Y,X;\u03b2)Y X]\u2212 1\nn\nn\u2211\ni=1\nyixi. (B.12)\nSo\n\u2016\u2207QMLRn (\u03b2\u2217|\u03b2)\u2212\u2207QMLR(\u03b2\u2217|\u03b2)\u2016\u221e\n\u2264 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nyixi \u2225\u2225\u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n(a)\n+ \u2225\u2225\u2225\u2225\u2225\u03b2 \u2217 \u2212 ( 1 n n\u2211\ni=1\nxix \u22a4 i ) \u03b2\u2217 \u2225\u2225\u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n(b)\n+ \u2225\u2225\u2225\u2225\u2225 2 n n\u2211\ni=1 w(yi,xi;\u03b2)yixi \u2212 2E [w(Y,X;\u03b2)Y X] \u2225\u2225\u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n(c)\n.\nNext we bound the above three terms (a), (b) and (c) respectively. Term (a). We let vector \u03b6 := 1n \u2211n i=1 yixi. Consider jth coordinate of \u03b6. For any j \u2208 [p], we have\n\u03b6j = 1\nn\nn\u2211\ni=1\nyixi,j,\nwhere xi,j is the jth coordinate of xi. Note that {yixij}ni=1 are independent copies of random variables (\u3008X,Z \u00b7\u03b2\u2217\u3009+W )Xj where X \u223c N (0, Ip), W \u223c N (0, \u03c32) and Z has Rademacher distribution. \u3008X,Z \u00b7\u03b2\u2217\u3009+W is sub-Gaussian random variable that has norm \u2016\u3008X,Z \u00b7\u03b2\u2217\u3009+W\u2016\u03c82 . \u221a \u2016\u03b2\u2217\u201622 + \u03c32. Also Xj is sub-Gaussian random variable that has norm \u2016Xj\u2016\u03c82 . 1. Then based on Lemma D.4, (\u3008X,Z \u00b7 \u03b2\u2217\u3009+W )Xj is sub-exponential with norm \u2016(\u3008X,Z \u00b7 \u03b2\u2217\u3009+W )Xj\u2016\u03c81 . \u221a \u2016\u03b2\u2217\u201622 + \u03c32. Following standard concentration result of sub-exponential random variables (e.g., Lemma D.2), there exists some constant C such that the following inequality\nPr (|\u03b6j| \u2265 t) \u2264 2 exp ( \u2212C t 2n \u2016\u03b2\u2217\u201622 + \u03c32 )\nholds for sufficiently small t > 0. Therefore,\nPr ( sup j\u2208[p] |\u03b6j | > t ) \u2264 2p exp ( \u2212C t 2n \u2016\u03b2\u2217\u201622 + \u03c32 ) .\nSetting the right hand side to be \u03b4/3, we have that, when n is sufficiently large (i.e., n \u2265 C(log p+ log(6/\u03b4)) for some constant C), with probability at least 1\u2212 \u03b4/3.\n\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nyixi \u2225\u2225\u2225\u2225\u2225 \u221e . (\u2016\u03b2\u2217\u20162 + \u03c3) \u221a log p+ log(6/\u03b4) n . (B.13)\nTerm (b). Now we let \u03b6 = \u03b2\u2217 \u2212 1nxixi\u03b2\u2217. For any j \u2208 [p],\n\u03b6j = 1\nn\nn\u2211\ni=1\n\u03b2\u2217j \u2212 xi,j\u3008xi,\u03b2\u2217\u3009.\nNote that {\u03b2\u2217j \u2212 xi,j\u3008xi,\u03b2\u2217\u3009}ni=1 are independent copies of random variable \u03b2\u2217j \u2212Xj\u3008X,\u03b2\u2217\u3009. Using similar analysis in bounding term (a), we claim that \u03b2\u2217j \u2212 Xj\u3008X,\u03b2\u2217\u3009 is centered sub-exponential random variable with norm \u2016\u03b2\u2217j \u2212 Xj\u3008X,\u03b2\u2217\u3009\u2016\u03c81 . \u2016\u03b2\u2217\u20162. Therefore, for sufficiently small t and some constant C,\nPr (|\u03b6j | \u2265 t) \u2264 2 exp ( \u2212C t 2n\n\u2016\u03b2\u2217\u201622\n) .\nUsing union bound implies that\nPr ( sup j\u2208[p] |\u03b6j| \u2265 t ) \u2264 2p \u00b7 exp ( \u2212C t 2n \u2016\u03b2\u2217\u201622 ) .\nSetting the right hand side to be \u03b4/3, we have that, when n is sufficiently large, \u2225\u2225\u2225\u2225\u2225\u03b2 \u2217 \u2212 ( 1 n n\u2211\ni=1\nxix \u22a4 i ) \u03b2\u2217 \u2225\u2225\u2225\u2225\u2225 \u221e . \u2016\u03b2\u2217\u20162 \u221a log p+ log(6/\u03b4) n (B.14)\nholds with probability at least 1\u2212 \u03b4/3. Term (c). The analysis of this term is similar to the previous two terms. We let\n\u03b6 := 1\nn\nn\u2211\ni=1\nw(yi,xi;\u03b2)yixi \u2212 E [w(Y,X;\u03b2)Y X] .\nFor any j \u2208 [p],\n\u03b6j = 1\nn\nn\u2211\ni=1\nw(yi,xi;\u03b2)yixi,j \u2212 E [w(Y,X;\u03b2)Y X] .\nNote that {w(yi,xi;\u03b2)yixi,j}ni=1 are independent copies of random variable w(Y,X;\u03b2)Y Xj . We know that Y is sub-Gaussian with norm \u2016Y \u2016\u03c82 . \u221a \u2016\u03b2\u2217\u201622 + \u03c32. Since w(Y,X;\u03b2) is bounded, w(Y,X;\u03b2)Y is also sub-Gaussian. Consequently, w(Y,X;\u03b2)Y Xj is sub-exponential. By standard concentration result, for some constant C and sufficiently small t,\nPr(|\u03b6j| \u2265 t) \u2264 2 exp ( \u2212C nt 2 \u2016\u03b2\u2217\u201622 + \u03c32 ) .\nTherefore,\nPr(sup j\u2208[p]\n|\u03b6j| \u2265 t) \u2264 2 exp ( \u2212C nt 2 \u2016\u03b2\u2217\u201622 + \u03c32 ) .\nSetting the right hand side to be \u03b4/3, we have that, when n is sufficiently large, \u2225\u2225\u2225\u2225\u2225 2 n n\u2211\ni=1 w(yi,xi;\u03b2)yixi \u2212 2E [w(Y,X;\u03b2)Y X] \u2225\u2225\u2225\u2225\u2225 \u221e . (\u2016\u03b2\u2217\u20162 + \u03b4) \u221a log p+ log(6/\u03b4) n (B.15)\nwith probability at least 1\u2212 \u03b4/3. Putting (B.13), (B.14) and (B.15) together completes the proof."}, {"heading": "B.5 Proof of Lemma 4.11", "text": "Similar to (B.10), we have that for any \u0393\u2032,\u0393 \u2208 Rp1\u00d7p2 ,\nQMLRn (\u0393 \u2032|\u0393)\u2212QMLRn (\u0393\u2217|\u0393)\u2212 \u3008\u2207QMLRn (\u0393\u2217|\u0393),\u0393\u2032 \u2212 \u0393\u2217\u3009 = \u2212\n1\n2n\nn\u2211\ni=1\n\u3008Xi,\u0393\u2032 \u2212 \u0393\u2217\u30092. (B.16)\nNote that \u0393\u2032 \u2212 \u0393\u2217 \u2208 C(S,S ; \u2016 \u00b7 \u2016\u2217). Let \u0398 := \u0393\u2032 \u2212 \u0393\u2217, we thus have \u2016\u0398S\u22a5\u2016\u2217 \u2264 2 \u00b7 \u2016\u0398S\u2016\u2217 + 2 \u00b7 \u221a 2\u03b8\u2016\u0398\u2016F .\nWe make use of the following result.\nLemma B.1. Let {Xi}ni=1 be n independent samples of random matrix X \u2208 Rp1\u00d7p2 where the entries are i.i.d. Gaussian random variable with distribution N (0, 1). There exits constants C1, C2 such that\n1\u221a n\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u3008Xi,\u0398\u30092 \u2265 1\n4 \u2016\u0398\u2016F \u2212 12 (\u221a p1 n + \u221a p2 n ) \u2016\u0398\u2016\u2217, for all \u0398 \u2208 Rp1\u00d7p2 ,\nwith probability at least 1\u2212 C1 exp(\u2212C2n).\nProof. See Proposition 1 in Negahban et al. (2011) for detailed proof.\nThen for our \u0398, using the above result yields that\n1\u221a n\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u3008Xi,\u0398\u30092 \u2265 1\n4 \u2016\u0398\u2016F \u2212 12 (\u221a p1 n + \u221a p2 n )( \u2016\u0398S\u2016\u2217 + \u2016\u0398S\u22a5\u2016\u2217 )\n\u2265 1 4 \u2016\u0398\u2016F \u2212 12 (\u221a p1 n + \u221a p2 n )( 3\u2016\u0398S\u2016\u2217 + 2 \u221a 2r\u2016\u0398\u2016F ) \u2265 [ 1\n4 \u2212 60\n\u221a 2\u03b8 (\u221a p1 n + \u221a p2 n )] \u2016\u0398\u2016F .\nSo when n \u2265 C\u03b8max{p1, p2} for sufficient large C, we have 1\u221an \u221a\u2211n i=1\u3008Xi,\u0398\u30092 \u2265 \u2016\u0398\u2016F / \u221a 20. Plugging this result back into (B.16) gives us \u03b3n = 1/20 thus completes the proof."}, {"heading": "B.6 Proof of Lemma 4.12", "text": "Parallel to (B.12), we have\n\u2207QMLRn (\u0393\u2217|\u0393)\u2212\u2207QMLR(\u0393\u2217|\u0393)\n= \u0393\u2217 \u2212 1 n\nn\u2211\ni=1\n\u3008Xi,\u0393\u2217\u3009\u0393\u2217 + 2\nn\nn\u2211\ni=1\nw(yi,Xi;\u0393)yiXi \u2212 2E [w(Y,X;\u0393)Y X]\u2212 1\nn\nn\u2211\ni=1\nyiXi.\nThe dual norm of nuclear norm is spectral norm. So we are interested in bounding the following term for fixed \u0393:\n\u2225\u2225\u2207QMLRn (\u0393\u2217|\u0393)\u2212\u2207QMLR(\u0393\u2217|\u0393) \u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nyiXi \u2225\u2225\u2225\u2225\u2225 2\ufe38 \ufe37\ufe37 \ufe38\nU1\n+ \u2225\u2225\u2225\u2225\u2225\u0393 \u2217 \u2212 1 n n\u2211\ni=1 \u3008Xi,\u0393\u2217\u3009Xi \u2225\u2225\u2225\u2225\u2225 2\ufe38 \ufe37\ufe37 \ufe38\nU2\n+ \u2225\u2225\u2225\u2225\u2225 2 n n\u2211\ni=1 w(yi,Xi;\u0393)yiXi \u2212 2E [w(Y,X;\u0393)Y X] \u2225\u2225\u2225\u2225\u2225 2\ufe38 \ufe37\ufe37 \ufe38\nU3\n.\nNext we bound the three terms U1, U2 and U3 respectively. Term U1. We first note that\nU1 = sup u \u2208 Sp1\u22121 v \u2208 Sp2\u22121\n1 n\nn\u2211\ni=1\nyi\u3008uv\u22a4,Xi\u3009.\nIn particular, we let\nZ(a, b) = sup u \u2208 aSp1\u22121 v \u2208 bSp2\u22121\n1 n\nn\u2211\ni=1\nyi\u3008uv\u22a4,Xi\u3009.\nWe thus have Z(a, b) = abZ(1, 1). We construct 1/4-covering sets of Sp1\u22121 and Sp2\u22121, which we denote as N1 and N2 respectively. Therefore, for any u \u2208 Sp\u22121,v \u2208 Sp2\u22121, we can always find u\u2032 \u2208 N1,v\u2032 \u2208 N2 such that \u2016u \u2212 u\u2032\u20162 \u2264 1/4, \u2016v \u2212 v\u2032\u20162 \u2264 1/4. Moreover, we have the following decomposition uv\u22a4 = u\u2032v\u2032\u22a4 + (u\u2212 u\u2032)v\u2032\u22a4 + u\u2032(v \u2212 v\u2032)\u22a4 + (u\u2212 u\u2032)(v \u2212 v\u2032)\u22a4. Therefore, we have\nZ(1, 1) \u2264 max u\u2208N1,v\u2208N2\n1 n\nn\u2211\ni=1\nyi\u3008uv\u22a4,Xi\u3009+ Z(1/4, 1) + Z(1/4, 1) + Z(1/4, 1/4),\nwhich implies that\nZ(1, 1) \u2264 16 7 max u\u2208N1,v\u2208N2 1 n\nn\u2211\ni=1\nyi\u3008uv\u22a4,Xi\u3009.\nFor any fixed u and v, {yi\u3008uv\u22a4,Xi\u3009}ni=1 are n independent copies of random variable Y \u3008uv\u22a4,X\u3009 where Y is sub-Gaussian with norm \u2016Y \u2016\u03c82 . \u221a \u2016\u0393\u2217\u20162F + \u03c32, \u3008uv\u22a4,X\u3009 is zero mean Gaussian with variance 1. Following Lemma D.4, Y \u3008uv\u22a4,X\u3009 is sub-exponential with norm \u2016Y \u3008uv\u22a4,X\u3009\u2016\u03c81 .\u221a \u2016\u0393\u2217\u20162F + \u03c32. Using concentration result in Lemma D.2, we have\nPr (\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nyi\u3008uv\u22a4,Xi\u3009 \u2223\u2223\u2223\u2223\u2223 \u2265 t ) \u2264 2 exp ( \u2212 Ct 2n \u2016\u0393\u2217\u20162F + \u03c32 )\nfor sufficiently small t > 0. Note that |N1| \u2264 9p1 , |N2| \u2264 9p2 . By applying union bounds over N1 and N2, we have\nPr ( max\nu\u2208N1,v\u2208N2\n1 n\nn\u2211\ni=1\nyi\u3008uv\u22a4,Xi\u3009 \u2265 t ) \u2264 2 \u00b7 9(p1+p2) exp ( \u2212 Ct 2n \u2016\u0393\u2217\u20162F + \u03c32 ) .\nBy setting the right hand side to be \u03b4/3, we have that if n \u2265 C(p1 + p2 + log(6/\u03b4)) for sufficiently large C, then\nU1 . (\u2016\u0393\u2217\u2016F + \u03c3) \u221a p1 + p2 + log(6/\u03b4)\nn (B.17)\nwith probability at least 1\u2212 \u03b4/3. Term U2. Parallel to the analysis of term U1, we have\nU2 = sup u \u2208 Sp1\u22121 v \u2208 Sp2\u22121\n\u3008uv\u22a4,\u0393\u2217\u3009 \u2212 1 n\nn\u2211\ni=1\n\u3008Xi,\u0393\u2217\u3009 \u00b7 \u3008uv\u22a4,Xi\u3009.\nWe construct 1/4-nets N1,N2 of Sp1\u22121 and Sp2\u22121 respectively. Then\nU2 \u2264 16\n7 max u\u2208N1,v\u2208N2 \u3008uv\u22a4,\u0393\u2217\u3009 \u2212 1 n\nn\u2211\ni=1\n\u3008Xi,\u0393\u2217\u3009 \u00b7 \u3008uv\u22a4,Xi\u3009.\nFor any fixed u,v, note that {\u3008Xi,\u0393\u2217\u3009\u00b7\u3008uv\u22a4,Xi\u3009}ni=1 are n independent samples of random variable \u3008X,\u0393\u2217\u3009 \u00b7 \u3008uv\u22a4,X\u3009 where \u3008X,\u0393\u2217\u3009 \u223c N (0, \u2016\u0393\u2217\u20162F ) and \u3008uv\u22a4,X\u3009 \u223c N (0, 1). So \u3008X,\u0393\u2217\u3009 \u00b7 \u3008uv\u22a4,X\u3009 is sub-exponential with norm O(\u2016\u0393\u2217\u2016F ). Using the centering argument (Lemma D.3) and concentration result (Lemma D.2), we have\nPr (\u2223\u2223\u2223\u2223\u2223\u3008uv \u22a4,\u0393\u2217\u3009 \u2212 1 n n\u2211\ni=1\n\u3008Xi,\u0393\u2217\u3009 \u00b7 \u3008uv\u22a4,Xi\u3009 \u2223\u2223\u2223\u2223\u2223 \u2265 t ) \u2264 2 \u00b7 exp ( \u2212C t 2n\n\u2016\u0393\u2217\u20162F\n)\nfor sufficiently small t. Using the union bound over sets N1,N2, we conclude that when n \u2265 C(p1 + p2 + log(6/\u03b4)) for sufficiently large C, we have\nU2 . \u2016\u0393\u2217\u2016F \u221a p1 + p2 + log(6/\u03b4)\nn (B.18)\nwith probability at least 1\u2212 \u03b4/3. Term U3. We first have\nU3 = sup u\u2208Sp1\u22121 v\u2208Sp2\u22121\n2 n\nn\u2211\ni=1\nw \u00b7 yi\u3008uv\u22a4,Xi\u3009 \u2212 2E [ w \u00b7 Y \u3008uv\u22a4,X\u3009 ] .\nSimilar to the analysis of the first two terms, by constructing N1,N2, we have\nU3 \u2264 16\n7 max u\u2208N1,v\u2208N2\n2 n\nn\u2211\ni=1\nw \u00b7 yi\u3008uv\u22a4,Xi\u3009 \u2212 2E [ w \u00b7 Y \u3008uv\u22a4,X\u3009 ] .\nNote that {wyi\u3008uv\u22a4,Xi\u3009}ni=1 are n independent samples of random variable wY \u3008uv\u22a4,X\u3009 where \u3008uv\u22a4,X\u3009 \u223c N (0, 1) and wY is sub-Gaussian with norm \u2016wY \u2016\u03c82 . \u221a \u2016\u0393\u2217\u20162F + \u03c32 since |w| \u2264 1. We thus have wY \u3008uv\u22a4,X\u3009 is sub-exponential with norm \u2016wY \u3008uv\u22a4,X\u3009\u2016\u03c81 . \u221a\n\u2016\u0393\u2217\u20162F + \u03c32. Then following the similar steps in analyzing the first two terms, we reach the conclusion that\nU3 . (\u2016\u0393\u2217\u2016F + \u03c3) \u221a p1 + p2 + log(6/\u03b4)\nn (B.19)\nwith probability at least 1\u2212 \u03b4/3 when n & p1 + p2 + log(6/\u03b4). Putting (B.17), (B.18) and (B.19) together completes the proof."}, {"heading": "C Proofs about Missing Covariate Regression", "text": "In this section, we provide the proofs for missing covariate regression model. We begin with a result that states several properties of the conditional correlation matrix, which play important roles in proving curvature conditions. Recall that, given samples (yi, zi,xi) and \u03b2, \u03a3\u03b2(yi, zi,xi) is given in (2.16). We let Z \u2208 Rp be random vector with i.i.d. binary entries such that Pr(Z1 = 1) = \u01eb. Define the population level correlation covariance matrix as\n\u03a3\u03b2 := E [\u03a3\u03b2(Y,Z,X)] .\nLemma C.1. For \u03a3\u03b2, we have the following decomposition\n\u03a3\u03b2 = \u01ebIp +\u03a31 \u2212\u03a32,\nwhere\n\u03a31 = E { [(1\u2212 Z)\u2299X + \u03bdZ \u2299 \u03b2] \u00b7 [(1\u2212 Z)\u2299X + \u03bdZ \u2299 \u03b2]\u22a4 } ,\n\u03a32 = E\n[ 1\n\u03c32 + \u2016Z \u2299 \u03b2\u201622 (Z \u2299 \u03b2)(Z \u2299 \u03b2)\u22a4\n] , \u03bd =\nY \u2212 \u3008\u03b2, (1\u2212 Z)\u2299X\u3009 \u03c32 + \u2016Z \u2299 \u03b2\u201622 .\nLet \u03b6 := (1 + \u03c9)\u03c1, we have\n\u03bbmin(\u03a31) \u2265 1\u2212 \u01eb\u2212 2\u03b62 \u221a \u01eb, (C.1) \u03bbmax(\u03a32) \u2264 \u03b62\u01eb, (C.2) \u03bbmax(\u03a3\u03b2) \u2264 1 + 2\u03b62 \u221a \u01eb+ (1 + \u03b62)\u03b62\u01eb. (C.3)\nIn particular, let \u03b2 = \u03b2\u2217, we have \u03a3\u03b2\u2217 = Ip.\nProof. The decomposition follows by taking expectation of (2.16). For \u03a31, expanding the bracket leads to \u03a31 = (1\u2212\u01eb)Ip+E { \u03bd[(1\u2212 Z)\u2299X](Z \u2299 \u03b2)\u22a4 + \u03bd(Z \u2299 \u03b2)[(1 \u2212 Z)\u2299X]\u22a4 }\n\ufe38 \ufe37\ufe37 \ufe38 M\n+E [ \u03bd2(Z \u2299 \u03b2)(Z \u2299 \u03b2)\u22a4 ]\n\ufe38 \ufe37\ufe37 \ufe38 N\n.\nFor term M, consider its spectral norm. Since it\u2019s symmetric, we have\n\u2016M\u20162 = sup u\u2208Sp\u22121 2 |E [\u03bd\u3008Z \u2299 \u03b2,u\u3009 \u00b7 \u3008(1\u2212 Z)\u2299X,u\u3009]|\n= 2 sup u\u2208Sp\u22121\n\u2223\u2223\u2223\u2223E [\n1\n\u03c32 + \u2016Z \u2299 \u03b2\u20162 \u3008(1\u2212 Z)\u2299 (\u03b2\u2217 \u2212 \u03b2),u\u3009 \u00b7 \u3008Z \u2299 \u03b2,u\u3009\n]\u2223\u2223\u2223\u2223\n\u2264 2 1 \u03c32 E [\u2016(1\u2212 Z)\u2299 (\u03b2\u2217 \u2212 \u03b2)\u20162\u2016Z \u2299 \u03b2\u20162] \u2264 2 1 \u03c32\n\u221a E [ \u2016(1\u2212 Z)\u2299 (\u03b2\u2217 \u2212 \u03b2)\u201622 \u00b7 \u2016Z \u2299 \u03b2\u201622 ]\n\u2264 2 1 \u03c32\n\u221a \u01eb(1\u2212 \u01eb)\u2016\u03b2 \u2212 \u03b2\u2217\u20162\u2016\u03b2\u20162 \u2264 2\u03c12\u03c9(1 + \u03c9) \u221a \u01eb(1\u2212 \u01eb) \u2264 2\u03b62\u221a\u01eb.\nwhere the second equality follows by taking expectation of X and Gaussian noise W , the last inequality follows from the definitions of \u03c9, \u03c1 given in Section 4.3. Note that N 0. Then the lower bound of \u03bbmin(\u03a31) follows by using \u03bbmin(\u03a31) \u2265 1\u2212 \u01eb\u2212 \u2016M\u20162. For \u03a32, we have\n\u03a32 = E\n[ 1\n\u03c32 + \u2016Z \u2299 \u03b2\u201622 (Z \u2299 \u03b2)(Z \u2299 \u03b2)\u22a4\n] 1\n\u03c32\n( (\u01eb\u2212 \u01eb2)diag(\u03b2 \u2299 \u03b2) + \u01eb2\u03b2\u03b2\u22a4 ) .\nTherefore, \u03bbmax(\u03a32) \u2264 \u03b62\u01eb. Note that\nN 1 \u03c34 E\n[ (Y \u2212 \u3008\u03b2, (1 \u2212 Z)\u2299X\u3009)2(Z \u2299 \u03b2)(Z \u2299 \u03b2)\u22a4 ]\n= 1\n\u03c34 E\n[ (\u03c32 + \u2016\u03b2\u2217 \u2212 (1\u2212 Z)\u2299 \u03b2\u201622)(Z \u2299 \u03b2)(Z \u2299 \u03b2)\u22a4 ]\n1 \u03c34\n(\u03c32 + \u2016\u03b2\u2217\u201622 + \u2016\u03b2 \u2212 \u03b2\u2217\u201622) ( (\u01eb\u2212 \u01eb2)diag(\u03b2 \u2299 \u03b2) + \u01eb2\u03b2\u03b2\u22a4 ) .\nWe thus have \u03bbmax(N) \u2264 1\u03c34 (\u03c32 + \u2016\u03b2\u2217\u201622 + \u2016\u03b2 \u2212 \u03b2\u2217\u201622)\u01eb\u2016\u03b2\u201622 \u2264 (1 + \u03b62)\u03b62\u01eb. The corresponding bound for \u03bbmax(\u03a3\u03b2) then follows from \u03bbmax(\u03a3\u03b2) \u2264 1 + \u03bbmax(M) + \u03bbmax(N).\nWhen \u03b2 = \u03b2\u2217, we have\nEX,W (\u03bd 2) =\nEX,W\n[ (\u3008X,\u03b2\u2217\u3009+W \u2212 \u3008X, (1 \u2212 Z)\u2299 \u03b2\u2217\u3009)2 ]\n(\u03c32 + \u2016Z \u2299 \u03b2\u2217\u201622)2 =\n1\n\u03c32 + \u2016Z \u2299 \u03b2\u2217\u201622 and\nEX,W (\u03bd(1\u2212 Z)\u2299X) = E [(\u3008X,\u03b2\u2217\u3009+W \u2212 \u3008X, (1 \u2212 Z)\u2299 \u03b2\u2217\u3009)(1\u2212 Z)\u2299X]\n\u03c32 + \u2016Z \u2299 \u03b2\u2217\u201622 =\n(1\u2212 Z)\u2299 Z \u2299 \u03b2\u2217 \u03c32 + \u2016Z \u2299 \u03b2\u2217\u201622 = 0.\nTherefore, M = 0 and N = \u03a32. We thus have \u03a3\u03b2\u2217 = \u01ebIp + (1\u2212 \u01eb)Ip = Ip."}, {"heading": "C.1 Proof of Lemma 4.14", "text": "In this example M(\u03b2\u2217) = (E [\u03a3\u03b2\u2217(Y,Z,X)])\u22121 E [Y \u00b5\u03b2\u2217(Y,Z,X)] . Following Lemma C.1, we have \u03a3\u03b2\u2217(Y,Z,X) = Ip. Meanwhile, we have\nE [Y \u00b5\u03b2\u2217(Y,Z,X)] = E\n[ (\u3008\u03b2\u2217,X\u3009 +W ) ( (1\u2212 Z)\u2299X + \u3008Z \u2299 \u03b2\n\u2217,X\u3009 +W \u03c32 + \u2016Z \u2299 \u03b2\u2217\u201622\nZ \u2299 \u03b2\u2217 )]\n= E [(1\u2212 Z)\u2299 \u03b2\u2217 + Z \u2299 \u03b2\u2217] = \u03b2\u2217.\nThus M(\u03b2\u2217) = \u03b2\u2217."}, {"heading": "C.2 Proof of Lemma 4.15", "text": "Following Lemma C.1, we have \u03a3\u03b2\u2217 = Ip. Therefore, Q MCR(\u00b7|\u03b2\u2217) is 1-strongly concave. For any \u03b2 \u2208 B(w\u2016\u03b2\u2217\u2016;\u03b2\u2217), following (C.3), we have that QMCR(\u00b7|\u03b2) is \u00b5-smooth with \u00b5 = 1 + 2\u03b62\u221a\u01eb + (1 + \u03b62)\u03b62\u01eb."}, {"heading": "C.3 Proof of Lemma 4.17", "text": "In order to show QMCRn (\u00b7|\u03b2) is \u03b3n-strongly concave over C(S,S ;R), since QMCRn (\u00b7|\u03b2) is quadratic, it\u2019s then equivalent to show\n1 n\nn\u2211\ni=1\nu\u22a4\u03a3\u03b2(yi, zi,xi)u \u2265 \u03b3n\u2016u\u201622\nfor all u \u2208 C(S,S,R). Expanding \u03a3\u03b2 gives us\n1 n\nn\u2211\ni=1\nu\u22a4\u03a3\u03b2(yi, zi,xi)u \u2265 1\nn\nn\u2211\ni=1\n\u3008\u00b5\u03b2(yi, zi,xi),u\u30092\n\ufe38 \ufe37\ufe37 \ufe38 L1\n\u2212 1 n\nn\u2211\ni=1\n( 1\n\u03c32 + \u2016zi \u2299 \u03b2\u201622\n) \u3008zi \u2299 \u03b2,u\u30092\n\ufe38 \ufe37\ufe37 \ufe38 L2\n.\nWe choose to bound each term using restricted eigenvalue argument in Lemma D.5. To ease notation, we let \u03bd := yi\u2212\u3008(1\u2212zi)\u2299\u03b2,xi\u3009 \u03c32+\u2016zi\u2299\u03b2\u201622 . Term L1. Note that \u00b5\u03b2(yi, zi,xi) are samples of \u00b5\u03b2(Y,Z,X) which is zero mean sub-Gaussian random vector with covariance matrix \u03a31 given in Lemma C.1. Moreover, we have \u03bbmin(\u03a31) \u2265 1 \u2212 \u01eb \u2212 2\u03b62\u221a\u01eb. By restricting \u01eb \u2264 1/4 and assuming \u01eb \u2264 C\u03b6\u22124 for sufficiently small C, we have \u03bbmin(\u03a31) \u2265 12 . Moreover\n\u2016\u00b5\u03b2(Y,Z,X)\u2016\u03c82 . \u2016(1 \u2212 Z)\u2299X\u2016\u03c82 + \u2016\u03bdZ \u2299 \u03b2\u2016\u03c82 . 1 + \u2016\u03bdZ \u2299 \u03b2\u2016\u03c82 .\nNote that \u2016\u03bdZ\u2299\u03b2\u2016\u03c82 = supu\u2208Sp\u22121 \u2016\u03bd\u3008Z\u2299\u03b2,u\u3009\u2016\u03c82 \u2264 \u2016\u03b2\u20162 \u00b7 \u2225\u2225|\u03bd| \u2225\u2225 \u03c82 \u2264 \u03c3\u22122\u2016\u03b2\u20162 \u00b7 \u2225\u2225|W+\u3008X,\u03b2\u2217\u2212(1\u2212\nZ)\u2299 \u03b2\u3009| \u2225\u2225 \u03c82\n. (1 + \u03c9)\u03c1+ (1 + \u03c9)2\u03c12. As \u03b6 := (1 + \u03c9)\u03c1. We thus have \u2016\u00b5\u03b2(Y,Z,X)\u2016\u03c82 . (1 + \u03b6)2. Using Lemma D.5 with the substitution \u03a3 = \u03a31 and X = \u00b5\u03b2(Y,Z,X), we claim that there exist constants Ci such that\nL1 \u2265 1\n4 \u2016u\u201622 \u2212 C0(1 + \u03b6)8\nlog p\nn \u2016u\u201621 for all u \u2208 Rp. (C.4)\nwith probability at least 1\u2212 C1 exp(\u2212C2n(1 + \u03b6)\u22128). Term L2. We now turn to term L2. We introduce n i.i.d. samples {pi}ni=1 of Rademacher random variable P with Pr(P = 1) = Pr(P = \u22121) = 1/2. Equivalently, we have\nL2 = 1\nn\nn\u2211\ni=1\n1\n\u03c32 + \u2016zi \u2299 \u03b2\u201622 \u3008pizi \u2299 \u03b2,u\u30092.\nNote that \u221a\n(\u03c32 + \u2016Z \u2299 \u03b2\u201622)\u22121PZ \u2299 \u03b2 is zero mean sub-Gaussian random vector with covariance matrix \u03a32 given in Lemma C.1. Moreover, we have \u03bbmax(\u03a32) \u2264 \u03b62\u01eb \u2264 1/12, where the last inequality follows by letting \u01eb \u2264 C\u03b6\u22122 for sufficiently small C. Also note that\n\u2225\u2225\u2225\u2225 \u221a (\u03c32 + \u2016Z \u2299 \u03b2\u201622)\u22121PZ \u2299 \u03b2 \u2225\u2225\u2225\u2225 \u03c82 . \u03c3\u22121\u2016Z \u2299 \u03b2\u2016\u03c82 . \u03b6.\nUsing Lemma D.5 with substitution \u03a3 = \u03a32 and X = \u221a\n(\u03c32 + \u2016Z \u2299 \u03b2\u201622)\u22121PZ\u2299\u03b2, we claim there exists constants C \u2032i such that\nL2 \u2264 1 8 \u2016u\u201622 + C \u20320max{\u03b64, 1} log p n \u2016u\u201621, for all u \u2208 Rp. (C.5)\nwith probability at least 1\u2212 C \u20321 exp(\u2212C \u20322nmin{\u03b6\u22124, 1}). Now we put (C.4) and (C.5) together. So we obtain\n1 n\nn\u2211\ni=1\nu\u22a4\u03a3\u03b2(yi, zi,xi)u \u2265 1 8 \u2016u\u201622 \u2212 (C0 + C \u20320)(1 + \u03b6)8 log p n \u2016u\u201621.\nFor any u \u2208 C(S,S;R), we have \u2016u\u20161 \u2264 5 \u221a s\u2016u\u20162. Consequently, when n \u2265 C(1 + \u03b6)8s log p for sufficiently large C, we have that, with high probability, QMCRn (\u00b7|\u03b2) is \u03b3n-strongly concave over C with \u03b3n = 1/9."}, {"heading": "C.4 Proof of Lemma 4.18", "text": "In this example,\n\u2016\u2207QMCRn (\u03b2\u2217|\u03b2)\u2212\u2207QMCR(\u03b2\u2217|\u03b2)\u2016R\u2217\n\u2264 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1 yi\u00b5\u03b2(yi, zi,xi)\u2212 E [Y \u00b5\u03b2(Y,Z,X)] \u2225\u2225\u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\nU1\n+ \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\n\u03a3\u03b2(yi, zi,xi)\u03b2 \u2217 \u2212 E [\u03a3\u03b2(Y,Z,X)]\u03b2\u2217 \u2225\u2225\u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\nU2\n.\nTo ease notation, we let \u03bd := yi\u2212\u3008(1\u2212zi)\u2299\u03b2,xi\u3009 \u03c32+\u2016zi\u2299\u03b2\u201622 . Next we bound the term U1 and U2 respectively. Term U1. Consider one coordinate of vector V := Y \u00b5\u03b2(Y,Z,X). For any j \u2208 [p], we have\nVj = Y [(1\u2212 Zj)Xj + \u03bdZj\u03b2j ].\nSo Vj is sub-exponential random variable since Y and (1\u2212Zj)Xj + \u03bdZj\u03b2j are both sub-Gaussians. Moreover, we have \u2016Y \u2016\u03c82 . \u03c3+\u2016\u03b2\u2217\u20162 and \u2016(1\u2212Zj)Xj+\u03bdZj\u03b2j\u2016\u03c82 . \u2016(1\u2212Zj)Xj\u2016\u03c82+\u2016\u03bdZj\u03b2j\u2016\u03c82 . 1 + \u03c3\u22122(\u03c3 + \u221a 1 + \u03c92\u2016\u03b2\u2217\u20162)\u2016\u03b2\u20162. The last inequality follows from the fact that \u03bd is sub-Gaussian\nwith \u2016\u03bd\u2016\u03c82 . \u03c3\u22122(\u03c3+ \u221a 1 + \u03c92\u2016\u03b2\u2217\u20162). We have \u2016Vi\u2016\u03c81 . \u2016Y \u2016\u03c82 \u00b7\u2016(1\u2212Zj)Xj+\u03bdZj\u03b2j\u2016\u03c82 . (1+\u03b6)3\u03c3, where \u03b6 := (1+\u03c9)\u03c1. By concentration result of sub-exponentials (Lemma D.2) and applying union bound, we have that there exists constant C such that for t . (1 + \u03b6)3\u03c3,\nPr(U1 \u2265 t) \u2264 pe \u00b7 exp(\u2212 Cnt2\n(1 + \u03b6)6\u03c32 ).\nSetting the right hand side to be \u03b4/2 implies that for n & log p+ log(2e/\u03b4),\nU1 . (1 + \u03b6) 3\u03c3\n\u221a log p+ log(2e/\u03b4)\nn (C.6)\nwith probability at least 1\u2212 \u03b4/2. Term U2. Term U2 can be further decomposed into several terms as follows\nU2 \u2264 \u2016a1\u2016\u221e + \u2016a2\u2016\u221e + \u2016a3\u2016\u221e + \u2016a4\u2016\u221e + \u03c3\u22122\u2016a5\u2016\u221e + \u2016a6\u2016\u221e,\nwhere\na1 = 1\nn\nn\u2211\ni=1\n\u2329 (1\u2212 zi)\u2299 xi,\u03b2\u2217 \u232a (1\u2212 zi)\u2299 xi \u2212 E [\u2329 (1\u2212 Z)\u2299X,\u03b2\u2217 \u232a (1\u2212 Z)\u2299X ] ,\na2 = 1\nn\nn\u2211\ni=1\n\u2329 \u03bdzi \u2299 \u03b2,\u03b2\u2217 \u232a (1\u2212 zi)\u2299 xi \u2212 E [\u2329 \u03bdZ \u2299 \u03b2,\u03b2\u2217 \u232a (1\u2212 Z)\u2299X ] ,\na3 = 1\nn\nn\u2211\ni=1\n\u2329 (1\u2212 zi)\u2299 xi,\u03b2\u2217 \u232a \u03bdzi \u2299 \u03b2 \u2212 E [\u2329 (1\u2212 Z)\u2299X,\u03b2\u2217 \u232a \u03bdZ \u2299 \u03b2 ] ,\na4 = 1\nn\nn\u2211\ni=1\n\u03bd2\u3008zi \u2299 \u03b2,\u03b2\u2217\u3009zi \u2299 \u03b2 \u2212 E [ \u03bd2\u3008Z \u2299 \u03b2,\u03b2\u2217\u3009Z \u2299 \u03b2 ] ,\na5 = 1\nn\nn\u2211\ni=1\n\u2329 zi \u2299 \u03b2,\u03b2\u2217 \u232a zi \u2299 \u03b2 \u2212 E [\u2329 Z \u2299 \u03b2,\u03b2\u2217 \u232a Z \u2299 \u03b2 ] , a6 = 1\nn\nn\u2211\ni=1\ndiag(zi)\u03b2 \u2217 \u2212 \u01eb\u03b2\u2217.\nThe key idea to bound the infinite norm of each term ai is the same: showing that each coordinate is finite summation of independent sub-Gaussian (or sub-exponential) random variables and applying concentration result and probabilistic union bound. For each term ai, i = 1, 2, . . . , 6, we have that for any j \u2208 [p],\n\u2016 \u2329 (1\u2212 Z)\u2299X,\u03b2\u2217 \u232a (1\u2212 Zj)\u2299Xj\u2016\u03c81 . \u2016\u03b2\u2217\u20162, \u2016 \u2329 \u03bdZ \u2299 \u03b2,\u03b2\u2217 \u232a (1\u2212 Zj)\u2299Xj\u2016\u03c81 . \u03c3(1 + \u03b6)\u03b62, \u2016 \u2329 (1\u2212 Z)\u2299X,\u03b2\u2217 \u232a \u03bdZj\u03b2j\u2016\u03c81 . \u03c3(1 + \u03b6)\u03b62, \u2016\u03bd2\u3008Z \u2299 \u03b2,\u03b2\u2217\u3009Zj\u03b2j\u2016\u03c81 . \u03c3(1 + \u03b62)\u03b63, \u03c3\u22122\u2016 \u2329 Z \u2299 \u03b2,\u03b2\u2217 \u232a Zj \u2299 \u03b2j\u2016\u03c82 . \u03c3\u03b63, \u2016\u01eb\u03b2\u2217j \u2016\u03c82 . \u01eb\u2016\u03b2\u2217\u2016\u221e\nrespectively. For simplicity, we treat coordinates of every ai as finite sum of sub-exponentials with \u03c81 norm O(\u03c3(1 + \u03b6) 5). Consequently, by concentration result in Lemma D.2, there exists constant C such that\nPr(U2 \u2265 t) \u2264 12p \u00b7 exp ( \u2212 Cnt 2\n\u03c32(1 + \u03b6)10\n)\nfor t . \u03c3(1 + \u03b6)5. By setting the right hand side to be \u03b4/2 in the above inequality, we have that when n & log p+ log(24/\u03b4),\nU2 . \u03c3(1 + \u03b6) 5\n\u221a log p+ log(24/\u03b4)\nn . (C.7)\nwith probability at least 1\u2212 \u03b4/2. Finally, putting (C.6) and (C.7) together completes the proof."}, {"heading": "D Supporting Lemmas", "text": "Lemma D.1. Suppose X1,X2, . . . ,Xn are n i.i.d. centered sub-Gaussian random variables with Orlicz norm \u2016X1\u2016\u03c82 \u2264 K. Then for every t \u2265 0, we have\nPr (\u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\nXi \u2223\u2223\u2223\u2223 \u2265 t ) \u2264 e \u00b7 exp ( \u2212Cnt 2\nK2\n) ,\nwhere C is an absolute constant.\nProof. See the proof of Proposition 5.10 in Vershynin (2010).\nLemma D.2. Suppose X1,X2, . . . ,Xn are n i.i.d. centered sub-exponential random variables with Orlicz norm \u2016X1\u2016\u03c81 \u2264 K. Then for every t > 0, we have\nPr (\u2223\u2223\u2223\u2223 1\nn\nn\u2211\ni=1\nXi \u2223\u2223\u2223\u2223 \u2265 t ) \u2264 2 \u00b7 exp ( \u2212Cmin { t2 K2 , t K } n ) ,\nwhere C is an absolute constant.\nProof. See the proof of Corollary 5.7 in Vershynin (2010).\nLemma D.3. Let X be sub-Gaussian random variable and Y be sub-exponential random variable. Then X \u2212 E[X] is also sub-Gaussian; Y \u2212 E[Y ] is also sub-exponential. Moreover, we have\n\u2016X \u2212 E[X]\u2016\u03c82 \u2264 2\u2016X\u2016\u03c82 , \u2016Y \u2212 E[Y ]\u2016\u03c81 \u2264 2\u2016Y \u2016\u03c81 .\nProof. See Remark 5.18 in Vershynin (2010).\nLemma D.4. Let X,Y be two sub-Gaussian random variables. Then Z = X \u00b7Y is sub-exponential random variable. Moreover, there exits constant C such that\n\u2016Z\u2016\u03c81 \u2264 C\u2016X\u2016\u03c82 \u00b7 \u2016Y \u2016\u03c82 .\nProof. It follows from the basic properties. We omit the details.\nLemma D.5. Let matrix X be an n-by-p random matrix with i.i.d. rows drawn from X, which is zero mean sub-Gaussian random vector with \u2016X\u2016\u03c82 \u2264 K and covariance matrix \u03a3. We let \u03bb1 := \u03bbmin(\u03a3), \u03bbp := \u03bbmax(\u03a3). (1) There exist constants Ci such that\n1 n \u2016Xu\u201622 \u2265 \u03bb1 2 \u2016u\u201622 \u2212 C0\u03bb1 max\n{ K4\n\u03bb21 , 1\n} log p\nn \u2016u\u201621, for all u \u2208 Rp,\nwith probability at least 1\u2212 C1 exp ( \u2212C2nmin { \u03bb2 1 K4 , 1 }) . (2) In Parallel, there exist constants C \u2032i such that\n1 n \u2016Xu\u201622 \u2264 3\u03bbp 2\n\u2016u\u201622 + C \u20320\u03bbpmax { K4\n\u03bb2p , 1\n} log p\nn \u2016u\u201621, for all u \u2208 Rp,\nwith probability at least 1\u2212 C \u20321 exp ( \u2212C \u20322nmin { \u03bb2p K4 , 1 }) .\nProof. It follows by putting Lemma 12 and Lemma 15 in Loh and Wainwright (2011) together.\nLemma D.6. Let X1 and X2 be independent random variables with distribution N (0, 1). For any positive constant C > 0, let event E := {C \u00b7 |X2| \u2265 |X1|}. Then we have (a)\nE [ |X1| \u2223\u2223 E ] \u00b7 Pr(E) =\n\u221a 2\n\u03c0\n[ 1\u2212 \u221a 1\nC2 + 1\n] .\n(b)\nE [ |X2| \u2223\u2223 E ] \u00b7 Pr(E) =\n\u221a 2\n\u03c0 C\u221a 1 + C2 .\n(c)\nE [ |X1X2| \u2223\u2223 E ] \u00b7 Pr(E) = 2C 2\n\u03c0(1 + C2) .\nProof. (a)\nE [ |X1| \u2223\u2223 E ] \u00b7 Pr(E) = 4 \u00b7\n\u222b \u221e\n0\n\u222b uC\n0\n1 2\u03c0 exp(\u22121 2 v2) exp(\u2212u\n2\n2 )vdvdu =\n\u221a 2\n\u03c0\n[ 1\u2212 \u221a 1\nC2 + 1\n] .\n(b)\nE [ |X2| \u2223\u2223 E ] \u00b7 Pr(E) = 4 \u00b7\n\u222b \u221e\n0\n\u222b \u221e\nv/C\n1 2\u03c0 exp(\u22121 2 v2) exp(\u2212u\n2\n2 )ududv =\n\u221a 2\n\u03c0 C\u221a 1 + C2 .\n(c)\nE [ |X1X2| \u2223\u2223 E ] \u00b7 Pr(E) = 4 \u00b7\n\u222b \u221e\n0\n\u222b \u221e\nv/C\n1 2\u03c0 exp(\u2212u\n2\n2 ) exp(\u2212v\n2\n2 )uvdudv\n= 2\n\u03c0\n\u222b \u221e\n0 exp(\u2212C\n2 + 1\n2 v2)vdv =\n2C2\n\u03c0(1 + C2) .\nLemma D.7. Let X \u223c N (0, \u03c32) and Z be Rademacher random variable taking values in {\u22121, 1}. Moreover, X and Z are independent. Function f(x, z; a, \u03b3) is defined as\nf(x, z; a, \u03b3) = x+ az\n1 + exp(\u22122(1+\u03b3) \u03c32\na(x+ az)) .\nThen for any a \u2208 R, \u03b3 \u2208 R, we have \u2223\u2223\u2223E [f(X,Z; a, \u03b3)] \u2212 a\n2\n\u2223\u2223\u2223 \u2264 min { 1\n2 |a\u03b3| exp(\u03b3 2a2 \u2212 a2 2\u03c32 ), \u03c3\u221a 2\u03c0\n+ |a| } .\nIn the special case \u03b3 = 0, we have E [f(X,Z; a, \u03b3)] = a/2.\nProof. First note that\nE [f(X,Z; a, \u03b3)] = 1\n2 E\n[ X + a\n1 + exp(\u22122(1+\u03b3) \u03c32\na(X + a)) + X \u2212 a 1 + exp(\u22122(1+\u03b3)\n\u03c32 a(X \u2212 a))\n]\n= 1\n2 E\n[ X + a\n1 + exp(\u22122(1+\u03b3) \u03c32\na(X + a)) + \u2212X \u2212 a 1 + exp(\u22122(1+\u03b3)\n\u03c32 a(\u2212X \u2212 a))\n] ,\nwhere the first equality is from taking expectation of Z, the second equality is from the fact that the distribution of X is symmetric around 0. Let X \u2032 = X + a, then we have\nE [f(X,Z; a, \u03b3)] = 1\n2 E\n[ X \u2032\n1 + exp(\u22122(1+\u03b3) \u03c32\naX \u2032) +\n\u2212X \u2032\n1 + exp(2(1+\u03b3) \u03c32 aX \u2032)\n]\n= 1\n2 E\n[ X \u2032 \u2212 2 exp(\u2212 2(1+\u03b3) \u03c32 aX \u2032)X \u2032\n1 + exp(\u22122(1+\u03b3) \u03c32 aX \u2032)\n] .\nUsing E [X \u2032] = a, we have E [f(X,Z; a, \u03b3)] \u2212 a/2 = E [ \u2212 exp(\u2212 2(1+\u03b3) \u03c32 aX \u2032)X \u2032\n1 + exp(\u22122(1+\u03b3)\u03c32 aX \u2032)\n]\n=\n\u222b \u221e\n\u2212\u221e\nexp(\u2212 (x\u2212a)2 2\u03c32\n)\u221a 2\u03c0\u03c3\n\u2212 exp(\u22122(1+\u03b3) \u03c32 ax)x 1 + exp(\u22122(1+\u03b3)\u03c32 ax) dx =\n\u222b \u221e\n\u2212\u221e\nexp(\u2212x2+a2 2\u03c32\n)x\u221a 2\u03c0\u03c3\n\u2212 exp(\u2212\u03b3ax \u03c32 )\nexp(a(1+\u03b3)x\u03c32 ) + exp( \u2212a(1+\u03b3)x \u03c32 ) dx\n=\n\u222b \u221e\n0\nexp(\u2212x2+a2 2\u03c32\n)x\u221a 2\u03c0\u03c3\nexp(\u03b3ax \u03c32 )\u2212 exp(\u2212\u03b3ax \u03c32 )\nexp(a(1+\u03b3)x\u03c32 ) + exp( \u2212a(1+\u03b3)x \u03c32 ) dx (D.1)\nWhen a\u03b3 \u2265 0, we have E [f(X,Z; a, \u03b3)] \u2212 a/2 \u2265 0. Under this setting, (D.1) yields that\nE [f(X,Z; a, \u03b3)] \u2212 a/2 \u2264 \u222b \u221e\n0\nexp(\u2212x2+a2 2\u03c32 )x\n2 \u221a 2\u03c0\u03c3\n[ exp( \u03b3ax\n\u03c32 )\u2212 exp(\u2212\u03b3ax \u03c32 ) ] dx\n= 1\n2 exp( \u03b32a2 \u2212 a2 2\u03c32 )\n\u222b \u221e\n0\n1\u221a 2\u03c0\u03c3\n[ exp ( \u2212(x\u2212 \u03b3a) 2\n2\u03c32\n) \u2212 exp ( \u2212(x+ \u03b3a) 2\n2\u03c32\n)] xdx\n= 1\n2 exp( \u03b32a2 \u2212 a2 2\u03c32 )\n\u222b \u221e\n\u2212\u221e 1\u221a 2\u03c0\u03c3 exp\n( \u2212(x\u2212 \u03b3a) 2\n2\u03c32\n) xdx = 1\n2 exp( \u03b32a2 \u2212 a2 2\u03c32 )\u03b3a,\nwhere the first inequality follows from the fact that x+1/x \u2265 2 for any x > 0, the second equality is from\n\u2212 \u222b \u221e\n0 exp\n( \u2212(x+ \u03b3a) 2\n2\u03c32\n) xdx = \u222b 0\n\u2212\u221e exp\n( \u2212(x\u2212 \u03b3a) 2\n2\u03c32\n) xdx.\nWhen a\u03b3 \u2264 0, using similar proof, we have 12 exp( \u03b32a2\u2212a2 2\u03c32 )\u03b3a \u2264 E [f(X,Z; a, \u03b3)] \u2212 a/2 \u2264 0. Combining the two cases, we prove that\n\u2223\u2223E [f(X,Z; a, \u03b3)] \u2212 a/2 \u2223\u2223 \u2264 1\n2 |a\u03b3| exp(\u03b3 2a2 \u2212 a2 2\u03c32 ). (D.2)\nIn the special case when \u03b3 = 0, we thus have E(f(X,Z; a, \u03b3)) = a/2.\nNote that when a\u03b3 \u2265 0, (D.1) also implies that\nE [f(X,Z; a, \u03b3)]\u2212 a/2 \u2264 \u222b \u221e\n0\nexp(\u2212x2+a2 2\u03c32\n)x\u221a 2\u03c0\u03c3\nexp(\u03b3ax \u03c32 )\nexp(a(1+\u03b3)x\u03c32 ) dx =\n\u222b \u221e\n0\nexp(\u2212 (x+a)2 2\u03c32\n)x\u221a 2\u03c0\u03c3 dx\n=\n\u222b \u221e\n0\nexp(\u2212 (x+a)2 2\u03c32\n)(x+ a)\u221a 2\u03c0\u03c3\ndx\u2212 \u222b \u221e\n0\nexp(\u2212 (x+a)2 2\u03c32\n)a\u221a 2\u03c0\u03c3 dx \u2264 \u03c3\u221a 2\u03c0 + |a|.\nSimilarly, when a\u03b3 \u2264 0, we have\nE [f(X,Z; a, \u03b3)] \u2212 a/2 \u2265 \u222b \u221e\n0\nexp(\u2212x2+a2 2\u03c32\n)x\u221a 2\u03c0\u03c3\n\u2212 exp(\u2212\u03b3ax \u03c32 )\nexp(\u2212a(1+\u03b3)x \u03c32\n) dx = \u2212\n\u222b \u221e\n0\nexp(\u2212 (x\u2212a)2 2\u03c32\n)x\u221a 2\u03c0\u03c3 dx\n= \u2212 \u222b \u221e\n0\nexp(\u2212 (x\u2212a)2 2\u03c32\n)(x\u2212 a)\u221a 2\u03c0\u03c3\ndx\u2212 \u222b \u221e\n0\nexp(\u2212 (x\u2212a)2 2\u03c32\n)a\u221a 2\u03c0\u03c3 dx \u2265 \u2212 \u03c3\u221a 2\u03c0 \u2212 |a|.\nTherefore, we have that \u2223\u2223E [f(X,Z; a, \u03b3)] \u2212 a/2 \u2223\u2223 \u2264 \u03c3\u221a\n2\u03c0 + |a|. (D.3)\nPutting (D.2) and (D.3) together completes the proof."}], "references": [{"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["Sivaraman Balakrishnan", "Martin J Wainwright", "Bin Yu"], "venue": "arXiv preprint arXiv:1408.2156,", "citeRegEx": "Balakrishnan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2014}, {"title": "Rop: Matrix recovery via rank-one projections", "author": ["T Tony Cai", "Anru Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "Cai and Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Cai and Zhang.", "year": 2015}, {"title": "A constrained 1 minimization approach to sparse precision matrix estimation", "author": ["Tony Cai", "Weidong Liu", "Xi Luo"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Cai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2011}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["Emmanuel Candes", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "Candes and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2007}, {"title": "Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s and Plan.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s and Plan.", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Spectral experts for estimating mixtures of linear regressions", "author": ["Arun Tejasvi Chaganty", "Percy Liang"], "venue": "arXiv preprint arXiv:1306.3729,", "citeRegEx": "Chaganty and Liang.,? \\Q2013\\E", "shortCiteRegEx": "Chaganty and Liang.", "year": 2013}, {"title": "Improved graph clustering", "author": ["Yudong Chen", "Sujay Sanghavi", "Huan Xu"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A convex formulation for mixed regression with two components: Minimax optimal rates", "author": ["Yudong Chen", "Xinyang Yi", "Constantine Caramanis"], "venue": "In Conf. on Learning Theory,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Exact and stable covariance estimation from quadratic sampling via convex programming", "author": ["Yuxin Chen", "Yuejie Chi", "Andrea Goldsmith"], "venue": "arXiv preprint arXiv:1310.0807,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society", "author": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin"], "venue": "Series B (methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity", "author": ["Po-Ling Loh", "Martin J Wainwright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Loh and Wainwright.,? \\Q2011\\E", "shortCiteRegEx": "Loh and Wainwright.", "year": 2011}, {"title": "Corrupted and missing predictors: Minimax bounds for high-dimensional linear regression", "author": ["Po-Ling Loh", "Martin J Wainwright"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "Loh and Wainwright.,? \\Q2012\\E", "shortCiteRegEx": "Loh and Wainwright.", "year": 2012}, {"title": "Asymptotic convergence properties of the em algorithm with respect to the overlap in the mixture", "author": ["Jinwen Ma", "Lei Xu"], "venue": null, "citeRegEx": "Ma and Xu.,? \\Q2005\\E", "shortCiteRegEx": "Ma and Xu.", "year": 2005}, {"title": "The EM algorithm and extensions, volume 382", "author": ["Geoffrey McLachlan", "Thriyambakam Krishnan"], "venue": null, "citeRegEx": "McLachlan and Krishnan.,? \\Q2007\\E", "shortCiteRegEx": "McLachlan and Krishnan.", "year": 2007}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand Negahban", "Bin Yu", "Martin J Wainwright", "Pradeep K Ravikumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Negahban et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2009}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "Negahban and Wainwright,? \\Q2011\\E", "shortCiteRegEx": "Negahban and Wainwright", "year": 2011}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo"], "venue": "SIAM review,", "citeRegEx": "Recht et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "L1-penalization for mixture regression models", "author": ["Nicolas St\u00e4dler", "Peter B\u00fchlmann", "Sara Van De Geer"], "venue": null, "citeRegEx": "St\u00e4dler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "St\u00e4dler et al\\.", "year": 2010}, {"title": "An analysis of the em algorithm and entropy-like proximal point methods", "author": ["Paul Tseng"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Tseng.,? \\Q2004\\E", "shortCiteRegEx": "Tseng.", "year": 2004}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Structured regularizers for high-dimensional problems: Statistical and computational issues", "author": ["Martin J Wainwright"], "venue": "Annual Review of Statistics and Its Application,", "citeRegEx": "Wainwright.,? \\Q2014\\E", "shortCiteRegEx": "Wainwright.", "year": 2014}, {"title": "High dimensional expectationmaximization algorithm: Statistical optimization and asymptotic normality", "author": ["Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu"], "venue": "arXiv preprint arXiv:1412.8729,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "On the convergence properties of the em algorithm", "author": ["C.F.Jeff Wu"], "venue": "The Annals of statistics,", "citeRegEx": "Wu.,? \\Q1983\\E", "shortCiteRegEx": "Wu.", "year": 1983}, {"title": "Alternating minimization for mixed linear regression", "author": ["Xinyang Yi", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "arXiv preprint arXiv:1310.3745,", "citeRegEx": "Yi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence.", "startOffset": 18, "endOffset": 45}, {"referenceID": 0, "context": "Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M -step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M -step using the state-of-the-art high-dimensional prescriptions (e.g., \u00e0 la Wainwright (2014)) is not guaranteed to provide this balance.", "startOffset": 18, "endOffset": 674}, {"referenceID": 22, "context": ", Wainwright (2014)) the regularizer should be chosen proportional to the target estimation error.", "startOffset": 2, "endOffset": 20}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement.", "startOffset": 2, "endOffset": 25}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement.", "startOffset": 2, "endOffset": 56}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently.", "startOffset": 2, "endOffset": 335}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently.", "startOffset": 2, "endOffset": 349}, {"referenceID": 9, "context": ", Dempster et al. (1977); McLachlan and Krishnan (2007)) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., Wu (1983); Tseng (2004); McLachlan and Krishnan (2007)), very little has been understood about general statistical guarantees until recently.", "startOffset": 2, "endOffset": 380}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.", "startOffset": 20, "endOffset": 47}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step.", "startOffset": 20, "endOffset": 519}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow \u201cclose\u201d to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step.", "startOffset": 20, "endOffset": 960}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow \u201cclose\u201d to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step. In contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization.", "startOffset": 20, "endOffset": 1208}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow \u201cclose\u201d to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step. In contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization. The central challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this must control the optimization error (related to the special structure of \u03b2\u2217), as well as the statistical error. Finally, we note that for finite mixture regression, St\u00e4dler et al.St\u00e4dler et al. (2010) consider an l1 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality.", "startOffset": 20, "endOffset": 1579}, {"referenceID": 0, "context": "Very recent work in Balakrishnan et al. (2014) establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems \u2013 low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in Balakrishnan et al. (2014)) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow \u201cclose\u201d to theM -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in Wang et al. (2014) treats high-dimensional EM using a truncated M -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step. In contrast to work in Wang et al. (2014), we pursue a high-dimensional extension via regularization. The central challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this must control the optimization error (related to the special structure of \u03b2\u2217), as well as the statistical error. Finally, we note that for finite mixture regression, St\u00e4dler et al.St\u00e4dler et al. (2010) consider an l1 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality. However, this work doesn\u2019t establish the theoretical properties of local optima arising from regularized EM. Our work addresses this issue from a local convergence perspective by using a novel choice of regularization. Notation: Let u = (u1, u2, . . . , up) \u22a4 \u2208 Rp be a vector and M = [Mi,j ] \u2208 Rp1\u00d7p2 be a matrix. The lq norm of u is defined as \u2016u\u2016p = ( \u2211p i=1 |ui|). We use \u2016M\u2016\u2217 to denote the nuclear norm of M and \u2016M\u20162 to denote its spectral norm. We use \u2299 to denote the Hadamard product between two vectors, i.e., u \u2299 v = (u1v1, u2v2, . . . , upvp)\u22a4. A p-by-p identity matrix is denoted as Ip. We use capital letter (e.g., X) to denote random variable, vector and matrix. For a sub-Gaussian (subexponential) random variable X, we use \u2016X\u2016\u03c82 (\u2016X\u2016\u03c81) to denote its Orlicz norm (see Vershynin (2010) for detailed definitions).", "startOffset": 20, "endOffset": 2490}, {"referenceID": 22, "context": "To provide some intuitions of such choice, we first note that from theory of high dimensional regularized M-estimator Wainwright (2014), suitable \u03bbn should be proportional to the target estimation error.", "startOffset": 118, "endOffset": 136}, {"referenceID": 0, "context": "Inspired by the low-dimensional analysis of EM in Balakrishnan et al. (2014), we expect the optimization error to decay geometrically, so we choose \u03ba \u2208 (0, 1).", "startOffset": 50, "endOffset": 77}, {"referenceID": 6, "context": "Mixed linear regression (MLR), as considered in some recent work (Chaganty and Liang, 2013; Yi et al., 2013; Chen et al., 2014b), is the problem of recovering two or more linear vectors from mixed linear measurements.", "startOffset": 65, "endOffset": 128}, {"referenceID": 25, "context": "Mixed linear regression (MLR), as considered in some recent work (Chaganty and Liang, 2013; Yi et al., 2013; Chen et al., 2014b), is the problem of recovering two or more linear vectors from mixed linear measurements.", "startOffset": 65, "endOffset": 128}, {"referenceID": 3, "context": ",Cand\u00e8s and Recht (2009); Recht et al.", "startOffset": 1, "endOffset": 25}, {"referenceID": 3, "context": ",Cand\u00e8s and Recht (2009); Recht et al. (2010); Cand\u00e8s and Plan (2011); Negahban et al.", "startOffset": 1, "endOffset": 46}, {"referenceID": 3, "context": "(2010); Cand\u00e8s and Plan (2011); Negahban et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 3, "context": "(2010); Cand\u00e8s and Plan (2011); Negahban et al. (2011); Jain et al.", "startOffset": 8, "endOffset": 55}, {"referenceID": 3, "context": "(2010); Cand\u00e8s and Plan (2011); Negahban et al. (2011); Jain et al. (2013); Chen et al.", "startOffset": 8, "endOffset": 75}, {"referenceID": 3, "context": "(2010); Cand\u00e8s and Plan (2011); Negahban et al. (2011); Jain et al. (2013); Chen et al. (2013); Cai and Zhang (2015)).", "startOffset": 8, "endOffset": 95}, {"referenceID": 1, "context": "(2013); Cai and Zhang (2015)).", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": ", Candes and Tao (2007); Negahban et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 3, "context": ", Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al.", "startOffset": 2, "endOffset": 48}, {"referenceID": 3, "context": ", Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al.", "startOffset": 2, "endOffset": 67}, {"referenceID": 3, "context": ", Candes and Tao (2007); Negahban et al. (2009); Wainwright (2014); Chen et al. (2014a)), has been shown to be useful, both empirically and theoretically, for high dimensional structural estimation.", "startOffset": 2, "endOffset": 88}, {"referenceID": 0, "context": "2 Conditions on Q(\u00b7|\u00b7) Next, we review three technical conditions, originally proposed by Balakrishnan et al. (2014), about population level Q(\u00b7|\u00b7) function.", "startOffset": 90, "endOffset": 117}, {"referenceID": 0, "context": "2 Conditions on Q(\u00b7|\u00b7) Next, we review three technical conditions, originally proposed by Balakrishnan et al. (2014), about population level Q(\u00b7|\u00b7) function. Recall that \u03a9 \u2286 Rp is the basin of attraction. It is well known that performance of EM algorithm is sensitive to initialization. Analyzing Algorithm 1 with any initial point is not desirable in this paper. Our theory is developed with focus on a r-neighbor region round \u03b2\u2217 that is defined as B(r;\u03b2\u2217) := { u \u2208 \u03a9, \u2016u\u2212 \u03b2\u2217\u2016 \u2264 r } . We first assume that Q(\u00b7|\u03b2\u2217) is self consistent as stated below. Condition 1. (Self Consistency) Function Q(\u00b7|\u03b2\u2217) is self consistent, namely \u03b2\u2217 = argmax \u03b2\u2208\u03a9 Q(\u03b2|\u03b2\u2217). It is usually assumed that \u03b2\u2217 maximizes the population log likelihood function. Under this condition, Condition 1 is always satisfied by following the classical theory of EM algorithmMcLachlan and Krishnan (2007). Basically, we require Q(\u00b7|\u03b2) is differentiable over \u03a9 for any \u03b2 \u2208 \u03a9.", "startOffset": 90, "endOffset": 865}, {"referenceID": 16, "context": "It\u2019s instructive to compare Condition 4 with a related condition proposed by Negahban et al. (2009) for analyzing high dimensional M-estimator.", "startOffset": 77, "endOffset": 100}, {"referenceID": 0, "context": "We note that, in Balakrishnan et al. (2014) and Wang et al.", "startOffset": 17, "endOffset": 44}, {"referenceID": 0, "context": "We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of \u2016Mn(\u03b2)\u2212M(\u03b2)\u20162 and \u2016Mn(\u03b2)\u2212M(\u03b2)\u2016\u221e respectively.", "startOffset": 17, "endOffset": 67}, {"referenceID": 0, "context": "We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of \u2016Mn(\u03b2)\u2212M(\u03b2)\u20162 and \u2016Mn(\u03b2)\u2212M(\u03b2)\u2016\u221e respectively. As mentioned earlier, in high dimensional setting, Mn(\u03b2) is not well defined in some models such as mixed linear regression. For mixed linear regression, Wang et al. (2014) resolves this issue by invoking a high dimensional inverse covariance matrix estimation algorithm proposed by Cai et al.", "startOffset": 17, "endOffset": 337}, {"referenceID": 0, "context": "We note that, in Balakrishnan et al. (2014) and Wang et al. (2014), the statistical error is charactrized in terms of \u2016Mn(\u03b2)\u2212M(\u03b2)\u20162 and \u2016Mn(\u03b2)\u2212M(\u03b2)\u2016\u221e respectively. As mentioned earlier, in high dimensional setting, Mn(\u03b2) is not well defined in some models such as mixed linear regression. For mixed linear regression, Wang et al. (2014) resolves this issue by invoking a high dimensional inverse covariance matrix estimation algorithm proposed by Cai et al. (2011). Our formulation (3.", "startOffset": 17, "endOffset": 465}, {"referenceID": 14, "context": "Note that the work in Ma and Xu (2005) provides empirical and theoretical evidences that in low SNR regime, where the overlap density of two Gaussian cluster is small, standard EM algorithm suffers from sublinear convergence asymptotically.", "startOffset": 22, "endOffset": 39}, {"referenceID": 0, "context": "See the proof of Lemma 3 in Balakrishnan et al. (2014). Now we turn to the conditions about QGMM n (\u00b7|\u00b7).", "startOffset": 28, "endOffset": 55}, {"referenceID": 7, "context": "The work in Chen et al. (2014b) shows that there exists an unavoidable phase transition of statistical rate from high SNR to low SNR.", "startOffset": 12, "endOffset": 32}, {"referenceID": 0, "context": "In Balakrishnan et al. (2014), it is proved that when r = 1 32\u2016\u03b2\u20162, there exists \u03c4 \u2208 [0, 1/2] such that QMLR(\u00b7|\u00b7) satisfies Condition 3 with parameter \u03c4 when \u03c1 is sufficiently large.", "startOffset": 3, "endOffset": 30}, {"referenceID": 0, "context": "in low dimensional analysis Balakrishnan et al. (2014), arises from the fundamental limits of EM algorithm.", "startOffset": 28, "endOffset": 55}, {"referenceID": 0, "context": "in low dimensional analysis Balakrishnan et al. (2014), arises from the fundamental limits of EM algorithm. It\u2019s worth to note that Chen et al. (2014b) establish near-optimal low dimensional estimation error that does not depend on \u2016\u03b2\u2217\u20162 based on a convex optimization approach.", "startOffset": 28, "endOffset": 152}, {"referenceID": 4, "context": "The established statistical rate matches (up to the logarithmic factor) the (single) low rank matrix estimation rate proved in Cand\u00e8s and Plan (2011); Negahban et al.", "startOffset": 127, "endOffset": 150}, {"referenceID": 4, "context": "The established statistical rate matches (up to the logarithmic factor) the (single) low rank matrix estimation rate proved in Cand\u00e8s and Plan (2011); Negahban et al. (2011), which is known to be minimax optimal.", "startOffset": 127, "endOffset": 174}, {"referenceID": 0, "context": "We revisit the following result about the gradient stability from Balakrishnan et al. (2014). Lemma 4.", "startOffset": 66, "endOffset": 93}, {"referenceID": 0, "context": "See the proof of Corollary 6 in Balakrishnan et al. (2014).", "startOffset": 32, "endOffset": 59}, {"referenceID": 12, "context": "This unusual constraint is in fact unavoidable, as pointed out in Loh and Wainwright (2012). We now turn to validate the conditions about finite sample function QMCR n (\u00b7|\u00b7).", "startOffset": 66, "endOffset": 92}, {"referenceID": 0, "context": "A similar result is proved in Balakrishnan et al. (2014). The slight difference is that Balakrishnan et al.", "startOffset": 30, "endOffset": 57}, {"referenceID": 0, "context": "A similar result is proved in Balakrishnan et al. (2014). The slight difference is that Balakrishnan et al. (2014) shows Lemma 6.", "startOffset": 30, "endOffset": 115}], "year": 2015, "abstractText": "Latent variable models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M -step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M -step using the state-of-the-art high-dimensional prescriptions (e.g., \u00e0 la Wainwright (2014)) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.", "creator": "LaTeX with hyperref package"}}}