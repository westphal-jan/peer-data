{"id": "1512.01322", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Fixed-Point Performance Analysis of Recurrent Neural Networks", "abstract": "Recurrent neural networks these shown important usual been many applications, however just limiting rates theoretical as functionality or software consulting implementations. The installing complexity can be much tumbled following minimizing the every - continuous created allowable and monitor. This complete analyzes the corresponding - right combined both recurrent neural telephone using hand bulldoze recently coefficient hence. The damping sensitivity 's each layer with RNNs is studied, much the impressive rates - moving optimization results minimizing within contribute of weights in need virtues however overall sometimes referred. A slang model including one phoneme recognition examples come developed.", "histories": [["v1", "Fri, 4 Dec 2015 06:07:28 GMT  (610kb,D)", "http://arxiv.org/abs/1512.01322v1", null], ["v2", "Sat, 23 Jan 2016 11:53:34 GMT  (611kb,D)", "http://arxiv.org/abs/1512.01322v2", null], ["v3", "Tue, 27 Sep 2016 11:42:34 GMT  (611kb,D)", "http://arxiv.org/abs/1512.01322v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sungho shin", "kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1512.01322"}, "pdf": {"name": "1512.01322.pdf", "metadata": {"source": "CRF", "title": "FIXED-POINT PERFORMANCE ANALYSIS OF RECURRENT NEURAL NETWORKS", "authors": ["Sungho Shin", "Kyuyeon Hwang", "Wonyong Sung"], "emails": ["shshin@dsp.snu.ac.kr,", "khwang@dsp.snu.ac.kr,", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "Index Terms\u2014 recurrent neural network, quantization, word length optimization, fixed-point optimization, deep neural network"}, {"heading": "1. INTRODUCTION", "text": "Recurrent Neural networks (RNNs) employ feedback path inside, and they are suitable for processing input data whose dimension is not fixed. Language models for automatic speech recognition systems, human action recognition and text generation are important applications using RNNs [1, 2, 3].\nAmong many types of RNNs [4, 5, 6], the most powerful one is the long short term memory (LSTM) RNN [5]. A standard LSTM RNN is depicted in Figure 1. A layer of LSTM RNN contains an input gate, an output gate, and a forget gate. An LSTM layer with N units demands a total of approximately 4N2 + 4NM + 5N weights where M is the previous layer size. Considering a character level language model that employs three 1024 size LSTM layers, the trained network demands about 22.3 mega weights. Thus, reducing the weights size in RNNs is very important for VLSI or embedded computing systems based implementations.\nThere have been many studies on efficient hardware implementation of artificial neural networks applying direct\nThis work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grants funded by the Ministry of Education, Science and Technology (MEST), Republic of Korea (No. 2012R1A2A2A06047297).\nquantization [7, 8, 9, 10]. Retraining based quantization that readjust the fixed-point weights by training after direct quantization of floating-point parameters was adopted in [11, 12, 13]. Previous research for retrain-based fixedpoint optimization of feedforward neural networks (FFNNs) and convolutional neural networks (CNNs) uses usually 3-8 bits for the weights and signals. However RNN (especially LSTM RNN) employs a more complex feed-back based structure and hence optimum quantization is challenging.\nIn this paper, we optimize the word-length of weights and signals for fixed-point LSTM RNNs using the retraining method. The proposed scheme consists of three parts which are floating point training, sensitivity analysis of fixed-point RNNs and retraining of fixed-point RNNs. As we know this is the first work that tries fixed-point quantization of large size recurrent neural networks.\nThe paper is organized as follows. In Section 2, the quantization procedure for weights and signals is given. Section 3 describes layerwise fixed-point sensitivity analysis for weights and signals. Experimental results are provided in Section 4 and concluding remarks follow in Section 5."}, {"heading": "2. RETRAIN-BASED WEIGHT QUANTIZATION", "text": "The retrain based quantization method includes the fixedpoint conversion process inside of the training procedure so that the network learns the quantization effects [12]. This method shows much better performance when the number of bits is small. In this method, the floating-point weights for RNNs are prepared with the backpropagation through time (BPTT) algorithm [14]. BPTT begins by unfolding a recurrent neural network through time, then training proceeds in a manner similar to adapting a feed-forward neural network with backpropagation.\nFor direct quantization, a uniform quantization function is employed and the function Q(\u00b7) is defined as follows:\nQ(w) = sgn(w) \u00b7\u2206 \u00b7min (\u230a |(w)|\n\u2206 + 0.5\n\u230b , M \u2212 1\n2\n) (1)\nwhere sgn(\u00b7) is the sign function, \u2206 is a quantization step size, and M represents the number of quantization levels.\nar X\niv :1\n51 2.\n01 32\n2v 1\n[ cs\n.L G\n] 4\nD ec\n2 01\nNote that M is normally an odd number since the weight values can be positive or negative. When M is 5, the weights are represented by \u22122\u2206, \u2212\u2206, 0, \u2206, 2\u2206, which can be represented in 3 bits.\nBefore starting the weights quantization, a quantization step size \u2206 has to be determined. For selecting a proper step size, L2 error minimization criteria is applied as adopted in [12]. The quantization error E is depicted as follows:\nE = 1\n2 N\u2211 i=1 ( Q(wi)\u2212 wi )2 , (2)\nwhere N is the number of weights, wi is the i-th weight value in floating-point. For minimizing the quantization error E, (2) can be changed as follows:\nE = 1\n2 N\u2211 i=1 ( \u2206 \u00b7 zi \u2212 wi )2 , (3)\nwhere zi is the integer membership of wi. The quantization error E is minimized by the following two step iterative ap-\nproach.\nz(t) = argmin z E(w, z,\u2206(t\u22121)) (4)\n\u2206(t) = argmin \u2206 E(w, z(t),\u2206), (5)\nwhere the superscript (t) indicates the iteration step. (5) can be solved by the derivative of the error with respect to \u2206(t) to zero.\n\u2206(t) =\nN\u2211 i=1\nwi \u00b7 z(t)i N\u2211 i=1 ( z (t) i\n)2 (6) The iteration stops when \u2206(t) is converged.\nAfter obtaining the fixed-point weights, zi\u2206, a retraining procedure follows. We maintain both floating-point weights and quantized ones, since applying BPTT algorithm directly with quantized weights usually does not work. This is because the amount of weights to be changed after each training step is much smaller than the quantized step size \u2206. Assuming that the RNN is unfolded, the algorithm can be described as follows:\nneti = \u2211 j\u2208Ai w (q) ij y (q) j\ny (q) i = Ri(\u03c6i(neti)) (7)\n\u03b4j = \u03c6 \u2032 j(netj) \u2211 i\u2208Pj \u03b4iw (q) ij (8)\n\u2202E\n\u2202wij = \u2212\u03b4iy(q)j (9)\nwij,new = wij \u2212 \u03b1 \u2329 \u2202E\n\u2202wij \u232a w\n(q) ij,new = Qij(wij,new) (10)\nwhere neti is the summed input value of unit i, \u03b4i is the error signal of unit i, wij is the weight from unit j to unit i, yj is the output signal of unit j, \u03b1 is the learning rate, Ai is the set of units anterior to unit i, Pj is the set of units posterior to unit j, R(\u00b7) is the signal quantizer, Q(\u00b7) is the weight quantizer, \u03c6(\u00b7) is the activation function, the superscript (q) indicates quantization, and \u3008\u00b7\u3009 is an average operation via the mini-batch. Equation (7), (8), (9), and (10) represent the forward, backward, gradient calculation, and weights update phases each."}, {"heading": "3. QUANTIZATION SENSITIVITY ANALYSIS", "text": "LSTM RNNs usually contain millions of weights and thousands of signals. Therefore, it is necessary to group them according to their range and the quantization sensitivity [15]. Fortunately, a neural network can easily be layerwisely\ngrouped. Throughout this sensitivity check, we can identify which layer in the neural network needs more bits for quantization. The network for phoneme recognition contains three hidden RNN layers. Thus the weights can be grouped into 7 groups, which are In-L1, L1, L1-L2, L2, L2-L3, L3, and L3-Out groups, where In-L1 connects input and the first LSTM layer and L1 is the recurrent path in the first level LSTM. Figure1.(b) illustrates the grouping. In the sensitivity analysis, we only quantize the weights of the selected group while those in other groups are unquantized. Signal layerwise sensitivity analysis also follows the same scheme. In the standard LSTM RNN, the activation functions are usually logistic sigmoid or tanh.\nsigmoid(x) = 1\n1 + e\u2212x (11)\ntanh(x) = ex + e\u2212x\nex \u2212 e\u2212x (12)\nThe output range of these two functions are limited by 0 to 1 and by -1 to 1, respectively. Therefore, the quantization step size \u2206 is determined by the quantization level M . For example, if the signal word-length is two bits (M is four.), the quantization points are 0/3, 1/3, 2/3, and 3/3 for the sigmoid and -1/1, 0/1, and 1/1 for the tanh. However signals of linear units are not bounded and their quantization range should be determined carefully. In our phoneme recognition example, each component of the input data is normalized to have zero mean and a unit variance over the training set. The input range is chosen to be from -3 to 3. One hot encoding is used for the input linear units in the language model example."}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "In this section, we will first show the result of the layerwise sensitivity analysis for signals and weights, and then fully quantized network performances. The proposed quantization strategy is evaluated using two RNNs, one for phoneme recognition and the other for character level language modeling. Advanced training techniques such as early stopping, adaptive learning rate, and nesterov momentum are employed."}, {"heading": "4.1. Phoneme Recognition", "text": "Phoneme recognition experiments were performed on the TIMIT corpus [16]. The detailed experimental conditions are the same with [17] except the mapping of output classes for scoring. The input layer consists of 123 linear units to accept real valued inputs. Three hidden LSTM layers have the same size of 512. The output layer consists of 61 softmax units which correspond to 61 target phoneme labels. The network is trained using Fractal with training parameters that are 32 forward steps and 64 backward steps with 64 streams [18]. Initial learning rate was 10\u22125 and it is decreased at proper timing until 10\u22127. Momentum was 0.9 [19] and adadelta was adopted for weights update [20]. The network demands approximately 5.5 mega weights. As a result it needs about 22MB with a 32bit floating-point format.\nFigure 2 shows the result of the layerwise sensitivity analysis for the weights. The original phoneme frame error rate was 27.26% with 512 LSTM layer size. The result indicates that all layers except the input-LSTM1 group shows the almost the same quantization sensitivity and requires only two bits. Input-LSTM1 weights group demands at least three quantization bits. In the signal sensitivity analysis, all layers need only three to five quantization bits.\nUsing the sensitivity analysis results, we construct a fully quantized LSTM RNN and the results are shown in Table1. The result shows that the phoneme error rate of 27.74% is achieved with only about 10% of the weight capacity needed"}, {"heading": "2-2-3-4-4-4-6 (10.52%) 6-6-7 1.848 1.573", "text": ""}, {"heading": "3-3-4-5-5-5-7 (13.64%) 6-6-7 1.770 1.537", "text": ""}, {"heading": "2-2-3-4-4-4-6 (10.52%) 7-7-8 5.113 1.573", "text": ""}, {"heading": "3-3-4-5-5-5-7 (13.64%) 7-7-8 1.970 1.523", "text": ""}, {"heading": "4-4-5-6-6-6-8 (16.75%) 8-8-9 1.560 1.500", "text": "for the floating-point version."}, {"heading": "4.2. Language Model", "text": "A character level language model was trained using English Wikipedia dataset which was used in [21]. Each character was put into the neural network input using their own ASCII code values with one-hot encoding, which needs 256 units.\nThe input layer contains 256 linear units to accommodate real valued inputs. Three hidden LSTM layers have the same size of 1024. The output layer consists of 256 softmax units that correspond to 256 character level one-hot encoding. The network is trained using Fractal with training parameters that are 128 forward steps and 128 backward steps with 64 streams [18]. Learning rate was decreased from 10\u22126 to 10\u22128 during training. Momentum was 0.9 and adadelta was adopted for weights update. This network needs approxi-\nmately 22.3 mega weights. Figure 3 shows the layerwise sensitivity analysis results for the weights. The bit per character (BPC) of the floatingpoint language model was 1.485. The analysis shows that the most sensitive weights group of the network is the L3Out layer. Note that the last RNN layer is connected to the softmax layer. The first weights group shows low sensitivity when compared to the phoneme recognition example. This is because one-hot encoding of the ASCII code is used as for the input in this language model. The network has two types of paths, the forward and the recurrent connections. For both paths, the layer that is close to the output shows higher quantization sensitivity. The sensitivity analysis of signals shows the minimum of four or five bits for quantization.\nWe next try fixed-point optimization of all signals and weights. While the sensitivity analysis quantizes only one group of weights or signals, the fixed-point optimization quantizes all the weights and signals simultaneously to find out the most optimum set of word-lengths. The results are summarized in Table 2. Unlike the phoneme recognition example, this application needs more quantization bits in addition to the sensitivity analysis results. A reasonable result was achieved with two more bits for both weights and signals than the sensitivity analysis result. The memory space needed is only 16.75%"}, {"heading": "5. CONCLUDING REMARKS", "text": "This work investigates the fixed-point characteristics of RNNs for phoneme recognition and language modeling. The retrainbased fixed-point optimization greatly reduces the wordlength of weights and signals. In the phoneme recognition example, most of the weights can be represented in 3 bits, while the language modeling needs 5 or 6 bits for obtaining near floating-point results. By this optimization, the weights capacity needed can be reduced to only 10% or 17% of that required for floating-point implementations. The reduced weights and signals can lead to efficient hardware implementations or higher cache memory hit ratio in software based systems."}, {"heading": "6. REFERENCES", "text": "[1] Toma\u0301s\u030c Mikolov, Stefan Kombrink, Luka\u0301s\u030c Burget, Jan Honza C\u030cernocky\u0300, and Sanjeev Khudanpur, \u201cExtensions of recurrent neural network language model,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5528\u20135531.\n[2] Moez Baccouche, Franck Mamalet, Christian Wolf, Christophe Garcia, and Atilla Baskurt, \u201cSequential deep learning for human action recognition,\u201d in Human Behavior Understanding, pp. 29\u201339. Springer, 2011.\n[3] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan, \u201cShow and tell: A neural image caption generator,\u201d arXiv preprint arXiv:1411.4555, 2014.\n[4] Jeffrey L Elman, \u201cFinding structure in time,\u201d Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.\n[5] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong shortterm memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[6] Herbert Jaeger, \u201cThe echo state approach to analysing and training recurrent neural networks-with an erratum note,\u201d Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, vol. 148, pp. 34, 2001.\n[7] Perry Moerland and Emile Fiesler, \u201cNeural network adaptations to hardware implementations,\u201d Tech. Rep., IDIAP, 1997.\n[8] Jordan L Holi and Jenq-Neng Hwang, \u201cFinite precision error analysis of neural network hardware implementations,\u201d Computers, IEEE Transactions on, vol. 42, no. 3, pp. 281\u2013290, 1993.\n[9] Gunhan Dundar and Kenneth Rose, \u201cThe effects of quantization on multilayer neural networks.,\u201d IEEE transactions on neural networks/a publication of the IEEE Neural Networks Council, vol. 6, no. 6, pp. 1446\u2013 1451, 1994.\n[10] Cle\u0301ment Farabet, Berin Martini, Polina Akselrod, Selc\u0327uk Talay, Yann LeCun, and Eugenio Culurciello, \u201cHardware accelerated convolutional neural networks for synthetic vision systems,\u201d in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 257\u2013260.\n[11] Chuan Zhang Tang and Hon Keung Kwan, \u201cMultilayer feedforward neural networks with single powers-of-two weights,\u201d Signal Processing, IEEE Transactions on, vol. 41, no. 8, pp. 2724\u20132727, 1993.\n[12] Kyuyeon Hwang and Wonyong Sung, \u201cFixed-point feedforward deep neural network design using weights+ 1, 0, and- 1,\u201d in Signal Processing Systems (SiPS), 2014 IEEE Workshop on. IEEE, 2014, pp. 1\u20136.\n[13] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung, \u201cFixed point optimization of deep convolutional neural networks for object recognition,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 1131\u20131135.\n[14] Paul J Werbos, \u201cBackpropagation through time: what it does and how to do it,\u201d Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.\n[15] Wonyong Sung and Ki-II Kum, \u201cSimulation-based word-length optimization method for fixed-point digital signal processing systems,\u201d Signal Processing, IEEE Transactions on, vol. 43, no. 12, pp. 3087\u20133090, 1995.\n[16] John S Garofolo, Lori F Lamel, William M Fisher, Jonathon G Fiscus, and David S Pallett, \u201cDarpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1,\u201d NASA STI/Recon Technical Report N, vol. 93, pp. 27403, 1993.\n[17] Alan Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.\n[18] Kyuyeon Hwang and Wonyong Sung, \u201cSingle stream parallelization of generalized lstm-like rnns on a gpu,\u201d arXiv preprint arXiv:1503.02852, 2015.\n[19] Robert A Jacobs, \u201cIncreased rates of convergence through learning rate adaptation,\u201d Neural networks, vol. 1, no. 4, pp. 295\u2013307, 1988.\n[20] Matthew D Zeiler, \u201cAdadelta: An adaptive learning rate method,\u201d arXiv preprint arXiv:1212.5701, 2012.\n[21] Ilya Sutskever, James Martens, and Geoffrey E Hinton, \u201cGenerating text with recurrent neural networks,\u201d in Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1017\u20131024."}], "references": [{"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Honza \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5528\u20135531.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential deep learning for human action recognition", "author": ["Moez Baccouche", "Franck Mamalet", "Christian Wolf", "Christophe Garcia", "Atilla Baskurt"], "venue": "Human Behavior Understanding, pp. 29\u201339. Springer, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "The echo state approach to analysing and training recurrent neural networks-with an erratum note", "author": ["Herbert Jaeger"], "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, vol. 148, pp. 34, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural network adaptations to hardware implementations", "author": ["Perry Moerland", "Emile Fiesler"], "venue": "Tech. Rep., IDIAP, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Finite precision error analysis of neural network hardware implementations", "author": ["Jordan L Holi", "Jenq-Neng Hwang"], "venue": "Computers, IEEE Transactions on, vol. 42, no. 3, pp. 281\u2013290, 1993.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "The effects of quantization on multilayer neural networks", "author": ["Gunhan Dundar", "Kenneth Rose"], "venue": "IEEE transactions on neural networks/a publication of the IEEE Neural Networks Council, vol. 6, no. 6, pp. 1446\u2013 1451, 1994.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Hardware accelerated convolutional neural networks for synthetic vision systems", "author": ["Cl\u00e9ment Farabet", "Berin Martini", "Polina Akselrod", "Sel\u00e7uk Talay", "Yann LeCun", "Eugenio Culurciello"], "venue": "Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 257\u2013260.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Multilayer feedforward neural networks with single powers-of-two weights", "author": ["Chuan Zhang Tang", "Hon Keung Kwan"], "venue": "Signal Processing, IEEE Transactions on, vol. 41, no. 8, pp. 2724\u20132727, 1993.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "Fixed-point feedforward deep neural network design using weights+ 1, 0, and- 1", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "Signal Processing Systems (SiPS), 2014 IEEE Workshop on. IEEE, 2014, pp. 1\u20136.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 1131\u20131135.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "Simulation-based word-length optimization method for fixed-point digital signal processing systems", "author": ["Wonyong Sung", "Ki-II Kum"], "venue": "Signal Processing, IEEE Transactions on, vol. 43, no. 12, pp. 3087\u20133090, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon Technical Report N, vol. 93, pp. 27403, 1993.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Single stream parallelization of generalized lstm-like rnns on a gpu", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "arXiv preprint arXiv:1503.02852, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["Robert A Jacobs"], "venue": "Neural networks, vol. 1, no. 4, pp. 295\u2013307, 1988.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1017\u20131024.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Language models for automatic speech recognition systems, human action recognition and text generation are important applications using RNNs [1, 2, 3].", "startOffset": 141, "endOffset": 150}, {"referenceID": 1, "context": "Language models for automatic speech recognition systems, human action recognition and text generation are important applications using RNNs [1, 2, 3].", "startOffset": 141, "endOffset": 150}, {"referenceID": 2, "context": "Language models for automatic speech recognition systems, human action recognition and text generation are important applications using RNNs [1, 2, 3].", "startOffset": 141, "endOffset": 150}, {"referenceID": 3, "context": "Among many types of RNNs [4, 5, 6], the most powerful one is the long short term memory (LSTM) RNN [5].", "startOffset": 25, "endOffset": 34}, {"referenceID": 4, "context": "Among many types of RNNs [4, 5, 6], the most powerful one is the long short term memory (LSTM) RNN [5].", "startOffset": 25, "endOffset": 34}, {"referenceID": 5, "context": "Among many types of RNNs [4, 5, 6], the most powerful one is the long short term memory (LSTM) RNN [5].", "startOffset": 25, "endOffset": 34}, {"referenceID": 4, "context": "Among many types of RNNs [4, 5, 6], the most powerful one is the long short term memory (LSTM) RNN [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "quantization [7, 8, 9, 10].", "startOffset": 13, "endOffset": 26}, {"referenceID": 7, "context": "quantization [7, 8, 9, 10].", "startOffset": 13, "endOffset": 26}, {"referenceID": 8, "context": "quantization [7, 8, 9, 10].", "startOffset": 13, "endOffset": 26}, {"referenceID": 9, "context": "quantization [7, 8, 9, 10].", "startOffset": 13, "endOffset": 26}, {"referenceID": 10, "context": "Retraining based quantization that readjust the fixed-point weights by training after direct quantization of floating-point parameters was adopted in [11, 12, 13].", "startOffset": 150, "endOffset": 162}, {"referenceID": 11, "context": "Retraining based quantization that readjust the fixed-point weights by training after direct quantization of floating-point parameters was adopted in [11, 12, 13].", "startOffset": 150, "endOffset": 162}, {"referenceID": 12, "context": "Retraining based quantization that readjust the fixed-point weights by training after direct quantization of floating-point parameters was adopted in [11, 12, 13].", "startOffset": 150, "endOffset": 162}, {"referenceID": 11, "context": "The retrain based quantization method includes the fixedpoint conversion process inside of the training procedure so that the network learns the quantization effects [12].", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "In this method, the floating-point weights for RNNs are prepared with the backpropagation through time (BPTT) algorithm [14].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "For selecting a proper step size, L2 error minimization criteria is applied as adopted in [12].", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "Therefore, it is necessary to group them according to their range and the quantization sensitivity [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "Phoneme recognition experiments were performed on the TIMIT corpus [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "The detailed experimental conditions are the same with [17] except the mapping of output classes for scoring.", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "The network is trained using Fractal with training parameters that are 32 forward steps and 64 backward steps with 64 streams [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "9 [19] and adadelta was adopted for weights update [20].", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "9 [19] and adadelta was adopted for weights update [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "A character level language model was trained using English Wikipedia dataset which was used in [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "The network is trained using Fractal with training parameters that are 128 forward steps and 128 backward steps with 64 streams [18].", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "Recurrent neural networks have shown excellent performance in many applications, however they require increased complexity in hardware or software based implementations. The hardware complexity can be much lowered by minimizing the word-length of weights and signals. This work analyzes the fixed-point performance of recurrent neural networks using a retrain based quantization method. The quantization sensitivity of each layer in RNNs is studied, and the overall fixedpoint optimization results minimizing the capacity of weights while not sacrificing the performance are presented. A language model and a phoneme recognition examples are used.", "creator": "LaTeX with hyperref package"}}}