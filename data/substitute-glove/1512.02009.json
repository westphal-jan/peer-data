{"id": "1512.02009", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Jointly Modeling Topics and Intents with Global Order Structure", "abstract": "Modeling document structure even of brings importance which liberalism studies took that automatically. The substitute of mean research is because plot the document denying structure by modeling correspondence still a mixture of taboo remember way rhetorical quote. While was pertaining them low unchanged while place regarding, and rhetorical functions of charged any same following certain brought 1998 epistemological. We authorize GMM - LDA, single views modeling based Bayesian unsupervised model, to analyses the definitive seeking structure interrogators this order information. Our model is oriented that has the quality immediately pepper the dedicatory most do fully learning. Additionally, imbecilic two-body find instead name allow similar rest significant posited between matters but intents. We perform useful in both 713-3602 same post-graduate settings, pre that similar ferocity common means designing made addition state - own - the - art cubics.", "histories": [["v1", "Mon, 7 Dec 2015 12:16:58 GMT  (264kb,D)", "http://arxiv.org/abs/1512.02009v1", "Accepted by AAAI 2016"]], "COMMENTS": "Accepted by AAAI 2016", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["bei chen", "jun zhu", "nan yang", "tian tian 0001", "ming zhou", "bo zhang"], "accepted": true, "id": "1512.02009"}, "pdf": {"name": "1512.02009.pdf", "metadata": {"source": "CRF", "title": "Jointly Modeling Topics and Intents with Global Order Structure", "authors": ["Bei Chen", "Jun Zhu", "Nan Yang", "Tian Tian", "Ming Zhou", "Bo Zhang"], "emails": ["{chenbei12@mails.,", "dcszj@,", "tiant13@mails.,", "dcszb@}tsinghua.edu.cn;", "mingzhou}@microsoft.com"], "sections": [{"heading": "Introduction", "text": "People often organize utterances into meaningful and coherent documents, conforming to certain conventions and underlying structures. For example, scripts (Frermann, Titov, and Pinkal 2014), scientific papers (O\u0301 Se\u0301aghdha and Teufel 2014), official mails, news and encyclopedia articles all have relatively fixed discourse structure and exhibit recurrent patterns. Learning the document structure is of great importance for discourse analysis and has various applications, such as text generation (Prasad et al. 2005) and text summarization (Louis, Joshi, and Nenkova 2010).\nThere are two important aspects of document structure learning: topic modeling and rhetorical structure modeling. Topic modeling assumes multiple topics often exist within a domain. It aims to discover the latent semantics of the documents, with many popular models such as Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003), in which each document is posited as an admixture over an underlying set of topics, and each word is draw from a specific topic. Rhetorical structure modeling aims to uncover the underlying organization of documents. Inspired by the discourse theory (Mann and Thompson 1988), each sentence in a document can be assigned a rhetorical function, or called in-\n\u2217Corresponding author. Copyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nIntent words Topic words Intent structure\nBACKGROUND: Modeling document structure is of great\nimportance for discourse analysis. OBJECTIVE: The goal of\nthis research is to capture the document intent structure. We\npresent GMM-LDA to jointly model the topics and intents.\nMETHOD: GMM-LDA is a topic modeling based Bayesian\nmodel that employs generalized Mallows model to capture the\nintent order structure and introduces binary variables to indicate\nthe types of words. RESULT: Experimental results on Chemical\nand Elements datasets demonstrate the superiority of our model.\nFigure 1: Demonstration of the intent and topic words. Stop words (in gray) can be removed by preprocessing.\ntent. For example, the sentences in a scientific paper may have different intents such as \u201cbackground\u201d, \u201cobjective\u201d, \u201cmethod\u201d and \u201cresult\u201d. Fig 1 presents an example of the intent structure of an abstract.\nDocument-level topics and sentence-level intents usually show contradictory characteristics. For example, it is often sensible to assume that the topics are relatively unchanged through one document (e.g., in LDA), while the sentences\u2019 intents usually change following certain order in discourse. Furthermore, each document often follows a progression of nonrecurring coherent intents (Halliday and Hasan 1976), and the sentences with the same intent tend to appear within the same block of a document. Based on these observations, it\u2019s natural to think that jointly considering these two incompatible structures can help to model document better.\nIn this paper, we present a hierarchical Bayesian model to discover both the topic structure and rhetorical structure of documents by jointly considering topics and the above order structure in discourse. To this end, we assume that all the words can be divided into two types: topic word and intent word. Specifically, topic words in a document are relevant to the document\u2019s topic and spread throughout the document, while intent words mainly contribute to the rhetorical functions of the sentences. Following the example in Fig. 1, the words \u201cdocument\u201d, \u201cdiscourse\u201d and \u201cintent\u201d are likely to be topic words; they indicate the specific research domain of this document. Meanwhile, the words \u201cresult\u201d, \u201cdataset\u201d and \u201cdemonstrate\u201d are likely to be intent words and a sentence with these words may have the intent structure label \u201cresult\u201d.\nar X\niv :1\n51 2.\n02 00\n9v 1\n[ cs\n.C L\n] 7\nD ec\n2 01\n5\nWe introduce a binary variable for each word to indicate its type, and model the topic structure and intent structure respectively using topic models. Inspired by the generalized Mallows model (GMM) (Fligner and Verducci 1986), we incorporate the intent order structure using GMM-Multi prior, which not only conforms to our intuition of nonrecurring coherent intents but also captures the global effects of the orders. To further improve the expressive ability, we present two important variants of GMM-LDA. One is to incorporate the known intent labels of sentences for supervised learning, and the other is to incorporate entropic regularization to better separate the words into two types under the regularized Bayesian inference framework (Zhu, Chen, and Xing 2014). Finally, we represent experiments on real datasets to demonstrate the effectiveness of our methods over several state-of-the-art baselines. To the best of our knowledge, we present the first model for topics and intents simultaneously, where the intent order structure is described globally by using GMM-Multi prior. In the rest paper, we first present GMM-LDA, followed by the supervised version and entropic regularization. Then we present experimental results with analysis. Finally, we discuss related work and conclude."}, {"heading": "Unsupervised GMM-LDA Model", "text": "We consider the following document structure learning problem. We are given a corpus D = {sd}Dd=1 with D documents, where a document sd is a sequence of Nd sentences denoted by sd = (wd1,wd2, ...,wdNd) and a sentence wds is a bag of Nds words denoted by wds = {wdsm}Ndsm=1. The size of the vocabulary is V . There are T topics and K intents in total. Topics are document-level, while intents are sentence-level. Our goal of document structure learning is to model the intent and the topic simultaneously and assign an intent label to each sentence in the corpus.\nWe build our models on the following assumptions, 1) Type: Each word in the corpus is either an intent word or a topic word; 2) Order: The intents of sentences within a document change following certain orders and the orders are similar within a domain; and 3) Coherence: The same intent does not appear in disconnected portions of a document.\nTo characterize the type assumption, we associate each word with a binary variable to indicate whether it is an intent word or a topic word. For the order and coherence assumptions, we introduce a GMM-Multi prior to model the intent order structure. We start with the description of GMM-Multi prior, then GMM-LDA in detail with its inference method."}, {"heading": "GMM-Multi Prior for Intent Ordering", "text": "According to the coherence assumption, the same intent could not be assigned to the unconnected portions of a document. To satisfy this assumption, we introduce a GMMMulti prior over possible intent permutations. GMM-Multi, motivated by (Chen et al. 2009), is an extension to the generalized Mallows model (Fligner and Verducci 1986). It concentrates probability mass on a small set of similar permutations around a canonical ordering \u03c00, which confirms to the intuition that the intent orders are similar within a domain. The inversion representation of permutations is used\ninstead of the direct order sequence: If we set the canonical ordering \u03c00 as the identity permutation (1, 2, ...,K), then any permutation \u03c0 can be denoted as a (K\u22121)-dimensional vector \u03c5 = (\u03c51, \u03c52, ..., \u03c5K\u22121), where \u03c5k is the count of the numbers in \u03c0 that are before k and greater than k. For instance, permutation \u03c0 = (2, 1, 4, 5, 3) can be represented as \u03c5 = (1, 0, 2, 0), where \u03c53 = 2 as there are two numbers, 4 and 5, that are before 3 and greater than 3. \u03c5K is omitted as it is always zero. Obviously, a one-to-one correlation exists between these two kinds of permutation representations. Furthermore, each element of \u03c5 is independent of each other. The marginal distribution over \u03c5k is\nGMMk(\u03c5k; \u03c1k) = e\u2212\u03c1k\u03c5k\n\u03c8k(\u03c1k) , (1)\nwhere \u03c8k(\u03c1k) = 1\u2212exp (\u2212(K\u2212k+1)\u03c1k)\n1\u2212exp (\u2212\u03c1k) is the normalization factor and \u03c1k > 0 is the dispersion parameter. Then the probability mass function with parameters \u03c1 = (\u03c11, \u03c12, ..., \u03c1K\u22121) can be written as the product over all the components, so that the number of parameters grows linearly with the number of intents. Due to the exponential form, the conjugate prior for \u03c1k is also an exponential distribution with two parameters :\nGMM0(\u03c1k; \u03c5k,0, \u03bd0) \u221d e(\u2212\u03c1k\u03c5k,0\u2212log\u03c8k(\u03c1k))\u03bd0 . (2)\nIntuitively, \u03bd0 is the number of previous trials and \u03c5k,0 is the average number in previous trials for \u03c5k. Let \u03c10 be the prior value for each \u03c1k. By setting the maximum likelihood estimate of \u03c1k to be \u03c10, we can obtain \u03c5k,0 = 1exp (\u03c10)\u22121 \u2212 K\u2212k+1 exp ((K\u2212k+1)\u03c10)\u22121 .\nThe GMM-Multi prior is defined over sentence intents zd via a generative process on a given inversion representation of permutation \u03c5d. Firstly, we draw a bag of intents ud (Nd elements) and each element uds\u223cMultinomial(\u03bb). Then \u2211 sI(uds = k) represents the number of sentences of intent k in this document. Secondly, we obtain the permutation of intents\u03c0d = Compute\u2212\u03c0(\u03c5d), where Compute\u2212\u03c0 transforms \u03c5d into the intent permutation \u03c0d. And finally, we obtain the intent structure of the sentences zd = Compute\u2212z(ud,\u03c0d), so that zds is the intent label for sentencewds. Compute\u2212z is the algorithm to obtain the intent structure zd using the bag of intents ud and the permutation \u03c0d. It arranges all the intent labels in ud as the order of \u03c0d with the same intent labels appearing together. For example, when ud = {1, 1, 2, 3, 3, 3, 5} and \u03c0d = (2, 1, 4, 5, 3), we can obtain zd = (2, 1, 1, 5, 3, 3, 3). Note that not all the K intent labels should appear in zd; it depends on the intent labels in ud. As in the example, ud does not contain 4, so 4 does not appear in the intent structure zd."}, {"heading": "Generative Process of GMM-LDA", "text": "Now we present GMM-LDA, an unsupervised Bayesian generative model, as illustrated in Fig. 2. GMM-LDA simultaneously models the topics (the blue part in Fig. 2) and the intents (the red part in Fig. 2). The binary variable bdsm denotes the type of word wdsm: if bdsm = 1, then wdsm is a topic word; if bdsm = 0, then wdsm is an intent word. Each\nsentence has a specific intent label zds, while each document sd has a topic mixing distribution \u03b8d. For topic words, there is a document-specific topic model. It is a hierarchical Bayesian model that posits each document as an admixture of T topics, where each topic \u03b2t is a multinomial distribution over a V -word vocabulary, drawn from a language model \u03b2t \u223c Dirichlet(\u03b20) (t \u2208 [T ]). For intent words, there is a rhetorical language model, in which the intent structure zd of document sd is generated from a bag of intent labels ud and an intent permutation \u03c0d follows the GMM-Multi prior. The total number of intents isK, and each intent\u03b1k is also a multinomial distribution over vocabulary, drawn from another language model \u03b1k \u223c Dirichlet(\u03b10) (k \u2208 [K]).\nFor each document sd, the generating process is 1. Draw a topic proportion \u03b8d \u223c Dirichlet(\u03b80). 2. Obtain the intent structure zd \u223c GMM-Multi(\u03c1,\u03bb), so\nthat zds is the intent label for sentence wds. 3. For each word wdsm in document d,\n(a) Draw an indicator bdsm \u223c Bernoulli(\u03b3). (b) If bdsm = 0, then wdsm is from intent part:\ndraw wdsm \u223c Multinomial(\u03b1zds). (c) if bdsm = 1, then wdsm is from topic part:\ndraw a topic tdsm \u223c Multinomial(\u03b8d), and draw wdsm \u223c Multinomial(\u03b2tdsm).\nFor fully-Bayesian GMM-LDA, we assume the following priors: \u03b3 \u223c Beta(\u03b30), \u03bb \u223c Dirichlet(\u03bb0), which is a distribution to express how likely each intent label is to appear regardless of positions, and each component of \u03c1 follows \u03c1k \u223c GMM0(\u03c10, \u03bd0), k \u2208 [K \u2212 1]. The variables subscripted with 0 are fixed prior hyperparameters."}, {"heading": "Collapsed Gibbs Sampling", "text": "Let M = {u,\u03c1,\u03c5, t, b} denote all the variables to be learned during training. Then with Bayes\u2019 theorem, the posterior distribution of GMM-LDA is\nq(M|D) \u221d p0(M)p(D|M), (3) where p0(M) is the prior and p(D|M) is the likelihood. Then it can be learned with a collapsed Gibbs sampler due to the conjugate priors. We naturally split the variables into four parts, namely u, \u03c1, \u03c5 and {t, b}, then sample them using their posterior distributions respectively.\nForu : For each document sd,ud is a bag of intent labels with Nd elements. We resample each element uds via:\nq(uds = x|D,M\u2212uds) \u221d p0(uds = x)p(sd|M,D\u2212sd)\n\u221d (f\u2212dsu=x + \u03bb0) K\u220f k=1 \u0393(f\u2212d0,k,\u00b7 + V \u03b10) \u0393(f0,k,\u00b7 + V \u03b10) V\u220f v=1 \u0393(f0,k,v + \u03b10) \u0393(f\u2212d0,k,v + \u03b10) ,\nwhere the subscript \u201c\u2212\u201d denotes that some elements are omitted from a set, e.g., M\u2212uds is the set M except uds and D\u2212sd is the set of all documents in D except sd. Let u = {ud}Dd=1 denote all the intent labels in the corpus. f\u2212dsu=x is the count of times that intent label x appears in u except uds. Let wv denote the v-th word in the vocabulary1, where v \u2208 [V ]. f0,k,v counts the times that wv appears as an intent word in the sentences with intent label k. Then, f0,k,\u00b7 = \u2211V v=1 f0,k,v . The superscript \u201c\u2212d\u201d indicates that the frequency is calculated over all documents except sd. For \u03c1 : We update each \u03c1k from its posterior distribution:\nq(\u03c1k|D,M\u2212\u03c1k)=GMM0 ( \u03c1k; \u2211D d=1\u03c5d,k+\u03c5k,0\u03bd0\nD+\u03bd0 , D+\u03bd0\n) ,\nSince the normalization constant is unknown, it is intractable to sample directly from GMM0. Fortunately, slice sampling (Neal 2003) can be used to solve this problem.\nFor \u03c5 : For the inversion representation \u03c5d of document sd, each \u03c5d,k can be resampled independently from its posterior distribution:\nq(\u03c5d,k = v|D,M\u2212\u03c5d,k)\u221dp0(\u03c5d,k = v)p(sd|M,D\u2212sd)\n\u221d p0(\u03c5d,k = v) K\u220f k=1 \u0393(f\u2212d0,k,\u00b7 + V \u03b10) \u0393(f0,k,\u00b7 + V \u03b10) V\u220f v=1 \u0393(f0,k,v + \u03b10) \u0393(f\u2212d0,k,v + \u03b10) ,\nwhere p0(\u03c5d,k = v) = GMMk(\u03c5d,k = v; \u03c1k) is the prior. For t, b : Since tdsm has meaningful value only when bdsm = 1, we jointly sample bdsm and tdsm for the topics. The joint distributions are\nq(bdsm = 1, tdsm = t|D,M\u2212{bdsm,tdsm})\n\u221d (f\u2212dsmb=1 + \u03b30) f\u2212dsm1,t,v + \u03b20\nf\u2212dsm1,t,\u00b7 + V \u03b20\nf\u2212dsm1,d,t + \u03b80\nf\u2212dsm1,d,\u00b7 + T\u03b80 ,\nq(bdsm = 0, tdsm = \u2205|D,M\u2212{bdsm,tdsm})\n\u221d (f\u2212dsmb=0 + \u03b30) f\u2212dsm0,zds,v + \u03b10\nf\u2212dsm0,zds,\u00b7 + V \u03b10 ,\nwhere fb=1 is the number of topic words in the corpus and fb=0 is the number of intent words. f1,t,v counts all the times that wv appears in the corpus with indicator variable value 1 and topic label t. Then, f1,t,\u00b7 = \u2211V v=1 f1,t,v . f1,d,t is the number of words in document sd with indicator variable value 1 and topic label t. Then f1,d,\u00b7 = \u2211T t=1 f1,d,t . The superscript \u201c\u2212dsm\u201d indicates that the frequency is calculated exceptwdsm. According to the joint distribution of bdsm and tdsm, we compute the T + 1 non-zero probabilities, then do normalization and sample from the resulting multinomial.\n1wv is different from wdsm. wdsm is a word in sentence wds ."}, {"heading": "Supervised GMM-LDA", "text": "GMM-LDA is unsupervised; it learns the document structure without human annotation. As suggested by existing supervised topic models (McAuliffe and Blei 2008; Wang, Blei, and Li 2009; Zhu, Ahmed, and Xing 2012), the predictive power can be greatly improved with a small amount of labeled documents. Here, we present a supervised GMM-LDA which combines the known intent labels of sentences during learning.\nWe consider the setting where a part of the documents in the corpus are labeled, that is, each sentence is assigned an intent label. Our goal is to develop a supervised model to learn the intent structures for the remaining unlabeled documents. The simplest way to leverage the label information is to use the labels directly during learning instead of sampling them. However, the GMM describes the intent order structure in a global way, which makes the process more complicated. In the unsupervised case, the canonical intent permutation is the identity ordering (1, 2, ...,K) as the intent numbers are completely symmetric and not linked to any meaningful intent label. However, the true intent labels are already known in the supervised case. Then a challenging problem is to determine the canonical permutation \u03c00. Moreover, for the part of labeled documents, we have the true intent structure zd, which is unnecessary to draw from the GMM-Multi prior. So the next challenging problem is how to leverage the known intent structure zd to help for learning. That is, how to obtain ud and \u03c0d using zd. Below, we discuss in detail on how to solve these problems.\nFor\u03c00: As shown in Fig.3, we present an approximate three-step algorithm to obtain the canonical permutation \u03c00. Step 1: We compute \u03c0\u2032d for each labeled document sd, where \u03c0\u2032d is the permutation of the Kd (6 K) intent labels appearing in document sd. We arrange the Kd intent labels in the order they appear in zd. However, there exist cases where the same intent label appears in disconnected portions of zd, which are rare in practice. When encountering these cases, we take the position that the label appears most consecutively in the sequence. If there are two or more of these occurrences, we take the first position (see Fig. 3 for some examples). Step 2: We introduce variables gij(i, j \u2208 [K]), where gij counts the times of the intent label i appearing before j in all \u03c0\u2032d. Then we define a directed graph G = (V, E), where V \u2208 [K] is the set of nodes and E = {(i, j)|gij >= gji; i, j \u2208 V} is the set of edges. Step 3:\nWe obtain the \u03c00 by calculating the topological sequence of G. If there are circles in G, we randomly break them. If there are multiple topological sequences, we randomly take one. In our experiments, the real situations are always in accordance with our intuition that only one topological sequence can be obtained from G.\nFor ud and \u03c0d : To combine the label information, we compute ud and \u03c0d using zd and \u03c00. ud can be obtained by directly putting all the ordered elements of zd into a bag. \u03c0d is a permutation of K numbers, which can be obtained by inserting the remaining K \u2212 Kd numbers into \u03c0\u2032d . We want \u03c0d to be as close to \u03c00 as possible, which is consistent with the idea of GMM. Specially, d(\u03c0d,\u03c00) is the distance from \u03c0d to \u03c00, which defined as the minimum number of swaps of adjacent elements needed to transform \u03c0d into the same order of \u03c00. Inspired by the idea of greedy method, we insert theK\u2212Kd numbers into \u03c0\u2032d one by one and each step need to minimize the distance. During supervised learning, when it comes to a labeled document sd, we directly take the values of \u03c0d and ud instead of updating them."}, {"heading": "GMM-LDA with Entropic Regularization", "text": "GMM-LDA jointly models the two incompatible structures of documents by using a binary variable to indicate the type (intent or topic) of each word. It can happen that a same word located in two different positions are assigned with different types, which is somewhat unreasonable. Most of the time, the type of a word can be decided regardless of which position it appears in. For instance, the words \u201cpropose\u201d and \u201cexperiment\u201d are more likely to be intent words regardless of their positions. In order to model the significant divergence between topics and intents, we introduce the entropy of the words to make our model more descriptive. As we know, in information theory, the entropy of a discrete random variable X = {x1, x2, ..., xn} can explicitly be written as H(X) = \u2212 \u2211n i=1 p(xi) log p(xi). Similarly, the entropy of a word wv in the vocabulary can be formulated as:\nH(bv) = \u2212 \u2211 i=0,1 p(bv = i) log p(bv = i), (4)\nwhere p(bv = i) denotes the probability that word wv appears as an intent word (i = 0) or an topic word (i = 1) in the corpus. Lower entropic value means better separation.\nAs GMM-LDA is under Bayesian inference framework, it is challengeable to incorporate the entropic knowledge. Nevertheless, regularized Bayesian inference framework (Zhu, Chen, and Xing 2014) provides us an alternative interpretation of Bayesian inference (Williams 1980) and can combine domain knowledge flexibly. Specifically, GMM-LDA with entropic regularization can be formulated as:\nmin q(M)\u2208P KL(q(M)||q(M|D)) + c V\u2211 v=1 H(bv), (5)\nwhereP is the space of probability distributions and c > 0 is a regularization parameter. When c = 0, the optimal solution of the Kullback-Leibler divergence KL(q||p) is q(M) = q(M|D), the standard Bayesian posterior distribution as in\nEq. (3). When c > 0, the entropic knowledge is imposed as a regularization constraint. With mean-filed assumption, the inference of Eq. (5) is similar to that of GMM-LDA. Moreover, the entropic regularization is only relevant to b, which can be combined with both the unsupervised GMM-LDA and the supervised GMM-LDA."}, {"heading": "Experimental Results", "text": "To demonstrate the efficacy of our models, we evaluate the performance on two tasks: unsupervised clustering and supervised classification. We use two real datasets: 1) Chemical (Guo et al. 2010): It contains 965 abstracts of scientific papers about 5 kinds of chemicals, and each abstract focuses on one of the 5 topics. Each sentence is annotated with one of the 7 intent labels: Background, Objective, Related Work, Method, Result, Conclusion and Future Work; and 2) Elements (Chen et al. 2009): It consists of 118 articles from the English Wikipedia, and each article talks about one of the 118 chemical elements in the periodic table. Each paragraph is annotated with an intent label. We take the 8 most frequently occurring intent labels: Top-level Segment, History, Isotopes, Applications, Occurrence, Notable Characteristics, Precautions and Compounds, and filter out paragraphs with other labels. Although the intent structure is paragraphlevel in Element, while it is sentence-level in Chemical, the word \u201csentence\u201d is used throughout the paper for simplicity. Tab. 2 summarizes the dataset statistics. Although both datasets are in chemistry domain, they have different characteristics that can be observed from the experimental results. In Chemical, the intent orders are relatively fixed due to the writing conventions of scientific papers, while they are more variable in Elements.\nData preprocessing involves removing a small set of stop words, tokens containing non-alphabetic characters, tokens appearing less than 3 times, tokens of length one and sentences with less than 5 valid tokens. We report the average results over 5 runs, while each run takes a sufficiently large\nnumber of iterations (e.g. 2000) to converge. Statistical significance is measured with t-test."}, {"heading": "Unsupervised Clustering", "text": "Our goal of unsupervised clustering is to learn an intent label for each sentence in the corpus without any true label information. Adjusted Rand Index (ARI)(Vinh, Epps, and Bailey 2010), recall, precision and F-score are used as our evaluation measures. F-score is the harmonic mean of recall and precision. For all the four measures, higher scores are better.\nWe consider two variants of our model: 1) GMM-LDA: Our unsupervised model; and 2) EGMM-LDA: GMM-LDA with entropic regularization. For hyperparameters, we set \u03b80 = 0.1, \u03bb0 = 0.1, \u03b10 = 0.1, \u03b20 = 0.1 and \u03b30 = 1, since we find that the results are insensitive to them. \u03bd0 is set to be 0.1 times the number of documents in the corpus. For EGMM-LDA, we set the regularization parameter c to be 0.1. The baseline methods we use are: 1) K-means: The feature used for each sentence is the bag of words. 2) Boilerplate-LDA: The model presented in (O\u0301 Se\u0301aghdha and Teufel 2014). 3) GMM: The intent part of GMM-LDA, which is the content model by (Chen et al. 2009) and can be implemented by fixing all indicator variables bdsm to 0 during learning; and 4) GMM-LDA (Uniform): The model assumes a uniform distribution over all intent permutations and can be implemented by fixing \u03c1 to zero.\nClustering performance: We set the number of topics T = 10 for Chemical and T = 5 for Elements. Tab. 1 shows the results. For Chemical, our two models outperform all other methods on four measures, with p-values smaller than 0.001 except for Boilerplate-LDA with K = 5. The exception of Boilerplate-LDA may be caused by the large variance in the results of multiple runs when K is small. Since the first-order Markov chain is used in Boilerplate-LDA for order structure learning, which only has a local view and is more susceptible to noise. Moreover, first-order Markov chain would select the same intent label for disconnected sentences within a document, which is against our intuition. Our models overcome these problems by using GMM and also achieve better performance. The simpler variants of our models achieve reasonable performance. GMM underperforms GMM-LDA, indicating that modeling topics and intents simultaneously provides a richer and more effective way to document structure learning. The bad performance\nof the uniform variant proves the indispensability to model the intent order structure. Moreover, our model yields better results with entropic regularization. As to Elements, the results are similar to that of Chemical. However, we can observe that GMM performs competitively in Elements, which shows the characteristic of this dataset that there is no obvious topic structure. We can also observe that the highest recall scores are obtained by K-means with very low precision scores, since K-means prefers to assign the same label to most of the sentences. It can thus be seen that the task is difficult and the richer models are required.\nHyperparameter \u03c10: \u03c10 controls the variability of the order structure and would be set according to different datasets. The model with large \u03c10 assigns massive probabilities around the canonical permutation and the order structure is relatively fixed, while the model with small \u03c10 relaxes the constraints and has a variety of orders far from the canonical one. For Elements with K = 10, we change \u03c10 from 0.125 to 4 and report the F-scores of GMM-LDA in Fig. 4. It can be observed that the performance is stable in a wide range (e.g. 0.5 < \u03c10 < 2). We set \u03c10 = 2 for all the experiments except for Elements with K = 10, in which we set \u03c10 = 1.\nTypes of words: To embody the intents and topics in the results, we assume that each word in the vocabulary is either an intent word or a topic word. If a word appears in the corpus more as an intent word than a topic word, we classify it as an intent word; otherwise, it is a topic word. Note that this additional condition is introduced only for the ease of demonstration. In order to see how our models separate these two types, we list 18 most commonly used words of each type in Chemical according to the result of EGMMLDA. As shown in Tab. 3, we can observe that almost all the intent words has rhetorical functions that can express the intents of sentences, while almost all the topic words are about chemical topics. The good separations justify our assumption and show the effectiveness of our models."}, {"heading": "Supervised Classification", "text": "Now, we evaluate our supervised models for classifying sentences. For each dataset, we randomly choose 20% documents; annotate their sentences with intent labels; and use them for training. Our goal is to learn the intent labels for the sentences in the remaining 80% documents. We report accuracy (ACC) and the ARI scores to show the improvements compared to the unsupervised learning. We again consider two variants of our model: 1) sGMM-LDA: Our supervised model; and 2) sEGMM-LDA: sGMM-LDA with entropic regularization. The baseline methods are : 1) SVM: We use the bag-of-words features, linear kernel and SVMLight tools (Joachims 1998). 2) sBoilerplate-LDA : The supervised version of Boilerplate-LDA, in which we fix the known labels during learning instead of updating them; and 3) sGMM : The intent part of our sGMM-LDA. All the settings are the same as that in the unsupervised learning.\nClassification performance: Tab. 4 presents the results. For Chemical, the best accuracies are achieved by our two models (p < 0.01), which again proves that our assumption of the two types of words is reasonable and the intent order structure can be better modeled by employing GMM. For Elements, sBoilerplate-LDA and sGMM perform competitively, while EGMM-LDA beats all the other methods (p < 0.05). It shows that our model is more robust with entropic regularization. The ARI scores improve a lot compared to that in Tab. 1, indicating that the predictive power can be largely improved with just 20% labeled documents.\nIntent words: We can obtain the canonical intent permutation by the known intent labels, at the same time the distribution over vocabulary can be learned. Tab. 5 shows the results on Chemical with sEGMM-LDA. We can observe that the canonical intent order (numbered from 0 to\n6) conforms to the convention in scientific writing. Moreover, from the high-frequency words of each intent, we can see that most of the words express the intent labels well. For instance, \u201cstudy\u201d and \u201cinvestigated\u201d express the intent Objective, while \u201cincreased\u201d and \u201csignificantly\u201d are for Result."}, {"heading": "Related Work", "text": "From the algorithmic perspective, our work is grounded in topic models, such as Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003), which have been widely developed for many NLP tasks. Instead of representing documents as bags of words, many expanded models take specific structural constraints into consideration (Purver et al. 2006; Gruber, Weiss, and Rosen-Zvi 2007). Among different models, our work has a closer relation to the models with order structure. For order modeling, Markov chain can only capture the dependence locally (O\u0301 Se\u0301aghdha and Teufel 2014; Barzilay and Lee 2004; Elsner, Austerweil, and Charniak 2007), while the generalized Mallows model (GMM) (Fligner and Verducci 1986) has a global view (Chen et al. 2009; Du, Pate, and Johnson 2015; Cheng, Hu\u0308hn, and Hu\u0308llermeier 2009). A more complete model can be obtained by dividing the words into different types. In early trial of zoneLDAb (Varga, Preotiuc-Pietro, and Ciravegna 2012), a type of words are for describing background, which are independent of the category of the sentence. Boilerplate-LDA (O\u0301 Se\u0301aghdha and Teufel 2014) also considers two types: document-specific topic words and rhetorical words. Three types of words are learnt by a rule-based method in (Nguyen and Shirai 2015). However, global order structure is not considered in these models. Therefore, jointly modeling topics and intents with global order structure is of great value."}, {"heading": "Conclusion and Future Work", "text": "We present GMM-LDA (both unsupervised and supervised) for document structure learning, which simultaneously model topics and intents. The generalized Mallows model is employed to model the intent order globally. Moreover, we consider the entropic regularization to make the model more descriptive. Our results demonstrate the reasonability of our intuitions and the effectiveness of our models. For future work, we are interested in making our models richer by combining local coherence constraints.\nAcknowledgments This work was mainly done when the first author was an intern at Microsoft Research Asia. The work was supported by the National Basic Research Program (973 Program) of China (Nos. 2013CB329403, 2012CB316301), National NSF of China (Nos. 61322308, 61332007), TNList Big Data Initiative, and Tsinghua Initiative Scientific Research Program (Nos. 20121088071, 20141080934)."}], "references": [{"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["Barzilay", "R. Lee 2004] Barzilay", "L. Lee"], "venue": null, "citeRegEx": "Barzilay et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2004}, {"title": "Content modeling using latent permutations", "author": ["Chen"], "venue": null, "citeRegEx": "Chen,? \\Q2009\\E", "shortCiteRegEx": "Chen", "year": 2009}, {"title": "Decision tree and instance-based learning for label ranking", "author": ["H\u00fchn Cheng", "W. H\u00fcllermeier 2009] Cheng", "J. H\u00fchn", "E. H\u00fcllermeier"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2009}, {"title": "Topic segmentation with an ordering-based topic model", "author": ["Pate Du", "L. Johnson 2015] Du", "J. Pate", "M. Johnson"], "venue": null, "citeRegEx": "Du et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "A unified local and global model for discourse coherence", "author": ["Austerweil Elsner", "M. Charniak 2007] Elsner", "J. Austerweil", "E. Charniak"], "venue": null, "citeRegEx": "Elsner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "Distance based ranking models. JRSS. Series B (Methodological) 359\u2013369", "author": ["Fligner", "M. Verducci 1986] Fligner", "J. Verducci"], "venue": null, "citeRegEx": "Fligner et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Fligner et al\\.", "year": 1986}, {"title": "A hierarchical bayesian model for unsupervised induction of script knowledge", "author": ["Titov Frermann", "L. Pinkal 2014] Frermann", "I. Titov", "M. Pinkal"], "venue": null, "citeRegEx": "Frermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Frermann et al\\.", "year": 2014}, {"title": "Hidden topic markov models", "author": ["Weiss Gruber", "A. Rosen-Zvi 2007] Gruber", "Y. Weiss", "M. Rosen-Zvi"], "venue": "In AISTATS", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Identifying the information structure of scientific abstracts: an investigation of three different schemes", "author": ["Guo"], "venue": "In Workshop on Biomedical Natural Language Processing", "citeRegEx": "Guo,? \\Q2010\\E", "shortCiteRegEx": "Guo", "year": 2010}, {"title": "Cohesion in english", "author": ["Halliday", "M. Hasan 1976] Halliday", "R. Hasan"], "venue": null, "citeRegEx": "Halliday et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Halliday et al\\.", "year": 1976}, {"title": "Discourse indicators for content selection in summarization", "author": ["Joshi Louis", "A. Nenkova 2010] Louis", "A. Joshi", "A. Nenkova"], "venue": "In Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Louis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Louis et al\\.", "year": 2010}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse 8(3):243\u2013281", "author": ["Mann", "W. Thompson 1988] Mann", "S. Thompson"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Mann et al\\.", "year": 1988}, {"title": "Supervised topic models", "author": ["McAuliffe", "J. Blei 2008] McAuliffe", "D. Blei"], "venue": "In NIPS", "citeRegEx": "McAuliffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "McAuliffe et al\\.", "year": 2008}, {"title": "Topic modeling based sentiment analysis on social media for stock market prediction", "author": ["Nguyen", "T. Shirai 2015] Nguyen", "K. Shirai"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Unsupervised learning of rhetorical structure with un-topic models", "author": ["\u00d3 S\u00e9aghdha", "D. Teufel 2014] \u00d3 S\u00e9aghdha", "S. Teufel"], "venue": "In COLING", "citeRegEx": "S\u00e9aghdha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "S\u00e9aghdha et al\\.", "year": 2014}, {"title": "The penn discourse treebank as a resource for natural language generation", "author": ["Prasad"], "venue": "In CL Workshop on Using Corpora for NLG", "citeRegEx": "Prasad,? \\Q2005\\E", "shortCiteRegEx": "Prasad", "year": 2005}, {"title": "Unsupervised topic modelling for multi-party spoken discourse", "author": ["Purver"], "venue": null, "citeRegEx": "Purver,? \\Q2006\\E", "shortCiteRegEx": "Purver", "year": 2006}, {"title": "Unsupervised document zone identification using probabilistic graphical models", "author": ["Preotiuc-Pietro Varga", "A. Ciravegna 2012] Varga", "D. PreotiucPietro", "F. Ciravegna"], "venue": null, "citeRegEx": "Varga et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Varga et al\\.", "year": 2012}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "author": ["Epps Vinh", "N. Bailey 2010] Vinh", "J. Epps", "J. Bailey"], "venue": null, "citeRegEx": "Vinh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vinh et al\\.", "year": 2010}, {"title": "Simultaneous image classification and annotation", "author": ["Blei Wang", "C. Li 2009] Wang", "D. Blei", "F. Li"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Medlda: Maximum margin supervised topic models", "author": ["Ahmed Zhu", "J. Xing 2012] Zhu", "A. Ahmed", "E. Xing"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2012}, {"title": "Bayesian inference with posterior regularization and applications to infinite latent svms", "author": ["Chen Zhu", "J. Xing 2014] Zhu", "N. Chen", "E. Xing"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Modeling document structure is of great importance for discourse analysis and related applications. The goal of this research is to capture the document intent structure by modeling documents as a mixture of topic words and rhetorical words. While the topics are relatively unchanged through one document, the rhetorical functions of sentences usually change following certain orders in discourse. We propose GMM-LDA, a topic modeling based Bayesian unsupervised model, to analyze the document intent structure cooperated with order information. Our model is flexible that has the ability to combine the annotations and do supervised learning. Additionally, entropic regularization can be introduced to model the significant divergence between topics and intents. We perform experiments in both unsupervised and supervised settings, results show the superiority of our model over several state-of-the-art baselines.", "creator": "LaTeX with hyperref package"}}}