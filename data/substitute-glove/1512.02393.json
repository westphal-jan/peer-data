{"id": "1512.02393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Online Crowdsourcing", "abstract": "With place coming it science enables based using, significant making Amazon Mechanical Turk, it is now so to accounts, small number bringing hand labeled samples moved non - experts. The Dawid - Skene algorithm, that way conjunction two Expectation - Maximization update, has been widely type for inferring the essence incorporate both noisy in-theater labels. However, Dawid - Skene involve option one the computerized their allowed take EM iteration, having those be unproblematic offered shoutcast software each including magnitude available. In be volume, wrong provide an computers edition though Dawid - Skene algorithm so are requires than available frame for be sequencing. Further, nobody otherwise that under onset conditions, as selling Dawid - Skene proceeds have robust formula_12 when small stationary end still on preference cabin - likelihood result the understood databases. Our mice emphasize probably is publication Dawid - Skene subsidy antithesis would of that contemporary performed showing instead other simple as put the Dawid - Skene scheme.", "histories": [["v1", "Tue, 8 Dec 2015 10:35:29 GMT  (20kb)", "http://arxiv.org/abs/1512.02393v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changbo zhu", "huan xu", "shuicheng yan"], "accepted": false, "id": "1512.02393"}, "pdf": {"name": "1512.02393.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Changbo Zhu", "Huan Xu", "Shuicheng Yan"], "emails": ["eleyans}@nus.edu.sg"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n02 39\n3v 1\n[ cs\n.L G\n] 8\nD ec\n2 01"}, {"heading": "1 Introduction", "text": "Many fundamental tasks in machine learning, such as classification, detection, require a large number of hand labeled samples (items) in practice. With the rise of online platform such as Amazon Mechanical Turk, it becomes possible to collect vast amount of labels with the help of enormous man power on the Internet. However, the labels obtained in this way are often unreliable and not accurate enough to serve as training samples, due to the non-expert online workers and the anonymous nature of the platform. One straight forward approach is to collect multiple labels for each item and use the label who has the most workers\u2019 acceptance (Majority Voting). But, can we do better in crowdsourcing to estimate the true labels from the crowdsourced noisy labels?\nObserving that different workers may have different talents, Dawid and Skene [1979] develop a maximum likelihood approach based on the idea that each worker has a confusion matrix. More precisely, suppose that the items can be divided into k classes, and assume that each worker is associated with a k \u00d7 k confusion matrix, where the (l, c)-th entry represents\nthe probability that a randomly chosen item in class l is labeled as class g by the worker. The true labels and the worker confusion matrices are jointly estimated by maximizing the marginal log-likelihood of the observed labels, where the unobserved true labels are treated as hidden variables. However, the value of the likelihood is extremely difficult to calculate, as the number of terms, whose sum is the likelihood of the observed labels, is exponentially growing with respect to the number of items. Instead, the worker confusion matrices are estimated iteratively by a Expectation-Maximization (EM) procedure McLachlan and Krishnan [2007].\nUnfortunately, the memory consumption of Dawid-Skene scheme increases linearly with the size of data, as the intrinsic structure of this methods requires to access all the data in each iteration. Due to the same reason, Dawid-Skene algorithm has difficulty when the data frame is supplied to the system indefinitely. Notice that in reality, it is highly possible that we can only collect worker labels in an indefinite manner. For example, different worker may have different labeling speed and we may want to add more items to some classes afterwards. Therefore, it is of great interests to derive an online version of Dawid-Skene algorithm, which can still achieve comparable accuracy.\nIn this paper, we provide an online version of Dawid-Skene algorithm, which requires only one data frame for each iteration. For the ease of presentation and analysis, we consider the simple model, i.e., at each iteration, one item and its corresponding labels are supplied to the online algorithm. This simple setup can be easily generalized to more general cases. We show that, under some non-restrictive conditions, the online Dawid-Skene scheme converges to a stationary point of the marginal log-likelihood of the observed labels. Notice that, since the marginal log-likelihood is non-convex, the original Dawid-Skene scheme only guarantees to converge to a stationary point as well.\nThe rest of the paper is organized as follows: in Section 2, we compare and contrast our work with related literature. In Section 3, we introduce the notations and then formulate the crowdsourcing problem rigorously. Next, in Section 4, the mathematical form of Dawid-Skene scheme is revisited. Then, the online algorithm is presented in Section 5. After that, theoretical analysis of online Dawid-Skene scheme is included in Section 6. Then, empirical performance is investigated in Section 7. Finally, the conclusion is in Section 8."}, {"heading": "2 Related work", "text": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that if the number of observations increases, their algorithm can infer the quality of contributions with error that converges to zero. Dalvi et al. [2013] relax the assumption that the graph between worker and item is either random or complete and propose another SVD-based algorithm which considers arbitrary worker-item graph. Also, under the one coin model setting and considering random regular worker-item graph, Karger et al. [2014] uses an iterative approach to infer the true binary labels and later generalize it to multi-class labeling tasks in Karger et al. [2013]. Focusing on learning a classifier, Chen et al. [2013]; Raykar et al. [2010]; Liu et al. [2012] proposes a Bayesian algorithm by imposing a prior distribution on the experts. By assuming that labels are generated by a probability distribution over workers, items, and labels Zhou et al. [2012, 2014] propose an algorithm by minimizing the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. This method can also output item difficulty and worker expertise as by-products. Zhou et al. [2014] observe that it is difficult to distinguish between two adjacent ordinal classes while distinguishing between two far-away classes is much easier and they propose an algorithm based on minimax conditional entropy. Initialization is very important for DawidSkene algorithm, as it aims to maximize a nonconcave function. Addressing this issue, Zhang et al. [2014] propose to initialize Dawid-Skene algorithm using spectral methods and show that the resulting algorithm has a high probability to converge to the global optimum.\nOur work is motivated by stochastic approximation methods and variants of online EM algorithms [Cappe\u0301 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Cappe\u0301 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step. Further, Cappe\u0301 and Moulines [2008] prove that, under some conditions, the online EM converges to a stationary points of the KL divergence between the marginal distribution of the observation and the model distribution, while we prove that our online Dawid-Skene scheme convergence to a stationary point of the the marginal log-likelihood functions. Notice our result directly connects to the objective function that we want to maximize. Sato and Ishii [2000] propose an online EM algorithm for Normalized Gaussian Network by introducing a discount factor to forget the previous incorrect estimated parameters. Later, Sato [2000] shows that if the probability\ndistributions for both observed and unobserved data belong to an exponential family, then their algorithm converges to the corresponding stationary point of the marginal log-likelihood function. But, this is not applicable to our case, as under the crowdsourcing setting, the joint probability distribution does not belong to the exponential family defined in Sato [2000]."}, {"heading": "3 Problem Setup", "text": ""}, {"heading": "3.1 Notation", "text": "In this paper, given an integer a, we use [a] to denote the integer set {1, 2, \u00b7 \u00b7 \u00b7 , a}. 3-mode tensors are denoted by upper case calligraphic letters (e.g. C,S \u2208 Rm\u00d7k\u00d7k); matrices are denoted by upper case boldface letters (e.g. Z); vectors are denoted by lower case boldface letters (e.g. z); and scalars are denoted by lower case letters (e.g. z). Given a 3-mode tensor C \u2208 Rm\u00d7k\u00d7k, we use cilg to denote the (i, l, g)-th entry of C. Given a matrix Z, we use zi to denote its i-th column, and zij to denote its (i, j)-th entry. Given a vector z, its i-th element is denoted by zi. Next, sets are denoted by blackboard bold characters (e.g. R,D,O,) and functions are denoted by Fraktur characters (e.g. F, H, V, T). In particular, we use 1 to denote the indicator function, E to denote the Expectation for some random variable and \u03a0 to denote a projection mapping."}, {"heading": "3.2 Estimate true labels form crowds", "text": "Throughout this paper, there are m workers, n items (objects) belonging to k classes (groups). We use yj to denote the true labels of item j \u2208 [n] and y to denote the true labels for all items. Denote by a scalar zij \u2208 R the label that worker i assigns to item j. When the assigned label is g \u2208 [k], we write zij = g. If item j is not labeled by worker i, we write zij = 0. Thus, we use the vector zj to denote all the worker labels corresponding to the jth item. Further more, we use the matrix Z to denote all the labels, whose j-th column is zj and (i, j)-th entry is zij . Our target is to estimate the true labels y from the worker labels Z."}, {"heading": "4 Dawid-Skene Scheme", "text": "In Dawid-Skene Scheme, assuming that the probability that worker i labels an item in class l as class g is independent of any particular chosen item, i.e., it is a constant over j \u2208 [n]. Let us denote the constant probability by cilg . Further, we restrict cilg to be strictly positive, i.e. cilg \u2208 (0, 1). Then, the tensor C, whose (i, l, g)-th element is cilg , is called the user confusion tensor. The joint likelihood of true labels y and observed labels Z , as a function of C, can be written as\nL(C;y,Z) =\nn \u220f\nj=1\nm \u220f\ni=1\nk \u220f\ng=1\n(ciyjg) 1(zij=g). (1)\nThen, the maximum likelihood estimates of the unknown true label y can be obtained by maximizing the marginal loglikelihood function of the observed worker labels\nl(C) := log\n\n\n\u2211\ny\u2208[k]n\nL(C;y,Z)\n\n . (2)\nThen, we can find the maximum likelihood estimate of the marginal log-likelihood (2) using the EM algorithm: E Step: Calculate the expected value of the log-likelihood function, with respect to the conditional distribution of y given Z under the current estimate of C\u0302:\nQ(C|C\u0302) : = E y|Z,C\u0302 [log (L(C;y,Z))] (3)\n=\nn \u2211\nj=1\n(\nk \u2211\nl=1\nP (yj = l|C\u0302,Z) log\n(\nk \u220f\ng=1\nm \u220f\ni=1\nc 1(zij=g) ilg\n))\n,\n(4)\nwhere for all j \u2208 [n], l \u2208 [k],\nP (yj = l|C\u0302,Z) =\nexp ( \u2211m\ni=1\n\u2211k g=1 1(zij = g) log(c\u0302ilg) )\n\u2211k l\u2032=1 exp ( \u2211m i=1 \u2211k g=1 1(zij = g) log(c\u0302il\u2032g) ) . (5)\nM Step: Find the estimate C\u0302 that maximizes the function Q(C|C\u0302):\nc\u0302ilg \u2190\nn \u2211\nj=1\nP (yj = l|C\u0302,Z)1(zij = g)\nk \u2211\ng\u2032=1\n(\nn \u2211\nj=1\nP (yj = l|C\u0302,Z)1(zij = g\u2032)\n) . (6)"}, {"heading": "5 Online Dawid-Skene Scheme", "text": "The basic idea of the proposed method is to replace the Maximization step by a stochastic approximation step, while keeping the Expectation step unchanged. More precisely, set\nsilg(n) = 1\nn\nn \u2211\nj=1\nP (yj = l|C\u0302,Z)1(zij = g).\nThen, the Maximization step of Dawid-Skene Scheme (6) can be rewritten as\nc\u0302ilg \u2190 silg(n)\nk \u2211\ng\u2032=1\nsilg\u2032 (n)\n.\nThen, we use S to denote the tensor whose (i, l, g)-th element is silg . With tensor S and the user confusion tensor C. The E step of Dawid-Skene scheme can be seen as calculating S based on C. Correspondingly, the M step can be seen as calculating C based on S.\nIn order to define the online algorithm rigorously, we need to make some assumptions on how the labels are supplied to the online algorithm. One obvious choice is that we select one of the n objects randomly and supply its corresponding labels to the online algorithm at each iteration. More precisely, we make the following assumption.\nAssumption 1. Give n items and their corresponding labels Z, at the beginning of each iteration, an item i \u2208 [n] is uniformly selected and its corresponding labels zi are supplied to the online algorithm.\nRemark: Assumption (1) is to simplify presentation and analysis. The online Dawid-Skene Scheme (7) and (8) can be easily generalized to more general and practical cases.\nLet C(0) be initialized when we do not observe any labels and C(j) denote the confusion tensor after the online algorithm have received the labels of j-th object, i.e. after seeing zj . Thus, the corresponding probability label for the j-th object based on C(j \u2212 1) is {P (yj = l|C(j \u2212 1), zj) | l \u2208 [k]}. In online Dawid-Skene Scheme, silg(j) at iteration j is calculated by the following stochastic approximation step,\nsilg(j) = silg(j \u2212 1)+\n\u03b7j (P (yj = l|C(j \u2212 1), zj)1(zij = g)\u2212 silg(j \u2212 1)) , (7)\nwith \u03b7j chosen to satisfy the following condition\n0 < \u03b7j < 1,\n\u221e \u2211\nj=1\n\u03b7j = \u221e,\n\u221e \u2211\nj=1\n\u03b72j < \u221e. (8)"}, {"heading": "6 Convergence Analysis", "text": "In this section, we show that under mild conditions, the online Dawid-Skene scheme with projection converges to a stationary point of the marginal log-likelihood (2)."}, {"heading": "6.1 Preliminary", "text": "In this subsection, we define some functions which capture the key update property of Dawid-Skene scheme and online Dawid-Skene scheme. Then, we analyse the online algorithm by investigating these functions. At iteration j, the M step can be seen as calculating C based on S. For online Dawid-Skene scheme, without loss of generality, we choose 0 < silg(0) < 1 for all i \u2208 [m], l \u2208 [k], g \u2208 [k], then it can be easily shown that 0 < silg(i) < 1 at any iteration i. So, for convenience, we define\nD = (0, 1)m\u00d7k\u00d7k.\nThen, given S \u2208 D, we definie the function G : D 7\u2192 D as\nG(S) = C, where cilg = silg\nk \u2211\ng\u2032=1\nsilg\u2032\n.\nAfter that, suppose we have n items in total and their corresponding worker labels Z, we define the function T : D 7\u2192 D to be\nT(S) = W \u2212S, (9)\nwhere the (i, l, g)-th entry of W is\nwilg = 1\nn\nn \u2211\nj=1\nP (yj = l|G(S), zj)1(zij = g)."}, {"heading": "6.2 Main Results", "text": "The function T defined in (9) reveals how Dawid-Skene Scheme updates the confusion matrix C. We make the following assumptions on T, under which we will show that the online algorithm can convergence to a stationary point of the marginal log-likelihood function.\nAssumption 2. Define O to be the set of zeros of function T, i.e. O = {S|T(S) = 0}. Assume O is finite and there exists non-negative continuously differentiable function V1, V2 with domain D and a compact set K \u2282 O such that\n1. for all S \u2208 D, \u3008\u2207V1(S),T(S)\u3009 \u2264 0.\n2. {S| \u3008\u2207V1(S),T(S)\u3009 = 0} = O.\n3. lim S\u2192\u2202D V2(S) = \u221e.\n4. for all S /\u2208 K, \u3008\u2207V2(S),T(S)\u3009 < 0.\nRemark:\n\u2022 Assumption (2) is based on the existence of two Lyapunov functions, i.e. V1 and V2. The existence of a Lyapunov is a standard condition to prove the asymptotic stability in stochastic approximation theory.\n\u2022 The existence of a Lyapunov function is usually theoretically non-restrictive, as is critical to the convergence of the trajectories of the corresponding vector field, see Delyon [2000, 1996]; Hahn and Baartz [1967] for more information.\nFor Dawid-Skene Scheme, we mainly focus on the open set D, as the joint distribution is not defined on the boundary of D. But, this provides difficulties for analyzing the the asymptotic properties, as C(j) may converge to the boundary. The naive way to avoid this difficulty is to assume that the stationary points lie inside a compact subset of D. A more elegant projection procedure is proposed in Chen et al. [1987]; Delyon [2000]. Thus, in the following analysis, we focus on the projected version of (7). Considering the projection procedure in Delyon [2000], we assume that there is a sequence of compact sets Kt, t = 0, 1, 2, \u00b7 \u00b7 \u00b7 , whose union is D. Then, we begin with t = 1, at each iteration j, S(j) is out of Kt, its value is reset arbitrarily in K0 and we increase t by one. Mathematically, we consider the following algorithm\ns\u0302ilg(j) = silg(j \u2212 1)+\n\u03b7j (P (yj = l|C(j \u2212 1), zj)1(zij = g)\u2212 silg(j \u2212 1)) ,\nif S\u0302(j) \u2208 Ktj\u22121 , then S(j) = S\u0302(j), tj = tj\u22121,\nif S\u0302(j) /\u2208 Ktj\u22121 , then S(j) = S \u2032 \u2208 K0, tj = tj\u22121 + 1,\nwhere S \u2032 is chosen arbitrarily from K0 and we use the following notation to denote the projected version of Dawid-Skene Scheme:\nsilg(j) pr = silg(j \u2212 1)\n+ \u03b7j (P (yj = l|C(j \u2212 1), zj)1(zij = g)\u2212 silg(j \u2212 1)) . (10)\nTheorem 1. Under Assumption 1 and 2, the online DawidSkene Scheme with projection (10) converges to a stationary point C\u2217 of the marginal log-likelihood (2), i.e.,\n\u2202l(C)\n\u2202C\n\u2223 \u2223 \u2223 \u2223\nC\u2217 = 0.\nProof. We divide the proof into the following three steps: Step1: in this step, we prove that all the points in O are stationary points of the marginal log-likelihood (2). To do this,\nwe first show that Dawid-Skene Scheme converges to one of the zeroes of the function T. Notice that each Expectation and Maximization step is equivalent to the following iterative steps\n1. Give C, calculate S, where the (i, l, g)-th entry of S is\nsilg = 1\nn\nn \u2211\nj=1\nP (yj = l|C,Z)1(zij = g). (11)\n2. Given S, calculate C by\nC = G(S). (12)\nLet S\u2217 be one of the points to which Dawid-Skene Scheme converges, i.e. S\u2217 will stay unchanged after updating using iterative steps (11) and (12). So, we have\ns\u2217ilg = 1\nn\nn \u2211\nj=1\nP (yj = l|G(S \u2217),Z)1(zij = g). (13)\nWriting (13) compactly, we have T(S\u2217) = 0. Thus, DawidSkene Scheme converges to one of the zeroes of the function T. Since Q(C|C\u0302) is continuous in both C and C\u0302, by Theorem 3.2 in McLachlan and Krishnan [2007], we know that DawidSkene Scheme converges to a stationary point of the marginal log-likelihood (2). Clearly, any point in O can be a convergent point of Dawid-Skene Scheme. So, we have\nO \u2286\n{\nC\u2217 | C\u2217 \u2208 D and \u2202l(C)\n\u2202C\n\u2223 \u2223 \u2223 \u2223\nC\u2217 = 0\n}\n.\nStep2: in this step, we show that the online Dawid-Skene Scheme can be seen as using stochastic approximation methods to find the zeroes of T . Notice that, at iteration j, for all i \u2208 [m], l \u2208 [k], g \u2208 [k], {silg(j)} are updated based on the value\nP (yj = l|C(j \u2212 1), zj)1(zij = g),\nwhich can be seen as a function of (C(j \u2212 1), zj). Thus, we define the function F(C, z) : D\u00d7 [k]m 7\u2192 [0, 1)m\u00d7k\u00d7k as\nF(C, z) = A,\nwhere the (i, l, g)-th entry of A is\nailg = P (y = l|C, z)1(zi = g)\n= exp\n(\n\u2211m i=1 \u2211k g=1 1(zi = g) log(cilg)\n)\n\u2211k l\u2032=1 exp ( \u2211m i=1 \u2211k g=1 1(zi = g) log(cil\u2032g) ) .\nNext, suppose z is sampled form {z1, z2, \u00b7 \u00b7 \u00b7 , zn} uniformly, given S \u2208 D, we have\nE [F(G(S), z)] = 1\nn\nn \u2211\nj=1\nF(G(S), zj) = T(S) + S.\nAlso notice that, using function F, the online step (10) can be written compactly as\nS(j) pr = S(j \u2212 1)\n+ \u03b7j (F(G(S(j \u2212 1)), zj)\u2212 S(j \u2212 1)) . (14)\nUnder assumption(1), at iteration j, zj is sampled form {z1, z2, \u00b7 \u00b7 \u00b7 , zn} uniformly. So, we have\nT(S(j \u2212 1)) = E [F(G(S(j \u2212 1)), zj)\u2212 S(j \u2212 1)] . (15)\nThe online iterative step (14) have the same form as Robbins-Monro algorithms Robbins and Monro [1951]; Kushner and Yin [1997]; Delyon [2000], which is used to find the zeros of function T. Step3: in this step, we show that the online Dawid-Skene Scheme converges to a stationary point of of the marginal loglikelihood (2). Given C \u2208 D, since the function F is bounded for all z \u2208 {z1, z2, \u00b7 \u00b7 \u00b7 , zn}. Thus, if z is selected uniformly from {z1, z2, \u00b7 \u00b7 \u00b7 , zn}, we have for any compact set K\u0303 \u2282 D\nsup C\u2208K\u0303\nE [ \u2016F(C, z)\u20162F ] < \u221e.\nUnder assumption (2) and (1), by Theorem 12 in Delyon [2000] (or equivalently stochastic approximation theories in Zhang and Liang [2008]; Kushner and Yin [1997]), we have\nS(j) \u2192 S\u2217 for some S\u2217 \u2208 O such that T(S\u2217) = 0.\nRemark: \u2022 Sato [2000] shows that if the distribution P (x, z|\u03b8) for\nboth observed data x and missing data z belongs to the following exponential family\nP (x, z|\u03b8) =\nexp [\u3008R(x, z), \u03b8\u3009+R0(x, z) \u2212M(\u03b8)] , (16)\nwhere \u03b8 denotes the parameters of interests, R(x, z) denote the set of sufficient statistics and M(\u03b8) is the normalization factor defined by\nexp [M(\u03b8)] = \u222b \u222b\nexp [\u3008R(x, z), \u03b8\u3009+R0(x, z)] dxdz.\nThen, the online EM algorithm converges to a stationary point of the corresponding marginal log-likelihood function. Notice this result does not apply to our case, as for Dawid-Skene Scheme, the probability distribution is not of the form (16). Comparing the proofs, Sato [2000] treats the online algorithm as a stochastic gradient method with the inverse of the Fisher information matrix being the coefficient matrix, while we prove our results by finding the zeros of the function T.\n\u2022 Alternatively, one can use the results provided in Cappe\u0301 and Moulines [2008] to show that the online Dawid-Skene Scheme converges to a stationary point of the KL divergence between the induced partial distribution P (y|C) and true distribution P\u0302 for y, while our results directly connects to the marginal log-likelihood function that Dawid-Skene Scheme tries to maximize.\n\u2022 Notice that due to nonconvexity, the best Dawid-Skene Scheme can hope to achieve is to converge to a stationary point of the marginal log-likelihood (2). Under Assumptions 1 and 2, our result means that the online Dawid-Skene Scheme is as good as the original one."}, {"heading": "7 Experiment", "text": "In this section, we investigate experimentally the performance and efficiency of the proposed online algorithm (7). The experiments are based on three datasets: one binary task and two multi-class tasks. The binary task is to recognize textual entailment Snow et al. [2008] (RTE). Multi-class tasks include labeling the images of 4 breads dogs from ImageNet Zhou et al. [2012] (DOG) and judging the relevance of query-URL pairs with a 5-level rating scale Zhou et al. [2012] (WEB). The characteristics of the data sets are summarized in table (2)."}, {"heading": "7.1 Step Size", "text": "Decreasing \u03b7j slowly with respect to j means the online algorithm learns at a fast rate and also means that it forgets the past inaccurate estimates at high speed. On the contrary,\nrapid decreasing of \u03b7j means the online scheme forgets the past slowly and learns from new information at a low rate. Thus, the speed of decreasing the learning rate \u03b7j is a tradeoff between learning and forgetting. Next, we investigate how to choose the learning rate in order to get the most accurate results.\nIn our experiment, two online algorithms corresponding to two set of step sizes (learning rate) satisfying Condition (8) are considered. Specifically, one set of step sizes are chosen to be decreasing linearly with respect to the number of iterations and the corresponding online algorithm is referred as online1. The other set of step sizes are chosen to be decreasing slower than linear with respect to the number of iterations and we call the corresponding online scheme online2. Selecting two tunable variable a, b \u2208 R, the algorithm online1 and online2 are summarized in the following:\n\u2022 Online1: \u03b7j = 1aj+b .\n\u2022 Online2: \u03b7j = bja with 0.5 < a < 1.\nNext, we investigate how the stability and the accuracy can be influenced by different tunable variable a, b \u2208 R on the data set WEB. The reason to choose the WEB data set is because it is relatively more complicated and has more samples, which makes the online algorithm more sensitive with respect to the tunable variables.\nThe experiment results are illustrated in Figure 1. The best error rate that online1 can get is 14.21 when a = 2, b = 1.5 and online2\u2019s best record is 14.32 when a = 0.99, b = 0.4. For online1, the lowest error rate is achieved at a = 2, which means that both learning too fast or too slow will hurt the performance. For online2, when the step size decreases too slow, i.e. a = 0.75, online2 have the worst error rate. Morever, online2 can get smaller error rate when a is closer to 1, which suggests that decreasing the learning rate linearly might be a good choice."}, {"heading": "7.2 Performace Comparison", "text": "Comparison with Dawid-Skene Scheme: we compare the convergence rate with respect to the number of epochs between online Dawid-Skene Scheme and the original version. Both algorithms starts from the same initialization. The comparison results are shown in Figure 2. The convergence rates between online online Dawid-Skene Scheme and the original one are comparable: both methods will converge after several epochs. For performance, the online Dawid-Skene Scheme works surprisingly well. Both online1 and online2 outperforms the Dawid-Skene Scheme on data sets RTE and WEB. On data set DOG, online2 is slightly worse than Dawid-Skene Scheme and online1 achieves the same error rate.\nComparison with other methods: we compare online1 and online2 with other methods which are also based on the generative model of Dawid-Skene. Specifically we compare online1, online2 and Dawid-Skene initialized by majority voting, majority voting method, Dawid-Skene scheme initialized by spectral method proposed by Zhang et al. [2014] (referred as Opt-Dawid-Skene), the multi-class labeling algorithm proposed by Karger et al. [2013] (referred as KOS). The results are summarized in table (1) and online DawidSkene scheme achieves state of the art results."}, {"heading": "8 Conclusion", "text": "We proposed an online version of Dawid-Skene scheme for crowdsourcing. Empirically, the proposed method outperforms the original Dawid-Skene scheme and various other methods, and achieves the state of the art results. From the theoretical point of view, we showed that the online DawidSkene scheme converges to a stationary point of the marginal log-likelihood of the observed data under the existence of two Lyapunov functions."}], "references": [{"title": "Online em algorithm for latent data models", "author": ["Olivier Capp\u00e9", "Eric Moulines"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Capp\u00e9 and Moulines.,? \\Q2008\\E", "shortCiteRegEx": "Capp\u00e9 and Moulines.", "year": 2008}, {"title": "Convergence and robustness of the robbins-monro algorithm truncated at randomly varying bounds", "author": ["Han-Fu Chen", "Lei Guo", "Ai-Jun Gao"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Chen et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1987}, {"title": "Optimistic knowledge gradient policy for optimal budget allocation in crowdsourcing", "author": ["Xi Chen", "Qihang Lin", "Dengyong Zhou"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Aggregating crowdsourced binary ratings", "author": ["Nilesh Dalvi", "Anirban Dasgupta", "Ravi Kumar", "Vibhor Rastogi"], "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "citeRegEx": "Dalvi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2013}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["Alexander Philip Dawid", "Allan M Skene"], "venue": "Applied statistics,", "citeRegEx": "Dawid and Skene.,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene.", "year": 1979}, {"title": "General results on the convergence of stochastic algorithms", "author": ["Bernard Delyon"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Delyon.,? \\Q1996\\E", "shortCiteRegEx": "Delyon.", "year": 1996}, {"title": "Stochastic approximation with decreasing gain: Convergence and asymptotic theory", "author": ["Bernard Delyon"], "venue": "Unpublished lecture notes, Universite\u0301 de Rennes,", "citeRegEx": "Delyon.,? \\Q2000\\E", "shortCiteRegEx": "Delyon.", "year": 2000}, {"title": "Who moderates the moderators?: crowdsourcing abuse detection in user-generated content", "author": ["Arpita Ghosh", "Satyen Kale", "Preston McAfee"], "venue": "In Proceedings of the 12th ACM conference on Electronic commerce,", "citeRegEx": "Ghosh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2011}, {"title": "Stability of motion, volume 422", "author": ["Wolfgang Hahn", "Arne P Baartz"], "venue": null, "citeRegEx": "Hahn and Baartz.,? \\Q1967\\E", "shortCiteRegEx": "Hahn and Baartz.", "year": 1967}, {"title": "Efficient crowdsourcing for multi-class labeling", "author": ["David R Karger", "Sewoong Oh", "Devavrat Shah"], "venue": "In ACM SIGMETRICS Performance Evaluation Review,", "citeRegEx": "Karger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2013}, {"title": "Budgetoptimal task allocation for reliable crowdsourcing systems", "author": ["David R Karger", "Sewoong Oh", "Devavrat Shah"], "venue": "Operations Research,", "citeRegEx": "Karger et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2014}, {"title": "Stochastic approximation algorithms and applications", "author": ["Harold J Kushner", "G George Yin"], "venue": null, "citeRegEx": "Kushner and Yin.,? \\Q1997\\E", "shortCiteRegEx": "Kushner and Yin.", "year": 1997}, {"title": "Variational inference for crowdsourcing", "author": ["Qiang Liu", "Jian Peng", "Alex T Ihler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "The EM algorithm and extensions, volume 382", "author": ["Geoffrey McLachlan", "Thriyambakam Krishnan"], "venue": null, "citeRegEx": "McLachlan and Krishnan.,? \\Q2007\\E", "shortCiteRegEx": "McLachlan and Krishnan.", "year": 2007}, {"title": "Learning from crowds", "author": ["Vikas C Raykar", "Shipeng Yu", "Linda H Zhao", "Gerardo Hermosillo Valadez", "Charles Florin", "Luca Bogoni", "Linda Moy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raykar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raykar et al\\.", "year": 2010}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "On-line em algorithm for the normalized gaussian network", "author": ["Masa-Aki Sato", "Shin Ishii"], "venue": "Neural computation,", "citeRegEx": "Sato and Ishii.,? \\Q2000\\E", "shortCiteRegEx": "Sato and Ishii.", "year": 2000}, {"title": "Convergence of stochastic approximation algorithms under irregular conditions", "author": ["Jian Zhang", "Faming Liang"], "venue": "Statistica Neerlandica,", "citeRegEx": "Zhang and Liang.,? \\Q2008\\E", "shortCiteRegEx": "Zhang and Liang.", "year": 2008}, {"title": "Spectral methods meet em: A provably optimal algorithm for crowdsourcing", "author": ["Yuchen Zhang", "Xi Chen", "Dengyong Zhou", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Learning from the wisdom of crowds by minimax entropy", "author": ["Dengyong Zhou", "Sumit Basu", "Yi Mao", "John C Platt"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Aggregating ordinal labels from crowds by minimax conditional entropy", "author": ["Dengyong Zhou", "Qiang Liu", "John Platt", "Christopher Meek"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "But, can we do better in crowdsourcing to estimate the true labels from the crowdsourced noisy labels? Observing that different workers may have different talents, Dawid and Skene [1979] develop a maximum likelihood approach based on the idea that each worker has a confusion matrix.", "startOffset": 164, "endOffset": 187}, {"referenceID": 4, "context": "But, can we do better in crowdsourcing to estimate the true labels from the crowdsourced noisy labels? Observing that different workers may have different talents, Dawid and Skene [1979] develop a maximum likelihood approach based on the idea that each worker has a confusion matrix. More precisely, suppose that the items can be divided into k classes, and assume that each worker is associated with a k \u00d7 k confusion matrix, where the (l, c)-th entry represents the probability that a randomly chosen item in class l is labeled as class g by the worker. The true labels and the worker confusion matrices are jointly estimated by maximizing the marginal log-likelihood of the observed labels, where the unobserved true labels are treated as hidden variables. However, the value of the likelihood is extremely difficult to calculate, as the number of terms, whose sum is the likelihood of the observed labels, is exponentially growing with respect to the number of items. Instead, the worker confusion matrices are estimated iteratively by a Expectation-Maximization (EM) procedure McLachlan and Krishnan [2007]. Unfortunately, the memory consumption of Dawid-Skene scheme increases linearly with the size of data, as the intrinsic structure of this methods requires to access all the data in each iteration.", "startOffset": 164, "endOffset": 1112}, {"referenceID": 12, "context": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 96, "endOffset": 201}, {"referenceID": 7, "context": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 96, "endOffset": 201}, {"referenceID": 3, "context": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 96, "endOffset": 201}, {"referenceID": 18, "context": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 96, "endOffset": 201}, {"referenceID": 4, "context": ", 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 66, "endOffset": 89}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 16, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 6, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 17, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 11, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings.", "startOffset": 14, "endOffset": 203}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that if the number of observations increases, their algorithm can infer the quality of contributions with error that converges to zero.", "startOffset": 14, "endOffset": 394}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that if the number of observations increases, their algorithm can infer the quality of contributions with error that converges to zero. Dalvi et al. [2013] relax the assumption that the graph between worker and item is either random or complete and propose another SVD-based algorithm which considers arbitrary worker-item graph.", "startOffset": 14, "endOffset": 555}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that if the number of observations increases, their algorithm can infer the quality of contributions with error that converges to zero. Dalvi et al. [2013] relax the assumption that the graph between worker and item is either random or complete and propose another SVD-based algorithm which considers arbitrary worker-item graph. Also, under the one coin model setting and considering random regular worker-item graph, Karger et al. [2014] uses an iterative approach to infer the true binary labels and later generalize it to multi-class labeling tasks in Karger et al.", "startOffset": 14, "endOffset": 839}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that if the number of observations increases, their algorithm can infer the quality of contributions with error that converges to zero. Dalvi et al. [2013] relax the assumption that the graph between worker and item is either random or complete and propose another SVD-based algorithm which considers arbitrary worker-item graph. Also, under the one coin model setting and considering random regular worker-item graph, Karger et al. [2014] uses an iterative approach to infer the true binary labels and later generalize it to multi-class labeling tasks in Karger et al. [2013]. Focusing on learning a classifier, Chen et al.", "startOffset": 14, "endOffset": 976}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al. [2010]; Liu et al.", "startOffset": 35, "endOffset": 76}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al. [2010]; Liu et al. [2012] proposes a Bayesian algorithm by imposing a prior distribution on the experts.", "startOffset": 35, "endOffset": 95}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al. [2010]; Liu et al. [2012] proposes a Bayesian algorithm by imposing a prior distribution on the experts. By assuming that labels are generated by a probability distribution over workers, items, and labels Zhou et al. [2012, 2014] propose an algorithm by minimizing the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. This method can also output item difficulty and worker expertise as by-products. Zhou et al. [2014] observe that it is difficult to distinguish between two adjacent ordinal classes while distinguishing between two far-away classes is much easier and they propose an algorithm based on minimax conditional entropy.", "startOffset": 35, "endOffset": 531}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al. [2010]; Liu et al. [2012] proposes a Bayesian algorithm by imposing a prior distribution on the experts. By assuming that labels are generated by a probability distribution over workers, items, and labels Zhou et al. [2012, 2014] propose an algorithm by minimizing the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. This method can also output item difficulty and worker expertise as by-products. Zhou et al. [2014] observe that it is difficult to distinguish between two adjacent ordinal classes while distinguishing between two far-away classes is much easier and they propose an algorithm based on minimax conditional entropy. Initialization is very important for DawidSkene algorithm, as it aims to maximize a nonconcave function. Addressing this issue, Zhang et al. [2014] propose to initialize Dawid-Skene algorithm using spectral methods and show that the resulting algorithm has a high probability to converge to the global optimum.", "startOffset": 35, "endOffset": 893}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step.", "startOffset": 96, "endOffset": 246}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step. Further, Capp\u00e9 and Moulines [2008] prove that, under some conditions, the online EM converges to a stationary points of the KL divergence between the marginal distribution of the observation and the model distribution, while we prove that our online Dawid-Skene scheme convergence to a stationary point of the the marginal log-likelihood functions.", "startOffset": 96, "endOffset": 513}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step. Further, Capp\u00e9 and Moulines [2008] prove that, under some conditions, the online EM converges to a stationary points of the KL divergence between the marginal distribution of the observation and the model distribution, while we prove that our online Dawid-Skene scheme convergence to a stationary point of the the marginal log-likelihood functions. Notice our result directly connects to the objective function that we want to maximize. Sato and Ishii [2000] propose an online EM algorithm for Normalized Gaussian Network by introducing a discount factor to forget the previous incorrect estimated parameters.", "startOffset": 96, "endOffset": 937}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step. Further, Capp\u00e9 and Moulines [2008] prove that, under some conditions, the online EM converges to a stationary points of the KL divergence between the marginal distribution of the observation and the model distribution, while we prove that our online Dawid-Skene scheme convergence to a stationary point of the the marginal log-likelihood functions. Notice our result directly connects to the objective function that we want to maximize. Sato and Ishii [2000] propose an online EM algorithm for Normalized Gaussian Network by introducing a discount factor to forget the previous incorrect estimated parameters. Later, Sato [2000] shows that if the probability distributions for both observed and unobserved data belong to an exponential family, then their algorithm converges to the corresponding stationary point of the marginal log-likelihood function.", "startOffset": 96, "endOffset": 1107}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step. Further, Capp\u00e9 and Moulines [2008] prove that, under some conditions, the online EM converges to a stationary points of the KL divergence between the marginal distribution of the observation and the model distribution, while we prove that our online Dawid-Skene scheme convergence to a stationary point of the the marginal log-likelihood functions. Notice our result directly connects to the objective function that we want to maximize. Sato and Ishii [2000] propose an online EM algorithm for Normalized Gaussian Network by introducing a discount factor to forget the previous incorrect estimated parameters. Later, Sato [2000] shows that if the probability distributions for both observed and unobserved data belong to an exponential family, then their algorithm converges to the corresponding stationary point of the marginal log-likelihood function. But, this is not applicable to our case, as under the crowdsourcing setting, the joint probability distribution does not belong to the exponential family defined in Sato [2000].", "startOffset": 96, "endOffset": 1509}, {"referenceID": 3, "context": "\u2022 The existence of a Lyapunov function is usually theoretically non-restrictive, as is critical to the convergence of the trajectories of the corresponding vector field, see Delyon [2000, 1996]; Hahn and Baartz [1967] for more information.", "startOffset": 174, "endOffset": 218}, {"referenceID": 1, "context": "A more elegant projection procedure is proposed in Chen et al. [1987]; Delyon [2000].", "startOffset": 51, "endOffset": 70}, {"referenceID": 1, "context": "A more elegant projection procedure is proposed in Chen et al. [1987]; Delyon [2000]. Thus, in the following analysis, we focus on the projected version of (7).", "startOffset": 51, "endOffset": 85}, {"referenceID": 1, "context": "A more elegant projection procedure is proposed in Chen et al. [1987]; Delyon [2000]. Thus, in the following analysis, we focus on the projected version of (7). Considering the projection procedure in Delyon [2000], we assume that there is a sequence of compact sets Kt, t = 0, 1, 2, \u00b7 \u00b7 \u00b7 , whose union is D.", "startOffset": 51, "endOffset": 215}, {"referenceID": 13, "context": "2 in McLachlan and Krishnan [2007], we know that DawidSkene Scheme converges to a stationary point of the marginal log-likelihood (2).", "startOffset": 5, "endOffset": 35}, {"referenceID": 12, "context": "(15) The online iterative step (14) have the same form as Robbins-Monro algorithms Robbins and Monro [1951]; Kushner and Yin [1997]; Delyon [2000], which is used to find the zeros of function T.", "startOffset": 83, "endOffset": 108}, {"referenceID": 9, "context": "(15) The online iterative step (14) have the same form as Robbins-Monro algorithms Robbins and Monro [1951]; Kushner and Yin [1997]; Delyon [2000], which is used to find the zeros of function T.", "startOffset": 109, "endOffset": 132}, {"referenceID": 5, "context": "(15) The online iterative step (14) have the same form as Robbins-Monro algorithms Robbins and Monro [1951]; Kushner and Yin [1997]; Delyon [2000], which is used to find the zeros of function T.", "startOffset": 133, "endOffset": 147}, {"referenceID": 5, "context": "Under assumption (2) and (1), by Theorem 12 in Delyon [2000] (or equivalently stochastic approximation theories in Zhang and Liang [2008]; Kushner and Yin [1997]), we have S(j) \u2192 S for some S \u2208 O such that T(S) = 0.", "startOffset": 47, "endOffset": 61}, {"referenceID": 5, "context": "Under assumption (2) and (1), by Theorem 12 in Delyon [2000] (or equivalently stochastic approximation theories in Zhang and Liang [2008]; Kushner and Yin [1997]), we have S(j) \u2192 S for some S \u2208 O such that T(S) = 0.", "startOffset": 47, "endOffset": 138}, {"referenceID": 5, "context": "Under assumption (2) and (1), by Theorem 12 in Delyon [2000] (or equivalently stochastic approximation theories in Zhang and Liang [2008]; Kushner and Yin [1997]), we have S(j) \u2192 S for some S \u2208 O such that T(S) = 0.", "startOffset": 47, "endOffset": 162}, {"referenceID": 0, "context": "\u2022 Alternatively, one can use the results provided in Capp\u00e9 and Moulines [2008] to show that the online Dawid-Skene Scheme converges to a stationary point of the KL divergence between the induced partial distribution P (y|C) and true distribution P\u0302 for y, while our results directly connects to the marginal log-likelihood function that Dawid-Skene Scheme tries to maximize.", "startOffset": 53, "endOffset": 79}, {"referenceID": 19, "context": "Multi-class tasks include labeling the images of 4 breads dogs from ImageNet Zhou et al. [2012] (DOG) and judging the relevance of query-URL pairs with a 5-level rating scale Zhou et al.", "startOffset": 77, "endOffset": 96}, {"referenceID": 19, "context": "Multi-class tasks include labeling the images of 4 breads dogs from ImageNet Zhou et al. [2012] (DOG) and judging the relevance of query-URL pairs with a 5-level rating scale Zhou et al. [2012] (WEB).", "startOffset": 77, "endOffset": 194}, {"referenceID": 16, "context": "Specifically we compare online1, online2 and Dawid-Skene initialized by majority voting, majority voting method, Dawid-Skene scheme initialized by spectral method proposed by Zhang et al. [2014] (referred as Opt-Dawid-Skene), the multi-class labeling algorithm proposed by Karger et al.", "startOffset": 175, "endOffset": 195}, {"referenceID": 9, "context": "[2014] (referred as Opt-Dawid-Skene), the multi-class labeling algorithm proposed by Karger et al. [2013] (referred as KOS).", "startOffset": 85, "endOffset": 106}], "year": 2015, "abstractText": "With the success of modern internet based platform, such as Amazon Mechanical Turk, it is now normal to collect a large number of hand labeled samples from non-experts. The DawidSkene algorithm, which is based on ExpectationMaximization update, has been widely used for inferring the true labels from noisy crowdsourced labels. However, Dawid-Skene scheme requires all the data to perform each EM iteration, and can be infeasible for streaming data or large scale data. In this paper, we provide an online version of DawidSkene algorithm that only requires one data frame for each iteration. Further, we prove that under mild conditions, the online Dawid-Skene scheme with projection converges to a stationary point of the marginal log-likelihood of the observed data. Our experiments demonstrate that the online DawidSkene scheme achieves state of the art performance comparing with other methods based on the DawidSkene scheme.", "creator": "LaTeX with hyperref package"}}}