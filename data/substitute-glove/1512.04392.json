{"id": "1512.04392", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Automatic Incident Classification for Big Traffic Data by Adaptive Boosting SVM", "abstract": "Modern north experience striking traffic slope working congestions regularly across it and time. Monitoring traffic possible ironically being needs contest for the Traffic Control still Surveillance Systems (TCSS ). In advanced TCSS, without but aware set requirements detect even classify furthermore visibility clashes among recently complication mainly lax, inhibit bike sometimes, abrupt or violating fire off stretches, etc. Although most TCSS various equipped way basic incident determine constructs, what number suggested fueled though be indeed types so being mobile tool even further classification. In philology, have. a lack according biological for Automated Incident Classification (AIC ). Therefore, just poem AIC procedure is to of it cut for safeties such matters. In the bill method, disrupting simultaneously are fulfilled nutrients some civilians episodes several converted actually spatial - jurisdiction (ST) switches. Based move the important include the ST constantly, put set there credible discrete check can projected coming utilized both extend makes trucks xml still cover a variety of causing situations. Next, another Mean - Shift waveform is widely to punish given effect for noise and extract significant style once and ST signals. The injected features are down indicated with kinds instance of routes usage: one normal use (inliers) both multiple clinically combining (outliers ). For called definitions, example utilize reduction classifier another assistants if analyzing outliers part speed computers automatically. Further, while Support Vector Machine (SVM) management algorithm most re have driving the model including addresses given categories created lithologies. In short, this builds careful another which an Adaptive Boosting Support Vector Machines (AB - SVM) requires. Experimental despite fox instance early changes AB - SVM used demonstrable little be as put. turn 92% forms validation when below.", "histories": [["v1", "Mon, 14 Dec 2015 16:23:59 GMT  (699kb)", "http://arxiv.org/abs/1512.04392v1", null], ["v2", "Mon, 28 Dec 2015 08:28:16 GMT  (504kb)", "http://arxiv.org/abs/1512.04392v2", "27 pages, 8 figures"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["li-li wang", "henry y t ngan", "nelson h c yung"], "accepted": false, "id": "1512.04392"}, "pdf": {"name": "1512.04392.pdf", "metadata": {"source": "CRF", "title": "Automatic Incident Classification for Big Traffic Data by Adaptive Boosting SVM", "authors": ["Li-Li Wang", "Henry Y.T. Ngan", "Nelson H.C. Yung"], "emails": ["llwang_hk@126.com,", "ytngan@hkbu.edu.hk,", "ypl.nyung@gmail.com"], "sections": [{"heading": null, "text": "Modern cities experience heavy traffic flows and congestions regularly across space and time. Monitoring traffic situations becomes an important challenge for the Traffic Control and Surveillance Systems (TCSS). In advanced TCSS, it is helpful to automatically detect and classify different traffic incidents such as severity of congestion, abnormal driving pattern, abrupt or illegal stop on road, etc. Although most TCSS are equipped with basic incident detection algorithms, they are however crude to be really useful as an automated tool for further classification. In literature, there is a lack of research for Automated Incident Classification (AIC). Therefore, a novel AIC method is proposed in this paper to tackle such challenges. In the proposed method, traffic signals are firstly extracted from captured videos and converted as spatial-temporal (ST) signals. Based on the characteristics of the ST signals, a set of realistic simulation data are generated to construct an extended big traffic database to cover a variety of traffic situations. Next, a Mean-Shift filter is introduced to suppress the effect of noise and extract significant features from the ST signals. The extracted features are\nthen associated with various types of traffic data: one normal type (inliers) and multiple abnormal types (outliers). For the classification, an adaptive boosting classifier is trained to detect outliers in traffic data automatically. Further, a Support Vector Machine (SVM) based method is adopted to train the model for identifying the categories of outliers. In short, this hybrid approach is called an Adaptive Boosting Support Vector Machines (AB-SVM) method. Experimental results show that the proposed AB-SVM method achieves a satisfied result with more than 92% classification accuracy on average.\n1. Introduction\nIn modern cities, traffic conditions are changing every moment and a single anomaly will affect the daily operations of transport and logistic corporations alike. As traffic data is collected from surveillance sensors such as digital video cameras and loop detectors on road networks, the data size is massive and easily contaminated with noise and errors. Most TCSS is controlled by human operators and an automated monitoring and response system is increasingly in need. As traffic situation changes rapidly, incident detection and classification becomes a necessity in TCSS. An ideal TCSS should be capable to carry out an AIC to differentiate various traffic incidents as well as hardware errors and transmission noise. In a simple sense, all normal traffic situations and error-free data can be considered as inliers, while abnormal traffic incidents such as congestions and vehicle stoppages as well as data errors can be regarded as outliers. Therefore, the research problem can simply focus on determining if any outlier exists in traffic data, and classifying them automatically.\nIn recent years, studies have been published for outlier detection (OD) [1-5]. In general, OD is to detect any datum which is deviated beyond a certain range from the majority of data. In [6, 7], the purpose of an OD is to identify data points appearing inconsistent with the majority of the data (inliers). OD is important as the outliers indicate potential abnormalities\nin many areas, for example, [1] suggested aircraft engine rotation defect detection, heart-rate monitors and fabric defect detection, or [3] mentioned traffic abnormality detection. There are three fundamental approaches for OD including unsupervised, semi-supervised and supervised ones. The common unsupervised methods include clustering-based method [8], distance-based method [9], and density-based method [10]. The clustering-based approach [8] defines an observation as an outlier if it deviates from the overall clustering pattern. The distance-based approach [9] assumes an observation is an outlier if the distances of a certain percentage of samples from a datum are larger than a given threshold. In the density-based approach [10], an observation is detected as an outlier if its local density is low. However, as mentioned, outliers may arise from different reasons in traffic data, such as traffic incidents, congestions due to peak hours, small volumes of vehicles, or data-capturing hardware failures. So far, researchers are interested to discover the unknown but meaningful categories of outliers, although most of these methods are not able to distinguish the types of anomalies. In fact, each type of outliers has its own characteristics. If we could identify the category of the current traffic state, this piece of information could be useful for traffic control. To identify the category of each outlier, one can resort to supervised learning approach [11-15]. This approach requires a pre-labeling of data as normal or abnormal which could be further classified into different categories. Based on these labeled training data, a supervised learning method is then adopted to a certain classifier to identify the category of an outlier after learning. In the literature, SVM [14] and adaptive boosting techniques [15] are the most popular methods and have achieved better classification performance than others. SVM as a supervised learning model which is widely employed for the aim of classification in machine learning. In [16], SVM-based method was successfully applied to recognize daily activity pattern for the forecasting purpose of travel demand. Adaptive boosting technique [15] as an ensemble method is capable of training a strong classifier by combining a series of\nmoderately accurate component classifiers. This method has been proven [15] to boost classification accuracy.\nIn order to identify certain categories of traffic incidents based on these learning methods, a massive traffic database for creating one or more classifiers is required. In order to achieve that, traffic data is collected via multiple sensors and is considered as big data by nature. In this research, traffic data were first collected from surveillance video cameras over 31 days. These traffic data were then converted into ST signals. These ST signals generally present high ST similarity within signals or among signals in the same period of each day. For evaluation purpose, an extension of the database with a simulated process of the traffic data is performed. Details of the above will be given in Section 2.2.\nNote that for abnormalities, they show different characteristics, and their quantity is generally assumed to be less than that of the normal data. The collected data sets are thereby imbalanced in terms of the number of samples available. If the imbalanced data set is used for learning, the performance of the learning algorithm(s) would degrade significantly [17], as most methods tend to build the classifiers from the majority category of data. As a result, the predictive accuracy is usually higher. However, the identification rate of the minority class is quite low. In extreme cases, all testing samples may be mis-classified as the majority category. This would be meaningless in an OD application. Researchers have come up with remedies for this imbalanced data problem. The commonly used methods include performing data redistribution [18] or classifier modification [19]. Performing data re-distribution means resampling the majority class or generating simulated minority class to achieve a more balance weighs for different data categories. Modifying the design of a classifier to adapt data characteristics is another choice to deal with the imbalanced data problem.\nTherefore, in this paper, we target to learn the daily traffic patterns at a four-arm junction by combining both AdaBoost and SVM (AB-SVM) methods. The goal of the proposed AB-\nSVM method is to not only identify any outliers that show inconsistency with the majority of traffic data, but also distinguish their categories for investigating detailed and useful information from it. The keys to AIC lie in determining which traffic flow signal representation is useful and meaningful in order to identify an incident type as well as in dealing with imbalance data as a whole. In brief, an AdaBoost method is firstly used to classify the imbalanced data as inliers or outliers. Afterward, a SVM model is trained up to classify the categories of outliers by using only abnormal ST signals. The advantages of this proposed strategy are that (i) it can reduce the training complexity compared with multi-class AdaBoost, (ii) data imbalance problem can be effectively abated by the hybrid AB-SVM techniques, and (iii) the categories of outliers are identified.\nThe rest of the paper is organized as follows. Related work including the generation\nmethod of big simulated traffic data, feature representation and review of AIC methods are given in Section 2. The proposed AB-SVM method to detect outliers and identify their categories is presented in Section 3. Experimental results are given and discussed in Section 4. Section 5 concludes the paper.\n2. Related work\nThis section discusses the collection of ST signals, the generation method of big simulated traffic data, signal representation method, and the existing methods of AIC on traffic database. 2.1 Extraction of ST signals\nTo identify traffic behaviors for the need of TCSS, massive traffic data collections for feeding machine learning classifiers are required. Fig. 1 shows the flowchart of traffic data collections. Details are introduced in this section.\nFig.1 Generation of big traffic database\nThe traffic video database employed in this paper was collected from a four-armed junction in Hong Kong as shown in Fig.2. The video database was recorded for 31 days from December 28, 2010 to January 27, 2011 with two sessions per day: 07:00-10:00 AM and 17:00-20:00 PM. According to the motion state of the traffic flow controlled by an array of traffic lights, the traffic flows of this 4-arm junction is characterized by four motion patterns (MPs) as shown in Fig.3. During the period of each MP, a traffic flow volume in its respective traffic direction is recorded. For example, when the current MP as shown in Fig. 2 (a) happens, four traffic flow volumes {wi, i = 1, 2, 3, 4} are recorded in four corresponding directions.\n(a)\n(b)\nFig.2 (a) Real scene of the 4-arm junction and (b) ideal map of the junction\nFig.3 Four MPs for vehicle counting: (a) MP1, (b) MP2, (c) MP3 and (4) MP4\nIn our research, we extract 19 ST signals from 19 traffic directions for each session to characterize the traffic conditions of this 4-arm junction for AIC. More specifically, there are 4 entry, 4 exit and 11 entry direction distribution (EDD) signals. To have consistent representation, we truncate 80 traffic cycles for each session with 3 hours. In other words, each ST signal is represented by this 3-hour session length (i.e. 80 cycles) as shown in Fig. 4(a). Due to an inconsistency of traffic flow behavior in weekends of the 31-day data, only data on weekdays (23 days in total) are used in our research. For the 4-arm junction, traffic signals are expressed as {zi, i=1,\u2026, 19} for 19 traffic directions. The signal in each cycle is a summation of traffic flow volumes (i.e. number of vehicle passing through) along its direction from all four traffic MPs. The 23 signals collected from each direction in 31 days are then individually analyzed to learn about the traffic behavior. Mathematically, the relationships between the signals and the MPs | = 1,\u2026 ,4 are denoted as\nAccording to our investigation on captured videos, the extracted traffic data may be grouped into three obvious categories: normal traffic data (inlier), abrupt low traffic volume state due to congestion or hardware failure (outlier), repeated jams (outlier). Fig. 4 depicts some examples of the three categories. Each category has its own characteristic as follows:\nTable 2 Categories of traffic signals\nCategory Inlier/Outlier Definition Characteristic of ST signals Figure\n1\nInlier\na) smooth traffic flow in the investigated direction b) vehicles pass through this intersection\nwithout any delay\nSteady wave pattern Fig. 4(a)\n2\nOutlier:\nslight jam\na) traffic flow is broken abruptly in the investigated direction b) vehicles pass through the intersection slowly due to slight traffic jams Abrupt low traffic volumes over 3 cycles Fig. 4(b)\nOutlier:\nhardware\nfailure\nsurveillance equipment found errors Hardware error with low\ntraffic volumes near to zero Fig. 4(c)\n3 Outlier:\nserious jam\na) low traffic flow in the investigated direction b) vehicles pass through the intersection slowly due to serious traffic jams Repeated low traffic volumes, may have an incident also\nFig. 4(d)\n(a) (b)\n(c) (d)\nFig.4 Categories of traffic data: (a) normal traffic ST signal, (b) ST signal with jams leading to low volumes, (c) ST signal with hardware failure, and (d) ST signal with repeated jams\n2.2 Generation of big simulated traffic data As only 23 ST signals are available for each traffic direction in the original captured data, it is not enough for learning based methods. Besides, this quantity of data does not represent every possible variation of each abnormal category. As a result, it limits the generation ability of most learning based methods. Still, it is very time-consuming to collect further massive amount of traffic data to rectify the problem. To strike a balance, we build an additive model of embedding Gaussian noise under a constraint of a high signal-noise-ratio (SNR) to generate a large amount of simulated data for the purpose of training and testing. Fig. 5 shows an example of the simulated ST signals and their distribution in 2D PCA space with different values of SNR. From the figure, we can see that the fluctuation of ST signals is quite large with a small value of SNR. In 2D PCA space, they are dispersed randomly due to large noises introduced. By contrast, the generated ST signals have small fluctuation and regular distribution in 2D PCA space when a higher value of SNR is used.\n0 20 40 60 80 0\n20\n40\n60\n80\nPM Entry S, Session 2\nCycles\nN u m b e r o f V e h ic le s\n0 20 40 60 80 0\n20\n40\n60\n80\nPM Entry S, Session 28\nCycles\nN u m b e r o f V e h ic le s\nPM Entry S, Session 8AM Entry S, Session 15\n0 20 40 60 80 0\n20\n40\n60\n80\nPM Entry S, Session 8\nCycles\nN u m b e r o f V e h ic le s\n0 20 40 60 80 0\n20\n40\n60\n80\nAM Entry S, Session 15\nCycles\nN u m b e r o f V e h ic le s\n(a) (b) (c)\n(d) (e) (f)\nFig.5 Generated ST signals and their distribution in 2D PCA space with different values of SNR based on the additive model\nOutliers are then synthesized over several consecutive cycles randomly by resembling their characteristics as shown in Table 2. Totally, a quantity of 322 (23*14) ST signals are generated from the original 23 ST signals by adding Gaussian white noise for each direction (totally 19 directions). Take traffic direction 2 in the PM session as an example. In this direction, there are 5 abnormal and 18 normal ST traffic signals. Each ST traffic signal $(%) will have a multiple of 14 to generate an overall 322 ST signals through the following procedures: Step 1: check all 23 ST signals &$ , # = 1,\u2026 ,23', and substitute abnormal signals with normal signals in adjacent days to form a new group of signals &$ (, # = 1,\u2026 ,23'. Step 2: add Gaussian white noise ) to each signal $ ( according to the predefined value of SNR.\n* ( = $ ( + ) , # = 1, \u2026 ,23 (1)\n0 20 40 60 80 0\n20\n40\n60\n80\n100\n120\n140\ncycle\nv o lu m n\nPM Entry ST 1: SNR=8\nOriginal signal Generated signal\n0 20 40 60 80 30\n40\n50\n60\n70\n80\n90\ncycle\nv o lu m n\nPM Entry ST 1: SNR=28\nOriginal signal Generated signal\n0 20 40 60 80 30\n40\n50\n60\n70\n80\n90\ncycle\nv o lu m n\nPM Entry ST 1: SNR=38 Original signal Generated signal\n0 50 100 150 200 250 300 -300\n-250\n-200\n-150\n-100\n-50\n0 PM Entry, Dir 2: SNR=8\nPCA1\nP C A 2\ninlier outlier new inlier new outlier\n250 300 350 400 450 500 -100\n-50\n0\n50\n100\n150 PM Entry, Dir 2: SNR=28\nPCA1\nP C A 2\ninlier outlier new inlier new outlier\n-500 -450 -400 -350 -300 -250 -150\n-100\n-50\n0\n50\n100 PM Entry, Dir 2: SNR=38\nPCA1\nP C A 2\ninlier outlier new inlier new outlier\nStep 3: choose M signals from &* (, # = 1,\u2026 ,23', and transform them into abnormal signals (outliers) according to eqns. (2)-(3) for two categories of outliers.\n* -(%) = ./\u210e, )\" \u2264 % \u2264 )\" + 2 * ((%), 3/\u210e45 #$4 % = 1,\u2026 ,80 (2) * --(%) = 8*(0, * ((%) \u2212 :) % = 1,\u2026 ,80 (3)\nwhere 2 \u2265 4 . We assume that it is an abnormal signal belonging to Category 2 if consecutive 4 or more traffic cycles with smaller number of traffic volumes happen in one session. N0 ranges from 1 to 23-L+1. According to our observation, the number of outliers in AM sessions is usually smaller than that in PM sessions. Therefore, we set M to 2 and 4 for the AM and PM sessions in our experiment, respectively.\n2.3 Feature extraction of ST signals A proper feature extraction could improve classification performance. As discussed in our previous work [20], big traffic data are easily contaminated with noise during data collection. Since these signals are very similar in the original ST domain, it is not easy to identify outliers. On the other hand, the complexity is too high if a whole piece of ST signal is directly input as one feature vector. According to our investigation, outliers in traffic data due to low vehicle volumes generally exist at least over several traffic cycles. In this paper, the mean of vehicles from four consecutive cycles is calculated. The process of mean from consecutive cycles for each ST signal is shown in Fig. 6, where f(%) = \u2211 *( )>? @ > 4\u2044 , and x(m) denotes the number of vehicles in the m th cycle. This averaging process is applied to all ST signals. The MeanShift filtering has the effect of eliminating signal values which are quite different from their surroundings. After accomplishing this process, the feature dimension of each signal is still high (i.e. 80 in this research). Principle Component Analysis (PCA) is well known to keep a signal quality by just extracting its main components, for which it reduces its dimensionality for feature representation as well. Herein, the 80-dimension feature vectors of each ST signal\nis analyzed by the PCA to keep just the first several components in order to reduce the complexity in the training and testing stages. The coefficients of the first several components, being a new domain with low-dimension, are used for AIC.\n2.4 Popular methods to be used for AIC on big traffic database Many countries in the world are suffering from traffic congestions in their highway and urban roads. Traffic incident is one of the major reasons [21] to traffic congestions. Delayed response to incidents would deteriorate the traffic further, and make incidents clearance and post-processing traffic more difficult. Therefore, it is indeed necessary to have an automatic and reliable incident detection and classification system. OD technique has been applied in traffic data analysis for several decades. Outlier means abnormal actions exist in traffic. Many OD algorithms have been developed for automatic incident detection (AID). Among these algorithms, machine learning algorithms [22-27] are most popular and widely investigated.\nSince 1990s, machine learning techniques are widely applied for AID. Various variations of Artificial Neural Network (ANN) were firstly investigated. In [22], multi-layer feedforward ANN (MLFANN) was used to detect incidents and showed better AID performance. In [23], Jin et. al adopted probabilistic ANN (PANN), and better detection success ratio (DSR) and false acceptance ratio FAR performance were achieved. To further improve AID performance, SVM [24-26] was investigated to detect outliers. Experimental results in [24] show that SVM can generate better AID performance than ANN. In [25], Yuan and Cheu\ndiscovered that SMVs with polynomial kernel and RBF kernel achieve high classification accuracy than MLFANN and PANN. With the development of machine learning, boosting technique was proposed to combine a set of weak learners to construct a single strong learner for classification. This concept is first proposed by Kearns and Valiant in 1988 and 1989 [27, 28]. At present, there are many boosting algorithms. The initial one was proposed by Freund in 1995 [29]. However, this method cannot support the subsequent weak learners adaptive to the instances which are misclassified by previous classifiers. In order to adapt to the weak learners, adaptive boosting (AdaBoost) algorithms, such as Real AdaBoost [30], Gentle AdaBoost [31] and others, were developed. Due to the outstanding performance of adaptive boosting methods, they have been applied to solve imbalanced dataset problems [32].\nNote that the above researches only focus on outlier detection in traffic data, while detailed incident categories are also very useful for traffic management and optimization of traffic road in a city. In this paper, we take advantages of AdaBoost method to detect outliers in imbalanced traffic dataset, and further make full use of SVM to learn the abnormal traffic behaviors to identify their categories.\n3. Automatic incident classification of traffic data\nIn general, the quantity of inliers (normal data) is usually much larger than that of the outliers (abnormal cases). However, the minority class is more interesting for the application of AIC. To discern the small number of outliers from a big database, we propose a hybrid method by taking advantages of both AdaBoost and SVM (AB-SVM) techniques to solve the classification problem of imbalanced dataset in this section. Fig. 7 depicts the flowchart of the AB-SVM classifier. The AB-SVM is developed for OD of traffic data by training a strong classifier, and further identification of the outliers categories based on trained support vectors.\nMore specifically, the training and testing data sets are first available based on the steps as depicted in Fig. 1. Features are then extracted based on the method in Section 2.3. PCA is subsequently adopted to reduce the feature vector dimension of the training data preceding the training process. In the AB-SVM framework, adaptive boosting (AdaBoost) technique is adopted for training a strong classifier to differentiate normal traffic signals (inlier) from abnormal ones (outliers), and SVM is employed to learn a base model from the low dimensional features of the outliers to classify their different types, such as jams leading to abrupt low volumes, repeated jams or hardware failure. If one traffic datum is detected as an outlier based on the AdaBoost classifier, the trained SVM model is further utilized to recognize the specific category of the predictive outlier. Details about this algorithm is illustrated as follows.\nFig.7 Flowchart of the proposed hybrid AB-SVM model"}, {"heading": "3.1 OD based on AdaBoost", "text": "Adaptive boosting (AdaBoost) is a particular machine learning method used to train a series of weak classifiers. During the training process for AdaBoost, the weights of the training samples are adaptively updated after each boosting iteration. The weights of the\ntraining samples which are misclassified by the current component classifier are increased, while the weights of the training samples with correct classification are decreased. Finally, the weak classifiers are combined linearly to form a strong classifier, which is expressed as B(*) = maxF \u2211 \u210eG(H)IG = maxF \u2211 \u210e(H, G, JG , K)IG (4) where ht is the t th weak learner, \u03b8 is a threshold, v denotes a feature vector in the PCA space, and G means that the fth component of v is used as input feature in weak learner \u210eG(H). The training procedures of the AdaBoost classifier for OD are summarized as follows: Step 1: Given a training set including ) positive and ) negative signals, and ) +) = ). (* , L ), \u2026 , (*M , LM) where &* ' M is the set of input signals in RD, and yi denotes the label of input signals. Step 2: Repeat for K = 0,1 Step 2.1 Initialize the weights , F ( , F = MN , MO 35 P = 0,1) of training samples. The weight of each training sample is inversely proportion to the number of samples in its own\ngroup. , F M denotes a probability distribution of training examples. Step 2.2 For each cycle t=1,2,\u2026T\nStep 2.2.1 Normalize the weights G, F as G, F = QR,ST\u2211 QR,UTVUWN , for i=1,\u2026,N (5) Step 2.2.2 Define a weak learner as \u210eG(H) = \u210e(H, G, JG , K) \u210e(H, G , JG , K) = 8X(HY > JG) + [ (6) Step 2.2.3 Evaluate error \\G(J) = \u2211 G, F (L F \u2212 \u210e(H , G , JG , K)) M (7) where L F \u2208 &\u22121,1' denotes the label of the training signal i. Step 2.2.4 Find the best weak learner \u210e(H, G , JG , K) with parameters 8, [, G , JG through\nminimizing \\G(J). Step 2.2.5 Update the classifier ^(H, K) for class c and weights of training samples\n^(H, K) = ^(H, K) + \u210e(H, G, JG , K) (8) G? , F = G, F exp (\u2212\u210e(H, G , JG , K)) (9)\nStep 2.2.6 If t<T, increase t by 1, and go to Step 2.2.1; otherwise go to Step 2 until all classes have been checked.\nStep 3: Output the final strong classifier as denoted in Eq. (4)."}, {"heading": "3.2 Abnormal incidents classification based on SVMs", "text": "In the field of machine learning, SVM technique [33] is widely used for classification problem. Compared with neural networks, SVM techniques are easy to be implemented and to offer satisfactory classification results in a wide variety of application domains, such as semantic image classification [34], handwritten recognition [35], and so on. In the AB-SVM, we apply SVMs [15] to classify abnormal traffic behaviors in traffic data. Experimental results show that the hybrid model achieves superior performance for AIC. Let C denote the set of abnormal traffic categories, and a = &1,\u2026 , 2' where L denotes the total number of outlier classes. Given M training samples from abnormal traffic signals, (* , P ), \u2026 , (*b, Pb) where &* ' b is the feature set of input signals in RD, and di denotes the label of input signals. To separate different abnormal traffic categories, the maximum margin to the hyper-plane can be obtained by solving the following optimization problem during the training process. cminQ,f,g \u2211 \u2016 @\u2016 + i\u2211 \u2211 j @\u2016k\u2016@lmSM @\u2208n $. /. : mSI \u2219 * \u2212 [mS \u2212 ( @I \u2219 * \u2212 [@) \u2265 1 \u2212 j @, j @ \u2265 0, # = 1, \u2026 , %; \u2208 &1,\u2026 2'\\P . (10) where @ \u2208 tu is a vector composed of weighting coefficients for class m, i is a regularization parameter, which controls the model complexity and the training error.\ni \u2211 \u2211 j @\u2016v\u2016@lmSM is a penalty term, which is used to penalize misclassified samples, and j denotes the distance of the sample from the margin if it is classified wrongly.\nBy introducing Lagrange multipliers &: ' M and dual transformation [36-38], the model parameters ( , [, j, :) are obtained. The category of abnormal traffic ST signals in the test set are predicted as P = $#w%( I* ) = $#w%(\u2211 :>P>*>* >\u2208xy ) = $#w%(\u2211 :>>\u2208xy P>z(*>, * )) (11) where SV denotes the set of support vectors, z(*>, * ) is a kernel function, and it is the inner product of two feature vectors. By using this kernel function, the training samples can be mapped from an input space to another feature space which makes samples more separated. There are three commonly used kernels for SVMs namely linear, polynomial and radial basis function (RBF). For the application of AIC, the SVM with linear kernels achieves outstanding performance. This will be discussed in Section 4."}, {"heading": "3.3 Testing based on the trained AB-SVM classifier", "text": "As a consequence, the testing stage can be carried out as depicted in Fig. 7 based on the hybrid method. Given a testing sample, the feature is first extracted and feature dimension is reduced based on PCA. The extracted feature vector with a low dimension is passed through the trained AdaBoost classifier. If it is classified as outliers, the learned SVM model is further adopted to identify its abnormal behavior. Otherwise this testing sample is a normal signal.\n4. Experimental result\nIn this experiment, we perform the AIC for 19 directions in the four-arm junction. Each direction includes three traffic states, and 345 traffic ST signals are used in the experimental work. A 5-fold evaluation is performed on these 345 traffic ST signals including 276 for training and 69 for testing. The experiments were performed on a platform using Intel(R)\nCore(TM)2 Duo CUP E8500 @3.16 GHz and RAM of 4.0 GB. The proposed algorithm is coded in MATLab. It takes about 63.30 seconds and 70.02 seconds by using the proposed AIC method for the AM and PM sessions, respectively.\n4.1 Classification accuracy\nTables 3 and 4 list the classification accuracies by using the proposed method with linear kernel for the AM and PM sessions, respectively. Since the features of the ST signal through PCA are more discriminant, therefore the proposed method shows good performance by using linear kernels. From the results, we can see that the proposed method achieves high classification performance (more than 92%). In regard of the average classification accuracy, the AM and PM sessions obtained 98.37% and 98.44%, respectively.\nTables 5 and 6 show the confusion matrix for all 19 traffic directions in the AM and PM sessions, respectively. From the results, we can see that inliers (normal data) are detected with a high degree of precision (almost 100%). The repeated traffic jam (category 3) is also easily recognized with an average of 99% success rate. The 1% is misclassified as slight traffic jam (category 2) in the AM session, and 1% of category 3 is misclassified as inliers in the PM session. Compared with Categories 1 and 3, slight traffic jam is more difficult to be recognized with 64% and 83% average success rates for the AM and PM sessions, respectively. For the AM session, 35% and 1% of slight traffic jam situations are misclassified as normal state (Category 1) and repeated traffic jam (Category 3), respectively. For the PM session, 15% and 2% of slight traffic jam cases are misclassified as normal state (Category 1) and repeated traffic jam (Category 3), respectively.\nTables 7 and 8 list the classification accuracies by using the proposed method with RBF kernel for the AM and PM sessions, respectively. From the results, we can see that the proposed method with linear kernels as shown in Tables 3 and 4 has better classification accuracies than that with the RBF kernel (average 94.26% (AM) and 89.82% (PM) classification accuracies). It means that the liner kernel is superior to the RBF kernel in the SVM model for the application of AIC with discriminative features.\n4.2 Relationship between classification accuracy and PCA dimension In order to have a discriminative feature to represent a ST signal, PCA is used to reduce the dimension of feature vector of a traffic signal before training. To study the variation of classification accuracy with the dimension of feature vectors, 17 groups of experiments with different dimensions of PCA feature vectors were performed. To produce robust results, a 5- fold cross-validation approach was used in each group of experiment. Fig. 8 shows the variation of averaging classification accuracy of the 19 traffic directions with respect to different dimensions of PCA feature vectors. From Fig. 8, an optimal classification performance is achieved when the dimension of PCA feature vector is set to 25 and 40 for the AM and PM sessions, respectively.\nFig.8 Variation of classification accuracy with different dimensions of PCA feature vector\n5. Conclusion\nIn this paper, an AIC method has been presented to identify traffic states in big traffic data. In the proposed method, traffic data are firstly represented as ST signals, and a set of simulation data are thereby generated to construct an extended big traffic database. A Mean-Shift filter is then used to extract the features of ST signals. In order to decrease training complexity, the PCA is adopted to reduce the feature dimension and extract discriminative representation of signal features. In the training stage, a hybrid model combines an adaptive boosting classifier and a SVM learning to detect outliers and further to identify the categories of outliers. Experimental results show that the proposed method achieves high classification accuracy.\nIn the future, we would explore the differences of traffic characteristics in different traffic directions. Based on the analysis, more discriminative feataures should be extracted for training and testing stages. It is expected that the slight traffic jams would also be discerned with the higher accuracy. Another research direction is to estimate the optical flow of objects of interest from camera videos instead of the ST signals. AIC would be performed based on the distribution of optical flow fields.\n0 20 40 60 80 96.5\n97\n97.5\n98\n98.5\nPCA dimension\nC la s s if ic a ti o n a c c u ra c y ( % )\nAM Session PM Session\nAcknowledgment\nThis research is supported by the grants of Hong Kong RGC GRF: 12201814 and HKBU FRG/14-15/054.\nReferences\n[1] E. S. Park, S. Turner, and C. H. Spiegelman, \"Empirical approaches to outlier detection in\nintelligent transportation systems data,\" Transportation Research Record: Journal of the Transportation Research Board, vol. 1840, pp. 21-30, 2003.\n[2] V. J. Hodge and J. Austin, \"A survey of outlier detection methodologies,\" Artificial Intelligence\nReview, vol. 22, pp. 85-126, 2004.\n[3] H. Y. Ngan, G. K. Pang, and N. H. Yung, \"Performance evaluation for motif-based patterned\ntexture defect detection,\" Automation Science and Engineering, IEEE Transactions on, vol. 7, pp. 58-72, 2010.\n[4] H. Y. Ngan, N. H. Yung, and A. G. Yeh, \"A comparative study of outlier detection for large-scale\ntraffic data by one-class SVM and kernel density estimation,\" in IS&T/SPIE Electronic Imaging, 2015, pp. 94050I-94050I-10.\n[5] T. T. Dang, H. Y. T. Ngan, and W.Liu, \"Distance-based k-nearest Neighbors Outlier Detection\nMethod in Large-scale Traffic Data,\" in Proceedings of 2015 IEEE International Conference on Digital Signal Processing, 2015.\n[6] F. E. Grubbs, \"Procedures for detecting outlying observations in samples,\" Technometrics, vol. 11,\npp. 1-21, 1969.\n[7] V. Barnett and T. Lewis, Outliers in statistical data vol. 3: Wiley New York, 1994.\n[8] A. Loureiro, L. Torgo, and C. Soares, \"Outlier detection using clustering methods: a data cleaning\napplication,\" in Proceedings of KDNet Symposium on Knowledge-based systems for the Public Sector, 2004.\n[9]V. Chandola, A. Banerjee, and V. Kumar, \"Outlier detection: A survey,\" ACM Computing Surveys,\n2007.\n[10]L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen, Classification and regression trees: CRC\npress, 1984.\n[11]B. E. Boser, I. M. Guyon, and V. N. Vapnik, \"A training algorithm for optimal margin classifiers,\"\nin Proceedings of the fifth annual workshop on Computational learning theory, 1992, pp. 144-152.\n[12]R. Caruana and A. Niculescu-Mizil, \"An empirical comparison of supervised learning algorithms,\"\nin Proceedings of the 23rd international conference on Machine learning, 2006, pp. 161-168.\n[13]C. Cortes and V. Vapnik, \"Support-vector networks,\" Machine learning, vol. 20, pp. 273-297,\n1995.\n[14]A. Torralba, K. P. Murphy, and W. T. Freeman, \"Sharing features: efficient boosting procedures\nfor multiclass object detection,\" in Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, 2004, pp. II-762-II-769 Vol. 2.\n[15]M. Allahviranloo and W. Recker, \"Daily activity pattern recognition by using support vector\nmachines with multiple classes,\" Transportation Research Part B: Methodological, vol. 58, pp. 16- 43, 2013.\n[16]N. Japkowicz and S. Stephen, \"The class imbalance problem: A systematic study,\" Intelligent data\nanalysis, vol. 6, pp. 429-449, 2002.\n[17]M. Kubat and S. Matwin, \"Addressing the curse of imbalanced training sets: one-sided selection,\"\nin ICML, 1997, pp. 179-186.\n[18]C. Chen, A. Liaw, and L. Breiman, \"Using random forest to learn imbalanced data,\" University of\nCalifornia, Berkeley, 2004.\n[19]H. Y. T. Ngan, Yung, N.H.C., Anthony G.O. Yeh, \"Outlier detection in traffic data based on the\ndirichlet process mixture model,\" in IET Intelligent Transport Systems, 2014.\n[20]J. Luk, C. Han, and D. A. Chin, \"Freeway incident detection: technologies and techniques,\" 2010.\n[21]R. L. Cheu and S. G. Ritchie, \"Automated detection of lane-blocking freeway incidents using\nartificial neural networks,\" Transportation Research Part C: Emerging Technologies, vol. 3, pp. 371-388, 1995.\n[22]X. Jin, R. L. Cheu, and D. Srinivasan, \"Development and adaptation of constructive probabilistic\nneural network in freeway incident detection,\" Transportation Research Part C: Emerging Technologies, vol. 10, pp. 121-147, 2002.\n[23]R. L. Cheu, D. Srinivasan, and E. T. Teh, \"Support vector machine models for freeway incident\ndetection,\" in Intelligent Transportation Systems, 2003. Proceedings. 2003 IEEE, 2003, pp. 238- 243.\n[24]F. Yuan and R. L. Cheu, \"Incident detection using support vector machines,\" Transportation\nResearch Part C: Emerging Technologies, vol. 11, pp. 309-328, 2003.\n[25]Z. Zhou and L.-y. Zhou, \"An Automatic Incident of Freeway Detection Algorithm Based on\nSupport Vector Machine,\" in Intelligence Information Processing and Trusted Computing (IPTC), 2010 International Symposium on, 2010, pp. 543-546.\n[26]M. Kearns, \"Thoughts on hypothesis boosting,\" Unpublished manuscript, vol. 45, p. 105, 1988.\n[27]M. Kearns and L. Valiant, \"Cryptographic limitations on learning Boolean formulae and finite\nautomata,\" Journal of the ACM (JACM), vol. 41, pp. 67-95, 1994.\n[28]R. E. Schapire, \"The strength of weak learnability,\" Machine learning, vol. 5, pp. 197-227, 1990.\n[29]B. Wu, H. Ai, C. Huang, and S. Lao, \"Fast rotation invariant multi-view face detection based on\nreal adaboost,\" in Automatic Face and Gesture Recognition, 2004. Proceedings. Sixth IEEE International Conference on, 2004, pp. 79-84.\n[30]R. Lienhart, A. Kuranov, and V. Pisarevsky, \"Empirical analysis of detection cascades of boosted\nclassifiers for rapid object detection,\" in Pattern Recognition, ed: Springer, 2003, pp. 297-304.\n[31]V. N. Vapnik and V. Vapnik, Statistical learning theory vol. 1: Wiley New York, 1998.\n[32]F. Hu, X. Liu, J. Dai, and H. Yu, \"A Novel Algorithm for Imbalance Data Classification Based on\nNeighborhood Hypergraph,\" The Scientific World Journal, vol. 2014, 2014.\n[33]J. Qin and N. H. Yung, \"Feature fusion within local region using localized maximum-margin\nlearning for scene categorization,\" Pattern Recognition, vol. 45, pp. 1671-1683, 2012.\n[34]M. M. Adankon and M. Cheriet, \"Model selection for the LS-SVM. Application to handwriting\nrecognition,\" Pattern Recognition, vol. 42, pp. 3264-3270, 2009.\n[35]K. Krammer and Y. Singer, \"On the algorithmic implementation of multi-class SVMs,\" Proc. of\nJMLR, 2001.\n[36]B. Scholkopf, K.-K. Sung, C. J. Burges, F. Girosi, P. Niyogi, T. Poggio, et al., \"Comparing\nsupport vector machines with Gaussian kernels to radial basis function classifiers,\" Signal Processing, IEEE Transactions on, vol. 45, pp. 2758-2765, 1997.\n[37]K.-B. Duan and S. S. Keerthi, \"Which is the best multiclass SVM method? An empirical study,\" in\nMultiple Classifier Systems, ed: Springer, 2005, pp. 278-285.\n[38]C.-C. Chang and C.-J. Lin, \"LIBSVM: a library for support vector machines,\" ACM Transactions\non Intelligent Systems and Technology (TIST), vol. 2, p. 27, 2011."}], "references": [{"title": "Empirical approaches to outlier detection in intelligent transportation systems data", "author": ["E.S. Park", "S. Turner", "C.H. Spiegelman"], "venue": "Transportation Research Record: Journal of the Transportation Research Board, vol. 1840, pp. 21-30, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1840}, {"title": "A survey of outlier detection methodologies", "author": ["V.J. Hodge", "J. Austin"], "venue": "Artificial Intelligence Review, vol. 22, pp. 85-126, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Performance evaluation for motif-based patterned texture defect detection", "author": ["H.Y. Ngan", "G.K. Pang", "N.H. Yung"], "venue": "Automation Science and Engineering, IEEE Transactions on, vol. 7, pp. 58-72, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "A comparative study of outlier detection for large-scale traffic data by one-class SVM and kernel density estimation", "author": ["H.Y. Ngan", "N.H. Yung", "A.G. Yeh"], "venue": "IS&T/SPIE Electronic Imaging, 2015, pp. 94050I-94050I-10.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Distance-based k-nearest Neighbors Outlier Detection Method in Large-scale Traffic Data", "author": ["T.T. Dang", "H.Y.T. Ngan", "W.Liu"], "venue": "Proceedings of 2015 IEEE International Conference on Digital Signal Processing, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Procedures for detecting outlying observations in samples", "author": ["F.E. Grubbs"], "venue": "Technometrics, vol. 11, pp. 1-21, 1969.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1969}, {"title": "Outliers in statistical data vol", "author": ["V. Barnett", "T. Lewis"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Outlier detection using clustering methods: a data cleaning application", "author": ["A. Loureiro", "L. Torgo", "C. Soares"], "venue": "Proceedings of KDNet Symposium on Knowledge-based systems for the Public Sector, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Outlier detection: A survey,", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Computing Surveys,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A training algorithm for optimal margin classifiers,", "author": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"], "venue": "Proceedings of the fifth annual workshop on Computational learning theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "An empirical comparison of supervised learning", "author": ["R. Caruana", "A. Niculescu-Mizil"], "venue": "Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Sharing features: efficient boosting procedures for multiclass object detection,", "author": ["A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Daily activity pattern recognition by using support vector machines with multiple classes,", "author": ["M. Allahviranloo", "W. Recker"], "venue": "Transportation Research Part B: Methodological,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "The class imbalance problem: A systematic study,", "author": ["N. Japkowicz", "S. Stephen"], "venue": "Intelligent data analysis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Addressing the curse of imbalanced training sets: one-sided selection,", "author": ["M. Kubat", "S. Matwin"], "venue": "in ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Using random forest to learn imbalanced data,", "author": ["C. Chen", "A. Liaw", "L. Breiman"], "venue": "University of California,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Outlier detection in traffic data based on the dirichlet process mixture model,", "author": ["H.Y.T. Ngan", "N.H.C. Yung", "Anthony G.O. Yeh"], "venue": "IET Intelligent Transport Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Freeway incident detection: technologies and techniques,", "author": ["J. Luk", "C. Han", "D.A. Chin"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Automated detection of lane-blocking freeway incidents using artificial neural networks,\" Transportation", "author": ["R.L. Cheu", "S.G. Ritchie"], "venue": "Research Part C: Emerging Technologies,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "Development and adaptation of constructive probabilistic neural network in freeway incident detection,", "author": ["X. Jin", "R.L. Cheu", "D. Srinivasan"], "venue": "Transportation Research Part C: Emerging Technologies,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Support vector machine models for freeway incident detection,\" in Intelligent Transportation", "author": ["R.L. Cheu", "D. Srinivasan", "E.T. Teh"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Incident detection using support vector machines,", "author": ["F. Yuan", "R.L. Cheu"], "venue": "Transportation Research Part C: Emerging Technologies,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "An Automatic Incident of Freeway Detection Algorithm Based on Support Vector Machine,\" in Intelligence Information Processing and Trusted Computing (IPTC)", "author": ["Z. Zhou", "L.-y. Zhou"], "venue": "International Symposium on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Thoughts on hypothesis boosting,", "author": ["M. Kearns"], "venue": "Unpublished manuscript,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1988}, {"title": "Cryptographic limitations on learning Boolean formulae and finite automata,", "author": ["M. Kearns", "L. Valiant"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1994}, {"title": "The strength of weak learnability,", "author": ["R.E. Schapire"], "venue": "Machine learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1990}, {"title": "Fast rotation invariant multi-view face detection based on real adaboost,\" in Automatic Face and Gesture Recognition, 2004. Proceedings", "author": ["B. Wu", "H. Ai", "C. Huang", "S. Lao"], "venue": "Sixth IEEE International Conference on,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Pisarevsky, \"Empirical analysis of detection cascades of boosted classifiers for rapid object detection,\" in Pattern Recognition", "author": ["R. Lienhart", "A. Kuranov"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "Statistical learning theory vol", "author": ["V.N. Vapnik", "V. Vapnik"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1998}, {"title": "A Novel Algorithm for Imbalance Data Classification Based on Neighborhood Hypergraph,", "author": ["F. Hu", "X. Liu", "J. Dai", "H. Yu"], "venue": "The Scientific World Journal,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Feature fusion within local region using localized maximum-margin learning for scene categorization,", "author": ["J. Qin", "N.H. Yung"], "venue": "Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Model selection for the LS-SVM. Application to handwriting recognition,", "author": ["M.M. Adankon", "M. Cheriet"], "venue": "Pattern Recognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "On the algorithmic implementation of multi-class SVMs,", "author": ["K. Krammer", "Y. Singer"], "venue": "Proc. of JMLR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Comparing support vector machines with Gaussian kernels to radial basis function classifiers,", "author": ["B. Scholkopf", "K.-K. Sung", "C.J. Burges", "F. Girosi", "P. Niyogi", "T. Poggio"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1997}, {"title": "Which is the best multiclass SVM method? An empirical study,\" in Multiple Classifier", "author": ["K.-B. Duan", "S.S. Keerthi"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "LIBSVM: a library for support vector machines,", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, studies have been published for outlier detection (OD) [1-5].", "startOffset": 72, "endOffset": 77}, {"referenceID": 1, "context": "In recent years, studies have been published for outlier detection (OD) [1-5].", "startOffset": 72, "endOffset": 77}, {"referenceID": 2, "context": "In recent years, studies have been published for outlier detection (OD) [1-5].", "startOffset": 72, "endOffset": 77}, {"referenceID": 3, "context": "In recent years, studies have been published for outlier detection (OD) [1-5].", "startOffset": 72, "endOffset": 77}, {"referenceID": 4, "context": "In recent years, studies have been published for outlier detection (OD) [1-5].", "startOffset": 72, "endOffset": 77}, {"referenceID": 5, "context": "In [6, 7], the purpose of an OD is to identify data points appearing inconsistent with the majority of the data (inliers).", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "In [6, 7], the purpose of an OD is to identify data points appearing inconsistent with the majority of the data (inliers).", "startOffset": 3, "endOffset": 9}, {"referenceID": 0, "context": "in many areas, for example, [1] suggested aircraft engine rotation defect detection, heart-rate monitors and fabric defect detection, or [3] mentioned traffic abnormality detection.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "in many areas, for example, [1] suggested aircraft engine rotation defect detection, heart-rate monitors and fabric defect detection, or [3] mentioned traffic abnormality detection.", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "The common unsupervised methods include clustering-based method [8], distance-based method [9], and density-based method [10].", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "The common unsupervised methods include clustering-based method [8], distance-based method [9], and density-based method [10].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "The clustering-based approach [8] defines an observation as an outlier if it deviates from the overall clustering pattern.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "The distance-based approach [9] assumes an observation is an outlier if the distances of a certain percentage of samples from a datum are larger than a given threshold.", "startOffset": 28, "endOffset": 31}, {"referenceID": 9, "context": "To identify the category of each outlier, one can resort to supervised learning approach [11-15].", "startOffset": 89, "endOffset": 96}, {"referenceID": 10, "context": "To identify the category of each outlier, one can resort to supervised learning approach [11-15].", "startOffset": 89, "endOffset": 96}, {"referenceID": 11, "context": "To identify the category of each outlier, one can resort to supervised learning approach [11-15].", "startOffset": 89, "endOffset": 96}, {"referenceID": 12, "context": "To identify the category of each outlier, one can resort to supervised learning approach [11-15].", "startOffset": 89, "endOffset": 96}, {"referenceID": 11, "context": "In the literature, SVM [14] and adaptive boosting techniques [15] are the most popular methods and have achieved better classification performance than others.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "In the literature, SVM [14] and adaptive boosting techniques [15] are the most popular methods and have achieved better classification performance than others.", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "In [16], SVM-based method was successfully applied to recognize daily activity pattern for the forecasting purpose of travel demand.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "Adaptive boosting technique [15] as an ensemble method is capable of training a strong classifier by combining a series of", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "This method has been proven [15] to boost classification accuracy.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "If the imbalanced data set is used for learning, the performance of the learning algorithm(s) would degrade significantly [17], as most methods tend to build the classifiers from the majority category of data.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "The commonly used methods include performing data redistribution [18] or classifier modification [19].", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "The commonly used methods include performing data redistribution [18] or classifier modification [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "As discussed in our previous work [20], big traffic data are easily contaminated with noise during data collection.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Traffic incident is one of the major reasons [21] to traffic congestions.", "startOffset": 45, "endOffset": 49}, {"referenceID": 19, "context": "Among these algorithms, machine learning algorithms [22-27] are most popular and widely investigated.", "startOffset": 52, "endOffset": 59}, {"referenceID": 20, "context": "Among these algorithms, machine learning algorithms [22-27] are most popular and widely investigated.", "startOffset": 52, "endOffset": 59}, {"referenceID": 21, "context": "Among these algorithms, machine learning algorithms [22-27] are most popular and widely investigated.", "startOffset": 52, "endOffset": 59}, {"referenceID": 22, "context": "Among these algorithms, machine learning algorithms [22-27] are most popular and widely investigated.", "startOffset": 52, "endOffset": 59}, {"referenceID": 23, "context": "Among these algorithms, machine learning algorithms [22-27] are most popular and widely investigated.", "startOffset": 52, "endOffset": 59}, {"referenceID": 24, "context": "Among these algorithms, machine learning algorithms [22-27] are most popular and widely investigated.", "startOffset": 52, "endOffset": 59}, {"referenceID": 19, "context": "In [22], multi-layer feedforward ANN (MLFANN) was used to detect incidents and showed better AID performance.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [23], Jin et.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "To further improve AID performance, SVM [24-26] was investigated to detect outliers.", "startOffset": 40, "endOffset": 47}, {"referenceID": 22, "context": "To further improve AID performance, SVM [24-26] was investigated to detect outliers.", "startOffset": 40, "endOffset": 47}, {"referenceID": 23, "context": "To further improve AID performance, SVM [24-26] was investigated to detect outliers.", "startOffset": 40, "endOffset": 47}, {"referenceID": 21, "context": "Experimental results in [24] show that SVM can generate better AID performance than ANN.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "In [25], Yuan and Cheu", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "This concept is first proposed by Kearns and Valiant in 1988 and 1989 [27, 28].", "startOffset": 70, "endOffset": 78}, {"referenceID": 25, "context": "This concept is first proposed by Kearns and Valiant in 1988 and 1989 [27, 28].", "startOffset": 70, "endOffset": 78}, {"referenceID": 26, "context": "The initial one was proposed by Freund in 1995 [29].", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "In order to adapt to the weak learners, adaptive boosting (AdaBoost) algorithms, such as Real AdaBoost [30], Gentle AdaBoost [31] and others, were developed.", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "In order to adapt to the weak learners, adaptive boosting (AdaBoost) algorithms, such as Real AdaBoost [30], Gentle AdaBoost [31] and others, were developed.", "startOffset": 125, "endOffset": 129}, {"referenceID": 29, "context": "Due to the outstanding performance of adaptive boosting methods, they have been applied to solve imbalanced dataset problems [32].", "startOffset": 125, "endOffset": 129}, {"referenceID": 30, "context": "In the field of machine learning, SVM technique [33] is widely used for classification problem.", "startOffset": 48, "endOffset": 52}, {"referenceID": 31, "context": "Compared with neural networks, SVM techniques are easy to be implemented and to offer satisfactory classification results in a wide variety of application domains, such as semantic image classification [34], handwritten recognition [35], and so on.", "startOffset": 202, "endOffset": 206}, {"referenceID": 32, "context": "Compared with neural networks, SVM techniques are easy to be implemented and to offer satisfactory classification results in a wide variety of application domains, such as semantic image classification [34], handwritten recognition [35], and so on.", "startOffset": 232, "endOffset": 236}, {"referenceID": 12, "context": "In the AB-SVM, we apply SVMs [15] to classify abnormal traffic behaviors in traffic data.", "startOffset": 29, "endOffset": 33}, {"referenceID": 33, "context": "By introducing Lagrange multipliers &: ' M and dual transformation [36-38], the model parameters ( , [, j, :) are obtained.", "startOffset": 67, "endOffset": 74}, {"referenceID": 34, "context": "By introducing Lagrange multipliers &: ' M and dual transformation [36-38], the model parameters ( , [, j, :) are obtained.", "startOffset": 67, "endOffset": 74}, {"referenceID": 35, "context": "By introducing Lagrange multipliers &: ' M and dual transformation [36-38], the model parameters ( , [, j, :) are obtained.", "startOffset": 67, "endOffset": 74}], "year": 2015, "abstractText": "Modern cities experience heavy traffic flows and congestions regularly across space and time. Monitoring traffic situations becomes an important challenge for the Traffic Control and Surveillance Systems (TCSS). In advanced TCSS, it is helpful to automatically detect and classify different traffic incidents such as severity of congestion, abnormal driving pattern, abrupt or illegal stop on road, etc. Although most TCSS are equipped with basic incident detection algorithms, they are however crude to be really useful as an automated tool for further classification. In literature, there is a lack of research for Automated Incident Classification (AIC). Therefore, a novel AIC method is proposed in this paper to tackle such challenges. In the proposed method, traffic signals are firstly extracted from captured videos and converted as spatial-temporal (ST) signals. Based on the characteristics of the ST signals, a set of realistic simulation data are generated to construct an extended big traffic database to cover a variety of traffic situations. Next, a Mean-Shift filter is introduced to suppress the effect of noise and extract significant features from the ST signals. The extracted features are then associated with various types of traffic data: one normal type (inliers) and multiple abnormal types (outliers). For the classification, an adaptive boosting classifier is trained to detect outliers in traffic data automatically. Further, a Support Vector Machine (SVM) based method is adopted to train the model for identifying the categories of outliers. In short, this hybrid approach is called an Adaptive Boosting Support Vector Machines (AB-SVM) method. Experimental results show that the proposed AB-SVM method achieves a satisfied result with more than 92% classification accuracy on average.", "creator": "PScript5.dll Version 5.2.2"}}}