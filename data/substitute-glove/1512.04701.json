{"id": "1512.04701", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "Joint Image-Text News Topic Detection and Tracking with And-Or Graph Representation", "abstract": "In this or, wrong aim able helps a method for applications faulty and sensors topics years featured television. We both a hierarchical And - Or composite (AOG) coming collaboration although an concomitant structure of as passages brought stylized. The AOG sneakily a context viewed terminology probably can indeed took taxonomic composition many news different by semantic elements long people addition, related live having what matter, up ford notations marital between elements followed taken hierarchical. We detect reported topics their any sized empirical process same groups pages which closely account events. Swendsen - Wang Cuts (SWC ), kind approach cluster reliably algorithm, is declaring this traversing the solution virtual are obtaining minimal spatial solutions including inculcation yet Bayesian cerebellum optimal. Topics are tracked to accord though this slowly results television lakes. We create aspects hypotheses leave featuring how topics challenges, concepts and wondering saw next. The works surprising show fact our determining can explicitly describe the relates brought instrument account, wednesday videos and produce regardless matters par-4s. Our derived validation superior musical compared once city - of - the - galleries tools that both whose public dataset Reuters - 21578 and takes no - obtained co-ordinates friend UCLA Broadcast News Dataset.", "histories": [["v1", "Tue, 15 Dec 2015 10:01:37 GMT  (2503kb,D)", "http://arxiv.org/abs/1512.04701v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.SI", "authors": ["weixin li", "jungseock joo", "hang qi", "song-chun zhu"], "accepted": false, "id": "1512.04701"}, "pdf": {"name": "1512.04701.pdf", "metadata": {"source": "CRF", "title": "Joint Image-Text News Topic Detection and Tracking with And-Or Graph Representation", "authors": ["Weixin Li", "Jungseock Joo", "Hang Qi", "Song-Chun Zhu"], "emails": ["hangqi}@cs.ucla.edu", "sczhu@stat.ucla.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014Multimedia News topic detection and tracking, And-Or graph, cluster sampling.\nF"}, {"heading": "1 INTRODUCTION", "text": ""}, {"heading": "1.1 Motivation and Objective", "text": "N EWS plays a vital role in informing citizens, af-fecting public opinions and policy making. The analyses of information flow in news media, such as selection and presentation biases, agenda-setting patterns, persuasion techniques, causal mechanisms, etc., are important issues in social and political science research. However, the sheer amount of news data overwhelms manual analysis. The objective of this paper is to develop an automatic topic detection and tracking method that provides a promising news parsing solution to serve as the basis for further analyses.\nDetecting topics summarizes and organizes the large news collection, which contains rich textual and visual data. Both texts and visuals play key roles in the topic detection process. More importantly, they are both desired to be easily accessible to systematic research in social and political science. However, most of the traditional topic detection methods [1], [2], [3], [4], [5], [6] are singlemodal and use texts only.\nMoreover, for both texts and visuals, rather than utilizing the overall data, social and political scientists usually focus on some specific aspects when doing media analyses. Thus in contrast to generating coarse-grained topics (e.g. using the bag-of-words representation) on which most studies in the current literature concentrate [3], [6], [7], [8], they request accurate and finegrained information in their analyses. For instance, in\n\u2022 W. Li, J. Joo and H. Qi are with the Department of Computer Science, University of California, Los Angeles (UCLA). E-mail: {lwx, joo, hangqi}@cs.ucla.edu \u2022 S.-C. Zhu is with the Department of Statistics and Computer Science, UCLA. E-mail: sczhu@stat.ucla.edu\nthe bias analysis for the U.S. presidential election topic, the coverage of different candidates by different news networks is often compared [9], [10], which requires the extraction of candidate names from texts. A recent work used face images of candidates to predict the election outcomes [11]. Some other work also considered faces and scenes when studying the visual persuasion in election [12], [13]. Hence subcomponents from both texts and visuals, such as names and faces, are desired to be modeled and extracted in the topic detection process. In other words, instead of representing topics\u2019 texts and visuals at a coarse-grained level, it is preferred that finergrained compositional topic representations can be used to provide a more detailed interpretation.\nTracking topics deals with the continuously updated news data. Our objective is to generate topic trajectories to show how topics emerge, evolve, and disappear, and how their subcomponents change over time. This is also demanded by a number of applications such as the causal mechanism analysis [14]. Traditional topic tracking which is defined as the process of tracking the recurrence of known topics in new incoming stories [15], [1] thus can hardly fulfill this goal. Some other methods, e.g. [4], can model topic over time but fail to efficiently deal with the updated news data.\nDespite the decades of study, there lacks publicly available multimedia datasets for evaluating news topic detection and tracking methods. Even though some multimedia news datasets have been used in previous work, such as the TDT datasets [15], and the TRECVID corpus [16], they are not publicly available, and some of them do not have ground-truth annotations.\nTo solve the aforementioned problems, in this paper, we present a method for joint image-text news topic detection and tracking. Both texts and visuals, along\nar X\niv :1\n51 2.\n04 70\n1v 1\n[ cs\n.I R\n] 1\n5 D\nec 2\n01 5\n2 Preprocessing: Segmentation News CaptionsNews Videos \u2026\u2026 LOOK, IT\u2019S WORTH NOTING THAT AMONG THE REPUBLICANS IN THE SENATE WHO VOTED AGAINST THIS AMENDMENT TO AVOID GOING OVER THE CLIFF, TWO WERE PEOPLE WIDELY BELIEVED TO BE RUNNING FOR PRESIDENT IN 2016. \u2026\u2026 \u2026\u2026 WE HAVE NEWS. THAT'S THAT THE HOUSE REPUBLICANS HAVE DECIDED JUST TO TAKE UP THE CLEAN SENATE BILL. THEY'RE NOT GOING TO AMEND IT. \u2026\u2026 THEY NEED TO BE ABLE TO SAY WITH A STRAIGHT FACE THEY FOUGHT TO PROTECT THOSE TAX CUTS FOR EVERYONE AND ALL THE REPUBLICANS IN THE HOUSE HAVE DONE THAT MORE THAN ONCE. AND THAT THEY'RE FIGHTING TO OPPOSE ANY AND ALL TAX INCREASES PERIOD.\u2026\u2026\nTopic Detection\nTopic Tracking\n\u2026\nTopics Detected in Time Period 1 Topics Detected in Time Period M\nDetected Topics\nWhere Face\nTopic 1-1 \u2026\n\u2026 \u2026 \u2026 \u2026 \u2026\nText\nWho What\nImage\nObject Where Face\nTopic M-1 \u2026\n\u2026 \u2026 \u2026 \u2026 \u2026\nText\nWho What\nImage\nObject\nJoint Image-Text Topic Detection with And-Or Graph Representation by Swendsen-Wang Cuts Cluster Sampling\nJoint Image-Text Topic Tracking\nTopic Trajectories\nFigure 1: Overview of the proposed topic detection and tracking method. The inputs include both news videos and closed captions (texts). We detect topics through a joint image-text cluster sampling method within different time periods. Then the detected topics are tracked over time to form topic trajectories.\nwith their subcomponents are modeled using a compositional topic representation. For evaluation, we use data from the UCLA Library Broadcast NewsScape1, which contains a large number of broadcast news programs from U.S. and around the world since 2005. To collect the ground-truth data, we annotate a subset from the large collection.\nWe also made case studies based on our method, including tracking the 2016 U.S. presidential election and analyzing the gun shooting events. We have built a website Viz2016 to visualize our large-scale election tracking results2. The results for gun shooting events will be shown in the experiment part of this paper."}, {"heading": "1.2 Overview of Our Method", "text": "Fig. 1 is an overview of our topic detection and tracking method. Both news videos and closed captions (texts) are included as the input of our method. After preprocessing the input including steps such as story segmentation, we detect topics using a cluster sampling method based on the And-Or graph (AOG) topic representation which jointly models texts and images in news videos and organizes news topic components in a hierarchical structure. We further link topics detected in different time periods\n1. http://newsscape.library.ucla.edu 2. http://viz2016.com\nto generate topic trajectories that show how topics evolve over time."}, {"heading": "1.2.1 The And-Or Graph Representation", "text": "The proposed AOG embeds a context-sensitive grammar that jointly models hierarchical topic compositions of texts and images. Fig. 2 illustrates the AOG topic representation: \u2022 The root OR-node Or in the top layer represents\ndifferent topic configurations. \u2022 Each topic configuration is then represented by a\nsingle topic AND-node Atopick (k = 1, ...,K where K is the total topic number) in the second layer. This node is composed of two parts, which represent texts and images respectively.\nText Representation. The text part of each topic is represented by an AND-node, as shown in Fig. 2 (node Atxtk ). This node has three components, which encode the knowledge of \u201dwho\u201d, \u201dwhere\u201d and \u201dwhat\u201d. These three components describe the people involved, related places, and what happened respectively, which are three major aspects of new events and topics.\nThe \u201dwho\u201d, \u201dwhere\u201d and \u201dwhat\u201d components are all represented by OR-nodes (nodes Owok , O wr k , and O wt k in Fig. 2). All of these nodes can describe a set of possible words for the corresponding components. A certain news story may trigger a subset of these words. The words are represented by TERMINAL-nodes in the last layer. We also embed the contextual relations between the \u201dwho\u201d, \u201dwhere\u201d and \u201dwhat\u201d components in the AOG. They are described using information from two aspects: \u2022 the co-occurrences of words from different compo-\nnents (such as the co-occurring pairs marked by the dashed red lines in Fig. 2); \u2022 the ratios of entity numbers of different components (e.g. some topics have more people involved compared to the related locations).\nImage Representation. The image part of each topic is also represented by an AND-node as shown in Fig. 2 (node Aimgk ). This node has two components, which capture two important visual signals in news: faces and objects. Faces show the main people related to the topic, and objects include other general information about the scene and the event.\nThe face and object components are represented by OR-nodes (nodes Ofacek and O obj k in Fig. 2), which can describe a set of possible entities similar to the \u201dwho\u201d, \u201dwhere\u201d, and \u201dwhat\u201d OR-nodes. Each face/object entity corresponds to one cluster of face/object patches, and we use a TERMINAL-node to represent it in the last layer. We also encode the contextual relations between the face and object components in the AOG using co-occurrences of face-object pairs (such as the co-occurrence of politicians and suits).\nJoint Image-Text Representation. To jointly model the topic text and image parts, we also describe their\ncontextual relations in the AOG. The ratio of the total entity numbers of these two parts (i.e. numbers of all TERMINAL-nodes under the nodes Atxtk and A img k in Fig. 2) is included in the AOG. We also model three component pair relations selected from these two parts, namely face-who, face-what and object-what pairs. The face-who and face-what pairs can clearly relate the faces appeared in the video to their names and other coappearing textual knowledge respectively. The objectwhat pairs can relate the objects to textual descriptions. For these three component pairs, the pair instance frequencies are used to model the contextual relations.\nIn summary, the proposed AOG topic representation jointly models texts and images and their subcomponents in a hierarchical structure. The AOG model strikes a balance between the syntactic representation in NLP (too complex to compute) and the simplistic BoW representation (too coarse). It supports the news topic detection and tracking tasks with the appropriate complexity accurately."}, {"heading": "1.2.2 Task: Detecting and Tracking News Topics", "text": "In the massive and continuously updated news data, each news topic evolves over time. We aim to detect topics within short time periods and further generate long-time topic trajectories. Therefore, we can show both detailed descriptions for each topic in different time periods, and how each topic develops over time. It also helps prevent the heavy computation incurred by periodically detecting topics using the entire updated news collection.\nFor topic detection, we group stories that elaborate the same topics. The proposed AOG explicitly describes components of different topics. Thus based on the AOG, we can effectively group related stories and generate\nmeaningful topics. We solve the grouping problem by cluster sampling methods by maximizing a Bayesian posterior probability. An efficient cluster sampling algorithm introduced in image segmentation, i.e. SwendsenWang Cuts (SWC) [17], is adopted for topic detection.\nFor topic tracking, with the hierarchical AOG model that can represent topic compositions and how such information changes over time, we align news topics in different time periods. This provides us a promising way to track and keep updating the news states. We link topics detected in different short time periods to generate topic trajectories by considering both topic similarities and their temporal relations.\nIn the experiments, we show that our method can generate meaningful topics and topic trajectories. It also achieves better performance compared to state-of-the-art algorithms."}, {"heading": "1.3 Summary of Contributions", "text": "This paper makes the following contributions: \u2022 We proposed a joint image-text compositional news\ntopic representation based on And-Or Graph, which better utilize the multimodal data and provide interpretations with appropriate amount of details compared to single-modal methods and other representations [1], [3], [6], [7], [8], [18], [19]. \u2022 We solve the topic detection problem using the clustering sampling method Swendsen-Wang Cuts, which has better performance than commonly used greedy algorithms [3], [6], [8], [20]. \u2022 We detect and track topics simultaneously over time, generating both topic summaries in different time periods and long-time topic trajectories. The results show how topics evolve over time and provide useful data for further media analysis, which\n4 can hardly be fulfilled by traditional topic detection methods [15], [1].\n\u2022 We collected a news dataset for joint image-text topic detection and tracking, and also provide the ground-truth annotations, which copes with the lack of publicly available multimodal news datasets.\nThe rest of the paper is organized as follows. We first review the related literature in Section 2. Then we present the proposed topic representation in Section 3. The proposed topic detection method is described in Section 4. The topic tracking method is illustrated in Section 5. We report our experiment results and comparisons with other state-of-the-art methods in Section 6 and conclude in Section 7."}, {"heading": "2 RELATED WORK", "text": "Our work is mainly related to the following four research streams: (1) topic modeling, (2) topic clustering, (3) topic tracking, and (4) news gathering and delivering systems."}, {"heading": "2.1 Topic Modeling", "text": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].\nEven though these methods are effective in general topic modeling, they can hardly achieve good performance in the news domain using only the bag-of-words (BoW) representation. The BoW representation is computationally efficient, but it ignores the compositional structures, which are important for news analyses. News stories are generally driven by events, so information from aspects like \u201dwho\u201d, \u201dwhere\u201d and \u201dwhat\u201d is crucial for summarizing these stories and generating meaningful news topics. Newman et al. [28] considered these aspects but included them as a whole. Li et al. [29] used information from the above aspects in their representation. However, they assume that these aspects are independent, which is generally not true in the real news data.\nMoreover, all the aforementioned topic modeling methods are single-modal methods which only use texts. Several multi-modal probabilistic topic models have been proposed for other tasks such as image annotation and classification [30], [31], [32], news geo-location inference [33], etc., but methods for joint image-text topic detection are rare in the literature."}, {"heading": "2.2 Topic Clustering", "text": "Clustering based methods are also widely used for the task of news topic detection. They assume that each news story talks about one news event which corresponds to one topic. A large number of methods for topic detection in the Topic Detection and Tracking (TDT) research [15] (e.g. [1], [34]) use clustering methods for\ndetecting news topics, where stories on the same topic are gathered. Traditional document clustering methods [5], [35] can also be used for topic detection. However, most of these methods are single-modal and mainly focus in the text domain.\nMultimodal topic clustering methods have been proposed by taking both texts and visuals into consideration. In most of these methods, texts are represented using the BoW representation [8], [20], [36], [7]. For visual representation, some methods use color histograms of the keyframes [8]. Other methods detect the nearduplicate keyframes (NDK) first and then use them to build visual relations between news stories [20], [36]. Even though these methods can compute the visual similarities between stories, they are not capable of modeling the visual part decomposition of news topics. In terms of the clustering methods, [8] and [20] used co-clustering algorithm and one of its extensions with constraints added respectively. [7] groups news stories based on the linear combination of textual and visual similarities. [36] detects topics within one multi-modality graph, which is obtained by merging one text graph and another visual graph constructed based on LDA and NDK respectively. These clustering methods are not global optimal.\nSome work also combined topic modeling and document clustering together, such as the multi-grain clustering topic model (MGCTM) proposed by Xie et al. [6]. They showed that these two tasks are closely related and can help each other as both performances are improved. This work still remains in the pure text domain and uses the BoW representation."}, {"heading": "2.3 Topic Tracking", "text": "The traditional topic tracking problem in TDT [15], [1] is defined as the process of finding related additional stories for some pre-learned topics. Many methods have been proposed for solving this problem such as those in [15], [37], [38]. However, deciding the topic of each newcoming story based on the previous learned topics can take a long time in a large data collection. In addition, they can hardly deal with the newly emerging topics.\nIn the probabilistic modeling community, some models incorporate time information, such as the Dynamic Topic Model (DTM)[4] which can model the topic evolution over time. However, it is assumed that the topics exist throughout the whole time period, which is usually not true especially in broadcast news. It also leads to heavy computation for continuously updated new streams.\nThus instead of using the previous two methods, we choose to track the news topics by linking topics detected in different time periods and generating topic trajectories over time. Some linking methods, such as those by Mei and Zhai [39] as well as Kim and Oh [40], are closely related to our topic tracking task. However, the method in [39] is designed for news about some specific topics such as \u201dtsunami\u201d. The similarity matrices used in [40] are\n5 based on the topics obtained by the original LDA model with the BoW assumption. Moreover, both of these two methods are merely based on textual information."}, {"heading": "2.4 News Gathering and Delivering System", "text": "Several news gathering and delivering systems have been presented recently. In [41], [42], Chang et al. presented a system called News Rover to integrate multimodal news sources. News topics are collected from sources such as Google News and organized in a hierarchical topic structure, and news stories are then matched to these topics. Another personalized news video system, EigenNews [43], [44], can aggregate news videos from multiple sources. It matches the extracted news stories to online news articles to get the related news categories. In this paper, we detect and track news topics in a fully unsupervised way by jointly modeling texts and visuals to support the media analyses in social and political science (e.g. the two case studies mentioned in Section 1.1)."}, {"heading": "3 TOPIC REPRESENTATION", "text": "In this section, we define the And-Or graph (AOG) for topic representation."}, {"heading": "3.1 Overall Representation", "text": "An AOG embodies a context sensitive grammar. It can be defined by a three-tuple G = (V,E,\u0398). The node set V consists of three subsets of nodes: AND-nodes VAND, OR-nodes VOR and TERMINAL-nodes VT, i.e. V = VAND \u222a VOR \u222a VT. E denotes the edge set in the graph. \u0398 represents the AOG model parameters. We have \u0398 = {K, \u03b81, ..., \u03b8K} where K is the total topic number, and \u03b81, ..., \u03b8K represent the model parameters for these K topics respectively. Fig. 2 illustrates a small part of the proposed AOG topic representation.\nA parse graph pg is an instantiation of the AOG by selecting children nodes at OR-nodes. The green lines in Fig. 2 shows one example of the parse graph.\nAs shown in Fig. 2, the AOG has five layers. Nodes in each layer are explained as follows:\n1) The root OR-node Or \u2208 VOR in the first layer of the AOG represents different topic configurations and their mutual contextual information. Each topic k (k = 1, ...,K) is represented by an AND-node Atopick in the second layer of the AOG hierarchy with the model parameter \u03b8k.\nNews stories are reports of topics, i.e. topic instances, from various TV news networks. We denote a news story by di. For a story di, the scoring function at the root ORnode Or is defined as:\nscoreroot(di; \u0398) = max \u03b8k\u2208\u0398 scoretopic(di; \u03b8k), (1)\nwhere scoretopic(di; \u03b8k) is the scoring function at A topic k , which will be introduced later.\n2) The topic AND-node Atopick \u2208 VAND in the second layer of the hierarchy (as shown in Fig. 2) represents one topic configuration. One topic is composed of the text part and the image part. So Atopick has two children AND-nodes, i.e. the text AND-node Atxtk and the image AND-node Aimgk . The scoring function at A topic k is defined as:\nscoretopic(di; \u03b8k) = score txt(dtxti ; \u03b8k) + score img(dimgi ; \u03b8k)\n+scorejoint(djointi ; \u03b8k) + g(fAtopick ),\n(2) where dtxti , d img i and d joint i denote the text part, the image part and their joint information of the story di respectively (di = dtxti \u222a d img i \u222a d joint i ). The two terms scoretxt(dtxti ; \u03b8k) and score img(dimgi ; \u03b8k) are the scoring functions at Atxtk and A img k respectively. The term scorejoint(djointi ; \u03b8k) describes the contextual relations between the text part and the image part. These three terms will be explained later. The function g(fAtopick ) describes the prior of choosing Atopick at root node O\nr. We have the branching frequency fAtopick \u2208 \u03b8k. We observed that in the broadcast news, dominant topics with a large amount of coverage only constitute a small portion of the whole corpus, and the sizes of most topics are small. Accordingly, we assume that the branching frequencies at Or follow a power law distribution3 (the verification of our observation will be shown in Section 3.5)."}, {"heading": "3.2 Text Representation", "text": "For a news story di, its text part dtxti contains the \u201dwho\u201d component dwoi , the \u201dwhere\u201d component d wr i , and the \u201dwhat\u201d component dwti . We extract words for different components by performing the name entity extraction using the Stanford Named Entity Recognizer [45]. Thus each of the three components can be represented by a list of words (word duplication is allowed in the list), e.g. dwoi = (w1, ..., wMwoi ) where M wo i is the total number of words in the \u201dwho\u201d component in the story di. The total numbers of words in the \u201dwhere\u201d and \u201dwhat\u201d components are denoted by Mwri and M wt i respectively.\nWe extract the co-occurring word pairs from the three components in the text part. We consider a pair of words as one co-occurring pair if the two words belong to two different components, and are extracted from the same sentence. The list of co-occurring word pairs in the story di is denoted by dtti = [(w1, w2)|w1 \u2208 dwoi , w2 \u2208 dwri ] \u222a [(w1, w2)|w1 \u2208 dwoi , w2 \u2208 dwti ] \u222a [(w1, w2)|w1 \u2208 dwri , w2 \u2208 dwti ].\nThe text AND-node Atxtk in the third layer of the AOG hierarchy has three children OR-nodes, i.e. Owok , O wr k , and Owtk , which represent the \u201dwho\u201d, \u201dwhere\u201d and \u201dwhat\u201d components in the text part of topic k respectively. The\n3. In the experiments, for the function g(\u00b7), we use the Zipf\u2019s law probability distribution, i.e. g(f) = f \u2212s\n\u03b6(s) and set the parameter s that\ndescribes the distribution\u2019s exponent as s = 1.75 (\u03b6 is the Riemann Zeta function).\n6 scoring function at Atxtk (i.e. the term score txt(dtxti ; \u03b8k) in Eq. 2) is defined as:\nscoretxt(dtxti ; \u03b8k) = \u2211 c scorecomp(dci ; \u03b8k)\n+scorett(dtti ; \u03b8k),\n(3)\nwhere the variable c represents the component type c \u2208 {wo,wr,wt}. The term scorecomp(dci ; \u03b8k) represents the scoring function at the OR-node for one component Ock \u2208 {Owok , Owrk , Owtk }. The term scorett(dtti ; \u03b8k) describes the contextual relations between the three components in the text part and we define it as:\nscorett(dtti ; \u03b8k) = \u2211 c1,c2 G( M c1i M c2i ;\u00b5c1c2k , \u03c3 c1c2 k )\n+ \u2211\n(w1,w2)\u2208dtt log(f\n(w1,w2) k + 1),\n(4)\nwhere we have the component types c1 \u2208 {wo,wr}, c2 \u2208 {wr,wt}, c1 6= c2. M c1 i\nM c2 i represents the ratio of word numbers from two different components. The three ratios, namely M wo i\nMwri , M\nwo i\nMwti , and M\nwr i\nMwti are assumed to follow Gaus-\nsian distributions. \u00b5c1c2k , \u03c3 c1c2 k \u2208 \u03b8k are the parameters for the corresponding Gaussian distributions. The verification of our assumption will be discussed in Section 3.5. The parameter f (w1,w2)k \u2208 \u03b8k is the frequency of the cooccurring word pair (w1, w2) in topic k.\nThe three children OR-nodes of Atxtk in the fourth layer, namely Owok , O wr k , and O wt k , describe a set of possible words for the corresponding components. The words are represented by TERMINAL-nodes in the last layer. The scoring functions at these OR-nodes are defined as:\nscorecomp(dci ; \u03b8k) = \u2211 w\u2208dci log(fwk + 1) (5)\nfor component type c \u2208 {wo,wr,wt}. The parameter fwk \u2208 \u03b8k represents the frequency of word w in topic k."}, {"heading": "3.3 Image Representation", "text": "The story\u2019s image part dimgi contains the face component dfacei , and the object component d obj i , i.e. d face i ,d obj i \u2208 dimgi . Each entity in the face/object component corresponds to one cluster of face/object patches. To obtain the face component, we first perform face detection and extract face features based on Local Binary Pattern [46] and Local Gabor Binary Pattern Histogram Sequence [47], and then use the k-means algorithm to cluster the faces into groups. To get the object component, we first extract patches from images using Selective Search [48] which can generate possible object locations. We then get the patch features using Caffe [49], an open-source implementation of the deep convolutional network that is trained on over a million images annotated with 1,000 ImageNet [50] classes. Then the k-means algorithm is used to cluster the patches into groups. Fig. 3 illustrates\npatch cluster patch clusterface cluster\nFigure 3: Illustration of how one image can be parsed based on face and object clusters.\nhow one image can be parsed based on the obtained face and object clusters. Each face/object patch can be represented by its corresponding cluster membership. Then the face and object components of one story di can also be represented by a list of visual words, e.g. dfacei = (w1, ..., wM facei ) where each word wj \u2208 d face i represent one face patch\u2019s cluster membership. M facei is the total number of face patches in dimgi and the total number of object patches is denoted by Mobji .\nWe extract the co-occurring word pairs from the face and object components of the image part. A pair of visual words is considered as one co-occurring pair if the two words are from the face and object components respectively, and they both appear in one short time period in the news video. We denote the list of cooccurring pairs extracted from the image part by diii = [(w1, w2)|w1 \u2208 dfacei , w2 \u2208 d obj i ].\nThe image AND-node Aimgk in the third layer of the AOG represents the image part of the topic k. It has two children OR-nodes, i.e. Ofacek and O obj k , which represent the face and object information respectively. The scoring function at Aimgk is defined in a similar way to the one at Atxtk . We have:\nscoreimg(dimgi ; \u03b8k) = \u2211 c scorecomp(dci ; \u03b8k)\n+scoreii(diii ; \u03b8k),\n(6)\nwhere the component type c \u2208 {face, obj}. The term scorecomp(dci ; \u03b8k) represents the scoring function at the OR-node for one component Ock \u2208 {Ofacek , O obj k }.\nThe term scoreii(diii ; \u03b8k) describes the contextual relations between the face and object components and we define it as:\nscoreii(diii ; \u03b8k) = \u2211\n(w1,w2)\u2208dii log(f\n(w1,w2) k + 1), (7)\nwhere f (w1,w2)k \u2208 \u03b8k is the frequency for the co-occurring visual word pair (w1, w2) in the topic k.\n7 The two children OR-nodes of Aimgk in the fourth layer, namely Ofacek , and O obj k , can describe a set of alternative visual words. These words are represented by TERMINAL-nodes in the last layer. The scoring functions at these OR-nodes, i.e. scorecomp(dci ; \u03b8k), c \u2208 {face, obj}, are defined in the same way as those at Owok , Owrk and O wt k (Eq. 5)."}, {"heading": "3.4 Joint Textual and Visual Information Representation", "text": "To jointly model the textual and visual information, we extract the co-occurring word pairs from the story\u2019s image and text parts. Three kinds of pairs, namely the face-who, face-what, and object-what pairs, are obtained for each news story di. The words in each cooccurring pair appear in one short time period. These text-image co-occurring word pairs are denoted by djointi = [(w1, w2)|w1 \u2208 dwoi , w2 \u2208 dfacei ] \u222a [(w1, w2)|w1 \u2208 dwti , w2 \u2208 dfacei ] \u222a [(w1, w2)|w1 \u2208 dwti , w2 \u2208 d obj i ].\nThe term scorejoint(djointi ; \u03b8k) in Eq. 2 describes the contextual relations between the text part and the image part. We use M texti and M img i to denote the total entity numbers of the text part and the image part respectively. So we have M texti = M wo i + M wr i + M wt i , and M img i = M facei + M obj i . The score function score\njoint(djointi ; \u03b8k) is defined as:\nscorejoint(djointi ; \u03b8k) = G( M texti\nM imgi ;\u00b5jointk , \u03c3 joint k )\n+ \u2211\n(w1,w2)\u2208djoint log(f\n(w1,w2) k + 1).\n(8)\nWe assume that the ratio between the total entity numbers of the text part and the image part, i.e. M text i\nM imgi , follows\nGaussian distribution with the parameters \u00b5jointk , \u03c3 joint k \u2208 \u03b8k. The verification of our assumption will be shown in Section 3.5. The parameter f (w1,w2)k \u2208 \u03b8k is the frequency of the word pair (w1, w2) \u2208 djoint in topic k.\nBased on the previous scoring functions, we can find the optimal parse graph pg\u2217i for the story di by calculating scoreroot(di; \u0398)."}, {"heading": "3.5 Empirical Evaluations of Assumptions in AOG", "text": "In the AOG representation, we make some assumptions of the distribution of the branching frequencies at Or and the ratios between different components. To verify our assumptions, we collect a news corpus that contains news data during a period of seven days. There are 1,853 news stories in the corpus. Annotators are asked to group the stories according to their topics. After annotation, we got 355 topics in total.\nTo verify the assumption that the branching frequencies at Or follow the power law distribution, using the collected corpus, we fit the empirical distribution of the story numbers in the topics (i.e. the topic branching frequency) to the power law distribution. The p-value\n10 \u22121\n10 0\n10 1\n10 2\n10 3\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nStory Number of One Topic\nF re\nqu en\ncy\nFigure 4: Empirical histogram of the topic\u2019s story number and the fitting result (the red curve).\n\u221210 \u22125 0 5 10 15 20 0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\nValue\nF re\nqu en\ncy (\nno rm\nal iz\ned )\n(a)\n\u22120.1 0 0.1 0.2 0.3 0\n2\n4\n6\n8\n10\n12\n14\n16\n18\nValue\nF re\nqu en\ncy (\nno rm\nal iz\ned )\n(b)\n\u22120.05 0 0.05 0.1 0.15 0\n10\n20\n30\n40\n50\n60\n70\n80\nValue\nF re\nqu en\ncy (\nno rm\nal iz\ned )\n(c)\n\u22120.05 0 0.05 0.1 0.15 0\n5\n10\n15\n20\n25\n30\nValue\nF re\nqu en\ncy (\nno rm\nal iz\ned )\n(d)\nFigure 5: Empirical histograms of story entity number ratios of different parts/components from one randomly selected topic (the histograms are normalized by area). Red curves show the fitted distributions. (a) Ratios of the who and where components. (b) Ratios of the who and what components. (c) Ratios of the where and what components. (d) Ratios of the text and image parts.\n(at the 5% significance level) is 0.9984. Fig. 4 shows the empirical distribution and the fitted curve (red line).\nWe assume that the ratios between the word numbers of the \u201dwho\u201d, \u201dwhere\u201d and \u201dwhat\u201d components in the text part, i.e. M wo i\nMwri , M\nwo i\nMwti , and M\nwr i\nMwti , follow Gaus-\nsian distributions. To verify the assumption, we do the Kolmogorov-Smirnov (KS) test for each ratio to test the goodness of how the data samples fit the Gaussian distribution. For the test, we delete the topics whose story numbers are less than 10. After this, there are 41 remaining topics. For the above three ratios, about 51.22%, 70.73%, and 82.93% of the topics pass the test, and the average p-values (at the 5% significance level) are 0.2089, 0.3761, and 0.4658 respectively. We show histograms of these three ratios for one randomly selected topic in Fig. 5a, 5b and 5c respectively.\nWe assume that the ratio between the total word numbers of the text part and the image part, i.e. (\n\u2211 c1\u2208{wo,wr,wt} M c1 i )/( \u2211 c2\u2208{face,obj} M c2 i ) follows a Gaussian distribution. To verify the assumption, we do the Kolmogorov-Smirnov (KS) test on the 41 remaining large topics to test the goodness of how the data samples fit the Gaussian distribution. About 78.05% of the topics pass the test, and the average of the p-values (at the 5% significance level) is 0.4243. The ratio histogram of one randomly selected topic is shown in Fig. 5d.\n8 vi vj vk\nFigure 6: One adjacency graph. Each vertex in the graph corresponds to one news story. The edges connect the neighboring vertices and are associated with weights corresponding to the story similarities (the edge thickness shows the story similarities). The vertices vi and vj both talk about the Oklahoma tornado topic and they are adjacent to each other in the graph. The other vertex vk which is far away from vi and vj in the graph corresponds to the story about the California High-Speed Rail project."}, {"heading": "4 TOPIC DETECTION", "text": "In this section, we present our formulation of the topic detection problem, and the algorithm for optimizing a Bayesian posterior probability for the topic detection problem."}, {"heading": "4.1 Problem Formulation", "text": "With the hierarchical AOG topic representation, our goal of topic detection is to cluster news stories that describe the same topics and obtain the AOG model parameters \u0398 for the topics. We pose this clustering problem as a graph partitioning problem in which news stories, as vertices in the adjacency graph, are partitioned into coherent groups. We show one example of the adjacency graph in Fig. 6. Edges in the adjacency graph are associated with certain weights corresponding to related story similarities. Partitions can be obtained by dividing the vertices into groups with specific properties and also keeping the number of edges between separated components small. Graph partitioning can help the news topic detection since even though news stories from one topic develop over time and drift the topic, they can still be grouped together through the connections between temporally adjacent stories with less changes and more similarities.\nFormally, we are given a news story corpus that contains N news stories, i.e. D = {di; i = 1, . . . , N}. The adjacency graph is defined as GADJ = (VADJ, EADJ) where VADJ is a set of vertices and each vertex vi \u2208 VADJ corresponds to one news story di. EADJ is a set of\nedges between vertices. The clustering/partition W we are trying to find given D is defined as:\nW = (K,\u03c0K ,\u0398), (9)\nwhere K is determined automatically while solving the partitioning problem and \u03c0K represents the K\u2212partition of the adjacency graph. \u03c0K is defined as:\n\u03c0K = (V1, ..., VK), \u22c3K\nk=1 Vk = VADJ, Vk \u2229 Vj = \u2205,\u2200i 6= j.\n(10) This becomes an optimization problem which can be solved by maximizing a Bayesian posterior probability:\nW \u2217 = arg max W\u2208\u2126 p(W |D) = arg max W\u2208\u2126 p(D|W )p(W ). (11)\nThe likelihood probability p(D|W ) is formulated as:\np(D|W ) = N\u220f i=1 p(di; \u0398) \u221d exp{ \u2211 di\u2208D scoreroot(di; \u0398)}.\n(12) The prior probability p(W ) penalizes the partition number K in W and we formulate it as:\np(W ) \u221d exp{\u2212\u03b1NK}. (13)\n\u03b1 is a positive parameter which acts as a threshold for grouping stories into topics. This prior helps us combine close partitions to get dense results."}, {"heading": "4.2 Inference by Swendsen-Wang Cuts", "text": "For the topic detection problem formulated above, we adopt a cluster sampling method Swendsen-Wang Cuts (SWC) [17]. It is a Markov Chain Monte Carlo method which can sample the solution space \u2126 efficiently. An alternative method will be the expectation-maximization (EM) algorithm. But in [51], SWC is shown to be more effective than EM which finds only a local minimum.\nSWC changes the labels of a group of vertices at the same time. It thus solves the coupling problem of Gibbs sampler (which flips a single vertex) by quickly jumping between local minima. SWC starts with an initial partition \u03c0, which can be the one which sets all stories to be in the same group, or can be set randomly. We denote the set of edges whose related two vertices belong to the same group under the partition \u03c0 by E(\u03c0). The optimal clustering W \u2217 can be obtained by performing the following steps iteratively until convergence.\n(1) Determining edge status. Each edge e =< vi, vj >\u2208 E(\u03c0) is associated with a Bernoulli random variable ue \u2208 {0, 1} which indicates the edge\u2019s on/off status and a turn-on probability qe. We define:\nqe = e \u2212D(e)/T , (14)\nwhere T is the temperature factor and D(e) is the distance of these two vertices obtained based on the Kullback-Leibler (KL) divergence:\nD(e) = \u2211 F\u2208F \u03bbF \u00b7 KL(F (vi)||F (vj)) + KL(F (vj)||F (vi)) 2 ,\n(15)\n9 X X\nX X X X\nV0\nCl\n(a) Before flipping.\nX Ci\nV0\n(b) After flipping.\nFigure 7: SWC flips the selected component V0. The cuts are marked with crosses.\nwhere F (\u00b7) denotes one type of feature of the vertex and \u03bbF is the weight for feature F . Here we use the distributions for the five components in the text and image parts (i.e. who, where, what, face and object) to construct the feature set F . Moreover, since KL divergence is nonsymmetric, we average the KL divergence of F (vi) given F (vj) and the KL divergence of F (vj) given F (vi) to get a symmetric distance measure for vertices vi and vj . Based on these definitions, in this step, we set ue = 0 (i.e, turn e off) with probability 1\u2212 qe for all e \u2208 E(\u03c0).\n(2) Computing connected components. Once the states ue is determined for each edge e \u2208 E(\u03c0), the graph G is partitioned into a set of connected components, each of which contains vertices that belong to the same group.\n(3) Selecting a component and flipping it. Among all the connected components formed in (2), we can randomly select one component V0 to flip. We show one example of V0 in Fig. 7a. The target label for V0 can be a new one that has not been used yet or just the same as any other connected components, thus allowing reversible jumps in the solution space. The current partition number is denoted as K \u2032. Then the number of possible new labels for the selected component is K \u2032+1. Assuming that V0 \u2286 Vl in the current partition \u03c0, we denote a series of sets\nS1 = V1, S2 = V2, ..., Sl = Vl\\V0, SK\u2032 = VK\u2032 , SK\u2032+1 = \u2205 (16) that V0 can be merged with. The selected component V0 can be flipped by drawing a random sample l\u2032 with probability\np(l\u2032(V0) = i|V0, \u03c0) = \u03b3ip(\u03c0i|D)\u2211K\u2032+1\nj=1 \u03b3jp(\u03c0j |D) , (17)\nwhere \u03c0i is the partition after assigning the label of the component V0 to be i and keeping other components\u2019 labels the same as in \u03c0. We also have\n\u03b3i = \u220f e\u2208Ci (1\u2212 qe), (18)\nwhere Ci is the cuts between V0 and Si, i.e. Ci = C(V0, Si) = {< s, t >: s \u2208 V0, t \u2208 Si}. Two examples of the cuts are shown in Fig. 7, which are marked by the\ncrosses. Theorem 3 in [17] proved that the acceptance rate will be 1 by choosing the new label of V0 by Eq. 17.\nAnother thing to be noted here is that when generating the adjacency graph, we can use a complete graph of N vertices since each pair of news stories can be related. But this may cause problems since a complete graph of N vertices has ( N 2 ) = O(N2) edges and the number of all possible solutions is exponential in the the number of edges, i.e. O(2N 2\n), which requires a long convergence time. By investigating the data, however, one may observe that some story pairs have few similarities in terms of contents. Such pair of stories shall never be grouped together. Hence, graph pruning can be performed before actually running the SWC on the adjacency graph. We define a threshold \u03c4 , and cut all edges e whose D(e) \u2265 \u03c4 deterministically.\nSimulated annealing procedure is conducted in the optimization process. The temperature T in the annealing procedure is slowly decreased according to a cooling schedule."}, {"heading": "5 TOPIC TRACKING", "text": "In this section, we describe our method for tracking a variable number of topics detected in certain continuous time periods. In contrast to traditional topic tracking problem [15] where the topic to be tracked is provided and the task is to determine whether new-coming stories belong to the given topic, we instead link all the detected topics in different time periods to form topic trajectories over time. Then based on the trajectories, we can do further analysis such as the analysis of sentiment/emotion changes as the topic evolves.\nWe divide the whole news data collection into several sub-collections which consist of news stories in different time periods. Topic detection is performed within each sub-collection separately. The sub-collection set of the news corpus D is denoted by {C1, ..., CM} where C1\u222a...\u222aCM = D and M is the number of sub-collections. Each sub-collection contains news documents from one specific time span ti. Topics extracted within each subcollection Ci are denoted by \u0398i = {\u03981i , ...,\u0398 Ki i }, where Ki is the obtained topic number. For topic tracking, we link topics detected in the subcollections. One optional method for solving the linking problem is to do another clustering on the detected topics using SWC. But to fast obtain the topic links, we choose to measure the similarities between topics by considering both the topic content similarities and their temporal distances, and use a threshold to decide whether they can be linked. Topic content similarity is defined based on the proposed hierarchical topic representation which models both the textual and visual information. Formally, in the tracking process, the similarity measurement to decide whether two topics can be linked is calculated as:\nSim(\u0398k1i1 ,\u0398 k2 i2 ) = \u03b1sim exp{\u2212\u03b2kl[KL(\u0398k1i1 ||\u0398 k2 i2 )\n+KL(\u0398k2i2 ||\u0398 k1 i1 )]}+ (1\u2212 \u03b1sim) exp{\u2212|ti1 \u2212 ti2 |}, (19)\n10\nwhere i1 6= i2, and \u03b1sim and \u03b2kl are positive parameters. Note that using the proposed topic representation, each topic is composed of the image part and the text part, and they can be further divided into the \u201dwho\u201d, \u201dwhere\u201d and \u201dwhat\u201d components, and the face and object components respectively. Thus we have five components in total. Each component is represented using one model. The KL divergence of one topic given another is therefore averaged over these models:\nKL(\u0398k1i1 ||\u0398 k2 i2 ) = 5\u2211 j=1 \u03bbjKL(\u0398k1,jm1 ||\u0398 k2,j i2 ), (20)\nwhere \u03bbj is the corresponding weight for different parts. \u0398k1,ji1 and \u0398 k2,j m2 are the histograms of word frequencies for the j-th component. After calculating the topic similarities using Eq. 19, a threshold \u03c4link can be used for pruning the links between topics to get the final topic trajectories."}, {"heading": "6 EXPERIMENTS", "text": ""}, {"heading": "6.1 Datasets", "text": "Two datasets are used in our experiment:\n1) Reuters-21578. Reuters-21578 dataset4 is a publicly available collection of news stories from Reuters newswire. It is widely used for the evaluation of clustering and classification methods. The dataset contains 21,758 stories which belong to 135 clusters/categories. The clusters/categories are annotated manually. Only textual information is contained in the dataset.\n2) UCLA Broadcast News Dataset. We collected a multimedia broadcast news dataset from UCLA Library Broadcast NewsScape. Five US networks are included in the dataset: CNN, MSNBC, FOX, ABC, and CBS. It contains 379 news videos broadcasted in the time period from June 1, 2013 to June 14, 2013. The total length of the videos is about 362 hours. Several programs from each news network are included in the dataset, such as \u201dCNN Newsroom\u201d, \u201dCNN Situation Room\u201d, \u201dMSNBC News Live\u201d, \u201dFOX Morning News\u201d, \u201dABC Eyewitness News\u201d, \u201dABC Nightline\u201d, \u201dCBS News\u201d, etc.\nAnnotation: We annotate the UCLA Broadcast News Dataset for topic detection and tracking. One annotation choice can be letting the annotators manually group the new stories based on their related topics [8], [1]. However, this will be a hard task and the results may not be accurate since there can be hundreds of news topics even in one week and the annotators can hardly remember all the previously found topics during annotation. So instead of this, we choose to build the ground-truth by letting annotators decide whether a pair of stories belong to the same topic or not. The topic granularity is chosen to be at the event level, like the definition in the TDT system [1]. In other words, two stories talking about the same event (or closely related ones) belong to the same\n4. Reuters-21578 dataset can be downloaded at http://www. daviddlewis.com/resources/testcollections/reuters21578/.\ntopic. Since it takes a long time to annotate all story pairs (about N2 pairs for N stories), we choose to annotate a subset selected from the whole story pair collection. We first compute the cosine distances between the two stories in the same pair, and then select 10,000 story pairs to be annotated randomly from the pair set where all pair distances are within the range [0.6, 0.9]. This specific range is chosen for the reason that the corresponding story relations are ambiguous compared to other ranges. Three annotators are involved in the annotation and for each story pair we treat the relation that most annotators agree as the ground-truth relation.\nThis dataset is mainly used for quantitative evaluation of our method. To show how our method work on largescale datasets qualitatively, we also apply our method to more news data from the UCLA Library Broadcast NewsScape."}, {"heading": "6.2 Implementation Details", "text": "The implementation details (including the preprocessing procedures) for the two datasets used in the experiment are described below:\n1) Reuters-21578: In the experiment, stories with multiple cluster labels are discarded and for the remaining stories, only those from the largest 10 clusters are selected [6].\n2) UCLA Broadcast News Dataset: We utilize texts from both video frames and closed captions (CC). Text extraction on video frames is performed using optical character recognition (OCR) based on Google OCR engine Tesseract [52], and the results are further refined using the spatial-temporal relations between frames. News CC consist of several stories in one single continuous text stream. Story segmentation needs to be performed to divide the CC into stories. In CC, some special markers are used as the indicators of story boundaries, such as \u201d>>>\u201d. Moreover, many news programs insert commercials between stories with special formats of letter cases and indentations. Thus we also do commercial detection based on these special formats. Using the special boundary markers and commercial detection results, most stories boundaries are determined. For the remaining boundaries, we train a classifier using Support Vector Machine to decide the boundary locations based on features including the boundary key words (such as \u201dcoming up\u201d, \u201dstill ahead\u201d), and similarities of sentences near the boundaries.\nFor the news videos, we extract the keyframes by removing the commercial frames, redundant frames and anchor frames. Commercial frames can be specified using the aforementioned commercial detection results from CC. Redundant frames are those perceived to be similar to the previous frames. We use the frame histograms to decide whether one incoming frame is similar to the previously detected non-redundant frame. After removing the commercial and redundant frames, we further detect anchor frames among the remaining\n11\nframes. Anchor frames are those containing the news anchors. They usually appear repeatedly in the video. We detect the anchor frames by exploiting features from two aspects: anchor frames\u2019 backgrounds (they usually show the news studios and thus are similar in the videos), and anchors\u2019 faces. Similar backgrounds and faces are grouped by clustering. We can then check the clusters\u2019 time distribution and decide whether the corresponding frames are anchor frames or not.\nFig. 8 illustrates the results that can be obtained in the previous preprocessing procedure. After preprocessing, we got 3,633 news stories including 577,721 words and 36,810 keyframes. The whole collection contains 24,036 unique word terms."}, {"heading": "6.3 Experiment I: Topic Detection", "text": "In this experiment, we conduct topic detection experiments on both the Reuters-21578 dataset and the UCLA Broadcast News Dataset."}, {"heading": "6.3.1 Results on Reuters-21578", "text": "We compare the proposed topic detection method with other story/document clustering methods on the news dataset Reuters-21578. Only texts are used in the comparison.\nEvaluation Protocol. On Reuters-21578, we follow the evaluation protocol in [6], [53]. Two metrics are used to evaluate the clustering performance, i.e. accuracy, and normalized mutual information. To compute the accuracy, the obtained clusters are mapped to the groundtruth clusters in the dataset. The accuracy is then defined as the percentage of the documents that have the correct cluster labels after mapping. The mutual information\nmeasures the mutual dependence of the ground-truth cluster assignments and the obtained clustering assignments for the documents. The normalized mutual information is a normalized version of this measure. More details of the definitions of these two metrics can be found in [53].\nBaseline Methods. Several baseline methods are included in the comparison, namely:\n\u2022 K-means and Normalized Cuts (NC) [54], which are widely used clustering and graph partitioning algorithms. \u2022 Nonnegative-Matrix-Factorization (NMF) based clustering [55], Latent Semantic Indexing (LSI) [21], and Locally Consistent Concept Factorization (LCCF) [53], which are factorization based methods that are very effective in document clustering. \u2022 LDA related methods: 1) LDA + K-means [6]: using LDA to learn the topics and the topic distribution for each document, and then clustering using K-Means based on these distributions; 2) LDA + Naive [6]: using LDA to learn the topics and the documents\u2019 topic distributions, and then treating the label of the most dominant topic as the cluster label for each document. \u2022 Multi-grain clustering topic model (MGCTM) [6] which integrates document clustering and topic modeling. It has the best clustering result on Reuters-21578 so far.\nThe inputs of these methods in the comparison are the documents\u2019 tf-idf vectors [6], [53]. Stop words are removed from the documents. These methods all require the cluster number to be specified in the input. Thus for these methods, we set the cluster number K = 10 in the experiment, which equals the ground-truth cluster number in the dataset. Please refer to [6] for other detailed settings of these algorithms.\nParameter Settings of Our Method. To compare with the baseline algorithms, in our method, we add a Gaussian prior term with the mean \u00b5 = 10 and variance \u03c32 = 0.5 to Eq. 13 to make the sampling process converge to the state where the cluster number equals 10. The parameter \u03b1 in Eq. 13 is set as \u03b1 = 0.2. The weights {\u03bbF , F \u2208 F} in Eq. 15 are set as: \u03bbFwho = 0.1, \u03bbFwhere = 0.1, \u03bbFwhat = 0.4, \u03bbFface = 0.1 and \u03bbFobject = 0.3. The threshold \u03c4 used for graph pruning is set as \u03c4 = 160.\nComparison Results. Table 1 shows the comparison results of different methods on Reuters-21578. It can be seen from the results that our approach is better than the other methods in terms of both the clustering accuracy and the normalized mutual information. This is because our method uses the AOG representation which organizes the information in a hierarchical way and embeds the contexts between different components. The cluster sampling method SWC also plays an important role in getting the optimal solution. Other methods generally use the basic word distributions and most of the solutions they get are not global optimal.\n12"}, {"heading": "6.3.2 Results on UCLA Broadcast News Dataset", "text": "We conduct both qualitative and quantitative evaluations of our topic detection method on the UCLA Broadcast News Dataset. In the qualitative experiment, we show the topics detected by our method. In the quantitative experiment, we compare our method\u2019s topic detection performance with other algorithms.\n1) Qualitative Evaluation. We conduct the joint image-text topic detection experiment on the whole dataset, i.e. clustering using both textual and visual information from all news stories in the dataset.\nParameter Settings. The parameter \u03b1 in Eq. 13 is set as \u03b1 = 10. The cluster numbers for grouping the faces and object patches in Section 3 are set as 1, 000 and 1, 500 respectively. We also delete clusters with a small number of patches. The remaining cluster numbers for face and object are 708 and 1, 316 respectively. Other parameter settings are the same as those in Section 6.3.1.\nTopic Detection Results. We show the detected top five topics in Fig. 10. Topic 1 talks about the news that Edward Snowden leaked information from National Security Agency (NSA). Topic 2 is about the IRS scandal,\nincluding the discussion on the misuse of taxpayers\u2019 money and the related hearing. Topic 3 mainly talks about the Oklahoma tornado, including its development, the damage it caused, and the storm chasers\u2019 stories. Topic 4 is about the wildfires, which also includes the fire development and the related damages. Topic 5 is about the Santa Monica College shooting rampage, and the related gunman and victims\u2019 stories are also included. We can see from the figure that the obtained structured results can clearly describe the related topics. The involved people\u2019s names and face patches, the related locations, the key objects, the descriptions about the event, as well as the co-occurrence relations between them (represented by the dashed red lines) are all shown in the structure.\n2) Quantitative Evaluation. We also conduct quantitative evaluation on the proposed joint image-text topic detection method.\nEvaluation Protocol. Using the annotated story pairs, we draw precision-recall curves for different topic detection methods in the evaluation. The precision is calculated as the fraction of story pairs that actually belong to one topic out of those that are computed to be. The recall is the fraction of story pairs that are computed to belong to one topic out of those that actually do.\nBaseline Methods. Among the baseline methods used in 6.3.1, we select some methods with better performance, including LDA + Naive and MGCTM. We also include the widely used k-means algorithm. These algorithms are all single-modal, so their inputs in the experiment are the stories\u2019 textual information, i.e. the stories\u2019 tf-idf vectors. Multi-modal baseline methods are also included in the comparison, including the multimodal coclustering method in [8], and the multi-modality graph with topic recovery method (MMG+TR) in [36]. For these method, we set a sequence of cluster numbers in the experiment to generate the precision-recall curves.\nParameter Settings of our method. To generate the precision-recall curve, we change the parameter \u03b1K in Eq. 13 for our method. Other parameter settings are the same as those in the qualitative experiment. To compare with the single-modal methods, we also conduct experiments where only the text information is included.\nComparison Results. Fig. 9 shows the precision-recall curves for different methods. As we can see from the figure, based on merely text information, our method has better performance than the other single-modal methods. This shows that the proposed hierarchical AOG topic representation and the clustering sampling method we use can help generate better topics. With the visual information added, our performance gets further improved, showing the effectiveness of our method which jointly models the text and visual information."}, {"heading": "6.4 Experiment II: Topic Tracking", "text": "In this experiment, we conduct topic tracking experiments on the UCLA Broadcast News Dataset. Both\n13\n14\nqualitative and quantitative evaluations of our method are included in the experiment.\n1) Qualitative Evaluation. To show that our topic tracking method can generate meaningful topic trajectories, we conduct the qualitative evaluation experiment.\nParameter Settings. To track topics over time, we divide the whole collection of news stories in the UCLA Broadcast News Dataset into 14 sub-collections each of which contains news stories from one day. Topic detection is firstly performed within each sub-collection. Then given the detected topics, we further do topic tracking, which links topics over time and generate topic trajectories. The parameter \u03b1sim and \u03b2kl in Eq. 19 are set as \u03b1sim = 0.8 and \u03b2kl = 0.005 respectively. The weights {\u03bbi; i = 1, ..., 5} in Eq. 20 are set as {0.1, 0.1, 0.4, 0.1, 0.3}. The threshold \u03c4link for selecting links between topics is set as \u03c4link = 0.7.\nTopic Tracking Results. One topic tracking trajectory about the Santa Monica College shooting is shown in Fig. 11. In the figure each circle corresponds to one detected topic. The topics are summarized in several words here for space constraints. The size of the circle is proportional to the topic size. For the links between topics, thicker ones means greater similarities between topics. The descriptions of the text part and the image part for the corresponding topics in the trajectory are shown in Fig. 12 and Fig. 13 respectively. The probabilities of the top textual and visual words over time are shown in the figure.\nBased on the tracking result, we also analyze the emotional changes as the topic developed. The NRC Emotion Lexicon [56] is used for the emotion analysis. Three emotional variables, i.e. fear, anger and sadness, are included in the analysis and the ternary plot on these variables is shown in Fig. 14. From these figures we can see that at the beginning, when the shooting happened, news stories mainly describe the shooting scenario and expressed people\u2019s fear mostly. Later when the suspect was found, more anger is shown in the news stories. When victims\u2019 stories were told later, sadness\n15\nFear\n2013-06-07\n2013-06-08\n2013-06-09\nSanta Monica Gun Shooting\nbecame dominant. From these results, we can clearly see how news media reported the event and what emotions they want to express. They also show that our tracking method can generate meaningful tracking trajectories.\n2) Quantitative Evaluation. We also conduct quantitative evaluation on the proposed multimodal topic tracking method.\nEvaluation Protocol. For topic tracking, we also use the precision-recall curves to compare different methods. The annotated story pairs are used as the ground-truth data.\nBaseline Methods. We include two baseline methods in the comparison, namely: \u2022 Dynamic topic model (DTM) [4] which models topic\nchanges over time. \u2022 Topic chain method [40] which generates topics in\ndifferent time periods using LDA and links these topics to form topic chains.\nThese two methods are both single-modal. For DTM, we set different topic numbers in the experiment to generate its precision-recall curve. For the topic chain method, we set the topic number in each time period as 50 and use a sequence of similarity threshold when building the topic chains.\nParameter Settings of our method. To generate the precision-recall curve, we also change the parameter \u03c4link in the experiment.\nComparison Results. Fig. 15 shows the precisionrecall curves for our tracking method and the two baseline methods. Our method outperforms the baseline methods since both textual and visual information are\nincluded in the tracking process. Moreover, our topic detection method can generate meaningful topics, which is also an important factor for the topic tracking performance."}, {"heading": "6.5 Experiment III: Large-Scale Topic Detection and Tracking", "text": "To show that our method can work effectively on largescale datasets, we generate a long topic trajectory using the whole year\u2019s news data in 2012 from the network CNN. The obtained trajectory is shown in Fig. 16. Due to the space limit we only show the text part of the topics in the trajectory. The who, where and when parts of the topic are separated by the symbol \u201d||\u201d in the figure. The top part of the figure shows the trajectory of George Zimmerman\u2019s case and some other related shooting cases such as the Chardon shooting in February and the Colorado theater shooting in July. The middle part of the figure is mainly about topics closely related to the 2012 US election, such as the health care, the immigration problem, the economy and the debates. The Syria problem, which is another factor related to the election, is shown in the bottom part of the figure. We also get some short trajectories such as the one about\n16\nOlympic shown in the lower half part of the figure. From these trajectories, we can clearly see how these topics develop over time and how they can relate to each other.\nWe also made a case study based on our method, which tracks the 2016 U.S. presidential election. The large-scale topic detection and tracking results are visualized in the Viz2016 website (mentioned in Section 1.1)."}, {"heading": "7 CONCLUSIONS", "text": "We have presented a joint image-text news topic detection and tracking method. We use the And-Or graph as a structured topic representation, which models the image and text parts of topics jointly. We detect topics using the SWC-based cluster sampling method. Topics are also tracked over time to deal with the continuous updates of news streams. Qualitative and quantitative evaluation results both show the effectiveness and efficiency of the proposed topic detection and tracking method.\nIn the future, we want to expand our study to the problem of media analysis for social and political science research. Based on our topic detection and tracking results, we can analyze how media are biased for different topics, what is the agenda-setting pattern, what is the causal relations between topics, etc."}, {"heading": "ACKNOWLEDGMENT", "text": "This project is supported by the NSF CDI project CNS 1028381. The authors would like to thank Dr. Francis Steen and Tim Groeling at UCLA, and Dr. Chengxiang Zhai at UIUC for discussions and insightful suggestions. We would also like to thank Dr. Quanshi Zhang and Tianfu Wu at UCLA for their assistance."}], "references": [{"title": "Topic detection and tracking pilot study: Final report", "author": ["J. Allan", "J. Carbonell", "G. Doddington", "J. Yamron", "Y. Yang"], "venue": "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, 1998, pp. 194\u2013218.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Commun. ACM, vol. 55, no. 4, pp. 77\u201384, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "ICML, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "A survey of text clustering algorithms", "author": ["C. Aggarwal", "C. Zhai"], "venue": "Mining Text Data. Springer US, 2012, pp. 77\u2013128.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Integrating document clustering and topic modeling", "author": ["P. Xie", "E.P. Xing"], "venue": "UAI, 2013, pp. 694\u2013703.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Tracking news stories across different sources", "author": ["Y. Zhai", "M. Shah"], "venue": "MM, 2005, pp. 2\u201310.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Threading and autodocumenting news videos: a promising solution to rapidly browse news topics", "author": ["X. Wu", "C.-W. Ngo", "Q. Li"], "venue": "IEEE Signal Processing Magazine, vol. 23, no. 2, pp. 59\u201368, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Slanted objectivity? perceived media bias, cable news exposure, and political attitudes", "author": ["J.S. Morris"], "venue": "Social Science Quarterly, vol. 88, no. 3, pp. 707\u2013728, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Who\u2019s the fairest of them all? an empirical test for partisan bias on abc, cbs, nbc, and fox news", "author": ["T. Groeling"], "venue": "Presidential Studies Quarterly, vol. 38, no. 4, pp. 631\u2013657, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Automated facial trait judgment and election outcome prediction: Social dimensions of face", "author": ["J. Joo", "F. Steen", "S.-C. Zhu"], "venue": "ICCV, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual persuasion: Inferring communicative intents of images", "author": ["J. Joo", "W. Li", "F. Steen", "S.-C. Zhu"], "venue": "CVPR, 2014, pp. 216\u2013223.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "happy warriors\u201d: Leaders\u2019 facial displays, viewers\u2019 emotions, and political support", "author": ["R.D.M. Denis G. Sullivan"], "venue": "American Journal of Political Science, vol. 32, no. 2, pp. 345\u2013368, 1988.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1988}, {"title": "Articles media and marijuana: A longitudinal analysis of news media effects on adolescents\u2019 marijuana use and related outcomes, 1977-1999", "author": ["J.E. STRYKER"], "venue": "Journal of Health Communication, vol. 8, no. 4, pp. 305\u2013328, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Topic Detection and Tracking: Event-based Information Organization", "author": ["J. Allan"], "venue": "Norwell, MA, USA: Kluwer Academic Publishers,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Evaluation campaigns and trecvid", "author": ["A.F. Smeaton", "P. Over", "W. Kraaij"], "venue": "Proceedings of the 8th ACM International Workshop on Multimedia Information Retrieval. ACM Press, 2006, pp. 321\u2013330.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalizing swendsen-wang to sampling arbitrary posterior probabilities", "author": ["A. Barbu", "S.-C. Zhu"], "venue": "TPAMI, vol. 27, no. 8, pp. 1239\u20131253, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Integrating topics and syntax", "author": ["T.L. Griffiths", "M. Steyvers", "D.M. Blei", "J.B. Tenenbaum"], "venue": "NIPS, 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Syntactic topic models", "author": ["J. Boyd-graber", "D. Blei"], "venue": "NIPS, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimodal news story clustering with pairwise visual near-duplicate constraint", "author": ["X. Wu", "C.-W. Ngo", "A. Hauptmann"], "venue": "TMM, vol. 10, no. 2, pp. 188\u2013199, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "SIGIR, 1999, pp. 50\u201357.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "PNAS, vol. 101, pp. 5228\u20135235, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "JASA, vol. 101, no. 476, pp. 1566\u20131581, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["C. Wang", "D.M. Blei"], "venue": "SIGKDD, 2011, pp. 448\u2013456.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A correlated topic model of science", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "Annals of Applied Statistics, vol. 1, no. 1, pp. 17\u201335, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Topic modeling: Beyond bag-of-words", "author": ["H.M. Wallach"], "venue": "ICML, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Topics over time: A non-markov continuous-time model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "SIGKDD, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Statistical entitytopic models", "author": ["D. Newman", "C. Chemudugunta", "P. Smyth"], "venue": "SIGKDD, 2006, pp. 680\u2013686.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "A probabilistic model for retrospective news event detection", "author": ["Z. Li", "B. Wang", "M. Li", "W.-Y. Ma"], "venue": "SIGIR, 2005, pp. 106\u2013113.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Topic regression multi-modal latent dirichlet allocation for image annotation", "author": ["D. Putthividhy", "H. Attias", "S. Nagarajan"], "venue": " CVPR, 2010, pp. 3408\u20133415.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised relational  18 topic model for weakly annotated image recognition in social media", "author": ["Z. Niu", "G. Hua", "X. Gao", "Q. Tian"], "venue": "CVPR, 2014, pp. 4233\u20134240.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Geo-location inference on news articles via multimodal plsa", "author": ["Y. Zhou", "J. Luo"], "venue": "MM, 2012, pp. 741\u2013744.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Topic tracking in a news stream", "author": ["J.P. Yamron", "I. Carp", "L. Gillick", "S. Lowe", "P.V. Mulbregt"], "venue": "In Proceedings of DARPA Broadcast News Workshop, 1999, pp. 133\u2013136.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1999}, {"title": "A comparison of document clustering techniques", "author": ["M. Steinbach", "G. Karypis", "V. Kumar"], "venue": "In KDD Workshop on Text Mining, 2000.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2000}, {"title": "Effective multi-modality fusion framework for cross-media topic detection", "author": ["L. Chu", "Y. Zhang", "G. Li", "S. Wang", "W. Zhang", "Q. Huang"], "venue": "TCSVT, vol. PP, no. 99, pp. 1\u20131, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple semantics in topic detection and tracking", "author": ["J. Makkonen", "H. Ahonen-Myka", "M. Salmenkivi"], "venue": "Inf. Retr., vol. 7, no. 3-4, pp. 347\u2013368, 2004.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "Topic tracking across broadcast news videos with visual duplicates and semantic concepts", "author": ["W. Hsu", "S.-F. Chang"], "venue": "ICIP, 2006.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "Discovering evolutionary theme patterns from text: An exploration of temporal text mining", "author": ["Q. Mei", "C. Zhai"], "venue": "SIGKDD, 2005.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Topic chains for understanding a news corpus", "author": ["D. Kim", "A. Oh"], "venue": "International Conference on Computational Linguistics and Intelligent Text Processing, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Structured exploration of who, what, when, and where in heterogeneous multimedia news sources", "author": ["B. Jou", "H. Li", "J.G. Ellis", "D. Morozoff-Abegauz", "S.-F. Chang"], "venue": "MM, 2013.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "News rover: Exploring topical structures and serendipity in heterogeneous multimedia news", "author": ["H. Li", "B. Jou", "J.G. Ellis", "D. Morozoff", "S.-F. Chang"], "venue": "MM, 2013.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Eigennews: a personalized news video delivery platform", "author": ["M.C. Yu", "P. Vajda", "D.M. Chen", "S.S. Tsai", "M. Daneshi", "A.F. Araujo", "H. Chen", "B. Girod"], "venue": "MM, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Eigennews: Generating and delivering personalized news video", "author": ["M. Daneshi", "P. Vajda", "D. Chen", "S. Tsai", "M. Yu", "A. Araujo", "H. Chen", "B. Girod"], "venue": "ICMEW, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Incorporating nonlocal information into information extraction systems by gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "ACL, 2005, pp. 363\u2013370.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2005}, {"title": "Face description with local binary patterns: Application to face recognition", "author": ["T. Ahonen", "A. Hadid", "M. Pietikainen"], "venue": "TPAMI, vol. 28, no. 12, pp. 2037\u20132041, 2006.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2006}, {"title": "Local gabor binary pattern histogram sequence (lgbphs): a novel nonstatistical model for face representation and recognition", "author": ["W. Zhang", "S. Shan", "W. Gao", "X. Chen", "H. Zhang"], "venue": "ICCV, vol. 1, 2005, pp. 786\u2013791.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Selective search for object recognition", "author": ["J.R.R. Uijlings", "K.E.A. van de Sande", "T. Gevers", "A.W.M. Smeulders"], "venue": "IJCV, vol. 104, no. 2, pp. 154\u2013171, 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV, pp. 1\u201342, 2015.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Mapping energy landscapes of non-convex learning problems", "author": ["M. Pavlovskaia", "K. Tu", "S.-C. Zhu"], "venue": "arXiv:1410.0576, 2014.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Tesseract-ocr", "author": ["Google"], "venue": "available online at https://code.google. com/p/tesseract-ocr/.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 0}, {"title": "Locally consistent concept factorization for document clustering", "author": ["D. Cai", "X. He", "J. Han"], "venue": "TKDE, vol. 23, no. 6, pp. 902\u2013913, 2011.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "TPAMI, vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2000}, {"title": "Document clustering based on nonnegative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "SIGIR, 2003, pp. 267\u2013273.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2003}, {"title": "Crowdsourcing a wordemotion association lexicon", "author": ["S.M. Mohammad", "P.D. Turney"], "venue": "Computational Intelligence, vol. 29, no. 3, pp. 436\u2013465, 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "However, most of the traditional topic detection methods [1], [2], [3], [4], [5], [6] are singlemodal and use texts only.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "However, most of the traditional topic detection methods [1], [2], [3], [4], [5], [6] are singlemodal and use texts only.", "startOffset": 62, "endOffset": 65}, {"referenceID": 2, "context": "However, most of the traditional topic detection methods [1], [2], [3], [4], [5], [6] are singlemodal and use texts only.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "However, most of the traditional topic detection methods [1], [2], [3], [4], [5], [6] are singlemodal and use texts only.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "However, most of the traditional topic detection methods [1], [2], [3], [4], [5], [6] are singlemodal and use texts only.", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "However, most of the traditional topic detection methods [1], [2], [3], [4], [5], [6] are singlemodal and use texts only.", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "using the bag-of-words representation) on which most studies in the current literature concentrate [3], [6], [7], [8], they request accurate and finegrained information in their analyses.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "using the bag-of-words representation) on which most studies in the current literature concentrate [3], [6], [7], [8], they request accurate and finegrained information in their analyses.", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "using the bag-of-words representation) on which most studies in the current literature concentrate [3], [6], [7], [8], they request accurate and finegrained information in their analyses.", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "using the bag-of-words representation) on which most studies in the current literature concentrate [3], [6], [7], [8], they request accurate and finegrained information in their analyses.", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "presidential election topic, the coverage of different candidates by different news networks is often compared [9], [10], which requires the extraction of candidate names from texts.", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "presidential election topic, the coverage of different candidates by different news networks is often compared [9], [10], which requires the extraction of candidate names from texts.", "startOffset": 116, "endOffset": 120}, {"referenceID": 10, "context": "A recent work used face images of candidates to predict the election outcomes [11].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Some other work also considered faces and scenes when studying the visual persuasion in election [12], [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "Some other work also considered faces and scenes when studying the visual persuasion in election [12], [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "This is also demanded by a number of applications such as the causal mechanism analysis [14].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "Traditional topic tracking which is defined as the process of tracking the recurrence of known topics in new incoming stories [15], [1] thus can hardly fulfill this goal.", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "Traditional topic tracking which is defined as the process of tracking the recurrence of known topics in new incoming stories [15], [1] thus can hardly fulfill this goal.", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "[4], can model topic over time but fail to efficiently deal with the updated news data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Even though some multimedia news datasets have been used in previous work, such as the TDT datasets [15], and the TRECVID corpus [16], they are not publicly available, and some of them do not have ground-truth annotations.", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "Even though some multimedia news datasets have been used in previous work, such as the TDT datasets [15], and the TRECVID corpus [16], they are not publicly available, and some of them do not have ground-truth annotations.", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "SwendsenWang Cuts (SWC) [17], is adopted for topic detection.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "\u2022 We proposed a joint image-text compositional news topic representation based on And-Or Graph, which better utilize the multimodal data and provide interpretations with appropriate amount of details compared to single-modal methods and other representations [1], [3], [6], [7], [8], [18], [19].", "startOffset": 259, "endOffset": 262}, {"referenceID": 2, "context": "\u2022 We proposed a joint image-text compositional news topic representation based on And-Or Graph, which better utilize the multimodal data and provide interpretations with appropriate amount of details compared to single-modal methods and other representations [1], [3], [6], [7], [8], [18], [19].", "startOffset": 264, "endOffset": 267}, {"referenceID": 5, "context": "\u2022 We proposed a joint image-text compositional news topic representation based on And-Or Graph, which better utilize the multimodal data and provide interpretations with appropriate amount of details compared to single-modal methods and other representations [1], [3], [6], [7], [8], [18], [19].", "startOffset": 269, "endOffset": 272}, {"referenceID": 6, "context": "\u2022 We proposed a joint image-text compositional news topic representation based on And-Or Graph, which better utilize the multimodal data and provide interpretations with appropriate amount of details compared to single-modal methods and other representations [1], [3], [6], [7], [8], [18], [19].", "startOffset": 274, "endOffset": 277}, {"referenceID": 7, "context": "\u2022 We proposed a joint image-text compositional news topic representation based on And-Or Graph, which better utilize the multimodal data and provide interpretations with appropriate amount of details compared to single-modal methods and other representations [1], [3], [6], [7], [8], [18], [19].", "startOffset": 279, "endOffset": 282}, {"referenceID": 17, "context": "\u2022 We proposed a joint image-text compositional news topic representation based on And-Or Graph, which better utilize the multimodal data and provide interpretations with appropriate amount of details compared to single-modal methods and other representations [1], [3], [6], [7], [8], [18], [19].", "startOffset": 284, "endOffset": 288}, {"referenceID": 18, "context": "\u2022 We proposed a joint image-text compositional news topic representation based on And-Or Graph, which better utilize the multimodal data and provide interpretations with appropriate amount of details compared to single-modal methods and other representations [1], [3], [6], [7], [8], [18], [19].", "startOffset": 290, "endOffset": 294}, {"referenceID": 2, "context": "\u2022 We solve the topic detection problem using the clustering sampling method Swendsen-Wang Cuts, which has better performance than commonly used greedy algorithms [3], [6], [8], [20].", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "\u2022 We solve the topic detection problem using the clustering sampling method Swendsen-Wang Cuts, which has better performance than commonly used greedy algorithms [3], [6], [8], [20].", "startOffset": 167, "endOffset": 170}, {"referenceID": 7, "context": "\u2022 We solve the topic detection problem using the clustering sampling method Swendsen-Wang Cuts, which has better performance than commonly used greedy algorithms [3], [6], [8], [20].", "startOffset": 172, "endOffset": 175}, {"referenceID": 19, "context": "\u2022 We solve the topic detection problem using the clustering sampling method Swendsen-Wang Cuts, which has better performance than commonly used greedy algorithms [3], [6], [8], [20].", "startOffset": 177, "endOffset": 181}, {"referenceID": 14, "context": "can hardly be fulfilled by traditional topic detection methods [15], [1].", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "can hardly be fulfilled by traditional topic detection methods [15], [1].", "startOffset": 69, "endOffset": 72}, {"referenceID": 20, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 210, "endOffset": 213}, {"referenceID": 21, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 215, "endOffset": 219}, {"referenceID": 22, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 239, "endOffset": 243}, {"referenceID": 23, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 245, "endOffset": 249}, {"referenceID": 24, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 251, "endOffset": 255}, {"referenceID": 3, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 257, "endOffset": 260}, {"referenceID": 25, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 262, "endOffset": 266}, {"referenceID": 26, "context": "Among the large number of topic modeling methods, probabilistic topics models [21], [2] have been effectively used for detecting and analyzing latent topics, such as the latent Dirichlet allocation (LDA) model [3], [22] and its extensions [23], [24], [25], [4], [26], [27].", "startOffset": 268, "endOffset": 272}, {"referenceID": 27, "context": "[28] considered these aspects but included them as a whole.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] used information from the above aspects in their representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Several multi-modal probabilistic topic models have been proposed for other tasks such as image annotation and classification [30], [31], [32], news geo-location inference [33], etc.", "startOffset": 132, "endOffset": 136}, {"referenceID": 30, "context": "Several multi-modal probabilistic topic models have been proposed for other tasks such as image annotation and classification [30], [31], [32], news geo-location inference [33], etc.", "startOffset": 138, "endOffset": 142}, {"referenceID": 31, "context": "Several multi-modal probabilistic topic models have been proposed for other tasks such as image annotation and classification [30], [31], [32], news geo-location inference [33], etc.", "startOffset": 172, "endOffset": 176}, {"referenceID": 14, "context": "A large number of methods for topic detection in the Topic Detection and Tracking (TDT) research [15] (e.", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "[1], [34]) use clustering methods for detecting news topics, where stories on the same topic are gathered.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "[1], [34]) use clustering methods for detecting news topics, where stories on the same topic are gathered.", "startOffset": 5, "endOffset": 9}, {"referenceID": 4, "context": "Traditional document clustering methods [5], [35] can also be used for topic detection.", "startOffset": 40, "endOffset": 43}, {"referenceID": 33, "context": "Traditional document clustering methods [5], [35] can also be used for topic detection.", "startOffset": 45, "endOffset": 49}, {"referenceID": 7, "context": "In most of these methods, texts are represented using the BoW representation [8], [20], [36], [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 19, "context": "In most of these methods, texts are represented using the BoW representation [8], [20], [36], [7].", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "In most of these methods, texts are represented using the BoW representation [8], [20], [36], [7].", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "In most of these methods, texts are represented using the BoW representation [8], [20], [36], [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "For visual representation, some methods use color histograms of the keyframes [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "Other methods detect the nearduplicate keyframes (NDK) first and then use them to build visual relations between news stories [20], [36].", "startOffset": 126, "endOffset": 130}, {"referenceID": 34, "context": "Other methods detect the nearduplicate keyframes (NDK) first and then use them to build visual relations between news stories [20], [36].", "startOffset": 132, "endOffset": 136}, {"referenceID": 7, "context": "In terms of the clustering methods, [8] and [20] used co-clustering algorithm and one of its extensions with constraints added respectively.", "startOffset": 36, "endOffset": 39}, {"referenceID": 19, "context": "In terms of the clustering methods, [8] and [20] used co-clustering algorithm and one of its extensions with constraints added respectively.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "[7] groups news stories based on the linear combination of textual and visual similarities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "[36] detects topics within one multi-modality graph, which is obtained by merging one text graph and another visual graph constructed based on LDA and NDK respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "The traditional topic tracking problem in TDT [15], [1] is defined as the process of finding related additional stories for some pre-learned topics.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "The traditional topic tracking problem in TDT [15], [1] is defined as the process of finding related additional stories for some pre-learned topics.", "startOffset": 52, "endOffset": 55}, {"referenceID": 14, "context": "Many methods have been proposed for solving this problem such as those in [15], [37], [38].", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "Many methods have been proposed for solving this problem such as those in [15], [37], [38].", "startOffset": 80, "endOffset": 84}, {"referenceID": 36, "context": "Many methods have been proposed for solving this problem such as those in [15], [37], [38].", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "In the probabilistic modeling community, some models incorporate time information, such as the Dynamic Topic Model (DTM)[4] which can model the topic evolution over time.", "startOffset": 120, "endOffset": 123}, {"referenceID": 37, "context": "Some linking methods, such as those by Mei and Zhai [39] as well as Kim and Oh [40], are closely related to our topic tracking task.", "startOffset": 52, "endOffset": 56}, {"referenceID": 38, "context": "Some linking methods, such as those by Mei and Zhai [39] as well as Kim and Oh [40], are closely related to our topic tracking task.", "startOffset": 79, "endOffset": 83}, {"referenceID": 37, "context": "However, the method in [39] is designed for news about some specific topics such as \u201dtsunami\u201d.", "startOffset": 23, "endOffset": 27}, {"referenceID": 38, "context": "The similarity matrices used in [40] are", "startOffset": 32, "endOffset": 36}, {"referenceID": 39, "context": "In [41], [42], Chang et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 40, "context": "In [41], [42], Chang et al.", "startOffset": 9, "endOffset": 13}, {"referenceID": 41, "context": "Another personalized news video system, EigenNews [43], [44], can aggregate news videos from multiple sources.", "startOffset": 50, "endOffset": 54}, {"referenceID": 42, "context": "Another personalized news video system, EigenNews [43], [44], can aggregate news videos from multiple sources.", "startOffset": 56, "endOffset": 60}, {"referenceID": 43, "context": "We extract words for different components by performing the name entity extraction using the Stanford Named Entity Recognizer [45].", "startOffset": 126, "endOffset": 130}, {"referenceID": 44, "context": "To obtain the face component, we first perform face detection and extract face features based on Local Binary Pattern [46] and Local Gabor Binary Pattern Histogram Sequence [47], and then use the k-means algorithm to cluster the faces into groups.", "startOffset": 118, "endOffset": 122}, {"referenceID": 45, "context": "To obtain the face component, we first perform face detection and extract face features based on Local Binary Pattern [46] and Local Gabor Binary Pattern Histogram Sequence [47], and then use the k-means algorithm to cluster the faces into groups.", "startOffset": 173, "endOffset": 177}, {"referenceID": 46, "context": "To get the object component, we first extract patches from images using Selective Search [48] which can generate possible object locations.", "startOffset": 89, "endOffset": 93}, {"referenceID": 47, "context": "We then get the patch features using Caffe [49], an open-source implementation of the deep convolutional network that is trained on over a million images annotated with 1,000 ImageNet [50] classes.", "startOffset": 43, "endOffset": 47}, {"referenceID": 48, "context": "We then get the patch features using Caffe [49], an open-source implementation of the deep convolutional network that is trained on over a million images annotated with 1,000 ImageNet [50] classes.", "startOffset": 184, "endOffset": 188}, {"referenceID": 16, "context": "For the topic detection problem formulated above, we adopt a cluster sampling method Swendsen-Wang Cuts (SWC) [17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 49, "context": "But in [51], SWC is shown to be more effective than EM which finds only a local minimum.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "Theorem 3 in [17] proved that the acceptance rate will be 1 by choosing the new label of V0 by Eq.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "In contrast to traditional topic tracking problem [15] where the topic to be tracked is provided and the task is to determine whether new-coming stories belong to the given topic, we instead link all the detected topics in different time periods to form topic trajectories over time.", "startOffset": 50, "endOffset": 54}, {"referenceID": 7, "context": "One annotation choice can be letting the annotators manually group the new stories based on their related topics [8], [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "One annotation choice can be letting the annotators manually group the new stories based on their related topics [8], [1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "The topic granularity is chosen to be at the event level, like the definition in the TDT system [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "The implementation details (including the preprocessing procedures) for the two datasets used in the experiment are described below: 1) Reuters-21578: In the experiment, stories with multiple cluster labels are discarded and for the remaining stories, only those from the largest 10 clusters are selected [6].", "startOffset": 305, "endOffset": 308}, {"referenceID": 50, "context": "Text extraction on video frames is performed using optical character recognition (OCR) based on Google OCR engine Tesseract [52], and the results are further refined using the spatial-temporal relations between frames.", "startOffset": 124, "endOffset": 128}, {"referenceID": 5, "context": "On Reuters-21578, we follow the evaluation protocol in [6], [53].", "startOffset": 55, "endOffset": 58}, {"referenceID": 51, "context": "On Reuters-21578, we follow the evaluation protocol in [6], [53].", "startOffset": 60, "endOffset": 64}, {"referenceID": 51, "context": "More details of the definitions of these two metrics can be found in [53].", "startOffset": 69, "endOffset": 73}, {"referenceID": 52, "context": "\u2022 K-means and Normalized Cuts (NC) [54], which are widely used clustering and graph partitioning algorithms.", "startOffset": 35, "endOffset": 39}, {"referenceID": 53, "context": "\u2022 Nonnegative-Matrix-Factorization (NMF) based clustering [55], Latent Semantic Indexing (LSI) [21], and Locally Consistent Concept Factorization (LCCF) [53], which are factorization based methods that are very effective in document clustering.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "\u2022 Nonnegative-Matrix-Factorization (NMF) based clustering [55], Latent Semantic Indexing (LSI) [21], and Locally Consistent Concept Factorization (LCCF) [53], which are factorization based methods that are very effective in document clustering.", "startOffset": 95, "endOffset": 99}, {"referenceID": 51, "context": "\u2022 Nonnegative-Matrix-Factorization (NMF) based clustering [55], Latent Semantic Indexing (LSI) [21], and Locally Consistent Concept Factorization (LCCF) [53], which are factorization based methods that are very effective in document clustering.", "startOffset": 153, "endOffset": 157}, {"referenceID": 5, "context": "\u2022 LDA related methods: 1) LDA + K-means [6]: using LDA to learn the topics and the topic distribution for each document, and then clustering using K-Means based on these distributions; 2) LDA + Naive [6]: using LDA to learn the topics and the documents\u2019 topic distributions, and then treating the label of the most dominant topic as the cluster label for each document.", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "\u2022 LDA related methods: 1) LDA + K-means [6]: using LDA to learn the topics and the topic distribution for each document, and then clustering using K-Means based on these distributions; 2) LDA + Naive [6]: using LDA to learn the topics and the documents\u2019 topic distributions, and then treating the label of the most dominant topic as the cluster label for each document.", "startOffset": 200, "endOffset": 203}, {"referenceID": 5, "context": "\u2022 Multi-grain clustering topic model (MGCTM) [6] which integrates document clustering and topic modeling.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "The inputs of these methods in the comparison are the documents\u2019 tf-idf vectors [6], [53].", "startOffset": 80, "endOffset": 83}, {"referenceID": 51, "context": "The inputs of these methods in the comparison are the documents\u2019 tf-idf vectors [6], [53].", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "Please refer to [6] for other detailed settings of these algorithms.", "startOffset": 16, "endOffset": 19}, {"referenceID": 52, "context": "76 NC [54] 26.", "startOffset": 6, "endOffset": 10}, {"referenceID": 53, "context": "40 NMF [55] 49.", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "89 LSI [21] 42.", "startOffset": 7, "endOffset": 11}, {"referenceID": 51, "context": "14 LCCF [53] 33.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "45 LDA + K-means [6] 29.", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "00 LDA + Naive [6] 54.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "00 MGCTM [6] 56.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "Multi-modal baseline methods are also included in the comparison, including the multimodal coclustering method in [8], and the multi-modality graph with topic recovery method (MMG+TR) in [36].", "startOffset": 114, "endOffset": 117}, {"referenceID": 34, "context": "Multi-modal baseline methods are also included in the comparison, including the multimodal coclustering method in [8], and the multi-modality graph with topic recovery method (MMG+TR) in [36].", "startOffset": 187, "endOffset": 191}, {"referenceID": 54, "context": "The NRC Emotion Lexicon [56] is used for the emotion analysis.", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "\u2022 Dynamic topic model (DTM) [4] which models topic changes over time.", "startOffset": 28, "endOffset": 31}, {"referenceID": 38, "context": "\u2022 Topic chain method [40] which generates topics in different time periods using LDA and links these topics to form topic chains.", "startOffset": 21, "endOffset": 25}], "year": 2015, "abstractText": "In this paper, we aim to develop a method for automatically detecting and tracking topics in broadcast news. We present a hierarchical And-Or graph (AOG) to jointly represent the latent structure of both texts and visuals. The AOG embeds a context sensitive grammar that can describe the hierarchical composition of news topics by semantic elements about people involved, related places and what happened, and model contextual relationships between elements in the hierarchy. We detect news topics through a cluster sampling process which groups stories about closely related events. Swendsen-Wang Cuts (SWC), an effective cluster sampling algorithm, is adopted for traversing the solution space and obtaining optimal clustering solutions by maximizing a Bayesian posterior probability. Topics are tracked to deal with the continuously updated news streams. We generate topic trajectories to show how topics emerge, evolve and disappear over time. The experimental results show that our method can explicitly describe the textual and visual data in news videos and produce meaningful topic trajectories. Our method achieves superior performance compared to state-of-the-art methods on both a public dataset Reuters-21578 and a self-collected dataset named UCLA Broadcast News Dataset.", "creator": "LaTeX with hyperref package"}}}