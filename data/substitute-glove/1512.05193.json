{"id": "1512.05193", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2015", "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs", "abstract": "How hold model it had more sodomy is next critical issue well have greater different processing (NLP) providing such as even schedule (AS ), literal require (PI) has translations clunkiness (TE ). Most remainder to (good) deals however still each steps began fine - switches while furthermore existing; (death) models entire prisoner instructed, taking to the factors but the number sentence; or (r.) contribution be during manually installation, underway - instance linguistic styles. This our mystery entire general Attention Based Convolutional Neural Network (ABCNN) because innovative piece runners among offences. We make including contributions. (happy) ABCNN else without under might a line e.g. for undertaking that require editing addition 18-month metres. (prince) We amendments three given schemes already evolve interests influence between cases into CNN; due, the representation seen up 25-year to into needs on counterpart. These attuned sentence identical mythological are now powerful people inhabited penalty manifestations. (richard) ABCNN versatility legislature - called - for - theater performance friday AS, PI and TE mundane.", "histories": [["v1", "Wed, 16 Dec 2015 14:55:17 GMT  (281kb,D)", "http://arxiv.org/abs/1512.05193v1", "11 pages, 2 figures"], ["v2", "Tue, 29 Dec 2015 10:39:53 GMT  (328kb,D)", "http://arxiv.org/abs/1512.05193v2", "12 pages, 3 figures"], ["v3", "Sat, 9 Apr 2016 11:59:39 GMT  (312kb,D)", "http://arxiv.org/abs/1512.05193v3", "Accepted by TACL, to appear"]], "COMMENTS": "11 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "hinrich sch\\\"utze", "bing xiang", "bowen zhou"], "accepted": true, "id": "1512.05193"}, "pdf": {"name": "1512.05193.pdf", "metadata": {"source": "CRF", "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs", "authors": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "emails": ["wenpeng@cis.lmu.de", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Schu\u0308tze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) and machine translation (Bahdanau et al., 2015).\nMost prior work models each sentence separately, without considering the impact of the other sen-\ntence. This neglects the mutual influence of the two sentences in the context of the task. It also contradicts what humans do when comparing two sentences. We usually focus on key parts of the first sentence by extracting parts from the second sentence that are related by identity, synomymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other.\nFigure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we should focus on. For AS, correctly answering s0 requires putting attention on \u201cgross\u201d: s+1 contains a corresponding element (\u201cearned\u201d) while s\u22121 does not. For PI, focus should be removed from \u201ctoday\u201d to correctly recognize (<s0, s+1 >) as paraphrases and (< s0, s\u22121 >) as non-paraphrases. For TE, we need to focus on either \u201cfull of people\u201d (to recognize TE for < s0, s+1 >) or on \u201coutdoors\u201d / \u201cindoors\u201d (to recognize non-TE for < s0, s\u22121 >). These examples show the need for an architecture that computes different representations of si for dif-\nar X\niv :1\n51 2.\n05 19\n3v 1\n[ cs\n.C L\n] 1\n6 D\nec 2\nferent si\u22121\u2019s (i \u2208 {0, 1}). In this paper, we present such an architecture: ABCNN, an attention-based convolutional neural network that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks.\nSome prior work proposes simple mechanisms that can be interpreted as controlling varying attention; e.g., Yih et al. (2013) employ word alignment to match related parts of the two sentences. In contrast, our attention scheme based on CNN is able to model relatedness between two parts fully automatically. Moreover, attention at multiple levels of granularity, not only at the word level, is achieved as we stack multiple convolution layers that increase abstraction. As far as we know, this is the first NLP paper that incorporates attention into CNNs.\nSection 2 discusses related work. Section 3 introduces BCNN, a network that models two sentences in parallel with shared weights, but without attention. Section 4 presents three different attention mechanisms and their realization in ABCNN, an architecture that is based on BCNN. Section 5 evaluates the models on AS, PI and TE tasks."}, {"heading": "2 Related Work", "text": "There has been a lot of neural network research on modeling sentence pairs for AS, PI and TE. For AS, Yu et al. (2014) present a bigram CNN to model question and answer candidates. Yang et al. (2015) extend this method and get state-of-the-art performance on the the WikiQA dataset (Section 5.2). Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2015) explored bidirectional long shortterm memory (LSTM, Hochreiter and Schmidhuber (1997)) in the same insurance-based dataset. Our approach is different because do not model the sentences by two independent neural networks in parallel, but instead as an interdependent sentence pair, using attention.\nFor PI, Blacoe and Lapata (2012) form sentence representations by summing up word embeddings. Socher et al. (2011) use recursive autoencoder (RAE) to model representations of local\nphrases in sentences, then pool similarity values of phrases from the two sentences as features for binary classification. Yin and Schu\u0308tze (2015a) present a similar model in which RAE is replaced by CNN. In all three papers, each sentence\u2019s representation is not influenced by the other\u2019s \u2013 in contrast to our attention-based model.\nFor TE, Bowman et al. (2015b) employ recursive neural networks to encode entailment on SICK (Marelli et al., 2014b). Rockta\u0308schel et al. (2015) present an attention-based LSTM for the Stanford Natural Language Inference corpus (Bowman et al., 2015a). Our system is the first CNN-based work.\nSome prior work aims to solve a general sentence matching problem. Hu et al. (2014) present two CNN architectures, ARC-I and ARC-II, for sentence matching. ARC-I focuses on sentence representation learning while ARC-II focuses on matching features on phrase level. Both systems were tested on sentence completion (SC), Tweet-Response matching and PI tasks. Yin and Schu\u0308tze (2015b) propose the more flexible MultiGranCNN architecture to model general sentence matching based on phrase matching on multiple levels on granularity and get promising results for PI and SC. Wan et al. (2015) try to match two sentences in AS and SC by multiple sentence representations, each coming from the local representations of two LSTMs. Our work is the first one to investigate attention for the general sentence matching task."}, {"heading": "3 BCNN: Basic Bi-CNN", "text": "We now introduce our basic (non-attention) CNN that is based on Siamese architecture, i.e., it consists of two weight-sharing CNNs, each processing one of the two sentences, and a final layer that solves the sentence pair task. See Figure 2. We refer to this architecture as BCNN. The next section will then introduce an attention architecture that extends BCNN. Table 1 gives our notational conventions.\nIn our implementation and also in the mathematical formalization of the model given below, we pad the two sentences to have the same length s = max(s0, s1). However, in the figures we show different lengths because this gives a better intuition of how the model works.\nBCNN has four types of layers.\nInput layer. In the example in the figure, the two input sentences have 5 and 7 words, respectively. Each word is represented as a d0-dimensional precomputed word2vec (Mikolov et al., 2013) embedding,1 d0 = 300. As a result, each sentence is represented as a feature map of dimension d0 \u00d7 s.\nConvolution layer. Let v1, v2, . . . , vs be the words of a sentence and ci \u2208 Rwd0 , 0 < i < s + w, the concatenated embeddings of vi\u2212w+1, . . . , vi where embeddings for vi, i < 1 and i > s, are set to zero. We then generate the representation pi \u2208 Rd1 for the phrase vi\u2212w+1, . . . , vi using the convolution\n1https://code.google.com/p/word2vec/\nweights W \u2208 Rd1\u00d7wd0 as follows:\npi = tanh(W \u00b7 ci + b) (1)\nwhere b \u2208 Rd1 is the bias. We use wide convolution; i.e., we apply the convolution weights W to words vi, i < 1 and i > s, because this makes sure that each word vi, 1 \u2264 i \u2264 s, can be detected by all weights in W \u2013 as opposed to only the rightmost (resp. leftmost) weights for initial (resp. final) words in narrow convolution.\nAverage pooling layer. Pooling (max, min, average etc) is commonly used to extract robust features from convolution. In this paper, we introduce attention weighting as an alternative, but use average pooling as a baseline as follows. For the output feature map of the last convolution layer, we do column-wise averaging over all columns, denoted as all-ap. This will generate a representation vector for each of the two sentences, shown as the top \u201cAverage pooling (all-ap)\u201d layer below \u201cLogistic regression\u201d in Figure 2. These two representations are then the basis for the sentence pair decision.\nFor the output feature map of non-final convolution layers, we do column-wise averaging over windows of w consecutive columns, denoted as w-ap; shown as the lower \u201cAverage pooling (w-ap)\u201d layer in Figure 2. For filter width w, a non-final convolution layer transforms an input feature map of s columns into a new feature map of s + w \u2212 1 columns; average pooling transforms this back to s columns. This architecture supports stacking an arbitrary number of convolution-pooling blocks to extract increasingly abstract features. Input features to the bottom layer are words, input features to the next layer are short phrases and so on. Each level generates more abstract features of higher granularity.\nOutput layer. The last layer is an output layer, chosen according to the task; e.g., for binary classification tasks, this layer is logistic regression (see Figure 2). Other output layers are introduced below.\nWe found that in most cases, performance is boosted if we provide the output of all pooling layers as input to the output layer. For each non-final average pooling layer, we perform w-ap (pooling over windows of w columns) as described above, but we also perform all-ap (pooling over all columns) and forward the result to the output layer. This\nimproves performance because representations from different layers cover the properties of the sentences at different levels of abstraction and all of these levels can be important for a particular sentence pair."}, {"heading": "4 ABCNN: Attention-Based BCNN", "text": "We introduce three different attention mechanisms for modeling sentence pairs into BCNN; see Figure 3."}, {"heading": "4.1 ABCNN-1", "text": "ABCNN-1 employs an attention feature matrix A to influence convolution. Attention features are intended to weight those units of si more highly in convolution that are relevant to a unit of s1\u2212i (i \u2208 {0, 1}); we use the term \u201cunit\u201d here to refer to words on the lowest level and to phrases on higher levels of the network. Figure 3(a) shows two unit representation feature maps in red: this part of ABCNN-1 is the same as in BCNN (see Figure 2). Each column is the representation of a unit, a word on the lowest level and a phrase on higher levels. We first describe the attention feature matrix A informally (layer \u201cConv input\u201d, middle column, in Figure 3(a)). A is generated by matching units of the left matrix with units of the right matrix such that the attention values of row i in A denote the attention distribution of the i-th unit of s0 with respect to s1 and the attention values of column j in A denote the attention distribution of the j-th unit of s1 with respect to s0. A can be viewed as a new feature map of s0 (resp. s1) in row (resp. column) direction because each row (resp. column) is a new feature vector of a unit in s0 (resp. s1). Thus, it makes sense to combine this new feature map with the representation feature maps and use both as input to convolution. We achieve this by transforming A into the two blue matrices in Figure 3(a) that have the same format as the representation feature maps. As a result, the new input of convolution has two feature maps for each sentence (shown in red and blue). Our motivation is that the attention feature map will guide the convolution to learn \u201ccounterpart-biased\u201d sentence representations.\nMore formally, let Fi,r \u2208 Rd\u00d7s be the representation feature map of sentence i (i \u2208 {0, 1}). Then we\ndefine the attention matrix A \u2208 Rs\u00d7s as follows:\nAi,j = match-score(F0,r[:, i],F1,r[:, j]) (2)\nThe function match-score can be defined in a variety of ways. We found that 1/(1 + |x\u2212 y|) works well where | \u00b7 | is Euclidean distance.\nGiven attention matrix A, we generate the attention feature map Fi,a for si as follows:\nF0,a = W0 \u00b7A\u1d40 (3) F1,a = W1 \u00b7A (4)\nThe weight matrices W0 \u2208 Rd\u00d7s, W1 \u2208 Rd\u00d7s are parameters of the model to be learned in training.2\nWe stack the representation feature map Fi,r and the attention feature map Fi,a as an order 3 tensor and feed it into convolution to generate a higherlevel representation feature map for si (i \u2208 {0, 1}). In Figure 3(a), s0 has has 5 units, s1 has 7. The output of convolution (shown in the top layer, filter width w = 3) is a higher-level representation feature map with 7 columns for s0 and 9 columns for s1."}, {"heading": "4.2 ABCNN-2", "text": "ABCNN-1 computes attention weights directly on the representation with the aim of improving the features computed by convolution. ABCNN-2 instead computes attention weights on the output of convolution with the aim of reweighting this convolution output. In the example shown in Figure 3(b), the feature maps output by convolution for s0 and s1 have 7 and 9 columns, respectively; each column is the representation of a unit. The attention matrix A compares all units in s0 with all units of s1. We sum all attention values for a unit to derive a single attention weight for that unit. This corresponds to summing all values in a row of A for s0 (resulting in the column vector of size 7 shown) and summing all values in a column for s1 (resulting in the row vector of size 9 shown).\nMore formally, let A \u2208 Rs\u00d7s be the attention matrix, a0,j = \u2211 A[j, :] the attention weight of unit j\nin s0, a1,j = \u2211\nA[:, j] the attention weight of unit j in s1 and Fci,r \u2208 Rd\u00d7(si+w\u22121) the output of convolution for si. Then the j-th column of the new\n2The weights of the two matrices are shared in our implementation to reduce the number of parameters of the model.\nfeature map Fpi,r generated by w-ap is derived by:\nFpi,r[:, j] = \u2211\nk=j:j+w\nai,k \u00b7 Fci,r[:, k], j = 1 . . . si\nNotice that Fpi,r \u2208 Rd\u00d7si , i.e., ABCNN-2 pooling generates an output feature map of the same size as the input feature map of convolution. This allows us to stack multiple convolution-plus-pooling blocks to extract features of increasing abstraction.\nThere are three main differences between ABCNN-1 and ABCNN-2. (i) Attention in ABCNN-1 impacts convolution indirectly while attention in ABCNN-2 influences pooling through direct attention weighting. (ii) ABCNN-1 requires the two matrices Wi to convert the attention matrix into attention feature maps; and the input to convolution has two times as many features maps. Thus, ABCNN-1 has more paramaters than ABCNN-2 and is more vulnerable to overfitting. (iii) As pooling appears after convolution, pooling handles larger-granularity units than convolution; e.g., if the input to convolution has word level granularity, then the input to pooling has phrase level granularity, the phrase size being equal to filter size w. Thus, ABCNN-1 and ABCNN-2 implement attention mechanisms for linguistic units of different granularity. This is exactly the motivation for the third ABCNN architecture, ABCNN-3."}, {"heading": "4.3 ABCNN-3", "text": "ABCNN-3 combines ABCNN-1 and ABCNN-2 by stacking them. See Figure 3(c). ABCNN-3 combines the strengths of ABCNN-1 and ABCNN-2 by allowing the attention mechanism to operate (i) on both the convolution and pooling parts of a convolution-plus-pooling block and (ii) on both the input granularity and the more abstract output granularity."}, {"heading": "5 Experiments", "text": "We test the proposed architectures on three tasks: answer selection, paraphrase identification and textual entailment."}, {"heading": "5.1 Common Training Setup", "text": "For all tasks, words are initialized by 300- dimensional word2vec embeddings and not changed\nduring training. A single randomly initialized embedding3 is created for all unknown words by uniform sampling from [-.01,.01]. We employ Adagrad (Duchi et al., 2011) and L2 regularization.\nNetwork configuration. Each network in the experiments below consists of (i) an initialization block b1 that initializes words by word2vec embeddings, (ii) a stack of k \u2212 1 convolution-pooling blocks b2, . . . , bk, computing increasingly abstract features, and (ii) one final LR layer (logistic regression layer) as shown in Figure 2.\nThe input to the LR layer consists of kn features \u2013 each block provides n similarity scores (such as cosine similarity). Figure 2 shows the two sentence vectors output by the final block bk of the stack; this is the basis of the last n similarity scores. As we explained in the final paragraph of Section 3, we perform all-ap pooling for all blocks, not just for bk. Thus we get one sentence representation each for s0 and s1 for each block b1, \u00b7 \u00b7 \u00b7 , bk. We compute n similarity scores for each block (based on the two sentence representations) and forward these kn scores as input to the LR layer.\nDepending on the task, we use different methods for computing the similarity score: see below.\nLayerwise training. In our training regime, we first train a network consisting of just one convpooling block b2. We then create a two-block network, initialize its first block with b2 and train b3 keeping the previously learned weights for b2 fixed. We repeat this procedure until all k \u2212 1 convpooling blocks are trained. We found that this training regime gives us good performance and shortens training times considerably. Since similarity scores of lower blocks are kept unchanged once they have been learned, this also has the nice effect that \u201csimple\u201d similarity scores (those based on surface features) are learned first and subsequent training phases can focus on complementary scores derived from more complex abstract features.\nClassifier. We found that performance increases if we do not use the output of the LR layer as the final decision, but instead train linear SVM or logistic regression with default parameters4 directly on the input to the LR layer (i.e., on the input that is gen-\n3This worked better than discarding unknown words. 4We use http://scikit-learn.org/stable/ for both.\nerated by the k-block stack after network training is completed). Direct training of SVMs/LR seems to get closer to the global optimum than gradient descent training of CNNs.\nTable 2 shows the values of the hyperparameters. Hyperparameters were tuned on dev."}, {"heading": "5.2 Answer Selection", "text": "We use WikiQA,5 an open domain question-answer dataset. We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 questioncandidate pairs in train, 1,130 pairs in dev and 2,352 pairs in test where we adopt the standard setup of only considering questions that have correct answers for evaluation. Following Yang et al. (2015), we truncate answers to 40 tokens.\nThe task is to rank the candidate answers based on their relatedness to the question. Evaluation measures are mean average precision (MAP) and mean reciprocal rank (MRR)."}, {"heading": "5.2.1 Baseline Systems", "text": "We compare with the seven systems considered by Yang et al. (2015): (i) WordCnt: count the number of non-stopwords in the question that also occur in the answer; (ii) WgtWordCnt: reweight the counts by the IDF values of the question words; (iii) LCLR (Yih et al., 2013) makes use of rich lexical semantic features, including word/lemma matching, WordNet (Miller, 1995) and distributional models; (iv) PV: Paragraph Vector (Le and Mikolov, 2014); (v) CNN: bigram convolutional neural network (Yu et al., 2014); (vi) PV-Cnt: combine PV with (i) and (ii); (vii) CNN-Cnt: combine CNN with (i) and (ii)."}, {"heading": "5.2.2 Task-Specific Setup", "text": "We use cosine similarity as the similarity score for this task. In addition, we use sentence lengths, WordCnt and WgtWordCnt. Thus, the final input to the LR layer has size k + 4: one cosine for each of the k blocks and the four additional features."}, {"heading": "5.2.3 Results", "text": "Table 3 shows performance of the baselines, of BCNN and of the three ABCNN architectures. For 5http://aka.ms/WikiQA (Yang et al., 2015)\nCNNs, we test one (one-conv) and two (two-conv) convolution-pooling blocks.\nThe non-attention network BCNN already performs better than the baselines. If we add attention mechanisms, then the performance further improves by several points. Comparing ABCNN-2 with ABCNN-1, we find ABCNN-2 is slightly better even though ABCNN-2 is the simpler architecture. If we combine ABCNN-1 and ABCNN-2 to form ABCNN-3, we get further improvement.6\nThis can be explained by ABCNN-3\u2019s ability to take attention of more fine-grained granularity into consideration in each convolution-pooling block while ABCNN-1 and ABCNN-2 consider attention only at convolution input or only at pooling input, respectively. We also find that stacking two convolution-pooling blocks does not bring consistent improvement and therefore do not test deeper architectures.\nIn summary, the attention mechanism performs better by a large margin on answer selection than previous work that does not use attention. This is evidence that attention is useful for this task.\n6 If we limit the input to LR layer to the k similarity scores in ABCNN-3 (two conv), results are .660 (MAP) / .677 (MRR)."}, {"heading": "5.3 Paraphrase Identification", "text": "We use Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004). The training set contains 2753 true / 1323 false and the test set 1147 true / 578 false paraphrase pairs. We randomly select 400 pairs from train and use them as dev set; but we still report results for training on the entire training set. For each triple (label, s0, s1) in train, we also add (label, s1, s0) to train to make best use of the training data. Systems are evaluated by accuracy and F1."}, {"heading": "5.3.1 Baseline Systems", "text": "We compare our system with top-performing neural network (NN) and non-NN systems. (i) RAE (Socher et al., 2011). Recursive autoencoder that learns representations of phrases in parsing trees, then forwards phrase-phrase similarity scores to classifier. (ii) Bi CNN-MI (Yin and Schu\u0308tze, 2015a). A bi-CNN architecture that detects multigranular phrases, models matching scores between phrase-phrase pairs and employs pretraining. (iii) MPSSM-CNN (He et al., 2015). Like Bi CNNMI, MPSSM-CNN stacks CNNs to extract sentence features at multiple levels of granularity and uses multiple types of pooling, then compares two sentences using multiple similarity metrics. This is the state-of-the-art NN system. (iv) MT (Madnani et al., 2012). MT treats paraphrase relationship as mutual translation and relies mainly on machine translation metrics.7 (v) MF-TF-KLD (Ji and Eisenstein, 2013), the state-of-the-art non-NN system. A matrix fac-\n7For better comparability of approaches in our experiments, we use a simple SVM classifier, which performs slightly worse than Madnani et al. (2012)\u2019s more complex meta-classifier.\ntorization approach in which the distributional features of a sentence include unigrams, higher-order n-grams and dependency pairs. Each feature is reweighted by a TF-KLD weight, similar to TF-IDF."}, {"heading": "5.3.2 Task-Specific Setup", "text": "In this task, we add the 15 MT features from (Madnani et al., 2012) and the lengths of the two sentences. In addition, we compute ROUGE-1, ROUGE-2 and ROUGE-SU4,8 which are scores measuring the match between the two sentences on (i) unigrams, (ii) bigrams and (iii) unigrams and skip-bigrams (maximum skip distance of four), respectively. In this task, we found transforming Euclidean distance into similarity score by 1/(1+ |x\u2212 y|) performs better than cosine similarity. Additionally, we use dynamic pooling (Yin and Schu\u0308tze, 2015a) of the attention matrix and forward pooled values of all blocks to the LR layer. This gives us slightly better performance than only forwarding sentence-level matching features."}, {"heading": "5.3.3 Results", "text": "Table 4 shows that BCNN is slightly worse than the state-of-the-art whereas ABCNN-1 roughly matches it. ABCNN-2 is slightly above the state-ofthe-art. ABCNN-3 outperforms the state-of-the-art clearly in accuracy and F1.9 Two convolution layers only bring small improvements over one.\n8http://www.isi.edu/licensed-sw/see/rouge (Lin, 2004) 9If we run ABCNN-3 (two conv) without the 15+3 \u201clinguistic\u201d features (i.e., MT and ROUGE), performance is 75.1/82.7."}, {"heading": "5.4 Textual Entailment", "text": "SemEval 2014 Task 1 (Marelli et al., 2014a) evaluates system predictions of textual entailment (TE) relations on sentence pairs from the SICK dataset (Marelli et al., 2014b). The three classes are entailment, contradiction and neutral. The sizes of SICK train, dev and test sets are 4439, 495 and 4906 pairs, respectively. We call this dataset ORIG.\nWe also create NONOVER, a copy of ORIG in which the words that occur in both sentences have been removed. A sentence in NONOVER is denoted by the special token <empty> if all words are removed. Table 5 shows three pairs from ORIG and their transformation in NONOVER. We observe that focusing on the non-overlapping parts provides clearer hints for TE than ORIG. In this task, we run two copies of each network, one for ORIG, one for\nNONOVER; these two networks have a single common LR layer.\nFollowing Lai and Hockenmaier (2014), we train our final system (after fixing of hyperparameters) on train and dev (4,934 pairs). Our evaluation measure is accuracy."}, {"heading": "5.4.1 Task-Specific Setup", "text": "We found that for this task forwarding two similarity scores from each block (instead of just one) is helpful. We use cosine similarity and Euclidean distance. As for paraphrase identification, we add the 15 MT features for each sentence pair based on the observation that entailed sentences are more likely to be paraphrase than contradictory sentences.\nWe use the following linguistic features. Negation. Negation obviously is an important feature for detecting contradiction. Feature NEG is set to 1 if either sentence contains \u201cno\u201d, \u201cnot\u201d, \u201cnobody\u201d, \u201cisn\u2019t\u201d and to 0 otherwise.\nNyms. Following Lai and Hockenmaier (2014), we use WordNet to detect synonyms, hypernyms, and antonyms in the pairs. But we do this on NONOVER (not on ORIG) to focus on what is critical for TE. Specifically, feature SYN is the number of word pairs in s0 and s1 that are synonyms. HYP0 (resp. HYP1) is the number of words in s0 (resp. s1) that have a hypernym in s1 (resp. s0). In addition, we collect all potential antonym pairs (PAP) using again NONOVER. We identify the matched chunks that occur in contradictory and neutral, but not in entailed pairs. We exclude synonyms and hypernyms and apply a frequency filter of n = 2. In contrast to (Lai and Hockenmaier, 2014), we constrain the PAP pairs to cosine similarity above 0.4 in word2vec embedding space as this discards many noise pairs. Feature ANT is the number of matched PAP antonyms in a sentence pair.\nLength. As before we use sentence length, both ORIG \u2013 LEN0O and LEN1O \u2013 and and NONOVER lengths: LEN0N and LEN1N.\nOn the whole, we have 24 extra features: 15 MT metrics, NEG, SYN, HYP0, HYP1, ANT, LEN0O, LEN1O, LEN0N and LEN1N."}, {"heading": "5.4.2 Results", "text": "Table 6 shows that our CNNs outperform the top three systems of SemEval. This demonstrates the\npromise of using deep learning for TE. Comparing ABCNN with BCNN, attention mechanism consistently improves performance. ABCNN-1 roughly has comparable performance as ABCNN-2 while ABCNN-3 has bigger improvement: a boost of 1.6 points.10"}, {"heading": "5.5 Summary", "text": "Our experimental results on the tasks of AS, PI and TE show that attention-based CNNs perform better than CNNs without attention mechanisms. ABCNN-2 generally outperforms ABCNN-1 and ABCNN-3 surpasses both. In all tasks, we did not find any big improvement for two-conv over oneconv. This is probably due to the limited size of training data. We expect that, as larger training sets become available, deeper ABCNNs will show their effectiveness.\nPrior work about attention mechanism in neural networks mostly relies on (bidirectional) LSTM. LSTM learns sentence representation with focus on a local word. This kind of sentence representations are used for attention-based system because they are supposed to mainly denote the local word while memorizing the whole context in the mean-\n10If we run ABCNN-3 (two conv) without the 24 linguistic features, the performance is 84.63.\ntime. In this work, we show that representations of local phrases detected by filters in CNN can also achieve attentions. It hints that encoding the whole context to form attention-based local representations is unnecessary. Considering the AS example in Figure 1 again, phrase \u201chow much\u201d in s0 can match phrase \u201c$161.5 million\u201d in s1 very well. Hence, CNN can also act as a good framework for attention mechanism."}, {"heading": "6 Conclusion", "text": "In this work, we presented three mechanisms to integrate attention into convolutional neural network for general sentence pair modeling tasks. Experiments in AS, PI and TE tasks all showed the effectiveness of attention-based CNNs."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "Lapata2012] William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "2015a. A large annotated corpus for learning natural language inference", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Recursive neural networks can learn logical semantics", "author": ["Christopher Potts", "Christopher D Manning"], "venue": "In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC),", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Dolan et al.2004] Bill Dolan", "Chris Quirk", "Chris Brockett"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Applying deep learning to answer selection: A study and an open task. arXiv preprint arXiv:1508.01585", "author": ["Feng et al.2015] Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou"], "venue": null, "citeRegEx": "Feng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["He et al.2015] Hua He", "Kevin Gimpel", "Jimmy Lin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Ji", "Eisenstein2013] Yangfeng Ji", "Jacob Eisenstein"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ji et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2013}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Lai", "Hockenmaier2014] Alice Lai", "Julia Hockenmaier"], "venue": "SemEval", "citeRegEx": "Lai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Text summarization branches out: Proceedings of the ACL-04 workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["Joel Tetreault", "Martin Chodorow"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Madnani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness", "author": ["Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "In Proceedings of LREC,", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Lstm-based deep learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108", "author": ["Tan et al.2015] Ming Tan", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Tan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "A deep architecture for semantic matching with multiple positional sentence representations. arXiv preprint arXiv:1511.08277", "author": ["Wan et al.2015] Shengxian Wan", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Liang Pang", "Xueqi Cheng"], "venue": null, "citeRegEx": "Wan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for opendomain question answering", "author": ["Yang et al.2015] Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih et al.2013] Wen-tau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "Convolutional neural network for paraphrase identification", "author": ["Yin", "Sch\u00fctze2015a] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "2015b. Multigrancnn: An architecture for general matching of text chunks on multiple levels of granularity", "author": ["Yin", "Sch\u00fctze2015b] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection. arXiv preprint arXiv:1412.1632", "author": ["Yu et al.2014] Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Zhao et al.2014] Jiang Zhao", "Tian Tian Zhu", "Man Lan"], "venue": "SemEval", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al.", "startOffset": 101, "endOffset": 137}, {"referenceID": 6, "context": "How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al.", "startOffset": 101, "endOffset": 137}, {"referenceID": 14, "context": ", 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch\u00fctze, 2015a), textual entailment (TE) (Marelli et al.", "startOffset": 40, "endOffset": 86}, {"referenceID": 0, "context": ", 2015a) and machine translation (Bahdanau et al., 2015).", "startOffset": 33, "endOffset": 56}, {"referenceID": 24, "context": ", Yih et al. (2013) employ word alignment to match related parts of the two sentences.", "startOffset": 2, "endOffset": 20}, {"referenceID": 27, "context": "For AS, Yu et al. (2014) present a bigram CNN to model", "startOffset": 8, "endOffset": 25}, {"referenceID": 21, "context": "Yang et al. (2015) extend this method and get state-of-the-art performance on the the WikiQA dataset (Section 5.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2015) explored bidirectional long shortterm memory (LSTM, Hochreiter and Schmidhuber (1997)) in the same insurance-based dataset.", "startOffset": 0, "endOffset": 117}, {"referenceID": 6, "context": "Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2015) explored bidirectional long shortterm memory (LSTM, Hochreiter and Schmidhuber (1997)) in the same insurance-based dataset.", "startOffset": 0, "endOffset": 203}, {"referenceID": 20, "context": "Socher et al. (2011) use recursive autoencoder (RAE) to model representations of local phrases in sentences, then pool similarity values of phrases from the two sentences as features for binary classification.", "startOffset": 0, "endOffset": 21}, {"referenceID": 20, "context": "Socher et al. (2011) use recursive autoencoder (RAE) to model representations of local phrases in sentences, then pool similarity values of phrases from the two sentences as features for binary classification. Yin and Sch\u00fctze (2015a) present a similar model in which RAE is replaced by CNN.", "startOffset": 0, "endOffset": 234}, {"referenceID": 2, "context": "For TE, Bowman et al. (2015b) employ recursive neural networks to encode entailment on SICK (Marelli et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 2, "context": "For TE, Bowman et al. (2015b) employ recursive neural networks to encode entailment on SICK (Marelli et al., 2014b). Rockt\u00e4schel et al. (2015) present an attention-based LSTM for the Stanford Natural Language Inference corpus (Bowman et al.", "startOffset": 8, "endOffset": 143}, {"referenceID": 9, "context": "Hu et al. (2014) present two CNN architectures, ARC-I and ARC-II, for sentence", "startOffset": 0, "endOffset": 17}, {"referenceID": 22, "context": "Wan et al. (2015) try to match two sentences in AS and SC by multi-", "startOffset": 0, "endOffset": 18}, {"referenceID": 17, "context": "Each word is represented as a d0-dimensional precomputed word2vec (Mikolov et al., 2013) embedding,1 d0 = 300.", "startOffset": 66, "endOffset": 88}, {"referenceID": 5, "context": "We employ Adagrad (Duchi et al., 2011) and L2 regularization.", "startOffset": 18, "endOffset": 38}, {"referenceID": 23, "context": "Following Yang et al. (2015), we", "startOffset": 10, "endOffset": 29}, {"referenceID": 23, "context": "We compare with the seven systems considered by Yang et al. (2015): (i) WordCnt: count the number of non-stopwords in the question that also oc-", "startOffset": 48, "endOffset": 67}, {"referenceID": 24, "context": "cur in the answer; (ii) WgtWordCnt: reweight the counts by the IDF values of the question words; (iii) LCLR (Yih et al., 2013) makes use of rich lexical semantic features, including word/lemma matching, WordNet (Miller, 1995) and distributional models;", "startOffset": 108, "endOffset": 126}, {"referenceID": 18, "context": ", 2013) makes use of rich lexical semantic features, including word/lemma matching, WordNet (Miller, 1995) and distributional models;", "startOffset": 92, "endOffset": 106}, {"referenceID": 27, "context": "(iv) PV: Paragraph Vector (Le and Mikolov, 2014); (v) CNN: bigram convolutional neural network (Yu et al., 2014); (vi) PV-Cnt: combine PV with (i) and (ii); (vii) CNN-Cnt: combine CNN with (i) and (ii).", "startOffset": 95, "endOffset": 112}, {"referenceID": 23, "context": "ms/WikiQA (Yang et al., 2015) method MAP MRR", "startOffset": 10, "endOffset": 29}, {"referenceID": 4, "context": "We use Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004).", "startOffset": 51, "endOffset": 71}, {"referenceID": 20, "context": "RAE (Socher et al., 2011).", "startOffset": 4, "endOffset": 25}, {"referenceID": 7, "context": "(iii) MPSSM-CNN (He et al., 2015).", "startOffset": 16, "endOffset": 33}, {"referenceID": 14, "context": "(iv) MT (Madnani et al., 2012).", "startOffset": 8, "endOffset": 30}, {"referenceID": 14, "context": "For better comparability of approaches in our experiments, we use a simple SVM classifier, which performs slightly worse than Madnani et al. (2012)\u2019s more complex meta-classifier.", "startOffset": 126, "endOffset": 148}, {"referenceID": 14, "context": "(Madnani et al., 2012) and the lengths of the two sentences.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "edu/licensed-sw/see/rouge (Lin, 2004) If we run ABCNN-3 (two conv) without the 15+3 \u201clinguistic\u201d features (i.", "startOffset": 26, "endOffset": 37}, {"referenceID": 28, "context": "6 (Zhao et al., 2014) Illinois-LH run1 84.", "startOffset": 2, "endOffset": 21}], "year": 2015, "abstractText": "How to model a pair of sentences is a critical issue in many natural language processing (NLP) tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence separately, without considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNN; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNN achieves state-of-the-art performance on AS, PI and TE tasks.", "creator": "LaTeX with hyperref package"}}}