{"id": "1512.05726", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "Semi-supervised Question Retrieval with Gated Convolutions", "abstract": "Question patiently educational are rapidly growing in size it no interfaces need decided refer to and utilizing allow frankly. In this boxes, let solutions without methodology one given semantically related ways. The task and need came 10) progress pieces for personal are own buried in edits witness following that question body work july) available functionalities are livestock besides fragmented, driven by delegates. We design similar comics combination entire hemorrhaging into double-entry models (gated cross-fertilization) hold temporarily map addresses well take semantic motifs. The used these its - equipped states an transducer - decoder envisaged (now room that title) later once basis means which whole selling upr, them fine - voice discriminatively years service annotations. Our recommendation demonstrates already all model yields 10 \\% modest over without as IR 7-iron, other august \\% over standard pathway network chipset (including CNNs and LSTMs) managed subroutines.", "histories": [["v1", "Thu, 17 Dec 2015 19:14:20 GMT  (75kb,D)", "http://arxiv.org/abs/1512.05726v1", null], ["v2", "Mon, 4 Apr 2016 00:29:15 GMT  (503kb,D)", "http://arxiv.org/abs/1512.05726v2", "NAACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["tao lei", "hrishikesh joshi", "regina barzilay", "tommi s jaakkola", "kateryna tymoshenko", "alessandro moschitti", "llu\u00eds m\u00e0rquez"], "accepted": true, "id": "1512.05726"}, "pdf": {"name": "1512.05726.pdf", "metadata": {"source": "CRF", "title": "Denoising Bodies to Titles: Retrieving Similar Questions with Recurrent Convolutional Models", "authors": ["Tao Lei", "Hrishikesh Joshi", "Regina Barzilay", "Tommi Jaakkola", "Katerina Tymoshenko", "Alessandro Moschitti", "Llu\u0131\u0301s M\u00e0rquez"], "emails": ["tommi}@csail.mit.edu", "tymoshenko@disi.unitn.it", "lmarquez}@qf.org.qa"], "sections": [{"heading": "1 Introduction", "text": "Question answering (QA) forums such as Stack Exchange2 are rapidly expanding and already contain millions of questions. The expanding scope and coverage of these forums also leads to many duplicate and interrelated questions. Several questions pertain to the same underlying issue and are therefore unnecessarily answered multiple times. By identifying related questions, we could effectively reuse existing\n1Our code and data are available at https://github. com/taolei87/rcnn\n2http://stackexchange.com/\nanswers, reducing response times and bloat. Unfortunately, in most forums, the process of identifying and referring to existing similar questions is done manually by forum participants with limited, scattered success.\nThe task of automatically retrieving similar questions to a given user\u2019s question has recently attracted significant attention and has become a testbed for various representation learning approaches (Zhou et al., 2015; dos Santos et al., 2015). However, the task has proven to be quite challenging\u2014for instance, dos Santos et al. (2015) report a 22.3% classification accuracy, yielding only a 4 percent gain over a simple word matching baseline.\nSeveral factors make the problem difficult. First, submitted questions are free-form descriptions, often long and contain extraneous information irrelevant to the main question being asked. For instance, the first question in Figure 1 pertains to booting Ubuntu using a USB stick but a large portion\nar X\niv :1\n51 2.\n05 72\n6v 1\n[ cs\n.C L\n] 1\n7 D\nec 2\nof the body contains tangential details that are idiosyncratic to this user such as references to Compaq pc, Webi and the error message. Not surprisingly, these features are not repeated in the second question in Figure 1 about a closely related topic. The extraneous detail can easily confuse simple wordmatching algorithms. Indeed, for this reason, some existing methods for question retrieval restrict attention to the question title only. While titles (when available) can succinctly summarize the intent, they also sometimes lack crucial detail available in the question body. For example, the title of the second question does not refer to installation from a USB drive. The second main reason for difficulty arises from the available annotations, which are limited and noisy. Indeed, the pairs of questions marked as similar by forum participants are largely incomplete. Our manual inspection of a sample set of questions from AskUbuntu3 shows that only 5% of similar pairs have been annotated by the users, with a precision of around 79%.\nIn this paper, we design a recurrent neural network model and an associated training paradigm to address these challenges. On a high level, our model is used as an encoder to map the title, body, or the combination to a vector representation. The resulting \u201cquestion vector\u201d representation is then compared to other questions via cosine similarity. We introduce several departures from typical architectures on a finer level. In particular, we incorporate adaptive gating in non-consecutive CNNs (Lei et al., 2015) so as to focus temporal averaging in these models on key pieces of the questions. Gating plays a similar role in LSTMs (Hochreiter and Schmidhuber, 1997) though LSTMs do not reach the same level of performance in our setting. Moreover, we counter the scattered annotations available from user-driven associations by training the model largely based on the entire corpus. The encoder is coupled with a decoder and trained to reproduce the title from the noisy question body. The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Rush et al., 2015). The resulting encoder is subsequently fine-tuned\n3http://askubuntu.com/\ndiscriminatively on the basis of limited annotations yielding an additional performance boost.\nWe evaluate our model on the AskUbuntu corpus from Stack Exchange used in prior work (dos Santos et al., 2015). During training, we directly utilize noisy pairs readily available in the forum, but to have a realistic evaluation of the system performance, we manually annotated 8K pairs of questions. This clean data is used in two splits, one for development and hyper parameter tuning and a second for testing. We evaluate our model and the baselines using standard information retrieval (IR) measures such as Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at n (P@n). Our full model achieves a P@1 of 64.5%, yielding 10% absolute improvement over a standard IR baseline, and 6% over standard neural network architectures (including CNNs and LSTMs)."}, {"heading": "2 Related Work", "text": "Given the growing popularity of community QA forums, question retrieval has emerged as an important area of research. Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013). More recent work relies on representation learning to go beyond word-based methods, aiming to capture semantic representations for more refined mappings. For instance, Zhou et al. (2015) learn word embeddings using category-based metadata information for questions. They define each question as a distribution which generates each word (embedding) independently, and use Fisher kernel to assess question similarities. Dos Santos et al. (2015) propose an approach which combines a convolutional neural network (CNN) and a bag-of-words representation for comparing questions. In contrast to (Zhou et al., 2015), our model treats each question as a word sequence as opposed to a bag of words, and we apply a recurrent convolutional model as opposed to the traditional CNN model used by dos Santos et al. (2015) to map questions into meaning representations. Further, we propose a training paradigm that utilizes the entire corpus of unannotated questions in a semi-supervised manner.\nRecent work on answer selection on community QA forums, similar to our task of question retrieval, has also involved the use of neural network architectures (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Similar to our work, these approaches apply neural network techniques, but focus on improving various other aspects of the model. For instance, Feng et al. (2015) explore different similarity measures beyond cosine similarity, and Tan et al. (2015) adopt the neural attention mechanism over RNNs to generate better answer representations given the questions as context."}, {"heading": "3 Question Retrieval Setup", "text": "We begin by introducing the basic discriminative setting for retrieving similar questions. Let q be a query question which generally consists of both a title sentence and a body section. For efficiency reasons, we do not compare q against all the other queries in the data base. Instead, we retrieve first a smaller candidate set of related questionsQ(q) using a standard IR engine, and then we apply the more sophisticated models only to this reduced set. The goal is to rank the candidate questions in Q(q) so that all the similar questions to q are ranked above the dissimilar ones. To do so, we define a similarity score s(q, p; \u03b8) with parameters \u03b8, where the similarity measures how closely candidate p \u2208 Q(q) is related to query q. The method of comparison can make use of the title and body of each question.\nThe scoring function s(\u00b7, \u00b7; \u03b8) can be optimized on the basis of annotated data D = { (qi, p + i , Q \u2212 i ) }\n, where (qi, p+i ) is a correct pair of similar questions and Q\u2212i is a negative set of questions deemed not similar to qi. The candidate set during training is just Q(qi) = {p+i } \u222a Q \u2212 i . The correct pairs of similar questions are obtained from available user annotations, while the negative set Q\u2212i is drawn randomly from the entire corpus with the idea that the likelihood of a positive match is small given the size of the corpus. During testing, we also make use of explicit manual annotations of positive matches.\nIn the purely discriminative setting, we use a maxmargin framework for learning (or fine-tuning) parameters \u03b8. Specifically, in a context of a particular training example where qi is paired with p+i , we\nminimize the max-margin loss L(\u03b8) defined as\nmax p\u2208Q(qi)\n{ s(qi, p; \u03b8)\u2212 s(qi, p+i ; \u03b8) + \u03b4(p, p + i ) } ,\nwhere \u03b4(\u00b7, \u00b7) denotes a non-negative margin. We set \u03b4(p, p+i ) to be a small constant when p 6= p + i and 0 otherwise. The parameters \u03b8 can be optimized through sub-gradients \u2202L/\u2202\u03b8 aggregated over small batches of the training instances.\nThere are two key problems that remain. First, we have to define and parametrize the scoring function s(q, p; \u03b8). We will design a recurrent neural network model for this purpose and use it as an encoder to map each question into its corresponding meaning representation. The resulting similarity function s(q, p; \u03b8) is just the cosine similarity between the corresponding representations. The parameters \u03b8 pertain to the neural network only. The second problem is to pre-train the parameters \u03b8 on the basis of the much larger unannotated corpus so as to offset the scarcity and limited coverage of training annotations. The resulting parameters are subsequently fine-tuned using the discriminative setup described above."}, {"heading": "4 Recurrent Convolutional Networks", "text": "We describe here our encoder model, i.e., the method for mapping the question title and body to a vector representation. Our approach is inspired by temporal convolutional neural networks (LeCun et al., 1998) and, in particular, its recent refinement (Lei et al., 2015), tailored to capture longerrange, non-consecutive patterns in a weighted manner. Such models can be used to effective summarize occurrences of patterns in text and aggregate them into a vector representation. However, the summary produced is not selective since all pattern occurrences are counted, weighted by how cohesive (non-consecutive) they are. In our problem, the question body tends to be very long and full of irrelevant words and fragments. Thus, we believe that interpreting the question body requires a more selective approach to pattern extraction.\nOur model successively reads tokens in the question title or body, denoted as {xi}li=1, and transforms this sequence into a sequence of states {hi}li=1. The resulting state sequence is subsequently aggregated into a single final vector repre-\nsentation for each text as discussed below. Our approach builds on (Lei et al., 2015), thus we begin by briefly outlining it. Let W1 and W2 denote filter matrices (as parameters) for pattern size n = 2. Lei et al. (2015) generate a sequence of states in response to tokens according to\nct\u2032,t = W1xt\u2032 +W2xt ct = \u2211 t\u2032<t \u03bbt\u2212t \u2032\u22121ct\u2032,t\nht = tanh(ct + b)\nwhere \u03bb \u2208 [0, 1) is a constant decay factor used to down-weight patterns with longer spans. The operations can be cast in a \u201crecurrent\u201d manner and evaluated with dynamic programming. The problem with the approach for our purposes is, however, that the weighting is the same for all, not triggered by the state ht\u22121 or the observed token xt.\nWe refine this model by learning context dependent weights. For example, if the current input token provides no relevant information (e.g., stop words, punctuation), the model should ignore it by incorporating the token with a vanishing weight. In contrast, strong semantic content words such as \u201cubuntu\u201d or \u201cwindows\u201d should be included with much larger weights. To achieve this effect we introduce neural gates similar to LSTMs to specify when and how to average the observed signals. The resulting architecture integrates recurrent networks with nonconsecutive convolutional models:\n\u03bbt = \u03c3(W \u03bbxt +U \u03bbht\u22121 + b \u03bb)\nc (1) t = \u03bbt c (1) t\u22121 + (1\u2212 \u03bbt) (W1xt) c (2) t = \u03bbt c (2) t\u22121 + (1\u2212 \u03bbt) (c (1) t\u22121 +W2xt)\n\u00b7 \u00b7 \u00b7\nc (n) t = \u03bbt c (n) t\u22121 + (1\u2212 \u03bbt) (c (n\u22121) t\u22121 +Wnxt)\nht = tanh(c (n) t + b)\nwhere \u03c3(\u00b7) is the sigmoid function and represents the element-wise product. Here c(1)t , \u00b7 \u00b7 \u00b7 , c (n) t are accumulator vectors that store weighted averages of 1-gram to n-gram features. When the gate \u03bbt = 0 (vector) for all t, the model represents a traditional CNN with filter width n. As \u03bbt > 0, however, c (n) t becomes the sum of an exponential number of terms,\nenumerating all possible n-grams within x1, \u00b7 \u00b7 \u00b7 ,xt (seen by expanding the formulas). Note that the gate \u03bbt(\u00b7) is parametrized and responds directly to the previous state and the token in question. We refer to this model as RCNN from here on.\nIn order to use the model as part of the discriminative question retrieval framework outlined earlier, we must condense the state sequence to a single vector. There are two simple alternative pooling strategies that we have explored\u2014either averaging over the states4 or simply taking the last one as the meaning representation. In addition, we apply the encoder to both the question title and body, and the final representation is computed as the average of the two resulting vectors.\nOnce the aggregation is specified, the parameters of the gate and the filter matrices can be learned in a purely discriminative fashion. Given that the available annotations are limited and user-guided, we instead use the discriminative training only for fine tuning an already trained model. The method of pretraining the model on the basis of the entire corpus of questions is discussed next."}, {"heading": "4.1 Pre-training Using the Entire Corpus", "text": "The number of questions in the AskUbuntu corpus far exceeds user annotations of pairs of similar questions. We can make use of this larger raw corpus in two different ways. First, since models take word embeddings as input we can tailor the embeddings to the specific vocabulary and expressions in this corpus. To this end, we run word2vec (Mikolov et al., 2013) on the raw corpus in addition to the Wikipedia dump. Second, and more importantly, we use individual questions as training examples for an auto-encoder constructed by pairing the encoder model (RCNN) with the corresponding decoder. The resulting encoder-decoder architecture is akin to those used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and summarization (Rush et al., 2015).\nOur encoder-decoder pair represents a conditional language model P (title|context), where the context can be any of (a) the original title itself, (b) the ques-\n4We also normalize state vectors before averaging, which empirically gets better performance.\ntion body and (c) the title/body of a similar question. All possible (title, context) pairs are used during training to optimize the likelihood of the words (and their order) in the titles. We use the question title as the target for two reasons. The question body contains more information than the title but also has many irrelevant details. As a result, we can view the title as a distilled summary of the noisy body, and the encoder-decoder model is trained to act as a denoising auto-encoder. Moreover, training a decoder for the title (rather than the body) is also much faster since titles tend to be short (around 10 words).\nThe encoders pre-trained in this manner are subsequently fine-tuned according to the discriminative criterion described already in Section 3."}, {"heading": "5 Alternative models", "text": "In order to ascertain whether RCNNs are necessary for good performance, we also train two alternative benchmark encoders (LSTMs and CNNs) for mapping questions to vector representations. LSTMbased encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our pretrained RCNN.\nLSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2014; Bowman et al., 2015; Rockta\u0308schel et al., 2015). Their success can be attributed to neural gates that adaptively read or discard information to/from internal memory states.\nIn our context, LSTM can be used similar to RCNN. The model successively reads tokens {xi}li=1 constituting the question title or body, and transforms this sequence into states {hi}li=1. Specifically, each recurrent step of the LSTM network takes as input the token xt, internal state ct\u22121, as well as the visible state ht\u22121, and generates the new pair of states ct,ht according to\nit = \u03c3(W ixt +U iht\u22121 + b i)\nft = \u03c3(W fxt +U fht\u22121 + b f )\not = \u03c3(W oxt +U oht\u22121 + b o) zt = tanh(W zxt +U zht\u22121 + b z)\nct = it zt + ft ct\u22121 ht = ot tanh(ct)\nwhere i, f and o are input, forget and output gates, respectively. Given the visible state sequence {hi}li=1, we can aggregate it to a single vector exactly as with RCNNs. The LSTM encoder can be pre-trained in the same way as well.\nCNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang and LeCun, 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation and associated filters map local chunks (windows) of the input into a feature representation. Concretely, if we let n denote the filter width, and W1, \u00b7 \u00b7 \u00b7 ,Wn the corresponding filter matrices, then the convolution operation is applied to each window of n consecutive words as follows:\nct = W1xt\u2212n+1 +W2xt\u2212n+2 + \u00b7 \u00b7 \u00b7+Wnxt ht = tanh(ct + b)\nThe sets of output state vectors {ht} produced in this case are typically referred to as feature maps. Since each vector in the feature map only pertains to local information, the last vector is not sufficient to capture the meaning of the entire sequence. Instead, we consider max-pooling or average-pooling to obtain the aggregate representation for the entire sequence."}, {"heading": "6 Experimental Setup", "text": "Dataset We use the Stack Exchange AskUbuntu dataset used in prior work (dos Santos et al., 2015). This dataset contains 167,765 unique questions, each consisting of a title and a body, and a set of user-marked similar question pairs. We provide various statistics based on this dataset in Table 1.\nTask Setup and Annotations User-marked similar question pairs on QA sites are often known to be incomplete. In order to evaluate this in our dataset, we took a sample set of questions paired\nwith 20 candidate questions retrieved by a search engine trained on the Ubuntu data. The search engine used the well-known BM25 model (Robertson and Zaragoza, 2009). Our manual evaluation of the candidates showed that only 5% of similar questions were also marked by users with a precision of 79%. Clearly, this low recall would not lead to a realistic evaluation if we used user marks as our gold standard. Thus, we needed manual annotations for the dev and test sets. Unfortunately, annotating all pairs (hundreds of thousands) is too costly also because this task requires experts of the domain and consequently it is not suitable for Mechanical Turk-based approaches. For this reason, we formulated the problem as a re-ranking task of the first 20 most similar questions retrieved by the BM25 model. This choice is rather reasonable as, in a realworld scenarios, the user would like to watch just a short list of similar questions.\nTraining Set Given the small size of the development set, we decide to exploit the user-marked questions to build a large weakly-supervised training set. We used such marked questions as positive examples since the user marks have high precision. However, the low recall does produce noise in the negative examples. Our approach to mitigate such problem was to randomly sample 20 negative candidates for each query question qi from all the Ubuntu questions (after removing the dev and test questions). This data was used for training our neural networks. In contrast, our strong baseline based on SVM achieved better accuracy when selecting the negative examples from the first 20 candidates provided by BM25, which of course were not marked\nas similar questions by the users.\nDev and Test Sets We re-constructed the new dev and test sets consisting of the first 200 questions from the dev and test sets provided by (dos Santos et al., 2015). For each of the above questions, we retrieved the top 20 similar candidates using BM25 trained on Ubuntu and manually annotated the resulting 8K pairs as similar or non-similar5.\nBaselines and Evaluation Metrics We evaluated neural network models including CNNs, LSTMs and RCNNs by comparing them with the following baselines:\n\u2022 BM25, we used the BM25 similarity measure provided by Apache Lucene.\n\u2022 TF-IDF, we ranked questions using cosine similarity based on a vector-based word representation for each question.\n\u2022 SVM, we trained a re-ranker using SVM-Light (Joachims, 2002) with a linear kernel incorporating various similarity measures from the DKPro similarity package (Ba\u0308r et al., 2013). This model has been shown to provide the state of the art in sentence similarity challenges.\nWe evaluated the models based on the following IR metrics: Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), Precision at 1 (P@1), and Precision at 5 (P@5).\nHyper-parameters We performed an extensive hyper-parameter search to identify the best model\n5The annotation task was initially carried out by two expert annotators, independently. The initial set was refined by comparing the annotations and asking to a third judge to make a final decision on disagreements. After the consensus of the annotation guidelines was reached (producing a Cohen\u2019s kappa coefficient of 0.73), the overall annotation was carried out by only one expert.\nfor the baselines and neural network models. For the TF-IDF baseline, we tried n-gram feature order n \u2208 {1, 2, 3} with and without stop words pruning. For the SVM baseline, we used the default SVMLight parameters whereas the dev data is only used to increase the training set size when testing on the test set. We also tried to give higher weight to dev instances but this did not result in any improvement.\nFor all the neural network models, we used Adam (Kingma and Ba, 2014) as the optimization method with the default setting suggested by the authors. We optimized other hyper-parameters with the following range of values: learning rate \u2208 {1e\u2212 3, 3e\u2212 4}, dropout (Hinton et al., 2012) probability \u2208 {0.1, 0.2, 0.3}, CNN feature width \u2208 {2, 3, 4}. We also tuned the pooling strategies and ensured each model has a comparable number of parameters. The default configurations of LSTMs, CNNs and RCNNs are shown in Table 2. We used MRR to identify the best training epoch and the model configuration. For the same model configuration, we report average performance across 5 independent runs.\nWord Vectors We ran word2vec (Mikolov et al., 2013) to obtain 200-dimensional word embeddings using all Stack Exchange data (excluding StackOverflow) and a large Wikipedia corpus. The word vectors are fixed to avoid over-fitting across all experiments."}, {"heading": "7 Results", "text": ""}, {"heading": "7.1 Overall Performance", "text": "Table 3 outlines the performance of the baselines and neural encoder models on the question simi-\nlarity task. The results show that our full model, RCNNs with pre-training, achieves the best performance across all metrics on both the dev and test sets. For instance, the full model gets a P@1 of 64.5% on the test set, outperforming word-based method BM25 by over 10%. Further, our RCNN model also outperforms the other neural encoder models and the baselines across all metrics. The ability of the RCNN model to outperform the other models indicates that the use of non-consecutive filters and a varying decay factor is effective in improving performance beyond traditional neural network models.\nTable 3 also demonstrates the performance gain from pre-training the RCNN encoder. The RCNN model when pre-trained on the entire corpus consistently gets better results across all the metrics."}, {"heading": "7.2 Discussion", "text": "Pooling strategy We analyze the effect of various pooling strategies for the neural network encoders. As shown in Table 4, our RCNN model outperforms CNNs and LSTMs regardless of the two pooling strategies explored. We also observe that simply using the last hidden state as the final representation achieves better results for the RCNN model.\nQuestion body Table 5 compares the performance of the TF-IDF baseline and the RCNN model when using question titles only or when using question titles along with question bodies. The TF-IDF baseline\u2019s performance improves very little when the question bodies are included.\nHowever, we find that the inclusion of the question bodies improves the performance of the RCNN\nMethod Dev Test MAP MRR P@1 P@5 MAP MRR P@1 P@5 CNNs, max-pooling 58.8 72.2 59.9 47.3 58.2 71.4 57.6 44.9 CNNs, mean-pooling 58.3 73.0 61.1 47.0 57.8 71.3 58.0 43.4 LSTMs, last state 56.9 70.4 57.6 46.1 57.8 69.8 56.6 42.8 LSTMs, mean-pooling 58.7 72.5 60.0 47.1 58.1 71.2 58.3 43.3 RCNNs, last state 59.9 73.4 61.8 49.4 62.6 75.0 64.2 46.0 RCNNs, mean-pooling 59.7 74.3 62.5 48.6 59.6 71.9 58.5 44.9\nTable 4: Choice of pooling strategies\nTF-IDF MAP MRR P@1 title only 54.3 66.8 52.7 title + body 53.2 67.1 53.8\nRCNNs, mean-pooling MAP MRR P@1 title only 55.6 68.7 54.8 title + body 59.6 71.9 58.5\nRCNNs, last state MAP MRR P@1 title only 58.9 73.0 61.5 title + body 62.6 75.0 64.2\nTable 5: Comparision between model variants when question bodies are used or not used. Numbers are reported on the test set.\nmodel, achieving 2% to 4% improvement with both model variations. The RCNN model\u2019s greater improvement as compared to TF-IDF\u2019s from the inclusion of the question bodies illustrates the ability of the model to pick out components that pertain most directly to the question being asked from the long, descriptive question bodies.\nPre-training Note that, during pre-training, the last hidden states generated by the neural encoder are used by the decoder to reproduce the question titles. It would be interesting to see how such states capture the meaning of questions. As shown in Figure 2, we compute the question similarities using these representations and evaluate MRR on the dev set. The representations generated by the RCNN encoder perform quite well, resulting in over 70% MRR without the subsequent fine-tuning.\nThe LSTM network does not perform as good as the RCNN model across a range of learning rates and dropout rates during pre-training. The LSTM encoder obtains only 65% MRR, being 5% worse\nTable 1 LSTM (train) LSTM (dev) RCNN (train) RCNN (dev) 0 77.478 58.34 69.063 66.89 1 68.011 62.16 50.880 67.63 2 59.174 65.69 44.205 69.86 3 53.219 65.12 40.437 69.90 4 49.071 65.33 37.910 69.37 5 45.999 66.00 36.042 69.98 6 43.658 66.08 34.567 69.49 7 41.767 66.49 33.374 70.43 8 40.228 66.00 32.397 69.37 9 38.933 65.72 31.578 70.01\n10 37.833 64.84 30.876 69.29 11 36.868 64.82 30.273 69.32 12 36.020 64.59 29.779 69.46 13 35.284 65.17 29.299 69.33 14 34.620 64.58 28.911 69.23 15 34.028 64.45 28.564 69.02 16 33.503 64.83 28.249 69.68 17 33.002 63.93 27.979 68.84 18 32.556 63.29 27.732 68.61 19 32.171 64.28 27.510 68.96 20 31.781 64.99 27.312 69.02\nM R R\n50.0\n56.8\n63.5\n70.3\n77.0\nLo ss\n25\n40\n55\n70\n85\nEpoch\n0 5 10 15 20\nLSTM RCNN\nFigure 2: Training loss (solid lines) versus MRR on the dev set (dotted lines) during pre-training. Red lines with diamonds are RCNNs and blue lines with triangles are LSTMs.\nthan the RCNN model. As a result, we do not observe a clear improvement by fine-tuning the LSTM encoder (when comparing to the result without pretraining and fine-tuning)."}, {"heading": "8 Conclusion", "text": "In this paper, we employed gated convolutions to map questions to their semantic representations, and demonstrate their effectiveness on the task of question retrieval in the community QA forums. This architecture enables the model to glean key pieces of information from lengthy, detail-riddled user questions. Pre-training within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus is integral to the model\u2019s success.\nIn future work, we plan to expand this model for the task of answer selection. Since this task has similar challenges as question retrieval, both pre-training and gated convolutions are likely to benefit overall performance. In addition, we plan to employ pretrained representations for the title generation task."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Dkpro similarity: An open source framework for text similarity", "author": ["B\u00e4r et al.2013] Daniel B\u00e4r", "Torsten Zesch", "Iryna Gurevych"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "B\u00e4r et al\\.,? \\Q2013\\E", "shortCiteRegEx": "B\u00e4r et al\\.", "year": 2013}, {"title": "A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning hybrid representations to retrieve semantically equivalent questions", "author": ["Luciano Barbosa", "Dasha Bogdanova", "Bianca Zadrozny"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Searching questions by identifying question topic and question focus", "author": ["Duan et al.2008] Huizhong Duan", "Yunbo Cao", "Chin-Yew Lin", "Yong Yu"], "venue": "In ACL,", "citeRegEx": "Duan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2008}, {"title": "Applying deep learning to answer selection: A study and an open task. arXiv preprint arXiv:1508.01585", "author": ["Feng et al.2015] Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou"], "venue": null, "citeRegEx": "Feng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao et al.2014] Jianfeng Gao", "Patrick Pantel", "Michael Gamon", "Xiaodong He", "Li Deng", "Yelong Shen"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Finding similar questions in large question and answer archives", "author": ["Jeon et al.2005] Jiwoon Jeon", "W Bruce Croft", "Joon Ho Lee"], "venue": "In Proceedings of the 14th ACM international conference on Information and knowledge management,", "citeRegEx": "Jeon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2005}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In ACM SIGKDD KDD", "citeRegEx": "Joachims.,? \\Q2002\\E", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al.1998] Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Lei et al.2015] Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Improving question recommendation by exploiting information need", "author": ["Li", "Manandhar2011] Shuguang Li", "Suresh Manandhar"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space. CoRR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The probabilistic relevance framework: BM25 and beyond", "author": ["Robertson", "Zaragoza2009] Stephen Robertson", "Hugo Zaragoza"], "venue": null, "citeRegEx": "Robertson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Robertson et al\\.", "year": 2009}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Word embedding based correlation model for question/answer matching", "author": ["Shen et al.2015] Yikang Shen", "Wenge Rong", "Nan Jiang", "Baolin Peng", "Jie Tang", "Zhang Xiong"], "venue": "arXiv preprint arXiv:1511.04646", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lstm-based deep learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108", "author": ["Tan et al.2015] Ming Tan", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Tan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Wang", "Nyberg2015] Di Wang", "Eric Nyberg"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Text understanding from scratch. arXiv preprint arXiv:1502.01710", "author": ["Zhang", "LeCun2015] Xiang Zhang", "Yann LeCun"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Improving question retrieval in community question answering using world knowledge", "author": ["Zhou et al.2013] Guangyou Zhou", "Yang Liu", "Fang Liu", "Daojian Zeng", "Jun Zhao"], "venue": "In Proceedings of the TwentyThird international joint conference on Artificial Intel-", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Learning continuous word embedding with metadata for question retrieval in community question answering", "author": ["Zhou et al.2015] Guangyou Zhou", "Tingting He", "Jun Zhao", "Po Hu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 31, "context": "The task of automatically retrieving similar questions to a given user\u2019s question has recently attracted significant attention and has become a testbed for various representation learning approaches (Zhou et al., 2015; dos Santos et al., 2015).", "startOffset": 199, "endOffset": 243}, {"referenceID": 4, "context": ", 2015; dos Santos et al., 2015). However, the task has proven to be quite challenging\u2014for instance, dos Santos et al. (2015) report a 22.", "startOffset": 12, "endOffset": 126}, {"referenceID": 18, "context": "In particular, we incorporate adaptive gating in non-consecutive CNNs (Lei et al., 2015) so as to focus temporal averaging in these models on key pieces of the questions.", "startOffset": 70, "endOffset": 88}, {"referenceID": 26, "context": "The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Rush et al., 2015).", "startOffset": 116, "endOffset": 209}, {"referenceID": 3, "context": "The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Rush et al., 2015).", "startOffset": 116, "endOffset": 209}, {"referenceID": 23, "context": "The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Rush et al., 2015).", "startOffset": 116, "endOffset": 209}, {"referenceID": 10, "context": "Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013).", "startOffset": 137, "endOffset": 218}, {"referenceID": 5, "context": "Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013).", "startOffset": 137, "endOffset": 218}, {"referenceID": 30, "context": "Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013).", "startOffset": 137, "endOffset": 218}, {"referenceID": 31, "context": "In contrast to (Zhou et al., 2015), our model treats each question as a word sequence as opposed to a bag of words, and we apply a recurrent convolutional model as opposed to the traditional CNN model used by dos Santos et al.", "startOffset": 15, "endOffset": 34}, {"referenceID": 4, "context": ", 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013). More recent work relies on representation learning to go beyond word-based methods, aiming to capture semantic representations for more refined mappings. For instance, Zhou et al. (2015) learn word embeddings using category-based metadata information for questions.", "startOffset": 32, "endOffset": 258}, {"referenceID": 4, "context": "Dos Santos et al. (2015) propose an approach which combines a convolutional neural network (CNN) and a bag-of-words representation for comparing questions.", "startOffset": 4, "endOffset": 25}, {"referenceID": 4, "context": "Dos Santos et al. (2015) propose an approach which combines a convolutional neural network (CNN) and a bag-of-words representation for comparing questions. In contrast to (Zhou et al., 2015), our model treats each question as a word sequence as opposed to a bag of words, and we apply a recurrent convolutional model as opposed to the traditional CNN model used by dos Santos et al. (2015) to map questions into meaning representations.", "startOffset": 4, "endOffset": 390}, {"referenceID": 25, "context": "Recent work on answer selection on community QA forums, similar to our task of question retrieval, has also involved the use of neural network architectures (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 6, "context": "Recent work on answer selection on community QA forums, similar to our task of question retrieval, has also involved the use of neural network architectures (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 27, "context": "Recent work on answer selection on community QA forums, similar to our task of question retrieval, has also involved the use of neural network architectures (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 6, "context": ", 2015; Feng et al., 2015; Tan et al., 2015). Similar to our work, these approaches apply neural network techniques, but focus on improving various other aspects of the model. For instance, Feng et al. (2015) explore different similarity measures beyond cosine similarity, and Tan et al.", "startOffset": 8, "endOffset": 209}, {"referenceID": 6, "context": ", 2015; Feng et al., 2015; Tan et al., 2015). Similar to our work, these approaches apply neural network techniques, but focus on improving various other aspects of the model. For instance, Feng et al. (2015) explore different similarity measures beyond cosine similarity, and Tan et al. (2015) adopt the neural attention mechanism over RNNs to generate better answer representations given the questions as context.", "startOffset": 8, "endOffset": 295}, {"referenceID": 17, "context": "Our approach is inspired by temporal convolutional neural networks (LeCun et al., 1998) and, in particular, its recent refinement (Lei et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 18, "context": ", 1998) and, in particular, its recent refinement (Lei et al., 2015), tailored to capture longerrange, non-consecutive patterns in a weighted manner.", "startOffset": 50, "endOffset": 68}, {"referenceID": 18, "context": "Our approach builds on (Lei et al., 2015), thus we begin by briefly outlining it.", "startOffset": 23, "endOffset": 41}, {"referenceID": 18, "context": "Our approach builds on (Lei et al., 2015), thus we begin by briefly outlining it. Let W1 and W2 denote filter matrices (as parameters) for pattern size n = 2. Lei et al. (2015) generate a sequence of states in response to tokens according to", "startOffset": 24, "endOffset": 177}, {"referenceID": 20, "context": "To this end, we run word2vec (Mikolov et al., 2013) on the raw corpus in addition to the Wikipedia dump.", "startOffset": 29, "endOffset": 51}, {"referenceID": 26, "context": "The resulting encoder-decoder architecture is akin to those used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and summarization (Rush et al.", "startOffset": 88, "endOffset": 162}, {"referenceID": 3, "context": "The resulting encoder-decoder architecture is akin to those used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and summarization (Rush et al.", "startOffset": 88, "endOffset": 162}, {"referenceID": 23, "context": ", 2014) and summarization (Rush et al., 2015).", "startOffset": 26, "endOffset": 45}, {"referenceID": 0, "context": "LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2014; Bowman et al., 2015; Rockt\u00e4schel et al., 2015).", "startOffset": 193, "endOffset": 263}, {"referenceID": 2, "context": "LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2014; Bowman et al., 2015; Rockt\u00e4schel et al., 2015).", "startOffset": 193, "endOffset": 263}, {"referenceID": 22, "context": "LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2014; Bowman et al., 2015; Rockt\u00e4schel et al., 2015).", "startOffset": 193, "endOffset": 263}, {"referenceID": 17, "context": "CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 13, "context": ", 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang and LeCun, 2015; Gao et al., 2014).", "startOffset": 65, "endOffset": 162}, {"referenceID": 15, "context": ", 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang and LeCun, 2015; Gao et al., 2014).", "startOffset": 65, "endOffset": 162}, {"referenceID": 14, "context": ", 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang and LeCun, 2015; Gao et al., 2014).", "startOffset": 65, "endOffset": 162}, {"referenceID": 7, "context": ", 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang and LeCun, 2015; Gao et al., 2014).", "startOffset": 65, "endOffset": 162}, {"referenceID": 11, "context": "\u2022 SVM, we trained a re-ranker using SVM-Light (Joachims, 2002) with a linear kernel incorporating various similarity measures from the DKPro similarity package (B\u00e4r et al.", "startOffset": 46, "endOffset": 62}, {"referenceID": 1, "context": "\u2022 SVM, we trained a re-ranker using SVM-Light (Joachims, 2002) with a linear kernel incorporating various similarity measures from the DKPro similarity package (B\u00e4r et al., 2013).", "startOffset": 160, "endOffset": 178}, {"referenceID": 8, "context": "We optimized other hyper-parameters with the following range of values: learning rate \u2208 {1e\u2212 3, 3e\u2212 4}, dropout (Hinton et al., 2012) probability \u2208 {0.", "startOffset": 112, "endOffset": 133}, {"referenceID": 20, "context": "Word Vectors We ran word2vec (Mikolov et al., 2013) to obtain 200-dimensional word embeddings using all Stack Exchange data (excluding StackOverflow) and a large Wikipedia corpus.", "startOffset": 29, "endOffset": 51}], "year": 2017, "abstractText": "Question answering forums are rapidly growing in size with no automated ability to refer to and reuse existing answers. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous detail in the question body and 2) available annotations are scarce and fragmented, driven by participants. We design a novel combination of recurrent and convolutional models (gated convolutions) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields 10% gain over a standard IR baseline, and 6% over standard neural network architectures (including CNNs and LSTMs) trained analogously.1", "creator": "LaTeX with hyperref package"}}}