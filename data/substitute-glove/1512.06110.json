{"id": "1512.06110", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2015", "title": "Morphological Inflection Generation Using Character Sequence to Sequence Learning", "abstract": "Morphological elicits survive goes brought task of gas full adjectival except of set so khinchin formula_5 same a particular foundational complicated. We concept the indeed of inflection models as came story sequence to linear improve problem and moreover an variant result the neural encoder - decoder model provided solving thought. Our model indeed translation minority besides can be trained in without accordance and fourth - supervised interfaces. We evaluate kind system next twice vsam made morphologically valuable languages also our each better or depending results which further legislative - of - beginning - design vehicles of inflection generation.", "histories": [["v1", "Fri, 18 Dec 2015 20:48:26 GMT  (103kb,D)", "http://arxiv.org/abs/1512.06110v1", null], ["v2", "Thu, 31 Dec 2015 17:23:32 GMT  (103kb,D)", "http://arxiv.org/abs/1512.06110v2", null], ["v3", "Tue, 22 Mar 2016 01:02:01 GMT  (103kb,D)", "http://arxiv.org/abs/1512.06110v3", "Proceedings of NAACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["manaal faruqui", "yulia tsvetkov", "graham neubig", "chris dyer"], "accepted": true, "id": "1512.06110"}, "pdf": {"name": "1512.06110.pdf", "metadata": {"source": "CRF", "title": "Morphological Inflection Generation Using Character Sequence to Sequence Learning", "authors": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer"], "emails": ["mfaruqui@cs.cmu.edu", "ytsvetko@cs.cmu.edu", "cdyer@cs.cmu.edu", "neubig@is.naist.jp"], "sections": [{"heading": null, "text": "Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation."}, {"heading": "1 Introduction", "text": "Inflection is the word-formation mechanism to express different grammatical categories such as tense, mood, voice, aspect, person, gender, number and case. Inflectional morphology is often realized by the concatenation of bound morphemes (prefixes and suffixes) to a root form or stem, but nonconcatenative processes such as ablaut and infixation are found in many languages as well. Table 1 shows the possible inflected forms of the German stem Kalb (calf) when it is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a\u2192a\u0308) and suffixation (e.g., +ern).\nInflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered\nan an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications.\nThe traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes ar X\niv :1\n51 2.\n06 11\n0v 1\n[ cs\n.C L\n] 1\n8 D\nec 2\n(e.g. affixation) or require careful feature engineering.\nIn this paper, we present a model of inflection generation based on a neural network sequence to sequence transducer. The root form is represented as sequence of characters, and this is the input to an encoder-decoder architecture (Cho et al., 2014; Sutskever et al., 2014). The model transforms its input to a sequence of output characters representing the inflected form (\u00a74). Our model makes no assumptions about morphological processes, and our features are simply the individual characters. The model is trained on pairs of root form and inflected forms obtained from inflection tables extracted from Wiktionary.1 We improve the supervised model with unsupervised data, by integrating a character language model trained on the vocabulary of the language.\nOur experiments show that the model achieves better or comparable results to state-of-the-art methods on the benchmark inflection generation tasks (\u00a75). For example, our model is able to learn longrange relations between character sequences in the string aiding the inflection generation process required by Finnish vowel harmony (\u00a76), which helps it obtain the current best results in that language."}, {"heading": "2 Inflection Generation: Background", "text": "Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table contains the inflected form of a given root word corresponding to different linguistic transformations (cf. Table 1). Figure 1 shows the inflection generation framework. Since the release of the Wiktionary dataset, several different models have reported performance on this dataset. As we are also using this dataset, we will now review these models.\n1www.wiktionary.org\nWe denote the models of Durrett and DeNero (2013), Ahlberg et al. (2014), Ahlberg et al. (2015), and Nicolai et al. (2015), by DDN13, AFH14, AFH15, and NCK15 respectively. These models perform inflection generation as string transduction and largely consist of three major components: (1) Character alignment of word forms in a table; (2) Extraction of string transformation rules; (3) Application of rules to new root forms.\nThe first step is learning of character alignments across inflected forms in a table. Figure 2 (a) shows alignment between three word forms of Kalb. Different models use different heuristic algorithms for alignments such as edit distance, dynamic edit distance (Eisner, 2002; Oncina and Sebban, 2006), and longest subsequence alignment (Bergroth et al., 2000). Aligning characters across word forms provide spans of characters that have changed and spans that remain unchanged. These spans are used to extract rules for inflection generation for different inflection types as shown in Figure 2 (b)\u2013(d).\nBy applying the extracted rules to new root forms, inflected words can be generated. DDN13 use a semi-Markov model (Sarawagi and Cohen, 2004) to predict what rules should be applied, using character n-grams (n = 1 to 4) as features. AFH14 and AFH15 use substring features extracted from words to match an input word to a rule table. NCK15 use a semi-Markov model inspired by DDN13, but additionally use target n-grams and joint n-grams as features sequences while selecting the rules.\nMotivation for our model. Morphology often makes references to segmental features, like place\nor manner of articulation, or voicing status (Chomsky and Halle, 1968). While these can be encoded as features in existing work, our approach treats segments as vectors of features \u201cnatively\u201d. Our approach represents every character as a bundle of continuous features, instead of using discrete surface character sequence features. Also, our model uses features as part of the transduction rules themselves, whereas in existing work features are only used to rescore rule applications.\nIn existing work, the learner implicitly specifies the class of rules that can be learned, such as \u201cdelete\u201d or \u201cconcatenate\u201d. To deal with phenomenona like segment lengthening in English: run \u2192 running; or reduplication in Hebrew: Kelev \u2192 Klavlav, Chatul \u2192 Chataltul; (or consonant gradation in Finnish), where the affixes are induced from characters of the root form, one must engineer a new rule class, which leads to poorer estimates due to data sparsity. By modeling inflection generation as a task of generating a character sequence, one character at a time, we do away with such problems."}, {"heading": "3 Neural Encoder-Decoder Models", "text": "Here, we describe briefly the underlying framework of our inflection generation model, called the recurrent neural network (RNN) encoder-decoder (Cho et al., 2014; Sutskever et al., 2014) which is used to transform an input sequence ~x to output sequence ~y. We represent an item by x, a sequence of items by ~x, vectors by x, matrices by X, and sequences of vectors by ~x."}, {"heading": "3.1 Formulation", "text": "In the encoder-decoder framework, an encoder reads a variable length input sequence, a sequence of vectors ~x = \u3008x1, \u00b7 \u00b7 \u00b7 ,xT \u3009 (corresponding to a sequence of input symbols ~x = \u3008x1, \u00b7 \u00b7 \u00b7 , xT \u3009) and generates a fixed-dimensional vector representation of the sequence. xt \u2208 Rl is an input vector of length l. The most common approach is to use an RNN such that:\nht = f(ht\u22121,xt) (1)\nwhere ht \u2208 Rn is a hidden state at time t, and f is generally a non-linear transformation, producing e := hT+1 as the input representation. The decoder is trained to predict the next output yt given the\nencoded input vector e and all the previously predicted outputs \u3008y1, \u00b7 \u00b7 \u00b7 yt\u22121\u3009. In other words, the decoder defines a probability over the output sequence ~y = \u3008y1, \u00b7 \u00b7 \u00b7 , yT \u2032\u3009 by decomposing the joint probability into ordered conditionals:\np(~y|~x) = \u220fT \u2032\nt=1 p(yt|e, \u3008y1, \u00b7 \u00b7 \u00b7 , yt\u22121\u3009) (2)\nWith a decoder RNN, we can first obtain the hidden layer at time t as: st = g(st\u22121, {e,yt\u22121}) and feed this into a softmax layer to obtain the conditional probability as:\np(yt = i|~e, ~y<t) = softmax(Wsst + bs)i (3)\nwhere, ~y<t = \u3008y1, \u00b7 \u00b7 \u00b7 , yt\u22121\u3009. In recent work, both f and g are generally LSTMs, a kind of RNN which we describe next."}, {"heading": "3.2 Long Short-Term Memory (LSTM)", "text": "In principle, RNNs allow retaining information from time steps in the distant past, but the nonlinear \u201csquashing\u201d functions applied in the calculation of each ht result in a decay of the error signal used in training with backpropagation. LSTMs are a variant of RNNs designed to cope with this \u201cvanishing gradient\u201d problem using an extra memory \u201ccell\u201d (Hochreiter and Schmidhuber, 1997; Graves, 2013). Past work explains the computation within an LSTM through the metaphors of deciding how much of the current input to pass into memory (it) or forget (ft). We refer interested readers to the original papers and present only the recursive equations updating the memory cell ct and hidden state ht given xt, the previous hidden state ht\u22121, and the memory cell ct\u22121:\nit = \u03c3(Wixxt +Wihht\u22121 +Wicct\u22121 + bi)\nft = 1\u2212 it ct = ft ct\u22121+\nit tanh(Wcxxt +Wchht\u22121 + bc) (4) ot = \u03c3(Woxxt +Wohht\u22121 +Wocct + bo)\nht = ot tanh(ct),\nwhere \u03c3 is the component-wise logistic sigmoid function and is the component-wise (Hadamard) product. Parameters are all represented using W and b. This formulation differs slightly from the classic LSTM formulation in that it makes use of\n\u201cpeephole connections\u201d (Gers et al., 2002) and defines the forget gate so that it sums with the input gate to 1 (Greff et al., 2015)."}, {"heading": "4 Inflection Generation Model", "text": "We frame the problem of inflection generation as a sequence to sequence learning problem of character sequences. The standard encoder-decoder models were designed for machine translation where the objective is to translate a sentence (sequence of words) from one language to a semantically equivalent sentence (sequence of words) in another language. We can easily port the encoder-decoder translation model for inflection generation. Our model predicts the sequence of characters in the inflected string given the characters in the root word (input).\nHowever, our problem differs from the above setting in two ways: (1) the input and output character sequences are mostly similar except for the inflections; (2) the input and output character sequences have different semantics. Regarding the first difference, taking the word play as an example, the inflected forms corresponding to past tense and continuous forms are played and playing. To better use this correspondence between the input and output sequence, we also feed the input sequence directly into the decoder:\nst = g(st\u22121, {e,yt\u22121,xt}) (5)\nwhere, g is the decoder LSTM, and xt and yt are the input and output character vectors respectively. Because the lengths of the input and output sequences are not equal, we feed an character in the decoder, indicating null input, once the input sequence runs out of characters. These character vectors are parameters that are learned by our model, exactly as other character vectors.\nRegarding the second difference, to provide the model the ability to learn the transformation of semantics from input to output, we apply an affine transformation on the encoded vector e:\ne\u2190Wtranse+ btrans (6)\nwhere, Wtrans,btrans are the transformation parameters. Also, in the encoder we use a bidirectional LSTM (Graves et al., 2005) instead of\na uni-directional LSTM, as it has been shown to capture the sequence information more effectively (Ling et al., 2015; Ballesteros et al., 2015b; Bahdanau et al., 2015). Our resultant inflection generation model is shown in Figure 3."}, {"heading": "4.1 Supervised Learning", "text": "The parameters of our model are the set of character vectors, the transformation parameters (Wtrans,btrans), and the parameters of the encoder and decoder LSTMs (\u00a73.2). We use negative loglikelihood of the output character sequence as the loss function:\n\u2212 log p(~y|~x) = \u2212 \u2211T \u2032\nt=1 log p(yt|e, ~y<t) (7)\nWe minimize the loss using stochastic updates with AdaDelta (Zeiler, 2012). This is our purely supervised model for inflection generation and we evaluate it in two different settings as established by previous work:\nFactored Model. In the first setting, we learn a separate model for each type of inflection independent of the other possible inflections. For example, in case of German nouns, we learn 8, and for German verbs, we learn 27 individual encoder-decoder inflection models (cf. Table 3). There is no parameter sharing across these models. We call these factored models of inflection generation.\nJoint Model. In the second setting, while learning a model for an inflection type, we also use the information of how the lemma inflects across all other inflection types i.e., the inflection table of a root form is used to learn different inflection models. We model this, by having the same encoder in the encoder-decoder model across all inflection models. The encoder in our model is learning a representation of the input character sequence. Because all inflection models take the same input but produce different outputs, we hypothesize that having the same encoder can lead to better estimates."}, {"heading": "4.2 Semi-supervised Learning", "text": "The model we described so far relies entirely on the availability of pairs of root form and inflected word form for learning to generate inflections. Although\nsuch supervised models can be used to obtain inflection generation models (Durrett and DeNero, 2013; Ahlberg et al., 2015), it has been shown that unlabeled data can generally improve the performance of such systems (Ahlberg et al., 2014; Nicolai et al., 2015). The vocabulary of the words of a language encode information about what correct sequences of characters in a language look like. Thus, we learn a language model over the character sequences in a vocabulary extracted from a large unlabeled corpus. We use this language model to make predictions about the next character in the sequence given the previous characters, in following two settings.\nOutput Reranking. In the first setting, we first train the inflection generation model using the supervised setting as described in \u00a74.1. While making predictions for inflections, we use beam search to generate possible output character sequences and rerank them using the language model probability along with other easily extractable features as described in Table 2. We use pairwise ranking optimization (PRO) to learn the reranking model (Hopkins and May, 2011). The reranker is trained on the beam output of dev set and evaluated on test set.\nLanguage Model Interpolation. In the second setting, we interpolate the probability of observing\nthe next character according to the language model with the probability according to our inflection generation model. Thus, the loss function becomes:\n\u2212log p(~y|~x) = 1 Z \u2211T \u2032 t=1 \u2212 log p(yt|e, ~y<t)\n\u2212 \u03bblog pLM(yt|~y<t) (8)\nwhere pLM (yt|~y<t) is the probability of observing the word yt given the history estimated according to a language model, \u03bb \u2208 [0, 1] is the interpolation parameter which is learned during training and Z is the normalization factor. This formulation lets us use any off-the-shelf pre-trained character language model easily (details in \u00a75)."}, {"heading": "4.3 Ensembling", "text": "Our loss functions (equ. 7 & 8) formulated using a neural network architecture are non-convex in nature and are thus difficult to optimize. It has been shown that taking an ensemble of models which were initialized differently and trained independently leads to improved performance (Hansen and Salamon, 1990; Collobert et al., 2011). Thus, for each model type used in this work, we report results obtained using an ensemble of models. So, while decoding we compute the probability of emitting a character as the product-of-experts of the individual models in the ensemble: pens(yt|\u00b7) = 1Z \u220fk i=1 pi(yt|\u00b7) 1 k where, pi(yt|\u00b7) is the probability according to i-th model and Z is the normalization factor."}, {"heading": "5 Experiments", "text": "We now conduct experiments using the described models. Note that not all previously published mod-\nels present results on all settings, and thus we compare our results to them wherever appropriate.\nHyperparameters. Across all models described in this paper, we use the following hyperparameters. In both the encoder and decoder models we use single layer LSTMs with the hidden vector of length 100. The length of character vectors is the size of character vocabulary according to each dataset. The parameters are regularized with `2, with the regularization constant 10\u22125. The number of models for ensembling are k = 5."}, {"heading": "5.1 Data", "text": "Durrett and DeNero (2013) published the Wiktionary inflection dataset with training, development and test splits. The development and test sets contain 200 inflection tables each and the training sets consist of the remaining data. This dataset contains inflections for German, Finnish and Spanish. This dataset was further augmented by (Nicolai et al., 2015), by adding Dutch verbs extracted from CELEX lexical database (Baayen et al., 1995), French verbs from Verbsite, an online French conjugation dictionary and Czech nouns and verbs from the Prague Dependnecy Treebank (Hajic\u030c et al., 2001). As the dataset for Czech contains many incomplete tables, we do not use it for our experiments. These datasets come with pre-specified training/dev/test splits, which we use. For each of these sets, the training data is restricted to 80% of the total inflection tables, with 10% for development and 10% for testing. We list the size of these datasets in Table 3.\nFor semi-supervised experiments, we train a 5- gram character language model with Witten-Bell smoothing (Bell et al., 1990) using the SRILM\ntoolkit (Stolcke, 2002). We train the character language models on the list of unique word types extracted from the Wikipedia dump for each language after filtering out words with characters unseen in the inflection generation training dataset. We obtained around 2 million unique words for each language."}, {"heading": "5.2 Results", "text": "Supervised Models. The individual inflected form accuracy for the factored model (\u00a74.1) is shown in Table 4. Across datasets, we obtain either comparable or better results than NCK15 while obtaining on average an accuracy of 96.20% which is higher than both DDN13 and NCK15. Our factored model performs better than DDN13 and NCK15 on datasets with large training set (ES-V, FI-V, FINA, NL-V, FR-V) as opposed to datasets with small training set (DE-N, DE-V). In the joint model setting (cf. Table 5), on average, we perform better than DDN13 and AFH14 but are behind AFH15 by 0.11%. Our model improves in performance over our factored model for DE-N, DE-V, and ES-V, which are the three smallest training datasets. Thus, parameter sharing across different inflection types helps the low-resourced scenarios.2\nSemi-supervised Models. We now evaluate the utility of character language models in inflection generation, in two different settings as described earlier (\u00a74.2). We use the factored model as our base model in the following experiments as it performed better than the joint model (cf. Table 4 & 5). Our reranking model which uses the character language\n2Although NCK15 provide results in the joint model setting, they also use raw data in the joint model which makes it incomparable to our model and other previous models.\nmodel along with other features (cf. Table 2) to select the best answer from a beam of predictions, improves over almost all the datasets with respect to the supervised model and is equal on average to AFH14 and NCK15 semi-supervised models with 96.45% accuracy. We obtain the best reported results on ES-V and FI-NA datasets (99.94% and 95.66% respectively). However, our second semi-supervised model, the interpolation model, on average obtains 96.08% and is surprisingly worse than our supervised model (96.20%).\nComparison to Other Architectures. Finally it is of interest how our proposed model compares to mode traditional neural models. We compare our\nmodel against a standard encoder-decoder model, and an encoder-decoder model with attention, both trained on root form to inflected form character sequences. In a standard encoder-decoder model (Sutskever et al., 2014), the encoded input sequence vector is fed into the hidden layer of the decoder as input, and is not available at every time step in contrast to our model, where we additionally feed in xt at every time step as in equ. 5. An attentional model computes a weighted average of the hidden layer of the input sequence, which is then used along with the decoder hidden layer to make a prediction (Bahdanau et al., 2015). These models also do not take the root form character sequence as inputs to the decoder. We also evaluate the utility of having an encoder which computes a representation of the input character sequence in a vector e by removing the encoder from our model in Figure 3. The results in Table 7 show that we outperform the encoder-decoder model, and the model without an encoder substantially. Our model is slightly better than the attentional encoder-decoder model, and is simpler as it does not have the additional attention layer."}, {"heading": "6 Analysis", "text": "Length of Inflected Forms. In Figure 4 we show how the prediction accuracy of an inflected form varies with respect to the length of the correct inflected form.3 To get stable estimates, we bin the in-\n3The plot of accuracy against root form length follows a similar pattern and we omit it here for brevity.\nflected forms according to their length: < 5, [5, 10), [10, 15), and \u2265 15. The accuracy for each bin is macro-averaged across 6 datasets4 for our factored model and the best models of DDN13 and NCK15. Our model consistently shows improvement in performance as word length increases and is significantly better than DDN13 on words of length more than 20 and is approximately equal to NCK15. On words of length< 5, we perform worse than DDN13 but better than NCK15. On average, our model has the least error margin across bins of different word length as compared to both DDN13 and NCK15. Using LSTMs in our model helps us make better predictions for long sequences, since they have the ability to capture long-range dependencies.\nFinnish Vowel Harmony. Our model obtains the current best result on the Finnish noun and adjective dataset, this dataset has the longest inflected words, some of which are> 30 characters long. Finnish exhibits vowel harmony, i.e, the occurrence of a vowel is controlled by other vowels in the word: in a word either only the front vowels (a\u0308, o\u0308, y) or the back vowels can appear (a, o, u) with the neutral vowels (e, i) having no impact on these occurrences. For example, our model correctly inflects painekeitin (pressure cooker) to obtain painekeittimilla, whereas NCK15 predicts painekeittimilla\u0308. The ability of our model to learn such long-range relations between these vowels helps capture vowel harmony. For FINA, our model obtains 99.87% for correctly predicting vowel harmony, and NCK15 obtains 98.50%.5 We plot the character vectors of these Finnish vowels (cf. Figure 5) using t-SNE projection (van der Maaten and Hinton, 2008) and observe that the vowels are correctly grouped with visible transition from the back to the front vowels."}, {"heading": "7 Related Work", "text": "In addition to the relevant work that we have mentioned in the background (\u00a72) and throughout the paper, we now briefly describe other areas of related work. Generation of inflectional morphology has been particularly useful in statistical machine\n4We remove DE-N as its the smallest and shows high variance in results.\n5The total no. of instances of vowel harmony in FI-NA are 4620.\ntranslation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Schu\u0308tze, 2015). Additional (recent) line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015b). These models have been especially successful when applied to morphologically rich languages, as they capture word formation pattern in addition to tokenlevel statistics."}, {"heading": "8 Conclusion", "text": "We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer. Our model obtains state-of-the-art results and performs at par or better than existing inflection generation models on seven different datasets. Our model is able to learn long-range dependencies within character sequences for inflection generation which makes it specially suitable for morphologically rich languages."}], "references": [{"title": "Markus Forsberg", "author": ["Malin Ahlberg"], "venue": "and Mans Hulden.", "citeRegEx": "Ahlberg et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Markus Forsberg", "author": ["Malin Ahlberg"], "venue": "and Mans Hulden.", "citeRegEx": "Ahlberg et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Richard Piepenbrock", "author": ["Harald R. Baayen"], "venue": "and Leon Gulikers.", "citeRegEx": "Baayen et al.1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Data-driven sentence generation with non-isomorphic trees", "author": ["Bernd Bohnet", "Simon Mille", "Leo Wanner"], "venue": "In Proc. of NAACL", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Improved transitionbased parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "John G Cleary", "author": ["Timothy C Bell"], "venue": "and Ian H Witten.", "citeRegEx": "Bell et al.1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Harri Hakonen", "author": ["Lasse Bergroth"], "venue": "and Timo Raita.", "citeRegEx": "Bergroth et al.2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Translating into morphologically rich languages with synthetic phrases", "author": ["Eva Schlinger", "Noah A. Smith", "Chris Dyer"], "venue": "In Proc. of EMNLP", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Knowledge-rich morphological priors for bayesian language models", "author": ["Noah A Smith", "Chris Dyer"], "venue": "In Proc. of NAACL", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The Sound Pattern of English", "author": ["Chomsky", "Halle1968] N. Chomsky", "M. Halle"], "venue": null, "citeRegEx": "Chomsky et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Chomsky et al\\.", "year": 1968}, {"title": "Combining distributional and morphological information for part of speech induction", "author": ["Alexander Clark"], "venue": "In Proc. of EACL", "citeRegEx": "Clark.,? \\Q2003\\E", "shortCiteRegEx": "Clark.", "year": 2003}, {"title": "Combining morpheme-based machine translation with post-processing morpheme prediction", "author": ["Clifton", "Sarkar2011] Ann Clifton", "Anoop Sarkar"], "venue": "In Proc. of ACL", "citeRegEx": "Clifton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clifton et al\\.", "year": 2011}, {"title": "Koray Kavukcuoglu", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen"], "venue": "and Pavel Kuksa.", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Morphological word-embeddings", "author": ["Cotterell", "Sch\u00fctze2015] Ryan Cotterell", "Hinrich Sch\u00fctze"], "venue": "In Proc. of NAACL", "citeRegEx": "Cotterell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cotterell et al\\.", "year": 2015}, {"title": "Discovering morphological paradigms from plain text using a dirichlet process mixture model", "author": ["Dreyer", "Eisner2011] Markus Dreyer", "Jason Eisner"], "venue": "In Proc. of EMNLP", "citeRegEx": "Dreyer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dreyer et al\\.", "year": 2011}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Durrett", "DeNero2013] Greg Durrett", "John DeNero"], "venue": "In Proc. of NAACL", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "Parameter estimation for probabilistic finite-state transducers", "author": ["Jason Eisner"], "venue": "In Proc. of ACL", "citeRegEx": "Eisner.,? \\Q2002\\E", "shortCiteRegEx": "Eisner.", "year": 2002}, {"title": "Aoife Cahill", "author": ["Alexander Fraser", "Marion Weller"], "venue": "and Fabienne Cap.", "citeRegEx": "Fraser et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Schraudolph", "author": ["Felix A. Gers", "Nicol N"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Gers et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving statistical MT through morphological analysis", "author": ["Goldwater", "McClosky2005] Sharon Goldwater", "David McClosky"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Goldwater et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2005}, {"title": "Santiago Fern\u00e1ndez", "author": ["Alex Graves"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Graves et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Jan Koutn\u0131\u0301k", "author": ["Klaus Greff", "Rupesh Kumar Srivastava"], "venue": "Bas R. Steunebrink, and J\u00fcrgen Schmidhuber.", "citeRegEx": "Greff et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Barbora Vidov\u00e1-Hladk\u00e1", "author": ["Jan Haji\u010d"], "venue": "and Petr Pajas.", "citeRegEx": "Haji\u010d et al.2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural network ensembles", "author": ["Hansen", "Salamon1990] Lars Kai Hansen", "Peter Salamon"], "venue": "In Proc. of PAMI", "citeRegEx": "Hansen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1990}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Tuning as ranking", "author": ["Hopkins", "May2011] Mark Hopkins", "Jonathan May"], "venue": "In Proc. of EMNLP", "citeRegEx": "Hopkins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2011}, {"title": "Generalizing inflection tables into paradigms with finite state operations", "author": ["Mans Hulden"], "venue": "In Proc. of the Joint Meeting of SIGMORPHON and SIGFSM", "citeRegEx": "Hulden.,? \\Q2014\\E", "shortCiteRegEx": "Hulden.", "year": 2014}, {"title": "Regular models of phonological rule systems", "author": ["Kaplan", "Kay1994] Ronald M Kaplan", "Martin Kay"], "venue": "Computational linguistics,", "citeRegEx": "Kaplan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kaplan et al\\.", "year": 1994}, {"title": "Twolevel morphology: A general computational model for word-form recognition and production", "author": ["Kimmo Koskenniemi"], "venue": "University of Helsinki", "citeRegEx": "Koskenniemi.,? \\Q1983\\E", "shortCiteRegEx": "Koskenniemi.", "year": 1983}, {"title": "Tiago Lu\u0131\u0301s", "author": ["Wang Ling"], "venue": "Lu\u0131\u0301s Marujo, R\u00e1mon Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso.", "citeRegEx": "Ling et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Christopher D", "author": ["Thang Luong", "Hieu Pham"], "venue": "Manning.", "citeRegEx": "Luong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alicia Burga", "author": ["Simon Mille"], "venue": "and Leo Wanner.", "citeRegEx": "Mille et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Kristina Toutanova", "author": ["Einat Minkov"], "venue": "and Hisami Suzuki.", "citeRegEx": "Minkov et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Colin Cherry", "author": ["Garrett Nicolai"], "venue": "and Grzegorz Kondrak.", "citeRegEx": "Nicolai et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "\u00d6zlem \u00e7etino\u011flu", "author": ["Kemal Oflazer"], "venue": "and Bilge Say.", "citeRegEx": "Oflazer et al.2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Error-tolerant finitestate recognition with applications to morphological analysis and spelling correction", "author": ["Kemal Oflazer"], "venue": "Computational Linguistics,", "citeRegEx": "Oflazer.,? \\Q1996\\E", "shortCiteRegEx": "Oflazer.", "year": 1996}, {"title": "Learning stochastic edit distance: Application in handwritten character recognition", "author": ["Oncina", "Sebban2006] Jose Oncina", "Marc Sebban"], "venue": "Pattern recognition,", "citeRegEx": "Oncina et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Oncina et al\\.", "year": 2006}, {"title": "Todd Ward", "author": ["Kishore Papineni", "Salim Roukos"], "venue": "and Wei-Jing Zhu.", "citeRegEx": "Papineni et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D. Santos", "Bianca Zadrozny"], "venue": "In Proc. of ICML", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Semi-markov conditional random fields for information extraction", "author": ["Sarawagi", "Cohen2004] Sunita Sarawagi", "William W Cohen"], "venue": "In Proc. of NIPS", "citeRegEx": "Sarawagi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi et al\\.", "year": 2004}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proc. of Interspeech", "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc VV Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Hisami Suzuki", "author": ["Kristina Toutanova"], "venue": "and Achim Ruopp.", "citeRegEx": "Toutanova et al.2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Visualizing Data using t-SNE", "author": ["van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Multilingual noise-robust supervised morphological analysis using the wordframe model", "author": ["Richard Wicentowski"], "venue": "In Proc. of SIGPHON", "citeRegEx": "Wicentowski.,? \\Q2004\\E", "shortCiteRegEx": "Wicentowski.", "year": 2004}, {"title": "Minimally supervised morphological analysis by multimodal alignment", "author": ["Yarowsky", "Wicentowski2000] David Yarowsky", "Richard Wicentowski"], "venue": "In Proc. of ACL", "citeRegEx": "Yarowsky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2000}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [], "year": 2017, "abstractText": "Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation.", "creator": "LaTeX with hyperref package"}}}