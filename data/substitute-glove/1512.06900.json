{"id": "1512.06900", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "Predicting the Co-Evolution of Event and Knowledge Graphs", "abstract": "Embedding learning, whose. k. close. competence learning, being all shown failed fully able to used large - decrease semantic opportunity graphs. A key interpretation is a imaging 's then knowledge graph to a tensor forms whose formats are noting by models using immaturity formulae of generalized networks. Knowledge formula_30 are normal treated time static: A knowledge graph tolerant more links never and contradict own access we although either truth values attributed together attacks is being back decomposition. In this press we priority but matter however work boolean outside triple since satisfy on time. We assume nothing reasons in also learning simplicial so prepare leaving elements large events, present the seems clear the events one the wireless it full knowledge bipartite. We northbound charge successful clearer sport. features both personal graph note data working information on coming participated. By forecasts country events, too also predict likely changes in the personal graph made actually determine any model another be evolution now the knowledge graph, with. Our biology demonstrate already still ways performer well in a laboratories specification, a compliance engine and a sensor computer application.", "histories": [["v1", "Mon, 21 Dec 2015 22:49:43 GMT  (80kb,D)", "http://arxiv.org/abs/1512.06900v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["crist\\'obal esteban", "volker tresp", "yinchong yang", "stephan baier", "denis krompa{\\ss}"], "accepted": false, "id": "1512.06900"}, "pdf": {"name": "1512.06900.pdf", "metadata": {"source": "CRF", "title": "Predicting the Co-Evolution of Event and Knowledge Graphs", "authors": ["Crist\u00f3bal Esteban", "Volker Tresp", "Yinchong Yang", "Stephan Baier", "Denis Krompa\u00df"], "emails": ["Cristobal.EstebanVolker.TrespDenis.Krompass@siemens.com", "Stephan.BaierYinchong.Yang@campus.lmu.de"], "sections": [{"heading": null, "text": "Keywords: Knowledge Graph, Representation Learning, Latent Variable Models, Link-Prediction"}, {"heading": "1 Introduction", "text": "In previous publications it was shown how a triple knowledge graph (KG) can be represented as a multiway array (tensor) and how a statistical model can be formed by deriving latent representations of generalized entities. Successful models are, e.g., RESCAL [22], Translational Embeddings Models [9], Neural Tensor Models [31] and the multiway neural networks as used in [10]. In these publications KGs were treated as static: A KG grew more links when more facts became available but the ground truth value associated with a link was considered time invariant. In this paper we address the issue of KGs where triple states depend on time. In the simplest case this might consider simple facts like \u201cObama is president of the United States\u201d, but only from 2008-2016. ar X iv :1\n51 2.\n06 90\n0v 1\n[ cs\n.L G\n] 2\n1 D\nec 2\n01 5\nAnother example is a patient whose status changes from sick to healthy or vice versa. Most popular KGs like Yago [32], DBpedia [1] Freebase [8] and the Google Knowledge Graph [30] have means to store temporal information.\nWithout loss of generality, we assume that changes in the KG always arrive in form of events, in the sense that the events are the gateway to the KG. For a given time step, events are described by a typically very sparse event triple graph, which contains facts that change some of the triples in the KG, e.g., from True to False and vice versa. KG triples which do not appear in the event graph are assumed unchanged.\nAn example might be the statement that a patient has a new diagnosis of diabetes, which is an information that first appears as an event in the event graph but is then also transmitted to the KG. Other events might be a prescription of a medication to lower the cholesterol level, the decision to measure the cholesterol level and the measurement result of the cholesterol level; so events can be, e.g., actions, decisions and measurements. In a similar way as the KG is represented as a KG tensor, the event graphs for all time steps can be represented as an event tensor. Statistical models for both the KG tensor and the event tensor can be derived based on latent representations derived from the tensor contents.\nAlthough the event tensor has a representation for time, it is by itself not a prediction model. Thus, we train a separate prediction model which estimates future events based on the latent representations of previous events in the event tensor and the latent representations of the involved generalized entities in the KG tensor. In this way, a prediction can, e.g., use both background information describing the status of a patient and can consider recent events. Since some future events will be absorbed into the KG, by predicting future events, we also predict likely changes in the KG and thus obtain a model for the evolution of the KG as well.\nThe paper is structured as follows. The next section discusses related work. In Section 3 we review statistical KG models based on latent representations of the involved generalized entities. In Section 4 we discuss the event tensor and its latent representations. In Section 5 we demonstrate how future events can be estimated using a prediction model that uses latent representations of the KG model and the event model as inputs. In the applications of this paper in Section 6, we consider patients and their changing states, users and their changing movie preferences and weather stations and their changing signal statistics. In the latter we show how \u2014in addition to event data\u2014 sensory data can be modeled. Section 7 contains our conclusions and discusses extensions."}, {"heading": "2 Related Work", "text": "There is a wide range of papers on the application of data mining and machine learning to KGs. Data mining attempts to find interesting KG patterns [5,27,24]. Some machine learning approaches attempt to extract close-to deterministic dependencies and ontological constructs [19,13,16]. The paper here focuses on sta-\ntistical machine learning in KG where representation learning has been proven to be very successful.\nThere is considerable prior work on the application of tensor models to temporal data, e.g., EEG data, and overviews can be found in [14] and [20]. In that work, prediction is typically not in focus, but instead one attempts to understand essential underlying temporal processes by analysing the derived latent representations.\nSome models consider a temporal parameter drift. Examples are the BPTF [36], and [11]. Our model has a more expressive dynamic by explicitly considering recent histories. Markov properties in tensor models were considered in [26,25]. In that work quadratic interactions between latent representations were considered. The approach described here is more general and also considers multiway neural networks as flexible function approximators.\nOur approach can also be related to the neural probabilistic language model [4], which coined the term representation learning. It can be considered an event model where the occurrence of a word is predicted based on most recent observed words using a neural network model with word representations as inputs. In our approach we consider that several events might be observed at a time instance and we consider a richer family of latent factor representations.\nThere is considerable recent work on dynamic graphs [17,23,33,28] with a strong focus on the Web graph and social graphs. That work is not immediately applicable to KGs but we plan to explore potential links as part of our future work."}, {"heading": "3 The Knowledge Graph Model", "text": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity. Here we consider a slight extension to the subject-predicate-object triple form by adding the value (es, ep, eo; Value) where Value is a function of s, p, o and can be the truth value of the triple or it can be a measurement. Thus (Jack, likes, Mary; True) states that Jack likes Mary, and (Jack, hasBloodTest, Cholesterol; 160) would indicate a particular blood cholesterol level for Jack. Note that es and eo represent the entities for subject index s and object index o. To simplify notation we also consider ep to be a generalized entity associated with predicate type with index p.\nA machine learning approach to inductive inference in KGs is based on the factor analysis of its adjacency tensor X where the tensor element xs,p,o is the associated Value of the triple (es, ep, eo). Here s = 1, . . . , S, p = 1, . . . , P , and o = 1, . . . , O. One can also define a second tensor \u0398KG with the same dimensions as X. It contains the natural parameters of the model and the connection to X. In the binary case one can use a Bernoulli likelihood with P (xs,p,o|\u03b8KGs,p,o) \u223c sig(\u03b8KGs,p,o), where sig(arg) = 1/(1 + exp(\u2212arg)) is the logistic function. If xs,p,o is a real number than we can use a Gaussian distribution with P (xs,p,o|\u03b8KGs,p,o) \u223c N (\u03b8KGs,p,o, \u03c32).\nIn representation learning, one assigns an r-dimensional latent vector to the entity e denoted by ae = (ae,0, ae,1, . . . , ae,r) T . We then model using one function\n\u03b8KGs,p,o = f KG(aes ,aep ,aeo)\nor, using one function for each predicate,\n\u03b8KGs,p,o = f KG p (aes ,aeo).\nFor example, the RESCAL model [22] is\n\u03b8KGs,p,o = r\u2211 k=1 r\u2211 l=1 Rp,k,laes,kaeo,l,\nwhere R \u2208 RP\u00d7r\u00d7r is the core tensor. In the multiway neural network model [10] one uses\n\u03b8KGs,p,o = NN(aes ,aep ,aeo)\nwhere NN stands for a neural network and where the inputs are concatenated. These approaches have been used very successfully to model large KGs, such as the Yago KG, the DBpedia KG and parts of the Google KG. It has been shown experimentally that models using latent factors perform well in these highdimensional and highly sparse domains. For a recent review, please consult [21].\nWe also consider an alternative representation. The idea is that the latent vector stands for the tensor entries associated with the corresponding entity. As an example, aes is the latent representation for all values associated with entity es, i.e., xs,:,:.\n1 It is then convenient to assume that one can calculate a so-called M -map of the form\naes = M subjectxs,:,:. (1)\nHere M subject \u2208 Rr\u00d7(PO) is a mapping matrix to be learned and xs,:,: is a column vector of size PO.2 For multilinear models it can be shown that such a representation is always possible; for other models this is a constraint on the latent factor representation. The advantage now is that the latent representations of an entity can be calculated in one simple vector matrix product, even for new entities not considered in training. We can define similar maps for all latent factors. For a given latent representation we can either learn the latent factors directly, or we learn an M -matrix.\nThe latent factors, the M -matrices, and the parameters in the functions can be trained with penalized log-likelihood cost functions described in the Appendix.\n1 If an entity can also appear as an object (o : eo = es), we need to include x:,:,o. 2 The M matrices are dense but one dimension is small (r), so in our settings we did\nnot run into storage problems. Initial experiments indicate that random projections can be used in case that computer memory becomes a limitation."}, {"heading": "4 The Event Model", "text": "Without loss of generality, we assume that changes in the KG always arrive in form of events, in the sense that the events are the gateway to the KG. For a given time step, events are described by a typically very sparse event triple graph, which contains facts that change some of the triples in the KG, e.g., from True to False and vice versa. KG triples which do not appear in the event graph are assumed unchanged.\nEvents might be, e.g., do a cholesterol measurement, the event cholesterol measurement, which specifies the value or the order take cholesterol lowering medicine, which determines that a particular medication is prescribed followed by dosage information.\nAt each time step events form triples which form a sparse triple graph and which specifies which facts become available. The event tensor is a four-way tensor Z with (es, ep, eo, et;Value) and tensor elements zs,p,o,t. We have introduced the generalized entity et to represent time. Note that the characteristics of the KG tensor and the event tensor are quite different. X is sparse and entries rarely change with time. Z is even sparser and nonzero entries typically \u201cappear\u201d more random. We model\n\u03b8events,p,o = f event(aes ,aep ,aeo ,aet).\nHere, aet is the latent representation of the generalized entity et. Alternatively, we consider a personalized representation of the form\n\u03b8pers-event,s=ip,o = f pers-event(aep ,aeo ,aes=i,t).\nHere, we have introduced the generalized entity es,t for a subject s = i at time t which stands for all events of entity s = i at time t.\nSince representations involving time need to be calculated online, we use M-maps of the form\naes,t = M subject, timezs,:,:,t\nThe cost functions are again described in the Appendix."}, {"heading": "5 The Prediction Model", "text": ""}, {"heading": "5.1 Predicting Events", "text": "Note that both the KG-tensor and the event tensor can only model information that was observed until time t but it would not be easy to derive predictions for future events, which would be of interest, e.g., for decision support. The key idea of the paper is that events are predicted using both latent representations of the KG and latent representations describing recently observed events.\nIn the prediction model we estimate future entries in the event tensor Z. The general form is\n\u03b8predicts,p,o,t = f predict(args) or \u03b8predicts,p,o,t = f predict p,o (args)\nwhere the first version uses a single function and the latter uses a different function for each (p, o)-pair.3 Here, args is from the sets of latent representations from the KG tensor and the event tensor.\nAn example of a prediction model is\n\u03b8predicts,p,o,t = f predict p,o (aes ,aes,t ,aes,t\u22121 , . . . ,aes,t\u2212T ).\nwhere the prediction is based on the latent representations of subject, object and predicate from the KG-tensor and of the time-specific representations from the event tensor.\nLet\u2019s consider an example. Let (es, ep, eo, et;Value) stand for (Patient, prescription, CholesterolMedication, Time; True). Here, aes is the profile of the patient, calculated from the KG model. Being constant, aes assumes the role of parameters in the prediction model. aes,t describes all that so far has happened to the patient at the same instance in time t (e.g., on the same day). aes,t\u22121 describes all that happened to the patient at the last instance in time and so on.\nWe model the functions by a multiway neural network with weight parameters W exploiting the great modeling flexibility of neural networks. The cost function for the prediction model is\ncostpredict = \u2212 \u2211\nzs,p,o,t\u2208Z logP (zs,p,o,t|\u03b8predicts,p,o,t (A,M,W )) (2)\n+\u03bbA\u2016A\u20162F + \u03bbW \u2016W\u20162F + \u03bbM\u2016M\u20162F . A stands for the parameters in latent representation andM stands for the parameters in the M -matrices. For a generalized entity for which we use an M -matrix, we penalize the entries in the M -matrix; for a generalized entity for which we directly estimate the latent representation we penalize the entries in the corresponding latent terms in A. Here, \u2016 \u00b7 \u2016F is the Frobenius norm and \u03bbA \u2265 0, \u03bbM \u2265 0 and \u03bbW \u2265 0 are regularization parameters."}, {"heading": "5.2 Predicting Changes in the KG", "text": "In our model, each change in the status of the KG is communicated via events. Thus each change in the KG first appears in the event tensor and predictions of events also implies predictions in the KG. The events that change the KG status are transferred into the KG and the latent representations of the KG, i.e., aes ,aep ,aeo , are re-estimated regularly (Figure 1)."}, {"heading": "5.3 More Cost Functions", "text": "Associated with each tensor model and prediction model, there is a cost function (see Appendix). In our experiments we obtained best results, when we used the cost function of the task we are trying to solve. In the most relevant prediction task we thus use the cost function in Equation 2. On the other hand, we obtained faster convergence for the prediction model if we initialize latent representations based on the KG model. 3 The different functions can be realized by the multiple outputs of a neural network."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Modeling Clinical Data", "text": "The study is based on a large data set collected from patients that suffered from kidney failure. The data was collected in the Charite\u0301 hospital in Berlin and it is the largest data collection of its kind in Europe. Once the kidney has failed, patients face a lifelong treatment and periodic visits to the clinic for the rest of their lives. After the transplant has been performed, the patient receives immunosuppressive therapy to avoid the rejection of the transplanted kidney. The patient must be controlled periodically to check the status of the kidney, adjust the treatment and take care of associated diseases, such as those that arise due to the immunosuppressive therapy. The dataset contains every event\nthat happened to each patient concerning the kidney failure and all its associated events: prescribed medications, hospitalizations, diagnoses, laboratory tests, etc. [18,29]. The database started being recorded more than 30 years ago and it is composed of dozens of tables with more than 4000 patients that underwent a renal transplant or are waiting for it. For example, the database contains more than 1200 medications that have been prescribed more than 250000 times, and the results of more than 450000 laboratory analyses.\nThis is particularly important for the estimation of drug-drug interactions (DDI) and adverse drug reactions (ADR) in patients after renal transplant.\nWe work with a subset of the variables available in the dataset. Specifically, we model medication prescriptions, ordered lab tests and lab test results. We transformed the tables into an event oriented representation where the subject is the patient and where time is a patient visit. We encoded the lab results in a binary format representing normal, high, and low values of a lab measurement, thus Value is always binary.\nThe prediction model is\n\u03b8predicts,p,o,t = f predict p,o (aes ,aet ,aes,t ,aes,t\u22121 , . . . ,aes,t\u2212T ).\nNote that we have a separate function for each (p, o)-pair. aes are patient properties as described in the KG. aes,t represents all events known that happened at visit t for the patient (e.g., the same visit for which we want to make a prediction). aes,t\u22121 represents all events for the patient at the last visit, etc. aet stands for the latent representation of all events at visit t for all patients and can model if events are explicitly dependent on the time since the transplant.\nRegarding the input window, we empirically found that T = 6 is optimal. The architecture is shown in Figure 2.\nThe first experiment consisted of predicting the events that will happen to patients in their next visit to the clinic given the events that were observed in the patients\u2019 previous visits to the clinic (i.e. by using the events that occurred to the patient from aes,t until aes,t\u22126). The experiment was performed 10 times with different random splits of the patients. Thus we truly predict performance on patients which were not considered in training! Table 1 shows how our proposed model outperforms the baseline models. The \u201cconstant predictor\u201d always predicts for each event the occurrence rate of such event (thus the most common event is given the highest probability of happening, followed by the second most common event, and so on). Note that we are particularly interested in the Area Under Precision-Recall Curve score due to the high sparsity of the data and our interest in predicting events that will actually happen, as opposed to the task of predicting which events will not be observed. In the last column of Table I we also report the time that it took to train for each model with the best set of hyperparameters in the first random split.\nNext we repeat the experiment including the KG-representation of the patient, which contains static variables of the patient such as blood type and gender, i.e., aes , and also used aet . Table 2 shows the improvement brought by the inclusion of the KG representation. The last row in Table 2 shows the result of making the predictions just with the KG representation of the patient (i.e. without the past event information), demonstrating clearly that information on past events is necessary to achieve best performance."}, {"heading": "6.2 Recommendation Engines", "text": "We used data from the MovieLens project with 943 users and 1682 movies.4 In the KG tensor we considered the triples (User, rates, Movie; Rating). For the event tensor, we considered the quadruples (User, watches, Movie, Time; Watched) and (User, rates, Movie, Time; Rating). Here, Rating \u2208 {1, . . . , 5} is the score the user assigned to the movie and Watched \u2208 {0, 1} indicates if the movie was watched and rated at time t. Time is the calendar week of the rating event. We define our training data to be 78176 events in the first 24 calender weeks and the test data to be 2664 events in the last 7 weeks. Note that in both datasets there are only 738 users since the remaining 205 users watched and rated their movies only in the test set.\nIt turned out that the movie ratings did not show dependencies on past events, so they could be predicted from the KG model alone with\n\u03b8predicts,rates,o,t = f predict rates,o (aes).\nWe obtained best results by modeling the function with a neural network with 1682 outputs (one for each movie). The user specific data was centered w.r.t. to their average and a numerical 0 would stand for a neutral rating. We obtain an RMSE score of 0.90 \u00b1 0.002 which is competitive with the best reported score of 0.89 on this data set [25]. But note that we predicted future ratings which is\n4 http://grouplens.org/datasets/movielens/\nmore difficult than predicting randomly chosen test ratings, as done in the other studies. Since we predict ordinal ratings, we used a Gaussian likelihood model.\nOf more interest in this paper is to predict if a user will decide to watch a movie at the next time step. We used a prediction model with\n\u03b8predicts,watches,o,t =\nfpredictwatches,o(aes ,aet ,aes, t ,aes,t\u22121 , . . . ,aes,t\u2212T ).\nHere, aes stands for the profile of the user as represented in the KG. aet stands for the latent representation of all events at time t and can model seasonal preferences for movies. aes,t stands for the latent representation of all movies that the user watched at time t. The architecture is shown in Figure 3. When training with only the prediction cost function we observe an AUROC an score of 0.728 \u00b1 0.001. We then explored sharing of statistical strength by optimizing jointly the M -matrices using all three cost functions costKG, costevent and costpredict and obtained a significant improvement with an AUROC score of 0.776 \u00b1 0.002. For comparison, we considered a pure KG-model and achieved an AUROC score of 0.756 \u00b1 0.007. Thus the information on past events leads to a small (but significant) improvement."}, {"heading": "6.3 Sensor Networks", "text": "In our third experiment we wanted to explore if our approach is also applicable to data from sensor networks. The main difference is now that the event tensor becomes a sensor tensor with subsymbolic measurements at all sensors at all times.\nImportant research issues for wind energy systems concern the accurate wind profile prediction, as it plays an important role in planning and designing of wind farms. Due to the complex intersections among large-scale geometrical parameters such as surface conditions, pressure, temperature, wind speed and wind direction, wind forecasting has been considered a very challenging task. In our analysis we used data from the Automated Surface Observing System (ASOS) units that are operated and controlled cooperatively in the United States by\nthe NWS, FAA and DOD5. We downloaded the data from the Iowa Environmental Mesonet (IEM)6. The data consists of 18 weather stations (the Entities) distributed in the central US, which provide measurements every minute. The measurements we considered are wind strength, wind direction, temperature, air pressure, dew point and visibility coefficient (the Attributes).\nIn the analysis we used data from 5 months from April 2008 to August 2008. The original database consists of 18 tables one for each station.\nThe event tensor is now a sensor tensor with quadruples (Station, measurement, SensorType, Time; Value), where Value is the sensor measurement for sensor SensorType at station Station at time Time. The KG-tensor is a longterm memory and maintains a track record of sensor measurement history.\nAs the dataset contains missing values we only considered the periods in which the data is complete. This results in a total of 130442 time steps for our dataset. In order to capture important patterns in the data and to reduce noise, we applied moving average smoothing using a Hanning window of 21 time steps. We split the data into train-, validation- and test set. The first four months of the dataset where used for training, and the last month as test set. 5 % of the training data where used for validation.\nWe considered three different prediction models with Gaussian likelihood functions, each with different latent representations at the input. The first model (Pred1) is\n\u03b8predicts,p,o,t = f predict p,o (aes ,aes,t\u22121 ,aes,t\u22122 , . . . ,aes,t\u2212T )\nwhere aes,t stands for all measurements of station es at time t and aes,t\u22121 ,aes,t\u22122 , . . . ,aes,t\u2212T can be considered a short term memory. aes,t\u22121 represents all measurements for station s between t \u2212 T \u2212 1 and t \u2212 1, i.e., and can represent complex sensor patterns over a longer period in time. Since measurements take on real values, a Gaussian likelihood model was used.\nThe second model (Pred2) is\n\u03b8predicts,p,o,t = f predict p,o (aes,t\u22121 , . . . ,aes,t\u2212T ,aet\u22121 , . . . ,aet\u2212T ).\n5 http://www.nws.noaa.gov/asos/ 6 https://mesonet.agron.iastate.edu/request/asos/1min.phtml\nHere, aet stands the latent representation of all measurements in the complete network at time t.\nAnd finally the third model (Pred3) combines the first two models and uses the combined sets of inputs. The architecture of Pred3 is shown in Figure 4.\nIn our experiments we considered the task of predicting 20 time steps into the future. All three models performed best with T = 10 and the rank of the latent representations being 20. Table 3 summarizes the results of the three prediction models together with three baseline models. The most basic baseline is to use the last observed value of each time series as a prediction. More enhanced baseline models are linear regression and feedforward neural networks using the previous history zs,:,:,t\u22121, zs,:,:,t\u22122, . . . , zs,:,:,t\u2212T of all time series of a station s as input. The experiments show that all three prediction models outperform the baselines. Pred1, which adds the personalization term for each sensor shows the best results. Pred2 performs only slightly better than the feedforward neural network. However, we assume that in sensor networks with a stronger cross correlation between the sensors, this model might prove its strength. Finally, the result of Pred3 shows that the combination of the multiple latent representations is too complex and does not outperform Pred1."}, {"heading": "7 Conclusions and Extensions", "text": "We have introduced an approach for modeling the temporal evolution of knowledge graphs and for the evolution of associated events and signals. We have demonstrated experimentally that models using latent representations perform well in these high-dimensional and highly sparse dynamic domains in a clinical application, a recommendation engine and a sensor network application. The clinical application is explored further in a funded project [35,12]. As part of future work we plan to test our approach in general streaming frameworks which often contain a context model, an event model and a sensor model, nicely fitting into our framework. In [34] we are exploring links between the presented approach and cognitive memory functions.\nIn general, we assumed a unique representation for an entity, for example we assume that aes is the same in the prediction model and the semantic model. Sometimes it makes sense to relax that assumption and only assume some form of a coupling. [15,3,2] contain extensive discussions on the transfer of latent representations."}, {"heading": "Appendix: Cost Functions", "text": "We consider cost functions for the KG tensor, the event tensor and the prediction model. The tilde notation X\u0303 indicates subsets which correspond to the facts known in training. If only positive facts with Value = True are known, as often the case in KGs, negative facts can be generated using, e.g., local closed world assumptions [21]. We use negative log-likelihood cost terms. For a Bernoulli likelihood, \u2212 logP (x|\u03b8) = log[1+exp{(1\u22122x)\u03b8}] (cross-entropy) and for a Gaussian likelihood \u2212 logP (x|\u03b8) = const+ 12\u03c32 (x\u2212\u03b8)\n2. We use regularization as described in Equation 2.\nWe describe the cost function in terms of the latent representations A and the M -mappings. W stands for the parameters in the functional mapping.\nKG The cost term for the semantic KG model is costKG = \u2212 \u2211\nxs,p,o\u2208X\u0303\nlogP (xs,p,o|\u03b8KGs,p,o(A,M,W ))\nEvents costevent = \u2212 \u2211 zs,p,o,t\u2208Z\u0303 logP (zs,p,o,t|\u03b8events,p,o,t(A,M,W ))\nPrediction Model The cost function for the prediction model is costpredict = \u2212 \u2211\nzs,p,o,t\u2208Z\u0303\nlogP (zs,p,o,t|\u03b8predicts,p,o,t (A,M,W ))"}], "references": [{"title": "DBpedia: A Nucleus for a Web of Open Data. In The Semantic Web, Lecture Notes in Computer Science", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Towards semantic web mining. In ISWC", "author": ["Bettina Berendt", "Andreas Hotho", "Gerd Stumme"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Linked Data ", "author": ["Tim Berners-Lee"], "venue": "Design Issues,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In SIGMOD,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "In SIGKDD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Temporal link prediction using matrix and tensor factorizations", "author": ["Daniel M. Dunlavy", "Tamara G. Kolda", "Evrim Acar"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Predicting sequences of clinical events by using a personalized temporal latent embedding model", "author": ["Crist\u00f3bal Esteban", "Danilo Schmidt", "Denis Krompa\u00df", "Volker Tresp"], "venue": "In IEEE ICHI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Dl-foil concept learning in description logics", "author": ["Nicola Fanizzi", "Claudia dAmato", "Floriana Esposito"], "venue": "In Inductive Logic Programming. Springer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Tensor Decompositions and Applications", "author": ["Tamara G. Kolda", "Brett W. Bader"], "venue": "SIAM Review,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Dl-learner: learning concepts in description logics", "author": ["Jens Lehmann"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Graphs over time: densification laws, shrinking diameters and possible explanations", "author": ["Jure Leskovec", "Jon M. Kleinberg", "Christos Faloutsos"], "venue": "In SIGKDD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "The resource description framework (RDF) as a modern structure for medical data", "author": ["Gabriela Lindemann", "Danilo Schmidt", "Thomas Schrader", "Dietmar Keune"], "venue": "International Journal of Medical, Health, Biomedical and Pharmaceutical Engineering,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Ontology learning for the semantic web", "author": ["Alexander Maedche", "Steffen Staab"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Applications of tensor (multiway array) factorizations and decompositions in data mining", "author": ["Morten M\u00f8rup"], "venue": "Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "A review of relational machine learning for knowledge graphs", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proc. of the IEEE,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Prediction and ranking algorithms for event-based network data", "author": ["Joshua O\u2019Madadhain", "Jon Hutchins", "Padhraic Smyth"], "venue": "SIGKDD Explorations,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Data mining with background knowledge from the web", "author": ["Heiko Paulheim", "Petar Ristoski", "Evgeny Mitichkin", "Christian Bizer"], "venue": "RapidMiner World,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Factorization Machines", "author": ["Steffen Rendle"], "venue": "Proceedings of the 10th IEEE International Conference on Data Mining. IEEE Computer Society,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Factorizing personalized markov chains for next-basket recommendation", "author": ["Steffen Rendle", "Christoph Freudenthaler", "Lars Schmidt-Thieme"], "venue": "In WWW,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Mining the semantic web", "author": ["Achim Rettinger", "Uta L\u00f6sch", "Volker Tresp", "Claudia d\u2019Amato", "Nicola Fanizzi"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Modeling dynamic behavior in large evolving graphs", "author": ["Ryan A. Rossi", "Brian Gallagher", "Jennifer Neville", "Keith Henderson"], "venue": "In WSDM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Tbase2 a web-based electronic patient", "author": ["K. Schr\u00f6ter", "G. Lindemann", "L. Fritsche"], "venue": "record. Fundamenta Informaticae,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Introducing the Knowledge Graph: things, not strings", "author": ["Amit Singhal"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Yago: A Core of Semantic Knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "In WWW,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Graphscope: parameter-free mining of large time-evolving graphs", "author": ["Jimeng Sun", "Christos Faloutsos", "Spiros Papadimitriou", "Philip S. Yu"], "venue": "In SIGKDD,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Learning with memory embeddings", "author": ["Volker Tresp", "Crist\u00f3bal Esteban", "Yinchong Yang", "Stephan Baier", "Denis Krompa\u00df"], "venue": "arXiv preprint arXiv:1511.07972,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Towards a new science of a clinical data intelligence", "author": ["Volker Tresp", "Sonja Zillner", "Maria J. Costa", "Yi Huang", "Alexander Cavallaro"], "venue": "In NIPS Workshop on Machine Learning for Clinical Data Analysis and Healthcare,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Temporal collaborative filtering with bayesian probabilistic tensor factorization", "author": ["Liang Xiong", "Xi Chen", "Tzu-Kuo Huang", "Jeff G. Schneider", "Jaime G. Carbonell"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": ", RESCAL [22], Translational Embeddings Models [9], Neural Tensor Models [31] and the multiway neural networks as used in [10].", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": ", RESCAL [22], Translational Embeddings Models [9], Neural Tensor Models [31] and the multiway neural networks as used in [10].", "startOffset": 47, "endOffset": 50}, {"referenceID": 28, "context": ", RESCAL [22], Translational Embeddings Models [9], Neural Tensor Models [31] and the multiway neural networks as used in [10].", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": ", RESCAL [22], Translational Embeddings Models [9], Neural Tensor Models [31] and the multiway neural networks as used in [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 29, "context": "Most popular KGs like Yago [32], DBpedia [1] Freebase [8] and the Google Knowledge Graph [30] have means to store temporal information.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "Most popular KGs like Yago [32], DBpedia [1] Freebase [8] and the Google Knowledge Graph [30] have means to store temporal information.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Most popular KGs like Yago [32], DBpedia [1] Freebase [8] and the Google Knowledge Graph [30] have means to store temporal information.", "startOffset": 54, "endOffset": 57}, {"referenceID": 27, "context": "Most popular KGs like Yago [32], DBpedia [1] Freebase [8] and the Google Knowledge Graph [30] have means to store temporal information.", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "Data mining attempts to find interesting KG patterns [5,27,24].", "startOffset": 53, "endOffset": 62}, {"referenceID": 24, "context": "Data mining attempts to find interesting KG patterns [5,27,24].", "startOffset": 53, "endOffset": 62}, {"referenceID": 21, "context": "Data mining attempts to find interesting KG patterns [5,27,24].", "startOffset": 53, "endOffset": 62}, {"referenceID": 16, "context": "Some machine learning approaches attempt to extract close-to deterministic dependencies and ontological constructs [19,13,16].", "startOffset": 115, "endOffset": 125}, {"referenceID": 11, "context": "Some machine learning approaches attempt to extract close-to deterministic dependencies and ontological constructs [19,13,16].", "startOffset": 115, "endOffset": 125}, {"referenceID": 13, "context": "Some machine learning approaches attempt to extract close-to deterministic dependencies and ontological constructs [19,13,16].", "startOffset": 115, "endOffset": 125}, {"referenceID": 12, "context": ", EEG data, and overviews can be found in [14] and [20].", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": ", EEG data, and overviews can be found in [14] and [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 33, "context": "Examples are the BPTF [36], and [11].", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "Examples are the BPTF [36], and [11].", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "Markov properties in tensor models were considered in [26,25].", "startOffset": 54, "endOffset": 61}, {"referenceID": 22, "context": "Markov properties in tensor models were considered in [26,25].", "startOffset": 54, "endOffset": 61}, {"referenceID": 3, "context": "Our approach can also be related to the neural probabilistic language model [4], which coined the term representation learning.", "startOffset": 76, "endOffset": 79}, {"referenceID": 14, "context": "There is considerable recent work on dynamic graphs [17,23,33,28] with a strong focus on the Web graph and social graphs.", "startOffset": 52, "endOffset": 65}, {"referenceID": 20, "context": "There is considerable recent work on dynamic graphs [17,23,33,28] with a strong focus on the Web graph and social graphs.", "startOffset": 52, "endOffset": 65}, {"referenceID": 30, "context": "There is considerable recent work on dynamic graphs [17,23,33,28] with a strong focus on the Web graph and social graphs.", "startOffset": 52, "endOffset": 65}, {"referenceID": 25, "context": "There is considerable recent work on dynamic graphs [17,23,33,28] with a strong focus on the Web graph and social graphs.", "startOffset": 52, "endOffset": 65}, {"referenceID": 5, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 58, "endOffset": 61}, {"referenceID": 29, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 86, "endOffset": 97}, {"referenceID": 0, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 86, "endOffset": 97}, {"referenceID": 6, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 86, "endOffset": 97}, {"referenceID": 27, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 86, "endOffset": 97}, {"referenceID": 19, "context": "For example, the RESCAL model [22] is", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "In the multiway neural network model [10] one uses \u03b8 s,p,o = NN(aes ,aep ,aeo)", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "For a recent review, please consult [21].", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "[18,29].", "startOffset": 0, "endOffset": 7}, {"referenceID": 26, "context": "[18,29].", "startOffset": 0, "endOffset": 7}, {"referenceID": 22, "context": "89 on this data set [25].", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "The clinical application is explored further in a funded project [35,12].", "startOffset": 65, "endOffset": 72}, {"referenceID": 10, "context": "The clinical application is explored further in a funded project [35,12].", "startOffset": 65, "endOffset": 72}, {"referenceID": 31, "context": "In [34] we are exploring links between the presented approach and cognitive memory functions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "[15,3,2] contain extensive discussions on the transfer of latent representations.", "startOffset": 0, "endOffset": 8}, {"referenceID": 1, "context": "[15,3,2] contain extensive discussions on the transfer of latent representations.", "startOffset": 0, "endOffset": 8}], "year": 2015, "abstractText": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Knowledge graphs are typically treated as static: A knowledge graph grows more links when more facts become available but the ground truth values associated with links is considered time invariant. In this paper we address the issue of knowledge graphs where triple states depend on time. We assume that changes in the knowledge graph always arrive in form of events, in the sense that the events are the gateway to the knowledge graph. We train an event prediction model which uses both knowledge graph background information and information on recent events. By predicting future events, we also predict likely changes in the knowledge graph and thus obtain a model for the evolution of the knowledge graph as well. Our experiments demonstrate that our approach performs well in a clinical application, a recommendation engine and a sensor network application.", "creator": "LaTeX with hyperref package"}}}