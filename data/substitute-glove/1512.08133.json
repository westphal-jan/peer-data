{"id": "1512.08133", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Dec-2015", "title": "The Utility of Abstaining in Binary Classification", "abstract": "We realize both could main configurations classification began installing learning, half perfect twist - instead classifier if allowed to preach on kind groundcolour, professing hatred days it true service re-released without committing to any statistical. This actually be motivated by computers makes clinics diagnosis though fraud pressures assessment, in well plausible attributing have avoid aftereffects consequence. We for well rather recent fueling of conversely trouble work in this area that epitome how carry abstentions therefore lead should fewer flaws year very general tutorial. Two areas are highlighted: the clearly finding both zero - error learning, of itself relevance trickier between predicting sufficiently not and overcome conclusions attribute. We review efficient algorithms with negates provisions for number own common along. We also focus connections time variety possible, various active communication, or find not strong easy of intended probes in this emerging along.", "histories": [["v1", "Sat, 26 Dec 2015 19:02:00 GMT  (630kb,D)", "http://arxiv.org/abs/1512.08133v1", "Short survey"]], "COMMENTS": "Short survey", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["akshay balsubramani"], "accepted": false, "id": "1512.08133"}, "pdf": {"name": "1512.08133.pdf", "metadata": {"source": "CRF", "title": "The Utility of Abstaining in Binary Classification", "authors": ["Akshay Balsubramani"], "emails": ["abalsubr@cs.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "Consider a general practice physician treating a patient with unusual or ambiguous symptoms. The general practitioner often does not have the capability or equipment to confidently diagnose such an ailment, but will be able to refer the patient to a specialist or hospital. Therefore, the GP is faced with a difficult choice: either make a potentially erroneous diagnosis and act on it (which can occasionally have catastrophic consequences) or avoid committing to such a diagnosis and refer the patient on instead (which will almost certainly cost extra time and resources).\nSuch a situation motivates the study of prediction algorithms which are able not only to form a hypothesis about the correct prediction, but also abstain entirely from making a prediction if they are not confident enough. In machine learning, these algorithms are often formulated as classifiers, and the field of classification with abstention is therefore of increasing research interest.\nIn this manuscript, we survey the problem of binary classification where the classifier is allowed to abstain."}, {"heading": "1.1 Outline", "text": "We will concern ourselves with the provable potential benefits of abstaining in two distinct scenarios. The first, covered in Section 2, is an online learning model free of distributional assumptions on the data, similar to the classical Mistake Bound model [Lit88]. The other, discussed in Section 3, is a standard statistical learning model in which the algorithm is initially given a chance to train on a set of labeled data, and the data are assumed to be drawn IID (independently and identically distributed) from a fixed distribution.\nThe abstain option in general, and each of the aforementioned models in particular, is intimately related to the well-studied paradigm of active learning, which aims to minimize the amount of labeled data required by a learner.\nOne idea that will recur in the survey, and is common to both settings we study, is the fundamental tradeoff between abstaining at a low rate and achieving low error when predictions are made. The intuitive reason for this is simple. A classifier will abstain exactly when it is unsure about which label to predict, and such unsureness implies that it would err on such examples relatively often if forced to predict on them. Traditionally, the classifier is indeed\nar X\niv :1\n51 2.\n08 13\n3v 1\n[ cs\n.L G\n] 2\n6 D\nec 2\n01 5\nforced to always predict; the possibility of doing better than this case is another spur to explore the utility of abstention.\nNote that we avoid discussing Bayesian or explicitly distribution-dependent methods here [Cho70], as they are outside the scope of the manuscript; we believe the work surveyed here is itself extremely general and provides a unique web of insights into the problem.\nDue to the volume of the material covered here and the necessary concision of this manuscript, full proofs of results will normally not be provided; the interested reader is referred to the appropriate references. However, we will endeavor to make the treatment here as standalone as possible, and provide proof sketches and intuitions whenever they are instructive."}, {"heading": "1.2 Preliminaries", "text": "We consider a standard binary classification setting in which there exist unlabeled data points x \u2208 X , each of which is associated to a label in Y , {\u22121,+1}; the pair (x, y) \u2208 X \u00d7 Y is referred to as a labeled data point. When the labels are deterministic, we can think of them as being generated by a map h\u2217 : X 7\u2192 Y , so that for all labeled data points (xi, yi) we have yi = h\u2217(xi).\nThe goal is to determine a classifier (a hypothesis) h : X 7\u2192 {\u22121,+1,\u22a5}, where the classifier can either predict one of the two admissible labels or output \u22a5, which is referred to as abstaining. The latter can be interpreted as \u201ddon\u2019t know,\u201d a statement of ignorance about which label is correct without committing at all to either. In this case, the risk of predicting the label of the given datum incorrectly is deemed to be excessive. The understanding is that a lower penalty is paid with the \u22a5 prediction than an incorrect \u00b11 prediction, which we call a mistake. The learning algorithms we consider are given a hypothesis class H, which is a set of hypotheses h : X 7\u2192 Y that is used to constrain the set of possible solutions and for computational reasons. We will consider algorithms which choose a single h \u2208 H, as well as algorithms which choose a weighted majority vote of all h \u2208 H. We also follow standard terminology in calling problems with h\u2217 \u2208 H realizable or separable. The non-realizable scenario is often referred to as the agnostic case.\nWe defer more specific notation to the body of the manuscript, where it is presented as needed."}, {"heading": "2 Abstaining in the KWIK Model", "text": "In this section, we describe a general model which has been influential in recent advances in the abstention literature. It will serve to provide useful insights into the possibilities afforded by abstention, even in a very general setting."}, {"heading": "2.1 KWIK: Formulation", "text": "The KWIK (\u201dKnows What It Knows\u201d) model [LLWS11] is a distribution-free way of formulating the online solvability of problems with zero incorrect predictions in total. Algorithms which accomplish this must be selfaware enough to recognize all points where they might make an incorrect prediction and output\u22a5 on them, and yet still predict (correctly) on a nontrivial number of examples. The name of the model refers to this self-awareness that it requires from algorithms, which distinguishes it from the otherwise similar Mistake Bound (MB) model.\nWe first review the salient features of the popular MB model as a useful reference point for further investigation."}, {"heading": "2.1.1 The Mistake Bound Model", "text": "The Mistake Bound (MB) model [Lit88] for binary classification is a general framework for the study of online binary classification algorithms. In it, the learning algorithm receives unlabeled data points chosen one at a time in some arbitrary way (by an adversary in the worst case). At the beginning of round t, the algorithm has the hypothesis ht, and sees an unlabeled point xt. It then predicts a label ht(xt) = y\u0302t \u2208 Y , after which the true label yt is revealed. The algorithm then makes any necessary internal modifications to update its hypothesis ht to ht+1. The next point xt+1 is then chosen, and the process repeats.\nIn this model, results of interest bound the total number of mistakes |{t : y\u0302t 6= yt}|made by the learning algorithm during the process, and are called mistake bounds. Typically, the true labels are assumed to be generated by some\nhypothesis in h\u2217 \u2208 H, so that yt = h\u2217(xt). In this setting, learnability is defined as follows. Here dim(H) represents some predefined notion of the complexity ofH (for finiteH, take dim(H) = |H|). Definition 2.1. An algorithm learns H in the MB model if for any \u03b4 \u2208 (0, 1), the algorithm makes a total of at most poly( 1\u03b4 , dim(H)) mistakes with probability at least 1\u2212 \u03b4 over entire runs of the algorithm.\nWell-known examples of mistake bounds include those for the perceptron [KWA97] and weighted majority [LW89] learning algorithms.\nSuch results are considered quite strong, since they imply that the algorithm eventually is mistake-free and because each unlabeled datum xt may even be chosen by an adversary with full knowledge of the past {xi, y\u0302i, yi}t\u22121i=1 . Guarantees in the standard setting of statistical learning, which we will consider more thoroughly in Section 3, can be seen to be less general primarily because they assume the unlabeled data xt to be drawn IID from a distribution that is fixed a priori.\nThe KWIK model, however, is a strict restriction of this MB model, as we will see next."}, {"heading": "2.1.2 The KWIK Model", "text": "The KWIK [LLWS11] model is used to analyze online supervised learning algorithms. The setting is very similar to the MB setting, with the algorithm seeing an adversarially chosen point xt on the tth round. However, the algorithm predicts y\u0302t \u2208 Y \u222a {\u22a5}, and only observes the true label when it predicts \u22a5. As in the previous discussion, we assume realizability: h\u2217 \u2208 H. Then we can define our goal in this model, KWIK-learnability: Definition 2.2. An algorithm learns H in the KWIK model if for any , \u03b4 \u2208 (0, 1), the following are true with probability at least 1\u2212 \u03b4 over entire runs of the algorithm:\n1. If y\u0302t 6=\u22a5, then |y\u0302t \u2212 yt| < .\n2. |{t : y\u0302t =\u22a5}|, the total number of times the algorithm abstains, is at most poly( 1 , 1 \u03b4 , dim(H)).\nIt is clear that this shares many similarities with Definition 2.1. Some notes on Defn. 2.2 are in order:\n\u2022 The first requirement corresponds to the algorithm making no mistakes when it does choose to predict, which is the hallmark of this framework. (Note: As pointed out by Li et al. [LLWS11], this does not make sense in the non-realizable case h\u2217 /\u2208 H. Therefore, the definition can be generalized to the nonrealizable case by relaxing the first requirement, though we only focus on the realizable case which most known results address.)\n\u2022 The definition applies equally to regression problems (though we have presented yt as being a discrete classification label). Another generalization is to the case when instead of observing the true label yt, the algorithm observes a possibly randomized quantity zt that depends on yt. For instance, when learning the bias of a biased coin, zt is a Bernoulli random variable with bias yt. We neglect to formalize this for simplicity, but the full statement is in [LLWS11].\nKWIK learnability is a fairly general condition for the same reasons discussed for MB learnability in Section 2.1.1. In fact, it is stronger: Proposition 2.3. Any KWIK algorithm which outputs \u22a5 at most B times can be converted into an MB algorithm for the same problem which makes at most B mistakes.\nProof. The MB algorithm is the same as the KWIK algorithm, except that it outputs a prediction every time the KWIK algorithm would output \u22a5.\nAs discussed earlier, this is due to the self-awareness required of the algorithm. An MB algorithm, by contrast, need only ensure it is learning correctly, as it blithely predicts without needing to consider its confidence in its own predictions.\nFor this reason, there are problems which are MB-learnable, but for which no KWIK algorithm exists; in a sense, the converse of Prop. 2.3 does not hold. One such problem is learning a Boolean singleton: the hypothesis class is hi : {0, 1}n 7\u2192 Y, i = 1, . . . , 2n, predicting +1 iff the input is a binary representation of i \u2212 1. This can be\nMB-learned with at most 1 mistake by simply predicting \u22121 always until the mistake is made. However, KWIKlearning it is more difficult - the restriction that the algorithm must be absolutely sure to predict is excessively strong, because in the worst case it is not sure until seeing 2n \u2212 1 examples (effectively brute-force searching the space for the +1 data point). For the formal details, the reader is referred to Example 1 of [SZB10].\nThe generality of the KWIK framework allows it to be used even for algorithms with adaptive sampling strategies, in which there are complex dependencies between the data points sampled. The framework was originally motivated by applications in reinforcement learning, where such situations arise regularly. It has proved a useful tool in proving optimality guarantees for such algorithms; a further discussion is beyond the scope of this manuscript, but the interested reader is referred to [LLWS11, DLL09, WSDL09, SS11], and to [AADK13] for a recent application to the setting of multi-armed bandits."}, {"heading": "2.2 KWIK: Algorithmic Building Blocks", "text": "As we have seen, the KWIK definition is extremely general. Remarkably, however, there are efficient KWIK algorithms for several nontrivial problems. These are for the most part derived from a few generic algorithms, which provide glimpses into the type of reasoning that is fruitful in this distribution-free setting. We therefore present some of these representative \u201dbuilding block\u201d algorithms, along with their associated proof ideas."}, {"heading": "2.2.1 Enumeration", "text": "The most ubiquitous and central such method, the enumeration algorithm, works in the realizable case when H is finite, and is laid out as Algorithm 1.\nAlgorithm 1 Enumeration 1: V \u2190 H 2: for i = 1, 2, . . . do 3: if h(xi) is the same \u2200h \u2208 V then 4: Choose any h \u2208 V and predict h(xi) 5: else 6: Output \u22a5 and receive label yi 7: V \u2190 {h \u2208 V : h(xi) = yi} 8: if |V| = 1 then 9: Terminate with final output V\nThe algorithm maintains a version space V of the hypotheses consistent with all data seen so far, paring down V appropriately as more data are revealed. This is a very natural strategy for making conservative predictions. The optimal hypothesis h\u2217 is always in V by construction; consequently, any predicted label y\u0302i always agrees with h\u2217(xi), which is the true label yi by the realizability assumption.\nEvery time the algorithm abstains, at least one hypothesis in V is predicting the wrong label, and such hypotheses are removed each iteration. Therefore, each abstention leads to the removal of at least one hypothesis from V , leading to a KWIK bound of |H| \u2212 1 abstentions. There is a natural correspondence between predicting \u22a5 here and simply requesting a label - abstention may be thought of as a couched label request. Given this and the simplicity of the enumeration procedure, it is not surprising to find that Algorithm 1 appears independently in active learning, a supervised learning paradigm where labels are only provided upon the learner\u2019s request [Set12]. These and other active learning connections are discussed in Sec. 4.\nAlthough the enumeration algorithm is appealingly general and simple, the major drawback is the same one confronted by the active learning literature: explicitly maintaining the version space by storing all its constituent hypotheses can be computationally intractable, requiring an exponential amount of memory!\nFortunately, as we will observe later in the manuscript, there are nontrivial special cases for which V can be implicitly maintained so that the operations of Algorithm 1 can be performed without actually storing V , even whenH is infinite."}, {"heading": "2.2.2 Other Basic KWIK Algorithms", "text": "In order to highlight the utility of KWIK, we will briefly touch upon a few other basic algorithms in this framework. All of them are efficient - they run in polynomial time.\n\u2022 Learning the bias of a coin: The algorithm simply abstains on the first T examples to get IID Bernoulli-distributed labels in order to estimate the bias, and thereafter predicts on every example according to the empirical estimate of the bias. Applying the Hoeffding bound yields a KWIK bound of T \u2208 O ( \u22122 ln(1/\u03b4) ) .\n\u2022 Learning the distribution of an n-sided die: This is the multinomial-distribution analogue of the above coin-learning problem. It can be solved by reduction to n \u201done-vs.-rest\u201d coin-learning problems, and the algorithm is the same as for the coin-learning case. A Chernoff bound analysis yields the KWIK bound O ( n \u22122 ln(n/\u03b4) ) .\n\u2022 Linear regression with no noise in Rn: This is KWIK-learnable by a beautifully simple algorithm that exploits linear algebraic structure. The algorithm maintains a running training set of points x and their corresponding linear function values f(x) = \u03b8 \u00b7 x, \u03b8 \u2208 Rn (unknown \u03b8). For any new point xt, the algorithm first checks if xt is in the linear span of the points in the training set so far, which can be done efficiently by solving a system of linear equations. If so, f(xt) is an efficiently determinable linear combination of the function values in the training set, and the algorithm can predict with certainty. Otherwise, it abstains and adds (xt, f(xt)) to the training set. The training set cannot grow beyond size n, so the KWIK bound is n.\n\u2022 Linear regression with additive white noise in Rn: Though neither the proof nor the algorithm are as simple as for the zero-noise case, the algorithm predicts using the least-squares solution, or abstains by assessing a confidence measure based on the eigenvalue spectrum of the data. The KWIK bound is O\u0303 ( n3 \u22124 ) . (Note: This setting has been studied independently for a much weaker oblivious, rather than\nadaptive, adversary; under this assumption, a much better bound of O\u0303 ( n \u22122 ) is proved in [CBGO09].)\n\u2022 Algorithms also exist for certain generic combinations of KWIK-learnable scenarios. For instance, if Hi, i = 1, . . . , n are KWIK-learnable, then so is \u222aiHi, even when observations are corrupted by white noise. Also KWIK-learnable are hypothesis classes whose domains and ranges are respectively Cartesian products of the domains and ranges of KWIK-learnable classesHi.\nThe intent here is to highlight that KWIK is a nontrivial model despite its extreme generality. We will now address a central issue of this survey - the tradeoff between errors and abstentions - by exploring the interesting consequences of allowing the learner to make some mistakes."}, {"heading": "2.3 Allowing a Few Mistakes in KWIK", "text": "Though KWIK has proved to be quite useful in the past few years, its restriction that the learner make absolutely no mistakes is somewhat extreme in many circumstances. As we will see, allowing a few mistakes in the framework can drastically improve abstention rates."}, {"heading": "2.3.1 Enumeration With a Few Mistakes", "text": "The cornerstone enumeration method of Algorithm 1 has recently been extended to allow up to k mistakes for some prespecified parameter k, again in an appealingly simple and generic manner [SZB10]. This newer algorithm, which we dub relaxed enumeration in our presentation, is outlined in Algorithm 2, and again is motivated by the realizable case for finiteH. (Note: Here, the true label of a point is revealed regardless of whether the algorithm abstains or not. This is still consistent with the original KWIK framework of Algorithm 1 because of that algorithm\u2019s zero-mistakes property.)\nAlgorithm 2 has an instructive interpretation as a generalization of Algorithm 1. Like its predecessor, it tracks a version space V and terminates when enough data have arrived to pare this down to a singleton set containing h\u2217. When an unlabeled datum xi arrives, a vote is taken over the hypotheses in V . The margin of the vote is used as a measure of confidence in deciding whether or not to predict. The threshold s that is used to make this decision is decreased in such a way that at most k mistakes are made; this is done by keeping track of the number of mistakes m made so far.\nAlgorithm 2 Relaxed Enumeration\n1: V \u2190 H, s\u2190 |H| k k+1 ,m\u2190 0 2: for i = 1, 2, . . . do 3: \u00b5 , min\n\u03bb\u2208{\u22121,+1} |{h \u2208 V : h(xi) = \u03bb}|, so that \u00b5 is the number of hypotheses in V predicting the minority\nlabel 4: if \u00b5 \u2264 s then 5: Predict with the majority, i.e. predict y\u0302i , arg max\n\u03bb\u2208{\u22121,+1} |{h \u2208 V : h(xi) = \u03bb}|\n6: else 7: Output \u22a5 8: Receive label yi 9: V \u2190 {h \u2208 V : h(xi) = yi}\n10: if y\u0302i 6= yi and y\u0302i 6=\u22a5 then 11: m\u2190 m+ 1 12: s\u2190 |V| k\u2212m k+1\u2212m\n13: if |V| = 1 then 14: Terminate with final output V\nDue to the tolerance for a few mistakes, the KWIK bound on the number of\u22a5s is far better than that of Algorithm 1. The proof uses an elegant inductive argument which is worth noting in full:\nProposition 2.4 ( [SZB10]). Algorithm 2, in the realizable case, makes at most k mistakes and outputs \u22a5 at most (k + 1) |H| 1 k+1 times.\nProof. The proof proceeds by induction in k. The first mistake made reduces |V| so that |V| < |H| k k+1 . Since the effective size of the hypothesis class being considered at any point is |V|, the inductive hypothesis is that after the\nfirst mistake, the algorithm will terminate after at most k \u2212 1 mistakes and k |V| 1 k < k ( |H| k k+1 ) 1 k = k |H| 1 k+1\nabstentions.\nThe base case of the induction concerns the number of abstentions before the first mistake. In this regime, s = |H| k k+1 , and \u00b5 > s, so there are at least |H| k k+1 hypotheses removed from V every iteration for predicting the wrong label. Since |V| = |H| at first, the algorithm abstains at most |H| |H| k k+1 = |H| 1 k+1 times before the first mistake.\nCombining this with the inductive hypothesis, the total number of \u22a5s is \u2264 k |H| 1 k+1 + |H| 1 k+1 = (k + 1) |H| 1 k+1 , which completes the induction.\nIn the original paper [SZB10], the analysis follows that of the \u201dEgg Dropping Game,\u201d a common brainteaser, which can be found in [GF08]. It appears to this author that the analysis of [SZB10] can in fact be significantly tightened and enhanced using further results in [GF08], which utilize a more complex inductive argument. As this is unpublished, however, we limit ourselves to pointing it out as an open problem.\nIn any case, it is apparent that even for small k, allowing a few mistakes leads to vast improvements over Algorithm 1 in the dependence on |H| - from O (|H|) to O ( |H| 1 k+1 ) . For instance, allowing just k = 1 mistake yields\na bound of 2 \u221a |H|.\nThis again highlights one of the most fundamental ideas in this survey - the tradeoff between predicting sufficiently often and avoiding prediction errors. Though zero-error learning is significant in its own right, it is a rather extreme condition that implies a similarly extreme cost of a mistake relative to the cost of abstaining. Most applications would be content to allow a few mistakes in return for a significantly more useful classifier (one that abstains less). This promise is exactly what Prop. 2.4 quantifies."}, {"heading": "2.3.2 Tractable Relaxed Enumeration for Linear Separators", "text": "Algorithm 2 in general suffers from the same computational intractability as Algorithm 1, because it too appears to require the version space to be explicitly stored. However, this can be circumvented in useful special cases, including when learning monotone disjunctions and linear separators [SZB10]. We discuss the intuition for the latter, as it is instructive; for formal proofs, refer to [SZB10].\nConcretely, let X = Rd be the data space with the Euclidean norm, and let H be the set of linear separators {w \u2208 Rd : \u2016w\u2016 = 1}, so that the prediction rule is hw(x) = sgn(w \u00b7 x). Denote the optimum separator by w\u2217. Assume the data are separable with margin \u03b3 , minx w \u2217\u00b7x \u2016x\u2016 > 0 - this loosely corresponds to the realizability assumption of relaxed enumeration. This is the setup used for the well-known standard perceptron algorithm [KWA97]. If points x1, . . . , xn have already been seen, it will also be useful to think of the problem as a linear program with d variables (one for each coordinate of w) and n halfspace constraints w \u00b7 xi > 0 for {i : yi = +1} and w \u00b7 xi < 0 for {i : yi = \u22121}. [SZB10] show how to efficiently implement Algorithm 2 here using the following insight: the algorithm does not need to store V , because its decision-making process only requires it to deal with |V| and estimate |{h \u2208 V : h(xi) = \u00b11}|. The first quantity, |V|, is the volume of the feasible set of the linear program (LP) mentioned above, and it contracts significantly whenever a mistake is made. The \u03b3-separability assumption implies that this feasible set cannot shrink below a certain volume, as there is some room around w\u2217 where consistent separators can fall. Combining these two facts controls the number of mistakes made, and finishes the use of |V|. However, the algorithm still needs to estimate the quantities |{h \u2208 V : h(xi) = \u00b11}| for arbitrary xi to decide whether or not to abstain. This is essentially the problem of estimating the relative volumes of two convex sets. Using Hit-and-Run [LV06], a tool for uniformly sampling from convex bodies, we can simply sample uniformly from the feasible set of the current LP, and see whether the sampled point is on the +1 or \u22121 side. Repeatedly sampling points essentially extends this idea to a Monte Carlo estimate of the relative volumes of |{h \u2208 V : h(xi) = +1}| and |{h \u2208 V : h(xi) = \u22121}|, which the algorithm can use to make its decision. It turns out that the number of points required can be managed to yield a polynomial-time algorithm which is equivalent to Algorithm 2 with high probability. Formalizing all this leads to the main result, which nicely complements the well-known perceptron mistake bound of O ( \u03b3\u22122 ) [SS12]:\nProposition 2.5 ( [SZB10]). Assume data are linearly \u03b3-separable in Rd, and define R = ( 1 + \u221a d \u03b3 )d . Then for\nany k > 0, a linear separator can be learned with at most k mistakes and O ( R1/k logR ) abstentions. Omitting logarithmic factors, this bound on\u22a5 is\u2248 O ((\u221a\nd \u03b3\n)d/k) for small \u03b3. This implies the appealing fact that\nthe mistake tolerance k needs only to scale linearly in the dimensionality d to achieve a fixed-exponent polynomial dependence on \u03b3.\nNote that although Algorithms 1 and 2 were formulated for finite H with the counting measure for simplicity, we have just sketched a tractable implementation for an infinite H with the Lebesgue measure. This type of efficient Monte Carlo sampling can widely broaden the potential usability of the algorithms; for instance, linear separators constitute a very usefulH in applications."}, {"heading": "3 Abstaining in Statistical Learning", "text": "We now change course somewhat to examine the effect of the abstain option in a standard statistical learning setting, where unlabeled data points xi are drawn IID from a distribution that is fixed beforehand. This can be thought of as a variation of the PAC (Probably Approximately Correct) model which is standard in learning theory.\nAs mentioned earlier, this setting is more restrictive than the KWIK model. It imposes an extra IID assumption on the data generation process, and also treats the batch case in which we begin tracking the algorithm\u2019s performance after it has already seen m labels, rather than penalizing the algorithm for performance at the start of the learning process as well. As might be expected, these more restrictive assumptions lead to a wider range of stronger results than are known for the KWIK case. These results will now be introduced.\nFormally, we now consider a standard batch learning setting where we have a training set S = {(x1, y1), . . . , (xm, ym)} of points drawn i.i.d. from a distribution D. The learning algorithm trains on these points and thereafter sees no more labels. To emphasize the point, the learner will never even see the labels of points on which it predicts \u22a5 (recall that this was the case in KWIK). Define the error of a hypothesis (h) = PrD(h(x) 6= y); this is analogous to the number of mistakes in the KWIK framework. Similarly, rather than speaking of the total number of abstentions, the appropriate quantity to discuss will be the probability of abstentions over D. We also will speak of the training error (error on S) \u0302 (h) = 1m \u2211m i=1 1(h(xi) 6= yi).\nWe will otherwise follow the pattern laid out in our KWIK discussion of Section 2, presenting results first for the zero-error case and then addressing the error-abstention tradeoff."}, {"heading": "3.1 An Algorithm for Zero-Error Learning", "text": "As we did in the KWIK overview of Section 2, we begin with the case when the algorithm is not allowed to make any erroneous predictions at all, and attempt to minimize the abstain probability. It is a nontrivial (and recent) result that this is even possible in a general setting. However, we will see how it can be done, and use the resulting algorithm as a point of departure to explore some powerful recent results in this vein.\nRecall that this zero-error scenario\u2019s KWIK analogue (no mistakes) was addressed by Algorithm 1, the enumeration algorithm. It turns out that an almost identical algorithm can be used for the realizable case in the statistical learning setting as well. This was introduced in recent work as the Consistent Selective Strategy (CSS) [EYW10], described in Algorithm 3.\nAlgorithm 3 Consistent Selective Strategy (CSS) 1: Given: training set S = {(x1, y1), . . . , (xm, ym)} 2: V \u2190 {h \u2208 H : h(xj) = yj \u2200j = 1, . . . ,m} 3: for any unlabeled point x do 4: if h(x) is the same \u2200h \u2208 V then 5: Choose any h \u2208 V and predict y\u0302 = h(x) 6: else 7: Output \u22a5\nCSS is essentially a batch version of Algorithm 1, the enumeration KWIK algorithm for the realizable case. It has zero error with certainty for the same reason as mentioned for Algorithm 1 - by construction, h\u2217 \u2208 V , and so any prediction must agree with h\u2217 and therefore be correct.\nAs with Algorithm 1, though, the significance of CSS lies in the fact that it does not abstain too often: Proposition 3.1 (Thm. 8, [EYW10]). For Algorithm 3,\nPrD (\u22a5) \u2264 1\nm\n( (ln 2) min(|H| , |X |) + ln 1\n\u03b4 ) Proof Sketch. Two hypotheses are equivalent if they have the same predictions on all points in X , so there are effectively K = 2min(|H|,|X |) possible subsets of H. Of these, the proof considers only the k \u2264 K which are not unanimous on all the data (only on at most 1 \u2212 of it, for some > 0). For any such subset (call it Gi, i \u2208 {1, . . . , k}), it is very likely that not all m points in the training set are drawn from Gi. This implies that V is different from Gi.\nIn this way, it can be argued that V is different from allGi. Since theGi are characterized only by being unanimous on\u2264 1\u2212 of the data, V must be unanimous on\u2265 1\u2212 of the data. Since we predict exactly when V is unanimous, we must predict quite often, so we cannot abstain very often.\nThe \u0398(1/m) dependence of the abstain probability establishes CSS as a nontrivial algorithm. In fact, there is a significant sense in which it is optimal! Proposition 3.2 (Thm. 7, [EYW10]). In the realizable case, any classifier which has zero error with certainty on any D with any h\u2217 satisfies the following condition: if it predicts (a non-\u22a5 label) on a point x, then so does CSS. Consequently, there is no such classifier with PrD (\u22a5) less than CSS.\nProof Sketch. The argument is given for strategies which do not randomize the choice of whether to abstain on a given point. The proof is by contradiction, so assume there is a classifier A that has zero error for any data distribution, and a point x0 /\u2208 S such that A(x0) 6=\u22a5 and CSS(x0) =\u22a5. By the construction of CSS, there is a hypothesis h2 \u2208 V such that A(x0) 6= h2(x0). Having fixed these facts, we now formulate a new classification problem (using the same H and X ) to prove the contradiction. Consider a new distribution Dn over X which puts positive probability on each of the m points in S and on x0, and zero probability on all other points in X . Also define the new label-generating hypothesis h\u2217n , h2. Then with positive probability (say \u03b4 > 0), an m-element training set drawn from Dn will be equal to S. Now A is deterministic and only depends on the training set, so A will still predict on x0 with probability \u03b4. But when it predicts on x0, it will be erroneous because we established that A(x0) 6= h2(x0) = h\u2217n(x0). So A does not have zero error with certainty, and we have our contradiction.\nThe argument for methods which randomize the abstain decision follows along the same lines.\nOf course, Prop. 3.2 does not preclude the existence of a classifier which abstains less than CSS but only has zero error with high probability. Nevertheless, the result establishes the conceptual importance of CSS as the prototypical zero-error learning algorithm in statistical learning. It is particularly notable that the argument carries over to randomized algorithms as well - all of them are trumped in this setting by the deterministic CSS.\nThis author is not aware of this optimality argument being made for the variety of other extant CSS-like algorithms, though the logic is potentially extensible to these algorithms. It could be a particularly useful perspective to mention in the KWIK case for the enumeration algorithm, for which a potential combinatorial extension of this result is currently being examined by this author as a byproduct of the investigations of this manuscript.\n3.2 Zero-Error Learning in the Infinite-H Case\nInterestingly, these favorable properties of CSS hold only for finite hypothesis classes; in fact, it is impossible in general for CSS to avoid abstaining everywhere whenH is infinite, even in the realizable case. The counterexample construction used to show this is of independent interest, and will now be outlined.\nThe construction is schematically shown in Figure 3.1. H is the class of linear separators in R2, and the data are uniformly distributed on two nonintersecting arcs, with one containing only positive examples and the other only negative examples as shown in the figure. It is clear that this is a realizable problem, because any separator lying between the arcs that does not intersect either one will have zero error. However, for any finite training set the CSS version space V will only be unanimous for data in a convex polygon-like region inscribed in each arc; an example is the two shaded regions in Figure 3.1. Unfortunately, these regions only contain a finite number of data points which collectively have measure zero. So although CSS will predict correctly where it essays a prediction, it becomes a trivial algorithm which will almost surely abstain.\nThis construction is a valuable one because similar arguments to the above make it a \u201dhard\u201d situation for many algorithms that depend on version space constructions and/or unanimity between hypotheses, which arise in other fields like active learning [DHM07]. Loosely speaking, it has the property thatH intuitively does not \u201dfit\u201d the data as represented; linear separators are not a natural choice for data whose intrinsic geometry is nonlinear. We will revisit this construction briefly later."}, {"heading": "3.3 Characterizing the Abstain-Error Tradeoff", "text": "We now relax the requirement of zero error from the abstaining classifier and address how abstention rates can be reduced if a low error rate is allowed. The discussion here is the IID-setting analogue to that in Sec. 2.3.\nThe tradeoff is illustrated in Figure 3.2, in which it is termed the risk-coverage tradeoff. Coverage is simply the probability that an algorithm predicts rather than abstaining, and therefore is exactly the complement of the abstain probability PrD (\u22a5) we have used so far. Risk is a generalization of error probability to arbitrary loss functions. For our purposes hereafter, it can be taken as synonymous with error probability.\nThe tradeoff for any algorithm is schematically shown by the dotted curve in the figure: lower risk can only be achieved at the expense of coverage. The upper axis (with r\u2217) represents the case when the algorithm is not allowed to abstain; this is the scenario in standard binary classification, and r\u2217 represents the risk of the algorithm in this case. The left axis represents the zero-error case that CSS addresses, and c\u2217 is the coverage here. Most\ninterestingly, upper and lower \u201denvelopes\u201d on this tradeoff graph can be proved for abstaining classifiers, setting limits on the risk-coverage tradeoff in the statistical learning setting.\nWe present the upper envelope only, as it is the more fundamental result as proved in [EYW10].\nProposition 3.3 (Thm. 37 from [EYW10]). Suppose d is the VC dimension of H, and fix \u03b4 \u2208 [0, 14 ]. Consider any classifier A with the abstain capability, and suppose it has abstain probability \u03c1 and has error rate R(A) on examples on which it predicts. Then there is a data distribution D such that with probability \u2265 \u03b4,\nR(A) \u2265 min ( 1\n2 \u2212 1 4\u03c1 , 1 2 \u2212 1 2\u03c1 + 1 16\u03c1m\n( d+ 16\n3 ln(1\u2212 2\u03b4)\n))\nNo proof is given here for space reasons, but the reader is referred to [EYW10] to examine the interesting D construction and the associated VC dimension argument central to the proof.\nThe lower envelope proof involves interpolating between the two extremes of standard learning and CSS by considering a CSS-like classifier that predicts when V is unanimous, but flips a coin of bias \u03b1 \u2208 [0, 1] to decide whether to predict even when V is not unanimous. (Clearly, the \u03b1 = 0 case corresponds to CSS and \u03b1 = 1 to standard learning). The lower envelope is proved only for this \u03b1-parametrized classifier in [EYW10]."}, {"heading": "3.4 Relaxing CSS for the Non-Realizable Case", "text": "Though we have characterized the fundamental abstention tradeoff, our presentation so far has lacked an algorithm which, like Algorithm 2 in the KWIK setting, exploits the tradeoff. We use this section and Section 3.5 to explore two such methods in turn.\nThe first method we outline is directly inspired by CSS, and generalizes it to the non-realizable case. Here the target hypothesis h\u2217 = arg min\nh\u2208H (h); the difference from the realizable case is that (h\u2217) 6= 0 in general. The\nlogic of CSS fails here, because h\u2217 is not necessarily in the version space V as calculated on the training set S; indeed, it is possible that V = \u2205. However, the IID assumption on the data means that S is fairly representative of the data distribution D. Therefore, if we relax the definition of V to include all low-training-error hypotheses (rather than zero-training-error hypotheses), we can trust that since \u0302 (h\u2217) \u2248 (h\u2217), h\u2217 will be in this newly defined Vrelaxed. This is precisely what is done in [WEY11]. For convenience, we dub the resulting method \u201dRelaxed CSS.\u201d It is given in Algorithm 4.\nAlgorithm 4 Relaxed CSS 1: Given: training set S = {(x1, y1), . . . , (xm, ym)}, algorithm failure probability \u03b4, VC dim. ofH = d 2: h\u0302\u2217 \u2190 arg minh\u2208H \u0302 (h)\n3: Vrelaxed \u2190 { h \u2208 H : \u0302 (h) \u2264 \u0302 ( h\u0302\u2217 ) + 4 \u221a 2d ln(2me/d)+ln(8/\u03b4)m } 4: for any unlabeled point x do 5: if h(x) is the same \u2200h \u2208 Vrelaxed then 6: Choose any h \u2208 Vrelaxed and predict y\u0302 = h(x) 7: else 8: Output \u22a5\nThis algorithm is shown in [WEY11] to satisfy good error and coverage bounds (with probability at least 1\u2212\u03b4 over the choice of S) if the \u201dexcess error\u201d 1(h(x) 6= y) \u2212 1(h\u2217(x) 6= y) is well-behaved over D for all h \u2208 H. More precisely, its error on the data on which it predicts is at most the error of h\u2217 on those data, with high probability. The full results are in [WEY11]; we omit them here due to space constraints in favor of discussing an interesting implementation trick which provides further connections to other work.\nAlgorithm 4, like CSS and both flavors of enumeration, faces the implementation difficulty of tracking a potentially exponentially large Vrelaxed so that it can detect unanimity or lack thereof. However, this task can be reduced to a suitably chosen empirical error minimization, as indicated in [WEY11]:\nProposition 3.4 (Paraphrase of Lemma 6.1 in [WEY11]). For any x \u2208 X , define h\u0302\u2217 as in the algorithm and h\u0303x , arg min\nh\u2208H {\u0302 (h) | h(x) 6= h\u0302\u2217(x)}. Then Algorithm 4 outputs\u22a5 iff \u0302\n( h\u0303x ) \u2212\u0302 ( h\u0302\u2217 ) \u2264 4 \u221a 2d ln(2me/d)+ln(8/\u03b4)m .\nProof. Working backwards, the condition \u0302 ( h\u0303x ) \u2212 \u0302 ( h\u0302\u2217 ) \u2264 4 \u221a 2d ln(2me/d)+ln(8/\u03b4)m is equivalent to h\u0303x \u2208 Vrelaxed. By construction, h\u0303x(x) 6= h\u0302\u2217(x). Both hypotheses are in Vrelaxed, so Vrelaxed is not unanimous on x, and the algorithm therefore abstains.\nThis simple argument reduces the generally intractable task of storing a version space to the task of empirical error minimization, which lacks the space complexity of the former and can be done efficiently in many cases (including for infinite H). For this reason, it appears in several version space-based algorithms (including CSS, for which a variant is presented in [EYW10]). This is discussed in Section 4.2."}, {"heading": "3.5 Aggregating Classifiers to Exploit the Tradeoff", "text": "We now turn our attention to a conceptually elegant algorithm that exploits the tradeoff in the statistical learning setting by taking a weighted majority (WM) vote of the classifiers in H. We have so far explored two algorithms that generalize the zero-error strategy in different settings in an attempt to address the tradeoff: the relaxed CSS of Algorithm 4 and the relaxed enumeration strategy of Algorithm 2. Reexamining these will motivate the new WM voting method that we cover in this section.\nAlgorithm 4 generalizes CSS to the non-realizable case - essential to address the tradeoff - by relaxing the strict training set consistency requirement on the version space in CSS, instead considering the lowest-\u0302 (h) hypotheses in H. Independently, Algorithm 2 generalizes enumeration to allow some mistakes in return for fewer abstentions - directly addressing the tradeoff - by relaxing the unanimity requirement on the version space for a prediction. Instead, it makes predict-or-abstain decisions based on the margin of a vote taken among hypotheses in the version space.\nCombining these two separate relaxation ideas, a sensible algorithm could make predict-or-abstain decisions based on the margin of a vote over the lowest-\u0302 (h) hypotheses in H, instead of assessing strict unanimity among only hypotheses with \u0302 (h) = 0. This precise intuition can be realized by weighting each hypothesis h according to \u0302 (h) and then deciding whether to predict based on the WM vote. The resulting algorithm, described in [FMS04], is shown as Algorithm 5 for finiteH.\nAlgorithm 5 Weighted Majority Voting with Abstention 1: Given: training set S = {(x1, y1), . . . , (xm, ym)}, failure probability \u03b4, tuning parameter \u03b8 \u2208 (0, 1/2)\n2: \u03b7 \u2190 ln(8 |H|)m 12\u2212\u03b8, \u2206\u2190 2 \u221a ln( \u221a 2/\u03b4) m + \u03b7 8m 3: for any unlabeled point x do\n4: L\u0302\u03b7(x) , 1\n\u03b7 ln  \u2211 h:h(x)=+1 e\u2212\u03b7\u0302(h)\n\u2211 h:h(x)=\u22121 e\u2212\u03b7\u0302(h)  5: if\n\u2223\u2223\u2223L\u0302\u03b7(x)\u2223\u2223\u2223 > \u2206 then 6: Predict y\u0302 = sgn(L\u0302\u03b7(x)) 7: else 8: Output \u22a5\nHere each hypothesis h are weighted as e\u2212\u03b7\u0302(h), so that the high-weight hypotheses have low empirical error - an implicit encoding of the idea behind Algorithm 4, because only the high-weight hypotheses participate much in the vote. \u03b7 > 0 is a learning rate that controls how much the weighting deviates from the default counting measure on H; a larger training set leads to a larger \u03b7, implying greater confidence that the up-weighted hypotheses are indeed the ones with lowest true error (h). (To see this, consider the limit m \u2192 \u221e, in which case \u2200h \u2208 H : \u0302 (h) \u2192 (h), and the weighting considers only arg minh\u2208H (h) while ignoring all other hypotheses). It can also be seen that the L\u0302\u03b7(x) thresholding achieves the same effect as Algorithm 2: predict if the vote is enough of a landslide, and otherwise abstain.\nAs with the work we have previously examined, the theoretical guarantees proved for this algorithm include bounds on the abstain and error probabilities:\nProposition 3.5 (Corollary 1 in [FMS04]). The error probability of Algorithm 5 is at most 2 (h\u2217)+O\u0303 ( m\u03b8\u2212 1 2 ) +\u03b4.\nAlso, for m \u2208 \u2126 ( ( \u221a ln(1/\u03b4) ln(|H|))1/\u03b8 ) , the abstain probability is at most 5 (h\u2217) +O (\u221a ln(1/\u03b4)+ln(|H|)\nm 1 2 \u2212\u03b8\n) .\nProof Sketch. The proof idea for the abstain bound is fairly standard and relies on concentration of L\u0302\u03b7(x) about L\u03b7(x), its non-empirical analogue in which \u0302 (h) is replaced by (h) in the calculations. The abstain threshold \u2206 therefore can be seen to quantify the finite-sample uncertainty in estimating L\u03b7(x) by L\u0302\u03b7(x).\nThe error bound, however, is proved differently. It essentially argues that the vote is dominated by \u201dgood\u201d hypotheses ( (h) close to (h\u2217)), and uses the tautology that there must be at least one such good hypothesis - h\u2217 itself - to argue that the vote too must perform well.\nThese arguments are quite general compared to other statistical learning arguments we have outlined, and hold equally well for the realizable and agnostic cases. They also hold for infiniteH with an additional explicit assumption that there is a nonzero mass of \u201dgood\u201d hypotheses.\nThese bounds are seen to be quite loose (in their dependence on m) compared to those in Sec. 3.3. However, they do characterize the tradeoff, and exhibit a further interesting property: the error bound is completely independent of H, as the dependence on hypothesis class complexity is corralled in the abstain probability. By comparison,\nthe standard \u201dOccam\u2019s Razor\u201d IID-learning bound on a vanilla binary classifier (h\u2217) + O (\u221a\nln(|H|/\u03b4) m\n) does\ndepend, albeit weakly, on |H| [BEHW87]. This is to our knowledge unique in the literature as another potential benefit of allowing the learner to abstain.\nThough there is no explicit version space here, the predictions of all h \u2208 H are nevertheless necessary, so the same intractability issues that plague CSS and enumeration recur here. This can be mitigated, for instance by carefully constructing a manageably finite \u201d -cover\u201d approximation of H [BB10]. However, it remains open how to use other tactics - such as the sampling and empirical error minimization ones previously mentioned - to address the problem.\nThe successive relaxations of unanimous-version-space arguments that we used to motivate this algorithm have the desirable consequence of making it much more robust. We illustrate this by noting that this WM algorithm is not foiled by the ill-matched geometry in the earlier construction of Figure 3.1. There are plenty of zero-error hypotheses inH in that scenario, and they dominate the vote, potentially leading to zero error for high enough m. Votes like this are well known to be desirable in another respect: they can perform far better than h\u2217 when the constituent hypotheses make diverse predictions, in effect compensating for each other\u2019s mistakes. This has further implications which are discussed in Sec. 5.\nFinally, the WM algorithm used here has several other suggestive interpretations (for instance, if the total L1\nnormalized weight of hypotheses predicting +1 on x is seen as an estimate of PrD (y = +1 | x), then L\u0302\u03b7(x) is basically applying a logit transform). Other connections to previous work on similar weighted majorities in a variety of settings are too numerous to discuss here but highlight the richness of this approach [FS97, AHK12, LW94]."}, {"heading": "4 Connections to Active Learning", "text": "Active learning [Set12] is a well-motivated supervised learning paradigm in which the learner attempts to minimize the number of labels it needs by requesting labels at its own discretion. As mentioned in Section 2.2.1, there is a natural correspondence between a \u22a5 output and a label request, linking abstain-based results with active learning. We have previously interspersed brief references to these connections where appropriate, but will now devote this section to elaborating on how the work outlined in this manuscript relates to active learning.\nA major goal of active learning in the realizable case is to learn a hypothesis of error using onlyO (polylog(1/ )) labels. This represents exponentially fewer labels than required by standard supervised learning, for which \u2126(1/ ) labels are needed [Das11]. This is an ambitious but sometimes achievable goal, as demonstrated by the canonical example of binary search as an active algorithm for learning a threshold on a line segment. It has also been proven for certain restricted special cases, like learning a linear separator when the data are uniformly distributed\non a sphere [FSST97, DKM09], but more general cases under which it is possible are much sought after in the community."}, {"heading": "4.1 Zero-Error Learning", "text": "The online mistake-free enumeration algorithm (Alg. 1), developed for the KWIK setting, has been independently known as the CAL algorithm after the authors who first introduced it in [CAL94], and is the forerunner of a significant strand of theoretically motivated active learning work [BBL06, BDL09, BHLZ10]. This is somewhat surprising because it is extremely conservative in settling for \u22a5, whereas active learning aims to minimize the number of label requests. Indeed, one of the largest open problems in active learning is to design and analyze algorithms which are more \u201daggressive\u201d in their label querying, rather than the more \u201dmellow\u201d CAL [Das11].\nThere is scant mention of this connection to active learning in the existing KWIK literature. However, we observed in Section 3 that the CSS algorithm is exactly analogous to enumeration in the IID-data setting of statistical learning. Recent work [EYW12] has shown a deep link between CSS and CAL (stated in terms of the VC dimension of H, a widely-used measure of the hypothesis class complexity, although other such measures would also suffice): Proposition 4.1 (Paraphrase of Thm.9 from [EYW12]). Suppose H is a hypothesis class with VC dimension d <\u221e. Then if CSS can learn it with PrD (\u22a5) \u2208 O ( polylog(m)\nm\n) , CAL will learn it using only O (polylog(1/ ))\nlabels.\nThis result motivates the further study of CSS to potentially yield highly desirable active learning results. A good beginning in this vein is also proved in [EYW12], using computational geometry tools from [EYW10] to show that CAL requests O (polylog(1/ )) labels when H is the class of linear separators in Rd and D is a mixture of a fixed finite number of Gaussians. There remain concerns about how such approaches scale with the dimension d (an argument is made in [EYW12] that CAL must request exponentially many labels in d), indicating that CAL has its limitations. More aggressive query strategies may be needed."}, {"heading": "4.2 Agnostic Learning", "text": "The idea of Algorithm 4 - to relax the strict training set consistency requirement of the version space in CSS and instead consider all hypothesis h with low enough \u0302 (h) - led to a major advance in active learning when it was successfully applied to CAL in [BBL06].\nThe implementation trick outlined earlier for Algorithm 4 - minimizing the empirical risk of a hypothetical dataset as a means of detecting unanimity among hypotheses in a relaxed version space - has also appeared several times in the abstention-related literature and in active learning [EYW10, SZB10, BHLZ10, DHM07, BBL06]. The \u201dexcess error\u201d is assumed to be well-behaved in the analysis of the relaxed CSS algorithm; such assumptions on the same quantity have been partially studied in active learning as the Tsybakov noise conditions [Tsy04], under which efficient active learning algorithms with provably favorable properties have been derived. In fact, the excess error turns out to be fundamental to this line of active learning work, as made explicit recently in [ABE12]."}, {"heading": "5 Future Work and Conclusion", "text": "The study of abstaining classifiers is far from complete, and there are several open areas to be addressed. Prominent among them is the study of abstaining classifiers without a version space. The version space idea has been central to much work so far because of its thoroughly-explored utility in the realizable case when zero error is allowed. This is somewhat of a toy case compared to most real applications, however, and necessitates constant efforts to make the resulting algorithms more tractable. It is also an unnecessarily brittle notion, as the construction of Figure 3.1 suggests.\nAnother application-motivated open problem is to refine the insights of the work discussed here to be sensitive to the relative costs of abstaining and erring on a prediction. These costs generally drive the tradeoff in applications and could therefore be assumed to be known beforehand. An ambitious goal in this spirit is to devise algorithms which can precisely quantify their own confidence in a prediction, in a probabilistic manner.\nAbstaining classifiers are also intimately connected to active learning, as discussed in Section 4. A range of ideas, from implementation tricks to algorithmic intuitions to the specific algorithms themselves, have been shown to recur in both fields. At present, there is very little work like [EYW12] that attempts to explicitly unify the fields\nfurther. Both fields have a long history and have similar goals, yet there are several ideas, like relaxed enumeration in the KWIK setting and the disagreement coefficient in active learning [BBL06], that are currently explored asymmetrically in one field. Progress on cross-pollinating these ideas would be of mutual enrichment to both areas.\nAnother area of potentially fruitful research (on which we have made progress) concerns improvements on Algorithm 5, the weighted aggregation algorithm. We observed that this algorithm can be seen as synthesizing intuitions from relaxed enumeration and relaxed CSS, and so we believe the classifier aggregation approach for abstention is worth deeper examination. In particular, majority votes are remarkably robust, and are known to perform better than any of their constituents when the constituent predictions are diverse [KW03]. Hypothesis weighting strategies that maximize this diversity could provably help performance [WLR06], both for abstaining classifiers and in active learning.\nWe hope this survey provides some insight into recent approaches to designing classifiers that can abstain, which should see increased deployment in various applications over the coming years."}, {"heading": "Acknowledgments", "text": "I would like to thank my committee - David Kriegman, Sanjoy Dasgupta, and Kamalika Chaudhuri - for their time and feedback at various stages of the research exam process. Thanks are also due to Yoav Freund for first introducing me to this area and for various helpful suggestions."}], "references": [{"title": "Large-scale bandit problems and kwik learning", "author": ["Jacob Abernethy", "Kareem Amin", "Moez Draief", "Michael Kearns"], "venue": "In ICML,", "citeRegEx": "Abernethy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2013}, {"title": "Active learning using smooth relative regret approximations with applications", "author": ["Nir Ailon", "Ron Begleiter", "Esther Ezra"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Ailon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2012}, {"title": "The multiplicative weights update method: a metaalgorithm and applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "A discriminative model for semi-supervised learning", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "J. ACM,", "citeRegEx": "Balcan and Blum.,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2010}, {"title": "Agnostic active learning without constraints", "author": ["Alina Beygelzimer", "Daniel Hsu", "John Langford", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2010}, {"title": "Improving generalization with active learning", "author": ["David A. Cohn", "Les E. Atlas", "Richard E. Ladner"], "venue": "Machine Learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "On optimum recognition error and reject tradeoff", "author": ["C. Chow"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Chow.,? \\Q1970\\E", "shortCiteRegEx": "Chow.", "year": 1970}, {"title": "Two faces of active learning", "author": ["Sanjoy Dasgupta"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Dasgupta.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta.", "year": 2011}, {"title": "A general agnostic active learning algorithm", "author": ["Sanjoy Dasgupta", "Daniel Hsu", "Claire Monteleoni"], "venue": null, "citeRegEx": "Dasgupta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2007}, {"title": "Analysis of perceptron-based active learning", "author": ["Sanjoy Dasgupta", "Adam Tauman Kalai", "Claire Monteleoni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dasgupta et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2009}, {"title": "On the foundations of noise-free selective classification", "author": ["Ran El-Yaniv", "Yair Wiener"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "El.Yaniv and Wiener.,? \\Q2010\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2010}, {"title": "Active learning via perfect selective classification", "author": ["Ran El-Yaniv", "Yair Wiener"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "El.Yaniv and Wiener.,? \\Q2012\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2012}, {"title": "Generalization bounds for averaged classifiers", "author": ["Yoav Freund", "Yishay Mansour", "Robert Schapire"], "venue": "The Annals of Statistics,", "citeRegEx": "Freund et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2004}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Yoav Freund", "H. Sebastian Seung", "Eli Shamir", "Naftali Tishby"], "venue": "Machine Learning,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["Ludmila I. Kuncheva", "Christopher J. Whitaker"], "venue": "Machine Learning,", "citeRegEx": "Kuncheva and Whitaker.,? \\Q2003\\E", "shortCiteRegEx": "Kuncheva and Whitaker.", "year": 2003}, {"title": "The perceptron algorithm versus winnow: Linear versus logarithmic mistake bounds when few input variables are relevant (technical note)", "author": ["Jyrki Kivinen", "Manfred K. Warmuth", "Peter Auer"], "venue": "Artif. Intell.,", "citeRegEx": "Kivinen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 1997}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["Nick Littlestone"], "venue": "In Machine Learning,", "citeRegEx": "Littlestone.,? \\Q1988\\E", "shortCiteRegEx": "Littlestone.", "year": 1988}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Lihong Li", "Michael L. Littman", "Thomas J. Walsh", "Alexander L. Strehl"], "venue": "Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Hit-and-run from a corner", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz", "Santosh Vempala"], "venue": "SIAM J. Comput.,", "citeRegEx": "Lov\u00e1sz and Vempala.,? \\Q2006\\E", "shortCiteRegEx": "Lov\u00e1sz and Vempala.", "year": 2006}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "In FOCS,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1989\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1989}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Inf. Comput.,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Active Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["Burr Settles"], "venue": null, "citeRegEx": "Settles.,? \\Q2012\\E", "shortCiteRegEx": "Settles.", "year": 2012}, {"title": "Agnostic kwik learning and efficient approximate reinforcement learning", "author": ["Istv\u00e1n Szita", "Csaba Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Szita and Szepesv\u00e1ri.,? \\Q2011\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri.", "year": 2011}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "Trading off mistakes and don\u2019t-know predictions", "author": ["Amin Sayedi", "Morteza Zadimoghaddam", "Avrim Blum"], "venue": null, "citeRegEx": "Sayedi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sayedi et al\\.", "year": 2010}, {"title": "Optimal aggregation of classiers in statistical learning", "author": ["Alexandre Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Tsybakov.,? \\Q2004\\E", "shortCiteRegEx": "Tsybakov.", "year": 2004}, {"title": "Agnostic selective classification", "author": ["Yair Wiener", "Ran El-Yaniv"], "venue": null, "citeRegEx": "Wiener and El.Yaniv.,? \\Q2011\\E", "shortCiteRegEx": "Wiener and El.Yaniv.", "year": 2011}, {"title": "Exploring compact reinforcement-learning representations with linear regression", "author": ["Thomas J. Walsh", "Istvan Szita", "Carlos Diuk", "Michael L. Littman"], "venue": null, "citeRegEx": "Walsh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Walsh et al\\.", "year": 2009}], "referenceMentions": [], "year": 2015, "abstractText": "We explore the problem of binary classification in machine learning, with a twist the classifier is allowed to abstain on any datum, professing ignorance about the true class label without committing to any prediction. This is directly motivated by applications like medical diagnosis and fraud risk assessment, in which incorrect predictions have potentially calamitous consequences. We focus on a recent spate of theoretically driven work in this area that characterizes how allowing abstentions can lead to fewer errors in very general settings. Two areas are highlighted: the surprising possibility of zero-error learning, and the fundamental tradeoff between predicting sufficiently often and avoiding incorrect predictions. We review efficient algorithms with provable guarantees for each of these areas. We also discuss connections to other scenarios, notably active learning, as they suggest promising directions of further inquiry in this emerging field.", "creator": "LaTeX with hyperref package"}}}