{"id": "1512.08787", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2015", "title": "Matrix Completion Under Monotonic Single Index Models", "abstract": "Most late results in matrix further choosing possible the algebra instead deciding latter increasing - battalion type that the columns they another goes union more cost - rank subspaces. In obviously - hopes settings, though, the variation pattern changes these example another broadly by the (more unknown) nonlinear transformation. This paper addresses entire challenge another multiplication completion in the face is such ferromagnets. Given part few observations though given matrix that are contents began proper a Lipschitz, indexicality formula_9 making next market relative compute, mind task is set estimate present remaining uninitialized printings. We legislation his drama compute operational method that non-consecutive for low - rank non-negative methodology and rectified function calculate to 1.4 which days matrix define. Mean rotated slight bounds provide insight into how good form formula_5 simply be estimated based on the normally, rank of now matrix along addition bringing the fourier theory. Empirical results then polypropylene there turn - its anonymized demonstrate main competitiveness than new proposed fairly.", "histories": [["v1", "Tue, 29 Dec 2015 20:52:41 GMT  (148kb,D)", "http://arxiv.org/abs/1512.08787v1", "21 pages, 5 figures, 1 table. Accepted for publication at NIPS 2015"]], "COMMENTS": "21 pages, 5 figures, 1 table. Accepted for publication at NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ravi sastry ganti mahapatruni", "laura balzano", "rebecca willett"], "accepted": true, "id": "1512.08787"}, "pdf": {"name": "1512.08787.pdf", "metadata": {"source": "CRF", "title": "Matrix Completion Under Monotonic Single Index Models", "authors": ["Ravi Ganti", "Laura Balzano"], "emails": ["gantimahapat@wisc.edu", "girasole@umich.edu", "rmwillett@wisc.edu"], "sections": [{"heading": "1 Introduction", "text": "In matrix completion, one has access to a matrix with only a few observed entries, and the task is to estimate the entire matrix using the observed entries. This problem has a plethora of applications such as collaborative filtering, recommender systems [1] and sensor networks [2]. Matrix completion has been well studied in machine learning, and we now know how to recover certain matrices given a few observed entries of the matrix [3, 4] when it is assumed to be low rank. Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8]. While recent work has focused on relaxing the incoherence and sampling conditions under which matrix completion succeeds, there has been little work for matrix completion when the underlying matrix is of high rank. More specifically, we shall assume that the matrix that we need to complete is obtained by applying some unknown, non-linear function to each element of an unknown low-rank matrix. Because of the application of a non-linear transformation, the resulting ratings matrix tends to have a large rank. To understand the effect of the application of non-linear transformation on a low-rank matrix, we shall consider the following simple experiment: Given an n \u00d7m matrix X , let X = \u2211m i=1 \u03c3iuiv > i be its SVD. The rank of the matrix X is the number of non-zero singular values. Given an \u2208 (0, 1), define the effective rank of X as follows:\nr (X) = min { k \u2208 N : \u221a\u2211m j=k+1 \u03c3 2 j\u2211m\nj=1 \u03c3 2 j\n\u2264 } . (1)\nar X\niv :1\n51 2.\n08 78\n7v 1\n[ st\nat .M\nL ]\nThe effective rank of X tells us the rank k of the lowest rank approximator X\u0302 that satisfies\n||X\u0302 \u2212X||F ||X||F \u2264 . (2)\nIn figure (1), we show the effect of applying a non-linear monotonic function g?(z) = 11+exp(\u2212cz) to the elements of a low-rank matrix Z. As c increases both the rank of X and its effective rank r (X) grow rapidly with c, rendering traditional matrix completion methods ineffective even in the presence of mild nonlinearities."}, {"heading": "1.1 Our Model and contributions", "text": "In this paper we consider the high-rank matrix completion problem where the data generating process is as follows: There is some unknown matrix Z? \u2208 Rn\u00d7m with m \u2264 n and of rank r m. A non-linear, monotonic, L- Lipschitz function g? is applied to each element of the matrix Z? to get another matrix M?. A noisy version of M?, which we call X , is observed on a subset of indices denoted by \u2126 \u2282 [n]\u00d7 [m].\nM?i,j = g ?(Z?i,j), \u2200i \u2208 [n], j \u2208 [m] (3)\nX\u2126 = (M ? +N)\u2126 (4)\nThe function g? is called the transfer function. We shall assume that E[N ] = 0, and the entries of N are i.i.d. We shall also assume that the index set \u2126 is generated uniformly at random with replacement from the set [n] \u00d7 [m] 1. Our task is to reliably estimate the entire matrix M? given observations of X on \u2126. We shall call the above model as Monotonic Matrix Completion (MMC). To illustrate our framework we shall consider the following two simple examples. In recommender systems users are required to provide discrete ratings of various objects. For example, in the Netflix problem users are required to rate movies on a scale of 1 \u2212 5 2. These discrete scores can be thought of as obtained by applying a rounding function to some ideal real valued score matrix given by the users. This real-valued score matrix may be well modeled by a low-rank matrix, but the application of the rounding function 3 increases the rank of the original low-rank matrix. Another important example is that of completion of Gaussian kernel matrices. Gaussian kernel matrices are used in kernel based learning methods. The Gaussian kernel matrix of a set of n points is an n\u00d7 n matrix obtained by applying the Gaussian function on an underlying Euclidean distance matrix. The Euclidean distance matrix is a low-rank matrix [9]. However, in many cases one cannot measure all pair-wise distances between objects, resulting in an incomplete Euclidean distance matrix and hence an incomplete kernel matrix. Completing the kernel matrix can then be viewed as completing a matrix of large rank.\nIn this paper we study this matrix completion problem and provide algorithms with provable error guarantees. Our contributions are as follows:\n1. In Section (3) we propose an optimization formulation to estimate matrices in the above described context. In order to do this we introduce two formulations, one using a squared\n1By [n] we denote the set {1, 2 . . . , n} 2This is typical of many other recommender engines such as Pandora.com, Last.fm and Amazon.com. 3Technically the rounding function is not a Lipschitz function but can be well approximated by a Lipschitz\nfunction.\nloss, which we call MMC - LS, and another using a calibrated loss function, which we call as MMC \u2212 c. For both these formulations we minimize w.r.t. M? and g?. This calibrated loss function has the property that the minimizer of the calibrated loss satisfies equation (3).\n2. We propose alternating minimization algorithms to solve our optimization problem. Our proposed algorithms, called MMC\u2212c and MMC-LS, alternate between solving a quadratic program to estimate g? and performing projected gradient descent updates to estimate the matrix Z?. MMC outputs the matrix M\u0302 where M\u0302i,j = g\u0302(Z\u0302i,j).\n3. In Section (4) we analyze the mean squared error (MSE) of the matrix M\u0302 returned by one step of the MMC\u2212 c algorithm. The upper bound on the MSE of the matrix M\u0302 output by MMC depends only on the rank r of the matrix Z? and not on the rank of matrix M?. This property makes our analysis useful because the matrix M? could be potentially high rank and our results imply reliable estimation of a high rank matrix with error guarantees that depend on the rank of the matrix Z?.\n4. We compare our proposed algorithms to state-of-art implementations of low rank matrix completion on both synthetic and real datasets (Section 5). We develop an ADMM algorithm to solve the quadratic program that is used to estimate g?. We believe that the proposed ADMM algorithm is useful in its own right and can be used in general isotonic regression problems elsewhere [10, 11]."}, {"heading": "2 Related work", "text": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8]. The recovery techniques proposed in these papers solve a convex optimization problem that minimizes the nuclear norm of the matrix subject to convex constraints. Progress has also been made on designing efficient algorithms to solve the ensuing convex optimization problem [12, 13, 14, 15]. Recovery techniques based on nuclear norm minimization guarantee matrix recovery under the condition that a) the matrix is low rank, b) the matrix is incoherent or not very spiky, and c) the entries are observed uniformly at random. Literature on high rank matrix completion is relatively sparse. When columns or rows of the matrix belong to a union of subspaces, then the matrix tends to be of high rank. For such high rank matrix completion problems algorithms have been proposed that exploit the fact that multiple low-rank subspaces can be learned by clustering the columns or rows and learning subspaces from each of the clusters. While Eriksson et al. [16] suggested looking at the neighbourhood of each incomplete point for completion, [17] used a combination of spectral clustering techniques as done in [18, 19] along with learning sparse representations via convex optimization to estimate the incomplete matrix. Singh et al. [20] consider a certain specific class of high-rank matrices that are obtained from ultra-metrics. In [21] the authors consider a model similar to ours, but instead of learning a single monotonic function, they learn multiple monotonic functions, one for each row of the matrix. However, unlike in this paper, their focus is on a ranking problem and their proposed algorithms lack theoretical guarantees.\nDavenport et al [22] studied the one-bit matrix completion problem. Their model is a special case of the matrix completion model considered in this paper. In the one-bit matrix completion problem we assume that g? is known and is the CDF of an appropriate probability distribution, and the matrix X is a boolean matrix where each entry takes the value 1 with probability Mi,j , and 0 with probability 1\u2212Mi,j . Since g? is known, the focus in one-bit matrix completion problems is accurate estimation of Z?.\nTo the best of our knowledge the MMC model considered in this paper has not been investigated before. The MMC model is inspired by the single-index model (SIM) that has been studied both in statistics [10, 11] and econometrics for regression problems [23, 24]. Our MMC model can be thought of as an extension of SIM to matrix completion problems."}, {"heading": "3 Algorithms for matrix completion", "text": "Our goal is to estimate g? and Z? from the model in equations (3- 4). We approach this problem via mathematical optimization. Before we discuss our algorithms, we mention in brief an algorithm\nfor the problem of learning Lipschitz, monotonic functions in 1- dimension. This algorithm will be used for learning the link function in MMC.\nThe LPAV algorithm: Suppose we are given data (p1, y1), . . . (pn, yn), where p1 \u2264 p2 . . . \u2264 pn, and y1, . . . , yn are real numbers. Let G def = {g : R \u2192 R, g is L-Lipschitz and monotonic}. The\nLPAV 4 algorithm introduced in [11] outputs the best function g\u0302 in G that minimizes \u2211n i=1(g(pi)\u2212 yi) 2. In order to do this, the LPAV first solves the following optimization problem:\nz\u0302 = arg min z\u2208Rn\n\u2016z \u2212 y\u201622 s.t. 0 \u2264 zj \u2212 zi \u2264 L(pj \u2212 pi) if pi \u2264 pj (5)\nwhere, g\u0302(pi) def = z\u0302i. This gives us the value of g\u0302 on a discrete set of points p1, . . . , pn. To get g\u0302 everywhere else on the real line, we simply perform linear interpolation as follows:\ng\u0302(\u03b6) =  z\u03021, if \u03b6 \u2264 p1 z\u0302n, if \u03b6 \u2265 pn \u00b5z\u0302i + (1\u2212 \u00b5)z\u0302i+1 if \u03b6 = \u00b5pi + (1\u2212 \u00b5)pi+1\n(6)"}, {"heading": "3.1 Squared loss minimization", "text": "A natural approach to the monotonic matrix completion problem is to learn g?, Z? via squared loss minimization. In order to do this we need to solve the following optimization problem:\nmin g,Z \u2211 \u2126 (g(Zi,j)\u2212Xi,j)2\ng : R\u2192 R is L-Lipschitz and monotonic rank(Z) \u2264 r.\n(7)\nThe problem is a non-convex optimization problem individually in parameters g, Z. A reasonable approach to solve this optimization problem would be to perform optimization w.r.t. each variable while keeping the other variable fixed. For instance, in iteration t, while estimating Z one would keep g fixed, to say gt\u22121, and then perform projected gradient descent w.r.t. Z. This leads to the following updates for Z:\nZti,j \u2190 Zt\u22121i,j \u2212 \u03b7(g t\u22121(Zt\u22121i,j )\u2212Xi,j)(g t\u22121)\u2032(Zt\u22121i,j ) ,\u2200(i, j) \u2208 \u2126 (8) Zt \u2190 Pr(Zt) (9)\nwhere \u03b7 > 0 is a step-size used in our projected gradient descent procedure, and Pr is projection on the rank r cone. The above update involves both the function gt\u22121 and its derivative (gt\u22121)\u2032. Since our link function is monotonic, one can use the LPAV algorithm to estimate this link function gt\u22121. Furthermore since LPAV estimates gt\u22121 as a piece-wise linear function, the function has a sub-differential everywhere and the sub-differential (gt\u22121)\u2032 can be obtained very cheaply. Hence, the projected gradient update shown in equation (8) along with the LPAV algorithm can be iteratively used to learn estimates forZ? and g?. We shall call this algorithm as MMC\u2212LS. Incorrect estimation of gt\u22121 will also lead to incorrect estimation of the derivative (gt\u22121)\u2032. Hence, we would expect MMC\u2212 LS to be less accurate than a learning algorithm that does not have to estimate (gt\u22121)\u2032. We next outline an approach that provides a principled way to derive updates for Zt and gt that does not require us to estimate derivatives of the transfer function, as in MMC\u2212 LS."}, {"heading": "3.2 Minimization of a calibrated loss function and the MMC algorithm.", "text": "Let \u03a6 : R \u2192 R be a differentiable function that satisfies \u03a6\u2032 = g?. Furthermore, since g? is a monotonic function, \u03a6 will be a convex loss function. Now, suppose g? (and hence \u03a6) is known. Consider the following function of Z\nL(Z; \u03a6,\u2126) = EX  \u2211 (i,j)\u2208\u2126 \u03a6(Zi,j)\u2212Xi,jZi,j  . (10) 4LPAV stands for Lipschitz Pool Adjacent Violator\nThe above loss function is convex in Z, since \u03a6 is convex. Differentiating the expression on the R.H.S. of Equation 10 w.r.t. Z, and setting it to 0, we get\u2211\n(i,j)\u2208\u2126\ng?(Zi,j)\u2212 EXi,j = 0. (11)\nThe MMC model shown in Equation (3) satisfies Equation (11) and is therefore a minimizer of the loss function L(Z; \u03a6,\u2126). Hence, the loss function (10) is \u201ccalibrated\u201d for the MMC model that we are interested in. The idea of using calibrated loss functions was first introduced for learning single index models [25]. When the transfer function is identity, \u03a6 is a quadratic function and we get the squared loss approach that we discussed in section (3.1).\nThe above discussion assumes that g? is known. However in the MMC model this is not the case. To get around this problem, we consider the following optimization problem\nmin \u03a6,Z L(\u03a6, Z; \u2126) = min \u03a6,Z EX \u2211 (i,j)\u2208\u2126 \u03a6(Zi,j)\u2212Xi,jZi,j (12)\nwhere \u03a6 : R\u2192 R is a convex function, with \u03a6\u2032 = g and Z \u2208 Rm\u00d7n is a low-rank matrix. Since, we know that g? is a Lipschitz, monotonic function, we shall solve a constrained optimization problem that enforces Lipschitz constraints on g and low rank constraints on Z. We consider the sample version of the optimization problem shown in equation (12).\nmin \u03a6\nrank(Z)\u2264r\nL(\u03a6, Z; \u2126) = min \u03a6,Z \u2211 (i,j)\u2208\u2126 \u03a6(Zi,j)\u2212Xi,jZi,j (13)\nThe pseudo-code of our algorithm MMC that solves the above optimization problem (13) is shown in algorithm (1). MMC optimizes for \u03a6 and Z alternatively, where we fix one variable and update another.\nAt the start of iteration t, we have at our disposal iterates g\u0302t\u22121, and Zt\u22121. To update our estimate of Z, we perform gradient descent with fixed \u03a6 such that \u03a6\u2032 = g\u0302t\u22121. Notice that the objective in equation (13) is convex w.r.t. Z. This is in contrast to the least squares objective where the objective in equation (7) is non-convex w.r.t. Z. The gradient of L(Z; \u03a6) w.r.t. Z is\n\u2207Zi,jL(Z; \u03a6) = \u2211\n(i,j)\u2208\u2126\ng\u0302t\u22121(Z\u0302t\u22121i,j )\u2212Xi,j . (14)\nGradient descent updates on Z\u0302t\u22121 using the above gradient calculation leads to an update of the form\nZ\u0302ti,j \u2190 Z\u0302t\u22121i,j \u2212 \u03b7(g\u0302 t\u22121(Z\u0302t\u22121i,j )\u2212Xi,j)1(i,j)\u2208\u2126\nZ\u0302t \u2190 Pr(Z\u0302t) (15)\nEquation (15) projects matrix Z\u0302t onto a cone of matrices of rank r. This entails performing SVD on Z\u0302t and retaining the top r singular vectors and singular values while discarding the rest. This is done in steps 4, 5 of Algorithm (1). As can be seen from the above equation we do not need to estimate derivative of g\u0302t\u22121. This, along with the convexity of the optimization problem in Equation (13) w.r.t. Z for a given \u03a6 are two of the key advantages of using a calibrated loss function over the previously proposed squared loss minimization formulation.\nOptimization over \u03a6. In round t of algorithm (1), we have Z\u0302t after performing steps 4, 5. Differentiating the objective function in equation (13) w.r.t. Z, we get that the optimal \u03a6 function should satisfy \u2211\n(i,j)\u2208\u2126\ng\u0302t(Z\u0302ti,j)\u2212Xi,j = 0, (16)\nwhere \u03a6\u2032 = g\u0302t. This provides us with a strategy to calculate g\u0302t. Let, X\u0302i,j def = g\u0302t(Z\u0302ti,j). Then solving the optimization problem in equation (16) is equivalent to solving the following optimization problem.\nmin X\u0302 \u2211 (i,j)\u2208\u2126 (X\u0302i,j \u2212Xi,j)2\nsubject to: 0 \u2264 \u2212X\u0302i,j + X\u0302k,l \u2264 L(Z\u0302tk,l \u2212 Z\u0302ti,j) if Z\u0302ti,j \u2264 Z\u0302tk,l, (i, j) \u2208 \u2126, (k, l) \u2208 \u2126 (17)\nwhere L is the Lipschitz constant of g?. We shall assume that L is known and does not need to be estimated. The gradient, w.r.t. X\u0302 , of the objective function, in equation (17), when set to zero is the same as Equation (16). The constraints enforce monotonicity of g\u0302t and the Lipschitz property of g\u0302t. The above optimization routine is exactly the LPAV algorithm. The solution X\u0302 obtained from solving the LPAV problem can be used to define g\u0302t on X\u2126. These two steps are repeated for T iterations. After T iterations we have g\u0302T defined on Z\u0302T\u2126 . In order to define g\u0302\nT everywhere else on the real line we perform linear interpolation as shown in equation (6).\nAlgorithm 1 Monotonic Matrix Completion (MMC) Input: Parameters \u03b7 > 0, T > 0, r, Data:X\u2126,\u2126 Output: M\u0302 = g\u0302T (Z\u0302T )\n1: Initialize Z\u03020 = mn|\u2126|X\u2126, where X\u2126 is the matrix X with zeros filled in at the unobserved locations. 2: Initialize g\u03020(z) = |\u2126|mnz 3: for t = 1, . . . , T do 4: Z\u0302ti,j \u2190 Z\u0302 t\u22121 i,j \u2212 \u03b7(g\u0302t\u22121(Z\u0302 t\u22121 i,j )\u2212Xi,j)1(i,j)\u2208\u2126 5: Z\u0302t \u2190 Pr(Z\u0302t) 6: Solve the optimization problem in (17) to get X\u0302 7: Set g\u0302t(Z\u0302ti,j) = X\u0302i,j for all (i, j) \u2208 \u2126. 8: end for 9: Obtain g\u0302T on the entire real line using linear interpolation shown in equation (6).\nLet us now explain our initialization procedure. Define X\u2126 def = \u2211|\u2126| j=1X \u25e6 \u2206j , where each \u2206j is a boolean mask with zeros everywhere and a 1 at an index corresponding to the index of an observed entry. A \u25e6 B is the Hadamard product, i.e. entry-wise product of matrices A,B. We have |\u2126| such boolean masks each corresponding to an observed entry. We initialize Z\u03020\u2126 to mn |\u2126|X\u2126 = mn |\u2126| \u2211|\u2126| j=1X \u25e6\u2206j . Because each observed index is assumed to be sampled uniformly at random with replacement, our initialization is guaranteed to be an unbiased estimate of X ."}, {"heading": "4 MSE Analysis of MMC", "text": "We shall analyze our algorithm, MMC, for the case of T = 1, under the modeling assumption shown in Equations (4) and (3). Additionally, we will assume that the matrices Z? and M? are bounded entry-wise in absolute value by 1. When T = 1, the MMC algorithm estimates Z\u0302, g\u0302 and M\u0302 as follows\nZ\u0302 = Pr ( mnX\u2126 |\u2126| ) . (18)\ng\u0302 is obtained by solving the LPAV problem from Equation (17) with Z\u0302 shown in Equation (18). This allows us to define M\u0302i,j = g\u0302(Z\u0302i,j),\u2200i = [n], j = [m].\nDefine the mean squared error (MSE) of our estimate M\u0302 as\nMSE(M\u0302) = E  1 mn n\u2211 i=1 m\u2211 j=1 (M\u0302i,j \u2212Mi,j)2  . (19)\nDenote by ||M || the spectral norm of a matrix M . We need the following additional technical assumptions:\nA1. \u2016Z?\u2016 = O( \u221a n). A2. \u03c3r+1(X) = O\u0303( \u221a n) with probability at least 1 \u2212 \u03b4, where O\u0303 hides terms logarithmic in\n1/\u03b4.\nZ? has entries bounded in absolute value by 1. This means that in the worst case, ||Z?|| = \u221a mn. Assumption A1 requires that the spectral norm of Z? is not very large. Assumption A2 is a weak assumption on the decay of the spectrum of M?. By assumption X = M? + N . Applying Weyl\u2019s inequality we get \u03c3r+1(X) \u2264 \u03c3r+1(M?) + \u03c31(N). Since N is a zero-mean noise matrix with independent bounded entries, N is a matrix with sub-Gaussian entries. This means that \u03c31(N) = O\u0303( \u221a n) with high probability. Hence, assumption A2 can be interpreted as imposing the condition \u03c3r+1(M ?) = O( \u221a n). This means that while M? could be full rank, the (r + 1)th singular value of M? cannot be too large.\nTheorem 1. Let \u00b51 def = E||N ||, \u00b52 def = E||N ||2. Let \u03b1 = ||M? \u2212 Z?||. Then, under assumptions A1 and A2, the MSE of the estimator output by MMC with T = 1 is given by\nMSE(M\u0302) = O\n(\u221a r\nm +\n\u221a mn log(n)\n|\u2126| +\nmn\n|\u2126|3/2 +\n\u221a r\nm \u221a n\n( \u00b51 +\n\u00b52\u221a n ) +\u221a\nr\u03b1\nm \u221a n\n( 1 +\n\u03b1\u221a n\n) + \u221a rmn log2(n)\n|\u2126|2\n) .\n(20)\nwhere O(\u00b7) notation hides universal constants, and the Lipschitz constant L of g?. We would like to mention that the result derived for MMC-1 can be made to hold true for T > 1, by an additional large deviation argument. The proof of our theorem is available in the appendix.\nInterpretation of our results: Our upper bounds on the MSE of MMC depends on the quantity \u03b1 = ||M?\u2212Z?||, and \u00b51, \u00b52. Since matrixN has independent zero-mean entries which are bounded in absolute value by 1, N is a sub-Gaussian matrix with independent entries. For such matrices \u00b51 = O( \u221a n), \u00b52 = O(n) (see Theorem 5.39 in [26]). With these settings we can simplify the expression in Equation (20) to\nMSE(M\u0302) = O\u0303\n(\u221a r\nm +\n\u221a mn log(n)\n|\u2126| +\nmn\n|\u2126|3/2 +\n\u221a r\u03b1\nm \u221a n\n( 1 +\n\u03b1\u221a n\n) + \u221a rmn log2(n)\n|\u2126|2\n) .\nA remarkable fact about our sample complexity results is that the sample complexity is independent of the rank of matrix M?, which could be large. Instead it depends on the rank of matrix Z? which we assume to be small. The dependence on M? is via the term \u03b1 = ||M?\u2212Z?||. From equation (4) it is evident that the best error guarantees are obtained when \u03b1 = O( \u221a n). For such values of \u03b1 equation (4) reduces to,\nMSE(M\u0302) = O\u0303\n(\u221a r\nm +\n\u221a mn log(n)\n|\u2126| +\nmn\n|\u2126|3/2 +\n\u221a mn\n|\u2126| +\n\u221a rmn log2(n)\n|\u2126|2\n) .\nThis result can be converted into a sample complexity bound as follows. If we are given |\u2126| = O\u0303( ( mn )2/3 ), then MSE(M\u0302) \u2264 \u221a r m + ."}, {"heading": "5 Experimental results", "text": "We compare the performance of MMC\u22121, MMC\u2212 c, MMC- LS, and nuclear norm based low-rank matrix completion (LRMC) [4] on various synthetic and real world datasets. The objective metric that we use to compare different algorithms is the root mean squared error (RMSE) of the algorithms on unobserved, test indices of the incomplete matrix."}, {"heading": "5.1 Efficient implementation of MMC", "text": "The MMC algorithm consists of three main steps. A gradient descent step, a projection step onto the set of rank r matrices, and a QP optimization step. On top of these one has to find out the correct values for the rank r, and the step size \u03b7 used in step 4 of MMC. A straight forward implementation of the above three steps can be coupled with a grid search over r and \u03b7. However, such an implementation would be inefficient and not scalable beyond small matrices. Since r is assumed to\nbe small one needs to find just the top few singular vectors of the intermediate matrices. We use PROPACK 5 package for an efficient SVD implementation. Rather than search for r on a grid, we use an increasing rank procedure to estimate r. The increasing rank procedure starts at a small value for r, say rmin and increases the current estimate of r, by rinc, whenever there is not much progress in the iterates. Precisely, the current estimate of r is increased by rinc if\n1\u2212 ||P\u2126(X\u0302 t \u2212X)||F\n||P\u2126(X\u0302t\u22121 \u2212X)||F \u2264 , (21)\nfor some small > 0. As we expect r to be small, our estimate for r is not allowed to grow beyond a certain rmax. The increasing rank procedure was inspired by the work of [15], where they demonstrated that such procedures are suitable for low-rank approximation problems.\nAn ADMM implementation for LPAV problem. The LPAV problem shown in Equation (17) is solved in each iteration of MMC. Hence, it is crucial that we have an efficient implementation of LPAV so that the overall algorithm is efficient. For ease of notation, we shall frame the LPAV problem as follows: Suppose we have a 1-dimensional regression problem with covariates z1 \u2264 z2 \u2264 . . . \u2264 zf and the corresponding targets x1, x2, . . . xf in R. The LPAV routine solves the problem\nmin y\u2208Rf f\u2211 i=1 (yi \u2212 xi)2\nsubject to: 0 \u2264 yi+1 \u2212 yi \u2264 L(zi+1 \u2212 zi), for all i = 1, . . . , f \u2212 1.\n(22)\nOne could transform the above optimization problem into a box-constrained convex QP, by rewriting the objective and the constraints using the transformation \u03b4i = yi+1 \u2212 yi. Box constrained QPs are well studied optimization problems and a plethora of efficient solvers exist. However, the above transformation destroys the simple least-squares structure of the objective, and in our experience the resulting box-constrained QP turned out to be harder to solve than the original optimization problem (22). Instead we use the Alternating Direction Method of Multipliers (ADMM) algorithm for our problem [27] which allows us to exploit the rich structure present in problem (22). We shall now explain ADMM updates as applied to problem (22). Let b be a vector in Rf , with the jth component being bj = L(zj+1 \u2212 zj). Let M1,M2 be two matrices in Rf\u22121\u00d7f , constructed as follows. The ith row of M1 has 1 in the ith column and \u22121 in the i + 1th column. The ith row of M2 has \u22121 in the ith column and 1 in the i+ 1th column. Rest of the entries in M1,M2 are zeros. Let M = ( M1 M2 ) be a matrix in R2p\u22122\u00d7p. Finally let b\u0304 = [ 0 b ] be a 2f \u22122 dimensional vector whose first f \u2212 1 entries are 0, and the last f \u2212 1 entries are the first f \u2212 1 entries of the vector b. With these definitions, problem (22) can be reformulated as\nmin y\u2208Rf f\u2211 i=1 (yi \u2212 xi)2\nsubject to: My \u2264 b\u0304.\n(23)\nIn order to derive efficient ADMM updates, we shall convert the inequality constraints to equality by introducing slack variables. This gets us the following reformulation\nmin y\u0304\u2208R3f\u22122 f\u2211 i=1 (y\u0304i \u2212 xi)2\nsubject to: M\u0304y\u0304 = b\u0304 [0f\u00d7f , If\u00d7f ]y\u0304 \u2265 0\n(24)\nwhere M\u0304 = [M2p\u22122\u00d7p, I2f\u22122\u00d72f\u22122]. Note that this problem has 3f \u2212 2 variables in contrast to the f \u2212 1 variables in Equation (23). The extra 2f \u2212 1 variables correpsond to the slack variables introduced when converting the inequality constraints in (23) to equality constraints in (24). The ADMM algorithm provides an iterative procedure for solving convex optimization problems of the form\nmin y\u0304,z f(y\u0304) + g(z)\nsubject to: Ay\u0304 +Bz = 0. (25)\n5Look at http://svt.stanford.edu/code.html for download instructions.\nWe can cast the optimization problem in Equation (24) in the above form by defining A = \u2212B = I ,\nf(y\u0304) =\n{\u2211f i=1(y\u0304i \u2212 xi)2 if M\u0304y\u0304 = b\n\u221e otherwise (26)\nand\ng(z) = { 0 if [0f\u00d7f , If\u00d7f ]z \u2265 0 \u221e otherwise.\nThe ADMM algorithm involves iteratively updating y\u0304, z and some additional variables until convergence. In our practical implementations we follow the advice as mentioned in [27] and set abs and rel to 10\u22122. Any value less than 10\u22122 did not yield much change in results.\nUpdating y\u0304: In order to update y\u0304 iteratively we need to solve the following optimization problem. Given two vectors zk, uk, and \u03b3 > 0, we solve\ny\u0304k+1 = arg min y\u0304 f\u2211 i=1 (y\u0304i \u2212 xi)2 + \u03b3 2 ||y\u0304 + uk \u2212 zk||2\nsubject to: M\u0304y\u0304 = b\u0304.\n(27)\nThis is an equality constrained QP, which can be solved in closed form by using the KKT conditions. The solution is obtained by solving the following set of sparse linear equations.[\nP3f\u22122\u00d73f\u22122 + \u03b3I3f\u22122\u00d73f\u22122 M\u0304 >\nM\u0304 02f\u22122\u00d72f\u22122\n] [ y\u0304k+1\n\u03bd\n] = [ \u2212(q + \u03b3(uk \u2212 zk))\nb\u0304,\n] (28)\nwhere P, q are defined as follows\nP = 2\n[ If\u00d7f 0f\u00d72f\u22122\n02f\u22122\u00d7f 02f\u22122\u00d72f\u22122\n] , q = \u22122 [ x\n02p\u22122\n] (29)\nThe above system of linear equations is very large with 5p \u2212 4 equations in 5p \u2212 4 unknowns but very sparse with 7p \u2212 6 non-zeros in the matrix on the LHS of Equation (28). Such large, sparse system of equations can be solved very efficiently, and in our numerical experiments we simply use the backslash operator in MATLAB to solve them.\nUpdates for z: The ADMM updates applied to z yield the following optimization problem\nzk+1 = arg min z\u2208R3p\u22122, ||z \u2212 (yk+1 + uk)||2.\nsubject to: zk \u2265 0 for k \u2265 p+ 1 (30)\nThe solution to the above optimization problem is trivial and can be written in closed form as follows\nzk+1j =\n{ yk+1j + u k j if j = 1, . . . p\nmax{yk+1j + ukj , 0} if j \u2265 p+ 1 (31)\nUpdates for u: The update for the intermediate variable u is given by the equation\nuk+1 = uk + y\u0304k+1 \u2212 zk+1. (32)\nHence, the ADMM algorithm for the LPAV routine requires iteratively updating y\u0304, z, u until desired tolerance levels."}, {"heading": "5.2 Synthetic experiments", "text": "For our synthetic experiments we generated a random 30 \u00d7 20 matrix Z? of rank 5 by taking the product of two random Gaussian matrices of size n \u00d7 r, and r \u00d7m, with n = 30,m = 20, r = 5. The matrix M? was generated using the function, g?(M?i,j) = 1/(1 + exp(\u2212cZ?i,j)), where c > 0. By increasing c, we increase the Lipschitz constant of the function g?, making the matrix completion task harder. For large enough c,Mi,j \u2248 sgn(Zi,j). We consider the noiseless version of the problem whereX = M?. Each entry in the matrixX was sampled with probability p, and the sampled entries\nare observed. This makes E|\u2126| = mnp. For our implementations we assume that r is unknown, and estimate it either (i) via the use of a dedicated validation set in the case of MMC \u2212 1 or (ii) adaptively, where we progressively increase the estimate of our rank until a sufficient decrease in error over the training set is achieved [15]. For an implementation of the LRMC algorithm we used a standard off-the-shelf implementation from TFOCS [28]. In order to speed up the run time of MMC, we also keep track of the training set error, and terminate iterations if the relative residual on the training set goes below a certain threshold 6. In the appendix we provide a plot that demonstrates that, for MMC\u2212c, the RMSE on the training dataset has a decreasing trend and reaches the required threshold in at most 50 iterations. Hence, we set T = 50. Figure (2) show the RMSE of each method for different values of p, c. As one can see from figure (2), the RMSE of all the methods improves for any given c as p increases. This is expected since as p increases E|\u2126| = pmn also increases. As c increases, g? becomes steeper increasing the effective rank of X . This makes matrix completion task hard. For small p, such as p = 0.2, MMC \u2212 1 is competitive with MMC \u2212 c and MMC\u2212LS and is often the best. In fact for small p, irrespective of the value of c, LRMC is far inferior to other methods. For larger p, MMC\u2212 c works the best achieving smaller RMSE over other methods."}, {"heading": "5.3 Experiments on real datasets", "text": "We performed experimental comparisons on four real world datasets: paper recommendation, Jester3, ML-100k, Cameraman. The source of our datasets is listed in the appendix. All of the above datasets, except the Cameraman dataset, are ratings datasets, where users have rated a few of the several different items. For the Jester-3 dataset we used 5 randomly chosen ratings for each user for training, 5 randomly chosen rating for validation and the remaining for testing. ML-100k comes with its own training and testing dataset. We used 20% of the training data for validation. For the Cameraman and the paper recommendation datasets 20% of the data was used for training, 20% for validation and the rest for testing. The baseline algorithm chosen for low rank matrix completion is LMaFit-A [15] 7.\nFor each of the datasets we report the RMSE of MMC \u2212 1, MMC \u2212 c, and LMaFit-A on the test sets. We excluded MMC-LS from these experiments because in all of our datasets the number of observed entries is a very small fraction of the total number of entries, and from our results on synthetic datasets we know that MMC\u2212 LS is not the best performing algorithm in such cases. Table 1 shows the RMSE over the test set of the different matrix completion methods. As we see the RMSE of MMC\u2212 c is the smallest of all the methods, surpassing LMaFit-A by a large margin."}, {"heading": "6 Conclusions and future work", "text": "We have investigated a new framework for high rank matrix completion problems called monotonic matrix completion. We proposed and studied an algorithm called MMC based on minimizing a calibrated loss function. In the future we would like to investigate if one could relax the technical assumptions involved in establishing our theoretical results."}, {"heading": "A Error Analysis of Monotonic Matrix Completion", "text": "We shall analyze our algorithm, MMC\u2212c, for the case of T = 1. Since for T = 1, MMC\u2212c and MMC\u2212LS are the same, we shall used the word MMC to refer to both the algorithms when T = 1. For T = 1, we have\nZ\u0302 = Pr ( mnX\u2126 |\u2126| ) (33)\ng\u0302 = LPAV (Z\u0302\u2126, X\u2126) (34)\nM\u0302i,j = g\u0302(Z\u0302i,j),\u2200i = [m], j = [n], (35)\nFinally, define the mean squared error (MSE) of our estimate M\u0302 can be defined as\nMSE(M\u0302) = E\n[ 1\nmn n\u2211 i=1 m\u2211 j=1\n(M\u0302i,j \u2212Mi,j)2 ] . (36)\nWe are interested in analyzing the MSE of M\u0302 output by MMC for T = 1. We shall make the following assumptions"}, {"heading": "B MMC model and technical assumptions", "text": "A1 \u2016Z?\u2016 = O( \u221a n), i.e. the spectral norm of Z? is of the order of \u221a n.\nA2. \u03c3r+1(X) = O( \u221a n) with probability at least 1\u2212 \u03b4.\nThe MMC model makes the following assumptions. These assumptions are the same as in the main paper. We enumerate it here for the sake of convenience.\nM1. X = M? +N .\nM2. EN = 0. M3. M?i,j = g\n?(Z?i,j) \u2200i = [n], j = [m]. M4. Assume that n \u2265 m, and rank(Z?) = r m. M5. Boundedness assumption: |Z?i,j | \u2264 1, |Xi,j | \u2264 1 for all i \u2208 [n], j \u2208 [m]. M6. g? : R\u2192 R is monotonic and L-Lipschitz. M7. The set \u2126 is generated by sampling uniformly at random with replacement from the index set [n] \u00d7\n[m].\nB.1 Notation\nAll of our matrices, unless explicitly stated, will be n\u00d7m with n \u2265 m. ||A|| is the spectral norm of matrix A, and ||A||? is the nuclear norm of matrix A.\nB.2 Towards proof of Theorem (1)\nWe begin with the following technical lemma that will be used in the proof. Lemma 1. Let G = {g|g : [\u2212W,W ]\u2192 [\u22121, 1] is monotonic and 1-Lipschitz}.With probability at least 1\u2212 \u03b4 over the sample z1, . . . , zn, the following statement is true for all g \u2208 G\u2223\u2223\u2223\u2223 1n\u2211(g(zi)\u2212 yi)2 \u2212 E(g(z)\u2212 y)2 \u2223\u2223\u2223\u2223 = O\u0303 (\u221a W n ) (37)\nwhere O\u0303 hides logarithmic dependence on n,W, 1/\u03b4.\nProof. Let R\u0302n(G) be the empirical Rademacher complexity of function class G, and let N\u221e( ,G) be the L\u221e covering number of the function clas G. From [11, Lemma 6] we know that\nN\u221e( ,G) \u2264 1 2 2W . (38)\nThe above covering number allows us to bound the empirical Rademacher complexity of the function class G via Dudley\u2019s entropy bound. Using [29, Lemma A.3], and the fact that N\u221e( ,G) \u2265 N2( ,G, z1, . . . , zn) we get\nR\u0302n(G) \u2264 inf \u03b1\u22650 4\u03b1+ 10 \u222b 1 \u03b1 \u221a logN\u221e( ,G) n d (39)\n\u2264 4\u03b1+ 10 \u222b 1 \u03b1\n\u221a 2W\nlog( 1 )\nn d (40) \u2264 4\u03b1+ 10 \u221a 2W\nn \u222b 1 \u03b1 1 d (41)\n\u2264 10 \u221a 2W\nn log\n( 4e\n10\n\u221a n\n2W\n) . (42)\nUsing a uniform convergence bound in terms of the Rademacher complexity of the function class [30, Theorem 8] we get the desired result.\nLemma 2. Let 2 = E[ 1mn \u2211 i,j(Z\u0302i,j \u2212 Z ? i,j) 2]. Then, under assumptions A1-A8, we have\nMSE(M\u0302) \u2264 O\n(\u221a mn log(n) |\u2126| + \u221a n |\u2126| + mn |\u2126|3/2 + \u221a mn |\u2126| + 2 + \u221a 2 )\nProof.\n1\nmn E [\u2211 i,j (M\u0302i,j \u2212M?i,j)2 ] = 1 mn E [\u2211 i,j (g\u0302(Z\u0302i,j)\u2212 g?(Z?i,j))2 ]\n(43)\n= 1\nmn E [\u2211 i,j ( g\u0302(Z\u0302i,j)\u2212 g?(Z\u0302i,j) + g?(Z\u0302i,j)\u2212 g?(Z?i,j) )2] (44)\n\u2264 2E\n[ 1\nmn \u2211 i,j ( g\u0302(Z\u0302i,j)\u2212 g?(Z\u0302i,j) )2] \ufe38 \ufe37\ufe37 \ufe38\nT1\n+2E [ 1\nmn\n( g?(Z\u0302i,j)\u2212 g?(Z?i,j) )2] \ufe38 \ufe37\ufe37 \ufe38\nT2\n(45)\n= 2T1 + 2T2. (46)\nWe shall bound T2 in terms of 2.\nBounding T2:\nT2 = 1 mn E \u2211 i,j (g?(Z\u0302i,j)\u2212 g?(Z?i,j))2 (47)\n(a) \u2264 1 mn E \u2211 i,j (Z\u0302i,j \u2212 Z?i,j)2 (48)\ndef = 2 (49)\nwhere inequality (a) follows from the fact that g? is 1-Lipschitz. Next we shall bound T1 in terms of 2 and other terms.\nBounding T1:\nE\n[ 1\nmn \u2211 i,j ( g\u0302(Z\u0302i,j)\u2212 g?(Z\u0302i,j) )2] = E [ 1 |\u2126| \u2211 \u2126 ( g\u0302(Z\u0302i,j)\u2212 g?(Z\u0302i,j) )2] \ufe38 \ufe37\ufe37 \ufe38\nT1,1\n+\nE\n[ 1\nmn \u2211 i,j ( g\u0302(Z\u0302i,j)\u2212 g?(Z\u0302i,j) )2] \u2212 E [ 1 |\u2126| \u2211 \u2126 ( g\u0302(Z\u0302i,j)\u2212 g?(Z\u0302i,j) )2] \ufe38 \ufe37\ufe37 \ufe38\n\u22061\n(50)\nNext we shall bound T1,1 as follows. Let G = {g : R\u2192 R|g is a monotonic and 1-Lipschitz function }. Since g\u0302, g? by definition belong to G, and since g\u0302 solves the optimization problem\ng\u0302 = arg min \u2211 \u2126 (g(Z\u0302i,j)\u2212Xi,j)2, (51)\nhence via the generalized Pythagorean inequality [31] we have\u2211 \u2126 (g\u0302(Z\u0302i,j)\u2212Xi,j)2 + \u2211 \u2126 (g\u0302(Z\u0302i,j)\u2212 g?(Z\u0302i,j))2 \u2264 \u2211 \u2126 (Xi,j \u2212 g?(Z\u0302i,j))2. (52)\nUsing Equation (52) we can bound T1,1 as follows\nT1,1 = E\n[ 1\n|\u2126| \u2211 \u2126 ( g\u0302(Z\u0302i,j)\u2212 g?(Z\u0302i,j)\n)2]\n\u2264 E\n[ 1\n|\u2126| \u2211 \u2126 (Xi,j \u2212 g?(Z\u0302i,j))2 \u2212 1 |\u2126| \u2211 \u2126 ( Xi,j \u2212 g\u0302(Z\u0302i,j)\n)2]\n= E\n[ 1\n|\u2126| \u2211 \u2126\n(Xi,j \u2212 g?(Z\u0302i,j))2 ] \u2212 E [ 1\n|\u2126| \u2211 \u2126\n(Xi,j \u2212 g?(Z?i,j))2 ]\n\ufe38 \ufe37\ufe37 \ufe38 I1\n+\nE\n[ 1\nmn \u2211 i,j\n(Xi,j \u2212 g?(Z?i,j))2 ] \u2212 E [ 1\nmn \u2211 i,j\n(Xi,j \u2212 g\u0302(Z\u0302i,j))2 ]\n\ufe38 \ufe37\ufe37 \ufe38 I2\n+\nE\n[ 1\n|\u2126| \u2211 \u2126\n(Xi,j \u2212 g?(Z?i,j))2 ] \u2212 E [ 1\nmn \u2211 i,j\n(Xi,j \u2212 g?(Z?i,j))2 ]\n\ufe38 \ufe37\ufe37 \ufe38 I3\n+\nE\n[ 1\nmn \u2211 i,j\n(Xi,j \u2212 g\u0302(Z\u0302i,j))2 ] \u2212 E [ 1\n|\u2126| \u2211 \u2126\n(Xi,j \u2212 g\u0302(Z\u0302i,j))2 ]\n\ufe38 \ufe37\ufe37 \ufe38 I4\n(53)\nWe shall look at each of the terms T2, T3, T4 and bound them separately. From assumption A1 we know that g?(Z?i,j) is the best estimator of Xi,j in mean squared. Hence, I2 \u2264 0. We next bound I1, I3, I4. |Z?|\u221e \u2264 1, and |X?|\u221e \u2264 1, hence |Xi,j \u2212g?(Z?i,j)| \u2264 2. If we call \u22063 the random variable whose expectation is I3, then \u22063 \u2264 4 surely. Moreover we can apply lemma (1) to guarantee that \u22063 \u2264 O (\u221a log(|\u2126|/\u03b4) |\u2126| ) with probability at least 1\u2212 \u03b4. Choose \u03b4 = 1\u221a |\u2126| . We then have\nI3 = E\u22063 \u2264 4\u03b4 + (1\u2212 \u03b4)O (\u221a log(|\u2126|/\u03b4) |\u2126| ) = O (\u221a log(|\u2126|) |\u2126| ) . (54)\nNext, we bound I4. This needs a slightly careful treatment, since Z\u0302i,j is random. Let A = 1p|\u2126|X \u25e6 \u2206. Let A = \u2211 \u03c3iuiv > i be the SVD of A with \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7\u03c3m. By definition Z\u0302 = Pr(A). Hence, A \u2212 Z =\n\u2211 i\u2265r+1 \u03c3iuiv > i . This means that\n|A\u2212 Z\u0302|\u221e \u2264 ||A\u2212 Z\u0302|| = || \u2211 i\u2265r+1 \u03c3iuiv > i ||\n= \u03c3r+1\n\u2264 \u03c31(A\u2212X) + \u03c3r+1(X) (55)\nWe shall now use the above bound on Z\u0302 \u2212A to obtain upper bound on |Z\u0302|\u221e as follows\n|Z\u0302|\u221e (a) \u2264 |Z\u0302 \u2212A|\u221e + |A\u2212X|\u221e + |X|\u221e (56) (b) \u2264 ||A\u2212X||+ |X|\u221e + ||A\u2212X||+ \u03c3r+1(X) (57) = 2||A\u2212X||+ |X|\u221e + \u03c3r+1(X) (58) = 2||A\u2212X||+ 1 + \u03c3r+1(X) (59) (c) \u2264 2||A\u2212X||+ 1 + \u03c3r+1(X) (60)\nTo obtain inequality (a) we used the triangle inequality, and to obtain inequality (b) we used Equation (55). Now, consider the event\nE1 = ||A\u2212X|| \u2264 2mn log ( m+n \u03b4 ) 3|\u2126| + \u221a 2 log(m+n \u03b4 )mn |\u2126|  . (61) From Lemma 5 we know that conditioned onX , P(E1) \u2265 1\u2212\u03b4 over the randomness in \u2126. Using equation (60) we get that on event E1\n|Z\u0302|\u221e = O ( \u03c3r+1(X) + mn log(m+n \u03b4 )\n|\u2126| +\n\u221a mn log((m+ n)/\u03b4)\n3|\u2126|\n) def = b (62)\nNow let I \u20324 be the term argument to the expectation operator in I4. Now let us define another event\nE11 = { I \u20324 \u2264 \u221a b log((m+ n)/\u03b4)\n|\u2126|\n} (63)\nUsing lemma (1), we get that P(E11) \u2265 1\u2212 \u03b4 over random choice of \u2126. Notice that I \u20324 \u2264 4 surely. We are now ready to calculate I4 as follows\nI4 = EXE\u2126|XI \u20324 (64) \u2264 EXP(E1)E\u2126|X,E1I \u2032 4 + 4P(E\u03041) (65)\n\u2264 EXP(E1)(P(E11)I \u20324 + 4P(E\u030411)) + 4P(E\u03041) (66)\n\u2264 8\u03b4 + EX\n\u221a b log((m+ n)/\u03b4)\n|\u2126| (67)\nSubstituting the value of b, and using \u03b4 = 1|\u2126| , and using the assumption that \u03c3r+1(X) \u2264 O( \u221a n) with high probability, we get that\nI4 = EXE\u2126|XI \u20324 (68)\n\u2264 8\u03b4 + EX \u221a\u221a\u221a\u221a 1 |\u2126|O ( \u03c3r+1(X) + mn log((m+ n)|\u2126|) |\u2126| + \u221a mn log((m+ n)|\u2126|) 3|\u2126| ) (69)\n\u2264 O (\u221a mn\n|\u2126|2 log 2 ((m+ n)|\u2126|)\n) (70)\nNotice that \u22061 uses g\u0302 \u2212 g? which is a 2 Lipchitz function. By perfoming a similar analysis as in I4 it is easy to show that \u22061 = O(I4).\nBounding I1.\nI1 = E\n[ 1\n|\u2126| \u2211 \u2126 (Xi,j \u2212 g?(Z\u0302i,j))2 \u2212 1 |\u2126| \u2211 \u2126\n(Xi,j \u2212 g?(Z?i,j))2 ]\n(71)\n= E\n[ 1\n|\u2126| \u2211 \u2126 (g?(Z?i,j)\u2212 g?(Z\u0302i,j))(2Xi,j \u2212 g?(Z\u0302i,j)\u2212 g?(Z?i,j))\n] (72)\n(a) \u2264 4E 1|\u2126| |g ?(Z?i,j)\u2212 g?(Z\u0302i,j)\u2016 (73)\n(b) \u2264 4E 1|\u2126| \u2211 \u2126 |Z?i,j \u2212 Z\u0302i,j | (74)\n= 4E 1 mn \u2211 i,j |Z?i,j \u2212 Z\u0302i,j |+ 4 ( E 1|\u2126| \u2211 \u2126 |Z?i,j \u2212 Z\u0302i,j | \u2212 E 1 mn \u2211 i,j |Z?i,j \u2212 Z\u0302i,j | ) \ufe38 \ufe37\ufe37 \ufe38\n\u22065\n(75)\n(c) \u2264 4E 1\nmn \u2211 i,j |Z?i,j \u2212 Z\u0302i,j |+ 4\u22065 (76)\n(d) \u2264 4 \u221a\nE 1 mn \u2211 i,j |Z?i,j \u2212 Z\u0302i,j |2 + 4\u22065 = 4( \u221a 2 + \u22065) (77)\nwhere, to get inequality (a) we used the fact that |Xi,j | \u2264 1 and |g?| \u2264 1. To get inequality (b) we used the fact that g? is 1 Lipschitz. To get inequality (c) we used concentration of measure. Finally, to get inequality (d) we used Jensen\u2019s inequality to bound E|x| \u2264 \u221a Ex2. Our next step is to bound \u22065.\nBounding \u22065: The idea is to consider the event E1 as done during bounding the term I4. Once again we shall consider the event\nE1 = ||A\u2212X|| \u2264 2mn log ( m+n \u03b4 ) 3|\u2126| + \u221a 2 log(m+n \u03b4 )mn |\u2126|  . (78) Similar to arguments there, we know from Equation (62) that on event E1\n|Z\u0302|\u221e = O ( \u03c3r+1(X) + mn log(m+n|\u2126| )\n|\u2126| +\n\u221a mn log((m+ n)/\u03b4)\n3|\u2126|\n) def = b\nConsider the collection of random variables \u03be1, . . . , \u03be|\u2126|, where each \u03bek takes the value Z?i,j \u2212 Z\u0302i,j , where (i, j) is chosen u.a.r. with replacement from [n] \u00d7 [m]. It is easy to see that each of \u03bek \u2208 [0, b + 1] on E1. Applying Hoeffding inequality we get on E1 with probability at least 1 \u2212 \u03b4 over the random choice of \u2126, and on event E1\n1 |\u2126| \u2211 \u2126 |Z?i,j \u2212 Z\u0302i,j | \u2212 \u2211 i,j |Z?i,j \u2212 Z\u0302i,j | \u2264\n\u221a (b+ 1)2\n2|\u2126| log(1/\u03b4) (79)\nBy arguments similar to the ones used in establishing bounds for I4, we get \u22065 \u2264 O ( log ((m+ n)|\u2126|) (\u221a n\n|\u2126| + mn |\u2126|3/2 +\n\u221a mn\n|\u2126|\n)) . (80)\nThis concludes our first set of calculations. With this we have\nMSE(M\u0302) = O\n(\u221a mn log(n) |\u2126| + \u221a n |\u2126| + mn |\u2126|3/2 + \u221a mn |\u2126| + 2 + \u221a 2 ) (81)\nThe rest of the proof establishes upper bounds on 2.\nB.3 Bounding 2.\nIn order to establish upper bound on 2 we first need the following projection lemma. This lemma is similar in spirit to a lemma of S.Chatterjee [32, Lemma 3.5]. Before we establish this lemma, we would like to clarify the notation that we use. Given a matrix A \u2208 Rn\u00d7m, with m \u2264 n, denote by \u03c31(A) \u2265 \u03c32(A) \u2265 . . . \u2265 \u03c3m(A) the singular values of A in decreasing order.\nLemma 3. Let A = \u2211m i=1 \u03c3ixiy > i be the SVD of a rectangular matrix A \u2208 Rm\u00d7n, with the singular values \u03c31 \u2265 \u03c32 . . . \u2265 \u03c3m arranged in decreasing order. Let B be an unknown m\u00d7 n matrix. Given 1 \u2264 r \u2264 m, let B\u0302 def = Pr(A) def = \u2211r i=1 \u03c3ixiy > i be the projection estimator of B. Then,\n||Pr(A)\u2212B||F \u2264 \u221a ||B||?(\u03c3r+1 + ||A\u2212B||) + 2 \u221a 2r(\u03c3r+1 + ||A\u2212B||). (82)\nProof. Let B = \u2211m i=1 \u03c4iuiv > i be the SVD of B with \u03c41 \u2265 \u03c42 \u2265 . . . \u03c4m.\n||B\u0302 \u2212B||F \u2264 ||B\u0302 \u2212G||F + ||G\u2212B||F , (83)\nand ||G\u2212B||2F = || \u2211 i\u2265r+1 \u03c4iuiv > i ||2F = \u2211 i\u2265r+1 \u03c42i \u2264 ( max i\u2265r+1 \u03c4i)||B||?. (84)\nLet \u03b41 \u2265 \u03b42 \u2265 . . . be the singular values of A\u2212B in decreasing order. Then from Weyl\u2019s inequality we know that\nmax i |\u03c3i \u2212 \u03c4i| \u2264 max i \u03b4i = ||A\u2212B||. (85)\nHence, for i \u2265 r + 1, \u03c4i \u2264 \u03c3i + ||A\u2212B|| \u2264 \u03c3r+1(A) + ||A\u2212B||. (86)\nThis allows us to conclude that maxi\u2265r+1 \u03c4i \u2264 \u03c3r+1(A) + ||A\u2212B||. Combined with Equation (84) we get\n||G\u2212B||2F \u2264 ||B||?(\u03c3r+1(A) + ||A\u2212B||). (87)\nNext, we shall upper bound the quantity ||B\u0302 \u2212 G||F . By construction, both B\u0302 and G are rank r matrices and hence B\u0302 \u2212G is a rank 2r matrix. This allows us to control the Frobenius norm of B\u0302 \u2212G via its spectral norm as follows\n||B\u0302 \u2212G||F \u2264 \u221a 2r||B\u0302 \u2212G|| (88)\nTo bound ||B\u0302 \u2212G|| consider the following decomposition\n||B\u0302 \u2212G|| \u2264 ||B\u0302 \u2212A||+ ||A\u2212B||+ ||B \u2212G||. (89)\nWe have ||B\u0302 \u2212A|| = || \u2211 i \u03c3ixiy > i || \u2264 \u03c3r+1. (90)\n||B \u2212G|| = || \u2211 i\u2265r+1 \u03c4iuiv > i || = \u03c4r+1 (a) \u2264 \u03c3r+1 + ||A\u2212B|| (91)\nwhere to get inequality (a) we used Equation (86). Combining Equations (89), (90), (91) we get\n||B\u0302 \u2212G|| \u2264 \u03c3r+1 + ||A\u2212B||+ \u03c3r+1 + ||A\u2212B|| = 2(\u03c3r+1 + ||A\u2212B||) (92)\nand using Equation (88) we get\n||B\u0302 \u2212G||F \u2264 2 \u221a 2r(\u03c3r+1 + ||A\u2212B||) (93)\nFinally using Equation (87) and Equation (93) we get\n||B\u0302 \u2212B||F \u2264 2 \u221a 2r(\u03c3r+1 + ||A\u2212B||) + \u221a ||B||?(\u03c3r+1 + ||A\u2212B||). (94)\nIn order to obtain an upper bound on 2 we shall use Lemma (3) with the following choices for matrices A,B: A def = 1\np|\u2126|X \u25e6\u2206\u2126, B def = Z?, Z\u0302 def = Pr(A).. We then get\n||Z\u0302 \u2212 Z?||F \u2264 \u221a ||Z?||?(\u03c3r+1 + ||A\u2212 Z?||) + 2 \u221a 2r(\u03c3r+1 + ||A\u2212 Z?||). (95)\nSince Z? is of rank r, we have ||Z?||? \u2264 r||Z?||. From triangle inequality ||A|| \u2264 ||A\u2212Z?||+ ||Z?||. These facts coupled with the fact that \u03c3r+1 \u2264 \u03c31 allows us to obtain\nE||Z\u0302 \u2212 Z?||2F \u2264 r||Z?||(2E||A\u2212 Z?||+ ||Z?||) + 8r(4E||A\u2212 Z?||2 + ||Z?||2). (96)\nNotice that 2 is a scaled version of E||Z\u0302 \u2212 Z?||2F . Let,\n\u03b21 def = E||A\u2212X|| (97)\n\u03b22 def = E||A\u2212X||2. (98)\nUsing the above definitions, Equation (96), the triangle inequality ||A\u2212Z?|| \u2264 ||A\u2212X||+ ||X\u2212Z?||, along with the elementary fact that (a+ b)2 \u2264 2a2 + 2b2, we obtain\nE||Z\u0302 \u2212 Z?||2F \u2264 r||Z?||(2\u03b21 + 2E||X \u2212 Z?||+ ||Z?||) + 8r(8\u03b22 + 8E||X \u2212 Z?||2 + ||Z?||2) (99) = r||Z?||(2E||X \u2212 Z?||+ ||Z?||) + 8r(8E||X \u2212 Z?||2 + ||Z?||2) + r(2\u03b21 + 64\u03b22).\n(100)\nBounding \u03b21, \u03b22. In order to bound \u03b21, \u03b22 we need upper bounds on spectral norm of sums of random matrices. Towards this, the following Bernstein inequality is useful\nTheorem 2 (Bernstein\u2019s inequality). Let S1, . . . Sk be independent, centered random matrices with common dimension n\u00d7m, and assume that each one of them is bounded\n||Sj || \u2264 L for each j \u2265 1. (101)\nLet M = \u2211k j=1 Sj , and let \u03bd(M) denote the matrix variance statistic of the sum\n\u03bd(M) = max { || k\u2211 j=1 ESjS>j ||, || k\u2211 j=1 ES>j Sj || } . (102)\nThen\n1.\nP(||M || \u2265 t) \u2264 (m+ n) exp (\n\u2212t2/2 \u03bd(M) + Lt/3\n) , (103)\nFurthermore\n2.\nEZ \u2264 \u221a 2\u03bd(M) log(m+ n) + 1\n3 L log(m+ n). (104)\nWe shall bound \u03b21 using part (ii) of Bernstein\u2019s inequality, and \u03b22 using part (ii) of Bernstein\u2019s inequality. The next two lemma\u2019s provide necessary material for bounding \u03b21, \u03b22.\nProposition 1. Let \u2206 be a random mask of size n \u00d7m, where a random location is chosen and set to 1, and rest of the entries are set to 0. Let X be a matrix of size n \u00d7m with entries bounded in absolute value by 1. Define S = 1\np X \u25e6\u2206\u2212X . Let p = 1 mn . Then,\n1. ||S|| \u2264 ||X||+ 1 p\n2. ES>S = ESS> = X \u25e6X \u2212XX>\nProof.\n||S|| = ||1 p X \u25e6\u2206\u2212X|| \u2264 ||X||+ ||1 p X \u25e6\u2206|| (a) \u2264 ||X||+ 1 p . (105)\nIn the above set of inequalities in order to derive (a) we used the fact that X \u25e6 \u2206 is an n \u00d7m matrix with a single non-zero entry bounded in absolute value by 1. Hence the spectral norm of this matrix will be bounded by 1. To derive the second part of the proposition we proceed as follows\nESS> = E(1 p X \u25e6\u2206\u2212X)(1 p X \u25e6\u2206\u2212X)> = E[ 1 p2 (X \u25e6\u2206)(X \u25e6\u2206)> \u2212 1 p (X \u25e6\u2206)X> \u2212 1 p X(X \u25e6\u2206)> +XX>].\n(106)\nVia elementary calculations, it is easy to verify that E [ 1\np2 (X \u25e6\u2206)(X \u25e6\u2206)>\n] = X \u25e6X (107)\nE [ 1\np X(X \u25e6\u2206)>\n] = E [ 1\np (X \u25e6\u2206)X>\n] = XX>. (108)\nThese identities allow us to conclude part (ii) of this proposition.\nWe are now ready to bound the quantities \u03b21, \u03b22\nLemma 4. Let p = 1 mn . Then,\n\u03b21 = E \u2225\u2225\u2225\u2225 1p|\u2126|X \u25e6\u2206\u2126 \u2212X \u2225\u2225\u2225\u2225 \u2264 \u221a 2 log(m+ n)||X \u25e6X \u2212XX>|| |\u2126| + log(m+ n)(p||X||+ 1) 3p|\u2126| . (109)\nProof.\n\u2225\u2225\u2225\u2225 1p|\u2126|X \u25e6\u2206\u2126 \u2212X \u2225\u2225\u2225\u2225 = 1|\u2126| \u2225\u2225\u2225\u2225\u2225\u2225\u2225 |\u2126|\u2211 j=1 (X \u25e6\u2206j \u2212X)\ufe38 \ufe37\ufe37 \ufe38 Sj \u2225\u2225\u2225\u2225\u2225\u2225\u2225 (110) Here \u22061, . . . ,\u2206\u2126 are random i.i.d. boolean masks with each of them having exactly one non-zero, whose location is chosen uniformly at random from [n] \u00d7 [m]. For this reason the matrices S1, . . . , S|\u2126| are i.i.d. matrices. It is easy to see that ESj = 0 for each j \u2265 1. Applying Bernstein\u2019s inequality (Theorem (2)) and using Proposition (1) to bound the necessary quantities we get that\nE \u2225\u2225\u2225\u2225 1p|\u2126|X \u25e6\u2206\u2126 \u2212X \u2225\u2225\u2225\u2225 = 1|\u2126| [\u221a 2 log(m+ n)|\u2126|||X \u25e6X \u2212XX>||+ log(m+ n) 3 (||X||+ 1 p ) ] (111)\n=\n\u221a 2 log(m+ n)||X \u25e6X \u2212XX>||\n|\u2126| + log(m+ n)(p||X||+ 1) 3p|\u2126| (112)\nNext we bound \u03b22.\nLemma 5. Let p = 1 mn . Then,\n\u03b22 = E \u2225\u2225\u2225\u2225 1p|\u2126|X \u25e6\u2206\u2126 \u2212X \u2225\u2225\u2225\u22252 \u2264 1 + (20mn log(n)3|\u2126| )2 + 10 log(n) |\u2126| ||X \u25e6X \u2212XX >||. (113)\nProof. Using part (i) of Bernstein\u2019s inequality we get that, for any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\n\u2016A\u2212X\u2016 \u2264 2 log\n( m+n \u03b4 ) 3|\u2126| ( ||X||+ 1 p ) + \u221a 2 log(m+n \u03b4 )||X \u25e6X \u2212XX>|| |\u2126| . (114)\nWorst case upper bound on \u2016A\u2212X\u2016 can be derived as follows\n\u2016A\u2212X\u2016 = || 1 p|\u2126|X \u25e6\u2206\u2126 \u2212X|| (115)\n\u2264 1 p|\u2126| ||X \u25e6\u2206\u2126||+ ||X|| (116)\n\u2264 1 p|\u2126| |\u2126|\u2211 j=1 ||X \u25e6\u2206j ||+ ||X|| (117)\n\u2264 1 p + ||X||. (118)\nUsing equations (114) and (115) we get\nE||A\u2212X||2 \u2264 (1\u2212\u03b4) 2 log (m+n\u03b4 ) 3|\u2126| ( ||X||+ 1 p ) + \u221a 2 log(m+n \u03b4 )||X \u25e6X \u2212XX>|| |\u2126| 2+\u03b4(1 p + ||X|| )2\n(119) Since each element of X is bounded by 1 in magnitude, we get that ||X|| \u2264 \u221a mn. Now, replace p = 1\nmn\nand choose \u03b4 = 1 (mn+ \u221a mn)2 . Using the inequality (a+ b)2 < 2a2 + 2b2 and over-approximating we get the desired result.\nFinal bound on 2. We are now ready to establish a bound on 2. In the next bound we shall no longer keep track of explicit constants. Instead in the following calculations we shall use a universal constant C > 0 whose value can change from one line to another.\nLemma 6. Let \u00b51 = E||N ||, \u00b52 = E||N ||2. Then, for some universal constant C > 0 we have 2 \u2264 O ( r\nm \u221a n\n(||M? \u2212 Z?||+ \u00b51) + r||M? \u2212 Z?||2 mn + r\u00b52 mn + r m + rmn log2(n) |\u2126|2\n) (120)\nProof. From Equation (99) we have\n2 \u2264 r||Z?||(2E||X \u2212 Z?||+ ||Z?||) + 8r(8E||X \u2212 Z?||2 + ||Z?||2) + r(2\u03b21 + 64\u03b22). (121)\nNow, using Lemma (4) and (5) to bound \u03b21, \u03b22, we get\n2 \u2264 Cr mn E [ ||Z?|| ||X \u2212 Z?||+ ||X \u2212 Z?||2 + ||Z?||2 +\n\u221a log(n)||X \u25e6X \u2212XX>||\n|\u2126| +\nlog(n) 3|\u2126| (||X||+mn) + 1 + m2n2 log2(n) |\u2126|2 + log(n) |\u2126| ||X \u25e6X \u2212XX >|| ] .\n(122)\nIn the above expectation the expectation is being taken w.r.t. the randomness in X due to additive noise of our model. We shall now compute the remaining expectations. For notational convenience, define \u00b51 = E||N ||, and \u00b52 = E||N ||2. Using the fact that X = M? +N , we get\nE\u2016X \u25e6X \u2212XX>\u2016 \u2264 E||X \u25e6X\u2016+ E||XX>|| (123) (a) \u2264 E||X||2 + E||(M? +N)(M? +N)>|| (124) (b) \u2264 E[||M?||2 + ||N ||2 + 2||M?|| ||N ||+ ||M?(M?)>||+M?N> +N(M?)> +NN>]\n(125)\n= 2||M?||2 + 2\u00b52 + 4||M?||\u00b51 (126)\nwhere to obtain inequality (a) we used the fact that ||A \u25e6 B|| \u2264 ||A||||B|| [33, Problem 1.6.13, page 23]. To obtain inequality (b) we used sub-additivity of norms, and the fact that spectral norm is sub-multiplicative. By Jensen\u2019s inequality we get\nE \u221a \u2016X \u25e6X \u2212XX>\u2016 \u2264 \u221a E\u2016X \u25e6X \u2212XX>\u2016 \u2264 \u221a 2||M?||2 + 2\u00b52 + 4\u00b51||M?|| (127)\nFinally using the sub-additivity of norms we get that\nE||X \u2212 Z?||2 = E||M? +N \u2212 Z?||2 \u2264 2||M? \u2212 Z?||2 + 2E||N ||2 = 2||M? \u2212 Z?||2 + 2\u00b52 (128) E||X \u2212 Z?|| = E||M? +N \u2212 Z?|| \u2264 E||M? \u2212 Z?||+ E||N || = E||M? \u2212 Z?||+ \u00b51 (129)\nNow, putting together Equations (122), (123), (127), (128), and substituting the worst case bound ||M?|| = C \u221a mn, we get\n2 \u2264 C [ r mn ||Z?|| (||M? \u2212 Z?||+ \u00b51) + r mn ||M? \u2212 Z?||2 + r mn (\u00b52 + ||Z?||2)+\nr\nmn\n\u221a log(n)\n|\u2126| ( mn+ \u00b51 \u221a mn+ \u00b52 ) + rmn log2(n) |\u2126|2 + r log(n) mn|\u2126| (mn+ \u00b52 + \u00b51 \u221a mn) ] . (130)\nWe can further simplify the above expression, by noting that the entries of N are bounded by 1, and hence \u00b51 = O( \u221a mn), \u00b52 = O(mn). Note that in reality \u00b51, \u00b52 are much smaller, and one could lose a lot of information by considering their worst case values. However, in order to simplify the above bound for 2 and make it interpretable, we shall selectively replace \u00b51, \u00b52 by \u221a mn,mn respectively, This allows us to gauge which terms are lower order terms and drop them. This gets us\n2 \u2264 O ( r\nm \u221a n\n(||M? \u2212 Z?||+ \u00b51) + r||M? \u2212 Z?||2 mn + r\u00b52 mn + r m + rmn log2(n) |\u2126|2\n) (131)\nC Proof of Theorem (1)\nFrom Lemma (2) we have\nMSE(M\u0302) \u2264 O\n(\u221a mn log(n) |\u2126| + \u221a n |\u2126| + mn |\u2126|3/2 + \u221a mn |\u2126| + 2 + \u221a 2 )\nFrom Lemma (6) we have 2 \u2264 O ( r\nm \u221a n\n(||M? \u2212 Z?||+ \u00b51) + r||M? \u2212 Z?||2 mn + r\u00b52 mn + r m + rmn log2(n) |\u2126|2\n) (132)\nPutting the above two equations together we get\nMSE(M\u0302) = O (\u221a r\nm +\n\u221a mn log(n)\n|\u2126| + mn |\u2126|3/2 +\n\u221a mn\n|\u2126| +\n\u221a r\nm \u221a n\n( \u00b51 +\n\u00b52\u221a n ) +\u221a\nr\u03b1\nm \u221a n\n( 1 +\n\u03b1\u221a n\n) + \u221a rmn log2(n) |\u2126|2 ) (133)"}, {"heading": "D Source for datasets", "text": "Here is where one can download the real world datasets on which all of our experiments were performed.\n1. Paper recommendation dataset:http://www.comp.nus.edu.sg/\u02dcsugiyama/ SchPaperRecData.html.\n2. Jester dataset: http://goldberg.berkeley.edu/jester-data/.\n3. Movie lens dataset: http://grouplens.org/datasets/movielens/\n4. Cameraman dataset: http://www.utdallas.edu/\u02dccxc123730/mh_bcs_spl.html"}, {"heading": "E RMSE plots with iterations", "text": "In Figure (3) we show how the RMSE of MMC\u2212c algorithm changes with iterations. These plots were made on the synthetic datasets that were used in our experiments. The value of p was set to 0.35. As one can see, on an average, there is a decreasing trend in the RMSE. This decrease is almost linear for small values of c and sub-linear for larger values of c."}], "references": [{"title": "Recommender systems", "author": ["Prem Melville", "Vikas Sindhwani"], "venue": "In Encyclopedia of machine learning. Springer,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Graph realization and low-rank matrix completion", "author": ["Mihai Cucuringu"], "venue": "PhD thesis, Princeton University,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "JMLR, 12:3413\u20133430,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Exact matrix completion via convex optimization. FOCM", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Candes", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["David Gross"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Convex optimization & Euclidean distance geometry", "author": ["Jon Dattorro"], "venue": "Lulu. com,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "author": ["Adam Tauman Kalai", "Ravi Sastry"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["Sham M Kakade", "Varun Kanade", "Ohad Shamir", "Adam Kalai"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Low-rank matrix completion by riemannian optimization", "author": ["Bart Vandereycken"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Riemannian pursuit for big matrix recovery", "author": ["Mingkui Tan", "Ivor W Tsang", "Li Wang", "Bart Vandereycken", "Sinno J Pan"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Rank-one matrix pursuit for matrix completion", "author": ["Zheng Wang", "Ming-Jun Lai", "Zhaosong Lu", "Wei Fan", "Hasan Davulcu", "Jieping Ye"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm", "author": ["Zaiwen Wen", "Wotao Yin", "Yin Zhang"], "venue": "Mathematical Programming Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "High-rank matrix completion", "author": ["Brian Eriksson", "Laura Balzano", "Robert Nowak"], "venue": "In AISTATS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Sparse subspace clustering with missing entries", "author": ["Congyuan Yang", "Daniel Robinson", "Rene Vidal"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A geometric analysis of subspace clustering with outliers", "author": ["Mahdi Soltanolkotabi", "Emmanuel J Candes"], "venue": "The Annals of Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["Ehsan Elhamifar", "Rene Vidal"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Completion of high-rank ultrametric matrices using selective entries", "author": ["Aarti Singh", "Akshay Krishnamurthy", "Sivaraman Balakrishnan", "Min Xu"], "venue": "In SPCOM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Retargeted matrix factorization for collaborative filtering", "author": ["Oluwasanmi Koyejo", "Sreangsu Acharyya", "Joydeep Ghosh"], "venue": "In Proceedings of the 7th ACM conference on Recommender systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "1-bit matrix completion", "author": ["Mark A Davenport", "Yaniv Plan", "Ewout van den Berg", "Mary Wootters"], "venue": "Information and Inference,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Semiparametric least squares (sls) and weighted sls estimation of single-index models", "author": ["Hidehiko Ichimura"], "venue": "Journal of Econometrics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Direct semiparametric estimation of single-index models with discrete covariates", "author": ["Joel L Horowitz", "Wolfgang H\u00e4rdle"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}, {"title": "Least squares revisited: Scalable approaches for multi-class prediction", "author": ["Alekh Agarwal", "Sham Kakade", "Nikos Karampatziakis", "Le Song", "Gregory Valiant"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Tfocs: Flexible first-order methods for rank minimization", "author": ["Stephen Becker", "E Candes", "M Grant"], "venue": "In Low-rank Matrix Optimization Symposium, SIAM Conference on Optimization,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Smoothness, low noise and fast rates", "author": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Matrix estimation by universal singular value thresholding", "author": ["Sourav Chatterjee"], "venue": "The Annals of Statistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "This problem has a plethora of applications such as collaborative filtering, recommender systems [1] and sensor networks [2].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "This problem has a plethora of applications such as collaborative filtering, recommender systems [1] and sensor networks [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Matrix completion has been well studied in machine learning, and we now know how to recover certain matrices given a few observed entries of the matrix [3, 4] when it is assumed to be low rank.", "startOffset": 152, "endOffset": 158}, {"referenceID": 3, "context": "Matrix completion has been well studied in machine learning, and we now know how to recover certain matrices given a few observed entries of the matrix [3, 4] when it is assumed to be low rank.", "startOffset": 152, "endOffset": 158}, {"referenceID": 2, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 3, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 4, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 5, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 6, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 7, "context": "Typical work in matrix completion assumes that the matrix to be recovered is incoherent, low rank, and entries are sampled uniformly at random [3, 4, 5, 6, 7, 8].", "startOffset": 143, "endOffset": 161}, {"referenceID": 8, "context": "The Euclidean distance matrix is a low-rank matrix [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "We believe that the proposed ADMM algorithm is useful in its own right and can be used in general isotonic regression problems elsewhere [10, 11].", "startOffset": 137, "endOffset": 145}, {"referenceID": 10, "context": "We believe that the proposed ADMM algorithm is useful in its own right and can be used in general isotonic regression problems elsewhere [10, 11].", "startOffset": 137, "endOffset": 145}, {"referenceID": 2, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 3, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 4, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 5, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 6, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 7, "context": "Classical matrix completion with and without noise has been investigated by several authors [3, 4, 5, 6, 7, 8].", "startOffset": 92, "endOffset": 110}, {"referenceID": 11, "context": "Progress has also been made on designing efficient algorithms to solve the ensuing convex optimization problem [12, 13, 14, 15].", "startOffset": 111, "endOffset": 127}, {"referenceID": 12, "context": "Progress has also been made on designing efficient algorithms to solve the ensuing convex optimization problem [12, 13, 14, 15].", "startOffset": 111, "endOffset": 127}, {"referenceID": 13, "context": "Progress has also been made on designing efficient algorithms to solve the ensuing convex optimization problem [12, 13, 14, 15].", "startOffset": 111, "endOffset": 127}, {"referenceID": 14, "context": "Progress has also been made on designing efficient algorithms to solve the ensuing convex optimization problem [12, 13, 14, 15].", "startOffset": 111, "endOffset": 127}, {"referenceID": 15, "context": "[16] suggested looking at the neighbourhood of each incomplete point for completion, [17] used a combination of spectral clustering techniques as done in [18, 19] along with learning sparse representations via convex optimization to estimate the incomplete matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[16] suggested looking at the neighbourhood of each incomplete point for completion, [17] used a combination of spectral clustering techniques as done in [18, 19] along with learning sparse representations via convex optimization to estimate the incomplete matrix.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "[16] suggested looking at the neighbourhood of each incomplete point for completion, [17] used a combination of spectral clustering techniques as done in [18, 19] along with learning sparse representations via convex optimization to estimate the incomplete matrix.", "startOffset": 154, "endOffset": 162}, {"referenceID": 18, "context": "[16] suggested looking at the neighbourhood of each incomplete point for completion, [17] used a combination of spectral clustering techniques as done in [18, 19] along with learning sparse representations via convex optimization to estimate the incomplete matrix.", "startOffset": 154, "endOffset": 162}, {"referenceID": 19, "context": "[20] consider a certain specific class of high-rank matrices that are obtained from ultra-metrics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [21] the authors consider a model similar to ours, but instead of learning a single monotonic function, they learn multiple monotonic functions, one for each row of the matrix.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Davenport et al [22] studied the one-bit matrix completion problem.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "The MMC model is inspired by the single-index model (SIM) that has been studied both in statistics [10, 11] and econometrics for regression problems [23, 24].", "startOffset": 99, "endOffset": 107}, {"referenceID": 10, "context": "The MMC model is inspired by the single-index model (SIM) that has been studied both in statistics [10, 11] and econometrics for regression problems [23, 24].", "startOffset": 99, "endOffset": 107}, {"referenceID": 22, "context": "The MMC model is inspired by the single-index model (SIM) that has been studied both in statistics [10, 11] and econometrics for regression problems [23, 24].", "startOffset": 149, "endOffset": 157}, {"referenceID": 23, "context": "The MMC model is inspired by the single-index model (SIM) that has been studied both in statistics [10, 11] and econometrics for regression problems [23, 24].", "startOffset": 149, "endOffset": 157}, {"referenceID": 10, "context": "The LPAV 4 algorithm introduced in [11] outputs the best function \u011d in G that minimizes \u2211n i=1(g(pi)\u2212 yi) .", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "The idea of using calibrated loss functions was first introduced for learning single index models [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "39 in [26]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "We compare the performance of MMC\u22121, MMC\u2212 c, MMC- LS, and nuclear norm based low-rank matrix completion (LRMC) [4] on various synthetic and real world datasets.", "startOffset": 111, "endOffset": 114}, {"referenceID": 14, "context": "The increasing rank procedure was inspired by the work of [15], where they demonstrated that such procedures are suitable for low-rank approximation problems.", "startOffset": 58, "endOffset": 62}, {"referenceID": 26, "context": "Instead we use the Alternating Direction Method of Multipliers (ADMM) algorithm for our problem [27] which allows us to exploit the rich structure present in problem (22).", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "In our practical implementations we follow the advice as mentioned in [27] and set abs and rel to 10\u22122.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "For our implementations we assume that r is unknown, and estimate it either (i) via the use of a dedicated validation set in the case of MMC \u2212 1 or (ii) adaptively, where we progressively increase the estimate of our rank until a sufficient decrease in error over the training set is achieved [15].", "startOffset": 293, "endOffset": 297}, {"referenceID": 27, "context": "For an implementation of the LRMC algorithm we used a standard off-the-shelf implementation from TFOCS [28].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "The baseline algorithm chosen for low rank matrix completion is LMaFit-A [15] 7.", "startOffset": 73, "endOffset": 77}], "year": 2015, "abstractText": "Most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces. In real-world settings, however, the linear structure underlying these models is distorted by a (typically unknown) nonlinear transformation. This paper addresses the challenge of matrix completion in the face of such nonlinearities. Given a few observations of a matrix that are obtained by applying a Lipschitz, monotonic function to a low rank matrix, our task is to estimate the remaining unobserved entries. We propose a novel matrix completion method that alternates between lowrank matrix estimation and monotonic function estimation to estimate the missing matrix elements. Mean squared error bounds provide insight into how well the matrix can be estimated based on the size, rank of the matrix and properties of the nonlinear transformation. Empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}