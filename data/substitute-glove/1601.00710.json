{"id": "1601.00710", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Multi-Source Neural Translation", "abstract": "We commercial close multi - regarding handheld translation model this tram probably to ability also probability in next target English string present French took German sources. Using given embryonic encoder - hand-held creation, we help which better methods and information all to + 4. dec. Bleu increases without top within a very demand face - called correlates spelling formula.", "histories": [["v1", "Tue, 5 Jan 2016 00:49:22 GMT  (259kb,D)", "http://arxiv.org/abs/1601.00710v1", "5 pages, 6 figures"]], "COMMENTS": "5 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["barret zoph", "kevin knight"], "accepted": true, "id": "1601.00710"}, "pdf": {"name": "1601.00710.pdf", "metadata": {"source": "CRF", "title": "Multi-Source Neural Translation", "authors": ["Barret Zoph"], "emails": ["zoph@isi.edu", "knight@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "Kay (2000) points out that if a document is translated once, it is likely to be translated again and again into other languages. This gives rise to an interesting idea: a human does the first translation by hand, then turns the rest over to machine translation (MT). The translation system now has two strings as input, which can reduce ambiguity via \u201ctriangulation\u201d (Kay\u2019s term). For example, the normally ambiguous English word \u201cbank\u201d may be more easily translated into French in the presence of a second, German input string containing the word \u201cFlussufer\u201d (river bank).\nOch and Ney (2001) describe such a multisource MT system. They first train separate bilingual MT systems F\u2192E, G\u2192E, etc. At runtime, they separately translate input strings f and g into candidate target strings e1 and e2, then select the best one of the two. A typical selection factor is the product of the system scores. Schwartz (2008) revisits such factors in the context of log-linear models and Bleu score, while Max et al. (2010) re-rank F\u2192E n-best lists using n-gram precision with respect to G\u2192E translations. Callison-Burch (2002) exploits hypothesis selection in multi-source MT to expand available corpora, via co-training.\nOthers use system combination techniques to merge hypotheses at the word level, creating the\nability to synthesize new translations outside those proposed by the single-source translators. These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al., 2009), and median strings (Gonza\u0301lez-Rubio and Casacuberta, 2010).\nThe above work all relies on base MT systems trained on bilingual data, using traditional methods. This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models. Previous authors possibly considered a three-dimensional translation table t(e|f, g) to be prohibitive.\nIn this paper, by contrast, we train a P(e|f, g) model directly on trilingual data, and we use that model to decode an (f, g) pair simultaneously. We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape. Our contributions are as follows: \u2022 We train a P(e|f, g) model directly on trilin-\ngual data, and we use it to decode a new source string pair (f, g) into target string e. \u2022 We show positive Bleu improvements over\nstrong single-source baselines. \u2022 We show that improvements are best when\nthe two source languages are more distant from each other. We are able to achieve these results using the framework of neural encoder-decoder models, where multi-target MT (Dong et al., 2015) and multi-source, cross-modal mappings have been explored (Luong et al., 2015a)."}, {"heading": "2 Multi-Source Neural MT", "text": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Castan\u0303o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recur-\nar X\niv :1\n60 1.\n00 71\n0v 1\n[ cs\n.C L\n] 5\nJ an\n2 01\n6\nrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector in a target sentence.1\nIn this paper, we use a four-layer encoderdecoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained for maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). For our baseline singlesource MT system we use two different models, one of which implements the local attention plus feed-input model from Luong et al. (2015b).\nFigure 2 shows our approach to multi-source MT. Each source language has its own encoder. The question is how to combine the hidden states and cell states from each encoder, to pass on to the decoder. Black combiner blocks implement a function whose input is two hidden states (h1 and h2) and two cell states (c1 and c2), and whose output is a single hidden state h and cell state c. We propose two combination methods."}, {"heading": "2.1 Basic Combination Method", "text": "The Basic method works by concatenating the two hidden states from the source encoders, applying a linear transformation Wc (size 2000 x 1000), then sending its output through a tanh non-linearity. This operation is represented by the equation:\nh = tanh ( Wc[h1;h2] ) (1)\nWc and all other weights in the network are\n1We follow previous authors in presenting the source sentence to the encoder in reverse order.\nlearned from example string triples drawn from a trilingual training corpus.\nThe new cell state is simply the sum of the two cell states from the encoders.\nc = c1 + c2 (2)\nWe also attempted to concatenate cell states and apply a linear transformation, but training diverges due to large cell values."}, {"heading": "2.2 Child-Sum Method", "text": "Our second combination method is inspired by the Child-Sum Tree-LSTMs of Tai et al. (2015). Here, we use an LSTM variant to combine the two hidden states and cells. The standard LSTM input, output, and new cell value are all calculated. Then cell states from each encoder get their own forget gates. The final cell state and hidden state are calculated as in a normal LSTM. More precisely:\ni = sigmoid ( W i1h1 +W i 2h2 ) (3)\nf = sigmoid ( W fi hi ) (4)\no = sigmoid ( W o1h1 +W o 2h2 ) (5)\nu = tanh ( W u1 h1 +W u 2 h2 ) (6)\nc = if uf + f1 c1 + f2 c2 (7)\nh = of tanh(cf ) (8)\nThis method employs eight new matrices (the W \u2019s in the above equations), each of size 1000 x 1000. The symbol represents an elementwise multiplication. In equation 3, i represents\nA B C <EOS> W X Y Z\n<EOS> Z Y X W\nthe input gate of a typical LSTM cell. In equation 4, there are two forget gates indexed by the subscript i that serve as the forget gates for each of the incoming cells for each of the encoders. In equation 5, o represents the output gate of a normal LSTM. i, f , o, and u are all size-1000 vectors."}, {"heading": "2.3 Multi-Source Attention", "text": "Our single-source attention model is modeled off the local-p attention model with feed input from Luong et al. (2015b), where hidden states from the top decoder layer can look back at the top hidden states from the encoder. The top decoder hidden state is combined with a weighted sum of the encoder hidden states, to make a better hidden state vector (h\u0303t), which is passed to the softmax output layer. With input-feeding, the hidden state from the attention model is sent down to the bottom decoder layer at the next time step.\nThe local-p attention model from Luong et al. (2015b) works as follows. First, a position to look at in the source encoder is predicted by equation 9:\npt = S \u00b7 sigmoid(vTp tanh(Wpht)) (9)\nS is the source sentence length, and vp and Wp are learned parameters, with vp being a vector of dimension 1000, and Wp being a matrix of dimension 1000 x 1000.\nAfter pt is computed, a window of size 2D + 1 is looked at in the top layer of the source encoder centered around pt (D = 10). For each hidden state in this window, we compute an alignment\nscore at(s), between 0 and 1. This alignment score is computed by equations 10, 11 and 12:\nat(s) = align(ht, hs)exp (\u2212(s\u2212 pt)2\n2\u03c32\n) (10)\nalign(ht, hs) = exp(score(ht, hs))\u2211 s\u2032 exp(score(ht, hs\u2032)) (11)\nscore(ht, hs) = h T t Wahs (12)\nIn equation 10, \u03c3 is set to be D/2 and s is the source index for that hidden state. Wa is a learnable parameter of dimension 1000 x 1000.\nOnce all of the alignments are calculated, ct is created by taking a weighted sum of all source hidden states multiplied by their alignment weight.\nThe final hidden state sent to the softmax layer is given by:\nh\u0303t = tanh ( Wc[ht; ct] ) (13)\nWe modify this attention model to look at both source encoders simultaneously. We create a context vector from each source encoder named c1t and c2t instead of the just ct in the single-source attention model:\nh\u0303t = tanh ( Wc[ht; c 1 t ; c 2 t ] )\n(14)\nIn our multi-source attention model we now have two pt variables, one for each source encoder.\nWe also have two separate sets of alignments and therefore now have two ct values denoted by c1t and c2t as mentioned above. We also have distinct Wa, vp, and Wp parameters for each encoder."}, {"heading": "3 Experiments", "text": "We use English, French, and German data from a subset of the WMT 2014 dataset (Bojar et al., 2014). Figure 3 shows statistics for our training set. For development, we use the 3000 sentences supplied by WMT. For testing, we use a 1503-line trilingual subset of the WMT test set.\nFor the single-source models, we follow the training procedure used in Luong et al. (2015b), but with 15 epochs and halving the learning rate every full epoch after the 10th epoch. We also rescale the normalized gradient when norm> 5. For training, we use a minibatch size of 128, a hidden state size of 1000, and dropout as in Zaremba et al. (2014). The dropout rate is 0.2, the initial parameter range is [-0.1, +0.1], and the learning rate is 1.0. For the normal and multi-source attention models, we adjust these parameters to 0.3, [-0.08, +0.08], and 0.7, respectively, to adjust for overfitting.\nSource 1: UNK Aspekte sind ebenfalls wichtig . Target: UNK aspects are important , too . Source 2: Les aspects UNK sont \u00e9galement importants .\nFigure 5: Action of the multi-attention model as the neural decoder generates target English from French/German sources (test set). Lines show strengths of at(s).\nTarget = German Source Method Ppl BLEU French \u2014 12.3 10.6 English \u2014 9.6 13.4 French+English Basic 9.1 14.5 French+English Child-Sum 9.5 14.4 English Attention 7.3 17.6 French+English B-Attent. 6.9 18.6 French+English CS-Attent. 7.1 18.2\nFigure 6: Multi-source MT results for target German, with source languages French and English.\nFigure 4 show our results for target English, with source languages being French and German). We see that the Basic combination method yields a +4.8 Bleu improvement over the strongest singlesource, attention-based system. It also improves Bleu by +2.2 over the non-attention baseline. The Child-Sum method gives improvements of +4.4 and +1.4. We also confirm that two copies of the same French input yields no BLEU improvement.\nFigure 5 shows the action of the multi-attention model during decoding.\nWhen our source languages are English and French (Figure 6), we observe smaller BLEU gains (up to +1.1). This is evidence that the more distinct the source languages, the better they disambiguate each other."}, {"heading": "4 Conclusion", "text": "We describe a multi-source neural MT system that gets up to +4.8 Bleu gains over a very strong attention-based, single-source baseline. We obtain this result through a novel encoder-vector combination method and a novel multi-attention system. We release the code for these experiments at https://github.com/isi-nlp/ Zoph_RNN."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Co-training for statistical machine translation", "author": ["C. Callison-Burch."], "venue": "Master\u2019s thesis, School of Informatics, University of Edinburgh.", "citeRegEx": "Callison.Burch.,? 2002", "shortCiteRegEx": "Callison.Burch.", "year": 2002}, {"title": "A connectionist approach to machine translation", "author": ["M.A. Casta\u00f1o", "F. Casacuberta."], "venue": "EUROSPEECH.", "citeRegEx": "Casta\u00f1o and Casacuberta.,? 1997", "shortCiteRegEx": "Casta\u00f1o and Casacuberta.", "year": 1997}, {"title": "How to make a Frenemy: Multitape FSTs for portmanteau generation", "author": ["A. Deri", "K. Knight."], "venue": "Proc. NAACL.", "citeRegEx": "Deri and Knight.,? 2015", "shortCiteRegEx": "Deri and Knight.", "year": 2015}, {"title": "Multi-task learning for multiple language translation", "author": ["D. Dong", "H. Wu", "W. he", "D. Yu", "H. Wang."], "venue": "Proc. ACL.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "On relations defined by generalized finite automata", "author": ["C. Elgot", "J. Mezei."], "venue": "IBM Journal of Research and Development, 9(1):47\u201368.", "citeRegEx": "Elgot and Mezei.,? 1965", "shortCiteRegEx": "Elgot and Mezei.", "year": 1965}, {"title": "A program for aligning sentences in bilingual corpora", "author": ["W. A Gale", "K. W Church."], "venue": "Computational linguistics, 19(1):75\u2013102.", "citeRegEx": "Gale and Church.,? 1993", "shortCiteRegEx": "Gale and Church.", "year": 1993}, {"title": "On the use of median string for multi-source translation", "author": ["J. Gonz\u00e1lez-Rubio", "F. Casacuberta."], "venue": "Proc. ICPR.", "citeRegEx": "Gonz\u00e1lez.Rubio and Casacuberta.,? 2010", "shortCiteRegEx": "Gonz\u00e1lez.Rubio and Casacuberta.", "year": 2010}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural Computation, 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Regular models of phonological rule systems", "author": ["R. Kaplan", "M. Kay."], "venue": "Computational Linguistics, 20(3):331\u2013378.", "citeRegEx": "Kaplan and Kay.,? 1994", "shortCiteRegEx": "Kaplan and Kay.", "year": 1994}, {"title": "Triangulation in translation", "author": ["M. Kay."], "venue": "Keynote at MT 2000 Conference, University of Exeter.", "citeRegEx": "Kay.,? 2000", "shortCiteRegEx": "Kay.", "year": 2000}, {"title": "Multi-task sequence to sequence learning", "author": ["M. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser."], "venue": "arXiv. http://arxiv.org/abs/1511.06114.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M. Luong", "H. Pham", "C. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment", "author": ["E. Matusov", "N. Ueffing", "H. Ney."], "venue": "Proc. EACL.", "citeRegEx": "Matusov et al\\.,? 2006", "shortCiteRegEx": "Matusov et al\\.", "year": 2006}, {"title": "Contrastive lexical evaluation of machine translation", "author": ["A. Max", "J. Crego", "F. Yvon."], "venue": "Proc. LREC.", "citeRegEx": "Max et al\\.,? 2010", "shortCiteRegEx": "Max et al\\.", "year": 2010}, {"title": "Asynchronous translations with recurrent neural nets", "author": ["R. Neco", "M. Forcada."], "venue": "International Conf. on Neural Networks, volume 4, pages 2535\u2013 2540.", "citeRegEx": "Neco and Forcada.,? 1997", "shortCiteRegEx": "Neco and Forcada.", "year": 1997}, {"title": "Statistical multi-source translation", "author": ["F.J. Och", "H. Ney."], "venue": "Proc. MT Summit.", "citeRegEx": "Och and Ney.,? 2001", "shortCiteRegEx": "Och and Ney.", "year": 2001}, {"title": "Word lattices for multi-source translation", "author": ["J. Schroeder", "T. Cohn", "P. Koehn."], "venue": "Proc. EACL.", "citeRegEx": "Schroeder et al\\.,? 2009", "shortCiteRegEx": "Schroeder et al\\.", "year": 2009}, {"title": "Multi-source translation methods", "author": ["L. Schwartz."], "venue": "Proc. AMTA.", "citeRegEx": "Schwartz.,? 2008", "shortCiteRegEx": "Schwartz.", "year": 2008}, {"title": "Text-translation alignment: Three languages are better than two", "author": ["M. Simard."], "venue": "Proc. EMNLP/VLC.", "citeRegEx": "Simard.,? 1999", "shortCiteRegEx": "Simard.", "year": 1999}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le."], "venue": "Proc. NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos."], "venue": "Proceedings of the IEEE, 78(10):1550\u20131560.", "citeRegEx": "Werbos.,? 1990", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals."], "venue": "CoRR, abs/1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al.", "startOffset": 41, "endOffset": 87}, {"referenceID": 17, "context": "These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al.", "startOffset": 41, "endOffset": 87}, {"referenceID": 17, "context": ", 2009), source-side string combination (Schroeder et al., 2009), and median strings (Gonz\u00e1lez-Rubio and Casacuberta, 2010).", "startOffset": 40, "endOffset": 64}, {"referenceID": 7, "context": ", 2009), and median strings (Gonz\u00e1lez-Rubio and Casacuberta, 2010).", "startOffset": 28, "endOffset": 66}, {"referenceID": 6, "context": "This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models.", "startOffset": 46, "endOffset": 69}, {"referenceID": 19, "context": "This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models.", "startOffset": 89, "endOffset": 103}, {"referenceID": 5, "context": "We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape.", "startOffset": 50, "endOffset": 118}, {"referenceID": 9, "context": "We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape.", "startOffset": 50, "endOffset": 118}, {"referenceID": 3, "context": "We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape.", "startOffset": 50, "endOffset": 118}, {"referenceID": 4, "context": "We are able to achieve these results using the framework of neural encoder-decoder models, where multi-target MT (Dong et al., 2015) and multi-source, cross-modal mappings have been explored (Luong et al.", "startOffset": 113, "endOffset": 132}, {"referenceID": 11, "context": ", 2015) and multi-source, cross-modal mappings have been explored (Luong et al., 2015a).", "startOffset": 66, "endOffset": 87}, {"referenceID": 1, "context": "Callison-Burch (2002) exploits hypothesis selection in multi-source MT to expand available corpora, via co-training.", "startOffset": 0, "endOffset": 22}, {"referenceID": 15, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 2, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 20, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 0, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 12, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurar X iv :1 60 1.", "startOffset": 47, "endOffset": 170}, {"referenceID": 20, "context": "Figure 1: The encoder-decoder framework for neural machine translation (NMT) (Sutskever et al., 2014).", "startOffset": 77, "endOffset": 101}, {"referenceID": 8, "context": "1 In this paper, we use a four-layer encoderdecoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained for maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990).", "startOffset": 111, "endOffset": 145}, {"referenceID": 22, "context": "1 In this paper, we use a four-layer encoderdecoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained for maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990).", "startOffset": 234, "endOffset": 248}, {"referenceID": 8, "context": "1 In this paper, we use a four-layer encoderdecoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained for maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). For our baseline singlesource MT system we use two different models, one of which implements the local attention plus feed-input model from Luong et al. (2015b). Figure 2 shows our approach to multi-source MT.", "startOffset": 112, "endOffset": 411}, {"referenceID": 21, "context": "Our second combination method is inspired by the Child-Sum Tree-LSTMs of Tai et al. (2015). Here, we use an LSTM variant to combine the two hidden states and cells.", "startOffset": 73, "endOffset": 91}, {"referenceID": 11, "context": "Our single-source attention model is modeled off the local-p attention model with feed input from Luong et al. (2015b), where hidden states from the top decoder layer can look back at the top hidden states from the encoder.", "startOffset": 98, "endOffset": 119}, {"referenceID": 11, "context": "Our single-source attention model is modeled off the local-p attention model with feed input from Luong et al. (2015b), where hidden states from the top decoder layer can look back at the top hidden states from the encoder. The top decoder hidden state is combined with a weighted sum of the encoder hidden states, to make a better hidden state vector (h\u0303t), which is passed to the softmax output layer. With input-feeding, the hidden state from the attention model is sent down to the bottom decoder layer at the next time step. The local-p attention model from Luong et al. (2015b) works as follows.", "startOffset": 98, "endOffset": 584}, {"referenceID": 11, "context": "For the single-source models, we follow the training procedure used in Luong et al. (2015b), but with 15 epochs and halving the learning rate every full epoch after the 10th epoch.", "startOffset": 71, "endOffset": 92}, {"referenceID": 11, "context": "For the single-source models, we follow the training procedure used in Luong et al. (2015b), but with 15 epochs and halving the learning rate every full epoch after the 10th epoch. We also rescale the normalized gradient when norm> 5. For training, we use a minibatch size of 128, a hidden state size of 1000, and dropout as in Zaremba et al. (2014). The dropout rate is 0.", "startOffset": 71, "endOffset": 350}], "year": 2016, "abstractText": "We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.", "creator": "TeX"}}}