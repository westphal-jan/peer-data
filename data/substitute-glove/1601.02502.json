{"id": "1601.02502", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2016", "title": "Trans-gram, Fast Cross-lingual Word-embeddings", "abstract": "We adopting Trans - citrate, a sometimes others computationally - components effective even appear our as align wordembeddings of giving variety of text, using either monolingual initial and also smaller set of sentence - diametrically measured. We could done set method it compute rotating wordembeddings for thirteen - one originated using English both a pivot language. We events indeed giving paradigm features particularly dominate east languages having, we do though have aligned actual, if though those other do though therefore however three pivots language. We also achieving assembly of the author announcement on standard ground - culti versions variant of gives spelling tasks.", "histories": [["v1", "Mon, 11 Jan 2016 16:12:32 GMT  (315kb)", "http://arxiv.org/abs/1601.02502v1", "EMNLP 2015"]], "COMMENTS": "EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jocelyn coulmance", "jean-marc marty", "guillaume wenzek", "amine benhalloum"], "accepted": true, "id": "1601.02502"}, "pdf": {"name": "1601.02502.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jean-Marc Marty", "Guillaume Wenzek"], "emails": ["joc@proxem.com", "jmm@proxem.com", "guw@proxem.com", "aba@proxem.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n02 50\n2v 1\n[ cs\n.C L\n] 1\n1 Ja\nn 20\n16\nWe introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks."}, {"heading": "1 Introduction", "text": "Word-embeddings are a representation of words with fixed-sized vectors. It is a distributed representation (Hinton, 1984) in the sense that there is not necessarily a one-to-one correspondence between vector dimensions and linguistic properties. The linguistic properties are distributed along the dimensions of the space.\nA popular method to compute wordembeddings is the Skip-gram model (Mikolov et al., 2013a). This algorithm learns high-quality word vectors with a computation cost much lower than previous methods. This allows the processing of very important amounts of data. For instance, a 1.6 billion words dataset can be processed in less than one day.\nSeveral authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015).\n\u2217These authors contributed equally.\nIn this article, we introduce a new method called Trans-gram, which learns word embeddings aligned across many languages, in a simple and efficient fashion, using only sentence alignments rather than word alignments. We compare our method with previous approaches on a crosslingual document classification task and on a word translation task and obtain state of the art results on these tasks. Additionally, word-embeddings for twenty-one languages are learned simultaneously - to our knowledge - for the first time, in less than two and a half hours. Furthermore, we illustrate some interesting properties that are captured such as cross-lingual analogies, e.g ~reyes \u2212 ~Mannde + ~femmefr \u2248 ~reginait which can be used for disambiguation."}, {"heading": "2 Review of Previous Work", "text": "A number of methods have been explored to train and align bilingual word-embeddings. These methods pursue two objectives: first, similar representations (i.e. spatially close) must be assigned to similar words (i.e. \u201csemantically close\u201d) within each language - this is the mono-lingual objective; second, similar representations must be assigned to similar words across languages - this is the cross-lingual objective.\nThe simplest approach consists in separating the mono-lingual optimization task from the crosslingual optimization task. This is for example the case in (Mikolov et al., 2013b). The idea is to separately train two sets of word-embeddings for each language and then to do a parametric estimation of the mapping between word-embeddings across languages. This method was further extended by (Faruqui and Dyer, 2014). Even though those algorithms proved to be viable and fast, it is not clear whether or not a simple mapping between whole languages exists. Moreover, they require word alignments which are a rare and expensive resource.\nAnother approach consists in focusing entirely on the cross-lingual objective. This was explored in (Hermann and Blunsom, 2013; Lauly et al., 2014) where every couple of aligned sentences is transformed into two fixed-size vectors. Then, the model minimizes the Euclidean distance between both vectors. This idea allows processing corpus aligned at sentence-level rather than word-level. However, it does not leverage the abundance of existing mono-lingual corpora .\nA popular approach is to jointly optimize the mono-lingual and cross-lingual objectives simultaneously. This is mostly done by minimizing the sum of mono-lingual loss functions for each language and the cross-lingual loss function. (Klementiev et al., 2012) proved this approach to be useful by obtaining state-of-the-art results on several tasks. (Gouws et al., 2015) extends their work with a more computationally-efficient implementation."}, {"heading": "3 From Skip-Gram to Trans-Gram", "text": ""}, {"heading": "3.1 Skip-gram", "text": "We briefly introduce the Skip-gram algorithm, as we will need it for further explanations. Skipgram allows to train word embeddings for a language using mono-lingual data. This method uses a dual representation for words. Each word w has two embeddings: a target vector, ~w (\u2208 RD), and a context vector, ~w (\u2208 RD). The algorithm tries to estimate the probability of a word w to appear in the context of a word c. More precisely we are learning the embeddings ~w, ~c so that: \u03c3(~w \u00b7 ~c) = P (w|c) where \u03c3 is the sigmoid function.\nA simplified version of the loss function minimized by Skip-gram is the following:\nJ = \u2211\ns\u2208C\n\u2211\nw\u2208s\n\u2211\nc\u2208s[w\u2212l:w+l]\n\u2212 log \u03c3(~w \u00b7 ~c) (1)\nwhere C is the set of sentences constituting the training corpus, and s[w \u2212 l : w + l] is a word window on the sentence s centered around w. For the sake of simplicity this equation does not include the \u201cnegative-sampling\u201d term, see (Mikolov et al., 2013a) for more details.\nSkip-gram can be seen as a materialization of the distributional hypothesis (Harris, 1968): \u201cWords used in similar contexts have similar meanings\u201d. We will now see how to extend this idea to cross-lingual contexts."}, {"heading": "3.2 Trans-gram", "text": "In this section we introduce Trans-gram, a new method to compute aligned word-embeddings for a variety of languages.\nOur method will minimize the summation of mono-lingual losses and cross-lingual losses. Like in BilBOWA (Gouws et al., 2015), we use Skipgram as a mono-lingual loss. Assuming we are trying to learn aligned word vectors for languages e (e.g. English) and f (e.g. French), we note Je and Jf the two mono-lingual losses.\nIn BilBOWA, the cross-lingual loss function is a distance between bag-of-words representations of two aligned sentences. But as (Levy and Goldberg, 2014) showed that the Skipgram loss function extracts interesting linguistic features, we wanted to use a loss function for the cross-lingual objective that will be closer to Skipgram than BilBOWA.\nTherefore, we introduce a new task, Transgram, similar to Skip-gram. Each English sentence se in our aligned corpus Ae,f is aligned with a French sentence sf . In Skip-gram, the context picked for a target word we in a sentence se is the set of words ce appearing in the window centered around we: se[we\u2212 l : we+ l]. In Trans-gram, the context picked for a target word we in a sentence se will be all the words cf appearing in sf . The loss can thus be written as:\n\u2126e,f = \u2211\n(se,sf )\u2208Ae,f\n\u2211\nwe\u2208se\n\u2211\ncf\u2208sf\n\u2212 log \u03c3( ~we \u00b7 ~cf )\n(2) This loss isn\u2019t symmetric with respect to the languages. We, therefore, use two cross-lingual objectives: \u2126e,f aligning e\u2019s target vectors and f \u2019s context vectors and \u2126f,e aligning f \u2019s target vectors and e\u2019s context vectors. By comparison BilBOWA only aligns e\u2019s target vectors and f \u2019s target vectors. The figure 1 illustrates the four objectives.\nNotice that we make the assumption that the meaning of a word is uniformly distributed in the whole sentence. This assumption, although a naive one, gave us in practice excellent results. Also our method uses only sentence-aligned corpus and not word-aligned corpus which are rarer.\nTo add a third language i (e.g. Italian), we just have to add 3 new objectives (Ji, \u2126e,i and \u2126i,e) to the global loss. If available we could also add \u2126f,i or \u2126i,f but in our case we only used corpora aligned with English."}, {"heading": "4 Implementation", "text": "In our experiments, we used the Europarl (Koehn, 2005) aligned corpora. Europarl-v7 has two peculiarities: firstly, the corpora are aligned at sentence-level; secondly each pair of languages contains English as one of its members: for instance, there is no French/Italian pair. In other words, English is used as a pivot language. No bi-lingual lexicons nor other bi-lingual datasets aligned at the word level were used.\nUsing only the Europarl-v7 texts as both monolingual and bilingual data, it took 10 minutes to align 2 languages, and two and a half hours to align the 21 languages of the corpus, in a 40 dimensional space on a 6 core computer. We also computed 300 dimensions vectors using the Wikipedia extracts provided by (Al-Rfou et al., 2013) as monolingual data for each language. The training time was 21 hours."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Reuters Cross-lingual Document Classification", "text": "We used a subset of the English and German sections of the Reuters RCV1/RCV2 corpora (Lewis and Li, 2004) (10000 documents each), as in (Klementiev et al., 2012), and we replicated the experimental setting. In the English dataset, there are four topics: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). We used these topics as our labels and we only selected documents labeled with a single topic. We trained our classifier on the articles of one language, where each document was represented using an IDF weighted sum of the vectors of its words, we then tested it on the articles of the other language. The classifier used was\nan averaged perceptron, and we used the implementation from (Klementiev et al., 2012)1. The word vectors were computed on the Europarl-v7 parallel corpus with size 40 like other methods. For this task only the target vectors where used.\nWe report the percentage precision obtained with our method, in comparison with other methods, in Table 1. The table also include results obtained with 300 dimensions vectors trained by Trans-gram with the Europarl-v7 as parallel corpus and the Wikipedia as mono-lingual corpus. The previous state of the art results were detained (Gouws et al., 2015) with BilBOWA and (Lauly et al., 2014) with their Bilingual Autoencoder model. This model learns word embeddings during a translation task that uses an encoder-decoder approach. We also report the scores from Klementiev et al. who introduced the task and the BiCVM model scores from (Hermann and Blunsom, 2013).\nThe results show an overall significant improvement over the other methods, with the added advantage of being computationally efficient."}, {"heading": "5.2 P@k Word Translation", "text": "Next we evaluated our method on a word translation task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were extracted from the publicly available WMT112 corpus. The experiments were done for two sets of translation: English to Spanish and Spanish to English. (Mikolov et al., 2013b) extracted the top 6K most frequent words and translated them with Google Translate. They used the top 5K pairs to train a translation matrix, and evaluated their method on the remaining 1K . As our English and\n1Thanks to S. Gouws for providing this implementation 2http://www.statmt.org/wmt11/\nMethod En \u2192 De De \u2192 En Speed-up in training time\nKlementiev et al. 77.6% 71.1% \u00d71 Bilingual Auto-encoder 91.8% 72.8% \u00d73 BiCVM 83.7% 71.4% \u00d7320 BilBOWA 86,5% 75% \u00d7800\nTrans-gram 87,8% 78,7% \u00d7600 Trans-gram (size 300 vectors EP+WIKI) 91,1% 78,4%\nTable 1: Comparison of Trans-gram with various methods for Reuters English/German classification\nMethod En \u2192 Es P@1 En \u2192 Es P@5 Es \u2192 En P@1 Es \u2192 En P@5\nEdit distance 13% 18% 24% 27% Bing 55% 71% Translation Matrix 33% 35% 51% 52% BilBOWA 39% 44% 51% 55%\nTrans-gram 45% 61% 47% 62%\nTable 2: Results on the translation task\nSpanish vectors are already aligned we don\u2019t need the 5K training pairs and use only the 1K test pairs.\nThe reported score, the translation precision P@k, is the fraction of test-pairs where the target translation (Google Translate) is one of the k translations proposed by our model. For a given English word, w, our model takes its target vectors ~w and proposes the k closest Spanish word using the co-similarity of their vectors to ~w. We compare ourselves to the \u201ctranslation matrix\u201d method and to the BilBowa aligned vectors. We also report the scores obtained by a trivial algorithm that uses edit-distance to determine the closest translation and by the Bing Translator service."}, {"heading": "6 Interesting properties", "text": ""}, {"heading": "6.1 Cross-lingual disambiguation", "text": "We now present the task of cross-lingual disambiguation as an example of possible uses of aligned multilingual vectors. The goal of this task is to find a suitable representation of each sense of a given polysemous word. The idea of our method is to look for a language in which the undesired senses are represented by unambiguous words and then to perform some arithmetic operation.\nLet\u2019s illustrate the process with a concrete example: consider the French word \u201ctrain\u201d, trainfr. The three closest Polish words to ~trainfr translate in English into \u201cnow\u201d, \u201ca train\u201d and \u201cwhen\u201d. This seems a poor matching. In fact, trainfr is polysemous. It can name a line of railroad cars, but it is also used to form progressive tenses. The French\n\u201cIl est en train de manger\u201d translates into \u201che is eating\u201d, or in Italian \u201csta mangiando\u201d.\nAs the Italian word \u201csta\u201d is used to form progressive tenses, it\u2019s a good candidate to disambiguate trainfr. Let\u2019s introduce the vector ~v = ~trainfr \u2212 ~stait. Now the three polish words closest to ~v translate in English into \u201ca train\u201d, \u201ca train\u201d and \u201crailroad\u201d. Therefore ~v is a better representation for the railroad sense of trainfr."}, {"heading": "6.2 Transfer of linguistic features", "text": "Another interesting property of the vectors generated by Trans-gram is the transfer of linguistic features through a pivot language that does not possess these features.\nLet\u2019s illustrate this by focusing on Latin languages, which possess some features that English does not, like rich conjugations. For example, in French and Italian the infinitives of \u201ceat\u201d are mangerfr and mangiareit, and the first plural persons are mangeonsfr and mangiamoit. Actually in our models we observe the following alignments: ~mangerfr \u2248 ~mangiareit and ~mangeonsfr \u2248 ~mangiamoit. It is thus remarkable to see that features not present in English match in languages aligned through English as the only pivot language. We also found similar transfers for the genders of adjectives and are currently studying other similar properties captured by Trans-gram."}, {"heading": "7 Conclusion", "text": "In this paper we provided the following contributions: Trans-gram, a new method to compute cross-lingual word-embeddings in a single word space; state of the art results on cross-lingual NLP tasks; a sketch of a cross-lingual calculus to help disambiguate polysemous words; the exhibition of linguistic features transfers through a pivotlanguage not possessing those features.\nWe are still exploring promising properties of the generated vectors and their applications in other NLP tasks (Sentiment Analysis, NER...)."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "arXiv preprint arXiv:1307.1662", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "Association for Computational Linguistics", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Multilingual distributed representations without word alignment", "author": ["Hermann", "Blunsom2013] Karl Moritz Hermann", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1312.6173", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": null, "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT Summit", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Lauly et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lauly et al\\.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis", "Li2004] Y", "Rose T", "D. D", "Yang", "F. Li"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 6, "context": "Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 2, "context": "Several authors came up with different methods to align word-embeddings across two languages (Klementiev et al., 2012; Mikolov et al., 2013b; Lauly et al., 2014; Gouws et al., 2015).", "startOffset": 93, "endOffset": 181}, {"referenceID": 6, "context": "This was explored in (Hermann and Blunsom, 2013; Lauly et al., 2014) where every couple of aligned sentences is transformed into two fixed-size vectors.", "startOffset": 21, "endOffset": 68}, {"referenceID": 4, "context": "(Klementiev et al., 2012) proved this approach to be useful by obtaining state-of-the-art results on several tasks.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "(Gouws et al., 2015) extends their work with a more computationally-efficient implementation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Like in BilBOWA (Gouws et al., 2015), we use Skipgram as a mono-lingual loss.", "startOffset": 16, "endOffset": 36}, {"referenceID": 5, "context": "In our experiments, we used the Europarl (Koehn, 2005) aligned corpora.", "startOffset": 41, "endOffset": 54}, {"referenceID": 0, "context": "We also computed 300 dimensions vectors using the Wikipedia extracts provided by (Al-Rfou et al., 2013) as monolingual data for each language.", "startOffset": 81, "endOffset": 103}, {"referenceID": 4, "context": "We used a subset of the English and German sections of the Reuters RCV1/RCV2 corpora (Lewis and Li, 2004) (10000 documents each), as in (Klementiev et al., 2012), and we replicated the experimental setting.", "startOffset": 136, "endOffset": 161}, {"referenceID": 4, "context": "The classifier used was an averaged perceptron, and we used the implementation from (Klementiev et al., 2012)1.", "startOffset": 84, "endOffset": 109}, {"referenceID": 2, "context": "The previous state of the art results were detained (Gouws et al., 2015) with BilBOWA and (Lauly et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 6, "context": ", 2015) with BilBOWA and (Lauly et al., 2014) with their Bilingual Autoencoder model.", "startOffset": 25, "endOffset": 45}, {"referenceID": 2, "context": ", 2013b) and used in (Gouws et al., 2015).", "startOffset": 21, "endOffset": 41}], "year": 2016, "abstractText": "We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.", "creator": "LaTeX with hyperref package"}}}