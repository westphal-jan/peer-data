{"id": "1601.02539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2016", "title": "Investigating gated recurrent neural networks for speech synthesis", "abstract": "Recently, complications neural networks (RNNs) has powerful dimension suvs have re - after as for danger instrumentation systems for approximate semantics speech inorganic (SPSS ). The long ends - position memory (LSTM) philosophy is clearly attractive because longer addresses once creeps steady-state difficulties leaving exception RNNs, any taking take have train. Although recent physics have indication similar LSTMs unlike achieve lowering better performance new SPSS yet eyes cow - picked auditory connections, little is \" reason why. Here we finally to did half questions: full) i thought LSTMs time well he a encoding specifications before SPSS; q) full component (e. g. , compression street, output floor, ... gate) instance seem same. We latter well quality accuracy whose a success whose experiments, resulting until a pact those first functional neo-gothic. The altered modernism by decreases fewer parameter even an LSTM, to reduced true complexity considerably simply borderline critical.", "histories": [["v1", "Mon, 11 Jan 2016 17:54:53 GMT  (26kb,D)", "http://arxiv.org/abs/1601.02539v1", "Accepted by ICASSP 2016"]], "COMMENTS": "Accepted by ICASSP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["zhizheng wu", "simon king"], "accepted": false, "id": "1601.02539"}, "pdf": {"name": "1601.02539.pdf", "metadata": {"source": "CRF", "title": "INVESTIGATING GATED RECURRENT NETWORKS FOR SPEECH SYNTHESIS", "authors": ["Zhizheng Wu", "Simon King"], "emails": ["zhizheng.wu@ed.ac.uk"], "sections": [{"heading": null, "text": "Index Terms\u2014 Speech synthesis, acoustic modelling, recurrent network network, gated recurrent network, long shortterm memory"}, {"heading": "1. INTRODUCTION", "text": "Statistical parametric speech synthesis (SPSS) has quite steadily advanced in naturalness in the past decade, as witnessed by the series of Blizzard Challenges [1]. However, the quality of synthetic speech produced by SPSS is still far below that of the natural human speech, and cannot compete with the best unit selection systems, which concatenate waveforms [2]. As suggested in [3], acoustic modelling, which captures the complex relationship between linguistic and acoustic representations, is a key limiting factor and is the focus of this work."}, {"heading": "1.1. Relation to prior work", "text": "Neural networks have re-emerged as a potential powerful acoustic model for SPSS. In [4, 5, 6, 7, 8], feed-forward neural networks are employed to map a linguistic representation derived from input text directly to acoustic features. In [9], a deep belief network (DBN) was used to model the relationship between linguistic and acoustic representations jointly.\nIn [10] and [11], mixture density networks (MDNs) and realvalued neural autoregressive density estimators (RNADEs) were proposed, respectively, to predict acoustic feature distributions given input linguistic features. These various implementations can be viewed as a replacement of the decision tree in HMM-based speech synthesis; they map linguistic features to acoustic features frame by frame through multiple hidden layers. However, the temporal sequence nature of speech is not explicitly modelled in the network architectures.\nTo include temporal constraints, we proposed to include contextual information by stacking low-dimensional bottleneck features from multiple consecutive frames [12]. Still in the DNN framework, minimum trajectory error training [13] or sequence error training criterion [14] have been proposed to minimise the utterance-level trajectory error rather than the frame-by-frame error. On the other hand, recurrent neural networks (RNNs) directly and elegantly include temporal information in the network architecture, making them attractive for modelling speech parameter trajectories. In [15], a standard RNN was employed to predict prosodic information for speech synthesis. In [16], two variants on standard RNNs, the Elman RNN and clockwork RNN, were investigated for speech synthesis.\nThe most widely used recurrent network in speech processing applications is the long short-term memory (LSTM) architecture. Because the LSTM addresses the vanishing gradient problem of the standard RNN, it is easier to train. In [17], an LSTM was employed to model the F0 contour. In [18], a bidirectional LSTM was employed to map a sequence of linguistic features to the corresponding sequence of acoustic features. In [19], an LSTM with a recurrent output layer was proposed to perform sequence mapping from linguistic to acoustic representations. These studies all formulate SPSS as sequence-to-sequence mapping and all demonstrate the effectiveness of LSTMs. However, LSTM architecture seems rather ad-hoc and it is not obvious what its various components are actually contributing to performance.\nThis raises at least two questions that have not been answered in previous studies: a) how exactly does the LSTM architecture model a speech parameter sequence; b) which components of the LSTM architecture are important, and which could be discarded. Answers to these questions may suggest better and perhaps simpler recurrent network architectures.\nar X\niv :1\n60 1.\n02 53\n9v 1\n[ cs\n.C L\n] 1\n1 Ja\nn 20\n16"}, {"heading": "1.2. The novelty of this work", "text": "We attempt to reach a better understanding of the \u201cblack-box\u201d LSTM architecture and our findings lead us to propose a simplified architecture for speech modelling.\nFirst, we give an analysis of the forget gate and memory cell in the LSTM architecture. Specifically, we visualise the activation of the forget gate to understand when the forget gate resets the memory cell state, and how the forget gate relates to speech structure. We analyse how the cell state correlates with the trajectory to be predicted. These visualisations enable us to understand how LSTMs model the temporal structure in speech synthesis. To the best of our knowledge, this is the first attempt to visually analyse the LSTM architecture in predicting a speech parameter sequence.\nSecond, we analyse the importance of each LSTM component for speech synthesis and propose a simplified architecture. The analysis is done empirically with several variants of the LSTM. Each removes a different component of the vanilla LSTM. The analysis was inspired by the studies in [20, 21], and we focus on the speech synthesis application. Based on this analysis, we present a simplified architecture, which only has the forget gate. The simplified architecture has significantly fewer parameters than the vanilla LSTM, and so reduces the computational cost of generation considerably without degrading the quality of the synthesised speech."}, {"heading": "2. LONG SHORT-TERM MEMORY", "text": "Standard RNNs are hard to train due to the well-known vanishing or exploding gradient problems [22, 23]. To address the vanishing gradient problem, the LSTM architecture was proposed, the basic idea of which was presented in [24]. The most commonly used architecture was described in [25], and is formulated as,\nit = \u03b4(W ixt +R iht\u22121 + p i ct\u22121 + bi)\nft = \u03b4(W fxt +R fht\u22121 + p f ct\u22121 + bf)\nct = ft ct\u22121 + it g(Wcxt +Rcht\u22121 + bc) ot = \u03b4(W oxt +R oht\u22121 + p\no ct + bo) ht = ot g(ct)\nIn these formulations, it, ft, ct, ot, and ht are the input gate, forget gate, cell state, output gate and block output at time instance t, respectively; \u03b4(\u00b7) and g(\u00b7) are the sigmoid and tangent activation functions, respectively; xt is the input at time t; W\u2217, and R\u2217 are the weight matrices applied on input and recurrent hidden units, respectively; p\u2217 and b\u2217 are the peep-hole connections and biases, respectively; and means element-wise product. We will call this the vanilla LSTM.\nThe central idea of the LSTM is the so-called memory cell c which maintains its state over time, and the gating units which are used to regulate the information flow into and out of the memory cell [20]. More specifically, the input gate can allow the input signal to adjust the cell state or prevent that\n(e.g., setting the input gate to zero); the output gate can allow the cell state to affect other neurons or block that; and the forget gate enables the cell to remember or forget its previous state. However, as discussed in [20, 21], the architecture might not be optimal for all the tasks, and the relative importance of each component is not at all clear."}, {"heading": "3. GATED RECURRENT NEURAL NETWORKS", "text": "In this section, we present several variants of the LSTM and propose a simplified version that only has the forget gate; it therefore has significantly fewer parameters and lower computational cost. As these variants all share with the LSTM the concept of a memory cell with gates, we will call them gated recurrent neural networks."}, {"heading": "3.1. Four variants on the LSTM", "text": "To assess the importance of each component, we start with four variants of the LSTM architecture. Each removes one component from the LSTM architecture, so we can understand how much each component contributes to performance. The differences with the vanilla LSTM are:\n\u2022 No Peep-holes (NPH): Set pi, pf , po to zero \u2022 No input gate (NIG): it = 1 \u2022 No forget gate (NFG): ft = 1 \u2022 No output gate (NOG): ot = 1\nIn the NFG variant, the past cell state will still contribute to the current cell state but without any controlling or scaling by the forget gate. Note that, when removing the input, forget or output gates, the number of parameters is reduced."}, {"heading": "3.2. Gated Recurrent Unit (GRU)", "text": "As an alternative to the LSTM, the Gated Recurrent Unit (GRU) architecture was proposed in [26]. In [27], the GRU was found to achieve better performance than the LSTM on some tasks. The GRU is formulated as:\nrt = \u03b4(W rxt +R rht\u22121 + b r) zt = \u03b4(W zxt +R zht\u22121 + b z)\nh\u0303t = g(W hxt + rt (Rhht\u22121) + bh)\nht = zt ht\u22121 + (1\u2212 zt) h\u0303t\nFrom these formulae, we can observed that the GRU architecture is similar to LSTM but without a separate memory cell. The GRU does not use peep-hole connections and output activation functions, and combines the input and forget gates into an update gate zt to balance between previous activation ht\u22121 and the candidate activation h\u0303t. The reset gate rt allows it to forget the previous state."}, {"heading": "3.3. Simplified LSTM (S-LSTM)", "text": "As we will see in the experiments reported in the next section, the input gate, output gate and peep-hole connections\ncan be removed without degrading speech synthesis performance significantly. Hence, we can propose an even simpler variant, that removes output gates and peep-hole connections, and replaces the input gate by the forget gate in the form of 1 \u2212 ft. In this way, only the forget gate is retained. This simplest variant can be written as:\nft = \u03b4(W fxt +R fht\u22121 + b f)\nct = ft ct\u22121 + (1\u2212 ft) g(Wcxt +Rcht\u22121 + bc) ht = g(ct)\nThe simplified architecture is similar to the GRU, except that it uses a memory cell state. The cell state is controlled by the forget gate only, which trades off between past cell state and current block input. When the activation of forget gate is small, the cell state will mainly depend on the block input, otherwise it will mainly copy the past cell state."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1. Experimental setup", "text": "A corpus from a British male speaker was employed in our experiments, divided into three subsets: training, development and testing (2400, 70 and 72 utterances). The sampling rate was 48 kHz, and we used the STRAIGHT vocoder [28] to extract 60-dimensional Mel-Cepstral Coefficients (MCCs), 25 band aperiodicities (BAPs), and fundamental frequency (F0) on log-scale, all at 5-ms frame step. All systems used the same acoustic features. F0 was linearly interpolated before modelling and a binary voiced/unvoiced feature was used to record voicing information. Dynamic features for MCCs, BAPs and F0 were also computed. The acoustic features were mean-variance normalised before modelling, and the mean and variance was restored at the generation time. At generation time, maximum likelihood parameter generation algorithm [29] was applied to smooth parameter trajectories.\nAll systems used the same input linguistic features comprising 601 features. 592 of these are binary features derived from linguistic context, such as quin-phone identities, partof-speech, positional information of phoneme, syllable, word and phrase, and the number of syllables, words and phrases,\netc. The remaining 9 numerical features capture frame position information, e.g., frame position in HMM state and phoneme. Linguistic features were normalised to [0.01 0.99] before modelling.\nIn all RNNs, we employed a three-layer feed-forward neural network at the bottom. On top of the feed-forward layers, we used the gated recurrent neural networks. The bottom feed-forward layers were intended to act as feature extraction layers, with 512 hidden units using tangent activation function in each layer. All RNN implementations used 256 units (e.g., LSTM blocks) in the recurrent layer. Hyperparameters for each system were optimised on the development set. We fixed the momentum, and only tuned learning rates."}, {"heading": "4.2. Analysis of LSTM", "text": "We first visualised the forget gate and cell state, which are thought to be the two most important components in modelling long-term temporal structure. The averaged activations (over the 256 units) of the forget gate as a function of the frame index is presented in Fig. 1. The red solid line is the forget gates averaged activations; blue dashed lines show phoneme boundaries. It is clear that the peaks of the forget gate activation trajectory have a strong correspondence with the phoneme boundaries; within a phoneme, the contribution of past cell state decays linearly. The forget gate is capturing some important temporal structure of speech; this is not surprising, since the phoneme boundaries are explicitly represented in the input linguistic features.\nThe memory cell should maintains its state over time [20] and so could store the trend of the trajectory to be predicted. To analyse the relationship between the cell states and the MCC trajectories, we computed the correlation between the cell states and the first MCC trajectory, and found that the 120th cell state has the highest correlation with the first MCC trajectory. The correlation is as high as 0.9. A comparison between them is presented in Fig. 2, which shows that the cell state tracks the shape of the MCC trajectory."}, {"heading": "4.3. Objective results", "text": "Even though objective measures might not always correlate with human perception, they offer a way to tune the systems\nand roughly predict model performance. The objective results are in Table 2. Compared to LSTM, NIG, NOG and NPH all achieve similar objective distortion, with considerably fewer parameters and lower generation time: the input gate, output gate and peep-hole connections are not necessary. The NFG system increases distortion considerably: the forget gate is important. This finding is consistent with [20].\nThe GRU system achieves similar performance to the LSTM system: even though it has even fewer parameters, it performs as well as NIG, NOG or NPH. This is also consistent with studies on other tasks [27, 21]. Although S-LSTM slightly increases MCD distortion from 4.14 dB to 4.19 dB compared to LSTM, it achieves similar performance on the other measures. The S-LSTM has about half the number of parameters in its recurrent layer compared to the LSTM, and reduces generation time from 214 seconds to 154 seconds. The generation time is the total time to generate all the 142 utterances in both development and testing sets.\nIn summary, the S-LSTM has the smallest number of parameters and achieves the fastest generation, whilst achieving similar objective results to the LSTM and GRU architectures."}, {"heading": "4.4. Subjective results", "text": "Subjective preference tests were conducted using 30 paid native English speakers. Each listener was asked to listen 20 pairs of synthesised utterances. The sentence was the same\nin both items within a pair, and was randomly selected from the 72 test sentences1. For each pair, the listener was asked to decide which one sounded more natural; a \u201cneutral\u201d option was allowed if the listener had no preference.\nPreference results are in Table 1. Comparing against the LSTM system, all the systems except NFG show no significant difference in preference.\nThe NFG system achieves only a 20.3% preference score when paired against the LSTM which is preferred 74.3% of the time. As with the objective results in Table 2, we conclude that the forget gate is the only critical component in the LSTM architecture; the input gate, output gate and peep-hole connections can be omitted.\nWe also compares the proposed S-LSTM system against with all other systems (except NFG, since it is worse than LSTM). Consistent with the objective results, the subjective results also demonstrate that S-LSTM is as good as any other systems."}, {"heading": "5. CONCLUSIONS", "text": "We have analysed the forget gate and cell state of the LSTM architecture, and examined the performance of several variants of LSTM. We conclude that:\n\u2022 The forget gate can learn the temporal structure of speech; its activations have a high correspondence with phone boundaries. \u2022 The memory cell maintains a state over time, which matched the shape of the trajectory to be predicted. \u2022 For this task, the forget gate is the only critical component of the LSTM; other components can be omitted with no reduction in naturalness.\nFrom these results, we propose a simplified LSTM architecture that only uses the critical forget gate. The simplified LSTM has significantly fewer parameters than the vanilla LSTM, but achieves similar performance in both objective and subjective evaluations.\nAcknowledgements: This research was supported by EPSRC Programme Grant EP/I031022/1, Natural Speech Technology (NST). The NST research data collection may be accessed at http://datashare.is.ed.ac.uk/handle/10283/786.\n1Samples are available at: http://homepages.inf.ed.ac.uk/ zwu2/demo/icassp16/lstm.html"}, {"heading": "6. REFERENCES", "text": "[1] Simon King, \u201cMeasuring a decade of progress in text-tospeech,\u201d Loquens, vol. 1, no. 1, 2014.\n[2] Andrew J Hunt and Alan W Black, \u201cUnit selection in a concatenative speech synthesis system using a large speech database,\u201d in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 1996.\n[3] Heiga Zen, Keiichi Tokuda, and Alan W Black, \u201cStatistical parametric speech synthesis,\u201d Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.\n[4] Heiga Zen, Andrew Senior, and Mike Schuster, \u201cStatistical parametric speech synthesis using deep neural networks,\u201d in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013.\n[5] Heng Lu, Simon King, and Oliver Watts, \u201cCombining a vector space representation of linguistic context with a deep neural network for text-to-speech synthesis,\u201d Proc. the 8th ISCA Speech Synthesis Workshop (SSW), 2013.\n[6] Yao Qian, Yuchen Fan, Wenping Hu, and Frank K Soong, \u201cOn the training aspects of deep neural network (DNN) for parametric TTS synthesis,\u201d in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014.\n[7] Qiong Hu, Zhizheng Wu, Korin Richmond, Junichi Yamagishi, Yannis Stylianou, and Ranniery Maia, \u201cFusion of multiple parameterisations for DNN-based sinusoidal speech synthesis with multi-task learning,\u201d in Proc. Interspeech, 2015.\n[8] Cassia Valentini-Botinhao, Zhizheng Wu, and Simon King, \u201cTowards minimum perceptual error training for DNN-based speech synthesis,\u201d in Proc. Interspeech, 2015.\n[9] Shiyin Kang, Xiaojun Qian, and Helen Meng, \u201cMultidistribution deep belief network for speech synthesis,\u201d in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013.\n[10] Heiga Zen and Andrew Senior, \u201cDeep mixture density networks for acoustic modeling in statistical parametric speech synthesis,\u201d in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014.\n[11] Benigno Uria, Iain Murray, Steve Renals, and Cassia Valentini, \u201cModelling acoustic feature dependencies with artificial neural networks: Trajectory-rnade,\u201d in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015.\n[12] Zhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts, and Simon King, \u201cDeep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis,\u201d in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015.\n[13] Zhizheng Wu and Simon King, \u201cMinimum trajectory error training for deep neural networks, combined with stacked bottleneck features,\u201d in Proc. Interspeech, 2015.\n[14] Yuchen Fan, Yao Qian, Frank K. Soong, and Lei He, \u201cSequence generation error (SGE) minimization based deep neural networks training for text-to-speech synthesis,\u201d in Proc. Interspeech, 2015.\n[15] Sin-Horng Chen, Shaw-Hwa Hwang, and Yih-Ru Wang, \u201cAn RNN-based prosodic information synthesizer for Mandarin text-to-speech,\u201d IEEE Transactions on Speech and Audio Processing, vol. 6, no. 3, pp. 226\u2013239, 1998.\n[16] Sivanand Achanta, Tejas Godambe, and Suryakanth V. Gangashetty, \u201cAn investigation of recurrent neural network architectures for statistical parametric speech synthesis,\u201d in Proc. Interspeech, 2015.\n[17] Raul Fernandez, Asaf Rendel, Bhuvana Ramabhadran, and Ron Hoory, \u201cProsody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks,\u201d in Proc. Interspeech, 2014.\n[18] Yuchen Fan, Yao Qian, Fenglong Xie, and Frank K. Soong, \u201cTTS synthesis with bidirectional LSTM based recurrent neural networks,\u201d in Proc. Interspeech, 2014.\n[19] Heiga Zen and Hasim Sak, \u201cUnidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis,\u201d in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015.\n[20] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u0131\u0301k, Bas R Steunebrink, and Ju\u0308rgen Schmidhuber, \u201cLstm: A search space odyssey,\u201d arXiv preprint arXiv:1503.04069, 2015.\n[21] Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever, \u201cAn empirical exploration of recurrent network architectures,\u201d in Proc. IEEE Int. Conf. on Machine Learning (ICML), 2015.\n[22] Yoshua Bengio, Patrice Simard, and Paolo Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013 166, 1994.\n[23] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, \u201cOn the difficulty of training recurrent neural networks,\u201d arXiv preprint arXiv:1211.5063, 2012.\n[24] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[25] Alex Graves and Ju\u0308rgen Schmidhuber, \u201cFramewise phoneme classification with bidirectional lstm and other neural network architectures,\u201d Neural Networks, vol. 18, no. 5, pp. 602\u2013610, 2005.\n[26] Kyunghyun Cho, Bart Van Merrie\u0308nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014.\n[27] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014.\n[28] Hideki Kawahara, Ikuyo Masuda-Katsuse, and Alain de Cheveigne\u0301, \u201cRestructuring speech representations using a pitch-adaptive time\u2013frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds,\u201d Speech communication, vol. 27, no. 3, pp. 187\u2013207, 1999.\n[29] Keiichi Tokuda, Takayoshi Yoshimura, Takashi Masuko, Takao Kobayashi, and Tadashi Kitamura, \u201cSpeech parameter generation algorithms for HMM-based speech synthesis,\u201d in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2000."}], "references": [{"title": "Measuring a decade of progress in text-tospeech", "author": ["Simon King"], "venue": "Loquens, vol. 1, no. 1, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Unit selection in a concatenative speech synthesis system using a large speech database", "author": ["Andrew J Hunt", "Alan W Black"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical parametric speech synthesis", "author": ["Heiga Zen", "Keiichi Tokuda", "Alan W Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["Heiga Zen", "Andrew Senior", "Mike Schuster"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining a vector space representation of linguistic context with a deep neural network for text-to-speech synthesis", "author": ["Heng Lu", "Simon King", "Oliver Watts"], "venue": "Proc. the 8th ISCA Speech Synthesis Workshop (SSW), 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "On the training aspects of deep neural network (DNN) for parametric TTS synthesis", "author": ["Yao Qian", "Yuchen Fan", "Wenping Hu", "Frank K Soong"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Fusion of multiple parameterisations for DNN-based sinusoidal speech synthesis with multi-task learning", "author": ["Qiong Hu", "Zhizheng Wu", "Korin Richmond", "Junichi Yamagishi", "Yannis Stylianou", "Ranniery Maia"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards minimum perceptual error training for DNN-based speech synthesis", "author": ["Cassia Valentini-Botinhao", "Zhizheng Wu", "Simon King"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Multidistribution deep belief network for speech synthesis", "author": ["Shiyin Kang", "Xiaojun Qian", "Helen Meng"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis", "author": ["Heiga Zen", "Andrew Senior"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Modelling acoustic feature dependencies with artificial neural networks: Trajectory-rnade", "author": ["Benigno Uria", "Iain Murray", "Steve Renals", "Cassia Valentini"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Zhizheng Wu", "Cassia Valentini-Botinhao", "Oliver Watts", "Simon King"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Minimum trajectory error training for deep neural networks, combined with stacked bottleneck features", "author": ["Zhizheng Wu", "Simon King"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence generation error (SGE) minimization based deep neural networks training for text-to-speech synthesis", "author": ["Yuchen Fan", "Yao Qian", "Frank K. Soong", "Lei He"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "An RNN-based prosodic information synthesizer for Mandarin text-to-speech", "author": ["Sin-Horng Chen", "Shaw-Hwa Hwang", "Yih-Ru Wang"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 6, no. 3, pp. 226\u2013239, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "An investigation of recurrent neural network architectures for statistical parametric speech synthesis", "author": ["Sivanand Achanta", "Tejas Godambe", "Suryakanth V. Gangashetty"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Prosody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks", "author": ["Raul Fernandez", "Asaf Rendel", "Bhuvana Ramabhadran", "Ron Hoory"], "venue": "Proc. Interspeech, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Yuchen Fan", "Yao Qian", "Fenglong Xie", "Frank K. Soong"], "venue": "Proc. Interspeech, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis", "author": ["Heiga Zen", "Hasim Sak"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "Proc. IEEE Int. Conf. on Machine Learning (ICML), 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013 166, 1994.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, vol. 18, no. 5, pp. 602\u2013610, 2005.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Restructuring speech representations using a pitch-adaptive time\u2013frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds", "author": ["Hideki Kawahara", "Ikuyo Masuda-Katsuse", "Alain de Cheveign\u00e9"], "venue": "Speech communication, vol. 27, no. 3, pp. 187\u2013207, 1999.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Speech parameter generation algorithms for HMM-based speech synthesis", "author": ["Keiichi Tokuda", "Takayoshi Yoshimura", "Takashi Masuko", "Takao Kobayashi", "Tadashi Kitamura"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2000.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Statistical parametric speech synthesis (SPSS) has quite steadily advanced in naturalness in the past decade, as witnessed by the series of Blizzard Challenges [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "However, the quality of synthetic speech produced by SPSS is still far below that of the natural human speech, and cannot compete with the best unit selection systems, which concatenate waveforms [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "As suggested in [3], acoustic modelling, which captures the complex relationship between linguistic and acoustic representations, is a key limiting factor and is the focus of this work.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "In [4, 5, 6, 7, 8], feed-forward neural networks are employed to map a linguistic representation derived from input text directly to acoustic features.", "startOffset": 3, "endOffset": 18}, {"referenceID": 4, "context": "In [4, 5, 6, 7, 8], feed-forward neural networks are employed to map a linguistic representation derived from input text directly to acoustic features.", "startOffset": 3, "endOffset": 18}, {"referenceID": 5, "context": "In [4, 5, 6, 7, 8], feed-forward neural networks are employed to map a linguistic representation derived from input text directly to acoustic features.", "startOffset": 3, "endOffset": 18}, {"referenceID": 6, "context": "In [4, 5, 6, 7, 8], feed-forward neural networks are employed to map a linguistic representation derived from input text directly to acoustic features.", "startOffset": 3, "endOffset": 18}, {"referenceID": 7, "context": "In [4, 5, 6, 7, 8], feed-forward neural networks are employed to map a linguistic representation derived from input text directly to acoustic features.", "startOffset": 3, "endOffset": 18}, {"referenceID": 8, "context": "In [9], a deep belief network (DBN) was used to model the relationship between linguistic and acoustic representations jointly.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "In [10] and [11], mixture density networks (MDNs) and realvalued neural autoregressive density estimators (RNADEs) were proposed, respectively, to predict acoustic feature distributions given input linguistic features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [10] and [11], mixture density networks (MDNs) and realvalued neural autoregressive density estimators (RNADEs) were proposed, respectively, to predict acoustic feature distributions given input linguistic features.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "To include temporal constraints, we proposed to include contextual information by stacking low-dimensional bottleneck features from multiple consecutive frames [12].", "startOffset": 160, "endOffset": 164}, {"referenceID": 12, "context": "Still in the DNN framework, minimum trajectory error training [13] or sequence error training criterion [14] have been proposed to minimise the utterance-level trajectory error rather than the frame-by-frame error.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Still in the DNN framework, minimum trajectory error training [13] or sequence error training criterion [14] have been proposed to minimise the utterance-level trajectory error rather than the frame-by-frame error.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "In [15], a standard RNN was employed to predict prosodic information for speech synthesis.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [16], two variants on standard RNNs, the Elman RNN and clockwork RNN, were investigated for speech synthesis.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17], an LSTM was employed to model the F0 contour.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [18], a bidirectional LSTM was employed to map a sequence of linguistic features to the corresponding sequence of acoustic features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [19], an LSTM with a recurrent output layer was proposed to perform sequence mapping from linguistic to acoustic representations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The analysis was inspired by the studies in [20, 21], and we focus on the speech synthesis application.", "startOffset": 44, "endOffset": 52}, {"referenceID": 20, "context": "The analysis was inspired by the studies in [20, 21], and we focus on the speech synthesis application.", "startOffset": 44, "endOffset": 52}, {"referenceID": 21, "context": "Standard RNNs are hard to train due to the well-known vanishing or exploding gradient problems [22, 23].", "startOffset": 95, "endOffset": 103}, {"referenceID": 22, "context": "Standard RNNs are hard to train due to the well-known vanishing or exploding gradient problems [22, 23].", "startOffset": 95, "endOffset": 103}, {"referenceID": 23, "context": "To address the vanishing gradient problem, the LSTM architecture was proposed, the basic idea of which was presented in [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 24, "context": "The most commonly used architecture was described in [25], and is formulated as,", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "The central idea of the LSTM is the so-called memory cell c which maintains its state over time, and the gating units which are used to regulate the information flow into and out of the memory cell [20].", "startOffset": 198, "endOffset": 202}, {"referenceID": 19, "context": "However, as discussed in [20, 21], the architecture might not be optimal for all the tasks, and the relative importance of each component is not at all clear.", "startOffset": 25, "endOffset": 33}, {"referenceID": 20, "context": "However, as discussed in [20, 21], the architecture might not be optimal for all the tasks, and the relative importance of each component is not at all clear.", "startOffset": 25, "endOffset": 33}, {"referenceID": 25, "context": "As an alternative to the LSTM, the Gated Recurrent Unit (GRU) architecture was proposed in [26].", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "In [27], the GRU was found to achieve better performance than the LSTM on some tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "The sampling rate was 48 kHz, and we used the STRAIGHT vocoder [28] to extract 60-dimensional Mel-Cepstral Coefficients (MCCs), 25 band aperiodicities (BAPs), and fundamental frequency (F0) on log-scale, all at 5-ms frame step.", "startOffset": 63, "endOffset": 67}, {"referenceID": 28, "context": "At generation time, maximum likelihood parameter generation algorithm [29] was applied to smooth parameter trajectories.", "startOffset": 70, "endOffset": 74}, {"referenceID": 19, "context": "The memory cell should maintains its state over time [20] and so could store the trend of the trajectory to be predicted.", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "This finding is consistent with [20].", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "This is also consistent with studies on other tasks [27, 21].", "startOffset": 52, "endOffset": 60}, {"referenceID": 20, "context": "This is also consistent with studies on other tasks [27, 21].", "startOffset": 52, "endOffset": 60}], "year": 2016, "abstractText": "Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feedforward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.", "creator": "LaTeX with hyperref package"}}}