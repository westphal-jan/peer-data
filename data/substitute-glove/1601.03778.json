{"id": "1601.03778", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2016", "title": "Trust from the past: Bayesian Personalized Ranking based Link Prediction in Knowledge Graphs", "abstract": "Estimating the expectation there with sites thought turned critical prepare those Knowledge Graph construction. Link comparison, they predicting the rate of his allows 1999 next describe matrix limited on prior state form place hold research upon within this ground. We seek brought Latent Feature Embedding based possible recommendation model which disappointing possible and utilize Bayesian Personalized Ranking its optimization abstraction for teaching feature that each predicate. Experimental testing off the - generate knowledge bases such not YAGO2 tv put our policies creativity substantially compared shown than four decision - many - crafts alternative. Furthermore, exactly been study the combined among the connection likelihood algorithm part necessarily mainly topological properties of the Knowledge Graph these present close corresponds geometries model to reason about its cuts highest seen lacked.", "histories": [["v1", "Thu, 14 Jan 2016 23:13:00 GMT  (63kb)", "http://arxiv.org/abs/1601.03778v1", null], ["v2", "Mon, 15 Feb 2016 05:12:32 GMT  (64kb)", "http://arxiv.org/abs/1601.03778v2", "SDM Workshop on Mining Networks and Graphs (MNG 2016), Miami, FL"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.IR", "authors": ["baichuan zhang", "sutanay choudhury", "mohammad al hasan", "xia ning", "khushbu agarwal", "sumit purohit", "paola pesntez cabrera"], "accepted": false, "id": "1601.03778"}, "pdf": {"name": "1601.03778.pdf", "metadata": {"source": "CRF", "title": "Trust from the past: Bayesian Personalized Ranking based Link Prediction in Knowledge Graphs", "authors": ["Baichuan Zhang", "Sutanay Choudhury", "Mohammad Al Hasan", "Xia Ning", "Khushbu Agarwal", "Sumit Purohit", "Paola Gabriela Pesntez Cabrera"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n03 77\n8v 1\n[ cs\n.L G\n] 1\n4 Ja\nn 20\nEstimating the confidence for a link is a critical task for Knowledge Graph construction. Link prediction, or predict-\ning the likelihood of a link in a knowledge graph based on prior state is a key research direction within this area. We\npropose a Latent Feature Embedding based link recommen-\ndation model for prediction task and utilize Bayesian Personalized Ranking based optimization technique for learning\nmodels for each predicate. Experimental results on large-\nscale knowledge bases such as YAGO2 show that our approach achieves substantially higher performance than sev-\neral state-of-art approaches. Furthermore, we also study the performance of the link prediction algorithm in terms of\ntopological properties of the Knowledge Graph and present\na linear regression model to reason about its expected level of accuracy."}, {"heading": "1 Introduction", "text": "The past few years have seen a surge in research on knowledge representations and algorithms for building knowledge graphs. A knowledge graph is a repository of information about entities, where entities can be any thing of interest such as people, location, organization or even scientific topics, concepts etc. An entity is frequently characterized by its association with other entities. As an example, capturing the knowledge about a company involves describing its products, listing key individuals, its locations etc. Similarly, knowledge about a person involves her name, date and place of birth, affiliation with organizations etc. Resource Description Framework (RDF) is a frequent choice for capturing the interactions between two entities. A RDF dataset is equivalent to a heterogeneous graph, where each vertex and edge can belong to different classes. The class information captures taxonomic hierarchies between the type of various entities and relations. As an\n\u2217Department of Computer and Information Science, Indiana University - Purdue University Indianapolis, USA\n\u2020Pacific Northwest National Laboratory, Richland, WA, USA \u2021Department of Computer Science, Washington State Univer-\nsity, Pullman, WA, USA\nexample, a knowledge graph may identify Kobe Bryant as a basketball player, while its ontology will indicate that a basketball player is a particular type of athlete. Thus, one will be able to query for most famous athletes in United States and find Kobe Bryant.\nWe are interested in streaming data sources in natural language text and continuous updating a knowledge graph based on newer facts that become available over time. Towards that, we extract triples from the text data sources using state of the art techniques such as OpenIE [9] and semantic role labeling [4]. After passing these triples through a filtering phase we need to decide if every triple should be added to the knowledge graph. We have a number of choices. A triple (or a fact) may be already present in the knowledge graph. Under such circumstances we may choose to update any related metadata. For other facts we need to decide if it should be added to the knowledge graph. This is an extremely challenging problem.\nConsider the following example. Given a social media post \u201cI wish Tom Cruise was the president of United States\u201d, a natural language processing engine will extract a triple (\u201cTom Cruise\u201d, \u201cpresident of\u201d, \u201cUnited States\u201d). On the other hand, a web crawler may find the fact that \u201cTom Cruise is president of Downtown Medical\u201d, resulting in the triple (\u201cTom Cruise\u201d, \u201cpresident of\u201d, \u201cDowntown Medical\u201d). Assuming that we do not have any information about the trustworthiness of the sources, can our prior knowledge of the entities mentioned in this triples allow us to decide on whether to admit or reject a triple? Also, once we decide to add a triple to the knowledge graph, it is desirable to have a confidence value associated with it. There can be multiple sources of confidence such as 1) trustworthiness of the data sources, 2) prior knowledge of subjects and objects, 3) a belief value reported by a natural language processing engine expressing its confidence in the correctness of parsing. This particular work is motivated by the second factor.\nWe adopt a link prediction 1 approach towards computing the confidence in a triple using the prior knowledge about its subject and object. Link prediction is a well established field [11] in the context of social network analysis. Lately, link prediction for heterogenous networks has received significant attention in the data mining community [5, 7, 27]. Bayesian personalized ranking (BPR) [23] based embedding model has been a major influence in this area and is the primary driver behind our approach. We build on the research in collaborative filtering for recommender systems that accept a user-item matrix; given a user-item pair, it returns a score indicating the likelihood of the user purchasing the item. Likewise, under this context, we build a BPR-based embedding model for every relation by casting the set of corresponding subjects and objects as a user-item matrix. Then our proposed method produces a real-valued score to measure the confidence of the given triple.\nIt is important that we quantitatively understand the accuracy of our prediction [28]. Given the same knowledge graph the prediction accuracy levels will vary from predicate to predicate. As an example, predicting one\u2019s school or workplace can be a much harder task than predicting one\u2019s liking for a local restaurant. Therefore, given two predicates \u201cworksAt\u201d and \u201clikes\u201d, we expect to see widely varying accuracy levels. Also, the average accuracy levels vary widely from one knowledge graph to another. The desire to obtain a quantitative grasp on prediction accuracy is complicated by a number of reasons: 1) Knowledge graphs constructed from web text or using machine reading approaches can have a very large number of predicates [6] that make manual verification difficult. 2) Creation of predicates, or the resultant graph structure is strongly shaped by the ontology, and the reification process used to generate RDF statements from a logical record in the data. Therefore, same data source can be represented in very different models and leads to different accuracy levels for the same predicate. 3) The effectiveness of knowledge graphs have inspired their construction from every imaginable data source: product inventories (at retailers such as Walmart), online social networks (such as Facebook), and web pages (Google\u2019s Knowledge Vault). As we move from one data source to another, it is very critical to understand what accuracy levels we can expect from a given predicate.\nOur contributions in this work are outlined below:\n1. We implement a Link Prediction approach for estimating confidence for triples in a Knowledge\n1We use link prediction and link recommendation interchangeably.\nGraph. Specifically, we borrow from successful approaches in the recommender systems domain, adopt the algorithms for knowledge graphs and perform a thorough evaluation on a prominent benchmark dataset.\n2. We propose a Latent Feature Embedding based link recommendation model for prediction task and utilize Bayesian Personalized Ranking based optimization technique for learning models for each predicate (section 4). Our experiments on the well known YAGO2 knowledge graph (in section 5) shows that the BPR approach outperforms other competing approaches for a significant set of predicates (Figure 1).\n3. We apply a linear regression model to quantitatively analyze the correlation between the prediction accuracy for each predicate and the topological structure of the induced subgraph of the original Knowledge Graph. Our studies show that metrics such as clustering coefficient or average degree can be used to reason about the expected level of prediction accuracy (Section 5.3, Figure 2)."}, {"heading": "2 Related Work", "text": "There is a large body of work on link prediction/relation extraction in knowledge graph. In terms of methodology, factorization based and related latent variable models [3, 8, 14, 22, 25], graphical model [15], and graph feature based method [18, 19] are considered.\nThere exists large number of works which focus on factorization based models. The common thread among the factorization methods is that they explain the triples via latent features of entities. [2] presents a tensor based model that decomposes each entity and predicate in knowledge graphs as a low dimensional vector. However, such a method fails to consider the symmetry property of the tensor. In order to solve this issue, [22] proposes a relational latent feature model, RESCAL, an efficient approach which uses a tensor factorization model that takes the inherent structure of relational data into account. By leveraging relational domain knowledge about entity type information, [3] proposes a tensor decomposition approach for relation extraction in knowledge base which is highly efficient in terms of time complexity. In addition, various other latent variable models, such as neural network based methods [6, 26], have been explored for link prediction task. However, the major drawback of neural network based models is their complexity and computational cost in model training and parameter tuning. Many of these models require tuning large number of parameters, thus finding the right combination of these parameters\nis often considered more of an art than science. Recently graphical models, such as Probabilistic Relational Models [10], Relational Markov Network [29], Markov Logic Network [15, 24] have also been used for link prediction in knowledge graph. For instance, [24] proposes a Markov Logic Network (MLN) based approach, which is a template language for defining potential functions on knowledge graph by logical formula. Despite its utility for modeling knowledge graph, issues such as rule learning difficulty, tractability problem, and parameter estimation poses implementation challenge for MLNs.\nGraph feature based approaches assume that the existence of an edge can be predicted by extracting features from the observed edges in the graph. Lao and Cohen [18,19] propose Path Ranking Algorithm (PRA) to perform random walk on the graph and compute the probability of each path. The main idea of PRA is to use these path probabilities as supervised features for each entity pair, and use any favorable classification model, such as logistic regression and SVM, to predict the probability of missing edge between entity pair in a knowledge graph.\nIt has been demonstrated [1] that no single based approach emerges as a clear winner. Instead, the merits of factorization models and graph feature models are often complementary with each other. Thus combining the advantages of different approaches for learning knowledge graph is a promising option. For instance, [21] proposed to use additive model, which is a linear combination between RESCAL and PRA. The combination results in not only speeding up the training time but increased accuracy. [16] combined a latent feature model with an additive term to learn from latent and neighborhood-based information on multirelational data. [6] fuses the output of PRA and neural network model as features for training a binary classifier. Our work strongly aligns with this combination approach. We begin with building on techniques that have proved successful for recommender systems and plan to incorporate graph based features in future work."}, {"heading": "3 Background and Problem Statement", "text": "Definition 1: We define the knowledge graph as a collection of triple facts G = (S, P,O), where s \u2208 S and o \u2208 O are the set of subject and object entities and p \u2208 P is the set of predicates or relations between them. G(s, p, o) = 1 if there is a direct link of type p from s to o, and G(s, p, o) = 0 otherwise.\nEach triple fact in knowledge graph is a statement interpreted as \u201cA relationship p holds between entities s and o\u201d. For instance, the statement \u201cKobe Bryant is a player of LA Lakers\u201d can be expressed by the following\ntriple fact (\u201cKobe Bryant\u201d, \u201cplaysFor\u201d, \u201cLA Lakers\u201d). Definition 2: For each relation p \u2208 P , we define Gp(Sp, Op) as a bipartite subgraph of G, where the corresponding set of entities sp \u2208 Sp, op \u2208 Op are connected by relation p, namely Gp(sp, op) = 1.\nProblem Statement: For every predicate p \u2208 P and given an entity pair (s, o) in Gp, our goal is to learn a link recommendation model Mp such that xs,o = Mp(s, o) is a real-valued score.\nDue to the fact that the produced real-valued score is not normalized, we compute the probability Pr(yps,o = 1), where yps,o is a binary random variable that is true iff Gp(s, o) = 1. We estimate this probability Pr using the logistic function as follows:\n(3.1) Pr(yps,o = 1) = 1\n1 + exp(\u2212xs,o)\nThus we interpret Pr(yps,o = 1) as the probability that a vertex (or subject) s in the knowledge graph G is in a relationship of given type p with another vertex (or the object) o."}, {"heading": "4 Methods", "text": "In this section, we describe our model, namely Latent Feature Embedding Model with Bayesian Personalized Ranking (BPR) based optimization technique that we propose for the task of link prediction in knowledge graph. In our link prediction setting, given predicate p, we first construct its bipartite subgraph Gp(Sp, Op). Then the optimal low dimensional embeddings for its corresponding subject and object entities sp \u2208 Sp, op \u2208 Op are learned by maximizing a ranking based distance function. The learning process relies on Stochastic Gradient Descent (SGD). The SGD based optimization technique iteratively updates the low dimensional representation of sp and op until convergence. Then the learned model is used for ranking the unobserved triple facts in descending order: triple facts with higher score values have an higher probability of representing true statement that leads to higher confidence to select for admission into knowledge graph."}, {"heading": "4.1 Latent Feature Based Embedding Model", "text": "For each predicate p, the model maps both its corresponding subject and object entites sp and op into lowdimensional continuous vector spaces, say Ups \u2208 IR 1\u00d7K and V po \u2208 IR 1\u00d7K respectively. We measure the compatibility between subject sp and object op as dot product of its corresponding latent vectors which is given as below:\n(4.2) xsp,op = (U p s )(V p o ) T + bpo\nwhere Up \u2208 IR|S|\u00d7K , V p \u2208 IR|O|\u00d7K , and bp \u2208\nIR|O|\u00d71. |S| and |O| denote the size of subject and object associated with predicate p respectively. K is the number of latent dimensions and bpo \u2208 IR is a bias term associated with object o. Given predicate p, the higher the score of xsp,op , the more similar the entities sp and op in the embedded low dimensional space, which provides higher confidence to include this triple fact into knowledge base."}, {"heading": "4.2 Bayesian Personalized Ranking", "text": "In collaborative filtering, positive-only data is known as implicit feedback/binary feedback. For example, in the eCommerce platform, some users only buy but does not rate items. Motivated by [23], we employ Bayesian Personalized Ranking (BPR) based approach for model learning. Specifically, in recommender system domain, given user-item matrix, BPR based approach assigns the preference of user for purchased item with higher score than un-purchased item. Likewise, under this context, we assign observed triple facts higher score than unobserved triple facts in knowledge base. We assume that unobserved facts are not necessarily negative, rather they are \u201cless preferable\u201d than the observed ones.\nFor our task, in each predicate p, we denote the observed subject/object entity pair as (sp, o + p ) and unobserved one as (sp, o \u2212 p ). The observed facts in our case are the existing link between sp and op given Gp and unobserved ones are the missing link between them. Given this fact, BPR maximizes the following ranking based distance function:\n(4.3) BPR = max\n\u0398p\n\u2211\n(sp,o + p ,o \u2212 p )\u2208Dp ln\u03c3(xsp,o+p \u2212 xsp,o\u2212p )\u2212 \u03bb\u0398p || \u0398p ||\n2\nwhere Dp is a set of samples generated from the training data for predicate p, Gp(sp, o + p ) = 1 and Gp(sp, o \u2212 p ) = 0. And xsp,o+p and xsp,o\u2212p are the predicted scores of subject sp on objects o + p and o \u2212 p respectively. We use the proposed latent feature based embedding model shown in Equation 4.2 to compute xsp,o+p and xsp,o\u2212p respectively. The last term in Equation 4.3 is a l2-norm regularization term used for model parameters \u0398p = {U\np, V p, bp} to avoid overfitting in the learning process. In addition, the logistic function \u03c3(.) in Equation 4.3 is defined as \u03c3(x) = 11+e\u2212x .\nNotice that the Equation 4.3 is differentiable, thus we employ the widely used SGD to maximize the objective. In particular, at each iteration, for given predicate p, we sample one observed entity pair (sp, o + p ) and one unobserved one (sp, o \u2212 p ) using uniform sampling technique. Then we iteratively update the model parameters \u0398p based on the sampled pairs. Specifically, for\neach training instance, we compute the derivative and update the corresponding parameters \u0398p by walking along the ascending gradient direction.\nFor each predicate p, given a training triple (sp, o + p , o \u2212 p ), the gradient of BPR objective in Equation 4.3 with respect to Ups , V p o+ , V p o\u2212 , bp o+ , bp o\u2212 can be computed as follows:\n\u2202BPR\n\u2202U p s\n= \u2202 ln\u03c3(xsp,o+p \u2212 xsp,o\u2212p )\n\u2202U p s\n\u2212 2\u03bbpsU p s\n= \u2202 ln\u03c3(x sp,o + p \u2212x sp,o \u2212 p )\n\u2202\u03c3(x sp,o + p \u2212x sp,o \u2212 p\n) \u00d7 \u2202\u03c3(x sp,o + p \u2212x sp,o \u2212 p )\n\u2202(x sp,o + p \u2212x sp,o \u2212 p )\n\u00d7 \u2202(x sp,o + p \u2212x sp,o \u2212 p )\n\u2202U p\ns\n\u2212 2\u03bbpsU p s\n= 1\n\u03c3(xsp,o+p \u2212 xsp,o\u2212p ) \u00d7 \u03c3(xsp,o+p \u2212 xsp,o\u2212p ) (\n1\u2212 \u03c3(xsp,o+p \u2212 xsp,o\u2212p ) ) \u00d7 (V p o+ \u2212 V p o\u2212 )\u2212 2\u03bbpsU p s\n= ( 1\u2212 \u03c3(xsp,o+p \u2212 xsp,o\u2212p ) ) (V p o+ \u2212 V p o\u2212 )\u2212 2\u03bbpsU p s\n(4.4)\nWe obtain the following using similar chain rule derivation.\n(4.5) \u2202BPR\n\u2202V p\no+\n= ( 1\u2212\u03c3(xsp,o+p \u2212xsp,o\u2212p ) ) \u00d7Ups \u22122\u03bb p o+ V p o+\n(4.6) \u2202BPR\n\u2202V p\no\u2212\n= ( 1\u2212 \u03c3(xsp ,o+p \u2212 xsp,o\u2212p ) ) \u00d7 (\u2212Ups )\u2212 2\u03bb p o\u2212 V p o\u2212\n(4.7) \u2202BPR\n\u2202b p\no+\n= ( 1\u2212 \u03c3(xsp,o+p \u2212 xsp,o\u2212p ) ) \u00d7 1\u2212 2\u03bbp o+ b p o+\n(4.8) \u2202BPR\n\u2202b p\no\u2212\n= ( 1\u2212 \u03c3(xsp,o+p \u2212 xsp,o\u2212p ) ) \u00d7 (\u22121)\u2212 2\u03bbp o\u2212 b p o\u2212\nNext, the parameters are updated as follows:\n(4.9) Ups = U p s + \u03b1\u00d7\n\u2202BPR\n\u2202U p s\n(4.10) V p o+ = V p o+\n+ \u03b1\u00d7 \u2202BPR\n\u2202V p\no+\n(4.11) V p o\u2212 = V p o\u2212\n+ \u03b1\u00d7 \u2202BPR\n\u2202V p\no\u2212\n(4.12) bp o+ = bp o+\n+ \u03b1\u00d7 \u2202BPR\n\u2202b p\no+\n(4.13) bp o\u2212 = bp o\u2212\n+ \u03b1\u00d7 \u2202BPR\n\u2202b p\no\u2212\nwhere \u03b1 is the learning rate.\nAlgorithm 1 Bayesian Personalized Ranking Based Latent Feature Embedding Model\nInput: latent dimension K, G, target predicate p Output: Up, V p, bp\n1: Given target predicate p and entire knowledge graph G, construct its bipartite subgraph, Gp 2: m = number of subject entities in Gp 3: n = number of object entities in Gp 4: Generate a set of training samples Dp =\n{(sp, o + p , o \u2212 p )} using uniform sampling technique\n5: Initialize Up as size m\u00d7K matrix with 0 mean and standard deviation 0.1 6: Initialize V p as size n\u00d7K matrix with 0 mean and stardard deviation 0.1 7: Initialize bp as size n\u00d71 column vector with 0 mean and stardard deviation 0.1 8: for all (sp, o + p , o \u2212 p ) \u2208 Dp do\n9: Update Ups based on Equation 4.9 10: Update V p\no+ based on Equation 4.10\n11: Update V p o\u2212\nbased on Equation 4.11 12: Update bp\no+ based on Equation 4.12\n13: Update bp o\u2212\nbased on Equation 4.13 14: end for 15: return Up, V p, bp"}, {"heading": "4.3 Pseudo-code", "text": "The pseudo-code of our proposed link prediction model is described in Algorithm 1. It takes the knowledge graph G and a specific target predicate p as input and generates the low dimensional latent matrices Up, V p, bp as output. Line 1 constucts the bipartite subgraph of predicate p, Gp given entire knowledge graph G. Line 2- 3 compute the number of subject and object entities as m and n in resultant bipartite subgraph Gp respectively. Line 4 generates a collection of triple samples using uniform sampling technique. Line 5-7 initialize the matrices Up, V p, bp using Gaussian distribution with 0 mean and 0.1 standard deviation, assuming all the entries in Up, V p and bp are independent. Line 8-14 update corresponding rows of matrices Up, V p, bp based on the sampled instance (sp, o + p , o \u2212 p ) in each iteration. As the sample generation step in line 4 is prior to the\nmodel parameter learning, thus the convergence criteria of Algorithm 1 is to iterate over all the sampled triples in Dp."}, {"heading": "5 Experiments and Results", "text": "This section presents our experimental analysis of the above algorithms for thirteen unique predicates in the well known YAGO2 knowledge graph [12]. We construct a model for each predicate and describe our evaluation strategies, including performance metrics and selection of state-of-the-art methods for benchmarking in section 5.1. We aim to answer two questions through our experiments:\nTable 1 shows the statistic of various YAGO2 relations used in our experiments. # Subject and # Object represent the number of subject and object entities associated with its corresponding predicate. The last column shown in Table 1 shows the number of facts for each relation in YAGO2. We run all the experiments on a 2.1 GHz Machine with 4GB memory running Linux operating system. The algorithms are implemented in Python language and used NumPy and SciPy libraries for linear algebra operations."}, {"heading": "5.1 Experimental Setting", "text": "For our experiment, in order to demonstrate the performance of our proposed link prediction model, we use the YAGO2 dataset and several evaluation metrics for all compared algorithms. Particularly, for each relation, we split the data into a training part, used for model training, and a test part, used for model evaluation. We\napply 5-time leave one out evaluation strategy, where for each subject, we randomly remove one fact (one subjectobject pair) and place it into test set Stest and remaining in the training set Strain. For every subject, the training model will generate a size-N ranked list of recommended objects for recommendation task. The evaluation is conducted by comparing the recommendation list of each subject and the object entity of that subject in the test set. Grid search is applied to find regularization parameters, and we set the values of parameters used in section 4.2 as \u03bbs = \u03bbo+ = \u03bbo\u2212 = \u03bbb = 0.005. For other model parameters, we fix learning rate \u03b1 = 0.2, number of latent factors K = 50, maximum number of iterations allowed maxIter = 50 respectively. For parameter in model evaluation, we set N = 10.\nIn order to illustrate the merit of our proposed approach, we compare our model with the following methods for link prediction in a knowledge graph. Since the problem we solve in this paper is similar to one-class item recommendation [23] in recommender systems, we consider the following state-of-the-art oneclass recommendation methods as baseline approaches for comparison.\n1. Random (Rand): For each relation, this method randomly selects subject-object entity pair for link recommendation task.\n2. Most Popular (MP): For each predicate in knowledge base, this method presents a nonpersonalized ranked object list based on how often object entities are connected among all subject entities.\n3. MF: The matrix factorization method is proposed by [17], which uses a point-wise strategy for solving one-class item recommendation problem.\nDuring the model evaluation stage, we use three popular metrics, namely Hit Rate (HR), Average Reciprocal Hit-Rank (ARHR), and Area Under Curve (AUC), to measure the link recommendation quality of our proposed approach in comparison to baseline methods. HR is defined as follows:\n(5.14) HR = #hits\n#subjects\nwhere #subjects is the total number of subject entities in test set, and #hits is the number of subjects whose object entity in the test set is recommended in the size-N recommendation list. The second evaluation metric, ARHR, considering the ranking of the recommended\nobject for each subject entity in knowledge graph, is defined below:\n(5.15) ARHR = 1\n#subjects\n#hits \u2211\ni=1\n1\npi\nwhere if an object of a subject is recommended for connection in knowledge graph which we name it hit under this scenario, pi is the position of the object in the ranked recommendation list. As we can see, ARHR is a weighted version of HR and it captures the importance of recommended object in the recommendation list.\nThe last metric, AUC is defined as follows:\n(5.16) AUC = 1#subjects \u2211 s\u2208subjects 1 |E(s)| \u2211 (o+,o\u2212)\u2208E(s) \u03b4(xs,o+ > xs,o\u2212)\nWhere E(s) = {(o+, o\u2212)|(s, o+) \u2208 Stest \u2229 (s, o \u2212) 6\u2208 (Stest \u222a Strain)}, and \u03b4() is the indicator function. For all of three metrics, higher values indicate better model performance. Specifically, the trivial AUC of a random predictor is 0.5 and the best value of AUC is 1."}, {"heading": "5.2 YAGO2 Relation Prediction Performance", "text": "Figure 1 shows the average link prediction performance for YAGO2 relations using various methods. Our proposed latent feature embedding approach shows significant improvement compared with other algorithms on most of relations in YAGO2. For instance, for all the YAGO2 predicates used in the experiment, our proposed model consistently outperforms MF based method, which demonstrates the empirical experience that pairwise ranking based method achieves much better performance than pointwise regression based method given implicit feedback for link recommendation task. Compared with Popularity based recommendation method MP, our method obtains better performance for most predicates. For example, predicates such as \u201cparticipate\u201d,\u201cconnect\u201d,\u201chasChild\u201d, and \u201cinfluence\u201d, our proposed model achieves more than 10 times better performance in terms of both HR and ARHR. However, for several predicates such as \u201cimport\u201d, \u201cexport\u201d, and \u201clanguage\u201d, MP based method performs the best among all the competing methods. The good performance of MP is owing to the semantic meaning of specific predicate. For instance, \u201cimport\u201d represents Country/Product relation in YAGO, which indicates the types of its subject and object entities are geographic region and commodity respectively. For such predicate, most popular object entities such as food, cloth, fuel are linked to most of countries, which helps\nMP based method maintain good link recommendation performance."}, {"heading": "5.3 Analysis and Discussion", "text": "Figure 1 shows that the link prediction model performance widely varies from predicate to predicate in the YAGO2 knowledge base. For example, the HR of predicate \u201cdealsWith\u201d is significantly better than \u201cown\u201d. Thus it is critical that we quantitatively understand the model performance across various relations in a knowledge graph. Recall from the problem statement that given a predicate p, our model Mp only accounts for the bipartite subgraph Gp. Motivated by [20], we study the impact of resultant graph structure of Gp on the performance of Mp.\nFor each predicate p, we compute several graph topology metrics on its bipartite subgraph Gp such as graph density, graph average degree, and clustering coefficient. Figure 2 shows the quantitative analysis between graph structure and link prediction model performance of each predicate. In each subfigure, x-axis represents the computed graph topology metric value of each predicate and y-axis denotes our proposed link prediction model performance in terms of HR, ARHR, and AUC. Each cross point shown in blue represents one specific YAGO2 predicate used in our experiments. Then we developed a linear regression model to understand the correlation between link prediction model performance and each graph metric. For each linear regression curve shown in red, we also report its slope, intercept, and correlation coefficient (rvalue) to capture the association trend.\nFrom Figure 2, both graph density and graph average degree show strong positive correlation signal with proposed link prediction model as demonstrated by rvalue. As our approach is inspired by collaborative filtering for recommender systems that accept a useritem matrix as input, for resultant graph of each predicate, higher graph density indicates higher matrix density in user-item matrix, which naturally leads to better recommendation performance in recommender system domain. Similar explanation can be adapted to graph average degree. For the clustering coefficient, it shows strong negative correlation signal with link prediction model performance. For instance, in terms of AUC, the rvalue is around \u22120.69. As clustering coefficient (cc) is the number of closed triples over the total number of triples in graph, smaller value of cc indicates lower fraction of closed triples in the graph. Based on the transitivity property of a social graph, which states the friends of your friend have high likelihood to be friends themselves [13], it is relatively easier for link predic-\ntion model to predict (i.e.,hit) such link with open triple property in the graph, which leads to better link prediction performance."}, {"heading": "6 Conclusion and Future Work", "text": "Inspired by the success of collaborative filtering algorithms for recommender systems, we propose a latent feature based embedding model for the task of link prediction in a knowledge graph. Our proposed method provides a measure of \u201cconfidence\u201d for adding a triple into the knowledge graph. We evaluate our implementation on the well known YAGO2 knowledge graph. The experiments show that our Bayesian Personalized Ranking based approach achieves better performance compared with two state-of-art recommender system models: Most Popular and Matrix Factorization. We also develop a linear regression model to quantitatively study the correlation between the performance of link prediction model itself and various topological metrics of the graph from which the models are constructed. The regression analysis shows strong correlation between the link prediction performance and graph topological features, such as graph density, average degree and clustering coefficient.\nFor a given predicate, we built link prediction models solely based on the bipartite subgraph of the original knowledge graph. However, as real-world experience suggests, the existence of a relation between two entities can also be predicted from the presence of other relations, either direct or through common neighbors. As an example, the knowledge of where someone studies and who they are friends with is useful to predict possible workplaces. Incorporating such intuition as \u201csocial signals\u201d into our current model will be the prime candidate for future work."}], "references": [{"title": "Constructing and mining web-scale knowledge graphs: Kdd 2014 tutorial", "author": ["A. Bordes", "E. Gabrilovich"], "venue": "SIGKDD, pages 1967\u20131967,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Parafac", "author": ["R. Bro"], "venue": "tutorial and applications. Chemometrics and Intelligent Laboratory Systems, pages 149\u2013171,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["K.-W. Chang", "W. tau Yih", "B. Yang", "C. Meek"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. ACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, pages 2493\u20132537,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Multirelational link prediction in heterogeneous information networks", "author": ["D. Davis", "R. Lichtenwalter", "N.V. Chawla"], "venue": "ASONAM, pages 281\u2013288. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "SIGKDD, KDD, pages 601\u2013 610,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Link prediction and recommendation across heterogeneous social networks", "author": ["Y. Dong", "J. Tang", "S. Wu", "J. Tian", "N.V. Chawla", "J. Rao", "H. Cao"], "venue": "ICDM, pages 181\u2013190. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Predicting rdf triples in incomplete knowledge bases with tensor factorization", "author": ["L. Drumond", "S. Rendle", "L. Schmidt-Thieme"], "venue": "Proceedings of the 27th Annual ACM Symposium on Applied Computing, pages 326\u2013 331,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Open information extraction from the web", "author": ["O. Etzioni", "M. Banko", "S. Soderland", "D.S. Weld"], "venue": "Communications of the ACM, pages 68\u201374,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning probabilistic relational models", "author": ["N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer"], "venue": "IJCAI, pages 1300\u20131309,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Link prediction using supervised learning", "author": ["M.A. Hasan", "V. Chaoji", "S. Salem", "M. Zaki"], "venue": "In Proc. of SDM 06 workshop on Link Analysis, Counterterrorism and Security,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Yago2: Exploring and querying world knowledge in time, space, context, and many languages", "author": ["J. Hoffart", "F.M. Suchanek", "K. Berberich", "E. Lewis- Kelham", "G. de Melo", "G. Weikum"], "venue": "In Proceedings of the 20th International Conference Companion on World Wide Web,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A latent factor model for highly multirelational data", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G.R. Obozinski"], "venue": "Advances in Neural Information Processing Systems 25, pages 3167\u20133175.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to refine an automatically extracted knowledge base using markov logic", "author": ["S. Jiang", "D. Lowd", "D. Dou"], "venue": "ICDM, pages 912\u2013917,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Link prediction in multi-relational graphs using additive models", "author": ["X. Jiang", "V. Tresp", "Y. Huang", "M. Nickel"], "venue": "SeRSy, pages 1\u201312,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, pages 30\u201337,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["N. Lao", "W.W. Cohen"], "venue": "Journal of Machine learning, pages 53\u201367,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["N. Lao", "T. Mitchell", "W.W. Cohen"], "venue": "EMNLP, pages 529\u2013539,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "The link prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "CIKM, pages 556\u2013559,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Reducing the rank in relational factorization models by including  observable patterns", "author": ["M. Nickel", "X. Jiang", "V. Tresp"], "venue": "NIPS, pages 1179\u20131187,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H. peter Kriegel"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "UAI, pages 452\u2013461,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning, pages 107\u2013136,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["S. Riedel", "L. Yao", "A. McCallum", "B.M. Marlin"], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, pages 74\u201384,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "NIPS, pages 926\u2013934,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "When will it happen?: relationship prediction in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "C.C. Aggarwal", "N.V. Chawla"], "venue": "WSDM, pages 663\u2013672. ACM,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Trust, but verify: Predicting contribution quality for knowledge base construction and curation", "author": ["C.H. Tan", "E. Agichtein", "P. Ipeirotis", "E. Gabrilovich"], "venue": "WSDM, pages 553\u2013562. ACM,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative probabilistic models for relational data", "author": ["B. Taskar", "P. Abbeel", "D. Koller"], "venue": "UAI, pages 485\u2013492,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 8, "context": "Towards that, we extract triples from the text data sources using state of the art techniques such as OpenIE [9] and semantic role labeling [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "Towards that, we extract triples from the text data sources using state of the art techniques such as OpenIE [9] and semantic role labeling [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 10, "context": "Link prediction is a well established field [11] in the context of social network analysis.", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "Lately, link prediction for heterogenous networks has received significant attention in the data mining community [5, 7, 27].", "startOffset": 114, "endOffset": 124}, {"referenceID": 6, "context": "Lately, link prediction for heterogenous networks has received significant attention in the data mining community [5, 7, 27].", "startOffset": 114, "endOffset": 124}, {"referenceID": 25, "context": "Lately, link prediction for heterogenous networks has received significant attention in the data mining community [5, 7, 27].", "startOffset": 114, "endOffset": 124}, {"referenceID": 21, "context": "Bayesian personalized ranking (BPR) [23] based embedding model has been a major influence in this area and is the primary driver behind our approach.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "It is important that we quantitatively understand the accuracy of our prediction [28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "The desire to obtain a quantitative grasp on prediction accuracy is complicated by a number of reasons: 1) Knowledge graphs constructed from web text or using machine reading approaches can have a very large number of predicates [6] that make manual verification difficult.", "startOffset": 229, "endOffset": 232}, {"referenceID": 2, "context": "In terms of methodology, factorization based and related latent variable models [3, 8, 14, 22, 25], graphical model [15], and graph feature based method [18, 19] are considered.", "startOffset": 80, "endOffset": 98}, {"referenceID": 7, "context": "In terms of methodology, factorization based and related latent variable models [3, 8, 14, 22, 25], graphical model [15], and graph feature based method [18, 19] are considered.", "startOffset": 80, "endOffset": 98}, {"referenceID": 12, "context": "In terms of methodology, factorization based and related latent variable models [3, 8, 14, 22, 25], graphical model [15], and graph feature based method [18, 19] are considered.", "startOffset": 80, "endOffset": 98}, {"referenceID": 20, "context": "In terms of methodology, factorization based and related latent variable models [3, 8, 14, 22, 25], graphical model [15], and graph feature based method [18, 19] are considered.", "startOffset": 80, "endOffset": 98}, {"referenceID": 23, "context": "In terms of methodology, factorization based and related latent variable models [3, 8, 14, 22, 25], graphical model [15], and graph feature based method [18, 19] are considered.", "startOffset": 80, "endOffset": 98}, {"referenceID": 13, "context": "In terms of methodology, factorization based and related latent variable models [3, 8, 14, 22, 25], graphical model [15], and graph feature based method [18, 19] are considered.", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "In terms of methodology, factorization based and related latent variable models [3, 8, 14, 22, 25], graphical model [15], and graph feature based method [18, 19] are considered.", "startOffset": 153, "endOffset": 161}, {"referenceID": 17, "context": "In terms of methodology, factorization based and related latent variable models [3, 8, 14, 22, 25], graphical model [15], and graph feature based method [18, 19] are considered.", "startOffset": 153, "endOffset": 161}, {"referenceID": 1, "context": "[2] presents a tensor based model that decomposes each entity and predicate in knowledge graphs as a low dimensional vector.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "In order to solve this issue, [22] proposes a relational latent feature model, RESCAL, an efficient approach which uses a tensor factorization model that takes the inherent structure of relational data into account.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "By leveraging relational domain knowledge about entity type information, [3] proposes a tensor decomposition approach for relation extraction in knowledge base which is highly efficient in terms of time complexity.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "In addition, various other latent variable models, such as neural network based methods [6, 26], have been explored for link prediction task.", "startOffset": 88, "endOffset": 95}, {"referenceID": 24, "context": "In addition, various other latent variable models, such as neural network based methods [6, 26], have been explored for link prediction task.", "startOffset": 88, "endOffset": 95}, {"referenceID": 9, "context": "Recently graphical models, such as Probabilistic Relational Models [10], Relational Markov Network [29], Markov Logic Network [15, 24] have also been used for link prediction in knowledge graph.", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "Recently graphical models, such as Probabilistic Relational Models [10], Relational Markov Network [29], Markov Logic Network [15, 24] have also been used for link prediction in knowledge graph.", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "Recently graphical models, such as Probabilistic Relational Models [10], Relational Markov Network [29], Markov Logic Network [15, 24] have also been used for link prediction in knowledge graph.", "startOffset": 126, "endOffset": 134}, {"referenceID": 22, "context": "Recently graphical models, such as Probabilistic Relational Models [10], Relational Markov Network [29], Markov Logic Network [15, 24] have also been used for link prediction in knowledge graph.", "startOffset": 126, "endOffset": 134}, {"referenceID": 22, "context": "For instance, [24] proposes a Markov Logic Network (MLN) based approach, which is a template language for defining potential functions on knowledge graph by logical formula.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "Lao and Cohen [18,19] propose Path Ranking Algorithm (PRA) to perform random walk on the graph and compute the probability of each path.", "startOffset": 14, "endOffset": 21}, {"referenceID": 17, "context": "Lao and Cohen [18,19] propose Path Ranking Algorithm (PRA) to perform random walk on the graph and compute the probability of each path.", "startOffset": 14, "endOffset": 21}, {"referenceID": 0, "context": "It has been demonstrated [1] that no single based approach emerges as a clear winner.", "startOffset": 25, "endOffset": 28}, {"referenceID": 19, "context": "For instance, [21] proposed to use additive model, which is a linear combination between RESCAL and PRA.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "[16] combined a latent feature model with an additive term to learn from latent and neighborhood-based information on multirelational data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] fuses the output of PRA and neural network model as features for training a binary classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "Motivated by [23], we employ Bayesian Personalized Ranking (BPR) based approach for model learning.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "This section presents our experimental analysis of the above algorithms for thirteen unique predicates in the well known YAGO2 knowledge graph [12].", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "Since the problem we solve in this paper is similar to one-class item recommendation [23] in recommender systems, we consider the following state-of-the-art oneclass recommendation methods as baseline approaches for comparison.", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "MF: The matrix factorization method is proposed by [17], which uses a point-wise strategy for solving one-class item recommendation problem.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "Motivated by [20], we study the impact of resultant graph structure of Gp on the performance of Mp.", "startOffset": 13, "endOffset": 17}], "year": 2017, "abstractText": "Estimating the confidence for a link is a critical task for Knowledge Graph construction. Link prediction, or predicting the likelihood of a link in a knowledge graph based on prior state is a key research direction within this area. We propose a Latent Feature Embedding based link recommendation model for prediction task and utilize Bayesian Personalized Ranking based optimization technique for learning models for each predicate. Experimental results on largescale knowledge bases such as YAGO2 show that our approach achieves substantially higher performance than several state-of-art approaches. Furthermore, we also study the performance of the link prediction algorithm in terms of topological properties of the Knowledge Graph and present a linear regression model to reason about its expected level", "creator": "LaTeX with hyperref package"}}}