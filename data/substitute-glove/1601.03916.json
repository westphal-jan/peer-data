{"id": "1601.03916", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "Multimodal Pivots for Image Caption Translation", "abstract": "We present kind particular that importantly bayesian machine translation of dark particulars under connectivity pivots relation later utilizes array. Image similarity does formula_2 by man d-pad proteins radio however operated down setting measures - away translation clock retrieval model where allusions of most similar images that not to rerank translation outputs. Our approach does rather whatever on second retention common in - exist parallel data being achieves improvements according 1. 4 BLEU since giving baselines.", "histories": [["v1", "Fri, 15 Jan 2016 13:42:04 GMT  (547kb,D)", "https://arxiv.org/abs/1601.03916v1", "in submission"], ["v2", "Mon, 21 Mar 2016 13:47:26 GMT  (461kb,D)", "http://arxiv.org/abs/1601.03916v2", "domain adaptation, VGG16 visual features"], ["v3", "Mon, 13 Jun 2016 16:52:09 GMT  (464kb,D)", "http://arxiv.org/abs/1601.03916v3", "Final version, accepted at ACL 2016. New section on Human Evaluation"]], "COMMENTS": "in submission", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["julian hitschler", "shigehiko schamoni", "stefan riezler"], "accepted": true, "id": "1601.03916"}, "pdf": {"name": "1601.03916.pdf", "metadata": {"source": "CRF", "title": "Multimodal Pivots for Image Caption Translation", "authors": ["Julian Hitschler", "Shigehiko Schamoni", "Stefan Riezler"], "emails": ["hitschler@cl.uni-heidelberg.de", "schamoni@cl.uni-heidelberg.de", "riezler@cl.uni-heidelberg.de"], "sections": [{"heading": "1 Introduction", "text": "Multimodal data consisting of images and natural language descriptions (henceforth called captions) are an abundant source of information that has led to a recent surge in research integrating language and vision. Recently, the aspect of multilinguality has been added to multimodal language processing in a shared task at the WMT16 conference.1 There is clearly also a practical demand for multilingual image captions, e.g., automatic translation of descriptions of art works would allow access to digitized art catalogues across language barriers and is thus of social and cultural interest; multilingual product descriptions are of high commercial interest since they would allow to widen e-commerce transactions automatically to international markets. However, while datasets of images and monolingual captions already include millions\n1http://www.statmt.org/wmt16/ multimodal-task.html\nof tuples (Ferraro et al., 2015), the largest multilingual datasets of images and captions known to the authors contain 20,000 (Grubinger et al., 2006) or 30,0002 triples of images with German and English descriptions.\nIn this paper, we want to address the problem of multilingual captioning from the perspective of statistical machine translation (SMT). In contrast to prior work on generating captions directly from images (Kulkarni et al. (2011), Karpathy and FeiFei (2015), Vinyals et al. (2015), inter alia), our goal is to integrate visual information into an SMT pipeline. Visual context provides orthogonal information that is free of the ambiguities of natural language, therefore it serves to disambiguate and to guide the translation process by grounding the translation of a source caption in the accompanying image. Since datasets consisting of source language captions, images, and target language captions are not available in large quantities, we would instead like to utilize large datasets of images and target-side monolingual captions to improve SMT models trained on modest amounts of parallel captions.\nLet the task of caption translation be defined as follows: For production of a target caption ei of an image i, a system may use as input an image caption for image i in the source language fi, as well as the image i itself. The system may safely assume that fi is relevant to i, i.e., the identification of relevant captions for i (Hodosh et al., 2013) is not itself part of the task of caption translation. In contrast to the inference problem of finding e\u0302 = argmaxe p(e|f) in text-based SMT, multimodal caption translation allows to take into consideration i as well as fi in finding e\u0302i:\ne\u0302i = argmax ei\np(ei|fi, i)\n2The dataset used at the WMT16 shared task is based on translations of Flickr30K captions (Rashtchian et al., 2010).\nar X\niv :1\n60 1.\n03 91\n6v 3\n[ cs\n.C L\n] 1\n3 Ju\nn 20\n16\nIn this paper, we approach caption translation by a general crosslingual reranking framework where for a given pair of source caption and image, monolingual captions in the target language are used to rerank the output of the SMT system. We present two approaches to retrieve target language captions for reranking by pivoting on images that are similar to the input image. One approach calculates image similarity based deep convolutional neural network (CNN) representations. Another approach calculates similarity in visual space by comparing manually annotated object categories. We compare the multimodal pivot approaches to reranking approaches that are based on text only, and to standard SMT baselines trained on parallel data. Compared to a strong baseline trained on 29,000 parallel caption data, we find improvements of over 1 BLEU point for reranking based on visual pivots. Notably, our reranking approach does not rely on large amounts of in-domain parallel data which are not available in practical scenarios such as e-commerce localization. However, in such scenarios, monolingual product descriptions are naturally given in large amounts, thus our work is a promising pilot study towards real-world caption translation."}, {"heading": "2 Related Work", "text": "Caption generation from images alone has only recently come into the scope of realistically solvable problems in image processing (Kulkarni et al. (2011), Karpathy and Fei-Fei (2015), Vinyals et al. (2015), inter alia). Recent approaches also employ reranking of image captions by measuring similarity between image and text using deep representations (Fang et al., 2015). The tool of choice in these works are neural networks whose deep representations have greatly increased the quality of feature representations of images, enabling robust and semantically salient analysis of image content. We rely on the CNN framework (Socher et al., 2014; Simonyan and Zisserman, 2015) to solve semantic classification and disambiguation tasks in NLP with the help of supervision signals from visual feedback. However, we consider image captioning as a different task than caption translation since it is not given the information of the source language string. Therefore we do not compare our work to caption generation models.\nIn the area of SMT, Wa\u0308schle and Riezler (2015) presented a framework for integrating a large, in-\ndomain, target-side monolingual corpus into machine translation by making use of techniques from crosslingual information retrieval. The intuition behind their approach is to generate one or several translation hypotheses using an SMT system, which act as queries to find matching, semantically similar sentences in the target side corpus. These can in turn be used as templates for refinement of the translation hypotheses, with the overall effect of improving translation quality. Our work can be seen as an extension of this method, with visual similarity feedback as additional constraint on the crosslingual retrieval model. Calixto et al. (2012) suggest using images as supplementary context information for statistical machine translation. They cite examples from the news domain where visual context could potentially be helpful in the disambiguation aspect of SMT and discuss possible features and distance metrics for context images, but do not report experiments involving a full SMT pipeline using visual context. In parallel to our work, Elliott et al. (2015) addressed the problem of caption translation from the perspective of neural machine translation.3 Their approach uses a model which is considerably more involved than ours and relies exclusively on the availability of parallel captions as training data. Both approaches crucially rely on neural networks, where they use a visually enriched neural encoder-decoder SMT approach, while we follow a retrieval paradigm for caption translation, using CNNs to compute similarity in visual space.\nIntegration of multimodal information into NLP problems has been another active area of recent research. For example, Silberer and Lapata (2014) show that distributional word embeddings grounded in visual representations outperform competitive baselines on term similarity scoring and word categorization tasks. The orthogonality of visual feedback has previously been exploited in a multilingual setting by Kiela et al. (2015) (relying on previous work by Bergsma and Van Durme (2011)), who induce a bilingual lexicon using term-specific multimodal representations obtained by querying the Google image\n3We replicated the results of Elliott et al. (2015) on the IAPR TC-12 data. However, we decided to not include their model as baseline in this paper since we found our hierarchical phrase-based baselines to yield considerably better results on IAPR TC-12 as well as on MS COCO.\nsearch engine.4 Funaki and Nakayama (2015) use visual similarity for crosslingual document retrieval in a multimodal and bilingual vector space obtained by generalized canonical correlation analysis, greatly reducing the need for parallel training data. The common element is that CNNbased visual similarity information is used as a \u201chub\u201d (Funaki and Nakayama, 2015) or pivot connecting corpora in two natural languages which lack direct parallelism, a strategy which we apply to the problem of caption translation."}, {"heading": "3 Models", "text": ""}, {"heading": "3.1 Overview", "text": "Following the basic approach set out by Wa\u0308schle and Riezler (2015), we use a crosslingual retrieval model to find sentences in a target language document collection C, and use these to rerank target language translations e of a source caption f .\nThe systems described in our work differ from that of Wa\u0308schle and Riezler (2015) in a number of aspects. Instead of a two-step architecture of coarse-grained and fine-grained retrieval, our system uses relevance scoring functions for retrieval of matches in the document collection C, and for\n4https://images.google.com/\nreranking of translation candidates that are based on inverse document frequency of terms (Spa\u0308rck Jones, 1972) and represent variants of the popular TF-IDF relevance measure.\nA schematic overview of our approach is given in Figure 1. It consists of the following components:\nInput: Source caption fi, image i, target-side collection C of image-captions pairs\nTranslation: Generate unique list Nfi of kn-best translations, generate unique list Rfi of krbest list of translations5 using MT decoder\nMultimodal retrieval: For list of translations Nfi , find set Mfi of km-most relevant pairs of images and captions in a target-side collection C, using a heuristic relevance scoring function S(m,Nfi , i),m \u2208 C\nCrosslingual reranking: Use list Mfi of imagecaption pairs to rerank list of translations Rfi , applying relevance scoring function F (r,Mfi) to all r \u2208 Rfi\nOutput: Determine best translation hypothesis e\u0302i by interpolating decoder score dr for a hypothesis r \u2208 Rfi with its relevance score F (r,Mfi) with weight \u03bb s.t.\ne\u0302i = argmax r\u2208Rfi dr + \u03bb \u00b7 F (r,Mfi)\nThe central concept is the scoring function S(m,Nfi , i) which defines three variants of target-side retrieval (TSR), all of which make use of the procedure outlined above. In the baseline text-based reranking model (TSR-TXT), we use relevance scoring function STXT . This function is purely text-based and does not make use of multimodal context information (as such, it comes closest to the models used for target-side retrieval in Wa\u0308schle and Riezler (2015)). In the retrieval model enhanced by visual information from a deep convolutional neural network (TSRCNN), the scoring function SCNN incorporates a textual relevance score with visual similarity information extracted from the neural network. Finally, we evaluate these models against a relevance score based on human object-category annotations (TSR-HCA), using the scoring function\n5In practice, the first hypothesis list may be reused. We distinguish between the two hypothesis lists Nfi and Rfi for notational clarity since in general, the two hypothesis lists need not be of equal length.\nSHCA. This function makes use of the object annotations available for the MS COCO corpus (Lin et al., 2014) to give an indication of the effectiveness of our automatically extracted visual similarity metric. The three models are discussed in detail below."}, {"heading": "3.2 Target Side Retrieval Models", "text": "Text-Based Target Side Retrieval. In the TSRTXT retrieval scenario, a match candidate m \u2208 C is scored in the following way:\nSTXT (m,Nfi) = Zm \u2211\nn\u2208Nfi\n\u2211 wn\u2208tok(n) \u2211 wm\u2208typ(m) \u03b4(wm, wn)idf(wm),\nwhere \u03b4 is the Kronecker \u03b4-function, Nfi is the set of the kn-best translation hypotheses for a source caption fi of image i by decoder score, typ(a) is a function yielding the set of types (unique tokens) contained in a caption a,6 tok(a) is a function yielding the tokens of caption a, idf(w) is the inverse document frequency (Spa\u0308rck Jones, 1972) of term w, and Zm = 1|typ(m)| is a normalization term introduced in order to avoid biasing the system towards long match candidates containing many low-frequency terms. Term frequencies were computed on monolingual data from Europarl (Koehn, 2005) and the News Commentary and News Discussions English datasets provided for the WMT15 workshop.7 Note that in this model, information from the image i is not used.\nMultimodal Target Side Retrieval using CNNs. In the TSR-CNN scenario, we supplement the textual target-side TSR model with visual similarity information from a deep convolutional neural network. We formalize this by introduction of the positive-semidefinite distance function v(ix, iy) \u2192 [0,\u221e) for images ix, iy (smaller values indicating more similar images). The relevance scoring function SCNN used in this model\n6The choice for per-type scoring of reference captions was primarily driven by performance considerations. Since captions rarely contain repetitions of low-frequency terms, this has very little effect in practice, other than to mitigate the influence of stopwords.\n7http://www.statmt.org/wmt15/ translation-task.html\ntakes the following form:\nSCNN (m,Nfi , i)\n=\n{ STXT (m,Nfi)e \u2212bv(im,i), v(im, i) < d\n0 otherwise,\nwhere im is the image to which the caption m refers and d is a cutoff maximum distance, above which match candidates are considered irrelevant, and b is a weight term which controls the impact of the visual distance score v(im, i) on the overall score.8\nOur visual distance measure v was computed using the VGG16 deep convolutional model of Simonyan and Zisserman (2015), which was pretrained on ImageNet (Russakovsky et al., 2014). We extracted feature values for all input and reference images from the penultimate fully-connected layer (fc7) of the model and computed the Euclidean distance between feature vectors of images. If no neighboring images fell within distance d, the text-based retrieval procedure STXT was used as a fallback strategy, which occurred 47 out of 500 times on our test data.\nTarget Side Retrieval by Human Category Annotations. For contrastive purposes, we evaluated a TSR-HCA retrieval model which makes use of the human object category annotations for MS COCO. Each image in the MS COCO corpus is annotated with object polygons classified into 91 categories of common objects. In this scenario, a match candidatem is scored in the following way:\nSHCA(m,Nfi , i)\n= \u03b4(cat(im), cat(i))STXT (m,Nfi),\nwhere cat(i) returns the set of object categories with which image i is annotated. The amounts to enforcing a strict match between the category annotations of i and the reference image im, thus pre-filtering the STXT scoring to captions for images with strict category match.9 In cases where i was annotated with a unique set of object categories and thus no match candidates with nonzero scores were returned by SHCA, STXT was used as a fallback strategy, which occurred 77 out of 500 times on our test data.\n8The value of b = 0.01 was found on development data and kept constant throughout the experiments.\n9Attempts to relax this strict matching criterion led to strong performance degradation on the development test set."}, {"heading": "3.3 Translation Candidate Re-scoring", "text": "The relevance score F (r,Mfi) used in the reranking model was computed in the following way for all three models:\nF (r,Mfi) =\nZMfi \u2211 m\u2208Mfi \u2211 wm\u2208typ(m) \u2211 wr\u2208tok(r) \u03b4(wm, wr)idf(wm)\nwith normalization term ZMfi = ( \u2211\nm\u2208Mfi\n|tok(m)|)\u22121,\nwhere r is a translation candidate and Mfi is a list of km-top target side retrieval matches. Because the model should return a score that is reflective of the relevance of r with respect toMfi , irrespective of the length of Mfi , normalization with respect to the token count of Mfi is necessary. The term ZMfi serves this purpose."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Bilingual Image-Caption Data", "text": "We constructed a German-English parallel dataset based on the MS COCO image corpus (Lin et al., 2014). 1,000 images were selected at random from the 2014 training section10 and, in a second step, one of their five English captions was chosen randomly. This caption was then translated into German by a native German speaker. Note that our experiments were performed with German as the source and English as the target language, therefore, our reference data was not produced by a single speaker but reflects the heterogeneity of the MS COCO dataset at large. The data was split into a development set of 250 captions, a development test set of 250 captions for testing work in progress, and a test set of 500 captions. For our retrieval experiments, we used only the images and captions that were not included in the development, development test or test data, a total of 81,822 images with 5 English captions per image. All data was tokenized and converted to lower case using the cdec11 utilities tokenized-anything.pl and lowercase.pl. For the German data, we\n10We constructed our parallel dataset using only the training rather than the validation section of MS COCO so as to keep the latter pristine for future work based on this research.\n11https://github.com/redpony/cdec\nperformed compound-splitting using the method described by Dyer (2009), as implemented by the cdec utility compound-split.pl. Table 1 gives an overview of the dataset. Our parallel development, development test and test data is publicly available.12"}, {"heading": "4.2 Translation Baselines", "text": "We compare our approach to two baseline machine translation systems, one trained on out-ofdomain data exclusively and one domain-adapted system. Table 2 gives an overview of the training data for the machine translation systems.\nOut-of-Domain Baseline. Our baseline SMT framework is hierarchical phrase-based translation using synchronous context free grammars (Chiang, 2007), as implemented by the cdec decoder (Dyer et al., 2010). Data from the Europarl (Koehn, 2005), News Commentary and Common Crawl corpora (Smith et al., 2013) as provided for the WMT15 workshop was used to train the translation model, with German as source and English as target language.\nLike the retrieval dataset, training, development and test data was tokenized and converted to lower case, using the same cdec tools. Sentences with lengths over 80 words in either the source or the target language were discarded before training. Source text compound splitting was performed using compound-split.pl. Alignments were extracted bidirectionally using the fast-align utility of cdec and symmetrized with the atools utility (also part of cdec) using the grow-diag-final-and symmetrization heuristic. The alignments were then used by the cdec grammar extractor to extract a synchronous context free grammar from the parallel data.\n12www.cl.uni-heidelberg.de/decoco/\nThe target language model was trained on monolingual data from Europarl, as well as the News Crawl and News Discussions English datasets provided for the WMT15 workshop (the same data as was used for estimating term frequencies for the retrieval models) with the KenLM toolkit (Heafield et al., 2013; Heafield, 2011).13\nWe optimized the parameters of the translation system for translation quality as measured by IBM BLEU (Papineni et al., 2002) using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003). For tuning the translation models used for extraction of the hypothesis lists for final evaluation, MIRA was run for 20 iterations on the development set, and the best run was chosen for final testing.\nIn-Domain Baseline. We also compared our models to a domain-adapted machine translation system. The domain-adapted system was identical to the out-of-domain system, except that it was supplied with additional parallel training data from the image caption domain. For this purpose, we used 29,000 parallel German-English image captions as provided for the WMT16 shared task on multimodal machine translation. The English captions in this dataset belong to the Flickr30k corpus (Rashtchian et al., 2010) and are very similar to those of the MS COCO corpus. The German captions are expert translations. The English captions were also used as additional training data for the target-side language model. We generated kn- and kr-best lists of translation candidates using this in-domain baseline system.\n13https://kheafield.com/code/kenlm/"}, {"heading": "4.3 Optimization of TSR Hyperparameters", "text": "For each of our retrieval models, we performed a step-wise exhaustive search of the hyperparameter space over the four system hyperparameters for IBM BLEU on the development set: The length of the kn-best list the entries of which are used as queries for retrieval; the number of km-bestmatching captions retrieved; the length of the final kr-best list used in reranking; the interpolation weight \u03bb of the relevance score F relative to the translation hypothesis log probability returned by the decoder. The parameter ranges to be explored were determined manually, by examining system output for prototypical examples. Table 3 gives an overview over the hyperparameter values obtained.\nFor TSR-CNN, we initially set the cutoff distance d to 90.0, after manually inspecting sets of nearest neighbors returned for various maximum distance values. After optimization of retrieval parameters, we performed an exhaustive search from d = 80.0 to d = 100.0, with step size 1.0 on the development set, while keeping all other hyperparameters fixed, which confirmed out initial choice of d = 90.0 as the optimal value.\nExplored parameter spaces were identical for all models and each model was evaluated on the test set using its own optimal configuration of hyperparameters."}, {"heading": "4.4 Significance Testing", "text": "Significance tests on the differences in translation quality were performed using the approximate randomization technique for measuring performance differences of machine translation systems described in Riezler and Maxwell (2005) and implemented by Clark et al. (2011) as part of the Multeval toolkit.14\n14https://github.com/jhclark/multeval"}, {"heading": "4.5 Experimental Results", "text": "Table 4 summarizes the results for all models on an unseen test set of 500 captions. Domain adaptation led to a considerable improvement of +4.1 BLEU and large improvements in terms of METEOR and Translation Edit Rate (TER). We found that the target-side retrieval model enhanced with multimodal pivots from a deep convolutional neural network, TSR-CNN and TSR-HCA, consistently outperformed both the domain-adapted cdec baseline, as well as the text-based target side retrieval model TSR-TXT. These models therefore achieve a performance gain which goes beyond the effect of generic domain-adaptation. The gain in performance for TSR-CNN and TSRHCA was significant at p < 0.05 for BLEU, METEOR, and TER. For all evaluation metrics, the difference between TSR-CNN and TSR-HCA was not significant, demonstrating that retrieval using our CNN-derived distance metric could match retrieval based the human object category annotations.\n15A baseline for which a random hypothesis was chosen from the top-5 candidates of the in-domain system lies between the other two baseline systems: 27.5 / 33.3 / 47.7 (BLEU / METEOR / TER).\nThe text-based retrieval baseline TSR-TXT never significantly outperformed the in-domain cdec baseline, but there were slight nominal improvements in terms of BLEU, METEOR and TER. This finding is actually consistent with Wa\u0308schle and Riezler (2015) who report performance gains for text-based, target side retrieval models only on highly technical, narrow-domain corpora and even report performance degradation on medium-diversity corpora such as Europarl. Our experiments show that it is the addition of visual similarity information by incorporation of multimodal pivots into the image-enhanced models TSR-CNN and TSR-HCA which makes such techniques effective on MS COCO, thus upholding our hypothesis that visual information can be exploited for improvement of caption translation."}, {"heading": "4.6 Human Evaluation", "text": "The in-domain baseline and TSR-CNN differed in their output in 169 out of 500 cases on the test set. These 169 cases were presented to a human judge alongside the German source captions in a double-blinded pairwise preference ranking experiment. The order of presentation was randomized for the two systems. The judge was asked to rank fluency and accuracy of the translations independently. The results are given in Figure 2. Overall, there was a clear preference for the output of TSRCNN."}, {"heading": "4.7 Examples", "text": "Table 5 shows example translations produced by both cdec baselines, TSR-TXT, TSR-CNN, and TSR-HCA, together with source caption, image, and reference translation. The visual information induced by target side captions of pivot images allows a disambiguation of translation alternatives such as \u201cskirt\u201d versus \u201crock (music)\u201d for the German \u201cRock\u201d, \u201cpole\u201d versus \u201cmast\u201d for the German \u201cMasten\u201d, and is able to repair mistranslations such as \u201cfoot\u201d instead of \u201cmouth\u201d for the German \u201cMaul\u201d."}, {"heading": "5 Conclusions and Further Work", "text": "We demonstrated that the incorporation of multimodal pivots into a target-side retrieval model improved SMT performance compared to a strong in-domain baseline in terms of BLEU, METEOR and TER on our parallel dataset derived from MS COCO. The gain in performance was comparable between a distance metric based on a deep convolutional network and one based on human object category annotations, demonstrating the effectiveness of the CNN-derived distance measure. Using our approach, SMT can, in certain cases, profit from multimodal context information. Crucially, this is possible without using large amounts of indomain parallel text data, but instead using large amounts of monolingual image captions that are more readily available.\nLearning semantically informative distance metrics using deep learning techniques is an area under active investigation (Wu et al., 2013; Wang et al., 2014; Wang et al., 2015). Despite the fact that our simple distance metric performed comparably to human object annotations, using such high-level semantic distance metrics for caption translation by multimodal pivots is a promising avenue for further research.\nThe results were achieved on one language pair (German-English) and one corpus (MS COCO) only. As with all retrieval-based methods, generalized statements about the relative performance on corpora of various domains, sizes and qualities are difficult to substantiate. This problem is aggravated in the multimodal case, since the relevance of captions with respect to images varies greatly between different corpora (Hodosh et al., 2013). In future work, we plan to evaluate our approach in more naturalistic settings, such machine translation for captions in online multimedia repositories\nsuch as Wikimedia Commons16 and digitized art catalogues, as well as e-commerce localization.\nA further avenue of future research is improving models such as that presented in Elliott et al. (2015) by crucial components of neural MT such as \u201cattention mechanisms\u201d. For example, the attention mechanism of Bahdanau et al. (2015) serves as a soft alignment that helps to guide the translation process by influencing the sequence in which source tokens are translated. A similar mechanism is used in Xu et al. (2015) to decide which part of the image should influence which part of the generated caption. Combining these two types of attention mechanisms in a neural caption translation model is a natural next step in caption translation. While this is beyond the scope of this work, our models should provide an informative baseline against which to evaluate such methods."}, {"heading": "Acknowledgments", "text": "This research was supported in part by DFG grant RI-2221/2-1 \u201cGrounding Statistical Machine Translation in Perception and Action\u201d, and by an Amazon Academic Research Award (AARA) \u201cMultimodal Pivots for Low Resource Machine Translation in E-Commerce Localization\u201d."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR), San Diego, California, USA.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning bilingual lexicons using the visual similarity of labeled web images", "author": ["Shane Bergsma", "Benjamin Van Durme."], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), Barcelona, Spain.", "citeRegEx": "Bergsma and Durme.,? 2011", "shortCiteRegEx": "Bergsma and Durme.", "year": 2011}, {"title": "Images as context in statistical machine translation", "author": ["Iacer Calixto", "Te\u00f3filo de Compos", "Lucia Specia."], "venue": "Proceedings of the Workshop on Vision and Language (VL), Sheffield, England, United Kingdom.", "citeRegEx": "Calixto et al\\.,? 2012", "shortCiteRegEx": "Calixto et al\\.", "year": 2012}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics, 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Better hypothesis testing for statistical https://commons.wikimedia.org/wiki/ Main_Page", "author": ["Jonathan Clark", "Chris Dyer", "Alon Lavie", "Noah Smith"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Koby Crammer", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 3:951\u2013991.", "citeRegEx": "Crammer and Singer.,? 2003", "shortCiteRegEx": "Crammer and Singer.", "year": 2003}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Johnathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Using a maximum entropy model to build segmentation lattices for mt", "author": ["Chris Dyer."], "venue": "Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT), Boul-", "citeRegEx": "Dyer.,? 2009", "shortCiteRegEx": "Dyer.", "year": 2009}, {"title": "Multi-language image description with neural sequence models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR, abs/1510.04709.", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Li Deng", "Margaret Mitchell", "Saurabh Gupta", "Piotr Dollar", "John C. Platt", "Forrest Iandola", "Jianfeng Gao", "C. Lawrence Zitnick", "Rupesh K. Srivastava", "Xiaodeng He", "Geoffrey Zweit."], "venue": "In Proceedings", "citeRegEx": "Fang et al\\.,? 2015", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "A survey of current datasets for vision and language research", "author": ["Francis Ferraro", "Nasrin Mostafazadeh", "Ting-Hao (Kenneth) Huang", "Lucy Vanderwende", "Jacob Devlin", "Michel Galley", "Margaret Mitchell"], "venue": "In Proceedings of the Conference on Em-", "citeRegEx": "Ferraro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2015}, {"title": "Imagemediated learning for zero-shot cross-lingual document retrieval", "author": ["Ruka Funaki", "Hideki Nakayama."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal.", "citeRegEx": "Funaki and Nakayama.,? 2015", "shortCiteRegEx": "Funaki and Nakayama.", "year": 2015}, {"title": "The IAPR TC-12 benchmark: A new evaluatioin resource for visual information systems", "author": ["Michael Grubinger", "Paul Clough", "Henning M\u00fcller", "Thomas Deselaers."], "venue": "In Proceedings of LREC, Genova, Italy.", "citeRegEx": "Grubinger et al\\.,? 2006", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), Sofia,", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT), Edinburgh, Scotland, United Kingdom.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrey Karpathy", "Li Fei-Fei."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, Massachusetts, USA.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Visual bilingual lexicon induction with transferred convnet features", "author": ["Douwe Kiela", "Ivan Vuli\u0107", "Stephen Clark."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal.", "citeRegEx": "Kiela et al\\.,? 2015", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "Proceedings of the Machine Translation Summit, Phuket, Thailand.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-", "citeRegEx": "Kulkarni et al\\.,? 2011", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Microsoft COCO: common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "Lubomir D. Bourdev", "Ross B. Girshick", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick."], "venue": "Computing Research Repos-", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ard", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, Pennsylva-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechan-", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "On some pitfalls in automatic evaluation and significance testing for mt", "author": ["Stefan Riezler", "John Maxwell."], "venue": "Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Methods for MT and Summarization (MTSE) at the 43rd Annual Meeting of", "citeRegEx": "Riezler and Maxwell.,? 2005", "shortCiteRegEx": "Riezler and Maxwell.", "year": 2005}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael S. Bernstein", "Alexander C. Berg", "Fei-Fei Li."], "venue": "Computing", "citeRegEx": "Russakovsky et al\\.,? 2014", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Carina Silberer", "Mirella Lapata."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), Baltimore, Maryland, USA.", "citeRegEx": "Silberer and Lapata.,? 2014", "shortCiteRegEx": "Silberer and Lapata.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR), San Diego, CA.", "citeRegEx": "Simonyan and Zisserman.,? 2015", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Dirt cheap web-scale parallel text from the Common Crawl", "author": ["Jason Smith", "Herve Saint-Amand", "Magdalena Plamada", "Philipp Koehn", "Chris Callison-Burch", "Adam Lopez."], "venue": "Proceedings of the Conference of the Association for Computational", "citeRegEx": "Smith et al\\.,? 2013", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["Karen Sp\u00e4rck Jones."], "venue": "Journal of Documentation, 28:11\u201321.", "citeRegEx": "Jones.,? 1972", "shortCiteRegEx": "Jones.", "year": 1972}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston,Massachusetts, USA.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Learning fine-grained image similarity with deep ranking", "author": ["Jiang Wang", "Yang Song", "Thomas Leung", "Chuck Rosenberg", "Jingbin Wang", "James Philbin", "Bo Chen", "Ying Wu."], "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Scalable similarity learning using large margin neighborhood embedding", "author": ["Zhaowen Wang", "Jianchao Yang", "Zhe Lin", "Jonathan Brandt", "Shiyu Chang", "Thomas Huang."], "venue": "Proceedings of the IEEE Winter Conference on Applications of Com-", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Integrating a large, monolingual corpus as translation memory into statistical machine translation", "author": ["Katharina W\u00e4schle", "Stefan Riezler."], "venue": "Proceedings of the 18th Annual Conference of the European Association for Machine Translation (EAMT), An-", "citeRegEx": "W\u00e4schle and Riezler.,? 2015", "shortCiteRegEx": "W\u00e4schle and Riezler.", "year": 2015}, {"title": "Online multimodal deep similarity learning with application to image retrieval", "author": ["Pengcheng Wu", "Steven C.H. Hoi", "Hao Xia", "Peilin Zhao", "Dayong Wang", "Chunyan Miao."], "venue": "Proceedings of the 21st ACM International Conference on Multimedia, Barcelona,", "citeRegEx": "Wu et al\\.,? 2013", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "Proceedings of the International Confer-", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "html of tuples (Ferraro et al., 2015), the largest multilingual datasets of images and captions known to the authors contain 20,000 (Grubinger et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 12, "context": ", 2015), the largest multilingual datasets of images and captions known to the authors contain 20,000 (Grubinger et al., 2006) or 30,0002 triples of images with German and English descriptions.", "startOffset": 102, "endOffset": 126}, {"referenceID": 19, "context": "In contrast to prior work on generating captions directly from images (Kulkarni et al. (2011), Karpathy and Fei-", "startOffset": 71, "endOffset": 94}, {"referenceID": 30, "context": "Fei (2015), Vinyals et al. (2015), inter alia), our goal is to integrate visual information into an SMT pipeline.", "startOffset": 12, "endOffset": 34}, {"referenceID": 15, "context": ", the identification of relevant captions for i (Hodosh et al., 2013) is not itself part of the task of caption translation.", "startOffset": 48, "endOffset": 69}, {"referenceID": 22, "context": "The dataset used at the WMT16 shared task is based on translations of Flickr30K captions (Rashtchian et al., 2010).", "startOffset": 89, "endOffset": 114}, {"referenceID": 9, "context": "Recent approaches also employ reranking of image captions by measuring similarity between image and text using deep representations (Fang et al., 2015).", "startOffset": 132, "endOffset": 151}, {"referenceID": 28, "context": "We rely on the CNN framework (Socher et al., 2014; Simonyan and Zisserman, 2015) to solve semantic classification and disambiguation tasks in NLP with the help of supervision signals from visual feedback.", "startOffset": 29, "endOffset": 80}, {"referenceID": 26, "context": "We rely on the CNN framework (Socher et al., 2014; Simonyan and Zisserman, 2015) to solve semantic classification and disambiguation tasks in NLP with the help of supervision signals from visual feedback.", "startOffset": 29, "endOffset": 80}, {"referenceID": 15, "context": "(2011), Karpathy and Fei-Fei (2015), Vinyals et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 15, "context": "(2011), Karpathy and Fei-Fei (2015), Vinyals et al. (2015), inter alia).", "startOffset": 8, "endOffset": 59}, {"referenceID": 32, "context": "In the area of SMT, W\u00e4schle and Riezler (2015) presented a framework for integrating a large, indomain, target-side monolingual corpus into machine translation by making use of techniques from crosslingual information retrieval.", "startOffset": 20, "endOffset": 47}, {"referenceID": 2, "context": "Calixto et al. (2012) suggest using images as supplementary context information for statistical machine translation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "In parallel to our work, Elliott et al. (2015) addressed the problem of caption transla-", "startOffset": 25, "endOffset": 47}, {"referenceID": 24, "context": "For example, Silberer and Lapata (2014) show that distributional word embeddings grounded in visual representations outperform competitive baselines on term similarity scoring and word categorization tasks.", "startOffset": 13, "endOffset": 40}, {"referenceID": 17, "context": "The orthogonality of visual feedback has previously been exploited in a multilingual setting by Kiela et al. (2015) (relying on previous work by Bergsma and Van Durme (2011)), who induce a bilingual lexicon using term-specific multimodal representations obtained by querying the Google image", "startOffset": 96, "endOffset": 116}, {"referenceID": 17, "context": "The orthogonality of visual feedback has previously been exploited in a multilingual setting by Kiela et al. (2015) (relying on previous work by Bergsma and Van Durme (2011)), who induce a bilingual lexicon using term-specific multimodal representations obtained by querying the Google image", "startOffset": 96, "endOffset": 174}, {"referenceID": 8, "context": "We replicated the results of Elliott et al. (2015) on the IAPR TC-12 data.", "startOffset": 29, "endOffset": 51}, {"referenceID": 11, "context": "4 Funaki and Nakayama (2015)", "startOffset": 2, "endOffset": 29}, {"referenceID": 11, "context": "based visual similarity information is used as a \u201chub\u201d (Funaki and Nakayama, 2015) or pivot connecting corpora in two natural languages which lack direct parallelism, a strategy which we apply to the problem of caption translation.", "startOffset": 55, "endOffset": 82}, {"referenceID": 33, "context": "Following the basic approach set out by W\u00e4schle and Riezler (2015), we use a crosslingual retrieval model to find sentences in a target language document collection C, and use these to rerank target language translations e of a source caption f .", "startOffset": 40, "endOffset": 67}, {"referenceID": 33, "context": "The systems described in our work differ from that of W\u00e4schle and Riezler (2015) in a number of aspects.", "startOffset": 54, "endOffset": 81}, {"referenceID": 33, "context": "This function is purely text-based and does not make use of multimodal context information (as such, it comes closest to the models used for target-side retrieval in W\u00e4schle and Riezler (2015)).", "startOffset": 166, "endOffset": 193}, {"referenceID": 20, "context": "This function makes use of the object annotations available for the MS COCO corpus (Lin et al., 2014) to give an indication of the effectiveness of our automatically extracted visual similarity metric.", "startOffset": 83, "endOffset": 101}, {"referenceID": 18, "context": "Term frequencies were computed on monolingual data from Europarl (Koehn, 2005) and the News Commentary and News Discussions English datasets provided for the WMT15 workshop.", "startOffset": 65, "endOffset": 78}, {"referenceID": 24, "context": "Our visual distance measure v was computed using the VGG16 deep convolutional model of Simonyan and Zisserman (2015), which was pretrained on ImageNet (Russakovsky et al., 2014).", "startOffset": 151, "endOffset": 177}, {"referenceID": 25, "context": "Our visual distance measure v was computed using the VGG16 deep convolutional model of Simonyan and Zisserman (2015), which was pretrained on ImageNet (Russakovsky et al.", "startOffset": 87, "endOffset": 117}, {"referenceID": 20, "context": "We constructed a German-English parallel dataset based on the MS COCO image corpus (Lin et al., 2014).", "startOffset": 83, "endOffset": 101}, {"referenceID": 7, "context": "performed compound-splitting using the method described by Dyer (2009), as implemented by the cdec utility compound-split.", "startOffset": 59, "endOffset": 71}, {"referenceID": 3, "context": "using synchronous context free grammars (Chiang, 2007), as implemented by the cdec decoder (Dyer et al.", "startOffset": 40, "endOffset": 54}, {"referenceID": 6, "context": "using synchronous context free grammars (Chiang, 2007), as implemented by the cdec decoder (Dyer et al., 2010).", "startOffset": 91, "endOffset": 110}, {"referenceID": 18, "context": "Data from the Europarl (Koehn, 2005), News Commentary and Common Crawl corpora (Smith et al.", "startOffset": 23, "endOffset": 36}, {"referenceID": 27, "context": "Data from the Europarl (Koehn, 2005), News Commentary and Common Crawl corpora (Smith et al., 2013) as provided for", "startOffset": 79, "endOffset": 99}, {"referenceID": 13, "context": "monolingual data from Europarl, as well as the News Crawl and News Discussions English datasets provided for the WMT15 workshop (the same data as was used for estimating term frequencies for the retrieval models) with the KenLM toolkit (Heafield et al., 2013; Heafield, 2011).", "startOffset": 236, "endOffset": 275}, {"referenceID": 14, "context": "monolingual data from Europarl, as well as the News Crawl and News Discussions English datasets provided for the WMT15 workshop (the same data as was used for estimating term frequencies for the retrieval models) with the KenLM toolkit (Heafield et al., 2013; Heafield, 2011).", "startOffset": 236, "endOffset": 275}, {"referenceID": 21, "context": "BLEU (Papineni et al., 2002) using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003).", "startOffset": 5, "endOffset": 28}, {"referenceID": 5, "context": ", 2002) using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003).", "startOffset": 58, "endOffset": 84}, {"referenceID": 22, "context": "The English captions in this dataset belong to the Flickr30k corpus (Rashtchian et al., 2010) and are very similar to those of the MS COCO corpus.", "startOffset": 68, "endOffset": 93}, {"referenceID": 22, "context": "Significance tests on the differences in translation quality were performed using the approximate randomization technique for measuring performance differences of machine translation systems described in Riezler and Maxwell (2005) and implemented by Clark et al.", "startOffset": 204, "endOffset": 231}, {"referenceID": 4, "context": "Significance tests on the differences in translation quality were performed using the approximate randomization technique for measuring performance differences of machine translation systems described in Riezler and Maxwell (2005) and implemented by Clark et al. (2011) as part of the Multeval toolkit.", "startOffset": 250, "endOffset": 270}, {"referenceID": 33, "context": "This finding is actually consistent with W\u00e4schle and Riezler (2015) who report performance gains for text-based, target side retrieval", "startOffset": 41, "endOffset": 68}, {"referenceID": 34, "context": "Learning semantically informative distance metrics using deep learning techniques is an area under active investigation (Wu et al., 2013; Wang et al., 2014; Wang et al., 2015).", "startOffset": 120, "endOffset": 175}, {"referenceID": 31, "context": "Learning semantically informative distance metrics using deep learning techniques is an area under active investigation (Wu et al., 2013; Wang et al., 2014; Wang et al., 2015).", "startOffset": 120, "endOffset": 175}, {"referenceID": 32, "context": "Learning semantically informative distance metrics using deep learning techniques is an area under active investigation (Wu et al., 2013; Wang et al., 2014; Wang et al., 2015).", "startOffset": 120, "endOffset": 175}, {"referenceID": 15, "context": "This problem is aggravated in the multimodal case, since the relevance of captions with respect to images varies greatly between different corpora (Hodosh et al., 2013).", "startOffset": 147, "endOffset": 168}, {"referenceID": 7, "context": "A further avenue of future research is improving models such as that presented in Elliott et al. (2015) by crucial components of neural MT such as \u201cattention mechanisms\u201d.", "startOffset": 82, "endOffset": 104}, {"referenceID": 0, "context": "For example, the attention mechanism of Bahdanau et al. (2015) serves as a soft alignment that helps to guide the translation process by influencing the sequence in which source tokens are translated.", "startOffset": 40, "endOffset": 63}, {"referenceID": 0, "context": "For example, the attention mechanism of Bahdanau et al. (2015) serves as a soft alignment that helps to guide the translation process by influencing the sequence in which source tokens are translated. A similar mechanism is used in Xu et al. (2015) to decide which part of the image should influence which part of the generated caption.", "startOffset": 40, "endOffset": 249}], "year": 2016, "abstractText": "We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs. Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolingually captioned images, and on state-ofthe-art convolutional neural networks to compute image similarities. Our experimental evaluation shows improvements of 1 BLEU point over strong baselines.", "creator": "TeX"}}}