{"id": "1601.05764", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2016", "title": "A Confidence-Based Approach for Balancing Fairness and Accuracy", "abstract": "We study three incorporating machine learning computational in after necessarily related algorithmic fairness: spatial boosts, support formula_5 machines, of logistic predictive. Our goal is the requires when high predictive included any learning modes then reducing the grade already only some homosexuals beat or because of even membership 2002 a sanctuary group.", "histories": [["v1", "Thu, 21 Jan 2016 19:48:07 GMT  (122kb,D)", "http://arxiv.org/abs/1601.05764v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CY", "authors": ["benjamin fish", "jeremy kun", "\\'ad\\'am d lelkes"], "accepted": false, "id": "1601.05764"}, "pdf": {"name": "1601.05764.pdf", "metadata": {"source": "CRF", "title": "A Confidence-Based Approach for Balancing Fairness and Accuracy", "authors": ["Benjamin Fish", "Jeremy Kun", "\u00c1d\u00e1m D. Lelkes"], "emails": ["alelke2}@uic.edu"], "sections": [{"heading": null, "text": "We study three classical machine learning algorithms in the context of algorithmic fairness: adaptive boosting, support vector machines, and logistic regression. Our goal is to maintain the high accuracy of these learning algorithms while reducing the degree to which they discriminate against individuals because of their membership in a protected group.\nOur first contribution is a method for achieving fairness by shifting the decision boundary for the protected group. The method is based on the theory of margins for boosting. Our method performs comparably to or outperforms previous algorithms in the fairness literature in terms of accuracy and low discrimination, while simultaneously allowing for a fast and transparent quantification of the trade-off between bias and error.\nOur second contribution addresses the shortcomings of the bias-error trade-off studied in most of the algorithmic fairness literature. We demonstrate that even hopelessly naive modifications of a biased algorithm, which cannot be reasonably said to be fair, can still achieve low bias and high accuracy. To help to distinguish between these naive algorithms and more sensible algorithms we propose a new measure of fairness, called resilience to random bias (RRB). We demonstrate that RRB distinguishes well between our naive and sensible fairness algorithms. RRB together with bias and accuracy provides a more complete picture of the fairness of an algorithm."}, {"heading": "1 Background and Motivation", "text": "1.1 Motivation Machine learning algorithms assume an increasingly large role in making decisions across many different areas of industry, finance, and government, from facial recognition and social network analysis to self-driving cars to data-based approaches in commerce, education, and policing. The decisions made by algorithms in these domains directly affect individ-\n\u2217University of Illinois at Chicago, Department of Mathematics, Statistics, and Computer Science \u2020{bfish3, jkun2, alelke2}@uic.edu\nual people, and not always for the better. Consequently, there has been a growing concern that machine learning algorithms, which are often poorly understood by those that use them, make discriminatory decisions.\nIf the data used for training the algorithm is biased, a machine learning algorithm will learn the bias and perpetuate discriminatory decisions against groups that are protected by law, even in the absence of \u201cdiscriminatory intent\u201d by the designers. A typical example is an algorithm serving predatory ads to protected groups. Such issues resulted in a 2014 report from the US Executive Office [12] which voiced concerns about discrimination in machine learning. The primary question we study in this paper is\nHow can we maintain high accuracy of a learning algorithm while reducing discriminatory biases?\nIn this paper we will focus on the issue of biased training data, which is one of the several possible causes of discriminatory outcomes in machine learning. In this setting, we have a protected attribute (e.g. race or gender) which we assert should be independent from the target attribute. For example, if the goal is to decide creditworthiness for loans and the protected attribute is gender, a classifier\u2019s prediction should not correlate with an applicant\u2019s gender. We say that the classifier achieves statistical parity if the protected subgroup is as likely as the broader population to have a given label.\nOf course, there might be situations where the target label depends on legitimate factors that correlate with the protected attribute. For example, if the protected attribute is gender and the target label is income, some argue that lower salaries for women can be partly explained by the fact that on average, men work longer hours than women. In this paper we assume that this is not the case. The issue of \u201cexplainable discrimination\u201d in machine learning was studied in [8].\nIn our setting, since we only have biased data, we cannot evaluate our classifiers against an unbiased ground truth. In particular only a biased classifier could achieve perfect accuracy; to achieve statistical parity in general one must be willing to reduce accuracy. Hence the natural goal is to find a classifier that\nar X\niv :1\n60 1.\n05 76\n4v 1\n[ cs\n.L G\n] 2\n1 Ja\nn 20\n16\nachieves statistical parity while minimizing error, or more generally to study the trade-off between bias and accuracy so as to make favorable trade-offs.\n1.2 Contributions Our first contribution in this paper is a method for optimizing this trade-off which we call the Shifted Decision Boundary (SDB). SDB is a generic method based on the theory of margins [2, 15], and it can be combined with any learning algorithm that produces a measure of confidence in its prediction (Section 2.1). In particular we combine SDB with boosting, support vector machines, and logistic regression, and it performs comparably to or outperforms previous algorithms in the fair learning literature. See Section 3 for its empirical evaluation. We also give a theorem based on the analysis in [15] bounding the loss of accuracy for SDB under weighted majority schemes (Section 2.4). SDB makes the assumptions on the bias explicit and transparent, so that the trade-off can be understood without a detailed understanding of the learning algorithm itself.\nUnfortunately, studying the bias-error trade-off is an incomplete picture of the fairness of an algorithm. The shortcomings were discussed in [3], e.g., in terms of how an adversary could achieve statistical parity while still targeting the protected group unfairly. We demonstrate these shortcomings in action even in the absence of adversarial manipulation. Among other methods, we show that modifying a classifier by randomly flipping certain output labels with a certain probability already outperforms much of the prior fairness literature in both accuracy and bias. Such a naive algorithm is obviously unfair because the relabeling is independent of the classification task. Our second contribution is a measure of fairness that addresses this shortcoming, which we call resilience to random bias. We define it in Section 2.5 and demonstrate that it distinguishes well between our naive baseline algorithms and SDB.\n1.3 Existing notions of fairness The study of fairness in machine learning is young, but there has been a lot of disparate work studying notions of what it means for data to be fair. Finding the \u201cright\u201d definition of fairness is a major challenge; see the extensive survey of [13] for a detailed discussion. Two prominent definitions of fairness that have emerged are statistical parity and k-nearest-neighbor consistency. We review them briefly now.\nStatistical parity: Let D be a distribution over a set of labeled examples X with labels l : X \u2192 {\u22121, 1} and a protected subset S \u2282 X. The bias of l with respect to D is defined as the difference in probability of an example in S having label 1 and the probability of an\nexample in SC having label 1, i.e.\nB(D,S) = Pr x\u223cD|SC [l(x) = 1]\u2212 Pr x\u223cD|S [l(x) = 1].\nThe bias of a hypothesis h is the same quantity with h(x) replacing l(x). If a hypothesis has low bias in absolute value we say it achieves statistical parity. Note that S represents the group we wish to protect from discrimination, and the bias represents the degree to which they have been discriminated against. The sign of bias indicates whether S or SC is discriminated against. A similar statistical measure called disparate impact was introduced and studied by Friedler et al. [4] based on the \u201c80% rule\u201d used in United States hiring law.\nDwork et al. [3] point out that statistical parity is only a measure of population-wide fairness. They provide a laundry list of ways one could achieve statistical parity while still exhibiting serious and unlawful discrimination. In particular, one can achieve statistical parity by flipping the labels of a certain number of arbitrarily chosen members of the disadvantaged group, regardless of the relation between the individuals and the classification task. In our experiments we show this already outperforms some of the leading algorithms in the fairness literature.\nDespite this, it is important to study the ability for learning algorithms to achieve statistical parity. For example, it might be reasonable to flip the labels of the \u201cmost qualified\u201d individuals of the disadvantaged group who are classified negatively. Some previous approaches assume the existence of a ranking or metric on individuals, or try to learn this ranking from data [6, 3]. By contrast, our SDB achieves statistical parity without the need for such a ranking.\nkNN -consistency: The second notion, due to [3], calls a classifier \u201cindividually fair\u201d if it classifies similar individuals similarly. They use k-nearest-neighbor to measure the consistency of labels of similar individuals. Note that \u201ccloseness\u201d is defined with respect to a metric chosen as part of the data cleaning and feature selection process. By contrast SDB does not require a metric on individuals.\n1.4 Previous work on fair algorithms Learning algorithms studied previously in the context of fairness include naive Bayes [1], decision trees [7], and logistic regression [9]. To the best of our knowledge we are the first to study boosting and SVM in this context, and our confidence-based analysis is new for both these and logistic regression.\nThe two main approaches in the literature are massaging and regularization. Massaging means changing the biased dataset before training to remove the bias in the hope that the learning algorithm trained on the\nnow unbiased data will be fair. Massaging is done in the previous literature based on a ranking learned from the biased data [6]. The regularization approach consists of adding a regularizer to an optimization objective which penalizes the classifier for discrimination [10]. While SDB can be thought of as a post-processing regularization, it does so in a way that makes the trade-off between bias and accuracy transparent and easily controlled.\nThere are two other notable approaches in the fairness literature. The first, introduced in [3], is a framework for maximizing the utility of a classification with the constraint that similar people be treated similarly. One shortcoming of this approach is that it relies on a metric on the data that tells us the similarity of individuals with respect to the classification task. Moreover, the work in [3] suggests that learning a suitably fair similarity metric from the data is as hard as the original problem of finding a fair classifier. Our SDB method does not require such a metric.\nThe \u201cLearning Fair Representations\u201d method of Zemel et al. [17] formulates the problem of fairness in terms of intermediate representations: the goal is to find a representation of the data which preserves as much information as possible from the original data while simultaneously obfuscating membership in the protected class. Given that in this paper we seek to make explicit the trade-off between bias and accuracy, we will not be able to hide membership in the protected class as Zemel et al. seeks to do. Rather, we align with the central thesis of [3], that knowing the protected feature is useful to promote fairness.\n1.5 Margins The theory of margins has provided a deep, foundational explanation for the generalization properties of algorithms such as AdaBoost and softmargin SVMs [2, 15]. A hypothesis f : X \u2192 [\u22121, 1] induces a margin for a labeled example marginf (x, y) = y \u00b7 f(x), where x \u2208 X is a data point and y \u2208 {\u22121, 1} is the correct label for x. The sign of the margin is positive if and only if f correctly labels x, and the magnitude indicates how confident f is in its prediction.\nAs an example of the power of margins, we quote a celebrated theorem on the generalization accuracy of weighted majority voting schemes in PAC-learning. Here a weighted majority vote is a function f(x) =\u2211N i=1 \u03b1ihi(x) for some hypotheses hi \u2208 H and \u03b1i \u2265\n0, \u2211 i \u03b1i = 1.\nTheorem 1.1. (Schapire et al. [15]) Let D be a distribution over X \u00d7 {\u22121, 1} and S be a sample of m examples chosen i.i.d. at random according to D. Let H be a set of hypotheses of VC-dimension d. Then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 every weighted\nmajority voting scheme satisfies the following for every \u03b8 > 0:\nPr D [yf(x) \u2264 0] \u2264 Pr S [yf(x) \u2264 \u03b8]+\nO ( 1\u221a m ( d log2(m/d) \u03b82 + log(1/\u03b4) )1/2)\nIn other words, the generalization error is bounded by the probability of a small margin on the sample. One can go on to show AdaBoost [14], a popular algorithm that produces a weighted voting scheme, performs well in this respect. Recall that the output of AdaBoost is a hypothesis which outputs the sign of a weighted majority vote \u2211 i \u03b1i, hi(x). Rather than measure the margin we measure the signed confidence of the boosting hypothesis on an unlabeled example x as\nconf(x) = \u2211T i=1 \u03b1ihi(x)\u2211T\ni=1 \u03b1i .\nThe magnitude of the confidence measures the agreement of the voters in their classification of an example.\nThe theoretical work on margins for boosting suggests that examples with small confidence are more likely to have incorrect labels than examples with large confidence. For example, we display in Figure 1 the signed confidence values for all examples and incorrectly predicted examples respectively. The incorrect examples have confidence centered around zero. One can leverage this for fairness by flipping negative labels of members of the protected class with a small confidence value. This is a rough sketch of the SDB method. The empirical results of SDB suggest that SDB achieves statistical parity with relatively little loss in accuracy. Indeed, we state a similar guarantee to Theorem 1.1 in Section 2.4 that solidifies this intuition.\nThe idea of a signed confidence generalizes nicely to other machine learning algorithms. We study support vector machines (SVM) which have a natural geometric notion of margin, and logistic regression which outputs a confidence in its prediction. For background on SVM, logistic regression, and AdaBoost, see [16].\n1.6 Interpretations of signed confidence Here we state how signed confidence is defined for each of the learning methods.\n1.6.1 AdaBoost Boosting algorithms work by combining base hypotheses, \u201crules of thumb\u201d that have a fixed edge over random guessing, into highly accurate predictors. In each round, a boosting algorithm finds the base hypothesis that achieves the smallest weighted error on the sample. It then increases the weights of\nthe incorrectly classified examples, thus forcing the base learner to improve the classification of difficult examples. In this paper we study AdaBoost, a ubiquitous boosting algorithm. For more on boosting, we refer the reader to [14].\nLet H be a set of base classifiers, and let (\u03b1t, ht) T t=1 be the weights and hypotheses output by AdaBoost after T rounds. The signed confidence of the hypothesis\nis conf(x) = \u2211T i=1 \u03b1ihi(x)\u2211T\ni=1 \u03b1i . In all of our experiments we\nboost decision stumps for T = 20 rounds.\n1.6.2 SVM The soft-margin SVM of Vapnik [2] outputs a maximum margin hyperplane w in a highdimensional space implicitly defined by a kernel K, and w can be expressed implicitly as a linear combination of the input vectors, say w\u2032. We define the confidence as the distance of a point from the separating hyperplane, i.e. conf(x) = K(w\u2032,x). For the Census Income and Singles datasets we use the standard Gaussian kernel, and for the German dataset we use a linear kernel (the datasets are described in Section 3).\n1.6.3 Logistic regression The classifier output by logistic regression has the form\nh(x) = sign(\u03c6(\u3008w,x\u3009)\u2212 1/2)\nwhere \u03c6(z) = 11+e\u2212z is the logistic function, and the vector w is found by empirical risk minimization (ERM)\nwith the standard logistic loss `(w, (x, y)) = log(1 + e\u2212y\u3008w,x\u3009) and L2 regularization. Here we define the confidence of logistic regression simply as the value that the classifier takes before rounding: conf(x) = \u03c6(\u3008w,x\u3009)."}, {"heading": "2 Methods and Technical Solutions", "text": "2.1 Shifted decision boundary In this section we define our methods. In what follows X is a labeled dataset, l(x) are the given labels, and S \u2282 X is the protected group. We further assume that members of S are less likely than SC to have label 1. First we describe our proposed method, called shifted decision boundary (SDB), and then we describe three techniques we use for baseline comparisons (in addition to comparing to previous literature).\nLet conf : X \u2192 [\u22121, 1] be a function corresponding to a classifier h(x) = sign(conf(x)), and define the decision boundary shift of \u03bb for S as the classifier h\u03bb : X \u2192 {\u22121, 1}, defined as\nh\u03bb(x) = { 1 if x \u2208 S, conf(x) \u2265 \u2212\u03bb sign(conf(x)) otherwise.\nThe SDB algorithm accepts as input confidences conf and finds the minimal error decision boundary shift for S that achieves statistical parity. That is, given conf and \u03b5 > 0, it produces a value \u03bb such that h\u03bb has minimal error subject to achieving statistical parity up to bias \u03b5.\n2.2 Naive baseline algorithms We define two naive baseline methods which are intended to be both baseline comparisons for our SDB algorithm and illustrations of the shortcomings of the bias-error trade-off.\nSimilarly to SDB, the random relabeling (RR) algorithm modifies a given hypothesis h by flipping labels. In particular, RR computes the probability p for which, if members of S with label \u22121 under h are flipped by h\u2032 to +1 randomly and independently with probability p, the bias of h\u2032 is zero in expectation. The classifier h\u2032 is then defined as the randomized classifier that flips members of S with label \u22121 with probability p and otherwise is the same as h.\nNext, we define random massaging (RM). Massaging strategies, introduced by [6], involve eliminating the bias of the training data by modifying the labels of data points, and then training a classifier on this data in the hope that the statistical parity of the training data will generalize to the test set as well. In our experiment, we massage the data randomly; i.e. we flip the labels of S from \u22121 to +1 independently at random with the probability needed to achieve statistical parity in expec-\ntation, as in RR. As we have already noted, these two baseline methods perform comparably to much of the previous literature in both bias and error. This illustrates that the semantics of why an algorithm achieves statistical parity is crucial part of its evaluation. As such, these two baselines can be useful for any analysis that measures bias and accuracy. Moreover, they can be used to determine the suitability of a new proposed measure of fairness.\n2.3 Fair weak learning Finally, we include a method which is based on a natural idea but is empirically suboptimal to SDB. Recall that boosting works by combining weak learners into a \u201cstrong\u201d classifier. It is natural to ask whether boosting keeps the fairness properties of the weak learners. Weak learners used in practice, such as decision stumps, have very low complexity, therefore it is easy to impose fairness constraints on them. In our fair weak learning (FWL) baseline we replace a standard boosting weak learner with one which tries to minimize a linear combination of error and bias and run the resulting boosting algorithm unchanged. The weak learner we use computes the decision stump which minimizes the sum of label error and bias of its induced hypothesis.\n2.4 Theoretical properties of SDB Because the SDB method only flips the labels of examples with small signed confidence, margin theory implies that it will not increase the error too much. We formalize this precisely below. This theorem, a direct corollary of Theorem 1.1, provides strong theoretical justification for our SDB method. To the best of our knowledge, SDB is the first empirically tested method for fair learning that has any specific guarantees for its accuracy.\nInformally, the theorem says that when a majority voting scheme is post-processed by the SDB technique, the resulting hypothesis maintains the generalization accuracy bounds in terms of the margin on the sample when the shift is small (\u03bb \u2264 \u03b8). But as the shift grows, the error bound increases proportionally to the fraction of the protected population that has large enough negative margins (i.e., in [\u2212\u03bb,\u2212\u03b8]).\nTheorem 2.1. Let X be finite and D,S,m,H, and d be as in Theorem 1.1. Let T \u2282 S be the subset of the sample in the protected class. Let \u03b4 > 0. Let err(m) be the tail error function from Theorem 1.1. For any A \u2282 X let A\u03bb,\u03b8 = {a \u2208 A : \u2212\u03bb \u2264 conf(a) \u2264 \u2212\u03b8}.\nThen with probability at least 1 \u2212 \u03b4, every function h\u03bb post-processed by SDB with weighted majority vote conf(x) and shift \u03bb > 0 satisfies the following for every\n\u03b8 > 0:\nPr D [yh\u03bb(x) \u2264 0] \u2264 Pr T\u03bb,\u03b8 [y \u00b7 conf(x) \u2265 \u2212\u03b8] Pr S [x \u2208 T\u03bb,\u03b8]\n+ Pr S\u2212T\u03bb,\u03b8 [y \u00b7 conf(x) \u2264 \u03b8] Pr S [x 6\u2208 T\u03bb,\u03b8] + max(err(|T\u03bb,\u03b8|), err(|TC\u03bb,\u03b8|))\nProof. The bound follows by conditioning on the event that h\u03bb flips the label, noticing \u2212 conf(x) is also a majority function, and applying Theorem 1.1 twice.\n2.5 Resilience to random bias One of the biggest challenges for designers of fair learning algorithms is the lack of good measures of fairness. The most popular measures are statistical measures of bias such as statistical parity. As Dwork et al. [3] have pointed out, statistical parity fails to capture all important aspects of fairness. In particular, it is easy to achieve statistical parity simply by flipping the labels of an arbitrary set of individuals in the protected class. A real-world example would be giving a raise to a random group of women to eliminate the gender disparity in wages. The root cause of this problem is that one does not have access to reliable (unbiased) ground truth labels. We propose to compensate for this by evaluating algorithms on synthetic bias. In doing this we make transparent the kind of bias a claimed \u201cfair\u201d algorithm protects against, and we can accurately measure its resilience to said bias.\nWe introduce a new notion of fairness called resilience to random bias (RRB). Informally we introduce a new, random feature which has no correlation with the target attribute, and then we introduce bias against individuals which have a certain value for this new feature. We call an algorithm fair if it can recover the original, unbiased labels. For RRB in particular, the synthetic bias is i.i.d. random against the protected group.\nCertainly, in practice, bias may not be of this form and we do not pretend that this notion captures all forms of bias. Rather, this notion seeks to model a comparatively mild form of bias \u2013 if an algorithm cannot recover from this type of random bias against a protected class then there is little reason to think it can handle other types of bias. In other words, we propose this as a minimally necessary condition but not necessarily a sufficient condition for individual fairness. Relating our RRB measure more formally to other notions of individual fairness is left for future work.\nWe formally define RRB as follows. Let X be a set of examples and D be a distribution over examples, with l : X \u2192 {\u22121, 1} a target labeling function. We first define a randomized process mapping (X,D, l) \u2192 (X\u0303, D\u0303, l\u0303). Let X\u0303 = X \u00d7 {\u22121, 1} and D\u0303 be the distribution on X\u0303 which is independently D on the X coordinate and uniform on the {\u22121, 1} coordinate.\nDenote by X\u03030 = {(x, b) \u2208 X\u0303 | b = 0} and call this the protected set. Finally, l\u0303(x, b) is fixed to either l(x) or \u2212l(x) independently at random for each (x, b) \u2208 X\u0303 according to the following:\nPr[l\u0303(x, b) = l(x)] = { 1 if b = 1 or l(x) = \u22121 1\u2212 \u03b7 if b = 0 and l(x) = 1 .\nIn other words, the positive labels of a randomly chosen protected subgroup are flipped to negative independently at random with probability \u03b7. We emphasize that the process mapping l 7\u2192 l\u0303 is randomized, but the map l\u0303(x, b) itself is fixed and deterministic. So an algorithm which queries labels from l\u0303 is given consistent answers. Now we define the resilience to random bias as follows:\nDefinition 1. Let (X,D, l), (X\u0303, D\u0303, l\u0303) be as above. Let h = A(D\u0303, l\u0303) be the output classifier of a learning algorithm A when given biased data as input. The resilience to random bias (RRB) of A with respect to (X,D, l) and discrimination rate 0 \u2264 \u03b7 < 1/2, denoted RRB\u03b7(A), is\nRRB\u03b7(A) = Pr D\u0303\n[h(x, b) = l(x) | b = 0, l(x) = 1]\nSimilarly to calculating statistical parity, RRB is estimated on a fixed dataset by simulating the process described above and outputing an empirical average."}, {"heading": "3 Empirical Evaluation", "text": "We measure our methods on label error, statistical parity, and RRB with \u03b7 = 0.2. In all of our experiments we split the datasets randomly into training, test, and model-selection subsets, and we output the average of 10 experiments.1\n3.1 Datasets The Census Income dataset [11], extracted from the 1994 Census database, contains demographic information about 48842 American adults. The prediction task is to determine whether a person earns over $50K a year. The dataset contains 16, 192 females (33%) and 32, 650 males. Note 30.38% of men and 10.93% of women reported earnings of more than $50K, therefore the bias of the dataset is 19.45%.\nThe German credit dataset [11] contains financial information about 1000 individuals who are classified into groups of good and bad credit risk. The \u201cgood\u201d credit group contains 699 individuals. Following the work of [6], we consider age as the protected attribute with a cut-off at 25. Only 59% of the younger people are\n1The code is available for reproducibility at https://github. com/j2kun/fkl-SDM16\nconsidered good credit risk, whereas of the 25 or older group, 72% are creditworthy, making the bias 13%.\nIn the Singles dataset, extracted from the marketing dataset of [5] by taking all respondents who identified as \u201csingle,\u201d the goal is to predict whether annual income of a household is greater than $25K from 13 other demographic attributes. The protected attribute is gender. The dataset contains 3, 653 data points, 1, 756 (48%) of which belong to the protected group. 34% of the dataset has a positive label. The bias is 9.8%.\n3.2 Results and analysis In this section we state our experimental results. They are summarized in Figure 2 for the Census Income, German, and Singles datasets, and the full set of numbers are in Tables 2, 3, and 4 respectively. For comparison, we also included the numbers for the Learning Fair Representations (LFR) method of [17] for the Census Income dataset, for Classification with No Discrimination (CND) method of [6], and for the Discrimination Aware Decision Tree (DADT) technique of [7] (specifically we use the numbers for the \u201cIGC+IGS Relab\u201d method). In [17] the authors implemented three other learning algorithms, these are unregularized logistic regression, Fair Naive-Bayes [6], and Regularized Logistic Regression [10]. These methods all had errors above 20% on the Census dataset and so we omit them for brevity. In [7] the authors implemented variations on the decision tree learning scheme, and the one we include has the highest accuracy, though they are all closely comparable. We reported all biases as unsigned. We were unable to access implementations of the prior authors\u2019\nalgorithms, so we were not able to reproduce their results or measure their algorithms with respect to RRB.\nTo investigate the trade-offs made by our SDB method more closely, Figures 3, 4, and 5 show the rate at which error increases as bias goes to zero. In many cases, a substantial reduction in bias can be achieved before there is any significant drop-off in accuracy.\nFor the Census Income dataset, the three SDB techniques outperform the baselines and outperform all the prior literature except for DADT. Both SDB algorithms achieve statistical parity with about 18% error. Moreover, these two SDB algorithms have the highest RRB, while SVM appears to overfit the random bias introduced by RRB more than the other algorithms. While DADT appears to achieve lower label error and comparable bias, we note that the standard deviation of the bias reported in [7] is 0.015 while for SDB (on the Census Income dataset) the standard deviations are at least one order of magnitude smaller.\nThe singles dataset shows a similar pattern, with SDB combined with logistic regression outperforming all other baselines. Note that in the instances where the baselines perform comparably to SDB, SDB tends to have a much larger resilience to random bias.\nThe German dataset is more puzzling. While two of the SDB techniques outperform the prior literature by a moderate margin, they do not outperform random relabeling or random massaging by a significant margin (and these baselines already outperform CND). Another curious observation is that label error stays constant as the decision boundary is shifted, as Figure 4 shows.\nNote again the difference in SVM kernels between the datasets. The Gaussian kernel performs well for the Census Income and Singles dataset. However, in the case of the German dataset, which is the smallest of the\nthree, with the Gaussian kernel every point becomes a support vector. This is not only a clear sign of overfitting but it also makes SDB useless since the model gives the same confidence for almost every data point.\nThese facts seem to be evidence that the German dataset (which has only about a thousand records) is too small to draw a significant conclusion. We nevertheless include it here for completeness and to show comparison with the previous literature.\nFair weak learning (FWL) does empirically reduce bias but does not achieve statistical parity in two of the three data sets. FWL performs worse on either label error or bias on each of the data sets and the trade-off between label error and bias cannot easily be controlled. It also does not seem easy to control this trade-off using either random massaging and random relabeling.\nOne notable advantage of SDB is that the tradeoff between label error and bias can be controlled after training. To decide how much bias and error we want to allow, we do not have to pick a hyper-parameter before training the algorithm, unlike for most other fair learning methods. This means that the computational cost of choosing the best point on the trade-off curve is very low, and the trade-off is transparent.\nThe results also highlight the usefulness of RRB as a measure of fairness. The RRB values across all datasets and algorithms we studied are in Table 1. In cases where random relabeling or random massaging performs comparably to SDB, the RRB measure is able to distinguish them, giving a lower score to the less reasonable baselines and a higher score to SDB. This suggests that the performance of fair learning algorithms should not be evaluated solely by their accuracy and bias."}, {"heading": "4 Significance and Impact", "text": "In this paper, we introduced a general method for balancing discrimination and label error. This method, which we call shifted decision boundary (SDB), is applicable to any learning algorithm which has an efficiently computable measure of confidence. We studied three such algorithms \u2013 AdaBoost, support vector machines, and linear regression \u2013 compared our methods to other methods proposed in the earlier literature and our own baselines, and empirically evaluated our methods\u2019 performances in terms of their resilience to random bias.\nOur method, in addition to outperforming much of the previous literature, has several other desirable properties. Unlike most other fair learning algorithms, SDB applied to AdaBoost has theoretical bounds on generalization error. Also, since the margin shift can be specified after the original learner has been trained on the data, a practitioner can easily evaluate the trade-off between error and bias and choose the most desirable point on the trade-off curve. This makes SDB a fast and transparent way to study the fairness properties of an algorithm.\nOur resilience to random bias (RRB) measure is a novel approach to evaluate the fairness of a learning algorithm. Although i.i.d. random bias is a simplified model of real-world discrimination, we posit that any algorithm which can be considered fair must be fair with respect to RRB. Moreover, RRB generalizes to an arbitrary distribution over the input data, and one could adapt it to well-studied models of bias in social science.\nAcknowledments\nWe would like to thank Lev Reyzin for helpful discussions."}], "references": [{"title": "Three naive bayes approaches for discrimination-free classification", "author": ["Toon Calders", "Sicco Verwer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Certifying and removing disparate impact", "author": ["Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "venue": "Proceedings of the 21th ACM SIGKDD Intl. Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Classifying without discriminating", "author": ["Faisal Kamiran", "Toon Calders"], "venue": "In 2nd Intl. Conference on Computer, Control and Communication,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Discrimination aware decision tree learning", "author": ["Faisal Kamiran", "Toon Calders", "Mykola Pechenizkiy"], "venue": "IEEE 10th Intl. Conference on Data Mining (ICDM),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Quantifying explainable discrimination and removing illegal discrimination in automated decision making", "author": ["Faisal Kamiran", "Indr\u0117 \u017dliobait\u0117", "Toon Calders"], "venue": "Knowledge and Information Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Fairness-aware classifier with prejudice remover regularizer", "author": ["Toshihiro Kamishima", "Shotaro Akaho", "Hideki Asoh", "Jun Sakuma"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Fairness-aware learning through regularization approach", "author": ["Toshihiro Kamishima", "Shotaro Akaho", "Jun Sakuma"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "A multidisciplinary survey on discrimination analysis", "author": ["Andrea Romei", "Salvatore Ruggieri"], "venue": "The Knowledge Engineering Review, 29:582\u2013638,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Boosting: Foundations and Algorithms", "author": ["Robert E. Schapire", "Yoav Freund"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["Robert E. Schapire", "Yoav Freund", "Peter Bartlett", "Wee Sun Lee"], "venue": "Annals of Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "The issue of \u201cexplainable discrimination\u201d in machine learning was studied in [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 10, "context": "SDB is a generic method based on the theory of margins [2, 15], and it can be combined with any learning algorithm that produces a measure of confidence in its prediction (Section 2.", "startOffset": 55, "endOffset": 62}, {"referenceID": 10, "context": "We also give a theorem based on the analysis in [15] bounding the loss of accuracy for SDB under weighted majority schemes (Section 2.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "The shortcomings were discussed in [3], e.", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "Finding the \u201cright\u201d definition of fairness is a major challenge; see the extensive survey of [13] for a detailed discussion.", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "[4] based on the \u201c80% rule\u201d used in United States hiring law.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] point out that statistical parity is only a measure of population-wide fairness.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Some previous approaches assume the existence of a ranking or metric on individuals, or try to learn this ranking from data [6, 3].", "startOffset": 124, "endOffset": 130}, {"referenceID": 1, "context": "Some previous approaches assume the existence of a ranking or metric on individuals, or try to learn this ranking from data [6, 3].", "startOffset": 124, "endOffset": 130}, {"referenceID": 1, "context": "kNN -consistency: The second notion, due to [3], calls a classifier \u201cindividually fair\u201d if it classifies similar individuals similarly.", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "4 Previous work on fair algorithms Learning algorithms studied previously in the context of fairness include naive Bayes [1], decision trees [7], and logistic regression [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "4 Previous work on fair algorithms Learning algorithms studied previously in the context of fairness include naive Bayes [1], decision trees [7], and logistic regression [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "4 Previous work on fair algorithms Learning algorithms studied previously in the context of fairness include naive Bayes [1], decision trees [7], and logistic regression [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "Massaging is done in the previous literature based on a ranking learned from the biased data [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "The regularization approach consists of adding a regularizer to an optimization objective which penalizes the classifier for discrimination [10].", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "The first, introduced in [3], is a framework for maximizing the utility of a classification with the constraint that similar people be treated similarly.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Moreover, the work in [3] suggests that learning a suitably fair similarity metric from the data is as hard as the original problem of finding a fair classifier.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "Rather, we align with the central thesis of [3], that knowing the protected feature is useful to promote fairness.", "startOffset": 44, "endOffset": 47}, {"referenceID": 10, "context": "5 Margins The theory of margins has provided a deep, foundational explanation for the generalization properties of algorithms such as AdaBoost and softmargin SVMs [2, 15].", "startOffset": 163, "endOffset": 170}, {"referenceID": 10, "context": "[15]) Let D be a distribution over X \u00d7 {\u22121, 1} and S be a sample of m examples chosen i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "One can go on to show AdaBoost [14], a popular algorithm that produces a weighted voting scheme, performs well in this respect.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "For background on SVM, logistic regression, and AdaBoost, see [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": "For more on boosting, we refer the reader to [14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "Massaging strategies, introduced by [6], involve eliminating the bias of the training data by modifying the labels of data points, and then training a classifier on this data in the hope that the statistical parity of the training data will generalize to the test set as well.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "[3] have pointed out, statistical parity fails to capture all important aspects of fairness.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Following the work of [6], we consider age as the protected attribute with a cut-off at 25.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "For comparison, we also included the numbers for the Learning Fair Representations (LFR) method of [17] for the Census Income dataset, for Classification with No Discrimination (CND) method of [6], and for the Discrimination Aware Decision Tree (DADT) technique of [7] (specifically we use the numbers for the \u201cIGC+IGS Relab\u201d method).", "startOffset": 193, "endOffset": 196}, {"referenceID": 4, "context": "For comparison, we also included the numbers for the Learning Fair Representations (LFR) method of [17] for the Census Income dataset, for Classification with No Discrimination (CND) method of [6], and for the Discrimination Aware Decision Tree (DADT) technique of [7] (specifically we use the numbers for the \u201cIGC+IGS Relab\u201d method).", "startOffset": 265, "endOffset": 268}, {"referenceID": 3, "context": "In [17] the authors implemented three other learning algorithms, these are unregularized logistic regression, Fair Naive-Bayes [6], and Regularized Logistic Regression [10].", "startOffset": 127, "endOffset": 130}, {"referenceID": 7, "context": "In [17] the authors implemented three other learning algorithms, these are unregularized logistic regression, Fair Naive-Bayes [6], and Regularized Logistic Regression [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 4, "context": "In [7] the authors implemented variations on the decision tree learning scheme, and the one we include has the highest accuracy, though they are all closely comparable.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "While DADT appears to achieve lower label error and comparable bias, we note that the standard deviation of the bias reported in [7] is 0.", "startOffset": 129, "endOffset": 132}], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}