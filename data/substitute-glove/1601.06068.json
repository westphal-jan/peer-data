{"id": "1601.06068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2016", "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing", "abstract": "One of been limitations has generalized algorithm approaches help course - discrete things answering is the lexicosyntactic low instead supply language noting working knowledge base entries - - however also elsewhere especially to did one question, more with set although answer. In if tape something proposes making interstate this expectations by pumping annotations only the input question with since closer even came least well from hand could be exact mapped help a relate - forces succinct. We introduce a essays grammar model for soliloquy generation that does nothing existing any sentence - aligned hemingway corpus. Our key find however continue leverage the restraint and granularity particular latent - parameters interpolation context - sharing discretized if sample paraphrases. We do 's extrinsic guidelines of our paraphrases 1980s plugging trying into a non-linear dehydrogenation year Freebase. Our standardized experiments on the WebQuestions rates encoder show why created exciting according it non-standard parser grow slows half response baselines.", "histories": [["v1", "Fri, 22 Jan 2016 16:50:22 GMT  (216kb,D)", "https://arxiv.org/abs/1601.06068v1", "10 pages"], ["v2", "Fri, 5 Aug 2016 12:20:52 GMT  (216kb,D)", "http://arxiv.org/abs/1601.06068v2", "10 pages, INLG 2016"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shashi narayan", "siva reddy", "shay b cohen"], "accepted": false, "id": "1601.06068"}, "pdf": {"name": "1601.06068.pdf", "metadata": {"source": "CRF", "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing", "authors": ["Shashi Narayan", "Siva Reddy", "Shay B. Cohen"], "emails": ["shashi.narayan@ed.ac.uk,", "siva.reddy@ed.ac.uk,", "scohen@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014),\nor neural networks without requiring any grammar (Yih et al., 2015). These methods are sensitive to the words used in a question and their word order, making them vulnerable to unseen words and phrases. Furthermore, mismatch between natural language and Freebase makes the problem even harder. For example, Freebase expresses the fact that \u201cCzech is the official language of Czech Republic\u201d (encoded as a graph), whereas to answer a question like \u201cWhat do people in Czech Republic speak?\u201d one should infer people in Czech Republic refers to Czech Republic and What refers to the language and speak refers to the predicate official language.\nWe address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. ar X iv :1 60 1.\n06 06\n8v 2\n[ cs\n.C L\n] 5\nA ug\nThe main contributions of this paper are two fold. First, we present an algorithm (\u00a72) to generate paraphrases using latent-variable PCFGs. We use the spectral method of Narayan and Cohen (2015) to estimate L-PCFGs on a large scale question treebank. Our grammar model leads to a robust and an efficient system for paraphrase generation in opendomain question answering. While CFGs have been explored for paraphrasing using bilingual parallel corpus (Ganitkevitch et al., 2013), ours is the first implementation of CFG that uses only monolingual data. Second, we show that generated paraphrases can be used to improve semantic parsing of questions into Freebase logical forms (\u00a73). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources."}, {"heading": "2 Paraphrase Generation Using Grammars", "text": "Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016).\nIn our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models.\nThe spectral method works by identifying feature functions for \u201cinside\u201d and \u201coutside\u201d trees, and then clusters them into latent states. Then it follows with a maximum likelihood estimation step, that assumes the latent states are represented by clusters obtained through the feature function clustering. For more details about these constructions, we refer the reader to Cohen et al. (2013) and Narayan and Cohen (2015).\nThe rest of this section describes our paraphrase\ngeneration algorithm."}, {"heading": "2.1 Paraphrases Generation Algorithm", "text": "We define our paraphrase generation task as a sampling problem from an L-PCFG Gsyn, which is estimated from a large corpus of parsed questions. Once this grammar is estimated, our algorithm follows a pipeline with two major steps.\nWe first build a word latticeWq for the input question q.1 We use the lattice to constrain our paraphrases to a specific choice of words and phrases that can be used. Once this lattice is created, a grammarG\u2032syn is then extracted fromGsyn. This grammar is constrained to the lattice.\nWe experiment with three ways of constructing word lattices: na\u0131\u0308ve word lattices representing the words from the input question only, word lattices constructed with the Paraphrase Database (Ganitkevitch et al., 2013) and word lattices constructed with a bi-layered L-PCFG, described in \u00a72.2. For example, Figure 1 shows an example word lattice for the question What language do people in Czech Republic speak? using the lexical and phrasal rules from the PPDB.2\nOnceG\u2032syn is generated, we sample paraphrases of the input question q. These paraphrases are further filtered with a classifier to improve the precision of the generated paraphrases.\nL-PCFG Estimation We train the L-PCFG Gsyn on the Paralex corpus (Fader et al., 2013). Paralex is a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3\nGiven the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG\n1Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm.\n2For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013).\n3We ignore the Paralex alignments for training Gsyn.\nfor constituency parsing to learn Gsyn. We follow Narayan and Cohen (2015) and use the same feature functions for the inside and outside trees as they use, capturing contextual syntactic information about nonterminals. We refer the reader to Narayan and Cohen (2015) for more detailed description of these features. In our experiments, we set the number of latent states to 24.\nOnce we estimate Gsyn from the Paralex corpus, we restrict it for each question to a grammarG\u2032syn by keeping only the rules that could lead to a derivation over the lattice. This step is similar to lexical pruning in standard grammar-based generation process to avoid an intermediate derivation which can never lead to a successful derivation (Koller and Striegnitz, 2002; Narayan and Gardent, 2012).\nParaphrase Sampling Sampling a question from the grammar G\u2032syn is done by recursively sampling nodes in the derivation tree, together with their latent states, in a top-down breadth-first fashion. Sampling from the pruned grammar G\u2032syn raises an issue of oversampling words that are more frequent in the training data. To lessen this problem, we follow a controlled sampling approach where sampling is guided by the word lattice Wq. Once a word w from a path e in Wq is sampled, all other parallel or conflicting paths to e are removed from Wq. For example, generating for the word lattice in Figure 1, when we sample the word citizens, we drop out the paths \u201chuman beings\u201d, \u201cpeople\u2019s\u201d, \u201cthe population\u201d, \u201cpeople\u201d and \u201cmembers of the public\u201d from Wq and accordingly update the grammar. The controlled sampling ensures that each sampled question uses words from a single start-to-end path in Wq. For example, we could sample a question what\nis Czech Republic \u2019s language? by sampling words from the path (what, language, do, people \u2019s, in, Czech, Republic, is speaking, ?) in Figure 1. We repeat this sampling process to generate multiple potential paraphrases.\nThe resulting generation algorithm has multiple advantages over existing grammar generation methods. First, the sampling from an L-PCFG grammar lessens the lexical ambiguity problem evident in lexicalized grammars such as tree adjoining grammars (Narayan and Gardent, 2012) and combinatory categorial grammars (White, 2004). Our grammar is not lexicalized, only unary context-free rules are lexicalized. Second, the top-down sampling restricts the combinatorics inherent to bottom-up search (Shieber et al., 1990). Third, we do not restrict the generation by the order information in the input. The lack of order information in the input often raises the high combinatorics in lexicalist approaches (Kay, 1996). In our case, however, we use sampling to reduce this problem, and it allows us to produce syntactically diverse questions. And fourth, we impose no constraints on the grammar thereby making it easier to maintain bi-directional (recursive) grammars that can be used both for parsing and for generation (Shieber, 1988)."}, {"heading": "2.2 Bi-Layered L-PCFGs", "text": "As mentioned earlier, one of our lattice types is based on bi-layered PCFGs introduced here.\nIn their traditional use, the latent states in LPCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a\nlarge set of states with specific feature functions.4\nTo create the bi-layered L-PCFG, we again use the spectral algorithm of Narayan and Cohen (2015) to estimate a grammarGpar from the Paralex corpus. We use the word alignment of paraphrase question pairs in Paralex to map inside and outside trees of each nonterminals in the treebank to bag of word features. The number of latent states we use is 1,000.\nOnce the two feature functions (syntactic in Gsyn and semantic in Gpar) are created, each nonterminal in the training treebank is assigned two latent states (cluster identifiers). Figure 2 shows an example annotation of trees for three paraphrase questions from the Paralex corpus. We compute the parameters of the bi-layered L-PCFGGlayered with a simple frequency count maximum likelihood estimate over this annotated treebank. As such, Glayered is a combination ofGsyn andGpar, resulting in 24,000 latent states (24 syntactic x 1000 semantic).\nConsider an example where we want to generate paraphrases for the question what day is nochebuena. Parsing it with Glayered will lead to the leftmost hybrid structure as shown in Figure 2. The assignment of the first latent states for each nonterminals ensures that we retrieve the correct syntactic representation of the sentence. Here, however, we are more interested in the second latent states assigned to each nonterminals which capture the paraphrase information of the sentence at various levels. For example, we have a unary lexical rule (NN-*-142 day) indicating that we observe day with NN of the paraphrase type 142. We could use this information to extract unary rules of the form (NN-*-142 w) in the treebank that will generate\n4For other cases of separating syntax from semantics in a similar way, see Mitchell and Steedman (2015).\nwords w which are paraphrases to day. Similarly, any node WHNP-*-291 in the treebank will generate paraphrases for what day, SBARQ-*-403, for what day is nochebuena. This way we will be able to generate paraphrases when is nochebuena and when is nochebuena celebrated as they both have SBARQ-*-403 as their roots.5\nTo generate a word latticeWq for a given question q, we parse q with the bi-layered grammar Glayered. For each rule of the form X-m1-m2 \u2192 w in the bilayered tree with X \u2208 P , m1 \u2208 {1, . . . , 24}, m2 \u2208 {1, . . . , 1000} and w a word in q, we extract rules of the form X-\u2217-m2 \u2192 w\u2032 from Glayered such that w\u2032 6= w. For each such (w,w\u2032), we add a path w\u2032 parallel to w in the word lattice."}, {"heading": "2.3 Paraphrase Classification", "text": "Our sampling algorithm overgenerates paraphrases which are incorrect. To improve its precision, we build a binary classifier to filter the generated paraphrases. We randomly select 100 distinct questions from the Paralex corpus and generate paraphrases using our generation algorithm with various lattice settings. We randomly select 1,000 pairs of inputsampled sentences and manually annotate them as \u201ccorrect\u201d or \u201cincorrect\u201d paraphrases.6 We train our classifier on this manually created training data.7 We\n5We found out that our Gpar grammar is not fine-grained enough and often merges different paraphrase information into the same latent state. This problem is often severe for nonterminals at the top level of the bilayered tree. Hence, we rely only on unary lexical rules (the rules that produce terminal nodes) to extract paraphrase patterns in our experiments.\n6We have 154 positive and 846 negative paraphrase pairs. 7We do not use the paraphrase pairs from the Paralex corpus to train our classifier, as they do not represent the distribution of our sampled paraphrases and the classifier trained on them performs poorly.\nfollow Madnani et al. (2012), who used MT metrics for paraphrase identification, and experiment with 8 MT metrics as features for our binary classifier. In addition, we experiment with a binary feature which checks if the sampled paraphrase preserves named entities from the input sentence. We use WEKA (Hall et al., 2009) to replicate the classifier of Madnani et al. (2012) with our new feature. We tune the feature set for our classifier on the development data."}, {"heading": "3 Semantic Parsing using Paraphrasing", "text": "In this section we describe how the paraphrase algorithm is used for converting natural language to Freebase queries. Following Reddy et al. (2014), we formalize the semantic parsing problem as a graph matching problem, i.e., finding the Freebase subgraph (grounded graph) that is isomorphic to the input question semantic structure (ungrounded graph).\nThis formulation has a major limitation that can be alleviated by using our paraphrase generation algorithm. Consider the question What language do people in Czech Republic speak?. The ungrounded graph corresponding to this question is shown in Figure 3(a). The Freebase grounded graph which results in correct answer is shown in Figure 3(d). Note that these two graphs are non-isomorphic making it impossible to derive the correct grounding from the ungrounded graph. In fact, at least 15% of the examples in our development set fail to satisfy isomorphic assumption. In order to address this problem, we use paraphrases of the input question to generate additional ungrounded graphs, with the aim that one of those paraphrases will have a structure isomorphic to the correct grounding. Figure 3(b) and Figure 3(c) are two such paraphrases which can be converted to Figure 3(d) as described in ??.\nFor a given input question, first we build ungrounded graphs from its paraphrases. We convert these graphs to Freebase graphs. To learn this mapping, we rely on manually assembled questionanswer pairs. For each training question, we first find the set of oracle grounded graphs\u2014Freebase subgraphs which when executed yield the correct answer\u2014derivable from the question\u2019s ungrounded graphs. These oracle graphs are then used to train a structured perceptron model. These steps are discussed in detail below."}, {"heading": "3.1 Ungrounded Graphs from Paraphrases", "text": "We use GRAPHPARSER (Reddy et al., 2014) to convert paraphrases to ungrounded graphs. This conversion involves three steps: 1) parsing the paraphrase using a CCG parser to extract syntactic derivations (Lewis and Steedman, 2014), 2) extracting logical forms from the CCG derivations (Bos et al., 2004), and 3) converting the logical forms to an ungrounded graph.8 The ungrounded graph for the example question and its paraphrases are shown in Figure 3(a), Figure 3(b) and Figure 3(c), respectively."}, {"heading": "3.2 Grounded Graphs from Ungrounded Graphs", "text": "The ungrounded graphs are grounded to Freebase subgraphs by mapping entity nodes, entity-entity edges and entity type nodes in the ungrounded graph to Freebase entities, relations and types, respectively. For example, the graph in Figure 3(b) can be converted to a Freebase graph in Figure 3(d) by replacing the entity node Czech Republic with the Freebase entity CZECHREPUBLIC, the edge (speak.arg2, speak.in) between x and Czech Republic with the Freebase relation (location.country.official language.2, location.country.official language.1), the type node language with the Freebase type language.human language, and the TARGET node remains intact. The rest of the nodes, edges and types are grounded to null. In a similar fashion, Figure 3(c) can be grounded to Figure 3(d), but not Figure 3(a) to Figure 3(d). If no paraphrase is isomorphic to the target grounded grounded graph, our grounding fails."}, {"heading": "3.3 Learning", "text": "We use a linear model to map ungrounded graphs to grounded ones. The parameters of the model are learned from question-answer pairs. For example, the question What language do people in Czech Republic speak? paired with its answer {CZECHLANGUAGE}. In line with most work on question answering against Freebase, we do not rely on annotated logical forms associated with the question for training and treat the mapping of a question to its grounded graph as latent.\n8Please see Reddy et al. (2014) for more details.\nLet q be a question, let p be a paraphrase, let u be an ungrounded graph for p, and let g be a grounded graph formed by grounding the nodes and edges of u to the knowledge baseK (throughout we use Freebase as the knowledge base). Following Reddy et al. (2014), we use beam search to find the highest scoring tuple of paraphrase, ungrounded and grounded graphs (p\u0302, u\u0302, g\u0302) under the model \u03b8 \u2208 Rn:\n(p\u0302, u\u0302, g\u0302) = arg max (p,u,g)\n\u03b8 \u00b7 \u03a6(p, u, g, q,K) ,\nwhere \u03a6(p, u, g, q,K) \u2208 Rn denotes the features for the tuple of paraphrase, ungrounded and grounded graphs. The feature function has access to the paraphrase, ungrounded and grounded graphs, the original question, as well as to the content of the knowledge base and the denotation |g|K (the denotation of a grounded graph is defined as the set of entities or attributes reachable at its TARGET node). See ?? for the features employed. The model parameters are estimated with the averaged structured perceptron (Collins, 2002). Given a training question-answer pair (q,A), the update is:\n\u03b8t+1 \u2190 \u03b8t+\u03a6(p+, u+, g+, q,K)\u2212\u03a6(p\u0302, u\u0302, g\u0302, q,K) ,\nwhere (p+, u+, g+) denotes the tuple of gold paraphrase, gold ungrounded and grounded graphs for\nq. Since we do not have direct access to the gold paraphrase and graphs, we instead rely on the set of oracle tuples, OK,A(q), as a proxy:\n(p+, u+, g+) = arg max (p,u,g)\u2208OK,A(q) \u03b8 \u00b7 \u03a6(p, u, g, q,K) ,\nwhere OK,A(q) is defined as the set of tuples (p, u, g) derivable from the question q, whose denotation |g|K has minimal F1-loss against the gold answerA. We find the oracle graphs for each question a priori by performing beam-search with a very large beam."}, {"heading": "4 Experimental Setup", "text": "Below, we give details on the evaluation dataset and baselines used for comparison. We also describe the model features and provide implementation details."}, {"heading": "4.1 Evaluation Data and Metric", "text": "We evaluate our approach on the WebQuestions dataset (Berant et al., 2013). WebQuestions consists of 5,810 question-answer pairs where questions represents real Google search queries. We use the standard train/test splits, with 3,778 train and 2,032 test questions. For our development experiments we tune the models on held-out data consisting of 30% training questions, while for final testing\nwe use the complete training data. We use average precision (avg P.), average recall (avg R.) and average F1 (avg F1) proposed by Berant et al. (2013) as evaluation metrics.9"}, {"heading": "4.2 Baselines", "text": "ORIGINAL We use GRAPHPARSER without paraphrases as our baseline. This gives an idea about the impact of using paraphrases.\nMT We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation (Quirk et al., 2004; Wubben et al., 2010). In particular, we use Moses (Koehn et al., 2007) to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions."}, {"heading": "4.3 Implementation Details", "text": "Entity Resolution For WebQuestions, we use 8 handcrafted part-of-speech patterns (e.g., the pattern (DT)?(JJ.?|NN.?){0,2}NN.? matches the noun phrase the big lebowski) to identify candidate named entity mention spans. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014). For each candidate mention span, we retrieve the top 10 entities according to the Freebase API.10 We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate n-best paraphrases that contains the entity mention spans. In the end, this process creates a total of 10n paraphrases. We generate ungrounded graphs for these paraphrases and treat the final entity disambiguation and paraphrase selection as part of the semantic parsing problem.11\nGRAPHPARSER Features. We use the features from Reddy et al. (2014). These include edge align-\n9https://github.com/percyliang/sempre/ blob/master/scripts/evaluation.py\n10http://developers.google.com/freebase/ 11To generate ungrounded graphs for a paraphrase, we treat\neach entity mention as a single word.\nments and stem overlaps between ungrounded and grounded graphs, and contextual features such as word and grounded relation pairs. In addition to these features, we add two new real-valued features \u2013 the paraphrase classifier\u2019s score and the entity disambiguation lattice score.\nBeam Search We use beam search to infer the highest scoring graph pair for a question. The search operates over entity-entity edges and entity type nodes of each ungrounded graph. For an entityentity edge, there are two operations: ground the edge to a Freebase relation, or skip the edge. Similarly, for an entity type node, there are two operations: ground the node to a Freebase type, or skip the node. We use a beam size of 100 in all our experiments."}, {"heading": "5 Results and Discussion", "text": "In this section, we present results from five different systems for our question-answering experiments: ORIGINAL, MT, NAIVE, PPDB and BILAYERED. First two are baseline systems. Other three systems use paraphrases generated from an L-PCFG grammar. NAIVE uses a word lattice with a single start-to-end path representing the input question itself, PPDB uses a word lattice constructed using the PPDB rules, and BILAYERED uses bi-layered LPCFG to build word lattices. Note that NAIVE does not require any parallel resource to train, PPDB requires an external paraphrase database, and BILAYERED, like MT, needs a parallel corpus with paraphrase pairs. We tune our classifier features and GRAPHPARSER features on the development data. We use the best setting from tuning for evaluation on the test data.\nResults on the Development Set Table 1 shows the results with our best settings on the development data. We found that oracle scores improve significantly with paraphrases. ORIGINAL achieves an oracle score of 65.1 whereas with paraphrases we achieve an F1 greater than 70 across all the models. This shows that with paraphrases we eliminate substantial mismatch between Freebase and ungrounded graphs. This trend continues for the final prediction with the paraphrasing models performing better than the ORIGINAL.\nAll our proposed paraphrasing models beat the MT baseline. Even the NAIVE model which does not use any parallel or external resource surpass the MT baseline in the final prediction. Upon error analysis, we found that the MT model produce too similar paraphrases, mostly with only inflectional variations. For the question What language do people in Czech Republic speak, the top ten paraphrases produced by MT are mostly formed by replacing words language with languages, do with does, people with person and speak with speaks. These paraphrases do not address the structural mismatch problem. In contrast, our grammar based models generate syntactically diverse paraphrases.\nOur PPDB model performs best across the paraphrase models (avg F1 = 47.9). We attribute its success to the high quality paraphrase rules from the external paraphrase database. For the BILAYERD model we found 1,000 latent semantic states is not sufficient for modeling topical differences. Though MT competes with NAIVE and BILAYERED, the performance of NAIVE is highly encouraging since it does not require any parallel corpus. Furthermore, we observe that the MT model has larger search space. The number of oracle graphs \u2013 the number of ways in which one can produce the correct Freebase grounding from the ungrounded graphs of the given question and its paraphrases \u2013 is higher for MT (77.2) than the grammar-based models (50\u201360).\nResults on the Test Set Table 2 shows our final results on the test data. We get similar results on the test data as we reported on the development data. Again, the PPDB model performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang\u2019s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use GRAPHPARSER as our semantic parsing\nframework for evaluating our paraphrases extrinsically. We leave plugging our paraphrases to other existing methods and other tasks for future work.\nError Analysis The upper bound of our paraphrasing methods is in the range of 71.2\u201371.8. We examine the reason where we lose the rest. For the PPDB model, the majority (78.4%) of the errors are partially correct answers occurring due to incomplete gold answer annotations or partially correct groundings. Note that the partially correct groundings may include incorrect paraphrases. 13.5% are due to mismatch between Freebase and the paraphrases produced, and the rest (8.1%) are due to wrong entity annotations."}, {"heading": "6 Conclusion", "text": "We described a grammar method to generate paraphrases for questions, and applied it to a question answering system based on semantic parsing. We showed that using paraphrases for a question answering system is a useful way to improve its performance. Our method is rather generic and can be applied to any question answering system."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Nitin Madnani for his help with the implementation of the paraphrase classifier. We would like to thank our anonymous reviewers for their insightful comments. This research was supported by an EPSRC grant (EP/L02411X/1), the H2020 project SUMMA (under grant agreement 688139), and a Google PhD Fellowship for the second author."}], "references": [{"title": "Learning to paraphrase: An unsupervised approach using multiple-sequence alignment", "author": ["Barzilay", "Lee2003] Regina Barzilay", "Lillian Lee"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Barzilay et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2003}, {"title": "More accurate question answering on Freebase", "author": ["Bast", "Haussmann2015] Hannah Bast", "Elmar Haussmann"], "venue": "In Proceedings of CIKM", "citeRegEx": "Bast et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bast et al\\.", "year": 2015}, {"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Liang2014] Jonathan Berant", "Percy Liang"], "venue": "In Proceedings of ACL", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Imitation learning of agenda-based semantic parsers. Transactions of the Association for Computational Linguistics, 3:545\u2013558", "author": ["Berant", "Liang2015] Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2015}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Wide-coverage semantic representations from a CCG parser", "author": ["Bos et al.2004] Johan Bos", "Stephen Clark", "Mark Steedman", "James R. Curran", "Julia Hockenmaier"], "venue": "In Proceedings of COLING", "citeRegEx": "Bos et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Coarse-to-fine n-best parsing and maxent discriminative reranking", "author": ["Charniak", "Johnson2005] Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of ACL", "citeRegEx": "Charniak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2005}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["Chen", "Mooney2011] David L. Chen", "Raymond J. Mooney"], "venue": "In Proceedings of AAAI", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Experiments with spectral learning of latent-variable PCFGs", "author": ["Cohen et al.2013] Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle Ungar"], "venue": "Proceedings of NAACL", "citeRegEx": "Cohen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2013}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Answering the question you wish they had asked: The impact of paraphrasing for question answering", "author": ["Duboue", "Jennifer Chu-Carroll"], "venue": "Proceedings of NAACLHLT", "citeRegEx": "Duboue et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Duboue et al\\.", "year": 2006}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Fader et al.2013] Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "venue": "In Proceedings of ACL", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "PPDB: The Paraphrase Database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "The WEKA data mining software: An update", "author": ["Hall et al.2009] Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H Witten"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Generation as dependency parsing", "author": ["Koller", "Striegnitz2002] Alexander Koller", "Kristina Striegnitz"], "venue": "In Proceedings of ACL", "citeRegEx": "Koller et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2002}, {"title": "Weakly supervised training of semantic parsers", "author": ["Krishnamurthy", "Tom Mitchell"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Generation that exploits corpusbased statistical knowledge", "author": ["Langkilde", "Knight1998] Irene Langkilde", "Kevin Knight"], "venue": "In Proceedings of ACLCOLING", "citeRegEx": "Langkilde et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Langkilde et al\\.", "year": 1998}, {"title": "A* CCG parsing with a supertag-factored model", "author": ["Lewis", "Steedman2014] Mike Lewis", "Mark Steedman"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Lewis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2014}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["Joel Tetreault", "Martin Chodorow"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Madnani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "Proceedings of ACL", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Probabilistic CFG with latent annotations", "author": ["Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of ACL", "citeRegEx": "Matsuzaki et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Matsuzaki et al\\.", "year": 2005}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox"], "venue": "Proceedings of ICML", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Orthogonality of syntax and semantics within distributional spaces", "author": ["Mitchell", "Steedman2015] Jeff Mitchell", "Mark Steedman"], "venue": "In Proceedings of ACL", "citeRegEx": "Mitchell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2015}, {"title": "Diversity in spectral learning for natural language parsing", "author": ["Narayan", "Cohen2015] Shashi Narayan", "Shay B. Cohen"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Narayan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narayan et al\\.", "year": 2015}, {"title": "Optimizing spectral learning for parsing", "author": ["Narayan", "Cohen2016] Shashi Narayan", "Shay B. Cohen"], "venue": "In Proceedings of ACL", "citeRegEx": "Narayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Narayan et al\\.", "year": 2016}, {"title": "Structure-driven lexicalist generation", "author": ["Narayan", "Gardent2012] Shashi Narayan", "Claire Gardent"], "venue": "In Proceedings of COLING", "citeRegEx": "Narayan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Narayan et al\\.", "year": 2012}, {"title": "Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences", "author": ["Pang et al.2003] Bo Pang", "Kevin Knight", "Daniel Marcu"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Pang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2003}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Petrov et al.2006] Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein"], "venue": "In Proceedings of COLING-ACL", "citeRegEx": "Petrov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Head-driven pcfgs with latent-head statistics", "author": ["Detlef Prescher"], "venue": "In Proceedings of IWPT", "citeRegEx": "Prescher.,? \\Q2005\\E", "shortCiteRegEx": "Prescher.", "year": 2005}, {"title": "Monolingual machine translation for paraphrase generation", "author": ["Quirk et al.2004] Chris Quirk", "Chris Brockett", "William B. Dolan"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Quirk et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Large-scale semantic parsing without question-answer pairs. Transactions of the Association for Computational Linguistics, 2:377\u2013392", "author": ["Reddy et al.2014] Siva Reddy", "Mirella Lapata", "Mark Steedman"], "venue": null, "citeRegEx": "Reddy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Transforming Dependency Structures to Logical Forms for Semantic Parsing", "author": ["Reddy et al.2016] Siva Reddy", "Oscar T\u00e4ckstr\u00f6m", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata"], "venue": null, "citeRegEx": "Reddy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2016}, {"title": "Statistical machine translation for query expansion in answer retrieval", "author": ["Alexander Vasserman", "Ioannis Tsochantaridis", "Vibhu Mittal", "Yi Liu"], "venue": "Proceedings of ACL", "citeRegEx": "Riezler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2007}, {"title": "Semantic head-driven generation", "author": ["Gertjan van Noord", "Fernando C.N. Pereira", "Robert C. Moore"], "venue": null, "citeRegEx": "Shieber et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Shieber et al\\.", "year": 1990}, {"title": "A uniform architecture for parsing and generation", "author": ["Stuart M. Shieber"], "venue": "In Proceedings of COLING", "citeRegEx": "Shieber.,? \\Q1988\\E", "shortCiteRegEx": "Shieber.", "year": 1988}, {"title": "Building a semantic parser overnight", "author": ["Wang et al.2015] Yushi Wang", "Jonathan Berant", "Percy Liang"], "venue": "In Proceedings of ACL", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Reining in ccg chart realization", "author": ["Michael White"], "venue": "Natural Language Generation,", "citeRegEx": "White.,? \\Q2004\\E", "shortCiteRegEx": "White.", "year": 2004}, {"title": "Learning for semantic parsing with statistical machine translation", "author": ["Wong", "Mooney2006] Yuk Wah Wong", "Raymond J. Mooney"], "venue": "In Proceedings of NAACL", "citeRegEx": "Wong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2006}, {"title": "Paraphrase generation as monolingual translation: Data and evaluation", "author": ["Wubben et al.2010] Sander Wubben", "Antal van den Bosch", "Emiel Krahmer"], "venue": "In Proceedings of INLG", "citeRegEx": "Wubben et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wubben et al\\.", "year": 2010}, {"title": "Question Answering on Freebase via Relation Extraction and Textual Evidence", "author": ["Xu et al.2016] Kun Xu", "Siva Reddy", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "venue": "Proceedings of ACL", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "S-MART: Novel tree-based structured learning algorithms applied to tweet entity linking", "author": ["Yang", "Chang2015] Yi Yang", "Ming-Wei Chang"], "venue": "In Proceedings of ACL", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Yih et al.2015] Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": "Proceedings of ACL", "citeRegEx": "Yih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Zettlemoyer", "Collins2005] Luke S. Zettlemoyer", "Michael Collins"], "venue": "In Proceedings of UAI", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 22, "context": "Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012).", "startOffset": 265, "endOffset": 288}, {"referenceID": 4, "context": "Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al.", "startOffset": 67, "endOffset": 88}, {"referenceID": 16, "context": ", 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), or neural networks without requiring any grammar (Yih et al.", "startOffset": 37, "endOffset": 83}, {"referenceID": 31, "context": ", 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), or neural networks without requiring any grammar (Yih et al.", "startOffset": 37, "endOffset": 83}, {"referenceID": 42, "context": ", 2014), or neural networks without requiring any grammar (Yih et al., 2015).", "startOffset": 58, "endOffset": 76}, {"referenceID": 11, "context": "Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015).", "startOffset": 60, "endOffset": 123}, {"referenceID": 36, "context": "Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015).", "startOffset": 60, "endOffset": 123}, {"referenceID": 33, "context": "Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014).", "startOffset": 90, "endOffset": 142}, {"referenceID": 12, "context": "While CFGs have been explored for paraphrasing using bilingual parallel corpus (Ganitkevitch et al., 2013), ours is the first implementation of CFG that uses only monolingual data.", "startOffset": 79, "endOffset": 106}, {"referenceID": 12, "context": "While CFGs have been explored for paraphrasing using bilingual parallel corpus (Ganitkevitch et al., 2013), ours is the first implementation of CFG that uses only monolingual data. Second, we show that generated paraphrases can be used to improve semantic parsing of questions into Freebase logical forms (\u00a73). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources.", "startOffset": 80, "endOffset": 364}, {"referenceID": 29, "context": "L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016).", "startOffset": 76, "endOffset": 207}, {"referenceID": 21, "context": "L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016).", "startOffset": 76, "endOffset": 207}, {"referenceID": 28, "context": "L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016).", "startOffset": 76, "endOffset": 207}, {"referenceID": 8, "context": "L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016).", "startOffset": 76, "endOffset": 207}, {"referenceID": 20, "context": "In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al.", "startOffset": 139, "endOffset": 163}, {"referenceID": 20, "context": "In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case.", "startOffset": 139, "endOffset": 188}, {"referenceID": 8, "context": "For more details about these constructions, we refer the reader to Cohen et al. (2013) and Narayan and Cohen (2015).", "startOffset": 67, "endOffset": 87}, {"referenceID": 8, "context": "For more details about these constructions, we refer the reader to Cohen et al. (2013) and Narayan and Cohen (2015).", "startOffset": 67, "endOffset": 116}, {"referenceID": 12, "context": "We experiment with three ways of constructing word lattices: na\u0131\u0308ve word lattices representing the words from the input question only, word lattices constructed with the Paraphrase Database (Ganitkevitch et al., 2013) and word lattices constructed with a bi-layered L-PCFG, described in \u00a72.", "startOffset": 190, "endOffset": 217}, {"referenceID": 11, "context": "L-PCFG Estimation We train the L-PCFG Gsyn on the Paralex corpus (Fader et al., 2013).", "startOffset": 65, "endOffset": 85}, {"referenceID": 27, "context": "Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004).", "startOffset": 115, "endOffset": 206}, {"referenceID": 30, "context": "Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004).", "startOffset": 115, "endOffset": 206}, {"referenceID": 12, "context": "For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013).", "startOffset": 88, "endOffset": 115}, {"referenceID": 37, "context": "First, the sampling from an L-PCFG grammar lessens the lexical ambiguity problem evident in lexicalized grammars such as tree adjoining grammars (Narayan and Gardent, 2012) and combinatory categorial grammars (White, 2004).", "startOffset": 209, "endOffset": 222}, {"referenceID": 34, "context": "Second, the top-down sampling restricts the combinatorics inherent to bottom-up search (Shieber et al., 1990).", "startOffset": 87, "endOffset": 109}, {"referenceID": 35, "context": "And fourth, we impose no constraints on the grammar thereby making it easier to maintain bi-directional (recursive) grammars that can be used both for parsing and for generation (Shieber, 1988).", "startOffset": 178, "endOffset": 193}, {"referenceID": 13, "context": "We use WEKA (Hall et al., 2009) to replicate the classifier of Madnani et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 18, "context": "follow Madnani et al. (2012), who used MT metrics for paraphrase identification, and experiment with 8 MT metrics as features for our binary classifier.", "startOffset": 7, "endOffset": 29}, {"referenceID": 13, "context": "We use WEKA (Hall et al., 2009) to replicate the classifier of Madnani et al. (2012) with our new feature.", "startOffset": 13, "endOffset": 85}, {"referenceID": 31, "context": "Following Reddy et al. (2014), we formalize the semantic parsing problem as a graph matching problem, i.", "startOffset": 10, "endOffset": 30}, {"referenceID": 31, "context": "We use GRAPHPARSER (Reddy et al., 2014) to convert paraphrases to ungrounded graphs.", "startOffset": 19, "endOffset": 39}, {"referenceID": 5, "context": "This conversion involves three steps: 1) parsing the paraphrase using a CCG parser to extract syntactic derivations (Lewis and Steedman, 2014), 2) extracting logical forms from the CCG derivations (Bos et al., 2004), and 3) converting the logical forms to an ungrounded graph.", "startOffset": 197, "endOffset": 215}, {"referenceID": 31, "context": "Please see Reddy et al. (2014) for more details.", "startOffset": 11, "endOffset": 31}, {"referenceID": 31, "context": "Following Reddy et al. (2014), we use beam search to find the highest scoring tuple of paraphrase, ungrounded and grounded graphs (p\u0302, \u00fb, \u011d) under the model \u03b8 \u2208 Rn:", "startOffset": 10, "endOffset": 30}, {"referenceID": 9, "context": "The model parameters are estimated with the averaged structured perceptron (Collins, 2002).", "startOffset": 75, "endOffset": 90}, {"referenceID": 4, "context": "We evaluate our approach on the WebQuestions dataset (Berant et al., 2013).", "startOffset": 53, "endOffset": 74}, {"referenceID": 2, "context": ") and average F1 (avg F1) proposed by Berant et al. (2013) as evaluation metrics.", "startOffset": 38, "endOffset": 59}, {"referenceID": 30, "context": "MT We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation (Quirk et al., 2004; Wubben et al., 2010).", "startOffset": 113, "endOffset": 154}, {"referenceID": 39, "context": "MT We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation (Quirk et al., 2004; Wubben et al., 2010).", "startOffset": 113, "endOffset": 154}, {"referenceID": 20, "context": "We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014).", "startOffset": 71, "endOffset": 93}, {"referenceID": 31, "context": "We use the features from Reddy et al. (2014). These include edge align-", "startOffset": 25, "endOffset": 45}, {"referenceID": 41, "context": "Yih et al. (2015) and Xu et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 40, "context": "(2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data.", "startOffset": 11, "endOffset": 28}], "year": 2016, "abstractText": "One of the limitations of semantic parsing approaches to open-domain question answering is the lexicosyntactic gap between natural language questions and knowledge base entries \u2013 there are many ways to ask a question, all with the same answer. In this paper we propose to bridge this gap by generating paraphrases of the input question with the goal that at least one of them will be correctly mapped to a knowledge-base query. We introduce a novel grammar model for paraphrase generation that does not require any sentence-aligned paraphrase corpus. Our key idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines.", "creator": "LaTeX with hyperref package"}}}