{"id": "1602.00163", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2016", "title": "A multiple instance learning approach for sequence data with across bag dependencies", "abstract": "In Multiple Instance Learning (MIL) sort from sequence internet, came mathematics data consist of a set of handbags moved besides tote follows a fifth among instances / montage. In instance whatever world internet making been practice-based, application firm, and mentions mining, comparing making random couple particular string better. genuine. In as, over terms within each bag if now structural between / few temporal relates each instead reasons in people socks. Thus, part simplified undertaken should take once money the difference between semantically related instances across bags. In instead collection, unfortunately present half titled MIL focuses up binary reference classifies: (1) ABClass but (2004) ABSim. In ABClass, turn pattern is 2004 but made vector on sense. For each sequence but way unknown vest, when discriminative hydrocele becomes applied before permanent would formula_1 full modifications designations result. Then, but aggregation instruction is additionally as only partial change he proper way producing three final result. In ABSim, exactly use a similarity instance between all simultaneous related place existence room turn given whereas alternate rest the learners oversized. An unknown suitcases is labeled with first briefcase they variety more rather sequences. We patent although discussion time though solve. bacterial Ionizing Radiation Resistance (IRR) comparison. We evaluated this foreign the proposed ideas it well particular Ionizing Radiation Resistance Bacteria (IRRB) and Ionizing Radiation Sensitive Bacteria (IRSB) 2011 was primary within related nodes DNA undergo membranes. The testing changes look way such ABClass between ABSim approaches mostly approach.", "histories": [["v1", "Sat, 30 Jan 2016 21:15:10 GMT  (302kb,D)", "http://arxiv.org/abs/1602.00163v1", "Submitted to Data Mining and Knowledge Discovery Journal"]], "COMMENTS": "Submitted to Data Mining and Knowledge Discovery Journal", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["manel zoghlami", "sabeur aridhi", "haitham sghaier", "mondher maddouri", "engelbert mephu nguifo"], "accepted": false, "id": "1602.00163"}, "pdf": {"name": "1602.00163.pdf", "metadata": {"source": "CRF", "title": "A multiple instance learning approach for sequence data with across bag dependencies", "authors": ["Manel Zoghlami", "Sabeur Aridhi", "Ha\u00eftham Sghaier", "Mondher Maddouri", "Engelbert Mephu Nguifo"], "emails": ["manel.zoghlami@gmail.com", "sabeur.aridhi@aalto.fi", "sghaier.haitham@gmail.com", "maddourimondher@yahoo.fr", "mephu@isima.fr"], "sections": [{"heading": null, "text": "Keywords Multiple instance learning; sequence data classification; across bag dependencies; prediction; bacterial ionizing radiation resistance"}, {"heading": "1. INTRODUCTION", "text": "Multiple Instance Learning (MIL) is a variation of classical learning methods that can be used to solve problems in which the labels are assigned to bags, i.e., a set of instances, rather than individual instances. MIL was originally introduced to solve the problem of drug activity prediction and polymorphism ambiguity [6]. Then, it has been applied to several problems such as Protein-Protein Interactions (PPI) [20] and image regions classification in computer vision [4]. For example, in drug activity prediction problem, each drug molecule is represented by a bag, and the alternative lowenergy shapes of the molecule are represented by the instances in the bag. In image regions classification, each image can be treated as a bag of segments that are modeled as instances, and the concept point representing the target object can be learned through MIL algorithms.\nOne major assumption of most existing MIL methods is that each bag contains a set of instances that are independently and identically distributed. But, many real world applications such as bioinformatics, web mining, and text mining have to deal with sequential and temporal data. When the tackled problem can be formulated as a MIL problem, each instance of each bag may have structural and/or temporal relation with other instances in other bags. Considering this issue, thus, the problem we want to solve in this work is the MIL problem in sequence data that present structural dependencies between instances of different bags. In this context, the learning data consist of a set of bags where each bag contains a set of sequences that are expressed differently for every bag.\nA variety of MIL algorithms have been developed such as Diverse Density [13], Citation-kNN [17], MI-SVM [2] and\nar X\niv :1\n60 2.\n00 16\n3v 1\n[ cs\n.L G\n] 3\n0 Ja\nHyDR-MI [22]. However, most of these algorithms are not suitable for the problem of sequence data prediction since they require an attribute-value format for their processed data. In addition, the major assumption of most existing MIL methods is that instances of different bags are independently and identically distributed. Nevertheless, in many applications, the dependencies between across bag instances naturally exist and if incorporated in classification models, they can potentially improve the prediction performance significantly [23].\nIn this paper, we deal with MIL problem for sequence data and we consider the case of data that present dependencies between instances of different bags. We first provide a formalization of the problem of multiple instance learning in sequence data. Then, we present two approaches that take into account relational structure information among across bag instances/sequences. The first approach is the ABClass approach which performs first a preprocessing step of the input sequences that consists in extracting motifs from the set of sequences. These motifs will be used as attributes/features to construct a binary table where each row corresponds to a sequence. Then a discriminative classifier is applied to the sequences of an unknown bag in order to assign its label. The second approach is the ABSim approach which uses a similarity measure between each sequence of an unknown bag and the corresponding sequences in the learning bags. We applied the proposed approaches to the problem of prediction of IRR in bacteria using MIL techniques introduced and described in [3]. The problem of IRR prediction in bacteria consists in learning a classifier that classifies a bacterium to either IRRB or IRSB. We describe an implementation of our algorithms and we present an experimental study that evaluates the performance of the proposed approaches in the case of IRR prediction in bacteria.\nThe remainder of this paper is organized as follows. Section 2 defines the problem of MIL for sequence data. In Section 3, we present an overview of some related works dealing with MIL problems. In Section 4, we describe the proposed MIL-based approaches for sequence data. In Section 5, we describe our experimental environment and we discuss the obtained results. Concluding points make the body of Section 6."}, {"heading": "2. BACKGROUND", "text": "In this section, we present the background information related to MIL in sequence data. We first describe the terminology and our problem formulation. Then, we introduce a simple use case that serves as a running example throughout this paper."}, {"heading": "2.1 Problem formulation", "text": "A sequence is an ordered list of events. An event can be represented as a symbolic value, a numerical value, a vector of values or a complex data type [19]. There are many types of sequences including symbolic sequences, simple time series and multivariate time series. In our work, we are interested in symbolic sequences since the protein sequences are described using symbols (amino acids). We denote \u03a3 an alphabet defined as a finite set of characters or symbols. A symbolic sequence is defined as an ordered\nlist of symbols. A sequence s with length P is written as: s = s1, s2, . . . , sP , where sl \u2208 \u03a3 is a symbol at position l. Let DB be a learning database that contains a set of n labeled bags DB = {(Bi, Yi), i = 1, 2 . . . , n} where Yi = {\u22121, 1} is the label of the bag Bi. Instances in Bi are sequences and are denoted by Bij . Formally Bi = {Bij , j = 1, 2 . . . ,m}, where m is the total number of instances in this bag. We note that there is a relation R between instances of different bags denoted across bag sequences relation. Each instance Bij of a bag Bi is related by R to the instance Bhj of an other bag Bh in DB.\nThe problem investigated in this work is to learn a multiple instance classifier from DB. Given a query bag Q = {Qk, k = 1, 2 . . . , q}, where q is the total number of instances in Q, the classifier should use sequential data in this bag and in each bag of DB to predict the label of Q."}, {"heading": "2.2 Running example", "text": "In order to illustrate our proposed approaches, we rely on the following running example. Let \u03a3 = {A,B, . . . , Z} be an alphabet. Let DB = {(B1,+1), (B2,+1), (B3,\u22121), (B4,\u22121)} a learning database that contains four bags. Positive bags consists of B1 and B2 and negative bags consists of B3 and B4.\nB1 =\n{ B11 = ABMSCD\nB12 = EFNOGH (1)\nB2 =\n{ B21 = EABZQCD\nB22 = CCGHDDEF (2)\nB3 =\n{ B31 = CDXYZ\nB32 = GHWXY (3)\nB4 =\n{ B41 = ABIJYZ\nB42 = EFY RTAB (4)\nWe note that B11, B21, B31 and B41 are related by an across bag relation R. The same applies to B12, B22, B32 and B42. We need to predict the class label of an unknown bag Q = {Q1, Q2} where:\nQ =\n{ Q1 = ABWXCD\nQ2 = EFXYGHN (5)"}, {"heading": "3. RELATED WORKS", "text": ""}, {"heading": "3.1 Sequence classification", "text": "Existing sequence classification methods can be divided into three large categories [19]: (1) feature based classification, (2) sequence distance based classification and (3) model based classification.\nIn feature based classification, a sequence is transformed into a feature vector. This representation scheme could lead to\nvery high-dimensional feature spaces. The feature extraction step is very important since it would impact on the classification results. This step should deal with many parameters such as the criteria used for selecting features (e.g. frequency and length) and the matchings type (i.e. exact or inexact with gaps). After adapting the input data format, a conventional classification method is applied.\nIn sequence distance based classification, a distance function should be defined to measure the similarity between a pair of sequences. Then an existing classification method could be used such as K nearest neighbor (KNN) or SVM. The distance function determines the quality of the classification significantly [19]. In a recent work [5], authors propose algorithms to learn sequential classifiers from long and noisy discrete-event sequences. The algorithms use a lightweight and flexible subsequence matching function and a subsequence enumeration strategy called pattern silhouettes.\nModel based classification methods define a classification model based on the probability distribution of the sequences over the different classes. This model is then used to classify unknown sequences. Naive Bayes is a simple model based classifier that makes the assumption that the features of the sequences are independent. Markov Model and Hidden Markov Model (HMM) could be used to model the dependencies among sequences."}, {"heading": "3.2 Multiple Instance Learning (MIL) approaches", "text": "In multiple instance learning, the training set is composed of n labeled bags. Each bag i in the training set contains m instances and has a bag label Yi \u2208 {\u22121,+1}. We notice that instance j of each bag has label Yij \u2208 {\u22121,+1}, but this label is not known during training. The most common assumption in this field is that a bag is labeled positive if at least one of its instances is positive, which can be expressed as follows:\nYi = max j (Yij). (6)\nThe task of MIL is to learn a classifier from the training set that correctly predicts unseen bags. Although MIL is quite similar to traditional supervised learning, the main difference between the two approaches can be found in the class labels provided by the data. According to the specification given by Dietterich et al. [6], in a traditional setting of machine learning, an object is represented by a feature vector (an instance), which is associated to a label. However, in a multiple instance setting, each object may have various instances.\nRecently, several MIL algorithms have been proposed including Diverse Density [13], Citation-kNN [17], MI-SVM [2], and HyDR-MI [22]. Diverse Density (DD) was proposed in [13] as a general framework for solving multiple instance learning problems. Diverse Density uses a probabilistic approach to maximize a measure of the intersection of the positive bags minus the union of the negative bags in feature space. The key point of DD approach is to find a concept point in the feature space that are close to at least one in-\nstance from every positive bag and meanwhile far away from instances in negative bags. The optimal concept point is defined as the one with the maximum diversity density, which is a measure of how many different positive bags have instances near the point, and how far the negative instances are away from that point.\nMI-SVM [2] is an adaptation of support vector machines (SVM ) to the MIL problem. The approach adopted by MISVM explicitly treats the label instance labels as unobserved hidden variables subject to constraints defined by their bag labels. The goal is to maximize the usual instance margin jointly over the unknown instance labels and a linear or kernelized discriminant function.\nIn [16], the authors present two variants of the K-nearest neighbor algorithm called Bayesian-KNN and CitationKNN. The Bayesian method computes the posterior probabilities of the label of an unknown bag based on labels of its neighbors. Citation-kNN algorithm classifies a bag based on the labels of both the references and citers of that bag.\nIn [22], HyDR-MI (which stands for hybrid dimensionality reduction method for multiple instance learning) is proposed as a feature subset selection method for MIL algorithms. The hybrid consists of the filter component based on an extension of the ReliefF algorithm [21] developed to work with MIL and the wrapper component based on a genetic algorithm that optimizes the search for the best feature subset from a reduced set of features, output by the filter component.\nIn [15], the authors present an SVM-based algorithm via Approximate Box Counting. They reformulate GMIL-1 algorithm using a kernel for a support vector machine to reduce its time complexity from exponential to polynomial. Computing the kernel is equivalent to counting the number of axis-parallel boxes in a discrete, bounded space that contain at least one point from each of the two multisets.\nIn [24], an optimization algorithm that deals with multiple instance learning on structured data (MILSD) is proposed. In MILSD there exists rich dependency/structure information between instances/bags that may be used to improve the performance of existing MIL algorithms. This additional information is represented using a graph that depicts the structure between either bags or instances. The proposed formulation deals with two sets of constraints caused by learning on instances within individual bags and learning on structured data and has a non-convex optimization problem. To solve this problem, authors present an iterative method based on constrained concave-convex procedure (CCCP). It is an optimization method that deals with the concave convex objective function with concave convex constraints [14]. However, in many real world applications, the number of the labeled bags as well as the number of links between bags are huge. To solve the problem efficiently, the cutting plane method [10] is used. The authors of [24] have present a novel adaption of the cutting plane method that can handle the two sets of constraints simultaneously: the goal is to find two small subsets of constraints from a larger constraint sets. They also summarize three scenarios of the structure information in MIL:\n\u2022 I-MILSD: the relational structures are on the instance level (either in the same bag or across bags).\n\u2022 B-MILSD: the structure information is available on the bag level.\n\u2022 BI-MILSD: the structure information is available on both instance level and bag level.\nApplying the above presented algorithms on our IRR problem leads to two problems. The first problem is that these algorithms can be used when the processed data can be simply represented by bags of instances and the labels are assigned to bags rather than individual instances. If applied on sequences, the input data should be presented in attribute-value format whereas we aim to use the input data without any adaptation of the format. For example, in [2], the empirical evaluation is done on three datasets: (1) MUSK dataset, (2) Corel dataset for image annotation and (3) TREC9 dataset for document categorisation. The last dataset contains sequence data. Terms are used to present the text. This leads to an extremely sparse and high dimensional attribute-value representation of the processed text. In[15] and [24], a dataset of protein sequences was used in the empirical evaluation. The goal is to identify Trx-fold Proteins. In each protein\u2019s primary sequence, the primary sequence motif (typically CxxC) that is known to exist in all Trx-fold proteins are found. Then, they extract a window of size 214 around each motif (20 residues upstream, 180 downstream). Each primary protein sequence is considered as a bag, and some of its subsequences (the extracted windows) are considered as instances. These subsequences are aligned mapped to a 8-dimensional feature space: 7 numeric properties [11] and an 8th feature that represents the residue\u2019s position. So we obtain an attribute-value format description of the dataset. The second problem is that they don\u2019t deal with the across bag relations that may exists between instances, except the algorithms in [24]. In [24], the alignment score is used to identify the additional structure information between proteins: if the score between a pair of proteins exceed 25, then authors consider that there exists a link between them. Only the B-MILSD algorithm was used in experiments."}, {"heading": "4. MIL-BASED APPROACHES FOR SEQUENCE DATA", "text": "In this section, we present the proposed approaches for MIL in sequence data. We also present the naive approach to deal with the problem of MIL in sequence data."}, {"heading": "4.1 Naive approach for MIL in sequence data", "text": "The simplest way to solve the problem of MIL for sequence data is to use standard MIL classifiers. However, most commonly used MIL algorithms require a uniform attributevalue format description of all instances of different bags. The naive approach for MIL in sequence data consists of a two step approach. The first step is a preprocessing step that transforms the set of sequences to an attribute-value matrix where each row corresponds to a sequence and each column corresponds to an attribute. The second step consists in applying an existing MIL classifier. Figure 1 illustrates the naive approach for MIL in sequence data.\nIn the case of sequence data, the most used technique to transform data to an attribute-value format is to extract motifs that serve as attributes. We note that finding a uniform description of all instances using a set of motifs is not always an easy task. Since our naive approach takes into account the across bag relations between instances, the preprocessing step extracts motifs from each set of related instances. The union of these extracted motifs is then used as features to construct an attribute-value matrix where each row corresponds to a sequence. The presence or the absence of an attribute in a sequence is respectively denoted by 1 or 0. Using this approach, we obtain an attribute-value matrix that contains a large number of motifs. It is worthwhile to mention that only a subset of the used attributes is representative for each processed sequence. Therefore, we may have a big sparse matrix when trying to present the whole sequence data using an attribute value format.\nWe apply the naive approach to our running example. Let AttributeList1 = {AB,CD, Y Z} be the list of features extracted from the instances B11, B21, B31 and B41. Let AttributeList2 = {EF,GH} be the list of features extracted from the instances B12, B22, B32 and B42. The union of AttributeList1 and AttributeList2 produces the list AttributeList = {AB,CD, Y Z,EF,GH}. In order to encode the learning sequence data, we generate the following attribute-value matrix denoted M :\nM = 1stinstance | 2ndinstance 1 1 0 0 0 | 0 0 0 1 1 B11 1 0 0 0 | 0 0 0 1 1 B20 1 1 0 0 | 0 0 0 0 1 B3\n1 0 1 0 0 | 1 0 0 1 0 B4\n(7)\nThe sparsity percentage of M is 60%. We note that if we have a big learning database, M could result to a huge and very sparse matrix.\n4.2 ABClass: Across bag sequences classification In order to avoid the use of one large vector of features to describe sequence data, we present ABClass, a novel approach that takes into account the across bag relations. Each set of related instances will be presented by its own motifs vector. This reduces the number of attributes that are not representative for the processed sequence. Instead of using a classifier that uses a large vector to describe all the sequences data, every vector of motifs will be used to produce a prediction result. These results will be then aggregated to have a final result. Based on the formalization, we propose an algorithm that discriminates bags by applying a classification model to each instance of the query bag. For each set of across bag sequences, we extract motifs and we construct a classification model. During the execution of the ABClass algorithm, we will use the following variables:\n\u2022 A matrix M to store the encoded data of the learning database.\n\u2022 A vector QV to store the encoded data of the query bag instances.\n\u2022 A vector PV to store prediction results of the classification.\nThe algorithm works as follows (see Algorithm 1). The\nAlgorithm 1 AcrossBagSequencesClassification(DB, Q)\nRequire: Learning database DB = {(Bi, Yi)|i = 1, 2, . . . , n} , Query bag Q = {Qk|k = 1, 2, . . . , q} Ensure: Prediction result P 1: for all Qk \u2208 Q do 2: AcrossBagSeqList\u2190 AcrossBagSeq(k,DB) 3: MotifList\u2190MotifExtractor(AcrossBagsList) 4: M \u2190 EncodeData(MotifList, AcrossBagsList) 5: QV \u2190 EncodeData(MotifList,Qk) 6: PVk \u2190 ApplyModel(QV,Model) 7: end for 8: P \u2190 Aggregate(PV ) 9: return P\nacrossBagSeq function is illustrated in Algorithm 2.\nAlgorithm 2 set acrossBagSeq (k, DB)\nRequire: Sequence index k, Learning database DB = {(Bi, Yi)|i = 1, 2, . . . , n} Ensure: A set of sequences S 1: S \u2190 \u2205 2: for all Bi \u2208 DB do 3: S \u2190 S \u222a {Bik} 4: end for 5: return S\nInformally, the main steps of the ABClass algorithm are:\n1. For each instance sequence Qk in the query bag Q, the related instances among bags of the learning database are grouped into a list (lines 1 to 3).\n2. The algorithm extracts motifs from the list of grouped instances. These motifs are used to encode instances in order to create a discriminative model (lines 4 and 5).\n3. ABClass uses the extracted motifs to represent the instance Qk of the unknown bag into a vector QV , Then it compares it with the corresponding model. The comparison results are stored in a vector PV (lines 6 and 7).\n4. ABClass applies an aggregation method to PV in order to compute the final prediction result P (line 9), which consists in a positive or a negative class label.\nWe notice that the proposed approach can be simply evaluated by the accuracy of its prediction result. Another option can be used is the rate of classification models that contributes to the prediction result.\nWe apply the ABClass approach to our running example. Since the query bag contains two instances Q1 and Q2, we will have two iterations followed by an aggregation step. First, the algorithm groups the set of bags that are related and extract the corresponding motifs.\nAcrossBagsList = {B11, B21, B31, B41} MotifList = {AB,CD,XY }\nThen, it generates the attribute-value matrix M describing the data.\nM = AB CD Y Z 1 1 0 B111 1 0 B210 1 1 B31\n1 0 1 B41\n(8)\nThe sparsity percentage of M is 33%. We note that the sparsity percentage of the produced matrix is reduced because there is no need to use the motifs extracted from instances {Bi2, i = 1, .., 4} to describe instances {Bi1, i = 1, .., 4}. A model is then created using the encoded data (Model = CreateModel(M)). Using the features list, a vector QV is generated to describe the first instance Q1 of the query bag.\nQV = 11 0  (9) By applying the model to the vector QV , we obtain the first prediction result and we store it into the vector PV .\nPV1 = ApplyModel(QV,Model)\nThe second iteration concerns the second instance Q2 of the query bag. We do the same instructions described in the first iteration.\nAcrossBagsList = {B21, B22, B32, B42} MotifList = {EF,GH}\nM = EF GH 1 1 B121 1 B221 0 B32\n0 1 B42\n(10)\nThe sparsity percentage of M is reduced to 25%. A model is then created using the encoded data (Model = CreateModel(M)). Using the features list, a vector QV is generated to describe the second instance Q2 of the query bag.\nQV = ( 1 1 ) (11)\nBy applying the model to the vector QV , we obtain the second prediction result and we store it into the vector PV .\nPV2 \u2190 ApplyModel(QV,Model)\nThe aggregation step is finally used to generate the final prediction result using the prediction vector PV .\nP = Aggregate(PV1, PV2)\n4.3 ABSim: Across bag sequences similarity According to the specificity of the processed data, a similarity measure can be defined and used to discriminate instances. We propose an algorithm that focuses on discriminating bags by measuring the similarity between each instance sequence in the query bag and corresponding related sequences in the different bags of the learning database. We note M a matrix used to store similarity measurement score vectors during the execution of the algorithm. The ABSim algorithm works as follows:\nInformally, the algorithm is described as follows:\nAlgorithm 5 WAMS(M , W )\nRequire: Similarity matrix M = {Mij |i = 1, 2 . . . , n and j = 1, 2 . . . , p}, Weight vector W = {wi|i = 1, 2 . . . , p} Ensure: A prediction result P 1: totalP \u2190 0 2: totalN \u2190 0 3: nbP \u2190 0 4: nbN \u2190 0 5: for i \u2208 [1; p] do 6: maxP \u2190 0 7: maxN \u2190 0 8: for j \u2208 [1;n] do 9: if Yj = +1 and maxP \u2265Mij then 10: maxP \u2190Mij 11: else if Yj = \u22121 and maxN \u2265Mij then 12: maxN \u2190Mij 13: end if 14: end for 15: if maxP \u2265 maxN then 16: totalP \u2190 totalP + (maxP \u00b7 wi) 17: nbP \u2190 nbP + 1 18: else 19: totalN \u2190 totalN + (maxN \u00b7 wi) 20: nbN \u2190 nbN + 1 21: end if 22: end for 23: avgP (M)\u2190 totalP /nbP 24: avgN (M)\u2190 totalN/nbN 25: if avgP (M) \u2265 avgN (M) then 26: P \u2190 +1 27: else 28: P \u2190 \u22121 29: end if 30: return P\nAlgorithm 3 set AcrossBagSequencesSimilarity(DB, Q)\nRequire: Learning database DB = {(Bi, Yi)|i = 1, 2 . . . , n} , Query bag Q = {Qk|k = 1, 2, . . . , p} Ensure: Prediction result P 1: for all Qk \u2208 Q do 2: for all Bi \u2208 DB do 3: Mik \u2190 similarityMeasure(Qk, Bik) {Bik is the in-\nstance number k in the bag Bi} 4: end for 5: end for 6: P \u2190 Aggregate(M) 7: return P\nAlgorithm 4 SMS(M)\nRequire: Similarity matrix M = {Mij |i = 1, 2 . . . , n and j = 1, 2 . . . , p} Ensure: A prediction result P 1: totalP \u2190 0 2: totalN \u2190 0 3: for i \u2208 [1;n] do 4: maxP \u2190 0 5: maxN \u2190 0 6: for j \u2208 [1; p] do 7: if Yj = +1 and maxP \u2265Mij then 8: maxP \u2190Mij 9: else if Yj = \u22121 and maxN \u2265Mij then 10: maxN \u2190Mij 11: end if 12: end for 13: if maxP \u2265 maxN then 14: totalP \u2190 totalP +maxP 15: else 16: totalN \u2190 totalN +maxN 17: end if 18: end for 19: if totalP \u2265 totalN then 20: P \u2190 +1 21: else 22: P \u2190 \u22121 23: end if 24: return P\n1. For each instance sequence Qk in the query bag Q, it computes the corresponding similarity measure scores (line 1 to 4). Similarity scores of all instances of the query bag are grouped into a matrix M (line 3). Element Mik corresponds to the similarity score between Qk of Q and Bik of Bi.\n2. An aggregation method is applied to M in order to compute the final prediction result P (line 6). According to the aggregation result, a class label is associated to the query Bag.\nIn our work, we define two aggregation methods: (1) Sum of Maximum Scores (SMS) and (2) Weighted Average of Maximum Scores (WAMS). Algorithms 4 and 5 illustrate the SMS and WAMS aggregation methods.\nFor each sequence in the query bacterium, we scan the corresponding line of M , which contains the obtained scores against all the other bags of the training database. The SMS method selects the maximum score among the similarity scores against bags that belong to the positive class label (which we call maxP ) and the maximum score among the similarity scores against bags that belong to the negative class label (which we call maxN ). It then compares these scores. If maxP is greater than maxN , it adds maxP to the total score of the positive class label (which we denote totalP (M)). Otherwise, it adds maxN to the total score of the negative class label (which we denote totalN (M)). When all selected sequences were processed, the SMS method compares total scores of positive class label and negative class label. If totalP (M) is greater than totalN (M), the prediction output is the positive class label. Otherwise, the prediction output is the negative class label.\nUsing the WAMS method, each sequence Qi has a given weight wi. For each sequence in the query bag, we scan the corresponding line of M , which contains the obtained scores against all other bags of the training database. The WAMS method selects the maximum score among the similarity scores against bags that belong to positive class label (which we denote maxP (M)) and the maximum score among the similarity scores against bags that belong to the negative class label (which we denote maxN (M)). It then compares these scores. If the maxP (M) is greater than maxN (M), it adds maxP (M) multiplied by the weight of the sequence to\nthe total score of the positive class label and it increments the number of positive bags having a max score. Otherwise, it adds maxN (M) multiplied by the weight of the sequence to the total score of the negative class label and it increments the number of negative bags having a max score. When all the selected sequences were processed, we compare the average of total scores of positive class labels (which we denote avgP (M)) and the average of total scores of negative class labels (which we denote avgN (M)). If avgP (M) is greater than avgN (M), the prediction output is the positive class label. Otherwise, the prediction output is the negative class label. In the case of IRR prediction in bacteria the positive (respectively negative) class label corresponds to IRRB (respectively IRSB).\nIn order to apply the ABSim approach to our running example, we need to use a similarity measure between sequences. Suppose that we use a very simple similarity measure that consists in the number of common symbols between two sequences. The first iteration computes the common symbols between the instance Q1 of the query bag and the four instances B11, B21, B31 and B41 and stores the result in the first column of the matrix M .\nM =  4 \u2212 B14 \u2212 B23 \u2212 B3\n2 \u2212 B4 (12)\nThe second iteration computes the common symbols between the instance Q2 of the query bag and the four instances B12, B22, B32 and B42 and stores the result in the second column of the matrix M .\nM =  4 5 B14 4 B23 4 B3\n2 3 B4\n(13)\nThe aggregation step applies SMS or WAMS aggregation algorithm on the matrix M in order to generate the final prediction result.\nUsing the SMS aggregation method, we have the following results:\ntotalP (M) = 9 totalN (M) = 0\nThe query bag Q is finally classified as positive.\nUsing the WAMS aggregation method, it is needed to specify a weight value for each instance. We suppose that all sequences have the same weight value, then we have the following results:\navgP (M) = 4.5\navgV (M) = 0\nThe query bag Q is finally classified as positive."}, {"heading": "5. EXPERIMENTS", "text": "We apply the proposed approaches to the problem of phenotype prediction of bacterial Ionizing Radiation Resistance (IRR) that can be formulated as a MIL problem for sequence data [3]. Bacteria represent the bags and primary structure of basal DNA repair proteins represent the sequences. In this context, an unknown bacterium is affiliated to either Ionizing Radiation Resistant Bacteria (IRRB) or Ionizing Radiation Sensitive Bacteria (IRSB). For our tests, we used the dataset described in [3]. This dataset consists of 28 bags (14 IRRB and 14 IRSB). Each bacterium/bag contains 25 to 31 instances that correspond to proteins implicated in basal DNA repair in IRRB [3]. Additional and more detailed information about our datasets and our experiments in general can be found in the following link: http://fc.isima.fr/~aridhi/MIL/."}, {"heading": "5.1 Experimental environment", "text": "Computations were carried out on a i7 CPU 2.49 GHz PC with 6 GB memory, operating on Linux Ubuntu. We used WEKA [8] data mining tool in order to apply existing multiple instance classifiers when using the naive approach. To deal with the ABSim approach, we used a local alignment technique as a similarity measure. In our tests, basic local alignment search tool (BLAST) [1] was used for computing local alignments."}, {"heading": "5.2 Experimental protocol", "text": "In our context, the Leave-One-Out (LOO) technique is considered to be the most objective test technique compared to the other techniques such as hold-out and cross validation. This can be explained by the fact that our training set contains a small number of bags. For each dataset (comprising n bags), only one bag is kept for the test and the remaining bags are used for the training. This action is repeated n times.\nIn order to evaluate the naive approach and the across bag sequences classification approach, we first encode the protein sequences of each bag using a set of features/motifs generated by an existing motif extraction method. Then, we apply an existing classifier to the encoded data. In our tests, we used DMS [12] as a motif extraction method. DMS allows building motifs that can discriminate a family of proteins from other ones. It first identifies motifs in the protein sequences. The extracted motifs are then filtered in order to keep only the discriminative and minimal ones. A substring is considered to be discriminative between the family F and the other families if it appears in F significantly more than in the other families. DMS extracts discriminative motifs according to \u03b1 and \u03b2 thresholds where \u03b1 is the minimum rate of motif occurrences in the sequences of a family F and \u03b2 is the maximum rate of motif occurrences in all sequences except those of the family F . In the following, we present the used motif extraction settings according to the values of \u03b1 and \u03b2:"}, {"heading": "5.3 Experimental results", "text": "In this section, we first provide accuracy and quality results of the proposed approaches. Then, we present a comparison of runtime values of both naive approach, ABClass and ABSim.\n5.3.1 Accuracy In order to use standard multiple instance classifiers, we apply a preprocessing technique that consists in extracting motifs from each set of protein sequences using the DMS\nmethod. Table 1 presents, for each value of \u03b1 and \u03b2, the number of extracted motifs from each set of orthologous protein sequences. For setting 5 (\u03b1 = 1 and \u03b2 = 0), there is no frequent and strictly discriminative motifs for most proteins. This is why we will not use these values of \u03b1 and \u03b2 for our next experiments. We note that the number of extracted motifs increases for high values of \u03b2 and low values of \u03b1. As presented in Table 1, the number of infrequent and non discriminative motifs is very high.\nIn order to encode data, the union of the extracted motifs from each protein is used. These motifs are used as attributes/features to construct a binary table where each row corresponds to sequence. The presence or the absence of an attribute in a sequence is denoted by 1 or 0, respectively. This binary table is called an attribute-value matrix. It is worthwhile to mention that the number of used motifs in the encoding step is huge. Consequently, the attributevalue matrix representing the data becomes large and sparse since only a small subset of the used motifs is representative for each protein. We show in Table 2 the sparsity of the attribute-value matrix which measure the fraction of zero elements over the total number of elements. The sparsity of our attribute-value matrix is generally proportional to the number of used motifs. For example, the sparsity of the matrix goes from 73.9% with 671 motifs to 91.1% with 8077 motifs. Figure 3 shows the accuracy values obtained using naive approach, ABClass approach and ABSim approach. Figures 3(a) and 3(b) show the impact of the set of motifs used in the preprocessing step on the results of the ABClass and the naive approaches. For example, using MISVM classifier, the accuracy varies from 53.5% to 78.5%. Although the motifs extracted using S1 motif extraction setting are discriminative, naive approach does not provide good accuracy results for this setting except for the MISMO classifier. The reason could be that the number of discriminative motifs for some proteins is limited to at most 10 as stressed in Table 1. Using the naive approach, the best accuracy is always provided by MISMO classifier (92.8%). Accuracy of other used multiple instance classifiers depends on the used motifs. Most of them provide good accuracy using the S3 setting (non discriminative motifs with medium frequencies). The number of extracted motifs per protein using this setting is between 228 to 1505 which is an acceptable number of motifs used to encode a protein sequence. Both ABClass and ABSim approaches provide good overall accuracy results since the least accuracy percentage is 89.2%. This clearly shows that our proposed approaches are efficient. Using ABSim approach with the SMS aggregation method provides a better accuracy result compared to the WAMS aggregation method. The best result was reached using ABClass approach, J48 classifier and the motif extraction settings S3 and S4. Using these two settings, a large number of non discriminative motifs are extracted (see Table 1).\nTable 3 presents the rate of classification models that contribute to predict the true class of each bacterium using ABClass approach. We present this rate for the two motif extraction settings that already provided the best accuracy values i.e., S3 and S4. The rate of successful classification models for B1, B11 and B15 are marked with bold text because these three bacteria generate always low rates\ncompared to the rate of successful classification models of the other bacteria. B1 presents variable rates that reach 68%. Although B11 is sometimes successfully classified, its higher successful classification models rate does not exceed 50%. The rate of B15 does not reach 50% which makes this bacterium always misclassified. These results may help to understand some characteristics of the studied bacteria. In particular, M. radiotolerans (B11) and B. abortus (B15) that present the lowest rates. It means that in most cases, M. radiotolerans is predicted as IRSB and B. abortus is predicted as IRRB; the former is an intracellular parasite [9] and the latter is an endosymbiont of most plant species [7]. A probable explanation for these two failed predictions is the increased rate of sequence evolution in endosymbiotic bacteria [18]. As our training set is composed mainly of members of the phylum Deinococcus-Thermus; expectedly, the Deinococcus bacteria (B2-B7) present a very high rate of successful classification models.\n5.3.2 Speedup Figure 4 shows the runtime of our approaches using different settings. It is worthwhile to mention that the runtime of our approaches varies in each LOO iteration according to the number of proteins contained in the query bacterium and the length of protein sequences of the learning set. The indicated runtime in Figure 4 is the average runtime of all the 28 iterations of the LOO evaluation technique. We note that when using the naive approach, the motif extraction is done once in the preprocessing step, whereas when using the ABClass approach it is executed in every LOO iteration.\nThe runtime values of the naive approach (Figures 4(a), 4(b), 4(c), 4(d) and 4(e)) and the ABClass approach (Figures 4(f), 4(g) and 4(h)) are mainly composed of two parts: the motif extraction runtime and the classification time. Generally, these two parts are inversely proportional. For example, a large number of non discriminative motifs is ex-\ntracted in about 9 seconds using the S4 motif extraction setting, then a larger runtime is needed in order to learn a classifier using such high number of motifs. We notice that the motif extraction runtime increases considerably when discriminative motifs are required in the preprocessing step. For example, it goes from 10 seconds for infrequent and non discriminative motifs to about 10 hours for the frequent and discriminative motifs. When the number of extracted motifs is low (frequent and discriminative motifs) the motif extraction time is high but the classification time is still reasonable. As stressed in Figure 4(i), the similarity based approach is much faster than both the naive approach and the ABClass method with most motif extraction settings (S1, S2 and S3). This is particularly due to the fact that the ABSim approach does not need a preprocessing step such as a motif extraction step.\nTaking into account the accuracy and the total runtime, the best result is obtained using the ABClass approach, J48 classifier and S4 motif extraction setting. We notice that MIDD which is an implementation of the Diverse Density algorithm [13] has the longer execution time with low accuracy compared to all other classifiers. According the above presented results, we mention that both ABClass and ABSim are efficient to perform MIL on sequence data that have dependencies between instances across bags. It is also important to recommend the use of ABSim approach when a similarity measure can be easily defined and to use ABClass approach when the data is already preprocessed."}, {"heading": "6. CONCLUSION", "text": "In this paper, we addressed the issue of multiple instance learning (MIL) in the case of sequence data. We focused on data that present dependencies between instances of different bags. We have described two novel approaches for MIL in sequence data: (1) ABClass and (2) ABSim. We\napplied the proposed approaches to the problem of prediction of ionizing radiation resistance (IRR) in bacteria. By running experiments, we have shown that the proposed approaches are efficient. We are able to successfully predict IRR of most bacteria, but we do not reach a 100% accuracy percentage using the different experimental settings with all proteins.\nIn the future work, we will study how the use of a priori knowledge can improve the efficiency of our algorithm. We specifically want to define weights for sequences by using a priori knowledge in the learning phase."}], "references": [{"title": "Basic local alignment search tool", "author": ["S.F. Altschul", "W. Gish", "W. Miller", "E.W. Myers", "D.J. Lipman"], "venue": "Journal of molecular biology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1990}, {"title": "Support Vector Machines for Multiple-Instance Learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Prediction of ionizing radiation resistance in bacteria using a multiple instance learning model", "author": ["S. Aridhi", "H. Sghaier", "M. Zoghlami", "M. Maddouri", "E.M. Nguifo"], "venue": "Journal of Computational Biology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Image categorization by learning and reasoning with regions", "author": ["Y. Chen", "J.Z. Wang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Learning sequential classifiers from long and noisy discrete-event sequences efficiently. Data Mining and Knowledge Discovery, pages", "author": ["G. Daf\u00e9", "A. Veloso", "M. Zaki", "W. Meira Jr."], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artif. Intell.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "1-aminocyclopropane-1-carboxylate (acc) deaminases from methylobacterium radiotolerans and methylobacterium nodulans with higher specificity for acc", "author": ["D.N. Fedorov", "G.A. Ekimova", "N.V. Doronina", "Y.A. Trotsenko"], "venue": "FEMS Microbiol Lett,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Completion of the genome sequence of brucella abortus and comparison to the highly similar genomes of brucella melitensis and brucella suis", "author": ["S.M. Halling", "B.D. Peterson-Burch", "B.J. Bricker", "R.L. Zuerner", "Z. Qing", "L.-L. Li", "V. Kapur", "D.P. Alt", "S.C. Olsen"], "venue": "J Bacteriol,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "The cutting-plane method for solving convex programs", "author": ["J. Kelley"], "venue": "Journal of the Society for Industrial and Applied Mathematics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1960}, {"title": "Identification of novel multitransmembrane proteins from genomic databases using quasi-periodic structural properties", "author": ["J. Kim", "E.N. Moriyama", "C.G. Warr", "P.J. Clyne", "J.R. Carlson"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Encoding of primary structures of biological macromolecules within a data mining perspective", "author": ["M. Maddouri", "M. Elloumi"], "venue": "J. Comput. Sci. Technol.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A Framework for Multiple- Instance Learning", "author": ["O. Maron", "T.L. P\u00e9rez"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Kernel methods for missing variables", "author": ["A.J. Smola", "S. Vishwanathan", "T. Hofmann"], "venue": "In Proceedings of International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Svm-based generalized multiple-instance learning via approximate box counting", "author": ["Q. Tao", "S. Scott", "N. Vinodchandran", "T.T. Osugi"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Solving the multiple-instance problem: A lazy learning approach", "author": ["J. Wang"], "venue": "Proc. 17th International Conf. on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Solving the multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J.-D. Zucker"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Increased rates of sequence evolution in endosymbiotic bacteria and fungi with small effective population sizes", "author": ["M. Woolfit", "L. Bromham"], "venue": "Mol Biol Evol,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "A brief survey on sequence classification", "author": ["Z. Xing", "J. Pei", "E. Keogh"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Predicting types of protein-protein interactions using a multipleinstance learning model", "author": ["H. Yamakawa", "K. Maruhashi", "Y. Nakao"], "venue": "New Frontiers in Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Relieff-mi: An extension of relieff to multiple instance learning", "author": ["A. Zafra", "M. Pechenizkiy", "S. Ventura"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Hydr-mi: A  hybrid algorithm to reduce dimensionality in multiple instance learning", "author": ["A. Zafra", "M. Pechenizkiy", "S. Ventura"], "venue": "Inf. Sci.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Multiple instance learning on structured data", "author": ["D. Zhang", "Y. Liu", "L. Si", "J. Zhang", "R.D. Lawrence"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Multiple instance learning on structured data", "author": ["D. Zhang", "Y. Liu", "L. Si", "J. Zhang", "R.D. Lawrence"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "MIL was originally introduced to solve the problem of drug activity prediction and polymorphism ambiguity [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 19, "context": "Then, it has been applied to several problems such as Protein-Protein Interactions (PPI) [20] and image regions classification in computer vision [4].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "Then, it has been applied to several problems such as Protein-Protein Interactions (PPI) [20] and image regions classification in computer vision [4].", "startOffset": 146, "endOffset": 149}, {"referenceID": 12, "context": "A variety of MIL algorithms have been developed such as Diverse Density [13], Citation-kNN [17], MI-SVM [2] and ar X iv :1 60 2.", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "A variety of MIL algorithms have been developed such as Diverse Density [13], Citation-kNN [17], MI-SVM [2] and ar X iv :1 60 2.", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "A variety of MIL algorithms have been developed such as Diverse Density [13], Citation-kNN [17], MI-SVM [2] and ar X iv :1 60 2.", "startOffset": 104, "endOffset": 107}, {"referenceID": 21, "context": "HyDR-MI [22].", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "Nevertheless, in many applications, the dependencies between across bag instances naturally exist and if incorporated in classification models, they can potentially improve the prediction performance significantly [23].", "startOffset": 214, "endOffset": 218}, {"referenceID": 2, "context": "We applied the proposed approaches to the problem of prediction of IRR in bacteria using MIL techniques introduced and described in [3].", "startOffset": 132, "endOffset": 135}, {"referenceID": 18, "context": "An event can be represented as a symbolic value, a numerical value, a vector of values or a complex data type [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "Existing sequence classification methods can be divided into three large categories [19]: (1) feature based classification, (2) sequence distance based classification and (3) model based classification.", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "The distance function determines the quality of the classification significantly [19].", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "In a recent work [5], authors propose algorithms to learn sequential classifiers from long and noisy discrete-event sequences.", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "[6], in a traditional setting of machine learning, an object is represented by a feature vector (an instance), which is associated to a label.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Recently, several MIL algorithms have been proposed including Diverse Density [13], Citation-kNN [17], MI-SVM [2], and HyDR-MI [22].", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "Recently, several MIL algorithms have been proposed including Diverse Density [13], Citation-kNN [17], MI-SVM [2], and HyDR-MI [22].", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "Recently, several MIL algorithms have been proposed including Diverse Density [13], Citation-kNN [17], MI-SVM [2], and HyDR-MI [22].", "startOffset": 110, "endOffset": 113}, {"referenceID": 21, "context": "Recently, several MIL algorithms have been proposed including Diverse Density [13], Citation-kNN [17], MI-SVM [2], and HyDR-MI [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "Diverse Density (DD) was proposed in [13] as a general framework for solving multiple instance learning problems.", "startOffset": 37, "endOffset": 41}, {"referenceID": 1, "context": "MI-SVM [2] is an adaptation of support vector machines (SVM ) to the MIL problem.", "startOffset": 7, "endOffset": 10}, {"referenceID": 15, "context": "In [16], the authors present two variants of the K-nearest neighbor algorithm called Bayesian-KNN and CitationKNN.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [22], HyDR-MI (which stands for hybrid dimensionality reduction method for multiple instance learning) is proposed as a feature subset selection method for MIL algorithms.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "The hybrid consists of the filter component based on an extension of the ReliefF algorithm [21] developed to work with MIL and the wrapper component based on a genetic algorithm that optimizes the search for the best feature subset from a reduced set of features, output by the filter component.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "In [15], the authors present an SVM-based algorithm via Approximate Box Counting.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "In [24], an optimization algorithm that deals with multiple instance learning on structured data (MILSD) is proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "It is an optimization method that deals with the concave convex objective function with concave convex constraints [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "To solve the problem efficiently, the cutting plane method [10] is used.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "The authors of [24] have present a novel adaption of the cutting plane method that can handle the two sets of constraints simultaneously: the goal is to find two small subsets of constraints from a larger constraint sets.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "For example, in [2], the empirical evaluation is done on three datasets: (1) MUSK dataset, (2) Corel dataset for image annotation and (3) TREC9 dataset for document categorisation.", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "In[15] and [24], a dataset of protein sequences was used in the empirical evaluation.", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "In[15] and [24], a dataset of protein sequences was used in the empirical evaluation.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "These subsequences are aligned mapped to a 8-dimensional feature space: 7 numeric properties [11] and an 8 feature that represents the residue\u2019s position.", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "The second problem is that they don\u2019t deal with the across bag relations that may exists between instances, except the algorithms in [24].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "In [24], the alignment score is used to identify the additional structure information between proteins: if the score between a pair of proteins exceed 25, then authors consider that there exists a link between them.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "We apply the proposed approaches to the problem of phenotype prediction of bacterial Ionizing Radiation Resistance (IRR) that can be formulated as a MIL problem for sequence data [3].", "startOffset": 179, "endOffset": 182}, {"referenceID": 2, "context": "For our tests, we used the dataset described in [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "Each bacterium/bag contains 25 to 31 instances that correspond to proteins implicated in basal DNA repair in IRRB [3].", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "We used WEKA [8] data mining tool in order to apply existing multiple instance classifiers when using the naive approach.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "In our tests, basic local alignment search tool (BLAST) [1] was used for computing local alignments.", "startOffset": 56, "endOffset": 59}, {"referenceID": 11, "context": "In our tests, we used DMS [12] as a motif extraction method.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "abortus is predicted as IRRB; the former is an intracellular parasite [9] and the latter is an endosymbiont of most plant species [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "abortus is predicted as IRRB; the former is an intracellular parasite [9] and the latter is an endosymbiont of most plant species [7].", "startOffset": 130, "endOffset": 133}, {"referenceID": 17, "context": "A probable explanation for these two failed predictions is the increased rate of sequence evolution in endosymbiotic bacteria [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "We notice that MIDD which is an implementation of the Diverse Density algorithm [13] has the longer execution time with low accuracy compared to all other classifiers.", "startOffset": 80, "endOffset": 84}], "year": 2016, "abstractText": "In Multiple Instance Learning (MIL) problem for sequence data, the learning data consist of a set of bags where each bag contains a set of instances/sequences. In many real world applications such as bioinformatics, web mining, and text mining, comparing a random couple of sequences makes no sense. In fact, each instance of each bag may have structural and/or temporal relation with other instances in other bags. Thus, the classification task should take into account the relation between semantically related instances across bags. In this paper, we present two novel MIL approaches for sequence data classification: (1) ABClass and (2) ABSim. In ABClass, each sequence is represented by one vector of attributes. For each sequence of the unknown bag, a discriminative classifier is applied in order to compute a partial classification result. Then, an aggregation method is applied to these partial results in order to generate the final result. In ABSim, we use a similarity measure between each sequence of the unknown bag and the corresponding sequences in the learning bags. An unknown bag is labeled with the bag that presents more similar sequences. We applied both approaches to the problem of bacterial Ionizing Radiation Resistance (IRR) prediction. We evaluated and discussed the proposed approaches on well known Ionizing Radiation Resistance Bacteria (IRRB) and Ionizing Radiation Sensitive Bacteria (IRSB) represented by primary structure of basal DNA repair proteins. The experimental results show that both ABClass and ABSim approaches are efficient.", "creator": "LaTeX with hyperref package"}}}