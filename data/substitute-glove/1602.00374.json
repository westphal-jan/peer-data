{"id": "1602.00374", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2016", "title": "ConfidentCare: A Clinical Decision Support System for Personalized Breast Cancer Screening", "abstract": "Breast transplant colonoscopy adopted threat to ensuring timely diagnosis created one regular screening taken found healthy individuals. Various undergo decisions now needed did get from standardized rather; why include: selecting first computerized tests be an dead to take, interpreting the test outcomes, most proceed what or not same woman should be referred to up validation fast. Such explain types formerly successful by physical practice guidelines (CPGs ), now represent a one - ideal - fits - so particular that particularly newer well work although on 2.4 the to regions, this guaranteeing that it do work this spatially over that christians. Since both prevent and benefits of screening sometimes day-to-day fact each treat features, personalized screening strict nothing are suited still long features many individuals are needed in order this would suggested the time tests are use their the break nurse. In requires however attention this this, reason present ConfidentCare: his computer - initially neurological will support directly made maggie making subscription-based screening advocated from once electronic health record (EHR) material. ConfidentCare logistics including regard clusters example similar patients, two learning is best screening education still favor although each plant. A cluster of patients itself a set entire immune that may elements (nytimes.com. g. age, cure approximate, family nation, caic. ), besides the specific plan has way set included applies on sort nevertheless way recommend up a ager yet her features by screening taken dozens. ConfidentCare integrals ensures for brought policy adopted put come contained including tuberculosis formula_38 a initialization effectiveness mandates now a high level of willingness. We appears reason our predictive out-performed beginning beginning CPGs in significant of cost - component brought facts consistently fall.", "histories": [["v1", "Mon, 1 Feb 2016 03:21:46 GMT  (1448kb)", "http://arxiv.org/abs/1602.00374v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ahmed m alaa", "kyeong h moon", "william hsu", "mihaela van der schaar"], "accepted": false, "id": "1602.00374"}, "pdf": {"name": "1602.00374.pdf", "metadata": {"source": "CRF", "title": "ConfidentCare: A Clinical Decision Support System for Personalized Breast Cancer Screening", "authors": ["Ahmed M. Alaa", "Kyeong H. Moon", "William Hsu"], "emails": ["ahmedmalaa@ucla.edu,", "mihaela@ee.ucla.edu).", "willhsu@mii.ucla.edu)."], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n00 37\n4v 1\n[ cs\n.L G\n] 1\nF eb\nBreast cancer screening policies attempt to achieve timely diagnosis by the regular screening of apparently healthy women. Various clinical decisions are needed to manage the screening process; those include: selecting the screening tests for a woman to take, interpreting the test outcomes, and deciding whether or not a woman should be referred to a diagnostic test. Such decisions are currently guided by clinical practice guidelines (CPGs), which represent a \u201cone-size-fits-all\u201d approach that are designed to work well on average for a population, without guaranteeing that it will work well uniformly over that population. Since the risks and benefits of screening are functions of each patients features, personalized screening policies that are tailored to the features of individuals are needed in order to ensure that the right tests are recommended to the right woman. In order to address this issue, we present ConfidentCare: a computer-aided clinical decision support system that learns a personalized screening policy from the electronic health record (EHR) data. ConfidentCare operates by recognizing clusters of \u201csimilar\u201d patients, and learning the \u201cbest\u201d screening policy to adopt for each cluster. A cluster of patients is a set of patients with similar features (e.g. age, breast density, family history, etc.), and the screening policy is a set of guidelines on what actions to recommend for a woman given her features and screening test scores. ConfidentCare utilizes an iterative algorithm that applies K-means clustering to the women\u2019s feature space, followed by learning an active classifier (decision tree) for every cluster. The algorithm ensures that the policy adopted for every cluster of patients satisfies a predefined accuracy requirement with a high level of confidence. We show that our algorithm outperforms the current CPGs in terms of cost-efficiency and false positive rates.\nIndex Terms\nBreast cancer, Confidence measures, Clinical decision support, Personalized medicine, Supervised learning.\nA. M. Alaa, K. H. Moon, and M. van der Schaar are with the Department of Electrical Engineering, University of California Los Angeles,\nUCLA, Los Angeles, CA, 90024, USA (e-mail: ahmedmalaa@ucla.edu, mihaela@ee.ucla.edu). This work was supported by the NSF.\nW. Hsu is with the Department of Radiological Sciences, UCLA, Los Angeles, CA 90024, USA (email: willhsu@mii.ucla.edu).\nFebruary 2, 2016 DRAFT\n1 ConfidentCare: A Clinical Decision Support\nSystem for Personalized Breast Cancer\nScreening\nI. INTRODUCTION\nPersonalized medicine is a new healthcare paradigm that aims to move beyond the current \u201cone-size-fits-all\u201d approach to medicine and, instead, takes into account the features and traits of individual patients: their genes, microbiomes, environments, and lifestyles [1]-[3]. Vast attention has been recently dedicated to research in personalized medicine that builds on data science and machine learning techniques in order to customize healthcare policies. For instance, the White House has led the \u201cprecision medicine initiative\u201d [4], which is scheduled for discussion in the American Association for the Advancement of Science annual meeting for the year 2016 [5]. Breast cancer screening is one example for a healthcare process that can potentially benefit from personalization. Screening is carried out in order to diagnose a woman with no apparent symptoms in a timely manner [6]-[8]. However, the screening process entails both benefits and costs that can differ from one patient to another [9], which signals the need for personalized screening policies that balance such benefits and costs in a customized manner.\nIn this paper, we present ConfidentCare: a clinical decision support system (CDSS) that is capable of learning and implementing a personalized screening policy for breast cancer. The personalized screening policy is learned from data in the electronic health record (EHR), and is aimed to issue recommendations for different women with different features on which when should they take screening tests, which specific tests to take, and in what sequence. ConfidentCare discovers subgroups of \u201csimilar\u201d patients from the EHR data, and learns how to construct a screening policy that will work well for each subgroup with a high level of confidence. Our approach can provide significant gains in terms of both the cost-efficiency, and the accuracy of the screening process as compared to other \u201cone-size-fits-all\u201d approaches adopted by current clinical practice guidelines (CPGs) that apply the same policy on all patients."}, {"heading": "A. Breast cancer screening and the need for personalization", "text": "While breast cancer screening is believed to reduce mortality rates [8], it is associated with the risks of \u201coverscreening\u201d, which leads to unnecessary costs, and \u201coverdiagnosis\u201d, which corresponds to false positive diagnoses that lead the patients to receive unnecessary treatments [9]. While different patients have different levels of risks for developing breast cancer [10]-[14]; different tests have different monetary costs, and different levels of accuracy that depend on the features of the patient [15]; common CPGs are aimed at populations, and are not typically tailored to specific individuals or significant subgroups [16]-[19].\nFebruary 2, 2016 DRAFT\n2 Being designed to work well on \u201caverage\u201d for a population of patients, following CPGs may lead to overscreening or overdiagnosis for specific subgroups of patients, such as young women at a high risk of developing breast cancer, or healthy older women who may have a relatively longer expected lifespan [20]. Moreover, some screening tests may work well for some patients, but not for others (e.g. a mammogram test will exhibit low accuracy for patients with high breast density [15]), which can either lead to \u201coverdiagnosis\u201d or poor tumor detection performance. Migrating from the \u201cone-size-fits-all\u201d screening and diagnosis policies adopted by CPGs to more individualized policies that recognizes and approaches different subgroups of patients is the essence of applying the personalized medicine paradigm to the breast cancer clinical environment [15], [20]-[23]."}, {"heading": "B. Contributions", "text": "ConfidentCare is a computer-aided clinical decision support system that assists clinicians in making decisions on which (sequence of) screening tests a woman should take given her features. ConfidentCare resorts to the realm of supervised learning in order to learn a personalized screening policy that is tailored to subgroups of patients. In particular, the system recognizes different subgroups of patients, learns the policy that fits each subgroup, and prompts recommendations for screening tests and clinical decisions that if followed will lead to a desired accuracy requirement with a desired level of confidence.\nFig. ?? offers a system-level illustration for ConfidentCare1. The system operates in two stages: an offline stage in which it learns from the EHR data how to cluster patients, and what policy to follow for every cluster, and an execution stage in which it applies learned policy to every woman by first matching her with the closest cluster of patients in the EHR, and then approach her with the policy associated with that cluster. The main features of ConfidentCare are:\n\u2022 ConfidentCare discovers a set of patients\u2019 subgroups. Given certain accuracy requirements and confidence levels\nset by the clinicians, ConfidentCare ensures that every subgroup of patients would experience a diagnostic accuracy, and a confidence level on that accuracy, that meets these requirements. Thus, unlike CPGs that perform well only on average, ConfidentCare ensures that performance is reasonable for every discovered subgroups of patients. \u2022 ConfidentCare ensures cost-efficiency, i.e. patients are not overscreened, and the sequence of recommended\nscreening tests minimizes the screening costs.\nWe show that ConfidentCare can improve the screening cost-efficiency when compared with CPGs, can offer performance guarantees for individual subgroups of patients with a desired level of confidence, and outperforms the \u201cone-size-fits-all\u201d approaches in terms of the accuracy of clinical decisions. Moreover, we show that ConfidentCare can achieve a finer granularity in its learned policy with respect to the patients feature space when it is provided with more training data. Our results emphasize the value of personalization in breast cancer clinical environments, and represent a first step towards individualizing breast cancer screening, diagnosis and treatment.\n1We will revisit this figure and give a more detailed explanation for the system components in the next Section\nFebruary 2, 2016 DRAFT\n3"}, {"heading": "C. Related works", "text": "1) Personalized (precision) medicine: While medical studies investigated the feasibility, potential and impact of applying the concepts of personalized medicine in the breast cancer clinical environments [1]-[3], [15]-[25], [28][29], none of these works provided specific tools or methods for building a personalized healthcare environment. For instance, in [15], it has been shown that CPGs, which recommend screening tests only based on the age ranges, such as the European Society for Medical Oncology (ESMO) CPG and the American Cancer Society (ACS) CPG, are not cost-efficient for many subgroups of patients, where cost-efficiency was measured in terms of \u201ccosts per quality-adjusted life-year\u201d, and the authors recommended that screening should be personalized on the basis of a patient\u2019s age, breast density, history of breast biopsy, and the family history of breast cancer. Similar results were portrayed in other medical studies [23]-[25], all suggesting that personalization using dimensions other than the age can yield more cost efficiency.\nPersonalizing breast cancer screening is envisioned to not only improve the cost-efficiency of the process, but also to improve the diagnostic accuracy. This is because current CPGs do not consider the individual features of a woman when recommending screening tests; thus decisions on which a woman needs to take an additional screening tests, or proceed to a diagnostic test (biopsies) are not tailored to the woman\u2019s individual features. Therefore, false negative diagnoses rates reported by clinicians who follow CPGs reflect the average accuracy over all the population of patients, but CPGs give no guarantee that the diagnostic accuracy and the associated confidence levels of their guidelines are reasonable for every subgroup of \u201csimilar\u201d patients [26][27]; such subgroups can be significantly different in their traits and hence may require being dealt with via different screening and diagnosis policies.\n2) Dynamic treatment regimes: Perhaps the work that relates most to this paper is that on Dynamic treatment regimes (DTRs) [31]-[35]. A DTR is typically a sequence of decision rules, with one rule per stage of clinical intervention, where each rule maps up-to-date patient information to a recommended treatment [31]. DTRs are constructed via reinforcement learning techniques, such as Q-learning, where the goal is to find an \u201coptimal treatment policy\u201d: a sequential mapping of the patient\u2019s information to recommended treatments that would maximize the patient\u2019s long term reward. However, these works profoundly differ from the setting we consider in the following aspects: DTRs are only focused on recommending treatments and do not consider screening and diagnoses; costefficiency is not considered in the design of DTR policies since they only consider the \u201cvalue of information\u201d in recommending treatments; and finally, while confidence measures can be computed for policies in DTRs [33], the policies themselves are not designed in a way that guarantees to the clinician a certain level of reliability for every subgroup of patients.\n3) Active classification for medical diagnosis: Screening and diagnostic clinical decisions typically involve \u201cpurchasing costly information\u201d for the patients, which relates to the paradigm of active learning [41]-[48]. We note that in our setting, clinicians \u201cpurchase\u201d costly features of the patients rather than purchasing unobserved labels, which makes our setting profoundly different from the conventional active learning framework [41]-[43].\nFebruary 2, 2016 DRAFT\n4\nClassification problems in which some features are costly are referred to as \u201cactive classification\u201d [44], or \u201cactive sensing\u201d [47]. Such problems have been addressed in the context of medical diagnosis in [44]-[48], but all these works correspond to solving an unconstrained optimization problem that targets the whole population, for which no personalized accuracy or confidence guarantees can be claimed. Table II positions our paper to the existing literature with respect to various aspects.\nThe rest of the paper is organized as follows. In Section II, we present the system components and the problem formulation for designing personalized screening policies. Next, in Section III, we propose the ConfidentCare algorithm. In Section IV, we carry out various experiments using a dataset collected at the UCLA medical center in order to highlight the advantages of ConfidentCare. Finally, in Section V, we draw our conclusions."}, {"heading": "II. CONFIDENTCARE: SYSTEM COMPONENTS AND OPERATION", "text": ""}, {"heading": "A. System operation", "text": "ConfidentCare is a computer-aided clinical decision support system that learns a personalized screening policy from the EHR data. By a \u201cpersonalized screening policy\u201d we mean: a procedure for recommending an action for the clinician to take based on the individual features of the patient, and the outcomes of the screening tests taken by that patient. An action can be: letting the patient take an additional screening test, proceed to a diagnostic test (e.g. biopsy), or just recommend a regular follow-up.\nThe tasks that ConfidentCare carries out can be summarized as follows:\n\u2022 Discover the granularity of the patient\u2019s population: The system is provided with training data from the EHR\nthat summarizes previous experiences of patients in terms of the screening tests they took, their test results, and their diagnoses. From such data, ConfidentCare recognizes different subgroups or clusters of patients who are similar in their features and can be approached using the same screening policy. \u2022 Learn the best policy for each subgroup of patients: Having discovered the distinct subgroups of patients\nfrom the training data, ConfidentCare finds the best screening policy for each of these subgroups; by a \u201cbest\u201d policy we mean: a policy that minimizes the screening costs while maintaining a desired level of diagnostic accuracy, with a high level of confidence that is set by the clinicians. The more training data provided to\nFebruary 2, 2016 DRAFT\n5 ConfidentCare, the more \u201cgranular\u201d is the learned policy in the sense that more subgroups of patients can be discovered, and thus the extent of personalization and precision would increase consequently. \u2022 Identify the incoming patients\u2019 subgroups and execute their personalized policies: After being trained,\nConfidentCare handles an incoming patient by observing her features, identifying the subgroup to which she belongs, and decides the appropriate screening policy that needs to be followed for her.\nConfidentCare can be thought of as an algorithm that stratifies the pool of patients into clusters, and automatically generates multiple CPGs, one for each cluster, in order to issue the best customized guidelines to follow for each cluster. The algorithm ensures that the accuracy of clinical decisions for each cluster satisfy a certain requirement with a certain confidence level.\nB. Idiosyncrasies of the breast cancer clinical environment\nPatients\u2019 features fall into two categories: personal features, and screening features. Personal features are observable at no cost, and are accessible without the need for taking any screening tests, for that they are provided by the patient herself via a questionnaire, etc. The personal features include numerical and categorical features such as: age, age at menarche, number of previous biopsies, breast density, age at first child birth, and the family history [15].\nScreening tests reveal another set of costly features for the patient, which we call: the screening features. The screening features comprise the radiological assessment of breast images, usually encoded in the form of BIRADS (Breast Imaging Report and Data System) scores [26]. The BI-RADS scores take values from the set {1, 2, 3, 4A, 4B, 4C, 5, 6}, the interpretation of which is given in Table II. BI-RADS scores of 3 or above are usually associated with followup tests or biopsy. The descriptions of all the personal and screening features are shown in Table III.\nConfidentCare considers three possible multimedia-based screening tests in the screening stage, which represent three different imaging modalities: mammogram (MG), ultrasound (US), and magnetic resonance imaging (MRI). Every screening test is associated with different costs and risks, which are functions of the patients\u2019 personal features. We consider a generic cost function that incorporates both the misclassification costs in addition to the monetary costs (the detailed cost model is provided in the next subsection) [25]. Other screening features can also include genetic ones, yet we do not consider these in this paper since such features are not revealed by the screening tests under consideration. However, ConfidentCare algorithm together with the theoretical framework tackled in this section can handle any generic class of features and tests, including genetic tests.\nConfidentCare recommends an action upon observing the outcome of a specific screening test. The actions can either be: recommend a regular (1 year) followup, recommend a diagnostic test (biopsy), or an intermediate recommendation for an additional (costly) screening test (short-ter followup). The final action recommended by the screening policy is either to proceed to a diagnostic test, or to take a regular followup (screening) test after 1 or 2 years. The accuracy measures that we adopt in this paper are: the false positive rate (FPR) and the false negative rate (FNR), which are defined as follows: the FPR is the probability that a patient with a negative true diagnosis (benign or no tumor) is recommended to proceed to a diagnostic test, whereas the FNR is the probability that a\nFebruary 2, 2016 DRAFT\n6\npatient with a positive true diagnosis (malignant tumor) is recommended to take a regular followup screening test [30].\nFebruary 2, 2016 DRAFT\n7"}, {"heading": "C. System components", "text": "ConfidentCare is required to deal with the environment specified above and carry out the three tasks mentioned earlier, which are: discovering the granularity of the patients\u2019 population, learning the appropriate policies for each subgroup of patients, and handling incoming patients by executing the learned, personalized policy that best matches their observed features and traits.\nIn the following, we describe the ConfidentCare algorithm, which implements those tasks using supervised\nlearning. The algorithm requires the following inputs from the clinician:\n\u2022 A training set comprising a set of patients with their associated features, screening tests taken, and their true\ndiagnoses.\n\u2022 A restrictions on the maximum tolerable FNR. \u2022 A desired confidence level on the FNR in the diagnoses issued by the system.\nProvided by the inputs above, ConfidentCare operates through two basic stages:\n\u2022 Offline policy construction stage: Given the training data and all the system inputs, ConfidentCare implements\nan iterative algorithm to cluster the patients\u2019 personal feature space, and then learns a separate active classifier for each cluster of patients. Each active classifier associated with a cluster of patients is designed such that it minimizes the overall screening costs, and meets the FNR and confidence requirements. The algorithm runs iteratively until it maximizes the number of patient clusters for which there exist active classifiers that can guarantee the performance and confidence requirements set by the clinician, thereby ensuring the maximum level of personalization, i.e. ensure that the space of all patients\u2019 personal features is segmented into the finer possible set of partitions, where the performance requirements hold for each of such partitions.\n\u2022 Policy execution stage: Having learned a policy based on the training data, ConfidentCare executes the policy\nby observing the personal features of an incoming patient, associate her with a cluster (and consequently, an already learned active classifier), and then the classifier handles the patient by recommending screening tests and observing the test outcomes, until a final action is recommended.\nFig. ?? illustrates the components and operation of ConfidentCare. In the offline policy construction stage, ConfidentCare is provided with training data from the EHR, the maximum tolerable FNR, and the desired level of confidence. ConfidentCare runs an iterative algorithm that clusters the patients\u2019 personal feature space, and learns the best active classifier (the most cost-efficient classifier that meets the FNR accuracy and confidence requirements) for each cluster. In the policy execution stage, ConfidentCare observes the personal features of the incoming patient, associates her with a patients cluster, and then recommends a sequence of screening tests to that patient until it issues a final recommendation.\nFor instance, assume that the set of personal features are given by a tuple (Age, breast density, number of first degree relatives with breast cancer). A patient with a personal features vector (55, 40%,0) is approached by ConfidentCare. The system associates the patient with a certain cluster of patients that it has learned from the EHR data. Let the best policy for screening patients in that cluster, as computed by ConfidentCare, is to start with\nFebruary 2, 2016 DRAFT\n8 mammogram. If the clinician followed such a recommendation, ConfidentCare observed the mammogram BI-RADS score, say a score of 1, and then it decides to issue a final recommendation for a regular followup. If the BI-RADS score was higher, say a score of 4A, then the system recommends an additional imaging test, e.g. an MRI, and then observes the BI-RADS score of the MRI before issuing further recommendations. The process proceeds until a final recommendation is issued."}, {"heading": "III. THE PERSONALIZED SCREENING POLICY DESIGN PROBLEM", "text": "ConfidentCare uses supervised learning to learn a personalized screening policy from the EHR. In this subsection,\nwe formally present the learning model under consideration.\n1) Patients\u2019 features: Let Xd, Xs, and Y be three spaces, where Xd is the patients\u2019 d-dimensional personal feature space, Xs = Bs is the s-dimensional space of all screening features, where B = {1, 2, 3, 4A, 4B, 4C, 5, 6}, and Y is the space of all possible diagnoses, i.e. Y = {0, 1}, where 0 corresponds to a negative diagnosis, and 1 corresponds to a positive diagnosis. The patients\u2019 feature space is (d+s)-dimensional and is given by X = Xd\u00d7Xs. Each instance in the feature space is a (d+ s)-dimensional vector x = (xd,xs) \u2208 X ,xd \u2208 Xd,xs \u2208 Xs, the entries of which correspond to the personal and screening features listed in Table III, and are drawn from an unknown stationary distribution D on X \u00d7 Y , i.e. (x, y) \u223c D, where y \u2208 Y , and Dx is the marginal distribution of the patients\u2019 features, i.e. x \u223c Dx. The set of s available tests is denoted by T , where |T | = s.\nThe personal features are accessible by ConfidentCare with no cost, whereas the screening features are costly, for that the patient needs to take screening tests to reveal their values. Initially, the entries of xs are blocked, i.e. they are all set to an unspecified value \u3008\u2217\u3009, and they are observable only whenever the corresponding screening tests are taken, and their costs are paid. We denote the space of all possible screening test observations as X \u2217s = {B, \u3008\u2217\u3009} s. ConfidentCare issues recommendations and decisions based on both the fully observed personal features xd, and a partially observed version of xs, which we denote as x\u2217s \u2208 X \u2217 s . The screening feature vector xs can indeed be fully observed, but this would be the case only if all the screening tests were carried out for a specific patient.\nIn order to clarify the different types of features and their observability, consider the following illustrative example. Assume that we only have two personal features: the age and the number of first degree relatives who developed breast cancer, whereas we have three screening tests T = {MG,MRI,US}. That is, we have that d = 2 and s = 3. Initially, ConfidentCare only observes the personal features, e.g. observing a feature vector (42, 1, \u3008\u2217\u3009 , \u3008\u2217\u3009 , \u3008\u2217\u3009) means that the patient\u2019s age is 42 years, she has one first degree relative with breast cancer, and she took no screening tests. Based on the learned policy, ConfidentCare then decides which test should the patient take. For instance, if the policy decides that the patient should take a mammogram test, then the feature vector can then be updated to be (42, 1, 2, \u3008\u2217\u3009 , \u3008\u2217\u3009), which means that the BI-RADS score of the mammogram is 2. ConfidentCare can then decide what action should be recommended given that the BI-RADS score of the mammogram is 2: classify the patient as one who needs to proceed to a diagnostic test, or classify the patient as one who just needs to take a regular followup test in a 1 year period, or request an additional screening test result in order to be able to issue a confident classification for the patient.\nFebruary 2, 2016 DRAFT\n2) Active classification: The process described in the previous subsection is a typical active classification process: a classifier aims to issue either a positive or a negative diagnosis (biopsy or regular followup) for patients based on their costly features (test outcomes). Such a classifier is active in the sense that it can query the clinician for costly feature information rather than passively dealing with a given chunk of data [44]. This setting should not be confused with conventional active learning, where labels (and not features) are the costly piece of information which the classifier may need to purchase [41][42]. In the following, we formally define an active classifier.\nDefinition 1: (Active classifier) An active classifier is a hypothesis (function)\nh : X \u2217s \u2192 Y \u222a T .\nThus, the active classifier either recommends a test in T , or issues a final recommendation y \u2208 Y , where y = 1 corresponds to recommending a biopsy (positive screening test result) and y = 0 is recommending a regular followup (negative screening test result), given the current, partially observed screening feature vector x\u2217s \u2208 X \u2217 s . Whenever a test is taken, the screening feature vector is updated, based upon which the classifier either issues a new recommendation.\nFor instance, the range of the function h in our setting can be {0, 1,MG,MRI,US}, i.e. Y = {0, 1} and T = {MG,MRI,US}. If h(x\u2217s) = 0 (or 1), then the classifier issues -with high confidence on the accuracy- a final recommendation for a biopsy or a regular followup for the patient with a screening feature vector x\u2217s \u2208 X \u2217 s , whereas if h(x\u2217s) = MG, then the classifier recommends the patient with a screening feature vector x \u2217 s to take a mammogram test. Note that if h((\u3008\u2217\u3009 , \u3008\u2217\u3009 , \u3008\u2217\u3009)) = 0, then the classifier recommends no tests for any patient.\n3) Designing active classifiers: Designing an active classifier for the breast cancer screening and diagnosis problem under consideration cannot rely on conventional loss functions, such as the 0 \u2212 1 loss function. This is because the classification problem involves costly decision making under uncertainty, and different types of diagnostic errors (false negatives and false positives) have very different consequences. Hence, our notion of\nFebruary 2, 2016 DRAFT\n10\nlearning needs to be decision-theoretic, and new objective functions and learning algorithms need to be defined and formulated.\nWe use an inductive bias approach for designing the active classifier; we restrict our learning algorithm to pick one hypothesis h from a specific hypothesis class H. That is, we compensate our lack of knowledge of the stationary distribution D by inducing a prior knowledge on the set of possible hypothesis that the learning algorithm can output: a common approach for designing agnostic learners [50]. Unlike the conventional supervised learning paradigm which picks a hypothesis that minimizes a loss function, we will design a learning algorithm that picks a hypothesis from H, such that the overall cost of screening is minimized, while maintaining the FNR to be below a predefined threshold, with a desired level of confidence; a common design objective for breast cancer clinical systems [27]. The screening cost involves both the monetary costs of the screening tests, as well as the misclassification cost reflected by the FPR. The FNR experienced by the patients when using an active classifier h is given by\nFNR(h) = P (h(x\u2217s) = 0 |h(x \u2217 s) \u2208 Y, y = 1) , (1)\nwhereas the FPR is given by\nFPR(h) = P (h(x\u2217s) = 1 |h(x \u2217 s) \u2208 Y, y = 0) . (2)\nThat is, the FNR is the probability that classifier h recommends a regular followup (outputs a 0) for a screening feature vector xs, when the patient takes all the recommended tests, given that the true diagnosis was 1, whereas the FPR is the probability that the classifier recommends a biopsy (outputs a 1) when the true diagnosis is 0. Both types of error are very different in terms of their implications, and one can easily see that the FNR is more crucial, since it corresponds to misdiagnosing a patient with breast cancer as being healthy [28]. Thus, the system must impose restrictions on the maximum tolerable FNR. On the other hand, the FPR is considered as a misclassification cost that we aim at minimizing given a constraint on the FNR [25].\nNow we define the screening cost function. Let cT be the monetary cost of test T \u2208 T , which is the same\nfor all patients, and let c\u0304T be the normalized monetary cost of test T , given by c\u0304T = cT\u2211 T \u2032 \u2208T c T \u2032 . Let c\u0304(h(xs)) be the total (normalized) monetary test costs that classifier h will pay in order to reach a final recommendation for a patient with screening feature vector xs. The average monetary cost of a hypothesis h is denoted as c\u0304(h), and is given by c\u0304(h) = E [c\u0304(h(xs))] , where the expectation is taken over the randomness of the screening test results. To illustrate how the cost of a hypothesis is computed, consider the following example. Let the normalized costs of MG, US, and MRI be 0.1, 0.2 and 0.7 respectively. Initially, the classifier observes x\u2217s = (\u3008\u2217\u3009 , \u3008\u2217\u3009 , \u3008\u2217\u3009) . Assume a hypothesis h1 and a patient with a screening features vector xs = (3, 1, 1). The hypothesis h1 has the following functional form: h1((\u3008\u2217\u3009 , \u3008\u2217\u3009 , \u3008\u2217\u3009)) = MG, i.e. it initially recommends a mammogram for every patient, h1((3, \u3008\u2217\u3009 , \u3008\u2217\u3009)) = MRI, and h1((3, 1, \u3008\u2217\u3009)) = 0. Hence, using h1, the screening cost is 0.8. Let h2 be another hypothesis with h2((\u3008\u2217\u3009 , \u3008\u2217\u3009 , \u3008\u2217\u3009)) = MG, h2((3, \u3008\u2217\u3009 , \u3008\u2217\u3009)) = 0. In this case, we have that c\u0304(h2) = 0.1, which is less than c\u0304(h1) = 0.8, yet it is clear that h2 has a higher risk for a false negative diagnosis.\nLet C(h) be the cost function for hypothesis h, which incorporates both the average monetary costs and the\naverage misclassification costs incurred by h. Formally, the cost function is defined as\nC(h) = \u03b3 FPR(h) + (1 \u2212 \u03b3) c\u0304(h), (3)\nFebruary 2, 2016 DRAFT\n11\nwhere \u03b3 \u2208 [0, 1] is a parameter that balances the importance of the misclassification costs compared to the monetary cost, i.e. \u03b3 = 0 means that ConfidentCare builds the classifiers by solely minimizing monetary costs, whereas \u03b3 = 1 means that ConfidentCare cares only about the misclassification costs. An optimal active classifier is denoted by h\u2217, and is the one that solves the following optimization problem\nmin h\u2208H C(h) s.t. FNR(h) \u2264 \u03b7. (4)\nObtaining the optimal solution for (4) requires knowledge of the distribution D, in order to compute the average FNR and cost in (4). However, D is not available for the (agnostic) learner. Instead, the learner relies on a size-m training sample Sm = (xi, yi)i\u2208[m], with Sm i.i.d \u223c D\u2297m, where D\u2297m is the product distribution of the m patientdiagnosis instances (xi, yi)i\u2208[m]. The training sample Sm feeds a learning algorithm A : Sm \u2192 H, where Sm is the space of all possible size-m training samples. The learning algorithm A simply tries to solve (4) by picking a hypothesis in H based only on the observed training sample Sm, and without knowing the underlying distribution D. Fig. 1 depicts the framework for learning and implementing an active classifier.\n4) Learnability of active classifiers: In order to evaluate the learner, and its ability to construct a reasonable solution for (4), we define a variant of the probably approximately correct criterion for learning active classifiers that minimize the classification costs with a constraint on the FNR (conventional definitions for PAC-learnability can be found in [44] and [50]). Our problem setting, and our notion of learning depart from conventional supervised learning in that the learner is concerned with finding a feasible, and (almost) optimal solution for a constrained optimization problem, rather than being concerned with minimizing an unconstrained loss function.\nIn the following, we define a variant for the notion of PAC-learnability, the probably approximately optimal\n(PAO) learnability, of a hypothesis set H that fits our problem setting.\nDefinition 2: (PAO-learning of active classifiers) We say that active classifiers drawn from the hypothesis set\nH are PAO-learnable using an algorithm A if:\n\u2022 H\u2217 = {h : \u2200h \u2208 H, FNR(h) \u2264 \u03b7} 6= \u2205, with h\u2217 = arg infh\u2208H\u2217 C(h). \u2022 For every (\u01ebc, \u01eb, \u03b4) \u2208 [0, 1]3, there exists a polynomial function N\u2217H(\u01eb, \u01ebc, \u03b4) = poly( 1 \u01ebc , 1\u01eb , 1 \u03b4 ), such that for\nevery m \u2265 N\u2217H(\u01eb, \u01ebc, \u03b4), we have that\nPSm\u223cD\u2297m (C (A (Sm)) \u2265 C(h \u2217) + \u01ebc) \u2264 \u03b4,\nPSm\u223cD\u2297m (FNR(A (Sm)) \u2265 FNR(h \u2217) + \u01eb) \u2264 \u03b4,\nwhere N\u2217H(\u01eb, \u01ebc, \u03b4) is the sample complexity of the classification problem.\nPAO-learnability reflects the nature of the learning task of the active classifier; a learning algorithm is \u201cgood\u201d if it picks the hypothesis that, with a probability 1\u2212 \u03b4, is within an \u01eb from the region of feasible region, and within an \u01ebc from the optimal solution. In that sense, a hypothesis set is PAO-learnable if there exists a learning algorithm that can find, with a certain level of confidence, a probably approximately feasible and optimal solution to (4).\nNote that the sample complexity N\u2217H(\u01eb, \u01ebc, \u03b4) does not depend on \u03b7, yet the feasibility of the optimization problem in (4), and hence the learnability of the hypothesis class, depends on both the value of \u03b7 and the hypotheses in H.\nFebruary 2, 2016 DRAFT\n12\nFrom a bias-variance decomposition point of view, one can view \u03b7 as a restriction on the amount of inductive bias a hypothesis set can have with respect to the FNR, whereas \u01eb, \u01ebc and \u03b4 are restrictions on the true cost and accuracy estimation errors that the agnostic learner would encounter. The threshold \u03b7 qualifies or disqualifies the whole hypothesis set H from being a feasible set for learning the active classifier, whereas the tuple (\u01eb, \u01ebc, \u03b4) decides how many training samples do we need in order to learn a qualified hypothesis set H. The notion of PAO-learnability can be thought of as a decision-theoretic variant of the conventional PAC-learnability, for that the learner is effectively solving a constrained cost-minimization problem.\n5) Patients feature space partitioning: ConfidentCare learns a different classifier separately for every subgroup of \u201csimilar\u201d patients, which is the essence of personalization. However, the clustering of patients into subgroups is not an input to the system, but rather a task that it has to carry out; ConfidentCare has to bundle patients into M subgroups, each of which will be assigned a different active classifier that is tailored to the features of the patients in that subgroup. The value of M reflects the level of personalization, i.e. the larger M is, the larger is the number of possible classifiers that are customized for every subgroup. Partitioning the patient\u2019s population into subgroups is carried out on the basis of the personal features of the patients; patients are categorized based on their personal, fully observable features.\nLet (Xd, dx) be a metric space associated with the personal feature space Xd, where dx is a distance metric, i.e. dx : Xd \u00d7Xd \u2192 R+. We define an M -partitioning \u03c0M (Xd, dx) over the metric space (Xd, dx) as a set of disjoint subsets of Xd, i.e. \u03c0M (Xd, dx) = {C1, C2, . . ., CM}, where Ci \u2286 Xd, \u22c3M i=1 Ci = Xd, and Cj \u22c2 Ci = \u2205, \u2200i 6= j. We define a function \u03c0M (Xd, dx;xd) as a map from the patient\u2019s personal feature vector xd to the index of the partition to which she belongs, i.e. \u03c0M (Xd, dx;xd) = j if xd \u2208 Cj .\nEach partition is simply a subgroup of patients who are believed to be \u201csimilar\u201d, where similarity is quantified by a distance metric. By \u201csimilar\u201d patients, we mean patients who have similar risks of developing breast cancer, and experience similar levels of accuracy for the different screening tests.\n6) Personalization and ConfidentCare\u2019s optimization problem: Given a certain partitioning \u03c0M (Xd, dx) of the personal feature space, the task of the learner is to learn an active classifier hj \u2208 H for each partition Cj , that provides (average) performance guarantees for the patients in that partition if the size of the training set is large enough, i.e. larger than the sample complexity2. This may not be feasible if the size of the training sample is not large enough in every partition, or if the hypothesis set has no feasible hypothesis that have a true FNR less than \u03b7 for the patients in that partition. The following definition captures the extent of granularity with which ConfidentCare can handle the patient\u2019s population.\nDefinition 3: (M -personalizable problems) We say that the problem (H, Sm, \u03b4, \u01eb, \u01ebc,D) is M -personalizable if there exists an M -partitioning \u03c0M (Xd, dx), such that for every partition Cj \u2208 \u03c0M (Xd, dx), H is PAO-learnable, and we have that mj \u2265 N\u2217H(\u01eb, \u01ebc, \u03b4), where mj = \u2223 \u2223Sjm \u2223 \u2223, and Sjm = |{(xi, yi) : i \u2208 [m],xi,d \u2208 Cj}|.\n2Note that the training set Sm is drawn from the total population of patients, but each active classifier associated with a certain partition is\ntrained using training instances that belong to that partition only.\nFebruary 2, 2016 DRAFT\n13\n\u03a0 =\n{\n\u03c0M (Xd, dx) = {C1, . . ., CM}\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2200Ci \u2229 Cj = \u2205, M \u22c3\ni=1\nCi = Xd, Ci \u2200M \u2208 {1, 2, . . ., |Xd|}\n}\n. (5)\nmax \u03c0M(Xd,dx)\u2208\u03a0 M\ns.t. (H, Sm, \u01eb, \u03b4, \u01ebc,D) is M -personalizable over \u03c0M (Xd, dx).\n(6)\nThat is, a problem is M -personalizable if H has a non-empty set of feasible hypotheses for every partition, and the number of training samples in every partition is greater than the sample complexity for learning H.\nConfidentCare is not provided with a feature space partitioning, but is rather required to construct it, i.e. the system should recognize the maximum number of patient subgroups for which it can construct separate active classifiers that meet the accuracy requirements. Partitioning Xd and designing an active classifier for every partition is equivalent to designing a personalized screening policy. Fig. 2 depicts the envisioned output of ConfidentCare for a 2D personal feature space: the feature space is partitioned into 4 partitions, and with each partition, an active classifier (a decision tree) is associated.\nLet \u03a0 be the set of all possible partitioning maps for the feature space as defined in (5). ConfidentCare aims at maximizing the granularity of its screening policy by partitioning the feature space into the maximum possible number of patient subgroups, such that the active classifier associated with each subgroup of patients ensures that the FNR of this subgroup does not exceed \u03b7, with a confidence level of 1\u2212\u03b4. Thus, ConfidentCare is required to solve the optimization problem in (6). Once the optimal partitioning \u03c0\u2217M (Xd, dx) is found by solving (6), the associated costoptimal classifiers are constructed by solving (4). Designing a screening policy computation algorithm is equivalent to designing a partitioning algorithm Apart : Sm \u2192 \u03a0, and a learning algorithm A : Sjm \u2192 H. ConfidentCare\nFebruary 2, 2016 DRAFT\n14\nwould operate by running the partitioning algorithm Apart to create a set of partitions of the personal feature space, and then running the learning algorithm A once for each partition in order to find the appropriate hypothesis for that partition. Ideally, ConfidentCare computes an optimal screening policy if the partitioning found by Apart is a solution to (6)."}, {"heading": "IV. CONFIDENTCARE ALGORITHM: ANALYSIS AND DESIGN", "text": "We start by analyzing the problem of constructing an optimal screening policy in the next subsection, before we\npresent our proposed algorithm in the following subsection."}, {"heading": "A. Optimal screening policies: analysis and technical challenges", "text": "We start exploring the limits of the policy design problem by computing an upper-bound on the maximum level\nof personalization that can be achieved by any screening policy in the following Theorem.\nTheorem 1: The maximum level of personalization that can be achieved for the problem (H, Sm, \u01eb, \u01ebc, \u03b4,D) is\nupper-bounded by\nM\u2217 \u2264\n\u230a\nm\nN\u2217(\u03b4, \u01eb, \u01ebc)\n\u230b\n,\nwhere M\u2217 is the solution for (6).\nProof See Appendix A.\nTheorem 1 captures the intuitive dependencies of the level of personalization on m and (\u01eb, \u01ebc, \u03b4). As the training sample size increases, a finer granularity of the screening policy can be achieved, whereas decreasing any of (\u01eb, \u01ebc, \u03b4) will lead to a coarser policy that has less level of personalization.\nWhile Theorem 1 gives an upper-bound on the possible level of personalization, it does not tell whether such a bound is indeed achievable, i.e. is there a computationally-efficient partitioning algorithm Apart, and a learning algorithm A, through which we can we construct an optimal personalized screening policy given a hypothesis set H and a training sample Sm? In fact, it can be shown that for any hypothesis class H, the problem of finding the maximum achievable level of personalization in (6) is NP-hard. Thus, there is no efficient polynomial-time algorithm Apart that can find the optimal partitioning of the personal feature space, and hence ConfidentCare has to discover the granularity of the personal feature space via a heuristic algorithm as we will show in the next subsection.\nNow we focus our attention to the learning algorithm A. Given that we have applied a heuristic partitioning algorithm Apart to the training data, and obtained a (suboptimal) partitioning \u03c0M (Xd, dx), what hypothesis set H should we use, and what learning algorithm A should we chose in order to learn the best active classifier for every partition? In order to answer such a question, we need to select both an appropriate hypothesis set and a corresponding learning algorithm. We start by studying the learnability of a specific class of hypothesis sets.\nTheorem 2: A finite hypothesis set H, with |H| < \u221e, is PAO-learnable over a partition Cj \u2208 \u03c0M (Xd, dx) if and\nonly if infh\u2208H FNRj(h) \u2264 \u03b7, where FNRj is the FNR of patients in partition Cj .\nFebruary 2, 2016 DRAFT\n15\nProof See Appendix B.\nWhile the finiteness of the hypothesis set H is known to the designer, one cannot determine whether such a hypothesis set can support an FNR that is less than \u03b7 since the distribution D is unknown to the learner. Thus, the learnability of a hypothesis set can only be determined in the learner\u2019s training phase, where the learner can infer from the training FNR estimate whether or not infh\u2208H FNR(h) \u2264 \u03b7. Theorem 2 also implies that solving the FNR-constrained cost minimization problem using the empirical estimates of both the cost and the FNR will lead to a solution that with probability 1 \u2212 \u03b4 will be within \u01ebc from the optimal value, and within \u01eb from the FNR constraint. Thus, an algorithm A that solves the constrained optimization problem in (4) \u201cempirically\u201d is a \u201cgood\u201d learner for the hypothesis set H. The key for the result of Theorem 2 is that if |H| < \u221e, then the FNR and cost functions are Glivenko-Cantelli classes [50], for which the uniform convergence property is satisfied, i.e. every large enough training sample can be used to obtain a \u201cfaithful\u201d estimate of the costs and the accuracies of all the hypotheses in the set H. We call the class of algorithms that solve optimization problem in (4) using the empirical cost and FNR measures as empirical constrained cost-minimizers (ECCM)."}, {"heading": "B. ConfidentCare design rationale", "text": "Based on Theorem 2 and the fact that (6) is NP-hard, we know that ConfidentCare will comprise a heuristic partitioning algorithm Apart that obtains an approximate solution for (6), and an ECCM learning algorithm A that picks a hypothesis in H for every partition. Since problem (6) is NP-hard, we use a Divide-and-Conquer approach to partition the feature space: we use a simple 2-mean clustering algorithm Apart to split the a given partition in the personal feature space, and we iteratively construct a decision tree using A for each partition of the feature space, and then split all partitions using Apart, until the algorithm A finds no feasible solution for (7) for any of the existing partitions if they are to be split further.\nThe algorithm A can be any ECCM algorithm, i.e. A solves the following optimization problem\nA(Sjm) = arg min h\u2208H\n1\nmj\n\u2211\n(x,y)\u2208Sjm\nc\u0304 (h(xs))\ns.t.\n\u2211\n(x,y)\u2208Sjm I{h(xs) 6=y,y=1}\n\u2211\n(x,y)\u2208Sjm I{y=1}\n\u2264 \u03b7 \u2212\n\u221a\nlog (|H|) + log ( 4 \u03b4 )\n2mj ,\n(7)\nwhere the constraint in (7) follows from the sample complexity of H, which is N\u2217 (\u01eb, \u01ebc, \u03b4) = log(4|H|/\u03b4) 2min{\u01eb2,\u01eb2c} ."}, {"heading": "C. ConfidentCare algorithm", "text": "The inputs to ConfidentCare algorithm can be formally given by\n\u2022 The size-m training data set Sm = (xi, yi)i\u2208[m]. \u2022 The FNR restriction \u03b7. \u2022 The confidence level 1\u2212 \u03b4.\nThe operation of ConfidentCare relies on a clustering algorithm that is a variant of Lloyd\u2019s K-means clustering algorithm [51]. However, our clustering algorithm will be restricted to splitting an input space into two clusters, thus\nFebruary 2, 2016 DRAFT\n16\nwe implement a 2-means clustering algorithm, for which we also exploit some prior information on the input space. That is, we exploit the risk assessments computed via the Gail model in order to initialize the clusters centroids [11]-[14], thereby ensuring fast convergence. Let G : Xd \u2192 [0, 1] be Gail\u2019s risk assessment function, i.e. a mapping from a patient\u2019s personal feature to a risk of developing breast cancer. Moreover, we use a distance metric that incorporates the risk assessment as computed by the Gail model in order to measure the distance between patients. The distance metric used by our algorithm is\nd(x, x \u2032 ) = \u03b2||x \u2212 x \u2032 ||+ (1 \u2212 \u03b2)|G(x) \u2212G(x \u2032 )|.\nThe parameter \u03b2 quantifies how much information from the Gail model are utilized to measure the similarity between patients. Setting \u03b2 = 0 is equivalent to stratifying the risk space, whereas \u03b2 = 1 is equivalent to stratifying the feature space. The value of \u03b2 needs to be learned as we show later in Section V-B.\nOur clustering function, which we call Split(X\u0304d, dx, \u03c4,\u2206) takes as inputs: a size-N subset of the personal feature\nspace (training set) X\u0304d = {x1d,x 2 d, . . .,x N d } \u2282 Xd, a distance metric dx, a Gail model parameter \u03c4 , and a precision level \u2206. The function carries out the following steps:\n\u2022 Compute the risk assessments { G(xid, \u03c4) }N i=1 for all vectors in the (finite) input space using the Gail model.\nThe parameter \u03c4 corresponds to the time interval over which the risk is assessed, i.e. G(xid, \u03c4) is the probability that a patient with a feature vector xd would develop a breast cancer in the next \u03c4 years.\n\u2022 Set the initial centroids to be \u00b51 = x i\u2217 d , where i\u2217 = argmini G(x i d, \u03c4), and \u00b52 = x i\u2217 d , where i \u2217 =\nargmaxiG(x i d, \u03c4).\n\u2022 Create two empty sets C1 and C2, which represent the members of each cluster. \u2022 Until convergence (where the stopping criterion is determined by \u2206), repeat the following: assign every vector\nx i d to C1 if dx(x i d, \u00b51) < dx(x i d, \u00b52), and assign it to C2 otherwise. Update the clusters\u2019 centroids as follows\n\u00b5j = 1\n|Cj|\nN \u2211\ni=1\nI x i d \u2208Cjx\ni d, j \u2208 {1, 2}.\n\u2022 Return the clusters\u2019 centroids \u00b51 and \u00b52.\nThe rationale behind selecting the initial centroids as being the feature vectors with maximum and minimum risk assessments is that those two patients\u2019 features are more likely to end up residing in different clusters. A detailed pseudocode for the clustering function is given in Algorithm 1. As we will show later, ConfidentCare will utilize this function to iteratively partition the personal feature space.\nFor a given feature space partitioning, ConfidentCare builds an active classifier that emulates a \u201cvirtual CPG\u201d for the set of patients within the partition. Designing the active classifier is equivalent to: following an inductive bias approach in which a specific hypothesis class H is picked, and designing an algorithm A that takes the training set Sm as an input and picks the \u201cbest\u201d hypothesis in H, i.e. A(Sm) \u2208 H.\nAdopting decision trees as a hypothesis set is advantageous since such a classifier is widely used and easily interpretable for medical applications [45]-[48]. As shown in Fig. 2, ConfidentCare will associate a decision tree active classifier with every partition of the personal feature space. Such a tree represents the policy to follow with\nFebruary 2, 2016 DRAFT\n17\nAlgorithm 1: Split(X\u0304d, dx, \u03c4,\u2206).\n1 Input: A set N training vectors X\u0304d, K > M , a distance metric dx, a Gail model parameter \u03c4 , and a precision\nlevel \u2206.\n2 Output: Two centroids \u00b51 and \u00b52; 3 Initialize D\u22121 = 1, D0 = 0, k = 0, and \u00b51 = x i\u2217 d , i\u2217 = argminiG(x i d, \u03c4), ; 4 \u00b52 = x i\u2217 d , i \u2217 = argmaxi G(x i d, \u03c4) ; 5 C1 = \u2205, C2 = \u2205 ; 6 while Dk\u22121\u2212DkDk > \u2206 do 7 C1 = { x i d \u2223 \u2223\u2200xid \u2208 Xd, dx(x i d, \u00b51) < dx(x i d, \u00b52) } ; 8 C2 = X\u0304d/C1; 9 \u00b51 = 1\n|C1| \u2211N i=1 Ixid\u2208C1 x i d;\n10 \u00b52 = 1\n|C2| \u2211N i=1 Ixid\u2208C2 x i d;\n11 Set k \u2190 k + 1; 12 Compute the 2-means objective function Dk = 1N \u22112 j=1 \u2211N i=1 Ixid\u2208Cj dx(x i d, \u00b5j); 13 end\npatients who belong to that partition; what tests to recommend and how to map the BI-RADS scores resulting from one test to a new test recommendation or a diagnostic decision.\nLearning the optimal decision tree h\u2217 \u2208 H is known to be an NP-hard problem [52], thus we resort to greedy algorithm A, which we call the confidence-based Cost-sensitive decision tree induction algorithm (ConfidentT ree). The main idea of ConfidentT ree is to select tests (nodes of the tree) in a greedy manner by using a splitting rule that operates as follows: in each step, label the leaves that come out of each possible test such that the pessimistic estimate for the FNR (given the confidence level 1 \u2212 \u03b4) is less than \u03b7, and then pick the test that maximizes the ratio between the information gain and the test cost. After growing such a tree, we apply post-pruning based on confidence intervals of error estimates [53]. If there is no possible labeling of the tree leaves that satisfy the FNR requirements, the algorithm reports the infeasibility of the FNR and confidence levels set by the clinician given the training set provided to the program. More precisely, the algorithm ConfidentT ree(Sm, \u03c0M (Xd, dx), j, \u03b7, 1 \u2212 \u03b4) takes the following inputs:\n\u2022 The size-m training set Sm. \u2022 The personal feature space partitioning \u03c0M (Xd, dx). \u2022 The index j of the partition for which we are designing the active classifier. \u2022 The FNR constraint \u03b7. \u2022 The confidence level 1\u2212 \u03b4.\nGiven these inputs, the algorithm then executes the following steps:\n\u2022 Extract the training instances that belong to partition Cj .\nFebruary 2, 2016 DRAFT\n18\n\u2022 Grow a decision tree with the nodes being the screening tests in T . The edges are the BI-RADS scores with\nthe following thresholds: BI-RADS < 3, BI-RADS \u2208 {3, 4}, and BI-RADS > 4. This classification is based on domain knowledge [26]; the first category corresponds to a probably negative diagnosis, the second corresponds to a suspicious outcome, whereas the third corresponds to a probably malignant tumor. \u2022 While growing the tree, the splitting rule is as follows: for each test, label the leaves such that the pessimistic\nestimate (see [53] for confidence interval and error estimates in the C4.5 algorithm) for the FNR is equal to \u03b7, and then compute the cost function for each test, and select the test that maximizes the ratio between the information gain and the cost function. \u2022 Apply post-pruning based on confidence intervals of the error estimates as in the C4.5 algorithm [53]. This\nstep is carried out in order to avoid overfitting.\n\u2022 If the pessimistic estimate for the FNR exceeds \u03b7, report the infeasibility of constructing a decision tree with\nthe given FNR and confidence requirements.\nA detailed pseudocode for ConfidentT ree is given in Algorithm 2. ConfidentCare invokes this algorithm whenever the personal feature space is partitioned, and the active classifiers need to be constructed.\nAlgorithm 2: ConfidentT ree(Sm, PartM (Xd, dx), j, \u03b7, 1\u2212 \u03b4)\n1 Input: A set of training instances Sm, a partitioning PartM (Xd, dx), a partition index j, Maximum tolerable\nFNR \u03b7, and confidence level 1\u2212 \u03b4.\n2 Output: A cost-sensitive decision-tree hj that can be used as an active classifier for partition Cj .; 3 Let B1 be the event that BI-RADS < 3, B2 be that BI-RADS \u2208 {3, 4}, and B3 be BI-RADS > 4 ; 4 Extract the training set that belong to the targeted partition Sjm = {(xi, yi) |\u2200i \u2208 [m],xi,d \u2208 Cj }; 5 For each test, label the leaves attached to edges B1, B2, and B3 such that the empirical FNR is less than the\nsolution of the following equation for F\u0302\n\u03b7 = F\u0302 + Q \u22121(\u03b4) 2n +Q \u22121(\u03b4)\n\u221a\nF\u0302 n \u2212 F\u0302 2 n + Q\u22121(\u03b4)2 4n2\n1 + Q \u22121(\u03b4)2\nn\n,\nwhere Q(.) is the Q-function and n is the number of training instances covered by the leaf for which the classification is 1. ;\n6 Given this labeling, let F\u0302p be the empirical value of the false positive rate, then pick the test s \u2208 T that\nmaximizes I(s;S j m)\n\u03b3F\u0302p+(1\u2212\u03b3)c\u0304s , where I(x; y) is the mutual information between x and y. ;\n7 Apply post-pruning using confidence intervals for error estimates: a node is pruned if the error estimate of its\ninduced sub-tree i s lower than error estimate of the node.\nConfidentCare uses the modules ConfidentT ree and Split in order to iteratively partition the feature space and construct active classifiers for each partition. ConfidentCare runs in two stages: the offline policy computation stage, and the policy execution stage. In the offline policy computation stage, the following steps are carried out:\n1) Use the Split function to split all current partitions of the personal feature space.\nFebruary 2, 2016 DRAFT\n19\n2) Use the ConfidentT ree to create new active classifiers for the split partitions, if constructing a decision tree\nfor a specific partition is infeasible, stop splitting this partition, otherwise go to step (1).\nAfter computing the policy, ConfidentCare handles the incoming patients in the policy execution stage as follows:\n1) Observe the personal features of the incoming patient, measure the distance between her feature vector and\nthe centroids of the learned partitions, and associate her with the closest partition and the associated active classifier. 2) Apply active classification to the patient. After each test outcome, ConfidentCare prompts a recommended\ntest (the next node in the decision tree), and an intermediate diagnosis together with an associated confidence interval. The clinician and the patient will then decide whether or not to proceed and take the next test.\nThe pseudocode for ConfidentCare in both the offline and online modes is given in Algorithm 3. In the following Theorem, we show that the greedy ConfidentCare algorithm can guarantee a reasonable performance.\nAlgorithm 3: ConfidentCare (Sm, \u03b4, \u03b7).\n1 Input:A training set Sm, required confidence level \u03b4, and FNR constraint \u03b7. 2 Output: A sequence of recommendations, intermediate diagnoses with confidence intervals, and a final\ndiagnosis;\n3 Offline policy computation stage: ; 4 Initialize M = \u221e, q = 0; 5 Initialize \u00b5 = \u2205 (set of centroids of the personal feature space) ; 6 Hyper-parameters \u03c4 , \u03b3, and \u2206 can be tuned through a validation set; 7 while q 6= M do 8 M = |\u00b5| ; 9 Create a partitioning Part(Xd, dx) based on the centroids in \u00b5 ;\n10 For j = 1 to M ;\n11 \u00b5 \u2192 Split(Xd, dx, \u03c4,\u2206);\n12 hj = ConfidentT ree(Sm, PartM (Xd, dx), j, \u03b7, 1 \u2212 \u03b4) ;\n13 If hj is infeasible: q \u2190 q + 1 ; 14 EndFor 15 end 16 Policy execution stage: ; 17 For the incoming patient i, find the partition it belongs to by computing the distance dx(xi,d, \u00b5j) for every\npartition Cj , and associate it with the partition j\u2217 that gives the minimum distance ;\n18 Use classifier hj\u2217 to recommend tests and issue diagnoses\nFebruary 2, 2016 DRAFT\n20\nFig. 3 demonstrates the operation of the iterative algorithm; in each iteration, partitions are split as long as a decision tree for the new partitions are feasible, and the corresponding decision trees are learned. The end result is a set of decision trees for the different partitions, representing different policies to be followed for every class of patients. Following the CPGs correspond to having a single decision tree for the entire personal feature space, which may consistently perform poorly over specific partitions of the feature space, i.e. specific subgroups of patients."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we demonstrate the operation of ConfidentCare by applying it to a real-world dataset for breast cancer patients. Moreover, we evaluate the performance of ConfidentCare and the added value of personalization by comparing it with CPGs, and policies that are designed in a \u201cone-size-fits-all\u201d fashion. We start by describing the dataset used in all the experiments in the following subsection."}, {"heading": "A. Data Description", "text": "We conduct all the experiments in this section using a de-identified dataset of 25,594 individuals who underwent screening via mammograms, MRIs and ultrasound at the UCLA medical center. The features associated with each individual are: age, breast density, ethnicity, gender, family history, age at menarche, age at the first child birth and hormonal history. Each individual has underwent at least one of three screening tests: a mammogram (MG), an MRI, an ultrasound (US), or a combination of those. With each test taken, a BI-RADS score is associated. Table IV shows the entries of the dataset and the features associated with every patient. The dataset is labeled by 0 for patients who have a negative diagnosis, and 1 for patients who have a positive diagnosis (malignant tumor). The dataset exhibits a sampling bias in the sense that patients who took a MG are much more than those who took an\nFebruary 2, 2016 DRAFT\n21\nUS or an MRI. Moreover, most patients exhibited negative test results. Table V lists the percentages of patients who took each screening test, and the percentage of patients with positive diagnoses. All features were converted into numerical values and normalized to take values between 0 and 1. The normalized monetary costs for MG, US, and MRI where set to 0.1, 0.2 and 0.7 respectively, and \u03b3 is set to 0.5. In the following subsection, we demonstrate the operation of ConfidentCare."}, {"heading": "B. Learning the distance metric", "text": "Recall from Section IV that clustering of the patients\u2019 personal feature space was carried out using a distance metric that combines both the feature values and the risk assessments as computed by the Gail risk model using\nFebruary 2, 2016 DRAFT\n22\nthe parameter \u03b2. Setting the parameter \u03b2 = 0 corresponds to risk stratification, whereas setting \u03b2 = 1 corresponds to stratifying the personal feature space while disregarding the prior information provided by the Gail model. Since the Gail model does not incorporate all the patients features (e.g. family history), one expects that the best choice of \u03b2 will be between 0 and 1, for that both the personal features and the risk assessments of the patients contains (non-redundant) information about patients\u2019 similarity. As shown in Fig. 4, for an FNR constraint of \u03b7 = 0.1 and confidence parameter of \u03b4 = 0.05, we found that \u03b2 = 0.75 is the best choice of the distance metric since it maximizes the system\u2019s accuracy (FNR and FPR). This means that for \u03b7 = 0.1 and \u03b4 = 0.05, it is better to incorporate more information from the personal features than from the risk assessment. Our interpretation for such a result is that since most of the patients in the dataset have a low to average risks as shown in the histogram plotted in Fig. 5, the information contained in the Gail risk assessment is not enough to differentiate between patients and bundle them into clusters. For the rest of this section, we use the value \u03b2 = 0.75 when running ConfidentCare in\nFebruary 2, 2016 DRAFT\n23\nall the experiments."}, {"heading": "C. ConfidentCare operation", "text": "In this subsection, we investigate the operation of ConfidentCare in terms of clustering and policy construction, endured monetary costs, and accuracy. As we can see in Fig. 6, ConfidentCare can (on average) discover more subgroups of patients for whom it can construct a screening policy with the desired confidence level as the size of the training data increases. This is intuitive since the more training instances are available, the more granular are the partitions that can be formed by the algorithm over the personal feature space. Note that for different settings for the constraint \u03b7, the possible levels of stratification are different. For a fixed size of the training data, as the FNR constraint becomes tighter, the level of personalization decreases. For instance, we can see in Fig. 6 that the\nFebruary 2, 2016 DRAFT\n24\nFebruary 2, 2016 DRAFT\n25\nexpected number of partitions for \u03b7 = 0.2 is greater than that for \u03b7 = 0.1, whereas for \u03b7 = 0.02 the system can never find any feasible partitioning of the feature space regardless of the size of the training data.\nFig. 7 shows the average (normalized) monetary costs endured by ConfidentCare for patients with different risk assessments. As the risk level increases, the costs increase consequently since ConfidentCaare would recommend more tests (including the expensive MRI test) to patients with high level of risk for developing breast cancer. This is again follows from the impact of personalization: only patients who need the screening tests are recommended to take it, for that the screening policy behaves differently for different patient subgroups.\nIn Fig. 8, we plot the FNR and FPR with respect to every partition constructed by the algorithm in a specific realization of ConfidentCare which was able to discover 4 partitions. It is clear that the FNR satisfies the constraint of \u03b7 = 0.1 for all partitions. The FPR for different partitions, for instance we can see that partition 2 has a FPR of 0, whereas other partitions have a non-zero FPR. In Fig. 9, we show the partitions (in a 2D subspace of the original personal feature space) and the constructed policy corresponding to each cluster. It can be seen that patients who are young in age and have low breast density are recommended to take no tests, whereas other subgroups are recommended to take a MG test. We also note that the policy is more aggressive for patients with high breast density, i.e. for partition 3, a relatively low BI-RADS score from a MG can still lead to a recommendation for an addition US or an MRI, whereas for other subgroups the policy is more conservative in terms of recommending additional screening tests. This is because detecting a tumor is more difficult for patients with high breast density.\nNote that Fig. 8 represents just a single realization of ConfidentCare, and thus it does not reveal the amount of confidence we have in the algorithm being satisfying the FNR constraint with a high probability. In order to verify the confidence level in the policy constructed by ConfidentCare, we run the algorithm for 100 runs and see the fraction of time where the FNR in the testing set for any partition exceeds the threshold \u03b7. It can be seen that this is bounded by the specified confidence level \u03b4."}, {"heading": "D. ConfidentCare performance evaluation", "text": "We compare the performance of ConfidentCare with that of the current clinical guidelines in order to assess the value of personalization in terms of cost-efficiency. We compare the monetary cost of ConfidentCare with that of the American Cancer Society (ACS) screening guidelines issued in 2015 [?]. The reason for selecting this specific CPG is that it already applies a coarse form of risk stratification: low, average and high risk women are recommended to take different sets of tests. In Fig. 11, we plot the distribution of the normalized monetary cost of ConfidentCare together with that of the ACS over different levels of risk. It is clear that ConfidentCare can save a significant amount of screening costs since it supports a finer stratification of the patients, and thus recommends screening tests only to patients who need them based on both their features the outcomes of the previous tests that they may have taken. The comparison in Fig. 11 is indeed subject to the selection of \u03b7 and \u03b4. The more we relax the FNR and confidence constraints, the more savings we attain for the monetary costs. The cost-efficiency of ConfidentCare depends on the selection of \u03b7 and \u03b4, which can be set by clinicians or institutions, and based on which the added value of a personalized system can be assessed.\nFebruary 2, 2016 DRAFT\n26\nFinally, we compare the accuracy of ConfidentCare with that of a single decision tree of tests that is designed in a \u201cone-size-fits-all\u201d fashion. In particular, we build a tree of tests using the conventional C4.5 algorithm [53], and then compare its performance with that of ConfidentCare with respect to every partition found by ConfidentCare. From Fig. 12, we can see that for the same realization illustrated in Fig. 8 and 9, both approaches have a comparable FNR, but ConfidentCare outperforms a single decision tree in terms of the FPR for all the 4 partitions. This is because ConfidentCare deals differently with women belonging to different subgroups as shown in Fig. 9, i.e. for instance women in partition 2 are not recommended to take any tests. In other words, ConfidentCare avoids recommending unnecessary tests, which reduces the rate of false positives. The average values of the FNR and FPR for 50 runs of ConfidentCare and a single decision tree are reported in Table VI, where a gain of 31.91% with respect to the FPR is reported.\nFebruary 2, 2016 DRAFT\n27\nTABLE VI: FNR and FPR for ConfidentCare (with \u03b7 = 0.1 and \u03b4 = 0.05) and a single C4.5 decision tree\nAlgorithm FNR FPR\nSingle C4.5 decision tree 0.0501 0.0488.\nConfidentCare 0.0512 0.037.\n40 45 50 55 60 65 70 75 80 85 90 0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nAge\nG a il m o d el\nri sk\na ss es sm\nen t (%\n)\nRepresentative patient 1 Representative patient 2 Representative patient 3 Representative patient 4\nRecommended screening frequency for representative patient 3\nRecommended screening frequency for representative patient 2\nFig. 13: Risk assessment over time for the representative patients (centroids) constructed by ConfidentCare."}, {"heading": "E. Discussions and future work", "text": "The screening policy considered in this paper was concerned with managing the short-term screening procedure, i.e. the policy was recommending a sequence of screening tests for the patient based on the outcomes of those tests, and such tests are expected to be taken in a relatively short time interval. Our framework can be extended to design policies that are concerned with the long-term patient outcomes, and are capable of not only recommend tests to the patient, but also recommend the frequency with which screening should be carried out for different subgroups of patients. To see how our framework can be extended to handle such a setting, we plot the risk of developing breast cancer over time for the representative agents (centroids) of 4 clusters constructed in one realization of the algorithm in Fig. 13. Each cluster exhibits a different rate of risk growth over time, i.e. for instance while clusters 3 and 4 in Fig. 13 comprise women of almost the same age, patients in cluster 3 develop a risk for breast cancer more quickly than patients in cluster 4 due to other factors (e.g. family history). Thus, ConfidentCare can be modified to not only recommend a sequence of tests to patients in different clusters, but also to compute the optimal frequency of screening (steps over time for which the patient need to be regularly screened) that would maximize a long-term objective function. Intuitively, the frequency of screening would depend on the slope of the risk assessment over time, i.e. clusters with steeper slopes would demand more frequent screening. Our framework is well suited to capture such a setting, and the ConfidentCare algorithm can be modified to construct a screening policy that maximizes long-term outcomes with high levels of confidence.\nFebruary 2, 2016 DRAFT\n28"}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, we presented ConfidentCare: a clinical decision support system that learns a personalized screening policy from the electronic health data record. ConfidentCare operates by stratifying the space of a woman? features and learning cost-effective, and accurate screening policy that is tailored to those features and are accurate with a high-level of confidence. ConfidentCare algorithm iteratively stratifies the patients\u2019 feature space into disjoint clusters and learns active classifiers associated with each cluster. We have shown that the proposed algorithm has the potential of improving the cost efficiency and accuracy of the screening process compared to current clinical practice guidelines, and state-of-the-art algorithms that do not consider personalization."}, {"heading": "ACKNOWLEDGMENT", "text": "We would like to thank Dr. Camelia Davtyan (Ronald Reagan UCLA Medical Center) for her valuable help and precious comments on the medical aspects of the paper. We also thank Dr. William Hoiles (UCLA) for the valuable discussions and comments that we had with him on this paper."}], "references": [{"title": "The path to personalized medicine", "author": ["M.A. Hamburg", "F.S. Collins"], "venue": "New England Journal of Medicine, vol. 363, no. 4, pp. 301-304, Jul. 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Cancer genomics: from discovery science to personalized medicine", "author": ["L. Chin", "J.N. Andersen", "P.A. Futreal"], "venue": "Nature medicine, vol. 17, no. 3, pp. 297-303, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Predictive, personalized, preventive, participatory (P4) cancer medicine", "author": ["L. Hood", "S.H. Friend"], "venue": "Nature Reviews Clinical Oncology, vo. 8, no. 3, pp. 184-187, Mar. 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "The White House Precision Medicine Initiative: Technology Turned Innovative Solutions", "author": ["D. Patil"], "venue": "AAAS Annual Meeting 2016, AAAS, Feb. 11-15, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Reduction in mortality from breast cancer after mass screening with mammography: randomised trial from the Breast Cancer Screening Working Group of the Swedish National Board of Health and Welfare", "author": ["L. Tabar"], "venue": "The Lancet, vol. 325, no. 8433, pp. 829-832, 1985.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1985}, {"title": "Modern mammography screening and breast cancer mortality: population study", "author": ["H. Weedon-Fekjaer", "R.R. Pal", "J.V. Lars"], "venue": "BMJ, vol. 348, no. 3701, pp. 1-8, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Breast cancer screening, incidence, and mortality across US counties", "author": ["C. Harding", "F. Pompei", "D. Burmistrov", "H.G. Welch", "R. Abebe", "R. Wilson"], "venue": "JAMA internal medicine.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Guidance on terminology", "author": ["N.J. Wald"], "venue": "Journal of Medical Screening, vol. 15, no. 1, pp. 50-50, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Clinical application of pharmacogenetics", "author": ["B.B. Spear", "M. Heath-Chiozzi", "J. Huff"], "venue": "Trends Mol. Med., vol. 7, no. 5, pp. 201-204, May 2001.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Mammographic breast density and the Gail model for breast cancer risk prediction in a screening population", "author": ["J.A. Tice"], "venue": "Breast cancer research and treatment, vol. 94, no. 2, pp. 115-122, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Projecting individualized probabilities of developing breast cancer for white females who are being examined annually", "author": ["M.H. Gail", "L.A. Brinton", "D.P. Byar", "D.K. Corle", "S.B. Green", "C. Schairer", "J.J. Mulvihill"], "venue": "J. Natl. Cancer Inst., vol. 81, no. 24, pp. 1879-1886, Dec. 1989.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1879}, {"title": "Validation studies for models projecting the risk of invasive and total breast cancer incidence", "author": ["J.P. Costantino", "M.H. Gail", "D. Pee", "S. Anderson", "C.K. Redmond", "J. Benichou", "H.S. Wieand"], "venue": "J. Natl. Cancer Inst., vol. 15, no. 91, pp. 1541-1548, Sep. 1999.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Weighing the Risks and Benefits of Tamoxifen Treatment for Preventing Breast Cancern", "author": ["M.H. Gail", "J.P. Costantino", "J. Bryant", "R. Croyle", "L. Freedman", "K. Helzlsouer", "V. Vogel"], "venue": "Journal of the National Cancer Institute, vol. 91, no. 21, pp. 1829-1846, 1999. February 2, 2016  DRAFT  29", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1829}, {"title": "Personalizing mammography by breast density and other risk factors for breast cancer: analysis of health benefits and cost-effectiveness", "author": ["J.T. Schousboe", "K. Kerlikowske", "A. Loh", "S.R. Cummings"], "venue": "Annals of internal medicine, vol. 155, no. 1m pp. 10-20, Jul. 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Locally recurrent or metastatic breast cancer: ESMO Clinical Practice Guidelines for diagnosis, treatment and follow-up", "author": ["F. Cardoso"], "venue": "Annals of oncology, vol. 23, no. 7, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Primary breast cancer: ESMO Clinical Practice Guidelines for diagnosis, treatment and follow-up", "author": ["S. Aebi"], "venue": "Annals of oncology,vol. 22, no. 6, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "American Cancer Society guidelines for the early detection of cancer", "author": ["R.A. Smith", "V. Cokkinides", "A.C. von Eschenbach", "B. Levin", "C. Cohen", "C.D. Runowicz", "S. Sener", "D. Saslow", "H.J. Eyre"], "venue": "CA: a cancer journal for clinicians, vol. 52, no. 1, pp. 8-22, Jan. 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "NCI remains committed to current mammography guidelines", "author": ["A.C. von Eschenbach"], "venue": "The oncologist, vol. 7, no. 3, pp. 170-171, 2002.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Breast cancer screening in an era of personalized regimens: A conceptual model and National Cancer Institute initiative for risk?based and preference?based approaches at a population level", "author": ["T. Onega"], "venue": "Cancer, vol. 120, no. 19, pp. 2955-2964, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Personalized medicine: Moving from correlation to causality in breast cancer", "author": ["S. Molinaro", "S. Pieroni", "F. Mariani", "M.N. Liebman"], "venue": "New Horizons in Translational Medicine, vol. 2, no. 2, Jan. 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Personalized Screening for Breast Cancer: A Wolf in Sheep\u2019s Clothing?", "author": ["S.A. Feig"], "venue": "American Journal of Roentgenology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Personalized Medicine in Breast Cancer: A Systematic Review", "author": ["S.-H. Cho", "J. Jeon", "S.I. Kim"], "venue": "J. Breast Cancer, vol. 15, no. 3, pp. 265-272, Sep. 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "To screen or not to screen women in their 40s for breast cancer: Is personalized risk-based screening the answer?", "author": ["J.S. Mandelblatt", "N. Stout", "A. Trentham-Dietz"], "venue": "Annals of internal medicine,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Analysis of health benefits and cost-effectiveness of mammography for breast cancer", "author": ["J.D. Keen"], "venue": "Annals of internal medicine, vol. 155, no. 8, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Breast imaging reporting and data system (BI-RADS)", "author": ["L. Liberman", "J.H. Menell"], "venue": "Radiologic Clinics of North America, vol. 40, no. 3, pp. 409-430, 2002.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Effects of age, breast density, ethnicity, and estrogen replacement therapy on screening mammographic sensitivity and cancer stage at diagnosis: review of 183,134 screening mammograms in Albuquerque, New Mexico", "author": ["R.D. Rosenberg"], "venue": "Radiology, vol. 209, no. 2, pp. 511-518, 1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Statistical issues and limitations in personalized medicine research with clinical trials", "author": ["D.B. Rubin", "M.J. van der Laan"], "venue": "The international journal of biostatistics, vol. 8, no. 1, Jul. 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Likelihood ratios for modern screening mammography: risk of breast cancer based on age and mammographic interpretation", "author": ["K. Kerlikowske"], "venue": "JAMA, vol. 276, no. 1, pp. 39-43, 1996.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Optimal dynamic treatment regimes", "author": ["S.A. Murphy"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 65, no. 2, pp. 331-355, May 2003.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Dynamic treatment regimes: Technical challenges and applications", "author": ["E.B. Laber", "D.J. Lizotte", "M. Qian", "W.E. Pelham", "S.A. Murphy"], "venue": "Electronic journal of statistics, vol. 8, no. 1, Jan. 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic treatment regimes", "author": ["B. Chakraborty", "S.A. Murphy"], "venue": "Annual review of statistics and its application, p. 447, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Demystifying optimal dynamic treatment regimes", "author": ["E.E. Moodie", "T.S. Richardson", "D.A. Stephens"], "venue": "Biometrics, vol. 63, no. 2, pp. 447-455, Jun. 2007.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Drug dosage individualization based on a random-effects linear model", "author": ["F.J. Diaz", "M.R. Cogollo", "E. Spina", "V. Santoro", "D.M. Rendon", "J. de Leon"], "venue": "Journal of biopharmaceutical statistics, vol. 22, no. 3, pp. 463-484, May 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "The application of adaboost for distributed, scalable and on-line learning", "author": ["W. Fan", "S.J. Stolfo", "J. Zhang"], "venue": "Proc. Fifth ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining, ACM, pp. 362?66, 1999.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational Learning Theory, pp. 23?7, Springer.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 0}, {"title": "Maloof,\u201cDynamic weighted majority: An ensemble method for drifting concepts,", "author": ["J.Z. Kolter", "A. M"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Warmuth,\u201cThe weighted majority algorithm,", "author": ["M.K.N. Littlestone"], "venue": "IEEE Annual Symposium on Foundations of Computer Science,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1989}, {"title": "Adaptive ensemble learning with confidence bounds for personalized diagnosis", "author": ["C. Tekin", "J. Yoon", "M. van der Schaar"], "venue": "accepted for AAAI Workshop on Expanding the Boundaries of Health Informatics using AI (HIAI\u201916): Making Proactive, Personalized, and Participatory Medicine A Reality, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Active learning using arbitrary binary valued queries", "author": ["R.S. Kulkarni", "K.M. Sanjoy", "J.N. Tsitsiklis"], "venue": "Machine Learning, vol. 11, no. 1, pp. 23-35, 1993.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1993}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "The Journal of Machine Learning Research, vol. 2, pp. 45-66, 2002.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}, {"title": "Active and semisupervised learning for the classification of remote sensing images", "author": ["C. Persello", "L. Bruzzone"], "venue": "IEEE Trans. Geosci. and Remote Sens., vol. 52, no. 11, pp. 6937-6956, Nov. 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning cost-sensitive active classifiers", "author": ["R. Greiner", "A.J. Grove", "D. Roth"], "venue": "Artificial Intelligence, vol. 139, no. 2, pp. 137-174, Aug. 2002.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Cost-sensitive decision trees applied to medical data", "author": ["A. Freitas", "A. Costa-Pereira", "P. Brazdil"], "venue": "Data Warehousing and Knowledge Discovery, Springer Berlin Heidelberg, pp. 303-312, Jan. 2007.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Decision trees with minimal costs", "author": ["C.X. Ling", "Q. Yang", "J. Wang", "S. Zhang"], "venue": "Proc. of ICML, p. 69, Jul. 2004.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "Active sensing", "author": ["S. Yu", "B. Krishnapuram", "R. Rosales", "R.B. Rao"], "venue": "International Conference on Artificial Intelligence and Statistics, 2009.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey of cost-sensitive decision tree induction algorithms", "author": ["S. Lomax", "S. Vadera"], "venue": "ACM Computing Surveys (CSUR), vol. 45, no. 2, 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Metric Spaces: Iteration and Application", "author": ["V. Bryant"], "venue": "Cambridge University Press, 1985.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1985}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S.S.-Shwartz", "S.B.-David"], "venue": "Cambridge University Press, 2014.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Least squares quantization in PCM", "author": ["P.S. Lloyd"], "venue": "IEEE Trans. Info. Theory, vol. 28, no. 2, pp. 129-137, 1982.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1982}, {"title": "Constructing optimal binary decision trees is NP-complete", "author": ["L. Hyafil", "Ronald L. Rivest"], "venue": "Information Processing Letters, vol. 5, no. 1, pp. 15-17, 1976.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1976}, {"title": "C4.5: programs for machine learning", "author": ["J.R. Quinlan"], "venue": "Elsevier, 2014. February 2, 2016  DRAFT  1  2  3  4  0 0.02 0.04 0.06 0.08 0.1 Partition  F  a  ls  e  n  eg  a  ti  v  e  ra  te  (F  N  R  ) \u03b7 = 0.1 and \u03b4 = 0.05 1  2  3  4  0 0.02 0.04 0.06 0.08 0.1 Partition  F  a  ls  e  p  o  si  ti  v  e  ra  te  (F  P  R  )  \u03b7", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Personalized medicine is a new healthcare paradigm that aims to move beyond the current \u201cone-size-fits-all\u201d approach to medicine and, instead, takes into account the features and traits of individual patients: their genes, microbiomes, environments, and lifestyles [1]-[3].", "startOffset": 278, "endOffset": 281}, {"referenceID": 2, "context": "INTRODUCTION Personalized medicine is a new healthcare paradigm that aims to move beyond the current \u201cone-size-fits-all\u201d approach to medicine and, instead, takes into account the features and traits of individual patients: their genes, microbiomes, environments, and lifestyles [1]-[3].", "startOffset": 282, "endOffset": 285}, {"referenceID": 3, "context": "For instance, the White House has led the \u201cprecision medicine initiative\u201d [4], which is scheduled for discussion in the American Association for the Advancement of Science annual meeting for the year 2016 [5].", "startOffset": 205, "endOffset": 208}, {"referenceID": 4, "context": "Screening is carried out in order to diagnose a woman with no apparent symptoms in a timely manner [6]-[8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Screening is carried out in order to diagnose a woman with no apparent symptoms in a timely manner [6]-[8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "However, the screening process entails both benefits and costs that can differ from one patient to another [9], which signals the need for personalized screening policies that balance such benefits and costs in a customized manner.", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "While breast cancer screening is believed to reduce mortality rates [8], it is associated with the risks of \u201coverscreening\u201d, which leads to unnecessary costs, and \u201coverdiagnosis\u201d, which corresponds to false positive diagnoses that lead the patients to receive unnecessary treatments [9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "While breast cancer screening is believed to reduce mortality rates [8], it is associated with the risks of \u201coverscreening\u201d, which leads to unnecessary costs, and \u201coverdiagnosis\u201d, which corresponds to false positive diagnoses that lead the patients to receive unnecessary treatments [9].", "startOffset": 283, "endOffset": 286}, {"referenceID": 8, "context": "While different patients have different levels of risks for developing breast cancer [10]-[14]; different tests have different monetary costs, and different levels of accuracy that depend on the features of the patient [15]; common CPGs are aimed at populations, and are not typically tailored to specific individuals or significant subgroups [16]-[19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "While different patients have different levels of risks for developing breast cancer [10]-[14]; different tests have different monetary costs, and different levels of accuracy that depend on the features of the patient [15]; common CPGs are aimed at populations, and are not typically tailored to specific individuals or significant subgroups [16]-[19].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "While different patients have different levels of risks for developing breast cancer [10]-[14]; different tests have different monetary costs, and different levels of accuracy that depend on the features of the patient [15]; common CPGs are aimed at populations, and are not typically tailored to specific individuals or significant subgroups [16]-[19].", "startOffset": 219, "endOffset": 223}, {"referenceID": 14, "context": "While different patients have different levels of risks for developing breast cancer [10]-[14]; different tests have different monetary costs, and different levels of accuracy that depend on the features of the patient [15]; common CPGs are aimed at populations, and are not typically tailored to specific individuals or significant subgroups [16]-[19].", "startOffset": 343, "endOffset": 347}, {"referenceID": 17, "context": "While different patients have different levels of risks for developing breast cancer [10]-[14]; different tests have different monetary costs, and different levels of accuracy that depend on the features of the patient [15]; common CPGs are aimed at populations, and are not typically tailored to specific individuals or significant subgroups [16]-[19].", "startOffset": 348, "endOffset": 352}, {"referenceID": 18, "context": "Being designed to work well on \u201caverage\u201d for a population of patients, following CPGs may lead to overscreening or overdiagnosis for specific subgroups of patients, such as young women at a high risk of developing breast cancer, or healthy older women who may have a relatively longer expected lifespan [20].", "startOffset": 303, "endOffset": 307}, {"referenceID": 13, "context": "a mammogram test will exhibit low accuracy for patients with high breast density [15]), which can either lead to \u201coverdiagnosis\u201d or poor tumor detection performance.", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "Migrating from the \u201cone-size-fits-all\u201d screening and diagnosis policies adopted by CPGs to more individualized policies that recognizes and approaches different subgroups of patients is the essence of applying the personalized medicine paradigm to the breast cancer clinical environment [15], [20]-[23].", "startOffset": 287, "endOffset": 291}, {"referenceID": 18, "context": "Migrating from the \u201cone-size-fits-all\u201d screening and diagnosis policies adopted by CPGs to more individualized policies that recognizes and approaches different subgroups of patients is the essence of applying the personalized medicine paradigm to the breast cancer clinical environment [15], [20]-[23].", "startOffset": 293, "endOffset": 297}, {"referenceID": 21, "context": "Migrating from the \u201cone-size-fits-all\u201d screening and diagnosis policies adopted by CPGs to more individualized policies that recognizes and approaches different subgroups of patients is the essence of applying the personalized medicine paradigm to the breast cancer clinical environment [15], [20]-[23].", "startOffset": 298, "endOffset": 302}, {"referenceID": 0, "context": "Related works 1) Personalized (precision) medicine: While medical studies investigated the feasibility, potential and impact of applying the concepts of personalized medicine in the breast cancer clinical environments [1]-[3], [15]-[25], [28][29], none of these works provided specific tools or methods for building a personalized healthcare environment.", "startOffset": 218, "endOffset": 221}, {"referenceID": 2, "context": "Related works 1) Personalized (precision) medicine: While medical studies investigated the feasibility, potential and impact of applying the concepts of personalized medicine in the breast cancer clinical environments [1]-[3], [15]-[25], [28][29], none of these works provided specific tools or methods for building a personalized healthcare environment.", "startOffset": 222, "endOffset": 225}, {"referenceID": 13, "context": "Related works 1) Personalized (precision) medicine: While medical studies investigated the feasibility, potential and impact of applying the concepts of personalized medicine in the breast cancer clinical environments [1]-[3], [15]-[25], [28][29], none of these works provided specific tools or methods for building a personalized healthcare environment.", "startOffset": 227, "endOffset": 231}, {"referenceID": 23, "context": "Related works 1) Personalized (precision) medicine: While medical studies investigated the feasibility, potential and impact of applying the concepts of personalized medicine in the breast cancer clinical environments [1]-[3], [15]-[25], [28][29], none of these works provided specific tools or methods for building a personalized healthcare environment.", "startOffset": 232, "endOffset": 236}, {"referenceID": 25, "context": "Related works 1) Personalized (precision) medicine: While medical studies investigated the feasibility, potential and impact of applying the concepts of personalized medicine in the breast cancer clinical environments [1]-[3], [15]-[25], [28][29], none of these works provided specific tools or methods for building a personalized healthcare environment.", "startOffset": 238, "endOffset": 242}, {"referenceID": 26, "context": "Related works 1) Personalized (precision) medicine: While medical studies investigated the feasibility, potential and impact of applying the concepts of personalized medicine in the breast cancer clinical environments [1]-[3], [15]-[25], [28][29], none of these works provided specific tools or methods for building a personalized healthcare environment.", "startOffset": 242, "endOffset": 246}, {"referenceID": 13, "context": "For instance, in [15], it has been shown that CPGs, which recommend screening tests only based on the age ranges, such as the European Society for Medical Oncology (ESMO) CPG and the American Cancer Society (ACS) CPG, are not cost-efficient for many subgroups of patients, where cost-efficiency was measured in terms of \u201ccosts per quality-adjusted life-year\u201d, and the authors recommended that screening should be personalized on the basis of a patient\u2019s age, breast density, history of breast biopsy, and the family history of breast cancer.", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "Similar results were portrayed in other medical studies [23]-[25], all suggesting that personalization using dimensions other than the age can yield more cost efficiency.", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "Similar results were portrayed in other medical studies [23]-[25], all suggesting that personalization using dimensions other than the age can yield more cost efficiency.", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "Therefore, false negative diagnoses rates reported by clinicians who follow CPGs reflect the average accuracy over all the population of patients, but CPGs give no guarantee that the diagnostic accuracy and the associated confidence levels of their guidelines are reasonable for every subgroup of \u201csimilar\u201d patients [26][27]; such subgroups can be significantly different in their traits and hence may require being dealt with via different screening and diagnosis policies.", "startOffset": 320, "endOffset": 324}, {"referenceID": 28, "context": "2) Dynamic treatment regimes: Perhaps the work that relates most to this paper is that on Dynamic treatment regimes (DTRs) [31]-[35].", "startOffset": 123, "endOffset": 127}, {"referenceID": 32, "context": "2) Dynamic treatment regimes: Perhaps the work that relates most to this paper is that on Dynamic treatment regimes (DTRs) [31]-[35].", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "A DTR is typically a sequence of decision rules, with one rule per stage of clinical intervention, where each rule maps up-to-date patient information to a recommended treatment [31].", "startOffset": 178, "endOffset": 182}, {"referenceID": 30, "context": "However, these works profoundly differ from the setting we consider in the following aspects: DTRs are only focused on recommending treatments and do not consider screening and diagnoses; costefficiency is not considered in the design of DTR policies since they only consider the \u201cvalue of information\u201d in recommending treatments; and finally, while confidence measures can be computed for policies in DTRs [33], the policies themselves are not designed in a way that guarantees to the clinician a certain level of reliability for every subgroup of patients.", "startOffset": 407, "endOffset": 411}, {"referenceID": 38, "context": "3) Active classification for medical diagnosis: Screening and diagnostic clinical decisions typically involve \u201cpurchasing costly information\u201d for the patients, which relates to the paradigm of active learning [41]-[48].", "startOffset": 209, "endOffset": 213}, {"referenceID": 45, "context": "3) Active classification for medical diagnosis: Screening and diagnostic clinical decisions typically involve \u201cpurchasing costly information\u201d for the patients, which relates to the paradigm of active learning [41]-[48].", "startOffset": 214, "endOffset": 218}, {"referenceID": 38, "context": "We note that in our setting, clinicians \u201cpurchase\u201d costly features of the patients rather than purchasing unobserved labels, which makes our setting profoundly different from the conventional active learning framework [41]-[43].", "startOffset": 218, "endOffset": 222}, {"referenceID": 40, "context": "We note that in our setting, clinicians \u201cpurchase\u201d costly features of the patients rather than purchasing unobserved labels, which makes our setting profoundly different from the conventional active learning framework [41]-[43].", "startOffset": 223, "endOffset": 227}, {"referenceID": 41, "context": "Classification problems in which some features are costly are referred to as \u201cactive classification\u201d [44], or \u201cactive sensing\u201d [47].", "startOffset": 101, "endOffset": 105}, {"referenceID": 44, "context": "Classification problems in which some features are costly are referred to as \u201cactive classification\u201d [44], or \u201cactive sensing\u201d [47].", "startOffset": 127, "endOffset": 131}, {"referenceID": 41, "context": "Such problems have been addressed in the context of medical diagnosis in [44]-[48], but all these works correspond to solving an unconstrained optimization problem that targets the whole population, for which no personalized accuracy or confidence guarantees can be claimed.", "startOffset": 73, "endOffset": 77}, {"referenceID": 45, "context": "Such problems have been addressed in the context of medical diagnosis in [44]-[48], but all these works correspond to solving an unconstrained optimization problem that targets the whole population, for which no personalized accuracy or confidence guarantees can be claimed.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "The personal features include numerical and categorical features such as: age, age at menarche, number of previous biopsies, breast density, age at first child birth, and the family history [15].", "startOffset": 190, "endOffset": 194}, {"referenceID": 23, "context": "We consider a generic cost function that incorporates both the misclassification costs in addition to the monetary costs (the detailed cost model is provided in the next subsection) [25].", "startOffset": 182, "endOffset": 186}, {"referenceID": 27, "context": "patient with a positive true diagnosis (malignant tumor) is recommended to take a regular followup screening test [30].", "startOffset": 114, "endOffset": 118}, {"referenceID": 41, "context": "Such a classifier is active in the sense that it can query the clinician for costly feature information rather than passively dealing with a given chunk of data [44].", "startOffset": 161, "endOffset": 165}, {"referenceID": 38, "context": "This setting should not be confused with conventional active learning, where labels (and not features) are the costly piece of information which the classifier may need to purchase [41][42].", "startOffset": 181, "endOffset": 185}, {"referenceID": 39, "context": "This setting should not be confused with conventional active learning, where labels (and not features) are the costly piece of information which the classifier may need to purchase [41][42].", "startOffset": 185, "endOffset": 189}, {"referenceID": 47, "context": "That is, we compensate our lack of knowledge of the stationary distribution D by inducing a prior knowledge on the set of possible hypothesis that the learning algorithm can output: a common approach for designing agnostic learners [50].", "startOffset": 232, "endOffset": 236}, {"referenceID": 24, "context": "Unlike the conventional supervised learning paradigm which picks a hypothesis that minimizes a loss function, we will design a learning algorithm that picks a hypothesis from H, such that the overall cost of screening is minimized, while maintaining the FNR to be below a predefined threshold, with a desired level of confidence; a common design objective for breast cancer clinical systems [27].", "startOffset": 391, "endOffset": 395}, {"referenceID": 25, "context": "Both types of error are very different in terms of their implications, and one can easily see that the FNR is more crucial, since it corresponds to misdiagnosing a patient with breast cancer as being healthy [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 23, "context": "On the other hand, the FPR is considered as a misclassification cost that we aim at minimizing given a constraint on the FNR [25].", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "where \u03b3 \u2208 [0, 1] is a parameter that balances the importance of the misclassification costs compared to the monetary cost, i.", "startOffset": 10, "endOffset": 16}, {"referenceID": 41, "context": "4) Learnability of active classifiers: In order to evaluate the learner, and its ability to construct a reasonable solution for (4), we define a variant of the probably approximately correct criterion for learning active classifiers that minimize the classification costs with a constraint on the FNR (conventional definitions for PAC-learnability can be found in [44] and [50]).", "startOffset": 364, "endOffset": 368}, {"referenceID": 47, "context": "4) Learnability of active classifiers: In order to evaluate the learner, and its ability to construct a reasonable solution for (4), we define a variant of the probably approximately correct criterion for learning active classifiers that minimize the classification costs with a constraint on the FNR (conventional definitions for PAC-learnability can be found in [44] and [50]).", "startOffset": 373, "endOffset": 377}, {"referenceID": 0, "context": "\u2022 For every (\u01ebc, \u01eb, \u03b4) \u2208 [0, 1], there exists a polynomial function N H(\u01eb, \u01ebc, \u03b4) = poly( 1 \u01ebc , 1\u01eb , 1 \u03b4 ), such that for every m \u2265 N H(\u01eb, \u01ebc, \u03b4), we have that PSm\u223cD\u2297m (C (A (Sm)) \u2265 C(h ) + \u01ebc) \u2264 \u03b4, PSm\u223cD\u2297m (FNR(A (Sm)) \u2265 FNR(h ) + \u01eb) \u2264 \u03b4, where N H(\u01eb, \u01ebc, \u03b4) is the sample complexity of the classification problem.", "startOffset": 25, "endOffset": 31}, {"referenceID": 47, "context": "The key for the result of Theorem 2 is that if |H| < \u221e, then the FNR and cost functions are Glivenko-Cantelli classes [50], for which the uniform convergence property is satisfied, i.", "startOffset": 118, "endOffset": 122}, {"referenceID": 48, "context": "The operation of ConfidentCare relies on a clustering algorithm that is a variant of Lloyd\u2019s K-means clustering algorithm [51].", "startOffset": 122, "endOffset": 126}, {"referenceID": 9, "context": "That is, we exploit the risk assessments computed via the Gail model in order to initialize the clusters centroids [11]-[14], thereby ensuring fast convergence.", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "That is, we exploit the risk assessments computed via the Gail model in order to initialize the clusters centroids [11]-[14], thereby ensuring fast convergence.", "startOffset": 120, "endOffset": 124}, {"referenceID": 0, "context": "Let G : Xd \u2192 [0, 1] be Gail\u2019s risk assessment function, i.", "startOffset": 13, "endOffset": 19}, {"referenceID": 42, "context": "Adopting decision trees as a hypothesis set is advantageous since such a classifier is widely used and easily interpretable for medical applications [45]-[48].", "startOffset": 149, "endOffset": 153}, {"referenceID": 45, "context": "Adopting decision trees as a hypothesis set is advantageous since such a classifier is widely used and easily interpretable for medical applications [45]-[48].", "startOffset": 154, "endOffset": 158}, {"referenceID": 49, "context": "Learning the optimal decision tree h \u2208 H is known to be an NP-hard problem [52], thus we resort to greedy algorithm A, which we call the confidence-based Cost-sensitive decision tree induction algorithm (ConfidentT ree).", "startOffset": 75, "endOffset": 79}, {"referenceID": 50, "context": "After growing such a tree, we apply post-pruning based on confidence intervals of error estimates [53].", "startOffset": 98, "endOffset": 102}, {"referenceID": 50, "context": "\u2022 While growing the tree, the splitting rule is as follows: for each test, label the leaves such that the pessimistic estimate (see [53] for confidence interval and error estimates in the C4.", "startOffset": 132, "endOffset": 136}, {"referenceID": 50, "context": "5 algorithm [53].", "startOffset": 12, "endOffset": 16}, {"referenceID": 50, "context": "5 algorithm [53], and then compare its performance with that of ConfidentCare with respect to every partition found by ConfidentCare.", "startOffset": 12, "endOffset": 16}], "year": 2016, "abstractText": "Breast cancer screening policies attempt to achieve timely diagnosis by the regular screening of apparently healthy women. Various clinical decisions are needed to manage the screening process; those include: selecting the screening tests for a woman to take, interpreting the test outcomes, and deciding whether or not a woman should be referred to a diagnostic test. Such decisions are currently guided by clinical practice guidelines (CPGs), which represent a \u201cone-size-fits-all\u201d approach that are designed to work well on average for a population, without guaranteeing that it will work well uniformly over that population. Since the risks and benefits of screening are functions of each patients features, personalized screening policies that are tailored to the features of individuals are needed in order to ensure that the right tests are recommended to the right woman. In order to address this issue, we present ConfidentCare: a computer-aided clinical decision support system that learns a personalized screening policy from the electronic health record (EHR) data. ConfidentCare operates by recognizing clusters of \u201csimilar\u201d patients, and learning the \u201cbest\u201d screening policy to adopt for each cluster. A cluster of patients is a set of patients with similar features (e.g. age, breast density, family history, etc.), and the screening policy is a set of guidelines on what actions to recommend for a woman given her features and screening test scores. ConfidentCare utilizes an iterative algorithm that applies K-means clustering to the women\u2019s feature space, followed by learning an active classifier (decision tree) for every cluster. The algorithm ensures that the policy adopted for every cluster of patients satisfies a predefined accuracy requirement with a high level of confidence. We show that our algorithm outperforms the current CPGs in terms of cost-efficiency and false positive rates.", "creator": null}}}