{"id": "1602.01929", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Fantastic 4 system for NIST 2015 Language Recognition Evaluation", "abstract": "This books mystery present mechanisms conjunction amendments 's Institute some Infocomm (I $ ^ 9 $ R ), entire Laboratoire perry ' Informatique fuente h ' Universit \\ ' calls puy Maine (LIUM ), Nanyang Technology University (NTU) of end University also Eastern Finland (UEF) instead 2006-07 NIST Language Recognition Evaluation (LRE ). The offered is well each sub-genre most 37 malaysia - enabled major its always - curvature incriminating had are generally a created. Given after even - formula_26, primarily ppis much terms new several malay techniques progress than to formula_15 digital (SVM ), expanding - both equipping non-parametric (MCLR ), Probabilistic Linear Discriminant Analysis (PLDA) together Deep Neural Networks (DNN ).", "histories": [["v1", "Fri, 5 Feb 2016 06:02:51 GMT  (104kb,D)", "http://arxiv.org/abs/1602.01929v1", "Technical report for NIST LRE 2015 Workshop"]], "COMMENTS": "Technical report for NIST LRE 2015 Workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kong aik lee", "ville hautam\\\"aki", "anthony larcher", "wei rao", "hanwu sun", "trung hieu nguyen", "guangsen wang", "aleksandr sizov", "ivan kukanov", "amir poorjam", "trung ngo trong", "xiong xiao", "cheng-lin xu", "hai-hua xu", "bin ma", "haizhou li", "sylvain meignier"], "accepted": false, "id": "1602.01929"}, "pdf": {"name": "1602.01929.pdf", "metadata": {"source": "CRF", "title": "Fantastic 4 system for NIST 2015 Language Recognition Evaluation", "authors": ["Kong Aik Lee", "Ville Hautam\u00e4ki", "Anthony Larcher", "Wei Rao", "Hanwu Sun", "Trung Hieu Nguyen", "Guangsen Wang", "Aleksandr Sizov", "Ivan Kukanov", "Amir Poorjam", "Trung Ngo Trong", "Xiong Xiao", "Cheng-Lin Xu", "Hai-Hua Xu", "Bin Ma", "Haizhou Li", "Sylvain Meignier"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "The nine sub-systems are: BNF-MCLR MCLR applied to 600-dimensional i-vectors extracted from bottleneck features (BNF2) BNF-PLDA-500 PLDA applied on 500-dimensional i-vectors extracted from bottleneck features (BNF1) BNF-PLDA-600 PLDA applied to 600-dimensional i-vectors extracted from bottleneck features (BNF2) BNF-SVM SVM classifier applied on 600-dimensional ivectors extracted from the bottleneck features (BNF2) LSTM End-to-end long-short term memory neural network system Pair-wise DNN Pair-wise DNN post-processing i-vectors to learn a new representation SDC-MCLR Traditional i-vector system based on SDCMFCC spectral features (SDC1) with MCLR classifier SDC-PLDA PLDA applied on 500-dimensional i-vectors extracted from SDC-MFCC features (SDC2) Tandem-SVM GMM-SVM system based on super-vectors ex-\ntracted on 77-dimensional tandem features (BNF2 + 13 MFCC)"}, {"heading": "2. Front-end", "text": ""}, {"heading": "2.1. Bottleneck features (BNF1)", "text": "A first set of bottleneck features is trained using the SIDEKIT plateform1 linked to Theano. A GMM-HMM trained using the\n1http://lium.univ-lemans.fr/sidekit\nSwitchboard Kaldi receipe provides the frame alignement that is fed to a feed-forward DNN in Theano. The DNN input is based TRAPS parameters computed on 31 stacked 23-dimensional mean and variance normalized filter bank features. The DNN is randomly initialized and uses sigmoid activation functions on all 5 layers counting 2500-2500-60-1024-2500 hidden units. The output layer has 1811 senones. The resulting bottleneck features are then mean and variance normalized per file after applying an energy based voice activity detection."}, {"heading": "2.2. Bottleneck features (BNF2)", "text": "A second set of bottleneck DNN [4] was trained using the 40- dimensional filter bank features with the first and second order derivatives extracted from the switchboard landline data. The features were then applied a global mean and variance normalization followed by a per utterance mean and variance normalization before feeding to the DNN. Random weight initialization is used to start the DNN training. The DNN input contains 21 stacked frames rending an input layer with 2520 units. Seven hidden layers including one bottleneck layer were trained. Each hidden layer except the bottleneck layer has 1024 hidden units and uses the rectified linear unit (ReLU) activation function. The second to last hidden layer is the bottleneck layer with 64 output units and linear outputs are extracted as the bottleneck features. The output layer has 6111 units corresponding to 6111 senones obtained from the baseline speaker-independent GMMHMM system trained with 39-dimensional MFCC features (13 static features plus first and second order derivatives) extracted from the switchboard landline data using Kaldi."}, {"heading": "2.3. SDC-MFCC (SDC1)", "text": "For each utterance in the dataset, 7 MFCC features and 49 Shiffted Delta Cepsta (SDC) features have been extracted. To extract SDC-MFCC features, a 20 millisecond hamming window which shifted for 10 millisecond was used. The SDC parameters (N-d-P-k) were configured as 7-1-3-7. These two sets of features have been concatenated to form a 56 dimensional SDC-MFCC feature. Standard energy VAD was applied to remove silent frames, SDC frame was decided to be speech if 70% of the context was deemed speech by VAD. Then, CMVN and feature warping were applied over the features to remove the linear channel effects, mitigate the effect of linear channel mismatch and warp the distribution of the features to the standard\nar X\niv :1\n60 2.\n01 92\n9v 1\n[ cs\n.C L\n] 5\nF eb\n2 01\n6\nnormal distribution."}, {"heading": "2.4. SDC-MFCC (SDC2)", "text": "A second set of SDC-MFCC features has been extracted following a similar process. The main differences with SDC1 are that this second set of SDC-MFCC has been extracted using the publicly available SIDEKIT platform and that the VAD is applied based on an estimated SNR."}, {"heading": "2.5. Stacked multilingual bottleneck features", "text": "Prior work [4] shows that using frame-level features (BNFs) extracted from ASR Deep Neural Network (DNN) with a bottleneck layer instead of shifted-delta cepstrum (SDC) [5] features is very effective in NIST LRE. NTU systems adopt this strategy. However, the difference with [4] is that we extracted the BNFs from a stacked multilingual bottleneck neural network. Figure 1 shows the framework of the stacked multilingual bottleneck neural network (MBNN) training [6, 7, 8, 9]. The class labels are changed to context-dependent (CD) states. During training, the two bottleneck (BN) neural networks (NNs) are multilingually trained successively. Once the first BN NN is finished training, it is fixed as a feature transform to train the second BN NN. After we finish the multilingual bottleneck network training, the two BN NNs are stacked to generate the BNFs for the unseen languages to train acoustic models, realizing MBNN based cross-lingual transfer. Original feature for training is 25-dim: 22 Mel filter-bank log energies plus 3 kaldi pitch features. Hamming window and DCT transformation are applied on 11 contextual features. The first BN NN is configured as 275-1500-1500-80(BN)-1500-7887 and the second 400-1500-1500-30(BN)-1500-7887. All BN layers employ linear-neurons while the other hidden-layers are sigmoid neurons. Networks are trained on GPU using gradient descent cross-entropy criterion without pre-training. Note that we used monophone state labels as SBN output. To obtain better alignment triphone system was used to conduct forced alignment instead, and then phone conversion is performed.\nMultilingual dataset released for OpenKWS2 by IARPA Babel program was used for pre-training stacked multilingual bottleneck neural network. The dataset was composed by 141.3 hours Cantonese, 78.4 hours Pashto, 77.2 hours Turkish, 84.5 hours Tagalog, 70 hours Tamil, and 87 hours Vietnamese. Then, 318-hours switchboard landline data was applied to tuning the second multilingual bottleneck neural network. Only the training data provided by NIST 2015 LRE was used for estimating UBM, i-vector extractor, and language models. Systems based on these features were only used in extended data condition submission."}, {"heading": "3. i-vector extraction", "text": ""}, {"heading": "3.1. I2R", "text": "The 64-dimensional bottleneck features (BNF2) are used for extracting the i-vectors. An energy-based voice activity detection (VAD) technique was applied to the raw bottleneck features to exclude the silence frames. The voiced frames were then used to train a universal background model with 1024 Gaussians with diagonal covariances. The diagonal UBM was then used as an initial point to train a full-covariance UBM with 1024 Gaussians. The full-covariance UBM is then used to train the total variability matrix and extract the i-vectors.\n2http://www.nist.gov/itl/iad/mig/openkws.cfm"}, {"heading": "3.2. LIUM", "text": "Two sets of i-vectors are extracted using SIDEKIT based on BNF1 and SDC2 features. Both i-vector extractors are based on diagonal covariance UBMs and use minimum-divergence criteria at each iteration. The number of distribution in the UBM is 512V for the bottleneck features and 1024 for the SDC features while the rank of the total variability matrix is 500 for both systems. i-vectors are then normalized using one itertion of the EFR algorithm [10]."}, {"heading": "3.3. NTU - Stacked Bottleneck i-vectors", "text": "The i-vector extractor is trained using the 30-dim bottleneck features (BNFs) plus their 1st- and 2nd-derivatives extracted from each utterance, leading to 90-dim feature vectors. It is followed by cepstral mean normalization with a window size of 3 seconds. An energy based voice activity detection (VAD) method was used to remove the silence frames. Then, these features are fed to the standard i-vector framework. The i-vector framework is based on a 2048 Gaussian UBM with full covariance. The same data set was used to train a total variability matrix (i-vector extractor) with 400 total factors. Given the ivectors, a Gaussian back-end was used to obtain the scores for each language."}, {"heading": "3.4. UEF", "text": "We fitted a GMM to acoustic features extracted from each speech signal. Since accurately fitting a separate GMM with high number of component for short utterances is difficult, parametric utterance adaptation techniques should be applied to adapt a universal background model (UBM). In this approach, the i-vector framework has been employed to adapt UBM means. So, a GMM with 512 mixture components was considered for the UBM. The parameters of the UBM and a 400 dimensional total variability subspace were trained using 2/3 of available training data. The inferred i-vectors were whitened and length-normalized."}, {"heading": "4. Core System Descriptions", "text": ""}, {"heading": "4.1. BNF-MCLR", "text": "The multiclass logistic regression (MCLR) system is based on the multi-class cross-entropy discriminative training in the score vector space. To this end, i-vectors were transformed into loglikelihood score vectors through a set of Gaussian distributions, each representing the distribution of the language class in the ivector space. As the amount of data is extremely imbalance among classes, with some languages limited to less than an hour of speech, we trained a global covariance matrix where language-specific covariance could be derived with a smoothing factor of 0.1. Given a test i-vector, a score vector is obtained by concatenating the log-likelihood scores from these Gaussian distribution. Discriminative training is further applied on the score vector. The multiclass FoCal toolkit was used for this purpose."}, {"heading": "4.2. SDC-MCLR", "text": "A linear Gaussian back-end was trained using 2/3 of available training data to compute log-likelihood scores for each utterance. To calibrate the scores, the scores of evaluation set were applied to a linear logistic regression which was trained with scores of 1/3 of training data. Finally, calibrated scores were scaled to log-likelihood ratio. In these experiments, FoCal toolkit has been used to calibrate the scores."}, {"heading": "4.3. BNF-PLDA-500", "text": "i-vectors extracted by the LIUM have been used to train a Gaussian-PLDA of rank 70. The resulting scores computed for each language are fed to a linear Gaussian Backend. Resulting scores are then rescaled for each of the cluster. Note that this system is entirely build on the SIDEKIT platform and that a tutorial will be released post eval to provide the entire training process."}, {"heading": "4.4. BNF-PLDA-600", "text": "The bottleneck i-vectors are used in this system.The i-vectors extracted from the files shorter than 1 second were removed from the training. The i-vectors are the applied with whitening and length norm. For each language cluster, a domain-adapted Simplified PLDA [11] (weighted likelihood approach) with 20- dimensional latent subspace is trained. The training data for each language within a cluster is used as the enrollment data and a adapted-prior (single Gaussian) for PLDA [3] is computed. The Gaussians are then fine-tuned with the MMI procedure [12]."}, {"heading": "4.5. BNF-SVM", "text": "We also use i-vector features to construct two more SVM systems. The i-vectors are directly feed into SVM system and train individual language model. The pair-wise testing method is used to generate the individual evaluation segment scores."}, {"heading": "4.6. End-to-end Long-short term memory (LSTM)", "text": "The audio data was preprocessed into 20 ms frames, overlapped by 10ms, into 13 Mel-frequency Cepstrum Coefficients (MFCCs) from 24 filter-banks using SIDEKIT. The first and second derivatives of all coefficients was also concatenated to the original coeffecients, giving a 39-dimensional vector. The coefficients were normalized to have mean 0 and standard devi-\nation 1 over each utterances. Additionally, LSTM need input is a sequence, we roll MFCC into a sequence of 20 frames. This process is performed for each speech file, and the remain frames are truncated. We used bidirectional recurrent neural network [13], in combination with LSTM [14], to bridge long time lags with access to the past and future context of the signal. We chose bidirectional LSTM (BLSTM) because our experiments with unidirectional LSTM gave worse results on the task. The network structure is specified in following table:\nType # Hidden Units Note Input layer (batch size, 20, 39) BLSTM 250 BLSTM 250 BLSTM 250 output is delayed by 15 Projection 512 Activation: rectifier Output 20 Softmax for 20 languages\nAll the network\u2019s parameters were initialized using Glorot\u2019s uniform mentioned in [15]. BLSTM layers used sigmoid as inner activation and tanh for output activation. For training the network we used categorical crossentropy objectives (i.e. onevs-all criterion) and RSMprop optimizer [16], which utilizes the magnitude of recent gradients to normalize the gradients for each backward training step. In order to prevent overfitting training set, we used 4 different techniques:\n\u2022 Early stopping based on generalization loss of tuning set [17].\n\u2022 L2 regularization (weights decay = 1e-4) on 2 Fully connected layers\n\u2022 Dropout with probability of ignoring an activation is uniform(0.3) [18]\n\u2022 Adding random Gaussian noise to perturb the weights.\nOur experiments showed that training deep model (i.e. BLSTM) on biased dataset will cause the whole network converges to the classes with higher proportion of samples. Hence, the training process was carefully executed with a fixed scenarios:\n1. Training 2 epochs with RSMprop, learning rate = 0.001, without noise and dropout, only L2 regularization and early stop enable.\n2. The next 2 epochs with almost the same configuration except dropout probability = 0.3 and learning rate = 0.0001.\n3. Learning rate reduced to 0.00001, with same configuration, 10 more epochs (early stop will stop the algorithm in advance before any strong overfit, and only the best weights (i.e. gave the best result on validation set) was saved.\n4. After that, we used 0.075 Gaussian noise to perturb the network for 10 more epochs without any dropout.\n5. Then, we performed cyclic training to reduce the effect unbalanced training set. The dataset was organized into 2 set, first set with full of training data, the second one was undersampling from training set with balanced distribution of all languages. The training configuration was as follow: every 2 epoch of training with full set, do 2 other epoch with undersampled set (learning rate = 0.00001, with dropout and noise used in turn). This procedure was repeated until no more improvement achieved and cancelled by early stop.\nTaken into account the biased distribution of training set, the network under training process was strongly influenced by the languages with large amount of data. We used prior knowledge from training distribution to calibrate the softmax score before calculated likelihood ratio. We only did calibration on the 3 clusters with the worst performance, addressed the fact that the network was already biased to provide the best result for 3 other clusters."}, {"heading": "4.7. Pair-wise DNN", "text": "Instead of using Gaussian back-ends, DNN post-processing is adopted to get a new representation from the i-vectors. The diagram of the DNN post-processing is shown in Figure 2. Each training sample consists of two input i-vectors and one label. The label is 1 if these two i-vectors are from the same language and 0 otherwise. Two i-vectors will be processed by the i-vector post-processing subnet which consists of one or more hidden layers and one linear transform. During training the gradient of the left and right subnets are computed individually, but their parameters tied and updated together to make sure that the two subnets have the same parameter at all time. The output of the subnets are new representation vector of the respective sentences and will be used for language recognition. The subnets are trained such that simple cosine distance is able to tell whether the two input i-vectors are from the same language. The network is trained layer-by-layer. For example, the first hidden layer is firstly trained until convergence. Then, we add the second hidden layer or the linear transform layer and train the whole network until convergence. The training usually converges after 10-20 epochs.\nTo create the training samples for the DNN, both positive i-vectors pairs where the two i-vectors are from the same languages and negative pairs are randomly generated. In the most basic setting, for each i-vector, we create one positive pair and one negative pair. To create more balanced training samples, we generate more positive/negative pairs for languages with little ivectors and this is found to be helpful on the tune set. We run the pair generation algorithm 20 times to create about 3 million training pairs."}, {"heading": "4.8. Tandem-SVM", "text": "The system uses the bottleneck feature super-vectors to construct kernels of support vector machines (SVMs). Given a language speech data, a GMM is estimated by using MAP adaptation of the means of the UBM. The global 1024 Gaussian mixture component model is trained based on the NIST\u2019s provided developed dataset. The means of mixture components in the GMM are concatenated to a GMM supervector. The features used is the 64 dimension bottleneck feature with additional 13 mfcc, which result in a feature space expansion from 64 to 56636, and 77 to 78848 in dimension, respectively."}, {"heading": "5. Extended data condition", "text": "Compared to the core systems, the contrastive systems rely on the stacked bottleneck feature i-vectors including MCLR in section 4.1, SVM-Ivector as in section 4.5, SVM-UBMMFCCBN77 in section 4.8 by replacing MFRCCBN77 with the 30-dimensional stacked bottleneck features and the pair-wise DNN in section 4.7."}, {"heading": "6. Fusion and submission", "text": "Fusion was decided to be the typical multi-class logistic regression. The development set of scores was divided into two parts, where model parameters were estimated with one part and applied to the held-out part. Regularization was experimented with, but as the improvement on held-out set was negligible it was discarded and final fusion scores were estimated without regularization. Fusion was applied to evaluation set and scores were turned into log-likelihood ratios on per language cluster basis."}, {"heading": "7. References", "text": "[1] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet,\n\u201cFront-end factor analysis for speaker verification,\u201d IEEE Transactions on Audio, Speech & Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.\n[2] W. M. Campbell, D. E. Sturim, D. A. Reynolds, and A. Solomonoff, \u201cSvm based speaker verification using a gmm supervector kernel and nap variability compensation,\u201d in in Proceedings of ICASSP, 2006, 2006, pp. 97\u2013100.\n[3] L. Chen, K. Lee, B. Ma, W. Guo, H. Li, and L. Dai, \u201cMinimum divergence estimation of speaker prior in multi-session PLDA scoring,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, 2014, pp. 4007\u20134011.\n[4] F. Richardson, D. Reynolds, and N. Dehak, \u201cDeep neural network approaches to speaker and language recognition,\u201d IEEE Signal Processing Letters, vol. 22, pp. 1671\u20131675, 2015.\n[5] H. Z. Li, B. Ma, and K. A. Lee, \u201cSpoken language recognition: from fundamentals to practice,\u201d Proceedings of the IEEE, vol. 101, pp. 1136\u20131159, 2013.\n[6] F. Grezl and M. Karafiat, \u201cSemi-supervised bootstrapping approach for neural network feature extractor training,\u201d in 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Olomouc, Dec. 2013.\n[7] F. Grezl, M. Karafiat, and K. Vesely, \u201cAdaptation of multilingual stacked bottle-neck neural network structure for new language,\u201d in Proc. of ICASSP 2014, Florence, Italy, May 2014.\n[8] H. H. Xu, H. Su, E. S. Chng, and H. Z. Li, \u201cSemi-supervised training for bottle-neck feature based DNN-HMM hybrid systems,\u201d in Proc. of Interspeech 2014, Singapore, Sep. 2014.\n[9] H. H. Xu, V. H. Do, X. Xiao, and E. S. Chng, \u201cA comparative study of BNF and DNN multilingual training on cross-lingual low-resource speech recognition,\u201d in Proc. of Interspeech 2015, Dresden, Germany, Sep. 2015.\n[10] P.-M. Bousquet, D. Matrouf, and J.-F. Bonastre, \u201cIntersession compensation and scoring methods in the i-vectors space for speaker recognition,\u201d 2011, pp. 485\u2013488.\n[11] D. Garcia-Romero and A. McCree, \u201cSupervised domain adaptation for i-vector based speaker recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, 2014, pp. 4047\u20134051.\n[12] A. McCree, \u201cMulticlass discriminative training of i-vector language recognition,\u201d in Speaker Odyssey 2014, 2014, pp. 166\u2013172.\n[13] M. Schuster and K. Paliwal, \u201cBidirectional recurrent neural networks,\u201d Trans. Sig. Proc., vol. 45, no. 11, pp. 2673\u20132681, Nov. 1997. [Online]. Available: http://dx.doi.org/10.1109/78.650093\n[14] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997. [Online]. Available: http://dx.doi.org/10.1162/neco.1997.9.8. 1735\n[15] X. Glorot and Y. Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks,\u201d International conference on artificial intelligence and statistics, pp. 249\u2013256, 2010.\n[16] T. Tieleman and G. Hinton, \u201cLecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude,\u201d COURSERA: Neural Networks for Machine Learning, 2012.\n[17] L. Prechelt, \u201cEarly stopping-but when?\u201d in Neural Networks: Tricks of the trade. Springer, 1998, pp. 55\u201369.\n[18] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: A simple way to prevent neural networks from overfitting,\u201d The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014."}], "references": [{"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech & Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Svm based speaker verification using a gmm supervector kernel and nap variability compensation", "author": ["W.M. Campbell", "D.E. Sturim", "D.A. Reynolds", "A. Solomonoff"], "venue": "in Proceedings of ICASSP, 2006, 2006, pp. 97\u2013100.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Minimum divergence estimation of speaker prior in multi-session PLDA scoring", "author": ["L. Chen", "K. Lee", "B. Ma", "W. Guo", "H. Li", "L. Dai"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, 2014, pp. 4007\u20134011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural network approaches to speaker and language recognition", "author": ["F. Richardson", "D. Reynolds", "N. Dehak"], "venue": "IEEE Signal Processing Letters, vol. 22, pp. 1671\u20131675, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Spoken language recognition: from fundamentals to practice", "author": ["H.Z. Li", "B. Ma", "K.A. Lee"], "venue": "Proceedings of the IEEE, vol. 101, pp. 1136\u20131159, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised bootstrapping approach for neural network feature extractor training", "author": ["F. Grezl", "M. Karafiat"], "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Olomouc, Dec. 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptation of multilingual stacked bottle-neck neural network structure for new language", "author": ["F. Grezl", "M. Karafiat", "K. Vesely"], "venue": "Proc. of ICASSP 2014, Florence, Italy, May 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised training for bottle-neck feature based DNN-HMM hybrid systems", "author": ["H.H. Xu", "H. Su", "E.S. Chng", "H.Z. Li"], "venue": "Proc. of Interspeech 2014, Singapore, Sep. 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A comparative study of BNF and DNN multilingual training on cross-lingual low-resource speech recognition", "author": ["H.H. Xu", "V.H. Do", "X. Xiao", "E.S. Chng"], "venue": "Proc. of Interspeech 2015, Dresden, Germany, Sep. 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Intersession compensation and scoring methods in the i-vectors space for speaker recognition", "author": ["P.-M. Bousquet", "D. Matrouf", "J.-F. Bonastre"], "venue": "2011, pp. 485\u2013488.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised domain adaptation for i-vector based speaker recognition", "author": ["D. Garcia-Romero", "A. McCree"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, 2014, pp. 4047\u20134051.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiclass discriminative training of i-vector language recognition", "author": ["A. McCree"], "venue": "Speaker Odyssey 2014, 2014, pp. 166\u2013172.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K. Paliwal"], "venue": "Trans. Sig. Proc., vol. 45, no. 11, pp. 2673\u20132681, Nov. 1997. [Online]. Available: http://dx.doi.org/10.1109/78.650093", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997. [Online]. Available: http://dx.doi.org/10.1162/neco.1997.9.8. 1735", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, pp. 249\u2013256, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURS- ERA: Neural Networks for Machine Learning, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Early stopping-but when?", "author": ["L. Prechelt"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1929}], "referenceMentions": [{"referenceID": 0, "context": "The submitted system is a fusion of nine sub-systems based on i-vectors [1] extracted from different types of features.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "Given the i-vectors, several classifiers are adopted for the language detection task including support vector machines (SVM) [2], multi-class logistic regression (MCLR), Probabilistic Linear Discriminant Analysis (PLDA) [3] and Deep Neural Networks (DNN).", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "Given the i-vectors, several classifiers are adopted for the language detection task including support vector machines (SVM) [2], multi-class logistic regression (MCLR), Probabilistic Linear Discriminant Analysis (PLDA) [3] and Deep Neural Networks (DNN).", "startOffset": 220, "endOffset": 223}, {"referenceID": 3, "context": "A second set of bottleneck DNN [4] was trained using the 40dimensional filter bank features with the first and second order derivatives extracted from the switchboard landline data.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Prior work [4] shows that using frame-level features (BNFs) extracted from ASR Deep Neural Network (DNN) with a bottleneck layer instead of shifted-delta cepstrum (SDC) [5] features is very effective in NIST LRE.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "Prior work [4] shows that using frame-level features (BNFs) extracted from ASR Deep Neural Network (DNN) with a bottleneck layer instead of shifted-delta cepstrum (SDC) [5] features is very effective in NIST LRE.", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "However, the difference with [4] is that we extracted the BNFs from a stacked multilingual bottleneck neural network.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "Figure 1 shows the framework of the stacked multilingual bottleneck neural network (MBNN) training [6, 7, 8, 9].", "startOffset": 99, "endOffset": 111}, {"referenceID": 6, "context": "Figure 1 shows the framework of the stacked multilingual bottleneck neural network (MBNN) training [6, 7, 8, 9].", "startOffset": 99, "endOffset": 111}, {"referenceID": 7, "context": "Figure 1 shows the framework of the stacked multilingual bottleneck neural network (MBNN) training [6, 7, 8, 9].", "startOffset": 99, "endOffset": 111}, {"referenceID": 8, "context": "Figure 1 shows the framework of the stacked multilingual bottleneck neural network (MBNN) training [6, 7, 8, 9].", "startOffset": 99, "endOffset": 111}, {"referenceID": 9, "context": "i-vectors are then normalized using one itertion of the EFR algorithm [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "For each language cluster, a domain-adapted Simplified PLDA [11] (weighted likelihood approach) with 20dimensional latent subspace is trained.", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "The training data for each language within a cluster is used as the enrollment data and a adapted-prior (single Gaussian) for PLDA [3] is computed.", "startOffset": 131, "endOffset": 134}, {"referenceID": 11, "context": "The Gaussians are then fine-tuned with the MMI procedure [12].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "We used bidirectional recurrent neural network [13], in combination with LSTM [14], to bridge long time lags with access to the past and future context of the signal.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "We used bidirectional recurrent neural network [13], in combination with LSTM [14], to bridge long time lags with access to the past and future context of the signal.", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "All the network\u2019s parameters were initialized using Glorot\u2019s uniform mentioned in [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "onevs-all criterion) and RSMprop optimizer [16], which utilizes the magnitude of recent gradients to normalize the gradients for each backward training step.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "\u2022 Early stopping based on generalization loss of tuning set [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "3) [18]", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "This article describes the systems jointly submitted by Institute for Infocomm (IR), the Laboratoire d\u2019Informatique de l\u2019Universit du Maine (LIUM), Nanyang Technology University (NTU) and the University of Eastern Finland (UEF) for 2015 NIST Language Recognition Evaluation (LRE). The submitted system is a fusion of nine sub-systems based on i-vectors [1] extracted from different types of features. Given the i-vectors, several classifiers are adopted for the language detection task including support vector machines (SVM) [2], multi-class logistic regression (MCLR), Probabilistic Linear Discriminant Analysis (PLDA) [3] and Deep Neural Networks (DNN).", "creator": "LaTeX with hyperref package"}}}