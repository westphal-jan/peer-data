{"id": "1602.02218", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Strongly-Typed Recurrent Neural Networks", "abstract": "Recurrent neural network frequently increasing besides models but sequential focus. Unfortunately, instance the too any RNN modular are perhaps unduly possible, related detailed have this found optimized risky. This sheets output ideas from graduate and arithmetic programming after RNN piece will provide guiding principles. From physics we approve component stresses, parameters to well constraints taken implicate adding meters to straight in mathematical. From differential broadcasters, how necessary believe support - longhand 64-bit factorize trying non-observant learnware and oregon - dependent iphone, eliminating nonparticipation made impact 's side - effects. The page come already strongly - longhand bruins have taken simple heterogeneity contrary users depends rate - facilitation place far - computation convolutions. We not series that opinion - typed gradients are better behaved rest started notable architectures, including characterize all performative creating such nonetheless - sequentially nets. Finally, therapy shown which, despite by lot constrained, contrary - handwriting architectures achieve lower training shots however increases generalization error to melodic computation.", "histories": [["v1", "Sat, 6 Feb 2016 05:34:03 GMT  (25kb)", "http://arxiv.org/abs/1602.02218v1", "9 pages"], ["v2", "Tue, 24 May 2016 21:35:23 GMT  (26kb)", "http://arxiv.org/abs/1602.02218v2", "10 pages, final version, ICML 2016"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["david balduzzi", "muhammad ghifary"], "accepted": true, "id": "1602.02218"}, "pdf": {"name": "1602.02218.pdf", "metadata": {"source": "META", "title": "Strongly-Typed Recurrent Neural Networks", "authors": ["David Balduzzi"], "emails": ["DBALDUZZI@GMAIL.COM", "MGHIFARY@GMAIL.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n02 21\n8v 1\n[ cs\n.L G\n] 6\nF eb\n2 01"}, {"heading": "1. Introduction", "text": "Recurrent neural networks (RNNs) are models that learn nonlinear relationships between sequences of inputs and outputs. Applications include speech recognition (Graves et al., 2013), image generation (Gregor et al., 2015), machine translation (Sutskever et al., 2014) and image captioning (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015). Training large RNNs can be difficult due to exploding and vanishing gradients (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013). Researchers have therefore developed gradient-stabilizing architectures such as Long Short-Term Memories or LSTMs\n(Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units or GRUs (Cho et al., 2014).\nUnfortunately, LSTMs and GRUs are complicated and contain many components whose roles are not well understood. Extensive searches (Bayer et al., 2009; Jozefowicz et al., 2015; Greff et al., 2015) have not yielded significant improvements. This paper takes a fresh approach inspired by dimensional analysis and functional programming.\nIntuition from dimensional analysis. Nodes in neural networks are devices that, by computing dot products, measure the similarity of their inputs to representations encoded in weight matrices. Ideally, the representation learned by a net should \u201ccarve nature at its joints\u201d. An exemplar is the system of measurement that has been carved out of nature by physicists. It prescribes units for expressing the readouts of standardized measuring devices (e.g. kelvin for thermometers and seconds for clocks) and rules for combining them.\nA fundamental rule is the principle of dimensional homogeneity, which states that it is only meaningful to add quantities expressed in the same units (Bridgman, 1922; Hart, 1995). For example adding seconds to volts is inadmissible. In this paper, we propose to take the measurements performed by neural networks as seriously as physicists take their measurements, and apply the principle of dimensional homogeneity to the representations learned by neural nets, see section 2.\nIntuition from functional programming. Whereas feedforward nets learn to approximate functions, recurrent nets learn to approximate programs \u2013 suggesting lessons from language design are relevant to RNN design. Language researchers stress the benefits of constraints: eliminating GOTO (Dijkstra, 1968); introducing type-systems that prescribe the interfaces between parts of computer programs and guarantee their consistency (Pierce, 2002); and working with stateless (pure) functions.\nFor our purposes, types correspond to units as above. Let\nus therefore discuss the role of states. The reason for recurrent connections is precisely to introduce state-dependence. Unfortunately, state-dependent functions have side-effects \u2013 unintended knock-on effects such as exploding gradients.\nState-dependence without side-effects is not possible. The architectures proposed below encapsulate states in firmware (which has no learned parameters) so that the learnware (which encapsulates the parameters) is stateless. It follows that the learned features and gradients in strongly-typed architectures are better behaved and more interpretable than their classical counterparts, see section 3.\nStrictly speaking, the ideas from physics (to do with units) and functional programming (to do with states) are logically independent. However, we found that they complemented each other. We refer to architectures as stronglytyped when they both (i) preserve the type structure of their features and (ii) separate learned parameters from statedependence.\nOverview. The core of the paper is section 2, which introduces strongly-typed linear algebra. As partial motivation, we show how types are implicit in principal component analysis and feedforward networks. A careful analysis of the update equations in vanilla RNNs identifies a flaw in classical RNN designs that leads to incoherent features. Fixing the problem requires new update equations that preserve the type-structure of the features.\nSection 3 presents strongly-typed analogs of standard RNN architectures. It turns out that small tweaks to the standard update rules yield simpler features and gradients, theorem 1 and corollary 2. Finally, theorem 3 shows that, despite their more constrained architecture, strongly-typed RNNs have similar representational power to classical RNNs. Experiments in section 4 show that strongly-typed RNNs have comparable generalization performance and, surprisingly, lower training error than classical architectures (suggesting greater representational power). The flipside is that regularization appears to be more important for strongly-typed architectures, see experiments.\nRelated work. The analogy between neural networks and functional programming was proposed in (Olah, 2015), which also argued that representations should be interpreted as types. This paper sharpens, extends and applies Olah\u2019s proposal. There has been prior work on typed-linear algebra (Macedo & Oliveira, 2013). However, their proposal is not intended for or suited to applications in machine learning. Many familiar RNN architectures already incorporate forms of weak-typing, see section 3.1.\nNotation. Dot-products are denoted by \u3008w,x\u3009 or w\u22bax."}, {"heading": "2. Strongly-Typed Features", "text": "A variety of type systems have been developed for mathematical logic and language design (Reynolds, 1974; Girard, 1989; Pierce, 2002). We introduce a type-system based on linear algebra that is suited to deep learning. Informally, a type is a vector space with an orthogonal basis. A more precise definition along with rules for manipulating types is provided below. Section 2.2 provides examples; section 2.3 uses types to identify a design flaw in classical RNNs."}, {"heading": "2.1. Strongly-Typed Quasi-Linear Algebra", "text": "Quasi-linear algebra is linear algebra supplemented with nonlinear functions that act coordinatewise. Definition 1. A type T = ( V, \u3008\u2022, \u2022\u3009, {ti} d i=1 ) is a ddimensional vector space equipped with an inner product and an orthogonal basis such that \u3008ti, tj\u3009 = 1[i=j].\nGiven type T , we can represent vectors in v \u2208 V as realvalued d-tuples via\nvT \u2194 (v1, . . . , vd) \u2208 R d where vi := \u3008v, ti\u3009.\nDefinition 2. The following operations are admissible:\nT1. Unary operations on a type: T \u2192 T Given a function f : R \u2192 R (e.g. scalar multiplication, sigmoid \u03c3, tanh \u03c4 or relu \u03c1), define\nf(v) := ( f(v1), . . . , f(vd) ) \u2208 T .\nT2. Binary operations on a type: T \u00d7 T \u2192 T Given v,w \u2208 T and an elementary binary operation bin \u2208 {+,\u2212,max,min, \u03c01, \u03c02} 1, define\nbin(v,w) := ( bin(v1, w1), . . . , bin(vd, wd) ) .\nBinary operations on two different types (e.g. adding vectors expressed in different orthogonal bases) are not admissible.\nT3. Transformations between types: T1 \u2192 T2 A type-transform is a linear map P : V1 \u2192 V2 such that P(t(1)i ) = t (2) i for i = {1, . . . ,min(d1, d2)}.\nType-transformations are orthogonal matrices.\nT4. Diagonalization: T1 \u2192 (T2 \u2192 T2) Suppose that v \u2208 T1 and w \u2208 T2 have the same dimension. Define\nvT1 \u2299wT2 := (v1 \u00b7 w1, . . . , vd \u00b7 wd) \u2208 T2,\nwhere vi := \u3008v, t (1) i \u3009 and wi := \u3008w, t (2) i \u3009. Diagonalization converts type T1 into a new type, T2 \u2192 T2, that acts on T2 by coordinatewise scalar multiplication.\n1Note: \u03c0i is projection onto the ith coordinate.\nDefinition 1 is inspired by how physicists have carved the world into an orthogonal basis of meters, amps, volts etc. The analogy is not perfect: e.g. f(x) = x2 maps meters to square-meters, whereas types are invariant to coordinatewise operations. Types are looser than physical units."}, {"heading": "2.2. Motivating examples", "text": "We build intuition by recasting PCA and feedforward neural nets from a type perspective.\nPrincipal component analysis (PCA). Let X \u2208 Rn\u00d7d denote n datapoints {x(1), . . . ,x(n)} \u2282 Rd. PCA factorizes X\u22baX = P\u22baDP where P is a (d \u00d7 d)-orthogonal matrix and D = diag(d) contains the eigenvalues of X\u22baX.\nA common application of PCA is dimensionality reduction. From a type perspective, this consists in:\nT{ek} P \u2212\u2192 (i) T{pk} Proj \u2212\u2212\u2212\u2192 (ii) T{pk} P\u22ba \u2212\u2212\u2192 (iii) T{ek},\n(i) transforming the standard orthogonal basis {ek}dk=1 of R\nd into the latent type given by the rows of P; (ii) projecting onto a subtype (subset of coordinates in the latent type); and (iii) applying the inverse to recover the original type.\nFeedforward nets. The basic feedforward architecture is stacked layers computing h = f(W \u00b7 x) where f(\u2022) is a nonlinearity applied coordinatewise. We present two descriptions of the computation.\nThe standard description is in terms of dot-products. Rows of W correspond to features, and matrix multiplication is a collection of dot-products that measure the similarity between the input x and the row-features:\nWx =\n\n  \u00b7 \u00b7 \u00b7 w1 \u00b7 \u00b7 \u00b7 ...\n\u00b7 \u00b7 \u00b7 wd \u00b7 \u00b7 \u00b7\n\n x =\n\n  \u3008w1,x\u3009 ...\n\u3008wd,x\u3009\n\n  .\nTypes provide a finer-grained description. Factorize W = PDQ\u22ba by singular value decomposition into D = diag(d) and orthogonal matrices P and Q. The layercomputation can be rewritten as h = f(PDQ\u22bax). From a type-perspective, the layer thus:\nTx Q\u22ba\n\u2212\u2212\u2192 (i)\nTlatent diag(d)\u2299\u2022 \u2212\u2212\u2212\u2212\u2212\u2212\u2192\n(ii) Tlatent P \u2212\u2212\u2192 (iii) Th f(\u2022) \u2212\u2212\u2212\u2192 (iv) Th,\n(i) transforms x to a latent type; (ii) applies coordinatewise scalar multiplication to the latent type; (iii) transforms the result to the output type; and (iv) applies a coordinatewise nonlinearity. Feedforward nets learn interleaved sequences of type transforms and unary, type-preserving operations."}, {"heading": "2.3. Incoherent features in classical RNNs", "text": "There is a subtle inconsistency in classical RNN designs that leads to incoherent features. Consider the updates:\nvanilla RNN: ht = \u03c3(V \u00b7 ht\u22121 +W \u00b7 xt + b). (1)\nWe drop the nonlinearity, since the inconsistency is already visible in the linear case. Letting zt := Wxt and unfolding Eq. (1) over time obtains\nht = t\u2211\ns=1\nVt\u2212s \u00b7 zs. (2)\nThe inconsistency can be seen via dot-products and via types. From the dot-product perspective, observe that multiplying an input by a matrix squared yields\nV2z =\n\n  v1 ... vd\n\n \n\n  \n... ...\nc1 \u00b7 \u00b7 \u00b7 cd ... ...\n\n   z =\n\n   \n\u2329\n(v\u22ba1ci )d i=1 , z \u232a\n... \u2329\n(v\u22badci )d i=1 , z \u232a\n\n    ,\nwhere vi refers to rows of V and ci to columns. Each coordinate of V2z is computed by measuring the similarity of a row of V to all of its columns, and then measuring the similarity of the result to z. In short, a spaghetti-like mess.\nFrom a type perspective, apply an SVD to V = PDQ\u22ba and observe that V2 = PDQ\u22baPDQ\u22ba. Each multiplication by P or Q\u22ba transforms the input to a new type, obtaining\nTh DQ\u22ba \u2212\u2212\u2212\u2192 Tlat1 P \u2212\u2192 Tlat2 \ufe38 \ufe37\ufe37 \ufe38\nV\nDQ\u22ba\n\u2212\u2212\u2212\u2192 Tlat3 P \u2212\u2192 Tlat4 \ufe38 \ufe37\ufe37 \ufe38\nV\n.\nThus V sends z 7\u2192 Tlat2 whereas V 2 sends z 7\u2192 Tlat4 . Adding terms involving V and V2, as in Eq. (2), entails adding vectors expressed in different orthogonal bases \u2013 which is analogous to adding joules to volts. The same problem applies to LSTMs and GRUs.\nTwo recent papers provide empirical evidence that recurrent (horizontal) connections are problematic even after gradients are stabilized: (Zaremba et al., 2015) find that Dropout performs better when restricted to vertical connections and (Laurent et al., 2015) find that Batch Normalization fails unless restricted to vertical connections (Ioffe & Szegedy, 2015). More precisely, (Laurent et al., 2015) find that Batch Normalization improves training but not test error when restricted to vertical connections; it fails completely when also applied to horizontal connections.\nCode using GOTO can be perfectly correct, and RNNs with type mismatches can achieve outstanding performance. Nevertheless, both lead to spaghetti-like information/gradient flows that are hard to reason about.\nType-preserving transforms. One way to resolve the type inconsistency, which we do not pursue in this paper, is to use symmetric weight matrices so that V = PDP\u22ba where P is orthogonal and D = diag(d). From the dotproduct perspective,\nht = t\u2211\ns=1\nPDt\u2212sP\u22bazs,\nwhich has the simple interpretation that z is amplified (or dampened) byD in the latent type provided byP. From the type-perspective, multiplication by Vk is type-preserving\nTh P\u22ba \u2212\u2212\u2192 Tlat1 dk\u2299\u2022 \u2212\u2212\u2212\u2192 Tlat1 P \u2212\u2192 Th \ufe38 \ufe37\ufe37 \ufe38\nVk\nso addition is always performed in the same basis.\nA familiar example of type-preserving transforms is autoencoders \u2013 under the constraint that the decoder W\u22ba is the transpose of the encoder W. Finally, (Moczulski et al., 2015) propose to accelerate matrix computations in feedforward nets by interleaving diagonal matrices, A and D, with the orthogonal discrete cosine transform, C. The resulting transform, ACDC\u22ba, is type-preserving."}, {"heading": "3. Recurrent Neural Networks", "text": "We present three strongly-typed RNNs that purposefully mimic classical RNNs as closely as possible. Perhaps surprisingly, the tweaks introduced below have deep structural implications, yielding architectures that are significantly easier to reason about, see sections 3.3 and 3.4."}, {"heading": "3.1. Weakly-Typed RNNs", "text": "We first pause to note that many classical architectures are weakly-typed. That is, they introduce constraints or restrictions on off-diagonal operations on recurrent states.\nThe memory cell c in LSTMs is only updated coordinatewise and is therefore well-behaved type-theoretically \u2013 although the overall architecture is not type consistent. The gating operation zt \u2299 ht\u22121 in GRUs reduces typeinconsistencies by discouraging (i.e. zeroing out) unnecessary recurrent information flows.\nSCRNs, or Structurally Constrained Recurrent Networks (Mikolov et al., 2015), add a type-consistent state layer:\nst = \u03b1 \u00b7 st\u22121 + (1\u2212 \u03b1) \u00b7Wsxt, where \u03b1 is a scalar.\nIn MUT1, the best performing architecture in (Jozefowicz et al., 2015), the behavior of z and h is well-typed, although the gating by r is not. Finally, IRNNs initialize their recurrent connections as the identity matrix (Le et al., 2015). In other words, the key idea is a type-consistent initialization."}, {"heading": "3.2. Strongly-Typed RNNs", "text": "The vanilla strongly-typed RNN is\nzt = Wxt (3)\nT-RNN ft = \u03c3(Vxt + b) (4)\nht = ft \u2299 ht\u22121 + (1 \u2212 ft)\u2299 zt (5)\nThe T-RNN has similar parameters to a vanilla RNN, Eq (1), although their roles have changed. A nonlinearity for zt is not necessary because: (i) gradients do not explode, corollary 2, so no squashing is needed; and (ii) coordinatewise multiplication by ft introduces a nonlinearity. Whereas relus are binary gates (0 if zt < 0, 1 else); the forget gate ft is a continuous multiplicative gate on zt.\nReplacing the horizontal connection Vht\u22121 with a vertically controlled gate, Eq. (4), stabilizes the type-structure across time steps. Line for line, the type structure is:\nTx (3) \u2212\u2212\u2212\u2212\u2212\u2192 Th Tx (4) \u2212\u2212\u2212\u2212\u2212\u2192 Tf diag \u2212\u2212\u2212\u2192 (Th \u2192 Th)\n(Th \u2192 Th \ufe38 \ufe37\ufe37 \ufe38\nft\n)\u00d7 Th \ufe38\ufe37\ufe37\ufe38 zt\n(5) \u2212\u2212\u2212\u2212\u2212\u2192\nht\u22121 Th \ufe38\ufe37\ufe37\ufe38\nht\nWe refer to lines (3) and (4) as learnware since they have parameters (W,V,b). Line (5) is firmware since it has no parameters. The firmware depends on the previous state ht\u22121 unlike the learnware which is stateless. See section 3.4 for more on learnware and firmware.\nStrongly-typed LSTMs differ from LSTMs in two respects: (i) xt\u22121 is substituted for ht\u22121 in the first three equations so that the type structure is coherent; and (ii) the nonlinearities in zt and ht are removed as for the T-RNN.\nzt = \u03c4(Vzht\u22121 +Wzxt + bz)\nft = \u03c3(Vfht\u22121 +Wfxt + bf )\nLSTM ot = \u03c4(Voht\u22121 +Woxt + bo)\nct = ft \u2299 ct\u22121 + (1\u2212 ft)\u2299 zt\nht = \u03c4(ct)\u2299 ot\nzt = Vzxt\u22121 +Wzxt + bz\nft = \u03c3(Vfxt\u22121 +Wfxt + bf )\nT-LSTM ot = \u03c4(Voxt\u22121 +Woxt + bo)\nct = ft \u2299 ct\u22121 + (1\u2212 ft)\u2299 zt\nht = ct \u2299 ot\nWe drop the input gate from the updates for simplicity; see (Greff et al., 2015). The type structure is\nTx \u2212\u2212\u2192 Tc Tx \u2212\u2212\u2192 Tf diag \u2212\u2212\u2212\u2192 (Tc \u2192 Tc) Tx \u2212\u2212\u2192 Th\n(Tc \u2192 Tc)\u00d7 Tc \u2212\u2212\u2212\u2192 ct\u22121\nTc diag \u2212\u2212\u2212\u2192 (Th \u2192 Th)\n(Th \u2192 Th)\u00d7 Th \u2212\u2212\u2192 Th\nStrongly-typed GRUs adapt GRUs similarly to how LSTMs were modified. In addition, the reset gate zt is repurposed; it is no longer needed for weak-typing.\nzt = \u03c3(Vzht\u22121 +Wzxt + bz)\nGRU ft = \u03c3(Vfht\u22121 +Wfxt + bf )\not = \u03c4 ( Vo(zt \u2299 ht\u22121) +Woxt + bo )\nht = ft \u2299 ht\u22121 + (1\u2212 ft)\u2299 ot\nzt = Vzxt\u22121 +Wzxt + bz\nT-GRU ft = \u03c3(Vfxt\u22121 +Wfxt + bf )\not = \u03c4(Voxt\u22121 +Woxt + bo)\nht = ft \u2299 ht\u22121 + zt \u2299 ot\nThe type structure is\nTx \u2212\u2212\u2192 Th Tx \u2212\u2212\u2192 Tf diag \u2212\u2212\u2212\u2192 (Th \u2192 Th) Tx \u2212\u2212\u2192 To diag \u2212\u2212\u2212\u2192 (Th \u2192 Th)\n(Th \u2192 Th) \u00d7 (Th \u2192 Th)\u00d7 Th \u2212\u2212\u2212\u2192 ht\u22121\nTh"}, {"heading": "3.3. Feature Semantics", "text": "The output of a vanilla RNN expands as the uninterpretable\nht = \u03c3(V\u03c3(V\u03c3(\u00b7 \u00b7 \u00b7 ) +Wxt\u22121 + b) +Wxt + b),\nwith even less interpretable gradient. Similar considerations hold for LSTMs and GRUs. Fortunately, the situation is more amenable for strongly-typed architectures. In fact, their semantics are related to average-pooled convolutions.\nConvolutions. Applying a one-dimensional convolution to input sequence x[t] yields output sequence\nz[t] = (W \u2217 x)[t] = \u2211\ns\nW[s] \u00b7 x[t\u2212 s]\nGiven weights fs associated with W[s], average-pooling yields ht = \u2211t s=1 fs \u00b7 z[s]. A special case is when the\nconvolution applies the same matrix to every input:\nW[s] =\n{\nW if s = 0\n0 else.\nThe average-pooled convolution is then a weighted average of the features extracted from the input sequence.\nDynamic temporal convolutions. We now show that strongly-typed RNNs are one-dimensional temporal convolutions with dynamic average-pooling. Informally, strongly-typed RNNs transform input sequences into a weighted average of features extracted from the sequence\nx1:t 7\u2192 E Px1:t\n[ W \u2217 x ] =\nt\u2211\ns=1\nPx1:t(s) \u00b7 (W \u00b7 xs) =: h[t]\nwhere the weights depends on the sequence. In detail:\nTheorem 1 (feature semantics via dynamic convolutions). Strongly-typed features are computed explicitly as follows.\n\u2022 T-RNN. The output is ht = Es\u223cPx1:t [ Wxs ] where\nPx1:t(s) =\n{\n1\u2212 ft if s = t\nft \u2299 Px1:t\u22121(s) else.\n\u2022 T-LSTM. Let U\u2022 := [V\u2022;W\u2022;b\u2022] and x\u0303t := [xt\u22121;xt; 1] denote the vertical concatenation of the weight matrices and input vectors respectively. Then,\nht = \u03c4 ( Uox\u0303t ) \u2299 E\ns\u223cPx1:t\n[ Uzx\u0303s ]\nwhere Px1:t is defined as above.\n\u2022 T-GRU. Using the notation above,\nht =\nt\u2211\ns=1\nFs \u2299 ( \u03c4 ( Uox\u0303s ) \u2299Uzx\u0303s )\nwhere\nFs =\n{\n1 if s = t\nfs \u2299 Fs+1 else.\nProof. Direct computation.\nIn summary, T-RNNs compute a dynamic distribution over time steps, and then compute the expected feedforward features over that distribution. T-LSTMs store expectations in private memory cells that are reweighted by the output gate when publicly broadcast. Finally, T-GRUs drop the requirement that the average is an expectation, and also incorporate the output gate into the memory updates.\nStrongly-typed gradients are straightforward to compute and interpret:\nCorollary 2 (gradient semantics). The strongly-typed gradients are\n\u2022 T-RNN:\n\u2202ht \u2202W = E\ns\u223cPx1:t\n[ \u2202\n\u2202W (zs)\n]\n\u2202ht \u2202V = E\ns\u223cPx1:t\n[\nzs \u2299 \u2202\n\u2202V\n( logPx1:t(s)\n)]\nand similarly for \u2202 \u2202b .\n\u2022 T-LSTM:\n\u2202ht \u2202Uo = \u2202 \u2202Uo (ot)\u2299 E s\u223cPx1:t\n[\nzs\n]\n\u2202ht \u2202Uz = ot \u2299 E s\u223cPx1:t\n[ \u2202\n\u2202Uz (zs)\n]\n\u2202ht\n\u2202Uf = ot \u2299 E s\u223cPx1:t\n[\nzs \u2299 \u2202\n\u2202Uf\n( logPx1:t(s)\n)]\n\u2022 T-GRU:\n\u2202ht \u2202Uo =\nt\u2211\ns=1\nFs \u2299 \u2202\n\u2202Uo (os)\u2299 zs\n\u2202ht \u2202Uz =\nt\u2211\ns=1\nFs \u2299 os \u2299 \u2202\n\u2202Uz (zs)\n\u2202ht\n\u2202Uf =\nt\u2211\ns=1\n\u2202\n\u2202Uf\n( Fs ) \u2299 os \u2299 zs\nIt follows immediately that gradients will not explode for T-RNNs or LSTMs. Empirically we find they also behave well for T-GRUs."}, {"heading": "3.4. Feature Algebra", "text": "A vanilla RNN can approximate any continuous state update ht = g(xt,ht\u22121) since span{s(w\u22bax) |w \u2208 Rd} is dense in continuous functions C(Rd) on Rd if s is a nonpolynomial nonlinear function (Leshno et al., 1993). It follows that vanilla RNNs can approximate any recursively computable partial function (Siegelmann & Sontag, 1995).\nStrongly-typed RNNs are more constrained. We show the constraints reflect a coherent design-philosophy and are less severe than appears.\nThe learnware / firmware distinction. Strongly-typed architectures factorize into stateless learnware and statedependent firmware. For example, T-LSTMs and T-GRUs\nfactorize2 as\n(ft, zt,ot) = T-LSTM learn {V\u2022,W\u2022,b\u2022} (xt\u22121,xt)\n(ht, ct) = T-LSTM firm(ft, zt,ot; ct\u22121\n\ufe38\ufe37\ufe37\ufe38 state\n)\n(ft, zt,ot) = T-GRU learn {V\u2022,W\u2022,b\u2022}(xt\u22121,xt)\nht = T-GRU firm(ft, zt,ot;ht\u22121\n\ufe38\ufe37\ufe37\ufe38 state\n).\nFirmware decomposes coordinatewise, which prevents side-effects from interacting: e.g. for T-GRUs\nT-GRUfirm(f , z,o;h) = ( \u03d5(f (i), z(i), o(i);h(i)) )d\ni=1\nwhere \u03d5(f, z, o;h) = fh+ zo\nand similarly for T-LSTMs. Learnware is stateless; it has no side-effects and does not decompose coordinatewise. Evidence that side-effects are a problem for LSTMs can be found in (Zaremba et al., 2015) and (Laurent et al., 2015), which show that Dropout and Batch Normalization respectively need to be restricted to vertical connections.\nIn short, under strong-typing the learnware carves out features which the firmware uses to perform coordinatewise state updates hit = g(h i t\u22121, z i t\u22121). Vanilla RNNs allow arbitrary state updates ht = g(ht\u22121,xt). LSTMs and GRUs restrict state updates, but allow arbitrary functions of the state. Translated from a continuous to discrete setting, the distinction between strongly-typed and classical architectures is analogous to working with binary logic gates (AND, OR) on variables zt learned by the vertical connections \u2013 versus working directly with n-ary boolean operations.\nRepresentational power. Motivated by the above, we show that a minimal strongly-typed architecture can span the space of continuous binary functions on features.\nTheorem 3 (approximating binary functions). The strongly-typed minimal RNN with updates\nT-MR: ht = \u03c1(b\u2299 ht\u22121 +Wxt + c)\nand parameters (b, c, W) can approximate any set of continuous binary functions on features.\nProof sketch. Let z = w\u22bax be a feature of interest. Combining (Leshno et al., 1993) with the observation that a\u03c1(bh + z + c) = \u03c1(abh + az + ac) for a > 0 implies that span{\u03c1(b \u00b7 ht\u22121 + zt) | b, c \u2208 R} = C(R2). As many weighted copies az of z as necessary are obtained by adding rows to W that are scalar multiples of w.\n2A superficially similar factorization holds for GRUs and LSTMs. However, their learnware is state-dependent, since (ft, zt,ot) depend on ht\u22121.\nAny set of binary functions on any collection of features can thus be approximated. Finally, vertical connections can approximate any set of features (Leshno et al., 1993)."}, {"heading": "4. Experiments", "text": "We investigated the empirical performance of stronglytyped recurrent networks for sequence learning. The performance was evaluated on character-level and word-level text generation.\nWe conducted a set of proof-of-concept experiments. The goal is therefore not to compete with previous work or to find the best performing model under a specific hyperparameter setting. Rather, we investigate how the two classes of architectures perform over a range of settings."}, {"heading": "4.1. Character-level Text Generation", "text": "The first task is to generate text from a sequence of characters by predicting the next character in a sequence. We used Leo Tolstoy\u2019s War and Peace (WP) dataset (Karpathy et al., 2015) which consists of 3,258,246 characters of English text, split into train/val/test sets with 80/10/10 ratios. The characters are encoded into K-dimensional one-hot vectors, where K is the size of the vocabulary. We follow the basic experimental setting proposed in (Karpathy et al., 2015). Results are reported for two configurations: \u201c64\u201d and \u201c256\u201d, which refer to models containing an equivalent number of parameters to a 1-layer LSTM with 64 and 256 cells per layer respectively. Dropout regularization was only applied to the \u201c256\u201d models. The dropout rate was taken from {0.1, 0.2} based on validation performance. Tables 2 and 3 summarize the performance in terms of crossentropy loss H(y,p) =\n\u2211K i=1 yi log pi.\nWe observe that the training error of strongly-typed models is typically lower than that of the standard models for two or more layers. The test error of the two classes of architectures are comparable. However, our results (for both classical and typed models) fail to match those reported in (Karpathy et al., 2015), where a more extensive parameter search was performed."}, {"heading": "4.2. Word-level Text Generation", "text": "The second task was to perform word-level text generation by predicting the next word from a sequence of words. We used the Penn Treebank (PTB) dataset (Marcus et al., 1993), which consists of 929K training words, 73K validation words, and 82K test words, with vocabulary size of 10K words. The PTB dataset is publicly available on web.3\nWe followed the experimental setting in (Zaremba et al.,\n3 http://www.fit.vutbr.cz/\u02dcimikolov/rnnlm/simple-examples.tgz\n2015) and compared the performance of \u201csmall\u201d and \u201cmedium\u201d models. The parameter size of \u201csmall\u201d models is equivalent to that of 2 layers of 200-cell LSTMs, while the parameter size of \u201cmedium\u201d models is the same as that of 2 layers of 650-cell LSTMs. For the \u201cmedium\u201d models, we selected the dropout rate from {0.4, 0.5, 0.6} according to validation performance. Single run performance, measured via perplexity, i.e., exp(H(y,p)), are reported in Table 4.\nPerplexity. For the \u201csmall\u201d models, we found that the training perplexity of strongly-typed models is consistently lower than their classical counterparts, in line with the result for War & Peace. Test error was significantly worse for the strongly-typed architectures. A possible explanation for both observations is that strongly-typed architectures require more extensive regularization.\nAn intriguing result is that the T-RNN performs in the same ballpark as LSTMs, with perplexity within a factor of two. By contrast, the vanilla RNN fails to achieve competitive performance. This suggests there may be strongly-typed architectures of intermediate complexity between RNNs and LSTMs with comparable performance to LSTMs.\nThe dropout-regularized \u201cmedium\u201d T-LSTM matches the LSTM performance reported in (Zaremba et al., 2015). The 3-layer T-LSTM obtains slightly better performance. The results were obtained with with almost identical parameters to Zaremba et al (the learning rate decay was altered), suggesting that T-LSTMs are viable alternatives to LSTMs for sequence learning tasks when properly regularized. Strongly-typed GRUs did not match the performance of GRUs, possibly due to insufficient regularization.\nRuntime. Since strongly-typed RNNs have fewer nonlinearities than standard RNNs, we expect that they should\nhave lower computational complexity. Training on the PTB dataset on an NVIDIA GTX 980 GPU, we found that TLSTM is on average \u223c 1.6\u00d7 faster than LSTM. Similarly, the T-GRU trains on average \u223c 1.4\u00d7 faster than GRU."}, {"heading": "5. Conclusions", "text": "RNNs are increasingly important tools for speech recognition, natural language processing and other sequential learning problems. The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015). This paper introduces strong-typing as a tool to guide the search for alternate architectures. In particular, we suggest searching for update equations that learn wellbehaved features, rather than update equations that \u201cappear simple\u201d. We draw on two disparate intuitions that turn out to be surprisingly compatible: (i) that neural networks are analogous to measuring devices (Balduzzi, 2012) and (ii) that training an RNN is analogous to writing code.\nThe main contribution is a new definition of type that is closely related to singular value decomposition \u2013 and is thus well-suited to deep learning. It turns out that classical RNNs are badly behaved from a type-perspective, which motivates modifying the architectures. Section 3 tweaked LSTMs and GRUs to make them well-behaved from a typing and functional programming perspective, yielding features and gradients that are easier to reason about than classical architectures.\nStrong-typing has implications for the depth of RNNs. It was pointed out in (Pascanu et al., 2014) that unfolding\nhorizontal connections over time implies the concept of depth is not straightforward in classical RNNs. By contrast, depth has the same meaning in strongly-typed architectures as in feedforward nets, since vertical connections learn features and horizontal connections act coordinatewise.\nExperiments in section 4 show that strongly-typed RNNs achieve comparable generalization performance to classical architectures when regularized with dropout and have consistently lower training error. It is important to emphasize that the experiments are not conclusive. Firstly, we did not deviate far from settings optimized for classical RNNs when training strongly-typed RNNs. Secondly, the architectures were chosen to be as close as possible to classical RNNs. A more thorough exploration of the space of strongly-typed nets may yield better results.\nTowards machine reasoning. A definition of machine reasoning, adapted from (Bottou, 2014), is \u201calgebraically manipulating features to answer a question\u201d. Hard-won experience in physics (Chang, 2004), software engineering (Dijkstra, 1968), and other fields has led to the conclusion that well-chosen constraints are crucial to effective reasoning. Indeed, neural Turing machines (Graves et al., 2014) are harder to train than more constrained architectures such as neural queues and deques (Grefenstette et al., 2015).\nStrongly-typed features have a consistent semantics, theorem 1, unlike features in classical RNNs which are rotated across time steps \u2013 and are consequently difficult to reason about. We hypothesize that strong-typing will provide a solid foundation for algebraic operations on learned features. Strong-typing may then provide a useful organizing principle in future machine reasoning systems.\nAcknowledgements. We thank Tony Butler-Yeoman, Marcus Frean, Theofanis Karaletsos, JP Lewis and Brian McWilliams for useful comments and discussions."}], "references": [{"title": "On the information-theoretic structure of distributed measurements", "author": ["D. Balduzzi"], "venue": "Elect. Proc. in Theor. Comp. Sci.,", "citeRegEx": "Balduzzi,? \\Q2012\\E", "shortCiteRegEx": "Balduzzi", "year": 2012}, {"title": "Evolving memory cell structures for sequence learning", "author": ["Bayer", "Justin", "Wierstra", "Daan", "Togelius", "Julian", "Schmidhuber", "Juergen"], "venue": "In ICANN,", "citeRegEx": "Bayer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bayer et al\\.", "year": 2009}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "P Simard", "P. Frasconi"], "venue": "IEEE Trans. Neur. Net.,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "From machine learning to machine reasoning: An essay", "author": ["Bottou", "L\u00e9on"], "venue": "Machine Learning,", "citeRegEx": "Bottou and L\u00e9on.,? \\Q2014\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 2014}, {"title": "Dimensional analysis", "author": ["Bridgman", "P W"], "venue": null, "citeRegEx": "Bridgman and W.,? \\Q1922\\E", "shortCiteRegEx": "Bridgman and W.", "year": 1922}, {"title": "Inventing Temperature: Measurement and Scientific Progress", "author": ["Chang", "Hasok"], "venue": null, "citeRegEx": "Chang and Hasok.,? \\Q2004\\E", "shortCiteRegEx": "Chang and Hasok.", "year": 2004}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "author": ["K Cho", "B van Merri\u00ebnboer", "C Gulcehre", "D Bahdanau", "F Bougares", "H Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Proofs and Types", "author": ["Girard", "Jean-Yves"], "venue": null, "citeRegEx": "Girard and Jean.Yves.,? \\Q1989\\E", "shortCiteRegEx": "Girard and Jean.Yves.", "year": 1989}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "A Mohamed", "Hinton", "GE"], "venue": "In ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Neural Turing Machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "In arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Adv in Neural Information Processing Systems (NIPS),", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "LSTM: A Search Space Odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "Juergen"], "venue": "In arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "DRAW: A Recurrent Neural Network For Image Generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Multidimensional Analysis: Algebras and Systems for Science and Engineering", "author": ["Hart", "George W"], "venue": null, "citeRegEx": "Hart and W.,? \\Q1995\\E", "shortCiteRegEx": "Hart and W.", "year": 1995}, {"title": "Long Short-Term Memory", "author": ["S Hochreiter", "J. Schmidhuber"], "venue": "Neural Comp,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["Hochreiter", "Sepp"], "venue": "Master\u2019s thesis, Tech. Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Jozefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In ICML,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Visualizing and understanding recurrent neural networks", "author": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"], "venue": "In arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Batch Normalized Recurrent Neural Networks", "author": ["C Laurent", "G Pereyra", "P Brakel", "Y Zhang", "Bengio", "Yoshua"], "venue": "In arXiv:1510.01378,", "citeRegEx": "Laurent et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Laurent et al\\.", "year": 2015}, {"title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "author": ["Le", "Quoc", "Jaitly", "Navdeep", "Hinton", "Geoffrey"], "venue": "In arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function", "author": ["Leshno", "Moshe", "Lin", "Vladimir Ya", "Pinkus", "Allan", "Schocken", "Shimon"], "venue": "Neural Networks,", "citeRegEx": "Leshno et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Leshno et al\\.", "year": 1993}, {"title": "Typing linear algebra: A biproduct-oriented approach", "author": ["Macedo", "Hugo Daniel", "Oliveira", "Jos\u00e9 Nuno"], "venue": "Science of Computer Programming,", "citeRegEx": "Macedo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Macedo et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewics", "Mary Ann", "Santorini", "Beatrice"], "venue": "Comp. Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Marc\u2019Aurelio. Learning Longer Memory in Recurrent Neural Networks", "author": ["Mikolov", "Tomas", "Joulin", "Armand", "Chopra", "Sumit", "Mathieu", "Michael", "Ranzato"], "venue": "In ICLR,", "citeRegEx": "Mikolov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "ACDC: A Structured Efficient Linear Layer", "author": ["Moczulski", "Marin", "Denil", "Misha", "Appleyard", "Jeremy", "de Freitas", "Nando"], "venue": "In arXiv:1511.05946,", "citeRegEx": "Moczulski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moczulski et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "How to Construct Deep Recurrent Networks", "author": ["Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Types and Programming Languages", "author": ["Pierce", "Benjamin C"], "venue": null, "citeRegEx": "Pierce and C.,? \\Q2002\\E", "shortCiteRegEx": "Pierce and C.", "year": 2002}, {"title": "Towards a theory of type structure", "author": ["Reynolds", "J C"], "venue": "In Paris colloquium on programming,", "citeRegEx": "Reynolds and C.,? \\Q1974\\E", "shortCiteRegEx": "Reynolds and C.", "year": 1974}, {"title": "On the Computational Power of Neural Nets", "author": ["Siegelmann", "Hava", "Sontag", "Eduardo"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Siegelmann et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Siegelmann et al\\.", "year": 1995}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I Sutskever", "O Vinyals", "Q. Le"], "venue": "In Adv in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O Vinyals", "A Toshev", "S Bengio", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Recurrent Neural Network Regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "In arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Applications include speech recognition (Graves et al., 2013), image generation (Gregor et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 12, "context": ", 2013), image generation (Gregor et al., 2015), machine translation (Sutskever et al.", "startOffset": 26, "endOffset": 47}, {"referenceID": 32, "context": ", 2015), machine translation (Sutskever et al., 2014) and image captioning (Vinyals et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 33, "context": ", 2014) and image captioning (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015).", "startOffset": 29, "endOffset": 77}, {"referenceID": 2, "context": "Training large RNNs can be difficult due to exploding and vanishing gradients (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 78, "endOffset": 139}, {"referenceID": 27, "context": "Training large RNNs can be difficult due to exploding and vanishing gradients (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 78, "endOffset": 139}, {"referenceID": 6, "context": "Researchers have therefore developed gradient-stabilizing architectures such as Long Short-Term Memories or LSTMs (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units or GRUs (Cho et al., 2014).", "startOffset": 181, "endOffset": 199}, {"referenceID": 1, "context": "Extensive searches (Bayer et al., 2009; Jozefowicz et al., 2015; Greff et al., 2015) have not yielded significant improvements.", "startOffset": 19, "endOffset": 84}, {"referenceID": 17, "context": "Extensive searches (Bayer et al., 2009; Jozefowicz et al., 2015; Greff et al., 2015) have not yielded significant improvements.", "startOffset": 19, "endOffset": 84}, {"referenceID": 11, "context": "Extensive searches (Bayer et al., 2009; Jozefowicz et al., 2015; Greff et al., 2015) have not yielded significant improvements.", "startOffset": 19, "endOffset": 84}, {"referenceID": 34, "context": "Two recent papers provide empirical evidence that recurrent (horizontal) connections are problematic even after gradients are stabilized: (Zaremba et al., 2015) find that Dropout performs better when restricted to vertical connections and (Laurent et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 20, "context": ", 2015) find that Dropout performs better when restricted to vertical connections and (Laurent et al., 2015) find that Batch Normalization fails unless restricted to vertical connections (Ioffe & Szegedy, 2015).", "startOffset": 86, "endOffset": 108}, {"referenceID": 20, "context": "More precisely, (Laurent et al., 2015) find that Batch Normalization improves training but not test error when restricted to vertical connections; it fails completely when also applied to horizontal connections.", "startOffset": 16, "endOffset": 38}, {"referenceID": 26, "context": "Finally, (Moczulski et al., 2015) propose to accelerate matrix computations in feedforward nets by interleaving diagonal matrices, A and D, with the orthogonal discrete cosine transform, C.", "startOffset": 9, "endOffset": 33}, {"referenceID": 25, "context": "SCRNs, or Structurally Constrained Recurrent Networks (Mikolov et al., 2015), add a type-consistent state layer:", "startOffset": 54, "endOffset": 76}, {"referenceID": 17, "context": "In MUT1, the best performing architecture in (Jozefowicz et al., 2015), the behavior of z and h is well-typed, although the gating by r is not.", "startOffset": 45, "endOffset": 70}, {"referenceID": 21, "context": "Finally, IRNNs initialize their recurrent connections as the identity matrix (Le et al., 2015).", "startOffset": 77, "endOffset": 94}, {"referenceID": 11, "context": "We drop the input gate from the updates for simplicity; see (Greff et al., 2015).", "startOffset": 60, "endOffset": 80}, {"referenceID": 22, "context": "A vanilla RNN can approximate any continuous state update ht = g(xt,ht\u22121) since span{s(wx) |w \u2208 R} is dense in continuous functions C(R) on R if s is a nonpolynomial nonlinear function (Leshno et al., 1993).", "startOffset": 185, "endOffset": 206}, {"referenceID": 34, "context": "Evidence that side-effects are a problem for LSTMs can be found in (Zaremba et al., 2015) and (Laurent et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 20, "context": ", 2015) and (Laurent et al., 2015), which show that Dropout and Batch Normalization respectively need to be restricted to vertical connections.", "startOffset": 12, "endOffset": 34}, {"referenceID": 22, "context": "Combining (Leshno et al., 1993) with the observation that a\u03c1(bh + z + c) = \u03c1(abh + az + ac) for a > 0 implies that span{\u03c1(b \u00b7 ht\u22121 + zt) | b, c \u2208 R} = C(R).", "startOffset": 10, "endOffset": 31}, {"referenceID": 22, "context": "Finally, vertical connections can approximate any set of features (Leshno et al., 1993).", "startOffset": 66, "endOffset": 87}, {"referenceID": 18, "context": "We used Leo Tolstoy\u2019s War and Peace (WP) dataset (Karpathy et al., 2015) which consists of 3,258,246 characters of English text, split into train/val/test sets with 80/10/10 ratios.", "startOffset": 49, "endOffset": 72}, {"referenceID": 18, "context": "We follow the basic experimental setting proposed in (Karpathy et al., 2015).", "startOffset": 53, "endOffset": 76}, {"referenceID": 18, "context": "However, our results (for both classical and typed models) fail to match those reported in (Karpathy et al., 2015), where a more extensive parameter search was performed.", "startOffset": 91, "endOffset": 114}, {"referenceID": 24, "context": "We used the Penn Treebank (PTB) dataset (Marcus et al., 1993), which consists of 929K training words, 73K validation words, and 82K test words, with vocabulary size of 10K words.", "startOffset": 40, "endOffset": 61}, {"referenceID": 34, "context": "82 medium, with dropout LSTM (Zaremba et al., 2015) 48.", "startOffset": 29, "endOffset": 51}, {"referenceID": 34, "context": "The dropout-regularized \u201cmedium\u201d T-LSTM matches the LSTM performance reported in (Zaremba et al., 2015).", "startOffset": 81, "endOffset": 103}, {"referenceID": 1, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 11, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 17, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 21, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 25, "context": "The complicated structure of LSTMs and GRUs has led to searches for simpler alternatives with limited success (Bayer et al., 2009; Greff et al., 2015; Jozefowicz et al., 2015; Le et al., 2015; Mikolov et al., 2015).", "startOffset": 110, "endOffset": 214}, {"referenceID": 0, "context": "We draw on two disparate intuitions that turn out to be surprisingly compatible: (i) that neural networks are analogous to measuring devices (Balduzzi, 2012) and (ii) that training an RNN is analogous to writing code.", "startOffset": 141, "endOffset": 157}, {"referenceID": 28, "context": "It was pointed out in (Pascanu et al., 2014) that unfolding horizontal connections over time implies the concept of depth is not straightforward in classical RNNs.", "startOffset": 22, "endOffset": 44}, {"referenceID": 9, "context": "Indeed, neural Turing machines (Graves et al., 2014) are harder to train than more constrained architectures such as neural queues and deques (Grefenstette et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 10, "context": ", 2014) are harder to train than more constrained architectures such as neural queues and deques (Grefenstette et al., 2015).", "startOffset": 97, "endOffset": 124}], "year": 2017, "abstractText": "Recurrent neural networks are increasing popular models for sequential learning. Unfortunately, although the most effective RNN architectures are perhaps excessively complicated, extensive searches have not found simpler alternatives. This paper imports ideas from physics and functional programming into RNN design to provide guiding principles. From physics we introduce type constraints, analogous to the constraints that disqualify adding meters to seconds in physics. From functional programming, we require that strongly-typed architectures factorize into stateless learnware and statedependent firmware, thereby ameliorating the impact of side-effects. The features learned by strongly-typed nets have a simple semantic interpretation via dynamic average-pooling on onedimensional convolutions. We also show that strongly-typed gradients are better behaved than in classical architectures, and characterize the representational power of strongly-typed nets. Finally, experiments show that, despite being more constrained, strongly-typed architectures achieve lower training error and comparable generalization error to classical architectures.", "creator": "LaTeX with hyperref package"}}}