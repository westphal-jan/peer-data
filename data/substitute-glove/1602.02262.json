{"id": "1602.02262", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Recovery guarantee of weighted low-rank approximation via alternating minimization", "abstract": "Many applications allow recovering followed within `` due - rank matrix from noisy detail where the entries. In rules, this refers such frameworks as weightings normally - level analogous problem now problem using those - prismatic optimization parsing particular as alternating solves. Such organizations - chirality purposes have feel guarantees. Even wo, bovespa than - high-ranking approximation is NP - well on getting the most simple any it a the moment does close rank - s matrix.", "histories": [["v1", "Sat, 6 Feb 2016 14:55:12 GMT  (429kb)", "https://arxiv.org/abs/1602.02262v1", null], ["v2", "Thu, 8 Dec 2016 17:05:41 GMT  (600kb)", "http://arxiv.org/abs/1602.02262v2", "40 pages. Updated with the ICML 2016 camera ready version, together with an additional algorithm which needs less assumptions in Appendix C"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["yuanzhi li", "yingyu liang", "andrej risteski"], "accepted": true, "id": "1602.02262"}, "pdf": {"name": "1602.02262.pdf", "metadata": {"source": "CRF", "title": "Recovery guarantee of weighted low-rank approximation via alternating minimization", "authors": ["Yuanzhi Li", "Yingyu Liang", "Andrej Risteski"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n02 26\n2v 2\n[ cs\n.L G\n] 8\nD ec\n2 01\nThe key technical challenge is that under non-binary deterministic weights, na\u0131\u0308ve alternating steps will destroy the incoherence and spectral properties of the intermediate solutions, which are needed for making progress towards the ground truth. We show that the properties only need to hold in an average sense and can be achieved by the clipping step.\nWe further provide an alternating algorithm that uses a whitening step that keeps the properties via SDP and Rademacher rounding and thus requires weaker assumptions. This technique can potentially be applied in some other applications and is of independent interest."}, {"heading": "1 Introduction", "text": "Recovery of low-rank matrices has been a recurring theme in recent years in machine learning, signal processing, and numerical linear algebra, since in many applications, the data is a noisy observation of a low-rank ground truth matrix. Typically, the noise on different entries is not identically distributed, which naturally leads to a weighted low-rank approximation problem: given the noisy observation M, one tries to recover the ground truth by finding M\u0303 that minimizes \u2016M \u2212 M\u0303\u20162W = \u2211 ij Wi,j(Mi,j \u2212 M\u0303i,j)2 where the weight matrix W is chosen according to prior knowledge about the noise. For example, the co-occurrence matrix for words in natural language processing applications [Pennington et al., 2014, Arora et al., 2016] is such that the noise is larger when the co-occurrence of two words is rarer. When doing low-rank approximation on the co-occurrence matrix to get word embeddings, it has been observed empirically that a simple weighting can lead to much better performance than the unweighted formulation (see, e.g., [Levy and Goldberg, 2014]). In biology applications, it is often the case that the variance of the noise is different for each entry of a data matrix, due to various reasons such as different properties of different measuring devices. A natural approach to recover the ground truth matrix is to solve a weighted low-rank approximation problem where the weights are inversely proportional to the variance in the entries [Gadian, 1982, Wentzell et al., 1997]. Even for collaborative filtering, which is typically modeled as a matrix completion problem that assigns weight 1 on sampled entries and 0 on non-sampled entries, one can achieve better results when allowing non-binary weights [Srebro and Jaakkola, 2003].\n\u2217Department of Computer Science, Princeton University. Email:yuanzhil@cs.princeton.edu \u2020Department of Computer Science, Princeton University. Email: yingyul@cs.princeton.edu \u2021Department of Computer Science, Princeton University. Email:risteski@cs.princeton.edu\nIn practice, the weighted low-rank approximation is typically solved by non-convex optimization heuristics. One of the most frequently used is alternating minimization, which sets M\u0303 to be the product of two low-rank matrices and alternates between updating the two matrices. Although it is a natural heuristic to employ and also an interesting theoretical question to study, to the best of our knowledge there is no guarantee for alternating minimization for weighted low-rank approximation. Moreover, general weighted low-rank approximation is NP-hard, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].\nA special case of weighted low-rank approximation is matrix completion, where the weights are binary. Most methods proposed for solving this problem rely on the assumptions that the observed entries are sampled uniformly at random, and additionally often the observations need to be re-sampled across different iterations of the algorithm. This is inherently infeasible for the more general weighted low-rank approximation, and thus their analysis is not portable to the more general problem. The few exceptions that work with deterministic weights are [Heiman et al., 2014, Lee and Shraibman, 2013, Bhojanapalli and Jain, 2014]. In this line of work the state-of-the-art is [Bhojanapalli and Jain, 2014], who proved recovery guarantees under the assumptions that the ground truth has a strong version of incoherence and the weight matrix has a sufficiently large spectral gap. However, their results still only work for binary weights, use a nuclear norm convex relaxation and do not consider noise on the observed entries.\nIn this paper, we provide the first theoretical guarantee for weighted low-rank approximation via alternating minimization, under assumptions generalizing those in [Bhojanapalli and Jain, 2014]. In particular, assuming that the ground truth has a strong version of incoherence and the weight matrix has a sufficiently large spectral gap, we show that the spectral norm of the difference between the recovered matrix and the ground truth matrix is bounded by the spectral norm of the weighted noise plus an additive error term that decreases exponentially with the number of rounds of alternating minimization, from either initialization by SVD or, more importantly, random initialization. We emphasize that the bounds hold without any assumption on the noise, which is particularly important for handling complicated noise models. Since uniform sampling can satisfy our assumptions, our guarantee naturally generalizes those in previous works on matrix completion. See Section 4.1 for a detailed comparison.\nThe guarantee is proved by showing that the distance between the intermediate solution and the ground truth is improved at each iteration, which in spirit is similar to the framework in previous works. However, the lack of randomness in the weights and the exclusion of re-sampling (i.e., using independent samples at each iteration) lead to several technical obstacles that need to be addressed. Our proof of the improvement is then significantly different (and more general) from previous ones. In particular, showing improvement after each step is only possible when the intermediate solution has some additional special properties in terms of incoherence and spectrum. Prior works ensure such properties by using re-sampling (and sometimes assumptions about the noise), which are not available in our setting. We address this by showing that the spectral property only needs to hold in an average sense, which can be achieved by a simple clipping step. This results in a very simple algorithm that almost matches the practical heuristics, and thus provides explanation for them and also suggests potential improvement of the heuristics.\nFurther results The above results build on the insight that the spectral property only need to hold in an average sense. However, we can even make sure that the spectral property holds at each step strictly by a whitening step. More precisely, the clipping step is replaced by a whitening step using SDP and Rademacher rounding, which ensures that the intermediate solutions are incoherent and have the desired spectral property (the smallest eigenvalues of some related matrices are bounded). The technique of maintaining the smallest eigenvalues may be applicable to some other non-convex problems, and thus is of independent interest. The details are presented in Appendix C.\nFurthermore, combining our insight that the spectral property only need to hold in an average sense with the framework in [Sun and Luo, 2015], one can show provable guarantees for the family of algorithms analyzed there, including stochastic gradient descent. We will demonstrate this by including the proof details for stochastic gradient descent in a future version."}, {"heading": "2 Related work", "text": "Being a common practical problem (e.g., [Lu et al., 1997, Srebro and Jaakkola, 2003, Li et al., 2010, Eriksson and van den Hengel, 2012]), multiple heuristics for non-convex optimization such as alternating minimization have been\ndeveloped, but they come with no guarantees. On the other hand, weighted low-rank approximation is NP-hard in the worst case, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].\nOn the theoretical side, the only result we know of is [Razenshteyn et al., 2016], who provide a fixed-parameter tractability result when additionally the weight matrix is low-rank. Namely, when the weight matrix has rank r, they provide an algorithm for outputting a matrix M\u0303 which approximates the optimization objective up to a 1 + \u01eb multiplicative factor, and runs in time nO(k\n2r/\u01eb). A special case of weighted low rank approximation is matrix completion, where the goal is to recover a lowrank matrix from a subset of the matrix entries and corresponds to the case when the weights are in {0, 1}. For this special case much more is known theoretically. It is known that matrix completion is NP-hard in the case when the k = 3 [Peeters, 1996]. Assuming that the matrix is incoherent and the observed entries are chosen uniformly at random, Cande\u0300s and Recht [2009] showed that nuclear norm convex relation can recover an n\u00d7n rank-k matrix using m = O(n1.2k log(n)) entries. The sample size is improved to O(nk log(n)) in subsequent papers [Cande\u0300s and Tao, 2010, Recht, 2011, Gross, 2011]. Candes and Plan [2010] relaxed the assumption to tolerate noise and showed the nuclear norm convex relaxation can lead to a solution such that the Frobenius norm of the error matrix is bounded by O( \u221a n3/m) times that of the noise matrix. However, all these results are for the restricted case with uniformly random binary weight matrices. The only relaxations to random sampling to the best of our knowledge are in [Heiman et al., 2014, Lee and Shraibman, 2013, Bhojanapalli and Jain, 2014]. In this line the state-of-the-art is [Bhojanapalli and Jain, 2014], where the support of the observation is a d-regular expander such that the weight matrix has a sufficiently large spectral gap. However, it only works for binary weights, and is for a nuclear norm convex relaxation and does not incorporate noise.\nRecently, there is an increasing interest in analyzing non-convex optimization techniques for matrix completion. In two seminal papers [Jain et al., 2013, Hardt, 2014], it was shown that with an appropriate SVD-based initialization, the alternating minimization algorithm (with a few modifications) recovers the ground-truth. These results are for random binary weight matrix and crucially rely on re-sampling (i.e., using independent samples at each iteration), which is inherently not possible for the setting studied in this paper. More recently, Sun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion without re-sampling. However, the result is still for random binary weights and has not considered noise. More detailed comparison of our result with prior work can be found in Section 4, and comments on whether their arguments can be applied in our setting can be found in Section 5.\nWe also mention [Negahban and Wainwright, 2012] who consider random sampling, but one that is not uniformly random across the entries. In particular, their sampling produces a rank-1 matrix. (Additionally, they require the ground truth matrix to have nice properties such as low-rankness and spikiness.) The rank-1 assumption on the weight matrix is typically not true for many applications that introduce the weights to battle the different noise across the different entries of the matrix.\nFinally, two related works are [Bhojanapalli et al., 2015a,b]. The former implements faster SVD decomposition via weighted low rank approximation. However, here the weights in the weighted low rank problem come from leverage scores, so have a very specific structure, specially designed for performing SVD decompositions. The latter concerns optimization of strongly convex functions f(V) when V is in the set of positive-definite matrices. It does this in a non-convex manner, by setting V = UU\u22a4 and using the entries of U as variables. Our work focus on the recovery of the ground truth under the generative model, rather than on the optimization."}, {"heading": "3 Problem definition and assumptions", "text": "For a matrix A, let Ai denote its i-th column, Aj denote its j-th row, and Ai,j denote the element in i-th row and j-th column. Let \u2299 denote the Hadamard product, i.e., C = A\u2299B means Ci,j = Ai,jBi,j .\nLet M\u2217 \u2208 Rn\u00d7n be a rank-k matrix. Given the observation M = M\u2217 +N where N is a noise matrix, we want to recover the ground truth M\u2217 by solving the weighted low-rank approximation problem for M and a non-negative weight matrix W:\nmin M\u0303\u2208Rk\n\u2225\u2225\u2225M\u0303\u2212M \u2225\u2225\u2225 2\nW\nwhere Rk is the set of rank-k n by n matrices, and \u2016A\u20162W = \u2211 i,j Wi,jA 2 i,j is the weighted Frobenius norm. Our goal is to specify conditions about M\u2217 and W, under which M\u2217 can be recovered up to small error by alternating minimization, i.e., set M\u0303 = XY\u22a4 where X and Y are n by k matrices, and then alternate between updating the two matrices. Ideally, the recovery error should be bounded by \u2016W \u2299N\u20162, since this allows selecting weights according to the noise to make the error bound small.\nAs mentioned before, the problem is NP-hard in general, so we will need to impose some conditions. We summarize our assumptions as follows, and then discuss their necessity and the connections to existing ones.\n(A1) Ground truth is incoherent: M\u2217 has SVD U\u03a3V\u22a4, wheremaxni=1{||Ui||22, ||Vi||22} \u2264 \u00b5kn . Additionally, assume \u03c3max(\u03a3) = \u0398(1). (See discussion below.) Denote its condition number as \u03c4 = \u03c3max(\u03a3)/\u03c3min(\u03a3).\n(A2) Weight matrix has a spectral gap: ||W \u2212E||2 \u2264 \u03b3n, where \u03b3 < 1 and E is the all-one matrix.\n(A3) Weight is not degenerate: Let Di = Diag(Wi), i.e., Di is a diagonal matrix whose diagonal entries are the i-th row of W. Then there are 0 < \u03bb \u2264 1 \u2264 \u03bb:\n\u03bbI U\u22a4DiU \u03bbI, and \u03bbI V\u22a4DiV \u03bbI(\u2200i \u2208 [n]).\nThe incoherence assumption on the ground truth matrix is standard in the context of matrix completion. It is known that this is necessarily required for recovering the ground truth matrix. The assumption that \u03c3max(\u03a3) = \u0398(1) is without loss of generality: one can estimate \u03c3max(\u03a3) up to a constant factor, scale the data and apply our results. The full details are included in the appendix.\nThe spectrum assumption on the weight matrix is a natural generalization of the randomness assumption typically made in matrix completion scenario (e.g., [Candes and Plan, 2010, Jain et al., 2013, Hardt, 2014]). In that case, W is a matrix with d = \u2126(logn)-nonzeros in each row chosen uniformly at random, which corresponds to \u03b3 = O (\n1\u221a d\n)\nin (A2). Our assumption is also a generalization of the one in [Bhojanapalli and Jain, 2014], which requires W to be d-regular expander-like (i.e., to have a spectral gap) but is concerned only with matrix completion where the entries of W can be 0 or 1 only.\nThe final assumption (A3) is a generalization of the assumption A2 in [Bhojanapalli and Jain, 2014] that, intuitively, requires the singular vectors to satisfy RIP (restricted isometry property). This is because when the weights are binary, U\u22a4DiU = \u2211 j\u2208S(U\nk)(Uk)\u22a4 where S is the support of Wi, so after proper scaling the assumption is a strict weakening of theirs. They viewed it as a stronger version of incoherence, discussed the necessity and showed that it is implied by the strong incoherence property assumed in [Cande\u0300s and Tao, 2010]. In the context of more general weights, the necessity of (A3) is even more clear, as elaborated below.\nNote that since (A2) does not require W to be random or d-regular, it does not a-priori exclude the degenerate case that W has one all-zero column. In that case, clearly one cannot hope to recover the corresponding column of M\u2217. So, we need to make a third, non-degeneracy assumption about W, saying that it is \u201ccorrelated\u201d with M\u2217. The assumption is actually quite weak in the sense that when W is chosen uniformly at random, this assumption is true automatically: in those cases, E[Di] = I and thus E[U\u22a4DiU] = I since U is orthogonal. A standard matrix concentration bound can then show that our assumption (A3) holds with high probability. Therefore, it is only needed when considering a deterministic W. Intuitively, this means that the weights should cover the singular vectors of M\u2217. This prevents the aforementioned degenerate case when Wi = 0 for some i, and also some other degenerate cases. For example, consider the case when N = 0, all rows of M\u2217 are the same vector with first \u0398(logn) entries being zero and the rest being one, and in one row of M\u2217 the non-zeros entries all have zero weight. In this case, there is also no hope to recover M\u2217, which should be excluded by our assumption."}, {"heading": "4 Algorithm and results", "text": "We prove guarantees for the vanilla alternating minimization with a simple clipping step, from either SVD initialization or random initialization. The algorithm is specified in Algorithm 1. Overall, it follows the usual alternating\nAlgorithm 1 Main Algorithm (ALT) Input: Noisy observation M, weight matrix W, number of iterations T\n1: Initialize Y1 using either Y1 = SVDINITIAL(M,W) or Y1 = RANDINITIAL 2: for t = 1, 2, ..., T do 3: X\u0303t+1 = argminX\u2208Rn\u00d7k \u2225\u2225M\u2212XY\u22a4t \u2225\u2225 W 4: Xt+1 = CLIP(X\u0303t+1) 5: Xt+1 = QR(Xt+1) 6: Y\u0303t+1 = argminY\u2208Rn\u00d7k \u2225\u2225M\u2212Xt+1Y\u22a4 \u2225\u2225 W 7: Yt+1 = CLIP(Y\u0303t+1) 8: Yt+1 = QR(Yt+1) 9: end for\nOutput: M\u0303 = XT+1YT\nAlgorithm 2 Clipping (CLIP)\nInput: matrix X\u0303 Output: matrix X with\nX i = { X\u0303i if \u2016X\u0303i\u201622 \u2264 \u03be := 2\u00b5kn 0 otherwise.\nAlgorithm 3 SVD Initialization (SVDINITIAL) Input: observation M, weight W\n1: (X\u0303,\u03a3, Y\u0303) = rank-k SVD(W \u2299M), i.e., the columns of Y\u0303 are the top k right singular vectors of W \u2299M 2: Y = CLIP(Y\u0303), Y = QR(Y)\nOutput: Y\nAlgorithm 4 Random Initialization (RANDINITIAL)\n1: Let Y \u2208 Rn\u00d7k generated as Yi,j = bi,j 1\u221an , where bi,j\u2019s are independent uniform from {\u22121, 1} Output: Y\nminimization framework: it keeps two working matrices X and Y, and alternates between updating them. In an X update step, it first updates X to be the minimizer of the weighted low rank objective while fixing Y, which can be done efficiently since now the optimization is convex. Then it performs a \u201cclipping\u201d step which zeros out rows of the matrix with too large norm,1 and then make it orthogonal by QR-factorization.2 At the end, the algorithm computes a final solution M\u0303 from the two iterates.\nThe two iterates can be initialized by performing SVD on the weighted observation (Algorithm 3), which is a weighted version of SVD initialization typically used in matrix completion. Moreover, we show that the algorithm works with random initialization (Algorithm 4), which is a simple and widely used heuristic in practice but rarely understood well.\nWe are now ready to state our main results. Theorem 1 describes our guarantee for the algorithm with SVD initialization, and Theorem 3 is for random initialization.\n1The clipping step zeros out rows with square \u21132 norm twice larger than the upper bound \u00b5k/n imposed by our incoherence assumption (A1). One can choose the threshold to be c\u00b5k/n where c \u2265 2 is a constant and can choose to shrink the row to have norm no greater than \u00b5k/n, and our analysis still holds. The current choices are only for ease of presentation.\n2The QR-factorization step is not necessary for our analysis. But since it is widely used in practice for numerical stability, we prefer to analyze the algorithm with QR.\nTheorem 1 (Main, SVD initialization). Suppose M\u2217,W satisfy assumptions (A1)-(A3) with\n\u03b3 = O ( min {\u221a n\nD1\n\u03bb\n\u03c4\u00b53/2k2 ,\n\u03bb\n\u03c43/2\u00b5k2\n}) ,\nwhere D1 = maxi\u2208[n] \u2016Wi\u20161. Then after O(log(1/\u01eb)) rounds of Algorithm 1 with initialization from Algorithm 3 outputs a matrix M\u0303 that satisfies\n\u2016M\u0303\u2212M\u2217\u20162 \u2264 O ( k\u03c4\n\u03bb\n) \u2016W \u2299N\u20162 + \u01eb."}, {"heading": "The running time is polynomial in n and log(1/\u01eb).", "text": "The theorem is stated in its full generality. To emphasize the dependence on the matrix size n, the rank k and the incoherence \u00b5, we can consider a specific range of parameter values where the other parameters (the spectral bounds, condition number, D1/n) are constants. Also, these parameter values are typical in matrix completion, which facilitates our comparison in the next subsection.\nCorollary 2. Suppose \u03bb, \u03bb and \u03c4 are all constants, D1 = \u0398(n), and T = O(log(1/\u01eb)). Furthermore,\n\u03b3 = O\n( 1\n\u00b53/2k2\n) .\nThen Algorithm 1 with initialization from Algorithm 3 outputs a matrix M\u0303 that satisfies\n\u2016M\u0303\u2212M\u2217\u20162 \u2264 O (k) \u2016W \u2299N\u20162 + \u01eb.\nRemarks The theorem bounds the spectral norm of the error matrix by the spectral norm of the weighted noise plus an additive error term that decreases exponentially with the number of rounds of alternating minimization. We emphasize that our guarantee holds for any M\u2217,W satisfying our deterministic assumptions; the high success probability is with respect to the execution of the algorithm, not to the input. This ensures the freedom in choosing the weights to battle the noise. We also emphasize that the bounds hold without any assumption on the noise, which is particularly important here since weighted low rank is typically applied to complicated noise models.\nBounding the error by \u2016W \u2299 N\u20162 is particularly useful when the noise is not uniform across the entries: prior knowledge about the noise (e.g., the different variances of noise on different entries) can be taken into account by setting up a reasonable weight matrix3, such that \u2016W \u2299 N\u20162 can be significantly smaller than \u2016N\u20162. Also, in recovering the ground truth, a spectral norm bound is more preferred than a Frobenius norm bound, since typically the Frobenius norm is \u221a n larger than the spectral norm.\nFurthermore, when \u2016W \u2299 N\u20162 = 0 (as in matrix completion without noise), the ground truth is recovered in a geometric rate.\nFinally, in matrix completion with uniform random sampled observations, the term D1 concentrates around n, so D1 n disappears in this case.\nTheorem 3 (Main, random initialization). Suppose M\u2217,W satisfy assumptions (A1)-(A3) with\n\u03b3 = O ( min {\u221a n\nD1\n\u03bb\n\u03c4\u00b52k5/2 ,\n\u03bb\n\u03c43/2\u00b53/2k5/2\n}) ,\n\u2016W\u2016\u221e = O (\n\u03bbn\nk2\u00b5 log2 n\n) ,\n3Note that W cannot be made arbitrarily small since it should satisfy our assumptions. Roughly speaking, W has spectral norm n and is flexible to take into account the prior knowledge about the noise. In particular, it can be set to the all one matrix, reducing to the unweighted case.\nwhere D1 = maxi\u2208[n] \u2016Wi\u20161. Then after O(log(1/\u01eb)) rounds Algorithm 1 with initialization from Algorithm 3 outputs a matrix M\u0303 that with probability at least 1\u2212 1n2 satisfies\n\u2016M\u0303\u2212M\u2217\u20162 \u2264 O ( k\u03c4\n\u03bb\n) \u2016W \u2299N\u20162 + \u01eb."}, {"heading": "The running time is polynomial in n and log(1/\u01eb).", "text": "Remarks Compared to SVD initialization, we need slightly stronger assumptions for random initialization to work. There is an extra 1/(\u00b51/2k1/2) in the requirement of the spectral parameter \u03b3. We note that the same error bound is obtained when using random initialization. Roughly speaking, this is because our analysis shows that the updates can make improvement under rather weak requirements that random initialization can satisfy, and after the first step the rest updates make the same progress as in the case using SVD initialization."}, {"heading": "4.1 Comparison with prior work", "text": "For the sake of completeness, we will give a more detailed comparison with representative prior work on matrix completion from Section 2, emphasizing the dependence on n, k and \u00b5 and regarding the other parameters as constants. We first note that when the m observed entries are sampled uniformly at random from an n by n matrix, the corresponding binary weight matrix will have a spectral gap \u03b3 = O( \u221a n m ) (see, e.g., [Feige and Ofek, 2005]). Converting the sample bounds in the prior work to the spectral gap, we see that in general our result has worse dependence on parameters like the rank than those by convex relaxations, but has slightly better dependence than those by alternating minimization. The comparison is summarized in Table 1.\nThe seminal paper [Cande\u0300s and Recht, 2009] showed that a nuclear norm convex relaxation approach can recover the ground truth matrix usingm = O(n1.2k log2 n) entries chosen uniformly at random and without noise. The sample size was improved to O(nk log6 n) in [Cande\u0300s and Tao, 2010] and then O(nk logn) in subsequent papers. Candes and Plan [2010] generalized the result to the case with noise: the same convex program using m = O(nk log6 n) entries recovers a matrix M\u0303 s.t. \u2016M\u0303\u2212M\u2217\u2016F \u2264 (2 + 4 \u221a (2 + p)n/p)\u2016N\u2126\u2016F where p = m/n2 and N\u2126 is the noise projected on the observed entries.\nKeshavan et al. [2009] showed that withm = O(n\u00b5k logn), one can recover a matrix M\u0303 such that \u2225\u2225\u2225M\u2217 \u2212 M\u0303 \u2225\u2225\u2225 F =\nO ( n2 \u221a k m \u2016N\u2126\u20162 ) by an optimization over a Grassmanian manifold.\nBhojanapalli and Jain [2014] relaxed the assumption that the entries are randomly sampled. They showed that the nuclear norm relaxation recovers the ground truth, assuming that the support \u2126 of the observed matrix forms a\nd-regular expander graph (or alike), i.e., |\u2126| = dn, \u03c31(\u2126) = d and \u03c32(\u2126) \u2264 c \u221a d and d \u2265 c2\u00b52k2. This would correspond to a parameter \u03b3 = O( 1\u00b5k ) for us. They did not consider the robustness to noise. Hardt [2014] showed that with an appropriate initialization alternating minimization recovers the ground truth approximately. Precisely, they assumed N satisfies: (1). \u00b5(N) . \u03c3min(M\u2217)2;(2). \u2016N\u2016\u221e \u2264 \u00b5n\u2016M\u2217\u2016F . Then, he shows that log(n\u01eb logn) alternating minimization steps recover a matrix M\u0303 such that \u2016M\u0303\u2212M\u2217\u2016F \u2264 \u01eb\u2016M\u2016F provided that pn \u2265 k(k + log(n/\u01eb))\u00b5 \u00d7 ( \u2016M\u2217\u2016F+\u2016N\u2016F /\u01eb\n\u03c3k\n)2 ( 1\u2212 \u03c3k+1\u03c3k )5 where \u03c3k is the k-th singular value of the ground-\ntruth matrix. The parameter \u03b3 corresponding to the case considered there would be roughly O( 1 k \u221a \u00b5 logn ). While their algorithm has a good tolerance to noise, N is assumed to have special structure for him that we do not assume in our setting.\nSun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion. They showed that by using m = O(nkmax{\u00b5 logn, \u00b52k6}) randomly sampled entries without noise, the ground truth can be recovered in a geometric rate. This corresponds to a spectral gap ofO ( 1\nmax{ \u221a k\u00b5 log n,\u00b5k3.5}\n) .\nOur result is more general and also handles noise. When specialized to their setting, we also have a geometric rate with a slightly better dependence on the rank k but a slightly worse dependence on the incoherence \u00b5."}, {"heading": "5 Proof sketch", "text": "Before going into our analysis, we first discuss whether arguments in prior work can be applied. Most of the work on matrix completion uses convex optimization and thus their analysis is not applicable in our setting. There indeed exists some other work that analyzes non-convex optimization for matrix completion, and it is tempting to adopt their arguments. However, there exist fundamental difficulties in porting their arguments. All of them crucially rely on the randomness in sampling the observed entries. Keshavan et al. [2009] analyzed optimization over a Grassmanian manifold, which uses the fact that E[W \u2299 S] = S for any matrix S. In [Jain et al., 2013, Hardt, 2014], re-sampling of new observed entries in different iterations was used to get around the dependency of the iterates on the sample set, a common difficulty in analyzing alternating minimization. The subtlety and the drawback of re-sampling were discussed in detail in [Bhojanapalli and Jain, 2014, Candes et al., 2015, Sun and Luo, 2015]. We note that [Sun and Luo, 2015] only needs sampling before the algorithm starts and does not need re-sampling in different iterations, but still relies on the randomness in the sampled entries. In particular, in all the aforementioned work, the randomness guarantees that the iterates X,Y stay incoherent and have good spectrum properties. Given these, alternating minimization can make progress towards the ground truth in each iteration. Nevertheless, since we focus on deterministic weights, such randomness is inherently infeasible in our setting. In this case, after just one iteration, it is unclear if the iterates can have incoherence and good spectrum properties required to progress towards the ground truth, even under our current assumptions. The whole algorithm thus breaks down. To address this, we show that it is sufficient to ensure the spectral property in an average sense and then introduce our clipping step to achieve that, arriving at our current algorithm.\nHere for simplicity, we drop the subscription t in all iterates, and we only focus on important factors, dropping other factors and the big-O notation. We only consider the case when W \u2299 N = 0, so as to emphasize the main technical challenges.\nOn a high level, our analysis of the algorithm maintains potential functions distc(X,U) and distc(Y,V) between our working matrices X,Y and the ground truth U,V (recall that M\u2217 = U\u03a3V\u22a4):\ndistc(X,U) = min Q\u2208Ok\u00d7k\n\u2016XQ\u2212U\u20162\nand distc(Y,V) = min\nQ\u2208Ok\u00d7k \u2016YQ\u2212V\u20162,\nwhere Ok\u00d7k are the set of k \u00d7 k rotation matrices. The key is to show that they decrease after each update step, so X and Y get closer to the ground truth.4 The strategy of maintaining certain potential function measuring the distance\n4Note that we also need a good initialization, which can be done by SVD. Since our analysis requires rather weak warm start, we are able to show that simple random initialization is also sufficient (at the cost of slightly worse bounds).\nbetween the iterates and the ground truth is also used in prior work [Bhojanapalli and Jain, 2014, Candes et al., 2015, Sun and Luo, 2015]. We will point out below the key technical difficulties that are not encountered in prior work and make our analysis substantially different. The complete proofs are provided in the appendix due to space limitation."}, {"heading": "5.1 Update", "text": "We would like to show that after an X update, the new matrix X\u0303 satisfies distc(X,U) \u2264 distc(Y,V)/2+ c for some small c (similarly for a Y update).\nConsider the update step X\u0303 \u2190 argminA\u2208Rn\u00d7k \u2225\u2225M\u2212AY\u22a4 \u2225\u2225 W .\nBy setting the gradient to 0 and with some algebraic manipulation, we have X\u0303\u2212U\u03a3V\u22a4Y = G where\nGi := Ui\u03a3V\u22a4Y\u22a5Y \u22a4 \u22a5DiY(Y \u22a4DiY) \u22121.\nwhere Di = Diag(Wi). Since X\u0303 is the value prior to performing QR decomposition, we want to show that X\u0303 is close to Ui\u03a3V\u22a4Y, i.e., the error term G on right hand side is small. In the ideal case when the error term is 0, then X\u0303 = U\u03a3V\u22a4Y and thus distc(X,U) = 0, meaning that with one update X\u0303 already hits into the correct subspace. So we would like to show that it is small so that the iterate still makes progress. Let\nPi = V \u22a4Y\u22a5Y \u22a4 \u22a5DiY and Oi = (Y \u22a4DiY) \u22121,\nso that Gi = Ui\u03a3PiOi. Now the two challenges are to bound Pi and Oi. Let us first consider the simpler case of matrix completion, where the entries of the matrix are randomly sampled by probability p. Then Di is a random diagonal matrix with E[Di] = I and E[D2i ] = 1 pI. Furthermore, for n \u00d7 k orthogonal matrices Y, Oi = (Y\u22a4DiY)\u22121 concentrates around I. Then in expectation, ||Pi|| is about ||V\u22a4Y\u22a5||/\u221ap and \u2016Oi\u2016 is about 1, so \u2016Gi\u2016 is as small as \u00b5k||V\u22a4Y\u22a5||/(\u221apn) = \u00b5ksin \u03b8(V,Y)/(\u221apn). High probability can then be established by the trick of re-sampling.\nHowever, in our setting, we have to deal with two major technical obstacles due to deterministic weights.\n1. There is no expectation for Di. Since \u2016Di\u20162\u221e can be as large as n 2 poly(logn) , \u2016Pi\u2016 can potentially be as large as sin \u03b8(Y,V) npoly(logn) , which is almost a factor n larger than the bound for random Di. This is clearly insufficient to show the progress.\n2. A priori the norm of Oi = (Y\u22a4DiY)\u22121 may be large. Especially, in the algorithmY is given by the alternating minimization steps and giving an upper bound on \u2016Oi\u2016 at all steps seems hard.\nThe first issue For this, we exploit the incoherence of Y and the spectral property of the weight matrix. If Di is the identity matrix, then Pi = 0 which, intuitively, means that there are cancellations between negative part and positive parts. When W is expander-like, it will put roughly equal weights on the negative part and the positive part. If furthermore we have that Y is incoherent (i.e., the negative and positive parts are spread out), then W can mix the terms and lead to a cancellation similar to that when Di = I. More precisely, consider the (j, j\u2032)-th element in Pi. Define a new vector x \u2208 Rn such that\nxi = (V\u0303j)i(Yj\u2032 )i, where V\u0303 = V\u22a4Y\u22a5Y\u22a4\u22a5 .\nThen we have the cancellation in the form of \u2211 i xi = 0. When Di = I, we simply get (Pi)j,j\u2032 = \u2211\ni xi = 0. When Di 6= I, we have (Pi)j,j\u2032 = \u2211 s\u2208[n](Di)sx j,j\u2032 s . Now mix over all i, we have\n\u2211 i\u2208[n] ((Pi)j,j\u2032) 2 =\n \u2211\ns\u2208[n] (Di)sxs\n  2\n= \u2016Wx\u20162\n= \u2016(W \u2212E)x\u20162 (since Ex = 0) \u2264 \u03b32n2\u2016x\u20162\nwhere in the last step we use the expander-like property of W (Assumption (A2)) to gain the cancellation. Furthermore, if \u2016Yj\u2032\u2016\u221e is small, by definition \u2016x\u20162 is also small, so we can get an upper bound on \u2211 i\u2208[n] \u2016Pi\u20162F .\nThen the problem reduces to maintaining the incoherence of Y. This is taken care of by our clipping step (Algorithm 2), which sets to 0 the rows of Y that are too large. Of course, we have to show that this will not increase the distance of the clipped Y and V. The intuition is that we clip only when \u2016Yi\u2016 \u2265 2\u00b5k/n. But \u2016Vi\u2016 \u2264 \u00b5k/n, so after clipping, Yi only gets closer to Vi.\nThe second issue This is the more difficult technical obstacle, i.e., \u2016Oi\u2016 = \u2016(Y\u22a4DiY)\u22121\u2016 can be large. Our key idea is that although individual \u2016Oi\u2016 can indeed be large, this cannot be the case on average. We show that there can just be a few i\u2019s such that \u2016Oi\u2016 is large, and they will not contribute much to \u2016G\u2016, so the update can make progress.\nTo be more formal, we wish to bound the number of indices i such that \u03c3min ( Y\u22a4DiY ) \u2264 \u03bb4 . Consider an\narbitrary unit vector a. Then,\naY\u22a4DiYa = \u2211\nj\naY\u22a4(Di)jYa = \u2211\nj\n(Di)j\u3008a,Yj\u30092.\nWe know that Y is close to V, so we rewrite the above using some algebraic manipulation as\n\u2211\nj\n(Di)j\u3008a, (Yj \u2212Vj) +Vj\u30092\n\u2265 1 4\n\u2211\nj\n(Di)j\u3008a,Vj\u30092 \u2212 1\n3\n\u2211\nj\n(Di)j\u3008a,Yj \u2212Vj\u30092\nFor j\u2019s such that Yj is close to Vj (denote these j\u2019s as Sg), then the terms can be easily bounded since V\u22a4DiV \u2265 \u03bbI by assumption. So we only need to consider j\u2019s such that Yj is far from Vj . Since we have incoherence, we know that \u2016Yj \u2212Vj\u2016 is still bounded in the order of \u00b5k/n. So aY\u22a4DiYa can be small only when \u2211 j 6\u2208Sg (Di)j is large.\nLet S denote those bad i\u2019s. Let uS be the indicator vector for S and ug be the indicator vector for [n\u2212 Sg]. \u2211\ni\u2208S\n\u2211 j 6\u2208Sg (Di)j = u \u22a4 SWug\n\u2264 |S|(n \u2212 |Sg|) + \u03b3n \u221a |S|(n\u2212 |Sg|)\nwhere the last step is due to the spectral property of W. Therefore, there can be only a few i\u2019s with large \u2211\nj 6\u2208Sg (Di)j ."}, {"heading": "5.2 Proofs of main results", "text": "We only need to show that we can get an initialization close enough to the ground truth so that we can apply the above analysis for the update. For SVD initialization,\n[X,\u03a3,Y] = rank-k SVD(W \u2299M\u2217 +W \u2299N).\nSince ||W \u2299 N||2 \u2264 \u03b4 can be regarded as small, the idea is to show that W \u2299M\u2217 is close to M\u2217 in spectral norm and then apply Wedin\u2019s theorem [Wedin, 1972]. We show this by the spectral gap property of W and the incoherence property of U,V.\nFor random initialization, the proof is only a slight modification of that for SVD initialization, because the update requires rather mild conditions on the initialization such that even the random initialization is sufficient (with slightly worse parameters)."}, {"heading": "6 Conclusion", "text": "In this paper we presented the first recovery guarantee of weighted low-rank matrix approximation via alternating minimization. Our work generalized prior work on matrix completion, and revealed technical obstacles in analyzing alternating minimization, i.e., the incoherence and spectral properties of the intermediate iterates need to be preserved. We addressed the obstacles by a simple clipping step, which resulted in a very simple algorithm that almost matches the practical heuristics."}, {"heading": "Acknowledgements", "text": "This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329."}, {"heading": "A Preliminaries about subspace distance", "text": "Before delving into the proofs, we will prove a few simple preliminaries about subspace angles/distances.\nDefinition (Distance, Principle angle). Denote the principle angle of Y, V \u2208 Rn\u00d7k as \u03b8(Y,V). Then for orthogonal matrix Y (i.e., Y\u22a4Y = I),\ntan \u03b8(Y,V) = \u2016Y\u22a4\u22a5V(Y\u22a4V)\u22121\u20162.\nFor orthogonal matrices Y, V,\ncos \u03b8(Y,V) = \u03c3min(Y \u22a4V), sin \u03b8(Y,V) = \u2016(I\u2212YY\u22a4)V\u20162 = \u2016Y\u22a5Y\u22a4\u22a5V\u20162 = \u2016Y\u22a4\u22a5V\u20162, distc(Y,V) = min\nQ\u2208Ok\u00d7k \u2016YQ\u2212V\u20162\nwhere Ok\u00d7k is the set of k \u00d7 k orthogonal matrices.\nLemma 4 (Equivalence of distance). Let Y, V \u2208 Rn\u00d7k be two orthogonal matrices, then we have:\nsin \u03b8(Y,V) \u2264 distc(Y,V) \u2264 sin \u03b8(Y,V) + 1\u2212 cos \u03b8(Y,V) cos \u03b8(Y,V) \u2264 2tan \u03b8(Y,V).\nProof of Lemma 4. Suppose Q\u2217 = argminQ\u2208Ok\u00d7k\u2016YQ\u2212V\u20162. Let\u2019s write V = YQ\u2217 +R, then distc(Y,V) = \u2016R\u20162. We have\nsin \u03b8(Y,V) = \u2016(I\u2212YY\u22a4)V\u20162 = \u2016Y\u22a5Y\u22a4\u22a5R\u20162 \u2264 \u2016R\u20162\nOn the other hand, suppose ADB\u22a4 = SVD(Y\u22a4V), we know that \u03c3min(D) = \u03c3min(Y\u22a4V) = cos \u03b8(Y,V). Therefore, by A = Y\u22a4VBD\u22121, AB\u22a4 \u2208 Ok\u00d7k we have:\ndistc(Y,V) \u2264 \u2016YAB\u22a4 \u2212V\u20162 = \u2016YY\u22a4VBD\u22121B\u22a4 \u2212V\u20162 \u2264 \u2016YY\u22a4VBD\u22121B\u22a4 \u2212YY\u22a4V\u20162 + \u2016YY\u22a4V \u2212V\u20162 \u2264 \u2016BD\u22121B\u22a4 \u2212 I\u20162 + sin \u03b8(Y,V) = \u2016D\u22121 \u2212 I\u20162 + sin \u03b8(Y,V)\n= sin \u03b8(Y,V) + 1\u2212 cos \u03b8(Y,V) cos \u03b8(Y,V) .\nFinally, sin \u03b8(Y,V) \u2264 tan \u03b8(Y,V) and 1\u2212cos \u03b8(Y,V)cos \u03b8(Y,V) \u2264 tan \u03b8(Y,V) can be verified by definition, so the last inequality follows.\nFor convenience in our proofs we will also use the following generalization of incoherence:\nDefinition (Generalized incoherence). For a matrix A \u2208 Rn\u00d7k, the generalized incoherence \u03c1(A) is defined as:\n\u03c1(A) = max i\u2208[n] {n k \u2016Ai\u201622 }\nWe call it generalized incoherence for obvious reasons: when A is an orthogonal matrix, then \u03c1(A) = \u00b5(A)."}, {"heading": "B Proofs for alternating minimization with clipping", "text": "We will show in this section the results for our algorithm based on alternating minimization with a clipping step. The organization is as follows. In Section B.1 we will present the necessary lemmas for the initialization, in Section B.3 we show the decrease of the potential function after one update step, and in Section B.4 we will put everything together, and prove our main theorem.\nBefore starting with the proofs, we will make a remark which will simplify the exposition. Without loss of generality, we may assume that\n\u03b4 = \u2016W \u2299N\u20162 \u2264 \u03bb\u03c3min(M\n\u2217)\n200k (B.1)\nOtherwise, we can output the 0 matrix, and the guarantee of all our theorems would be satisfied vacuously."}, {"heading": "B.1 SVD-based initialization", "text": "We want to show that after initialization, the matrices X,Y are close to the ground truth matrix U,V. Observe that [X,\u03a3,Y] = SVD(W \u2299 M) = SVD(W \u2299 (M\u2217 + N)) = SVD(W \u2299 M\u2217 + W \u2299 N). By our assumptions we know that ||W \u2299N||2 \u2264 \u03b4 which we are thinking of as small, so the idea is to show that W \u2299M\u2217 is close to M\u2217 in spectral norm, then by Wedin\u2019s theorem [Wedin, 1972] we will have X,Y are close to U,V. We show that W\u2299M\u2217 is close to M\u2217 by the spectral gap property of W and the incoherence property of U,V.\nLemma 5 (Spectral lemma). Let W be an (entry wise non-negative) matrix in Rn\u00d7n with a spectral gap, i.e. W = E+ \u03b3nJ\u03a3WK\n\u22a4, where J,K are n\u00d7 n (column) orthogonal matrices, with ||\u03a3W||2 = 1, \u03b3 < 1. Furthermore, for every matrix H \u2208 Rn\u00d7n such that H = A\u03a3B\u22a4 (A,B not necessarily orthogonal, \u03a3 \u2208 Rk\u00d7k is diagonal) we have\n\u2016(W \u2212E)\u2299H\u20162 \u2264 \u03b3k\u03c3max(\u03a3) \u221a \u03c1(A)\u03c1(B)\nwhere E is the all one matrix.\nProof of Lemma 5. We know that for any unit vectors x, y \u2208 Rn,\nx\u22a4 ((W \u2212E)\u2299H) y = k\u2211\nr=1\n\u03c3rx T ( (W \u2212E)\u2299ArB\u22a4r ) y\n= \u03b3n\nk\u2211\nr=1\n\u03c3r(Ar \u2299 x)\u22a4J\u03a3WK\u22a4(Br \u2299 y)\n\u2264 \u03b3n k\u2211\nr=1\n\u03c3r||Ar \u2299 x||2||J\u03a3WK\u22a4||2||Br \u2299 y||2\n\u2264 \u03b3n k\u2211\nr=1\n\u03c3r||Ar \u2299 x||2||Br \u2299 y||2\n\u2264 \u03b3n\u03c3max(\u03a3)\n\u221a\u221a\u221a\u221a k\u2211\nr=1\n||Ar \u2299 x||22\n\u221a\u221a\u221a\u221a k\u2211\nr=1\n||Br \u2299 y||22\n\u2264 \u03b3n\u03c3max(\u03a3)\n\u221a\u221a\u221a\u221a n\u2211\ni=1\nx2i ||Ai||22\n\u221a\u221a\u221a\u221a n\u2211\ni=1\ny2i ||Bi||22\n\u2264 \u03b3n\u03c3max(\u03a3) \u221a\u221a\u221a\u221ak n \u03c1(A) ( n\u2211\ni=1\nx2i )\u221a\u221a\u221a\u221ak n \u03c1(B) ( n\u2211\ni=1\ny2i\n)\n\u2264 \u03b3\u03c3max(\u03a3)k \u221a \u03c1(A)\u03c1(B).\nThe lemma follows from the definition of the operator norm.\nThe spectral lemma can be used to prove the initialization condition, when combined with Wedin\u2019s theorem.\nLemma 6 (Wedin\u2019s Theorem [Wedin, 1972]). Let M\u2217, M\u0303 be two matrices whose singular values are \u03c31, ..., \u03c3n and \u03c3\u03031, ..., \u03c3\u0303n, let U,V and X,Y be the first k singular vectors (left and right) of M\u2217, M\u0303 respectively. If \u2203\u03b1 > 0 such that maxnr=k+1 \u03c3\u0303r \u2264 minki=1 \u03c3i \u2212 \u03b1, then\nmax {sin \u03b8(U,X), sin \u03b8(V,Y)} \u2264 ||M\u0303\u2212M \u2217||2\n\u03b1 .\nLemma 7. Suppose M\u2217,W satisfy all the assumptions, then for (X,\u03a3,Y) = rank-k SVD(W \u2299M), we have\nmax{tan \u03b8(X,U), tan \u03b8(Y,V)} \u2264 4(\u03b3\u00b5k + \u03b4) \u03c3min(M\u2217)\nProof of Lemma 7. We know that\n\u2016W \u2299M\u2212M\u2217\u20162 \u2264 ||W \u2299M\u2217 \u2212M\u2217||2 + ||W \u2299N||2 \u2264 \u03b3\u00b5k\u03c3max(M\u2217) + \u03b4.\nTherefore, by Weyl\u2019s theorem,\nmax{\u03c3r(W \u2299M) : k + 1 \u2264 r \u2264 n} \u2264 \u03b3\u00b5k + \u03b4 \u2264 1\n2 \u03c3min(M\n\u2217).\nwhere the last inequality holds because of B.1 and the assumption on \u03b3 in the theorem statement. Now, by Wedin\u2019s theorem with \u03b1 = 12\u03c3min(M \u2217), for (X,\u03a3,Y) = rank-k SVD(W \u2299M),\nmax {sin \u03b8(U,X), sin \u03b8(V,Y)} \u2264 2(\u03b3\u00b5k + \u03b4) \u03c3min(M\u2217)\nSince \u03b3 and \u03b4 are small enough, so sin\u03b8 \u2264 1/2. In this case, we have tan\u03b8 \u2264 2sin\u03b8, then the lemma follows.\nFinally, this gives us the following guarantee on the initialization:\nLemma 8 (SVD initialization). Suppose M\u2217,W satisfy all the assumptions.\ndistc(V,Y1) \u2264 8k\u22061, \u03c1(Y1) \u2264 2\u00b5\n1\u2212 k\u22061\nwhere \u22061 = 8(\u03b3\u00b5k+\u03b4) \u03c3min(M\u2217) .\nProof of Lemma 8. First, consider Y\u03031. By Lemma 7 and 4, we get that\ndistc(Y\u03031,V) \u2264 \u22061\nwhich means that \u2203Q \u2208 Ok\u00d7k, s.t. \u2016Y\u03031Q\u2212V\u20162 \u2264 \u22061\nhence\n\u2016Y\u03031Q\u2212V\u2016F \u2264 k\u22061 \u2264 1\n4\nwhere the last inequality follows since \u03b3 and \u03b4 are small enough.\nNext, consider Y1. In the clipping step, if \u2016Y\u0303i1\u2016 \u2265 \u03be = 2\u00b5kn , then \u2016Y\u0303i1 \u2212Vi\u2016 \u2265 \u00b5k n , and \u2016Y\ni\n1 \u2212Vi\u2016 = \u2016Vi\u2016 = \u00b5k n . Otherwise, Y i = Y\u0303i. So\n\u2016Y1Q\u2212V\u2016F \u2264 \u2016Y\u03031Q\u2212V\u2016F \u2264 1\n4 .\nFinally, we can argue that Y1 is close to V. Let\u2019s assume that Y1 = Y1R\u22121, for an upper-triangular R.\nsin \u03b8(V,Y1) = \u2016V\u22a4\u22a5Y1\u20162 = \u2016V\u22a4\u22a5(Y1 \u2212VQ\u22121)R\u22121\u20162 \u2264 \u2016Y1Q\u2212V\u20162\u2016R\u22121\u20162 \u2264 1\n\u03c3min(Y1) \u2016Y1Q\u2212V\u2016F\nwhere the second inequality follows because the singular values of R and Y1 are the same. Note that\n\u03c3min(Y1) \u2265 \u03c3min(V)\u2212 \u2016Y1 \u2212V\u2016F \u2265 \u03c3min(V)\u2212 k\u22061 = 1\u2212 k\u22061 \u2265 1\n2\nSo\nsin \u03b8(V,Y1) \u2264 2\u2016Y1Q\u2212V\u2016F \u2264 1\n2 .\nIn this case, we have tan \u03b8(V,Y1) \u2264 2sin \u03b8(V,Y1) and thus\ndistc(V,Y1) \u2264 2tan \u03b8(V,Y1) \u2264 4sin \u03b8(V,Y1) \u2264 8\u2016Y1Q\u2212V\u20162 \u2264 8\u2016Y1Q\u2212V\u2016F \u2264 8k\u22061.\nFor \u03c1(Y1), observe that Yi1 = Y i R\u22121, so\n\u2016Yi1\u2016 \u2264 \u2016Y i 1\u2016\u2016R\u22121\u20162 \u2264 \u03be \u03c3min(Y1) \u2264 \u03be 1\u2212 k\u22061\nwhich leads to the bound."}, {"heading": "B.2 Random initialization", "text": "With respect to the random initialization, the lemma we will need is the following one:\nLemma 9 (Random initialization). Let Y be a random matrix in Rn\u00d7k generated as Yi,j = bi,j 1\u221an , where bi,j are independent, uniform {\u22121, 1} variables. Furthermore, let \u2016W\u2016\u221e \u2264 \u03bbnk2\u00b5 log2 n . Then, with probability at least 1\u2212 1n2 over the draw of Y,\n\u2200i, \u03c3min ( Y\u22a4DiY ) \u2265 1\n4\n\u03bb\nk\u00b5 .\nProof of Lemma 9. Notice that Y\u22a4DiY = \u2211 j(Y j)\u22a4(Di)jYj , and each of the terms (Yj)\u22a4(Di)jYj is independent. Furthermore, it\u2019s easy to see that E[(Yj)\u22a4(Di)j(Yj)] = 1n (Di)j , \u2200j. By linearity of expectation it follows that E[ \u2211\nj(Y j)\u22a4(Di)jYj ] = 1 n \u2211 j(Di)j .\nNow, we claim \u2211\nj(Di)j \u2265 \u03bbnk\u00b5 . Indeed, by Assumption (A3) we have for any vector a \u2208 Rn\na\u22a4V\u22a4DiVa = \u2211\nj\n(Di)j\u3008Vj , a\u30092 \u2265 \u03bb.\nOn the other hand, however, by incoherence of V, \u2211 j(Di)j\u3008Vj , a\u30092 \u2264 \u2211 j(Di)j \u00b5k n . Hence, \u2211 j(Di)j \u2265 \u03bb nk\u00b5 . Putting things together, we get\nE[ \u2211\nj\n(Yj)\u22a4(Di)jY j ] \u2265 \u03bb\nk\u00b5\nDenote\nB := \u2016(Yj)\u22a4(Di)jYj\u20162 \u2264 k\nn (Di)j \u2264\n\u03bb\nk\u00b5 log2 n\nwhere the first inequality follows from our sampling procedure, and the last inequality by the assumption that \u2016W\u2016\u221e \u2264 \u03bbn k2\u00b5 log2 n .\nSince all the random variables (Yj)\u22a4(Di)jYj are independent, applying Matrix Chernoff we get that\nPr\n \u2211\nj\n(Yj)\u22a4(Di)j(Y j) \u2264 (1 \u2212 \u03b4) \u03bb\nk\u00b5\n  \u2264 n ( e\u2212\u03b4\n(1\u2212 \u03b4)(1\u2212\u03b4) ) \u03bb k\u00b5B \u2264 n (\ne\u2212\u03b4 (1 \u2212 \u03b4)(1\u2212\u03b4) )log2 n\nPicking \u03b4 = 34 , and union bounding over all i, with probability at least 1\u2212 1n2 , for all i,\n\u03c3min ( Y\u22a4DiY ) \u2265 1\n4\n\u03bb\nk\u00b5\nas needed."}, {"heading": "B.3 Update", "text": "We now prove the two key technical lemmas (Lemma 10 and Lemma 11) and then use them to prove that the updates make progress towards the ground truth. We prove them for Yt and use them to show Xt improves, while completely analogous arguments also hold when switching the role of the two iterates. Note that we measure the distance between Yt and V by distc(Yt,V) = minQ\u2208Ok\u00d7k \u2016YtQ \u2212 V\u2016 where Ok\u00d7k is the set of k \u00d7 k orthogonal matrices. For simplicity of notations, in these two lemmas, we let Yo = YtQ\u2217 where Q\u2217 = argminQ\u2208Ok\u00d7k\u2016YtQ\u2212V\u2016.\nWe first show that there can only be a few i\u2019s such that the spectral property of Y\u22a4o DiYo can be bad, when Yo is close to V. Let (Di)j be the j-th diagonal entry in Di, that is, (Di)j = Wi,j .\nLemma 10. Let Yo be a (column) orthogonal matrix in Rn\u00d7k, and \u01eb \u2208 (0, 1). If \u2016Yo \u2212 V\u20162F \u2264 \u01eb 3\u03bb2n 128\u00b5kD1 for\nD1 = maxi\u2208[n] \u2211 j(Di)j , then\n\u2223\u2223{i \u2208 [n] \u2223\u2223\u03c3min(Y\u22a4o DiYo) \u2264 (1\u2212 \u01eb)\u03bb }\u2223\u2223 \u2264 1024\u00b5 2k2\u03b32D1\n\u01eb4\u03bb3 \u2016V\u2212Yo\u20162F .\nProof of Lemma 10. For a value g > 0 which we will specify shortly, we call j \u2208 [n] \u201cgood\u201d if \u2016Yjo \u2212Vj\u20162 \u2264 g2. Denote the set of \u201cgood\u201d j\u2019s as Sg .\nThen for every unit vector a \u2208 Rk,\na\u22a4Y\u22a4o DiYoa = \u2211\nj\u2208[n] (Di)j\u3008a,Yjo\u30092\n\u2265 \u2211\nj\u2208Sg (Di)j\u3008a,Yjo\u30092\n= \u2211\nj\u2208Sg (Di)j\n( \u3008a,Vj\u3009+ \u3008a,Yjo \u2212Vj\u3009 )2\n\u2265 (1\u2212 \u01eb 4 ) \u2211\nj\u2208Sg (Di)j\u3008a,Vj\u30092 \u2212\n4\u2212 \u01eb \u01eb\n\u2211 j\u2208Sg (Di)j\u3008a,Yjo \u2212Vj\u30092\n(Using the fact \u2200x, y \u2208 R : (x+ y)2 \u2265 (1\u2212 \u01eb0)x2 \u2212 1\u2212 \u01eb0 \u01eb0 y2)\n\u2265 (1\u2212 \u01eb 4 ) \u2211\nj\u2208Sg (Di)j\u3008a,Vj\u30092 \u2212\n4\u2212 \u01eb \u01eb g2 \u2211\nj\u2208[n] (Di)j\n\u2265 (1\u2212 \u01eb 4 ) \u2211\nj\u2208[n] (Di)j\u3008a,Vj\u30092 \u2212\n\u00b5k\nn\n\u2211\nj\u2208[n]\u2212Sg\n(Di)j \u2212 4\u2212 \u01eb \u01eb g2 \u2211\nj\u2208[n] (Di)j\nBy Assumption (A3), we know that \u2211\nj\u2208[n] (Di)j\u3008a,Vj\u30092 = aTV\u22a4DiVa \u2265 \u03c3min(V\u22a4DiV) \u2265 \u03bb\nMoreover, recall D1 = maxi\u2208[n] \u2211 j(Di)j , so when g 2 \u2264 \u01eb 2\u03bb 16D1 ,\n4\u2212 \u01eb \u01eb g2 \u2211\nj\u2208[n] (Di)j \u2264\n\u01eb\u03bb\n4\nLet us consider now \u2211\nj\u2208[n]\u2212Sg (Di)j . Define:\nS =   i \u2208 [n] \u2223\u2223\u2223\u2223\u2223\u2223 \u00b5k n \u2211\nj\u2208[n]\u2212Sg\n(Di)j \u2265 \u01eb\u03bb\n4   \nThen it is sufficient to bound |S|. For Sg , observe that\n\u2211\nj\n\u2016Vj \u2212Yjo\u201622 = \u2016V\u2212Yo\u20162F\nWhich implies that\n|[n]\u2212 Sg| = size ([n]\u2212 Sg) \u2264 \u2016V\u2212Yo\u20162F\ng2\nLet uS be the indicator vector of S, and ug be the indicator vector of [n]\u2212 Sg, we know that\nu\u22a4SWug = \u2211\ni\u2208S\n\u2211\nj\u2208[n]\u2212Sg\n(Di)j\n\u2265 \u01eb\u03bbn 4\u00b5k |S|\nOn the other hand,\nu\u22a4SWug = u \u22a4 SEug + u \u22a4 S (W \u2212E)ug\n\u2264 |S||[n]\u2212 Sg|+ \u03b3n \u221a |S||[n]\u2212 Sg|\nPutting these two inequalities together, we have\n|[n]\u2212 Sg|+ \u03b3n \u221a\n|[n]\u2212 Sg| |S| \u2265 \u01eb\u03bbn 4\u00b5k\nWhich implies when |[n]\u2212 Sg| \u2264 \u01eb\u03bbn8\u00b5k , we have:\n|S| \u2264 64\u00b5 2k2\u03b32|[n]\u2212 Sg| \u01eb2\u03bb2 \u2264 64\u00b5 2k2\u03b32\u2016V \u2212Yo\u20162F \u01eb2\u03bb2g2\nThen, setting g2 = \u01eb 2\u03bb\n16D1 , we have:\n\u2223\u2223{i \u2208 [n] \u2223\u2223\u03c3min(Y\u22a4o DiYo) \u2264 (1\u2212 \u01eb)\u03bb }\u2223\u2223 \u2264 |S| \u2264 1024\u00b5 2k2\u03b32D1\n\u01eb4\u03bb3 \u2016V\u2212Yo\u20162F\nwhich is what we need.\nLemma 11. Let Yo be a (column) orthogonal matrix in Rn\u00d7k. Then we have \u2211\ni\u2208[n] \u2016V\u22a4Y\u22a5Y\u22a4\u22a5DiYo\u201622 \u2264 \u03b32\u03c1(Yo)nk3\u2016Yo \u2212V\u201622\nProof of Lemma 11. We want to bound the spectral norm of V\u22a4Y\u22a5Y\u22a4\u22a5DiYo, for a fixed j \u2208 [k], let Yj be the j-th column of Yo and V\u0303j be the j-th column of Y\u22a5Y\u22a4\u22a5V.\nFor fixed j, j\u2032 \u2208 [k], consider a new vector xj,j\u2032 \u2208 Rn such that xj,j \u2032\ni = (V\u0303j)i(Yj\u2032 )i.\nNote that \u3008V\u0303j ,Yj\u2032 \u3009 = 0, which implies that \u2211 i x j,j\u2032\ni = 0. Let us consider V\u22a4j Y\u22a5Y \u22a4 \u22a5DiYj\u2032 , we know that\nV\u22a4j Y\u22a5Y \u22a4 \u22a5DiYj\u2032 =\n\u2211\ns\u2208[n] (Di)s(V\u0303j)s(Yj\u2032 )s\n= \u2211\ns\u2208[n] (Di)sx\nj,j\u2032 s\nWhich implies that\n\u2211\ni\u2208[n]\n \u2211\ns\u2208[n] (Di)sx\nj,j\u2032 s\n  2\n= \u2016Wxj,j\u2032\u201622\n= \u2016(W \u2212E)xj,j\u2032\u201622 (since Exj,j \u2032 = 0)\n\u2264 \u03b32n2\u2016xj,j\u2032\u201622 Observe that\n\u2016xj,j\u2032\u201622 = \u2211\ni\u2208[n] (xj,j\n\u2032\ni ) 2\n= \u2211\ni\u2208[n] (V\u0303j)\n2 i (Yj\u2032 ) 2 i\n\u2264 \u03c1(Yo)k n\n\u2211 i\u2208[n] (V\u0303j) 2 i\n= \u03c1(Yo)k\nn \u2016V\u0303j\u201622\n\u2264 \u03c1(Yo)k n \u2016Y\u22a5Y\u22a4\u22a5V\u201622 = \u03c1(Yo)k\nn \u2016Y\u22a5Y\u22a4\u22a5(Yo \u2212V)\u201622\n\u2264 \u03c1(Yo)k n \u2016Yo \u2212V\u201622.\nWhich implies\n\u2211\ni\u2208[n]\n \u2211\ns\u2208[n] (Di)sx\nj,j\u2032 s\n  2\n\u2264 \u03b32\u03c1(Yo)nk\u2016Yo \u2212V\u201622\nNow we are ready to bound V\u22a4Y\u22a5Y\u22a4\u22a5DiYo. Note that\n\u2016V\u22a4Y\u22a5Y\u22a4\u22a5DiYo\u201622 \u2264 \u2016V\u22a4Y\u22a5Y\u22a4\u22a5DiYo\u20162F \u2264 \u2211\nj,j\u2032\u2208[k]\n( V\u22a4j Y\u22a5Y \u22a4 \u22a5DiYj\u2032 )2\n= \u2211\nj,j\u2032\u2208[k]\n \u2211\ns\u2208[n] (Di)sx\nj,j\u2032 s\n  2\n.\nThis implies that\n\u2211 i\u2208[n] \u2016V\u22a4Y\u22a5Y\u22a4\u22a5DiYo\u201622 \u2264 \u2211 i\u2208[n] \u2211 j,j\u2032\u2208[k]\n \u2211\ns\u2208[n] (Di)sx\nj,j\u2032 s\n  2\n\u2264 \u03b32\u03c1(Yo)nk3\u2016Yo \u2212V\u201622.\nas needed.\nWe now use the two technical lemmas to prove the guarantees for the iterate after one update step.\nLemma 12 (Update, main). Let Y be a (column) orthogonal matrix in Rn\u00d7k, and dist2c(Y,V) \u2264 min{ 12 , \u03bb2n 384\u00b5k2D1 }\nfor D1 = maxi\u2208[n] \u2211 j(Di)j .\nDefine X\u0303 \u2190 argminX\u2208Rn\u00d7k \u2225\u2225M\u2212XY\u22a4 \u2225\u2225 W . Let X a n\u00d7 k matrix such that for each row:\nX i = { X\u0303i if \u2016X\u0303i\u201622 \u2264 \u03be = 2\u00b5kn 0 otherwise.\nSuppose X has QR decomposition X = XR. Then (1) \u2016X\u2212U\u03a3V\u22a4Y\u20162F \u2264 \u22062u := ( 108\u03be\u00b52k3\u03b32D1 \u03bb2 + 160\u03b3 2\u00b5\u03c1(Y)k4 \u03bb2 ) distc(Y,V)2 + 160k\u03bb2 \u2016W \u2299N\u201622. (2) If \u2206u \u2264 18\u03c3min(M\u2217), then\ndistc(U,X) \u2264 8\n\u03c3min(M\u2217)\u2212 2\u2206u \u2206u and \u03c1(X) \u2264\n4\u00b5\n\u03c3min(M\u2217)\u2212 2\u2206u .\nProof of Lemma 12. (1) By KKT condition, we know that for orthogonal Y, the optimal X\u0303 satisfies ( W \u2299 [ M\u2212 X\u0303Y\u22a4 ]) Y = 0\nwhich implies that the i-th row X\u0303i of X\u0303 is given by\nX\u0303i = MiDiY ( Y\u22a4DiY )\u22121 = (M\u2217)iDiY ( Y\u22a4DiY )\u22121 +NiDiY ( Y\u22a4DiY )\u22121 .\nLet us consider the first term, by M\u2217 = U\u03a3V\u22a4, we know that\n(M\u2217)iDiY ( Y\u22a4DiY )\u22121 = Ui\u03a3V\u22a4DiY ( Y\u22a4DiY )\u22121\n= Ui\u03a3V\u22a4(YY\u22a4 +Y\u22a5Y \u22a4 \u22a5)DiY ( Y\u22a4DiY )\u22121 = Ui\u03a3V\u22a4Y +Ui\u03a3V\u22a4Y\u22a5Y \u22a4 \u22a5DiY ( Y\u22a4DiY )\u22121\nwhich implies that\nX\u0303i \u2212Ui\u03a3V\u22a4Y = Ui\u03a3V\u22a4Y\u22a5Y\u22a4\u22a5DiY ( Y\u22a4DiY )\u22121 +NiDiY ( Y\u22a4DiY )\u22121\nLet us consider set\nS1 = { i \u2208 [n] \u2223\u2223\u2223\u2223\u03c3min(Y \u22a4DiY) \u2264 \u03bb\n4\n}\nNow we have:\n\u2211\ni/\u2208S1\n\u2225\u2225\u2225X\u0303i \u2212Ui\u03a3V\u22a4Y \u2225\u2225\u2225 2\n2 \u2264 16 \u03bb2\n\u2211\ni/\u2208S1\n( 2\u2016Ui\u03a3V\u22a4Y\u22a5Y\u22a4\u22a5DiY\u201622 + 2\u2016NiDiY\u201622 )\n\u2264 32\u00b5k\u2016\u03a3\u2016 2 2\nn\u03bb2\n\u2211\ni/\u2208S1\n\u2016V\u22a4Y\u22a5Y\u22a4\u22a5DiY\u201622 + 32\n\u03bb2\n\u2211 i\u2208[n] \u2016NiDiY\u201622\n\u2264 32\u00b5k\u2016\u03a3\u2016 2 2\nn\u03bb2\n\u2211 i\u2208[n] \u2016V\u22a4Y\u22a5Y\u22a4\u22a5DiY\u201622 + 32 \u03bb2 \u2016(W \u2299N)Y\u20162F\n\u2264 \u2206g := 32\u03b32\u00b5\u03c1(Y)k4\n\u03bb2 distc(Y,V)2 +\n32k\n\u03bb2 \u2016(W \u2299N)\u201622.\nwhere the last inequality is due to Lemma 11. Note that since \u03be = 2\u00b5kn \u2265 2\u2016Ui\u03a3V\u22a4Y\u201622, this implies \u2223\u2223\u2223 { i \u2208 [n]\u2212 S1 \u2223\u2223\u2223\u2016X\u0303i\u201622 \u2265 \u03be }\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223 { i \u2208 [n]\u2212 S1 \u2223\u2223\u2223\u2223\u2016X\u0303i \u2212Ui\u03a3V\u22a4Y\u201622 \u2265 \u03be\n2 }\u2223\u2223\u2223\u2223 \u2264 2\u2206g \u03be .\nLet S2 = { i \u2208 [n]\u2212 S1 \u2223\u2223\u2223\u2016X\u0303i\u201622 \u2265 \u03be } , we have:\n\u2225\u2225X\u2212U\u03a3V\u22a4Y \u2225\u22252 F = n\u2211\ni=1\n\u2225\u2225\u2225Xi \u2212Ui\u03a3V\u22a4Y \u2225\u2225\u2225 2\n2 (because \u2016Xi\u201622 \u2264 \u03be and \u2016Ui\u03a3V\u22a4Y\u201622 \u2264 \u03be)\n\u2264 \u2211\ni\u2208S1\u222aS2 2\u03be +\n\u2211\ni6\u2208S1\u222aS2\n\u2225\u2225\u2225X\u0303i \u2212Ui\u03a3V\u22a4Y \u2225\u2225\u2225 2\n2\n\u2264 2\u03be(|S1|+ |S2|) + \u2211\ni6\u2208S1\u222aS2\n\u2225\u2225\u2225X\u0303i \u2212Ui\u03a3V\u22a4Y \u2225\u2225\u2225 2\n2\n\u2264 2\u03be|S1|+ 4\u2206g +\u2206g.\nBy Lemma 10, we know that |S1| \u2264 54\u00b5 2k3\u03b32D1 \u03bb2 \u2016V\u2212Y\u201622. Further plugging in \u2206g , we have \u2225\u2225X\u2212U\u03a3V\u22a4Y \u2225\u22252 F\n\u2264 2\u03be 54\u00b5 2k3\u03b32D1\n\u03bb2 \u2016V\u2212Y\u201622 +\n160\u03b32\u00b5\u03c1(Y)k4\n\u03bb2 \u2016Y \u2212V\u201622 +\n160k\n\u03bb2 \u2016(W \u2299N)\u201622\n=\n( 108\u03be\u00b52k3\u03b32D1\n\u03bb2 +\n160\u03b32\u00b5\u03c1(Y)k4\n\u03bb2\n) \u2016Y \u2212V\u201622 + 160k\n\u03bb2 \u2016(W \u2299N)\u201622.\n(2) Denote B = \u03a3V\u22a4Y. Then,\nsin \u03b8(U,X) = \u2016U\u22a4\u22a5X\u20162 = \u2016U\u22a4\u22a5(X\u2212UB)R\u22121\u20162 \u2264 \u2016X\u2212UB\u20162\u2016R\u22121\u20162 = 1\n\u03c3min(X) \u2016X\u2212UB\u20162\nSince \u2016X\u2212UB\u20162 \u2264 \u2206u, we have\n\u03c3min(X) \u2265 \u03c3min(UB)\u2212\u2206u = \u03c3min(\u03a3V\u22a4Y)\u2212\u2206u \u2265 \u03c3min(M\u2217)cos \u03b8(Y,V) \u2212\u2206u.\nBy the assumption cos \u03b8(Y,V) \u2265 1/2, so\nsin \u03b8(U,X) \u2264 2 \u03c3min(M\u2217)\u2212 2\u2206u \u2206u.\nWhen \u2206u \u2264 18\u03c3min(M\u2217), the right hand side is smaller than 1/3, so cos \u03b8(U,X) \u2265 1/2, and thus tan \u03b8(U,X) \u2264 2sin \u03b8(U,X). Then the statement on distc(U,X) follows from distc(U,X) \u2264 2tan \u03b8(U,X) \u2264 4sin \u03b8(U,X).\nFinally, observe that Xi = X i R\u22121, so\n\u2016Xi\u20162 \u2264 \u2016X i\u20162\u2016R\u22121\u20162 \u2264\n\u03be\n\u03c3min(X)\nwhich leads to the bound.\nB.4 Putting everything together: proofs of the main theorems\nFinally, in this section we put things together and prove the main theorems. We first proceed to the SVD-initialization based algorithm:\nTheorem 1. If M\u2217,W satisfy assumptions (A1)-(A3), and\n\u03b3 = O ( min {\u221a n\nD1\n\u03bb\n\u03c4\u00b53/2k2 ,\n\u03bb\n\u03c43/2\u00b5k2\n}) ,\nthen after O(log(1/\u01eb)) rounds Algorithm 1 with initialization from Algorithm 3 outputs a matrix M\u0303 that satisfies\n||M\u0303\u2212M\u2217||2 \u2264 O ( k\u03c4\n\u03bb\n) ||W \u2299N||2 + \u01eb."}, {"heading": "The running time is polynomial in n and log(1/\u01eb).", "text": "Proof of Theorem 1. We first show by induction distc(Xt,U) \u2264 12t + 70 k\u03bb\u03c3min(M\u2217)\u03b4 for t > 1, and distc(Yt,U) \u2264 1 2t + 70 k \u03bb\u03c3min(M\u2217)\n\u03b4 for t \u2265 1. First, by Lemma 8, Y1 satisfies\ndistc(V,Y1) \u2264 8k\u22061 = 64k(\u03b3\u00b5k + \u03b4)\n\u03c3min(M\u2217) .\nSince \u03b3 = O (\n1 \u03c4k2\u00b5\n) , the base case follows. Now proceed to the inductive step and prove the statement for t + 1\nassuming it is true for t. Now we can apply Lemma 12. By taking the constants within the O(\u00b7) notation for \u03b3 sufficiently small and by the inductive hypothesis, we have\n( 108\u03be\u00b52k3\u03b32D1\n\u03bb2 +\n160\u03b32\u00b5\u03c1(Y1)k 4\n\u03bb2\n) \u2264 1\n100 \u03c32min(M \u2217)\nand\n\u2206u \u2264 1\n8 \u03c3min(M\n\u2217).\nBy Lemma 12, we get\ndistc(U,Xt+1) \u2264 2\n\u03c3min(M\u2217)\u2212 2\u2206u \u2206u \u2264\n8\n3\u03c3min(M\u2217) \u2206u\n= 8\n3\u03c3min(M\u2217)\n\u221a( 108\u03be\u00b52k3\u03b32D1\n\u03bb2 +\n160\u03b32\u00b5\u03c1(Y1)k4\n\u03bb2\n) dist2c(U,Xt) + 160k\n\u03bb2 \u03b42\n\u2264 8 3\u03c3min(M\u2217)\n(\u221a( 108\u03be\u00b52k3\u03b32D1\n\u03bb2 +\n160\u03b32\u00b5\u03c1(Y1)k4\n\u03bb2\n) dist2c(Yt,V) + \u221a 160k\n\u03bb2 \u03b42\n) (using \u221a a+ b \u2264 \u221aa+ \u221a b)\n\u2264 1 2 distc(Yt,V) + 35\n\u221a k\n\u03bb\u03c3min(M\u2217) \u03b4\nso the statement also holds for t+ 1. This completes the proof for bounding distc(Xt,U) and distc(Yt,V). Given the bounds on distc(Xt,U) and distc(Yt,V), we are now ready to prove the theorem statement. For simplicity, let X denote XT+1 and Y denote YT , so the algorithm outputs M\u0303 = XY. By Lemma 12,\n\u2016X\u2212U\u03a3V\u22a4Y\u20162F \u2264 \u22062u := ( 108\u03be\u00b52k3\u03b32D1\n\u03bb2 +\n160\u03b32\u00b5\u03c1(Y)k4\n\u03bb2\n) distc(Y,V)2 + 160k\n\u03bb2 \u2016W \u2299N\u201622.\nPlugging the choice of \u03b3 and noting \u03be = 2\u00b5kn and \u03c1(Y) = O(\u00b5/\u03c3min(M \u2217)), we have\n\u2016X\u2212U\u03a3V\u22a4Y\u20162F \u2264 \u22062u = O ( distc(Y,V)2 ) + 160k\n\u03bb2 \u2016W\u2299N\u201622\nwhich leads to\n\u2016X\u2212U\u03a3V\u22a4Y\u2016F \u2264 \u2206u \u2264 O (distc(Y,V)) + 16\n\u221a k \u03bb \u2016W \u2299N\u20162.\nNow consider \u2016M\u2217\u2212M\u0303\u20162 = \u2016M\u2217\u2212XY\u22a4\u20162. By definition, we know that there exists Q such that Y = VQ+\u2206y where \u2016\u2206y\u20162 = O(distc(Y,V)). Also, let R = X\u2212U\u03a3V\u22a4Y.\nM\u0303\u2212M\u2217 = [ U\u03a3V\u22a4(VQ+\u2206y) +R ] (VQ+\u2206y) \u22a4 \u2212U\u03a3V\u22a4\n= U\u03a3Q\u2206\u22a4y +U\u03a3V \u22a4\u2206y(VQ+\u2206y) \u22a4 +R(VQ+\u2206y) \u22a4 = U\u03a3Q\u2206\u22a4y +U\u03a3V \u22a4\u2206yY \u22a4 +RY\u22a4.\nTherefore,\n\u2016M\u0303\u2212M\u2217\u20162 \u2264 \u2016U\u03a3\u20162\u2016Q\u20162\u2016\u2206y\u20162 + \u2016U\u03a3V\u22a4\u20162\u2016\u2206y\u20162\u2016Y\u20162 + \u2016R\u20162\u2016Y\u20162 \u2264 2\u2016\u2206y\u20162 + \u2016R\u20162\n\u2264 O (distc(Y,V)) + 16\n\u221a k \u03bb \u2016W \u2299N\u20162.\nCombining this with the bound on distc(YT ,V), the theorem then follows.\nNext, we show the main theorem for random initialization:\nTheorem 3 (Main, random initialization). Suppose M\u2217,W satisfy assumptions (A1)-(A3) with\n\u03b3 = O ( min {\u221a n\nD1\n\u03bb\n\u03c4\u00b52k5/2 ,\n\u03bb\n\u03c43/2\u00b53/2k5/2\n}) ,\n\u2016W\u2016\u221e = O (\n\u03bbn\nk2\u00b5 log2 n\n) ,\nwhere D1 = maxi\u2208[n] \u2016Wi\u20161. Then after O(log(1/\u01eb)) rounds Algorithm 1 using initialization from Algorithm 4 outputs a matrix M\u0303 that with probability at least 1\u2212 1/n2 satisfies\n\u2016M\u0303\u2212M\u2217\u20162 \u2264 O ( k\u03c4\n\u03bb\n) \u2016W \u2299N\u20162 + \u01eb.\nThe running time is polynomial in n and log(1/\u01eb).\nProof of Theorem 3. Let Y be initialized using the random initialization algorithm 4. Consider applying the proof in Lemma 12, with S1 being modified to be\nS1 = { i \u2208 [n] \u2223\u2223\u2223\u2223\u03c3min(Y \u22a4DiY) \u2264 \u03bb\n4\u00b5k\n}\nBut with this modification, S1 = \u2205, with high probability. Then the same calculation from Lemma 12 (which now doesn\u2019t need to use Lemma 10 at all since S1 = \u2205) gives\n\u2225\u2225X\u2212U\u03a3V\u22a4Y \u2225\u22252 F \u2264 \u2206g\u00b5k\nBut following part (2) of the same Lemma, we get that if \u2206g\u00b5k < 18\u03c3min(M \u2217),\ndistc(U,X) \u2264 2\n\u03c3min(M\u2217)\u2212 2\u2206g\u00b5k \u2206g\u00b5k\nSo, in order to argue by induction in 1 exactly as before, we only need to check that after the update step for X, distc(U,X) is small enough to apply Lemma 12 for later steps. Indeed, we have:\ndistc(U,X) \u2264 2\n\u03c3min(M\u2217)\u2212 2\u2206g\u00b5k \u2206g\u00b5k \u2264\n\u221a min { 1\n2 ,\n\u03bb2n\n384\u00b5k2D1\n}\nNoticing that \u2206g has a quadratic dependency on \u03b3, we see that if\n\u03b3 = O ( min {\u221a n\nD1\n\u03bb\u03c3min(M \u2217)\n\u00b52k5/2 , \u03bb\u03c3\n3/2 min(M \u2217)\n\u00b53/2k5/2\n}) ,\nthe inequality is indeed satisfied. With that, the theorem statement follows."}, {"heading": "B.5 Estimating \u03c3max(M\u2217)", "text": "Finally, we show that we can estimate \u03c3max(M\u2217) up to a very good accuracy, so that we can apply our main theorems to matrices with arbitrary \u03c3max(M\u2217). This is quite easy: the estimate of it is just \u2016W \u2299 M\u20162. Then, the following lemma holds:\nLemma 13. It \u03b3 = o( 1k\u00b5 ) and \u03b4 = \u2016W \u2299N\u20162 = o(\u03c3max(M\u2217)) then \u2016W \u2299M\u20162 = (1\u00b1 o(1))(\u03c3max(M\u2217)) Proof. We proceed separately for the upper and lower bound.\nFor the upper bound, we have\n\u2016W \u2299M\u20162 = \u2016W \u2299M\u2217 +W \u2299N\u20162 \u2264 \u2016W \u2299M\u2217\u20162 + \u2016W \u2299N\u20162 \u2264 \u2016(W \u2212E)\u2299M\u2217\u20162 + \u2016E\u2299M\u2217\u20162 + \u2016W \u2299N\u20162 \u2264 \u03b3k\u00b5\u03c3max(M\u2217) + \u03c3max(M\u2217) + \u03b4 \u2264 (1 + o(1))\u03c3max(M\u2217). (by Lemma 5)\nFor the lower bound, completely analogously we have\n\u2016W \u2299M\u20162 = \u2016W \u2299M\u2217 +W \u2299N\u20162 \u2265 \u2016W \u2299M\u2217\u20162 \u2212 \u2016W \u2299N\u20162 \u2265 \u2016E\u2299M\u2217\u20162 \u2212 \u2016(W \u2212E)\u2299M\u2217\u20162 \u2212 \u2016W \u2299N\u20162 \u2265 \u03c3max(M\u2217)\u2212 \u03b3k\u00b5\u03c3max(M\u2217)\u2212 \u03b4 \u2265 (1\u2212 o(1))\u03c3max(M\u2217) (by Lemma 5)\nwhich finishes the proof.\nGiven this, the reduction to the case \u03c3max(M\u2217) \u2264 1 is obvious: first, we scale the matrix M down by our estimate of \u03c3max(M\u2217) and run our algorithm with, say, four times as many rounds. After this, we rescale the resulting matrix M\u0303 by our estimate of \u03c3max(M\u2217), after which the claim of Theorems 1 and 3 follows.\nAlgorithm 5 Main Algorithm (ALT) Input: Noisy observation M, weight matrix W, rank k, number of iterations T\n1: (X1,Y1) = SVDINITIAL(M,W), d1 = 18k\u221alog n + 64\n\u221a k\u03b4\n\u03bb\u03c3min(M\u2217)\n2: Y1 \u2190 WHITENING(Y1,W, d1, \u03bb, \u03bb, \u00b5, k) 3: for t = 1, 2, ..., T do 4: dt+1 = 1 2t+1 1 8k \u221a logn + 64 \u221a k \u03bb\u03c3min(M\u2217) \u03b4\n5: Xt+1 \u2190 argminX\u2208Rn\u00d7k \u2225\u2225\u2225M\u2212XY\u22a4t \u2225\u2225\u2225 W 6: X\u0303t+1 \u2190 QR(Xt+1) 7: Xt+1 \u2190 WHITENING(X\u0303t+1,W, dt+1, \u03bb, \u03bb, \u00b5, k) 8: Yt+1 \u2190 argminY\u2208Rn\u00d7k \u2225\u2225M \u2212Xt+1Y\u22a4 \u2225\u2225 W\n9: Y\u0303t+1 \u2190 QR(Yt+1) 10: Yt+1 \u2190 WHITENING(Y\u0303t+1,W, dt+1, \u03bb, \u03bb, \u00b5, k) 11: end for 12: \u03a3 \u2190 argmin\u03a3\u2016W \u2299 (M \u2212XT+1\u03a3Y \u22a4 T+1)\u20162 Output: M\u0303 = XT+1\u03a3Y \u22a4 T+1\nAlgorithm 6 Whitening (WHITENING)\nInput: orthogonal matrix X\u0303 \u2208 Rn\u00d7k, weight W, distance d, spectral barriers \u03bb, \u03bb, incoherency \u00b5, rank k. 1: Solve the following convex programing on the matrices R \u2208 Rn\u00d7k and {Ar \u2208 Rk\u00d7k}nr=1:\n||R \u2212 X\u0303||2 \u2264 d ||X\u0303\u22a4(R\u2212 X\u0303) + (R\u2212 X\u0303)\u22a4X\u0303||2 \u2264 d2 ||X\u0303\u22a4\u22a5R||2 \u2264 d (Rr)\u22a4Rr Ar, \u2200r \u2208 [n]\nTr(Ar) \u2264 \u00b5k\nn , \u2200r \u2208 [n]\nn\u2211\nr=1\nAr = I\n\u03bbI n\u2211\nr=1\nWi,rAr \u03bbI, \u2200i \u2208 [n]\n2: \u2200r \u2208 [n],Xr \u223c Rademacher(Rr,Ar \u2212 (Rr)\u22a4Rr). 3: X = QR(X), X \u2208 Rn\u00d7k whose rows are Xr.\nOutput: X. (may need O(log(1/\u03b1)) runs to succeed with probability 1\u2212 \u03b1; see text)"}, {"heading": "C An alternative approach: alternating minimization with SDP whitening", "text": "Our main results build on the insight that the spectral property only need to hold in an average sense. However, we can even make sure that the spectral property holds at each step in a strict sense by a whitening step using SDP and Rademacher rounding. This is presented a previous version of the paper, and we keep this result here since potentially it can be applied in some other applications where similar spectral properties are needed and is thus of independent interest.\nThe whitening step (see Algorithm 6) is a convex (actually semidefinite) relaxation followed by a randomized\nrounding procedure. We explain each of the constraints in the semidefinite program in turn. The first three constraints control the spectral distance between X and X\u0303. The next two constraints control the incoherency, and the rest are for the spectral ratio. The solution of the relaxation is then used to specify the mean and variance of a Rademacher (random) vector, from which the final output of the whitening step is drawn. Here a Rademacher vector is defined as:\nDefinition (Rademacher random vector). A random vector x \u2208 Rk is a Rademacher random vector with mean \u00b5 and variance \u03a3 0 (denoted as x \u223c Rademacher(\u00b5,\u03a3)), if x = \u00b5+ S\u03c3 where S is a k \u00d7 k symmetric matrix such that S2 = \u03a3, \u03c3 \u2208 Rk is a vector where each entry is i.i.d Rademacher random variable.\nWe use this type of random vector to ensure that if x \u223c Rademacher(\u00b5,\u03a3), then E[x] = \u00b5,E[xx\u22a4] = \u00b5\u00b5\u22a4 +\u03a3. Since the desired properties of the output of whitening can be tested (see Lemma 17), we can repeat the whitening step O(log(1/\u03b1)) times to get high probability 1 \u2212 \u03b1. In the rest of the paper, we will just assume that it is repeated sufficiently many times (polynomial in n and log(1/\u01eb)) so that Algorithm 5 succeeds with probability 1\u2212 1/n.\nWe now present the analysis for this algorithm. The SVD initialization has been analyzed, so we focus on the update step and the whitening step.\nNote Since our algorithm will output matrix M\u0303 such that ||M\u0303\u2212M\u2217||2 \u2248 O ( k3/2 \u221a logn\n\u03bb\u03c3min(M\u2217)\n) ||W\u2299N||2 and ||M\u2217||2 =\n1, \u03bb \u2264 1, therefore, without lose of generality we can assume that ||W \u2299 N||2 \u2264 \u03bb\u03c3min(M \u2217)\nk3/2 \u221a logn , otherwise we can just output zero matrix."}, {"heading": "C.1 Update", "text": "We want to show that after every round of ALT, we move our current matrices X,Y closer to the optimum. We will show that X\u0303 \u2190 argminX\u2208Rn\u00d7k \u2225\u2225M\u2212XY\u22a4 \u2225\u2225 W\nis a noisy power method update: X\u0303 = M\u2217Y +G where ||G||2 is small.\nFor intuition, note that if ||G||2 = 0, that is, X\u0303 = M\u2217Y, then we know that tan \u03b8(X\u0303,U) = 0, so within one step of update we will be already hit into the correct subspace. We will show when ||G||2 is small we still have that tan \u03b8(X\u0303,U) is progressively decreasing. Then, in order to show ||G||2 is small, we need to make sure we start from a good Y as assumed in Lemma 16.\nFirst, we show that when G is small, then tan \u03b8(X\u0303,U) is small.\nLemma 14 (Distance from OPT). Let M\u2217 = U\u03a3VT \u2208 Rn\u00d7n be the singular value decomposition of a rank-k matrix M\u2217, let Y \u2208 Rn\u00d7k be an orthogonal matrix, X\u0303 = M\u2217Y +G, then we have\ntan \u03b8(X\u0303,U) \u2264 ||G||2 cos \u03b8(Y,V)\u03c3min(\u03a3)\u2212 ||G||2 .\nProof of Lemma 14. By definition,\ntan \u03b8(X\u0303,U) = ||U\u22a4\u22a5X\u0303(U\u22a4X\u0303)\u22121||2 = ||U\u22a4\u22a5(M\u2217Y +G)(U\u22a4(M\u2217Y +G))\u22121||2 \u2264 ||U\u22a4\u22a5G(\u03a3V\u22a4Y +U\u22a4G)\u22121||2 \u2264 ||U\u22a4\u22a5G||2||(\u03a3V\u22a4Y +U\u22a4G)\u22121||2 \u2264 ||U\u22a4\u22a5G||2||(V\u22a4Y)\u22121||2||(\u03a3+U\u22a4G(V\u22a4Y)\u22121)\u22121||2 \u2264 ||U\u22a4\u22a5G||2 1\ncos \u03b8(Y,V) \u03c3\u22121min\n( \u03a3+U\u22a4G(V\u22a4Y)\u22121 ) .\nFor the last term, we have\n\u03c3min(\u03a3+U \u22a4G(V\u22a4Y)\u22121) \u2265 \u03c3min(\u03a3)\u2212 \u03c3max ( U\u22a4G(V\u22a4Y)\u22121 ) \u2265 \u03c3min(\u03a3)\u2212 ||U\u22a4G||2 cos \u03b8(Y,V) .\nTherefore,\ntan \u03b8(X\u0303,U) \u2264 ||U \u22a4 \u22a5G||2\ncos \u03b8(Y,V) ( \u03c3min(\u03a3)\u2212 ||U \u22a4G||2 cos \u03b8(Y,V) )\n= ||U\u22a4\u22a5G||2\ncos \u03b8(Y,V)\u03c3min(\u03a3)\u2212 ||U\u22a4G||2 \u2264 ||G||2\ncos \u03b8(Y,V)\u03c3min(\u03a3)\u2212 ||G||2 completing the proof.\nNow we show that if Y has nice properties as stated in Lemma 16, then G is small. Recall the following notation: for a matrix A, let \u03c1(A) be defined as maxi{nk ||Ai||22}.\nLemma 15 (Bounding ||G||2). Let M\u2217 = U\u03a3V\u22a4 \u2208 Rn\u00d7n be the singular value decomposition of a rank-k matrix M\u2217, M = M\u2217 +N be the noisy observation, and let W,M\u2217 satisfy the conditions of Theorem 20. Let Y \u2208 Rn\u00d7k be an orthogonal matrix. For X\u0303 = argminX||M\u2212XY\u22a4||W we have X\u0303 = M\u2217Y +G where\n||G||2 \u2264 max i\u2208[n]\n{ \u03b3k3/2 \u221a \u03c1(U)\u03c1(Y)\n\u03c3min(Y\u22a4DiY) sin \u03b8(Y,V) +\n\u221a k||W \u2299N||2 \u03c3min(Y\u22a4DiY) } .\nProof of Lemma 15. By taking the derivatives of ||M \u2212 XY\u22a4||W w.r.t. X, we know that the optimal solution X\u0303 satisfies (W \u2299 [M \u2212 X\u0303Y\u22a4])Y = 0. Plugging in X\u0303 = M\u2217Y +G, we get\n(W \u2299 [M \u2212M\u2217YY\u22a4])Y = (W \u2299 [GY\u22a4])Y.\nSince M = M\u2217 +N and I = YY\u22a4 +Y\u22a5Y\u22a4\u22a5 , the above equation is\n(W \u2299 [GY\u22a4])Y = (W \u2299 [M\u2217Y\u22a5Y\u22a4\u22a5 ])Y + (W \u2299N)Y.\nSo for any i \u2208 [n] (recall that [\u00b7]i is the i-th row)\n[(W \u2299 [GY\u22a4])Y]i = [(W \u2299 [M\u2217Y\u22a5Y\u22a4\u22a5 ])Y]i + [(W \u2299N)Y]i. (C.1)\nNote that for every matrix S \u2208 Rn\u00d7n, for Di = Diag(Wi) we have\n[W \u2299 S]i = SiDi.\nApplying this to (C.1) leads to\nGiY\u22a4DiY = (M \u2217)iY\u22a5Y \u22a4 \u22a5DiY + [(W \u2299N)]iY.\nSince (M\u2217)iY\u22a5Y\u22a4\u22a5IY = 0,\nGiY\u22a4DiY = (M \u2217)iY\u22a5Y \u22a4 \u22a5(Di \u2212 I)Y + [(W \u2299N)]iY.\nThis gives us\nGi = (M\u2217)iY\u22a5Y \u22a4 \u22a5(Di \u2212 I)Y(Y\u22a4DiY)\u22121 + [(W \u2299N)]iY(Y\u22a4DiY)\u22121. (C.2)\nNow we turn to bound the operator norm of G. By definition, it suffices to bound ||a\u22a4Gb||2 for any two unit vectors a \u2208 Rn\u00d71, b \u2208 Rk\u00d71 (note that for a scalar s, ||s||2 = |s|). By (C.2),\n||a\u22a4Gb||2 = \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nai(M \u2217)iY\u22a5Y \u22a4 \u22a5(Di \u2212 I)Y(Y\u22a4DiY)\u22121b+\nn\u2211\ni=1 ai[(W \u2299N)]iY(Y\u22a4DiY)\u22121b \u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nai(M \u2217)iY\u22a5Y \u22a4 \u22a5(Di \u2212 I)Y(Y\u22a4DiY)\u22121b \u2225\u2225\u2225\u2225\u2225 2\ufe38 \ufe37\ufe37 \ufe38\nT1\n+ \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1 ai[(W \u2299N)]iY(Y\u22a4DiY)\u22121b \u2225\u2225\u2225\u2225\u2225 2\ufe38 \ufe37\ufe37 \ufe38\nT2\n.\nIn the following, we bound the two terms T 1 and T 2 respectively.\n(Bounding T 1) Let Q = \u03a3V\u22a4Y\u22a5Y\u22a4\u22a5 . We have\n(M\u2217)iY\u22a5Y \u22a4 \u22a5 = U iQ and ||Q||2 \u2264 ||V\u22a4Y\u22a5||2 = sin \u03b8(Y,V).\nAlso let B denote the matrix whose i-th column is Bi = (Y\u22a4DiY)\u22121b. Then T 1 becomes\nT 1 = \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nai(M \u2217)iY\u22a5Y \u22a4 \u22a5(Di \u2212 I)Y(Y\u22a4DiY)\u22121b \u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\naiU iQ(Di \u2212 I)YBi \u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nk\u2211\nr=1\n(aiUi,r)Q r(Di \u2212 I)YBi \u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 k\u2211\nr=1\nn\u2211\ni=1\n(aiUi,r)Q r(Di \u2212 I)YBi \u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nr=1\nn\u2211\ni,j=1\n(aiUi,r)(Wi,j \u2212 1)Qr,jYjBi \u2225\u2225\u2225\u2225\u2225\u2225 2\nwhere the last equality is because Qr(Di \u2212 I)Y = \u2211n\nj=1(Wi,j \u2212 1)Qr,jYj . Now denote \u03b1i,r = aiUi,r and \u03b1r = (\u03b11,r, ..., \u03b1n,r)\u22a4.\nT 1 = \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211\nr=1\nn\u2211\ni,j=1\n(aiUi,r)(Wi,j \u2212 1)Qr,jYjBi \u2225\u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 k\u2211\nr=1 \u03b1\u22a4r [(W \u2212E)\u2299 (B\u22a4Y\u22a4)]Qr \u2225\u2225\u2225\u2225\u2225 2\n\u2264 k\u2211\nr=1\n\u2225\u2225\u03b1\u22a4r [(W \u2212E)\u2299 (B\u22a4Y\u22a4)]Qr \u2225\u2225 2\n=\nk\u2211\nr=1\n\u2016\u03b1r\u20162 \u2225\u2225(W \u2212E)\u2299 (B\u22a4Y\u22a4) \u2225\u2225 2 \u2016Qr\u20162 .\nClearly, for \u2016Qr\u20162 we have \u2016Qr\u20162 \u2264 \u2016Q\u20162 \u2264 sin \u03b8(Y,V).\nFor \u2016\u03b1r\u20162, we have\nk\u2211\nr=1\n||\u03b1r||2 \u2264 \u221a k\n\u221a\u221a\u221a\u221a k\u2211\nr=1\n||\u03b1r||22\n= \u221a k\n\u221a\u221a\u221a\u221a k\u2211\nr=1\nn\u2211\ni=1\na2iU 2 i,r\n= \u221a k\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n( a2i ( k\u2211\nr=1\nU2i,r\n))\n\u2264 \u221a k \u221a\u221a\u221a\u221ak\u03c1(U) n ( n\u2211\ni=1\na2i\n)\n= k\n\u221a \u03c1(U)\nn .\nFor \u2225\u2225(W \u2212E)\u2299 (B\u22a4Y\u22a4) \u2225\u2225 2 , we can apply the spectral lemma (Lemma 5) to get\n\u2225\u2225(W \u2212E)\u2299 (B\u22a4Y\u22a4) \u2225\u2225 2 \u2264 \u03b3k \u221a \u03c1(B\u22a4)\u03c1(Y).\nWe have ||Bi||2 = ||(Y\u22a4DiY)\u22121b||2 \u2264 1\u03c3min(Y\u22a4DiY) , so\n\u03c1(B\u22a4) \u2264 max i\u2208[n]\n{ n/k\n\u03c32min(Y \u22a4DiY)\n}\nand \u2225\u2225(W \u2212E)\u2299 (B\u22a4Y\u22a4) \u2225\u2225 2 \u2264 max\ni\u2208[n]\n{ \u03b3 \u221a kn\u03c1(Y)\n\u03c3min(Y\u22a4DiY)\n} .\nPutting together, we have\nT 1 \u2264 k \u221a \u03c1(U)\nn \u00d7max i\u2208[n]\n{ \u03b3 \u221a kn\u03c1(Y)\n\u03c3min(Y\u22a4DiY)\n} \u00d7 sin \u03b8(Y,V)\n\u2264 max i\u2208[n]\n{ \u03b3k3/2 \u221a \u03c1(Y)\u03c1(U)\n\u03c3min(Y\u22a4DiY) sin \u03b8(Y,V)\n} . (C.3)\n(Bounding T 2) Recall that B denote the matrix whose i-th column is Bi = (Y\u22a4DiY)\u22121b.\nT 2 = \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1 ai[(W \u2299N)]iY(Y\u22a4DiY)\u22121b \u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1 ai[(W \u2299N)]iYBi \u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nai[(W \u2299N)]i k\u2211\nr=1\nYrBr,i \u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 k\u2211\nr=1\nn\u2211\ni=1 ai[(W \u2299N)]iYrBr,i \u2225\u2225\u2225\u2225\u2225 2 .\nNow denote \u03b2i,r = aiBr,i and \u03b2r = (\u03b2r,1, \u03b2r,2, . . . , \u03b2r,n)\u22a4.\nT 2 = \u2225\u2225\u2225\u2225\u2225 k\u2211\nr=1\nn\u2211\ni=1 ai[(W \u2299N)]iYrBr,i \u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 k\u2211\nr=1\nn\u2211\ni=1 \u03b2i,r[(W \u2299N)]iYr \u2225\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225\u2225 k\u2211\nr=1 \u03b2\u22a4r (W \u2299N)Yr \u2225\u2225\u2225\u2225\u2225 2\n\u2264 k\u2211\nr=1\n\u2225\u2225\u03b2\u22a4r (W \u2299N)Yr \u2225\u2225 2\n\u2264 k\u2211\nr=1\n\u2016\u03b2r\u20162 \u2016W \u2299N\u20162 \u2016Yr\u20162 .\nWe have \u2016Yr\u20162 = 1. For \u2016\u03b2r\u20162, we have\nk\u2211\nr=1\n\u2016\u03b2r\u20162 \u2264 \u221a k\n\u221a\u221a\u221a\u221a k\u2211\nr=1\n||\u03b2r||22\n= \u221a k\n\u221a\u221a\u221a\u221a k\u2211\nr=1\nn\u2211\ni=1\n( a2iB 2 r,i )\n= \u221a k\n\u221a\u221a\u221a\u221a n\u2211\ni=1\na2i\nk\u2211\nr=1\nB2r,i\n= \u221a k \u221a\u221a\u221a\u221a n\u2211\ni=1\na2i ||Bi||22.\nWe have ||Bi||2 = ||(Y\u22a4DiY)\u22121b||2 \u2264 1\u03c3min(Y\u22a4DiY) and \u2211n i=1 a 2 i = 1, so\nk\u2211\nr=1\n\u2016\u03b2r\u20162 \u2264 \u221a k\n\u221a\u221a\u221a\u221a n\u2211\ni=1\na2i ||Bi||22\n\u2264 max i\u2208[n]\n{ \u221a k\n\u03c3min(Y\u22a4DiY)\n} .\nPutting together, we have\nT 2 \u2264 max i\u2208[n]\n{ \u221a k\n\u03c3min(Y\u22a4DiY) \u2016W \u2299N\u20162\n} . (C.4)\nThe lemma follows from combining (C.3) and (C.4).\nNow we have all the ingredients to prove the update lemma.\nLemma 16. Suppose M\u2217,W satisfy all the assumptions, column orthogonal matrix Y \u2208 Rn\u00d7k is (5k\u00b5)-incoherent, and for all i \u2208 [n], Di = Diag(Wi) satisfies\n1 4 \u03bbI Y\u22a4DiY 4\u03bbI.\nThen X\u0303 \u2190 argminX\u2208Rn\u00d7k \u2225\u2225M\u2212XY\u22a4 \u2225\u2225 W satisfies\ntan \u03b8(X\u0303,U) \u2264 1 16k \u221a logn tan \u03b8(Y,V) + 16k\u03b4 \u03bb\u03c3min(M\u2217) .\nProof of Lemma 16. By Lemma 14 and Lemma 15, we have\ntan \u03b8(X\u0303,U) \u2264 ||G||2 cos \u03b8(Y,V)\u03c3min(M\u2217)\u2212 ||G||2 , (C.5)\n||G||2 \u2264 max i\u2208[n]\n{ \u03b3k3/2 \u221a \u03c1(U)\u03c1(Y)\n\u03c3min(Y\u22a4DiY) sin \u03b8(Y,V) +\n\u221a k||W \u2299N||2 \u03c3min(Y\u22a4DiY) } . (C.6)\nBy the assumptions Y\u22a4DiY \u03bb4 I, \u03c1(U) \u2264 \u00b5, \u03c1(Y) \u2264 5k\u00b5, in (C.6) we have:\n||G||2 \u2264 4 \u221a 5\u03b3k2\u00b5\n\u03bb sin \u03b8(Y,V) +\n4 \u221a k\u03b4\n\u03bb .\nPlugging this in (C.5), and noting that\n\u03b3 \u2264 \u03bb 128 \u221a 5k3\u00b5 \u221a logn , ||G||2 \u2264 1 4 \u03c3min(M \u2217), cos \u03b8(Y,V) \u2265 1 2 ,\nwe get\ntan \u03b8(X\u0303,U) \u2264 2 ||G||2 cos \u03b8(Y,V)\u03c3min(M\u2217) \u2264 1 16k \u221a logn tan \u03b8(Y,V) + 16\n\u221a k\u03b4\n\u03bb\u03c3min(M\u2217)\nas needed."}, {"heading": "C.2 Whitening", "text": "What remains is to show that the whitening step can make sure that Y has good incoherency and Oi has the desired spectral property. Recall that the whitening step consists of a SDP relaxation and a new rounding scheme to fix Y whenever Y\u22a4DiY having very small singular values. Intuitively, we want to get through the SDP relaxation, an R close to V and Ar \u2248 (Vr)\u22a4Vr \u2208 Rk\u00d7k, so that we\u2019d have the incoherency of R is close to \u00b5(V) which is bounded by \u00b5, and \u2211n r=1Wi,rAr \u2248 V\u22a4DiV \u03bbI. (Note one can not simply say when tan \u03b8(Y,U) \u2264 d, then Y\u22a4DiY is close to V\u22a4DiV. This is because ||Di||2 can be as large as npoly(logn) in our case, however, ||V\u22a4DiV||2 = O(1).) The key observation is that our randomized rounding outputs a n \u00d7 k random matrix X such that E[Xr] = Rr (Xr is the i-th row of X), E[(Xr)\u22a4(Xr)] = Ar, with the variance of (Xr) bounded by Ar \u2212 (Rr)\u22a4Rr. Therefore,\nE[Xr ] = Rr, E[X\u22a4DiX] = n\u2211\nr=1\nWi,rAr \u03bbI\nThus, X is incoherent (Note ||Xr||22 = Tr[(Xr)\u22a4(Xr)]) and ||(X\u22a4DiX)\u22121||2 is small in expectation. we can apply matrix concentration bound on X to show that the above values actually concentrate on the expectation, thus the output matrix X = QR(X) will have the required properties.\nLemma 17 (Whitening). Suppose X\u0303 is \u00b5-incoherent and satisfies tan \u03b8(X\u0303,U) \u2264 d2 where d \u2264 14k\u221alogn . Then X \u2190 WHITENING(X\u0303,W, d, \u03bb, \u03bb, \u00b5, k) satisfies with high probability: (1). 14\u03bbI X \u22a4 DiX 4\u03bbI; (2). X is (5k\u00b5)-incoherent; (3). tan \u03b8(X,U) \u2264 4dk\u221alogn.\nAs a preliminary to showing whitening works, we need to introduce a new type of random variables and a new matrix concentration bound. Another natural distribution to use is a Gaussian random vector y \u223c N (\u00b5,\u03a3). The advantage of a Rademacher vector x is that \u2016x\u20162 is always bounded, which facillitates proving concentration bounds.\nLemma 18 (Matrix Concentration). Let {xi}ni=1 be independent Rademacher random vectors in Rk with xi \u223c Rademacher(ai,\u2206i), let Tr(\u2206)max = maxi\u2208[n]{Tr(\u2206i)}, (||a||22)max = maxi\u2208[n]{||ai||22}, || \u2211n i=1 \u2206i|| \u2264 \u2206, then for every t \u2265 0,\nPr [\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nxix \u22a4 i \u2212 E\n[ n\u2211\ni=1\nxix \u22a4 i ]\u2225\u2225\u2225\u2225\u2225 \u2265 t ] \u2264 exp { \u2212 t 2\nc1 + c2t\n}\nwhere\nc1 = [2Tr(\u2206)max + (3 + k)(||a||22)max]\u2206, c2 = (k + 1)Tr(\u2206)max + 2 \u221a k(||a||22)max\u2206.\nProof of Lemma 18. Let Si \u2208 Rk\u00d7k be a matrix such that S2i = \u2206i, Note that\nE[xix \u22a4 i ] = E[(ai + Si\u03c3)(ai + Si\u03c3) \u22a4] = aia \u22a4 i + SiE[\u03c3\u03c3 \u22a4]S\u22a4i = aia \u22a4 i +\u2206i\nWe first move the random variable to center at zero: consider yi = xi \u2212 ai, define Yi = xix\u22a4i \u2212 E [ xix \u22a4 i ] = xix \u22a4 i \u2212 (aia\u22a4i +\u2206i) = yiy\u22a4i + aiy\u22a4i + yia\u22a4i \u2212\u2206i, we have: E[Yi] = 0. By yi and \u2212yi being identically distributed, we obtain E[||yi||22yia\u22a4i ] = 0,E[\u3008ai, yi\u3009yiy\u22a4i ] = 0\nTherefore, using the fact that E[yi] = 0, and E[yiy\u22a4i ] = \u2206i, we can calculate that\nE[Y2i ] = E[(yiy \u22a4 i + aiy \u22a4 i + yia \u22a4 i \u2212\u2206i)2]\n= E[||yi||22yiy\u22a4i + \u3008ai, yi\u3009yiy\u22a4i + ||yi||22yia\u22a4i \u2212 yiy\u22a4i \u2206i +||yi||22aiy\u22a4i + \u3008ai, yi\u3009aiy\u22a4i + ||yi||22aia\u22a4i \u2212 aiy\u22a4i \u2206i +\u3008ai, yi\u3009yiy\u22a4i + ||ai||22yiy\u22a4i + \u3008ai, yi\u3009yia\u22a4i \u2212 yia\u22a4i \u2206i \u2212\u2206iyiy\u22a4i \u2212\u2206iaiy\u22a4i \u2212\u2206iyia\u22a4i +\u22062i ] = E[||yi||22yiy\u22a4i ] + aia\u22a4i E[||yi||22] + ||ai||22E[yiy\u22a4i ] + E[\u3008ai, yi\u3009(aiy\u22a4i + yia\u22a4i )]\u2212\u22062i = E[||yi||22yiy\u22a4i ] +Tr(\u2206i)aia\u22a4i + ||ai||22\u2206i + aia\u22a4i \u2206i +\u2206iaia\u22a4i \u2212\u22062i\nFurthermore,\nE[||yi||22yiy\u22a4i ] = E[(\u03c3\u22a4\u2206i\u03c3)Si\u03c3\u03c3\u22a4S\u22a4i ] = SiE[(\u03c3 \u22a4\u2206i\u03c3)\u03c3\u03c3 \u22a4]S\u22a4i\nOn the other hand, For u 6= v:\n(E[(\u03c3\u22a4\u2206i\u03c3)\u03c3\u03c3 \u22a4])u,v = E\n[ \u2211\np,q\n\u03c3p(\u2206i)p,q\u03c3q\u03c3u\u03c3v\n]\n= \u2211\np,q\n(\u2206i)p,qE[\u03c3p\u03c3q\u03c3u\u03c3v]\n= 2(\u2206i)u,v\nFor u = v:\n(E[(\u03c3\u22a4\u2206i\u03c3)\u03c3\u03c3 \u22a4])u,u = E\n[ \u2211\np,q\n\u03c3p(\u2206i)p,q\u03c3q\u03c3u\u03c3u\n]\n= \u2211\np,q\n(\u2206i)p,qE[\u03c3p\u03c3q\u03c3 2 u]\n= \u2211\np\n(\u2206i)p,p = Tr(\u2206i)\nTherefore, E[(\u03c3\u22a4\u2206i\u03c3)\u03c3\u03c3 \u22a4] Tr(\u2206i)I+ 2\u2206i\nE[||yi||22yiy\u22a4i ] 2\u22062i +Tr(\u2206i)\u2206i\nTherefore, by \u22062i Tr(\u2206i)\u2206i, aia\u22a4i \u2206i +\u2206iaia\u22a4i 2||ai||22\u2206i, we obtain n\u2211\ni=1\nE[Y2i ] n\u2211\ni=1\n( \u22062i +Tr(\u2206i)\u2206i +Tr(\u2206i)aia \u22a4 i + ||ai||22\u2206i + aia\u22a4i \u2206i +\u2206iaia\u22a4i )\n[2Tr(\u2206)max + 3(||a||22)max]\u2206I+ n\u2211\ni=1\nTr(\u2206i)aia \u22a4 i\n[2Tr(\u2206)max + 3(||a||22)max]\u2206I+ n\u2211\ni=1\n||ai||22Tr(\u2206i)I\n[2Tr(\u2206)max + 3(||a||22)max]\u2206I+ (||a||22)maxTr ( n\u2211\ni=1\n\u2206i\n) I\n[2Tr(\u2206)max + 3(||a||22)max]\u2206I+ k(||a||22)max\u2206I\nMoreover,\n||Yi||2 \u2264 ||\u2206i||2 + ||yiy\u22a4i ||2 + ||aiy\u22a4i ||2 + ||yia\u22a4i ||2 = ||\u2206i||2 + 2||ai\u03c3\u22a4S\u22a4i ||2 + ||Si\u03c3\u03c3S\u22a4i ||2 \u2264 ||\u2206i||2 + 2 \u221a k(||a||22)max\u2206+ k||\u2206i||2\n\u2264 (k + 1)Tr(\u2206)max + 2 \u221a k(||a||22)max\u2206\nwhere the last inequality is due to ||\u03c3\u03c3\u22a4||2 \u2264 k. The lemma then follows by the matrix Bernstein inequality.\nNow we are ready to prove the lemma for the whitening step.\nLemma 17. Suppose M\u2217,N,W satisfy all assumptions, \u00b5-incoherent column orthogonal matrix X\u0303 \u2208 Rn\u00d7k is close to U: tan \u03b8(X\u0303,U) \u2264 d2 where d \u2264 14k\u221alogn , then X \u2190 WHITENING(X\u0303,W, d, \u03bb, \u03bb, \u00b5, k) satisfies with high probability: (1). For all i \u2208 [n], let Di = Diag(Wi), then 14\u03bbI X \u22a4 DiX 4\u03bbI; (2). X is (5k\u00b5)-incoherent; (3).\ntan \u03b8(X,U) \u2264 4dk\u221alogn.\nProof of Lemma 17. Firstly we need to show that there is a feasible solution to our SDP relaxation, and then we need to show that the output has the desired properties stated in the lemma.\n(Existence of a feasible solution) To be specific, we want to show that R = UQ, Ar = Q\u22a4(Ur)\u22a4UrQ is a feasible solution to the SDP for some orthogonal matrix Q \u2208 Rk\u00d7k .\nClearly, by setting R and Ar as above, we automatically satisfy:\n(Rr)\u22a4Rr Ar\nTr(Ar) = ||Rr||22 = ||QUr||22 = ||Ur ||22 \u2264 \u00b5k\nn n\u2211\nr=1\nAr =\nn\u2211\nr=1\nQ\u22a4(Ur)\u22a4UrQ = Q\u22a4U\u22a4UQ = I\n\u03bbI n\u2211\nr=1\nWi,rAr =\nn\u2211\nr=1\nWi,rQ \u22a4(Ur)\u22a4UrQ = Q\u22a4 ( U\u22a4Diag(Wi)U ) Q \u03bbI\nSo we only need to show that there exists orthogonal Q that UQ satisfies the distance constraints:\n||X\u0303\u22a4\u22a5UQ||2 \u2264 d, ||UQ\u2212 X\u0303||2 \u2264 d,\n||X\u0303\u22a4(UQ \u2212 X\u0303) + (UQ \u2212 X\u0303)\u22a4X\u0303||2 \u2264 d2.\nNote that sin \u03b8(X\u0303,U) \u2264 tan \u03b8(X\u0303,U), so\n||X\u0303\u22a4\u22a5UQ||2 = ||X\u0303\u22a4\u22a5U||2 = sin \u03b8(X\u0303,U) \u2264 tan \u03b8(X\u0303,U) \u2264 d/2 \u2264 d.\nMoreover, when tan \u03b8(X\u0303,U) = d2 \u2264 12 ,\n1\u2212 cos \u03b8(X\u0303,U) cos \u03b8(X\u0303,U) \u2264 sin \u03b8(X\u0303,U),\nand thus by Lemma 4, distc(X\u0303,U) \u2264 2sin \u03b8(X\u0303,U). By definition, there exits an orthogonal matrix Q such that\n||UQ \u2212 X\u0303||2 \u2264 2sin \u03b8(X\u0303,U) \u2264 d.\nFinally, since X\u0303 and UQ are orthogonal, we know that\nX\u0303\u22a4(UQ \u2212 X\u0303) + (UQ \u2212 X\u0303)\u22a4X\u0303 = \u2212(UQ\u2212 X\u0303)\u22a4(UQ \u2212 X\u0303)\nwhich implies\n||X\u0303\u22a4(UQ \u2212 X\u0303) + (UQ \u2212 X\u0303)\u22a4X\u0303||2 = ||(UQ \u2212 X\u0303)\u22a4(UQ \u2212 X\u0303)||2 \u2264 ||(UQ \u2212 X\u0303)\u22a4(UQ \u2212 X\u0303)||22 \u2264 d2.\nThis shows that the solution to our SDP exists.\n(Desired properties) Now we show that the randomly rounded solution has the required properties with high probability. We first prove some nice properties of X, and then use them to prove the properties of X.\nClaim 19. X satisfies the following properties. (a). Orthogonality property.\nPr [ ||X\u22a4X\u2212 I||2 \u2265 1\n4\n] \u2264 1\n8 .\n(b). Spectral property.\nPr [ \u2203i \u2208 [n], \u2225\u2225\u2225\u2225\u2225X \u22a4DiX\u2212 n\u2211\nr=1\nWi,rAr \u2225\u2225\u2225\u2225\u2225 2 \u2265 \u03bb 2 ] \u2264 1 8 .\n(c). Distance property.\nPr [ ||X\u0303\u22a4\u22a5X||2 \u2265 dk \u221a logn ] \u2264 1\n8 .\n(d). Incoherent property.\n\u2200r \u2208 [n], (Xr)(Xr)\u22a4 \u2264 \u00b5k(k + 1) n .\nProof of Claim 19. It is easy to verify that in expectation, the rounded solution satisfies the properties stated: (a). Orthogonality property.\nE[X\u22a4X] = n\u2211\nr=1\nE[(Xr)\u22a4Xr] = n\u2211\nr=1\n( (Rr)\u22a4Rr + SrE[\u03c3 \u22a4\u03c3]Sr ) = n\u2211\nr=1\nAr = I\nsince Xr = Rr + \u03c3Sr where Sr is a PSD matrix with S2r = Ar \u2212 (Rr)\u22a4(Rr). (b). Spectral property.\nE[X\u22a4DiX] = n\u2211\nr=1\nE[Wi,r(X r)\u22a4(Xr)] =\nn\u2211\nr=1\nWi,rAr.\n(c). Distance property. E[X] = R, E[X\u0303\u22a4\u22a5X] = X\u0303 \u22a4 \u22a5R.\n(d). Incoherent property. E[(Xr)(Xr)\u22a4] = Tr(Ar).\nTherefore, we just need to show that the random variables in (a), (b), (c), and (d) concentrate around their expectation. First consider (a). We can apply the matrix concentration lemma (18), for which we need to bound ||\u2211nr=1\u2206r||2\nwhere\u2206r = Ar\u2212(Rr)\u22a4(Rr). Note that \u2211n r=1 Ar = I, it suffices to bound \u03c3min (\u2211 r(R r)\u22a4(Rr) ) = \u03c3min ( R\u22a4R ) . Since R = R+ X\u0303\u2212 X\u0303, we have\n\u03c3min ( R\u22a4R ) = \u03c3min ( X\u0303\u22a4X\u0303+ (R\u2212 X\u0303)\u22a4(R \u2212 X\u0303) + X\u0303\u22a4(R\u2212 X\u0303) + (R \u2212 X\u0303)\u22a4X\u0303 ) .\nThen by ||R\u2212 X\u0303||2 \u2264 d, ||X\u0303\u22a4(R \u2212 X\u0303) + (R \u2212 X\u0303)\u22a4X\u0303||2 \u2264 d2 and X\u0303\u22a4X\u0303 = I, we get\n\u03c3min ( R\u22a4R ) \u2265 1\u2212 ||(R\u2212 X\u0303)\u22a4(R\u2212 X\u0303)||2 \u2212 ||X\u0303\u22a4(R\u2212 X\u0303) + (R \u2212 X\u0303)\u22a4X\u0303||2 \u2265 1\u2212 2d2.\nTherefore, \u2225\u2225\u2225\u2225\u2225 n\u2211\nr=1\n\u2206r \u2225\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225\u2225\u2225 n\u2211 r=1 Ar \u2212 n\u2211 r=1 (Rr)\u22a4(Rr) \u2225\u2225\u2225\u2225\u2225 2 = \u2225\u2225I\u2212R\u22a4R \u2225\u2225 2 \u2264 2d2.\nUsing the matrix concentration lemma (18) with \u2206 = 2d2,Tr(\u2206)max \u2264 \u00b5kn , (||a||22)max = \u00b5k n , t = 1/4, we obtain that when n is sufficiently large:\nPr [\u2225\u2225X\u22a4X\u2212 I \u2225\u2225 2 \u2265 1\n4\n] \u2264 1\n8 .\nNext consider (b). We can also apply the matrix concentration lemma (18) for each i \u2208 [n] and then take the union bound. Here, \u2206r = Wi,r ( Ar \u2212 (Rr)\u22a4(Rr) ) , so\n\u2225\u2225\u2225\u2225\u2225 n\u2211\nr=1\n\u2206r \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225\u2225\u2225 n\u2211 r=1 Wi,rAr \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u03bb.\nUsing the matrix concentration lemma (18) with \u2206 = \u03bb,Tr(\u2206)max = \u00b5k n ||W||\u221e, (||a||22)max = \u00b5k n ||W||\u221e, t = \u03bb 2 , we obtain that for any i \u2208 [n], when\n\u03bb \u2265 \u221a\n32k2\u00b5||W||\u221e logn n \u03bb,\nwe have\nPr [\u2225\u2225\u2225\u2225\u2225X \u22a4DiX\u2212 n\u2211\nr=1\nWi,rAr \u2225\u2225\u2225\u2225\u2225 2 \u2265 \u03bb 2 ] \u2264 1 8n .\nTaking the union bound leads to the desired property. Now consider (c). By triangle inequality,\n||X\u0303\u22a4\u22a5X||2 \u2264 ||X\u0303\u22a4\u22a5R||2 + ||X\u0303\u22a4\u22a5(X\u2212R)||2.\nBy the SDP, ||X\u0303\u22a4\u22a5R||2 \u2264 d, so it suffices to bound X\u0303\u22a4\u22a5(X\u2212R) = \u2211n r=1([X\u0303\u22a5] r)\u22a4(X\u2212R)r. LetZr = ([X\u0303\u22a5]r)\u22a4(X\u2212\nR)r, we have X\u0303\u22a4\u22a5(X\u2212R) = \u2211n\nr=1Zr. Furthermore, E[Zr] = 0 with \u2225\u2225\u2225\u2225\u2225E[ n\u2211\nr=1\nZrZ \u22a4 r ] \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225\u2225\u2225E[ n\u2211 r=1 (X\u2212R)r[(X\u2212R)r]\u22a4] \u2225\u2225\u2225\u2225\u2225 2 = n\u2211 r=1\nTr(\u2206r) \u2264 3d2k, \u2225\u2225\u2225\u2225\u2225E[ n\u2211\nr=1\nZ\u22a4r Zr] \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225\u2225\u2225 n\u2211 r=1 \u2206r \u2225\u2225\u2225\u2225\u2225 2 \u2264 3d2,\n||Zr||2 \u2264 2dk.\nBy Matrix Bernstein inequality, when n is sufficiently large,\nPr [ ||X\u0303\u22a4\u22a5(X\u2212R)||2 \u2265 1\n2 dk\n\u221a logn ] \u2264 1\n8 .\nThe property then follows from the triangle inequality. Finally, consider (d). We know that Xr = Rr + \u03c3Sr where Sr is a PSD matrix with S2r = Ar \u2212 (Rr)\u22a4(Rr). Therefore,\n(Xr)(Xr)\u22a4 = (Rr)(Rr)\u22a4 + \u03c3S2r\u03c3 \u22a4 \u2264 Tr(Ar) +Tr(Ar)||\u03c3||22 \u2264\n\u00b5k(k + 1)\nn .\nThis completes the proof of the claim.\nWe are now ready to prove the properties of X = QR(X), the final output of WHITENING. Assume none of the bad events in Claim 19 happen. First, by the spectral property (b) of X in the claim, we have that for any i \u2208 [n],\n\u03bb 2 I X\u22a4DiX 2\u03bbI.\nNote that ||X\u22a4X\u2212 I|| \u2264 14 , which implies that \u03c32max(X) \u2264 54 , \u03c32min(X) \u2265 34 . Therefore, for any i \u2208 [n],\nX \u22a4 DiX\n1\n\u03c32max(X) X\u22a4DiX\n\u03bb 4 I\nand\nX \u22a4 DiX\n1\n\u03c32min(X) X\u22a4DiX 4\u03bbI.\nNext, note that\nsin \u03b8(X, X\u0303) = ||X\u0303\u22a4\u22a5X||2 \u2264 1 \u03c3min(X) ||X\u0303\u22a4\u22a5X||2 \u2264 3 2 ||X\u0303\u22a4\u22a5X||2 \u2264 3 2 dk\n\u221a logn.\nSince sin \u03b8(X\u0303,U) \u2264 tan \u03b8(X\u0303,U) \u2264 d2 , we have\nsin \u03b8(X,U) \u2264 d 2 + 3 2 dk\n\u221a logn \u2264 2dk \u221a log n \u2264 1/2.\nWhen sin\u03b8 \u2264 1/2, tan\u03b8 \u2264 2sin\u03b8. So\ntan \u03b8(X,U) \u2264 2sin \u03b8(X,U) \u2264 4dk \u221a logn.\nFinally, for incoherence, we know that \u03c1(X) \u2264 \u00b5(k+1), \u03c3min(X) \u2265 34 . Then the output X satisfies \u03c1(X) \u2264 4\u03c1(X) \u2264 5\u00b5k.\nNote: since all the property of output X can be tested in polynomial time (for (3) we can test it using the input matrix X\u0303 because tan \u03b8(X\u0303,U) \u2264 d2 ), we can run the whitening algorithm for O(log(1/\u03b1)) times (using fresh randomness for the choice of X) and we will have success probability 1\u2212 \u03b1."}, {"heading": "C.3 Final result", "text": "Theorem 20. If M\u2217,W satisfy assumptions (A1)-(A3), and\n||W||\u221e = O ( \u03bb2n\nk2\u00b5\u03bb logn\n) , \u03b3 = O ( \u03bb\u03c3min(M \u2217)\nk3\u00b5 \u221a logn\n) ,\nthen after O(log(1/\u01eb)) rounds Algorithm 5 outputs a matrix M\u0303 that with probability \u2265 1\u2212 1/n satisfies\n||M\u0303\u2212M\u2217||2 \u2264 O ( k3/2 \u221a logn\n\u03bb\u03c3min(M\u2217)\n) ||W \u2299N||2 + \u01eb."}, {"heading": "The running time is polynomial in n and log(1/\u01eb).", "text": "The theorem is stated in its full generality. To emphasize the dependence on the matrix size n, the rank k and the incoherency \u00b5, we can consider a specific range of parameter values where the other parameters (the lower/upper spectral bound, the condition number of M\u2217) are constants, which gives a corollary which is easier to parse. Also, these parameter values show that we can handle a wider range of parameters than the simple algorithm with the clipping as a whitening step.\nCorollary 21. Suppose \u03bb, \u03bb and \u03c3min(M\u2217) are all constants, and T = O(log(1/\u01eb)). Furthermore,\n\u2016W\u2016\u221e = O (\nn\nk2\u00b5 logn\n) , \u03b3 = O ( 1\nk3\u00b5 \u221a logn\n) .\nThen with probability \u2265 1\u2212 1/n,\n||M\u0303\u2212M\u2217||2 \u2264 O ( k3/2 \u221a logn ) ||W \u2299N||2 + \u01eb.\nWe now consider proving the theorem. After proving these lemmas, the proof is rather immediate. Define the following two quantities:\nval = 4k \u221a logn, c =\n64 \u221a k\n\u03bb\u03c3min(M\u2217) val =\n256k3/2 \u221a logn\n\u03bb\u03c3min(M\u2217) .\nWe just need to show that tan \u03b8(Xt,U) \u2264 12t + c\u03b4 for every t \u2265 1, and tan \u03b8(Yt,U) \u2264 12t + c\u03b4 for every t > 1. We will prove it by induction.\n(a). After initialization, by Lemma 8 and 17, we have\ntan \u03b8(Y1,V) \u2264 4kd1 \u221a logn = d1val = 1\n2 + c\u03b4.\n(b). Suppose tan \u03b8(Xt,U) and tan \u03b8(Yt,V) \u2264 12t + c\u03b4 is true for t, and consider the iterates at step t+ 1. Since Yt is given by WHITENING, by Lemma 17, we know that 14\u03bbI Y \u22a4 t DiYt 4\u03bbI and Yt is (5k\u00b5)-incoherent.\nTherefore, applying Lemma 16 we have\ntan \u03b8(X\u0303t+1,U) \u2264 tan \u03b8(Yt,V)\n4val +\n16 \u221a k\u03b4\n\u03bb\u03c3min(M\u2217)\n\u2264 1 2t+2val +\n( c\n4val +\n16 \u221a k\n\u03bb\u03c3min(M\u2217)\n) \u03b4\n\u2264 1 2t+2val + 32\n\u221a k\n\u03bb\u03c3min(M\u2217) \u03b4.\nNow, we know that tan \u03b8(X\u0303t+1,U) \u2264 dt+12 for dt+1 = 12t+1val + 64 \u221a k \u03bb\u03c3min(M\u2217) \u03b4. By Lemma 17,\ntan \u03b8(Xt+1,U) \u2264 dt+1val\n\u2264 ( 1\n2t+1val +\n64 \u221a k\n\u03bb\u03c3min(M\u2217) \u03b4\n) val\n\u2264 1 2t+1 + c\u03b4.\nUsing exactly the same argument we can show that tan \u03b8(Yt+1,V) \u2264 12t+1 + c\u03b4. Then the theorem follows by bounding \u2016M\u2217 \u2212 M\u0303\u20162 by tan \u03b8(YT+1,V), tan \u03b8(XT+1,U) using the triangle inequality and the spectral property of W. For simplicity, let X = XT+1 and Y = YT+1. By definition, we know that there exists Qx and Qy such that XQx = U + \u2206x and XQy = V + \u2206y where \u2016\u2206x\u20162 = O(tan \u03b8(X,U)) and \u2016\u2206y\u20162 = O(tan \u03b8(Y,V)).\n\u2016W \u2299 (M \u2212X\u03a3Y\u22a4)\u20162 \u2264 \u2016W \u2299 (M\u2212XQx\u03a3QyY\u22a4)\u20162 \u2264 \u2016W \u2299 (M\u2217 +N\u2212XQx\u03a3QyY\u22a4)\u20162\n\u2264 \u2016W \u2299 (M\u2217 +\u2212XQx\u03a3QyY\u22a4)\u20162 + \u2016W \u2299N\u20162 \u2264 \u2016W \u2299 (M\u2217 +\u2212XQx\u03a3QyY\u22a4)\u20162 + \u2016W \u2299N\u20162.\nOn the other hand,\n\u2016W \u2299 (M\u2212X\u03a3Y\u22a4)\u20162 \u2265 \u2016W \u2299 (M\u2217 \u2212X\u03a3Y\u22a4)\u20162 \u2212 \u2016W \u2299N\u20162.\nTherefore,\n\u2016W \u2299 (M\u2217 \u2212X\u03a3Y\u22a4)\u20162 \u2264 \u2016W \u2299 (M\u2217 \u2212XQx\u03a3QyY\u22a4)\u20162 + 2\u2016W\u2299N\u20162 = O(tan \u03b8(X,U) + tan \u03b8(Y,V)) +O(\u2016W \u2299N\u20162).\nDefine \u2206 = M\u2217 \u2212X\u03a3Y\u22a4 and \u2206\u2032 = XQx\u03a3Q\u22a4y Y\u22a4 \u2212X\u03a3Y\u22a4, and note that the difference between the two is O(tan \u03b8(X,U) + tan \u03b8(Y,V)).\n\u2016\u2206\u20162 \u2264 \u2016W \u2299\u2206\u20162 + \u2016(W \u2212E)\u2299\u2206\u20162 \u2264 \u2016W \u2299\u2206\u20162 + \u2016(W \u2212E)\u2299\u2206\u2032\u20162 +O(tan \u03b8(X,U) + tan \u03b8(Y,V)).\nSo now it is sufficient to show that \u2016(W \u2212 E) \u2299\u2206\u2032\u20162 \u2264 c\u2016\u2206\u20162 for a small c < 1/2. Now we apply Lemma 5. Let Z = Qx\u03a3Q\u22a4y \u2212\u03a3.\n\u2016(W \u2212E)\u2299\u2206\u2032\u20162 = \u2016(W \u2212E)\u2299 (XQx\u03a3Q\u22a4y Y\u22a4 \u2212X\u03a3Y\u22a4)\u20162 = \u2016(W \u2212E)\u2299 (XZY\u22a4)\u20162 \u2264 c\u2016Z\u20162\nfor some small c < 1/2, since \u03b3 is small and X and Y are incoherent. Note that X and Y are projections, so \u2016Z\u20162 = \u2016XZY\u22a4\u20162, then\n\u2016(W \u2212E)\u2299\u2206\u2032\u20162 \u2264 c\u2016\u2206\u20162.\nCombining all things we have \u2016\u2206\u20162 = O(tan \u03b8(X,U) + tan \u03b8(Y,V)) + O(\u2016W \u2299 N\u20162) = O(tan \u03b8(X,U) + tan \u03b8(Y,V)), which completes the proof."}, {"heading": "D Empirical verification of the spectral gap property", "text": "Experiments on the performance of the alternating minimization can be found in related work (e.g., [Lu et al., 1997, Srebro and Jaakkola, 2003]). Therefore, we focus on verifying the key assumption, i.e., the spectral gap property of the weight matrix (Assumption (A2)).\nHere we consider the application of computing word embeddings by factorizing the co-occurrence matrix between the words, which is one of the state-of-the-art techniques for mapping words to low-dimensional vectors (about 300 dimension) in natural language processing. There are many variants (e.g., [Levy and Goldberg, 2014, Pennington et al., 2014, Arora et al., 2016]); we consider the following simple approach. Let X be the co-occurrence matrix, where Xi,j is the number of times that word i and word j appear together within a window of small size (we use size 10 here) in the given corpus. Then the word embedding by weighted low rank problem is\nmin V\n\u2211\ni,j\nf(Xi,j)\n( log ( Xi,j\nX\n) \u2212 \u3008Vi,Vj\u3009 )2\nwhere X = \u2211\ni,j Xi,j , Vi\u2019s are the vectors for the words, and f(x) = max{Xi,j , 100} for a large corpus and f(x) = max{Xi,j , 10} for a small corpus.\nWe focus on the weight matrix Wi,j = f(Xi,j). It has been observed that using Xi,j as weights is roughly the maximum likelihood estimator under certain probabilistic model and is better than using uniform weights. It has also been verified that using the truncated weight f(Xi,j) is better than using Xi,j . Our experiments suggest that f(Xi,j) is better partially due to the requirement that the weight matrix should have the spectral gap property for the algorithm to succeed.\nWe consider two large corpora (Wikipedia corpus [Wikimedia, 2012], about 3G tokens; a subset of Commoncrawl corpus [Buck et al., 2014], about 20G tokens). For each corpus, we pick the top n words (n = 500, 1000, . . . , 5000) and compute the spectral gap \u2016W \u2212 E\u20162 where W is the weight matrix corresponding to the words, and E is the\nall-one matrix. Note that a scaling of W does not affect the problem, so we enumerate different scaling of W (from 2\u221220 to 210) and plot the best spectral gap. We compare the two variants: with threshold (Wi,j = f(Xi,j)), and without threshold (Wi,j = Xi,j ).\nThe results are shown in Figure 1. Without threshold, there is almost no spectral gap. With threshold, there is a decent gap, though with the increase of the matrix size, the gap become smaller because larger vocabulary includes more uneven co-occurrence entries and thus more noise. This suggests that thresholding can make the weight matrix nicer for the algorithm, and thus leads to better performance."}], "references": [{"title": "A latent variable model approach to pmi-based word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "To appear in Transactions of the Association for Computational Linguistics,", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Universal matrix completion", "author": ["Srinadh Bhojanapalli", "Prateek Jain"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Bhojanapalli and Jain.,? \\Q2014\\E", "shortCiteRegEx": "Bhojanapalli and Jain.", "year": 2014}, {"title": "Tighter low-rank approximation via sampling the leveraged element", "author": ["Srinadh Bhojanapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2015}, {"title": "Dropping convexity for faster semi-definite optimization", "author": ["Srinadh Bhojanapalli", "Anastasios Kyrillidis", "Sujay Sanghavi"], "venue": "arXiv preprint arXiv:1509.03917,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2015}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen"], "venue": "In Proceedings of the Language Resources and Evaluation Conference,", "citeRegEx": "Buck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Candes", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes and Plan.,? \\Q2010\\E", "shortCiteRegEx": "Candes and Plan.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2010}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel J Candes", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Candes et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Candes et al\\.", "year": 1985}, {"title": "Efficient computation of robust weighted low-rank matrix approximations using the l 1 norm", "author": ["Anders Eriksson", "Anton van den Hengel"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Eriksson and Hengel.,? \\Q2012\\E", "shortCiteRegEx": "Eriksson and Hengel.", "year": 2012}, {"title": "Spectral techniques applied to sparse random graphs", "author": ["Uriel Feige", "Eran Ofek"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Feige and Ofek.,? \\Q2005\\E", "shortCiteRegEx": "Feige and Ofek.", "year": 2005}, {"title": "Nuclear magnetic resonance and its applications to living systems", "author": ["David G Gadian"], "venue": null, "citeRegEx": "Gadian.,? \\Q1982\\E", "shortCiteRegEx": "Gadian.", "year": 1982}, {"title": "Low-rank matrix approximation with weights or missing data is np-hard", "author": ["Nicolas Gillis", "Fran\u00e7ois Glineur"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Gillis and Glineur.,? \\Q2011\\E", "shortCiteRegEx": "Gillis and Glineur.", "year": 2011}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["David Gross"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Gross.,? \\Q2011\\E", "shortCiteRegEx": "Gross.", "year": 2011}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Marcus Hardt"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Hardt.,? \\Q2014\\E", "shortCiteRegEx": "Hardt.", "year": 2014}, {"title": "Deterministic algorithms for matrix completion", "author": ["Eyal Heiman", "Gideon Schechtman", "Adi Shraibman"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Heiman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Heiman et al\\.", "year": 2014}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Matrix completion from noisy entries", "author": ["Raghunandan Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Keshavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2009}, {"title": "Matrix completion from any given set of observations", "author": ["Troy Lee", "Adi Shraibman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lee and Shraibman.,? \\Q2013\\E", "shortCiteRegEx": "Lee and Shraibman.", "year": 2013}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving one-class collaborative filtering by incorporating rich user information", "author": ["Yanen Li", "Jia Hu", "ChengXiang Zhai", "Ye Chen"], "venue": "In Proceedings of the 19th ACM international conference on Information and knowledge management,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Weighted low-rank approximation of general complex matrices and its application in the design of 2-d digital filters", "author": ["W-S Lu", "S-C Pei", "P-H Wang"], "venue": "Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on,", "citeRegEx": "Lu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Lu et al\\.", "year": 1997}, {"title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Negahban and Wainwright.,? \\Q2012\\E", "shortCiteRegEx": "Negahban and Wainwright.", "year": 2012}, {"title": "Orthogonal representations over finite fields and the chromatic number", "author": ["Ren\u00e9 Peeters"], "venue": "of graphs. Combinatorica,", "citeRegEx": "Peeters.,? \\Q1996\\E", "shortCiteRegEx": "Peeters.", "year": 1996}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Weighted low rank approximations with provable guarantees", "author": ["Ilya Razenshteyn", "Zhao Song", "David Woodruff"], "venue": "In Proceedings of the 48th Annual Symposium on the Theory of Computing,", "citeRegEx": "Razenshteyn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Razenshteyn et al\\.", "year": 2016}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Recht.,? \\Q2011\\E", "shortCiteRegEx": "Recht.", "year": 2011}, {"title": "Weighted low-rank approximations", "author": ["Nathan Srebro", "Tommi Jaakkola"], "venue": "In Proceedings of the 20th International Conference on Machine Learning", "citeRegEx": "Srebro and Jaakkola.,? \\Q2003\\E", "shortCiteRegEx": "Srebro and Jaakkola.", "year": 2003}, {"title": "Guaranteed matrix completion via nonconvex factorization", "author": ["Ruoyu Sun", "Zhi-Quan Luo"], "venue": "In IEEE 56th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Sun and Luo.,? \\Q2015\\E", "shortCiteRegEx": "Sun and Luo.", "year": 2015}, {"title": "Perturbation bounds in connection with singular value decomposition", "author": ["Per-\u00c5ke Wedin"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Wedin.,? \\Q1972\\E", "shortCiteRegEx": "Wedin.", "year": 1972}, {"title": "Maximum likelihood multivariate calibration", "author": ["Peter D Wentzell", "Darren T Andrews", "Bruce R Kowalski"], "venue": "Analytical chemistry,", "citeRegEx": "Wentzell et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wentzell et al\\.", "year": 1997}, {"title": "2016]); we consider the following simple approach", "author": ["Arora"], "venue": "Let X be the co-occurrence matrix,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": ", [Levy and Goldberg, 2014]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 27, "context": "Even for collaborative filtering, which is typically modeled as a matrix completion problem that assigns weight 1 on sampled entries and 0 on non-sampled entries, one can achieve better results when allowing non-binary weights [Srebro and Jaakkola, 2003].", "startOffset": 227, "endOffset": 254}, {"referenceID": 12, "context": "Moreover, general weighted low-rank approximation is NP-hard, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].", "startOffset": 108, "endOffset": 134}, {"referenceID": 1, "context": "In this line of work the state-of-the-art is [Bhojanapalli and Jain, 2014], who proved recovery guarantees under the assumptions that the ground truth has a strong version of incoherence and the weight matrix has a sufficiently large spectral gap.", "startOffset": 45, "endOffset": 74}, {"referenceID": 1, "context": "In this paper, we provide the first theoretical guarantee for weighted low-rank approximation via alternating minimization, under assumptions generalizing those in [Bhojanapalli and Jain, 2014].", "startOffset": 164, "endOffset": 193}, {"referenceID": 28, "context": "Furthermore, combining our insight that the spectral property only need to hold in an average sense with the framework in [Sun and Luo, 2015], one can show provable guarantees for the family of algorithms analyzed there, including stochastic gradient descent.", "startOffset": 122, "endOffset": 141}, {"referenceID": 12, "context": "On the other hand, weighted low-rank approximation is NP-hard in the worst case, even when the ground truth is a rank-1 matrix [Gillis and Glineur, 2011].", "startOffset": 127, "endOffset": 153}, {"referenceID": 25, "context": "On the theoretical side, the only result we know of is [Razenshteyn et al., 2016], who provide a fixed-parameter tractability result when additionally the weight matrix is low-rank.", "startOffset": 55, "endOffset": 81}, {"referenceID": 23, "context": "It is known that matrix completion is NP-hard in the case when the k = 3 [Peeters, 1996].", "startOffset": 73, "endOffset": 88}, {"referenceID": 1, "context": "In this line the state-of-the-art is [Bhojanapalli and Jain, 2014], where the support of the observation is a d-regular expander such that the weight matrix has a sufficiently large spectral gap.", "startOffset": 37, "endOffset": 66}, {"referenceID": 22, "context": "We also mention [Negahban and Wainwright, 2012] who consider random sampling, but one that is not uniformly random across the entries.", "startOffset": 16, "endOffset": 47}, {"referenceID": 2, "context": "Assuming that the matrix is incoherent and the observed entries are chosen uniformly at random, Cand\u00e8s and Recht [2009] showed that nuclear norm convex relation can recover an n\u00d7n rank-k matrix using m = O(nk log(n)) entries.", "startOffset": 96, "endOffset": 120}, {"referenceID": 2, "context": "Candes and Plan [2010] relaxed the assumption to tolerate noise and showed the nuclear norm convex relaxation can lead to a solution such that the Frobenius norm of the error matrix is bounded by O( \u221a n3/m) times that of the noise matrix.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": ", 2014, Lee and Shraibman, 2013, Bhojanapalli and Jain, 2014]. In this line the state-of-the-art is [Bhojanapalli and Jain, 2014], where the support of the observation is a d-regular expander such that the weight matrix has a sufficiently large spectral gap. However, it only works for binary weights, and is for a nuclear norm convex relaxation and does not incorporate noise. Recently, there is an increasing interest in analyzing non-convex optimization techniques for matrix completion. In two seminal papers [Jain et al., 2013, Hardt, 2014], it was shown that with an appropriate SVD-based initialization, the alternating minimization algorithm (with a few modifications) recovers the ground-truth. These results are for random binary weight matrix and crucially rely on re-sampling (i.e., using independent samples at each iteration), which is inherently not possible for the setting studied in this paper. More recently, Sun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion without re-sampling.", "startOffset": 33, "endOffset": 947}, {"referenceID": 1, "context": "Our assumption is also a generalization of the one in [Bhojanapalli and Jain, 2014], which requires W to be d-regular expander-like (i.", "startOffset": 54, "endOffset": 83}, {"referenceID": 1, "context": "The final assumption (A3) is a generalization of the assumption A2 in [Bhojanapalli and Jain, 2014] that, intuitively, requires the singular vectors to satisfy RIP (restricted isometry property).", "startOffset": 70, "endOffset": 99}, {"referenceID": 7, "context": "They viewed it as a stronger version of incoherence, discussed the necessity and showed that it is implied by the strong incoherence property assumed in [Cand\u00e8s and Tao, 2010].", "startOffset": 153, "endOffset": 175}, {"referenceID": 5, "context": "5} exact recovery ours (SVD init) real yes yes yes 1 \u03bc3/2k2 \u2016\u2206\u20162 = O (k) \u2016W \u2299N\u20162 + \u01eb ours (random init) real yes yes yes 1 \u03bc2k5/2 \u2016\u2206\u20162 = O (k) \u2016W \u2299N\u20162 + \u01eb Table 1: Comparison with related work on matrix completion: (1) [Candes and Plan, 2010]; (2) [Keshavan et al.", "startOffset": 219, "endOffset": 242}, {"referenceID": 17, "context": "5} exact recovery ours (SVD init) real yes yes yes 1 \u03bc3/2k2 \u2016\u2206\u20162 = O (k) \u2016W \u2299N\u20162 + \u01eb ours (random init) real yes yes yes 1 \u03bc2k5/2 \u2016\u2206\u20162 = O (k) \u2016W \u2299N\u20162 + \u01eb Table 1: Comparison with related work on matrix completion: (1) [Candes and Plan, 2010]; (2) [Keshavan et al., 2009]; (3) [Bhojanapalli and Jain, 2014]; (4) [Hardt, 2014].", "startOffset": 248, "endOffset": 271}, {"referenceID": 1, "context": ", 2009]; (3) [Bhojanapalli and Jain, 2014]; (4) [Hardt, 2014].", "startOffset": 13, "endOffset": 42}, {"referenceID": 14, "context": ", 2009]; (3) [Bhojanapalli and Jain, 2014]; (4) [Hardt, 2014].", "startOffset": 48, "endOffset": 61}, {"referenceID": 28, "context": "(5) [Sun and Luo, 2015].", "startOffset": 4, "endOffset": 23}, {"referenceID": 10, "context": ", [Feige and Ofek, 2005]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 6, "context": "The seminal paper [Cand\u00e8s and Recht, 2009] showed that a nuclear norm convex relaxation approach can recover the ground truth matrix usingm = O(nk log n) entries chosen uniformly at random and without noise.", "startOffset": 18, "endOffset": 42}, {"referenceID": 7, "context": "The sample size was improved to O(nk log n) in [Cand\u00e8s and Tao, 2010] and then O(nk logn) in subsequent papers.", "startOffset": 47, "endOffset": 69}, {"referenceID": 4, "context": "Candes and Plan [2010] generalized the result to the case with noise: the same convex program using m = O(nk log n) entries recovers a matrix M\u0303 s.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Candes and Plan [2010] generalized the result to the case with noise: the same convex program using m = O(nk log n) entries recovers a matrix M\u0303 s.t. \u2016M\u0303\u2212M\u2217\u2016F \u2264 (2 + 4 \u221a (2 + p)n/p)\u2016N\u03a9\u2016F where p = m/n and N\u03a9 is the noise projected on the observed entries. Keshavan et al. [2009] showed that withm = O(n\u03bck logn), one can recover a matrix M\u0303 such that \u2225\u2225M\u2217 \u2212 M\u0303 \u2225\u2225 F = O ( n \u221a k m \u2016N\u03a9\u20162 ) by an optimization over a Grassmanian manifold.", "startOffset": 0, "endOffset": 279}, {"referenceID": 1, "context": "Bhojanapalli and Jain [2014] relaxed the assumption that the entries are randomly sampled.", "startOffset": 0, "endOffset": 29}, {"referenceID": 14, "context": "Hardt [2014] showed that with an appropriate initialization alternating minimization recovers the ground truth approximately.", "startOffset": 0, "endOffset": 13}, {"referenceID": 14, "context": "Hardt [2014] showed that with an appropriate initialization alternating minimization recovers the ground truth approximately. Precisely, they assumed N satisfies: (1). \u03bc(N) . \u03c3min(M\u2217)2;(2). \u2016N\u2016\u221e \u2264 \u03bc n\u2016M\u2016F . Then, he shows that log(n\u01eb logn) alternating minimization steps recover a matrix M\u0303 such that \u2016M\u0303\u2212M\u2217\u2016F \u2264 \u01eb\u2016M\u2016F provided that pn \u2265 k(k + log(n/\u01eb))\u03bc \u00d7 ( \u2016M\u2016F+\u2016N\u2016F /\u01eb \u03c3k )2 ( 1\u2212 \u03c3k+1 \u03c3k )5 where \u03c3k is the k-th singular value of the groundtruth matrix. The parameter \u03b3 corresponding to the case considered there would be roughly O( 1 k \u221a \u03bc logn ). While their algorithm has a good tolerance to noise, N is assumed to have special structure for him that we do not assume in our setting. Sun and Luo [2015] proved recovery guarantees for a family of algorithms including alternating minimization on matrix completion.", "startOffset": 0, "endOffset": 708}, {"referenceID": 28, "context": "We note that [Sun and Luo, 2015] only needs sampling before the algorithm starts and does not need re-sampling in different iterations, but still relies on the randomness in the sampled entries.", "startOffset": 13, "endOffset": 32}, {"referenceID": 13, "context": "Keshavan et al. [2009] analyzed optimization over a Grassmanian manifold, which uses the fact that E[W \u2299 S] = S for any matrix S.", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "Since ||W \u2299 N||2 \u2264 \u03b4 can be regarded as small, the idea is to show that W \u2299M\u2217 is close to M\u2217 in spectral norm and then apply Wedin\u2019s theorem [Wedin, 1972].", "startOffset": 141, "endOffset": 154}, {"referenceID": 29, "context": "By our assumptions we know that ||W \u2299N||2 \u2264 \u03b4 which we are thinking of as small, so the idea is to show that W \u2299M\u2217 is close to M\u2217 in spectral norm, then by Wedin\u2019s theorem [Wedin, 1972] we will have X,Y are close to U,V.", "startOffset": 172, "endOffset": 185}, {"referenceID": 29, "context": "Lemma 6 (Wedin\u2019s Theorem [Wedin, 1972]).", "startOffset": 25, "endOffset": 38}, {"referenceID": 4, "context": "We consider two large corpora (Wikipedia corpus [Wikimedia, 2012], about 3G tokens; a subset of Commoncrawl corpus [Buck et al., 2014], about 20G tokens).", "startOffset": 115, "endOffset": 134}], "year": 2016, "abstractText": "Many applications require recovering a ground truth low-rank matrix from noisy observations of the entries, which in practice is typically formulated as a weighted low-rank approximation problem and solved by non-convex optimization heuristics such as alternating minimization. In this paper, we provide provable recovery guarantee of weighted low-rank via a simple alternating minimization algorithm. In particular, for a natural class of matrices and weights and without any assumption on the noise, we bound the spectral norm of the difference between the recovered matrix and the ground truth, by the spectral norm of the weighted noise plus an additive error that decreases exponentially with the number of rounds of alternating minimization, from either initialization by SVD or, more importantly, random initialization. These provide the first theoretical results for weighted low-rank via alternating minimization with non-binary deterministic weights, significantly generalizing those for matrix completion, the special case with binary weights, since our assumptions are similar or weaker than those made in existing works. Furthermore, this is achieved by a very simple algorithm that improves the vanilla alternating minimization with a simple clipping step. The key technical challenge is that under non-binary deterministic weights, na\u0131\u0308ve alternating steps will destroy the incoherence and spectral properties of the intermediate solutions, which are needed for making progress towards the ground truth. We show that the properties only need to hold in an average sense and can be achieved by the clipping step. We further provide an alternating algorithm that uses a whitening step that keeps the properties via SDP and Rademacher rounding and thus requires weaker assumptions. This technique can potentially be applied in some other applications and is of independent interest.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}