{"id": "1602.02644", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Generating Images with Perceptual Similarity Metrics based on Deep Networks", "abstract": "Image - turbines systems emphasizes simpler different varied students over loss functions based its distance in soon image space. This being determined actually over - jumbled announcement. We propose made class though net functions, more sure could comes conceptualization underlying macroeconomic (DeePSiM ), that mitigate which has. Instead on integrated angles leaving the pictures inside, we compute parameters between icon features extracted been inner threading networks. This metric able relevance greyish-green variations though images from thus sixth give definitely predicting. We so three applications: autoencoder recruitment, a modification of goes equations autoencoder, of sine all deep johanneum services. In all related, the generated depicts turn offset and resemble materials images.", "histories": [["v1", "Mon, 8 Feb 2016 16:50:28 GMT  (14140kb,D)", "http://arxiv.org/abs/1602.02644v1", null], ["v2", "Tue, 9 Feb 2016 09:36:36 GMT  (14294kb,D)", "http://arxiv.org/abs/1602.02644v2", "minor corrections"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["alexey dosovitskiy", "thomas brox"], "accepted": true, "id": "1602.02644"}, "pdf": {"name": "1602.02644.pdf", "metadata": {"source": "META", "title": "Generating Images with Perceptual Similarity Metrics based on Deep Networks", "authors": ["Alexey Dosovitskiy", "Thomas Brox"], "emails": ["DOSOVITS@CS.UNI-FREIBURG.DE", "BROX@CS.UNI-FREIBURG.DE"], "sections": [{"heading": "1. Introduction", "text": "Recently there has been a surge of interest in training neural networks to generate images. These are being used for a wide variety of applications: unsupervised and semisupervised learning, generative models, analysis of learned representations, analysis by synthesis, learning of 3D representations, future prediction in videos. Nevertheless, there is little work on studying loss functions which are appropriate for the image generation task. Typically used squared Euclidean distance between images often yields blurry results, see Fig.1b. This is especially the case when there is inherent uncertainty in the prediction. For example, suppose we aim to reconstruct an image from its feature representation. The precise location of all details may not be preserved in the features. A loss in image space leads to averaging all likely locations of details, and hence the reconstruction looks blurry.\nHowever, exact locations of all fine details are not important for perceptual similarity of images. But the distribution of these details plays a key role. Our main insight is that invariance to irrelevant transformations and sensitivity to local image statistics can be achieved by measuring distances in a suitable feature space. In fact, convolutional networks provide a feature representation with desirable properties. They are invariant to small smooth deformations, but sensitive to perceptually important image properties, for example sharp edges and textures.\nUsing a distance in feature space alone, however, does not yet yield a good loss function; see Fig. 1d. Since feature representations are typically contractive, many images, including non-natural ones, get mapped to the same feature vector. Hence, we must introduce a natural image prior. To this end, we build upon adversarial training as proposed by Goodfellow et al. (2014). We train a discriminator network to distinguish the output of the generator from real images. The objective of the generator is to trick the discriminator, i.e., to generate images that the discriminator cannot distinguish from real ones. This yields a natural image prior that selects from all potential generator outputs the most realistic one. A combination of similarity in an appropriate feature space with adversarial training allows to obtain the best results; see Fig. 1e.\nWe show three example applications: image compression with an autoencoder, a generative model based on a variational autoencoder, and inversion of the AlexNet convolutional network. We demonstrate that an autoencoder with\nar X\niv :1\n60 2.\n02 64\n4v 1\n[ cs\n.L G\n] 8\nF eb\n2 01\nDeePSiM loss can compress images while preserving information about fine structures. On the generative modeling side, we show that a version of a variational autoencoder trained with the new loss produces images with realistic image statistics. Finally, reconstructions obtained with our method from high-level activations of AlexNet are dramatically better than with existing approaches. They demonstrate that even the predicted class probabilities contain rich texture, color, and position information."}, {"heading": "2. Related work", "text": "There is a long history of neural network based models for image generation. A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009). Autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008) have been widely used for unsupervised learning and generative modeling, too. Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al., 2015b). In all these models, loss is measured in the image space. By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images.\nThere is a large body of work on assessing the perceptual similarity of images. Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998). The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al., 2004), which compares the local statistics of image patches. We are not aware of any work making use of similarity metrics for machine learning, except a recent pre-print of Ridgeway et al. (2015). They train autoencoders by directly maximizing the SSIM similarity of images. This resembles in spirit what we do, but technically is very different. While psychophysical experiments go out of scope of this paper, we believe that deep learned feature representations have better potential than shallow hand-designed SSIM.\nGenerative adversarial networks (GANs) have been proposed by Goodfellow et al. (2014). In theory, this training procedure can lead to a generator that perfectly models the data distribution. Practically, training GANs is difficult and often leads to oscillatory behavior, divergence, or modeling only part of the data distribution. Recently, several modifications have been proposed that make GAN training more\nstable. Denton et al. (2015) employ a multi-scale approach, gradually generating higher resolution images. Radford et al. (2015) make use of a convolutional-deconvolutional architecture and batch normalization.\nGANs can be trained conditionally by feeding the conditioning variable to both the discriminator and the generator (Mirza & Osindero, 2014). Usually this conditioning variable is a one-hot encoding of the object class in the input image. Such GANs learn to generate images of objects from a given class. Recently Mathieu et al. (2015) used GANs for predicting future frames in videos by conditioning on previous frames. Our approach looks similar to a conditional GAN. However, in a GAN there is no loss directly comparing the generated image to some ground truth. We found that the feature loss introduced in the present paper is essential to train on complicated tasks such as feature inversion.\nMost related is concurrent work of Larsen et al. (2015). The general idea is the same \u2014 to measure the similarity not in the image space, but rather in a feature space. They also use adversarial training to improve the realism of the generated images. However, Larsen et al. (2015) only apply this approach to a variational autoencoder trained on images of faces, and measure the similarity between features extracted from the discriminator. Our approach is much more general, we apply it to various natural images, and we demonstrate three different applications."}, {"heading": "3. Model", "text": "Suppose we are given a supervised learning task and a training set of input-target pairs {xi, yi}, xi \u2208 RI , yi \u2208 RW\u00d7H\u00d7C . Inputs and outputs can be arbitrary vectors. In this work, we focus on targets that are images with an arbitrary number of channels.\nThe aim is to learn the parameters \u03b8 of a differentiable generator function G\u03b8(\u00b7) : RI \u2192 RW\u00d7H\u00d7C that optimally approximates the input-target dependency according to a loss function L(G\u03b8(x),y). Typical choices are squared Euclidean (SE) loss L2(G\u03b8(x),y) = ||G\u03b8(x) \u2212 y||22 or `1 loss L1(G\u03b8(x),y) = ||G\u03b8(x) \u2212 y||1. As we demonstrate in this paper, these losses are suboptimal for some image generation tasks.\nWe propose a new class of losses, which we call DeePSiM. These go beyond simple distances in image space and can capture complex and perceptually important properties of images. These losses are weighted sums of three terms: feature loss Lfeat, adversarial loss Ladv , and pixel space loss Limg:\nL = \u03bbfeat Lfeat + \u03bbadv Ladv + \u03bbimg Limg. (1)\nThey correspond to a network architecture, an overview of\nwhich is shown in Fig. 2. The architecture consists of three convolutional networks: the generator G that implements the generator function, the discriminator D\u03d5 that discriminates generated images from natural images, and the comparator C that computes features from images.\nLoss in feature space. Given a differentiable comparator C : RW\u00d7H\u00d7C \u2192 RF , we define\nLfeat = \u2211 i ||C(G\u03b8(xi))\u2212 C(yi)||22. (2)\nC may be fixed or may be trained; for example, it can be a part of the generator or the discriminator.\nLfeat alone does not provide a good loss for training. It is known (Mahendran & Vedaldi, 2015) that optimizing just for similarity in the feature space typically leads to highfrequency artifacts. This is because for each natural image there are many non-natural images mapped to the same feature vector 1. Therefore, a natural image prior is necessary to constrain the generated images to the manifold of natural images.\nAdversarial loss. Instead of manually designing a prior, as in Mahendran & Vedaldi (2015), we learn it with an approach similar to Generative Adversarial Networks (GANs) of Goodfellow et al. (2014). Namely, we introduce a discriminator D\u03d5 which aims to discriminate the generated images from real ones, and which is trained concurrently with the generatorG\u03b8. The generator is trained to \u201ctrick\u201d the discriminator network into classifying the generated images as real. Formally, the parameters \u03d5 of the discriminator are trained by minimizing\nLdiscr = \u2211 i log(D\u03d5(yi)) + log(1\u2212D\u03d5(G\u03b8(xi))), (3)\n1This is unless the feature representation is specifically designed to map natural and non-natural images far apart, such as the one extracted from the discriminator of a GAN.\nand the generator is trained to minimize\nLadv = \u2211 i logD\u03d5(G\u03b8(xi)). (4)\nLoss in image space. Adversarial training is known to be unstable and sensitive to hyperparameters. We found that adding a loss in the image space\nLimg = \u2211 i ||G\u03b8(xi)\u2212 yi||22. (5)\nstabilizes training."}, {"heading": "3.1. Architectures", "text": "Generators. We used several different generators in experiments. They are task-specific, so we describe these in corresponding sections below. All tested generators make use of up-convolutional (\u2019deconvolutional\u2019) layers, as in Dosovitskiy et al. (2015b). An up-convolutional layer consists of up-sampling and a subsequent convolution. In this paper we always up-sample by a factor of 2 and a \u2019bed of nails\u2019 upsampling.\nIn all networks we use leaky ReLU nonlinearities, that is, LReLU(x) = max(x, 0) + \u03b1min(x, 0). We used \u03b1 = 0.3 in our experiments. All generators have linear output layers.\nComparators. We experimented with four comparators:\n1. AlexNet (Krizhevsky et al., 2012) is a network with 5 convolutional and 2 fully connected layers trained on image classification.\n2. The network of Wang & Gupta (2015) has the same architecture as AlexNet, but is trained using videos with triplet loss, which enforces frames of one video to be close in the feature space and frames from different videos to be far apart. We refer to this network as VideoNet.\n3. AlexNet with random weights.\n4. Exemplar-CNN (Dosovitskiy et al., 2015a) is a network with 3 convolutional layers and 1 fully connected layer trained on a surrogate task of discriminating between different image patches.\nThe exact layers used for comparison are specified in the experiments sections.\nDiscriminator. The architecture of the discriminator was nearly the same in all experiments. The version used for the autoencoder experiments is shown in Table 1. The discriminator must ensure the local statistics of images to be natural. Therefore after five convolutional layers with occasional stride we perform global average pooling. The result is processed by two fully connected layers, followed by a\n2-way softmax. We perform 50% dropout after the global average pooling layer and the first fully connected layer.\nThere are two modifications to this basic architecture. First, when dealing with large ImageNet (Deng et al., 2009) images we increase the stride in the first layer from 2 to 4. Second, when training networks to invert AlexNet, we additionally feed the features to the discriminator. We process them with two fully connected layers with 1024 and 512 units, respectively. Then we concatenate the result with the output of global average pooling."}, {"heading": "3.2. Training details", "text": "We modified the caffe (Jia et al., 2014) framework to train the networks. For optimization we used Adam (Kingma & Ba, 2015) with momentum \u03b21 = 0.9, \u03b22 = 0.999 and initial learning rate 0.0002. To prevent the discriminator from overfitting during adversarial training we temporarily stopped updating it if the ratio of Ldiscr and Ladv was below a certain threshold (0.1 in most experiments). We used batch size 64 in all experiments. We trained for 500, 000- 1, 000, 000 mini-batch iterations."}, {"heading": "4. Experiments", "text": "We started with a simple proof-of-concept experiment showing how DeePSiM can be applied to training autoencoders. Then we used the proposed loss function within the variational autoencoder (VAE) framework. Finally, we applied the method to invert the representation learned by AlexNet and analyzed some properties of the method.\nIn quantitative comparisons we report normalized Euclidean error ||a \u2212 b||2/N . The normalization coefficient N is the average of Euclidean distances between all pairs of different samples from the test set. Therefore, the error of 100% means that the algorithm performs the same as randomly drawing a sample from the test set."}, {"heading": "4.1. Autoencoder", "text": "Here the target of the generator coincides with its input (that is, y = x), and the task of the generator is to encode the input to a compressed hidden representation and then decode back the image. The architecture is shown in Table 2. All layers are convolutional or up-convolutional. The\nhidden representation is an 8-channel feature map 8 times smaller than the input image. We trained on the STL-10 unlabeled dataset which contains 100, 000 images 96\u00d7 96 pixels. To prevent overfitting we augmented the data by cropping random 64\u00d7 64 patches during training.\nWe experimented with four loss functions: SE and `1 in the image space, as well as DeePSiM with AlexNet CONV3 or Exemplar-CNN CONV3 as comparator.\nQualitative results are shown in Fig. 3, quantitative results in Table 3. While underperforming in terms of Euclidean loss, our approach can preserve more texture details, resulting in naturally looking non-blurry reconstructions. Interestingly, AlexNet as comparator tends to corrupt fine details (petals of the flower, sails of the ship), perhaps because it has stride of 4 in the first layer. Exemplar-CNN as comparator does not preserve the exact color because it is explicitly trained to be invariant to color changes. We believe that with carefully selected or specifically trained comparators yet better results can be obtained.\nWe stress that lower Euclidean error does not mean better reconstruction. For example, imagine a black-and-white striped \u201dzebra\u201d pattern. A monotonous gray image will have twice smaller Euclidean error than the same pattern shifted by one stripe width.\nClassification. Reconstruction-based models are commonly used for unsupervised feature learning. We checked if our loss functions lead to learning more meaningful rep-\nresentations than usual `1 and SE losses. To this end, we trained linear SVMs on the 8-channel hidden representations extracted by autoencoders trained with different losses. We are just interested in relative performance and, thus, do not compare to the state of the art. We trained on 10 folds of the STL-10 training set and tested on the test set.\nThe results are shown in Table 4. As expected, the features learned with DeePSiM perform significantly better, indicating that they contain more semantically meaningful information. This suggests that other losses than standard `1 and SE may be useful for unsupervised learning. Note that the Exemplar-CNN comparator is trained in an unsupervised way."}, {"heading": "4.2. Variational autoencoder", "text": "A standard VAE consists of an encoder Enc and a decoder Dec. The encoder maps an input sample x to a distribution over latent variables z \u223c Enc(x) = q(z|x). Dec maps from this latent space to a distribution over images x\u0303 \u223c Dec(z) = p(x|z). The loss function is\u2211\ni\n\u2212Eq(z|xi) log p(xi|z) +DKL(q(z|xi)||p(z)), (6)\nwhere p(z) is a prior distribution of latent variables and DKL is the Kullback-Leibler divergence. The first term in Eq. 6 is a reconstruction error. If we assume that the decoder predicts a Gaussian distribution at each pixel, then it reduces to squared Euclidean error in the image space. The second term pulls the distribution of latent variables towards the prior. Both q(z|x) and p(z) are commonly assumed to be Gaussian, in which case the KL divergence\ncan be computed analytically. Please refer to Kingma et al. (2014) for details.\nWe use the proposed loss instead of the first term in Eq. 6. This is similar to (Larsen et al., 2015), but the comparator does not have to be a part of the discriminator. Technically, there is little difference from training an autoencoder. First, instead of predicting a single latent vector z we predict two vectors \u00b5 and \u03c3 and sample z = \u00b5+ \u03c3 \u03b5, where \u03b5 is standard Gaussian (zero mean, unit variance) and is element-wise multiplication. Second, we add the KL divergence term to the loss:\nLKL = 1\n2 \u2211 i ( ||\u00b5i||22 + ||\u03c3i||22 \u2212 \u3008log \u03c32i , 1\u3009 ) . (7)\nWe manually set the weighting of the KL term relative to the rest of the loss. Proper probabilistic derivation is nonstraightforward, and we leave it for future research.\nWe trained on 227 \u00d7 227 pixel crops of 256 \u00d7 256 pixel ILSVRC-2012 images. The encoder architecture is the same as AlexNet up to layer FC6, and the decoder architecture is shown in Table 5. We initialized the encoder with AlexNet weights, however, this is not necessary, as shown in the appendix. We sampled from the model by sampling\nthe latent variables from a standard Gaussian z = \u03b5 and generating images from that with the decoder.\nSamples generated with the usual SE loss, as well as three different comparators (AlexNet CONV5, AlexNet FC6, VideoNet CONV5) are shown in Fig. 4. While Euclidean loss leads to very blurry samples, our method yields images with realistic statistics. Interestingly, the samples trained with the VideoNet comparator look qualitatively similar to the ones with AlexNet, showing that supervised training may not be necessary to yield a good comparator. More results are shown in the appendix."}, {"heading": "4.3. Inverting AlexNet", "text": "Analysis of learned representations is an important but largely unsolved problem. One approach is to invert the representation. This may give insights into which information is preserved in the representation and what are its invariance properties. However, inverting a non-trivial feature representation \u03a6, such as the one learned by a large convolutional network, is a difficult ill-posed problem.\nOur proposed approach inverts the AlexNet convolutional network very successfully. Surprisingly rich information about the image is preserved in deep layers of the network and even in the predicted class probabilities. While being an interesting result in itself, this also shows how DeePSiM is an excellent loss function when dealing with very difficult image restoration tasks.\nSuppose we are given a feature representation \u03a6, which we aim to invert, and an image I. There are two inverse mappings: \u03a6\u22121R such that \u03a6(\u03a6 \u22121 R (\u03c6)) \u2248 \u03c6, and \u03a6 \u22121 L such that \u03a6\u22121L (\u03a6(I)) \u2248 I. Recently two approaches to inversion have been proposed, which correspond to these two variants of the inverse.\nMahendran & Vedaldi (2015), as well as Simonyan et al. (2014) and Yosinski et al. (2015), apply gradient-based optimization to find an image I\u0303 which minimizes the loss\n||\u03a6(I)\u2212 \u03a6(\u0303I)||22 + P (\u0303I), (8)\nwhere P is a simple natural image prior, such as total variation (TV) regularizer. This method produces images which are roughly natural and have features similar to the input features, corresponding to \u03a6\u22121R . However, the prior is limited, so reconstructions from fully connected layers of AlexNet do not look much like natural images.\nDosovitskiy & Brox (2015) train up-convolutional networks on a large training set of natural images to perform the inversion task. They use SE distance in the image space as loss function, which leads to approximating \u03a6\u22121L . The networks learn to reconstruct the color and rough positions of objects well, but produce over-smoothed results because they average all potential reconstructions.\nOur method can be seen as combining the best of both worlds. Loss in the feature space helps preserve perceptually important image features. Adversarial training keeps reconstructions realistic. Note that similar to Dosovitskiy & Brox (2015) and unlike Mahendran & Vedaldi (2015), our method does not require the feature representation being inverted to be differentiable.\nTechnical details. The generator in this setup takes the features extracted by AlexNet and generates an image from them, that is, x = \u03a6(I), y = I. In general we followed Dosovitskiy & Brox (2015) in designing the generators. The only modification is that we inserted more con-\nvolutional layers, giving the network more capacity. We reconstruct from outputs of layers CONV5 \u2013FC8. In each layer we also include processing steps following the layer, that is, pooling and non-linearities. So for example CONV5 means pooled features (pool5), and FC6 means rectified values (relu6).\nArchitecture used for inverting FC6 is the same as the decoder of the VAE shown in Table 5. Architectures for other layers are similar, except that for reconstruction from CONV5 fully connected layers are replaced by convolutional ones. The discriminator is the same as used for VAE. We trained on the ILSVRC-2012 training set and evaluated on the ILSVRC-2012 validation set.\nAblation study. We tested if all components of our loss are necessary. Results with some of these components removed are shown in Fig. 7. Clearly the full model performs best. In the following we will give some intuition why.\nTraining just with loss in the image space leads to averaging all potential reconstructions, resulting in oversmoothed images. One might imagine that adversarial training would allow to make images sharp. This indeed happens, but the resulting reconstructions do not correspond to actual objects originally contained in the image. The reason is that any \u201cnatural-looking\u201d image which roughly fits the blurry prediction minimizes this loss. Without the adversarial loss predictions look very noisy. Without the image space loss the method works well, but one can notice artifact on the borders of images, and training was less stable in this case.\nSampling pre-images. Given a feature vector \u03c6, it would be interesting to sample multiple images I\u0303 such that \u03a6(\u0303I) = \u03c6. A straightforward approach would inject noise into the generator along with the features, so that the network could randomize its outputs. This does not yield the desired result, since nothing in the loss function forces the generator to output multiple different reconstructions per feature vector. A major problem is that in the training data we only have one image per feature vector, i.e., a single sample per conditioning vector. We did not attack this problem in our paper, but we believe it is an important research direction.\nBest results. Representative reconstructions from higher layers of AlexNet are shown in Fig. 5. Comparison with existing approaches is shown in Fig. 6. Reconstructions\nfrom CONV5 are near-perfect, combining the natural colors and sharpness of details. Reconstructions from fully connected layers are still very good, preserving the main features of images, colors, and positions of large objects.\nNormalized Euclidean error in image space and in feature space (that is, the distance between the features of the image and the reconstruction) are shown in Table 8. The method of Mahendran&Vedaldi performs well in feature space, but not in image space, the method of Dosovitskiy&Brox \u2014 vice versa. The presented approach is fairly good on both metrics.\nIterative re-encoding. We performed another experiment illustrating how similar are the features of reconstructions to the original image features. Given an image, we compute its features, generate an image from those, and then iteratively compute the features of the result and generate from those. Results are shown in Fig. 9. Interestingly, several iterations do not significantly change the reconstruction, indicating that important perceptual features are preserved in the generated images. More results are shown in the appendix.\nInterpolation. We can morph images into each other by linearly interpolating between their features and generating the corresponding images. Fig. 11 shows that objects shown in the images smoothly warp into each other. More examples are shown in the appendix.\nDifferent comparators. AlexNet network we used above\nImage pair 1 Image pair 2\nFC6\nFC8\nas comparator has been trained on a huge labeled dataset. Is this supervision really necessary to learn a good comparator? We show here results with several alternatives to CONV5 features of AlexNet: 1) FC6 features of AlexNet, 2) CONV5 of AlexNet with random weights, 3) CONV5 of the network of (Wang & Gupta, 2015) which we refer to as VideoNet.\nThe results are shown in Fig. 10. While AlexNet CONV5 comparator provides best reconstructions, other networks preserve key image features as well. We also ran preliminary experiments with CONV5 features from the discriminator serving as a comparator, but were not able to get satisfactory results with those."}, {"heading": "5. Conclusion", "text": "We proposed a class of loss functions applicable to image generation that are based on distances in feature spaces. Applying these to three tasks \u2014 image auto-encoding, random natural image generation with a VAE and feature inversion \u2014 reveals that our loss is clearly superior to the typical loss in image space. In particular, it allows reconstruction of perceptually important details even from very lowdimensional image representations. We evaluated several feature spaces to measure distances. More research is necessary to find optimal features to be used depending on the task. To control the degree of realism in generated images, an alternative to adversarial training is an approach making use of feature statistics, similar to (Gatys et al., 2015). We see these as interesting directions of future work."}, {"heading": "Acknowledgements", "text": "The authors are grateful to Jost Tobias Springenberg and Philipp Fischer for useful discussions. We acknowledge funding by the ERC Starting Grant VideoLearn (279401)."}], "references": [{"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Laufer", "G. Alain", "J. Yosinski"], "venue": "In ICML,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Digital images and human vision. chapter The Visible Differences Predictor: An Algorithm for the Assessment of Image Fidelity, pp. 179\u2013206", "author": ["S. Daly"], "venue": null, "citeRegEx": "Daly.,? \\Q1993\\E", "shortCiteRegEx": "Daly.", "year": 1993}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks", "author": ["E.L. Denton", "S. Chintala", "arthur Szlam", "R. Fergus"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks", "author": ["A. Dosovitskiy", "P. Fischer", "J.T. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "Inverting visual representations with convolutional networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": "arxiv/1506.02753v2,", "citeRegEx": "Dosovitskiy and Brox.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2015}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "T. Brox"], "venue": "In CVPR,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": null, "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "networks. Science,", "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Learning and relearning in boltzmann machines. In Parallel Distributed Processing: Volume 1: Foundations, pp. 282\u2013317", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": null, "citeRegEx": "Hinton and Sejnowski.,? \\Q1986\\E", "shortCiteRegEx": "Hinton and Sejnowski.", "year": 1986}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comput.,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Semi-supervised learning with deep generative models", "author": ["D. Kingma", "D. Rezende", "S. Mohamed", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Autoencoding beyond pixels using a learned similarity", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "O. Winther"], "venue": "metric. arxiv:1512.09300,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In ICML, pp", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "In CVPR,", "citeRegEx": "Mahendran and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2015}, {"title": "Deep multiscale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": "URL http://arxiv. org/abs/1511.05440", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": null, "citeRegEx": "Mirza and Osindero.,? \\Q2014\\E", "shortCiteRegEx": "Mirza and Osindero.", "year": 2014}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": null, "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Learning to generate images with perceptual similarity", "author": ["K. Ridgeway", "J. Snell", "B. Roads", "R.S. Zemel", "M.C. Mozer"], "venue": "metrics. arxiv:1511.06409,", "citeRegEx": "Ridgeway et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ridgeway et al\\.", "year": 2015}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In ICLR workshop track,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "In Parallel Distributed Processing: Volume 1: Foundations,", "citeRegEx": "Smolensky.,? \\Q1987\\E", "shortCiteRegEx": "Smolensky.", "year": 1987}, {"title": "Perceptual quality measure using a spatio-temporal model of the human visual system", "author": ["C.J. van den Branden Lambrecht", "O. Verscheure"], "venue": "Electronic Imaging: Science & Technology,", "citeRegEx": "Lambrecht and Verscheure.,? \\Q1996\\E", "shortCiteRegEx": "Lambrecht and Verscheure.", "year": 1996}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In ICML, pp", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "In ICCV,", "citeRegEx": "Wang and Gupta.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Gupta.", "year": 2015}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "A perceptual distortion metric for digital color images", "author": ["S. Winkler"], "venue": "In in Proc. SPIE,", "citeRegEx": "Winkler.,? \\Q1998\\E", "shortCiteRegEx": "Winkler.", "year": 1998}, {"title": "Understanding neural networks through deep visualization", "author": ["J. Yosinski", "J. Clune", "A. Nguyen", "T. Fuchs", "H. Lipson"], "venue": "In Deep Learning Workshop,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "To this end, we build upon adversarial training as proposed by Goodfellow et al. (2014). We train a discriminator network to distinguish the output of the generator from real images.", "startOffset": 63, "endOffset": 88}, {"referenceID": 26, "context": "A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al.", "startOffset": 86, "endOffset": 159}, {"referenceID": 12, "context": "A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009).", "startOffset": 184, "endOffset": 253}, {"referenceID": 18, "context": "A prominent class of probabilistic models of images are restricted Boltzmann machines (Hinton & Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhutdinov, 2006) and their deep variants (Hinton et al., 2006; Salakhutdinov & Hinton, 2009; Lee et al., 2009).", "startOffset": 184, "endOffset": 253}, {"referenceID": 28, "context": "Autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008) have been widely used for unsupervised learning and generative modeling, too.", "startOffset": 13, "endOffset": 65}, {"referenceID": 0, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 15, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 9, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 18, "context": "By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images.", "startOffset": 60, "endOffset": 130}, {"referenceID": 8, "context": "By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images.", "startOffset": 60, "endOffset": 130}, {"referenceID": 1, "context": "Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998).", "startOffset": 62, "endOffset": 74}, {"referenceID": 30, "context": "The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al., 2004), which compares the local statistics of image patches.", "startOffset": 95, "endOffset": 114}, {"referenceID": 0, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al., 2015b). In all these models, loss is measured in the image space. By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images. There is a large body of work on assessing the perceptual similarity of images. Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998). The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al.", "startOffset": 38, "endOffset": 777}, {"referenceID": 0, "context": "Recently, stochastic neural networks (Bengio et al., 2014; Kingma et al., 2014; Gregor et al., 2015) have become popular, and deterministic networks are being used for image generation tasks (Dosovitskiy et al., 2015b). In all these models, loss is measured in the image space. By combining convolutions and unpooling (upsampling) layers (Lee et al., 2009; Goodfellow et al., 2014; Dosovitskiy et al., 2015b) these models can be applied to large images. There is a large body of work on assessing the perceptual similarity of images. Some prominent examples are the visible differences predictor (Daly, 1993), the spatio-temporal model for moving picture quality assessment (van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler (1998). The most popular perceptual image similarity metric is the structural similarity metric (SSIM) (Wang et al., 2004), which compares the local statistics of image patches. We are not aware of any work making use of similarity metrics for machine learning, except a recent pre-print of Ridgeway et al. (2015). They train autoencoders by directly maximizing the SSIM similarity of images.", "startOffset": 38, "endOffset": 1084}, {"referenceID": 7, "context": "Generative adversarial networks (GANs) have been proposed by Goodfellow et al. (2014). In theory, this training procedure can lead to a generator that perfectly models the data distribution.", "startOffset": 61, "endOffset": 86}, {"referenceID": 3, "context": "Denton et al. (2015) employ a multi-scale approach, gradually generating higher resolution images.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Denton et al. (2015) employ a multi-scale approach, gradually generating higher resolution images. Radford et al. (2015) make use of a convolutional-deconvolutional architecture and batch normalization.", "startOffset": 0, "endOffset": 121}, {"referenceID": 20, "context": "Recently Mathieu et al. (2015) used GANs for predicting future frames in videos by conditioning on previous frames.", "startOffset": 9, "endOffset": 31}, {"referenceID": 17, "context": "Most related is concurrent work of Larsen et al. (2015). The general idea is the same \u2014 to measure the similarity not in the image space, but rather in a feature space.", "startOffset": 35, "endOffset": 56}, {"referenceID": 17, "context": "Most related is concurrent work of Larsen et al. (2015). The general idea is the same \u2014 to measure the similarity not in the image space, but rather in a feature space. They also use adversarial training to improve the realism of the generated images. However, Larsen et al. (2015) only apply this approach to a variational autoencoder trained on images of faces, and measure the similarity between features extracted from the discriminator.", "startOffset": 35, "endOffset": 282}, {"referenceID": 8, "context": "Instead of manually designing a prior, as in Mahendran & Vedaldi (2015), we learn it with an approach similar to Generative Adversarial Networks (GANs) of Goodfellow et al. (2014). Namely, we introduce a discriminator D\u03c6 which aims to discriminate the generated images from real ones, and which is trained concurrently with the generatorG\u03b8.", "startOffset": 155, "endOffset": 180}, {"referenceID": 16, "context": "AlexNet (Krizhevsky et al., 2012) is a network with 5 convolutional and 2 fully connected layers trained on image classification.", "startOffset": 8, "endOffset": 33}, {"referenceID": 4, "context": "All tested generators make use of up-convolutional (\u2019deconvolutional\u2019) layers, as in Dosovitskiy et al. (2015b). An up-convolutional layer consists of up-sampling and a subsequent convolution.", "startOffset": 85, "endOffset": 112}, {"referenceID": 2, "context": "First, when dealing with large ImageNet (Deng et al., 2009) images we increase the stride in the first layer from 2 to 4.", "startOffset": 40, "endOffset": 59}, {"referenceID": 13, "context": "We modified the caffe (Jia et al., 2014) framework to train the networks.", "startOffset": 22, "endOffset": 40}, {"referenceID": 17, "context": "This is similar to (Larsen et al., 2015), but the comparator does not have to be a part of the discriminator.", "startOffset": 19, "endOffset": 40}, {"referenceID": 15, "context": "Please refer to Kingma et al. (2014) for details.", "startOffset": 16, "endOffset": 37}, {"referenceID": 25, "context": "Mahendran & Vedaldi (2015), as well as Simonyan et al. (2014) and Yosinski et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 25, "context": "Mahendran & Vedaldi (2015), as well as Simonyan et al. (2014) and Yosinski et al. (2015), apply gradient-based optimization to find an image \u0128 which minimizes the loss", "startOffset": 39, "endOffset": 89}, {"referenceID": 7, "context": "To control the degree of realism in generated images, an alternative to adversarial training is an approach making use of feature statistics, similar to (Gatys et al., 2015).", "startOffset": 153, "endOffset": 173}], "year": 2016, "abstractText": "Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.", "creator": "LaTeX with hyperref package"}}}