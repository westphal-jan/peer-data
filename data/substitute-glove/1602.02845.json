{"id": "1602.02845", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Online Active Linear Regression via Thresholding", "abstract": "We consider several problem of google active learning to collect data for regression analytical. Specifically, things come a decision maker to seen same limited methodology issue even take outsourced we but maturity linear residing typical. Our goal although both use simulation clearly provide continuing gains two impersonal random combining the observations. To have on, enough block contribution important a novel proportion - focus predictive well terms of observations; get deduce creating as and aspects lower bounds. We present apply understand approach although to re-examining regression. Simulations view the graphical there somewhat robust: instead private exception benefits over passive variations measurements because followed previously example - world aggregated that exhibit high tetragonal and below dimensionality - - - significantly impact end mean and coefficients taken the squared foul.", "histories": [["v1", "Tue, 9 Feb 2016 02:51:12 GMT  (3761kb,D)", "https://arxiv.org/abs/1602.02845v1", null], ["v2", "Wed, 10 Feb 2016 17:53:33 GMT  (3761kb,D)", "http://arxiv.org/abs/1602.02845v2", null], ["v3", "Thu, 23 Jun 2016 18:36:58 GMT  (7567kb,D)", "http://arxiv.org/abs/1602.02845v3", null], ["v4", "Wed, 21 Dec 2016 13:36:50 GMT  (7532kb,D)", "http://arxiv.org/abs/1602.02845v4", "Published in AAAI 2017"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["carlos riquelme", "ramesh johari", "baosen zhang"], "accepted": true, "id": "1602.02845"}, "pdf": {"name": "1602.02845.pdf", "metadata": {"source": "CRF", "title": "Online Active Linear Regression via Thresholding", "authors": ["Carlos Riquelme", "Ramesh Johari", "Baosen Zhang"], "emails": ["rikel@stanford.edu", "rjohari@stanford.edu", "zhangbao@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "This paper studies online active learning for estimation of linear models. Active learning is motivated by the premise that in many sequential data collection scenarios, labeling or obtaining output from observations is costly. Thus ongoing decisions must be made about whether to collect data on a particular unit of observation. Active learning has a rich history; see, e.g., [3, 6, 7, 8, 17].\nAs a motivating example, suppose that an online marketing organization plans to send display advertising promotions to a new target market. Their goal is to estimate the revenue that can be expected for an individual with a given covariate vector. Unfortunately, providing the promotion and collecting data on each individual is costly. Thus the goal of the marketing organization is to acquire first the most \u201cinformative\u201d observations. They must do this in an online fashion: opportunities to display the promotion to individuals arrive sequentially over time. In online active learning, this is achieved by selecting those observational units (target individuals in this case) that provide the most information to the model fitting procedure.\nLinear models are ubiquitous in both theory and practice\u2014often used even in settings where the data may exhibit strong nonlinearity\u2014in large part because of their interpretability, flexibility, and simplicity. As a consequence, in practice, people tend to add a large number of features and interactions to the model, hoping to capture the right signal at the expense of introducing some noise. Moreover, the input space can be updated and extended iteratively after data collection if the decision maker feels predictions on a held-out set are not good enough. As a consequence, often times the number of covariates becomes higher than the number of available observations. In those cases, selecting the subsequent most informative data is even more critical. Accordingly, our focus is on actively choosing observations for optimal prediction of the resulting high-dimensional linear models.\nOur main contributions are as follows. We initially focus on standard linear models, and build the theory that we later extend to high dimensional settings. First, we develop an algorithm that sequentially selects observations if they have sufficiently large norm, in an appropriate space (dependent on the data-generating distribution). Second, we provide a comprehensive theoretical analysis of our algorithm, including upper and lower bounds. We focus on minimizing mean squared prediction error (MSE), and show a high probability upper bound on the MSE of our approach (cf. Theorem 3.1). In addition, we provide a lower bound on the best possible achievable performance in high probability\nar X\niv :1\n60 2.\n02 84\n5v 4\n[ st\nat .M\nL ]\n2 1\nD ec\n2 01\nand expectation (cf. Section 4). In some distributional settings of interest we show that this lower bound structurally matches our upper bound, suggesting our algorithm is near-optimal.\nThe results above show that the improvement of active learning progressively weakens as the dimension of the data grows, and a new approach is needed. To tackle our original goal and address this degradation, under standard sparsity assumptions, we design an adaptive extension of the thresholding algorithm that initially devotes some budget to learn the sparsity pattern of the model, in order to subsequently apply active learning to the relevant lower dimensional subspace. We find that in this setting, the active learning algorithm provides significant benefit over passive random sampling. Theoretical guarantees are given in Theorem 3.3.\nFinally, we empirically evaluate our algorithm\u2019s performance. Our tests on real world data show our approach is remarkably robust: the gain of active learning remains significant even in settings that fall outside our theory. Our results suggest that the threshold-based rule may be a valuable tool to leverage in observation-limited environments, even when the assumptions of our theory may not exactly hold.\nActive learning has mainly been studied for classification; see, e.g., [1, 2, 9, 10, 28]. For regression, see, e.g., [5, 18, 24] and the references within. A closely related work to our setting is [23]: they study online or stream-based active learning for linear regression, with random design. They propose a theoretical algorithm that partitions the space by stratification based on Monte-Carlo methods, where a recently proposed algorithm for linear regression [14] is used as a black box. It converges to the globally optimal oracle risk under possibly misspecified models (with suitable assumptions). Due to the relatively weak model assumptions, they achieve a constant gain over passive learning. As we adopt stronger assumptions (well-specified model), we are able to achieve larger than constant gains, with a computationally simpler algorithm. Suppose covariate vectors are Gaussian with dimension d; the total number of observations is n; and the algorithm is allowed to label at most k of them. Then, we beat the standard \u03c32d/k MSE to obtain \u03c32d2/[kd + 2(\u03b4 \u2212 1)k log k] when n = k\u03b4, so active learning truly improves performance when k = \u2126(exp(d)) or \u03b4 = \u2126(d). While [23] does not tackle high-dimensional settings, we overcome the exponential data requirements via l1-regularization.\nThe remainder of the paper is organized as follows. We define our setting in Section 2. In Section 3, we introduce the algorithm and provide analysis of a corresponding upper bound. Lower bounds are given in Section 4. Simulations are presented in Section 5, and Section 6 concludes."}, {"heading": "2 Problem Definition", "text": "The online active learning problem for regression is defined as follows. We sequentially observe n covariate vectors in a d-dimensional space Xi \u2208 Rd, which are i.i.d. When presented with the i-th observation, we must choose whether we want to label it or not, i.e., choose to observe the outcome. If we decide to label the observation, then we obtain Y i \u2208 R. Otherwise, we do not see its label, and the outcome remains unknown. We can label at most k out of the n observations.\nWe assume covariates are distributed according to some known distribution D, with zero mean EX = 0, and covariance matrix \u03a3 = EXXT . We relax this assumption later. In addition, we assume that Y follows a linear model: Y = XT\u03b2\u2217 + , where \u03b2\u2217 \u2208 Rd and \u223c N (0, \u03c32) i.i.d. We denote observations by X,Xi \u2208 Rd, components by Xj \u2208 R, and sets in boldface: X \u2208 Rk\u00d7d,Y \u2208 Rk.\nAfter selecting k observations, (X,Y), we output an estimate \u03b2\u0302k \u2208 Rd, with no intercept.1 Our goal is to minimize the expected MSE of \u03b2\u0302k in \u03a3 norm, i.e. E\u2016\u03b2\u0302k \u2212 \u03b2\u2217\u20162\u03a3, under random design; that is, when the Xi\u2019s are random and the algorithm may be randomized. This is related to the A-optimality criterion, [22]. We use the experimentation budget to minimize the variance of \u03b2\u0302k by sampling X from a different thresholded distribution. Minimizing expected MSE is equivalent to minimizing the trace of the normalized inverse of the Fisher information matrix XTX,\nE[(Y \u2212XT \u03b2\u0302k)2] = E[\u2016\u03b2\u0302k \u2212 \u03b2\u2217\u20162\u03a3] + \u03c32 = \u03c32 E [ Tr(\u03a3(XTX)\u22121) ] + \u03c32\n1We assume covariates and outcome are centered.\nwhere expectations are over all sources of randomness. In this setting, the OLS estimator is the best linear unbiased estimator by the Gauss\u2013Markov Theorem. Also, for any set X of k i.i.d. observations, \u03b2\u0302k := \u03b2\u0302 OLS k has sampling distribution \u03b2\u0302k | X \u223c N (\u03b2\u2217, \u03c32(XTX)\u22121), [13]. In Section 3.3, we tackle high-dimensionality, where k \u2264 d, via Lasso estimators within a two-stage algorithm."}, {"heading": "3 Algorithm and Main Results", "text": "In this section we motivate the algorithm, state the main result quantifying its performance for general distributions, and provide a high-level overview of the proof. A corollary for the Gaussian distribution is presented, and we also extend the algorithm by making the threshold adaptive. Finally, we show how to generalize the results to sparse linear regression. In Appendix E, we derive a CLT approximation with guarantees that is useful in complex or unknown distributional settings.\nWithout loss of generality, we assume that each observation is white, that is, E[XXT ] is the identity matrix. For correlated observations X \u2032, we apply X := D\u22121/2UTX \u2032 to whiten them, \u03a3 = UDUT (see Appendix A). Note that Tr(\u03a3(X\u2032TX\u2032)\u22121) = Tr((XTX)\u22121).\nWe bound the whitened trace as\nd \u03bbmax(XTX) \u2264 Tr((XTX)\u22121) \u2264 d \u03bbmin(XTX) . (1)\nTo minimize the expected MSE, we need to maximize the minimum eigenvalue of XTX with high probability. The thresholding procedure in Algorithm 1 maximizes the minimum eigenvalue of XTX through two observations. First, since the sum of eigenvalues of XTX is the trace of XTX, which is in turn the sum of the norm of the observations, the algorithm chooses observations of large (weighted) norm. Second, the eigenvalues of XTX should be balanced, that is, have similar magnitudes. This is achieved by selecting the appropriate weights for the norm.\nLet \u03be \u2208 Rd+ be a vector of weights defining the norm \u2016X\u20162\u03be = \u2211d j=1 \u03bejX 2 j . Let \u0393 > 0 be a threshold. Algorithm 1 simply selects the observations with \u03be-weighted norm larger than \u0393. The selected observations can be thought as i.i.d. samples from an induced distribution D\u0304: the original distribution conditional on \u2016X\u2016\u03be \u2265 \u0393. Suppose k observations are chosen and denoted by X \u2208 Rk\u00d7d. Then EXTX = \u2211k i=1 EX iXi T = \u2211k i=1H\ni = kH , where H is the covariance matrix with respect to D\u0304. This covariance matrix is diagonal under density symmetry assumptions, as thresholding preserves uncorrelation; its diagonal terms are\nHjj = ED\u0304X 2 j = ED[X 2 j | \u2016X\u2016\u03be \u2265 \u0393] =: \u03c6j . (2)\nHence, \u03bbmin(EXTX) = kminj \u03c6j , and \u03bbmax(EXTX) = kmaxj \u03c6j . The main technical result in Theorem 3.1 is to link the eigenvalues of the random matrix XTX to its deterministic counter part EXTX. From the above calculations, the goal is to find (\u03be,\u0393) such that minj \u03c6j \u2248 maxj \u03c6j , and both are as large as possible. The first objective is achieved when there exists some \u03c6 such that\nED[X 2 j | \u2016X\u2016\u03be \u2265 \u0393] = \u03c6j = \u03c6, for all j. (3)\nWe note that if X has independent components with the same marginal distribution (after whitening), then it suffices to choose \u03bej = 1 for all j. It is necessary to choose unequal weights when the marginal distributions of the components are different, e.g., some are Gaussian and some are uniform, or components are dependent. For joint Gaussian, whitening removes dependencies, so we set \u03bej = 1."}, {"heading": "3.1 Thresholding Algorithm", "text": "The algorithm is simple. For each incoming observation Xi we compute its weighted norm \u2016Xi\u2016\u03be (possibly after whitening if necessary). If the norm is above the threshold \u0393, then we select the observation, otherwise we ignore it. We stop when we have collected k observations. Note that random sampling is equivalent to setting \u0393 = 0.\nWe want to catch the k largest observations given our budget, therefore we require that \u0393 satisfies\nPD (\u2016X\u2016\u03be \u2265 \u0393) = k/n. (4)\nAlgorithm 1 Thresholding Algorithm. 1: Set (\u03be,\u0393) \u2208 Rd+1 satisfying (3) and (4). 2: Set S = \u2205. 3: for observation 1 \u2264 i \u2264 n do 4: Observe Xi. 5: Compute Xi = D\u22121/2UTXi. 6: if \u2016Xi\u2016\u03be > \u0393 or k \u2212 |S| = n\u2212 i+ 1 then 7: Choose Xi: S = S \u222aXi. 8: if |S| = k then 9: break. 10: end if 11: end if 12: end for 13: Return OLS estimate \u03b2\u0302 based on observations in S.\nIf we apply this rule to n independent observations coming from D, on average we select k of them: the \u03be\u2212largest. If (\u03be,\u0393) is a solution to (3) and (4), then (c \u03be, \u221a c \u0393) is also a solution for any c > 0. So we require \u2211 i \u03bei = d.\nAlgorithm 1 can be seen as a regularizing process similar to ridge regression, where the amount of regularization depends on the distribution D and the budget ratio k/n; it improves the conditioning of the problem.\nGuarantees when \u03a3 is unknown can be derived as follows: we allocate an initial sequence of points to estimation of the inverse of the covariance matrix, and the remainder to labeling (where we no longer update our estimate). In this manner observations remain independent. Note that O(d) observations are required for accurate recovery when D is subgaussian, and O(d log d) if subexponential, [26]. Errors by using the estimate to whiten and make decisions are bounded, small with high probability (via Cauchy\u2013Schwarz), and the result is equivalent to using a slightly worse threshold.\nAlgorithm 1 b Adaptive Thresholding Algorithm. 1: Set S = \u2205. 2: for observation 1 \u2264 i \u2264 n do 3: Observe Xi, estimate \u03a3\u0302i = U\u0302iD\u0302iU\u0302Ti . 4: Compute Xi = D\u0302\u22121/2i U\u0302 T i Xi.\n5: Let (\u03bei,\u0393i) satisfy (3) and (5). 6: if \u2016Xi\u2016\u03bei > \u0393i or k \u2212 |S|=n\u2212 i+ 1 then 7: Choose Xi: S = S \u222aXi. 8: if |S| = k then 9: break.\n10: end if 11: end if 12: end for 13: Return OLS estimate \u03b2\u0302 based on observations in S.\nAlgorithm 1 keeps the threshold fixed from the beginning, leading to a mathematically convenient analysis, as it generates i.i.d. observations. However, Algorithm 1b, which is adaptive and updates its parameters after each observation, produces slightly better results, as we empirically show in Appendix K. Before making a decision on Xi, Algorithm 1b finds (\u03bei,\u0393i) satisfying (3) and\nPD ( \u2016Xi\u2016\u03bei \u2265 \u0393i ) = k \u2212 |Si\u22121| n\u2212 i+ 1 , (5)\nwhere |Si\u22121| is the number of observations already labeled. The idea is identical: set the threshold to capture, on average, the number of observations still to be labeled, that is k \u2212 |Si\u22121|, out of the number still to be observed, n\u2212 i+ 1.\nImportantly, active learning not only decreases the expected MSE, but also its variance. Since the variance of the MSE for fixed X depends on \u2211 j 1/\u03bbj(X\nTX)2 [13], it is also minimized by selecting observations that lead to large eigenvalues of XTX."}, {"heading": "3.2 Main Theorem", "text": "Theorem 3.1 states that by sampling k observations from D\u0304 where (\u03be,\u0393) satisfy (3), the estimation performance is significantly improved, compared to randomly sampling k observations from the original distribution. Section 4 shows the gain in Theorem 3.1 essentially cannot be improved and Algorithm 1 is optimal. A sketch of the proof is provided at the end of this section (see Appendix B).\nTheorem 3.1 Let n > k > d. Assume observations X \u2208 Rd are distributed according to subgaussian D with covariance matrix \u03a3 \u2208 Rd\u00d7d. Also, assume marginal densities are symmetric around zero after whitening. Let X be a k \u00d7 d matrix with k observations sampled from the distribution induced by the thresholding rule with parameters (\u03be,\u0393) \u2208 Rd+1+ satisfying (3). Let \u03b1 > 0, so that t = \u03b1 \u221a k \u2212 C \u221a d > 0, then, with probability at least 1\u2212 2 exp(\u2212ct2)\nTr(\u03a3(XTX)\u22121) \u2264 d (1\u2212 \u03b1)2 \u03c6k , (6)\nwhere constants c, C depend on the subgaussian norm of D\u0304.\nWhile Theorem 3.1 is stated in fairly general terms, we can apply the result to specific settings. We first present the Gaussian case where white components are independent. The proof is in Appendix D.\nCorollary 3.2 If the observations in Theorem 3.1 are jointly Gaussian with covariance matrix \u03a3 \u2208 Rd\u00d7d, \u03bej = 1 for all j = 1, . . . , d, and \u0393 = C\u0304 \u221a d+ 2 log(n/k), for some constant C\u0304 \u2265 1, then with probability at least 1\u2212 2 exp(\u2212ct2) we have that\nTr(\u03a3(XTX)\u22121) \u2264 d (1\u2212 \u03b1)2 (\n1 + 2 log(n/k)d\n) k . (7)\nThe MSE of random sampling for white Gaussian data is proportional to d/(k\u2212d\u22121), by the inverse Wishart distribution. Active learning provides a gain factor of order 1/(1 + 2 log(n/k)/d) with high probability (a very similar 1 \u2212 \u03b1 term shows up for random sampling). Note that our algorithm may select fewer than k observations. Then, when the number of observations yet to be seen equals the remaining labeling budget, we should select all of them (equivalent to random sampling). The number of observations with \u2016X\u2016\u03be > \u0393 has binomial distribution, is highly concentrated around its mean k, with variance k(1\u2212 k/n). By the Chernoff Bounds, the probability that the algorithm selects fewer than k\u2212C \u2032 \u221a k decreases exponentially fast in C \u2032. Thus, these deviations are dominated in the bound of Theorem 3.1 by the leading term. In practice, one may set the threshold in (4) by choosing k(1 + ) observations for some small > 0, or use the adaptive threshold in Algorithm 1b."}, {"heading": "3.3 Sparsity and Regularization", "text": "The gain provided by active learning in our setting suffers from the curse of dimensionality, as it diminishes very fast when d increases, and Section 4 shows the gain cannot be improved in general. For high dimensional settings (where k \u2264 d) we assume s-sparsity in \u03b2, that is, we assume the support of \u03b2 contains at most s non-zero components, for some s d. In Appendix J, we also provide related results for Ridge regression.\nWe state the two-stage Sparse Thresholding Algorithm (see Algorithm 2) and show this algorithm effectively overcomes the curse of dimensionality. For simplicity, we assume the data is Gaussian, D = N (0,\u03a3). Based, for example, on the results of [25] and Theorem 1 in [16], we could extend our results to subgaussian data via the Orthogonal Matching Pursuit algorithm for recovery. The two-stage algorithm works as follows. First, we focus on recovering the true support, S = S(\u03b2), by selecting the very first k1 observations (without thresholding), and computing the Lasso estimator \u03b2\u03021. Second, we assign the weights \u03be: for i \u2208 S(\u03b2\u03021), we set \u03bei = 1, otherwise we set \u03bei = 0. Then, we\napply the thresholding rule to select the remaining k2 = k \u2212 k1 observations. While observations are collected in all dimensions, our final estimate \u03b2\u03022 is the OLS estimator computed only including the observations selected in the second stage, and exclusively in those dimensions in S(\u03b2\u03021).\nNote that, in general, the points that end up being selected by our algorithm are informational outliers, while not necessarily geometric outliers in the original space. After applying the whitening transformation, ignoring some dimensions based on the Lasso results, and then thresholding based on a weighted norm possibly learnt from data (say, if components are not independent, and we recover the covariance matrix in a online fashion), the algorithm is able to identify good points for the underlying data distribution and \u03b2.\nAlgorithm 2 Sparse Thresholding Algorithm. 1: Set S1 = \u2205, S2 = \u2205. Let k = k1 + k2, n = k1 + n2. 2: for observation 1 \u2264 i \u2264 k1 do 3: Observe Xi. Choose Xi: S1 = S1 \u222aXi. 4: end for 5: Set \u03b3 = 1/2, \u03bb = \u221a 4\u03c32 log(d)/\u03b32k1.\n6: Compute Lasso estimate \u03b2\u03021 on S1, regularization \u03bb. 7: Set weights: \u03bei = 1 if i \u2208 S(\u03b2\u03021), \u03bei = 0 otherwise. 8: Set \u0393 = C \u221a s+ 2 log(n2/k2). 9: Factorize \u03a3S(\u03b2\u03021)S(\u03b2\u03021) = UDU T .\n10: for observation k1 + 1 \u2264 i \u2264 n do 11: Observe Xi \u2208 Rd. Restrict to XiS := XiS(\u03b2\u03021) \u2208 R s. 12: Compute XiS = D\u22121/2UTXiS . 13: if \u2016XiS\u2016\u03be > \u0393 or k2 \u2212 |S2| = n\u2212 i+ 1 then 14: Choose XiS : S2 = S2 \u222aXiS . 15: if |S2| = k2 then 16: break. 17: end if 18: end if 19: end for 20: Return OLS estimate \u03b2\u03022 based on observations in S2.\nTheorem 3.3 summarizes the performance of Algorithm 2; it requires the standard assumptions on \u03a3, \u03bb and mini |\u03b2i| for support recovery (see Theorem 3 in [27]).\nTheorem 3.3 Let D = N (0,\u03a3). Assume \u03a3, \u03bb and mini |\u03b2i| satisfy the standard conditions given in Theorem 3 of [27]. Assume we run the Sparse Thresholding algorithm with k1 = C \u2032s log d observations to recover the support of \u03b2, for an appropriate C \u2032 \u2265 0. Let X2 be k2 = k \u2212 k1 observations sampled via thresholding on S(\u03b2\u03021). It follows that for \u03b1 > 0 such that t = \u03b1 \u221a k2 \u2212\nC \u221a s > 0, there exist some universal constants c1, c2, and c, C that depend on the subgaussian norm of D\u0304 | S(\u03b2\u03021), such that with probability at least\n1\u2212 2e\u2212min(c2 min(s,log(d\u2212s))\u2212log(c1),ct 2\u2212log(2))\nit holds that Tr(\u03a3SS(X T 2 X2) \u22121) \u2264 s (1\u2212 \u03b1)2 (\n1 + 2 log(n2/k2)s ) k2 .\nPerformance for random sampling with the Lasso estimator is O(s log d/k). A regime of interest is s d, k = C1s log d, and n = C2 d, for large enough C1, and C2 > 0. In that case, Algorithm 2 leads to a bound of order smaller than 1/ log(d), as opposed to a weaker constant guarantee for random sampling. The gain is at least a log d factor with high probability. The proof is in Appendix H. In practice, the performance of the algorithm is improved by using all the k observations to fit the final estimate \u03b2\u03022, as shown in simulations. However, in that case, observations are no longer i.i.d. Also, using thresholding to select the initial k1 observations decreases the probability of making a mistake in support recovery. In Section 5 we provide simulations comparing different methods."}, {"heading": "3.4 Proof of Theorem 3.1", "text": "The complete proof of Theorem 3.1 is in Appendix B. We only provide a sketch here. The proof is a direct application of spectral results in [26], which are derived via a covering argument using a discrete net N on the unit Euclidean sphere Sd\u22121, together with a Bernstein-type concentration inequality that controls deviations of \u2016Xw\u20162 for each element w \u2208 N in the net. Finally, a union bound is taken over the net. Importantly, the proof shows that if our algorithm uses (\u03be,\u0393) which are approximate solutions to (3), then (10) still holds with minj ED\u0304X2j in the denominator of the RHS, instead of \u03c6. This fact can be quite useful in practice, when F is unknown. We can devote some initial budget X1, . . . , XT to recover F, and then find (\u03be,\u0393) approximately solving (3) and (4) under F\u0302. Note that no labeling is required.\nAlso, the result can be extended to subexponential distributions. In this case, the probabilistic bound will be weaker (including a d term in front of the exponential). More generally, our probabilistic bounds are strongest when k \u2265 Cd log d for some constant C \u2265 0, a common situation in active learning [23], where super-linear requirements in d seem unavoidable in noisy settings. A simple bound for the parameter \u03c6 can be calculated as follows. Assume there exists (\u03be,\u0393) such that \u03c6j = \u03c6 and consider the weighted squared norm Z\u03be = \u2211d j=1 \u03bejX 2 j . Then ED\u0304 [Z\u03be] = \u2211d j=1 \u03bejED\u0304 [ X2j ]\n=\u2211d j=1 \u03bej\u03c6j = d\u03c6, and \u03c6 = ED [ Z\u03be | Z\u03be \u2265 \u03932 ] /d \u2265 \u03932/d = F\u22121Z\u03be (1\u2212 k/n)/d, which implies that 1/\u03bbmin(EXTX) = 1/k\u03c6 \u2264 d/k\u03932. For specific distributions, \u03932/d can be easily computed. The last inequality is close to equality in cases where the conditional density decays extremely fast for values of \u2211d j=1 \u03bejX 2 j above \u0393\n2. Heavy-tailed distributions allocate mass to significantly higher values, and \u03c6 could be much larger than \u03932/d."}, {"heading": "4 Lower Bound", "text": "In this section we derive a lower bound for the k > d setting. Suppose all the data are given. Again choose the k observations with largest norms, denoted by X\u2032. To minimize the prediction error, the best possible X\u2032TX\u2032 is diagonal, with identical entries, and trace equal to the sum of the norms. No selection algorithm, online or offline, can do better. Algorithm 1 achieves this by selecting observations with large norms and uncorrelated entries (through whitening if necessary). Theorem 4.1 captures this intuition.\nTheorem 4.1 Let A be an algorithm for the problem we described in Section 2. Then,\nEA Tr(\u03a3(X TX)\u22121) \u2265 d\n2 E [\u2211k i=1 ||X(i)||2 ] (8)\n\u2265 d k E [ 1 d maxi\u2208[n] ||Xi||2 ] , where X(i) is the white observation with the i-th largest norm. Moreover, fix \u03b1 \u2208 (0, 1). Let F be the cdf of maxi\u2208[n] ||Xi||2. Then, Tr(\u03a3(XTX)\u22121) \u2265 d2/k F\u22121(1\u2212 \u03b1) with probability at least 1\u2212 \u03b1.\nThe proof is in Appendix E. The upper bound in Theorem 3.1 has a similar structure, with denominator equal to k\u03c6. By Theorem 3.1, \u03c6 = ED[X2j | \u2016X\u20162\u03be \u2265 \u03932] for every component j. Hence, summing over all components: k\u03c6 = kED\u0304 [ \u2016X\u20162/d ] . The latter expectation is taken with respect to D\u0304, which only captures the k expected \u03be-largest observations out of n, as opposed to k ED[(1/k) \u2211k i=1 ||X(i)||2/d] in (8). The weights \u03be simply account for the fact that, in reality, we cannot make all components have equal norm, something we implicitly assumed in our lower bound.\nWe specialize the lower bound to the Gaussian setting, for which we computed the upper bound of Theorem 3.1. The proofs are based on the Fisher-Tippett Theorem and the Gumbel distribution; see Appendix F.\nCorollary 4.2 For Gaussian observations Xi \u223c N (0,\u03a3) and large n, for any algorithm A\nEA Tr(\u03a3(X TX)\u22121) \u2265 d k (\n2 logn d + log log n ) . Moreover, let \u03b1 \u2208 (0, 1). Then, for any A with probability at least 1\u2212 \u03b1 and C = 2 log \u0393(d/2)/d,\nTr(\u03a3(XTX)\u22121) \u2265 d/k 2 logn d + log log n\u2212 1 d log log 1 1\u2212\u03b1 \u2212 C\nThe results from Corollary 3.2 have the same structure as the lower bound; hence in this setting our algorithm is near optimal. Similar results and conclusions are derived for the CLT approximation in Appendix I."}, {"heading": "5 Simulations", "text": "We conducted experiments in various settings: regularized estimators in high-dimensions, and the basic thresholding approach in real-world data to explore its performance on strongly non-linear environments.\nRegularized Estimators. We compare the performance in high-dimensional settings of random sampling and Algorithm 1 \u2014both with an appropriately adjusted Lasso estimator\u2014 against Algorithm 2, which takes into account the structure of the problem (s d). For completeness, we also show\nthe performance of Algorithm 2 when all observations are included in the final OLS estimate, and that of random sampling (RS) and Algorithm 1 (Thr) when the true support S is known in advance, and the OLS computed on S. In Figure 1 (a), we see that Algorithm 2 dramatically reduces the MSE, while in Figure 1 (b) we zoom-in to see that, quite remarkably, Algorithm 2 using all observations for the final estimate outperforms random sampling that knows the sparsity pattern in hindsight. We used k1 = (2/3)k for recovery. More experiments are provided in Appendix K.\nReal-World Data. We show the results of Algorithm 1b (online \u03a3 estimation) with the simplest distributional assumption (Gaussian threshold, \u03bej = 1) versus random sampling on publicly available real-world datasets (UCI, [20]), measuring test squared prediction error. We fix a sequence of values of n, together with k = \u221a n, and for each pair (n, k) we run a number of iterations. In each one, we randomly split the dataset in training (n observations, random order), and test (rest of them). Finally, \u03b2\u0302OLS is computed on selected observations, and the prediction error estimated on the test set. All datasets are initially centered to have zero means (covariates and response). Confidence intervals are provided.\nWe first analyze the Physicochemical Properties of Protein Tertiary Structure dataset (45730 observations), where we predict the size of the residue, based on d = 9 variables, including the total surface area of the protein and its molecular mass. Figure 2 (a) shows the results; Algorithm 1b outperforms random sampling for all values of (n, k). The reduction in variance is substantial. In the Bike Sharing dataset [12] we predict the number of hourly users of the service, given weather conditions, including temperature, wind speed, humidity, and temporal covariates. There are 17379 observations, and we use d = 12 covariates. Our estimator has lower mean, median and variance MSE than random sampling; Figure 2 (b). Finally, for the YearPredictionMSD dataset [4], we predict the year a song was released based on d = 90 covariates, mainly metadata and audio features. There are 99799 observations. The MSE and variance did strongly improve; Figure 2 (c).\nIn the examples we see that, while active learning leads to strong improvements in MSE and variance reduction for moderate values of k with respect to d, the gain vanishes when k grows large. This was expected; the reason might be that by sampling so many outliers, we end up learning about parts of the space where heavy non-linearities arise, which may not be important to the test distribution. However, the motivation of active learning are situations of limited labeling budget, and hybrid approaches combining random sampling and thresholding could be easily implemented if needed."}, {"heading": "6 Conclusion", "text": "Our paper provides a comprehensive analysis of thresholding algorithms for online active learning of linear regression models, which are shown to perform well both theoretically and empirically. Several natural open directions suggest themselves. Additional robustness could be guaranteed in other settings by combining our algorithm as a \u201cblack box\u201d with other approaches: for example, some addition of random sampling or stratified sampling could be used to determine if significant nonlinearity is present, and to determine the fraction of observations that are collected via thresholding."}, {"heading": "7 Acknowledgments", "text": "The authors would like to thank Sven Schmit for his excellent comments and suggestions, Mohammad Ghavamzadeh for fruitful discussions, and the anonymous reviewers for their valuable feedback. We gratefully acknowledge support from the National Science Foundation under grants CMMI-1234955, CNS-1343253, and CNS-1544548."}, {"heading": "A Whitening", "text": "Before thresholding the norm of incoming observations, it is useful to decorrelate and standardize their components, i.e., to whiten the data. Then, we apply the algorithm to uncorrelated covariates, with zero mean and unit variance (not necessarily independent). The covariance matrix \u03a3 can be decomposed as \u03a3 = UDUT , where U is orthogonal, and D diagonal with dii = \u03bbi(\u03a3). We whiten each observation to X\u0304 = D\u22121/2UTX \u2208 Rd\u00d71 (while for X \u2208 Rk\u00d7d, X\u0304 = XUD\u22121/2), so that EX\u0304X\u0304T = Id. We denote whitened observations by X\u0304 and X\u0304 in the appendix. After some algebra we see that,\nd \u03bbmax(X\u0304T X\u0304) \u2264 Tr(\u03a3(XTX)\u22121) = Tr((X\u0304T X\u0304)\u22121) \u2264 d \u03bbmin(X\u0304T X\u0304) . (9)\nWe focus on algorithms that maximize the minimum eigenvalue of X\u0304T X\u0304 with high probability, or, in general, leading to large and even eigenvalues of X\u0304T X\u0304."}, {"heading": "B Proof of Theorem 3.1", "text": "Theorem B.1 Let n > k > d. Assume observations X \u2208 Rd are distributed according to subgaussian D with covariance matrix \u03a3 \u2208 Rd\u00d7d. Also, assume marginal densities are symmetric around zero after whitening. Let X be a k \u00d7 d matrix with k observations sampled from the distribution induced by the thresholding rule with parameters (\u03be,\u0393) \u2208 Rd+1+ satisfying (3). Let \u03b1 > 0, so that t = \u03b1 \u221a k \u2212 C \u221a d > 0, then, with probability at least 1\u2212 2 exp(\u2212ct2)\nTr(\u03a3(XTX)\u22121) \u2264 d (1\u2212 \u03b1)2 \u03c6k , (10)\nwhere constants c, C depend on the subgaussian norm of D\u0304.\nProof We would like to choose k out of n observations X1, . . . , Xn \u223c F iid. Assume our sampling induces a new distribution F\u0304. The loss we want to minimize for our OLS estimate \u03b2\u0302 = \u03b2\u0302(X,Y) is\nEX,Y\u223cF\u0304,X\u223cF\n[( XT \u03b2\u0302 \u2212XT\u03b2 )2] = \u03c32 EX,Y\u223cF\u0304 [ Tr ( \u03a3(XTX)\u22121 )] , (11)\nwhere we assumed Gaussian noise with variance \u03c32.\nLet us see how we construct F\u0304. We sample X \u223c F, we whiten the observation Z = \u03a3\u22121/2X , and then we select it or not according to a fixed thresholding rule. If \u2016Z\u2016\u03be \u2265 \u0393, then we keep X = \u03a31/2Z.\nWe choose \u03be and \u0393 so that there exists \u03c6 > 0, such that for all i = 1, . . . , d,\nEW\u223cF\u0304[W 2 (i)] = \u03c6, (12)\nwhere W(i) denotes the i-th component of W \u223c F\u0304. Note that F\u0304 = F\u0304(\u03be,\u0393).\nZ is a linear transformation of X; W is not a linear transformation of Z.\nMoreover, the covariance matrix of F\u0304 is \u03a3F\u0304 = \u03c6 Id. If F is a general subgaussian distribution, thresholding could change the mean away from zero.\nAssume after running our algorithm, we end up with X \u2208 Rk\u00d7d. We denote by W the observations after whitening, note that by design every w \u2208W passed our test: \u2016w\u2016\u03be \u2265 \u0393. In other words, w \u223c F\u0304. We see that W = X\u03a3\u22121/2 or, alternatively, X = W\u03a31/2.\nNow, we can derive\nTr ( \u03a3(XTX)\u22121 ) = Tr ( \u03a3 ( \u03a31/2WTW\u03a31/2 )\u22121)\n(13)\n= Tr (( \u03a3\u22121/2\u03a31/2WTW\u03a31/2\u03a3\u22121/2 )\u22121) (14)\n= Tr (( WTW )\u22121)\n(15) = Tr ( \u03a3 1/2\nF\u0304 \u03a3\u22121 F\u0304 \u03a3 1/2 F\u0304\n( WTW )\u22121) (16)\n= Tr (\n\u03a3\u22121 F\u0304\n( W\u0304TW\u0304 )\u22121) (17)\n= Tr\n( 1\n\u03c6 Id ( W\u0304TW\u0304 )\u22121) (18)\n= 1 \u03c6 Tr (( W\u0304TW\u0304 )\u22121)\n(19)\n\u2264 d \u03c6\n1 \u03bbmin ( W\u0304TW\u0304 ) = d \u03c6 k\n1 \u03bbmin ( 1 kW\u0304 TW\u0304 ) . (20)\nwhere W\u0304 is actually white data. Thus, note that W\u0304TW\u0304/k \u2192 Id as k \u2192\u221e.\nAssume that F is subgaussian such that if k > d, then XTX has full rank with probability one. Thresholding will not change the shape of the tails of the distribution, F\u0304 will also be subgaussian.\nAt this point, we need to measure how fast \u03bbmin ( W\u0304TW\u0304 ) /k goes to 1. We can use Theorem 5.39 in [26] which guarantees that, for \u03b1 > 0 such that t = \u03b1 \u221a k \u2212 C \u221a d > 0, with probability at least 1\u2212 2 exp(\u2212ct2) we have\n\u03bbmin\n( 1\nk W\u0304TW\u0304\n) \u2265 (1\u2212 \u03b1)2, (21)\nas W\u0304 is white subgaussian. It follows that for \u03b1 > C \u221a d/ \u221a k, with probability at least 1 \u2212 2 exp(\u2212ct2)\nTr ( \u03a3(XTX)\u22121 ) \u2264 d\n(1\u2212 \u03b1)2 \u03c6 k . (22)\nNote that 1/(1\u2212 \u03b1) \u2248 1 +O( \u221a d/k)."}, {"heading": "C Proof of Tr(X\u22121) \u2265 Tr(Diag(X)\u22121)", "text": "In order to justify that we want S = XTX to be as close as possible to diagonal, we show the following lemma. Under our assumptions S is symmetric positive definite with probability 1.\nLemma C.1 Let X be a n\u00d7 n symmetric positive definite matrix. Then,\nTr(X\u22121) \u2265 Tr(Diag(X)\u22121), (23)\nwhere Diag(\u00b7) returns a diagonal matrix with the same diagonal as the argument.\nIn other words, we show that for all positive definite matrices with the same diagonal elements, the diagonal matrix (matrix with all off diagonal elements being 0) has the least trace after the inverse operation.\nProof We show this by induction. Consider a 2\u00d7 2 matrix\nX = [ a b b c ] (24)\nand Tr(X\u22121) =\n1\nac\u2212 b2 (a+ c) (25)\nsince ac\u2212 b2 > 0 (X is positive definite), the above expression is minimized when b2 = 0, that is, X is diagonal.\nAssume the statement is true for all n\u00d7 n matrices. Let X be a (n+ 1)\u00d7 (n+ 1) positive definite matrix. Decompose it as\nX = [ A b bT c ] . (26)\nBy the block inverse formula, (see for example [21])\nTr(X\u22121) = Tr(A\u22121) + 1\nk +\n1 k Tr(A\u22121bbTA\u22121), (27)\nwhere k = c\u2212 bTA\u22121b. Note k > 0 by Schur\u2019s complement for positive definite matrices. Using the induction hypothesis, Tr(A\u22121) \u2265 Tr(Diag(A)\u22121). By the positive definiteness of A, bTA\u22121b \u2265 0, therefore 1k \u2265 1 c .\nAlso, Tr(A\u22121bbTA\u22121) \u2265 0. Thus,\nTr(X\u22121) \u2265 Tr(A\u22121) + 1 c = Tr(Diag(X)\u22121), (28)\nand the result follows."}, {"heading": "D Proof of Corollary 3.2", "text": "Corollary D.1 If the observations in Theorem 3.1 are jointly Gaussian with covariance matrix \u03a3 \u2208 Rd\u00d7d, \u03bej = 1 for all j = 1, . . . , d, and \u0393 = C\u0304 \u221a d+ 2 log(n/k), for some constant C\u0304 \u2265 1, then with probability at least 1\u2212 2 exp(\u2212ct2) we have that\nTr(\u03a3(XTX)\u22121) \u2264 d (1\u2212 \u03b1)2 (\n1 + 2 log(n/k)d\n) k . (29)\nProof We have to show that \u03bej = 1 for all j, and \u0393 = C \u221a d+ 2 log(n/k) satisfy the equations\nPD ( \u2016X\u0304\u2016\u03be \u2265 \u0393 ) = \u03b1 = k\nn , (30)\nED[X\u0304 2 j | \u2016X\u0304\u20162\u03be \u2265 \u03932] = \u03c6, for all j, (31)\nand \u03c6 > (1 + 2 log(n/k)/d). The components of X\u0304 are independent, as observations are jointly Gaussian. It immediately follows that \u03bej = 1, for all 1 \u2264 j \u2264 d. Thus,\nZ\u03be = d\u2211 j=1 X\u0304j \u223c \u03c72d, \u03932 = F\u22121\u03c72d ( 1\u2212 k n ) . (32)\nThe value of Z\u03be is strongly concentrated around its mean, EZ\u03be = d. We now use two tail approximations to obtain our desired result.\nBy [19], we have that P(Z\u03be \u2212 d \u2265 2 \u221a dx+ 2x) \u2264 exp(\u2212x). (33)\nIf we take exp(\u2212x) = \u03b1, then x = log(n/k). In this case, we conclude that\nP ( Z\u03be \u2265 d+ 2 log (n k ) + 2 \u221a d log (n k )) \u2264 \u03b1 = k n . (34)\nNote that P(\u2016X\u0304\u2016\u03be > \u0393) = P(Z\u03be > \u03932) = \u03b1. Therefore, by definition\n\u0393 \u2264 \u221a d+ 2 log (n k ) + 2 \u221a d log (n k ) . (35)\nOn the other hand, we would like to show that P ( Z\u03be \u2265 d+ 2 log (n k )) \u2265 \u03b1, (36)\nas that would directly imply that \u0393 \u2265 \u221a d+ 2 log (n/k).\nWe can use Proposition 3.1 of [15]. For d > 2 and x > d\u2212 2,\nP(Z\u03be \u2265 x) \u2265 1\u2212 e\u22122\n2\nx\nx\u2212 d+ 2 \u221a d exp\n{ \u22121\n2\n( x\u2212 d\u2212 (d\u2212 2) log (x d ) + log d )} .\nTake x = d+ 2\u03c8, where \u03c8 = log(n/k). It follows that\nP(Z\u03be \u2265 d+ 2\u03c8) \u2265 1\u2212 e\u22122\n2\nd+ 2\u03c8\n2 \u221a d+ 2\u03c8 exp\n{ \u22121\n2\n( 2\u03c8 \u2212 (d\u2212 2) log ( 1 + 2\u03c8\nd\n) + log d )} = 1\u2212 e\u22122\n2\nd+ 2\u03c8\n2 \u221a d+ 2\u03c8 exp\n{ d\u2212 2\n2 log\n( 1 + 2\u03c8\nd\n) \u2212 1\n2 log d\n} exp {\u2212\u03c8}\n\u2265 exp {\u2212\u03c8} , (37)\nwhere we assumed, for example, d \u2265 9 and n/k > 17 (as in Proposition 5.1 of [15]). In any case, in those rare cases (in our context) where d < 9 and n/k very small, the previous bound still holds if we subtract a small constant C \u2208 [0, 5/2] from the LHS: P(Z\u03be \u2265 d+ 2\u03c8 \u2212 C).\nEquivalently, from (37) P(Z\u03be \u2265 d+ 2 log(n/k)) \u2265 k/n = \u03b1. (38)\nWe conclude that \u221a d+ 2 log (n k ) \u2264 \u0393 \u2264 \u221a d+ 2 log (n k ) + 2 \u221a d log (n k ) . (39)\nFinally, we have that\n\u03c6 \u2265 \u0393 2 d \u2265 1 + 2 log (n/k) d . (40)\nBy Theorem B.1, the corollary follows."}, {"heading": "E CLT Approximation", "text": "As we explain in the main text, it is sometimes difficult to directly compute the distribution of the \u03be-norm of a white observation, given by Z\u03be. Recall that \u03932 = F\u22121Z\u03be (1\u2212 k/n). Fortunately, Z\u03be is the sum of d random variables, and, in high-dimensional spaces, a CLT approximation can help us to choose a good threshold. In this section we derive some theoretical guarantees.\nThe CLT is a good idea for bounded variables (as the square is still bounded, and therefore subgaussian), but if the underlying components Xj are unbounded subgaussian, Z\u03be will be at least subexponential\u2014as the square of a subgaussian random variable is subexponential, [26]\u2014, and a higher threshold \u2014like that coming from chi-squared\u2014 is more appropriate.\nIn addition, in the context of heavy-tails, catastrophic effects are expected, as P(maxj Xj > t) \u223c P( \u2211 j Xj > t), leading to observations dominated by single dimensions.\nAssume that components X\u0304j are independent (while not necessarily identically distributed). By Lyapunov\u2019s CLT, one can show that2\nZ\u03be = d\u2211 j=1 \u03bejX\u0304 2 j \u2248 N d, d\u2211 j=1 \u03be2j ( E[X\u03044j ]\u2212 1 ) . It follows that \u0393 satisfies PD ( \u2016X\u0304i\u2016\u03be \u2265 \u0393 ) = k/n if\n\u03932 \u2248 d+ \u03a6\u22121 (\n1\u2212 k n ) \u221a\u221a\u221a\u221a d\u2211 i=1 \u03be2i ( E[X\u03044i ]\u2212 1 ) .\nIn the sequel, assume d is large enough, and the approximation error is negligible. Define \u03b3 = \u221a\u2211d\ni=1 \u03be 2 i ( E[X\u03044i ]\u2212 1 ) .\nCorollary E.1 Assume Z\u03be = N ( d, \u03b32 ) and \u03932 = d+ \u03b3 \u03a6\u22121 (1\u2212 k/n), with \u03bej satisfying (3). Let \u03b1 > 0, so that t = \u03b1 \u221a k \u2212 C \u221a d > 0. then with probability at least 1\u2212 2 exp(\u2212ct2) we have that\nTr(\u03a3(XTX)\u22121) \u2264 d (1\u2212 \u03b1)2 k ( 1 + \u03b3 \u221a 2 log(n/k) d \u2212O ( \u03b3 log log(n/k) d \u221a log(n/k) )) . (41) Proof Note that, by definition, \u2016X\u20162\u03be \u223c Z\u03be and \u0393 jointly solve the equations required by Theorem B.1. In order to apply the theorem, all we need to do is to estimate the magnitude of\n\u03c6 = ED[X 2 j | \u2016X\u20162\u03be \u2265 \u03932] \u2265\n\u03932\nd = 1 +\n\u03b3 d \u03a6\u22121 (1\u2212 k/n) . (42)\nTherefore, we want to find bounds on tail probabilities of the normal distribution. By Theorem 2.1 of [15], we have that for small k/n\u221a\n2 log(n/k)\u2212 log(4 log(n/k)) + 2 2 \u221a 2 log(n/k) \u2264 \u03a6\u22121\n( 1\u2212 k\nn\n) (43)\n\u2264 \u221a\n2 log(n/k)\u2212 log(2 log(n/k)) + 3/2 2 \u221a 2 log(n/k) , (44)\nand the result follows.\nWe can also show how to apply the previous result to independent uniform distributions centered around zero. In that case, we have that the fourth moment is E[X\u03044j ] = 9/5, so \u03b3 = \u221a 4 5d, leading to a gain factor\n\u03c6 = ( 1 + \u221a 8 log(n/k)\n5d \u2212 o ( log log(n/k)\u221a d log(n/k) )) ."}, {"heading": "F Proof of Theorem 4.1", "text": "Theorem F.1 Let A be an algorithm for the problem we described in Section 2. Then,\nEA Tr(\u03a3(X TX)\u22121) \u2265 d\n2 E [\u2211k i=1 ||X\u0304(i)||2 ] (45)\n\u2265 d k E [ 1 d maxi\u2208[n] ||X\u0304i||2 ] , 2Some mild additional moment/regularity conditions on each X\u0304j are required to satisfy Lyapunov\u2019s Condition.\nwhere X\u0304(i) is the white observation with the i-th largest norm. Moreover, fix \u03b1 \u2208 (0, 1). Let F be the cdf of maxi\u2208[n] ||Xi||2. Then, with probability at least 1\u2212 \u03b1\nTr(\u03a3(XTX)\u22121) \u2265 d2/k F\u22121(1\u2212 \u03b1). (46)\nProof We want to minimize Tr(\u03a3(XTX)\u22121) = Tr((X\u0304T X\u0304)\u22121). Let us define S = X\u0304T X\u0304. One can prove that H \u2192 Tr(H\u22121) is convex for symmetric positive definite matrices H . It then follows by Jensen\u2019s Inequality (assuming k > d, so S is symmetric positive definite with high probability)\nETr(S\u22121) \u2265 Tr((ES)\u22121) = d\u2211 j=1\n1\n\u03bbj(ES) . (47)\nLet ES be the expected value of S for an arbitrary algorithm A that selects its observations sequentially. We want to understand what is the minimum possible value the RHS of (47) can take. The sum of eigenvalues is upper bounded by\nd\u2211 j=1 \u03bbj(ES) = Tr(ES) = d\u2211 j=1 E(Sjj) = d\u2211 j=1 k\u2211 i=1 E[X\u03042ij ]\n= k\u2211 i=1 E[||X\u0304i||2]\n\u2264 E [ k\u2211 i=1 ||X\u0304(i)||2 ]\n\u2264 k E [ max i\u2208[n] ||X\u0304i||2 ] ,\nwhere X\u0304(i) denotes the observation with the i-th largest norm. Because ES is symmetric positive definite, its eigenvalues are real non-negative, so that\n0 < \u03bbmin(ES) \u2264 Tr(ES)\nd \u2264\nE [\u2211k i=1 ||X\u0304(i)||2 ]\nd \u2264 k E\n[ maxi\u2208[n] ||X\u0304i||2 ] d .\nWe conclude that the solution to the minimization problem of (47) \u2014that is, when all eigenvalues are equal\u2014 is lower bounded by\nETr(S\u22121) \u2265 d\u2211 j=1\n1 \u03bbj(ES) \u2265 d\n2 E [\u2211k i=1 ||X\u0304(i)||2 ] \u2265 d2 k E [ maxi\u2208[n] ||X\u0304i||2 ] , which proves (45).\nIn order to prove the high-probability statement (46), note that\nTr(\u03a3(XTX)\u22121) = Tr((X\u0304T X\u0304)\u22121) = d\u2211 i=1\n1\n\u03bbi(X\u0304T X\u0304)\n\u2265 d\u2211 i=1 1\u2211k j=1 \u2016X\u0304j\u20162/d \u2265 d 2\u2211k\nj=1 \u2016X\u0304(j)\u20162 \u2265 d\n2\nkmaxi\u2208[n] \u2016X\u0304i\u20162 . (48)\nWe directly conclude that with probability at least 1\u2212 \u03b1, max i\u2208[n] \u2016X\u0304i\u20162 \u2264 F\u22121(1\u2212 \u03b1) (49)\nas F is the cdf of maxi\u2208[n] \u2016X\u0304i\u20162. It follows that with probability at least 1\u2212 \u03b1,\nTr(\u03a3(XTX)\u22121) \u2265 d 2\nk F\u22121(1\u2212 \u03b1) . (50)"}, {"heading": "G Proof of Corollary 4.2", "text": "Corollary G.1 For Gaussian observations Xi \u223c N (0,\u03a3) and large n, for any algorithm A\nEA Tr(\u03a3(X TX)\u22121) \u2265 d k (\n2 logn d + log log n ) . (51) Moreover, let \u03b1 \u2208 (0, 1). Then, for any A with probability at least 1\u2212 \u03b1 and C = 2 log \u0393(d/2)/d,\nTr(\u03a3(XTX)\u22121) \u2265 d k (\n2 logn d + log log n\u2212 1 d log log 1 1\u2212\u03b1 \u2212 C ) . (52) Proof In order to apply Theorem F.1, we need to upper bound E [ maxi\u2208[n] ||X\u0304i||2 ] , where X\u0304i is a d-dimensional gaussian random variable with identity covariance matrix. In other words, we need to upper bound the expected maximum of n chi-squared random variables with d degrees of freedom.\nLet us start by proving (51). We can use extreme value theory to find the limiting distribution of the maximum of n random variables. Firstly, note that the chi-squared distribution is a particular case of the Gamma distribution. More specifically, \u03c72d \u223c \u0393(d/2, 2). If we parameterize the \u0393 distribution by \u03b1 (shape) and \u03b2 (rate), then \u03b1 = d/2 and \u03b2 = 1/2.\nBy the Fisher-Tippett Theorem we know that there are only three limiting distributions for limn\u2192\u221eX(n) = limn\u2192\u221emaxi\u2264nXi, where the Xi are iid random variables, namely, Frechet, Weibull and Gumbel distributions. It is known that the Gamma distribution is in the max-domain of attraction of the Gumbel distribution. Further, the normalizing constants are known (see Chapter 3 of [11]). In particular, we know that if X(n) := maxi\u2208[n] ||X\u0304i||2\nlim n\u2192\u221e\nP ( X(n) \u2264 2x+ 2 lnn+ 2(d/2\u2212 1) ln lnn\u2212 2 ln \u0393(d/2) ) = \u039b(x) = e\u2212e \u2212x . (53)\nWe can assume that the asymptotic limit holds, as n is in practice very large, and compute the mean value of X(n). As X(n) is a positive random variable,\nE[X(n)] = \u222b \u221e 0 P ( X(n) \u2265 t ) dt (54)\n= \u222b \u221e 0 (1\u2212P ( X(n) \u2264 t ) ) dt (55)\nWe make the change of variables t = 2x + C, where C = 2 lnn + (d \u2212 2) ln lnn \u2212 2 ln \u0393(d/2). Then,\nE[X(n)] = \u222b \u221e 0 P ( X(n) \u2265 t ) dt (56)\n= \u222b \u221e \u2212C/2 2(1\u2212P ( X(n) \u2264 2x+ C ) ) dx (57)\n\u2248 \u222b \u221e \u2212C/2 2(1\u2212 e\u2212e \u2212x ) dx (58)\n= \u222b 0 \u2212C/2 2(1\u2212 e\u2212e \u2212x ) dx+ \u222b \u221e 0 2(1\u2212 e\u2212e \u2212x ) dx (59)\n\u2264 \u222b 0 \u2212C/2 2 dx+ 2\u03b3 = C + 2\u03b3, (60)\nwhere \u03b3 is the Euler\u2013Mascheroni constant. We conclude that\nE[X(n)] \u2264 C + 2\u03b3 \u2264 2 lnn+ (d\u2212 2) ln lnn. (61)\nIf we take the largest k observations, and assume we could split the weight equally among all dimensions (which is desirable), we see that the best we can do in expectation is upper bounded by\nk d E[X(n)] \u2264 k\n( 2 lnn\nd + ln lnn\n) . (62)\nNow, let us prove (52). The following inequalities simplify our task to finding a high-probability upper bound on maxi\u2208[n] \u2016X\u0304i\u20162. We have that\nTr(\u03a3(XTX)\u22121) = Tr((X\u0304T X\u0304)\u22121) = d\u2211 i=1\n1\n\u03bbi(X\u0304T X\u0304)\n\u2265 d\u2211 i=1 1\u2211k j=1 \u2016X\u0304j\u20162/d \u2265 d 2\u2211k\nj=1 \u2016X\u0304(j)\u20162 \u2265 d\n2\nkmaxi\u2208[n] \u2016X\u0304i\u20162 . (63)\nFix \u03b1 \u2208 [0, 1]. We need to find a constant Q such that with probability at least 1 \u2212 \u03b1, Q \u2265 maxi\u2208[n] \u2016X\u0304i\u20162, so that we conclude that Tr(\u03a3(XTX)\u22121) \u2265 d2/Qk with high probability. By (53) we know that\nlim n\u2192\u221e\nP ( X(n) \u2264 2x+ 2 lnn+ 2(d/2\u2212 1) ln lnn\u2212 2 ln \u0393(d/2) ) = \u039b(x) = e\u2212e \u2212x . (64)\nFor large n, we assume the previous upper bound for X(n) is exact. We want to find Q > 0 such that P ( X(n) \u2264 Q ) = 1\u2212 \u03b1. Note that if 1\u2212 \u03b1 = e\u2212e\u2212x , then\nx = \u2212 log log 1 1\u2212 \u03b1 . (65)\nIt follows that Q = 2 lnn + 2(d/2 \u2212 1) ln lnn \u2212 log log(1 \u2212 \u03b1)\u22121 \u2212 2 ln \u0393(d/2). Finally, (52) follows as\nQ d = 2 log n d + log log n\u2212 log log(1\u2212 \u03b1) \u22121 + 2 ln \u0393(d/2) d . (66)"}, {"heading": "H Proof of Theorem 3.3", "text": "Recall the Sparse Thresholding Algorithm below. We show the following theorem.\nTheorem H.1 Let D = N (0,\u03a3). Assume \u03a3, \u03bb and mini |\u03b2i| satisfy the standard conditions given in Theorem 3 of [27]. Assume we run the Sparse Thresholding algorithm with k1 = C \u2032s log d observations to recover the support of \u03b2, for an appropriate C \u2032 \u2265 0. Let X2 be k2 = k \u2212 k1 observations sampled via thresholding on S(\u03b2\u03021). It follows that for \u03b1 > 0 such that t = \u03b1 \u221a k2 \u2212\nC \u221a s > 0, there exist some universal constants c1, c2, and c, C that depend on the subgaussian norm of D\u0304 | S(\u03b2\u03021), such that with probability at least\n1\u2212 2e\u2212min(c2 min(s,log(d\u2212s))\u2212log(c1),ct 2\u2212log(2))\nit holds that Tr(\u03a3SS(X T 2 X2) \u22121) \u2264 s (1\u2212 \u03b1)2 (\n1 + 2 log(n2/k2)s ) k2 .\nFor support recovery, we use Theorem 3 from [27]:\nTheorem H.2 Consider the linear model with random Gaussian design\nY = X\u03b2\u2217 + , with k i.i.d. rows xi \u223c N (0,\u03a3) \u2208 Rd, (67)\nwith noise \u223c N (0, \u03c32 Idk\u00d7k). Assume the covariance matrix \u03a3 satisfies\n\u2016\u03a3SCS(\u03a3SS)\u22121\u2016\u221e \u2264 (1\u2212 \u03b3), for some \u03b3 \u2208 (0, 1], (68)\nAlgorithm 3 Sparse Thresholding Algorithm. 1: Set S1 = \u2205, S2 = \u2205. Let k = k1 + k2, n = k1 + n2. 2: for observation 1 \u2264 i \u2264 k1 do 3: Observe Xi. Choose Xi: S1 = S1 \u222aXi. 4: end for 5: Set \u03b3 = 1/2, \u03bb = \u221a 4\u03c32 log(d)/\u03b32k1.\n6: Compute Lasso estimate \u03b2\u03021 based on S1, with regularization \u03bb. 7: Set weights: \u03bei = 1 if i \u2208 S(\u03b2\u03021), \u03bei = 0 otherwise. 8: Set \u0393 = C \u221a s+ 2 log(n2/k2). Factorize \u03a3S(\u03b2\u03021)S(\u03b2\u03021) = UDU\nT . 9: for observation k1 + 1 \u2264 i \u2264 n do\n10: Observe Xi \u2208 Rd. Restrict to XiS := XiS(\u03b2\u03021) \u2208 R s. 11: Compute XiS = D\u22121/2UTXiS . 12: if \u2016XiS\u2016\u03be > \u0393 or k2 \u2212 |S2| = n\u2212 i+ 1 then 13: Choose XiS : S2 = S2 \u222aXiS . 14: if |S2| = k2 then 15: break. 16: end if 17: end if 18: end for 19: Return OLS estimate \u03b2\u03022 with observations in S2.\n\u03bbmin(\u03a3SS) \u2265 Cmin > 0. (69)\nLet |S| = s. Consider the family of regularization parameters for \u03c6d \u2265 2\n\u03bbk(\u03c6d) =\n\u221a \u03c6d \u03c1u(\u03a3SCS)\n\u03b32 2\u03c32 log(d) k . (70)\nIf for some fixed \u03b4 > 0, the sequence (k, d, s) and regularization sequence {\u03bbk} satisfy\nk\n2s log(d\u2212 s) \u2265 (1 + \u03b4) \u03b8u(\u03a3)\n( 1 +\n\u03c32Cmin \u03bb2ks\n) , (71)\nthen the following holds with prob at least 1\u2212 c1 exp(\u2212c2 min{s, log(d\u2212 s)}):\n1. The Lasso has a unique solution \u03b2\u0302 with support in S (i.e. S(\u03b2\u0302) \u2282 S(\u03b2\u2217)).\n2. Define the gap\ng(\u03bbk) := c3\u03bbk\u2016\u03a3\u22121/2SS \u2016 2 \u221e + 20\n\u221a \u03c32 log s\nCmin k . (72)\nThen, if \u03b2min := mini\u2208S |\u03b2\u2217i | > g(\u03bbk), the signed support S\u00b1(\u03b2\u0302) is identical to S\u00b1(\u03b2\u2217), and moreover \u2016\u03b2\u0302S \u2212 \u03b2\u2217S\u2016\u221e \u2264 g(\u03bbk).\nThe required definitions to apply the previous theorem are\n\u03c1l(\u03a3) = 1\n2 min i 6=j (\u03a3ii + \u03a3jj \u2212 2\u03a3ij) , \u03c1u(\u03a3) = max i \u03a3ii, (73)\n\u03b8l(\u03a3) = \u03c1l(\u03a3SC |S)\nCmax(2\u2212 \u03b3(\u03a3))2 , \u03b8l(\u03a3) =\n\u03c1u(\u03a3SC |S) Cmin\u03b32(\u03a3) . (74)\nProof (Theorem H1)\nLet X \u223c N (0,\u03a3) with \u03a3 satisfying (68) and (69). Let \u03bbk(\u03c6d) be like in (70), for some \u03c6d > 2. Assume we choose the number of observations k1 in the first stage to be at least\nk1 \u2265 2(1 + \u03b4) \u03b8u(\u03a3) (\n1 + \u03c32Cmin \u03bb2ks\n) s log(d\u2212 s) (75)\n= C(\u03a3, d, s) s log(d\u2212 s), (76) and that \u03b2min is greater than (72). Then, with probability at least\n1\u2212 c1 exp(\u2212c2 min{s, log(d\u2212 s)}),\nwe recover the right support S(\u03b2\u2217) = S(\u03b2\u0302) in the first stage of the algorithm.\nConditional on this event, we apply our algorithm on the remaining observations. In the second stage, we only look at those dimensions in S(\u03b2\u0302), by setting weights \u03beS(\u03b2\u0302) = 1, and zero otherwise. Finally, we run OLS along the dimensions in the recovered support, and using the observations collected during the second stage. Importantly, note that the new observations are N (0,\u03a3SS).\nWe can now apply our original results. Denote by X2 \u2208 Rk2\u00d7s the set of observations collected in the second stage of the algorithm. In particular, by Corollary D.1, we conclude that for \u03b1 > 0 such that t = \u03b1 \u221a k2 \u2212 C \u221a s > 0, the following holds with probability at least 1\u2212 2 exp(\u2212ct2)\nTr(\u03a3SS(X T 2 X2)\n\u22121) \u2264 s (1\u2212 \u03b1)2 (\n1 + 2 log(n2/k2)s ) k2 . (77)\nUnder the event that the recovery is correct, the contribution to the MSE of the components of \u03b2 that are not in its support is zero. In other words,\n\u2016\u03b2 \u2212 \u03b2\u03022\u20162\u03a3 = (\u03b2 \u2212 \u03b2\u03022)T\u03a3(\u03b2 \u2212 \u03b2\u03022) (78) = (\u03b2S \u2212 \u03b2\u03022S)T\u03a3SS(\u03b2S \u2212 \u03b2\u03022S) = \u2016\u03b2S \u2212 \u03b2\u03022S\u20162\u03a3SS . (79)\nAs the events that the first and second stages succeed are independent, we conclude (77) holds with probability at least\n1\u2212 c1e\u2212c2 min{s,log(d\u2212s)} \u2212 2e\u2212ct 2 \u2265 (80)\n1\u2212 2e\u2212min(c2 min(s,log(d\u2212s))\u2212log(c1),ct 2\u2212log(2)). (81)"}, {"heading": "I Proof of CLT Lower Bound", "text": "Corollary I.1 Assume the norm of white observations is distributed according to Z\u03be = N ( d, \u03b32 ) . Then, we have that for any algorithm A\nEA Tr(\u03a3(X TX)\u22121) \u2265 d(\n1 + \u03b3d \u221a 2 log n ) k . (82)\nProof By Theorem F.1, we need to compute E [ maxi\u2208[n] ||Xi||2 ] .\nBy assumption \u2016Xi\u20162 \u223c N ( d, \u03b32 ) for each i, which implies\nE [ max i\u2208[n] ||Xi||2 ] = E [ d+ max i\u2208[n] \u03b3 ||Xi||2 \u2212 d \u03b3 ] (83)\n\u2264 d+ \u03b3 E [ max i\u2208[n] N (0, 1) ] (84)\n\u2264 d+ \u03b3 \u221a 2 log n, (85)\nand the result follows."}, {"heading": "J Ridge Regression", "text": "Regularized linear estimators also benefit from large and balanced observations. We show that, under mild assumptions, the performance of the ridge regression is directly aligned with that of previous sections.\nThe ridge estimator is \u03b2\u0302\u03bb = ( XTX + \u03bbI )\u22121 XTY, given (X,Y) and \u03bb > 0. The following result\nshows how large values of \u03bbmin(XTX) help to control the MSE of \u03b2\u0302\u03bb. As the optimal penalty parameter \u03bb\u2217 is unknown until the end of the data collection process, we assume it is uniformly random in a small interval.\nTheorem J.1 Let R > 0. Assume the penalty parameter for ridge regression is chosen uniformly at random \u03bb\u2217 \u223c U [0, R]. Then, the MSE of \u03b2\u0302\u03bb\u2217 is upper bounded by\nE\u03bb\u2217,X \u2016\u03b2\u0302\u03bb\u2217 \u2212 \u03b2\u2217\u20162 \u2264 EX f ( \u03bbmin(X TX) ) , (86)\nwhere f is the following decreasing function of \u03bbmin:\nf(\u03bbmin) = \u03c32 d\n\u03bbmin +R + \u2016\u03b2\u2217\u201622\n( 1\u2212 2\u03bbmin\nR log\n( 1 + R\n\u03bbmin\n) +\n\u03bbmin \u03bbmin +R\n) . (87)\nProof The SVD decomposition of X = USV T implies that XTX = V SUTUSV T = V S2V T , where U and V are orthogonal matrices.\nWe define W = ( XTX + \u03bbI )\u22121 , and see that\nW = ( V (S2 + \u03bbI)V T )\u22121 = V Diag\n( 1\ns2jj + \u03bb )d j=1 V T .\nIn this case, the MSE of \u03b2\u0302\u03bb has two sources: squared bias and the trace of the covariance matrix. The covariance matrix of \u03b2\u0302\u03bb is Cov(\u03b2\u0302\u03bb) = \u03c32 WXTXW, while its bias is given by \u2212\u03bbW\u03b2\u2217 (see [13]). Thus,\nCov(\u03b2\u0302\u03bb) = \u03c32 V Diag\n( s2jj\n(s2jj + \u03bb) 2 )d j=1 V T . (88)\nNote that s2jj = \u03bbj , where sjj\u2019s are the singular values of X, and \u03bbj\u2019s the eigenvalues of X TX. As V is orthogonal, Tr [ Cov(\u03b2\u0302\u03bb) ] = \u03c32 \u2211d j=1 \u03bbj/(\u03bbj + \u03bb) 2.\nUnfortunately, in practice, the value of \u03bb is unknown before collecting the data. A common technique consists in using an additional validation set to choose the optimal regularization parameter \u03bb\u2217. Generally, in supervised learning, the validation set comes from the same distribution as the test set, while in active learning it does not. As in the unregularized case, we want to train on unlikely data, but we want to test on likely data. We achieve robustness against this fact as follows. We fix some fairly large R > 0 such that we assume \u03bb\u2217 \u2208 (0, R). We treat \u03bb\u2217 as a random variable, and we impose a uniform prior D\u03bb over (0, R).\nThen, we see that\nE\u03bb\u2217\u223cD\u03bb [ Tr [ Cov(\u03b2\u0302\u03bb\u2217) ]] = \u03c32 d\u2211 j=1 \u03bbj \u222b R 0\n1 (\u03bbj + \u03bb)2 1 R d\u03bb\n= \u03c32 d\u2211 j=1\n1 \u03bbj +R \u2264 \u03c3\n2 d\n\u03bbmin +R . (89)\nThe squared bias can be upper bounded by\n\u03bb2 \u03b2\u2217TWTW\u03b2\u2217 = \u03b2\u2217TV Diag\n[ \u03bb2\n(\u03bbj + \u03bb)2 ] j V T\u03b2\u2217\n\u2264 \u2016\u03b2\u2217\u201622 max i\n( \u03bb\n\u03bbj + \u03bb )2 = \u2016\u03b2\u2217\u201622 ( \u03bb\n\u03bbmin + \u03bb\n)2 . (90)\nfor every \u03bb > 0, as \u03bbj \u2265 0 for all j. Taking expectations on both sides of (90) with respect to \u03bb\u2217 \u223c D\u03bb, and after some algebra\nED\u03bbBias 2(\u03b2\u0302\u03bb\u2217)\n\u2016\u03b2\u2217\u201622 \u2264 1\u2212 2\u03bbmin R log\n( 1 + R\n\u03bbmin\n) +\n\u03bbmin \u03bbmin +R , (91)\nwhere the RHS is a decreasing function of \u03bbmin that tends to zero as \u03bbmin grows.\nIt follows that E \u2016\u03b2\u0302\u03bb\u2217 \u2212 \u03b2\u2217\u20162 can be controlled by minimizing \u03bbmin(XTX), and we can focus on minimizing \u03bbmin(X\u0304T X\u0304) by the equivalence shown in the Problem Definition section of the main paper."}, {"heading": "K Simulations", "text": "We conducted several experiments in various settings. We present here some experiments that complement those showed in the main paper. In particular, we show experiments for linear models, synthetic linear models, synthetic non-linear data, and additional regularized and real-world datasets.\nK.1 Linear Models\nWe first empirically show the results proved in Theorem B.1. For a sequence of values of n, we choose k = \u221a n observations in Rd, with fixed d = 10. The observations are generated according to N (0, Id), and y follows a linear model with \u03b2i \u223c U(\u22125, 5). For each tuple (n, k) we repeat the experiment 200 times, and compute the squared error (\u03b2\u2217 is known). The results in Figure 3 (a) show the average MSE of Algorithm 1 significantly outperforms that of random sampling. We also see a strong variance reduction. Figure 3 (b) restricts the comparison to fixed and adaptive threshold algorithms; while the latter outperforms the former, the difference is small. In Figure 3 (c) we keep n and d fixed, and vary k. Finally, in Figure 4 (a) we show the case where \u03a3 6= Id. For completeness, we repeated the simulation with observations generated according to a joint Gaussian distribution with a random covariance matrix that had Tr(\u03a3) = 21.59, \u03bbmin = 0.65, and \u03bbmax = 3.97. Figure 4 (a) shows that thresholding algorithms outperform random sampling in a similar way as in the white case presented in the paper. Also, Figure 4 (b) shows how the adaptive threshold slightly beats the fixed one.\nFinally, in Figure 4 (c), we show the results of simulations when observations are sampled from Laplace correlated marginals (through a Gaussian Copula). We compare random sampling to two versions of the thresholding algorithm. The most simple one, denoted by Unif-Weig Algorithm, assigns uniform weights (i.e., \u03bei = 1 for all i). On the other hand, denote by Opt-Weig Algorithm the algorithm that uses the optimal weights (previously pre-computed, in this case maxi \u03bei/mini \u03bei \u2248 7, independent variables tend to require higher weights). As one would expect, the latter does better than the former. However, it is remarkable that the difference between random and thresholding is way more substantial than the difference between optimal and approximate thresholding, an observation that can be very useful in practice.\nK.2 Synthetic Non-Linear Data\nThe theory and algorithms presented in this paper are based on the linearity of the model. To understand the impact of this assumption, we perform an experiment where the response model was\ny = xT\u03b2 + \u03c8xTx for various values of \u03c8, and \u03b2i \u223c U(\u22125, 5). Note that high-order terms and transformations can easily be included in the design matrix (not done in this case). As expected, the results in Figure 5 show an intersection point. The active learning algorithms are robust to some level of non-linearity but, at some point, random sampling becomes more effective.\nK.3 Regularization\nAn appealing property of the proposed algorithms is that their gain is preserved under regularized estimators such as ridge and lasso. This is specially relevant as it allows for higher dimensional models where transformations and interactions of the original variables are added to better capture non-linearities in the data and regularization is used to avoid overfitting. In fact, our algorithm can be thought of as a type of regularizing process.\nWe repeated the first experiment from the linear model simulations, using the ridge estimator with \u03bb = 0.01. Figure 6 (a) shows that the average MSEs of Algorithms 1 and 1b strongly outperform the results of random sampling. Their variance is less than 30% that of random sampling in all cases.\nWe performed two experiments with Lasso estimators to investigate the behavior of our algorithms in the presence of sparse models. We do not test Algorithm 2 here, but only simple thresholding approaches. First, we fixed n = 5000, k = 150, d = 70 and white Gaussian data. The dimension of the latent subspace, or effective dimension of the model, ranges from deff = 5 to deff = 70. Results are shown in Figure 6 (b). Algorithm 1 and Algorithm 1b strongly improve the performance of random sampling, while their variance is at most half that of random sampling.\nIn the second experiment, we fixed deff = 7, and progressively increased the dimension of the space d from d = 70 to d = 450. Also, we kept fixed n = 1000 and k = 100. Results are shown in Figure 6 (c).\nThresholding algorithms consistently decrease the MSE of the lasso estimator with respect to random sampling, even though we are adding a large number of purely noisy dimensions. The reason is simple. While these algorithms do not actively try to find the latent subspace (Algorithm 2 does), their observations will be, on average, larger in those dimensions too. There may be ways to leverage this\nfact, like batched approaches where weights \u03be are updated by giving more importance to promising dimensions.\nK.4 Real World Datasets\nThe Combined Cycle Power dataset has 9568 observations. The outcome is the net hourly electrical energy output of the plant, and it has d = 4 covariates: temperature, pressure, humidity, and exhaust vacuum. In Figure 7, we see the phenomenon explained in the main paper (for large k, the gain vanishes). In this case, and after adding all second order interactions, active learning solves the problem. Random sampling with interactions is not shown as the error was much larger.\nIn addition, in Figure 8 we show the scatterplots of the datasets used in the paper (we omitted the YearPredictionMSD dataset as d = 90)."}], "references": [{"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Margin based active learning", "author": ["M.-F. Balcan", "A. Broder", "T. Zhang"], "venue": "In Learning Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "The true sample complexity of active learning", "author": ["M.-F. Balcan", "S. Hanneke", "J.W. Vaughan"], "venue": "Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Maximizing expected model change for active learning in regression", "author": ["W. Cai", "Y. Zhang", "J. Zhou"], "venue": "In Data Mining (ICDM),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Minimax bounds for active learning", "author": ["R.M. Castro", "R.D. Nowak"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Active learning with statistical models", "author": ["D.A. Cohn", "Z. Ghahramani", "M.I. Jordan"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Hierarchical sampling for active learning", "author": ["S. Dasgupta", "D. Hsu"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "C. Monteleoni", "D.J. Hsu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Modelling extremal events, volume 33", "author": ["P. Embrechts", "C. Kl\u00fcppelberg", "T. Mikosch"], "venue": "Springer Science & Business Media,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Event labeling combining ensemble detectors and background knowledge", "author": ["H. Fanaee-T", "J. Gama"], "venue": "Progress in Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["A.E. Hoerl", "R.W. Kennard"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1970}, {"title": "Heavy-tailed regression with a generalized median-of-means", "author": ["D. Hsu", "S. Sabato"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Inequalities for quantiles of the chi-square distribution", "author": ["T. Inglot"], "venue": "Probability and Mathematical Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Variable selection in high-dimension with random designs and orthogonal matching pursuit", "author": ["A. Joseph"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Rademacher complexities and bounding the excess risk in active learning", "author": ["V. Koltchinskii"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Nonmyopic active learning of gaussian processes: an explorationexploitation approach", "author": ["A. Krause", "C. Guestrin"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["B. Laurent", "P. Massart"], "venue": "Annals of Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Optimal design of experiments, volume", "author": ["F. Pukelsheim"], "venue": "siam,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1993}, {"title": "Active regression by stratification", "author": ["S. Sabato", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Pool-based active learning in approximate linear regression", "author": ["M. Sugiyama", "S. Nakajima"], "venue": "Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Signal recovery from partial information via orthogonal matching pursuit", "author": ["J. Tropp", "A.C. Gilbert"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery usingconstrained quadratic programming (lasso)", "author": ["M.J. Wainwright"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Noise-adaptive margin-based active learning and lower bounds under tsybakov noise condition", "author": ["Y. Wang", "A. Singh"], "venue": "arXiv preprint arXiv:1406.5383,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": ", [3, 6, 7, 8, 17].", "startOffset": 2, "endOffset": 18}, {"referenceID": 4, "context": ", [3, 6, 7, 8, 17].", "startOffset": 2, "endOffset": 18}, {"referenceID": 5, "context": ", [3, 6, 7, 8, 17].", "startOffset": 2, "endOffset": 18}, {"referenceID": 6, "context": ", [3, 6, 7, 8, 17].", "startOffset": 2, "endOffset": 18}, {"referenceID": 15, "context": ", [3, 6, 7, 8, 17].", "startOffset": 2, "endOffset": 18}, {"referenceID": 0, "context": ", [1, 2, 9, 10, 28].", "startOffset": 2, "endOffset": 19}, {"referenceID": 1, "context": ", [1, 2, 9, 10, 28].", "startOffset": 2, "endOffset": 19}, {"referenceID": 7, "context": ", [1, 2, 9, 10, 28].", "startOffset": 2, "endOffset": 19}, {"referenceID": 8, "context": ", [1, 2, 9, 10, 28].", "startOffset": 2, "endOffset": 19}, {"referenceID": 24, "context": ", [1, 2, 9, 10, 28].", "startOffset": 2, "endOffset": 19}, {"referenceID": 3, "context": ", [5, 18, 24] and the references within.", "startOffset": 2, "endOffset": 13}, {"referenceID": 16, "context": ", [5, 18, 24] and the references within.", "startOffset": 2, "endOffset": 13}, {"referenceID": 20, "context": ", [5, 18, 24] and the references within.", "startOffset": 2, "endOffset": 13}, {"referenceID": 19, "context": "A closely related work to our setting is [23]: they study online or stream-based active learning for linear regression, with random design.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "They propose a theoretical algorithm that partitions the space by stratification based on Monte-Carlo methods, where a recently proposed algorithm for linear regression [14] is used as a black box.", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "While [23] does not tackle high-dimensional settings, we overcome the exponential data requirements via l1-regularization.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "This is related to the A-optimality criterion, [22].", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "observations, \u03b2\u0302k := \u03b2\u0302 OLS k has sampling distribution \u03b2\u0302k | X \u223c N (\u03b2\u2217, \u03c32(XTX)\u22121), [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "Note that O(d) observations are required for accurate recovery when D is subgaussian, and O(d log d) if subexponential, [26].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "Since the variance of the MSE for fixed X depends on \u2211 j 1/\u03bbj(X X) [13], it is also minimized by selecting observations that lead to large eigenvalues of XX.", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "Based, for example, on the results of [25] and Theorem 1 in [16], we could extend our results to subgaussian data via the Orthogonal Matching Pursuit algorithm for recovery.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Based, for example, on the results of [25] and Theorem 1 in [16], we could extend our results to subgaussian data via the Orthogonal Matching Pursuit algorithm for recovery.", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "3 summarizes the performance of Algorithm 2; it requires the standard assumptions on \u03a3, \u03bb and mini |\u03b2i| for support recovery (see Theorem 3 in [27]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 23, "context": "Assume \u03a3, \u03bb and mini |\u03b2i| satisfy the standard conditions given in Theorem 3 of [27].", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "The proof is a direct application of spectral results in [26], which are derived via a covering argument using a discrete net N on the unit Euclidean sphere Sd\u22121, together with a Bernstein-type concentration inequality that controls deviations of \u2016Xw\u20162 for each element w \u2208 N in the net.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "More generally, our probabilistic bounds are strongest when k \u2265 Cd log d for some constant C \u2265 0, a common situation in active learning [23], where super-linear requirements in d seem unavoidable in noisy settings.", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "In the Bike Sharing dataset [12] we predict the number of hourly users of the service, given weather conditions, including temperature, wind speed, humidity, and temporal covariates.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "References [1] M.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[22] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[23] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[25] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[26] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[27] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "39 in [26] which guarantees that, for \u03b1 > 0 such that t = \u03b1 \u221a k \u2212 C \u221a d > 0, with probability at least 1\u2212 2 exp(\u2212ct) we have", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "By [19], we have that P(Z\u03be \u2212 d \u2265 2 \u221a dx+ 2x) \u2264 exp(\u2212x).", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "1 of [15].", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "1 of [15]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "The CLT is a good idea for bounded variables (as the square is still bounded, and therefore subgaussian), but if the underlying components Xj are unbounded subgaussian, Z\u03be will be at least subexponential\u2014as the square of a subgaussian random variable is subexponential, [26]\u2014, and a higher threshold \u2014like that coming from chi-squared\u2014 is more appropriate.", "startOffset": 270, "endOffset": 274}, {"referenceID": 13, "context": "1 of [15], we have that for small k/n \u221a 2 log(n/k)\u2212 log(4 log(n/k)) + 2 2 \u221a 2 log(n/k) \u2264 \u03a6\u22121 ( 1\u2212 k n ) (43)", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "Further, the normalizing constants are known (see Chapter 3 of [11]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "Fix \u03b1 \u2208 [0, 1].", "startOffset": 8, "endOffset": 14}, {"referenceID": 23, "context": "Assume \u03a3, \u03bb and mini |\u03b2i| satisfy the standard conditions given in Theorem 3 of [27].", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "For support recovery, we use Theorem 3 from [27]: Theorem H.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "The covariance matrix of \u03b2\u0302\u03bb is Cov(\u03b2\u0302\u03bb) = \u03c3 WXXW, while its bias is given by \u2212\u03bbW\u03b2\u2217 (see [13]).", "startOffset": 89, "endOffset": 93}], "year": 2016, "abstractText": "We consider the problem of online active learning to collect data for regression modeling. Specifically, we consider a decision maker with a limited experimentation budget who must efficiently learn an underlying linear population model. Our main contribution is a novel threshold-based algorithm for selection of most informative observations; we characterize its performance and fundamental lower bounds. We extend the algorithm and its guarantees to sparse linear regression in high-dimensional settings. Simulations suggest the algorithm is remarkably robust: it provides significant benefits over passive random sampling in real-world datasets that exhibit high nonlinearity and high dimensionality \u2014 significantly reducing both the mean and variance of the squared error.", "creator": "LaTeX with hyperref package"}}}