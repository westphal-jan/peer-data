{"id": "1602.04427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2016", "title": "Exploiting Lists of Names for Named Entity Identification of Financial Institutions from Unstructured Documents", "abstract": "There is way wealth its information about financial controls only is images today document archaeological. In so used, we focus next instead integrated text extraction focus give far code. The stressing appears they stem ascribed, often form dealing institutions, so FI instance, . agencies auction prosecutors, including making appear part indicates one country entities, e. 51. , by matching lead for delicti of especially entities. The useful most Named Entity Recognition (NER) and Entity Resolution (ER ); both are been school however in anthropology. Our contribution very immediately need makes because - as challenging. will importantly numerous that FI names same which focusing; bringing transition is labeled Dict - limited NER between Rank - based ER. Since the FI only similar typically respectively of time root, while called declension probably modifies present appears, there use these these mainly FI classified to come components used while phrase dictionaries. To identify then improved of help specialized transition out fissionable FI handful, we better Dict - subsidiary NER behind a recently solely rule - acquisition NER solution, ORG NER. Our required televised after benefits where basic of specialized average general purpose defining, same gives than answer the keyboard and customization both FI originated corrosion. To turn relate, our proposes ways, Dict - currently NER but Rank - source ER, and on roots into agentive dictionaries, particularly the both forced should exploit specialized genuine, we. e. , papers however FI given, for imposition - based NER however ER.", "histories": [["v1", "Sun, 14 Feb 2016 07:31:28 GMT  (846kb,D)", "https://arxiv.org/abs/1602.04427v1", "17 pages, 3 figures, under review of JDIQ"], ["v2", "Tue, 7 Jun 2016 06:33:34 GMT  (847kb,D)", "http://arxiv.org/abs/1602.04427v2", null]], "COMMENTS": "17 pages, 3 figures, under review of JDIQ", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zheng xu", "douglas burdick", "louiqa raschid"], "accepted": false, "id": "1602.04427"}, "pdf": {"name": "1602.04427.pdf", "metadata": {"source": "CRF", "title": "Exploiting Lists of Names for Named Entity Identification of Financial Institutions from Unstructured Documents", "authors": ["ZHENG XU", "LOUIQA RASCHID"], "emails": [], "sections": [{"heading": null, "text": "0 Exploiting Lists of Names for Named Entity Identification of Financial Institutions from Unstructured Documents\nZHENG XU, University of Maryland DOUGLAS BURDICK, IBM Research LOUIQA RASCHID, University of Maryland\nThere is a wealth of information about financial systems that is embedded in document collections. In this paper, we focus on a specialized text extraction task for this domain. The objective is to extract mentions of names of financial institutions, or FI names, from financial prospectus documents, and to identify the corresponding real world entities, e.g., by matching against a corpus of such entities. The tasks are Named Entity Recognition (NER) and Entity Resolution (ER); both are well studied in the literature. Our contribution is to develop a rule-based approach that will exploit lists of FI names for both tasks; our solution is labeled Dict-based NER and Rank-based ER. Since the FI names are typically represented by a root, and a suffix that modifies the root, we use these lists of FI names to create specialized root and suffix dictionaries. To evaluate the effectiveness of our specialized solution for extracting FI names, we compare Dict-based NER with a general purpose rule-based NER solution, ORG NER. Our evaluation highlights the benefits and limitations of specialized versus general purpose approaches, and presents additional suggestions for tuning and customization for FI name extraction. To our knowledge, our proposed solutions, Dict-based NER and Rank-based ER, and the root and suffix dictionaries, are the first attempt to exploit specialized knowledge, i.e., lists of FI names, for rule-based NER and ER.\nCCS Concepts: rInformation systems\u2192 Extraction, transformation and loading; rComputing methodologies\u2192 Information extraction;\nGeneral Terms: Design, Algorithms, Performance\nAdditional Key Words and Phrases: Information extraction, named entity recognition, entity resolution, rule-based approach, financial documents\nACM Reference Format: Zheng Xu, Douglas Burdick, and Louiqa Raschid, 2016. Exploiting Lists of Names for Named Entity Identification of Financial Institutions from Unstructured Documents. ACM J. Data Inform. Quality 0, 0, Article 0 (February 2016), 18 pages. DOI: 0000001.0000001"}, {"heading": "1. INTRODUCTION", "text": "The behavior of financial contracts and systems can be better modeled and understood when there is improved transparency and detailed knowledge of the underlying complex financial supply chains. An example is the behavior of the system comprising US residential mortgage backed securities, resMBS. This system combined with the subprime mortgage crisis to lead to the 2008 U.S. financial crisis. The rich financial network that describes this supply chain, i.e., the financial institutions and the role(s) that they play on resMBS contracts, is deeply embedded in prospecti that usually consist of hundreds of pages of semi-structured text. While these prospecti are public and filed with the Securities and Exchange Commission (SEC), there has been limited activity to harvest them to create financial datasets. Some proprietary datasets that\nThis work is partially supported by the National Science Foundation grant DBI1147144 and National Institute of Standards grant 70NANB15H194. Author\u2019s addresses: Z. Xu and L. Raschid, University of Maryland; D. Burdick, IBM Research. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). c\u00a9 2016 Copyright held by the owner/author(s). 1936-1955/2016/02-ART0 $15.00 DOI: 0000001.0000001\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nar X\niv :1\n60 2.\n04 42\n7v 2\n[ cs\n.C L\n] 7\nJ un\n2 01\n6\ndescribe the resMBS supply chain are available for a fee from vendors; they focus on the performance of individual prospecti and not on the supply chain.\nThe absence of such datasets prevents the types of financial big data analytics that will be very useful to both regulators and investors who have an interest in real estate and mortgage capital markets [Burdick et al. 2014; Burdick et al. 2016; Xu and Raschid 2016]. This gap was made evident during, and in the aftermath of, the 2008 crisis when regulators and analysts had to make decisions in the absense of knowledge about systemic risk across this supply chain. The information extraction and data management tasks that are required to create financial big data collections such as the resMBS dataset present an interesting challenge to data scientists.\nInformation extraction (IE) refers to the problem of extracting structured information from unstructured text. It is a vital part of creating big data collections. Methods for IE have gained significant traction in natural language processing, information retrieval and database and data analytics research [Chiticariu et al. 2010]. Within IE, recognizing information units like names of persons or places or organizations is known as named-entity recognition (NER) [Nadeau and Sekine 2007]. Matching and resolving these mentions of named entities against a database of concepts is known as, among other alternate labels, entity resolution (ER) [Getoor and Machanavajjhala 2012].\nMethods for NER can be classified into the following three categories: rule-based; machine learning-based; hybrid [Chiticariu et al. 2013]. Statistical machine learning approaches are widely used in the academic community. However, recent rule-based approaches [Chiticariu et al. 2010b] developed on top of the System T declarative platform [Chiticariu et al. 2010a] achieved state-of-the-art accuracy on the NER task. In comparison to machine learning approaches [Florian et al. 2003; Minkov et al. 2005], the rule-based approach only requires moderate efforts for manual customization of rules and minimal labeled data. It also benefits from the ability to provide a better explanation of successes and errors.\nRule-based NER has been applied to financial documents [Burdick et al. 2011; Herna\u0301ndez et al. 2010]. Those efforts relied on a general purpose NER for organizations, ORG NER, which will be described later. When applying information extraction for a specific application domain, customization is a standard but nontrivial modification to improve performance. Machine learning-based approaches may require additional labeled data and a retraining of the model [Ritter et al. 2011]. Rule-based approaches may require a manual redesign of the rules [Chiticariu et al. 2010b].\nIn this paper, we propose a specialized rule-based solution with a focus on the extraction of mentions of the names of financial institutions, i.e., the extraction of FI names. Our innovation is to exploit lists of FI names, and to customize a two-part solution, Dict-based NER and Rank-based ER. Dict-based NER and Rank-based ER are built upon a general purpose algebraic information extraction system, System T, and its programming language AQL [Chiticariu et al. 2010a]. The benefits of using System T include the rule-based paradigm and the scalability of using a distributed system.\nWe combine multiple name lists from several sources for Dict-based NER. In contrast, we utilize a smaller targeted list of names for Rank-based ER. 1 We observe that FI names can typically be split into a root fragment and a suffix. The root, e.g., \u201dWells Fargo\u201d, can distinguish among financial institutions. The suffix typically identifies the type of institution and are usually common among a lot of FIs, for example, \u201dBank\u201d, \u201dN.A.\u201d or \u201dNational Association\u201d. A root dictionary and a suffix dictionary are explicitly generated from the lists of FI names, and Dict-based NER will utilize a dictionary\n1We used several noisy lists, e.g., from the SEC. A more targeted list was obtained from ABSNet, www. absnet.net/, a vendor providing data and analytics for a range of asset backed securities.\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nmatching function to perform extraction based on the dictionaries. For Rank-based ER, we develop a scoring function to select the best matches against a corpus of FI entity names. The concept of distinguishing root and suffix of the FI names are essential in both modules, as will be discussed.\nWe evaluate the effectiveness of Dict-based NER and Rank-based ER by extracting names of financial institutions from a collection of over 5000 resMBS prospecti that were filed with the SEC between 2000 and 2008 2. Dict-based NER recognizes and extracts the mentions of FI names and Rank-based ER links these extracted mentions against a corpus of FI entity names from ABSNet. We used the general purpose ORG NER as a control for comparison. The evaluation was manually validated over a sampled subset of prospecti.\nAfter appropriate tuning, the general purpose ORG NER yielded good precision and recall. We observed that most errors for ORG NER appeared to be incomplete extractions. Figure 1 shows some fragments from financial prospecti. The reasons for the errors made by ORG NER and the challenge of tuning ORG NER for the specialized task of FI name extraction are discussed in a later section. The specialized Dict-based NER improved on the performance of ORG NER. However, it was limited in its ability to generalize the approach beyond the entries provided in the dictionaries. This limitation was particularly noted when Dict-based NER encountered a prospectus from an FI, where the training prospecti did not include examples from that FI, i.e., a previously unseen FI. In this case, the root and suffix dictionaries may not have entries that could help in the matching task. The details are presented in the paper.\nWe expect that our approach to be widely applicable across many types of financial documents. Moreover, our practical approach demonstrates the benefits of exploring and exploiting extra sources, such as lists of names, for domain specific tasks in information extraction. The idea of splitting a name of an entity into distinguishable part and common part can also be utilized across other application domains. The proposed approach is intuitive and unsupervised, which makes it extremely easy for users to get familiar with. It falls within the scope of rule-based approach, which needs no labeled training data. Comparing with rule-based general purpose approaches, the required manual efforts for customization are relatively little.\nTwo key conclusions are that exploiting lists of FI names, and splitting functional dictionaries (explicitly or implicitly), a la the Dict-based NER and Rank-based ER method, can improve on a general purpose solution. However, these specialized solutions have limitations with respect to generalizing their capability. This is particularly the case when handling a prospectus from an unseen FI. A comprehensive solution may require both specialized and general purpose solutions. There is also a need for additional extensions, e.g., a regular expression based customization.\nThis paper is organized as follows: Section 2 provides a motivating example and provides an overview of the proposed specialized solution for the extraction of financial institutions, which includes Dict-based NER and Rank-based ER. The details of Dictbased NER and Rank-based ER are described in section 3 and section 4), respectively. Section 5 presents the results of an extensive evaluation and manual validation; Dictbased ER is compared against the general purpose ORG NER."}, {"heading": "2. PROPOSED SOLUTION", "text": "In this section, we describe our approach for NER and ER, to extract and resolve FI names from unstructured resMBS prospecti. We use some examples from Figure 1 to illustrate the challenges. As mentioned earlier, our innovation is to exploit lists of FI names as an external resource, for both NER and ER tasks. Based on the observation\n2We downloaded the documents from the SEC website http://www.sec.gov/\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nthat the names of financial institutions can typically be split into a root fragment and a suffix, we exploit a root dictionary and a suffix dictionary for Dict-based NER and Rank-based ER.\n2.1. Motivating Example\nFigure 1 shows some examples of text from prospecti. The task of extraction of the mentions of FI names is challenging for the following reasons:\n\u2014 Financial institutions may have long complex names and most general purpose NER approaches may fail to handle such names. For example the issuer (issuing entity) of a resMBS contract is often a trust that is formed for this purpose. Its name may include a numeric suffix that is not typically expected in a name. This is illustrated in Figure 1 (b) and (c). \u2014 The complex layout of the resMBS prospectus, which is a legal document, makes it difficult to identify mentions of the financial institutions. There are several templates defining the structure of the resMBS prospectus and they often do not provide obvious tags that can be used for the NER task. \u2014 The financial institution name often appears in an individual line that may be free of additional text so that it lacks context, natural language features and structure tags. Further, due to the abnormal format of some prospecti, names can break across several lines. Names are also sometimes capitalized. 3 Those specific formatting issues are difficult for conventional NER.\n3Financial contracts use capital letters in paragraphs to emphasize the words.\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\n\u2014 Similarly, entity resolution (ER) is difficult since mentions for the same institution may vary widely. A financial institution may be mentioned using different names and/or abbreviations, e.g., \u201dWells Fargo\u201d, \u201dWells Fargo Bank\u201d, \u201dWells Fargo Bank, N.A.\u201d and \u201dWells Fargo Bank National Association\u201d. This is illustrated in Figure 1 (a) and (d). In this case, all of these names may represent FIs that are affiliated with a single parent or focal FI."}, {"heading": "2.2. System Overview", "text": "We summarize the pipeline of Figure 2. The NER task will extract mentions of FIs from input resMBS prospecti. ER will resolve those mentions against a targeted list of FIs. Dict-based NER is based on dictionary matching. While matching, we first use the root dictionary to extract the distinct root fragment of the name of the financial institution. We then append the suffix to the root to generate the complete name of the financial institution. The two dictionaries are generated from external sources of name lists which may be noisy and incomplete. We carefully design the parsing and dictionary matching task to be tolerant of the noisy name lists, and to improve recall, i.e., the coverage of names of FIs. We note that the robustness and scalability of dictionary matching is due to the benefits that come from using the System T platform [Chiticariu et al. 2010a].\nBy using separate root and suffix dictionaries, and the ability to combine knowledge from both, we extend the capability of our approach. We can use a combination of known root and suffix values to infer new names of FIs, to handle abbreviations, etc. For example, by combining root and suffix values from \u201dWells Fargo Bank\u201d and \u201dCountrywide MBS\u201d, we could also infer and extract additional names of FIs including \u201dWells Fargo MBS\u201d and \u201dCountrywide Bank\u201d. We discuss the details of Dict-based NER in Section 3.\nThe goal of the ER task is to resolve mentions and map (one or more) mentions to a single financial institution (FI). For example, \u201dWells Fargo Bank\u201d, \u201dWells Fargo Bank, N.A.\u201d, and \u201dWells Fargo Bank National Association\u201d should all (potentially) be mapped to \u201dWells Fargo\u201d. Rank-based ER exploits a corpus of a targeted and normalized list of names of FIs. We consider each FI name in this list to be a document, and we use a bag- of-words model on the corpus of FI names for this task.\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nWe develop a scoring function that is inspired by term frequency and inverse document frequency (TF-IDF). Rank-based ER also uses several heuristics based on the observed properties of FI names. For each FI mention, the scoring function will be used to create a ranking and to find the best match from the corpus. We use a threshold on the score to retain valid matches. Rank-based ER uses an inverted index and is efficient and easy to parallelize. Unlike NER, the root and suffix fragments of the name are not separated and are incorporated into the scoring function. We discuss the details of Rank-based ER in Section 4."}, {"heading": "3. DICT-BASED NER", "text": "In this section, we present the details of the Dict-based NER module. This includes the tasks of dictionary generation and dictionary based matching. Dict-based NER makes the following assumptions:\n\u2014 The FI names are composed of a distinguishable part, i.e. root fragment and a modifier, i.e., suffix. The root fragment tends to be distinct. The suffix does not show much variation across multiple mentions of the FI. \u2014 An (almost) complete list(s) of formal names for financial institutions (FIs) is available so that we can effectively construct dictionaries from the list(s). \u2014 A relatively similar version of the formal name of an FI will appear at least once in the document, so that Dict-based NER can use the dictionary to extract at least one mention of the FI that will match the formal name of the FI."}, {"heading": "3.1. Dictionary Generation", "text": "Next, we present details on dictionary generation from a lists of names. We use lists from the following three sources:\n\u2014 A list of organization names from the SEC; it included 174851 unique names. This list was noisy since it contained the names of many organizations that are not FIs. Of greater concern is that it was incomplete. \u2014 A list that was utilized by ORG NER on SystemT [Chiticariu et al. 2010b]; it contained 6874 names. \u2014 A small customized list of FI names that was manually constructed using fifteen prospecti 4. This list of FI names included approximately 50 names and was very valuable to improve precision. Unfortunately this list was also incomplete.\nUsing the fifteen prospecti as a guide, we developed the following heuristics to generate entries in the root and suffix dictionaries:\n\u2014 Remove short text that occurs after \u2019\\\u2019, \u2019/\u2019, \u2019#\u2019, e.g., B HANAUER & CO /BD\u2019. This rule helps overcome several instances of noisy names, in particular when using the SEC list. \u2014 Skip names that have less than 5 characters, such as \u2019O\u2019. \u2014 When a name includes a comma, in the following format: \u2019A, B, C, ..., D, E\u2019, we add\nthe tokens \u2019A\u2019, \u2019A, B\u2019, \u2019A, B, C\u2019 and, \u2019A, B, C. . . , D\u2019 to the root dictionary. Further, we may add the tokens \u2019B\u2019, \u2019C\u2019, . . . , \u2019D\u2019 to the root dictionary if the token is long. \u2014 In this case, we also add \u2019,E\u2019, \u2019,D\u2019, . . . , \u2019,C\u2019, and \u2019,B\u2019 to the suffix dictionary. For example, from \u2019SOUTHEAST INVESTMENTS, N.C., INC.\u2019, we add \u2019SOUTH-\n4We use 15 example files from the following 15 distinct FIs: BANC OF AMERICA, BEAR STERNS, COUNTRYWIDE, CREDIT SUISSE FIRST BOSTON, FIRST HORIZON, GOLDMAN SACHS, INDYMAC, MASTR, MERRILL LYNCH, MORGAN STANLEY, RALI, RESIDENTIAL FUNDING, SASC, WAMU, WELLS FARGO\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nEAST INVESTMENTS\u2019 and \u2019SOUTHEAST INVESTMENTS, N.C.\u2019 to the root dictionary. We further add \u2019, N.C.\u2019 and \u2019,INC.\u2019 to the suffix dictionary. \u2014 When a name does not include a comma, add the last token of the name to the suffix dictionary, if the name does not contain the token \u2019OF\u2019. Add the last two tokens to the suffix dictionary if the last token is short or contains a lot of digits. \u2014 Add the whole name, the name without the suffix, the name without the first token, etc. to the root dictionary; this will help to improve recall. For example, from \u2019J.P. MORGAN ALTERNATIVE LOAN TRUST 2006-A1\u2019, we will add \u2019J.P. MORGAN ALTERNATIVE LOAN TRUST 2006-A1\u2019, \u2019J.P. MORGAN ALTERNATIVE LOAN\u2019, and \u2019MORGAN ALTERNATIVE LOAN\u2019 to the root dictionary, and \u2019TRUST 2006-A1\u2019 to the suffix dictionary. \u2014 For a name that contains special tokens such as \u2019BANK\u2019, \u2019FUND\u2019, \u2019TRUST\u2019, etc. we split the name into two parts. We add the first fragment to the root dictionary and the last fragment to the suffix directory. However, if one of the fragments contains \u2019OF\u2019, add the fragment that includes the part with the \u2019OF\u2019 to the root dictionary. For example, from \u2019SAVINGS BANK OF THE FINGER LAKES FSB\u2019, we will add \u2019OF THE FINGER LAKES FSB\u2019 to the root dictionary. \u2014 For a long name with several tokens, compute tri-grams and add them to the root dictionary. The tri-gram should not contain stop words such as \u2019THE\u2019, \u2019OF\u2019, etc. \u2014 We utilize filters to remove tokens from the root and suffix dictionary. For example, we use an address filter to remove \u2019STREET\u2019, \u2019CENTER\u2019, etc. and a location filter to remove city names, etc. \u2014 In addition to the tokens, our suffix dictionary contains regular expressions that will mix and match tokens and numeric values. For example, a financial institution that is set up as a special purpose vehicle will have a name that includes the following: \u2019TRUST 2006-1\u2019, \u2019SERIES 2005-HE3\u2019, etc.\nThe above heuristics generated 354514 entries in the root dictionary and 26412 entries in the suffix dictionary."}, {"heading": "3.2. Dictionary Matching", "text": "After generating the root and suffix dictionary, we use the dictionary matching function of System T [Chiticariu et al. 2010a] to match the root and suffix of the extracted FI mentions. The dictionary matching function is based on tokens. It is robust and can handle line break characters and other unexpected format issues. We consider the root fragment in a mention as the unique and important fragment and focus on a good match with entries in the root dictionary. The suffix fragment is iteratively matched (and the suffix keyword is appended), until we exhaust any possible additions from the suffix directory.\nFor further tuning, we include an additional customized root and suffix dictionary and a dictionary of invalid elements. These three dictionaries are customization points that can be used beyond this specialized task to extract FI names. The customized root and suffix dictionary are used in a similar manner as the root and suffix dictionary that was automatically generated from the list of FI names; this customization can further improve recall. The dictionary of invalid elements is used to delete unexpected or potentially incorrect mentions and can improve precision.\nDespite the robust and effective dictionary matching function of System T, we encountered the following difficult cases which require further effort.\n\u2014 Entity names that were split across several lines of text or across multiple columns. This case is partially solved since the dictionary matching function is based on tokens, and is insensitive to line breaks. An example of an unsolved case is \u2019Wells /n abc def xxx /n Fargo Bank\u2019.\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\n\u2014 Entity names that were split into multiple fragments and where there were unrelated sentences between the fragments of the entity name. This case remains unsolved."}, {"heading": "3.3. Comparison of Dict-based NER and ORG NER", "text": "ORG NER is a sophisticated general purpose rule-based NER tool that is also built on the System T platform. It achieved state-of-the-art performance on several standard NER tasks [Chiticariu et al. 2010b]. ORG NER has multiple customization points which are exposed as user-defined dictionaries. These dictionaries allow ORG NER to be tuned for a variety of specialized domains. ORG NER encountered several challenges when extracting mentions of FI names. A majority of the errors involve the incomplete extraction of FI mentions; the reasons are as follows:\n\u2014 First, the complex suffix templates for participant FI names in this dataset cannot be easily captured by a suffix dictionary. An example is a template that may contain a date indicating when the prospectus was filed, or that may contain a serial number for identification of the prospectus within a series. An example is the following FI mention: \u201dAAMES MORTGAGE INVESTMENT TRUST 20XX-Y\u201d, where X and Y can be any digit value. For this scenario, a regular expression based customization point for suffix identification may be appropriate. \u2014 Next, even if an exhaustive dictionary of all suffix variations were available, similar to the dictionary created by dict-based NER, a general purpose NER may only achieve an incomplete match for the suffix. ORG NER uses a complex set of rules to recognize and process a suffix. The number of tokens, capitalization, and punctuation elements contained in a suffix for FI mention would normally indicate the occurrence of multiple named entity mentions (within the mention span) or it may indicate the superset of a mention. Handling such cases can confuse ORG NER rules and may prevent ORG NER from completely utilizing the suffix dictionary entries. This typically leads to an incomplete exraction of the complete and complex suffix. \u2014 Finally, ORG NER relies on a complex combination of sentence boundary clues including whitespace, newlines, punctuation, and capitalization to identify sentences. It also makes the assumption that a named entity mention does not span multiple sentences. Such clues for sentence identification and the heuristic for extracting a single mention perform reasonably well for most unstructured text. Unfortunately, both the sentence identification and the heuristic fail when processing the header and summary sections of the resMBS prospecti for FI mentions. This scenario, together with the two previous scenarios, typically results in an incomplete extraction by ORG NER. In particular, it will lead to an incomplete extraction of the suffix fragment of the FI name.\nWe summarize the comparison of ORG NER and Dict-based NER in Table I."}, {"heading": "4. RANK-BASED ER", "text": "The goal of the ER task is to resolve mentions and map (one or more) mentions to a single financial institution (FI) name. For example, \u201dWells Fargo Bank\u201d, \u201dWells Fargo Bank, N.A.\u201d, and \u201dWells Fargo Bank National Association\u201d should all (potentially) be mapped to \u201dWells Fargo\u201d. Our specialized solution, Rank-based ER, exploits a corpus of names of FIs. We assume that there exists a pre-defined corpus that has been normalized, is targeted to this specialized task, and can cover a majority of mentions in the resMBS prospecti. We use a corpus that was obtained from ABSNet5.\n5http://www.absnet.net/ABSNet/\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nWe consider each FI name in this list to be a document, and we use a bag- of-words model on the corpus of FI names for this task. We develop a scoring function that is inspired by term frequency and inverse document frequency (TF-IDF). Rank-based ER also uses several heuristics based on the observed properties of FI names. For each FI mention, the scoring function will be used to create a ranking and to find the best match from the corpus. We use a threshold on the score to retain valid matches. Rankbased ER uses an inverted index and is efficient and easy to parallelize. Unlike NER, the root and suffix fragments of the name are not separated and are incorporated into the scoring function."}, {"heading": "4.1. Index Construction", "text": "The bag-of-words model uses an inverted index over the corpus of FI names. To improve the efficiency of index search, we perform the following pre-processing steps over both the query, i.e., the FI mention in the document, and the FI names in the corpus.\n\u2014 We maintain a list of stop words; this includes words such as \u2019the\u2019 and more specialized words such as \u2019LLC\u2019. We remove stop words and punctuation characters from the mentions. \u2014 We maintain a mapping from abbreviations in the mentions to words or fragments in the corpus. For example, we map from \u2019WaMu\u2019 to \u2019Washington Mutual\u2019."}, {"heading": "4.2. Scoring Function", "text": "A query corresponds to a mention of an FI in the document and is represented by q = q0q1 . . . qn, where each qi is a token. We create a candidate list from the corpus of all FI names that include at least one qi and rank the list. We use the following heuristics to develop the scoring function for ranking:\n\u2014 Recall that an FI name comprises a root that is unique and a suffix. The order of tokens in the FI name is important with the first few tokens being the most important. \u2014 If a candidate FI name from the corpus is a substring of the query, then there is a high probability of a successful match from the query to the candidate.\nLet p = {p0p1 . . . pm} represent a candidate name from the corpus, where pj is a word token. We define a mapping function from q to p as follows:\nmap(qi,p) = { j, qi \u2208 q, pj \u2208 p, qi = pj ,\u2200pk = qi, j \u2264 k, \u22121, qi \u2208 q,\u2200pk \u2208 p, qi 6= pk,\n(1)\nWe identify the index j or the j-th token of the candidate p that forms the first match for token qi from the query.\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nWe define an indicator function to signal if the query token qi exists in the candidate p as follows:\nsgn(qi,p) = { 0, map(qi,p) = \u22121, 1, map(qi,p) \u2265 0,\n(2)\nWe define a weight for each query token, w(qi), as the inverse document frequency (IDF) value. This corresponds to the heuristic that the root fragment is typically unique and is very important to a successful match. We utilize a weight decay function 0.5i to reflect the importance of the (order of the ) first few tokens. We also maintain a set of tokens for which we manually adjust the weight, e.g, we reduce the weight for the token \u2019Structured\u2019 since it occurs in a moderate frequency, but is relatively noninformative.\nWe define a scoring function that consists of the following three factors: (1) The first factor sq(q,p) corresponds to the weighted summation of all the matching tokens in the query q. (2) The second factor sc(q,p) corresponds to the count of matching tokens in the query. (3) the third factor sb(q,p) is a bonus when the candidate from the corpus is a substring of the query.\nsq(q,p) = imax\u2211 i=0 0.5i \u2217 sgn(qi,p) \u2217 w(qi) (3)\nwhere imax = maxi{i : \u2200k < i,map(qi,p) > map(qk,p)}.\nsc(q,p) =\n\u2211jmax j=0 0.5\nj \u2217 sgn(pj ,q)\u2211m j=0 0.5 j (4)\nwhere jmax = maxj{j : \u2200k < j,map(pj ,p) > map(pk,p)}.\nsb(q,p) =\n{\u2211imin+m i=imin\n0.5i\u2211n i=0 0.5\ni , \u2203imin,\u2200k \u2208 {0, 1, . . .m}, qimin+k = pk 0 otherwise . (5)\nThe final scoring function combines the three factors as follows:\nscore(q,p) = sq(q,p) \u2217 sc(q,p) + sb(q,p) (6) We use a threshold on value of the scoring function to decide whether the mapped result is valid. We determined a threshold through experiments and tuning and found threshold of 0.085 worked well for the resMBS dataset."}, {"heading": "5. EXPERIMENTS", "text": "We evaluate the effectiveness of Dict-based NER and Rank-based ER to extract names of financial institutions from a collection of over 5000 resMBS prospecti that were filed with the SEC between 2000 and 2008. Each document is uniquely labelled by the filing financial institution and a unique identifier, the Central Index Key (CIK). We note that there is no labeled training data available a priori, nor are there multiple pre-populated dictionaries that could be customized. Hence, all the dictionaries had to be constructed from scratch and we performed an exhaustive manual evaluation, albeit with a limited number of documents.\nWe use Dict-based NER to recognize and extract the mentions of financial institutions and we use Rank-based ER to link those extracted mentions to a corpus that\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nwas obtained from ABSNet. We used the general purpose ORG NER as a control for comparison with Dict-based NER. We discuss the performance of Dict-based NER and ORG NER in Section 5.1, and that of Rank-based ER in Section 5.2, respectively."}, {"heading": "5.1. Dict-based NER and ORG NER", "text": "We use a small number of randomly sampled documents for the evaluation. We use 15 documents for dictionary construction as discussed in Section 3.1. We further use an additional 13 documents 6 to tune the customized dictionary described in Section 3.2. We use the same 28 documents to tune ORG NER.\nFor dictionary and index construction in Section 3.1, we use multiple external sources, as discussed. The SEC file contains 174851 names; however, only a small number of these are FI names. We also use a set of 6874 names from ORG NER. Finally, a customized collection of about 50 FI names were extracted from the 15 tuning documents. Overall, we generate 354514 entries in the root dictionary and 26412 entries in the suffix dictionary.\nThe FI names are typically located in the header and summary sections of the resMBS prospectus. Further, a financial institution that plays the role of an issuer files the prospectus and may have a significant impact on the selection of other FIs. In order to perform an unbiased evaluation we consider the following options:\n\u2014 We perform an evaluation of FI name mentions from the header and summary sections, and also across the entire document. We consider a collection of twenty three unseen test prospecti. Of these 23 prospecti, we evaluate the extraction of mentions of FI names from the header and the summary section for 18 documents and from the entire document for 5 documents. \u2014 For the header and summary evaluation, the 18 unseen prospecti are from 12 institutions 7. Among these 12 institutions, we did not consider training prospecti from four institutions 8 and this corresponded to five documents. \u2014 We evaluate mentions from the entire document for 5 documents. These five prospecti are filed (sponsored) by five institutions 9. Among these five institutions and five documents, we did not utilize prospecti from two institutions 10 as training prospecti; this corresponded to two documents from unseen FIs.\nWe consider the following measures:\n\u2014 ALL: This is the count of all the FI mentions that are extracted from the header and summary sections. \u2014 WRO: This refers to FI names that are extracted and then found to be completely incorrect, e.g., the string May Be Limited By Book-Entry. \u2014 PAR: A partial extraction refers to FI mentions that have an overlap of tokens with the correctly matching FI name. For example, ABN AMRO and Servicer ABN AMRO Mortgage have an overlap with the correct match ABN AMRO Mortgage Group, Inc.. The partial extraction results have a high probability to be correctly mapped to the correct FI name in the corpus in the next step of Rank-based ER.\n6Those documents are from 9 institutions, AAMES, ABN AMRO, ABSC, ACE, AMERICAN HOME, BANC OF AMERICA, EAR STERNS, COUNTRYWIDE, and INDYMAC. 7The twelve FI names are ABN AMRO, ACCREDITED, AMERICAN GENERAL, AMERICAN HOME, BANC OF AMERICA, BEAR STERNS, COUNTRYWIDE, INDYMAC, LEHMAN, WACHOVIA, WAMU and WELLS FARGO. 8The four FI names are ACCREDITED, AMERICAN GENERAL, LEHMAN and WACHOVIA. 9The five FI names are ABN AMRO, BANK OF AMERICA, BEAR STERNS, LEHMAN and WACHOVIA. 10The two FI names are LEHMAN and WACHOVIA.\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\n\u2014 MIS: A missing extraction refers to an FI mention in the document that is completely overlooked, e.g., Second Street Funding.\nWe consider all four measures for the task of FI name extraction from the summary and header, but we only consider ALL and WRO for the human validation task when we consider the extraction of FI names from the entire text of the prospecti. This is because each prospectus can typically include hundreds of pages of text; reviewing all partial and missing extractions would take significant effort. Despite these practical limitations, we believe that our evaluation and human validation results are fairly robust and representative, as will be discussed.\nWe compute precision (PRE), recall (REC), partial precision (PAR PRE) and partial recall (PAR REC). The results labeled PAR are considered to be incorrect when computing precision (PRE) and recall (REC). They are considered to be correct when computing partial precision (PAR PRE) and partial recall (PAR REc). The values for precision and recall are calculated as follows:\nPRE = ALL\u2212WRO\u2212 PAR\nALL (7)\nPAR PRE = ALL\u2212WRO\nALL (8)\nREC = ALL\u2212WRO\u2212 PAR\nALL\u2212WRO\u2212 PAR + MIS (9)\nPAR REC = ALL\u2212WRO\nALL\u2212WRO + MIS (10)\nWe also calculated the F1 score as follows:\nF1 = 2 \u2217 PRE \u2217 REC PRE + REC\n(11)\nPAR F1 = 2 \u2217 PAR PRE \u2217 PAR REC PAR PRE + PAR REC\n(12)\nWe present the results of a human validation of the extraction of FI names in Table II, Table III and Table IV. Table II shows the results for Dict-based NER and ORG NER for 18 testing documents. We present results for the document sections, header and summary, both separately and together. Table III shows the results for 5 documents from unseen FIs, i.e., we did not use training prospecti from these unseen FIs. We observe that Dict-based NER demonstrates promising results.\nThe recall of Dict-based NER is comparable to that of ORG NER, while the precision of Dict-based NER is consistently better. To explain, ORG NER often misses the issuing entity from the prospectus. The format for the FI name is typically XXX XXX Trust, Series XXXX-XXX. It often is a newly formed institution and appears in a single line in the header section. ORG NER will also miss institutions, or extract a lot of partial results, when several FI mentions appear in close vicinity of each other, separated by a comma. An example is the list of mentions of FI names of resMBS servicers as seen in Figure 1(a).\nThe precision of Dict-based NER is good for many reasons, e.g., dictionary construction discards some common tokens that may cause errors during mention extraction. Further, the dictionary matching step of Dict-based NER is robust to line breaks. In contrast, ORG NER uses line breaks as a heuristic for extraction. This decision by Dict-based NER significantly reduces the number of partial extractions. In addition, ORG NER often uses heuristics, e.g., the use of capitalization, which is not suitable for FI name extraction from resMBS prospecti.\nWe observe that Dict-based NER is robust and shows similar performance for both the header and summary sections. In contrast, ORG NER has greater variance across the two sections and performs worse for the header; see Table II. To explain, the header section is more challenging since it is less well structured and stylized and contains less contextual text that can be used for the extraction of FI names. In addition, contextual text may be misinterpreted and may lead to incorrect FI name extraction. We observe that this often happens for ORG NER with test documents from unseen FIs, i.e., we did not include training prospecti from those FIs.\nThe recall of Dict-based NER drops when extracting FI mentions from entire prospecti. One challenge is the use of abbreviations of FI name mentions, e.g., \"WMC\" can represent \"Wachovia Mortgage Corp.\". Such abbreviations are localized to specific contracts filed by the related FI and these mentions cannot be processed without some contextual text from the contracts. We observe that in many cases, the abbreviation of the FI name will first be introduced together with the full FI name in Summary and Header sections. Subsequently, the abbreviations will be used without the full FI name. ORG NER has rules to handle many general cases and it has the capability to cope with this challenge. This is reflected in the correct number of extractions reported in Table IV.\nDict-based NER generally performs well for both precision and recall. As expected, it exhibits the best performance for FI name extraction when processing test prospecti where the sponsoring (filing) institution has previously provided training prospecti.\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nThis can be used to benefit Dict-based NER since the most popular (Top 15) sponsoring FIs file more than 80% of the prospecti. Thus, there are opportunities to further tune the performance of Dict-based NER.\nTo further understand the performance of Dict-based NER and ORG NER, we consider their performance on partial precision (PAR PRE) and observe that they show similar performance. Both approaches had difficulty extracting complete FI name mentions. The reasons for these partial FI name extractions were described in detail in Section 3.3.\nFinally, the more robust performance of the specialized domain specific Dict-based NER, in comparison to the general purpose ORG NER, can be explained by the ability to more easily tune Dict-based NER. Dict-based NER is less complex and has fewer rules. It is able to benefit from customizing the various dictionaries using the training prospecti. This is reflected in the improved performance for the previously seen FIs versus the unseen FIs.\nTo conclude, the experiment with documents from unseen institutions demonstrates the generalizability of both approaches. Further, when facing a scenario where less sample documents are available, ORG NER showed higher recall but lower precision. It also had almost identical F1 scores when considering partial precision and recall."}, {"heading": "5.2. Rank-based ER", "text": "The evaluation of Rank-based ER is performed as follows:\n\u2014 Extract mentions of FI names from the header and summary sections of all 5131 prospecti. Filter mentions to include those mentions that are adjacent to a keyword that may indicate that the financial institution plays a specific role in the financial contract following [Burdick et al. 2016]. Example keywords from Figure 1 are \u201dServicers\u201d, \u201dIssuer\u201d and \u201dSponsor\u201d. This step yields 53354 mentions. \u2014 Perform pre-processing and de-duplication to produce 5535 unique mentions of FI names. \u2014 Find the best match for each mention against the ABSNet corpus of FI names; there are 393 normalized names in this corpus. Produce a tuple (unique FI name mention, ABSNet name, score) for each unique FI name mention. \u2014 We rank the 5535 tuples by the mapping score, and draw the precision-recall curve as follows: For some value of the threshold and for all tuples whose score is above this threshold, we determine the count of true positives by manually checking the count of correct mappings between the extracted FI names and the ABSNet corpus. An extracted FI name that cannot be successfully mapped to the ABSNet corpus by Rank-based ER is considered to be incorrect. The reasons for the incorrectness include both an incomplete ABSNet corpus as well as errors during extraction of the FI name. We use the count of true positives, i.e., the count of correct matching tuples above the threshold, and the count of all tuples above the threshold, to calculate precision and recall for a threshold. The precision-recall curve is generated by varying this threshold. \u2014 We could empirically fix a feasible threshold by looking at the precision-recall curve. A threshold that achieves high precision and moderate recall is selected. After selecting the threshold, we evaluate on the 53354 non-deduplicated mentions. In this case, we determine the count of true positives whose matching score with the entry from the ABSNet corpus exceeds the threshold. If the value is lower than the threshold, then this is considered a mismatch.\nWe compare the results of Rank-based ER against the following baseline methods:\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\n\u2014 A baseline that uses the IDF weight. Term frequency is usually 1 since the FI names are short, and hence is not considered. \u2014 A baseline using the scoring function sq from Equation 3; it reflects the root and suffix heuristic of Section 4.2. \u2014 A baseline using the scoring function sq \u2217 sc from Equation 4 that also considers the order of the tokens discussed in Section 4.2.\nFigure 3 shows the precision-recall curve for Rank-based ER; it outperforms all three baselines. The two baselines that consider (simpler) scoring functions also outperform the IDF baseline.\nConsider the performance of Rank-based ER; it is labeled as sq \u2217 sc + sb in Figure 3. Note that this figure reports on the results across the unique FI mentions. The precision-recall curve shows that the precision maintains a consistently high value, across a large range of threshold values. We fixed the threshold at 0.085 for this set of experiments; this resulted in a precision of 99.95% and a recall of 69.66%. We observe that almost all the issues in precision and recall are caused by either incomplete of ABSNet names or the extracted mentions are incorrect, which demonstrate the effectiveness of our scoring function.\nNext, we consider the recall across all extracted FI mentions with the fixed threshold. We report only on pseudo recall since we consider all the tuples with a score above the threshold to be true positives instead of manually labelling 50000+ tuples. Since the precision of Rank-based ER is almost 100%, the pseudo recall is representative. We\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nobtain a much higher value for pseudo recall of 88.27%. To explain, when considering all mentions, the popular financial institutions that participate in many resMBS contracts typically can find a match in the ABSNet corpus. These popular FI names may appear multiple times across different contracts. In addition, if there is an incorrect extraction, the error in the FI name will only be recorded once and the error will not be duplicated.\nTo further improve the recall, we dig into details of the existing issues in rank-based ER. We observe that several issuer institutions can not find a mapping in ABSNet, such as \u201dHSI Asset Securitization Corporation Trust XXXX-XXXX\u201d, \u201dMASTR ADJUSTABLE RATE MORTGAGES TRUST XXX-X\u201d, \u201dRALI SERIES XXXX-XXX TRUST \u201d, \u201dRASC SERIES XXXX-XXX TRUST\u201d, \u201dCiticorp Mortgage Securities Trust Series XXXX-X\u201d, and \u201dRAMP SERIES XXXX-XXX TRUST\u201d. For those issuers, we can not map the name abbreviation to an informative institution name. We should extend our normalized list in rank-based ER to include those abbreviations. However, it is a nontrivial task since those names are not common names and are also difficult to recognize for experts.\nMoreover, some examples of extraction could not possibly recognized are, \u201dALTERNATIVE LOAN TRUST XXXX-XXXX\u201d, and \u201dASSET BACKED NOTES SERIES XXXXX\u201d. Those names cannot be recognized without context information and cannot be solved by the rank-based ER framework. We need to go back to the documents to find relevant descriptions for those names to correctly recognize them."}, {"heading": "6. CONCLUSION", "text": "We proposed a specialized rule-based solution for the extraction of FI names. Our innovation is to exploit lists of FI names, and to customize a two-part solution, Dict-based NER and Rank-based ER. Dict-based NER and Rank-based ER are built upon the algebraic information extraction system, System T. Dict-based NER can be viewed as a specialization of the general purpose ORG NER that is also available on the System T platform.\nWe combine multiple lists of FI names from several sources for Dict-based NER. In contrast, we utilize a smaller targeted list of names for Rank-based ER. We observe that FI names can typically be split into a root fragment and a suffix. We generate a root dictionary and a suffix dictionary from the lists of FI names, and Dict-based NER will utilize a dictionary matching function to perform extraction. The root and suffix dictionaries can synergistically help both modules in extracting the root and the suffix. For Rank-based ER, we develop a scoring function to select the best matches against a corpus of FI entity names.\nWe evaluate the effectiveness of Dict-based NER and Rank-based ER to extract FI names from a collection of over 5000 resMBS prospecti and we compare Dict-based NER with ORG NER. The recall of Dict-based NER is comparable to that of ORG NER, while the precision of Dict-based NER is consistently better. To explain, there are several cases where ORG NER will miss FI names or will extract partial results. Dict-based NER is helped by the root and suffix dictionaries and other heuristics to avoid these cases. We observe that Dict-based NER is robust and shows similar performance for both the header and summary sections. In contrast, ORG NER has greater variance across the two sections and performs worse for the header. To explain, the header section is more challenging since it is less well structured and stylized and contains less contextual text that can be used for the extraction of FI names. In addition, contextual text may be misinterpreted and may lead to incorrect FI name extraction.\nThere are several lessons learned from this experience that can be used to improve upon our current solutions and to develop solutions for other specialized NER and ER tasks.\nACM Journal of Data and Information quality, Vol. 0, No. 0, Article 0, Publication date: February 2016.\nOur first lesson is that a general purpose NER such as ORG NER will benefit from more extensive dictionaries to capture domain and task specific knowledge. These could be created using approaches similar to those used for Dict-based NER. An example of an external list of FI names would be the names of all financial institutions that have been issued a CIK. Another example is the use of the root and suffix dictionaries.\nOur next lesson goes beyond the dictionary based customization discussed in the paper. A general purpose NER such as ORG NER may benefit from additional types of customization points. Recall that the names of issuers of the resMBS contracts, issuer FI names, were FI names that had been further modified. A potential solution would be to include a regular expression based customization that would similarly extend FI names and recognize the names of issuer FIs.\nOur final lesson is very positive since we believe that both the general purpose ORG NER and the special purpose Dict-based NER and Rank-based ER can be applied with additional minimal customization to a range of other collections. This includes the prospecti for other classes of asset backed securities, e.g., ABS that are created by pooling auto loans, student loans, etc. Prospectus documents for asset backed securities share similar formatting templates as the resMBS prospecti, e.g., relevant information is captured in a header or summary section. Additionally, FI names for financial entities participanting in ABS prospecti follow similar naming conventions, e.g., using a root and suffix. Applying our techniques for dict-based NER and rank-based ER to the wider class of ABS prospectuses remains as future work."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Soham De, Howard Ho, Rajasekar Krishnamurthy and Michael Shao for their feedback."}], "references": [{"title": "resMBS: Constructing a Financial Supply Chain Graph from Financial Prospecti", "author": ["Doug Burdick", "Soham De", "Louiqa Raschid", "Mingchao Shao", "Zheng Xu", "Elena Zotkina."], "venue": "SIGMOD DSMM workshop. ACM.", "citeRegEx": "Burdick et al\\.,? 2016", "shortCiteRegEx": "Burdick et al\\.", "year": 2016}, {"title": "Data Science Challenges in Real Estate Asset and Capital Markets", "author": ["Douglas Burdick", "Michael Franklin", "Paulo Issler", "Rajasekar Krishnamurthy", "Lucian Popa", "Louiqa Raschid", "Richard Stanton", "Nancy Wallace."], "venue": "Proceedings of the International Workshop on Data Science for Macro-Modeling. ACM, 1\u20135.", "citeRegEx": "Burdick et al\\.,? 2014", "shortCiteRegEx": "Burdick et al\\.", "year": 2014}, {"title": "Extracting, Linking and Integrating Data from Public Sources: A Financial Case Study", "author": ["Douglas Burdick", "Mauricio A Hern\u00e1ndez", "Howard Ho", "Georgia Koutrika", "Rajasekar Krishnamurthy", "Lucian Popa", "Ioana Stanoi", "Shivakumar Vaithyanathan", "Sanjiv R Das."], "venue": "IEEE Data Eng. Bull. 34, 3 (2011), 60\u201367.", "citeRegEx": "Burdick et al\\.,? 2011", "shortCiteRegEx": "Burdick et al\\.", "year": 2011}, {"title": "SystemT: an algebraic approach to declarative information extraction", "author": ["Laura Chiticariu", "Rajasekar Krishnamurthy", "Yunyao Li", "Sriram Raghavan", "Frederick R Reiss", "Shivakumar Vaithyanathan."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 128\u2013137.", "citeRegEx": "Chiticariu et al\\.,? 2010a", "shortCiteRegEx": "Chiticariu et al\\.", "year": 2010}, {"title": "Domain adaptation of rule-based annotators for named-entity recognition tasks", "author": ["Laura Chiticariu", "Rajasekar Krishnamurthy", "Yunyao Li", "Frederick Reiss", "Shivakumar Vaithyanathan."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 1002\u20131012.", "citeRegEx": "Chiticariu et al\\.,? 2010b", "shortCiteRegEx": "Chiticariu et al\\.", "year": 2010}, {"title": "Enterprise information extraction: recent developments and open challenges", "author": ["Laura Chiticariu", "Yunyao Li", "Sriram Raghavan", "Frederick R Reiss."], "venue": "Proceedings of the 2010 ACM SIGMOD International Conference on Management of data. ACM, 1257\u20131258.", "citeRegEx": "Chiticariu et al\\.,? 2010", "shortCiteRegEx": "Chiticariu et al\\.", "year": 2010}, {"title": "Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems", "author": ["Laura Chiticariu", "Yunyao Li", "Frederick R Reiss."], "venue": "EMNLP. 827\u2013832.", "citeRegEx": "Chiticariu et al\\.,? 2013", "shortCiteRegEx": "Chiticariu et al\\.", "year": 2013}, {"title": "Named entity recognition through classifier combination", "author": ["Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang."], "venue": "Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003-Volume 4. Association for Computational Linguistics, 168\u2013171.", "citeRegEx": "Florian et al\\.,? 2003", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Entity resolution: theory, practice & open challenges", "author": ["Lise Getoor", "Ashwin Machanavajjhala."], "venue": "Proceedings of the VLDB Endowment 5, 12 (2012), 2018\u20132019.", "citeRegEx": "Getoor and Machanavajjhala.,? 2012", "shortCiteRegEx": "Getoor and Machanavajjhala.", "year": 2012}, {"title": "Unleashing the Power of Public Data for Financial Risk Measurement, Regulation, and Governance", "author": ["Mauricio A Hern\u00e1ndez", "Howard Ho", "Georgia Koutrika", "Rajasekar Krishnamurthy", "Lucian Popa", "Ioana R Stanoi", "Shivakumar Vaithyanathan", "Sanjiv Das."], "venue": "WWW (2010).", "citeRegEx": "Hern\u00e1ndez et al\\.,? 2010", "shortCiteRegEx": "Hern\u00e1ndez et al\\.", "year": 2010}, {"title": "Extracting personal names from email: Applying named entity recognition to informal text", "author": ["Einat Minkov", "Richard C Wang", "William W Cohen."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 443\u2013450.", "citeRegEx": "Minkov et al\\.,? 2005", "shortCiteRegEx": "Minkov et al\\.", "year": 2005}, {"title": "A survey of named entity recognition and classification", "author": ["David Nadeau", "Satoshi Sekine."], "venue": "Lingvisticae Investigationes 30, 1 (2007), 3\u201326.", "citeRegEx": "Nadeau and Sekine.,? 2007", "shortCiteRegEx": "Nadeau and Sekine.", "year": 2007}, {"title": "Probabilistic Financial Community Models with Latent Dirichlet Allocation for Financial Supply Chains", "author": ["Zheng Xu", "Louiqa Raschid."], "venue": "SIGMOD DSMM workshop. ACM.", "citeRegEx": "Xu and Raschid.,? 2016", "shortCiteRegEx": "Xu and Raschid.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "The absence of such datasets prevents the types of financial big data analytics that will be very useful to both regulators and investors who have an interest in real estate and mortgage capital markets [Burdick et al. 2014; Burdick et al. 2016; Xu and Raschid 2016].", "startOffset": 203, "endOffset": 266}, {"referenceID": 0, "context": "The absence of such datasets prevents the types of financial big data analytics that will be very useful to both regulators and investors who have an interest in real estate and mortgage capital markets [Burdick et al. 2014; Burdick et al. 2016; Xu and Raschid 2016].", "startOffset": 203, "endOffset": 266}, {"referenceID": 5, "context": "Methods for IE have gained significant traction in natural language processing, information retrieval and database and data analytics research [Chiticariu et al. 2010].", "startOffset": 143, "endOffset": 167}, {"referenceID": 6, "context": "Methods for NER can be classified into the following three categories: rule-based; machine learning-based; hybrid [Chiticariu et al. 2013].", "startOffset": 114, "endOffset": 138}, {"referenceID": 4, "context": "However, recent rule-based approaches [Chiticariu et al. 2010b] developed on top of the System T declarative platform [Chiticariu et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 3, "context": "2010b] developed on top of the System T declarative platform [Chiticariu et al. 2010a] achieved state-of-the-art accuracy on the NER task.", "startOffset": 61, "endOffset": 86}, {"referenceID": 7, "context": "In comparison to machine learning approaches [Florian et al. 2003; Minkov et al. 2005], the rule-based approach only requires moderate efforts for manual customization of rules and minimal labeled data.", "startOffset": 45, "endOffset": 86}, {"referenceID": 10, "context": "In comparison to machine learning approaches [Florian et al. 2003; Minkov et al. 2005], the rule-based approach only requires moderate efforts for manual customization of rules and minimal labeled data.", "startOffset": 45, "endOffset": 86}, {"referenceID": 2, "context": "Rule-based NER has been applied to financial documents [Burdick et al. 2011; Hern\u00e1ndez et al. 2010].", "startOffset": 55, "endOffset": 99}, {"referenceID": 9, "context": "Rule-based NER has been applied to financial documents [Burdick et al. 2011; Hern\u00e1ndez et al. 2010].", "startOffset": 55, "endOffset": 99}, {"referenceID": 4, "context": "Rule-based approaches may require a manual redesign of the rules [Chiticariu et al. 2010b].", "startOffset": 65, "endOffset": 90}, {"referenceID": 3, "context": "Dict-based NER and Rank-based ER are built upon a general purpose algebraic information extraction system, System T, and its programming language AQL [Chiticariu et al. 2010a].", "startOffset": 150, "endOffset": 175}, {"referenceID": 3, "context": "We note that the robustness and scalability of dictionary matching is due to the benefits that come from using the System T platform [Chiticariu et al. 2010a].", "startOffset": 133, "endOffset": 158}, {"referenceID": 4, "context": "\u2014 A list that was utilized by ORG NER on SystemT [Chiticariu et al. 2010b]; it contained 6874 names.", "startOffset": 49, "endOffset": 74}, {"referenceID": 3, "context": "After generating the root and suffix dictionary, we use the dictionary matching function of System T [Chiticariu et al. 2010a] to match the root and suffix of the extracted FI mentions.", "startOffset": 101, "endOffset": 126}, {"referenceID": 4, "context": "It achieved state-of-the-art performance on several standard NER tasks [Chiticariu et al. 2010b].", "startOffset": 71, "endOffset": 96}, {"referenceID": 4, "context": "Comparison of Dict-based NER and ORG NER [Chiticariu et al. 2010b], which are rule-based methods.", "startOffset": 41, "endOffset": 66}, {"referenceID": 4, "context": "All measures for Dict-based NER and ORG NER [Chiticariu et al. 2010b] for the extraction of FI names from the header and summary section for the 18 testing documents.", "startOffset": 44, "endOffset": 69}, {"referenceID": 4, "context": "All measures for Dict-based NER and ORG NER [Chiticariu et al. 2010b] for the extraction of FI names from the header and summary section for 5 out of 18 testing documents; these documents were sponsored (filed) by FIs where we did not use prospecti filed by these FIs as training prospecti, i.", "startOffset": 44, "endOffset": 69}, {"referenceID": 4, "context": "Partical precision and recall measures for Dict-based NER and ORG NER [Chiticariu et al. 2010b] for the extraction of FI names from the entire document.", "startOffset": 70, "endOffset": 95}, {"referenceID": 0, "context": "Filter mentions to include those mentions that are adjacent to a keyword that may indicate that the financial institution plays a specific role in the financial contract following [Burdick et al. 2016].", "startOffset": 180, "endOffset": 201}], "year": 2016, "abstractText": "There is a wealth of information about financial systems that is embedded in document collections. In this paper, we focus on a specialized text extraction task for this domain. The objective is to extract mentions of names of financial institutions, or FI names, from financial prospectus documents, and to identify the corresponding real world entities, e.g., by matching against a corpus of such entities. The tasks are Named Entity Recognition (NER) and Entity Resolution (ER); both are well studied in the literature. Our contribution is to develop a rule-based approach that will exploit lists of FI names for both tasks; our solution is labeled Dict-based NER and Rank-based ER. Since the FI names are typically represented by a root, and a suffix that modifies the root, we use these lists of FI names to create specialized root and suffix dictionaries. To evaluate the effectiveness of our specialized solution for extracting FI names, we compare Dict-based NER with a general purpose rule-based NER solution, ORG NER. Our evaluation highlights the benefits and limitations of specialized versus general purpose approaches, and presents additional suggestions for tuning and customization for FI name extraction. To our knowledge, our proposed solutions, Dict-based NER and Rank-based ER, and the root and suffix dictionaries, are the first attempt to exploit specialized knowledge, i.e., lists of FI names, for rule-based NER and ER. CCS Concepts: rInformation systems\u2192 Extraction, transformation and loading; rComputing methodologies\u2192 Information extraction;", "creator": "TeX"}}}