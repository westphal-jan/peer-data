{"id": "1602.04906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Segmentation Rectification for Video Cutout via One-Class Structured Learning", "abstract": "Recent works to collaborative video example lampshades mainly working on utilized strong gaze - background (FB) affixes came species-specific filtering. However, full consultant off optimally sealed accurate out long FB classification because spatially, and held reporting often accumulate rapidly, fears significant accuracy same part optically canvas. In every started, want rest end further set hold leadership comes problem, and we call which american difficult \\ emph {segmentation latinisation }. Our key observation is all present dangerous splintering internet false less these wrongdoing opinion uncharacteristic were handled equally day the conventional work. We, spliced, oppose whether visualized must these six common \u2014 fault. To this effect, there propose taken comedy pores Markov Random Field (MRF) using first how created needed. We also reject the with - established functional learning creation given speak since optimal model down measurement. Additionally, could blueprint new movie one - unlike structured SVM (OSSVM) full nonetheless altitude half 's structured practical process. Our calculation entirely scope to RGB - D photographs came usually. Comprehensive experiments on both RGB rest RGB - D contents demonstrate that our simple work providing tool comparison unhip form hub-and-spoke measurement tools passed last the regional - of - the - art features cutout designed, and only due also suggest the future longevity of our method in invisible scrim controls.", "histories": [["v1", "Tue, 16 Feb 2016 04:31:20 GMT  (3520kb,D)", "http://arxiv.org/abs/1602.04906v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["junyan wang", "sai-kit yeung", "jue wang", "kun zhou"], "accepted": false, "id": "1602.04906"}, "pdf": {"name": "1602.04906.pdf", "metadata": {"source": "CRF", "title": "Segmentation Rectification for Video Cutout via One-Class Structured Learning", "authors": ["Junyan Wang", "Sai-Kit Yeung", "Jue Wang", "Kun Zhou"], "emails": ["ejywang@ucla.edu", "saikit@sutd.edu.sg", "juewang@ieee.org", "kunzhou@acm.org."], "sections": [{"heading": null, "text": "Fig. 0: Given a keyframe segmentation provided by the user (left), our approach generates accurate object cutout results in subsequent frames fully automatically (middle), which can be used for creating a novel compositing (right).\nIndex Terms\u2014Video cutout, segmentation rectification, one-class structured SVM, object segmentation\nF"}, {"heading": "1 INTRODUCTION", "text": "V IDEO cutout, as one of the most successful applicationsof computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6]. While practically useful systems have been developed, some fundamental problems still remain unattended. In this paper, we address video cutout from a newly identified fundamental aspect.\n\u2022 Junyan Wang is with Doheny Eye Institute at University of California, Los Angeles, CA 90033, USA E-mail: ejywang@ucla.edu \u2022 Sai-Kit Yeung is with the Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Singapore, 487372. E-mail: e-mail:saikit@sutd.edu.sg \u2022 Jue Wang is with Adobe Research, Seattle, WA 98103, USA. E-mail: e-mail:juewang@ieee.org \u2022 Kun Zhun is with the State Key Laboratory of CAD&CG, Zhejiang University, Hangzhou, China, 310058. E-mail: kunzhou@acm.org.\nManuscript received ; revised"}, {"heading": "1.1 Related works", "text": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion. performing keyframe image segmentation and refinement; (2) Foreground-background classification. performing classification on other frames given the keyframe segmentation; (3) Segmentation refinement. converting the classifier outputs to final cutout results on non-keyframes. Steps 2 and 3 are usually iteratively applied to subsequent frames until the user creates a new keyframe due to occurrence of visible errors.\nThe keyframe segmentation step can be done using interactive single image segmentation techniques. In foregroundbackground classification, with the help of motion estimation, foreground classifiers at different scales are constructed/trained from the segmented object in the current frame, and then applied to other frames. The classifiers generate a soft foreground probability map, which is incorporated into a segmentation model in the segmentation refinement step to generate the final object mask on the nonkeyframes.\nar X\niv :1\n60 2.\n04 90\n6v 1\n[ cs\n.C V\n] 1\n6 Fe\nb 20\n16\n2\nFig. 2: An overview of our approach.\nThe Foreground-background classification and segmentation refinement are often considered as a single module called segmentation propagation, and most previous works focus only on designing new foreground-background classifiers, while little attention has been paid to the optimal estimation of the segmentation given the classifier output. In Video Snapcut [5], Bai et al. applied the conventional Markov random field (MRF) to the classification output to refine the result. More recently, Zhong et. al [6] applied matting directly after classification. The matting step behaves like random walk segmentation [10] and the latter is closely related to the MRF model [11]. We observe that when the errors from the classifiers tend to bias toward either the false positive or false negative error, the common MRF or matting, which treat the two types of errors equally, would fail to remove the errors satisfactorily. As an example, Figure 1 shows that common method could over shrink the spurious foreground regions produced by the classifier.\nMore recently, Fan et al. [9] proposed a novel method for propagating segmentation to non-successive frames in order to handle large object displacement. The propagated segmentation mask was refined using geodesic active contour (GAC) model [12] and the level set method [13], rather than MRF with graph cuts. Similar to the MRF based frameworks, GAC with level set method has also not been used to handle the possibly asymmetrically distributed FP and FN. Besides, other video cutout frameworks have been proposed [14], [15], [16]. Our contribution is parallel to these directions towards accurate and user-friendly video cutout."}, {"heading": "1.2 Contributions", "text": "In this paper, we address the possibly asymmetrically distributed FP and FN errors in the output of foregroundbackground classifier, which we call segmentation rectifica-\ntion. The significance of this subproblem in the context of video cutout is first identified to the best of our knowledge.\nOur contribution is twofold. First, we propose a novel bilayer MRF in which the data term can treat the false positive and false negative errors from any given classifier differently using separate weights. Second, we propose a novel one-class structured support vector machine (OSSVM) model to learn the weights, as a computationally more favorable alternative to the conventional two-class structured SVM (2CSSVM) frameworks [17], [18]. We further establish the conditional equivalence between the OSSVM and the conventional (2CSSVM). This theoretical justification of OSSVM is also new in the context of structured learning. Figure 2 illustrates the flowchart of our method.\nOur proposed method for segmentation rectification adapts to different classifiers and achieves significant improvement on error reduction over previous methods [5], [6] in segmentation propagation in the experiments. Note that the confidence map adopted in [6] can be used to eliminate unreliable/ambiguous results from classifier output. However, it does not tell where the classifier is overconfident. Our method can remove the error in the classification regardless of the confidence of the classification."}, {"heading": "1.3 Organization", "text": "The rest of the paper is organized as follows. Section 2 derives our bilayer MRF model, Section 3 describes the training process and Section 4 discuses the practical concerns for video cutout. Section 5 details our experiment. Finally, we conclude our work and discuss about the difference between video cutout and other video segmentation tasks in Section in Section 6."}, {"heading": "2 MODELING SEGMENTATION RECTIFICATION", "text": ""}, {"heading": "2.1 Segmentation refinement in video cutout", "text": "In conventional video cutout systems [5], [6], the classifier output is refined by using the MRF-based segmentation model. The MRF model can be written as:\nmin f \u2211 p\u2208P Up(fp) + \u2211 {p,q}\u2208N Vpq(fp, fq), (1)\nwhere p refers to a pixel, P is the set of all pixels, fp is the pixel label and N is a neighbourhood system. Up and Vpq are the conventional unary and pairwise terms. The unary term can be used to represent the hard constraint given by the user, such as the seeds of foreground and\n3\nbackground regions, or it can be a region model or a shape prior. The pairwise potential is often used to model the object boundaries, and it has also been used to represent advanced priors in segmentation [19].\nThe unary term for incorporating foregroundbackground model can be written in the following form:\nUp(fp) = \u2211 p\u2208P (fp \u2212 hp)2\n= \u2211 p\u2208P fp + hp \u2212 2fphp\n= \u2211 p\u2208P hp(1\u2212 fp) + fp(1\u2212 hp)\n= \u2211 p\u2208P fphp + fphp.\n(2)\nwhere hp is the classifier output (or probability map that gives the classifier output), hp and fp are both binary, fp = 1\u2212 fp and hp = 1\u2212 hp. The unary term is a pixelwise shape distance between the label f and the classifier output h. The above equation also provides a decomposition of the unary term, which implies that the unary term above can be naturally related to the two types of errors in segmentation, i.e., the false positives (FP) (background that wrongly considered as foreground) fphp, and the false negatives (FN) (missing foreground) fphp, as illustrated in Figure 3. FPR is defined as\nFPR = # of wrongly classified foreground pixels\ntotal # of pixels (3)\nand FNR is defined as\nFNR = # of wrongly classified background pixels\ntotal # of pixels . (4)"}, {"heading": "2.2 A case study on the classification error", "text": "Here, we conduct a quantitative study on the classification errors produced by the state-of-the-art foregroundbackground classifier for video cutout [6]. In this study, we perform segmentation propagation using Zhong et al.\u2019s classifier [6] on all consecutive frame pairs in their training dataset, and we compute the false positive ratio (FPR) and false negative ratio (FNR) for each target frame. The quantitative results are shown in Figure 4: the FPR is about 11.22 times greater than the FNR, implying that the two terms in Eq. (2) should not be considered as equally important in the model. This case study disproves the universal fidelity\nFig. 4: FPR vs. FNR from Zhong et al.\u2019s FB classifier [6]\nof the unary term based on symmetric distance measure in the conventional MRF model. There may be multiple complicated causes of this phenomenon and we omit the indepth analysis on this specific classifier, since our method is generically applicable to removing errors from any classifier."}, {"heading": "2.3 A generic shape distance function", "text": "Our segmentation rectification method is inspired by the shape-prior based MRF segmentation model [20], [21] in which shape distance is used in the segmentation model for handling occlusion and background clutter. In this work, we view the data term as shape distance and we show why and how we could reformulate this shape distance for segmentation rectification.\nFrom Eq. (2), we observe that the common shape distance used in the data term of the MRF uniquely breaks down into FP and FN with equal weights. To handle the possibly asymmetrically distributed FP and FN errors, we propose the following generic shape distance function to model the relationship between the classifier output and the true segmentation:\nSw(f, h) = \u2211\n{p,q}\u2208Nfh\nwoutsidepq fphq + w inside pq fphq, (5)\nwhere Nfh denotes the neighborhood system across f and h. In this formulation, woutsidepq and w inside pq are two unknown data-dependent balancing weights. The neighborhood system across f and h yields a novel graph as visualized in Figure 6. The graph is in a bilayer structure: one layer is defined on the image to represent the unknown segmentation label f t at frame t, and the other layer is used to represent the propagated label ht. We have the following observation for different configurations of weights when p = q:\nweights effect woutsidepq = w inside pq Penalize FP and FN equally woutsidepq > w inside pq Penalize more FP than FN woutsidepq < w inside pq Penalize more FN than FP\nThis means the FP and FN can be treated differently with proper weights. Besides, by considering all the qs in Nfh in\n4\nthe formulation, the noise in the classification result can also be reduced, and the resultant Sw can be viewed as the local average distance."}, {"heading": "2.4 The MRF model for segmentation rectification", "text": "We can now plug our generic shape decision function in Eq. (5) into the conventional MRF for segmentation propagation to arrive at a novel model for segmentation rectification:\nmin ft\nSw(f t, ht) + \u2211 {p,p\u2032}\u2208N \u03b4e(f t p, f t p\u2032), (6)\nwhere ht, f t are the hypothesis segmentation label and the unknown label at frame t,N is the neighborhood system for f t.\nTo comply with the standard graph-cuts representation of MRF energy, we may rewrite Sw(f t, ht) as follows\nSw(f t, ht) = \u2211 {p,q}\u2208Nfh \u03b4s(f t p, h t q) (7)\nwhere Nfh denotes the neighborhood system for the graph defined on ht and f t, \u03b4s(f tp, h t q) is defined according to the generic shape distance measure, shown in Eq. (5), as:\n\u03b4s(f t p, h t q) =\n{ winsidepq , if h t q = 0, f t p = 1\nwoutsidepq , if h t q = 1, f t p = 0\n(8)\nwhere winsidepq and w outside pq are to be determined. \u03b4e(f t p, f t p\u2032) in Eq. (6) corresponds to the boundary edge model proposed\nFig. 7: Idea of SSVM. Red dots represents the \u201cbad\u201d class, and the blue dots represent the \u201cgood\u201d class. The black line separating the two classes is the underlying classifier \u03a8 = T , where T is a thresholding constant. The value of T is implicit in our method.\nin [22], which has been shown to be effective for interactive segmentation:\n\u03b4e(f t p, f t p\u2032) = w edge \u00b7 wepq \u2223\u2223f tp \u2212 f tp\u2032 \u2223\u2223, (9)\nwhere wedge is the model weight to be determined. wepp\u2032 in [22] was defined as:\nwepp\u2032 = { exp(\u22125Ie(p, p\u2032)), Ie(p, p\u2032) 6= 0 20, Otherwise (10)\nwhere Ie(p, p\u2032) is 1 if either p or p\u2032 is on edge. Otherwise, value is 0. We also normalize the weight by wepp\u2032 = wepp\u2032/max{p,p\u2032}\u2208N (w e pp\u2032).\nThe input to the graph cuts solver includes the graph structure, the estimated segmentation of the current image (It) and the edge map (Ie) of the current image. The output is the rectified segmentation (f t).\nThe energy function in our bilayer MRF model in Eq. (6) can be rewritten in the following compact form:\nE(f t|w, ht, wepp\u2032) = w \u00b7\u03a8(ht, wepp\u2032 , f t), (11)\nwhere \u00b7 is the vector dot product, wepp\u2032 is defined in Eq. (10), the weight vector is defined as w =[ wedge, winsidepq , w outside pq |{p, q} \u2208 Nfh ] , and\n\u03a8 =\n \u2211pp\u2032 wepp\u2032 \u2223\u2223f tp \u2212 f tp\u2032 \u2223\u2223, {p, p\u2032} \u2208 N\u2211\npq(1\u2212 htq)f tp, {p, q} \u2208 Nfh\u2211 pq h t q(1\u2212 f tp), {p, q} \u2208 Nfh  , (12) Note that there can be multiple terms for {p, q} \u2208 Nfh as shown in Figure 6. Throughout this paper we mainly consider the following parameterization of w: w = [w1, w2, ..., w11]T= [wedge, winsidep,q0 , w inside p,q1 , w inside p,q2 , w inside p,q3 , w inside p,q4 , w outside p,q0 , woutsidep,q1 , w outside p,q2 , w outside p,q3 , w outside p,q4 ] T ."}, {"heading": "3 LEARNING THE OPTIMAL MODEL FOR SEGMENTATION RECTIFICATION WITH ONE-CLASS STRUCTURED SVM", "text": "Based on the previous observations, it becomes crucial to determine the optimal parameters in the proposed MRF model. One popular method for this task is known as structured learning [17], [18], [23], [24]. The basic idea is to consider the MRF parameter learning problem as a classification problem where good segmentations form one class and bad segmentations are the other class. This idea is illustrated in Figure 7. However, this framework requires searching\n5 for negative label samples fk, or the worst case [17], [18], [23], [24], which can be time-consuming. To remove the need for negative samples, we propose to apply the oneclass SVM, instead of the conventional two-class formulation [17], to the structured learning problem. The one-class SVM only requires samples from one class, e.g. positive class, for training [25], [26], [27]. Thus, the resultant oneclass structured SVM (OSSVM) will also only require the\npositive samples which are the images paired with ground truth segmentations."}, {"heading": "3.1 The two-class structured SVM", "text": "Before we present our model, we briefly describe the conventional two-class structured SSVM (2CSSVM). We begin with the generic compact form of MRF. The MRF energy for one image can be written as an inner product form w.r.t. the weights w:\nE(f |w,x) = w \u00b7\u03a8(x, f) (13)\nwhere w is defined in Eq. (11), and x = {ht, wepp\u2032}, f = f t to be consistent with Eq. (11). Note that this general notations adopted in this subsection allows us to easily extend our approach to other MRF models.\nWe expect the MRF energy to be as small as possible for the ground truth f\u2217 and we expect it to be as large as possible for other non-ideal f . This principle can be used for learning the weight vector w from data [17], [18], and it can be expressed as:\nmin w,b,~\u03be\n1 2 \u2016w\u20162 + C N N\u2211 k=1 \u03bek\ns.t.: 2w \u00b7\u03a8(xk, fk)\u2212 b \u2265 +1\u2212 \u03bek 2w \u00b7\u03a8(xk, f\u2217k )\u2212 b \u2264 \u22121 + \u03bek k = 1, 2, 3, ..., N,\n(14)\nwhere C is a constant, k is the sample id, N is the number of samples, the constants b is a bias in conventional decision function and it\u2019s unnecessary in segmentation, and \u03bek is a lack variable that tolerates errors in the training data. In the above model, the margin that separates the positive and negative samples are maximized by minimizing \u2016w\u20162.\nSince the positive and negative samples are paired, the respective constraints for each pair of samples can be combined to yield the following constraint:\nw \u00b7\u03a8(xk, fk)\u2212w \u00b7\u03a8(xk, f\u2217k ) \u2265 \u2206(fk, f\u2217k )\u2212 \u03bek (15)\nDirectly combining the constraints in Eq. (14) gives \u2206 = 1. For segmentation problem, \u2206 is a specialized cost and is often set as \u2206 = mean(|fk \u2212 f\u2217k |2) [28]. With the additional requirements of positiveness and boundedness of w, the above yields the conventional SSVM of the following form [17], [18], [23], [24].\nmin w,~\u03be\n1 2 \u2016w\u20162 + C N N\u2211 k=1 \u03bek\ns.t.: \u2200k, w \u00b7 (\u03a8(xk, fk)\u2212\u03a8(xk, f\u2217k )) \u2265 \u2206k \u2212 \u03bek,\u2211 i wi = 1,w \u2265 0. (16)\nwhere \u2206k = \u2206(fk, f\u2217k ). Note that we further imposed a normalization constraint for the weights.\nFig. 8: Idea of OSSVM. The threshold line, \u03a8 = T \u2032, is estimated based on the \u201cgood\u201d class only. The value of T \u2032 is implicit in our method."}, {"heading": "3.2 One-class structured SVM", "text": "By simply dropping the constraint for non-ideal segmentation fk in Eq. (14) and removing the irrelevant parameter b, we obtain\nmin w,~\u03b5\n1 2 \u2016w\u20162 + C N N\u2211 k=1 \u03b5k\ns.t.: \u2200k, w \u00b7\u03a8(xk, f\u2217k ) \u2264 \u22121 + \u03b5k,\u2211 i wi = 1,w \u2265 0. , (17)\nwhere we used \u03b5k instead of \u03bek to differentiate from the original SSVM formulations. We call this model one-class structured support vector machine (OSSVM), since it does not reply on fk. This optimization problem is well-known as one class support vector machine, and it has been thoroughly studied previously in the classification literature [25]. Here we further establish the rationale of the OSSVM in the context of structured learning.\nWe observe that the OSSVM, requiring only groundtruth masks, is conditionally equivalent to the conventional two-class SSVM model in which both of non-ideal segmentations and ground-truth segmentations are used. The formal statement is as follows. Theorem 3.1. The OSSVM model in Eq. (17) is identical to the\ntwo class SSVM model in Eq. (16) if both of the following conditions are true: (I) \u2206(fk, f\u2217k ) = 1; (II) \u2200k,\u03a8(xk, fk) = bk, where bk is an arbitrary constant\nvector with equal elements. Its total sum is denoted by Bk.\nThere are obviously infinitely many such non-ideal segmentations for many common potentials and the condition allows bk to vary for different k. We defer its proof to the Appendix.\nIn a nutshell, this OSSVM model tries to maximize the \u201cmargin\u201d from the energy corresponding to all groundtruth samples to the smallest energy produced by the same sample set, such that the margin between the energy of the positive samples and the potential energy of unknown negative samples is also maximized to some extent. We visualize this idea in Figure 8. Edge prior. A price of removing the negative samples is the accuracy of the model. Without negative sample, the training data may not be sufficient to yield satisfactory MRF model. We thus propose to impose a prior during the OSSVM learning. This can be crucial to even two-class SSVM when the ground-truth data itself contains errors. Our\n6 Algorithm 1: Two-class SSVM learning [24]\nInput : Training images {xk|k = 1, 2, ..., N} paired with predicted masks {hk|k = 1, 2, ..., N} from any classifier and ground truth masks {f\u2217k |k = 1, 2, ..., N}\nOutput: Learned weights w\u2217 for the MRF potentials 1 w0 \u2190 ~1; 2 W \u2190 \u2205; 3 forall the Images do 4 \u03a8\u2217k \u2190 \u03a8(f\u2217k |xk, hk);\\\\By Eq. (12) 5 while Not Converged do 6 forall the Images do 7 fk \u2190 min\nf \u2032k\n\u2206(f \u2032k, f \u2217 k ) + w j\u03a8(f \u2032k|xk, hk);\n8 \u03a8k \u2190 \u03a8(fk|xk, hk); 9 \u2206\u03a8k \u2190 \u03a8jk \u2212\u03a8\u2217k;\n10 \u2206k \u2190 \u2206(fk, f\u2217k ); \\\\Cutting plane generation\n11 W \u2190W \u22c3 {\u2206\u03a8k,\u2206k|k = 1, ..., N};\n12 wj+1 \u2190  minw,~\u03be 1 2\u2016w\u2016 2 + C|W| \u2211|W| n=1 \u03bek s.t.: \u2200{\u2206\u03a8n,\u2206n} \u2208 W, w \u00b7\u2206 \u03a8n \u2265 \u2206n \u2212 \u03ben,\u2211\ni wi = 1,w \u2265 0\n;\n13 \\\\| \u00b7 | denotes number of elements 14 j \u2190 j + 1; 15 w\u2217 \u2190 wj ;\nprior is that the edge term is important to segmentation. Thus, the weight wedge in Eq. (9), which is actually w1, has to be large. This is motivated by the fact that the edge cue is almost always valid, i.e. the final segmentation boundary should always adhere to image edges. To this effect, we propose to maximize the weight on edge features as much as possible. The corresponding one-class SVM model with edge prior for structured learning can be written as follows:\nmin w,~\u03b5\n1 2 \u2016w\u20162 + C N ( N\u2211 k=1 \u03b5k ) \u2212 wedge\ns.t.: \u2200k, w \u00b7\u03a8(xk, f\u2217k ) \u2264 \u22121 + \u03b5k\u2211 i wi = 1,w \u2265 0,\n(18)"}, {"heading": "3.3 Learning algorithms", "text": "Both of the SSVM and the OSSVM can be used for learning the weights in our model. We adopt the cutting plane algorithm for two-class SSVM [24]. We include the pseudocode for this method in Algorithm 1 for self-containedness. The pseudocode for our OSSVM learning method is presented in Algorithm 2. After obtaining the weights w\u2217, we can use them in the rectification model shown in Eq. (6) or Eq. (11)."}, {"heading": "4 PRACTICAL CONCERNS IN IMPLEMENTATION", "text": ""}, {"heading": "4.1 The shrinking bias of graph cut", "text": "It is well known that graph cut for MRF model with 2ndorder pairwise potential suffers from the shrinking bias [19], [29]. The recently proposed local forground-background\nAlgorithm 2: OSSVM learning Input : Same as in Algorithm 1 Output: Same as in Algorithm 1 1 forall the Images do 2 \u03a8\u2217k \u2190 \u03a8(f\u2217k |xk, hk);\\\\By Eq. (12)\n3 w\u2217 \u2190  min w,~\u03b5\n1 2\u2016w\u2016 2 + CN (\u2211N k=1 \u03b5k ) \u2212 wedge\ns.t.: \u2200k, w \u00b7\u03a8\u2217k \u2264 \u22121 + \u03b5k,\u2211 i wi = 1,w \u2265 0\n;\n\\\\According to Eq. (18)\nclassifiers [5], [6] are shown to be able to correct local errors near the object boundary. The shrinking bias falls into this category as it introduces small errors near the boundary. Hence, we propose to feed the results of the rectified segmentation to re-train the foreground-background classifiers on the current frame, and use the updated classifiers to perform classification in the same frame again, to avoid the shrinking bias."}, {"heading": "4.2 Computational complexity", "text": "There exist quite a few efficient algorithms for solving graph cuts, i.e. the max-flow/min-cut problem. The computational time for the Boykov-Kolmogorov (BK) algorithm on a 2 MP image on CPU is about 160 ms, and the GPU implementation of graph cuts can be 2 times faster than on the CPU [30]. The foreground-background classifier we use is the one reported in [6], and its average computational time is about 1.5s for one frame on a PC with quad-core 3.3 GHz CPUs. Optical flows and edge maps can all be precomputed. worth showing due to the page limit. We include them in the supplementary material."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "In the experiments, we compare our method with stateof-the-art methods for full sequence cutout with the initial keyframe segmentation. We avoid end-to-end system comparison since the interactive segmentation phase, i.e., Step 1, in the video cutout system is out of the focus of this work. We are unable to present all the results worth showing due to the page limit. We include them in the supplementary material."}, {"heading": "5.1 Experimental Setup", "text": "Datasets. We mainly evaluate our method on the dataset proposed in Zhong et al. [6]. The main advantage of this dataset is that the ground truth segmentation for each frame has been provided. The training data from Zhong et al.\u2019s dataset contains 15 video sequences in total, 9 for training and 6 for testing. The Training set we used in this work contains 2012 frames from their 9 training sequences, and the Test set contains 741 frames from their testing sequences. Learning the weights. We use the training set to learn the parameters {winsidepq , woutsidepq , wedge} of the bilayer MRF with both the conventional SSVM and our one-class SSVM as presented in section 3.\nWe considered applying our framework to rectifying the output of two classifiers. One is the Gaussian mixture model\n7\nTABLE 1: Learned weights for the bilayer MRF illustrated in Fig. 6. A is trained with GMM classifier. B is trained with Zhong et al.\u2019s classifier [6].\nw w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11\nA 2CSSVM 0.52 0.01 0.01 0.01 0.01 0.01 0.048 0.089 0.098 0.098 0.093\nOSSVM w/o prior 0.00068 0.0059 0.0058 0.0059 0.0059 0.0059 0.24 0.14 0.2 0.18 0.219 OSSVM with prior 0.091 0.006 0.0059 0.006 0.006 0.006 0.25 0.12 0.17 0.14 0.19\nB 2CSSVM 0.170 0.0233 0.0424 0.0385 0.0380 0.045 0.115 0.134 0.130 0.129 0.136\nOSSVM w/o prior 0.0004 0.017 0.016 0.016 0.016 0.016 0.5 0.083 0.11 0.12 0.099 OSSVM with prior 0.091 0.016 0.015 0.016 0.016 0.015 0.44 0.079 0.11 0.12 0.093\nwhere w = [w1, w2, ..., w11]T = [wedge, winsidep,q0 , w inside p,q1 , winsidep,q2 , winsidep,q3 , winsidep,q4 , woutsidep,q0 , woutsidep,q1 , woutsidep,q2 , woutsidep,q3 , woutsidep,q4 ]T\nFig. 9: Effectiveness of our segmentation rectification for GMM classifier.\n(GMM), which is a typical global foreground-background (FB) classifier, and the other is the state-of-the-art local FB classifier proposed in [6]. We apply the FB classifiers to all the consecutive frame pairs to generate the classifier output to be rectified. The classifier outputs, the ground truth segmentations, together with the images are then fed into the structured learning framework. Table 1 shows the trained parameters for both GMM and Zhong et al.\u2019s classifier using different learning models. We empirically chose maximum iteration number to be 10 for the training process. The training time for OSSVM is about 0.062 seconds in MATLAB on Intel Core i7-4700MQ Processor, while the training time for the conventional two class SSVM is about 7500 seconds (10 iteractions).\nIt is interesting to note that the weights show strong asymmetry structure, and the learned weights would penalize more false positive than false negative. Besides, the weights for GMM are more uniform for both inside and outside weights. This means the results by GMM is very noisy and strong smoothness is required for rectifying the GMM classifier. Evaluation metrics. We mainly use boundary deviation to measure the segmentation performance. It is defined as the average distance from the estimated boundary to the ground truth boundary. Methods for evaluation. We mainly evaluate two variations of our method in the experiments: FB classifier + graph cuts with weights learned by two class structured SVM (2CSSVM), and FB classifier + graph cuts with weights learned by one class structured SVM (OSSVM). We shall call them: Our method (2CSSVM), Our method (OSSVM). We applied our method to GMM based FB classifier and the state-of-the-art FB classifier proposed in [6]. We main compare with the FB classifier + Matting, as adopted in [6], and FB classifier + uniformly weighted graph cuts (UWGC), which is adopted in Rotobrush [5].\n8 Original image FB Classifier [6] + Matting FB Classifier + UWGC Zoom in Our method (2CSSVM) Zoom in Our method (OSSVM) Zoom in\nFig. 11: Results of fully automatic segmentation propagation for the \u201cBear\u201d sequence on frames 5 (top) and 9 (bottom), given the same keyframe segmentation. The background is whitened for visualization.\nOriginal image FB Classifier [6] + Matting FB Classifier + UWGC Zoom in Our method (2CSSVM) Zoom in Our method (OSSVM) Zoom in\nFig. 12: Results of fully automatic segmentation propagation for the \u201cCar\u201d sequence on frames 22 (top) and 24 (bottom), given the same keyframe segmentation. The background is whitened for visualization."}, {"heading": "5.2 Rectifying global classifier", "text": "We first apply our framework to the global Gaussian Mixture Model (GMM) classifier for segmentation rectification. Generally, a global classifier like GMM is not suitable to video cutout [5], we conduct the experiment to validate the generality of our framework.\nIn the segmentation propagation, we train a GMM classifier using the ground truth segmentation on frame t, and apply it on frame t+ 1 as the propagated segmentation, and then apply our rectification approach with different weights, e.g., learned with or without prior as shown in Table 1, for further refinement. The average boundary deviation are summarized in Figure 9. Some typical results are shown in Figure 10. The results suggest that our rectification approach with learned weights can significantly improve the segmentation results generated by the GMM classifier.\nDue to the well-known non-local behavior of GMM based classifier, it\u2019s not surprising to see that 2CSSVM and OSSVM with prior significantly outperform all other cases. The experimental results shown in this subsection also imply that the errors from GMM for image cutout can be more effectively removed by using our method, compared with the conventional methods based on graph cuts and matting [31], [32], and the 2CSSVM outperforms the rest in this case while the OSSVM is the best alternative when computational efficiency in learning is a concern. Note that as shown in the visual results in Figure 10, significant errors can still be observed in the refined output, and this reasserts that global classifier such as GMM alone is not suitable for video cutout.\nIt is also interesting to note that global classifiers, such as GMM, are commonly adopted in image cutout [31], [32]. Hence, these results suggests that our method may be applied to image cutout as well."}, {"heading": "5.3 Rectifying local classifier", "text": "We further demonstrate the efficacy of our approach for rectifying the state-of-the-art local classifier proposed in [6]. We perform the full sequence propagation on the Test set and Figure 15 shows the quantitative results. Again, we see the segmentation errors by matting and graph cuts with uniform weights accumulates more rapidly than ours. After 10 frames of propagation, matting and graph cut generates 4 times and 2 times more error than our method respectively. We also present the visual results for a \u201cCar\u201d sequence in the Test set in Figure 12, and a \u201cBear\u201d sequence from the Training set in Figure 11.\nWe also compare our method with Video SnapCut (Rotobrush) [5] and [6] on additional video sequences. Some of the typical results are shown in Figures 13 and 14. We can observe that the Rotobrush could suffer from the temporal discontinuity while segmentation error could easily accumulate in other methods based on Zhong et al.\u2019s classifier."}, {"heading": "5.4 Experiment on RGB-D videos", "text": "3D movies and videos have now gained increased popularity. An extra depth channel in addition to the RGB channels may be handily available sooner or later. Since our OSSVM framework can be easily extended to handling the extra depth dimension, we also apply our method to RGB-D data for evaluation. The segmentation rectification model for RGB-D data is the same as Eq. (11), except that the potential is defined as\n\u03a8RGBD =  \u2211 pp\u2032 w eRGB pp\u2032 \u2223\u2223f tp \u2212 f tp\u2032 \u2223\u2223, {p, p\u2032} \u2208 N\u2211 pp\u2032 w eD pp\u2032\n\u2223\u2223f tp \u2212 f tp\u2032 \u2223\u2223, {p, p\u2032} \u2208 N\u2211 pq(1\u2212 htq)f tp, {p, q} \u2208 Nfh\u2211 pq h t q(1\u2212 f tp), {p, q} \u2208 Nfh\n , (19)\n9 FB C la ss ifi er [6 ]+ M at ti ng\nFB C\nla ss\nifi er\n+ U\nW G C A do be R ot ob ru\nsh O ur m et ho d (2 C SS V M\n) O ur m et ho d (O SS V M )\nFig. 13: Results of fully automatic segmentation propagation for the \u201cCatwalk\u201d sequence on frames 1 (left), 6 (middle) and 19 (right), given keyframe segmentation. The background is darkened for visualization.\nweDpp\u2032 = { exp(\u22125IeD(p, p\u2032)), IeD(p, p\u2032) 6= 0 20, Otherwise , (20)\nwhere IeD is the edge map from depth channel, weRGBpp\u2032 = wepp\u2032 has been defined in Eq. (10).\nSimilar to the RGB case, the OSSVM model for RGB-D is\nmin w,~\u03b5\n1 2 \u2016w\u20162 + C N ( N\u2211 k=1 \u03b5k ) \u2212 wedgeRGB\ns.t.: \u2200k, w \u00b7\u03a8RGBD(xk, f\u2217k ) \u2264 \u22121 + \u03b5k, wedgeRGB \u2264 w\nedge D ,\u2211\ni\nwi = 1,w \u2265 0,\n(21)\nin which we added one additional constraint to the model to represent our prior that the edge term from depth is more reliable than that from RGB values and this reformulation does not require additional free parameters.\nThe dataset we used is from the INRIA 3D movie dataset [33]. Since a number of sequences in the original dataset contain very dark or motion-blurred objects, we select a subset containing 22 sequences with identifiable object boundaries in RGB domain for this experiment. Note that visually identifiable boundaries are required in the context of video cutout and for ground truth delineation. There are a total of 835 frames in the selected subset. Besides, the original dataset only provides the keyframe segmentations. Therefore, we manually cutout each frame.\n10\nThe visual comparison of results from the related methods are shown in Figure 16. From the visual results, we can observe that the results are comparable and our OSSVM on RGB-D outperforms others in general. The quantitative results are shown in Figure 17 and Table 3, which further validated our observation. The first impression is that the overall errors are only around 3 pixels small for many of the methods up to 10 subsequent frames. Besides, we see that our OSSVM on RGB-D and 2CSSVM on RGB-D are very comparable and OSSVM is still better than the other methods.\nThe comparable performance is due to that the object/background motion in the videos in this dataset are often either very small or abrupt, and the method may perform equally good or bad on most of them. The RGB-D\nvideo cutout system would further benefit from a dedicated RGB-D object classifier which is beyound the scope of this paper."}, {"heading": "5.5 Cross validation for prior weight selection", "text": "There is an optional edge prior in our main formulation of OSSVM. The default prior value, which is the coefficient of \u2212wedge, is 1. From our experiment, we observe that this prior is crucial to the performance. To select the optimal weights, we perform 10-fold cross validation with 7 possible prior weights on a uniform grid {0, 0.5, 1, 1.5, 2, 2.5, 3}, for both of the RGB and RGB-D datasets used in our experiments. The best prior is the one gives overall smallest error in all the 10-fold cross validation.\n11\nTABLE 3: Average boundary deviation (ABD) for 10 frame segmentation propagation\nMethod Zhong et al\u2019s +Matting +Uniform GC +OCSSVM+prior +2CSSVM OCSSVM+prior\nClassifier in RGB in RGBD in RGB in RGBD in RGBD ABD(pixel) 11.01 12.73 3.61 3.35 3.07 2.92\nFig. 16: Segmentation results on two RGB-D sequences (zoom in to see details). In each example, the left most is the initial keyframe segmentation. From the second left to the end are the results by Zhong et al.\u2019s classifier [6], Classifier with Matting, OSSVM with RGB only, uniform weights for GC on RGB-D, 2CSSVM for RGB-D and OSSVM for RGB-D. The top row shows the result on the 5th frame from the keyframe, and the bottom row shows the zoom-in for the boxed regions in the top row.\nThe quantitative results are shown in Figure 18. We can observe that the error in the 10-fold experiments becomes smallest for Zhong et al\u2019s dataset if the prior is 1 and the error becomes smallest for the RGB-D dataset if the prior is 0.5. When the prior on the edge weight is too high, the weights on the data term may be too small to produce semantically meaningful results."}, {"heading": "5.6 Limitations", "text": "Although experiments show that our rectification approach can effectively improve the performance of existing video cutout systems, it may fail in challenging cases. A typical failure case is imperfect edge extraction. State-of-the art edge detection techniques are often reliable, but their results may still contain errors. Such errors may impair the segmentation rectification. See Figure 19 for such an example. The edge map shows that there is a clear strong edge caused by the shadow in-between the legs, causing segmentation rectification to be less effective in this region.\nAnother common problem for segmentation propagation is having abrupt changes on the object itself, especially the abrupt emergence of object parts. Figure 20 shows a typical example. Apart from more user interactions, we believe this problem can be solved by using a more sophisticated propagation model, such as a long-term shape prior."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "We propose a novel generic approach to automatically rectify the propagated segmentation in video cutout systems. The core idea of our work is to incorporate a generic shape distance measure in a bilayer MRF framework learned from data to remove the intrinsic bias of the classifier in the segmentation propagation step. This work is motivated by our observation that different classifiers bias toward FP and FN differently, but they were treated equally in the previous video cutout systems. We found that FP and FN can be\n12\ntreated differently in our bilayer MRF, and the optimal form of the MRF can be learned from the data. Extensive evaluation demonstrates that our approach can significantly improve the state-of-the-art video cutout systems in segmentation accuracy.\nThere are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40]. In computer vision, those problems can be thought of as shape tracking problem, and errors are generally tolerable. Convenient user interaction is also not a concern to them. In contrast, the interactive video cutout problem does not tolerate visible errors in the segmentation and user-friendly interaction is a crucial concern. It has been proven that the state-of-the-art video cutout frameworks are particularly suitable for the video cutout problem. In most of the works in vision, global optimization frameworks for whole sequences are often adopted. It has been noted in [5] that localized optimization allows user to have better control of the video cutout process. Our method is proposed dedicatedly for video cutout. Its extension for generic video segmentation tasks has yet to be explored.\nAPPENDIX Proof of Theorem 3.1 We first substitute the two conditions stated in the theorem into the constraint in Eq. (16) and we obtain\nw \u00b7 (\u03a8(xk, fk)\u2212\u03a8(xk, f\u2217k )) \u2265 \u2206k \u2212 \u03bek \u21d4w \u00b7 (bk \u2212\u03a8(xk, f\u2217k )) \u2265 1\u2212 \u03bek \u21d4w \u00b7\u03a8(xk, f\u2217k ) \u2264 \u22121 + \u03be\u0302k\n(22)\nwhere have applied the normalization constraint \u2211 i wi = 1\nand we set \u03be\u0302k = Bk + \u03bek, Bk = \u2211 j [bk]j . The above gives us the constraint in the OSSVM formulation in Eq. (17). The objective function in Eq. (16) can accordingly be rewritten as\n1 2 \u2016w\u20162 + C N N\u2211 k=1 (\u03be\u0302k \u2212Bk)\n= 1 2 \u2016w\u20162 + C N N\u2211 k=1 \u03be\u0302k \u2212 C N N\u2211 k=1 Bk\n(23)\nwhere the last term is a constant. By omitting the constant we obtain the objective function in OSSVM Eq. (17).\nLastly, due to the one-to-one correspondence between \u03be\u0302k and \u03bek for any k, we know that optimizing over w and {\u03bek} is equivalent to optimizing over w and {\u03be\u0302k}, which completes the proof."}], "references": [{"title": "Keyframe-based tracking for rotoscoping and animation,", "author": ["A. Agarwala", "A. Hertzmann", "D.H. Salesin", "S.M. Seitz"], "venue": "in SIGGRAPH,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Video object cut and paste,", "author": ["Y. Li", "J. Sun", "H.-Y. Shum"], "venue": "in SIGGRAPH,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Interactive video cutout,", "author": ["J. Wang", "P. Bhat", "R.A. Colburn", "M. Agrawala", "M.F. Cohen"], "venue": "in SIGGRAPH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Video snapcut: Robust video object cutout using localized classifiers,", "author": ["X. Bai", "J. Wang", "D. Simons", "G. Sapiro"], "venue": "in SIGGRAPH,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Discontinuity-aware video object cutout,", "author": ["F. Zhong", "X. Qin", "Q. Peng", "X. Meng"], "venue": "in SIGGRAPH Asia,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Dynamic color flow: a motionadaptive color model for object segmentation in video,", "author": ["X. Bai", "J. Wang", "G. Sapiro"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Transductive segmentation of live video with non-stationary background,", "author": ["F. Zhong", "X. Qin", "Q. Peng"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Jumpcut: Non-successive mask transfer and interpolation for video cutout,", "author": ["Q. Fan", "F. Zhong", "D. Lischinski", "D. Cohen-Or", "B. Chen"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Random walks for image segmentation,", "author": ["L. Grady"], "venue": "IEEE TPAMI, vol. 28,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "A seeded image segmentation framework unifying graph cuts and random walker which yields a new algorithm.,", "author": ["A.K. Sinop", "L. Grady"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Geodesic active contour,", "author": ["V. Caselles", "R. Kimmel", "G. Sapiro"], "venue": "International Journal of Computer Vision, vol. 22,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Fronts propagating with curvaturedependent speed: Algorithms based on Hamilton-Jacobi formulations,", "author": ["S. Osher", "J.A. Sethian"], "venue": "Journal of Computational Physics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1988}, {"title": "Video brush: A novel interface for efficient video cutout,", "author": ["R.-F. Tong", "Y. Zhang", "M. Ding"], "venue": "in CGF,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "EXCOL: an extract-and-complete layering approach to cartoon animation reusing,", "author": ["L. Zhang", "H. Huang", "H. Fu"], "venue": "IEEE TVCG,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Efficient video cutout by paint selection,", "author": ["Y. Zhang", "Y.-L. Tang", "K.-L. Cheng"], "venue": "Journal of Computer Science and Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Learning structured prediction models: A large margin approach,", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "in ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Large margin methods for structured and interdependent output variables,", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, vol", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Star shape prior for graph-cut image segmentation,", "author": ["O. Veksler"], "venue": "in ECCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Interactive graph cut based segmentation with shape priors,", "author": ["D. Freedman", "T. Zhang"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Shape prior segmentation of multiple objects with graph cuts,", "author": ["N. Vu", "B. Manjunath"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Active visual segmentation,", "author": ["A.K. Mishra", "Y. Aloimonos", "L.F. Cheong", "A. Kassim"], "venue": "IEEE TPAMI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Learning crfs using graph cuts,", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "in ECCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Cutting-plane training of structural svms,", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Estimating the support of a high-dimensional distribution,", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J.C. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural Computation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "One-class svm for learning in image retrieval,", "author": ["Y. Chen", "X.S. Zhou", "T.S. Huang"], "venue": "in ICIP,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "One-class svms for document classification,", "author": ["L.M. Manevitz", "M. Yousef"], "venue": "JMLR, vol", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Structured learning and prediction in computer vision,", "author": ["S. Nowozin", "C.H. Lampert"], "venue": "Foundations and Trends R  \u00a9 in Computer Graphics and Vision,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Geodesic graph cut for interactive image segmentation,", "author": ["B.L. Price", "B. Morse", "S. Cohen"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Cuda cuts: Fast graph cuts on the gpu,", "author": ["V. Vineet", "P. Narayanan"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "grabcut\u201d: interactive foreground extraction using iterated graph cuts,", "author": ["C. Rother", "V. Kolmogorov", "A. Blake"], "venue": "in ACM SIG- GRAPH,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Interactive graph cuts for optimal boundary & region segmentation of objects in n-d images,", "author": ["Y. Boykov", "M.-P. Jolly"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Pose estimation and segmentation of people in 3d movies,", "author": ["G. Seguin", "K. Alahari", "J. Sivic", "I. Laptev"], "venue": "IEEE TPAMI,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Motion layer extraction in the presence of occlusion using graph cuts,", "author": ["J. Xiao", "M. Shah"], "venue": "IEEE TPAMI, vol. 27,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "Key-segments for video object segmentation,", "author": ["Y.J. Lee", "J. Kim", "K. Grauman"], "venue": "in ICCV, pp. 1995\u20132002,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Foreground segmentation of live videos using locally competing 1svms,", "author": ["M. Gong", "L. Cheng"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Fast object segmentation in unconstrained video,", "author": ["A. Papazoglou", "V. Ferrari"], "venue": "in ICCV, pp", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Deep learning shape priors for object segmentation,", "author": ["F. Chen", "H. Yu", "R. Hu", "X. Zeng"], "venue": "in CVPR, pp", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Supervoxel-consistent foreground propagation in video,", "author": ["S.D. Jain", "K. Grauman"], "venue": "in ECCV,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Classifier based graph construction for video segmentation,", "author": ["A. Khoreva", "F. Galasso", "M. Hein", "B. Schiele"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 190, "endOffset": 193}, {"referenceID": 3, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 195, "endOffset": 198}, {"referenceID": 4, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 200, "endOffset": 203}, {"referenceID": 3, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "ground truth of t+ 1 Result by [6] Our method", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "In Video Snapcut [5], Bai et al.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "al [6] applied matting directly after classification.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "The matting step behaves like random walk segmentation [10] and the latter is closely related to the MRF model [11].", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "The matting step behaves like random walk segmentation [10] and the latter is closely related to the MRF model [11].", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "[9] proposed a novel method for propagating segmentation to non-successive frames in order to handle large object displacement.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The propagated segmentation mask was refined using geodesic active contour (GAC) model [12] and the level set method [13], rather than MRF with graph cuts.", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "The propagated segmentation mask was refined using geodesic active contour (GAC) model [12] and the level set method [13], rather than MRF with graph cuts.", "startOffset": 117, "endOffset": 121}, {"referenceID": 12, "context": "Besides, other video cutout frameworks have been proposed [14], [15], [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "Besides, other video cutout frameworks have been proposed [14], [15], [16].", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Besides, other video cutout frameworks have been proposed [14], [15], [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Second, we propose a novel one-class structured support vector machine (OSSVM) model to learn the weights, as a computationally more favorable alternative to the conventional two-class structured SVM (2CSSVM) frameworks [17], [18].", "startOffset": 220, "endOffset": 224}, {"referenceID": 16, "context": "Second, we propose a novel one-class structured support vector machine (OSSVM) model to learn the weights, as a computationally more favorable alternative to the conventional two-class structured SVM (2CSSVM) frameworks [17], [18].", "startOffset": 226, "endOffset": 230}, {"referenceID": 3, "context": "Our proposed method for segmentation rectification adapts to different classifiers and achieves significant improvement on error reduction over previous methods [5], [6] in segmentation propagation in the experiments.", "startOffset": 161, "endOffset": 164}, {"referenceID": 4, "context": "Our proposed method for segmentation rectification adapts to different classifiers and achieves significant improvement on error reduction over previous methods [5], [6] in segmentation propagation in the experiments.", "startOffset": 166, "endOffset": 169}, {"referenceID": 4, "context": "Note that the confidence map adopted in [6] can be used to eliminate unreliable/ambiguous results from classifier output.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "In conventional video cutout systems [5], [6], the classifier output is refined by using the MRF-based segmentation model.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "In conventional video cutout systems [5], [6], the classifier output is refined by using the MRF-based segmentation model.", "startOffset": 42, "endOffset": 45}, {"referenceID": 17, "context": "The pairwise potential is often used to model the object boundaries, and it has also been used to represent advanced priors in segmentation [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 4, "context": "2 A case study on the classification error Here, we conduct a quantitative study on the classification errors produced by the state-of-the-art foregroundbackground classifier for video cutout [6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 4, "context": "\u2019s classifier [6] on all consecutive frame pairs in their training dataset, and we compute the false positive ratio (FPR) and false negative ratio (FNR) for each target frame.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "\u2019s FB classifier [6]", "startOffset": 17, "endOffset": 20}, {"referenceID": 18, "context": "Our segmentation rectification method is inspired by the shape-prior based MRF segmentation model [20], [21] in which shape distance is used in the segmentation model for handling occlusion and background clutter.", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "Our segmentation rectification method is inspired by the shape-prior based MRF segmentation model [20], [21] in which shape distance is used in the segmentation model for handling occlusion and background clutter.", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "in [22], which has been shown to be effective for interactive segmentation:", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "w pp\u2032 in [22] was defined as:", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "One popular method for this task is known as structured learning [17], [18], [23], [24].", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "One popular method for this task is known as structured learning [17], [18], [23], [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 21, "context": "One popular method for this task is known as structured learning [17], [18], [23], [24].", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "One popular method for this task is known as structured learning [17], [18], [23], [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "for negative label samples fk, or the worst case [17], [18], [23], [24], which can be time-consuming.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "for negative label samples fk, or the worst case [17], [18], [23], [24], which can be time-consuming.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "for negative label samples fk, or the worst case [17], [18], [23], [24], which can be time-consuming.", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "for negative label samples fk, or the worst case [17], [18], [23], [24], which can be time-consuming.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "To remove the need for negative samples, we propose to apply the oneclass SVM, instead of the conventional two-class formulation [17], to the structured learning problem.", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "positive class, for training [25], [26], [27].", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "positive class, for training [25], [26], [27].", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "positive class, for training [25], [26], [27].", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "This principle can be used for learning the weight vector w from data [17], [18], and it can be expressed as:", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "This principle can be used for learning the weight vector w from data [17], [18], and it can be expressed as:", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "For segmentation problem, \u2206 is a specialized cost and is often set as \u2206 = mean(|fk \u2212 f\u2217 k |) [28].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "With the additional requirements of positiveness and boundedness of w, the above yields the conventional SSVM of the following form [17], [18], [23], [24].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "With the additional requirements of positiveness and boundedness of w, the above yields the conventional SSVM of the following form [17], [18], [23], [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "With the additional requirements of positiveness and boundedness of w, the above yields the conventional SSVM of the following form [17], [18], [23], [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "With the additional requirements of positiveness and boundedness of w, the above yields the conventional SSVM of the following form [17], [18], [23], [24].", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "This optimization problem is well-known as one class support vector machine, and it has been thoroughly studied previously in the classification literature [25].", "startOffset": 156, "endOffset": 160}, {"referenceID": 22, "context": "Algorithm 1: Two-class SSVM learning [24]", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "We adopt the cutting plane algorithm for two-class SSVM [24].", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "It is well known that graph cut for MRF model with 2ndorder pairwise potential suffers from the shrinking bias [19], [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "It is well known that graph cut for MRF model with 2ndorder pairwise potential suffers from the shrinking bias [19], [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "classifiers [5], [6] are shown to be able to correct local errors near the object boundary.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "classifiers [5], [6] are shown to be able to correct local errors near the object boundary.", "startOffset": 17, "endOffset": 20}, {"referenceID": 28, "context": "The computational time for the Boykov-Kolmogorov (BK) algorithm on a 2 MP image on CPU is about 160 ms, and the GPU implementation of graph cuts can be 2 times faster than on the CPU [30].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "The foreground-background classifier we use is the one reported in [6], and its average computational time is about 1.", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "\u2019s classifier [6].", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "(GMM), which is a typical global foreground-background (FB) classifier, and the other is the state-of-the-art local FB classifier proposed in [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "We applied our method to GMM based FB classifier and the state-of-the-art FB classifier proposed in [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "We main compare with the FB classifier + Matting, as adopted in [6], and FB classifier + uniformly weighted graph cuts (UWGC), which is adopted in Rotobrush [5].", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "We main compare with the FB classifier + Matting, as adopted in [6], and FB classifier + uniformly weighted graph cuts (UWGC), which is adopted in Rotobrush [5].", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "Original image FB Classifier [6] + Matting FB Classifier + UWGC Zoom in Our method (2CSSVM) Zoom in Our method (OSSVM) Zoom in Fig.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Original image FB Classifier [6] + Matting FB Classifier + UWGC Zoom in Our method (2CSSVM) Zoom in Our method (OSSVM) Zoom in Fig.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Generally, a global classifier like GMM is not suitable to video cutout [5], we conduct the experiment to validate the generality of our framework.", "startOffset": 72, "endOffset": 75}, {"referenceID": 29, "context": "The experimental results shown in this subsection also imply that the errors from GMM for image cutout can be more effectively removed by using our method, compared with the conventional methods based on graph cuts and matting [31], [32], and the 2CSSVM outperforms the rest in this case while the OSSVM is the best alternative when computational efficiency in learning is a concern.", "startOffset": 227, "endOffset": 231}, {"referenceID": 30, "context": "The experimental results shown in this subsection also imply that the errors from GMM for image cutout can be more effectively removed by using our method, compared with the conventional methods based on graph cuts and matting [31], [32], and the 2CSSVM outperforms the rest in this case while the OSSVM is the best alternative when computational efficiency in learning is a concern.", "startOffset": 233, "endOffset": 237}, {"referenceID": 29, "context": "It is also interesting to note that global classifiers, such as GMM, are commonly adopted in image cutout [31], [32].", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "It is also interesting to note that global classifiers, such as GMM, are commonly adopted in image cutout [31], [32].", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "We further demonstrate the efficacy of our approach for rectifying the state-of-the-art local classifier proposed in [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "We also compare our method with Video SnapCut (Rotobrush) [5] and [6] on additional video sequences.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "We also compare our method with Video SnapCut (Rotobrush) [5] and [6] on additional video sequences.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "FB C la ss ifi er [6 ]+", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "The dataset we used is from the INRIA 3D movie dataset [33].", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "FB C la ss ifi er [6 ] +", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "\u2019s classifier [6], Classifier with Matting, OSSVM with RGB only, uniform weights for GC on RGB-D, 2CSSVM for RGB-D and OSSVM for RGB-D.", "startOffset": 14, "endOffset": 17}, {"referenceID": 32, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 94, "endOffset": 98}, {"referenceID": 34, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 100, "endOffset": 104}, {"referenceID": 35, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 106, "endOffset": 110}, {"referenceID": 36, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 112, "endOffset": 116}, {"referenceID": 37, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 118, "endOffset": 122}, {"referenceID": 38, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "It has been noted in [5] that localized optimization allows user to have better control of the video cutout process.", "startOffset": 21, "endOffset": 24}], "year": 2016, "abstractText": "Recent works on interactive video object cutout mainly focus on designing dynamic foreground-background (FB) classifiers for segmentation propagation. However, the research on optimally removing errors from the FB classification is sparse, and the errors often accumulate rapidly, causing significant errors in the propagated frames. In this work, we take the initial steps to addressing this problem, and we call this new task segmentation rectification. Our key observation is that the possibly asymmetrically distributed false positive and false negative errors were handled equally in the conventional methods. We, alternatively, propose to optimally remove these two types of errors. To this effect, we propose a novel bilayer Markov Random Field (MRF) model for this new task. We also adopt the well-established structured learning framework to learn the optimal model from data. Additionally, we propose a novel one-class structured SVM (OSSVM) which greatly speeds up the structured learning process. Our method naturally extends to RGB-D videos as well. Comprehensive experiments on both RGB and RGB-D data demonstrate that our simple and effective method significantly outperforms the segmentation propagation methods adopted in the state-of-the-art video cutout systems, and the results also suggest the potential usefulness of our method in image cutout system. Fig. 0: Given a keyframe segmentation provided by the user (left), our approach generates accurate object cutout results in subsequent frames fully automatically (middle), which can be used for creating a novel compositing (right).", "creator": "LaTeX with hyperref package"}}}