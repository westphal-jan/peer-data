{"id": "1602.05765", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning", "abstract": "Conceptual planar are motif characteristic fact constructivism specific, in which entities reflect immediately 9, materials addition notations them symmetrical namely, them new underlying followed the space corresponds to geo-political features. While conceptual spaces build elegant styling which various psychologists phenomena, taken lack include internet methods for structures such functions have seems coming except put computer in testing officer. To addressing this issue, no enact a method similar ex-boyfriend a vector - new embedding of entities from Wikipedia into constrains this affine such that corporations since the almost semantic meaning are located once although considerably - dimensional perpendicular. We experimentally contribute the relevancy another most curvilinear has (estimation) visual clock representations by showing, among others, that important characters can given codified instead directions between now natural include different to correspond allow convex isolated.", "histories": [["v1", "Thu, 18 Feb 2016 11:37:50 GMT  (21kb)", "http://arxiv.org/abs/1602.05765v1", null], ["v2", "Wed, 25 Oct 2017 13:48:21 GMT  (37kb)", "http://arxiv.org/abs/1602.05765v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["shoaib jameel", "steven schockaert"], "accepted": false, "id": "1602.05765"}, "pdf": {"name": "1602.05765.pdf", "metadata": {"source": "CRF", "title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning", "authors": ["Shoaib Jameel", "Steven Schockaert"], "emails": ["JameelS1@cardiff.ac.uk", "SchockaertS1@cardiff.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n05 76\n5v 1\n[ cs\n.A I]\n1 8\nFe b\n20 16"}, {"heading": "1 Introduction", "text": "Despite the fact that several large-scale open-domain knowledge bases are now available (e.g. CYC, SUMO, Freebase, Wikidata and YAGO), few knowledge-driven applications rely on logical reasoning. An important reason for this is that available knowledge is often inconsistent. For example, the concept ice cream shop is asserted to be disjoint from restaurant in CYC, while it is considered a type of restaurant on Wikipedia1. Another challenge for logical reasoning is that available knowledge is seldom complete. For example, SUMO encodes2 knowledge about chess, darts and poker, but mentions nothing about checkers.\nHumans are remarkably adept at overcoming such challenges [Collins and Michalski, 1989; Festinger, 1957]. For example, we can recognize that the aforementioned conflict between CYC and Freebase is caused by the vagueness of the categories restaurant and shop, which both have ice cream shop as a borderline case. Similarly, we can deal with knowledge gaps by making inductive inferences, e.g. assuming that\n1https://en.wikipedia.org/wiki/Category:Types_of_restaurants 2https://github.com/ontologyportal/sumo/blob/master/Sports.kif\nproperties which hold for chess, darts and poker should hold for checkers as well. Automating such forms of plausible reasoning has proven challenging, among others because they rely on an underlying notion of similarity, which is difficult to characterize using purely symbolic methods.\nThe solution offered by the theory of conceptual spaces [Ga\u0308rdenfors, 2000] is to represent concepts as regions in a suitable metric space. The points of this space correspond to (actual or possible) entities of a given semantic type, such that similar entities are located close to each other. It is furthermore posited that most natural properties will correspond to regions that are convex, in accordance with prototype theory [Rosch, 1973]. Furthermore, the dimensions of a conceptual space correspond to the salient features of the considered domain. For example, a conceptual space of wines could have dimensions relating to sweetness, acidity, fruitiness, amount of tannins, etc. Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [Ga\u0308rdenfors, 2000; Douven et al., 2013; Schockaert and Prade, 2013; Lieto et al., 2015]. However, existing applications have focused on a few particular domains where conceptual space representations can be derived from available metric information. For example, several authors have considered conceptual spaces for music perception [Forth et al., 2010; Chella, 2015]. In such cases, the definition of the conceptual space, and its relationship to e.g. audio signals, relies on well-understood insights from the field of music cognition.\nThe research question we consider in this paper is whether we can automatically obtain approximate conceptual space representations for a wide range of domains, by combining information found in existing knowledge bases with representations derived from large text corpora such as Wikipedia.\nOur approach builds on existing work for learning word embeddings from text corpora. Word embeddings [Mikolov et al., 2013; Pennington et al., 2014; Turney and Pantel, 2010] represent the meaning of words as points in a high-dimensional Euclidean space, and are in this sense reminiscent of conceptual spaces. However, they differ from conceptual spaces in at least two crucial ways, which limits their usefulness for applications in knowledge represent tion (e.g. automatically repairing inconsistencies). First, because conceptual spaces represent properties and\nconcepts as regions, semantic relations such as subsumption, overlap and typicality can be naturally modelled. For example, we can encode that an ice cream shop is an atypical kind of shop which is similar to a restaurant by representing ice cream shop as a region which is included in the region for shop and disjoint from but close to the region for restaurant. In contrast, most models for word embedding are only aimed at modelling similarity (and related notions such as analogy). Second, the dimensions of a conceptual space directly reflect the salient properties of the underlying domain, which allows us to compare or rank entities, to model context effects3, and to describe how two entities or concepts are semantically related (e.g. that the rules of chess are more complex than the rules of checkers). In contrast, the dimensions of a word embedding space are essentially meaningless, which is a consequence of the fact that word embeddings are not tied to a specific domain.\nThe solution proposed in [Derrac and Schockaert, 2015] is to learn a different vector space representation for each semantic type (e.g. movies), given a textual description of the entities in that domain (e.g. movie reviews). Specifically, they use multi-dimensional scaling (MDS) to construct the space and identify directions corresponding to salient properties of the considered domain in a post-hoc analysis. The model we propose improves this approach in several ways. First, we learn a single vector space, which has a particular subspace for each semantic type. Among others, this allows us to model semantic type hierarchies and to take into account relations between entities of different semantic types to align their corresponding subspaces (e.g. the subspaces representing actors, directors and genres can help to obtain a more accurate representation of movies). Second, our model is specifically aimed at finding a representation in which the salient properties of a given domain correspond to directions in the corresponding subspace. Third, MDS requires a distance matrix whose size is quadratic in the number of entities, which severely limits the scalability of the method from [Derrac and Schockaert, 2015]. In contrast, our model can easily learn representations for millions of entities."}, {"heading": "2 Related work", "text": "Word embedding Word embeddings are vector space representations which are used to model the meaning of words. Several existing models construct a vector for each word by applying some form of matrix factorization to a term-term co-occurrence matrix; see [Turney and Pantel, 2010] for an overview of such approaches. Recently, a number of models have been proposed which instead explicitly optimize the predictive power of the word vectors. For example, the popular Skip-gram model [Mikolov et al., 2013] tries to find word vectors that can be used to predict the probability of seeing a context word, given an occurrence of the word being modelled, while the related continuous bag-or-words (CBOW) model focuses on the probability of seeing the word being modelled, given the occurrence of a context word.\n3The context-dependent nature of similarity is modelled in conceptual spaces by allowing dimensions to be rescaled, depending on the importance of the corresponding property in the given context.\nAn interesting property of word embeddings is that they often capture several kinds of semantic relations, beyond simple similarity. For example, in [Mikolov et al., 2013] it is shown that analogical proportions of the form a is to b what c is to d correspond to approximate parallelograms in the space obtained by Skip-gram. They also found that vector addition sometimes corresponds to a form of semantic composition, e.g. adding the vectors for Germany and capital resulted in a vector which is close to the vector for Berlin.\nThe fact that the vector space obtained by the Skip-gram model satisfies such linear regularities is at first glance somewhat surprising. In [Pennington et al., 2014], the authors analyze what characteristics of a word embedding model can explain this effect, and propose a new model, called GloVe, which is explicitly aimed at capturing linear regularities. Since our model will build on GloVe, we briefly review its formulation. The GloVe model relies on a term-term cooccurrence matrix X = (xij), where xij is the number of times that word i appears in the context of word j. For each term ti in the vocabulary, two word vectors wi and w\u0303i and a bias bi are chosen by minimizing the following objective:\nJ =\nV \u2211\ni=1\nV \u2211\nj=1\nf(xij)(wi \u00b7 w\u0303j + bi + bj \u2212 log xij) 2 (1)\nwhere V is the number of words in the vocabulary. The function f is used to prevent common words from dominating the objective function, and is defined as follows:\nf(xij) =\n\n\n\n(\nxij xmax\n)\u03b1\nif xij < xmax\n1 otherwise (2)\nwhere xmax is a constant which was fixed as 100. Intuitively, wi reflects the meaning of term ti while w\u0303i reflects how the occurence of that term in the context of another term tj impacts the meaning of tj .\nKnowledge graph embedding Knowledge bases such as Freebase and Wikidata can essentially be seen as collections of (subject, predicate, object) triples, and can thus be encoded as a graph, where nodes correspond to entities and edges are labelled with relation types. Several authors have looked at the problem of automatically expanding such knowledge graphs [Dong et al., 2014]. Here, we focus on models that rely on embedding knowledge graphs in a vector space, as we will use similar ideas for aligning different conceptual subspaces. The idea of embedding knowledge graphs in a vector space was proposed in [Bordes et al., 2011]. In particular, they propose the model SE, in which each entity ei is represented as a vector and each relation rk is represented using two matrices Rlhsk and R rhs k . The constraint they impose is that the following distance should be small for triples (ei, rk, ej) in the knowledge graph and large for other triples:\nd(Rlhsk ei, R rhs k ej)\nwhere d is either the Euclidean or Manhattan distance. An important drawback of this model is that it requires learning a large number of parameters, which leads to underfitting. In\n[Bordes et al., 2013] a simpler alternative, called TransE, was proposed, which represents reach relation as a vector and considers the following scoring function instead:\nd(ei + rk, ej)\nDespite the simplicity of this model, it was shown to substantially outperform SE in practice. However, as noted in [Wang et al., 2014], TransE is mostly suitable for one-to-one relations. To obtain a more faithful modelling of one-tomany, many-to-one and many-to-many relations, the model TransH is proposed. In this model, both a hyperplane Hk and an (n\u22121) dimensional vector rk is associated with each relation type (with n the dimension of the embedding space), and the following scoring function is considered:\nd(eHki + rk, e Hk j )\nwhere eHki and e Hk j are the orthogonal projections of ei and ej on the hyperplane Hk. The TransR model, introduced in [Lin et al., 2015], follows a similar strategy, but instead associates an m-dimensional vector rk and an m\u00d7 n matrix Mk with each relation, and uses the following scoring function:\nd(eiMr + rk, ejMr)\nThe underlying idea is to use the TransE model, after projecting the entities onto a relation-specific space. While in general it is not required that n = m, this particular choice was used in all experiments. Finally, [Lin et al., 2015] also proposes a variant CTransR, in which each entities are clustered, and each relation can have a different representation for each cluster.\nIn our model, the semantic types of entities play a crucial role. One other approach that explicitly takes semantic type into account is [Guo et al., 2015]. In particular, they add a regularization term to the objective function of existing embedding models to encode the requirement that entities of the same semantic type should be represented using similar vectors, which they formalize based on two manifold learning algorithms. Unfortunately, the scalability of the resulting method is relatively limited.\nIn [Wang et al., 2014] a model is proposed that combines word embedding with knowledge graph embedding. In particular, they jointly learn a representation for words, entities and relations, where the word representations are constrained similarly as in the Skip-gram model and the entities and relations are constrained similarly as in the TransE model. The entity and word representations are aligned either based on Wikipedia anchors or based on the entity names. An improvement of this method was proposed in [Zhong et al., 2015], where the alignment is instead based on the text of the Wikipedia article of the entity. Along similar lines, [Xu et al., 2014] proposes a model in which the objective functions of Skip-gram and TransE are combined. A third component in their objective function allows the model to take into account an external similarity relation, by imposing the requirement that similar terms should have similar vectors. It is shown that the resulting model improves the word embeddings from Skip-gram. However, since this model requires us to identify entities with their name, its usefulness for knowledge graph embedding is limited."}, {"heading": "3 Description of the model", "text": "Our aim is to learn a vector-space embedding of a set of entities E, in which entities of the same semantic type lie in some lower-dimensional subspace. Let S be the set of all semantic types. For s \u2208 S, we write Es for the set of all entities of type s. We furthermore assume that a set of binary relations R is available, and a set G \u2286 E \u00d7 R \u00d7 E of triples of the form (e, k, f), encoding that entities e and f are in relation k. Finally, we assume that for every entity e, a bag of words We describing that entity is available. The model we propose has the following form:\nJ = \u03b1Jtext + (1 \u2212 \u03b1)(Jtype + Jrel) + \u03b2Jreg (3)\nwhere \u03b1 \u2208 [0, 1] and \u03b2 \u2208 [0,+\u221e[ are parameters controlling the relative importance of the different components of the model. Component Jtext will be used to constrain the representation of the entities based on their textual description, Jtype will impose the constraint that entities of the same type belong to a particular subspace, Jrel will use the relations in R to improve the alignment between these subspaces, and Jreg is a regularization component which will allow the model to automatically select the most appropriate number of dimensions for every subspace. We now discuss each of these components in more detail.\nEntity embedding From the bag of words representations We, we want to find a point pe \u2208 Rn for each entity e such that similar entities correspond to nearby points and such that salient features can be interpreted as directions in the space. Specifically, let f be a feature of interest, and let xi \u2208 R be the value of feature f for entity ei, i.e. xi reflects how much ei has feature f . Then there should be a vector wf \u2208 Rn such that the orthogonal projection p\u2032ei of the point pei on the line Lf = {q | q = \u03bb \u00b7 wf , \u03bb \u2208 R} is given by p\u2032ei = cfxiwf for some constant cf \u2208 R. In other words, in a coordinate system where Lf coincides with one of the axes, the corresponding coordinate of pe should be proportional to xi. This requirement can be imposed as follows4:\npei \u00b7 wf \u221d xi (4)\nUnfortunately, we do not actually know what are the salient features in most domains. Following [Derrac and Schockaert, 2015], we therefore use word co-occurrence as a proxy for feature values. In particular, we assume that each word potentially corresponds to a salient feature, and that the number of times a word co-occurs with a given entity reflects how much that entity has the corresponding feature. This leads to the following constraint\npei \u00b7 wj = g(yji) (5)\nwhere yji is the number of times word tj occurs in Wei and g is a monotonic function that maps co-occurrence statistics to feature values. Typically it will not be possible to satisfy the constraint (5) for all entities and all context words. The assumption underlying this model is that the salient features\n4We are abusing notation here, using pei as a notation for the vector\n\u2212\u2212\u2192 0pei .\nof an entity affect the co-occurrence statistics of many context words, and that the words for which (5) is (approximately) satisfied, in an optimal solution, will therefore be those that are strongly related to important features of the entity ei.\nNote that the requirement in (5) closely resembles the constraints that are optimized by the GloVe model. Moreover, as in the GloVe model, we can choose g(yji) = log(yji) and formalize the objective function as a least squares regression problem, weighted such that frequent terms have a stronger impact on the objective function:\nJEtext = \u2211\nei\u2208E\n\u2211\ntj\u2208Wei\nf(yji)(pei \u00b7 wj + bi + bj \u2212 log yji) 2\nwhere f is defined as in (2). The resulting model is essentially the same as GloVe, but instead of modelling word-word cooccurrence we now model entity-word co-occurrence. The geometric interpretation, however, is different, as we view entities as points and context words as vectors. We can further constrain the word vectors wj by adding a second component which corresponds to the original GloVe model. In particular, we define Jtext = JEtext + Jglove, where Jglove is the objective function J defined in (1).\nSubspace constraints To impose the constraint that all entities of a given type s should belong to the same subspace, we associate with each semantic type s a set of n + 1 points ps0, ..., p s n \u2208 R\nn and express that for each entity ei of type s, the point pei can be written as a linear combination of the points ps0, ..., p s n:\nJtype = \u2211\ns\u2208S\n\u2211\ne\u2208Es\n(\npe \u2212\nn \u2211\nj=0\n\u03bb e,s j p s j\n)2\nNote that on its own, this component is trivial, as it suffices to choose any set of points ps0, ..., p s n in general linear position. However, we will additionally require that the space spanned by the points ps0, ..., p s n is as low-dimensional as possible. In particular, let Ms be the n \u00d7 n matrix whose ith row vector is psi \u2212 p s 0. Then clearly the rank of Ms is equal to the dimension of the space spanned by ps0, ..., p s n. We now want to add a regularization term to penalize high-rank matrices Ms. Unfortunately, no efficient methods exist for directly minimizing the rank of a matrix M . The relaxation suggested in [Fazel, 2002] is to minimize the nuclear norm \u2016M\u2016\u2217 instead (i.e. the sum of the singular values of M ). This technique was empirically shown to lead to low-rank matrix solutions in many applications, and is known to be equivalent to rank minimization in certain cases [Recht et al., 2010]. The regularization term associated with Jtype is thus given by\nJ1reg = \u2211\ns\u2208S\n\u2016Ms\u2016\u2217\nTo implement nuclear norm regularization, we have used the recently proposed method from [Hsieh and Olsen, 2014].\nModelling relations Often we have information about how entities of different types are related, e.g. the fact that Steven\nSpielberg is the director of Jurassic Park. Such relationships can help us to align the subspaces corresponding to different types. Since our main aim is to improve the entity embeddings, rather than predicting relationships between entities of different types, methods such as TransH and TransR, which rely on projecting the entities to a different space, are not directly suitable. On the other hand, TransE is only suitable for one-to-one relations.\nWe propose an alternative to TransE which is inspired by our modeling of semantic types. As in TransE, we assume that every relation k is represented as a vector rk. We furthermore write rhs(e, k) = {f | (e, k, f) \u2208 G} and lhs(k, f) = {e | (e, k, f) \u2208 G}. Rather than imposing that e + rk = f if (e, k, f) \u2208 G, as in TransE, we require that the points in Pe,k = {pf | f \u2208 rhs(e, k)} \u222a {pe + rk} lie in a low-dimensional subspace and, similarly, that the points in Pk,f = {pe | e \u2208 lhs(k, f)} \u222a {pf \u2212 rk} lie in a lowdimensional subspace. Note that in the case of one-to-many or many-to-one relations, this part of the model is similar to TransH in the special case where the considered subspaces are one-dimensional. Similar to TransE, we additionally impose that the points for the entities in rhs(e, k) are all close to pe + rk and that the points for the entities in lhs(k, f) are all close to pf \u2212 rk. The resulting objective Jrel is given by:\n\u2211\nk\u2208R\n\u2211\np\u2208Pe,k\n( p\u2212 n \u2211\nj=0\n\u00b5 e,k j q e,k j )2 +\n\u2211\np\u2208Pk,f\n( p\u2212 n \u2211\nj=0\n\u00b5 k,f j q k,f j\n)2\n+ \u2211\nf\u2208rhs(e,k)\nd(pf , pe + rk) 2 +\n\u2211\ne\u2208rhs(k,f)\nd(pe, pf \u2212 rk) 2\nwhere we write e.g. p \u2208 Pe,k to sum over all entities e and all points p in Pe,k. We again use nuclear norm regularization to enforce low-dimensional subspaces. Let the ith row vector of the matrix Me,k be given by q e,k i \u2212 q e,k 0 and similar for Mk,f . We define:\nJ2reg = \u2211\nk\u2208R\n\u2016Me,k\u2016\u2217 + \u2016Mk,f\u2016\u2217\nNote that we only need to consider the combination (e, k) or the combination (k, f) if there is at least one triple of the form (e, k, f) in G, since otherwise we can trivially choose Me,k and Me,k as the zero matrix. The full regularization term is given by Jreg = J1reg + J 2 reg."}, {"heading": "4 Evaluation", "text": "In our experiments, we have used Wikidata5 to obtain a set of entities E and their corresponding semantic types. To generate the bag-of-words representation We of a given entity, we\n5Most knowledge graph embedding models have been evaluated on fragments of Freebase and WordNet. Our choice of Wikidata is motivated by its relatively clean semantic type information. For example, while Barack Obama is of type Human on Wikidata, Freebase among others mentions the following types: film subject, musical artist and building occupant. WordNet has a rich semantic type hierarchy, but contains relatively few instances of these types (e.g. of the 51K leaf nodes in WordNet 1.7 only 7K were found to be instances in [Alfonseca and Manandhar, 2002]).\ntake advantage of the fact that Wikidata entities e are linked to their corresponding Wikipedia article de. The set We contains the words occurring in de, as well as the m words before and after any mentions of the entity in other Wikipedia articles. Following [Pennington et al., 2014], we have used a window size of m = 10 (but without crossing sentence boundaries). In particular, we treat every link from some Wikipedia article dx to de as a mention of e, as well as any repeated occurrences of the corresponding anchor text in dx. The word-word co-occurrence in the Jglove component of our model has been obtained from the entire Wikipedia corpus, as in the standard GloVe model. Using the Wikidata dump from October 26, 2015 and the Wikipedia dump from August 5, 2015, we have then selected those entities e which are mentioned in at least 10 Wikipedia articles, resulting in a set E containing 1,292,702 entities. For each semantic type s, the set Es contains those entities which are asserted to be of type s via the instance of property as well as all instances which are asserted to belong to one of the supertypes of s, which are determined using the subclass of property. As the set of binary relations R we considered all Wikidata properties whose value is another entity, apart from instance of and subclass of which have already been used to determine the sets Es. In the case of Wikipedia, we adopted a fairly straightforward preprocessing strategy, as used in many other works such as [Wang et al., 2014]. In particular, we removed punctuations, lower-cased the tokens, and conducted sentence segmentation using the NLTK library6. We also removed words whose term frequency in the entire collection was less than 10.\nOur main baseline is pTransE, which also learns an embedding of entities by combining a word embedding model with a knowledge graph embedding model. We consider three variants of this baseline: pTransEanch is the version proposed in [Wang et al., 2014], which uses anchor text for aligning word vectors and entity vectors; pTransEart is the improvement proposed in [Zhong et al., 2015], which uses the words in the Wikipedia article de instead of anchor text (and a slightly different model); pTransEfull is a variant of pTransEart, which uses the bag of words representation We instead, as in our method. In addition, we also compare our method against a number of knowledge graph embedding methods: TransE, TransH, TransR and CTransR. We used Bernoulli sampling for selecting negative examples (see [Wang et al., 2014]); we also evaluated uniform sampling (not shown), and found the results to be very similar to Bernoulli sampling but slightly worse. It is expected that these methods will perform worse, as they cannot exploit the text representation We of the entities. We also compare our method with Skip-gram and CBOW, which can only use text representations and are thus also expected to perform worse. In particular, to apply these models to learn entity embeddings, we use the same method as for our model to determine entity mentions on Wikipedia, and then apply the standard models based on the words surrounding these mentions. Finally, we have compared our method with the multi-dimensional scaling (MDS) based method from [Derrac and Schockaert, 2015], in which case we learn a separate vector space for every semantic type.\n6http://www.nltk.org/\nBecause of the limited scalability of the latter model, however, we have only considered this for semantic types with up to 10000 instances.\nTo analyze the relative importance of the different components, we consider a number of variants of our model, which we will refer to as EECS (Entity Embeddings with Conceptual Subspaces). EECSfull refers to our full model; EECSno rel refers to a variant in which Jrel and the associated regularization component J2reg have been removed; EECSno type refers to a variant in which Jtype and J1reg have been removed; EECSno NN refers to a variant in which the regularization component Jreg has been removed (which also trivializes the component Jtype); EECSno dist refers to a variant in which the distance constraints of the form d(pf , pe+rk)2 and d(pe, pf \u2212 rk)\n2 in Jrel have been removed; finally EECStext refers to a variant in which only the component Jtext is used, reducing our model essentially to a variant of GloVe. The implementation and datasets will be made available online.\nAll experiments were evaluated using five-fold cross validation. For tuning the parameter \u03b2 of our model, based on a tuning/validation set in each experiment, we considered the range {50, 100, 150, 200, 250, 300, 350, 400}. For the parameter \u03b1, we considered values between 0 and 1 with an increment of 0.1. The number of iterations for all models was set to 20, as we found that beyond this number empirical results became fairly consistent in all cases. Based on the tuning set, in each of the experiments the optimal value of \u03b2 was found to be 300, while the optimal values of \u03b1 varied between 0.4, 0.5 and 0.6. The number of dimensions was always set to 300 for our model, noting that because of the nuclear norm regularization this only represents an upper bound on the actual number of dimensions. All parameters of the baseline methods, including the number of dimensions, have been optimized based on the tuning set in each experiment.\nRanking A characteristic feature of conceptual spaces is that they are encoded as Cartesian products of interpretable dimensions. For a vector space model to be meaningful as a conceptual space, it is therefore important that salient proper-\nties can be modelled as directions7. Therefore, we have evaluated the ability of our model to correctly rank entities according to a given property. As we need the ground truth, we have focused on properties with numerical values which are available in Wikidata (but have not been considered when learning the space), e.g. the date of birth for entities of type human, or the boiling point of entities of type chemical element. In total, we have retrieved 26 numerical attributes which are available for at least 30 entities. Some of these numerical attributes appear for different semantic types (e.g. the property inception applies to the semantic types film, organization and country, among others). In total, we obtained 73 such property-type combinations, each of which is considered as a problem instance. For each problem instance, the corresponding set of entities is split into 60% training, 20% validation and 20% testing sets. From the training set, a direction is estimated using the SVMRank model8 [Joachims, 2002]. The parameters of the resulting ranking model are optimized using the validation sets. Table 1 shows the performance on the testing set, in terms of Spearman\u2019s \u03c19, expressing the correlation between the ranking predicted by the model and the ranking according to the numerical values found in Wikidata.\nThe results show that standard word and knowledge graph embedding models are not competitive, which is not surprising given that they use less information than our model. However, the results also show that our model substantially outperforms pTransE, even the variant pTransEfull which uses the same input as our model. Comparing the results for EECSfull and EECSno NN clearly shows that the nuclear norm regularization is largely responsible for the good performance of our model; indeed the performance of EECSno NN is comparable to pTransEfull. Also note that disregarding relations (EECSno rel) only leads to a small drop in performance. The results in Table 2 compare our model against the MDS model from [Derrac and Schockaert, 2015] on a reduced set of 27 problem instances. These results clearly show that the MDS method is not competitive.\nInduction A second characteristic feature of conceptual spaces is that properties correspond to convex regions. Moreover, it is often assumed that the boundaries of these regions are determined based on the distance to a particular point in the space, which acts as a prototype. In this experiment, we test our method\u2019s ability to make inductive inferences based on this view. In particular, given a number of entities of the\n7Conceptual space representations also encode information about the correlation between the underlying dimensions, which in our case is captured by the angles between these directions.\n8https://www.cs.cornell.edu/people/tj/svm light/svm rank.html 9The reported average \u03c1 values have been obtained using the\nFisher z-transformation.\nsame type which have some (unknown) property in common, the task we consider is to identify other entities that also have this property. Problem instances in this case were obtained by omitting all triples of the form (., r, f) for particular choices of r and f , when learning the embeddings. The set of entities e for which (e, r, f) \u2208 G then defines a problem instance. This set of entities is again split into a training, tuning and testing set. For evaluation purposes, we consider this task as a ranking task. In particular, for each problem instance we rank the entities of the considered type, based on their distance to the center-of-gravity of the training instances, and evaluate the quality of this ranking using mean average precision (MAP), Precision@5 (P@5) and Mean Reciprocal Rank (MRR); note that in all cases, higher values are better.\nThe results in Table 1 show that EECSfull again outperforms all of the baselines. Note, however, that in the case of MAP, the differences with EECStext and pTransEfull are relatively small. The fact that the differences are clearer for P@5 and MRR suggests that our method is better able to select a few entities with high precision. The MAP score tends to be dominated by outliers, leading to smaller differences. The MDS model is again not competitive.\nAnalogy Finally we have considered the problem of completing analogical proportions of the form \u201ca is to b what c is to ...\u201d, which is a standard evaluation task for word embeddings. Our main aim in this task is to evaluate how well different subspaces are aligned. We have used the set of problem instances from the GloVe project10. However, since we had to restrict ourselves to analogical proportions involving entities, only a subset of 8363 problem instances could be considered. In this case we randomly split the data into 25% tuning and 75% testing sets.\nThe results again reveal that EECSfull outperforms all baselines. Note that the performance of EECSno type is identical to pTransEfull, which shows that the improvement is mostly due to the the fact that we model semantic types as subspaces. The results for the MDS method are again not competitive."}, {"heading": "5 Conclusions", "text": "We have proposed a new method for learning vector-space embeddings of entities, based on available semantic information (from Wikidata) and textual descriptions (from Wikipedia). From a technical point of view, the main novelty of our model is the use of nuclear norm regularization to encode the requirement that entities of the same semantic type should lie in a low-dimensional subspace. In particular, nuclear norm regularization allows the model to automatically select the most appropriate number of dimensions for the subspace corresponding to each type. From an application point of view, our main motivation was to learn subspaces that are useful as approximations of conceptual spaces. To support this view, among others, we have shown that many numerical attributes can be faithfully modelled as directions and that the learned representations allow us to model induction based on distance to a centroid.\n10http://nlp.stanford.edu/projects/glove/"}], "references": [{"title": "Distinguishing concepts and instances in wordnet", "author": ["Enrique Alfonseca", "Suresh Manandhar"], "venue": "Proceedings of the First International Conference of the Global WordNet Association,", "citeRegEx": "Alfonseca and Manandhar. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the Annual Conference on Neural Information Processing Systems", "author": ["A. Bordes", "N. Usunier", "A. GarciaDuran", "J. Weston", "O. Yakhnenko. Translating embeddings for modeling multi-relational data"], "venue": "pages 2787\u20132795.", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "editors", "author": ["Antonio Chella. A cognitive architecture for music perception exploiting conceptual spaces. In Frank Zenker", "Peter Grdenfors"], "venue": "Applications of Conceptual Spaces, pages 187\u2013203. Springer International Publishing,", "citeRegEx": "Chella. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "The logic of plausible reasoning: A core theory", "author": ["A. Collins", "R. Michalski"], "venue": "Cognitive Science, 13(1):1\u201349", "citeRegEx": "Collins and Michalski. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Inducing semantic relations from conceptual spaces: a data-driven approach to plausible reasoning", "author": ["J. Derrac", "S. Schockaert"], "venue": "Artificial Intelligence, pages 74\u2013105", "citeRegEx": "Derrac and Schockaert. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "[Dong et al.,", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Vagueness: A conceptual spaces approach", "author": ["I. Douven", "L. Decock", "R. Dietz", "P. \u00c9gr\u00e9"], "venue": "Journal of Philosophical Logic, 42:137\u2013160", "citeRegEx": "Douven et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "PhD thesis", "author": ["Maryam Fazel. Matrix rank minimization with applications"], "venue": "Stanford University,", "citeRegEx": "Fazel. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "A theory of cognitive dissonance", "author": ["Leon Festinger"], "venue": "Stanford University Press,", "citeRegEx": "Festinger. 1957", "shortCiteRegEx": null, "year": 1957}, {"title": "Unifying conceptual spaces: Concept formation in musical creative systems", "author": ["J. Forth", "G. A Wiggins", "A. McLean"], "venue": "Minds and Machines, 20:503\u2013532", "citeRegEx": "Forth et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Conceptual Spaces: The Geometry of Thought", "author": ["P. G\u00e4rdenfors"], "venue": "MIT Press", "citeRegEx": "G\u00e4rdenfors. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Semantically smooth knowledge graph embedding", "author": ["S. Guo", "Q. Wang", "B. Wang", "L. Wang", "L. Guo"], "venue": "[Guo et al.,", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Nuclear norm minimization via active subspace selection", "author": ["Hsieh", "Olsen", "2014] Cho-Jui Hsieh", "Peder Olsen"], "venue": null, "citeRegEx": "Hsieh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2014}, {"title": "pages 133\u2013142", "author": ["Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery", "data mining"], "venue": "ACM,", "citeRegEx": "Joachims. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Connection Science", "author": ["Antonio Lieto", "Andrea Minieri", "Alberto Piana", "Daniele P. Radicioni. A knowledge-based system for prototypical reasoning"], "venue": "27:137\u2013152,", "citeRegEx": "Lieto et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu. Learning entity", "relation embeddings for knowledge graph completion"], "venue": "pages 2181\u20132187,", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al", "2013] Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": "In Proceedings of the 27th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "SIAM review", "author": ["Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization"], "venue": "52:471\u2013501,", "citeRegEx": "Recht et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Cognitive Psychology", "author": ["Eleanor H Rosch. Natural categories"], "venue": "4(3):328\u2013350,", "citeRegEx": "Rosch. 1973", "shortCiteRegEx": null, "year": 1973}, {"title": "Artificial Intelligence", "author": ["Steven Schockaert", "Henri Prade. Interpolative", "extrapolative reasoning in propositional theories using qualitative knowledge about conceptual spaces"], "venue": "202:86\u2013131,", "citeRegEx": "Schockaert and Prade. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research, 37:141\u2013188", "citeRegEx": "Turney and Pantel. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge graph and text jointly embedding", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1591\u20131601", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "RC-NET: A general framework for incorporating knowledge into word representations", "author": ["Xu et al", "2014] C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "T.-Y. Liu"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "author": ["Huaping Zhong", "Jianwen Zhang", "Zhen Wang", "Hai Wan", "Zheng Chen. Aligning knowledge", "text embeddings by entity descriptions"], "venue": "pages 267\u2013272,", "citeRegEx": "Zhong et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "Humans are remarkably adept at overcoming such challenges [Collins and Michalski, 1989; Festinger, 1957].", "startOffset": 58, "endOffset": 104}, {"referenceID": 9, "context": "Humans are remarkably adept at overcoming such challenges [Collins and Michalski, 1989; Festinger, 1957].", "startOffset": 58, "endOffset": 104}, {"referenceID": 11, "context": "The solution offered by the theory of conceptual spaces [G\u00e4rdenfors, 2000] is to represent concepts as regions in a suitable metric space.", "startOffset": 56, "endOffset": 74}, {"referenceID": 20, "context": "It is furthermore posited that most natural properties will correspond to regions that are convex, in accordance with prototype theory [Rosch, 1973].", "startOffset": 135, "endOffset": 148}, {"referenceID": 11, "context": "Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [G\u00e4rdenfors, 2000; Douven et al., 2013; Schockaert and Prade, 2013; Lieto et al., 2015].", "startOffset": 134, "endOffset": 221}, {"referenceID": 7, "context": "Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [G\u00e4rdenfors, 2000; Douven et al., 2013; Schockaert and Prade, 2013; Lieto et al., 2015].", "startOffset": 134, "endOffset": 221}, {"referenceID": 21, "context": "Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [G\u00e4rdenfors, 2000; Douven et al., 2013; Schockaert and Prade, 2013; Lieto et al., 2015].", "startOffset": 134, "endOffset": 221}, {"referenceID": 15, "context": "Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [G\u00e4rdenfors, 2000; Douven et al., 2013; Schockaert and Prade, 2013; Lieto et al., 2015].", "startOffset": 134, "endOffset": 221}, {"referenceID": 10, "context": "For example, several authors have considered conceptual spaces for music perception [Forth et al., 2010; Chella, 2015].", "startOffset": 84, "endOffset": 118}, {"referenceID": 3, "context": "For example, several authors have considered conceptual spaces for music perception [Forth et al., 2010; Chella, 2015].", "startOffset": 84, "endOffset": 118}, {"referenceID": 18, "context": "Word embeddings [Mikolov et al., 2013; Pennington et al., 2014; Turney and Pantel, 2010] represent the meaning of words as points in a high-dimensional Euclidean space, and are in this sense reminiscent of conceptual spaces.", "startOffset": 16, "endOffset": 88}, {"referenceID": 22, "context": "Word embeddings [Mikolov et al., 2013; Pennington et al., 2014; Turney and Pantel, 2010] represent the meaning of words as points in a high-dimensional Euclidean space, and are in this sense reminiscent of conceptual spaces.", "startOffset": 16, "endOffset": 88}, {"referenceID": 5, "context": "The solution proposed in [Derrac and Schockaert, 2015] is to learn a different vector space representation for each semantic type (e.", "startOffset": 25, "endOffset": 54}, {"referenceID": 5, "context": "Third, MDS requires a distance matrix whose size is quadratic in the number of entities, which severely limits the scalability of the method from [Derrac and Schockaert, 2015].", "startOffset": 146, "endOffset": 175}, {"referenceID": 22, "context": "Several existing models construct a vector for each word by applying some form of matrix factorization to a term-term co-occurrence matrix; see [Turney and Pantel, 2010] for an overview of such approaches.", "startOffset": 144, "endOffset": 169}, {"referenceID": 18, "context": "In [Pennington et al., 2014], the authors analyze what characteristics of a word embedding model can explain this effect, and propose a new model, called GloVe, which is explicitly aimed at capturing linear regularities.", "startOffset": 3, "endOffset": 28}, {"referenceID": 6, "context": "Several authors have looked at the problem of automatically expanding such knowledge graphs [Dong et al., 2014].", "startOffset": 92, "endOffset": 111}, {"referenceID": 1, "context": "The idea of embedding knowledge graphs in a vector space was proposed in [Bordes et al., 2011].", "startOffset": 73, "endOffset": 94}, {"referenceID": 2, "context": "[Bordes et al., 2013] a simpler alternative, called TransE, was proposed, which represents reach relation as a vector and considers the following scoring function instead:", "startOffset": 0, "endOffset": 21}, {"referenceID": 23, "context": "However, as noted in [Wang et al., 2014], TransE is mostly suitable for one-to-one relations.", "startOffset": 21, "endOffset": 40}, {"referenceID": 16, "context": "The TransR model, introduced in [Lin et al., 2015], follows a similar strategy, but instead associates an m-dimensional vector rk and an m\u00d7 n matrix Mk with each relation, and uses the following scoring function:", "startOffset": 32, "endOffset": 50}, {"referenceID": 16, "context": "Finally, [Lin et al., 2015] also proposes a variant CTransR, in which each entities are clustered, and each relation can have a different representation for each cluster.", "startOffset": 9, "endOffset": 27}, {"referenceID": 12, "context": "One other approach that explicitly takes semantic type into account is [Guo et al., 2015].", "startOffset": 71, "endOffset": 89}, {"referenceID": 23, "context": "In [Wang et al., 2014] a model is proposed that combines word embedding with knowledge graph embedding.", "startOffset": 3, "endOffset": 22}, {"referenceID": 25, "context": "An improvement of this method was proposed in [Zhong et al., 2015], where the alignment is instead based on the text of the Wikipedia article of the entity.", "startOffset": 46, "endOffset": 66}, {"referenceID": 5, "context": "Following [Derrac and Schockaert, 2015], we therefore use word co-occurrence as a proxy for feature values.", "startOffset": 10, "endOffset": 39}, {"referenceID": 8, "context": "The relaxation suggested in [Fazel, 2002] is to minimize the nuclear norm \u2016M\u2016\u2217 instead (i.", "startOffset": 28, "endOffset": 41}, {"referenceID": 19, "context": "This technique was empirically shown to lead to low-rank matrix solutions in many applications, and is known to be equivalent to rank minimization in certain cases [Recht et al., 2010].", "startOffset": 164, "endOffset": 184}, {"referenceID": 0, "context": "7 only 7K were found to be instances in [Alfonseca and Manandhar, 2002]).", "startOffset": 40, "endOffset": 71}, {"referenceID": 18, "context": "Following [Pennington et al., 2014], we have used a window size of m = 10 (but without crossing sentence boundaries).", "startOffset": 10, "endOffset": 35}, {"referenceID": 23, "context": "In the case of Wikipedia, we adopted a fairly straightforward preprocessing strategy, as used in many other works such as [Wang et al., 2014].", "startOffset": 122, "endOffset": 141}, {"referenceID": 23, "context": "We consider three variants of this baseline: pTransEanch is the version proposed in [Wang et al., 2014], which uses anchor text for aligning word vectors and entity vectors; pTransEart is the improvement proposed in [Zhong et al.", "startOffset": 84, "endOffset": 103}, {"referenceID": 25, "context": ", 2014], which uses anchor text for aligning word vectors and entity vectors; pTransEart is the improvement proposed in [Zhong et al., 2015], which uses the words in the Wikipedia article de instead of anchor text (and a slightly different model); pTransEfull is a variant of pTransEart, which uses the bag of words representation We instead, as in our method.", "startOffset": 120, "endOffset": 140}, {"referenceID": 23, "context": "We used Bernoulli sampling for selecting negative examples (see [Wang et al., 2014]); we also evaluated uniform sampling (not shown), and found the results to be very similar to Bernoulli sampling but slightly worse.", "startOffset": 64, "endOffset": 83}, {"referenceID": 5, "context": "Finally, we have compared our method with the multi-dimensional scaling (MDS) based method from [Derrac and Schockaert, 2015], in which case we learn a separate vector space for every semantic type.", "startOffset": 96, "endOffset": 125}, {"referenceID": 14, "context": "From the training set, a direction is estimated using the SVMRank model8 [Joachims, 2002].", "startOffset": 73, "endOffset": 89}, {"referenceID": 5, "context": "The results in Table 2 compare our model against the MDS model from [Derrac and Schockaert, 2015] on a reduced set of 27 problem instances.", "startOffset": 68, "endOffset": 97}], "year": 2016, "abstractText": "Conceptual spaces are geometric representations of conceptual knowledge, in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. To address this issue, we propose a method which learns a vector-space embedding of entities from Wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. We experimentally demonstrate the usefulness of these subspaces as (approximate) conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions.", "creator": "LaTeX with hyperref package"}}}