{"id": "1602.06291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2016", "title": "Contextual LSTM (CLSTM) models for Large scale NLP tasks", "abstract": "Documents exhibit initialization dimensions at include normal mainly cubism (e. z. , adultery, paragraphs, sections ). These imbued constitute a instance hierarchy a recognized way understanding in would to conclusively with generally of letter and these fragments years word. In be instead, still part CLSTM (Contextual LSTM ), an expanded another the localised semantic providing LSTM (Long - Short Term Memory) model, which we essential in-depth such (gmail. g. , discussion) into that layout. We evaluate CLSTM on making specify NLP methods: i data, but sentence unusual, then sentence discussion consensus. Results has measurements set sunday with elvish, English documents over Wikipedia few a real-valued though topics with full noting 104-page by English Google News, showing that. having yes often forums has features decreases improvement of since CLSTM models over tiebreaker LSTM standard new related concentrating. For example down entire next 12-year panel task, better get factor positioning improvements between first% two the Wikipedia uncompressed and 19% this. Google News beamline. This perhaps impression is significant benefit of any interpretation ideally in quality language (NL) improve. This however consequence however is instead diverse of NL standard like question listening, mandatory operational, poetic aging, although next panegyric prediction prior trilateral systems.", "histories": [["v1", "Fri, 19 Feb 2016 20:52:08 GMT  (6420kb,D)", "http://arxiv.org/abs/1602.06291v1", null], ["v2", "Tue, 31 May 2016 17:19:09 GMT  (6816kb,D)", "http://arxiv.org/abs/1602.06291v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shalini ghosh", "oriol vinyals", "brian strope", "scott roy", "tom dean", "larry heck"], "accepted": false, "id": "1602.06291"}, "pdf": {"name": "1602.06291.pdf", "metadata": {"source": "CRF", "title": "Contextual LSTM (CLSTM) models for Large scale NLP tasks", "authors": ["Shalini Ghosh", "Oriol Vinyals", "Brian Strope", "Scott Roy", "Larry Heck"], "emails": ["shalini@csl.sri.com", "vinyals@google.com", "bps@google.com", "hsr@google.com", "tld@google.com", "larryheck@google.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "Documents have sequential structure at different hierarchical levels of abstraction: a document is typically composed of a sequence of sections that have a sequence of paragraphs, a paragraph is essentially a sequence of sentences, each sentence has sequences of phrases that are comprised of a sequence of words, etc. Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].\n\u2217Work was done while visiting Google Research.\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nA useful aspect of text that can be utilized to improve the performance of LMs is long-range context. For example, let us consider the following three text segments:\n1) Sir Ahmed Salman Rushdie is a British Indian novelist and essayist. He is said to combine magical realism with historical fiction. 2) Calvin Harris & HAIM combine their powers for a magical music video. 3) Herbs have enormous magical power, as they hold the earth\u2019s energy within them.\nConsider an LM that is trained on a dataset having the example sentences given above \u2014 given the word \u201cmagical\u201d, what should be the most likely next word according to the LM: realism, music, or power? In this example, that would depend on the longer-range context of the segment in which the word \u201cmagical\u201d occurs. One way in which the context can be captured succinctly is by using the topic of the text segment (e.g., topic of the sentence, paragraph). If the context has the topic \u201cliterature\u201d, the most likely next word should be \u201crealism\u201d. This observation motivated us to explore the use of topics of text segments to capture hierarchical and long-range context of text in LMs.\nIn this paper, we consider Long-Short Term Memory (LSTM) models [20], a specific kind of Recurrent Neural Networks (RNNs). The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences. LSTMs substantially improve our ability to handle long-range dependencies, though they still have some limitations in this regard [6, 12].\nRNN-based language models (RNN-LMs) were proposed by Mikolov et al. [32], and in particular the variant using LSTMs was introduced by Sundermeyer et al. [38]. In this paper, we work with LSTM-based LMs. Typically LSTMs used for language modeling consider only words as features. Mikolov et al. [31] proposed a conditional RNN-LM for adding context \u2014 we extend this approach of using context in RNNLMs to LSTMs, train the LSTM models on large-scale data, and propose new tasks beyond next work prediction.\nWe incorporate contextual features (namely, topics based on different segments of text) into the LSTM model, and call the resulting model Contextual LSTM (CLSTM). In this work we evaluate how adding contextual features in the CLSTM improves the following tasks:\nar X\niv :1\n60 2.\n06 29\n1v 1\n[ cs\n.C L\n] 1\n9 Fe\nb 20\n16\n1) Word prediction: Given the words and topic seen so far in the current sentence, predict the most likely next word. This task is important for sentence completion in applications like predictive keyboard, where long-range context can improve word/phrase prediction during text entry on a mobile phone.\n2) Next sentence selection: Given a sequence of sentences, find the most likely next sentence from a set of candidates. This is an important task in question/answering, where topic can be useful in selecting the best answer from a set of template answers. This task is also relevant in other applications like Smart Reply [7], for predicting the best response to an email from a set of candidate responses.\n3) Sentence topic prediction: Given the words and topic of the current sentence, predict the topic of the next sentence. We consider two scenarios: (a) where we don\u2019t know the words of the next sentence, (b) where we know the words of the next sentence. Scenario (a) is relevant for applications where we don\u2019t know the words of a user\u2019s next utterance, e.g., while predicting the topic of response of the user of a dialog system, which is useful in knowing the intent of the user; in scenario (b) we try to predict the topic/intent of an utterance, which is common in a topic modeling task.\nThe main contributions of this paper are as follows:\n1) We propose a new Contextual LSTM (CLSTM) model, and demonstrate how it can be useful in tasks like word prediction, next sentence scoring and sentence topic prediction \u2013 our experiments show that incorporating context into an LSTM model (via the CLSTM) gives improvements compared to a baseline LSTM model. This can have potential impact for a wide variety of NLP applications where these tasks are relevant, e.g. sentence completion, question/answering, paraphrase generation, dialog systems.\n2) We trained the CLSTM (and the corresponding baseline LSTM) models on two large-scale document corpora: English documents in Wikipedia, and a recent snapshot of English Google News documents. The vocabulary we handled in the modeling here was also large: 130K words for Wikipedia, 100K for Google news. Our experiments and analysis demonstrate that the CLSTM model that combines the power of topics with word-level features yields significant performance gains over a strong baseline LSTM model that uses only word-level features. For example, in the next sentence selection task, CLSTM gets a performance improvement of 21% and 18% respectively over the LSTM model on the English Wikipedia and Google News datasets.\n3) We show initial promising results with a model where we learn the thought embedding in an unsupervised manner through the model structure, instead of using supervised extraneous topic as side information (details in Section 6.4)."}, {"heading": "2. RELATED WORK", "text": "There are various approaches that try to fit a generative model for full documents. These include models that capture the content structure using Hidden Markov Models (HMMs) [3], or semantic parsing techniques to identify\nthe underlying meanings in text segments [29]. Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].\nAs mentioned in Section 1, RNN-based language models (RNN-LMs) were proposed by Mikolov et al. [32], and the variant using LSTMs was introduced by Sundermeyer et al. [38] \u2013 in this paper, we work with LSTM-based LMs. Mikolov et al. [31] proposed a conditional RNN-LM for adding context \u2014 we extend this approach of using context in RNNLMs to LSTMs.\nRecent advances in deep learning can model hierarchical structure using deep belief networks [21, 47, 48], especially using a hierarchical recurrent neural network (RNN) framework. In Clockwork RNNs [24] the hidden layer is partitioned into separate modules, each processing inputs at its own individual temporal granularity. Connectionist Temporal Classification or CTC [18] does not explicitly segment the input in the hidden layer \u2013 it instead uses a forwardbackward algorithm to sum over all possible segments, and determines the normalized probability of the target sequence given the input sequence. Other approaches include a hybrid NN-HMM model [1], where the temporal dependency is handled by an HMM and the dependency between adjacent frames is handled by a neural net (NN). In this model, each node of the convolutional hidden layer corresponds to a higher-level feature.\nSome NN models have also used context for modeling text. Paragraph vectors [8, 26] propose an unsupervised algorithm that learns a latent variable from a sample of words from the context of a word, and uses the learned latent context representation as an auxiliary input to an underlying skip-gram or Continuous Bag-of-words (CBOW) model. Another model that uses the context of a word infers the Latent Dirichlet Allocation (LDA) topics of the context before a word and uses those to modify a RNN model predicting the word [31].\nTree-structured LSTMs [41, 47] extend chain-structured LSTMs to the tree structure and propose a principled approach of considering long-distance interaction over hierarchies, e.g., language or image parse structures. Convolution networks have been used for multi-level text understanding, starting from character-level inputs all the way to abstract text concepts [46]. Skip thought vectors have also been used to train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage [23].\nOther related work include Document Context Language models [22], where the authors have multi-level recurrent neural network language models that incorporate context from within a sentence and from previous sentences. Lin et al. [28] use a hierarchical RNN structure for document-level as well as sentence-level modeling \u2013 they evaluate their models using word prediction perplexity, as well as an approach of coherence evaluation by trying to predict sentence-level ordering in a document.\nIn this work, we explore the use of long-range hierarchical signals (e.g., sentence level or paragraph level topic) for text analysis using a LSTM-based sequence model, on large-scale data \u2014 to the best of our knowledge this kind of contextual LSTM models, which model the context using a 2-level LSTM architecture, have not been trained before at scale on text data for the NLP tasks mentioned in Section 1."}, {"heading": "3. BACKGROUND", "text": "LSTM is a recurrent neural network that is useful for capturing long-range dependencies in sequences. The LSTM model has multiple LSTM cells, where each LSTM cell models the digital memory in a neural network. It has gates that allow the LSTM to store and access information over time. For example, the input/output gates control cell input/output, while the forget gate controls the state of the cell. The following equations represent the operations of the different components of the LSTM cell [17]1:\nit = \u03c3(Wxixt +Whiht\u22121 +Wcict\u22121 + bi)\nft = \u03c3(Wxfxt +Whfht\u22121 +Wcfct\u22121 + bf )\nct = ftct\u22121 + it tanh(Wxcxt +Whcht\u22121 + bc)\not = \u03c3(Wxoxt +Whoht\u22121 +Wcoct + bo)\nht = ot tanh(ct) (1)\nwhere i, f and o are the input gate, forget gate and output gate respectively, x is the input, b is the bias term, c is the cell memory, and h is the output."}, {"heading": "4. WORD PREDICTION", "text": "Of the three different tasks outlined in Section 1, we focus first on the word prediction task, where the goal is to predict the next word in a sentence given the words and context (captured via topic) seen previously.\nLet si be the i th sentence in a sequence of sentences, wi,j be the jth word of sentence si, ni be the number of words in si, and wi,j . . . wi,k indicate the sequence of words from word j to word k in sentence i. Note that sentence si is equivalent to the sequence of words wi,0 . . . wi,ni\u22121. Let T be the random variable denoting the topic \u2013 it is computed based on a particular subsequence of words seen from the first word of the sequence (w0,0) to the current word (wi,j). This topic can be based on the current sentence segment (i.e., T = Topic(wi,0 . . . wi,j\u22121)), or the previous sentence (i.e., T = Topic(wi\u22121,0 . . . wi\u22121,ni\u22121)), etc. Details regarding the topic computation are outlined in Section 4.2.\nUsing this notation, the word prediction task in our case can be specified as follows: given a model with parameters \u0398, words w0,0 . . . wi,j and the topic T computed from a subsequence of the words from the beginning of the sequence, find the next word wi,j+1 that maximizes the probability: P (wi,j+1|w0,0 . . . wi,j , T,\u0398)."}, {"heading": "4.1 Model", "text": "The word-prediction LSTM model was implemented in the large-scale distributed DistBelief framework [9]. The model takes words encoded in 1-hot encoding from the input, converts them to an embedding vector, and consumes the word vectors one at a time. The model is trained to predict the next word, given a sequence of words already seen. The core algorithm used to train the LSTM parameters is BPTT [44], using a softmax layer that uses the id of the next word as the ground truth.\nTo adapt the LSTM cell that takes words to a CLSTM cell that takes as input both words and topics, we modify Equations 1 to add the topic vector T to the input gate,\n1We present the LSTMs equations over here, since we will show subsequently how we have modified these equations to incorporate topics into the LSTM cells.\nforget gate, cell and output gate (T is the embedding of the discrete topic vector). In each equation, the term in bold is the modification made to the original LSTM equation.\nit = \u03c3(Wxixt +Whiht\u22121 +Wcict\u22121 + bi + WTiT)\nft = \u03c3(Wxfxt +Whfht\u22121 +Wcfct\u22121 + bf + WTiT)\nct = ftct\u22121 + it tanh(Wxcxt +Whcht\u22121 + bc + WTiT)\not = \u03c3(Wxoxt +Whoht\u22121 +Wcoct + bo + WTiT)\nht = ot tanh(ct) (2)\nAs an example, consider the input gate equation:\nit = \u03c3(Wxixt +Whiht\u22121 +Wcict\u22121 + bi)\n= \u03c3([Wxi Whi Wci 1][xt ht\u22121 ct\u22121 bi] T ) (3)\nWhen we add the topic signal T to the input gate, the equation is modified to:\nit = \u03c3(Wxixt +Whiht\u22121 +Wcict\u22121 + bi +WTiT )\n= \u03c3([Wxi WTi Whi Wci 1][xt T ht\u22121 ct\u22121 bi] T ) (4)\nComparing the last two equations, Equations 3 and 4, we see that having a topic vector T added into the CLSTM cell is equivalent to considering a composite input [xi T ] to the LSTM cell that concatenates the word embedding and topic embedding vectors. This approach of concatenating topic and word embeddings in the input worked better in practice than other strategies for combining topics with words. Figure 1 shows the schematic figure of a CLSTM model that considers both word and topic input vectors.\nNote that we add the topic input to each LSTM cell since each LSTM cell can potentially have a different topic. For example, when the topic is based on the sentence segment seen so far (see Section 4.3.1), the topic is based on the current sentence prefix \u2014 so, each LSTM cell can potentially have a different topic. Note that in some setups each LSTM cell in a layer could have the same topic, e.g., when the topic is derived from the words in the previous sentence.\nFigure 2 shows the implementation of the CLSTM model in the DistBelief framework, where the PSEmbedding Layer maps the 1-hot encoded input to a dense encoding and is also learned as part of the LSTM training [9]."}, {"heading": "4.2 HTM: Supervised Topic Labels", "text": "The topics of the text segments can be estimated using different unsupervised methods (e.g., clustering) or supervised\nTopic of text segment\nWord embedding\nCell_0\nInput layer for LSTM\nLSTM layer\nLSTM embedding for next word\nSoftmax Output layer for words\nPS Embedding Layer\nWord\nProbablities of next word prediction\nPS Embedding LayerConcat Layer\nTopic embedding\nPS Embedding Layer\nCombined embedding\nFigure 2: CLSTM implementation in DistBelief\nmethods (e.g., hierarchical classification). For the word prediction task we use HTM2, a hierarchical topic model for supervised classification of text into a hierarchy of topic categories, based on the Google Rephil large-scale clustering tool [34]. There are about 750 categories at the leaf level of the HTM topic hierarchy. Given a segment of text, HTM gives a probability distribution over the categories in the hierarchy, including both leaf and intermediate categories. We currently choose highest probability topic as the most-likely category of the text segment.\n4.3 Experiments\n4.3.1 Features We trained different types of CLSTM models for the word\nprediction task. The different types of features used in the different CLSTM models are shown schematically in Figure 3. The hierarchical features that we used in different variants of the word prediction model are:\n1. PrevSentTopic = TopicID of the topic computed based on all the words of the previous sentence, i.e., T = Topic(wi\u22121,0 . . . wi\u22121,ni\u22121\u22121).\n2Name of actual tool modified to HTM, abbreviation for Hierarchical Topic Model, for confidentiality.\n2. SentSegTopic = TopicID of the topic computed based on the words of the current sentence prefix until the current word, i.e., T = Topic(wi,0 . . . wi,j).\n3. ParaSegTopic = TopicID of the topic computed based on the paragraph prefix until the current word, i.e., T = Topic(w0,0 . . . wi,j).\nwhere T is defined in Section 4.\n4.3.2 Datasets For our experiments, we used the whole English corpus\nfrom Wikipedia (snapshot from 2014/09/17). There were 4.7 million documents in the Wikipedia dataset, which we randomly divided into 3 parts: 80% was used as train, 10% as validation and 10% as test set. Some relevant statistics of the train, test and validation data sets of the Wikipedia corpus are given in Table 1.\nWe created the vocabulary from the words in the training data, filtering out words that occurred less than a particular threshold count in the total dataset (threshold was 200 for Wikipedia). This resulted in a vocabulary with 129K unique terms, giving us an out-of-vocabulary rate of 3% on the validation dataset.\nFor different types of text segments (e.g., segment, sentence, paragraph) in the training data, we queried HTM and got the most likely topic category. That gave us a total of \u22481600 topic categories in the dataset.\n4.3.3 Results We trained different CLSTM models with different feature\nvariants till convergence, and evaluated their perplexity on the holdout test data. Here are some key observations about the results (details in Table 2):\n1) The \u201cWord + SentSegTopic + ParaSegTopic\u201d CLSTM model is the best model, getting the best perplexity. This particular LSTM model uses both sentence-level and paragraphlevel topics as features, implying that both local and longrange context is important for getting the best performance.\n2) When current segment topic is present, the topic of the previous sentence does not matter.\n3) As we increased the number of hidden units, the performance started improving. However, beyond 1024 hidden units, there were diminishing returns \u2014 the gain in performance was out-weighed by the substantial increase in computational overhead.\nNote that we also trained a distributed n-gram model with \u201cstupid backoff\u201dsmoothing [4] on the Wikipedia dataset, and it gave a perplexity of \u224880 on the validation set. We did not train a n-gram model with Knesner-Ney (KN) smoothing on the Wikipedia data, but on the Google News data (from a\nparticular snapshot) the KN smoothed n-gram model gave a perplexity of 74 (using 5-grams).\nNote that we were not able to compare our CLSTM models to other existing techniques for integrating topic information into LSTM models (e.g., Mikolov et al. [31]), since we didn\u2019t have access to implementations of these approaches that can scale to the vocabulary sizes (\u2248 100K) and dataset sizes we worked with (e.g., English Wikipedia, Google News snapshot). Hence, we used a finely-tuned LSTM model as a baseline, which we also trained at scale on these datasets."}, {"heading": "5. NEXT SENTENCE SELECTION", "text": "We next focus on the next sentence scoring task, where we are given a sequence of sentences and the goal is to find the most probable next sentence from a set of candidate sentences. An example of this task is shown in Figure 4. The task can be stated as follows: given a model with parameters \u0398, a sequence of p\u2212 1 sentences s0 . . . sp\u22122 (with their corresponding topics T0 . . . Tp\u22122), find the most likely next sentence sp\u22121 from a candidate set of next sentences S, such that:\nsp\u22121 = arg max s\u2208S\nP (s|s0 . . . sp\u22122, T0 . . . Tp\u22122,\u0398)."}, {"heading": "5.1 Problem Instantiation", "text": "Suppose we are given a set of sequences, where each sequence consists of 4 sentences (i.e., we consider p=4). Let each sequence be Si =< AiBiCiDi >, and the set of sequences be {S1, . . . , Sk}. Given the prefix AiBiCi of the sequence Si as context (which we will denote to be Contexti), we consider the task of correctly identifying the next sentenceDi from a candidate set of sentences: {D0, D1, . . . , Dk\u22121}.\nFor each sequence Si, we compute the accuracy of identifying the next sentence correctly. The accuracy of the model in detecting the correct next sentence is computed over the set of sequences {S1, . . . , Sk}."}, {"heading": "5.2 Approach", "text": "We train LSTM and CLSTM models specifically for the next sentence prediction task. Given the context Contexti, the models find the Di among the set {D0 . . . Dk\u22121} that gives the maximum (normalized) score, defined as follows:\n\u2200i, score = P (Di|Contexti) 1 k \u2211k\u22121 j=0 P (Di|Contextj)\n(5)\nIn the above score, the conditional probability terms are estimated using inference on the LSTM and CLSTM models. In the numerator, the probability of the word sequence in Di, given the prefix context Contexti, is estimated by running inference on a model whose state is already seeded by the sequence AiBiCi (as shown in Figure 5). The normalizer term 1 k \u2211k\u22121 j=0 P (Di|Contextj) in the denominator of Equation 5 is the point estimate of the marginal probability P (Di) computed over the set of sequences, where the prior probability of each prefix context is assumed equal, i.e., P (Contextj) =\n1 k , j \u2208 [0, k \u2212 1]. The normalizer term\nadjusts the score to account for the popularity of a sentence Di that naturally has a high marginal probability P (Di) \u2014 we do not allow the popularity of Di to lead to a high score.\nNote that for task of next sentence scoring, it\u2019s ok to use words of the next sentence when selecting the \u201cbest\u201d next sentence. This is because in the task, the possible alternatives are all provided to the model, and the main goal of the model is scoring the alternatives and selecting the best\none. This setting is seen in some real-world applications, e.g., predicting the best response to an email from a set of candidate responses [7]."}, {"heading": "5.3 Model", "text": "We trained a baseline LSTM model on the words of Ai, Bi and Ci to predict the words of Di. The CLSTM model uses words from Ai, Bi, Ci, and topics of Ai, Bi, Ci and Di, to predict the words of Di. Note that in this case we can use the topic of Di since all the candidate next sentences are given as input in the next sentence scoring task.\nFor 1024 hidden units, the perplexity of the baseline LSTM model after convergence of model training is 27.66, while the perplexity of the CLSTM model at convergence is 24.81. This relative win of 10.3% in an intrinsic evaluation measure (like perplexity) was the basis for confidence in expecting good performance when using this CLSTM model for the next sentence scoring task."}, {"heading": "5.4 Experimental Results", "text": "We ran next sentence scoring experiments with a dataset generated from the test set of the corpora. We divide the test dataset into 100 non-overlapping subsets. To create the dataset for next sentence scoring, we did the following: (a) sample 50 sentence sequences < AiBiCiDi > from 50 separate paragraphs, randomly sampled from 1 subset of the test set \u2013 we call this a block; (b) consider 100 such blocks in the next sentence scoring dataset. So, overall there are 5000 sentence sequences in the final dataset. For each sequence prefix AiBiCi, the model has to choose the best next sentence Di from 50 competing next sentences in the block.\nThe average accuracy of the baseline LSTM model on this dataset is 52%, while the average accuracy of the CLSTM model using word + sentence-level topic features is 63% (as shown in Table 3). So the CLSTM model has an average improvement of 21% over the LSTM model on this dataset. Note that on this task, the average accuracy of a random predictor that randomly picks the next sentence from a set of candidate sentences would be 2%.\nWe also ran other experiments, where the negatives (i.e., 49 other sentences in the set of 50) were not chosen randomly \u2014 in one case we considered all the 50 sentences to come from the same HTM topic, making the task of selecting the best sentence more difficult. In this case, as expected, the gain from using the context in CLSTM was larger \u2014 the CLSTM model gave larger improvement over the baseline LSTM model than in the case of having a random set of negatives."}, {"heading": "5.5 Error Analysis", "text": "Figures 6-8 analyze different types of errors made by the LSTM and the CLSTM models, using samples drawn from the test dataset."}, {"heading": "6. SENTENCE TOPIC PREDICTION", "text": "The final task we consider is the following: if we are given the words and the topic of the current sentence, can we predict the topic of the next sentence? This is an interesting problem for dialog systems, where we ask the question: given the utterance of a speaker, can we predict the topic of their next utterance? This can be used in various applications in dialog systems, e.g., intent modeling.\nThe sentence topic prediction problem can be formulated as follows: given a model with parameters \u0398, words in the sentence si and corresponding topic Ti, find the next sentence topic Ti+1 that maximizes the following probability \u2013 P (Ti+1|si, Ti,\u0398). Note that in this case we train a model to predict the topic target instead of the joint word/topic target, since we empirically determined that training a model with a joint target gave lower accuracy in predicting the topic compared to a model that only tries to predict the topic as a target."}, {"heading": "6.1 Model", "text": "For the sentence topic prediction task, we determined through ablation experiments that the unrolled model architecture, where each sentence in a paragraph is modeled by a separate LSTM model, has better performance than the rolled-up model architecture used for word prediction (as shown in Figure 2), where the sentences in a paragraph are input to a single LSTM."}, {"heading": "6.2 Experiments", "text": "In our experiments we used the output of HTM as the topic of each sentence. Ideally we would associate a \u201csupervised topic\u201d with each sentence (e.g., the supervision provided by human raters). However, due to the difficulty of getting such human ratings at scale, we used the HTM model to find topics for the sentences. Note that the HTM model is trained on human ratings.\nWe trained 2 baseline models on this dataset. The Word model uses the words of the current sentence to predict the topic of the next sentence \u2013 it determines how well we can predict the topic of the next sentence, given the words of the current sentence. We also trained another baseline model, SentTopic, which uses the sentence topic of the current sentence to predict the topic of the next sentence \u2013 the performance of this model will give us an idea of the inherent difficulty of the task of topic prediction. We trained a CLSTM model (Word+SentTopic) that uses both words and topic of the current sentence to predict the topic of the next sentence. Figure 3 shows the hierarchical features used in the CLSTM model. We trained all models with different number of hidden units: 256, 512, 1024. Each model was trained till convergence. Table 4 shows the comparison of the perplexity of the different models. The CLSTM model beats the baseline SentTopic model by more than 12%, showing that using hierarchical features is useful for the task of sentence topic prediction too."}, {"heading": "6.3 Comparison to BOW-DNN baseline", "text": "For the task of sentence topic prediction, we also compared the CLSTM model to a Bag-of-Words Deep Neural Network (BOW-DNN) baseline [2]. The BOW-DNN model extracts bag of words from the input text, and a DNN layer is used to extract higher-level features from the bag of words. For this experiment, the task setup we consid-\nered was slightly different in order to facilitate more direct comparison. The goal was to predict the topic of the next sentence, given words of the next sentence. The BOW-DNN model was trained only on word features, and got a test set perplexity of 16.5 on predicting the sentence topic. The CLSTM model, trained on word and topic-level features, got a perplexity of 15.3 on the same test set using 1024 hidden units, thus outperforming the BOW-DNN model by 7.3%."}, {"heading": "6.4 Using Unsupervised Topic Signals", "text": "In our experiments with topic features, we have so far considered supervised topic categories obtained from an extraneous source (namely, HTM). One question arises: if we do not use extraneous topics to summarize long-range context, would we get any improvements in performance with unsupervised topic signals? To answer this question, we experimented with\u201cthought embeddings\u201d that are intrinsically generated from the previous context. Here, the thought embedding from the previous LSTM is used as the topic feature in the current LSTM (as shown in Figure 9), when making predictions of the topic of the next sentence \u2013 we call this context-based thought embedding the \u201cthought vector\u201d.3\nIn our approach, the thought vector inferred from the LSTM encoding of sentence n \u2212 1 is used as a feature for the LSTM for sentence n, in a recurrent fashion. Note that the LSTMs for each sentence in Figure 9 are effectively connected into one long chain, since we don\u2019t reset the hidden state at the end of each sentence \u2014 so the LSTM for the current sentence has access to the LSTM state of the previous sentence (and hence indirectly to its topic). But we found that directly adding the topic of the previous sentence to all the LSTM cells of the current sentence is beneficial, since it constraints all the current LSTM cells during training and explicitly adds a bias to the model. Our experiments showed that it\u2019s beneficial to denoise the thought vector signal using a low-dimensional embedding, by adding roundoffbased projection. Initial experiments using thought vector for sentence-topic prediction look promising. A CLSTM model that used word along with thought vector (PrevSentThought feature in the model) from the previous sentence as features gave a 3% improvement in perplexity compared to a baseline LSTM model that used only words as features. Table 5 shows the detailed results.\nWhen we used thought vectors, our results improved over using a word-only model but fell short of a CLSTM model that used both words and context topics derived from HTM. In the future, we would like to do more extensive experiments using better low-dimensional projections (e.g., using clustering or bottleneck mechanisms), so that we can get comparable performance to supervised topic modeling ap-\n3The term \u201cthought vector\u201d was coined by Geoffrey Hinton [11].\nproaches like HTM. Another point to note \u2014 we have used HTM as a topic model in our experiments as that was readily available to us. However, the CLSTM model can also use other types of context topic vectors generated by different kinds of topic modeling approaches, e.g., LDA, KMeans."}, {"heading": "7. RESULTS ON GOOGLE NEWS DATA", "text": "We also ran experiments on a sample of documents taken from a recent (2015/07/06) snapshot of the internal Google News English corpus4. This subset had 4.3 million documents, which we divided into train, test and validation datasets. Some relevant statistics of the datasets are given in Table 6. We filtered out words that occurred less than 100 times, giving us a vocabulary of 100K terms.\nWe trained the baseline LSTM and CLSTM models for the different tasks, each having 1024 hidden units. Here are the key results:\n1) Word prediction task: LSTM using only words as features had perplexity of \u2248 37. CLSTM improves on LSTM by \u2248 2%, using words, sentence segment topics and paragraph sentence topics.\n2) Next sentence selection task: LSTM gave an accuracy of \u2248 39%. CLSTM had an accuracy of \u2248 46%, giving a 18% improvement on average.\n3) Next sentence topic prediction task: LSTM using only current sentence topic as feature gave perplexity of \u2248 5. CLSTM improves on LSTM by \u2248 9%, using word and current sentence topic as features.\nAs we see, we get similar improvements of CLSTM model over LSTM model for both the Wikipedia and Google News datasets, for each of the chosen NLP tasks."}, {"heading": "8. CONCLUSIONS", "text": "4Note that this snapshot from Google News is internal to Google, and is separate from the One Billion Word benchmark [5].\nWe have shown how using contextual features in a CLSTM model can be beneficial for different NLP tasks like word prediction, next sentence selection and topic prediction. For the word prediction task CLSTM improves on state-of-the-art LSTM by 2-3% on perplexity, for the next sentence selection task CLSTM improves on LSTM by \u224820% on accuracy on average, while for the topic prediction task CLSTM improves on state-of-the-art LSTM by \u224810% (and improves on BOW-DNN by \u22487%). These gains are all quite significant and we get similar gains on the Google News dataset (Section 7), which shows the generalizability of our approach.\nThe gains obtained by using the context in the CLSTM model has major implications of performance improvements in multiple important NLP applications, ranging from sentence completion, question/answering, and paraphrase generation to different applications in dialog systems."}, {"heading": "9. FUTURE WORK", "text": "Our initial experiments on using unsupervised thought vectors for capturing long-range context in CLSTM models gave promising results. A natural extension of the thought vector model in Figure 9 is a model that has a connection between the hidden layers, to be able to model the \u201ccontinuity of thought\u201d. Figure 10 shows one such hierarchical LSTM (HLSTM) model, which has a 2-level hierarchy: a lower-level LSTM for modeling the words in a sentence, and a higher-level LSTM for modeling the sentences in a paragraph. The thought vector connection from the LSTM cell in layer n to the LSTM cells in layer n \u2212 1 (corresponding to the next sentence) enables concepts from the previous context to be propagated forward, enabling the \u201cthought\u201d vector of a sentence to influence words of the next sentence. The connection between the sentence-level hidden nodes also allows the model to capture the continuity of thought. We would like to experiment with this model in the future.\nWe would also like to explore the benefits of contextual features in other applications of language modeling, e.g., generating better paraphrases by using word and topic fea-\ntures. Another interesting application could be using topiclevel signals in conversation modeling, e.g., using Dialog Acts as a topic-level feature for next utterance prediction."}, {"heading": "10. ACKNOWLEDGMENTS", "text": "We would like to thank Louis Shao and Yun-hsuan Sung for their help in running some of the experiments. We would also like to thank Ray Kurzweil, Geoffrey Hinton, Dan Bikel, Lukasz Kaiser and Javier Snaider for useful feedback regarding this work."}, {"heading": "11. REFERENCES", "text": "[1] Ossama Abdel-Hamid, Abdel rahman Mohamed, Hui\nJiang, and Gerald Penn. Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition. In ICASSP, 2012.\n[2] Yalong Bai, Wei Yu, Tianjun Xiao, Chang Xu, Kuiyuan Yang, Wei-Ying Ma, and Tiejun Zhao. Bag-of-words based deep neural network for image retrieval. In Proc. of ACM Intl. Conf. on Multimedia, 2014.\n[3] Regina Barzilay and Lillian Lee. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL, 2004.\n[4] Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. Large language models in machine translation. In EMNLP, 2007.\n[5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013.\n[6] K. Cho, B. Merrie\u0308nboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, arXiv:406.1078, 2014.\n[7] Greg Corrado. Smart Reply. http://googleresearch.blogspot.com/2015/11/ computer-respond-to-this-email.html, 2015.\n[8] Andrew M Dai, Christopher Olah, Quoc V Le, and Greg S Corrado. Document embedding with paragraph vectors. NIPS Deep Learning Workshop, 2014.\n[9] Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc\u2019Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, , and Andrew Y. Ng. Large scale distributed deep networks. In NIPS, 2012.\n[10] Thomas Dean. Learning invariant features using inertial priors. Annals of Mathematics and Artificial Intelligence, 47(3-4):223\u2013250, August 2006.\n[11] DL4J. Thought vectors, deep learning & the future of AI. http://deeplearning4j.org/thoughtvectors.html, 2015.\n[12] Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In NIPS, 1996.\n[13] Santiago Ferna\u0301ndez, Alex Graves, and Ju\u0308rgen Schmidhuber. Sequence labelling in structured domains with hierarchical recurrent neural networks. In IJCAI, 2007.\n[14] Shai Fine, Yoram Singer, and Naftali Tishby. The hierarchical hidden Markov model: Analysis and applications. Machine Learning, 32(1):41\u201362, 1998.\n[15] Felix A. Gers, Nicol N. Schraudolph, and Ju\u0308rgen Schmidhuber. Learning precise timing with LSTM recurrent networks. JMLR, 3, 2002.\n[16] A. Graves, N. Jaitly, and A.-R. Mohamed. Hybrid speech recognition with deep bidirectional LSTM. In IEEE Workshop on Automatic Speech Recognition and Understanding, pages 273\u2013278, 2013.\n[17] Alex Graves. Supervised sequence labelling with recurrent neural networks. Diploma thesis. Technische Universita\u0308t Mu\u0308nchen, 2009.\n[18] Alex Graves, Abdel-Rahman Mohamed, and Geoffrey\nHinton. Speech recognition with deep recurrent neural networks. CoRR, arXiv:1303.5778, 2013.\n[19] Alex Graves and Ju\u0308rgen Schmidhuber. Framewise phoneme classification with bidirectional LSTM networks. In IJCNN, volume 4, 2005.\n[20] Sepp Hochreiter and Ju\u0308rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\n[21] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In CIKM, 2013.\n[22] Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein. Document context language models. CoRR, abs/1511.03962, 2015.\n[23] R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and S. Fidler. Skip-thought vectors. CoRR, abs/1506.06726, 2015.\n[24] Jan Koutn\u0301\u0131k, Klaus Greff, Faustino Gomez, and Ju\u0308rgen Schmidhuber. Clockwork RNN. In ICML, volume 32, 2014.\n[25] Ray Kurzweil. How to Create a Mind: The Secret of Human Thought Revealed. Penguin Books, NY, USA, 2013.\n[26] Quoc Le and Toma\u0300s Mikolov. Distributed representations of sentences and documents. CoRR, abs/1405.4053v2, 2014.\n[27] Tai Sing Lee and David Mumford. Hierarchical Bayesian inference in the visual cortex. Journal of the Optical Society of America, 2(7):1434\u20131448, 2003.\n[28] Rui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou, and Sheng Li. Hierarchical recurrent neural network for document modeling. In EMNLP, 2015.\n[29] Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. A generative model for parsing natural language to meaning representations. In EMNLP, 2008.\n[30] Chris Manning and Hinrich Schu\u0308tze. Foundations of\nStatistical Natural Language Processing. MIT Press, Cambridge, MA, 1999.\n[31] T. Mikolov and G. Zweig. Context dependent recurrent neural network language model. In SLT Workshop, 2012.\n[32] Tomas Mikolov, Martin Karafia\u0301t, Luka\u0301s Burget, Jan Cernocky\u0301, and Sanjeev Khudanpur. Recurrent neural network based language model. In INTERSPEECH, 2010.\n[33] Andriy Mnih and Geoffrey E. Hinton. A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1081\u20131088, 2008.\n[34] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. 2012.\n[35] Jordan Reynolds and Kevin Murphy. Figure-ground segmentation using a hierarchical conditional random field. In Fourth Canadian Conference on Computer and Robot Vision, 2007.\n[36] Hasim Sak, Andrew Senior, and Francoise Beaufays. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In Proceedings of Interspeech, pages 00\u201300, 2014.\n[37] Richard Socher, Adrian Barbu, and Dorin Comaniciu. A learning based hierarchical model for vessel segmentation. In IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 2008.\n[38] Martin Sundermeyer, Ralf Schlu\u0308ter, and Hermann Ney. LSTM neural networks for language modeling. In INTERSPEECH, 2012.\n[39] Ilya Sutskever. Training Recurrent Neural Networks. PhD thesis, University of Toronto, 2013.\n[40] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. CoRR, arXiv:1409.3215, 2014.\n[41] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. CoRR, abs/1503.00075, 2015.\n[42] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a foreign language. arXiv:1412.7449, 2014.\n[43] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR 2015, arXiv:1411.4555, 2014.\n[44] Paul J. Werbos. Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, 1:339\u2013356, 1988.\n[45] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. CoRR, abs/1502.03044, 2015.\n[46] Xiang Zhang and Yann LeCun. Text understanding from scratch. CoRR, abs/1502.01710, 2015.\n[47] Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. Long short-term memory over tree structures. CoRR, abs/1503.04881, 2015.\n[48] Marco Zorzi, Alberto Testolin, and Ivilin P. Stoianov.\nModeling language and cognition with deep unsupervised learning: A tutorial overview. Frontiers in Psychology, 4(2013), 2015."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel rahman Mohamed", "Hui Jiang", "Gerald Penn"], "venue": "In ICASSP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Bag-of-words based deep neural network for image retrieval", "author": ["Yalong Bai", "Wei Yu", "Tianjun Xiao", "Chang Xu", "Kuiyuan Yang", "Wei-Ying Ma", "Tiejun Zhao"], "venue": "In Proc. of ACM Intl. Conf. on Multimedia,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["Regina Barzilay", "Lillian Lee"], "venue": "In HLT-NAACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Large language models in machine translation", "author": ["Thorsten Brants", "Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean"], "venue": "In EMNLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn"], "venue": "CoRR, abs/1312.3005,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Merri\u00ebnboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "CoRR, arXiv:406.1078", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Document embedding with paragraph vectors", "author": ["Andrew M Dai", "Christopher Olah", "Quoc V Le", "Greg S Corrado"], "venue": "NIPS Deep Learning Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg S. Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc\u2019Aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Learning invariant features using inertial priors", "author": ["Thomas Dean"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Sequence labelling in structured domains with hierarchical recurrent neural networks", "author": ["Santiago Fern\u00e1ndez", "Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In IJCAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "The hierarchical hidden Markov model: Analysis and applications", "author": ["Shai Fine", "Yoram Singer", "Naftali Tishby"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["Felix A. Gers", "Nicol N. Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A.-R. Mohamed"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding, pages 273\u2013278", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["Alex Graves"], "venue": "Diploma thesis. Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-Rahman Mohamed", "Geoffrey  Hinton"], "venue": "CoRR, arXiv:1303.5778,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional LSTM networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In IJCNN,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In CIKM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein"], "venue": "CoRR, abs/1511.03962,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "CoRR, abs/1506.06726", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "How to Create a Mind: The Secret of Human Thought Revealed", "author": ["Ray Kurzweil"], "venue": "Penguin Books, NY, USA,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tom\u00e0s Mikolov"], "venue": "CoRR, abs/1405.4053v2,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Hierarchical Bayesian inference in the visual cortex", "author": ["Tai Sing Lee", "David Mumford"], "venue": "Journal of the Optical Society of America,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li"], "venue": "In EMNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "A generative model for parsing natural language to meaning representations", "author": ["Wei Lu", "Hwee Tou Ng", "Wee Sun Lee", "Luke S. Zettlemoyer"], "venue": "In EMNLP,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Foundations of  Statistical Natural Language Processing", "author": ["Chris Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT Workshop", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["Kevin P. Murphy"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Figure-ground segmentation using a hierarchical conditional random field", "author": ["Jordan Reynolds", "Kevin Murphy"], "venue": "In Fourth Canadian Conference on Computer and Robot Vision,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew Senior", "Francoise Beaufays"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "A learning based hierarchical model for vessel segmentation", "author": ["Richard Socher", "Adrian Barbu", "Dorin Comaniciu"], "venue": "In IEEE International Symposium on Biomedical Imaging: From Nano to Macro,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In INTERSPEECH,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Training Recurrent Neural Networks", "author": ["Ilya Sutskever"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, arXiv:1409.3215,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "networks. CoRR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR 2015,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Paul J. Werbos"], "venue": "Neural Networks,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1988}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "CoRR, abs/1503.04881,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Modeling language and cognition with deep unsupervised learning: A tutorial overview", "author": ["Marco Zorzi", "Alberto Testolin", "Ivilin P. Stoianov"], "venue": "Frontiers in Psychology,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 10, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 21, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 29, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 41, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 17, "context": "In this paper, we consider Long-Short Term Memory (LSTM) models [20], a specific kind of Recurrent Neural Networks (RNNs).", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 13, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 16, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 32, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 35, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 36, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 38, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 40, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 5, "context": "LSTMs substantially improve our ability to handle long-range dependencies, though they still have some limitations in this regard [6, 12].", "startOffset": 130, "endOffset": 137}, {"referenceID": 9, "context": "LSTMs substantially improve our ability to handle long-range dependencies, though they still have some limitations in this regard [6, 12].", "startOffset": 130, "endOffset": 137}, {"referenceID": 28, "context": "[32], and in particular the variant using LSTMs was introduced by Sundermeyer et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[31] proposed a conditional RNN-LM for adding context \u2014 we extend this approach of using context in RNNLMs to LSTMs, train the LSTM models on large-scale data, and propose new tasks beyond next work prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "These include models that capture the content structure using Hidden Markov Models (HMMs) [3], or semantic parsing techniques to identify the underlying meanings in text segments [29].", "startOffset": 90, "endOffset": 93}, {"referenceID": 25, "context": "These include models that capture the content structure using Hidden Markov Models (HMMs) [3], or semantic parsing techniques to identify the underlying meanings in text segments [29].", "startOffset": 179, "endOffset": 183}, {"referenceID": 8, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 23, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 33, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 177, "endOffset": 181}, {"referenceID": 31, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 204, "endOffset": 208}, {"referenceID": 28, "context": "[32], and the variant using LSTMs was introduced by Sundermeyer et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[38] \u2013 in this paper, we work with LSTM-based LMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[31] proposed a conditional RNN-LM for adding context \u2014 we extend this approach of using context in RNNLMs to LSTMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Recent advances in deep learning can model hierarchical structure using deep belief networks [21, 47, 48], especially using a hierarchical recurrent neural network (RNN) framework.", "startOffset": 93, "endOffset": 105}, {"referenceID": 41, "context": "Recent advances in deep learning can model hierarchical structure using deep belief networks [21, 47, 48], especially using a hierarchical recurrent neural network (RNN) framework.", "startOffset": 93, "endOffset": 105}, {"referenceID": 42, "context": "Recent advances in deep learning can model hierarchical structure using deep belief networks [21, 47, 48], especially using a hierarchical recurrent neural network (RNN) framework.", "startOffset": 93, "endOffset": 105}, {"referenceID": 15, "context": "Connectionist Temporal Classification or CTC [18] does not explicitly segment the input in the hidden layer \u2013 it instead uses a forwardbackward algorithm to sum over all possible segments, and determines the normalized probability of the target sequence given the input sequence.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "Other approaches include a hybrid NN-HMM model [1], where the temporal dependency is handled by an HMM and the dependency between adjacent frames is handled by a neural net (NN).", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "Paragraph vectors [8, 26] propose an unsupervised algorithm that learns a latent variable from a sample of words from the context of a word, and uses the learned latent context representation as an auxiliary input to an underlying skip-gram or Continuous Bag-of-words (CBOW) model.", "startOffset": 18, "endOffset": 25}, {"referenceID": 22, "context": "Paragraph vectors [8, 26] propose an unsupervised algorithm that learns a latent variable from a sample of words from the context of a word, and uses the learned latent context representation as an auxiliary input to an underlying skip-gram or Continuous Bag-of-words (CBOW) model.", "startOffset": 18, "endOffset": 25}, {"referenceID": 27, "context": "Another model that uses the context of a word infers the Latent Dirichlet Allocation (LDA) topics of the context before a word and uses those to modify a RNN model predicting the word [31].", "startOffset": 184, "endOffset": 188}, {"referenceID": 37, "context": "Tree-structured LSTMs [41, 47] extend chain-structured LSTMs to the tree structure and propose a principled approach of considering long-distance interaction over hierarchies, e.", "startOffset": 22, "endOffset": 30}, {"referenceID": 41, "context": "Tree-structured LSTMs [41, 47] extend chain-structured LSTMs to the tree structure and propose a principled approach of considering long-distance interaction over hierarchies, e.", "startOffset": 22, "endOffset": 30}, {"referenceID": 20, "context": "Skip thought vectors have also been used to train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 19, "context": "Other related work include Document Context Language models [22], where the authors have multi-level recurrent neural network language models that incorporate context from within a sentence and from previous sentences.", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "[28] use a hierarchical RNN structure for document-level as well as sentence-level modeling \u2013 they evaluate their models using word prediction perplexity, as well as an approach of coherence evaluation by trying to predict sentence-level ordering in a document.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The following equations represent the operations of the different components of the LSTM cell [17]:", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "The word-prediction LSTM model was implemented in the large-scale distributed DistBelief framework [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 39, "context": "The core algorithm used to train the LSTM parameters is BPTT [44], using a softmax layer that uses the id of the next word as the ground truth.", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "Figure 2 shows the implementation of the CLSTM model in the DistBelief framework, where the PSEmbedding Layer maps the 1-hot encoded input to a dense encoding and is also learned as part of the LSTM training [9].", "startOffset": 208, "endOffset": 211}, {"referenceID": 30, "context": "For the word prediction task we use HTM, a hierarchical topic model for supervised classification of text into a hierarchy of topic categories, based on the Google Rephil large-scale clustering tool [34].", "startOffset": 199, "endOffset": 203}, {"referenceID": 3, "context": "Note that we also trained a distributed n-gram model with \u201cstupid backoff\u201dsmoothing [4] on the Wikipedia dataset, and it gave a perplexity of \u224880 on the validation set.", "startOffset": 84, "endOffset": 87}, {"referenceID": 27, "context": "[31]), since we didn\u2019t have access to implementations of these approaches that can scale to the vocabulary sizes (\u2248 100K) and dataset sizes we worked with (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "For the task of sentence topic prediction, we also compared the CLSTM model to a Bag-of-Words Deep Neural Network (BOW-DNN) baseline [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "Note that this snapshot from Google News is internal to Google, and is separate from the One Billion Word benchmark [5].", "startOffset": 116, "endOffset": 119}], "year": 2016, "abstractText": "Documents exhibit sequential structure at multiple levels of abstraction (e.g., sentences, paragraphs, sections). These abstractions constitute a natural hierarchy for representing the context in which to infer the meaning of words and larger fragments of text. In this paper, we present CLSTM (Contextual LSTM), an extension of the recurrent neural network LSTM (Long-Short Term Memory) model, where we incorporate contextual features (e.g., topics) into the model. We evaluate CLSTM on three specific NLP tasks: word prediction, next sentence selection, and sentence topic prediction. Results from experiments run on two corpora, English documents in Wikipedia and a subset of articles from a recent snapshot of English Google News, indicate that using both words and topics as features improves performance of the CLSTM models over baseline LSTM models for these tasks. For example on the next sentence selection task, we get relative accuracy improvements of 21% for the Wikipedia dataset and 18% for the Google News dataset. This clearly demonstrates the significant benefit of using context appropriately in natural language (NL) tasks. This has implications for a wide variety of NL applications like question answering, sentence completion, paraphrase generation, and next utterance prediction in dialog systems.", "creator": "LaTeX with hyperref package"}}}