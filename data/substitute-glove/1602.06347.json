{"id": "1602.06347", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2016", "title": "Distributed Constraint Optimization Problems and Applications: A Survey", "abstract": "The start of Multi - Agent System (MAS) of neither several areas form research within Artificial Intelligence, through an widely these negative last industrial under used anything - as documentation. Within a MAS, baluchistan agents evolve to pursue experience interests and / only although benefit rare objectives. Distributed Constraint Optimization Problems (DCOPs) would once in same fact main by walker architectures rest administer later agents ' territory behavior, along latter algorithms and system suvs usually particularly appeared over core as the unusual no. During rest monday still, several template instead 's DCOP vision even switching him to support MAS 1998 complex, turn - time, working uncertain environments.", "histories": [["v1", "Sat, 20 Feb 2016 00:23:10 GMT  (748kb,D)", "https://arxiv.org/abs/1602.06347v1", null], ["v2", "Wed, 6 Jul 2016 14:59:14 GMT  (898kb,D)", "http://arxiv.org/abs/1602.06347v2", null], ["v3", "Thu, 11 May 2017 03:00:42 GMT  (287kb,D)", "http://arxiv.org/abs/1602.06347v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.MA", "authors": ["ferdinando fioretto", "enrico pontelli", "william yeoh"], "accepted": false, "id": "1602.06347"}, "pdf": {"name": "1602.06347.pdf", "metadata": {"source": "CRF", "title": "Distributed Constraint Optimization Problems and Applications: A Survey", "authors": ["Ferdinando Fioretto", "Enrico Pontelli", "William Yeoh"], "emails": ["FIORETTO@UMICH.EDU", "EPONTELL@CS.NMSU.EDU", "WYEOH@CS.NMSU.EDU"], "sections": [{"heading": null, "text": "gence, with an increasingly important impact in industrial and other real-world applications. In a MAS, autonomous agents interact to pursue personal interests and/or to achieve common objectives. Distributed Constraint Optimization Problems (DCOPs) have emerged as a prominent agent model to govern the agents\u2019 autonomous behavior, where both algorithms and communication models are driven by the structure of the specific problem. During the last decade, several extensions to the DCOP model have been proposed to enable support of MAS in complex, realtime, and uncertain environments.\nThis survey provides an overview of the DCOP model, offering a classification of its multiple extensions and addressing both resolution methods and applications that find a natural mapping within each class of DCOPs. The proposed classification suggests several future perspectives for DCOP extensions, and identifies challenges in the design of efficient resolution algorithms, possibly through the adaptation of strategies from different areas."}, {"heading": "1. Introduction", "text": "An agent can be defined as an entity (or computer program) that behaves autonomously within an arbitrary system in the pursuit of some goals (Wooldridge, 2009). A multi-agent system (MAS) is a system where multiple agents interact in the pursuit of such goals. Within a MAS, agents may interact with each other directly, via communication acts, or indirectly, by acting on the shared environment. In addition, agents may decide to cooperate, to achieve a common goal, or to compete, to serve their own interests at the expense of other agents. In particular, agents may form cooperative teams, which can in turn compete against other teams of agents. Multi-agent systems play an important role in distributed artificial intelligence, thanks to their ability to model a wide variety of real-world scenarios, where information and control are decentralized and distributed among a set of agents.\nc\u00a91993 AI Access Foundation. All rights reserved.\nar X\niv :1\n60 2.\n06 34\n7v 3\n[ cs\n.A I]\n1 1\nFigure 1 illustrates a MAS, representing a sensor network scenario, where a group of agents, equipped with sensors, seeks to determine the position of some targets\u2014identified in the figure as star-shaped objects. Agents may interact with each other\u2014the dotted lines in the figure define the interaction graph. Agents may move away from the current position\u2014the directional arrows illustrate such actions. In addition, various events that dynamically obstruct the sensors of the agents (e.g., the presence of an obstacle along the sensing range of an agent) may dynamically occur.\nWithin a MAS, an agent is: \u2022 Autonomous, as it operates without the direct intervention of humans or other entities and has\nfull control over its own actions and internal state (e.g., in the example, an agent can decide to sense, to move, etc.); \u2022 Interactant, in the sense that it interacts with other agents in order to achieve its objectives\n(e.g., in the example, agents may exchange information concerning results of sensing activities); \u2022 Reactive, as it responds to changes that occur in the environment and/or to the requests from\nother agents (e.g., in the example, agents may react with a move to the sudden appearance of obstacles). \u2022 Proactive, because of its goal-driven behavior, which allows the agent to take initiatives beyond\nthe reactions in response to its environment. Agent architectures are the fundamental mechanisms underlying the autonomous agent components, supporting their behavior in real-world, dynamic and uncertain environments. Currently, agent architectures based on Decision Theory, Game Theory, and Constraint Programming have successfully been developed and are popular in the Autonomous Agents and Multi-Agent Systems (AAMAS) community.\nDecision Theory (DT) (Raiffa, 1968) assumes that the agent and the environment are inherently uncertain, and DT models such uncertainty explicitly. Acting in complex and dynamic environments requires agents to deal with various sources of uncertainty. The Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) framework (Bernstein, Givan, Immerman, & Zilberstein, 2002) is one of the most general multi-agent frameworks, focused on team coordination in presence of uncertainty about agents\u2019 actions and observations. The ability to capture a wide range of complex scenarios makes Dec-POMDPs of central interest within MAS research. However, the result of this generality is a high complexity for generating optimal solutions. Dec-POMDPs are non-deterministic exponential (NEXP) complete (Bernstein et al., 2002), even for two-agent problems, and scalability\nremains a critical challenge (Amato, Chowdhary, Geramifard, Ure, & Kochenderfer, 2013).\nGame Theory (GT) (Binmore, 1992) studies interactions between self-interested agents, aiming at maximizing the welfare of the participants. Some of the most compelling applications of game theory to MAS have been in the area of auctions and negotiations (Kraus, 1997; Noriega & Sierra, 1999; Parsons & Wooldridge, 2002). Such approaches model the trading process by which agents can reach agreements on matters of common interest, using market oriented and cooperative mechanisms, such as reaching Nash Equilibria (NEs). Typical resolution approaches aim at deriving a set of equilibrium strategies for each agent, such that, when these strategies are employed, no agent can profit by unilaterally deviating from their strategies. A limitation of GT-based approaches is the lack of an agent\u2019s ability to reason upon a global objective, as the underlying model relies on the interactions of self-interested agents.\nConstraint Programming (CP) (Rossi, Beek, & Walsh, 2006) aims at solving decision-making problems formulated as optimization problems of some real-world objective. Constraint programs use the notion of constraints\u2014i.e., relations among entities of the problems (variables)\u2014in both problem modeling and problem solving. CP relies on inference techniques that prevent the exploration of those parts of the solution search space whose assignments to variables are inconsistent with the constraints and/or dominated with respect to the objective function. Distributed Constraint Optimization Problems (DCOPs) (Modi, Shen, Tambe, & Yokoo, 2005; Petcu & Faltings, 2005b; Gershman, Meisels, & Zivan, 2009; Yeoh & Yokoo, 2012) are problems where agents need to coordinate their value assignments, in a decentralized manner, to optimize their objective functions. DCOPs focus on attaining a global optimum given the interaction graph of a collection of agents. This approach is flexible and can effectively model a wide range of problems. In general, problem solving and communication strategies are directly linked in DCOPs. This feature makes DCOP\u2019s algorithmic components suitable for exploiting the structure of the interaction graph to generate efficient solutions.\nThe absence of a framework to model dynamic problems and uncertainty makes DCOPs unsuitable at solving certain classes of multi-agent problems, such as those characterized by action uncertainty and dynamic environments. However, since its original introduction, the DCOP model has undergone a process of continuous evolution to capture diverse characteristics of agent behavior and the environment in which they operate. Researchers have proposed a number of DCOP frameworks that differ from each other in terms of expressivity and classes of problem they can target, extending the DCOP model to handle both dynamic and uncertain environments. However, current research has not explored how the different DCOP frameworks relate to each other within the general MAS context, which is critical to understand: (a) What resolution methods could be borrowed from other MAS paradigms, and (b) What applications can be most effectively modeled within each framework. While there are important existing surveys for Distributed Constraint Satisfaction (Yokoo & Hirayama, 2000) and Distributed Constraint Optimization (Meisels, 2008), this survey aims to comprehensively analyze and categorize the different DCOP frameworks proposed by the MAS community. We do so by presenting an extensive review of the DCOP model and its extensions, the different resolution methods, as well as a number of applications modeled within each particular DCOP extension. This analysis also provides opportunities to identify open areas and discuss future directions in the general DCOP research area.\nThis survey paper is organized as follows. In the next section, we provide an overview on two relevant Constraint Satisfaction models and their generalization to the distributed cases. We then introduce, in Section 3, DCOPs, give an overview on the representation and coordination models adopted in DCOP resolution, and propose a classification of the different variants of DCOPs based on the characteristics of the agents and the environment. Section 4 presents the classical DCOP model as well as two notable extensions characterized, respectively, by asymmetric function utilities and multi-objective optimization. In Section 5, we present DCOP models where the environment changes over time. In Section 6, we discuss DCOP models in which agents act under uncertainty and may have partial knowledge of the environment in which they act. Section 7 discusses DCOP models in which agents are non-cooperative. For each of these models, we introduce their formal definitions, related concepts, and resolution algorithms; Table 6 summarizes the complexity of the various classes of problems. In Section 8, we describe a number of applications that have been proposed in the DCOP literature. Section 9 provides a critical review on the various DCOP variants surveyed and focuses on their applicability. Additionally, it describes some potential future directions for research. Finally, Section 10 provides some concluding remarks. To facilitate the reading of this survey, Table 1 summarizes the most commonly used symbols and notations."}, {"heading": "2. Overview of (Distributed) Constraint Satisfaction and Optimization", "text": "In this section, we provide an overview of several Constraint Satisfaction models, which form the foundation of DCOPs. Figure 2 illustrates the relations among these models."}, {"heading": "2.1 Constraint Satisfaction Problems", "text": "Constraint Satisfaction Problems (CSPs) (Golomb & Baumert, 1965; Mackworth & Freuder, 1985; Apt, 2003; Rossi et al., 2006) are decision problems that involve the assignment of values to variables, under a set of specified constraints on how variable values should be related to each other. A number of problems can be formulated as CSPs, including resource allocation, vehicle routing, circuit diagnosis, scheduling, and bioinformatics. Over the years, CSPs have become the paradigm of choice to address hard combinatorial problems, drawing and integrating insights from diverse domains, including Artificial Intelligence and Operations Research (Rossi et al., 2006).\nFormally, a CSP is a tuple \u3008X,D,C\u3009, where: \u2022 X={x1, . . . , xn} is a finite set of variables. \u2022 D = {D1, . . . , Dn} is a set of finite domains for the variables in X, with Di being the set of\npossible values for the variable xi. \u2022 C is a finite set of constraints over subsets of X, where a constraint ci, defined on the k variables xi1 , . . . , xik , is a relation ci \u2286 \"kj=1Dij , where {i1, . . . , ik} \u2286 {1, . . . , n}. The set of variables xi = {xi1 , . . . , xik} is referred to as the scope of ci.1 ci is called a unary constraint if k = 1 and a binary constraint if k = 2. For all other values of k, the constraint is called a k-ary constraint.2\nA partial assignment is a value assignment for a proper subset of variables from X that is consistent with their respective domains, i.e., it is a partial function \u03c3 : X \u2192 \u22c3n i=1Di such that, for each xj \u2208 X, if \u03c3(xj) is defined, then \u03c3(xj) \u2208 Dj . An assignment is complete if it assigns a value to each variable in X. We will use the notation \u03c3 to denote a complete assignment, and, for a set of variables V = {xi1 , . . . , xih} \u2286 X, \u03c3V = \u3008\u03c3(xi1), . . . , \u03c3(xih)\u3009 to denote the projection of the values in \u03c3 associated to the variables in V, where i1 < \u00b7 \u00b7 \u00b7 < ih. The goal in a CSP is to find a complete assignment \u03c3 such that, for each ci \u2208 C, \u03c3xi \u2208 ci, that is, a complete assignment that satisfies all the problem constraints. Such a complete assignment is called a solution of the CSP."}, {"heading": "2.2 Weighted Constraint Satisfaction Problems", "text": "A solution of a CSP must satisfy all its constraints. In many practical cases, however, it is not possible to satisfy all constraints of a problem. At the same time, it is desirable to consider complete assignments whose constraints can be violated (according to a violation degree) and in which preferences among them can be expressed. To faithfully represent such properties, researchers introduced the notion of Weighted Constraint Satisfaction Problems (WCSPs) (Shapiro & Haral-\n1. We will assume the presence of a fixed ordering of variables. 2. A constraint with k = 3 is also called a ternary constraint and a constraint with k = n is also called a global\nconstraint.\nick, 1981; Larrosa, 2002), which are problems whose constraints are considered as preferences, specifying to what extent they are satisfied (or violated).\nA WCSP is a tuple \u3008X,D,F\u3009, where X and D are the set of variables and their domains as defined in a CSP, and F is a set of weighted constraints. A weighted constraint fi \u2208 F is a function, fi : \"xj\u2208xi Dj \u2192 R+ \u222a {\u22a5}, where xi \u2286 X is the scope of fi and \u22a5 is a special element used to denote that a given combination of values for the variables in xi is not allowed, and it has the property that a+\u22a5 = \u22a5+ a = \u22a5, for all a \u2208 R+. The cost of an assignment \u03c3 is the sum of the evaluation of the constraints involving all the variables in \u03c3. A solution is a complete assignment with cost different from \u22a5, and an optimal solution is a solution with minimal cost.\nThus, a WCSP is a generalization of a CSP which, in turn, can be seen as a WCSP whose constraints use exclusively the costs 0 and \u22a5. The terms WCSP and Constraint Optimization Problem (COP) have been used interchangeably in the literature and the use of the latter term has been widely adopted in the recent years."}, {"heading": "2.3 Distributed Constraint Satisfaction Problems", "text": "When the elements of a CSP are distributed among a set of autonomous agents, we refer to it as a Distributed Constraint Satisfaction Problem (DisCSP) (Yokoo, Durfee, Ishida, & Kuwabara, 1998; Yokoo, 2001). Formally, a DisCSP is described by a tuple \u3008A,X,D,C, \u03b1\u3009, where X, D, and C are the set of variables, their domains, and the set of constraints, as defined in a CSP; A={a1, . . . , am} is a finite set of autonomous agents; and \u03b1 : X \u2192 A is a surjective function, from variables to agents, which assigns the control of each variable x \u2208 X to an agent \u03b1(x). The goal in a DisCSP is to find a complete assignment that satisfies all the constraints of the problem.\nDisCSPs can be seen as an extension of CSPs to the multi-agent case, where agents communicate with each other to assign values to the variables they control so as to satisfy all the problem constraints. For a more detailed treatment of the argument we address the reader to (Rossi et al., 2006) (Chapter 20)."}, {"heading": "2.4 Distributed Constraint Optimization Problems", "text": "Similar to the generalization of CSPs to COPs, the Distributed Constraint Optimization Problem (DCOP) (Modi et al., 2005; Petcu & Faltings, 2005b; Gershman et al., 2009; Yeoh & Yokoo, 2012) model emerges as a generalization of the DisCSP model, where constraints specify a degree of preference over their violation, rather than a boolean satisfaction metric. DCOPs can also be viewed as an extension of the COP framework to the multi-agent case, where agents control variables and constraints, and they need to coordinate the value assignment for the variables they control so as to optimize a global objective function. We formally introduce the DCOP framework in the next section."}, {"heading": "3. DCOP Classification", "text": "The DCOP model has undergone a process of continuous evolution to capture diverse characteristics of agent behavior and the environment in which they operate. We propose a classification of DCOP models from a Multi-Agent Systems perspective, which accounts for the different assumptions made about the behavior of agents and their interactions with the environment. The classification is based on the following elements (summarized in Table 2): \u2022 Agent Behavior: This parameter captures the stochastic nature of the effects of an action being\nexecuted. In particular, we distinguish between deterministic and stochastic effects. \u2022 Agent Knowledge: This parameter captures the knowledge of an agent about its own state and\nthe environment. We distinguish between total and partial (i.e., incomplete) knowledge. \u2022 Agent Teamwork: This parameter characterizes the approach undertaken by (teams of) agents\nto solve a distributed problem. It can be either a cooperative resolution approach or a competitive resolution approach. In the former class, all agents cooperate to achieve a common goal (i.e., they all optimize a global cost function). In the latter class, each agent (or team of agents) seeks to achieve its own individual goal (i.e., they each optimize their individual cost functions). \u2022 Environment Behavior: This parameter captures the exogenous properties of the environment.\nWe distinguish between deterministic and stochastic responses of the environment to the execution of an action. \u2022 Environment Evolution: This parameter captures whether the DCOP is static (i.e., it does not\nchange over time) or dynamic (i.e., it changes over time). Figure 3 illustrates a categorization of the DCOP models proposed to date from a MAS perspective. In particular, we focus on the DCOP models proposed at the junction of Constraint\nE nv\nir on\nm en t E vo lu tio n\nEnvironment Behavior\nDETERMINISTIC STOCHASTIC\nSTATIC classical-DCOP probabilistic-DCOP\nDYNAMIC dynamic-DCOP \u2014\nTable 3: DCOPs Models\nGame Theory\nDecision Theory\nConstraint Programming\nclassical-DCOP\nAsymmetric-DCOP\nMultiObjective-DCOP Auction Negotiation\nMMDP Dec-MDP\nDec-POMDP\nDynamic-DCOP Probabilistic-DCOP\nDynamic Asymmetric-DCOP\nDynamic MultiObjective-DCOP\nFigure 3: DCOPs Within a MAS Perspective\nProgramming (CP), Game Theory (GT), and Decision Theory (DT). The classical DCOP model is directly inherited from CP and characterized by a static model, a deterministic environment and agent behavior, total agent knowledge, and with cooperative agents. Concepts from auctions and negotiations, traditionally explored in GT, have influenced the DCOP framework, leading to Asymmetric DCOPs, which has asymmetric agent payoffs, and Multi-Objective DCOPs. The DCOP framework has borrowed fundamental DT concepts related to modeling uncertain and dynamic environments, resulting in models like Probabilistic DCOPs and Dynamic DCOPs. Researchers from the DCOP community have also designed solutions that inherit from all of the three communities.\nIn the next sections, we will describe the different DCOP frameworks, starting with classical DCOPs before proceeding to its various extensions. We will focus on a categorization based on three dimensions: Agent knowledge, environment behavior, and environment evolution. We assume a deterministic agent behavior, fully cooperative agent teamwork, and total agent knowledge (unless otherwise specified), as they are, by far, common assumptions adopted by the DCOP community. The DCOP models associated to this categorization are summarized in Table 3. The bottom-right entry of the table is left empty, indicating a promising model with dynamic and uncertain environments that, to the best of our knowledge, has not been explored yet. There has been only a modest amount of effort in modeling the different aspects of teamwork within the DCOP community. We will describe a formalism that has been adopted to model DCOPs with mixed cooperative and competitive agents in Section 7."}, {"heading": "4. Classical DCOP", "text": "With respect to our categorization, in the classical DCOP model (Modi et al., 2005; Petcu & Faltings, 2005b; Gershman et al., 2009; Yeoh & Yokoo, 2012), the agents are completely cooperative and they have deterministic behavior and total knowledge. Additionally, the environment is static and deterministic. In this section, we review the formal definitions of classical DCOPs, present\nsome relevant solving algorithms, and provide details of selected variants of classical DCOPs of particular interest."}, {"heading": "4.1 Definition", "text": "A classical DCOP is described by a tuple P = \u3008A,X,D,F, \u03b1\u3009, where: \u2022 A={a1, . . . , am} is a finite set of agents. \u2022 X={x1, . . . , xn} is a finite set of variables, with n \u2265 m. \u2022 D={D1, . . . , Dn} is a finite set of domains for the variables in X, with Di being the domain of\nvariable xi. \u2022 F={f1, . . . , fk} is a finite set of objective functions, with fi : \"xj\u2208xi Dj \u2192 R+ \u222a {\u22a5}, where\nsimilar to WCSPs, xi \u2286 X is the set of variables relevant to fi, referred to as the scope of fi. Each function fi represents a factor in a global objective function Fg(X) = \u2211k i=1 fi(x\ni). In the DCOP literature, the functions fi are also called constraints, cost functions, utility functions, or reward functions. \u2022 \u03b1 : X \u2192 A is a total and onto function, from variables to agents, which assigns the control of\neach variable x \u2208 X to an agent \u03b1(x). With a slight abuse of notation, we will use \u03b1(fi) to denote the set of agents whose variables are involved in the scope of fi, i.e., \u03b1(fi) = {\u03b1(x) | x \u2208 xi}. A partial assignment is a value assignment for a proper subset of variables of X. An assignment is complete if it assigns a value to each variable in X. For a given complete assignment \u03c3, we say that a constraint fi is satisfied by \u03c3 if fi(\u03c3xi) 6= \u22a5. A complete assignment is a solution of a DCOP if it satisfies all its constraints. The goal in a DCOP is to find a solution that minimizes3 the total problem cost expressed by its cost functions:\n\u03c3\u2217 := argmin \u03c3\u2208\u03a3 Fg(\u03c3) = argmin \u03c3\u2208\u03a3 \u2211 fi\u2208F fi(\u03c3xi), (1)\nwhere \u03a3 is the state space, defined as the set of all possible solutions. Let us also introduce the following notations. Given an agent ai, we denote with Lai = {xj \u2208 X | \u03b1(xj) = ai} the set of variables controlled by agent ai, or its local variables, and we denote with Nai = {a\u2032i \u2208 A | ai 6= a\u2032i, \u2203fj \u2208 F, xr, xs \u2208 xj . \u03b1(xr) = ai \u2227 \u03b1(xs) = a\u2032i} the set of its neighboring agents. A constraint fi is said to be hard if \u2200\u03c3 \u2208 \u03a3 we have that fi(\u03c3xi) \u2208 {0,\u22a5}. Otherwise, the constraint is said to be soft.\nFinding an optimal solution for a classical DCOP is known to be NP-hard (Modi et al., 2005).\n3. Alternatively, one can define a maximization problem by substituting the argmin operator in Equation 1 with argmax. Typically, if the objective functions are referred to as utility functions or reward functions, then the DCOP is a maximization problem. Conversely, if the objective functions are referred to as cost functions, then the DCOP is a minimization problem."}, {"heading": "4.2 DCOP: Representation and Coordination", "text": "Representation in DCOPs plays a fundamental role, both from an agent coordination perspective and from an algorithmic perspective. We discuss here the most predominant representations adopted in various DCOP algorithms. Let us start by describing some widely adopted assumptions regarding agent knowledge and coordination, which will apply throughout this document, unless otherwise stated:\ni. A variable and its domain are known exclusively to the agent controlling it and its neighboring agents.\nii. Each agent knows the cost values of the constraints involving at least one of its local variables. No other agent has knowledge about such constraints.\niii. Each agent knows exclusively (and it may communicate with) its own neighboring agents."}, {"heading": "4.2.1 CONSTRAINT GRAPH", "text": "Given a DCOP P , GP = (X, EC) is the constraint graph of P , where an undirected edge {x, y} \u2208 EC exists if and only if there exists fj \u2208 F such that {x, y} \u2286 xj . A constraint graph is a standard way to visualize a DCOP instance. It underlines the agents\u2019 locality of interactions and therefore it is commonly adopted by DCOP resolution algorithms.\nGiven an ordering o on X, we say that a variable xi has a higher priority with respect to a variable xj if xi appears before xj in o. Given a constraint graph GP and an ordering o on its nodes, the induced graph G\u2217P on o is the graph obtained by connecting nodes, processed in increasing order of priority, to all their higher-priority neighbors. For a given node, the number of higher-priority neighbors is referred to as its width. The induced width w\u2217o of GP is the maximum width over all the nodes of G\u2217P on ordering o.\nFigure 4(a) shows an example constraint graph of a DCOP with four agents a1 through a4, each controlling one variable with domain {0,1}. There are two constraint: a k-ary constraint f123 with scope x123 = {x1, x2, x3} and represented by a clique among x1, x2, and x3; and a binary constraint f24 with scope x24 = {x2, x4}."}, {"heading": "4.2.2 PSEUDO-TREE", "text": "A number of DCOP algorithms require a partial ordering among the agents. In particular, when such an order is derived from a depth-first search (DFS) exploration, the resulting structure is known as a (DFS) pseudo-tree. A pseudo-tree arrangement for a DCOP P is a subgraph TP = \u3008X, ET \u3009 ofGP such that TP is a spanning tree ofGP \u2013 i.e., a connected subgraph ofGP containing all the nodes and being a rooted tree \u2013 with the following additional condition: for each x, y \u2208 X, if {x, y} \u2286 xi for some fi \u2208 F, then x, y appear in the same branch of TP (i.e., x is an ancestor of y in TP or vice versa). Edges of GP that are in (respectively out of) ET are called tree edges (respectively backedges). The tree edges connect parent-child nodes, while backedges connect a node with its pseudo-parents and its pseudo-children. We use Cai , PCai , Pai , and PPai to denote the set of children, pseudo-children, parent, and pseudo-parents of the agent ai.\nBoth constraint graph and pseudo-tree representations cannot deal explicitly with k-ary constraints (with k > 2). A typical artifact to deal with such constraints in a pseudo-tree representation is to introduce a virtual variable that monitors the value assignments for all the variables in the scope of the constraint, and generates the cost values (Bowring, Tambe, & Yokoo, 2006) \u2013 the role of the virtual variables can be delegated to one of the variables participating in the constraint (Pecora, Modi, & Scerri, 2006; Matsui, Matsuo, Silaghi, Hirayama, & Yokoo, 2008).\nFigure 4(b) shows one possible pseudo-tree of the example DCOP in Figure 4(a), where Ca1 = {x2}, PCa1 ={x3}, Pa4 ={x2}, and PPa3 ={x1}. The solid lines are tree edges and dotted lines are backedges."}, {"heading": "4.2.3 FACTOR GRAPH", "text": "Another way to represent DCOPs is through a factor graph (Kschischang, Frey, & Loeliger, 2001). A factor graph is a bipartite graph used to represent the factorization of a function. In particular, given the global objective function Fg, the corresponding factor graph FP = \u3008X,F, EF \u3009 is composed of variable nodes xi \u2208 X, factor nodes fj \u2208 F, and edgesEF such that there is an undirected edge between factor node fj and variable node xi if xi \u2208 xj .\nFactor graphs can handle k-ary constraints explicitly. To do so, they use a similar method as that adopted within pseudo-trees with such constraints: They delegate the control of a factor node to one of the agents controlling a variable in the scope of the constraint. From an algorithmic perspective, the algorithms designed over factor graphs can directly handle k-ary constraints, while algorithms designed over pseudo-trees require changes in the algorithm design so to delegate the control of the k-ary constraints to some particular entity.\nFigure 4(c) shows the factor graph of the example DCOP in Figure 4(a), where each agent ai controls its variable xi and, in addition, a3 controls the constraint f123 and a4 controls constraint f24."}, {"heading": "4.3 Algorithms", "text": "The field of classical DCOPs is mature and a number of different resolution algorithms have been proposed. DCOP algorithms can be classified as being either complete or incomplete, based on whether they can guarantee the optimal solution or they trade optimality for shorter execution times, producing near-optimal solutions. They can also be characterized based on their runtime characteristics (e.g., their runtime complexity and whether they are anytime algorithms), their memory requirements, and their communication requirements (e.g., the number and size of messages that they send and whether they communicate through peer-to-peer channels or by broadcasting to all agents). Table 4 tabulates the properties of a number of key DCOP algorithms that we will describe in more detail in Sections 4.3.4 and 4.3.5. As all these algorithms were originally developed under the assumption that each agent controls exactly one variable, we will describe the properties under this same assumption as well. These properties may change when generalizing the algorithms to allow for agents to control multiple variables, but they will depend on how the algorithms are generalized. Throughout this document, we will often adopt the following notation when discussing the complexity of the algorithms: \u2022 n = |X| refers to the number of variables in the problem; in Table 4, n also refers to the number\nof agents in the problem since each agent has exactly one variable; \u2022 d = maxDi\u2208D |Di| refers to the size of the largest domain; \u2022 w\u2217 refers to the induced width of the pseudo-tree; \u2022 l = maxai\u2208A |Nai | refers to the largest number of neighboring agents; and \u2022 ` refers to the number of iterations in incomplete algorithms.\nIn addition, each of these classes can be categorized into several groups, depending on the degree of locality exploited by the algorithms, the way local information is updated, and the type of exploration process adopted. We describe these different categories next."}, {"heading": "4.3.1 PARTIAL CENTRALIZATION", "text": "In general, the DCOP solving process is decentralized, driving DCOP algorithms to follow the agent knowledge and communication restrictions described in Section 4.2. However, some algorithms explore methods to centralize the decisions to be taken by a group of agents, by delegating them to one of the agents in the group. Such approaches explore the concept of partial centralization (Hirayama & Yokoo, 1997; Mailler & Lesser, 2004; Petcu, Faltings, & Mailler, 2007), and thus they are classified as partially centralized algorithms. Typically, partial centralization improves the algorithms\u2019 performance allowing agents to coordinate their local assignments more efficiently. However, such performance enhancement comes with a loss of information privacy, as the centralizing agent needs to be granted access to the local subproblem of other agents in the group (Greenstadt, Grosz, & Smith, 2007; Mailler & Lesser, 2004). In contrast, fully decentralized algorithms inherently reduce the amount of information privacy at cost of a larger communication effort."}, {"heading": "4.3.2 SYNCHRONICITY", "text": "DCOP algorithms can enhance their effectiveness by exploiting distributed and parallel processing. Based on the way the agents update their local information, DCOP algorithms are classified as synchronous or asynchronous. Asynchronous algorithms allow agents to update the assignment for their variables based solely on their local view of the problem, and thus independently from the actual decisions of the other agents (Modi et al., 2005; Farinelli, Rogers, Petcu, & Jennings, 2008; Gershman et al., 2009). In contrast, synchronous algorithms constrain the agents decisions to follow a particular order, typically enforced by the representation structure adopted (Mailler & Lesser, 2004; Petcu & Faltings, 2005b; Pearce & Tambe, 2007).\nSynchronous methods tend to delay the actions of some agents guaranteeing that their local view of the problem is always consistent with that of the other agents. In contrast, asynchronous methods tend to minimize the idle-time of the agents, which in turn can react quickly to each message being processed; however, they provide no guarantee on the consistency of the state of the local view of each agent. Such effect has been studied by Peri and Meisels (2013), concluding that inconsistent agents\u2019 views may cause a negative impact on network load and algorithm performance, and that introducing some level of synchronization may be beneficial for some algorithms, enhancing their performance."}, {"heading": "4.3.3 EXPLORATION PROCESS", "text": "The resolution process adopted by each algorithm can be classified in three categories (Yeoh, 2010): \u2022 Search-based methods are based on the use of search techniques to explore the space of possible\nsolutions. These methods are often derived from corresponding search techniques developed for centralized AI search problems, such as best-first search and depth-first search.\n\u2022 Inference-based methods are derived from dynamic programming and belief propagation techniques. These methods allow agents to exploit the structure of the constraint graph to aggregate costs from their neighbors, effectively reducing the problem size at each step of the algorithm. \u2022 Sampling-based methods are incomplete approaches that sample the search space to approximate\na function (typically, a probability distribution) as a product of statistical inference. Figure 5 illustrates a taxonomy of classical DCOP algorithms. In the following subsections, we briefly describe some representative complete and incomplete algorithms from each of the classes introduced above. A detailed description of the DCOP algorithms is beyond the scope of this manuscript. We refer the interested readers to the original articles that introduce each algorithm."}, {"heading": "4.3.4 COMPLETE ALGORITHMS", "text": "Some of the algorithms described below were originally designed to solve the variant of DCOPs that maximizes rewards, while others solve the variant that minimizes costs. However, the algorithms that maximize rewards can be easily adapted to minimize costs. For consistency, we describe\nthe version of the algorithms that focus on minimization of costs. We also describe their quality, runtime, memory, and communication characteristics as summarized in Table 4.\nSyncBB (Hirayama & Yokoo, 1997). Synchronous Branch-and-Bound (SyncBB) is a complete, synchronous, search-based algorithm that can be considered as a distributed version of a branchand-bound algorithm. It uses a complete ordering of the agents in order to extend a Current Partial Assignment (CPA) via a synchronous communication process. The CPA holds the assignments of all the variables controlled by all the visited agents, and, in addition, functions as a mechanism to propagate bound information. The algorithm prunes those parts of the search space whose solution quality is sub-optimal, by exploiting the bounds that are updated at each step of the algorithm.\nIn the worst case, SyncBB agents performO(dn) number of operations since the lowest priority agent needs to enumerate through all possible value combinations for all variables. While, by default, it is not an anytime algorithm, it can be easily extended to have an anytime property since it is a branch-and-bound algorithm. The memory requirement per SyncBB agent is O(n) since the lowest priority agent will need to store the value assignment of all variables. In terms of communication requirement, SyncBB agents send O(dn) number of messages; specifically, the lowest priority agent will enumerate through all possible value combination for all variables and sends a message for each combination. The largest message, which contains the value assignment of all variables, is of size O(n). Finally, SyncBB agents only communicate through peer-to-peer connections (i.e., only to their neighboring agents) and do not broadcast.\nAFB (Gershman et al., 2009). Asynchronous Forward Bounding (AFB) is a complete, asynchronous, search-based algorithm that can be considered as an asynchronous version of SyncBB. In this algorithm, agents communicate their cost estimates, which in turn are used to compute bounds and prune the search space. In AFB, agents extend a CPA sequentially, provided that the lower bound on their costs does not exceed the global upper bound, that is, the cost of the best solution found so far. Each agent performing an assignment (the \u201cassigning\u201d agent) triggers asynchronous checks of bounds, by sending forward messages containing copies of the CPA to agents that have not yet assigned their variables. The unassigned agents that receive a CPA estimate the lower bound of the CPA given their local view of the constraint graph and send their estimates back to the agent that originated the forward message. This assigning agent will receive these estimates asynchronously and aggregate them into an updated lower bound. If the updated lower bound exceeds the current upper bound, the agent initiates a backtracking phase.\nThe runtime, memory, and communication characteristics of AFB are identical to those of SyncBB for the same reasons. The only exception is that AFB agents broadcast some of their messages while SyncBB agents do not.\nADOPT (Modi et al., 2005). Asynchronous Distributed OPTimization (ADOPT) is a complete, asynchronous, search-based algorithm that can be considered as a distributed version of a memorybounded best-first search algorithm. It makes use of a DFS pseudo-tree ordering of the agents. The algorithm relies on maintaining, in each agent, lower and upper bounds on the solution cost for the subtree rooted at its node in the DFS tree. Agents explore partial assignments in best-first order, that is, in increasing lower bound order. They use COST messages (propagated upwards in the\nDFS pseudo-tree) and THRESHOLD and VALUE messages (propagated downwards in the tree) to iteratively tighten the lower and upper bounds, until the lower bound of the minimum cost solution is equal to its upper bound. ADOPT agents store lower bounds as thresholds, which can be used to prune partial assignments that are provably sub-optimal.\nSimilar to SyncBB and AFB, ADOPT agents perform O(dn) number of operations since the lowest priority agent needs to enumerate through all possible value combinations for all variables when the pseudo-tree degenerates into a pseudo-chain. It is also not an anytime algorithm as it is a best-first search algorithm. The memory requirement per ADOPT agent is O(n+ ld), where O(n) is used to store a context, which is the value assignment of all higher-priority variables, and O(ld) is used to store the lower and upper bounds for each domain value and variable belonging to the agent\u2019s child agents. Finally, its communication characteristics are also identical to that of SyncBB and for the same reasons as in SyncBB.\nADOPT has been extended in several ways. In particular, BnB-ADOPT (Yeoh, Felner, & Koenig, 2010; Gutierrez & Meseguer, 2012b) uses a branch-and-bound method to reduce the amount of computation performed during search, and ADOPT(k) combines both ADOPT and BnBADOPT into an integrated algorithm (Gutierrez, Meseguer, & Yeoh, 2011). There are also extensions that trade solution optimality for smaller runtimes (Yeoh, Sun, & Koenig, 2009a), extensions that use more memory for smaller runtimes (Yeoh, Varakantham, & Koenig, 2009b), and extensions that maintain soft arc-consistency (Bessiere, Gutierrez, & Meseguer, 2012; Bessiere, Brito, Gutierrez, & Meseguer, 2014; Gutierrez & Meseguer, 2012a; Gutierrez, Lee, Lei, Mak, & Meseguer, 2013).\nFinally, the No-Commitment Branch and Bound (NCBB) algorithm, proposed by Chechetka and Sycara (2006), can be considered as a variant of ADOPT and SyncBB. Similar to ADOPT, NCBB agents exploit the structure defined by a pseudo-tree order to decompose the global cost function. This allow the agents to search non-intersecting parts of the search space concurrently. Another main feature of NCBB is the eager propagation of lower bounds on solution cost: An NCBB agent propagates its lower bound every time it learns about its ancestors\u2019 assignments. This feature provides an efficient pruning of the search space (Chechetka & Sycara, 2006). The runtime, memory, and communication characteristics of NCBB are the same as those of ADOPT except that NCBB is an anytime algorithm.\nConcFB (Netzer, Grubshtein, & Meisels, 2012). Concurrent Forward Bounding (ConcFB) is a complete, asynchronous, search-based algorithm that essentially runs multiple parallel versions of AFB concurrently. By running multiple concurrent search procedures, it is able to quickly find a solution, apply a forward bounding process to detect regions of the search space to prune, and to dynamically create new search processes when detecting promising sub-spaces. Similar to AFB, it uses a complete ordering of agents and variables instead of pseudo-trees. As such, it is able to simplify the management of reordering heuristics, which can provide substantial speed up to the search process (Zivan & Meisels, 2006).\nThe algorithm operates as follows: Each agent maintains a global upper bound, which is updated during the search process. The highest-priority agent begins the process by generating a\nnumber of different search processes (SP), one for each value of its variable. It then sends an LB Request message to all unassigned agents. This LB Request message contains the current CPA and triggers a calculation of the lower bounds of the receiving agents, which are sent back to the sender agent via a LB Report message. If the sum of the aggregated costs and the current CPA cost is no smaller than the current upper bound, the agent selects another value for its variable and repeats the process. If the agent has exhausted all value assignments for its variable, then it backtracks, sending the CPA to the last assigning agent. If the CPA cost is lower than the current upper bound, then it forwards the CPA message to the next non-assigned agent. Upon receiving a CPA message, the agent repeats the above process. When the lowest-priority agent finds a solution and finds a new upper bound, it broadcasts the upper bound via a UB message, which is then stored by each each agent.\nNetzer et al. (2012) described a series of enhancements that can be used to speed up the search process of ConcFB, including dynamic variable ordering and dynamic splitting. Note that despite the process within a subproblem is carried out in a synchronous fashion, different subproblems are explored independently. Thus, the agents act asynchronously and concurrently. The runtime, memory, and communication characteristics of ConcFB are identical to those of AFB since it is essentially an algorithm that runs multiple parallel versions of AFB concurrently.\nDPOP (Petcu & Faltings, 2005b). Distributed Pseudo-tree Optimization Procedure (DPOP) is a complete, synchronous, inference-based algorithm that makes use of a DFS pseudo-tree ordering of the agents. It involves three phases. In the first phase, the agents order themselves into a DFS pseudo-tree. In the second phase, called the UTIL propagation phase, each agent, starting from the leaves of the pseudo-tree, aggregates the costs in its subtree for each value combination of variables in its separator.4 The aggregated costs are encoded in a UTIL message, which is propagated from children to their parents, up to the root. In the third phase, called the VALUE propagation phase, each agent, starting from the root of the pseudo-tree, selects the optimal value for its variable. The optimal values are calculated based on the UTIL messages received from the agent\u2019s children and the VALUE message received from its parent. The VALUE messages contain the optimal values of the agents and are propagated from parents to their children, down to the leaves of the pseudo-tree.\nIn the worst case, DPOP agents perform O(dw \u2217 ) number of operations. When an agent optimizes for each value combination of variables in its separator, it takes O(dw \u2217 ) operations since there are w\u2217 variables in the separator set in the worst case. It is not an anytime algorithm as it terminates upon finding its first solution, which is an optimal solution. The memory requirement per DPOP agent is O(dw \u2217 ) since it needs to store all value combinations of variables in its separator. In terms of communication requirement, DPOP agents send O(n) messages in total; O(n) UTIL messages are propagated up the pseudo-tree and O(n) VALUE messages are propagated down the pseudo-tree. The largest message sent by an agent, which contains the aggregated costs in its subtree for each value combination of variables in its separator, is O(dw \u2217 ). Finally, DPOP agents only\n4. The separator of ai contains all ancestors of ai in the pseudo-tree (through tree edges or backedges) that are connected to ai or one of its descendants.\ncommunicate through peer-to-peer connections (i.e., only to their neighboring agents) and do not broadcast.\nDPOP has also been extended in several ways to enhance its performance and capabilities. ODPOP and MB-DPOP trade runtimes for smaller memory requirements (Petcu & Faltings, 2006, 2007a), A-DPOP trades solution optimality for smaller runtimes (Petcu & Faltings, 2005a), SSDPOP trades runtime for increased privacy (Greenstadt et al., 2007), PC-DPOP trades privacy for smaller runtimes (Petcu et al., 2007), H-DPOP propagates hard constraints for smaller runtimes (Kumar, Petcu, & Faltings, 2008), BrC-DPOP enforces branch consistency for smaller runtimes (Fioretto, Le, Yeoh, Pontelli, & Son, 2014), and ASP-DPOP is a declarative version of DPOP that uses Answer Set Programming (Le, Son, Pontelli, & Yeoh, 2015).\nOptAPO (Mailler & Lesser, 2004). Optimal Asynchronous Partial Overlay (OptAPO) is a complete, asynchronous, search-based algorithm. It trades agent privacy for smaller runtimes through partial centralization. It employs a cooperative mediation schema, where agents can act as mediators and propose value assignments to other agents. In particular, agents check if there is a conflict with some neighboring agent. If a conflict is found, the agent with the highest priority acts as a mediator. During mediation, OptAPO solves subproblems using a centralized branch-and-boundbased search, and when solutions of overlapping subproblems still have conflicting assignments, the solving agents increase the degree of centralization to resolve them. By sharing their knowledge with centralized entities, agents can improve their local decisions, reducing the communication costs. For instance, the algorithm has been shown to be superior to ADOPT on simple combinatorial problems (Mailler & Lesser, 2004). However, it is possible that several mediators solve overlapping problems, duplicating efforts (Petcu et al., 2007), which can be a bottleneck especially for dense problems.\nIn the worst case, OptAPO agents perform O(dn) number of operations as a mediator agent may solve the entire problem. Like ADOPT and DPOP, OptAPO is not an anytime algorithm. The memory requirement per OptAPO agent is O(nd) since it needs to store all value combinations of variables in its mediation group, which is of size O(n). In terms of communication requirement, OptAPO agents send O(dn) messages in the worst case, though the number of messages decreases with increasing partial centralization. The size of the messages is bounded by O(d+ n), where in the initialization phase of each mediation step, each agent sends its domain to its neighbors and the list of variables that it seeks to mediate. Finally, the agents only communicate through peer-to-peer connections (i.e., only to their neighboring agents) and do not broadcast.\nThe original version of OptAPO has been shown to be incomplete due to the asynchronicity of the different mediators\u2019 groups, which can lead to race conditions. Grinshpoun and Meisels (2008) proposed a complete variant that remedies this issue."}, {"heading": "4.3.5 INCOMPLETE ALGORITHMS", "text": "As some incomplete algorithms were also originally designed to maximize rewards, similar to Section 4.3.5, we also describe variants of these algorithms that minimize costs here to maintain consistency.\nMax-Sum (Farinelli et al., 2008). Max-Sum is an incomplete, synchronous, inference-based algorithm based on belief propagation. It operates on factor graphs by performing a marginalization process of the cost functions, and optimizing the costs for each given variable. This process is performed by recursively propagating messages between variable nodes and functions nodes. The value assignments take into account their impact on the marginalized cost function. Max-Sum is guaranteed to converge to an optimal solution in acyclic graphs, but convergence is not guaranteed on cyclic graphs.\nIn the worst case, Max-Sum agents perform O(dl) number of operations in each iteration, where each agent needs to optimize for all value combinations of neighboring variables. It is not an anytime algorithm. The memory requirement per Max-Sum agent isO(dl) since it needs to store all value combinations of neighboring variables. In terms of communication requirement, in the worst case, each Max-Sum agent sendsO(l) messages in each iteration, one to each of its neighbor. Thus, the total number of messages sent across all agents is O(`nl). Each message is of size O(d) as it needs to contain the current aggregated costs of all the agent\u2019s variable\u2019s values. Finally, the agents only communicate through peer-to-peer connections (i.e., only to their neighboring agents) and do not broadcast.\nMax-Sum has also been extended in several ways to improve it. Bounded Max-Sum is able to bound the quality of the solutions found by removing a subset of edges from a cyclic DCOP graph to make it acyclic, and by running Max-Sum to solve the acyclic problem (Rogers, Farinelli, Stranders, & Jennings, 2011), Improved Bounded Max-Sum improves on the error bounds (Rollon & Larrosa, 2012), and Max-Sum ADVP guarantees convergence in acyclic graphs through a twophase value propagation phase (Zivan & Peled, 2012). Max-Sum and its extensions have been successfully employed to solve a number of large scale, complex MAS applications (see Section 8).\nRegion Optimal (Pearce & Tambe, 2007). Region-optimal algorithms are incomplete, synchronous, search-based algorithms that allow users to specify regions of the constraint graph and solve the subproblem within each region optimally. Regions may be defined to have a maximum size of k agents (Pearce & Tambe, 2007), t hops from each agent (Kiekintveld, Yin, Kumar, & Tambe, 2010), or a combination of both size and hops (Vinyals, Shieh, Cerquides, RodriguezAguilar, Yin, Tambe, & Bowring, 2011). The concept of k-optimality is defined with respect to the number of agents whose assignments conflict, whose set is denoted by c(\u03c3, \u03c3\u2032), for two assignments \u03c3 and \u03c3\u2032. The deviating cost of \u03c3 with respect to \u03c3\u2032, denoted by \u2206(\u03c3, \u03c3\u2032), is defined as the difference of the aggregated cost associated to the assignment \u03c3 (F (\u03c3)) minus the cost associated to \u03c3\u2032 (F (\u03c3\u2032)). An assignment \u03c3 is k-optimal if \u2200\u03c3\u2032 \u2208 \u03a3, such that |c(\u03c3, \u03c3\u2032)| \u2264 k, we have that \u2206(\u03c3, \u03c3\u2032) \u2265 0. In contrast, the concept of t-distance emphasizes the number of hops from a central agent a of the region \u2126t(a), that is the set of agents which are separated from a by at most t hops. An assignment \u03c3 is t-distance optimal if, \u2200\u03c3\u2032 \u2208 \u03a3, F (\u03c3) \u2265 F (\u03c3\u2032) with c(\u03c3, \u03c3\u2032) \u2286 \u2126t(a), for any a \u2208 A. Therefore, the solutions found have theoretical error bounds that are a function of k and/or t.\nIn the worst case, region-optimal agents perform O(dw \u2217 ) number of operations in each iteration, as each agent runs DPOP to solve the problem within each region optimally. It is also an\nanytime algorithm as solutions of improving quality are found until they are region-optimal. The memory requirement per region-optimal agent is O(dw \u2217 ) since its region may have an induced width of w\u2217 and it uses DPOP to solve the problem within its region. In terms of communication requirement, in the worst case, each region-optimal agent sends O(n) messages, one to each agent within its region. Thus, the total number of messages sent across all agents is O(`n2). Each message is of size O(dw \u2217 ) as it uses DPOP. Finally, the agents do broadcast, but only to all agents within its region \u2013 either to a distance of bk2c or t hops away. An asynchronous version of regional-optimal algorithms, called Distributed Asynchronous Local Optimization (DALO), was proposed by Kiekintveld et al. (2010). The DALO simulator provides a mechanism to coordinate the decision of local groups of agents based on the concepts of k-optimality and t-distance.\nMGM (Maheswaran, Pearce, & Tambe, 2004a). Maximum Gain Message (MGM) is an incomplete, synchronous, search-based algorithm that performs a distributed local search. Each agent starts by assigning a random value to each of its variables. Then, it sends this information to all its neighbors. Upon receiving the values of its neighbors, it calculates the maximum gain (i.e., the maximum decrease in cost) if it changes its value and sends this information to all its neighbors as well. Upon receiving the gains of its neighbors, it changes its value if its gain is the largest among its neighbors. This process repeats until a termination condition is met. MGM provides no quality guarantees on the returned solution.\nIn the worst case, MGM agents perform O(ld) number of operations in each iteration, as each agent needs to compute the cost for each of its values by taking into account the values of all its neighbors. It is also an anytime algorithm since agents only change their values when they have a non-negative gain. The memory requirement per MGM agent is O(l) since it needs to store the values of all its neighboring agents. In terms of communication requirement, in the worst case, each MGM agent sends O(l) messages, one to each of its neighboring agents. Thus, the total number of messages sent across all agents is O(`nl). Each message is of constant size O(1) as it contains either the agent\u2019s current value or the agent\u2019s current gain. Finally, the agents only communicate through peer-to-peer connections (i.e., only to their neighboring agents) and do not broadcast.\nDSA (Zhang, Wang, Xing, & Wittenberg, 2005). Distributed Stochastic Algorithm (DSA) is an incomplete, synchronous, search-based algorithm that is similar to MGM, except that each agent does not send its gains to its neighbors and it does not change its value to the value with the maximum gain. Instead, it decides stochastically if it takes on the value with the maximum gain or other values with smaller gains. This stochasticity allows DSA to escape from local minima. Similar to MGM, it repeats the process until a termination condition is met, and it cannot provide quality guarantees on the returned solution. The runtime, memory, and communication characteristics of DSA are identical to those of MGM since it is essentially a stochastic variant of MGM.\nDUCT (Ottens, Dimitrakakis, & Faltings, 2012). The Distributed Upper Confidence Tree (DUCT) algorithm is an incomplete, synchronous, sampling-based algorithm that is inspired by MonteCarlo Tree Search and employs confidence bounds to solve DCOPs. DUCT emulates a search process analogous to that of ADOPT, where agents select the values to assign to their variables\naccording to the information encoded in their context messages (i.e., the assignments to all the variables in the receiving variable\u2019s separator). However, rather than systematically selecting the next value to assign to their own variables, DUCT agents sample such values. To focus on promising assignments, DUCT constructs a confidence bound B, such that cost associated to the best value for any context is at least B, and hence agents sample the choice with the lowest bound. This process is started by the root agent of the pseudo-tree: After sampling a value for its variable, it communicates its assignment to its children in a context message. When an agent receives this message, it repeats this process until the leaf agents are reached. When the leaf agents choose a value assignment, they calculate the cost within their context and propagate this information up to the tree in a cost message. This process continues for a given number of iterations or until convergence is achieved, i.e., until the sampled values in two successive iterations do not change. Therefore, DUCT is able to provide quality guarantees on the returned solution.\nIn the worst case, DUCT agents perform O(ld) number of operations in each iteration, as each agent needs to compute the cost for each of its values by taking into account the values of all its neighbors. It is also an anytime algorithm since the quality guarantee improves with increasing number of iterations. The memory requirement per DUCT agent is O(dn)5 in the worst case since it needs to store the best cost for all possible contexts. In terms of communication requirement, in each iteration, each DUCT agent sends one message to its parent in the pseudo-tree and one message to each of its children in the pseudo-tree. Thus, the total number of messages sent across all agents is O(`n). Each message is of size O(n) in the worst case; context messages contain the value assignment for all higher priority agents. Finally, the agents only communicate through peer-to-peer connections (i.e., only to their neighboring agents) and do not broadcast.\nD-Gibbs (Nguyen, Yeoh, & Lau, 2013). The Distributed Gibbs (D-Gibbs) algorithm is an incomplete, synchronous, sampling-based algorithm that extends the Gibbs sampling process (Geman & Geman, 1984) by tailoring it to solve DCOPs in a decentralized manner. The Gibbs sampling process is a centralized Markov Chain Monte-Carlo algorithm that can be used to approximate joint probability distributions. By mapping DCOPs to maximum a-posteriori estimation problems, probabilistic inference algorithms like Gibbs sampling can be used to solve DCOPs.\nLike DUCT, it too operates on a pseudo-tree, and the agents sample sequentially from the root of the pseudo-tree down to the leaves. Like DUCT, each agent also stores a context (i.e., the current assignment to all the variables in its separator) and it samples based on this information. Specifically, it computes the probability for each of its values given its context and chooses its current value based on this probability distribution. After it chooses its value, it informs its lower priority neighbors of its value, and its children agents start to sample. This process continues until all the leaf agents sample. Cost information is propagated up the pseudo-tree. This process continues for a fixed number of iterations or until convergence. Like DUCT, D-Gibbs is also able to provide quality guarantees on the returned solution.\n5. It is actually O(dt), where t is the depth of the pseudo-tree. However, in the worst case when the pseudo-tree degenerates into a pseudo-chain, then t = n.\nThe runtime characteristics of D-Gibbs are identical to that of DUCT and for the same reasons. However, its memory requirements are smaller: The memory requirement per D-Gibbs agent is O(l) since it needs to store the current values of all its neighbors. In terms of communication requirement, in each iteration, each D-Gibbs agent sends O(l) messages, one to each of its neighbor. Thus, the total number of messages sent across all agents is O(`nl). Each message is of constant size O(1) since they contain only the current value of the agent or partial cost of its solution. Finally, the agents only communicate through peer-to-peer connections (i.e., only to their neighboring agents) and do not broadcast.\nA version of the algorithm that speeds up the agents\u2019 sampling process with Graphical Processing Units (GPUs) is described in (Fioretto, Yeoh, & Pontelli, 2016a)."}, {"heading": "4.4 Tradeoffs Between the Various DCOP Algorithms", "text": "The various DCOP algorithms discussed above provide a good coverage across many different characteristics that may be important in different applications. As such, how well suited an algorithm is for an application depends on how well the algorithm\u2019s characteristics match up to the application\u2019s characteristics. We now discuss below several suggestions for the types of algorithms that we recommend based on the characteristics of the application at hand."}, {"heading": "4.4.1 COMPLETE ALGORITHMS", "text": "In general, we make the following recommendations when one is limited to complete algorithms only (e.g., when optimality is a requirement of the application): \u2022 If the agents in the application have large amounts of memory and it is faster to send few large\nmessages than many small messages, then DPOP is preferred over the search-based algorithms (e.g., SyncBB, AFB, ADOPT, ConcFB, OptAPO). The reason is that many of the search algorithms\u2019 performance typically perform some amount of redundant computation and communication. Thus, the overall runtime when using DPOP is usually smaller than when using the search algorithms. \u2022 If the agents in the application have limited amounts of memory, then one has to use the search-\nbased algorithms (e.g., SyncBB, AFB, ADOPT, ConcFB, OptAPO), which have small memory requirements. The exception is when the problem has a small induced width (e.g., the constraint graph is acyclic), in which case DPOP is also preferred. \u2022 If partial centralization is allowed by the application, then OptAPO is preferred as it has been\nshown to outperform many of the other search algorithms (Mailler & Lesser, 2004). \u2022 Otherwise, ConcFB is recommended as it has been shown to outperform AFB due to the con-\ncurrent search (Netzer et al., 2012), and AFB has been shown to outperform ADOPT and SyncBB (Gershman et al., 2009). The exception is if the application does not permit agents to broadcast, in which case both ConcFB and AFB cannot be used and one is restricted to use either ADOPT or SyncBB. While one can use either algorithm, many of the ADOPT variants (e.g., BnB-ADOPT, NCBB) have been shown to significantly outperform both algorithms\nwhile maintaining the same runtime, memory, and communication requirements (Chechetka & Sycara, 2006; Yeoh et al., 2010)."}, {"heading": "4.4.2 INCOMPLETE ALGORITHMS", "text": "In terms of incomplete algorithms, we make the following recommendations: \u2022 If the solution returned must have an accompanying quality guarantee, then, one can choose to\nuse Bounded Max-Sum, region-optimal algorithms, DUCT, or D-Gibbs. Bounded Max-Sum allows users to choose the error bound as a function of the different subsets of edges that can be removed from the graph to make it acyclic. Region-optimal algorithms allow users to parameterize the error bound according to the size of the region k or the number of hops t that the solution should be optimal for. Finally, DUCT and D-Gibbs allow users to parameterize the error bound based on the number of sampling iterations to conduct. The error bounds for these two algorithms are also probabilistic bounds (i.e., the likelihood that the quality of the solution is within an error bound is a function of the number of iterations). Therefore, the choice of algorithm will depend on the type of error bound one would like to impose on the solutions. One may also choose to use a number of extensions of complete algorithms (e.g., Weighted (BnB-)ADOPT and A-DPOP) that allow users to parameterize the error bound, which will also affect the degree of speedup. \u2022 If the solution returned need not have quality guarantees, then one can also use Max-Sum, MGM,\nor DSA. Between these three algorithms, their performance depends on a number of factors: If the problem has large domain sizes, MGM and DSA is likely to do better since the memory and computational complexities of Max-Sum grows exponentially with the domain size. However, if the problem has small induced widths (e.g., the constraint graph is acyclic), then Max-Sum is very efficient. It is even guaranteed to find optimal solutions when the induced width is 1. In general, Max-Sum tends to find better solutions especially when considering the improvements proposed in recent literature on incomplete classical DCOP algorithms (Zivan, Parash, Cohen, Peled, & Okamoto, 2017). \u2022 If the problem has hard constraints (i.e., certain value combinations are prohibited), then the\nsampling algorithms (i.e., DUCT and D-Gibbs) cannot be used as they are not able to handle such problems. They require the cost functions to be smooth, and exploit that characteristic to explore the whole search space. Thus, one is restricted to the other incomplete algorithms. \u2022 In general, MGM and DSA are good robust benchmarks as they tend to find reasonable solutions\nin practice. However, if specific problem characteristics are known, such as the ones discussed above, then certain algorithms may be able to exploit them to find even better solutions."}, {"heading": "4.5 Notable Variant: Asymmetric DCOPs", "text": "Asymmetric DCOPs (Grinshpoun, Grubshtein, Zivan, Netzer, & Meisels, 2013) are used to model multi-agent problems where agents controlling variables in the scope of a cost function can incur to different costs, given a fixed join assignment. Such a problem cannot be naturally represented by\nclassical DCOPs, which require that all agents controlling variables participating in a cost function incur to the same cost as each other."}, {"heading": "4.5.1 DEFINITION", "text": "An Asymmetric DCOP is defined by a tuple \u3008A,X,D,F, \u03b1\u3009, where A,X,D, and \u03b1 are as defined in Definition 4.1, and each cost function fi \u2208 F is defined as: fi : \"xj\u2208xi Dj \u00d7 \u03b1(fi) \u2192 (R+ \u222a {\u22a5}). In other words, an Asymmetric DCOP is a DCOP where the cost that an agent incurs from a cost function may differ from the cost that another agent incurs from the same cost function.\nAs costs for participating agents may differ from each other, the goal in Asymmetric DCOPs is also different from the goal in classical DCOPs. Given a cost function fj \u2208 F and complete assignment \u03c3, let fj(\u03c3, ai) denote the cost incurred by agent ai from cost function fj with complete assignment \u03c3. Then, the goal in Asymmetric DCOPs is to find the solution \u03c3\u2217:\n\u03c3\u2217 := argmin \u03c3\u2208\u03a3 \u2211 fj\u2208F \u2211 ai\u2208\u03b1(fj) fj(\u03c3xj , ai) (2)\nAs in classical DCOPs, solving Asymmetric DCOPs is NP-hard. In particular, it is possible to reduce any Asymmetric DCOP to an equivalent classical DCOP by introducing a polynomial number of variables and constraints, as described in the next section."}, {"heading": "4.5.2 RELATION TO CLASSICAL DCOPS", "text": "One way to solve MAS problems with asymmetric costs via classical DCOPs is through the Private Event As Variables (PEAV) model (Maheswaran et al., 2004a). It can capture asymmetric costs by introducing, for each agent, as many \u201cmirror\u201d variables as the number of variables held by neighboring agents. The consistency with the neighbors\u2019 state variables is imposed by a set of equality constraints. However, this formalism suffers from scalability problems, as it may result in a significant increase in the number of variables in a DCOP. In addition, Grinshpoun et al. (2013) showed that most of the existing incomplete classical DCOP algorithms cannot be used to effectively solve Asymmetric DCOPs, even when the problems are reformulated through the PEAV model. They show that such algorithms are unable to distinguish between different solutions that satisfy all hard constraints, resulting in a convergence to one of those solutions and the inability to escape that local optimum. Therefore, it is important to design specialized algorithms to solve Asymmetric DCOPs."}, {"heading": "4.5.3 ALGORITHMS", "text": "The current research direction in the design of Asymmetric DCOP algorithms has focused on adapting existing classical DCOP algorithms to handle the asymmetric costs. Asymmetric DCOPs require that each agent, whose variables participate in a cost function, coordinate the aggregation of their individual costs. To do so, two approaches have been identified (Brito, Meisels, Meseguer, & Zivan, 2009):\n\u2022 A two-phase strategy, where only one side of the constraint (i.e., the cost induced by one agent) is considered in the first phase. The other side(s) (i.e., the cost induced by the other agent(s)) is considered in the second phase once a complete assignment is produced. As a result, the costs of all agents are aggregated. \u2022 A single-phase strategy, which requires a systematic check of each side of the constraint before\nreaching a complete assignment. Checking each side of the constraint is often referred to as back checking, a process that can be performed either synchronously or asynchronously.\nCOMPLETE ALGORITHMS\nSyncABB-2ph (Grinshpoun et al., 2013). Synchronous Asymmetric Branch and Bound - 2-phase (SyncABB-2ph) is a complete, synchronous, search-based algorithm that extends SyncBB with the two-phase strategy. Phase 1 emulates SyncBB, where each agent considers the costs of its constraints with higher-priority agents. Phase 2 starts once a complete assignment is found. During this phase, each agent aggregates the sides of the constraints that were not considered during Phase 1 and verifies that the known bound is not exceeded. If the bound is exceeded, Phase 2 ends and the agents restart Phase 1 by backtracking and resuming the search from the lower priority agent that exceeded the bound. The worst case runtime, memory, and communication requirements of this algorithm are the same as those of SyncBB.\nSyncABB-1ph (Grinshpoun et al., 2013; Levit, Grinshpoun, Meisels, & Bazzan, 2013). Synchronous Asymmetric Branch and Bound - 1-phase (SyncABB-1ph) is a complete, synchronous, search-based algorithm that extends SyncBB with the one-phase strategy. Each agent, after having extended the CPA, updates the bound with its local cost associated to the constraints involving its variables\u2013as done in SyncBB. In addition, the CPA is sent back to the assigned agents to update its bound via a sequence of back checking operations. The worst case runtime, memory, and communication requirements of this algorithm are the same as those of SyncBB.\nATWB (Grinshpoun et al., 2013). The Asymmetric Two-Way Bounding (ATWB) algorithm is a complete, asynchronous, search-based algorithm that extends AFB to accommodate both forward bounding and backward bounding. The forward bounding is performed analogously to AFB. The backward bounding, instead, is achieved by sending copies of the CPA backward to the agents whose assignments are included in the CPA. Similar to what is done in AFB, agents that receive a copy of the CPA compute their estimates and send them forward to the assigning agent. The worst case runtime, memory, and communication requirements of this algorithm are the same as those of AFB.\nINCOMPLETE ALGORITHMS\nACLS (Grinshpoun et al., 2013). Asymmetric Coordinated Local Search (ACLS) is an incomplete, synchronous, search-based algorithm that extends DSA. After a random value initialization, each agent exchanges its values with all its neighboring agents. At the end of this step, each agent knows the values of all its neighboring agents, and identifies all possible improving assignments for its\nown variables, given the current neighbors choices. Each agent then picks one such assignments, according to the distribution of gains (i.e., reductions in costs) from each proposal assignment, and exchanges it with its neighbors. When an agent receives a proposal assignment, it responds with the evaluation of its side of the constraints, resulting from its current assignment and the proposal assignments of the other agents participating in the constraint. After receiving the evaluations from each of its neighbors, each agent estimates the potential gain or loss derived from its assignment, and commits to a change with a given probability, similar to agents in DSA, to escape from local minima. The worst case runtime, memory, and communication requirements of this algorithm are the same as those of DSA.\nMCS-MGM (Grinshpoun et al., 2013). Minimal Constraint Sharing MGM (MCS-MGM) is an incomplete, synchronous, search-based algorithm that extends MGM by considering each side of the constraint. Like MGM, the agents operate in an iterative fashion, where they exchange their current values at the start of each iteration. Afterwards, each agent sends the cost for its side of each constraint to its neighboring agents that participate in the same constraint.6 Upon receiving this information, each agent knows the total cost for each constraint \u2013 by adding together the costs of both sides of the constraint. Therefore, like in MGM, the agent can calculate the maximum gain (i.e., maximum reduction in costs) if it changes its values, and will send this information to all its neighbors. Upon receiving the gains of its neighbors, each agent changes its value if its gain is the largest among its neighbors. The worst case runtime, memory, and communication requirements of this algorithm are the same as those of MGM."}, {"heading": "4.6 Notable Variant: Multi-Objective DCOPs", "text": "Multi-Objective Optimization (MOO) (Miettinen, 1999; Marler & Arora, 2004) aims at solving problems involving more than one objective function to be optimized simultaneously. In a MOO problem, optimal decisions need to accommodate potentially conflicting objectives. MultiObjective DCOPs extend MOO problems and DCOPs (Delle Fave, Stranders, Rogers, & Jennings, 2011)."}, {"heading": "4.6.1 DEFINITION", "text": "A Multi-Objective DCOP (MO-DCOP) is defined by a tuple \u3008A,X,D, ~F, \u03b1\u3009, where A,X,D, and \u03b1 are as defined in Definition 4.1, and ~F = [F1, . . . , Fh]T is a vector of multi-objective functions, where each Fi is a set of objective functions fj as defined in Definition 4.1. For a complete assignment \u03c3 of a MO-DCOP, let the cost for \u03c3 according to the ith multi-objective optimization function set Fi, where 1 \u2264 i \u2264 h, be\nFi(\u03c3) := \u2211 fj\u2208Fi fj(\u03c3xj ) (3)\n6. We actually describe a version of the algorithm with a guarantee that it will converge to a local optima. In the original version of the algorithm, which does not have such guarantee, each agent sends the cost only if its gain with the neighbor\u2019s new values is larger than the neighbor\u2019s last known gain.\nThe goal of a MO-DCOP is to find a complete assignment \u03c3\u2217 such that:\n\u03c3\u2217 := argmin \u03c3\u2208\u03a3 ~F(\u03c3) = argmin \u03c3\u2208\u03a3 [F1(\u03c3), . . . , Fh(\u03c3)] T (4)\nwhere ~F(\u03c3) is a cost vector for the MO-DCOP. A solution to a MO-DCOP involves the optimization of a set of partially-ordered assignments. Note that we consider, in the above definition, point-wise comparison of vectors\u2014i.e., ~F(\u03c3) \u2264 ~F(\u03c3\u2032) if Fi(\u03c3) \u2264 Fi(\u03c3\u2032) for all 1 \u2264 i \u2264 h. Typically, there is no single global solution where all the objectives are optimized at the same time. Thus, solutions of a MO-DCOP are characterized by the concept of Pareto optimality, which can be defined through the concept of dominance:\nDefinition 1 (Dominance) A solution \u03c3 \u2208 \u03a3 is dominated by a solution \u03c3\u2217 \u2208 \u03a3 iff ~F(\u03c3\u2217) \u2264 ~F(\u03c3) and Fi(\u03c3\u2217) < Fi(\u03c3) for at least one Fi.\nDefinition 2 (Pareto Optimality) A solution \u03c3\u2217 \u2208 \u03a3 is Pareto optimal iff it is not dominated by any other solution.\nTherefore, a solution is Pareto optimal iff there is no other solution that improves at least one objective function without deteriorating the cost of another function. Another important concept is the Pareto front:\nDefinition 3 (Pareto Front) The Pareto front is the set of all cost vectors of all Pareto optimal solutions.\nSolving a MO-DCOP is equivalent to finding the Pareto front. However, even for tree-structured MO-DCOPs, the size of the Pareto front may be exponential in the number of variables.7 Thus, multi-objective algorithms often provide solutions that may not be Pareto optimal but may satisfy other criteria that are significant for practical applications. A widely-adopted criterion is that of weak Pareto optimality:\nDefinition 4 (Weak Pareto Optimality) A solution \u03c3\u2217 \u2208 \u03a3 is weakly Pareto optimal iff there is no other solution \u03c3 \u2208 \u03a3 such that ~F(\u03c3) < ~F(\u03c3\u2217).\nIn other words, a solution is weakly Pareto optimal if there is no other solution that improves all of the objective functions simultaneously. An alternative approach to Pareto optimality is one that uses the concept of utopia points:\nDefinition 5 (Utopia Point) A cost vector ~F\u25e6 = [F \u25e61 , . . . , F \u25e6h ] T is a utopia point iff F \u25e6i = max\u03c3\u2208\u03a3 Fi(\u03c3) for all 1 \u2264 i \u2264 h.\n7. In the worst case, every possible solution is a Pareto optimal solution.\nThus, a utopia point is the vector of costs obtained by independently optimizing h DCOPs, each associated to one objective of the multi-objective function vector. In general, ~F\u25e6 is unattainable. Therefore, different approaches focus on finding a compromise solution (Salukvad, 1971), which is a Pareto optimal solution that is close to the utopia point. The concept of close is dependent on the approach adopted.\nSimilar to their centralized counterpart, MO-DCOPs have been shown to be NP-hard (their decision versions), and #P-hard (the related counting versions), and to have exponentially many efficient solutions and non-dominated points (Gla\u00dfer, Reitwie\u00dfner, Schmitz, & Witek, 2010)."}, {"heading": "4.6.2 ALGORITHMS", "text": "We categorize the proposed MO-DCOP algorithms into two classes: complete and incomplete algorithms, according to their ability to find the complete set of Pareto optimal solutions or only a subset of it.\nCOMPLETE ALGORITHMS\nMO-SBB (Medi, Okimoto, & Inoue, 2014). Multi-Objective Synchronous Branch and Bound (MO-SBB) is a complete, synchronous, search-based algorithm that extends SyncBB. It uses an analogous search strategy to that of the mono-objective SyncBB: After establishing a complete ordering, MO-SBB agents extend a CPA with their own value assignments and the current associated cost vectors. Once a non-dominated solution is found, it is broadcasted to all agents, which add the solution to a list of global bounds. Thus, agents maintains an approximation of the Pareto front, which is used to bound the exploration, and extend the CPA only if the new partial assignment is not dominated by solutions in the list of global bounds. When the algorithm terminates, it returns the set of Pareto optimal solutions obtained by filtering the list of global bounds by dominance. The worst case runtime and communication requirements of this algorithm are the same as those of SyncBB. In terms of memory requirement, each MO-SBB agent needs O(np) amount of memory, where p is the size of the Pareto set.\nPseudo-tree Based Algorithm (Matsui, Silaghi, Hirayama, Yokoo, & Matsuo, 2012). The proposed algorithm is a complete, asynchronous, search-based algorithm that extends ADOPT. It introduces the notion of boundaries on the vectors of multi-objective values, which extends the concept of lower and upper bounds to vectors of values. The proposed approach starts with the assumption that |F1| = \u00b7 \u00b7 \u00b7 = |Fh| = k. Furthermore, functions within each Fi are sorted according to a predefined ordering, and for each 1 \u2264 j \u2264 k, we have that the scope of f ij (i.e., the jth function in Fi) is the same for each i (i.e., all functions in the same position in different Fi have the same scope). Thus, without loss of generality, we will refer to the scope of f ij as x\nj . In such a context, for 1 \u2264 j \u2264 k, given a complete assignment \u03c3, we define the vector of cost\nvalues ~\u03c3j = [\u03c31fj , . . . , \u03c3 h fj ] = [f1j (\u03c3xj ), . . . , f h j (\u03c3xj )]. The notion of non-dominance is applied to these vectors, where a vector ~\u03c3j is non-dominated iff there is no other vector ~\u03c3\u2032j such that ub(\u03c3\u2032j\ns) \u2264 lb(\u03c3sj ) for all 1 \u2264 s \u2264 h and ub(\u03c3\u2032js) < lb(\u03c3sj ) for at least one s. The algorithm uses the notion of non-dominance for bounded vectors to retain exclusively non-dominated vectors.\nThe worst case runtime and communication requirements of this algorithm are the same as those of ADOPT. In terms of memory requirement, each agent needs O(np) amount of memory. However, notice that the number of combinations of cost vectors grows exponentially with the number of tuples of cost values, in the worst case. This algorithm has also been extended to solve Asymmetric MO-DCOPs (Matsui, Silaghi, Hirayama, Yokoo, & Matsuo, 2014), which is an extension of both Asymmetric DCOPs and MO-DCOPs.\nINCOMPLETE ALGORITHMS\nB-MOMS (Delle Fave et al., 2011). Bounded Multi-Objective Max-Sum (B-MOMS) is an incomplete, asynchronous, inference-based algorithm, and was the first MO-DCOP algorithm introduced. It extends Bounded Max-Sum to compute bound approximations for MO-DCOPs. It consists of three phases. The Bounding Phase generates an acyclic subgraph of the multi-objective factor graph, using a generalization of the maximum spanning tree problem to vector weights. During the Max-sum Phase, the agents coordinate to find the Pareto optimal set of solutions to the acyclic factor graph generated in the bounding phase. This is achieved by extending the addition and marginal maximization operators adopted in Max-Sum to the case of multiple objectives. Finally, the Value Propagation Phase allows agents to select a consistent variable assignment, as there may multiple Pareto optimal solutions. The bounds provided by the algorithm are computed using the notion of utopia points.\nThe worst case runtime requirement of this algorithm is the same as those of Max-Sum. In terms of communication requirement, the number of messages sent is also like Max-Sum, but the size of each message is nowO(pdn). In terms of memory requirement, each B-MOMS agent needs O(pdn) amount of memory to store and process the messages received.\nDP-AOF (Okimoto, Clement, & Inoue, 2013). Dynamic Programming based on Aggregate Objective Functions (DP-AOF) is an incomplete, synchronous, inference-based algorithm. It adapts the AOF technique (Miettinen, 1999), designed to solve centralized multi-objective optimization problems, to solve MO-DCOPs. Centralized AOF adopts a scalarization to convert a MOO problem into a single objective optimization. This is done by assigning weights (\u03b11, . . . , \u03b1h) to each of the functions in the objective vector [F1, . . . , Fh]T such that \u2211h i=1 \u03b1i = 1 and \u03b1i > 0 for\nall 1 \u2264 i \u2264 h. The resulting mono-objective function \u2211h\ni=1 \u03b1i Fi can be solved using any monoobjective optimization technique with guarantee to find a Pareto optimal solution (Miettinen, 1999).\nDP-AOF proceeds in two phases. First, it computes the utopia point ~F\u25e6 by solving as many mono-objective DCOPs as the number of objective functions in the MO-DCOP. DP-AOF uses DPOP to solve these mono-objective DCOPs. It then constructs a new problem building upon the solutions obtained from the first phase. Such a problem is used to assign weights to each objective function of the MO-DCOP to construct the new mono-objective function in the same way as centralized AOF, which then can be solved optimally. The worst case runtime, memory, and communication requirements of this algorithm are the same as those of DPOP, except that the number of operations and the number of messages are larger by a factor of h since it runs DPOP h times to solve the h mono-objective DCOPs.\nMO-DPOPLp (Okimoto, Schwind, Clement, & Inoue, 2014). Multi-Objective Lp-norm based Distributed Pseudo-tree Optimization Procedure (MO-DPOPLp) is an incomplete, synchronous, inference-based algorithm. It adapts DPOP using a scalarization measure based on the Lp-norm to find a subset of the Pareto front of a MO-DCOP. Similar to DP-AOF, the algorithm proceeds in two phases. Its first phase is the same as the first phase of DP-AOF: It solves h mono-objective DCOPs using DPOP to find the utopia point ~F\u25e6. In the second phase, the agents coordinate to find a solution that minimizes the distance from ~F\u25e6 according to the Lp-norm. The algorithm is guaranteed to find a Pareto optimal solution only when the L1-norm (Manhattan norm) is adopted. In this case, MO-DPOPL1 finds a Pareto optimal solution that minimizes the average cost values of all objectives. The worst case runtime, memory, and communication requirements of this algorithm are the same as those of DP-AOF.\nDIPLS (Wack, Okimoto, Clement, & Inoue, 2014). Distributed Iterated Pareto Local Search (DIPLS) is an incomplete, synchronous, search-based algorithm. It extends the Pareto Local Search (PLS) algorithm (Paquete, Chiarandini, & Sttzle, 2004), which is a hill climbing algorithm designed to solve centralized multi-objective optimization problems, to solve MO-DCOPs. The idea behind DIPLS is to evolve an initial solution toward the Pareto front. To do so, it starts from an initial set of random assignments, and applies PLS iteratively to generate new non-dominated solutions. DIPLS requires a total ordering of agents and elects one agent as the controller. At each iteration, the controller filters the set of solutions by dominance and broadcasts them to the agents in the MODCOP. Upon receiving a solution, an agent generates a list of neighboring solutions by modifying the assignments of the variables that it controls, and sends them back to the controller. When the controller receives the messages from all agents, it proceeds to filter (by dominance) the set of solutions received, and if a new non-dominated solution is found, it repeats the process.\nThe worst case runtime of this algorithm is O(`kp) as the controller agent is required to check the dominance of the newly generated solutions at each iteration. In terms of memory requirement, DIPLS agents use O(np) space to store the Pareto front. Finally, in terms of communication requirement, the controller agent broadcasts messages that contain the current Pareto front. Thus, the message size is O(np)."}, {"heading": "5. Dynamic DCOPs", "text": "Within a real-world MAS application, agents often act in dynamic environments that evolve over time. For instance, in a disaster management search and rescue scenario, new information (e.g., the number of victims in particular locations, or priorities on the buildings to evacuate) typically becomes available in an incremental manner. Thus, the information flow modifies the environment over time. To cope with such a requirement, researchers have introduced the Dynamic DCOP (DDCOP) model, where cost functions can change during the problem solving process, agents may fail, and new agents may be added to the DCOP being solved. With respect to our categorization described in Section 3, in the D-DCOP model, the agents are completely cooperative and they have\ndeterministic behavior and total knowledge. On the other hand, the environment is dynamic and deterministic."}, {"heading": "5.1 Definition", "text": "The Dynamic DCOP (D-DCOP) model is defined as a sequence of classical DCOPs: D1, . . . ,DT , where each Dt = \u3008At,Xt,Dt,Ft, \u03b1t\u3009 is a DCOP representing the problem at time step t, for 1 \u2264 t \u2264 T . The goal in a D-DCOP is to solve the DCOP at each time step optimally. We assume that the agents have total knowledge about their current environment (i.e., the current DCOP), but they are unaware of changes to the problem in future time steps.\nIn a dynamic system, agents are required to adapt as fast as possible to environmental changes. Stability (Dijkstra, 1974; Verfaillie & Jussien, 2005) is a core algorithmic concept, where an algorithm seeks to minimize the number of steps that it requires to converge to a solution each time the problem changes. In such a context, these converged solutions are also called stable solutions. Self-stabilization is a related concept derived from the area of fault-tolerance:\nDefinition 6 (Self-stabilization) A system is self-stabilizing iff the following two properties hold: \u2022 Convergence: The system reaches a stable solution in a finite number of steps, starting from any given state. In the DCOP context, this property expresses the ability of the agents to coordinate a joint variables assignment that optimizes the problem at time step t + 1, starting from an assignment of the problem\u2019s variables at time step t. \u2022 Closure: The system remains in a stable solution, provided that no changes in the environ-\nment happens. In the DCOP context, this means that agents do not change the assignment for their variables after converging to a solution.\nAn extension of the concept of self-stabilization is that of super-stabilization (Dolev & Herman, 1995), which focuses on stabilization after topological changes. In the context of D-DCOPs, differently from self-stabilizing algorithms, where convergence after a single change in the constraint graph can be as slow as the convergence from an arbitrary starting state, super-stabilizing algorithms take special care of the time required to adapt to a single change in the constraint graph.\nSolving D-DCOPs is NP-hard, as it requires to solve each DCOP of the D-DCOP independently."}, {"heading": "5.2 Algorithms", "text": "In principle, one could use classical DCOP algorithms to solve the DCOP Dt at each time step 1 \u2264 t \u2264 T . However, the dynamic environment evolution encourages firm requirements on the algorithm design in order for the agents to respond automatically and efficiently to environmental changes over time. In particular, D-DCOP algorithms often follow the self-stabilizing property. As in the previous sections, we categorize the algorithms as being either complete or incomplete, according to their ability to determine the optimal solution at each time step."}, {"heading": "5.2.1 COMPLETE ALGORITHMS", "text": "S-DPOP (Petcu & Faltings, 2005c). Self-stabilizing DPOP (S-DPOP) is a synchronous, inference-based algorithm that extends DPOP to handle dynamic environments. It is composed of three self-stabilizing phases: (1) A self-stabilizing DFS pseudo-tree generation, whose goal is to create and maintain a DFS pseudo-tree structure; (2) A self-stabilizing algorithm for the UTIL propagation phase; and (3) A self-stabilizing algorithm for the VALUE propagation phase. These procedures work as in DPOP and they are invoked whenever any change in the DCOP problem sequence is revealed. Petcu and Faltings (2005c) discuss two self-stabilizing extensions, namely super-stabilization and fault-containment, which can be used to provide guarantees about the way the system transitions from a valid state to the next, after an environment change.\nThe worst case runtime, memory, and communication requirements of this algorithm to solve the DCOP at each time step are the same as those of DPOP. Additionally, upon changes to the problem, S-DPOP stabilizes after at most \u03c4 UTIL messages and k VALUE messages, where \u03c4 is the depth of the pseudo-tree and k is the number of cost functions of the problem.\nI-ADOPT and I-BnB-ADOPT (Yeoh, Varakantham, Sun, & Koenig, 2011). Incremental Anyspace ADOPT (I-ADOPT) and Incremental Any-space BnB-ADOPT (I-BnB-ADOPT) are asynchronous, search-based algorithms that extend ADOPT and BnB-ADOPT, respectively. In the incremental any-space versions of the algorithms, each agent maintains bounds for multiple contexts; in contrast, agents in ADOPT and BnB-ADOPT maintain bounds for one context only. By doing so, when solving the next DCOP in the sequence, agents may reuse the bounds information computed in the previous DCOP. In particular, the algorithms identify affected agents, which are agents that cannot reuse the information computed in the previous iterations, and they recompute bounds exclusively for such agents.\nThe worst case runtime and communication requirements of this algorithm to solve the DCOP at each time step are the same as those of ADOPT. However, since these algorithms have the anyspace property, their minimal memory requirements are the same as those of ADOPT but they can use more memory, if available, to speed up the algorithms."}, {"heading": "5.2.2 INCOMPLETE ALGORITHMS", "text": "SBDO (Billiau, Chang, & Ghose, 2012a). Support Based Distributed Optimization (SBDO) is an asynchronous search-based algorithm that extends the Support Based Distributed Search algorithm (Harvey, Chang, & Ghose, 2007) to the multi-agent case. It uses two types of messages: is-good and no-good. Is-good messages contain an ordered partial assignment and are exchanged among neighboring agents upon a change in their value assignments. Each agent, upon receiving a message, decides what value to assign to its own variables, attempting to minimize their local costs, and communicates such decisions to its neighboring agents via is-good messages. No-good messages are used in response to violations of hard constraints, or in response to obsolete assignments. A no-good message is augmented with a justification, that is, the set of hard constraints that are violated, and are saved locally within each agent. This information is used to discard partial assign-\nments that are supersets of one of the known no-goods. The changes of the dynamic environment are communicated via messages, which are sent from the environment to the agents. In particular, changes in hard constraints require the update of all the justifications in all no-goods.\nThe worst case runtime, memory, and communication requirements of this algorithm are the same as those of SyncBB each time the problem changes.\nFMS (Ramchurn, Farinelli, Macarthur, & Jennings, 2010). Fast Max-Sum (FMS) is an asynchronous inference-based algorithm that extends Max-Sum to the Dynamic DCOP model. As in Max-Sum, the algorithm operates on a factor graph. Solution stability is maintained by recomputing only those factors that changed between the previous DCOP Dt\u22121 and the current DCOP Dt. Ramchurn et al. (2010) exploit domain-specific properties in a task allocation problem to reduce the number of states over which each factor has to compute its solution. In addition, FMS is able to efficiently manage addition or removal of tasks (e.g., factors), by performing message propagation exclusively on the factor graph regions that are affected by such topological changes. The worst case runtime, memory, and communication requirements of this algorithm to solve the DCOP at each time step are the same as those of Max-Sum.\nFMS has been extended in several ways. Bounded Fast Max-Sum provides bounds on the solution found, as well as it guarantees super-stabilization (Macarthur, Farinelli, Ramchurn, & Jennings, 2010). Branch-and-Bound Fast Max-Sum (BnB-FMS) extends FMS providing online domain pruning using a branch-and-bound technique (Macarthur, Farinelli, Ramchurn, & Jennings, 2011)."}, {"heading": "5.3 Notable Variants: D-DCOPs with Commitment Deadlines or Markovian Properties", "text": "We now describe several notable variants of D-DCOPs and their corresponding algorithms.\nRS-DPOP (Petcu & Faltings, 2007b). In this proposed model, agents have commitment deadlines and stability constraints. In other words, some of the variables may be unassigned at a given point in time, while others must be assigned within a specific deadline. Commitment deadlines are either hard or soft. Hard commitments model irreversible processes. When a hard committed variable is assigned, its value cannot be changed. Soft commitments model contracts with penalties. If a soft committed variable xti has been assigned at time step t, its value can be changed at time step t\n\u2032 > t, at the price of a cost penalty. Such costs are modeled via stability constraints, which are defined as binary relations si : Di \u00d7Di \u2192 R+, representing the cost of changing the value of variable xi from time step t to time step t+ 1. Given the set of stability constraints S \u2286 F, at each time step t, the goal is to find a solution \u03c3\u2217t :\n\u03c3\u2217t := argmin \u03c3\u2208\u03a3 Fg(\u03c3) + \u2211 sj\u2208S sj(\u03c3 \u2217 t\u22121(xj), \u03c3(xj))  . The latter term accounts for the penalties associated to the value assignment updates for the soft\ncommitted variables. RS-DPOP has the same order complexity as S-DPOP.\nTo solve this problem, Petcu and Faltings (2007b) extended S-DPOP to RS-DPOP.8 Like SDPOP, it is a synchronous, inference-based algorithm, Unlike S-DPOP, it\u2019s UTIL and VALUE propagation phases now take into account the commitment deadlines. The worst case runtime, memory, and communication requirements of this algorithm to solve the DCOP at each time step are the same as those of S-DPOP.\nDistributed Q-learning and R-learning (Nguyen, Yeoh, Lau, Zilberstein, & Zhang, 2014). In this proposed model, called Markovian Dynamic DCOPs (MD-DCOPs), the DCOP in the next time step Dt+1 depends on the solution (i.e., assignment of all variables) adopted by the agents for the DCOP in the current time step Dt. However, the transition function between these two DCOPs are not known to the agents and the agents must, thus, learn them. The Distributed Q-learning and Rlearning algorithms are synchronous reinforcement-learning-based algorithms that extend the centralized Q-learning (Abounadi, Bertsekas, & Borkar, 2001) and centralized R-learning (Schwartz, 1993; Mahadevan, 1996) algorithms. Each agent maintains Q-values and R-values for each \u03c3t\u22121, dti pair, where \u03c3t\u22121 is the solution for the DCOP Dt\u22121 and dti is the value of its variables in the cost function f ti \u2208 Ft. These Q- and R-values represent the predicted cost the agent will incur if it assigns its variables values according to dti when \u03c3t\u22121 is the previous solution. The agents repeatedly refine these values at every time step and choose the values with the minimum Q- or R-value at each time step.\nThe worst case runtime, communication, and memory requirements of these two algorithms to solve the DCOP at each time step are the same as those of DPOP, as they use DPOP as a subroutine to update the Q- and R-values. The exception is that agents in the Distributed Q-learning algorithm also broadcast their value assignments at each time step to all other agents. Thus, they send O(m2) messages in each time step instead of the O(m) complexity of DPOP.9\nA very similar and related model is the Proactive Dynamic DCOPs (PD-DCOPs) (Hoang, Fioretto, Hou, Yokoo, Yeoh, & Zivan, 2016; Hoang, Hou, Fioretto, Yokoo, Yeoh, & Zivan, 2017), where the transition function between two subsequent DCOPs are known and can be exploited by the agents. Additionally, another key difference between these two models are that the DCOP in the next time step Dt+1 does not depend on the solution in the current time step, but instead depend on the values of random variables at the current time step. Researchers have introduced a number of offline proactive and online reactive algorithms to solve this problem (Hoang et al., 2016, 2017)."}, {"heading": "6. Probabilistic DCOPs", "text": "So far, we have discussed DCOP models that can model MAS problems in environments that are deterministic. However, many real-world applications are characterized by environments with stochastic behavior. In other words, there are exogenous events that can influence the outcome of an agent\u2019s action. For example, weather conditions or the state of a malfunctioning device can affect the cost of an agent\u2019s action. To cope with such scenarios, researchers have introduced\n8. The full name of the algorithm was not provided by Petcu and Faltings (2007b). 9. We count a single broadcast message as m peer-to-peer messages, where m is the number of agents in the problem.\nProbabilistic DCOP (P-DCOP) models, where the uncertainty in the state of the environment is modeled through stochasticity in the cost functions. With respect to our categorization, described in Section 3, in the P-DCOP model, the agents are completely cooperative and they have deterministic behavior. Additionally, the environment is static and stochastic. While a large body of research has focused on problems where agents have total knowledge, we will discuss a subclass of P-DCOPs where the agents\u2019 knowledge of the environment is limited, and agents must balance exploration of the unknown environment and the exploitation of the known costs."}, {"heading": "6.1 Definition", "text": "A common strategy to model uncertainty is to augment the outcome of the cost functions with a stochastic character (Atlas & Decker, 2010; Stranders, Delle Fave, Rogers, & Jennings, 2011; Nguyen, Yeoh, & Lau, 2012). Another method is to introduce additional random variables to the cost functions, which simulate exogenous uncontrollable traits (Le\u0301aute\u0301 & Faltings, 2009, 2011; Wang, Sycara, & Scerri, 2011). To cope with such a variety, we introduce the Probabilistic DCOP (P-DCOP) model, which generalizes the proposed models of uncertainty. A P-DCOP is defined by a tuple \u3008A,X,D,F, \u03b1, I,\u2126,P, E ,U\u3009, where A and D are as defined in Definition 4.1. In addition, \u2022 X is a mixed set of decision variables and random variables. \u2022 I = {r1, . . . , rq} \u2286 X is a set of random variables modeling uncontrollable stochastic events,\nsuch as weather or a malfunctioning device. \u2022 F = {f1, . . . , fk} is the set of cost functions, each defined over a mixed set of decision variables\nand random variables, and such that each value combination of the decision variables on the cost function, results in a probability distribution. As a result, fi is itself a random variable, given the local value assignment \u03c3xi\\I and a realization for the random variables involved in fi. \u2022 \u03b1 : X \\I \u2192 A is a mapping from decision variables to agents. Notice that random variables are\nnot controlled by any agent, as their outcomes do not depend on the agents\u2019 actions. \u2022 \u2126 = {\u21261, . . . ,\u2126q} is the (possibly discrete) set of events for the random variables (e.g., different\nweather conditions or stress levels a device is subjected to) such that each random variable ri \u2208 I takes values in \u2126i. In other words, \u2126i is the domain of random variable ri. \u2022 P = {p1, . . . , pq} is a set of probability distributions for the random variables, such that pi :\n\u2126i \u2192 [0, 1] \u2286 R assigns a probability value to an event for ri and \u222b \u03c9\u2208\u2126i pi(\u03c9) d\u03c9 = 1 for each\nrandom variable ri \u2208 I. \u2022 E is an evaluator function from random variables to real values, that, given an assignment of\nvalues to the decision variables, summarizes the distribution of the aggregated cost functions. \u2022 U is a utility function that given a random variable returns an ordered set of different outcomes,\nand it is based on the decision maker preferences. This function is needed when the cost functions have uncertain outcomes, and thus these distributions are not readily comparable.\nThe goal in a P-DCOP is to find a solution \u03c3\u2217, that is, an assignment of values to all the decision variables, such that:\n\u03c3\u2217 := arg min/max \u03c3\u2208\u03a3 E \u2211 fi\u2208F U ( fi(\u03c3xi\\I) ) (5) where argmin or argmax are selected depending on the algorithm adopted, \u2211 is the operator that is used to aggregate the values from the functions fi \u2208 F. Typically such an operator is a summation, however, to handle continuous distributions, other operators have been proposed.\nThe probability distribution over the domain of random variables ri \u2208 I is called a belief. An assignments of all random variables in I describes a (possible) scenario governed by the environment. As the random variables are not under the control of the agents, they act independently of the decision variables. Specifically, their beliefs are drawn from probability distributions. Furthermore, they are assumed to be independent of each other and, thus, they model independent sources of exogenous uncertainty.\nThe utility function U enables us to compare the uncertain cost outcomes of the cost functions. In general, the utility function is non-decreasing, that is, the lower the cost, the higher the utility. However, the utility function should be defined for the specific application of interest. For example, in farming, the utility increases with the amount of produce harvested. However, farmers may prefer a smaller but highly certain amount of produce harvested over a larger but highly uncertain and, thus, risky outcome.\nThe evaluation function E is used to summarize in one criterion the costs of a given assignment that depends on the random variables. A possible evaluation function is the expectation function: E [\u00b7] = E[\u00b7].\nLet us now introduce some concepts that are commonly adopted in the study of P-DCOPs.\nDefinition 7 (Convolution) The convolution of the probability density function (PDF) f(x) and g(x) of two independent random variables X and Y is the integral of the product of the two functions after one is reversed and shifted:\nh(z) = (f \u2217 g)(z) := \u222b \u221e \u2212\u221e f(\u03c4) g(z \u2212 \u03c4) d\u03c4 = \u222b \u221e \u2212\u221e f(z \u2212 \u03c4) g(\u03c4) d\u03c4 (6)\nIt produces a new PDF h(z) that defines the overlapping area between f(x) and g(y) as a function of the quantity that one of the original functions is translated by. In other words, the convolution is a method of determination of the sum of two random variables. The counterpart for the distribution of the sum Z = X + Y of two independent discrete variables is:\nP (Z = z) = \u221e\u2211\nk=\u2212\u221e P (X = k)P (Y = z \u2212 k). (7)\nIn a P-DCOP, the value returned by a function fi, for an assignment on its scope xi, is a random variable Vi (Vi \u223c fi(xi)). Thus, the global value \u2211 fi\u2208F Vi is also a random variable,\nwhose probability density function is the convolution of the PDFs of the individual Vi\u2019s. Thus, the concept of convolution of two PDFs in a P-DCOP is related to the summation of the utilities of two cost functions in classical DCOPs.\nA common concept in optimization with uncertainty is that of ranking a set of random variables {r1, r2, . . . } with Cumulative PDFs (CDFs) {F1(x), F2(x), . . . }. Such distributions are also commonly called lotteries, a concept related to that of stochastic dominance, which is a form of stochastic ordering based on preference regarding outcomes. It refers to situations where a probability distribution over possible outcomes can be ranked as superior to another.\nThe first-order stochastic dominance refers to the situation when one lottery is unambiguously better than another:\nDefinition 8 (First-Order Stochastic Dominance) Given two random variables ri and rj with CDFs Fi(x) and Fj(x), respectively, Fi first-order stochastically dominates Fj iff:\nFi(x) \u2264 Fj(x), (8)\nfor all x with a strict inequality over some interval.\nIf Fi first-order stochastically dominates Fj , then Fi necessarily has a strictly smaller expected value: E[Fi(x)] < E[Fj(x)]. In other words, if Fi dominates Fj , then the decision maker prefers Fi over Fj regardless of his utility function U is, as long as it is weakly increasing.\nIt is not always the case that one CDF will first-order stochastically dominate another. In such a case, one can use the second-order stochastic dominance to compare them. The latter refers to the situation when one lottery is unambiguously less risky than another:\nDefinition 9 (Second-Order Stochastic Dominance) Given two random variables ri and rj with CDFs Fi(x) and Fj(x), respectively, Fi second-order stochastically dominates Fj iff:\u222b c\n\u2212\u221e Fi(x) dx \u2264 \u222b c \u2212\u221e Fj(x) dx, (9)\nfor all c with a strict inequality for some values of c.\nIf Fi second-order stochastically dominates Fj , then E[Fi(x)] \u2264 E[Fj(x)]. If Equation 9 holds for all c \u2265 c\u2032, for some sufficiently large c\u2032, then E[Fi(x)] = E[Fj(x)]. In this case, as both lotteries are equal in expectation, the decision maker prefers the lottery Fi, which has less variance and is, thus, less risky.\nAnother common concept in P-DCOPs is that of regret. In decision theory, regret expresses the negative emotion arising from learning that a different solution than the one adopted, would have had a more favorable outcome. In P-DCOPs the regret of a given solution is typically defined as the difference between its associated cost and that of the theoretical optimal solution. The notion of regret is especially useful in allowing agents to make robust decisions in settings where they have limited information about the cost functions.\nAn important type of regret is the minimax regret. Minimax regret is a decision rule used to minimize the possible loss for a worst case (i.e, maximum) regret. As opposed to the (expected) regret, minimax regret is independent of the probabilities of the various outcomes. Thus, minimax regret could be used when the probabilities of the outcomes are unknown or difficult to estimate.\nSolving P-DCOPs is PSPACE-hard, as in general, the process is required to remember a solution for each possible state associated to the uncertain random variables. The study of complexity classes for P-DCOPs is largely unexplored. Thus, we foresee this as a potential direction for future research, in which particular focus could be given in determining fragments of P-DCOPs characterized by lower complexity than the one above."}, {"heading": "6.2 Algorithms", "text": "Like Classical DCOPs and Dynamic DCOPs, where researchers have proposed various algorithmic solutions, in P-DCOPs, researchers have also introduced a number of algorithms. However, unlike Classical DCOPs and Dynamic DCOPs, where the algorithms solve the same problem, P-DCOP algorithms approach the problem uncertainty in different ways and are, thus, solving different variants of the problem. This is due to the greater modeling flexibility offered by the P-DCOP framework. As such, the proposed algorithms are often not directly comparable one another. We categorize P-DCOP algorithms into complete and incomplete algorithms, according to their ability to guarantee to find the optimal solutions or not, for a given evaluator and utility functions. Unless otherwise specified the ordering operator in Equation 5 refers to the argmax operator."}, {"heading": "6.2.1 COMPLETE ALGORITHMS", "text": "E[DPOP] (Le\u0301aute\u0301 & Faltings, 2011). E[DPOP] is a synchronous, sampling-based and inferencebased algorithm. It can be either complete or incomplete based on the E[DPOP] variant used, which we describe below. E[DPOP] uses a collaborative sampling strategy, where all agents concerned with a given random variable agree on a common sample set that will be used to estimate the PDF of that random variable. Agents performing collaborative sampling independently propose sample sets for the random variables influencing the variables they control, and elect one agent among themselves as responsible for combining the proposed sample sets into one. The algorithm is defined over P-DCOPs with I 6= \u2205, and deterministic cost function outcomes, that is, for each combination of values for the variables in xi, fi(\u03c3xi\\I) is a degenerate distribution (i.e., a distribution that results in a single value) and the utility function U is the identity function. E is an arbitrary evaluator function summing over all functions in F.\nE[DPOP] builds on top of DPOP, and proceeds in four phases: In Phase 1, the agents order themselves into a pseudo-tree ignoring the random variables. In Phase 2, the agents bind random variables to some decision variable. In Phases 3 and 4, the agents run the UTIL and VALUE propagation phases like in DPOP except that random variables are sampled. Based on different strategies adopted in binding the random variables in Phase 2, the algorithm has two variants (Le\u0301aute\u0301 & Faltings, 2009). In Local-E[DPOP], a random variable ri \u2208 I is assigned to each decision variable\nresponsible for enforcing a constraint involving ri. In this approach, the agents do not collaborate by exchanging information about how their utilities depend on the random variables. In contrast, Global-E[DPOP] assigns ri to the lowest common ancestor agent,10 which is responsible for combining the proposed samples. While this additional information can produce higher-quality solutions, both algorithms are generally incomplete. One exception is when the evaluation function E adopted is linear, as in the case of the expectation function, in which case the algorithms are complete.\nThe worst case runtime, memory, and communication requirements of this algorithm are the same as those of DPOP. The exception is the message size of Global-E[DPOP], which is O(dw \u2217 sq), where s is the largest sample set size; the UTIL messages have this size when the root as well as all leaves of the pseudo-tree are constrained with all q random variables.\nSD-DPOP (Nguyen et al., 2012). Stochastic Dominance DPOP (SD-DPOP) also operates on a P-DCOP model, where I = \u2205, E is the second order stochastic dominance criteria, and \u03a3 denotes the convolution of the distributions fi(\u03c3xi\\I), while U is the identity function. It is a complete synchronous inference-based algorithm that extends DPOP to solve P-DCOPs. Similar to DPOP, it has three phases. In Phase 1, like DPOP, it constructs a pseudo-tree. In Phase 2, instead of summing up costs, the agents convolve cost functions, and instead of propagating costs up the pseudo-tree, they propagate convolved cost functions. In Phase 3, like DPOP, the agents choose values for their variables. However, instead of choosing values that minimize the cost of their subtrees, the agents choose their values according to the second-order stochastic dominance criteria.\nLike DPOP, SD-DPOP requires a linear number of messages. In addition, in SD-DPOP, VALUE messages contain each Pareto optimal value of the sending agent, and UTIL messages contain a representation of the cost function for each Pareto optimal solution and each combination of values of the parent and pseudo-parents of the sending agents. Thus, for continuous PDFs that could be represented by mean and variance, the message size is O(pdw \u2217 ), where p is the size of the Pareto set. If the cost functions are represented by discretized bins, then the message size is O(bpdw \u2217 ), where b is the maximum number of bins used to represent a cost function. The memory requirement of each SD-DPOP agent is similarly O(pdw \u2217 ) or O(bpdw \u2217 ) as above, as they need to store and process the messages received. The worst case runtime requirement and the number of messages sent by SD-DPOP are the same as those of DPOP."}, {"heading": "6.2.2 INCOMPLETE ALGORITHMS", "text": "DNEA (Atlas & Decker, 2010). The P-DCOP model proposed by Atlas and Decker (2010) is characterized by uncertainty exclusively at the level of the outcome of the cost functions, and not due to random variables. Thus, I = \u2205. In addition, the utility function U is the identity function, while E is a given evaluator function (e.g., the expectation) for the functions fi \u2208 F. In such settings, by employing the evaluation function, they show that one can reduce the uncertainty\n10. The agent that is separated by the smallest number of tree edges from all variables constrained with the given random variable.\nassociated to each cost function to the deterministic case. Thus, one can solve the proposed PDCOP problems using classical DCOP approaches.\nIn particular, they propose the Distributed Neighbor Exchange Algorithm (DNEA), which is an incomplete, synchronous, search-based algorithm that is similar to DSA. Each agent starts by assigning a random value to each of its variables and sends this information to all its neighbors. Upon receiving the values of its neighbors, it computes a cost vector, which contains the costs for each possible combination of values for all its variables under the assumption that its neighbors\u2019 values are those in the messages received. It then sends this cost vector to all its neighbors. Upon receiving the cost vector of its neighbors, it computes the best value for each of its variables, assigns those values to its variables probabilistically, and sends the assigned values to all its neighbors. This process repeats until a termination condition is satisfied.\nThe worst case runtime requirement of this algorithm is O(`(ld+ d2)). In terms of communication requirement, the number of messages sent is O(`nl), and the size of each message is O(d). In terms of memory requirement, each DNEA agent needs O(ld) amount of memory to store and process the messages received.\nU-GDL (Stranders et al., 2011). The P-DCOP model proposed by Stranders et al. (2011) also assumes that the cost functions are not dependent on random variables. Thus, I = \u2205. Additionally, they assume that E is the expectation of the convolution (\u03a3) of the distributions fi(\u03c3xi\\I), and U is a given risk function. They propose the Uncertain Generalized Distributive Law (U-GDL) algorithm, which is an incomplete asynchronous inference-based algorithm similar to Max-Sum, and operates on acyclic graphs. A cyclic constraint graphG is converted into an acyclic graph G\u0302 by merging variables until the new resulting graph contains no cycles. Merging two variables creates a new one whose domain is the cartesian product of the domain of the merged variables. U-DGL extends the Generalized Distributive Law (GDL) algorithm (Aji & McEliece, 2000) by redefining the (min,+)11 algebra to the setting where costs are random variables rather than scalars. The + operator is extended to perform convolution of two random variables. To cope with the potential issue that not all PDFs are closed under convolution, Stranders et al. (2011) suggest to resort to sampling methods to approximate such operations. The min operator is defined to distribute over convolution and to select the minimal elements from a set of random variables based on their expected cost. In order to filter partial potential solutions that can never achieve global optimality, the authors introduce a first-order stochastic dominance condition, which is employed in the context of the min operator. They also discuss necessary and sufficient conditions for dominance, where the former discards all dominated solutions, but it might also discard some non-dominated solution as well \u2013 this is equivalent to using a classical DCOP algorithms to solve the P-DCOP model adopted in their work. The latter preserves optimal solutions, but retains, in general, sub-optimal ones as well.\nThe worst case runtime requirement of this algorithm is O(pd\u0302l), where d\u0302 is the size of the largest domain of the merged variables in G\u0302. In terms of memory requirement, each U-GDL agent\n11. U-GDL was originally defined for maximization problems. We adapt its presentation to minimization problems for consistency of the DCOP models objective presented in this survey.\nneeds O(pd\u0302l) amount of memory to store all value combinations of neighboring variables for each solution in its current Pareto frontier. In terms of communication requirement, the number of messages sent is O(\u03b4(G\u0302)), where \u03b4(G\u0302) is the diameter of the resulting acyclic graph, and the size of each message is O(pd\u0302) as agents need to send the current aggregated costs of all the agent\u2019s variable\u2019s values for each solution in its current Pareto frontier."}, {"heading": "6.3 Notable Variant: P-DCOPs with Partial Agent Knowledge", "text": "We now describe a class of Probabilistic DCOPs where agents have partial knowledge about the environment. That is, the cost functions are only partially known and, therefore, agents may discover the unknown costs via exploration (Taylor, Jain, Tandon, Yokoo, & Tambe, 2011). The new model aims at capturing those domains where agents have an \u201cexplorative nature,\u201d i.e., one of the agents\u2019 goals is to acquire knowledge about the environment in which they act. Agents are concerned with a total, online, cost achievable in a limited time frame. In this context, agents must balance the coordinated exploration of the unknown environment and the exploitation of the known portion of the costs, in order to minimize the global utility. This model was originally called Distributed Coordination of Exploration and Exploitation (DCEE) (Taylor, Jain, Jin, Yokoo, & Tambe, 2010)."}, {"heading": "6.3.1 DEFINITION", "text": "The P-DCOP model for agents with partial knowledge is described by extending the P-DCOP model introduced in Section 6.1, as follows: \u3008A,X,D,F, \u03b1, I,\u2126,P, E ,U , T \u3009, where T > 0 is a finite time horizon characterizing the time within which the agents can exploit the unknown cost functions and explore the search space. The goal in such a P-DCOP problem is to find a set of complete assignments ~\u03c3\u2217 = [\u03c3\u22171, . . . , \u03c3 \u2217 T ] that minimizes the utility of the cumulative cost within the finite time horizon T :\n~\u03c3\u2217 := arg min/max \u03c31,...,\u03c3T E  T\u2211 t=0 \u2211 fi\u2208F U ( fi(\u03c3 t xi\\I) ) (10) where \u03c3t \u2208 \u03a3 denotes a solution at time step t. In other words, agents have at most T time steps to modify the value of their decision variables, and solve T P-DCOP problems by acquiring more and more knowledge on the environment as the time unrolls."}, {"heading": "6.3.2 ALGORITHMS", "text": "In a stochastic and a priori unknown environment, the cost functions need to be learned online through interactions between the agents and their environment. Thus, the algorithms presented in this section are targeted to coordinate agents to solve a sequence of optimization problems in order to simultaneously reduce uncertainty about the local cost functions (exploration) and optimize the global objective (exploitation). In addition, the following algorithms are incomplete.\nBE-Rebid (Taylor et al., 2010). The Balanced Exploration Rebid (BE-Rebid) is a synchronous, search-based algorithm that solves P-DCOPs with I = \u2205, and U and E are the identity functions. It\nextends MGM as it calculates and communicates its expected gain. The algorithm is introduced in the context of a wireless sensor network problem, where agents can perform small movements in order to enhance their communication capabilities, which are characterized by the distance between pairs of agents. Each agent can perform three actions: stay in the current position, explore another position, or backtrack to a previously explored position and halt movement. In each time step, BERebid computes the expected cost of executing the explore or backtrack actions, assuming complete knowledge of the underlying distribution of the cost functions. Exploring is evaluated by using order statistics and is based on the cost of the best value found during exploration. Backtracking to a known position results in a cost associated to the backtracked state for the remainder of the time steps (i.e., it stays in that state).\nFollowing the region-optimal approaches presented in the context of classical DCOPs, the Taylor et al. (2011) propose a version of the algorithm, called BE-Rebid-2, that allows pairs of agents to explore in a coordinated fashion. Interestingly, in such settings, the authors find that increasing coordination (measured by the number of agents that can execute a joint action) can decrease solution quality. This phenomenon is referred to as team uncertainty penalty. The worst case runtime and communication requirements of this algorithm are the same as those of MGM.\nHeist (Stranders, Tran-Thanh, Fave, Rogers, & Jennings, 2012). HEIST is a synchronous, inference-based algorithm that solves P-DCOPs with I = \u2205, E is the expectation function, and U is the identity function. Thus, it aims at minimizing the expected utility of the cumulative cost function Fg, within the finite time horizon T . It does so by modifying a Multi-Armed Bandit (MAB) approach (Vermorel & Mohri, 2005) to a distributed scenario. A MAB is a slot machine with multiple arms, each of which yields a cost, drawn from an unknown but fixed probability distribution. It trades exploration and exploitation by pulling the arms in order to minimize the cumulative cost over a finite horizon. To cope with the uncertain and stochastic nature of the cost functions, HEIST models each cost function as a MAB, such that the joined assignment of the variables in the scope of the given cost function becomes an arm of that bandit. It seeks to minimize the expected cumulative optimization cost received over a finite time horizon by repeatedly pulling the MAB arms to select the joint action with the highest estimated upper confidence bound (UCB) (Auer, Cesa-Bianchi, & Fischer, 2002) on the sum of the local gains received in a single time step. To do so, it employs a belief propagation algorithm, known as generalized distributive law (GDL) (Aji & McEliece, 2000), in order to minimize the UCB in a decentralized fashion. Stranders et al. (2012) show that HEIST enables agents to balance between exploration and exploitation, and derive optimal asymptotic bounds on the regret of the global cumulative cost attained.\nThe worst case runtime requirement of this algorithm is O(`Tdl) as each agent computes the maximum marginal UCB for its variable assignment for each time step before the horizon. In terms of memory requirement, each Heist agent needs O(dl) amount of memory to store all value combinations of neighboring variables. In terms of communication requirement, the number of messages sent is O(`T l), one to each neighbor and time step in each iteration, and the size of each message is O(Td) as it contains the aggregated costs of all the agent\u2019s variable\u2019s values for each time step.\nICG-Max-Sum (Wu & Jennings, 2013). The Iterative Constraint Generation Max-Sum (ICGMax-Sum) algorithm is a synchronous, inference-based algorithm that solves P-DCOPs with I 6= \u2205, E is the identity function, and U(fi(\u00b7)) is the maximal regret function. The algorithm aims at minimizing the sum of maximal regrets for all the functions in F. Furthermore, the horizon is T = 1. Thus, unlike the previous algorithms, IGC-Max-Sum does not attempt to learn the outcome of the cost functions. Its objective is to find robust solutions to the uncertain problem distributions; it does so by finding the solution that minimizes the maximum regret. The algorithm extends the Iterative Constraint Generation (ICG) method (Benders, 1962; Regan & Boutilier, 2010) to the decentralized case, by decomposing the overall problem into a master problem and a subproblem that are iteratively solved until convergence. At each iteration, the resolutions of these problems are attained by using Max-Sum. The master problem solves a relaxation of the minimax regret goal, where only a subset of all possible joint beliefs is considered, attempting to minimize the loss for the worst case derived from the considered joint belief. Once it generates a solution, the subproblem finds the maximally violated constraint associated to such a solution. This is referred to as the witness point, indicating that the current solution is not the best one in terms of the minimax regret. This point is added to the set of joint beliefs considered by the master problem, and the process is repeated until no new witness points can be found.\nThe worst case runtime requirement of this algorithm is O(`|I|dl), which is dominated by the master problem, whose computation is exponential in the number of variables in the scope of the associated cost function for each belief and iteration of the algorithm. In terms of memory requirement, each ICG-Max-Sum agent needs O(|I|dl) amount of memory (dominated by the master problem again) to store all value combinations of neighboring variables for each belief. In terms of communication requirement, the number of messages sent is the same as that of Max-Sum since two parallel iterations of Max-Sum is executed in each ICG-Max-Sum iteration. However, the size of each message is O(|I|d) as it contains the aggregated cost of all the agent\u2019s variable\u2019s value for each belief.\nA variation of this algorithm that aims at minimizing the expected regret, rather than minimizing the maximum regret, was introduced by Le, Fioretto, Yeoh, Son, and Pontelli (2016)."}, {"heading": "7. Quantified DCOPs", "text": "The various extensions of the DCOP model discussed so far differ from each other in terms of agent behavior (deterministic vs. stochastic), agent knowledge (total vs. partial), environmental behavior (deterministic vs. stochastic), and environment evolution (static vs. dynamic). But in terms of the agent teamwork, all of the models assume the agents are completely cooperative. Recently, researchers have introduced the Quantified DCOP (QDCOP) model (Matsui, Matsuo, Silaghi, Hirayama, Yokoo, & Baba, 2010), which assumes a subset of agents to be adversarial, that is, the agents are partially cooperative or competitive."}, {"heading": "7.1 Definition", "text": "The Quantified DCOP (QDCOP) model (Matsui et al., 2010) adapts the Quantified Constraint Satisfaction Problem (QCSP) (Benedetti, Lallouet, & Vautard, 2008) and Quantified Distributed CSP (QDCSP) (Baba, Iwasaki, Yokoo, Silaghi, Hirayama, & Matsui, 2010; Baba, Joe, Iwasaki, & Yokoo, 2011) models to DCOPs. In QCSPs and QDCSPs, all variables are associated to quantifiers and the constraints should be satisfied independently of the value taken by universally quantified variables. Analogously, in QDCOPs, existential (\u2203) and universal (\u2200) quantifiers are introduced to differentiate the cooperative agents from the adversarial ones.\nA QDCOP has the form Q(F) := q0x0 . . . qnxn.12 Q is a sequence of quantified variables, where each qi \u2208 {\u2203, \u2200} quantifies the variable xi. The goal of a QDCOP is to find a global optimal solution of the corresponding DCOP. However, a universally quantified variable is not coordinated nor assigned, as the result has to hold when it takes any value from its domain. In contrast, an existentially quantified variable takes exactly one value from its domain, as in (cooperative) DCOPs. Thus, the optimal solution of a QDCOP may be different from that of the corresponding DCOP. While a DCOP solution defines a single value, associated to its cost, a QDCOP defines upper and lower bounds to the optimal solution. In particular, the best choice in a QDCOP defines the smallest lower bound. In the worst case, the universally quantified variables can worsen the overall objective as much as possible. Therefore, the worst case defines the smallest upper bound. While finding an optimal solution for a DCOP is NP-hard, solving a QDCOP is, in general, P-SPACE-hard (Benedetti et al., 2008; Lallouet, Lee, Mak, & Yip, 2015)."}, {"heading": "7.2 Algorithms", "text": "QDCOPs impose a rigid order on the variables, which reflects the correct order of evaluation of the quantifiers. Therefore, classical DCOP algorithms cannot be directly applied to solve QDCOPs. Matsui et al. (2010) proposed several variations of ADOPT to solve QDCOPs, which are all based on a DFS pseudo-tree ordering. To keep the ordering of the quantifiers unchanged, the pseudo-tree can be reshaped by applying extra null edges for each pair of nodes, if necessary.\nAll the algorithms presented here are complete, and are based on the intuition that universally quantified variables can be seen as adversarial virtual agents, whose goal is to minimize the overall objective. Following this intuition and the pseudo-tree modifications discussed above, pseudo-treebased DCOP algorithms can be extended to solve QDCOP.\nMin-max ADOPT (Matsui et al., 2010). Min-max ADOPT is an asynchronous, search-based algorithm that extends ADOPT to solve QDCOPs. It uses VALUE messages to communicate values of the variables, and COST messages to announce their costs, similar to ADOPT. Each agent, starting from the root of the pseudo-tree, assigns values to its variables and propagates them to its neighboring agents with lower priority. Upon receiving VALUE messages from all higher-priority neighbors, the agent updates its context and repeats the same process by choosing an assignment\n12. In the original proposal, the set F is separated in a set of constraints C, representing relationships among variables, and a set of functions F , assigning values to each valid assignment.\nthat minimizes its local cost. In Min-max ADOPT, the existentially quantified variables are used to compute the lower bound, while the universally quantified variables are used to compute the upper bound. This process is executed until the root agent detects that the upper bound is equal to the lower bound. This algorithm has a relatively simple structure and does not adopt any major pruning strategy.\nAlpha-beta ADOPT (Matsui et al., 2010). Alpha-beta ADOPT is an asynchronous, search-based algorithm that extends Min-max ADOPT by adapting the alpha-beta search strategy, a common pruning strategy adopted in game-tree search. This strategy employs two boundary parameters, alpha and beta, representing the lower bound and the upper bound for each possible cost of an assignment, respectively. Alpha represents the lower bound, controlled by the universally quantified variables, while beta represents the upper bound, controlled by the existentially quantified variables. Lower bound and upper bound can be modified exclusively by universally quantified and existentially quantified variables, respectively. In Alpha-beta ADOPT, when an agent reports the cost value of the current partial assignment, its parent reduces the alpha/beta threshold accordingly. Thus, the new alpha/beta values are used to prune the search when an agent detects that the current assignment cannot be better than any other solution already evaluated. Alpha and beta values are obtained using a backtracking technique similar to how thresholds are obtained through backtracking in the original ADOPT.\nBi-threshold ADOPT (Matsui et al., 2010). Bi-threshold ADOPT extends ADOPT by employing two backtracking thresholds instead of one as in ADOPT. In ADOPT, each agent ai maintains the threshold invariant lb\u2217i \u2264 ti \u2264 ub\u2217i , where lb\u2217i and ub\u2217i are the smallest lower and upper bounds, respectively, of the agent over all of its values, and ti is the threshold of the agent. In contrast, in Bi-threshold ADOPT, each agent maintains the threshold invariant lb\u2217i \u2264 t\u03b1i \u2264 t \u03b2 i \u2264 ub\u2217i , where t\u03b1i is a lower bound on the threshold, similar to alpha in Alpha-beta ADOPT, and t\u03b2i is an upper bound on the threshold, similar to beta in Alpha-beta ADOPT."}, {"heading": "8. DCOP Applications", "text": "DCOP models have been adopted to represent a wide range of MAS applications, thanks to their ability to capture essential and fundamental MAS aspects as well as the support for the development of general domain-independent algorithms, including applications in wireless sensor networks, power networks, and scheduling. We provide here a description of some of the most compelling applications as well as a general overview of their corresponding DCOP models. A comprehensive list of DCOP applications, categorized according to the DCOP classification of Table 2, is given in Table 5."}, {"heading": "8.1 Disaster Management and Coordination Problems", "text": "Disaster management and coordination problems refer to how to efficiently and effectively respond to an emergency. In such scenarios, low-powered mobile devices that require limited bandwidth are\noften deployed and utilized. Due to their decentralized nature, the DCOP approach fits naturally with this application. We now describe several problems within this application domain.\nDisaster Evacuation Problems. In a disaster scenario, moving evacuees to the closest refuge shelter can quickly overwhelm shelter capacities. A number of researchers have proposed a DCOP model for disaster evacuation, in which several groups of evacuees have to be led to available shelters (Carpenter, Dugan, Kopena, Lass, Naik, Nguyen, Sultanik, Modi, & Regli, 2007; Kopena, Sultanik, Lass, Nguyen, Dugan, Modi, & Regli, 2008; Lass, Kopena, Sultanik, Nguyen, Dugan, Modi, & Regli, 2008a; Lass, Regli, Kaplan, Mitkus, & Sim, 2008b; Kinoshita, Iizuka, & Iizuka, 2013). Group leaders can communicate via mobile devices to monitor and coordinate actions. Each group is represented by a DCOP agent managing variables that represent shelter allocations. Thus, the domain of each variable corresponds to the available shelters. Group sizes and shelter capacity, as well as additional group requirements (e.g., medical needs) and the distance of a group to shelters, are encoded as cost functions. Solving the DCOP ensures an assignment of all groups\nto shelters that minimizes overflow, such that groups receive the services they need and their travel distances are minimal.\nCoalition Formation with Spatial and Temporal Constraints Problems. In a Coalition Formation with Spatial and Temporal Constraints (CFST) problem (Ramchurn et al., 2010; PujolGonzalez, Cerquides, Farinelli, Meseguer, & Rodriguez-Aguilar, 2015; Steinbauer & Kleiner, 2012), ambulance and fire brigade agents cooperate in order to react efficiently to an emergency scenario so as to rescue victims and extinguish fires located in different parts of a city. Agents can travel from one location to another in a given time. Each task (i.e., rescuing a victim, extinguishing a fire) has a deadline, representing the time until which the victim will survive, and a workload, denoting the amount of time necessary to rescue the victim or put out the fire. The locations of the victims and the fires may be unknown to the agents, and need to be discovered at runtime, which requires agents to dynamically update the sequence of the tasks they will attempt, taking account of two main constraints: (1) Spatial constraints, which model where an agent can travel and at what time; and (2) Temporal constraints, which model task deadlines and completion times. Agents may also form coalitions to execute a given task faster or if the requirements of a given task cannot be met by a single agent. Hence, the agents\u2019 arrival times at each task need to be coordinated in order to form the desired coalition. The objective is to maximize the number of tasks to be completed.\nA DCOP formalization for the CFST is described by Ramchurn et al. (2010), where ambulances and fire brigades are modeled as DCOP agents. Each agent controls a variable that encodes the current task that the agent will attempt, and whose domain represents task locations. Unary constraints restrict the set of reachable locations, according to distance from a destination and the victim\u2019s deadline. Agents\u2019 coalitions are defined as groups of agents traveling to the same location. Each task is associated to a utility, which encodes the success for a coalition to complete such task. The goal is to find an assignment of agents to tasks that maximizes the overall utility.\nA variant of the CFST problem, called the Law Enforcement Problem (LEP), was introduced by Amador, Okamoto, and Zivan (2014). Similar to the CFST problem, in a LEP, police officers need to execute a number of tasks, and may form coalitions to improve their response quality. Additionally, and differently from the CFST, new tasks can be revealed to the agents dynamically over time. Agents can choose to interrupt their current task to perform a new revealed task at the cost of a penalty that is proportional to the importance of the task being interrupted."}, {"heading": "8.2 Radio Frequency Allocation Problems", "text": "The performance of a wireless local area network (WLAN) depends on the channel assignments among neighboring access points (APs). Neighboring transmissions occurring in APs on the same channel or adjacent channels degrade network performance due to transmission interference. Typically, in dense urban areas, APs may belong to different administrative domains, whose control are delegated to different entities. Thus, a distributed approach to the channel assignment is necessary.\nCooperative Channel Assignment Problems. In a cooperative channel assignment problem (Hollos, Karl, & Wolisz, 2004; Monteiro, Pellenz, Penna, Enembreck, Souza, & Pujolle, 2012a; Monteiro, Pujolle, Pellenz, Penna, Enembreck, & Demo Souza, 2012b), APs need to be config-\nured in order to reduce the overall interference between simultaneous transmissions on neighboring channels. Monteiro et al. (2012a, 2012b) proposed a DCOP-based approach for cooperative channel assignment in WLANs where APs may belong to different administration entities. In the proposed model, each AP is represented by a DCOP agent, which controls a decision variable modeling a choice for the AP\u2019s channels. The signal-to-interference-and-noise ratio perceived by an AP or a client is modeled as a cost function, as the overall concurrent transmissions occurring in the same channel and in partially overlapped adjacent channels. The goal is to find an assignment of channels to APs that minimizes the total interference experienced in the WLAN.\nXie, Howitt, and Raja (2007) and Mir, Merghem-Boulahia, and Ga\u0131\u0308ti (2010) study dynamic solutions to the problem of allocating and utilizing the wireless network\u2019s available spectrum. In such a problem, the agents operate in a dynamic radio frequency environment that is composed of time-varying interference sources, which are periodically sampled and measured."}, {"heading": "8.3 Recommendation Systems", "text": "Recommendation systems are tools that provide user-tailored information about items to users. These systems provide information that is tailored to the characteristics and preferences of the users.\nGroup Recommendation Problems. Like individual recommendations, group recommendations need to take into account the preferences of all group members and formulate a recommendation that suits the whole group. Lorenzi, dos Santos, Ferreira Jr, and Bazzan (2008) proposes a DCOPbased travel package recommendation system for groups of users. The users in the group share a common goal (the travel package recommendation), and have individual preferences for each travel service (hotel, flight companies, tour operators, etc). In such a problem, the objective is to find a group recommendation that optimizes the users\u2019 preferences. The proposed DCOP solution is composed of two types of agents: user agents and recommender agents. Each user agent controls a decision variable that models that user\u2019s travel choices, while each recommender agent controls a decision variable that models a travel service supplier\u2019s recommendations. User travel preferences are modeled via unary constraints on the user agents\u2019 decision variables. Binary constraints between user agents in a group and their associated recommender agent ensure that each user\u2019s choice in the group is compatible to the recommendation of the recommender agent. The goal is to find the best recommendation for the entire group."}, {"heading": "8.4 Scheduling Problems", "text": "Scheduling problems are an important class of problems, and they have been long studied in the area of Constraint Programming and Operations Research (Giffler & Thompson, 1960; Solomon, 1987; Minton, Johnston, Philips, & Laird, 1992; Hentenryck & Michel, 2009). In such problems, time schedules are to be associated to resource usage. The problem is made particularly difficult when the scheduling process needs to be coordinated in a distributed manner across several entities. In such a context, many scheduling problems can be naturally mapped to DCOPs.\nDistributed Meeting Scheduling Problems. The distributed meeting scheduling problem captures generic scheduling problems where one wishes to schedule a set of events within a time range (Jennings & Jackson, 1995; Garrido & Sycara, 1996). Each event is defined by: (i) the resources required for it to be completed, (ii) the time required for it to be completed, within which it holds the required resources, and (iii) the cost of using such resources at a given time. A scheduling conflict occurs if two events with at least one common resource are scheduled in overlapping time slots. The goal is to maximize the utilities over all the resources, defined as the net gain between the opportunity benefit and opportunity cost of scheduling various events.\nMaheswaran, Tambe, Bowring, Pearce, and Varakantham (2004b) discuss three possible DCOP formulations for this problem: Time slots as variables (TSAV), events as variables (EAV), and private events as variables (PEAV). We describe the EAV formulation and refer the reader to the original article for the other two formulations and additional details. In the EAV formulation, events are considered as decision variables. Each variable can take on a value from the time slot range that is sufficiently early to schedule the required resources for the required amount of time or zero to denote that an event is not scheduled. If a variable takes on a non-zero value, then all its required resources cannot be assigned to any other overlapping event.\nWater Allocation Scheduling Problems. The management of water resources in large-scale systems is often associated with multiple institutionally-independent decision makers, which may represent different and conflicting interests, such as flood prevention, hydropower production, and water supply (Giuliani, Castelletti, Amigoni, & Cai, 2014). The aim of such problems is to find an efficient use of water allocation and distribution according to the different users\u2019 interests.\nGiuliani et al. (2014) formalize a regulatory mechanism in water management as a DCOP. The model involves several active human agents and passive ecological agents. Each agent is associated with an objective function that it seeks to maximize. Active agents make decisions about the amount of water to divert from the river or to be released from a dam in order to maximize their corresponding objective functions. Passive agents, on the other hand, represent ecological interests through their associated objective functions and do not make decisions. The agents model different water supplies for cities and agricultural districts, hydropower productions, and ecological preservation. The goal is to optimize the agents\u2019 objective functions, satisfying hard (physical) constraints and maximizing the soft (normative) constraints, which aim at protecting the interests of the passive agents. A solution to such problem, which makes use of a multi-objective DCOP formalization, is presented in (Amigoni, Castelletti, & Giuliani, 2015).\nVessel Rotation Planning Problem. In a large port, vessels are scheduled to visit different terminals for loading and unloading operations. The sequence of terminals visited by a vessel is called the vessel rotation. Due to the different nature of the terminal visits (e.g., loading, unloading containers, different inland shipping activities) there are often dependencies between activities performed at the terminals. Additionally, vessel operators have their own preferences on when to visit a particular terminal. The vessel rotation planning problem (VRPP) (Li, Negenborn, & Lodewijks, 2016) describes the problem of assigning rotations to vessels, consisting of sequences of visits to terminals and arrival and departure times of vessels to terminals in a port area, while\nsatisfying the terminals\u2019 visits dependencies and taking into account operators\u2019 preferences. Due to the geographically distributed nature of the resources (vessels, terminals, loading facilities) and the distributed coordination process undertaken by vessels operators this problem fits naturally in the DCOP framework.\nLi et al. (2016) proposed a DCOP model for the VRPP, where vessels and terminals are modeled as DCOP agents. The problem variables are the time slot for which a vessel i is at terminal j (xij), the arrival and departure times of vessel i at terminal j (aij and dij , respectively), and the number of time steps a vessel i waits at terminal j (wij). A vessel agent i governs variables xij , aij , dij , and wij , for each terminal j that it can visit, while a terminal agent controls auxiliary variables yij for each vessel i that can visit j, which has the same value of xij and is used by terminal agents to represent the terminal capacities. The problem constraints represent the preferences of vessels of being at a given terminal in a given time, the penalty occurring for a waiting vessel, and the time windows during which a vessel can visit a terminal.\nPatient Scheduling Problems. Medical appointments scheduling problems are related to meeting scheduling problems, as they need to associate patients to resources (e.g., doctors, medical machinery) and times, but they require different types of constraints. Patients may require several services from different departments within the same hospital or in multiple hospitals. In general, the objective is to minimize the patient treatment waiting time under limited resource conditions, as well as to ensure efficient resource usage, taking into account patient preferences.\nHannebauer and Mu\u0308ller (2001) formulate the problem of scheduling patients to diagnostic units in an hospital as a DCOP, where appointments are modeled as variables, whose domains describe times, durations, and locations. The constraints of the problem model the schedule feasibility, the patient preferences over hospitalization times, the workplace constraints, which restricts the types of appointment for a given workplace, and diagnostic unit constraints, which model resource usage.\nBilliau, Chang, Ghose, and Miller (2012b, 2012c) propose a Dynamic DCOP model for a radiotherapy patients scheduling problem. In this problem, each agent represents a patient, and it controls variables that represent private information (e.g., type of tumor, number of radiation doses per day, the use of chemotherapy) and public information (e.g., current schedule of the radiotherapy machine). The constraints of the problem model the duration of each daily treatment, as well as tumor-specific treatment restrictions. The problem objective considers patient waiting times to receive their treatment, patient priorities (based on tumor aggressiveness), and patient preferences."}, {"heading": "8.5 Sensor Network Problems", "text": "Sensor networks typically consist of a large number of inexpensive and autonomous sensor nodes, constrained by a limited communication range and battery life. These networks have been deployed for environmental sensing (temperature, humidity, etc.), military applications (e.g., battlefield surveillance), and target tracking (Akyildiz, Su, Sankarasubramaniam, & Cayirci, 2002). When deploying sensor networks, it may not be possible to pre-determine the position of each sensor node. The distributed nature of the problem and the presence of several communication and sensing constraints create a natural fit for DCOPs to solve a wide range of related applications.\nTarget Tracking Problems. In a typical target tracking application (Zhang et al., 2005; Matsui & Matsuo, 2008; Jain, Taylor, Tambe, & Yokoo, 2009; Ota, Matsui, & Matsuo, 2009; Stranders, Farinelli, Rogers, & Jennings, 2009; Semnani & Basir, 2013), a collection of small Doppler sensors are scattered in an area to detect possible moving targets in the region. Each sensor is batterypowered, can communicate with one another through radio communication, and can scan and detect an object within a fixed range. Communication incurs an energy cost. Thus, to save energy, a sensor may turn itself off. Multiple sensors may be necessary to detect a single target with high accuracy. The overall objective is to maximize the number of targets detected, as quickly as possible and, at the same time, preserve energy so as to prolong the system\u2019s lifetime\nZhang et al. (2005) moldes a simplified version of the above problem as a weighted graph coloring problem, where the total weight of violated constraints needs to be minimized. A node corresponds to a sensor, an edge between two nodes represents the constraint of a shared region between agents, and the weight captures the importance of the common region. The size of the common region reflects the amount of energy loss when two sensors scan the shared region at the same time. Each color corresponds to a time slot in which a sector is scanned. A node must have at least one color so that the corresponding sector is scanned at least once. This graph coloring problem is mapped to a DCOP, where agents represent nodes, agent\u2019s variables represent the agents decision on their color, and cost functions represent the graph edges.\nSemnani and Basir (2013) use a hierarchical DCOP approach to scale to larger problems. The authors partition the original problem into n local regions, and use n DCOPs to solve the smaller subproblems. Their solutions are then combined in a hierarchical approach, solved by a DCOP that encompasses variables and constraints shared among the connected regions of the lower hierarchy DCOPs.\nRobotic Network Optimization Problems. The robotic network optimization problem describes a sensor network problem where sensors are placed on top of robots that have limited movement capability. In such a problem, robots can make small movements to optimize the wireless connectivity with their neighbors, without affecting the network topology (Choxi & Modi, 2007; Jain et al., 2009).\nJain et al. (2009) proposed a DCOP formulation where each robot is represented by an agent. Each agent controls one variable describing the decision on the robots\u2019 possible movements. Thus, the variables\u2019 domains consist of the valid positions the agent can move to. The cost functions of the problem model the power loss (or gain, depending on the optimization criteria) of the wireless link from a transmitter and a receiver robot, and depend on their positions. Radio communication in wireless sensor networks have a predictable signal strength loss that is roughly inversely proportional to the square of the distance between transmitter and receiver. However, radio wave interference is very difficult to predict (Molisch, 2012). Thus, Jain et al. (2009) use a P-DCOPbased approach with partial agent knowledge to capture the robot\u2019s partial knowledge on its cost functions, and to balance exploration of the unknown costs and exploitation of the known portion of the costs.\nMobile Sensor Team Problem. The Mobile Sensor Team (MST) problem is similar to the target tracking problem with the difference that agents are capable of moving autonomously within the environment and that time is modeled explicitly as a discrete sequence of time steps. In an MST, agents are placed on a grid. For an agent ai, cur pos i denotes the agent\u2019s current position; SRi denotes the agent\u2019s perception sensing range, which determines the coverage range within which an agent can detect targets; MRi denotes the agent\u2019s mobility range, which defines the maximum distance that the agent can move within a single time step; and cred i denotes the agent\u2019s credibility, which reflects the likelihood of the correctness of the detected targets. The targets are defined implicitly through an environmental requirement (ER) function, which defines, for each point in space, the minimum joint credibility value (the sum of the credibility variables) required for that point to be sensed. In such a representation, targets are points p with ER(p) > 0. Given a set of agents SRp whose sensing range covers a target p, the remaining coverage requirement of p is the environmental requirement diminished by the joint credibility of the agents currently covering p: Cur REQ(p) = max{0, ER(p) \u2211 ai\u2208SRp cred i}, where : R \u00d7 R \u2192 R is an operator that defines how the environmental requirement decreases by the joint credibility. The goal of the agents is to find positions that minimize the values of Cur REQ for all targets.\nMST problems are modeled through a subclass of Dynamic DCOPs, named DCOP MST (Zivan, Glinton, & Sycara, 2009; Yedidsion, Zivan, & Farinelli, 2014; Yedidsion & Zivan, 2014). Each agent ai controls one variable xi representing its position, and whose domain contains all locations within MRi of cur pos i. Thus, the domains are updated each time the agent moves.\n13 The constraint Cp of a target p involves exclusively those agents ai whose variable\u2019s domain includes a location within SRi of p. Thus, at each time step, both domains and constraints may change. As a consequence, the constraint graph changes as well\u2013the neighbors of each agent has to be updated at each time step. Finally, in a DCOP MST two agents are neighbors if their sensing areas overlap.\nSensor Sleep Scheduling Problem. Wireless sensor nodes are equipped with a radio, which can be used to communicate with neighboring nodes, and a limited power source. These sensor nodes are often deployed in inaccessible terrains, thus replacing their power sources may not be possible. The wireless sensor sleeping scheduling problem aims at switching on/off a particular sensor node component (such as the sensor or the radio) for a certain period of time, so to ensure power conservation, maximizing the lifetime of the sensor network.\nChachra and Marefat (2006) proposed a DCOP model for this problem, where each sensor is an agent whose variables denote its status (on or off) for each time step. Hard constraints are employed to enforced that if a sensor is on, then all its neighbors should be off, and that sensors cannot stay on for two consecutive time steps. The overall objective is to minimize the delay induced in the network.\nA similar problem is solved by Stranders et al. (2009), where sensors are also able to harvest energy from the environment (e.g. using a photo-voltaic cell or vibration-harvesting microgenerators). In such a context, the goal is to find a schedule that maximizes the probability of detecting\n13. An alternative representation is that of modeling the domain of each variable as the entire grid, and to constrain the agent variable, at each agent move, in order to hide those points lying outside MRi.\nevents while maintaining energy-neutral operations (that is, exhibit an indefinite lifetime for each of the agents)."}, {"heading": "8.6 Service-Oriented Computing Problems", "text": "The service-oriented computing paradigm is one that relies on sharing resources over a network, focusing on maximizing the effectiveness of the shared resources, which are used by multiple applications. Efficient solutions with optimal use of resources are crucial in this paradigm and have a wide industrial impact (Moreno-Vozmediano, Montero, & Llorente, 2013). The distributed nature of the resources and the privacy concerns arising when different clouds are involved in the deployment, makes DCOP appealing to solve a range of problems in this paradigm (Mejias & Roy, 2010).\nApplication Component Placement Problems. An application component placement (ACP) problem is defined over a network of servers offering storage devices with various capabilities, and component-based application with requirements for processing, communication, and storage (Jin, Cao, & Li, 2011; Li, Wang, Ding, & Li, 2014). The ACP problem is a problem of deciding which server to assign to each application component. The component-based application is described by a set of characteristics that establish their requirements in terms of hardware (e.g., CPU speed, storage capacity) as well as constraints between components of the same application (e.g., minimum bandwidth, secure communication channel requirement). When the APC involves deployment on multiple clouds data privacy must be preserved. Additionally, in cloud environments computing resources are shared by many applications and the infrastructure is dynamically changing, making centralized solutions unfeasible.\nJin et al. (2011) proposed a DCOP model for the ACP problem where servers bid for a component to host, with an emphasis that is proportional to the affinity of the server characteristics and the component hardware and software requirements. Each server is modeled by an agent, which controls a decision variable representing the server bids. Thus, the domain of each variable is the set of possible components that may be deployed on the server. Unary functions express utility for each component. Hard constraints are employed to ensure that each component is deployed exactly on a single server, and that two components are placed between servers satisfying the required communication bandwidth. The objective is to find a feasible assignment of component to servers that maximizes the utilities.\nServer Allocation Problems. Services-oriented middleware networks are composed of entities that can both provide and require multiple services connected within a physical network. In turn, each service can be provided by multiple servers and can serve multiple clients. A service request from a given client, takes into account various quality of service (QoS) parameters (e.g., service response time, service completion time). When a client generates a service request, it can be satisfied by any of the servers offering such requests. The server allocation problem is a problem of selecting servers to allocate services, ensuring maximum social welfare, while meeting the QoS requirements of all clients.\nChoudhury, Dey, Dutta, and Choudhury (2014) presented a DCOP model for this problem where agents correspond to network entities, variables correspond to services, and their values are either 1, if the associated agent is willing to provide/forward the service, or 0, otherwise. Clients\u2019 service requests are mapped to servers\u2019 service offers, accounting for the delays that occur when traversing between intermediate nodes using a routing multicast protocol (Yan, Lee, Shen, & Qiao, 2013). Moreover, in order to provide a service, all requested QoS requirements need to be satisfied. The utility associated to each variable is the combination of the utility for such a node when acting as a service consumer, a service provider, and a service forwarder, and depends on several parameters, such as available GPU cycles, battery power, memory, and bandwidth. The problem may change dynamically when a new request is made, or when a new service is offered or released, and as such can be modeled as a Dynamic DCOP."}, {"heading": "8.7 Smart Grid and Smart Homes Problems", "text": "The smart grid is a vision of the future electricity grid, where bidirectional flow of both electricity and information, in an automated fashion, improves efficiency and reliability of energy production and distribution. The development of smart grids poses several challenges: (1) How to deal with the increasing electric grid utilization due to growth of loads, such as electric vehicles (EVs) and heat pumps; (2) How to efficiently integrate a diverse range of energy sources, including intermittent generators, into the power network; and (3) How to deal with the uncertainty in the equipment as well as in the participation of consumers through demand-side technologies. Due to the distributed and dynamic nature of loads and generators participating in the smart grid, agent-based decentralized autonomous control of smaller distributed microgrids is a very compelling solution (Davidson, Dolan, McArthur, & Ault, 2009; Ramchurn, Vytelingum, Rogers, & Jennings, 2012). In particular, several agent-based decentralized optimization solutions have been explored to deliver this vision (Kumar, Faltings, & Petcu, 2009; Matsui & Matsuo, 2011; Miller, Ramchurn, & Rogers, 2012; Jain, Ranade, Gupta, & Pontelli, 2012). The following is a list of the most prominent DCOP approaches for smart grid and smart homes applications.\nEconomic Dispatch Problems. Economic Dispatch (ED) is the problem of coordinating the various settings of the power generators in order to meet the power loads with the lowest cost possible, while satisfying the physical network constraints (Wood & Wollenberg, 2012). Researchers have cast this problem as a DCOP (Miller et al., 2012; Jain et al., 2012; Gupta, Jain, Yeoh, Ranade, & Pontelli, 2013a; Athanasiadis, Kockar, & McArthur, 2013), considering a network of nodes (agents), each of which relays power to other nodes, but can also contain a combination of generators and loads. Generators are distributed across nodes, and are represented through variables whose domain describe a certain set of discrete power outputs. The power lines connecting nodes of the networks, are also associated to DCOP variables, each of which has a thermal capacity describing the maximum power that it can safely carry. The DCOP function describes a particular optimization criteria, such as minimizing the carbon emissions of generators within the network, as well as the imposed load and network constraints. In particular, the constraints ensure that the\noverall demand and supply are in balance and that thermal capacity constraints of the power lines are satisfied.\nA version of the ED problem with a planning horizon has been proposed by Fioretto, Yeoh, Pontelli, Ma, and Ranade (2017b). This formalization extends the ED model described above by capturing the physical restrictions of transmission lines, power loads, and power generators arising when deploying a sequence of solutions (set-points for generators and loads) over time. In particular, it models the maximum incremental power that can be supplied or reduced in one time step to each power generator, which depends on the mechanical characteristics of the generators. This problem has been cast as a Dynamic DCOP, where, in addition to the variables and constraints of the classic ED problem, a set of constraints is introduced between each two consecutive time steps to limit the maximum variation of the generators\u2019 output.\nPower Supply Restoration Problems. After (multiple) line failures, a power grid must be reconfigured to ensure restoration of power supply. A power network distribution is a network of power lines connected by switching devices (SDs) and fed by circuit breakers (CBs). SDs are analogous to sinks (transformer stations), while CBs are analogous to power sources. Both of these devices can operate in two states: open or closed. Closed SDs consume some power and forward the rest of it on other lines. Open SDs stop power flow. CBs feed the network when they are closed. The configuration of the devices\u2019 state is such that energy flow traversing CBs takes the form of a (feeder) tree, and that no SD is powered by more than a single power line. Flow conservation and transmission line capacity constraints must be enforced. The power supply restoration problem is the problem of finding a configuration that ensures power restoration for the maximum number of sinks affected by the line failures.\nResearchers have proposed a DCOP formulation for this problem (Kumar et al., 2009; Agrawal, Kumar, & Varakantham, 2015). In such a framework, each node of the distribution network is controlled by an agent that owns all variables and constraints corresponding to that node. Two DCOP variables are associated to each network node: A load variable and a direction variable. Load variables model the amount of incoming flow for sink nodes, and the number of sinks fed for power source nodes. Direction variables model all the possibilities of feeding a node, as the set of possible configurations in which its neighboring nodes can forward power to it. The acyclicity of the power flow and the flow conservation are modeled as constraints. The former restricts the power path to be a tree as well as defines the optimization criterion. The latter enforces Kirchhoff\u2019s law, that the amount of incoming power flow to the node i must equal the sum of power consumed at i and the amount of power forwarded to other nodes.\nMicrogrid Islanding Problems. A microgrid islanding problem is the problem of creating islands (i.e., clusters of generator units and loads able to operate without external energy supply) in response to major power outages and blackouts. Gupta, Yeoh, Pontelli, Jain, and Ranade (2013b) formalized this problem as a DCOP, where agents represent nodes in the network, and each agent has its own power generation and power consumption capabilities. Variables represent the amount of power that an agent generates and consumes, as well as transmission line flows and switch status between network nodes. Flow variables are constrained by their maximum transmission line\ncapacities, while switches are modeled as binary variables that can be turned on or off. Flow conservation are modeled as constraints to enforce Kirchhoff\u2019 s law. The goal is to find a switching configuration that minimizes the unserved load of the system.\nProsumer Energy Trading Problems. In its more general form, a smart grid is populated by prosumers capable of both generating and consuming resources. The prosumer energy trading problem aims at setting market-based prices for prosumers to directly trade over the smart grid, while taking account of the grid constraints. This problem has been cast as an optimization problem called the energy allocation problem, where, given a graph with nodes representing prosumers and edges describing transmission lines connecting adjacent prosumers, the goal is to find an allocation that maximizes the benefits of all the prosumers while satisfying the capacity constraints of the network.\nCerquides, Picard, and Rodr\u0131\u0301guez-Aguilar (2015) proposed a DCOP formalization for this problem, where each prosumer is an agent. Variables are associated with edges of the energy trading network (i, j) and describe the number of units of energy that prosumer i sells to/buys from prosumer j. Thus, two variables (i, j) and (j, i) are associated to each edge of the network. For each prosumer, an energy balance constraint models the utility of a given instantiation of its offers as the sum of the offers associated to the energy traded with each of its neighbor. Line capacity and flow conservation constraints ensure that the energy traded along the transmission lines is within their maximum capacity and is consistent with Kirchhoff\u2019s law.\nSmart Building Devices Scheduling Problems. A smart building is a residential or commercial building that is partially automated through the introduction of smart devices (e.g., smart thermostats, circulator heating, washing machines). Due to the availability of a variety of smart plugs allowing users to intelligently control devices by remotely switching them on and off, smart device scheduling can be executed by householders without the control of a centralized authority. The distributed nature of smart devices within a smart building, and of smart buildings within a neighborhood, as well as data privacy concerns, make this domain suitable to DCOP solutions.\nWithin a smart building, the Smart Environment Configuration Problem (SECP) proposed by Rust, Picard, and Ramparany (2016) is the problem of coordinating several smart devices (light bulbs, roller shutters, luminosity sensors, etc.) whose actions affect the building environment, with the goal of reaching a desired goal state (e.g., a given luminosity level for a room). This resource allocation problem is cast to a DCOP in which devices are described via DCOP agents, each of which controls a single variable describing the devices\u2019 action. The domain of the variables model the possible device\u2019s actions. Finally, the impact of the device\u2019s action onto the building environment is captured by a set of soft constraints. The goal is that of finding an action assignment for the building devices which satisfies the given goals states while minimizing the some cost function (e.g., the energy consumption of each device).\nWithin a smart city, the Smart Home Device Scheduling Problem (SHDS) proposed by Fioretto, Yeoh, and Pontelli (2017a) is the problem of coordinating the schedule of several smart homes\u2019 devices so to minimize the aggregated energy peaks as well as to minimize the users energy bill cost, when adopting a real-time energy price schema. In the SHDS problem, each smart home controls\na set of smart sensors and smart devices. Within each smart home, users establish scheduling rules defining goal states upon certain home\u2019s properties (e.g., reaching a certain room temperature by a given time of the day, or to charge the battery of an electric vehicle of at least some amount during the night). Each device\u2019s action has an associated energy consumption and a cost (which depends on the time of the day in which the action is executed and on its required power). Each user in a smart home attempts to find a schedule for its smart devices that satisfies its scheduling rules while minimizing the energy cost. Additionally, multiple smart homes coordinate their devices\u2019 schedules so to minimize the daily energy peak consumption. Fioretto et. al map the SHDS problem as a DCOP where each agent models a smart home. Agents control multiple variables, each representing a smart device (a sensor or an actuator). Similar to the SECP problem, variables\u2019 domains model the possible actions of a device, and constraints capture the actions\u2019 costs, energy consumption, and user preferences. Additionally, a set of hard constraints is used to model the user\u2019s temporal goals. A dataset for the SHDS problem has been recently released (Kluegel, Iqbal, Fioretto, Yeoh, & Pontelli, 2017)."}, {"heading": "8.8 Supply Chain Management Problems", "text": "The management of large businesses involves the management of the flow of goods from suppliers to customers. This flow of goods is called a supply chain. Supply chains have to be carefully managed to ensure that a sufficient quantity of raw material is available at factories for production and a sufficient quantity of processed goods is available at stores for consumers to buy. Additionally, since goods can be purchased from different producers and sold to different consumers, there is also the need to consider how much to buy/sell the goods and who to buy/sell the goods to. In such an environment, information, decision making and control are inherently decentralized.\nSupply Chain Formation Problems. A supply chain formation problem is the process of determining the participants in a supply chain, who will exchange what, with whom, and the terms of the exchange. Several DCOP-based approaches for this problem have been proposed in the literature (Gaudreault, Frayret, & Pesant, 2009; Penya-Alba, Cerquides, Rodriguez-Aguilar, & Vinyals, 2012; Penya-Alba, Vinyals, Cerquides, & Rodriguez-Aguilar, 2012; Winsper & Chli, 2013; PenyaAlba, Vinyals, Cerquides, & Rodriguez-Aguilar, 2014). In general, they rely on the notion of a Task Dependency Network (TDN) introduced by Walsh and Wellman (2000), which is a graphbased representation to capture dependencies among production processes. A TDN is a bipartite directed acyclic graph representing exchanges, where nodes in the graph correspond to producers and consumers and edges in the graph correspond to feasible exchanges of goods between the producers and consumers. A path from potential producers and consumers defines a feasible supply chain configuration and the goal is to find the feasible configuration that optimizes a particular cost function. A DCOP encoding for this problem models producers and consumers as agents, each of which controls a variable describing the agent\u2019s decision regarding whom to buy from/sell to as well as its associated quantities and costs. Cost functions are associated with each edge of the TDN encoding the willingness of the producer and consumer to trade with each other.\nA dynamic version of the above formalization has been investigated by Chli and Winsper (2015). Such a model allows for the entry and departure of producers and consumers as well as changes in properties of the problem (e.g., prices of goods, production capacity of producers, and consumption requirements of consumers)."}, {"heading": "8.9 Traffic Flow Control Problems", "text": "A challenge for the increase of transportation demand is to enforce traffic flow control using the existing infrastructure, such as traffic lights, loop detectors, and cameras. Coordinating the actions of these individual devices aims at smoothing the traffic flow at the network level. Such coordinated actions often generates coherent traffic control plans faster and more accurately compared to those of a human traffic operator (Van Katwijk, De Schutter, & Hellendoorn, 2009). Due to the distributed nature of such devices, multi-agent solutions are particularly suitable for this class of problems.\nTraffic Light Synchronization Problem. This problem is the problem of finding a synchronization schema for the traffic lights in adjacent intersections that creates green waves, which are waves of vehicles that are traveling at a given speed and are able to cross multiple intersections without stopping at red lights.\nde Oliveira, Bazzan, and Lesser (2005) and Junges and Bazzan (2008) model this problem as a DCOP, where agents represent traffic lights, each controlling one variable that models the coordination direction for the associated traffic signal. Thus, the domain of the variables is given by two possible directions of coordination (north-south/south-north, east-west/west-east). Conflicts that arise when two neighboring traffic signals choose different directions are modeled as constraints. Cost functions are defined to model the number of incoming vehicles in a junction, and the costs are influenced by whether two adjacent agents agree on a direction of coordination or not. The goal is to minimize the global cost. Due to the dynamic nature of the problem, the proposed algorithms fall under the umbrella of Dynamic DCOP algorithms.\nPham, Tawfik, and Taylor (2013) and Brys, Pham, and Taylor (2014) proposed a Probabilistic DCOP-based approach with partial agent knowledge to solve the traffic light synchronization problem in the context where agents have partial information about their cost functions. In particular, the authors argue that traffic patterns may vary during time and, thus, the agents should learn them to update their cost functions. In this context, agents are given a limited amount of time to explore different signal duration intervals, whose associated costs are learned by evaluating the average travel time of the first 100 cars traveling across the agent\u2019s traffic light."}, {"heading": "9. Analyses and Perspectives on DCOPs", "text": "DCOPs have emerged as a popular formalism for distributed reasoning and coordination in multiagent systems. It provides an elegant modeling framework, which offers flexibility in both the agents\u2019 reasoning and coordination strategies. Its ability to support the notions of preferences and constraints makes it suitable to model a variety of multi-agent optimization problems. Preferences\nare a central concept of decision making and arise in a multitude of contexts in multi-agent systems. They are fundamental for the analysis of human choice behavior, and allow agents to express their inclinations through specific actions and behaviors. Constraints have been long studied in centralized systems (Rossi et al., 2006) and have been proved especially practical and efficient for modeling and solving resource allocation and scheduling problems. They are naturally handled within DCOPs and offer a flexible and effective mean to model a variety of complex problems. In addition, DCOPs support several aspects that are crucial in multi-agent systems, such as agent privacy, autonomy in reasoning, and cooperation.\nThe classical DCOP notion is unable to capture important aspects of the problem related with the environment characteristics, such as partial observability, environment evolution, and uncertainty. Therefore, several DCOP model extensions have been recently presented. Each DCOP model imposes distinctive algorithmic requirements, which concern both the agent\u2019s reasoning and cooperation aspects. These requirements, in turn, are strictly related to the characteristics of the problem domain. Due to the performance variability imposed by such requirements, an appropriate selection of the DCOP model and algorithm is essential to obtain desirable performances in realistic application domains."}, {"heading": "9.1 Comparative Overview of DCOP Models", "text": "Classical DCOPs can be used to represent a wide range of MAS applications where agents in a team need to work cooperatively to achieve a single goal in a static, deterministic, and fully observable environment. Exploring the domain structural properties, as well as understanding the requirements of the problem designer, is crucial to design and apply effective DCOP algorithms. A discussion on how to choose an appropriate algorithm based on the characteristic of the application at hand is provided in Section 4.4.\nAsymmetric DCOPs are suitable when constrained agents incur different costs for a joint action, which arise especially in scenarios where privacy is a particular concern, where agents cannot reveal to the other agents the costs associated to their putative actions. Examples of problems that can be suitably modeled as Asymmetric DCOPs are resource allocation problems where agents may have different gains from using the same resource, and where their preferences and constraints regarding usage time slots and durations are expected to be different. Asymmetric DCOPs are particularly attractive to model those domains that can be represented as graphical games (Kearns, Littman, & Singh, 2001), and where constraint reasoning could be actively used to exploit problem structure. In a graphical game, the utilities of each agent are affected exclusively by its neighboring agents. It is important to note that, even though the Asymmetric DCOP model bears similarities with many game-theoretic approaches, these two models are fundamentally different. While gametheoretic agents are self-interested, and their non-cooperative actions lead to a desirable global target, Asymmetric DCOP agents are cooperative and seek to minimize the global cost even at the expense of larger local costs.\nMulti-Objective DCOPs are tailored to represent those classes of distributed problems that cannot be modeled with a single optimization function. The observations above for classical DCOPs\nstill hold for Multi-Objective DCOPs and, in addition, agents cooperate to optimize multiple objectives. A number of MAS applications fall in the category of multi-objective optimization. One example is that of disaster management and coordination problems, where agents coordinate to effectively respond to an emergency scenario (see Section 8.1). Due to memory requirements, which are proportional to the number of objectives and the size of the Pareto set, incomplete strategies seem particularly promising for this research area.\nDynamic DCOPs capture the dynamic behavior of the evolving environment in which agents act. Dynamic environments play a fundamental role in real-wold MAS applications. Virtually all complex MAS applications involve dynamic situations, which may restructure the network topology due to agent movements, or bring additional information to the problem being solved. For example, in a search and rescue operation during disaster management, as the environment evolves over time, new information becomes available about the civilians to be rescued and new agencies may arrive at any time to help conduct rescue operations (see Section 8.1). In a smart grid domain, real-time pricing is commonly enforced. Thus, agent preferences need to be adapted over time, while energy costs are updated (see Section 8.7). Dynamic DCOPs are therefore a modern area that presents an exciting field for groundbreaking research.\nProbabilistic DCOPs extend the classical DCOP model to include the capability of handling uncertain events, allowing DCOP agents to handle a wider range of applications. In particular, Probabilistic DCOPs are suitable to capture those applications characterized by a static environment evolution with exogenous uncertain events (e.g., when the actions of agents on the environment can have different outcomes, based on external, uncertain factors) and, yet, agents have total knowledge of their own actions and of the observable environment and act in a fully cooperative context. The domain of multi-agent task planning and scheduling encompasses diverse problems that require complex models and robust solutions under uncertainty (see Section 8.4). The Probabilistic DCOP model for agents with partial knowledge is suitable to model those applications where agents have no prior knowledge of how the environment reacts to some of the actions. In such a model, the agents are aware of their own actions, which are performed deterministically. However, there is uncertainty in the cost associated to such actions, which is influenced by uncertain events that can be discovered over time. Thus, a common approach in such cases is to resort to sampling strategies, in order to obtain simple approximations of the probability distributions \u2013 in the form of sample realizations of the probabilistic costs. Due to the uncertainty arising in such problems, it is especially appealing to recur to solutions that adopt approaches to decision making under uncertainty, such as minimax, maximin, and regret-based methods.\nAs outlined in Table 3, more research effort is needed to solve DCOP models in which agents act in a combined uncertain and dynamic environment. This is by far the most realistic setting for teams of agents acting within a MAS. More generally, a coordination strategy that can adapt to the situation where the environment or network problem is evolving dynamically and rapidly and where several scenarios require different approaches to coordination, has not yet been studied.\nFinally, Quantified DCOPs model adversarial agents, which are common in many MAS applications. In a Quantified DCOP, universally quantified variables can be considered as the choice of the nature of an adversary. Quantified DCOPs can thus formalize application problems where\na team of agents seeks to limit the effect of the adversarial agents, as well as problems associated to planning under uncertainty. Examples of relevant applications include distributed surveillance planning problems, where sensors in a network need to coordinate their surveillance areas to detect intruders, whose positions are unknown and who are trying to avoid detection. Thus, the sensors try to find a robust plan that can handle different intruding scenarios (see Section 8.5).\nIn terms of complexity, solving classical, Asymmetric, Multi-Objective, and Dynamic DCOPs optimally is NP-hard. In contrast, the Probabilistic DCOP extension to include the capability of handling uncertain events and the Quantified DCOP extension to model adversarial agents comes at a cost of increasing complexity. Both Probabilistic DCOPs and Quantified DCOPs are PSPACEhard. Table 6 summarizes the complexity of the DCOP models discussed in this survey."}, {"heading": "9.2 Algorithms and Theoretical Analysis", "text": "Despite the fact that classical DCOPs have reached a sufficient level of maturity from the algorithmic perspective, most of the other proposed formalisms fall short on both algorithmic and theoretical foundations. The proposed algorithms mostly extend classical DCOP algorithms and, therefore, they result in similar performance. Investigating strategies that are based on different backgrounds could help propel the evolution of this area.\nInter-disciplinary Research: In particular, further investigations on relating DCOPs to the areas of game theory and decision theory are necessary. Similar to the work of Chapman, Rogers, and Jennings (2008), where the authors study relationship between DCOPs and potential games, analyzing the relationship of DCOPs to auctions mechanisms could shed light on how to effectively address coordination and reasoning strategies in DCOP with partially cooperative agents. Another direction is to relate DCOP with machine learning techniques. For instance, as outlined by Kumar and Zilberstein (2011) and Ghosh, Kumar, and Varakantham (2015), one can use inference-based algorithms, such as expectation-maximization, and convex optimization machinery to develop efficient message-passing algorithm for solving large DCOPs. Additionally, merging insights from decision theory, such as handling partial observability, with the inherent DCOP ability of naturally exploiting problem structure could result in improved performance and/or refined models (Nair, Varakantham, Tambe, & Yokoo, 2005; Hoang et al., 2016, 2017).\nAnytime Mechanisms: Due to the complexity of the DCOP models, the study of incomplete approaches to solve large DCOPs whose agents may act in a dynamic and/or uncertain environment\nseems particularly suitable. Within the current incomplete methods proposed, a considerable effort has been employed in developing anytime algorithms (Zilberstein, 1996). Anytime algorithms can return a valid solution even if the DCOP agents are interrupted at any time before the algorithm terminates, and are expected to seek for solutions of increasing quality as they keep running (Zivan, Okamoto, & Peled, 2014). In addition to the anytime mechanism, which constrains the problem resolution within a particular time requirement, any-space algorithms have been proposed to limit the amount of space needed to an agent during problem resolution (Petcu & Faltings, 2007a; Yeoh et al., 2009b, 2011; Gutierrez et al., 2011). Similar to these mechanisms, we see an urge for investigating algorithms that allow problem resolution within any particular network load restriction. For instance, in a congested network agents might need to exchange smaller messages, or to communicate less frequently and/or with a restricted set of neighbors. Situations like these may arise in problems where a multitude of interconnected systems share the same medium, and thus limited bandwidth and interference may cause unsuitable delays. This context may be exacerbated with the advent of concepts such as the Internet of the Things, which expects to see million of connected devices, possibly sharing several mediums (Atzori, Iera, & Morabito, 2010; Chen, 2012; Miorandi, Sicari, De Pellegrini, & Chlamtac, 2012). Thus, orthogonal to the direction pursed by anytime and any-space algorithms, we envision the development of any-communication procedures.\nCoordination and Cooperation Study: Another open question is related to agent coordination. It has been observed that simple coordination strategies give good results in extremes (high or low) agent workload environments. However, at intermediate workload levels, such strategies lose their effectiveness, and more complex coordination strategies are necessary (Lesser, Decker, Wagner, Carver, Garvey, Horling, Neiman, Podorozhny, Prasad, Raja, et al., 2004; Zhang & Lesser, 2013). For example, Zhang and Lesser (2013) study agent learning, where coordination is driven by a dynamic decomposition of a DCOP, each solving smaller independent subproblems. Their proposal effectively produced near-optimal agent policies for learning and significantly reduced the amount of communication. These empirical observations suggest the existence of phase transition behaviors occurring at the level of agent coordination. A formal understanding of these phenomena, which looks at the environment, agents\u2019 local reasoning strategies, their assumptions on other agents states, as well as team coordination, could help researchers to understand the inherent complexity of coordination problems. Such a formal framework could be useful to build new and more efficient coordination strategies for a wide variety of multi-agent applications, perhaps resembling the way studies of phase transitions of NP-hard problems (Monasson, Zecchina, Kirkpatrick, Selman, & Troyansky, 1999) led to the understanding of problem complexity and creation of effective heuristics and search strategies."}, {"heading": "9.3 Evaluation Metrics and Benchmarks", "text": "DCOP Solving Assumptions Modeling many real-world problems as DCOPs often require each agent to solve large complex subproblems, each requiring many variables and constraints. A limitation of most DCOP algorithms is the assumption that each agent controls exactly one variable, and that all constraints are binary. Such assumptions simplify the algorithm organization and pre-\nsentation. To operate under this assumption, reformulation techniques are commonly adpoted to transform a general DCOP into one where each agent controls exclusively one variable. There are two commonly used reformulation techniques (Burke & Brown, 2006; Yokoo, 2001): (i) Compilation (or complex variables) where each agent creates a new pseudo-variable whose domain is the Cartesian product of the domains of all variables of the agent; and (ii) Decomposition (or virtual agents) where each agent creates a pseudo-agent for each of its variables. While both techniques are relatively simple, they can be inefficient. In compilation, the memory requirement for each agent grows exponentially with the number of variables that it controls. In decomposition, the DCOP algorithms will treat two pseudo-agents as independent entities, resulting in unnecessary computation and communication costs. A more realistic view is to allow each agent to solve its local subproblem (in a centralized fashion) since it is independent of the subproblems of other agents. The agent\u2019s subproblem resolution can then explore techniques from centralized reasoning, such as constraint optimization problems, linear programming, and graphical models. One can even exploit novel hardware platforms, such as graphical processing units (GPUs) to parallelize such solvers (Fioretto et al., 2016a; Fioretto, Le, Yeoh, Pontelli, & Son, 2015; Bistaffa, Farinelli, & Bombieri, 2014).\nResearchers have proposed re-designed versions of existing algorithms to be able to handle multi-variables agents problems (Davin & Modi, 2006; Khanna, Sattar, Hansen, & Stantic, 2009; Portway & Durfee, 2010). Additionally, recently, Grinshpoun (2015) and Fioretto, Yeoh, and Pontelli (2016b) have proposed general DCOP problem decompositions which are able to solve problems where agents control multiple variables.\nDCOP Modeling Language: Despite the wide applicability of the DCOP model, unfortunately, there is no general language being used to formally specify a DCOP. While there are several DCOP simulators, that include implementations of various DCOP algorithms using a common language specification (Sultanik, Modi, & Regli, 2007; Le\u0301aute\u0301, Ottens, & Szymanek, 2009; Wahbi, Ezzahir, Bessiere, & Bouyakhf, 2011), by and large, most stand-alone algorithms specify DCOPs in an ad-hoc manner. As a result, it is often inconvenient to experimentally compare different algorithms. More importantly, the great majority of such languages requires constraints values to be specified explicitly. Such requirement makes unpractical to convert problems which are naturally defined as mathematical optimization problems (such as Mixed Integer Programs) into an explicit form which specifies the utilities for each value combination of the variables in the scope of the problem constraints. The adoption of a common distributed constraint modeling language, which allows one to express constraints as standard algebraic or logic expressions, may be beneficial for developing standard benchmarks. Additionally, it would provide a tool for researchers outside the AI communities to model and test the applicability of new problems, extending the applicability of DCOPs to new areas. For instance, within the CP community, the adoption of the MiniZinc language (Nethercote, Stuckey, Becket, Brand, Duck, & Tack, 2007) to model CSPs and COPs has gained wide traction, and it is becoming a modeling choice even outside the CP community.\nDCOP Simulators: DCOP simulators are multi-agent software tools that simulate the execution of DCOP algorithms. These framework are a valid resource both for researchers within and out-\nside the MAS community, whom can implement, test, and compare different DCOP models or algorithms. There are currently several MAS simulators which have been adopted by researchers to develop and compare DCOP algorithms: \u2022 DisChoco (Wahbi et al., 2011) is a Java open-source framework for simulating DCOP algorithms\nwhere agents are executed asynchronously (each in a separate execution thread) and communication is implemented via messages passing. The simulator includes some problem generators and the possibility to take into account message loss, message corruption, and message delay. It supports models in which agents control multiple variables and problems with n-ary constraints. Additionally, it has the ability to be executed in a distributed environment. \u2022 Frodo (Le\u0301aute\u0301 et al., 2009) is a Java open-source framework for DCOP which implements sev-\neral complete and incomplete classic DCOP algorithms, including DPOP and some of it variants, Max-Sum, MGM, and DSA. It can be executed either in a synchronous or in an asynchronous mode. It provides a graphical user interface (GUI) to visualize the problem structure and algorithm execution, and it includes some problem generators for benchmarking. It supports problems with n-ary constraints and models in which agents need to control the assignment of several variables. Additionally, it has the ability to be executed in a distributed environment. Finally, in addition to simulating DCOP algorithms in a centralized environment, it provides support for executing agents in a distributed environment. \u2022 AgentZero (Lutati, Gontmakher, Lando, Netzer, Meisels, & Grubshtein, 2014) is a Java open-\nsource framework for various MAS applications. It provides distributed runtime environment, which can be used to simulate both synchronous and asynchronous algorithms. AgentZero offers several high-level APIs which allow to easily handle complex tasks, such as the construction of a pseudo-tree and the evaluation of assignments. Additionally, it provides some built-in performance measures and some visualization tools which can be especially useful to debug distributed programs. It support the notion of message delays, message corruptions, and message losses. While it does not supports models in which agents control multiple variables, it can deal with n-ary constraints.\nIn addition to using DCOP simulators, researchers have developed and deployed DCOP algorithms using more general MAS platforms. In particular, the Java Agent DEvelopment Framework (JADE) (Bellifemine, Caire, & Greenwood, 2007) is a multi-agent platform that is compliant with the FIPA specifications.14 It includes a runtime environment where agents act, a set of APIs to define agents\u2019 behaviors, and a suite of graphical tools that can be used to monitor the agents activity. JADE agents\u2019 messages are specified by the ACL language, defined by the FIPA international standard for agent interoperability. JADE allows the deployment of agents on several environments, including mobile and wireless environments and fault-tolerant platforms.\nWe provide a summary of the simulators\u2019 features in Table 7, in which we describe whether the simulator execution mode supports synchronous or asynchronous algorithms; whether it provides visualization tools; the type of implementation of messages passing simulation (i.e., using pointers, deep copying the message structure into the receiving agent\u2019s message queue, etc.); if the\n14. http://www.fipa.org/\nframework provides support to simulate/handle message delays, message corruptions, and message losses; if it supports DCOP models in which agents control multiple variables and n-ary constraints; if the framework provides tools for debugging the algorithms; the programming language in which the simulator is implemented; whether it provides documentation; and whether it provides support for deployment. For a comprehensive review of multi-agent platforms, we refer the interested reader to the survey by Kravari and Bassiliades (2015).\nThe development of DCOP simulators have provided researchers with useful tools to develop new DCOP algorithms and to facilitate comparison of existing DCOP algorithms. However, all the DCOP simulators described above focus on the classical DCOP model, while Dynamic and Probabilistic DCOP simulators have received little attention. Thus, we see the need of developing high-fidelity simulators for Dynamic DCOPs, which simulate how the environment can vary over time, as well as for Probabilistic DCOPs, which simulate the stochastic nature of exogenous events.\nEvaluation and Metrics: Another open question in this research area concerns the definition of a systematic process to evaluate and compare the DCOP algorithms. There are multiple metrics that can be used to measure the runtime of an algorithm, such as the number of non-concurrent constraint checks (NCCCs) (Meisels, Kaplansky, Razgon, & Zivan, 2002), the simulated runtime (Sultanik, Lass, & Regli, 2008), and the number of cycles (Lynch, 1996). However, there is no consensus on a standard metric. Such issues, combined with absence of a general DCOP language, makes it inconvenient to experimentally compare different algorithms. In addition, new proposed algorithms are, in general, evaluated on arbitrary benchmarks, some inherited from the CSP literature, some other from approximations of real-world problems. To cope with these issues, it would be useful to develop a benchmark repository, perhaps by taking inspiration from the efforts made by the CP community with CSPLib,15 a library of test problems for constraint solvers, or by the planning community with their international planning competitions.16\n15. http://www.csplib.org/ 16. http://icaps-conference.org/index.php/Main/Competitions"}, {"heading": "10. Conclusions", "text": "DCOPs have emerged as a popular formalism for distributed reasoning and coordination in multiagent systems. Due to its limitation to support complex, real-time, and uncertain environment, researchers have introduced several model extensions to handle dynamic and uncertain environments, as well as different levels of cooperation among the agents.\nWhile DCOPs\u2019 theoretical foundation and algorithmic frameworks have matured significantly over the past decade, their applicability to realistic domains is lagging behind. This survey aims at linking the DCOP theoretical framework and solving strategies with a set of potential applications where its applicability is having or may have a significant impact.\nIn this survey, we provided an analysis of the recent advances made by the AAMAS community within the DCOP framework and propose a categorization based on agent characteristics, environment properties, and type of teamwork adopted. Within the proposed classification, we (1) presented a review of the characteristics of the different algorithmic solutions; (2) discussed a number of application domains that can be naturally modeled within each DCOP framework; and (3) identified some potential directions for future work with regards to agent coordination, algorithm scalability, modeling languages, and evaluation criteria of DCOP models and algorithms."}, {"heading": "Acknowledgments", "text": "The research in this paper has been partially supported by NSF grants DBI-1458595, CBET1401639, HRD-1345232 and DGE-0947465."}], "references": [{"title": "Learning algorithms for Markov decision processes with average cost", "author": ["J. Abounadi", "D. Bertsekas", "V. Borkar"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Abounadi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Abounadi et al\\.", "year": 2001}, {"title": "Near-optimal decentralized power supply restoration in smart grids", "author": ["P. Agrawal", "A. Kumar", "P. Varakantham"], "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "The generalized distributive law", "author": ["S.M. Aji", "R.J. McEliece"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Aji and McEliece,? \\Q2000\\E", "shortCiteRegEx": "Aji and McEliece", "year": 2000}, {"title": "Wireless sensor networks: a survey", "author": ["I.F. Akyildiz", "W. Su", "Y. Sankarasubramaniam", "E. Cayirci"], "venue": "Computer networks,", "citeRegEx": "Akyildiz et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Akyildiz et al\\.", "year": 2002}, {"title": "Dynamic multi-agent task allocation with spatial and temporal constraints", "author": ["S. Amador", "S. Okamoto", "R. Zivan"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Amador et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Amador et al\\.", "year": 2014}, {"title": "Decentralized control of partially observable markov decision processes", "author": ["C. Amato", "G. Chowdhary", "A. Geramifard", "N.K. Ure", "M.J. Kochenderfer"], "venue": "In Proceedings of the Annual Conference on Decision and Control (CDC),", "citeRegEx": "Amato et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2013}, {"title": "Modeling the management of water resources systems using Multi-Objective DCOPs", "author": ["F. Amigoni", "A. Castelletti", "M. Giuliani"], "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Amigoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amigoni et al\\.", "year": 2015}, {"title": "Principles of Constraint Programming", "author": ["K. Apt"], "venue": "Cambridge University Press.", "citeRegEx": "Apt,? 2003", "shortCiteRegEx": "Apt", "year": 2003}, {"title": "Distributed constraint optimisation for flexible network management", "author": ["D. Athanasiadis", "I. Kockar", "S. McArthur"], "venue": "In Innovative Smart Grid Technologies Europe (ISGT EUROPE),", "citeRegEx": "Athanasiadis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Athanasiadis et al\\.", "year": 2013}, {"title": "Coordination for uncertain outcomes using distributed neighbor exchange", "author": ["J. Atlas", "K. Decker"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Atlas and Decker,? \\Q2010\\E", "shortCiteRegEx": "Atlas and Decker", "year": 2010}, {"title": "The internet of things: A survey", "author": ["L. Atzori", "A. Iera", "G. Morabito"], "venue": "Computer networks,", "citeRegEx": "Atzori et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Atzori et al\\.", "year": 2010}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Cooperative problem solving against adversary: Quantified distributed constraint satisfaction problem", "author": ["S. Baba", "A. Iwasaki", "M. Yokoo", "M.C. Silaghi", "K. Hirayama", "T. Matsui"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Baba et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baba et al\\.", "year": 2010}, {"title": "Real-time solving of quantified csps based on montecarlo game tree search", "author": ["S. Baba", "Y. Joe", "A. Iwasaki", "M. Yokoo"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Baba et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baba et al\\.", "year": 2011}, {"title": "Developing multi-agent systems with JADE", "author": ["F.L. Bellifemine", "G. Caire", "D. Greenwood"], "venue": null, "citeRegEx": "Bellifemine et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bellifemine et al\\.", "year": 2007}, {"title": "Partitioning procedures for solving mixed-variables programming problems", "author": ["J.F. Benders"], "venue": "Numerische mathematik, 4(1), 238\u2013252.", "citeRegEx": "Benders,? 1962", "shortCiteRegEx": "Benders", "year": 1962}, {"title": "Quantified constraint optimization", "author": ["M. Benedetti", "A. Lallouet", "J. Vautard"], "venue": "In Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "Benedetti et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Benedetti et al\\.", "year": 2008}, {"title": "The complexity of decentralized control of markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of operations research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Global constraints in distributed constraint satisfaction and optimization", "author": ["C. Bessiere", "I. Brito", "P. Gutierrez", "P. Meseguer"], "venue": "Computer Journal,", "citeRegEx": "Bessiere et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bessiere et al\\.", "year": 2014}, {"title": "Including soft global constraints in DCOPs", "author": ["C. Bessiere", "P. Gutierrez", "P. Meseguer"], "venue": "In Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "Bessiere et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bessiere et al\\.", "year": 2012}, {"title": "SBDO: A new robust approach to dynamic distributed constraint optimisation", "author": ["G. Billiau", "C.F. Chang", "A. Ghose"], "venue": "In Proceedings of the Principles and Practice of Multi-Agent Systems (PRIMA),", "citeRegEx": "Billiau et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Billiau et al\\.", "year": 2012}, {"title": "Using distributed agents for patient scheduling", "author": ["G. Billiau", "C.F. Chang", "A. Ghose", "A.A. Miller"], "venue": "In Proceedings of the Principles and Practice of Multi-Agent Systems (PRIMA),", "citeRegEx": "Billiau et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Billiau et al\\.", "year": 2012}, {"title": "Support-based distributed optimisation: An approach to radiotherapy scheduling", "author": ["G. Billiau", "C.F. Chang", "A. Ghose", "A. Miller"], "venue": "In Electronic Healthcare,", "citeRegEx": "Billiau et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Billiau et al\\.", "year": 2012}, {"title": "Fun and games, a text on game theory", "author": ["K. Binmore"], "venue": "DC Heath and Company.", "citeRegEx": "Binmore,? 1992", "shortCiteRegEx": "Binmore", "year": 1992}, {"title": "Optimising memory management for belief propagation in junction trees using gpgpus", "author": ["F. Bistaffa", "A. Farinelli", "N. Bombieri"], "venue": "In Proceeding of the International Conference on Parallel and Distributed Systems (ICPADS),", "citeRegEx": "Bistaffa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bistaffa et al\\.", "year": 2014}, {"title": "Multiply-constrained distributed constraint optimization", "author": ["E. Bowring", "M. Tambe", "M. Yokoo"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Bowring et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bowring et al\\.", "year": 2006}, {"title": "Distributed constraint satisfaction with partially known", "author": ["I. Brito", "A. Meisels", "P. Meseguer", "R. Zivan"], "venue": "constraints. Constraints,", "citeRegEx": "Brito et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brito et al\\.", "year": 2009}, {"title": "Distributed learning and multi-objectivity in traffic light control", "author": ["T. Brys", "T.T. Pham", "M.E. Taylor"], "venue": "Connection Science,", "citeRegEx": "Brys et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brys et al\\.", "year": 2014}, {"title": "Efficiently handling complex local problems in distributed constraint optimisation", "author": ["D. Burke", "K. Brown"], "venue": "In Proceedings of the European Conference on Artificial Intelligence (ECAI),", "citeRegEx": "Burke and Brown,? \\Q2006\\E", "shortCiteRegEx": "Burke and Brown", "year": 2006}, {"title": "Intelligent systems demonstration: disaster evacuation support", "author": ["C.J. Carpenter", "C.J. Dugan", "J.B. Kopena", "R.N. Lass", "G. Naik", "D.N. Nguyen", "E. Sultanik", "P.J. Modi", "W.C. Regli"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Carpenter et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Carpenter et al\\.", "year": 2007}, {"title": "Designing a marketplace for the trading and distribution of energy in the smart grid", "author": ["J. Cerquides", "G. Picard", "J.A. Rodr\u0131\u0301guez-Aguilar"], "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Cerquides et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cerquides et al\\.", "year": 2015}, {"title": "Distributed algorithms for sleep scheduling in wireless sensor networks", "author": ["S. Chachra", "M. Marefat"], "venue": "In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Chachra and Marefat,? \\Q2006\\E", "shortCiteRegEx": "Chachra and Marefat", "year": 2006}, {"title": "A parameterisation of algorithms for distributed constraint optimisation via potential games", "author": ["A. Chapman", "A. Rogers", "N. Jennings"], "venue": "In International Workshop on Distributed Constraint Reasoning (DCR),", "citeRegEx": "Chapman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chapman et al\\.", "year": 2008}, {"title": "No-commitment branch and bound search for distributed constraint optimization", "author": ["A. Chechetka", "K. Sycara"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Chechetka and Sycara,? \\Q2006\\E", "shortCiteRegEx": "Chechetka and Sycara", "year": 2006}, {"title": "Challenges and opportunities of internet of things", "author": ["Chen", "Y.-K."], "venue": "Design Automation Conference (ASP-DAC), 2012 17th Asia and South Pacific, pp. 383\u2013388. IEEE.", "citeRegEx": "Chen and Y..K.,? 2012", "shortCiteRegEx": "Chen and Y..K.", "year": 2012}, {"title": "Using the Max-Sum algorithm for supply chain emergence in dynamic multiunit environments", "author": ["M. Chli", "M. Winsper"], "venue": "IEEE T. Systems, Man, and Cybernetics: Systems,", "citeRegEx": "Chli and Winsper,? \\Q2015\\E", "shortCiteRegEx": "Chli and Winsper", "year": 2015}, {"title": "A multi-agent based optimised server selection scheme for SOC in pervasive environment", "author": ["B. Choudhury", "P. Dey", "A. Dutta", "S. Choudhury"], "venue": "In Advances in Practical Applications of Heterogeneous Multi-Agent Systems. The PAAMS Collection,", "citeRegEx": "Choudhury et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choudhury et al\\.", "year": 2014}, {"title": "A distributed constraint optimization approach to wireless network optimization", "author": ["H. Choxi", "P. Modi"], "venue": "In Proceedings of the AAAI-07 Workshop on Configuration,", "citeRegEx": "Choxi and Modi,? \\Q2007\\E", "shortCiteRegEx": "Choxi and Modi", "year": 2007}, {"title": "The use of constraint programming for the autonomous management of power flows", "author": ["E. Davidson", "M. Dolan", "S. McArthur", "G. Ault"], "venue": "In Intelligent System Applications to Power Systems,", "citeRegEx": "Davidson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Davidson et al\\.", "year": 2009}, {"title": "Hierarchical variable ordering for multiagent agreement problems", "author": ["J. Davin", "P. Modi"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Davin and Modi,? \\Q2006\\E", "shortCiteRegEx": "Davin and Modi", "year": 2006}, {"title": "Using cooperative mediation to coordinate traffic lights: a case study", "author": ["D. de Oliveira", "A.L. Bazzan", "V. Lesser"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Oliveira et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Oliveira et al\\.", "year": 2005}, {"title": "Bounded decentralised coordination over multiple objectives", "author": ["F.M. Delle Fave", "R. Stranders", "A. Rogers", "N.R. Jennings"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Fave et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fave et al\\.", "year": 2011}, {"title": "Self-stabilization in spite of distributed control", "author": ["E.W. Dijkstra"], "venue": "Communication of the ACM, 17(11), 643\u2013644.", "citeRegEx": "Dijkstra,? 1974", "shortCiteRegEx": "Dijkstra", "year": 1974}, {"title": "Superstabilizing protocols for dynamic distributed systems", "author": ["S. Dolev", "T. Herman"], "venue": "In Proceedings of the fourteenth annual ACM symposium on Principles of distributed computing,", "citeRegEx": "Dolev and Herman,? \\Q1995\\E", "shortCiteRegEx": "Dolev and Herman", "year": 1995}, {"title": "Decentralised coordination of low-power embedded devices using the Max-Sum algorithm", "author": ["A. Farinelli", "A. Rogers", "A. Petcu", "N. Jennings"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Farinelli et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Farinelli et al\\.", "year": 2008}, {"title": "Improving DPOP with branch consistency for solving distributed constraint optimization problems", "author": ["F. Fioretto", "T. Le", "W. Yeoh", "E. Pontelli", "T.C. Son"], "venue": "In Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "Fioretto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fioretto et al\\.", "year": 2014}, {"title": "Exploiting GPUs in solving (distributed) constraint optimization problems with dynamic programming", "author": ["F. Fioretto", "T. Le", "W. Yeoh", "E. Pontelli", "T.C. Son"], "venue": "In Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "Fioretto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fioretto et al\\.", "year": 2015}, {"title": "A dynamic programming-based MCMC framework for solving DCOPs with GPUs", "author": ["F. Fioretto", "W. Yeoh", "E. Pontelli"], "venue": "In Proceedings of Principles and Practice of Constraint Programming (CP),", "citeRegEx": "Fioretto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fioretto et al\\.", "year": 2016}, {"title": "Multi-variable agents decomposition for DCOPs", "author": ["F. Fioretto", "W. Yeoh", "E. Pontelli"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Fioretto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fioretto et al\\.", "year": 2016}, {"title": "A multiagent system approach to scheduling devices in smart homes", "author": ["F. Fioretto", "W. Yeoh", "E. Pontelli"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Fioretto et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Fioretto et al\\.", "year": 2017}, {"title": "A DCOP approach to the economic dispatch with demand response", "author": ["F. Fioretto", "W. Yeoh", "E. Pontelli", "Y. Ma", "S. Ranade"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Fioretto et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Fioretto et al\\.", "year": 2017}, {"title": "Multi-agent meeting scheduling: Preliminary experimental results", "author": ["L. Garrido", "K. Sycara"], "venue": "In Proceedings of the Second International Conference on Multiagent Systems,", "citeRegEx": "Garrido and Sycara,? \\Q1996\\E", "shortCiteRegEx": "Garrido and Sycara", "year": 1996}, {"title": "Distributed search for supply chain coordination", "author": ["J. Gaudreault", "Frayret", "J.-M", "G. Pesant"], "venue": "Computers in Industry,", "citeRegEx": "Gaudreault et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gaudreault et al\\.", "year": 2009}, {"title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "Asynchronous Forward-Bounding for distributed COPs", "author": ["A. Gershman", "A. Meisels", "R. Zivan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gershman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2009}, {"title": "Probabilistic inference based message-passing for resource constrained DCOPs", "author": ["S. Ghosh", "A. Kumar", "P. Varakantham"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Ghosh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "Algorithms for solving production-scheduling problems", "author": ["B. Giffler", "G.L. Thompson"], "venue": "Operations research,", "citeRegEx": "Giffler and Thompson,? \\Q1960\\E", "shortCiteRegEx": "Giffler and Thompson", "year": 1960}, {"title": "Multiagent systems and distributed constraint reasoning for regulatory mechanism design in water management", "author": ["M. Giuliani", "A. Castelletti", "F. Amigoni", "X. Cai"], "venue": "Journal of Water Resources Planning and Management,", "citeRegEx": "Giuliani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Giuliani et al\\.", "year": 2014}, {"title": "Approximability and hardness in multiobjective optimization", "author": ["C. Gla\u00dfer", "C. Reitwie\u00dfner", "H. Schmitz", "M. Witek"], "venue": "In Programs, Proofs,", "citeRegEx": "Gla\u00dfer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gla\u00dfer et al\\.", "year": 2010}, {"title": "SSDPOP: Improving the privacy of DCOP with secret sharing", "author": ["R. Greenstadt", "B. Grosz", "M. Smith"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Greenstadt et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Greenstadt et al\\.", "year": 2007}, {"title": "Clustering variables by their agents", "author": ["T. Grinshpoun"], "venue": "Proceedings of the International Joint Conferences on Web Intelligence and Intelligent Agent Technologies (WI-IAT), Vol. 2, pp. 250\u2013256. IEEE.", "citeRegEx": "Grinshpoun,? 2015", "shortCiteRegEx": "Grinshpoun", "year": 2015}, {"title": "Asymmetric distributed constraint optimization problems", "author": ["T. Grinshpoun", "A. Grubshtein", "R. Zivan", "A. Netzer", "A. Meisels"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Grinshpoun et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grinshpoun et al\\.", "year": 2013}, {"title": "Completeness and performance of the apo algorithm", "author": ["T. Grinshpoun", "A. Meisels"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Grinshpoun and Meisels,? \\Q2008\\E", "shortCiteRegEx": "Grinshpoun and Meisels", "year": 2008}, {"title": "Solving customer-driven microgrid optimization problems as DCOPs", "author": ["S. Gupta", "P. Jain", "W. Yeoh", "S. Ranade", "E. Pontelli"], "venue": "In Proc. of the Distributed Constraint Reasoning Workshop,", "citeRegEx": "Gupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2013}, {"title": "Modeling microgrid islanding problems as DCOPs", "author": ["S. Gupta", "W. Yeoh", "E. Pontelli", "P. Jain", "S. Ranade"], "venue": "In North American Power Symposium (NAPS),", "citeRegEx": "Gupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2013}, {"title": "Maintaining soft arc consistencies in BnBADOPT during search", "author": ["P. Gutierrez", "J. Lee", "K.M. Lei", "T. Mak", "P. Meseguer"], "venue": "In Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "Gutierrez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gutierrez et al\\.", "year": 2013}, {"title": "Removing redundant messages in n-ary BnB-ADOPT", "author": ["P. Gutierrez", "P. Meseguer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gutierrez and Meseguer,? \\Q2012\\E", "shortCiteRegEx": "Gutierrez and Meseguer", "year": 2012}, {"title": "Generalizing ADOPT and BnB-ADOPT", "author": ["P. Gutierrez", "P. Meseguer", "W. Yeoh"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Gutierrez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gutierrez et al\\.", "year": 2011}, {"title": "Distributed constraint optimization for medical appointment scheduling", "author": ["M. Hannebauer", "S. M\u00fcller"], "venue": "In Proceedings of the fifth international conference on Autonomous agents,", "citeRegEx": "Hannebauer and M\u00fcller,? \\Q2001\\E", "shortCiteRegEx": "Hannebauer and M\u00fcller", "year": 2001}, {"title": "Support-based distributed search: a new approach for multiagent constraint processing", "author": ["P. Harvey", "C.F. Chang", "A. Ghose"], "venue": "In Argumentation in Multi-Agent Systems,", "citeRegEx": "Harvey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Harvey et al\\.", "year": 2007}, {"title": "Constraint-based local search", "author": ["P.V. Hentenryck", "L. Michel"], "venue": null, "citeRegEx": "Hentenryck and Michel,? \\Q2009\\E", "shortCiteRegEx": "Hentenryck and Michel", "year": 2009}, {"title": "Distributed partial constraint satisfaction problem", "author": ["K. Hirayama", "M. Yokoo"], "venue": "In Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "Hirayama and Yokoo,? \\Q1997\\E", "shortCiteRegEx": "Hirayama and Yokoo", "year": 1997}, {"title": "Proactive dynamic distributed constraint optimization", "author": ["K.D. Hoang", "F. Fioretto", "P. Hou", "M. Yokoo", "W. Yeoh", "R. Zivan"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Hoang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hoang et al\\.", "year": 2016}, {"title": "Infinite-horizon proactive dynamic dcops", "author": ["K.D. Hoang", "P. Hou", "F. Fioretto", "M. Yokoo", "W. Yeoh", "R. Zivan"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Hoang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hoang et al\\.", "year": 2017}, {"title": "Regionalizing global optimization algorithms to improve the operation of large ad hoc networks", "author": ["D. Hollos", "H. Karl", "A. Wolisz"], "venue": "In Proceedings of the Wireless Communications and Networking Conference (WCNC),", "citeRegEx": "Hollos et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hollos et al\\.", "year": 2004}, {"title": "DCOPs meet the real world: Exploring unknown reward matrices with applications to mobile sensor networks", "author": ["M. Jain", "M.E. Taylor", "M. Tambe", "M. Yokoo"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Jain et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2009}, {"title": "Optimum operation of a customer-driven microgrid: A comprehensive approach", "author": ["P. Jain", "S.J. Ranade", "S. Gupta", "E. Pontelli"], "venue": "In Power Electronics, Drives and Energy Systems (PEDES),", "citeRegEx": "Jain et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2012}, {"title": "Agent-based meeting scheduling: A design and implementation", "author": ["N.R. Jennings", "A. Jackson"], "venue": "Electronics letters,", "citeRegEx": "Jennings and Jackson,? \\Q1995\\E", "shortCiteRegEx": "Jennings and Jackson", "year": 1995}, {"title": "A distributed application component placement approach for cloud computing environment", "author": ["Z. Jin", "J. Cao", "M. Li"], "venue": "In Proceedings of the International Conference on Dependable, Autonomic and Secure Computing (DASC),", "citeRegEx": "Jin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2011}, {"title": "Evaluating the performance of DCOP algorithms in a real world, dynamic problem", "author": ["R. Junges", "A.L. Bazzan"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Junges and Bazzan,? \\Q2008\\E", "shortCiteRegEx": "Junges and Bazzan", "year": 2008}, {"title": "Graphical models for game theory", "author": ["M. Kearns", "M.L. Littman", "S. Singh"], "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Kearns et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2001}, {"title": "An efficient algorithm for solving dynamic complex DCOP problems", "author": ["S. Khanna", "A. Sattar", "D. Hansen", "B. Stantic"], "venue": "In Proceedings of the International Joint Conferences on Web Intelligence and Intelligent Agent Technologies (WI-IAT),", "citeRegEx": "Khanna et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Khanna et al\\.", "year": 2009}, {"title": "Asynchronous algorithms for approximate distributed constraint optimization with quality bounds", "author": ["C. Kiekintveld", "Z. Yin", "A. Kumar", "M. Tambe"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Kiekintveld et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kiekintveld et al\\.", "year": 2010}, {"title": "Effective disaster evacuation by solving the distributed constraint optimization problem", "author": ["K. Kinoshita", "K. Iizuka", "Y. Iizuka"], "venue": "In Proceedings of the International Conference on Advanced Applied Informatics (IIAIAAI),", "citeRegEx": "Kinoshita et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kinoshita et al\\.", "year": 2013}, {"title": "A realistic dataset for the smart home device scheduling problem for DCOPs", "author": ["W. Kluegel", "M.A. Iqbal", "F. Fioretto", "W. Yeoh", "E. Pontelli"], "venue": "In Proceeding of the International Workshop on Optimisation in Multi-Agent Systems (OPTMAS)", "citeRegEx": "Kluegel et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kluegel et al\\.", "year": 2017}, {"title": "Distributed coordination of first responders", "author": ["J.B. Kopena", "E.A. Sultanik", "R.N. Lass", "D.N. Nguyen", "C.J. Dugan", "P.J. Modi", "W.C. Regli"], "venue": "Internet Computing,", "citeRegEx": "Kopena et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kopena et al\\.", "year": 2008}, {"title": "Negotiation and cooperation in multi-agent environments", "author": ["S. Kraus"], "venue": "Artificial Intelligence, 94(1), 79\u201397.", "citeRegEx": "Kraus,? 1997", "shortCiteRegEx": "Kraus", "year": 1997}, {"title": "A survey of agent platforms", "author": ["K. Kravari", "N. Bassiliades"], "venue": "Journal of Artificial Societies and Social Simulation,", "citeRegEx": "Kravari and Bassiliades,? \\Q2015\\E", "shortCiteRegEx": "Kravari and Bassiliades", "year": 2015}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "Loeliger", "H.-A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "Distributed constraint optimization with structured resource constraints", "author": ["A. Kumar", "B. Faltings", "A. Petcu"], "venue": "In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "H-DPOP: Using hard constraints for search space pruning in DCOP", "author": ["A. Kumar", "A. Petcu", "B. Faltings"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Kumar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2008}, {"title": "Message-passing algorithms for quadratic programming formulations of map estimation", "author": ["A. Kumar", "S. Zilberstein"], "venue": "In Proceedings of the Twenty-Seventh Conference Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Kumar and Zilberstein,? \\Q2011\\E", "shortCiteRegEx": "Kumar and Zilberstein", "year": 2011}, {"title": "Ultra-weak solutions and consistency enforcement in minimax weighted constraint satisfaction", "author": ["A. Lallouet", "J.H.M. Lee", "T.W.K. Mak", "J. Yip"], "venue": null, "citeRegEx": "Lallouet et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lallouet et al\\.", "year": 2015}, {"title": "Node and arc consistency in weighted CSP", "author": ["J. Larrosa"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 48\u201353.", "citeRegEx": "Larrosa,? 2002", "shortCiteRegEx": "Larrosa", "year": 2002}, {"title": "Coordination of first responders under communication and resource constraints", "author": ["R.N. Lass", "J.B. Kopena", "E.A. Sultanik", "D.N. Nguyen", "C.P. Dugan", "P.J. Modi", "W.C. Regli"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Lass et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lass et al\\.", "year": 2008}, {"title": "Facilitating communication for first responders using dynamic distributed constraint optimization", "author": ["R.N. Lass", "W.C. Regli", "A. Kaplan", "M. Mitkus", "J.J. Sim"], "venue": "In Proceedings of the Symposium on Technologies for Homeland Security (IEEE HST),", "citeRegEx": "Lass et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lass et al\\.", "year": 2008}, {"title": "ER-DCOPs: A framework for distributed constraint optimization with uncertainty in constraint utilities", "author": ["T. Le", "F. Fioretto", "W. Yeoh", "T.C. Son", "E. Pontelli"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Le et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Le et al\\.", "year": 2016}, {"title": "Solving distributed constraint optimization problems with logic programming", "author": ["T. Le", "T.C. Son", "E. Pontelli", "W. Yeoh"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Distributed constraint optimization under stochastic uncertainty", "author": ["T. L\u00e9aut\u00e9", "B. Faltings"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "L\u00e9aut\u00e9 and Faltings,? \\Q2011\\E", "shortCiteRegEx": "L\u00e9aut\u00e9 and Faltings", "year": 2011}, {"title": "Frodo 2.0: An open-source framework for distributed constraint optimization", "author": ["T. L\u00e9aut\u00e9", "B. Ottens", "R. Szymanek"], "venue": "In International Workshop on Distributed Constraint Reasoning (DCR),", "citeRegEx": "L\u00e9aut\u00e9 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "L\u00e9aut\u00e9 et al\\.", "year": 2009}, {"title": "Evolution of the GPGP/TAEMS domain-independent coordination framework", "author": ["V. Lesser", "K. Decker", "T. Wagner", "N. Carver", "A. Garvey", "B. Horling", "D. Neiman", "R. Podorozhny", "M.N. Prasad", "A Raja"], "venue": "Journal of Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Lesser et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lesser et al\\.", "year": 2004}, {"title": "Taxation search in boolean games", "author": ["V. Levit", "T. Grinshpoun", "A. Meisels", "A.L. Bazzan"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Levit et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Levit et al\\.", "year": 2013}, {"title": "Distributed constraint optimization for addressing vessel rotation planning problems", "author": ["S. Li", "R.R. Negenborn", "G. Lodewijks"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "MABP: an optimal resource allocation approach in data center networks", "author": ["X. Li", "H. Wang", "B. Ding"], "venue": "Science China Information Sciences,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Optimizing preferences within groups: A case study on travel recommendation", "author": ["F. Lorenzi", "F. dos Santos", "P.R. Ferreira Jr.", "A.L. Bazzan"], "venue": "In Advances in Artificial Intelligence (SBIA),", "citeRegEx": "Lorenzi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lorenzi et al\\.", "year": 2008}, {"title": "AgentZero: A framework for simulating and evaluating multi-agent algorithms", "author": ["B. Lutati", "I. Gontmakher", "M. Lando", "A. Netzer", "A. Meisels", "A. Grubshtein"], "venue": "In Agent-Oriented Software Engineering,", "citeRegEx": "Lutati et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lutati et al\\.", "year": 2014}, {"title": "Distributed algorithms", "author": ["N.A. Lynch"], "venue": "Morgan Kaufmann.", "citeRegEx": "Lynch,? 1996", "shortCiteRegEx": "Lynch", "year": 1996}, {"title": "Efficient, superstabilizing decentralised optimisation for dynamic task allocation environments", "author": ["K. Macarthur", "A. Farinelli", "S. Ramchurn", "N. Jennings"], "venue": "In International Workshop on Optimization In Multi-Agent Systems,", "citeRegEx": "Macarthur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Macarthur et al\\.", "year": 2010}, {"title": "A distributed anytime algorithm for dynamic task allocation in multi-agent systems", "author": ["K. Macarthur", "A. Farinelli", "S. Ramchurn", "N. Jennings"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Macarthur et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Macarthur et al\\.", "year": 2011}, {"title": "The complexity of some polynomial network consistency algorithms for constraint satisfaction problems", "author": ["A.K. Mackworth", "E.C. Freuder"], "venue": "Artificial intelligence,", "citeRegEx": "Mackworth and Freuder,? \\Q1985\\E", "shortCiteRegEx": "Mackworth and Freuder", "year": 1985}, {"title": "Average reward reinforcement learning: Foundations, algorithms, and empirical results", "author": ["S. Mahadevan"], "venue": "Machine Learning, 22(1\u20133), 159\u2013195.", "citeRegEx": "Mahadevan,? 1996", "shortCiteRegEx": "Mahadevan", "year": 1996}, {"title": "Distributed algorithms for DCOP: A graphical gamebased approach", "author": ["R. Maheswaran", "J. Pearce", "M. Tambe"], "venue": "In Proceedings of the International Conference on Parallel and Distributed Computing Systems (PDCS),", "citeRegEx": "Maheswaran et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Maheswaran et al\\.", "year": 2004}, {"title": "Taking DCOP to the real world: Efficient complete solutions for distributed multi-event scheduling", "author": ["R.T. Maheswaran", "M. Tambe", "E. Bowring", "J.P. Pearce", "P. Varakantham"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Maheswaran et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Maheswaran et al\\.", "year": 2004}, {"title": "Solving distributed constraint optimization problems using cooperative mediation", "author": ["R. Mailler", "V. Lesser"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Mailler and Lesser,? \\Q2004\\E", "shortCiteRegEx": "Mailler and Lesser", "year": 2004}, {"title": "Survey of multi-objective optimization methods for engineering", "author": ["R.T. Marler", "J.S. Arora"], "venue": "Structural and multidisciplinary optimization,", "citeRegEx": "Marler and Arora,? \\Q2004\\E", "shortCiteRegEx": "Marler and Arora", "year": 2004}, {"title": "A quantified distributed constraint optimization problem", "author": ["T. Matsui", "H. Matsuo", "M.C. Silaghi", "K. Hirayama", "M. Yokoo", "S. Baba"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Matsui et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Matsui et al\\.", "year": 2010}, {"title": "A formalization for distributed cooperative sensor resource allocation", "author": ["T. Matsui", "H. Matsuo"], "venue": "In Agent and Multi-Agent Systems: Technologies and Applications,", "citeRegEx": "Matsui and Matsuo,? \\Q2008\\E", "shortCiteRegEx": "Matsui and Matsuo", "year": 2008}, {"title": "A distributed cooperative model for resource supply networks", "author": ["T. Matsui", "H. Matsuo"], "venue": "In Proceedings of the International MultiConference of Engineers and Computer Scientists,", "citeRegEx": "Matsui and Matsuo,? \\Q2011\\E", "shortCiteRegEx": "Matsui and Matsuo", "year": 2011}, {"title": "Resource constrained distributed constraint optimization with virtual variables", "author": ["T. Matsui", "H. Matsuo", "M. Silaghi", "K. Hirayama", "M. Yokoo"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Matsui et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Matsui et al\\.", "year": 2008}, {"title": "Distributed search method with bounded cost vectors on multiple objective DCOPs", "author": ["T. Matsui", "M. Silaghi", "K. Hirayama", "M. Yokoo", "H. Matsuo"], "venue": "In Proceedings of the Principles and Practice of Multi-Agent Systems (PRIMA),", "citeRegEx": "Matsui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matsui et al\\.", "year": 2012}, {"title": "Leximin multiple objective optimization for preferences of agents", "author": ["T. Matsui", "M. Silaghi", "K. Hirayama", "M. Yokoo", "H. Matsuo"], "venue": "In Proceedings of the Principles and Practice of Multi-Agent Systems (PRIMA),", "citeRegEx": "Matsui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Matsui et al\\.", "year": 2014}, {"title": "A two-phase complete algorithm for multi-objective distributed constraint optimization", "author": ["A. Medi", "T. Okimoto", "K. Inoue"], "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics (JACIII),", "citeRegEx": "Medi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Medi et al\\.", "year": 2014}, {"title": "Distributed Search by Constrained Agents: algorithms, performance, communication", "author": ["A. Meisels"], "venue": "Springer Science & Business Media.", "citeRegEx": "Meisels,? 2008", "shortCiteRegEx": "Meisels", "year": 2008}, {"title": "Comparing performance of distributed constraints processing algorithms", "author": ["A. Meisels", "E. Kaplansky", "I. Razgon", "R. Zivan"], "venue": "In International Workshop on Distributed Constraint Reasoning (DCR),", "citeRegEx": "Meisels et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Meisels et al\\.", "year": 2002}, {"title": "From mini-clouds to cloud computing", "author": ["B. Mejias", "P.V. Roy"], "venue": "In International Conference on Self-Adaptive and Self-Organizing Systems Workshop (SASOW),", "citeRegEx": "Mejias and Roy,? \\Q2010\\E", "shortCiteRegEx": "Mejias and Roy", "year": 2010}, {"title": "Nonlinear multiobjective optimization, Vol", "author": ["K. Miettinen"], "venue": "12. Springer Berlin Heidelberg.", "citeRegEx": "Miettinen,? 1999", "shortCiteRegEx": "Miettinen", "year": 1999}, {"title": "Optimal decentralised dispatch of embedded generation in the smart grid", "author": ["S. Miller", "S.D. Ramchurn", "A. Rogers"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Miller et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2012}, {"title": "Minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problems", "author": ["S. Minton", "M.D. Johnston", "A.B. Philips", "P. Laird"], "venue": "Artificial Intelligence,", "citeRegEx": "Minton et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Minton et al\\.", "year": 1992}, {"title": "Internet of things: Vision, applications and research challenges", "author": ["D. Miorandi", "S. Sicari", "F. De Pellegrini", "I. Chlamtac"], "venue": "Ad Hoc Networks,", "citeRegEx": "Miorandi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Miorandi et al\\.", "year": 2012}, {"title": "A cooperative multiagent based spectrum sharing", "author": ["U. Mir", "L. Merghem-Boulahia", "D. Ga\u0131\u0308ti"], "venue": "In Advanced International Conference on Telecommunications (AICT),", "citeRegEx": "Mir et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mir et al\\.", "year": 2010}, {"title": "ADOPT: Asynchronous distributed constraint optimization with quality guarantees", "author": ["P. Modi", "Shen", "W.-M", "M. Tambe", "M. Yokoo"], "venue": "Artificial Intelligence,", "citeRegEx": "Modi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Modi et al\\.", "year": 2005}, {"title": "Wireless Communications, Vol", "author": ["A.F. Molisch"], "venue": "34. John Wiley & Sons.", "citeRegEx": "Molisch,? 2012", "shortCiteRegEx": "Molisch", "year": 2012}, {"title": "Determining computational complexity from characteristic phase", "author": ["R. Monasson", "R. Zecchina", "S. Kirkpatrick", "B. Selman", "L. Troyansky"], "venue": "transitions. Nature,", "citeRegEx": "Monasson et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Monasson et al\\.", "year": 1999}, {"title": "Channel allocation algorithms for WLANs using distributed optimization. AEU-International", "author": ["T.L. Monteiro", "M.E. Pellenz", "M.C. Penna", "F. Enembreck", "R.D. Souza", "G. Pujolle"], "venue": "Journal of Electronics and Communications,", "citeRegEx": "Monteiro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Monteiro et al\\.", "year": 2012}, {"title": "A multi-agent approach to optimal channel assignment in WLANs", "author": ["T.L. Monteiro", "G. Pujolle", "M.E. Pellenz", "M.C. Penna", "F. Enembreck", "R. Demo Souza"], "venue": "In Proceedings of the Wireless Communications and Networking Conference (WCNC),", "citeRegEx": "Monteiro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Monteiro et al\\.", "year": 2012}, {"title": "Key challenges in cloud computing: Enabling the future internet of services", "author": ["R. Moreno-Vozmediano", "R.S. Montero", "I.M. Llorente"], "venue": "Internet Computing,", "citeRegEx": "Moreno.Vozmediano et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Moreno.Vozmediano et al\\.", "year": 2013}, {"title": "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs", "author": ["R. Nair", "P. Varakantham", "M. Tambe", "M. Yokoo"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Nair et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2005}, {"title": "Minizinc: Towards a standard CP modelling language", "author": ["N. Nethercote", "P.J. Stuckey", "R. Becket", "S. Brand", "G.J. Duck", "G. Tack"], "venue": "In Proceedings of the 13th International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "Nethercote et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nethercote et al\\.", "year": 2007}, {"title": "Concurrent forward bounding for distributed constraint optimization problems", "author": ["A. Netzer", "A. Grubshtein", "A. Meisels"], "venue": "Artificial Intelligence,", "citeRegEx": "Netzer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2012}, {"title": "Stochastic dominance in stochastic DCOPs for risk-sensitive applications", "author": ["D.T. Nguyen", "W. Yeoh", "H.C. Lau"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Nguyen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2012}, {"title": "Distributed Gibbs: A memory-bounded sampling-based DCOP algorithm", "author": ["D.T. Nguyen", "W. Yeoh", "H.C. Lau"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Nguyen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2013}, {"title": "Decentralized multi-agent reinforcement learning in average-reward dynamic DCOPs", "author": ["D.T. Nguyen", "W. Yeoh", "H.C. Lau", "S. Zilberstein", "C. Zhang"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Auctions and multi-agent systems", "author": ["P. Noriega", "C. Sierra"], "venue": "In Intelligent Information Agents,", "citeRegEx": "Noriega and Sierra,? \\Q1999\\E", "shortCiteRegEx": "Noriega and Sierra", "year": 1999}, {"title": "AOF-based algorithm for dynamic Multi-Objective distributed constraint optimization", "author": ["T. Okimoto", "M. Clement", "K. Inoue"], "venue": "In Multi-disciplinary Trends in Artificial Intelligence (MIWAI),", "citeRegEx": "Okimoto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Okimoto et al\\.", "year": 2013}, {"title": "Lp-Norm based algorithm for multi-objective distributed constraint optimization (Extended Abstract)", "author": ["T. Okimoto", "N. Schwind", "M. Clement", "K. Inoue"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Okimoto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Okimoto et al\\.", "year": 2014}, {"title": "Layered distributed constraint optimization problem for resource allocation problem in distributed sensor networks", "author": ["K. Ota", "T. Matsui", "H. Matsuo"], "venue": "In Proceedings of the Principles and Practice of Multi-Agent Systems (PRIMA),", "citeRegEx": "Ota et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ota et al\\.", "year": 2009}, {"title": "DUCT: An upper confidence bound approach to distributed constraint optimization problems", "author": ["B. Ottens", "C. Dimitrakakis", "B. Faltings"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Ottens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ottens et al\\.", "year": 2012}, {"title": "Pareto local optimum sets in the biobjective traveling salesman problem: An experimental study", "author": ["L. Paquete", "M. Chiarandini", "T. Sttzle"], "venue": "In Metaheuristics for Multiobjective Optimisation,", "citeRegEx": "Paquete et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Paquete et al\\.", "year": 2004}, {"title": "Game theory and decision theory in multi-agent systems", "author": ["S. Parsons", "M. Wooldridge"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Parsons and Wooldridge,? \\Q2002\\E", "shortCiteRegEx": "Parsons and Wooldridge", "year": 2002}, {"title": "Quality guarantees on k-optimal solutions for distributed constraint optimization problems", "author": ["J. Pearce", "M. Tambe"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Pearce and Tambe,? \\Q2007\\E", "shortCiteRegEx": "Pearce and Tambe", "year": 2007}, {"title": "Reasoning about and dynamically posting n-ary constraints in adopt", "author": ["F. Pecora", "P. Modi", "P. Scerri"], "venue": "In International Workshop on Distributed Constraint Reasoning (DCR),", "citeRegEx": "Pecora et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pecora et al\\.", "year": 2006}, {"title": "Scalable decentralized supply chain formation through binarized belief propagation", "author": ["T. Penya-Alba", "J. Cerquides", "J.A. Rodriguez-Aguilar", "M. Vinyals"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Penya.Alba et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Penya.Alba et al\\.", "year": 2012}, {"title": "Exploiting Max-Sum for the decentralized assembly of high-valued supply chains", "author": ["T. Penya-Alba", "M. Vinyals", "J. Cerquides", "J.A. Rodriguez-Aguilar"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Penya.Alba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Penya.Alba et al\\.", "year": 2014}, {"title": "A scalable message-passing algorithm for supply chain formation", "author": ["T. Penya-Alba", "M. Vinyals", "J. Cerquides", "J.A. Rodriguez-Aguilar"], "venue": "In AAAI,", "citeRegEx": "Penya.Alba et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Penya.Alba et al\\.", "year": 2012}, {"title": "Synchronizing for performance-DCOP algorithms", "author": ["O. Peri", "A. Meisels"], "venue": "In ICAART,", "citeRegEx": "Peri and Meisels,? \\Q2013\\E", "shortCiteRegEx": "Peri and Meisels", "year": 2013}, {"title": "Approximations in distributed optimization", "author": ["A. Petcu", "B. Faltings"], "venue": "In Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "Petcu and Faltings,? \\Q2005\\E", "shortCiteRegEx": "Petcu and Faltings", "year": 2005}, {"title": "A scalable method for multiagent constraint optimization", "author": ["A. Petcu", "B. Faltings"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Petcu and Faltings,? \\Q2005\\E", "shortCiteRegEx": "Petcu and Faltings", "year": 2005}, {"title": "Superstabilizing, fault-containing distributed combinatorial optimization", "author": ["A. Petcu", "B. Faltings"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Petcu and Faltings,? \\Q2005\\E", "shortCiteRegEx": "Petcu and Faltings", "year": 2005}, {"title": "ODPOP: An algorithm for open/distributed constraint optimization", "author": ["A. Petcu", "B. Faltings"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Petcu and Faltings,? \\Q2006\\E", "shortCiteRegEx": "Petcu and Faltings", "year": 2006}, {"title": "MB-DPOP: A new memory-bounded algorithm for distributed optimization", "author": ["A. Petcu", "B. Faltings"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Petcu and Faltings,? \\Q2007\\E", "shortCiteRegEx": "Petcu and Faltings", "year": 2007}, {"title": "Optimal solution stability in dynamic, distributed constraint optimization", "author": ["A. Petcu", "B. Faltings"], "venue": "In Proceedings of the International Conference on Intelligent Agent Technology (IAT),", "citeRegEx": "Petcu and Faltings,? \\Q2007\\E", "shortCiteRegEx": "Petcu and Faltings", "year": 2007}, {"title": "PC-DPOP: A new partial centralization algorithm for distributed optimization", "author": ["A. Petcu", "B. Faltings", "R. Mailler"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Petcu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Petcu et al\\.", "year": 2007}, {"title": "A simple, naive agent-based model for the optimization of a system of traffic lights: Insights from an exploratory experiment", "author": ["T. Pham", "A. Tawfik", "M. Taylor"], "venue": "In Proceedings of the conference on agent-based modeling in transportation planning and operations,", "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "The multi variable multi constrained distributed constraint optimization framework", "author": ["C. Portway", "E.H. Durfee"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Portway and Durfee,? \\Q2010\\E", "shortCiteRegEx": "Portway and Durfee", "year": 2010}, {"title": "Efficient inter-team task allocation in RoboCup rescue", "author": ["M. Pujol-Gonzalez", "J. Cerquides", "A. Farinelli", "P. Meseguer", "J.A. Rodriguez-Aguilar"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Pujol.Gonzalez et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pujol.Gonzalez et al\\.", "year": 2015}, {"title": "Decision analysis: introductory lectures on choices under uncertainty", "author": ["H. Raiffa"], "venue": "Addison-Wesley.", "citeRegEx": "Raiffa,? 1968", "shortCiteRegEx": "Raiffa", "year": 1968}, {"title": "Decentralized coordination in RoboCup rescue", "author": ["S.D. Ramchurn", "A. Farinelli", "K.S. Macarthur", "N.R. Jennings"], "venue": "Computer Journal,", "citeRegEx": "Ramchurn et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ramchurn et al\\.", "year": 2010}, {"title": "Putting the \u2019smarts\u2019 into the smart grid: A grand challenge for artificial intelligence", "author": ["S.D. Ramchurn", "P. Vytelingum", "A. Rogers", "N.R. Jennings"], "venue": "Commun. ACM,", "citeRegEx": "Ramchurn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ramchurn et al\\.", "year": 2012}, {"title": "Robust policy computation in reward-uncertain MDPs using nondominated policies", "author": ["K. Regan", "C. Boutilier"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Regan and Boutilier,? \\Q2010\\E", "shortCiteRegEx": "Regan and Boutilier", "year": 2010}, {"title": "Bounded approximate decentralised coordination via the Max-Sum algorithm", "author": ["A. Rogers", "A. Farinelli", "R. Stranders", "N. Jennings"], "venue": "Artificial Intelligence,", "citeRegEx": "Rogers et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rogers et al\\.", "year": 2011}, {"title": "Improved Bounded Max-Sum for distributed constraint optimization", "author": ["E. Rollon", "J. Larrosa"], "venue": "In Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "Rollon and Larrosa,? \\Q2012\\E", "shortCiteRegEx": "Rollon and Larrosa", "year": 2012}, {"title": "Handbook of Constraint Programming (Foundations of Artificial Intelligence)", "author": ["F. Rossi", "Beek", "P. v", "T. Walsh"], "venue": null, "citeRegEx": "Rossi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rossi et al\\.", "year": 2006}, {"title": "Using message-passing DCOP algorithms to solve energyefficient smart environment configuration problems", "author": ["P. Rust", "G. Picard", "F. Ramparany"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Rust et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rust et al\\.", "year": 2016}, {"title": "Optimization of vector functionals", "author": ["M.E. Salukvad"], "venue": "I. programming of optimal trajectories. Automation and remote Control, 32(8), 5\u201315.", "citeRegEx": "Salukvad,? 1971", "shortCiteRegEx": "Salukvad", "year": 1971}, {"title": "A reinforcement learning method for maximizing undiscounted rewards", "author": ["A. Schwartz"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pp. 298\u2013305.", "citeRegEx": "Schwartz,? 1993", "shortCiteRegEx": "Schwartz", "year": 1993}, {"title": "Target to sensor allocation: A hierarchical dynamic distributed constraint optimization approach", "author": ["S.H. Semnani", "O.A. Basir"], "venue": "Computer Communications,", "citeRegEx": "Semnani and Basir,? \\Q2013\\E", "shortCiteRegEx": "Semnani and Basir", "year": 2013}, {"title": "Structural descriptions and inexact matching", "author": ["L.G. Shapiro", "R.M. Haralick"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shapiro and Haralick,? \\Q1981\\E", "shortCiteRegEx": "Shapiro and Haralick", "year": 1981}, {"title": "Algorithms for the vehicle routing and scheduling problems with time window constraints", "author": ["M.M. Solomon"], "venue": "Operations research, 35(2), 254\u2013265.", "citeRegEx": "Solomon,? 1987", "shortCiteRegEx": "Solomon", "year": 1987}, {"title": "Towards CSP-based mission dispatching in C2/C4I systems", "author": ["G. Steinbauer", "A. Kleiner"], "venue": "In Safety, Security, and Rescue Robotics (SSRR),", "citeRegEx": "Steinbauer and Kleiner,? \\Q2012\\E", "shortCiteRegEx": "Steinbauer and Kleiner", "year": 2012}, {"title": "U-GDL: A decentralised algorithm for DCOPs with uncertainty", "author": ["R. Stranders", "F.M. Delle Fave", "A. Rogers", "N. Jennings"], "venue": "Tech. rep.,", "citeRegEx": "Stranders et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Stranders et al\\.", "year": 2011}, {"title": "Decentralised coordination of continuously valued control parameters using the Max-Sum algorithm", "author": ["R. Stranders", "A. Farinelli", "A. Rogers", "N.R. Jennings"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Stranders et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Stranders et al\\.", "year": 2009}, {"title": "DCOPs and bandits: Exploration and exploitation in decentralised coordination", "author": ["R. Stranders", "L. Tran-Thanh", "F.M.D. Fave", "A. Rogers", "N.R. Jennings"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Stranders et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stranders et al\\.", "year": 2012}, {"title": "On modeling multiagent task scheduling as a distributed constraint optimization problem", "author": ["E. Sultanik", "P.J. Modi", "W.C. Regli"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Sultanik et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sultanik et al\\.", "year": 2007}, {"title": "DCOPolis: A framework for simulating and deploying distributed constraint reasoning algorithms", "author": ["E.A. Sultanik", "R.N. Lass", "W.C. Regli"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS): demo papers,", "citeRegEx": "Sultanik et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sultanik et al\\.", "year": 2008}, {"title": "When should there be a me in team?: distributed multi-agent optimization under uncertainty", "author": ["M.E. Taylor", "M. Jain", "Y. Jin", "M. Yokoo", "M. Tambe"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Taylor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2010}, {"title": "Distributed on-line multi-agent optimization under uncertainty: Balancing exploration and exploitation", "author": ["M.E. Taylor", "M. Jain", "P. Tandon", "M. Yokoo", "M. Tambe"], "venue": "Advances in Complex Systems,", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "Multi-agent control of traffic networks: Algorithm and case study", "author": ["R. Van Katwijk", "B. De Schutter", "J. Hellendoorn"], "venue": "In International IEEE Conference on Intelligent Transportation Systems (ITSC),", "citeRegEx": "Katwijk et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Katwijk et al\\.", "year": 2009}, {"title": "Constraint solving in uncertain and dynamic environments", "author": ["G. Verfaillie", "N. Jussien"], "venue": "A survey. Constraints,", "citeRegEx": "Verfaillie and Jussien,? \\Q2005\\E", "shortCiteRegEx": "Verfaillie and Jussien", "year": 2005}, {"title": "Multi-armed bandit algorithms and empirical evaluation", "author": ["J. Vermorel", "M. Mohri"], "venue": "In Proceedings of the European Conference on Machine Learning (ECML),", "citeRegEx": "Vermorel and Mohri,? \\Q2005\\E", "shortCiteRegEx": "Vermorel and Mohri", "year": 2005}, {"title": "Quality guarantees for region optimal DCOP algorithms", "author": ["M. Vinyals", "E. Shieh", "J. Cerquides", "J. Rodriguez-Aguilar", "Z. Yin", "M. Tambe", "E. Bowring"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Vinyals et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2011}, {"title": "Local search based approximate algorithm for Multi-Objective DCOPs", "author": ["M. Wack", "T. Okimoto", "M. Clement", "K. Inoue"], "venue": "In Proceedings of the Principles and Practice of Multi-Agent Systems (PRIMA),", "citeRegEx": "Wack et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wack et al\\.", "year": 2014}, {"title": "Dischoco 2: A platform for distributed constraint reasoning", "author": ["M. Wahbi", "R. Ezzahir", "C. Bessiere", "Bouyakhf", "E.-H"], "venue": "International Workshop on Distributed Constraint Reasoning (DCR),", "citeRegEx": "Wahbi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wahbi et al\\.", "year": 2011}, {"title": "Modeling supply chain formation in multiagent systems", "author": ["W.E. Walsh", "M.P. Wellman"], "venue": "In Agent mediated electronic commerce II,", "citeRegEx": "Walsh and Wellman,? \\Q2000\\E", "shortCiteRegEx": "Walsh and Wellman", "year": 2000}, {"title": "Towards an understanding of the value of cooperation in uncertain world", "author": ["Y. Wang", "K. Sycara", "P. Scerri"], "venue": "In Proceedings of the International Joint Conferences on Web Intelligence and Intelligent Agent Technologies (WI-IAT),", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Decentralized supply chain formation using Max-Sum loopy belief propagation", "author": ["M. Winsper", "M. Chli"], "venue": "Computational Intelligence,", "citeRegEx": "Winsper and Chli,? \\Q2013\\E", "shortCiteRegEx": "Winsper and Chli", "year": 2013}, {"title": "Power generation, operation, and control", "author": ["A.J. Wood", "B.F. Wollenberg"], "venue": null, "citeRegEx": "Wood and Wollenberg,? \\Q2012\\E", "shortCiteRegEx": "Wood and Wollenberg", "year": 2012}, {"title": "An introduction to multiagent systems", "author": ["M. Wooldridge"], "venue": "John Wiley & Sons.", "citeRegEx": "Wooldridge,? 2009", "shortCiteRegEx": "Wooldridge", "year": 2009}, {"title": "Regret-based multi-agent coordination with uncertain task rewards. arXiv preprint arXiv:1309.1973", "author": ["F. Wu", "N.R. Jennings"], "venue": null, "citeRegEx": "Wu and Jennings,? \\Q2013\\E", "shortCiteRegEx": "Wu and Jennings", "year": 2013}, {"title": "Cognitive radio resource management using multi-agent systems", "author": ["J. Xie", "I. Howitt", "A. Raja"], "venue": "In Proceedings of the Consumer Communications and Networking Conference (CCNC),", "citeRegEx": "Xie et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2007}, {"title": "Novel branching-router-based multicast routing protocol with mobility support", "author": ["Z. Yan", "Lee", "J.-H", "S. Shen", "C. Qiao"], "venue": "Parallel and Distributed Systems, IEEE Transactions on,", "citeRegEx": "Yan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2013}, {"title": "Applying DCOP MST to a team of mobile robots with directional sensing abilities. In International Joint Workshop on Optimization In Multi-Agent Systems and Distributed Constraint Reasoning (OPTMAS-DCR)", "author": ["H. Yedidsion", "R. Zivan"], "venue": null, "citeRegEx": "Yedidsion and Zivan,? \\Q2014\\E", "shortCiteRegEx": "Yedidsion and Zivan", "year": 2014}, {"title": "Explorative Max-Sum for teams of mobile sensing agents", "author": ["H. Yedidsion", "R. Zivan", "A. Farinelli"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Yedidsion et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yedidsion et al\\.", "year": 2014}, {"title": "Speeding up Distributed Constraint Optimization search algorithms", "author": ["W. Yeoh"], "venue": "Ph.D. thesis, University of Southern California, Los Angeles (United States).", "citeRegEx": "Yeoh,? 2010", "shortCiteRegEx": "Yeoh", "year": 2010}, {"title": "BnB-ADOPT: An asynchronous branch-and-bound DCOP algorithm", "author": ["W. Yeoh", "A. Felner", "S. Koenig"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Yeoh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yeoh et al\\.", "year": 2010}, {"title": "Trading off solution quality for faster computation in DCOP search algorithms", "author": ["W. Yeoh", "X. Sun", "S. Koenig"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Yeoh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yeoh et al\\.", "year": 2009}, {"title": "Caching schemes for DCOP search algorithms", "author": ["W. Yeoh", "P. Varakantham", "S. Koenig"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Yeoh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yeoh et al\\.", "year": 2009}, {"title": "Incremental DCOP search algorithms for solving dynamic DCOPs (Extended Abstract)", "author": ["W. Yeoh", "P. Varakantham", "X. Sun", "S. Koenig"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Yeoh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yeoh et al\\.", "year": 2011}, {"title": "Distributed problem solving", "author": ["W. Yeoh", "M. Yokoo"], "venue": "AI Magazine,", "citeRegEx": "Yeoh and Yokoo,? \\Q2012\\E", "shortCiteRegEx": "Yeoh and Yokoo", "year": 2012}, {"title": "Distributed constraint satisfaction: Foundation of cooperation in Multi-agent Systems", "author": ["M. Yokoo"], "venue": null, "citeRegEx": "Yokoo,? \\Q2001\\E", "shortCiteRegEx": "Yokoo", "year": 2001}, {"title": "The distributed constraint satisfaction problem: Formalization and algorithms. Knowledge and Data Engineering", "author": ["M. Yokoo", "E.H. Durfee", "T. Ishida", "K. Kuwabara"], "venue": "IEEE Transactions on,", "citeRegEx": "Yokoo et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Yokoo et al\\.", "year": 1998}, {"title": "Algorithms for distributed constraint satisfaction: A review", "author": ["M. Yokoo", "K. Hirayama"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Yokoo and Hirayama,? \\Q2000\\E", "shortCiteRegEx": "Yokoo and Hirayama", "year": 2000}, {"title": "Coordinating multi-agent reinforcement learning with limited communication", "author": ["C. Zhang", "V. Lesser"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Zhang and Lesser,? \\Q2013\\E", "shortCiteRegEx": "Zhang and Lesser", "year": 2013}, {"title": "Distributed stochastic search and distributed breakout: Properties, comparison and applications to constraint optimization problems in sensor networks", "author": ["W. Zhang", "G. Wang", "Z. Xing", "L. Wittenberg"], "venue": "Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2005}, {"title": "Using anytime algorithms in intelligent systems", "author": ["S. Zilberstein"], "venue": "AI magazine, 17(3), 73.", "citeRegEx": "Zilberstein,? 1996", "shortCiteRegEx": "Zilberstein", "year": 1996}, {"title": "Distributed constraint optimization for large teams of mobile sensing agents", "author": ["R. Zivan", "R. Glinton", "K. Sycara"], "venue": "In Proceedings of the International Joint Conferences on Web Intelligence and Intelligent Agent Technologies (WI-IAT),", "citeRegEx": "Zivan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zivan et al\\.", "year": 2009}, {"title": "Dynamic ordering for asynchronous backtracking on", "author": ["R. Zivan", "A. Meisels"], "venue": "DisCSPs. Constraints,", "citeRegEx": "Zivan and Meisels,? \\Q2006\\E", "shortCiteRegEx": "Zivan and Meisels", "year": 2006}, {"title": "Explorative anytime local search for distributed constraint optimization", "author": ["R. Zivan", "S. Okamoto", "H. Peled"], "venue": "Artificial Intelligence,", "citeRegEx": "Zivan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zivan et al\\.", "year": 2014}, {"title": "Balancing exploration and exploitation in incomplete min/max-sum inference for distributed constraint optimization", "author": ["R. Zivan", "T. Parash", "L. Cohen", "H. Peled", "S. Okamoto"], "venue": "Journal of Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Zivan et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zivan et al\\.", "year": 2017}, {"title": "Max/Min-Sum distributed constraint optimization through value propagation on an alternating DAG", "author": ["R. Zivan", "H. Peled"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Zivan and Peled,? \\Q2012\\E", "shortCiteRegEx": "Zivan and Peled", "year": 2012}], "referenceMentions": [{"referenceID": 196, "context": "An agent can be defined as an entity (or computer program) that behaves autonomously within an arbitrary system in the pursuit of some goals (Wooldridge, 2009).", "startOffset": 141, "endOffset": 159}, {"referenceID": 165, "context": "Decision Theory (DT) (Raiffa, 1968) assumes that the agent and the environment are inherently uncertain, and DT models such uncertainty explicitly.", "startOffset": 21, "endOffset": 35}, {"referenceID": 17, "context": "Dec-POMDPs are non-deterministic exponential (NEXP) complete (Bernstein et al., 2002), even for two-agent problems, and scalability a2 a1", "startOffset": 61, "endOffset": 85}, {"referenceID": 23, "context": "Game Theory (GT) (Binmore, 1992) studies interactions between self-interested agents, aiming at maximizing the welfare of the participants.", "startOffset": 17, "endOffset": 32}, {"referenceID": 86, "context": "Some of the most compelling applications of game theory to MAS have been in the area of auctions and negotiations (Kraus, 1997; Noriega & Sierra, 1999; Parsons & Wooldridge, 2002).", "startOffset": 114, "endOffset": 179}, {"referenceID": 122, "context": "While there are important existing surveys for Distributed Constraint Satisfaction (Yokoo & Hirayama, 2000) and Distributed Constraint Optimization (Meisels, 2008), this survey aims to comprehensively analyze and categorize the different DCOP frameworks proposed by the MAS community.", "startOffset": 148, "endOffset": 163}, {"referenceID": 7, "context": "Constraint Satisfaction Problems (CSPs) (Golomb & Baumert, 1965; Mackworth & Freuder, 1985; Apt, 2003; Rossi et al., 2006) are decision problems that involve the assignment of values to variables, under a set of specified constraints on how variable values should be related to each other.", "startOffset": 40, "endOffset": 122}, {"referenceID": 171, "context": "Constraint Satisfaction Problems (CSPs) (Golomb & Baumert, 1965; Mackworth & Freuder, 1985; Apt, 2003; Rossi et al., 2006) are decision problems that involve the assignment of values to variables, under a set of specified constraints on how variable values should be related to each other.", "startOffset": 40, "endOffset": 122}, {"referenceID": 171, "context": "Over the years, CSPs have become the paradigm of choice to address hard combinatorial problems, drawing and integrating insights from diverse domains, including Artificial Intelligence and Operations Research (Rossi et al., 2006).", "startOffset": 209, "endOffset": 229}, {"referenceID": 208, "context": "When the elements of a CSP are distributed among a set of autonomous agents, we refer to it as a Distributed Constraint Satisfaction Problem (DisCSP) (Yokoo, Durfee, Ishida, & Kuwabara, 1998; Yokoo, 2001).", "startOffset": 150, "endOffset": 204}, {"referenceID": 171, "context": "For a more detailed treatment of the argument we address the reader to (Rossi et al., 2006) (Chapter 20).", "startOffset": 71, "endOffset": 91}, {"referenceID": 130, "context": "Similar to the generalization of CSPs to COPs, the Distributed Constraint Optimization Problem (DCOP) (Modi et al., 2005; Petcu & Faltings, 2005b; Gershman et al., 2009; Yeoh & Yokoo, 2012) model emerges as a generalization of the DisCSP model, where constraints specify a degree of preference over their violation, rather than a boolean satisfaction metric.", "startOffset": 102, "endOffset": 189}, {"referenceID": 54, "context": "Similar to the generalization of CSPs to COPs, the Distributed Constraint Optimization Problem (DCOP) (Modi et al., 2005; Petcu & Faltings, 2005b; Gershman et al., 2009; Yeoh & Yokoo, 2012) model emerges as a generalization of the DisCSP model, where constraints specify a degree of preference over their violation, rather than a boolean satisfaction metric.", "startOffset": 102, "endOffset": 189}, {"referenceID": 130, "context": "With respect to our categorization, in the classical DCOP model (Modi et al., 2005; Petcu & Faltings, 2005b; Gershman et al., 2009; Yeoh & Yokoo, 2012), the agents are completely cooperative and they have deterministic behavior and total knowledge.", "startOffset": 64, "endOffset": 151}, {"referenceID": 54, "context": "With respect to our categorization, in the classical DCOP model (Modi et al., 2005; Petcu & Faltings, 2005b; Gershman et al., 2009; Yeoh & Yokoo, 2012), the agents are completely cooperative and they have deterministic behavior and total knowledge.", "startOffset": 64, "endOffset": 151}, {"referenceID": 130, "context": "Finding an optimal solution for a classical DCOP is known to be NP-hard (Modi et al., 2005).", "startOffset": 72, "endOffset": 91}, {"referenceID": 130, "context": "Asynchronous algorithms allow agents to update the assignment for their variables based solely on their local view of the problem, and thus independently from the actual decisions of the other agents (Modi et al., 2005; Farinelli, Rogers, Petcu, & Jennings, 2008; Gershman et al., 2009).", "startOffset": 200, "endOffset": 286}, {"referenceID": 54, "context": "Asynchronous algorithms allow agents to update the assignment for their variables based solely on their local view of the problem, and thus independently from the actual decisions of the other agents (Modi et al., 2005; Farinelli, Rogers, Petcu, & Jennings, 2008; Gershman et al., 2009).", "startOffset": 200, "endOffset": 286}, {"referenceID": 54, "context": ", 2005; Farinelli, Rogers, Petcu, & Jennings, 2008; Gershman et al., 2009). In contrast, synchronous algorithms constrain the agents decisions to follow a particular order, typically enforced by the representation structure adopted (Mailler & Lesser, 2004; Petcu & Faltings, 2005b; Pearce & Tambe, 2007). Synchronous methods tend to delay the actions of some agents guaranteeing that their local view of the problem is always consistent with that of the other agents. In contrast, asynchronous methods tend to minimize the idle-time of the agents, which in turn can react quickly to each message being processed; however, they provide no guarantee on the consistency of the state of the local view of each agent. Such effect has been studied by Peri and Meisels (2013), concluding that inconsistent agents\u2019 views may cause a negative impact on network load and algorithm performance, and that introducing some level of synchronization may be beneficial for some algorithms, enhancing their performance.", "startOffset": 52, "endOffset": 769}, {"referenceID": 202, "context": "The resolution process adopted by each algorithm can be classified in three categories (Yeoh, 2010): \u2022 Search-based methods are based on the use of search techniques to explore the space of possible solutions.", "startOffset": 87, "endOffset": 99}, {"referenceID": 54, "context": "AFB (Gershman et al., 2009).", "startOffset": 4, "endOffset": 27}, {"referenceID": 130, "context": "ADOPT (Modi et al., 2005).", "startOffset": 6, "endOffset": 25}, {"referenceID": 33, "context": "Finally, the No-Commitment Branch and Bound (NCBB) algorithm, proposed by Chechetka and Sycara (2006), can be considered as a variant of ADOPT and SyncBB.", "startOffset": 74, "endOffset": 102}, {"referenceID": 138, "context": "Netzer et al. (2012) described a series of enhancements that can be used to speed up the search process of ConcFB, including dynamic variable ordering and dynamic splitting.", "startOffset": 0, "endOffset": 21}, {"referenceID": 59, "context": "ODPOP and MB-DPOP trade runtimes for smaller memory requirements (Petcu & Faltings, 2006, 2007a), A-DPOP trades solution optimality for smaller runtimes (Petcu & Faltings, 2005a), SSDPOP trades runtime for increased privacy (Greenstadt et al., 2007), PC-DPOP trades privacy for smaller runtimes (Petcu et al.", "startOffset": 224, "endOffset": 249}, {"referenceID": 161, "context": ", 2007), PC-DPOP trades privacy for smaller runtimes (Petcu et al., 2007), H-DPOP propagates hard constraints for smaller runtimes (Kumar, Petcu, & Faltings, 2008), BrC-DPOP enforces branch consistency for smaller runtimes (Fioretto, Le, Yeoh, Pontelli, & Son, 2014), and ASP-DPOP is a declarative version of DPOP that uses Answer Set Programming (Le, Son, Pontelli, & Yeoh, 2015).", "startOffset": 53, "endOffset": 73}, {"referenceID": 161, "context": "However, it is possible that several mediators solve overlapping problems, duplicating efforts (Petcu et al., 2007), which can be a bottleneck especially for dense problems.", "startOffset": 95, "endOffset": 115}, {"referenceID": 60, "context": "Grinshpoun and Meisels (2008) proposed a complete variant that remedies this issue.", "startOffset": 0, "endOffset": 30}, {"referenceID": 44, "context": "Max-Sum (Farinelli et al., 2008).", "startOffset": 8, "endOffset": 32}, {"referenceID": 82, "context": "An asynchronous version of regional-optimal algorithms, called Distributed Asynchronous Local Optimization (DALO), was proposed by Kiekintveld et al. (2010). The DALO simulator provides a mechanism to coordinate the decision of local groups of agents based on the concepts of k-optimality and t-distance.", "startOffset": 131, "endOffset": 157}, {"referenceID": 138, "context": "\u2022 Otherwise, ConcFB is recommended as it has been shown to outperform AFB due to the concurrent search (Netzer et al., 2012), and AFB has been shown to outperform ADOPT and SyncBB (Gershman et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 54, "context": ", 2012), and AFB has been shown to outperform ADOPT and SyncBB (Gershman et al., 2009).", "startOffset": 63, "endOffset": 86}, {"referenceID": 203, "context": "while maintaining the same runtime, memory, and communication requirements (Chechetka & Sycara, 2006; Yeoh et al., 2010).", "startOffset": 75, "endOffset": 120}, {"referenceID": 7, "context": "It can capture asymmetric costs by introducing, for each agent, as many \u201cmirror\u201d variables as the number of variables held by neighboring agents. The consistency with the neighbors\u2019 state variables is imposed by a set of equality constraints. However, this formalism suffers from scalability problems, as it may result in a significant increase in the number of variables in a DCOP. In addition, Grinshpoun et al. (2013) showed that most of the existing incomplete classical DCOP algorithms cannot be used to effectively solve Asymmetric DCOPs, even when the problems are reformulated through the PEAV model.", "startOffset": 8, "endOffset": 421}, {"referenceID": 61, "context": "SyncABB-2ph (Grinshpoun et al., 2013).", "startOffset": 12, "endOffset": 37}, {"referenceID": 61, "context": "SyncABB-1ph (Grinshpoun et al., 2013; Levit, Grinshpoun, Meisels, & Bazzan, 2013).", "startOffset": 12, "endOffset": 81}, {"referenceID": 61, "context": "ATWB (Grinshpoun et al., 2013).", "startOffset": 5, "endOffset": 30}, {"referenceID": 61, "context": "ACLS (Grinshpoun et al., 2013).", "startOffset": 5, "endOffset": 30}, {"referenceID": 61, "context": "MCS-MGM (Grinshpoun et al., 2013).", "startOffset": 8, "endOffset": 33}, {"referenceID": 125, "context": "Multi-Objective Optimization (MOO) (Miettinen, 1999; Marler & Arora, 2004) aims at solving problems involving more than one objective function to be optimized simultaneously.", "startOffset": 35, "endOffset": 74}, {"referenceID": 173, "context": "Therefore, different approaches focus on finding a compromise solution (Salukvad, 1971), which is a Pareto optimal solution that is close to the utopia point.", "startOffset": 71, "endOffset": 87}, {"referenceID": 125, "context": "It adapts the AOF technique (Miettinen, 1999), designed to solve centralized multi-objective optimization problems, to solve MO-DCOPs.", "startOffset": 28, "endOffset": 45}, {"referenceID": 125, "context": "The resulting mono-objective function \u2211h i=1 \u03b1i Fi can be solved using any monoobjective optimization technique with guarantee to find a Pareto optimal solution (Miettinen, 1999).", "startOffset": 161, "endOffset": 178}, {"referenceID": 42, "context": "Stability (Dijkstra, 1974; Verfaillie & Jussien, 2005) is a core algorithmic concept, where an algorithm seeks to minimize the number of steps that it requires to converge to a solution each time the problem changes.", "startOffset": 10, "endOffset": 54}, {"referenceID": 155, "context": "Petcu and Faltings (2005c) discuss two self-stabilizing extensions, namely super-stabilization and fault-containment, which can be used to provide guarantees about the way the system transitions from a valid state to the next, after an environment change.", "startOffset": 0, "endOffset": 27}, {"referenceID": 166, "context": "Ramchurn et al. (2010) exploit domain-specific properties in a task allocation problem to reduce the number of states over which each factor has to compute its solution.", "startOffset": 0, "endOffset": 23}, {"referenceID": 155, "context": "To solve this problem, Petcu and Faltings (2007b) extended S-DPOP to RS-DPOP.", "startOffset": 23, "endOffset": 50}, {"referenceID": 174, "context": "The Distributed Q-learning and Rlearning algorithms are synchronous reinforcement-learning-based algorithms that extend the centralized Q-learning (Abounadi, Bertsekas, & Borkar, 2001) and centralized R-learning (Schwartz, 1993; Mahadevan, 1996) algorithms.", "startOffset": 212, "endOffset": 245}, {"referenceID": 110, "context": "The Distributed Q-learning and Rlearning algorithms are synchronous reinforcement-learning-based algorithms that extend the centralized Q-learning (Abounadi, Bertsekas, & Borkar, 2001) and centralized R-learning (Schwartz, 1993; Mahadevan, 1996) algorithms.", "startOffset": 212, "endOffset": 245}, {"referenceID": 155, "context": "The full name of the algorithm was not provided by Petcu and Faltings (2007b). 9.", "startOffset": 51, "endOffset": 78}, {"referenceID": 139, "context": "SD-DPOP (Nguyen et al., 2012).", "startOffset": 8, "endOffset": 29}, {"referenceID": 9, "context": "The P-DCOP model proposed by Atlas and Decker (2010) is characterized by uncertainty exclusively at the level of the outcome of the cost functions, and not due to random variables.", "startOffset": 29, "endOffset": 53}, {"referenceID": 179, "context": "U-GDL (Stranders et al., 2011).", "startOffset": 6, "endOffset": 30}, {"referenceID": 179, "context": "U-GDL (Stranders et al., 2011). The P-DCOP model proposed by Stranders et al. (2011) also assumes that the cost functions are not dependent on random variables.", "startOffset": 7, "endOffset": 85}, {"referenceID": 179, "context": "U-GDL (Stranders et al., 2011). The P-DCOP model proposed by Stranders et al. (2011) also assumes that the cost functions are not dependent on random variables. Thus, I = \u2205. Additionally, they assume that E is the expectation of the convolution (\u03a3) of the distributions fi(\u03c3xi\\I), and U is a given risk function. They propose the Uncertain Generalized Distributive Law (U-GDL) algorithm, which is an incomplete asynchronous inference-based algorithm similar to Max-Sum, and operates on acyclic graphs. A cyclic constraint graphG is converted into an acyclic graph \u011c by merging variables until the new resulting graph contains no cycles. Merging two variables creates a new one whose domain is the cartesian product of the domain of the merged variables. U-DGL extends the Generalized Distributive Law (GDL) algorithm (Aji & McEliece, 2000) by redefining the (min,+)11 algebra to the setting where costs are random variables rather than scalars. The + operator is extended to perform convolution of two random variables. To cope with the potential issue that not all PDFs are closed under convolution, Stranders et al. (2011) suggest to resort to sampling methods to approximate such operations.", "startOffset": 7, "endOffset": 1125}, {"referenceID": 184, "context": "BE-Rebid (Taylor et al., 2010).", "startOffset": 9, "endOffset": 30}, {"referenceID": 184, "context": "Following the region-optimal approaches presented in the context of classical DCOPs, the Taylor et al. (2011) propose a version of the algorithm, called BE-Rebid-2, that allows pairs of agents to explore in a coordinated fashion.", "startOffset": 89, "endOffset": 110}, {"referenceID": 179, "context": "Stranders et al. (2012) show that HEIST enables agents to balance between exploration and exploitation, and derive optimal asymptotic bounds on the regret of the global cumulative cost attained.", "startOffset": 0, "endOffset": 24}, {"referenceID": 15, "context": "The algorithm extends the Iterative Constraint Generation (ICG) method (Benders, 1962; Regan & Boutilier, 2010) to the decentralized case, by decomposing the overall problem into a master problem and a subproblem that are iteratively solved until convergence.", "startOffset": 71, "endOffset": 111}, {"referenceID": 15, "context": "The algorithm extends the Iterative Constraint Generation (ICG) method (Benders, 1962; Regan & Boutilier, 2010) to the decentralized case, by decomposing the overall problem into a master problem and a subproblem that are iteratively solved until convergence. At each iteration, the resolutions of these problems are attained by using Max-Sum. The master problem solves a relaxation of the minimax regret goal, where only a subset of all possible joint beliefs is considered, attempting to minimize the loss for the worst case derived from the considered joint belief. Once it generates a solution, the subproblem finds the maximally violated constraint associated to such a solution. This is referred to as the witness point, indicating that the current solution is not the best one in terms of the minimax regret. This point is added to the set of joint beliefs considered by the master problem, and the process is repeated until no new witness points can be found. The worst case runtime requirement of this algorithm is O(`|I|dl), which is dominated by the master problem, whose computation is exponential in the number of variables in the scope of the associated cost function for each belief and iteration of the algorithm. In terms of memory requirement, each ICG-Max-Sum agent needs O(|I|dl) amount of memory (dominated by the master problem again) to store all value combinations of neighboring variables for each belief. In terms of communication requirement, the number of messages sent is the same as that of Max-Sum since two parallel iterations of Max-Sum is executed in each ICG-Max-Sum iteration. However, the size of each message is O(|I|d) as it contains the aggregated cost of all the agent\u2019s variable\u2019s value for each belief. A variation of this algorithm that aims at minimizing the expected regret, rather than minimizing the maximum regret, was introduced by Le, Fioretto, Yeoh, Son, and Pontelli (2016).", "startOffset": 72, "endOffset": 1927}, {"referenceID": 115, "context": "The Quantified DCOP (QDCOP) model (Matsui et al., 2010) adapts the Quantified Constraint Satisfaction Problem (QCSP) (Benedetti, Lallouet, & Vautard, 2008) and Quantified Distributed CSP (QDCSP) (Baba, Iwasaki, Yokoo, Silaghi, Hirayama, & Matsui, 2010; Baba, Joe, Iwasaki, & Yokoo, 2011) models to DCOPs.", "startOffset": 34, "endOffset": 55}, {"referenceID": 16, "context": "While finding an optimal solution for a DCOP is NP-hard, solving a QDCOP is, in general, P-SPACE-hard (Benedetti et al., 2008; Lallouet, Lee, Mak, & Yip, 2015).", "startOffset": 102, "endOffset": 159}, {"referenceID": 115, "context": "Matsui et al. (2010) proposed several variations of ADOPT to solve QDCOPs, which are all based on a DFS pseudo-tree ordering.", "startOffset": 0, "endOffset": 21}, {"referenceID": 115, "context": "Min-max ADOPT (Matsui et al., 2010).", "startOffset": 14, "endOffset": 35}, {"referenceID": 115, "context": "Alpha-beta ADOPT (Matsui et al., 2010).", "startOffset": 17, "endOffset": 38}, {"referenceID": 115, "context": "Bi-threshold ADOPT (Matsui et al., 2010).", "startOffset": 19, "endOffset": 40}, {"referenceID": 166, "context": "In a Coalition Formation with Spatial and Temporal Constraints (CFST) problem (Ramchurn et al., 2010; PujolGonzalez, Cerquides, Farinelli, Meseguer, & Rodriguez-Aguilar, 2015; Steinbauer & Kleiner, 2012), ambulance and fire brigade agents cooperate in order to react efficiently to an emergency scenario so as to rescue victims and extinguish fires located in different parts of a city.", "startOffset": 78, "endOffset": 203}, {"referenceID": 166, "context": "In a Coalition Formation with Spatial and Temporal Constraints (CFST) problem (Ramchurn et al., 2010; PujolGonzalez, Cerquides, Farinelli, Meseguer, & Rodriguez-Aguilar, 2015; Steinbauer & Kleiner, 2012), ambulance and fire brigade agents cooperate in order to react efficiently to an emergency scenario so as to rescue victims and extinguish fires located in different parts of a city. Agents can travel from one location to another in a given time. Each task (i.e., rescuing a victim, extinguishing a fire) has a deadline, representing the time until which the victim will survive, and a workload, denoting the amount of time necessary to rescue the victim or put out the fire. The locations of the victims and the fires may be unknown to the agents, and need to be discovered at runtime, which requires agents to dynamically update the sequence of the tasks they will attempt, taking account of two main constraints: (1) Spatial constraints, which model where an agent can travel and at what time; and (2) Temporal constraints, which model task deadlines and completion times. Agents may also form coalitions to execute a given task faster or if the requirements of a given task cannot be met by a single agent. Hence, the agents\u2019 arrival times at each task need to be coordinated in order to form the desired coalition. The objective is to maximize the number of tasks to be completed. A DCOP formalization for the CFST is described by Ramchurn et al. (2010), where ambulances and fire brigades are modeled as DCOP agents.", "startOffset": 79, "endOffset": 1463}, {"referenceID": 166, "context": "In a Coalition Formation with Spatial and Temporal Constraints (CFST) problem (Ramchurn et al., 2010; PujolGonzalez, Cerquides, Farinelli, Meseguer, & Rodriguez-Aguilar, 2015; Steinbauer & Kleiner, 2012), ambulance and fire brigade agents cooperate in order to react efficiently to an emergency scenario so as to rescue victims and extinguish fires located in different parts of a city. Agents can travel from one location to another in a given time. Each task (i.e., rescuing a victim, extinguishing a fire) has a deadline, representing the time until which the victim will survive, and a workload, denoting the amount of time necessary to rescue the victim or put out the fire. The locations of the victims and the fires may be unknown to the agents, and need to be discovered at runtime, which requires agents to dynamically update the sequence of the tasks they will attempt, taking account of two main constraints: (1) Spatial constraints, which model where an agent can travel and at what time; and (2) Temporal constraints, which model task deadlines and completion times. Agents may also form coalitions to execute a given task faster or if the requirements of a given task cannot be met by a single agent. Hence, the agents\u2019 arrival times at each task need to be coordinated in order to form the desired coalition. The objective is to maximize the number of tasks to be completed. A DCOP formalization for the CFST is described by Ramchurn et al. (2010), where ambulances and fire brigades are modeled as DCOP agents. Each agent controls a variable that encodes the current task that the agent will attempt, and whose domain represents task locations. Unary constraints restrict the set of reachable locations, according to distance from a destination and the victim\u2019s deadline. Agents\u2019 coalitions are defined as groups of agents traveling to the same location. Each task is associated to a utility, which encodes the success for a coalition to complete such task. The goal is to find an assignment of agents to tasks that maximizes the overall utility. A variant of the CFST problem, called the Law Enforcement Problem (LEP), was introduced by Amador, Okamoto, and Zivan (2014). Similar to the CFST problem, in a LEP, police officers need to execute a number of tasks, and may form coalitions to improve their response quality.", "startOffset": 79, "endOffset": 2188}, {"referenceID": 133, "context": "Monteiro et al. (2012a, 2012b) proposed a DCOP-based approach for cooperative channel assignment in WLANs where APs may belong to different administration entities. In the proposed model, each AP is represented by a DCOP agent, which controls a decision variable modeling a choice for the AP\u2019s channels. The signal-to-interference-and-noise ratio perceived by an AP or a client is modeled as a cost function, as the overall concurrent transmissions occurring in the same channel and in partially overlapped adjacent channels. The goal is to find an assignment of channels to APs that minimizes the total interference experienced in the WLAN. Xie, Howitt, and Raja (2007) and Mir, Merghem-Boulahia, and Ga\u0131\u0308ti (2010) study dynamic solutions to the problem of allocating and utilizing the wireless network\u2019s available spectrum.", "startOffset": 0, "endOffset": 671}, {"referenceID": 133, "context": "Monteiro et al. (2012a, 2012b) proposed a DCOP-based approach for cooperative channel assignment in WLANs where APs may belong to different administration entities. In the proposed model, each AP is represented by a DCOP agent, which controls a decision variable modeling a choice for the AP\u2019s channels. The signal-to-interference-and-noise ratio perceived by an AP or a client is modeled as a cost function, as the overall concurrent transmissions occurring in the same channel and in partially overlapped adjacent channels. The goal is to find an assignment of channels to APs that minimizes the total interference experienced in the WLAN. Xie, Howitt, and Raja (2007) and Mir, Merghem-Boulahia, and Ga\u0131\u0308ti (2010) study dynamic solutions to the problem of allocating and utilizing the wireless network\u2019s available spectrum.", "startOffset": 0, "endOffset": 716}, {"referenceID": 177, "context": "Scheduling problems are an important class of problems, and they have been long studied in the area of Constraint Programming and Operations Research (Giffler & Thompson, 1960; Solomon, 1987; Minton, Johnston, Philips, & Laird, 1992; Hentenryck & Michel, 2009).", "startOffset": 150, "endOffset": 260}, {"referenceID": 7, "context": "The distributed meeting scheduling problem captures generic scheduling problems where one wishes to schedule a set of events within a time range (Jennings & Jackson, 1995; Garrido & Sycara, 1996). Each event is defined by: (i) the resources required for it to be completed, (ii) the time required for it to be completed, within which it holds the required resources, and (iii) the cost of using such resources at a given time. A scheduling conflict occurs if two events with at least one common resource are scheduled in overlapping time slots. The goal is to maximize the utilities over all the resources, defined as the net gain between the opportunity benefit and opportunity cost of scheduling various events. Maheswaran, Tambe, Bowring, Pearce, and Varakantham (2004b) discuss three possible DCOP formulations for this problem: Time slots as variables (TSAV), events as variables (EAV), and private events as variables (PEAV).", "startOffset": 44, "endOffset": 774}, {"referenceID": 57, "context": "Giuliani et al. (2014) formalize a regulatory mechanism in water management as a DCOP.", "startOffset": 0, "endOffset": 23}, {"referenceID": 102, "context": "Li et al. (2016) proposed a DCOP model for the VRPP, where vessels and terminals are modeled as DCOP agents.", "startOffset": 0, "endOffset": 17}, {"referenceID": 68, "context": "Hannebauer and M\u00fcller (2001) formulate the problem of scheduling patients to diagnostic units in an hospital as a DCOP, where appointments are modeled as variables, whose domains describe times, durations, and locations.", "startOffset": 0, "endOffset": 29}, {"referenceID": 212, "context": "In a typical target tracking application (Zhang et al., 2005; Matsui & Matsuo, 2008; Jain, Taylor, Tambe, & Yokoo, 2009; Ota, Matsui, & Matsuo, 2009; Stranders, Farinelli, Rogers, & Jennings, 2009; Semnani & Basir, 2013), a collection of small Doppler sensors are scattered in an area to detect possible moving targets in the region.", "startOffset": 41, "endOffset": 220}, {"referenceID": 206, "context": ", 2005; Matsui & Matsuo, 2008; Jain, Taylor, Tambe, & Yokoo, 2009; Ota, Matsui, & Matsuo, 2009; Stranders, Farinelli, Rogers, & Jennings, 2009; Semnani & Basir, 2013), a collection of small Doppler sensors are scattered in an area to detect possible moving targets in the region. Each sensor is batterypowered, can communicate with one another through radio communication, and can scan and detect an object within a fixed range. Communication incurs an energy cost. Thus, to save energy, a sensor may turn itself off. Multiple sensors may be necessary to detect a single target with high accuracy. The overall objective is to maximize the number of targets detected, as quickly as possible and, at the same time, preserve energy so as to prolong the system\u2019s lifetime Zhang et al. (2005) moldes a simplified version of the above problem as a weighted graph coloring problem, where the total weight of violated constraints needs to be minimized.", "startOffset": 54, "endOffset": 788}, {"referenceID": 7, "context": "A node corresponds to a sensor, an edge between two nodes represents the constraint of a shared region between agents, and the weight captures the importance of the common region. The size of the common region reflects the amount of energy loss when two sensors scan the shared region at the same time. Each color corresponds to a time slot in which a sector is scanned. A node must have at least one color so that the corresponding sector is scanned at least once. This graph coloring problem is mapped to a DCOP, where agents represent nodes, agent\u2019s variables represent the agents decision on their color, and cost functions represent the graph edges. Semnani and Basir (2013) use a hierarchical DCOP approach to scale to larger problems.", "startOffset": 135, "endOffset": 680}, {"referenceID": 75, "context": "In such a problem, robots can make small movements to optimize the wireless connectivity with their neighbors, without affecting the network topology (Choxi & Modi, 2007; Jain et al., 2009).", "startOffset": 150, "endOffset": 189}, {"referenceID": 131, "context": "However, radio wave interference is very difficult to predict (Molisch, 2012).", "startOffset": 62, "endOffset": 77}, {"referenceID": 74, "context": "In such a problem, robots can make small movements to optimize the wireless connectivity with their neighbors, without affecting the network topology (Choxi & Modi, 2007; Jain et al., 2009). Jain et al. (2009) proposed a DCOP formulation where each robot is represented by an agent.", "startOffset": 171, "endOffset": 210}, {"referenceID": 74, "context": "In such a problem, robots can make small movements to optimize the wireless connectivity with their neighbors, without affecting the network topology (Choxi & Modi, 2007; Jain et al., 2009). Jain et al. (2009) proposed a DCOP formulation where each robot is represented by an agent. Each agent controls one variable describing the decision on the robots\u2019 possible movements. Thus, the variables\u2019 domains consist of the valid positions the agent can move to. The cost functions of the problem model the power loss (or gain, depending on the optimization criteria) of the wireless link from a transmitter and a receiver robot, and depend on their positions. Radio communication in wireless sensor networks have a predictable signal strength loss that is roughly inversely proportional to the square of the distance between transmitter and receiver. However, radio wave interference is very difficult to predict (Molisch, 2012). Thus, Jain et al. (2009) use a P-DCOPbased approach with partial agent knowledge to capture the robot\u2019s partial knowledge on its cost functions, and to balance exploration of the unknown costs and exploitation of the known portion of the costs.", "startOffset": 171, "endOffset": 951}, {"referenceID": 31, "context": "Chachra and Marefat (2006) proposed a DCOP model for this problem, where each sensor is an agent whose variables denote its status (on or off) for each time step.", "startOffset": 0, "endOffset": 27}, {"referenceID": 31, "context": "Chachra and Marefat (2006) proposed a DCOP model for this problem, where each sensor is an agent whose variables denote its status (on or off) for each time step. Hard constraints are employed to enforced that if a sensor is on, then all its neighbors should be off, and that sensors cannot stay on for two consecutive time steps. The overall objective is to minimize the delay induced in the network. A similar problem is solved by Stranders et al. (2009), where sensors are also able to harvest energy from the environment (e.", "startOffset": 0, "endOffset": 457}, {"referenceID": 78, "context": "Jin et al. (2011) proposed a DCOP model for the ACP problem where servers bid for a component to host, with an emphasis that is proportional to the affinity of the server characteristics and the component hardware and software requirements.", "startOffset": 0, "endOffset": 18}, {"referenceID": 126, "context": "Researchers have cast this problem as a DCOP (Miller et al., 2012; Jain et al., 2012; Gupta, Jain, Yeoh, Ranade, & Pontelli, 2013a; Athanasiadis, Kockar, & McArthur, 2013), considering a network of nodes (agents), each of which relays power to other nodes, but can also contain a combination of generators and loads.", "startOffset": 45, "endOffset": 171}, {"referenceID": 76, "context": "Researchers have cast this problem as a DCOP (Miller et al., 2012; Jain et al., 2012; Gupta, Jain, Yeoh, Ranade, & Pontelli, 2013a; Athanasiadis, Kockar, & McArthur, 2013), considering a network of nodes (agents), each of which relays power to other nodes, but can also contain a combination of generators and loads.", "startOffset": 45, "endOffset": 171}, {"referenceID": 201, "context": "A version of the ED problem with a planning horizon has been proposed by Fioretto, Yeoh, Pontelli, Ma, and Ranade (2017b). This formalization extends the ED model described above by capturing the physical restrictions of transmission lines, power loads, and power generators arising when deploying a sequence of solutions (set-points for generators and loads) over time.", "startOffset": 83, "endOffset": 122}, {"referenceID": 89, "context": "Researchers have proposed a DCOP formulation for this problem (Kumar et al., 2009; Agrawal, Kumar, & Varakantham, 2015).", "startOffset": 62, "endOffset": 119}, {"referenceID": 202, "context": "Gupta, Yeoh, Pontelli, Jain, and Ranade (2013b) formalized this problem as a DCOP, where agents represent nodes in the network, and each agent has its own power generation and power consumption capabilities.", "startOffset": 7, "endOffset": 48}, {"referenceID": 7, "context": "Finally, the impact of the device\u2019s action onto the building environment is captured by a set of soft constraints. The goal is that of finding an action assignment for the building devices which satisfies the given goals states while minimizing the some cost function (e.g., the energy consumption of each device). Within a smart city, the Smart Home Device Scheduling Problem (SHDS) proposed by Fioretto, Yeoh, and Pontelli (2017a) is the problem of coordinating the schedule of several smart homes\u2019 devices so to minimize the aggregated energy peaks as well as to minimize the users energy bill cost, when adopting a real-time energy price schema.", "startOffset": 77, "endOffset": 433}, {"referenceID": 191, "context": "In general, they rely on the notion of a Task Dependency Network (TDN) introduced by Walsh and Wellman (2000), which is a graphbased representation to capture dependencies among production processes.", "startOffset": 85, "endOffset": 110}, {"referenceID": 35, "context": "A dynamic version of the above formalization has been investigated by Chli and Winsper (2015). Such a model allows for the entry and departure of producers and consumers as well as changes in properties of the problem (e.", "startOffset": 70, "endOffset": 94}, {"referenceID": 79, "context": "de Oliveira, Bazzan, and Lesser (2005) and Junges and Bazzan (2008) model this problem as a DCOP, where agents represent traffic lights, each controlling one variable that models the coordination direction for the associated traffic signal.", "startOffset": 43, "endOffset": 68}, {"referenceID": 79, "context": "de Oliveira, Bazzan, and Lesser (2005) and Junges and Bazzan (2008) model this problem as a DCOP, where agents represent traffic lights, each controlling one variable that models the coordination direction for the associated traffic signal. Thus, the domain of the variables is given by two possible directions of coordination (north-south/south-north, east-west/west-east). Conflicts that arise when two neighboring traffic signals choose different directions are modeled as constraints. Cost functions are defined to model the number of incoming vehicles in a junction, and the costs are influenced by whether two adjacent agents agree on a direction of coordination or not. The goal is to minimize the global cost. Due to the dynamic nature of the problem, the proposed algorithms fall under the umbrella of Dynamic DCOP algorithms. Pham, Tawfik, and Taylor (2013) and Brys, Pham, and Taylor (2014) proposed a Probabilistic DCOP-based approach with partial agent knowledge to solve the traffic light synchronization problem in the context where agents have partial information about their cost functions.", "startOffset": 43, "endOffset": 868}, {"referenceID": 79, "context": "de Oliveira, Bazzan, and Lesser (2005) and Junges and Bazzan (2008) model this problem as a DCOP, where agents represent traffic lights, each controlling one variable that models the coordination direction for the associated traffic signal. Thus, the domain of the variables is given by two possible directions of coordination (north-south/south-north, east-west/west-east). Conflicts that arise when two neighboring traffic signals choose different directions are modeled as constraints. Cost functions are defined to model the number of incoming vehicles in a junction, and the costs are influenced by whether two adjacent agents agree on a direction of coordination or not. The goal is to minimize the global cost. Due to the dynamic nature of the problem, the proposed algorithms fall under the umbrella of Dynamic DCOP algorithms. Pham, Tawfik, and Taylor (2013) and Brys, Pham, and Taylor (2014) proposed a Probabilistic DCOP-based approach with partial agent knowledge to solve the traffic light synchronization problem in the context where agents have partial information about their cost functions.", "startOffset": 43, "endOffset": 902}, {"referenceID": 171, "context": "Constraints have been long studied in centralized systems (Rossi et al., 2006) and have been proved especially practical and efficient for modeling and solving resource allocation and scheduling problems.", "startOffset": 58, "endOffset": 78}, {"referenceID": 89, "context": "For instance, as outlined by Kumar and Zilberstein (2011) and Ghosh, Kumar, and Varakantham (2015), one can use inference-based algorithms, such as expectation-maximization, and convex optimization machinery to develop efficient message-passing algorithm for solving large DCOPs.", "startOffset": 29, "endOffset": 58}, {"referenceID": 89, "context": "For instance, as outlined by Kumar and Zilberstein (2011) and Ghosh, Kumar, and Varakantham (2015), one can use inference-based algorithms, such as expectation-maximization, and convex optimization machinery to develop efficient message-passing algorithm for solving large DCOPs.", "startOffset": 29, "endOffset": 99}, {"referenceID": 213, "context": "Within the current incomplete methods proposed, a considerable effort has been employed in developing anytime algorithms (Zilberstein, 1996).", "startOffset": 121, "endOffset": 140}, {"referenceID": 67, "context": "In addition to the anytime mechanism, which constrains the problem resolution within a particular time requirement, any-space algorithms have been proposed to limit the amount of space needed to an agent during problem resolution (Petcu & Faltings, 2007a; Yeoh et al., 2009b, 2011; Gutierrez et al., 2011).", "startOffset": 230, "endOffset": 305}, {"referenceID": 211, "context": "For example, Zhang and Lesser (2013) study agent learning, where coordination is driven by a dynamic decomposition of a DCOP, each solving smaller independent subproblems.", "startOffset": 13, "endOffset": 37}, {"referenceID": 208, "context": "There are two commonly used reformulation techniques (Burke & Brown, 2006; Yokoo, 2001): (i) Compilation (or complex variables) where each agent creates a new pseudo-variable whose domain is the Cartesian product of the domains of all variables of the agent; and (ii) Decomposition (or virtual agents) where each agent creates a pseudo-agent for each of its variables.", "startOffset": 53, "endOffset": 87}, {"referenceID": 45, "context": "One can even exploit novel hardware platforms, such as graphical processing units (GPUs) to parallelize such solvers (Fioretto et al., 2016a; Fioretto, Le, Yeoh, Pontelli, & Son, 2015; Bistaffa, Farinelli, & Bombieri, 2014). Researchers have proposed re-designed versions of existing algorithms to be able to handle multi-variables agents problems (Davin & Modi, 2006; Khanna, Sattar, Hansen, & Stantic, 2009; Portway & Durfee, 2010). Additionally, recently, Grinshpoun (2015) and Fioretto, Yeoh, and Pontelli (2016b) have proposed general DCOP problem decompositions which are able to solve problems where agents control multiple variables.", "startOffset": 118, "endOffset": 477}, {"referenceID": 45, "context": "One can even exploit novel hardware platforms, such as graphical processing units (GPUs) to parallelize such solvers (Fioretto et al., 2016a; Fioretto, Le, Yeoh, Pontelli, & Son, 2015; Bistaffa, Farinelli, & Bombieri, 2014). Researchers have proposed re-designed versions of existing algorithms to be able to handle multi-variables agents problems (Davin & Modi, 2006; Khanna, Sattar, Hansen, & Stantic, 2009; Portway & Durfee, 2010). Additionally, recently, Grinshpoun (2015) and Fioretto, Yeoh, and Pontelli (2016b) have proposed general DCOP problem decompositions which are able to solve problems where agents control multiple variables.", "startOffset": 118, "endOffset": 518}, {"referenceID": 191, "context": "There are currently several MAS simulators which have been adopted by researchers to develop and compare DCOP algorithms: \u2022 DisChoco (Wahbi et al., 2011) is a Java open-source framework for simulating DCOP algorithms where agents are executed asynchronously (each in a separate execution thread) and communication is implemented via messages passing.", "startOffset": 133, "endOffset": 153}, {"referenceID": 99, "context": "\u2022 Frodo (L\u00e9aut\u00e9 et al., 2009) is a Java open-source framework for DCOP which implements several complete and incomplete classic DCOP algorithms, including DPOP and some of it variants, Max-Sum, MGM, and DSA.", "startOffset": 8, "endOffset": 29}, {"referenceID": 87, "context": "For a comprehensive review of multi-agent platforms, we refer the interested reader to the survey by Kravari and Bassiliades (2015). The development of DCOP simulators have provided researchers with useful tools to develop new DCOP algorithms and to facilitate comparison of existing DCOP algorithms.", "startOffset": 101, "endOffset": 132}, {"referenceID": 106, "context": "There are multiple metrics that can be used to measure the runtime of an algorithm, such as the number of non-concurrent constraint checks (NCCCs) (Meisels, Kaplansky, Razgon, & Zivan, 2002), the simulated runtime (Sultanik, Lass, & Regli, 2008), and the number of cycles (Lynch, 1996).", "startOffset": 272, "endOffset": 285}], "year": 2017, "abstractText": "The field of Multi-Agent System (MAS) is an active area of research within Artificial Intelligence, with an increasingly important impact in industrial and other real-world applications. In a MAS, autonomous agents interact to pursue personal interests and/or to achieve common objectives. Distributed Constraint Optimization Problems (DCOPs) have emerged as a prominent agent model to govern the agents\u2019 autonomous behavior, where both algorithms and communication models are driven by the structure of the specific problem. During the last decade, several extensions to the DCOP model have been proposed to enable support of MAS in complex, realtime, and uncertain environments. This survey provides an overview of the DCOP model, offering a classification of its multiple extensions and addressing both resolution methods and applications that find a natural mapping within each class of DCOPs. The proposed classification suggests several future perspectives for DCOP extensions, and identifies challenges in the design of efficient resolution algorithms, possibly through the adaptation of strategies from different areas.", "creator": "TeX"}}}