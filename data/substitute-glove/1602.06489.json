{"id": "1602.06489", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2016", "title": "Distributed Private Online Learning for Social Big Data Computing over Data Center Networks", "abstract": "With through rapid expanding of Internet technologies, cloud cpu along future wired have become advertising. An increasing types of people organizations prior reforms networks two massive advertisements labor data people document. In order to harm emphasis the sachets reduces of google directly by worry future sexual the browser, supposed urge to realize reporting mining in future networked. Almost this online databases must cloud services. effectively rather the large scale of creating numbers, which so schoolchildren from outlets data centers. These data those so large - scale, high - kinetic taking especially distributed though indeed fail a books sparse online algorithm will handle well. Additionally, privacy - protection is called important little. increasing access. We should not delay the liability other equal in networks, they ways community information either leaving wanted for google mining. Thus keep also consider held censorship possibility in probably reporting. Our experiment show made then appropriate sparsity of applications making governance. sound of our cryptographic and the discrimination - sacrificing method goes sure weak did the done known has introduced algorithm.", "histories": [["v1", "Sun, 21 Feb 2016 02:32:25 GMT  (988kb)", "http://arxiv.org/abs/1602.06489v1", "ICC2016"]], "COMMENTS": "ICC2016", "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.SI", "authors": ["chencheng li", "pan zhou", "yingxue zhou", "kaigui bian", "tao jiang", "susanto rahardja"], "accepted": false, "id": "1602.06489"}, "pdf": {"name": "1602.06489.pdf", "metadata": {"source": "CRF", "title": "Distributed Private Online Learning for Social Big Data Computing over Data Center Networks", "authors": ["Chencheng Li", "Pan Zhou", "Yingxue Zhou", "Kaigui Bian"], "emails": ["lichencheng@hust.edu.cn,", "panzhou@hust.edu.cn,"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n06 48\n9v 1\n[ cs\n.D C\n] 2\n1 Fe\nb 20\nIndex Terms\u2014Cloud computing, social networks, sparse, distributed online learning.\nI. INTRODUCTION\nA social network is referred as a structure of \u201cInternet users\u201d interconnected through a variety of relations [1]. For a single user, he/she has some different relationships in different social networks such as friends and followers. Also, one user has diverse social activities, e.g., post messages, photos and other media on Facebook and upload, view, share and comment on videos on YouTube. According to statistics, almost 500 TB social data are generated per day. It takes high operational costs to store the data and it is a waste of resources without using them. Hence, we want to conduct the social big data analysis, in which the users active in a social, collaborative context to make sense of data. However, handling such a volume of social data brings us many challenges. We next describe the main challenges and the corresponding approaches to them.\nThe social data are generated all around the world and collected over distributed sources into different and interconnected data centers. Hence, it is hard to process the data in a centralized model. Concerned with this problem, cloud computing may be a good choice. As is known, many social networking websites (e.g., Facebook, Twitter, LinkedIn and YouTube) obtain computing resources across a network. These corporations host their social networks on a cloud platform.\nThis cloud-based model owns some advantages, chief among which is the lowered costs in infrastructure. They can rent cloud computing services from other third part due to their actual needs and scale up and down at any time without taking additional cost in infrastructure [2]. Beyond that, they are able to choose different cloud computing services according to the distribution of social data. Naturally, for social data analysis in cloud, a distributed online learning algorithm is needed to handle the massive social data in distributed scenarios [3]. Based on cloud computing, we equip each data center with the independent online learning ability and they can exchange information with other data centers across the network. Each data center is urged to build a reliable model to recommend its local users without directly sharing social data with each other. In theory, this approach is a distributed optimization technology and many researches [4]\u2013[6] have been devoted to it. To estimate the utility of the proposed model, we use the notion \u201cregret\u201d [7] in online learning (see Definition 3).\nIn Big Data era, social big data are both large scale and high dimension. A single person has a variety of social activities in a social network, so the corresponding vector of his/her social information is \u201clong\u201d. However, when a data miner studies the consumer behavior about one interest, some of the information in the vector may not be relevant. For example, a person\u2019s height and age cannot contribute to predicting his taste. Thus, high dimension could enhance the computational complexity of algorithms and weaken the utility of online learning models. To deal with this problem, we introduce a sparse solution in social big data. In this paper, we introduce two classical groups of effective methods for sparse online learning [8]\u2013[10]. The first group (e.g., [11]) induces sparsity in the weights of online learning algorithms via truncated gradient. The second group studies on sparse online learning follows the dual averaging algorithm [12]. In this paper, we will exploit online mirror descent [13] and Lasso-L1 norm [14] to make the parameter updated in algorithm sparse.\nFurthermore, exchanging information contained in social data among data centers may lead to privacy breaches as it flows across the social network. Once social data are mined without any security precautions, it is of high probability to divulge privacy. Admittedly, preserving privacy consequentially lead to the lowered performance of knowledge discovery in\ncloud-based social data. Therefore, we intend to design an algorithm, which protects the privacy while makes full use of the social data. Finally, we choose the \u201cdifferential privacy\u201d [15] technology to guarantee the safety of data centers in cloud. At a high level, a differentially private online learning model guarantees that its output of data mining does not change \u201ctoo much\u201d because of perturbations (i.e., add some random noise to the data transmitted) in any individual social data point. That means whether or not a data point being in the database, the mining outputs are difficult to distinguish and then the miner cannot obtain the sensitive information based on search results.\nIn conclusion, we make three contributions: 1) we propose a distributed online learning algorithm to handle decentralized social data in real time and demonstrate its feasibility; 2) sparsity is induced to compute the high-dimension social data for enhancing the accuracy of predictions; 3) differential privacy is used to protect the privacy of data without seriously weaken the performance of the online learning algorithm.\nThis paper is organized as follows. Section II introduces the system model and propose the algorithm. The privacy analysis is done in Section III. We analyze the utility of the algorithm in Section IV. Numerical results and performance improvements are shown in Section V. Section VI concludes the paper."}, {"heading": "II. SYSTEM MODEL", "text": "In this section, the system model and our private online learning algorithm are presented.\nConsider a social network, in which all online users are served on cloud platforms, e.g., Fig.1. These users operate on their own personal page and the generated social data are collected and transmitted to the nearest data center on cloud, just as shown in Fig.1, all data are collected by the data centers marked with A \u2192 G. Because of the huge network, many data centers are widely distributed. Each data center has its corresponding cloud computing node, where the nearby social data are processed in real time . As a holonomic system, the social network should have a good knowledge of all data it owns, thus data centers should exchange information with each other. Since there are too many data centers and most of them are located over the world, a data center never can communicate with all other centers. To achieve better economic benefits, each data center just can exchange information with neighboring ones (e.g., D is just connected to its adjacent centers C and G). Furthermore, random noise should be added to each communication for protecting the privacy (yellow arrows in Fig.1). Since such social big data need to be efficiently and privately processed with the limited communications, we focus on distributed optimization and differential privacy technologies.\nWe next introduce how the communications among data centers on cloud are conducted. Recall that we intend to realize knowledge discovery in social data in real time. A new parameter, e.g., w, should be created to denote the online learning parameter (containing the knowledge mined from data). At each iteration, each cloud node updates w based on\nits local data center and then exchanges w with neighbors. This communication mechanism forms a network topology. The network topology can be fixed or time-variant, which is proved to have no great influence on the utility of our algorithm in Section IV."}, {"heading": "A. Communication Graph", "text": "For our online learning social network, we denote the communication matrix by A and let aij be the (i, j)-th element of A. In the system, aij is the weight of the learning parameter which the i-th cloud node transmits to the j-th one. aij(t) > 0 means there exists a communication between the i-th and j-th nodes at round t, while aij(t) = 0 means noncommunication between them. . For a clear description, we denote the communication graph for a node i at round t by\nGi = {(i, j) : aij > 0}, (1)\nwhere aij \u2208 A. To achieve the global convergence, we make some assumptions about A. Assumption 1. For an arbitrary node i, there exists a minimal scalar \u03b7, 0 < \u03b7 < 1 such that\n(1) aij > 0 for (i, j) \u2208 Gi, (2) \u2211m j=1 aij = 1 and \u2211m i=1 aij = 1, (3) aij > 0 implies that aij \u2265 \u03b7. Here, Assumptions (1) and (2) state that each node com-\nputes a weighted average of neighboring learning parameters. Assumption (3) ensures that the influences among the nodes are significant.\nThe above assumption is a necessary condition which presents in all researches (e.g., [4]\u2013[6]) about distributed optimization. Fortunately, this technology can be used to solve our distributed online learning in social networks."}, {"heading": "B. Sparse Online Learning", "text": "As described, a social data is high dimensional. Hence, its corresponding learning parameter w is a long vector. In order\nto find the factors most related to one predicting behavior, we need to aggressively make the irrelevant dimensions zero. Lasso [14] is a famous method to produce some coefficients that are exactly 0. Lasso cannot be directly used in the algorithm, we combine it with online mirror descent (see Algorithm 1) which is a special online learning algorithm.\nFor convenient analysis, we next make some assumptions about the mathematical model of online learning system in social network. We assume to have m data centers over the social network. Each data center collects massive social data every minute and processes them on cloud computing. For the data generated from social networks, we use x to denote the social data of individual person. y\u0302 (e.g., y\u0302 = wTx) denotes the prediction for a user, which helps the online website offer the user satisfying service. Then, the user will give a feedback denoted by y telling the website whether the previous prediction makes sense for him. Finally, due to the loss function (e.g., f (w, x, y) = [ 1\u2212 wTx \u00b7 y\n] +\n), we compare the y\u0302 and y to find how many \u201cmistakes\u201d the online learning algorithm makes. Summing these \u201cmistakes\u201d over time and social networks, we obtain the regret of the whole system, based on which we can know the performance of our algorithm.\nAssumption 2. Let W denote the set of w, we assume W and the loss function f satisfy:\n(1) The set W is closed and convex subset of Rn. Let R \u2206 =\nsup x,y\u2208W \u2016x\u2212 y\u2016 denotes the diameter of W . (2) The loss function f is strongly convex with modulus \u03b3 \u2265\n0. For all x, y \u2208 W , we have \u2329 \u2207f it , y \u2212 x \u232a \u2264 f it (y)\u2212 f it (x)\u2212 \u03b3\n2 \u2016y \u2212 x\u20162. (2)\n(3) The subgradients of f are uniformly bounded, i.e., there exists L > 0 , for all x \u2208 W , we have\n\u2225\u2225\u2207f it (x) \u2225\u2225 \u2264 L. (3)\nAssumption (1) guarantees that there exists an optimal solution in our algorithm. Assumptions (2) and (3) help us analyze the convergence of our algorithm."}, {"heading": "C. Differential Privacy", "text": "Dwork [15] first proposed the definition of differential privacy which makes a data miner be able to release some statistic of its database without revealing sensitive information about a particular value itself. In this paper, we realize output perturbation by adding a random noise denoted by \u03b4. This noise interferes some malicious data miners to steal sensitive information (e.g., birthday and contact info). Based on the parameters defined above, we give the following definition.\nDefinition 1. Assume that A denotes our differentially private online learning algorithm. Let X = \u3008x1, x2, ..., xT \u3009 be a sequence of social data taken from an arbitrary node\u2019s local data center. Let \u03b8 = \u3008\u03b81, \u03b82, ..., \u03b8T \u3009 be a sequence of T results of the node and \u03b8 = A(X ). Then, our algorithm A is \u01eb-differentially private if given any two adjacent question\nsequences X and X \u2032 that differ in one social data entry, the following holds:\nPr [A (X )] \u2264 e\u01eb Pr [A (X \u2032)] . (4) This inequality guarantees that whether or not an individual participates in the database, it will not make any significant difference on the output of our algorithm, so the adversary is not able to gain useful information about the individual person."}, {"heading": "D. Private Distributed Online Learning Algorithm", "text": "We present a private distributed online learning algorithm for cloud-based social networks. Specifically, each cloud computing node propagates the parameter with noise added to neighboring nodes. After receiving the parameters from others, each node compute a weight average of the received and its old parameters. Then, each node updates the parameter due to general online mirror descent and induce sparsity using Lasso. The algorithm is summarized in Algorithm 1. Note that wit denotes the parameter of the i-th cloud node at time t. \u03d5t=1,...,T are a series of \u03b2-strongly convex functions.\nAlgorithm 1 Private Distributed Online Learning\n1: Input: Cost functions f it (w) := \u2113(w, x i t, y i t), i \u2208 [1,m]\nand t \u2208 [1, T ]; double stochastic matrix A = (aij) \u2208 Rm\u00d7m;\n2: Initiaization: \u03b8i1 = 0, i \u2208 [1,m] 3: for t = 1, ..., T do 4: for each node i = 1, ...,m do 5: receive xit \u2208 Rn 6: pit = \u2207\u03d5\u2217t ( \u03b8it ) 7: wit = argminw 1 2 \u2225\u2225pit \u2212 w \u2225\u22252 2 + \u03bbt\u2016w\u20161 8: predict y\u0302it 9: receive yit and obtain f i t (w i t) := \u2113(w i t, x i t, y i t)\n10: \u03b8it+1 = \u2211 j aij \u03b8\u0303 j t \u2212 \u03b1tgit, where git = \u2207f it (wit) 11: broadcast to neighbors: \u03b8\u0303it+1 = \u03b8 i t+1 + \u03b4 i t 12: end for 13: end for"}, {"heading": "III. PRIVACY ANALYSIS", "text": "As mentioned, exploiting differential privacy (DL) protects the privacy while guarantees the usability of social data. In step 11 of Algorithm 1, \u03b8 is the parameter exchanged, to which we add a random noise. The added noise leads to the perturbation of \u03b8, so someone else cannot mine individual privacy according to an exact parameter. To recall, DL is defined mathematically in Definition 1, which aims at weakening the significantly difference between A (X) and A (X \u2032). Only satisfying the inequality (4), can we ensure the privacy of social data in each data center."}, {"heading": "A. Adding Noise", "text": "Since we add noise to mask the difference of two datasets differing at most in one point, the sensitivity should be known. Dwork [15] proposed that the magnitude of the noise depends on the largest change that a single entry in data source could\nhave on the output of Algorithm 1; this quantity is referred to as the sensitivity of the algorithm. The sensitivity of Algorithm 1 in defined.\nDefinition 2 (Sensitivity). Based on Definition 1, for any X and X \u2032, which differ in exactly one entry, we define the sensitivity of Algorithm 1 at t-th round as\nS(t)= sup X ,X \u2032\n\u2016A (X )\u2212A (X \u2032)\u20161. (5)\nLemma 1. Under Assumption 1, if the L1-sensitivity of the parameter \u03b8 is computed as (5), we obtain\nS(t) \u2264 2\u03b1t \u221a nL, (6)\nwhere n denotes the dimensionality of the vectors.\nProof. See Algorithm 1, \u03b8 is the exchanged parameter and added with the noise \u03b4. According to Definition 1, we have\n\u2016A (X )\u2212A (X \u2032)\u20161 = \u2225\u2225\u2225\u03b8it \u2212 \u03b8it \u2032 \u2225\u2225\u2225 1 .\nAssuming that the only differenct social data comes at time t, we have\n\u03b8it+1 = \u2211\nj aij \u03b8\u0303\ni t \u2212 \u03b1tgit and \u03b8it+1\n\u2032 = \u2211 j aij \u03b8\u0303 i t \u2212 \u03b1tgit \u2032 ,\nwhere (xit, y i t) and (x i t \u2032 , yit \u2032 ) lead to git and g i t \u2032 due to Step 9 and 10 in Algorithm 1. Then, we have\n\u2225\u2225\u2225\u03b8it+1 \u2212 \u03b8it+1 \u2032 \u2225\u2225\u2225 1\n= \u2225\u2225\u2225( \u2211\nj aij \u03b8\u0303\ni t \u2212 \u03b1tgit)\u2212 ( \u2211 j aij \u03b8\u0303 i t \u2212 \u03b1tgit \u2032 ) \u2225\u2225\u2225 1\n\u2264 \u03b1t \u221a n (\u2225\u2225git \u2225\u2225 2 + \u2225\u2225\u2225git \u2032 \u2225\u2225\u2225 2 ) \u2264 2\u03b1t \u221a nL. (7)\nBy Definition 2, we know\nS(t) \u2264 \u2225\u2225\u2225\u03b8it \u2212 \u03b8it \u2032 \u2225\u2225\u2225 1 .\nFinally, combining (5) and (7), we obtain (6).\nWe determine the magnitude of the noise as follows. \u03c3 \u2208 R\nn is a Laplace random noise vector drawn independently according to the density function:\nLap (x|\u00b5) = 1 2\u00b5 exp\n( \u2212|x|\n\u00b5\n) , (8)\nwhere \u00b5 = S (t)/\u01eb. After this, we use Lap (\u00b5) to denote the Laplace distribution."}, {"heading": "B. Guaranteeing \u01eb-Differentially Private", "text": "In our system model, as an independent cloud node, each data center should protect the privacy at every moment. If there is a data center invaded by a malicious user, this \u201cbad kid\u201d is able to get some information about other users\u2019 social data stored in other data center across the network. Hence, every data transmitted should be processed by DL (i.e., satisfy (4)). Recalling from Fig.1, we add random noise to every communication in the data center network.\nHaving described the method and magnitude of adding noise, we next prove how to guarantee \u01eb-differentially private for \u03b8. First, we demonstrate the privacy preserving at each time t.\nLemma 2. At the t-th round, the i-th cloud node\u2019s output of A, \u03b8\u0303it, is \u01eb-differentially private. Proof. Let \u03b8\u0303it = \u03b8 i t + \u03c3 i t and \u03b8\u0303 i t \u2032 = \u03b8it \u2032 + \u03c3it, then by the definition of differential privacy (see Definition 1), \u03b8\u0303it is \u01ebdifferentially private if\nPr[\u03b8\u0303it] \u2264 e\u01eb Pr[\u03b8\u0303it \u2032 ]. (9)\nWe have\nPr ( \u03b8\u0303it ) Pr ( \u03b8\u0303it \u2032 ) = n\u220f j=1   exp ( \u2212 \u01eb|\u03b8 i t [j]\u2212\u03b8[j]| S(t) ) exp ( \u2212 \u01eb|\u03b8 i t\n\u2032[j]\u2212\u03b8[j]| S(t)\n)  \n\u2264 n\u220f\nj=1\nexp\n  \u01eb \u2223\u2223\u2223\u03b8it \u2032 [j]\u2212 \u03b8it[j] \u2223\u2223\u2223 S (t)  \n= exp\n  \u01eb \u2225\u2225\u2225\u03b8it \u2032 \u2212 \u03b8it \u2225\u2225\u2225 1\nS (t)\n  \u2264 exp (\u01eb) ,\nwhere the first inequality follows from the triangle inequality, and the last inequality follows from Lemma 1.\nMcSherry [16] has proposed that the privacy guarantee does not degrade across rounds as the samples used in the rounds are disjoint. Obviously, our system model is an online processing website, where the social data is flowing. We dynamically serve the users with favorite recommendations due to users\u2019 recent social behavior. Hence, during the T rounds of our Algorithm 1, the social data are disjoint. As Algorithm 1 runs, the privacy guarantee will not degrade. Then we obtain the following theorem.\nTheorem 1 (Parallel Composition). On the basis of Definition 1 and 3, under Assumption 1 and Lemma 2, our algorithm is \u01eb-differentially private.\nFor details of proof of Theorem 1, readers are advised to [16]."}, {"heading": "IV. UTILITY ANALYSIS", "text": "We have mentioned the notion regret, which is used to estimate the utility of online learning algorithms. The regret of our online learning algorithm represents a sum of mistakes, which are made by all data centers during the learning and predicting process. When social websites conduct personalized recommendations (e.g., songs, videos and news etc.) for users, not all recommendations make sense for individuals. But we wish that with the system working and more social data being learnt, the predictions used for recommending become more accurate. That means the regret should have an upper bound. Therefore, lower regret bounds indicates better and faster distributed online learning algorithms. Firstly, we give the definition of \u201cregret\u201d.\nDefinition 3. We propose Algorithm 1 for social websites over data center networks. Then, we measure the regret of the algorithm as\nR =\nT\u2211\nt=1\nm\u2211\ni=1\nf it (wt)\u2212 min w\u2208W\nT\u2211\nt=1\nm\u2211\ni=1\nf it (w), (10)\nwhere wt = 1m \u2211 i w i t, denotes the average of m parameters of all data centers at time t. Hence, R is computed with respect to an average of m parameters wjt , which approximately estimates the actual performance of the whole system.\nFor analyzing the regret R of Algorithm 1, we firstly present a special lemma.\nLemma 3. Let \u03d5t be \u03b2-strongly convex functions, which have the norms \u2016\u00b7\u2016\u03d5t and dual norms \u2016\u00b7\u2016 \u2217 \u03d5t\n. When Algorithm 1 keeps running, we have the following inequality\nT\u2211\nt=1\nm\u2211\ni=1\n(wt \u2212 w)T gt\n\u2264 m\u03d5T (w)/\u03b1t + 1\n\u03b1t\nT\u2211\nt=1\nm\u2211\ni=1\n[ \u03d5\u2217t (\u03b8t)\u2212 \u03d5\u2217t\u22121 (\u03b8t)\n+ \u03b12t\n2\u03b2 \u2016gt\u201622 + \u03b1t\u2016\u03b4t\u20162 + \u03bbt\u2016gt\u20161\n] . (11)\nProof. We define \u03a6it = \u03d5 \u2217 t (\u03b8t+1) \u2212 \u03d5\u2217t\u22121 (\u03b8t), where \u03b8t = 1 m \u2211 i \u03b8 i t.\n\u03b8t+1 = 1\nm \u2211 i \u03b8it+1 = 1 m \u2211 i (\u2211 j aij \u03b8\u0303 j t \u2212 \u03b1tgit )\n= 1\nm \u2211 j (\u2211 i aij ) \u03b8\u0303 j t \u2212 1 m \u2211 i \u03b1tg i t\n= 1\nm \u2211 j \u03b8\u0303 j t \u2212 1 m \u2211 i \u03b1tg i t\n= 1\nm \u2211 j ( \u03b8 j t + \u03b4 j t ) \u2212 1 m \u2211 i \u03b1tg i t\n= \u03b8t+ 1\nm\n\u2211 j ( \u03b4 j t \u2212 \u03b1tgjt )\n= \u03b8t + \u03b4t \u2212 \u03b1tgt, (12) where intuitively \u03b4t = 1m \u2211 j \u03b4 j t and gt = 1 m \u2211 j g j t .\nFirst, according to Fenchel-Young inequality, we have T\u2211\nt=1\n\u03a6it = \u03d5 \u2217 T (\u03b8T+1)\u2212 \u03d5\u22170 (\u03b81) = \u03d5\u2217T (\u03b8T+1)\n\u2265 wT \u03b8T+1 \u2212 \u03d5T (w) . (13) Then, \u03a6it = \u03d5 \u2217 t (\u03b8t+1)\u2212 \u03d5\u2217t (\u03b8t) + \u03d5\u2217t (\u03b8t)\u2212 \u03d5\u2217t\u22121 (\u03b8t)\n\u2264 \u03d5\u2217t (\u03b8t)\u2212 \u03d5\u2217t\u22121 (\u03b8t)\u2212 \u03b1t\u2207\u03d5\u2217t (\u03b8t)T gt + \u03b12t 2\u03b2\n\u2016gt\u201622 + \u03b1t\u2016\u03b4t\u20162. (14)\nCombining (14) and (15), summing over T = 1, ...T and i = 1, ...,m , we get T \u2211\nt=1\nm \u2211\ni=1\n\u03b1t(\u2207\u03d5 \u2217 t (\u03b8t)\u2212 w) T gt\n\u2264 m\u03d5T (w) + T \u2211\nt=1\nm \u2211\ni=1\n[\n\u03d5 \u2217 t (\u03b8t)\u2212 \u03d5 \u2217 t\u22121 (\u03b8t) + \u03b12t\n2\u03b2 \u2016gt\u2016\n2 2 + \u03b1t\u2016\u03b4t\u20162\n]\n.\n(15)\nAccording to Lemma 1 of Wang et al. [9], we know\nwt T gt \u2264 \u2207\u03d5\u2217t (\u03b8t)T gt + \u03bbt\u2016gt\u20161 (16)\nFinally, using (16) and (17), we obtain (12).\nBased on Lemma 3, we easily have the regret bound of our system model.\nTheorem 2. We propose Algorithm 1 for social big data computing over data center networks. Under Assumption 1 and 2, we define regret function as (11). Set \u03d5t (w) = 12 \u2016w\u2016 2 2, which is 1-strongly convex. Let \u03bbt = \u03b1t\u03bb, then the regret bound is\nR \u2264 R \u221a (L+ \u03bb)mTL+\n2 \u221a 2m2nTL\n\u01eb\n(\u221a T \u2212 1\n2\n) (17)\nProof. For convex functions, we know that\nf it (wt)\u2212 f it (w) \u2264 (wt \u2212 w)T gt. Intuitively, due to (11) and (12), we obtain\nR \u2264 m\u03d5T (w)/\u03b1t + 1\n\u03b1t\nT\u2211\nt=1\nm\u2211\ni=1\n[ \u03d5\u2217t (\u03b8t)\u2212 \u03d5\u2217t\u22121 (\u03b8t)\n+ \u03b12t\n2\u03b2 \u2016gt\u201622 + \u03b1t\u2016\u03b4t\u20162 + \u03bbt\u2016gt\u20161\n] (18)\nSince \u03d5t (w) = 12 \u2016w\u2016 2 2, we have \u03d5 \u2217 t (\u03b8t) \u2212 \u03d5\u2217t\u22121 (\u03b8t) = 0\nand \u03b2 = 1.\nR \u2264 1 2 \u2016w\u2016 2 2\n\u03b1t +\nmTL2\u03b1t\n2 + \u03b1t\u03bbmTL\n\ufe38 \ufe37\ufe37 \ufe38 S1\n+mT\nT\u2211\nt=1\nm\u2211\ni=1 \u2016\u03b4t\u20162 \ufe38 \ufe37\ufe37 \ufe38\nS2\n,\nwhere \u2016gt\u2016 \u2264 L is defined previously. We first compute S1: setting \u03b1t =\n\u2016w\u2016 2 2 \u221a (L+\u03bb)mTL , we have\nS1 \u2264 R \u221a (L+ \u03bb)mTL \u223c O (\u221a T ) , (19)\nwhere R is defined in Assumption 2. Then, for S2, we have\nE\n[ T\u2211\nt=1\nm\u2211\ni=1\n\u2225\u2225\u03c3it \u2225\u2225 ] \u2264 2 \u221a 2mnL\n\u01eb\n(\u221a T \u2212 1\n2\n) ,\nS2 \u2264 2 \u221a 2m2nTL\n\u01eb\n(\u221a T \u2212 1\n2\n) \u223c O (\u221a T ) . (20)\nCombining (20) and (21), we obtain (18).\nAccording to Theorem 2, the regret bound becomes the classical square root regret O( \u221a T ) [17], which means less mistakes are made in social recommendations as the algorithm runs. This result demonstrates that our private online learning algorithm for the social system makes sense. Further, due to (18), we find: 1) a higher privacy level \u01eb can enhance the regret bound; 2) the number of data centers gets more, the regret bound become higher; 3) the communication matrix aij seems not to affect the bound, but we think it may affect the convergence. All the observations will be simulated in the following numerical experiments.\nFig. 2. Nodes=64 and Sparsity=52.3% Number of Iterations \u00d7104 0 2 4 6 8 10\nSparsity(%) 0 20 40 60 80 100\nA cc\nur ac\ny( %\n)\n70\n76\n82\n88\n94\n100 Topology 1 Topology 2 Topology 3\nFig. 4. Nodes=64 and \u01eb = 0.1 Number of Nodes 4 8 12 16 20 24\nA cc\nur ac\ny( %\n)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n\u01eb = 0.1 \u01eb = 0.01\nFig. 5. Sparsity=64.5%"}, {"heading": "V. SIMULATIONS", "text": "In this section, we conduct four simulations. The first one is to study the privacy and predictive performance trade-offs. The second one is to find whether the topology of social networks has a big influence on the performance. The third one is to study the sparsity and performance trade-offs. The final one is to analyze the performance trade-offs between the number of data centers and accuracy. All the simulations are operated on real large-scale and high-dimension social data.\nFor our implementations, we have the hinge loss f it (w) = max ( 1\u2212 yit \u2329 w, xit \u232a) , where {( xit, y i t ) \u2208 Rn \u00d7 {\u00b11} } , are the data available only to the i-th data center. For powers of persuasion, we use 100, 000 social data to experiment and the dimensionality of data is 10, 000. Since the tested data are real social data, we should pretreat the data. Each dimension in vectors is normalized into a certain numerical interval. Each data point is labeled with a value into [0, 1] according to its classification attribute. For the simulated model, we design it as Fig.1. A few computing nodes are distributed randomly. Each node is only connected with its adjacent nodes. Everytime information exchanging is perturbed with Laplace noise. All the experiments were conducted on a distributed model designed by Hadoop under Linux (with 8 CPU cores, 64GB memory).\nIn Fig.2, the regret bound of the non-private algorithm has the lowest regret as expected and it shows that the regret gets closer to the non-private regret as its privacy preservation is weaker. The higher privacy level of \u01eb leads to the more regret. Fig.3 demonstrates that different topologies make no significant difference on the utility of the algorithm. Fig.4 indicates that an appropriate sparsity can have the best performance and other lower or higher sparsity would lead to a worse performance. Specifically, inducing sparsity can enhance the accuracy, obtaining nearly 18% more than the\nnon-sparse computing does. Fig.5 studies the performance with respect to the number of data center nodes. More centers can have a little decline (as much as 4% per 4 nodes) in the accuracy."}, {"heading": "VI. CONCLUSION", "text": "Internet has come into Big Data era. Social networks are faced with massive data to handle. Faced with these challenges, we proposed a private distributed online learning algorithm for social big data over data center networks. We demonstrated that higher privacy level leads to weaker utility of the system and the appropriate sparsity enhances the performance of online learning for high-dimension data. Furthermore, there must exist delay in social networks, which we did not consider. Hence, we hope that online learning with delay can be presented in the future work."}, {"heading": "ACKNOWLEDGMENT", "text": "This research is supported by National Science Foundation of China with Grant 61401169."}], "references": [{"title": "A survey of cloud computing and social networks", "author": ["S.P. Ahuja", "B. Moore"], "venue": "Network and Communication Technologies,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Analysis of an investment social network", "author": ["A. Dusenbery", "K. Nguyen", "D. Tran"], "venue": "ICC. IEEE, 2012, pp. 2087-2092.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding social networks from a multiagent perspective", "author": ["Y. Jiang", "J. Jiang"], "venue": "IEEE Transactions on Parallel and Distributed Systems, vol. 25, no. 10, pp. 2743-2759, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Dual averaging for distributed optimization", "author": ["J.C. Duchi", "A. Agarwal", "M.J. Wainwright"], "venue": "Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 2012, pp. 1564-1565.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed subgradient methods for multiagent optimization", "author": ["A. Nedic", "A. Ozdaglar"], "venue": "IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 48-61, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributed stochastic subgradient projection algorithms for convex optimization", "author": ["S. Ram", "A. Nedic", "V. Veeravalli"], "venue": "Journal of optimization theory and applications, vol. 147, no. 3, pp. 516-545, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 2, pp. 107- 194, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale distributed sparse precision estimation", "author": ["H. Wang", "A. Banerjee", "C.-J. Hsieh", "P.K. Ravikumar", "I.S. Dhillon"], "venue": "In NIPS, 2013, pp. 584-592.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A framework of sparse online learning and its applications", "author": ["D. Wang", "P. Wu", "P. Zhao", "S.C. Hoi"], "venue": "arXiv preprint arXiv:1507.07146, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic methods for L1- regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 1865-1892, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1865}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "Journal of Machine Learning Research, 2009, pp. 777-801.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 2543-2596, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Composite objective mirror descent", "author": ["J.C. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari"], "venue": "COLT. Citeseer, 2010, pp. 14-26.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 267-288, 1996.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Differential privacy", "author": ["C. Dwork"], "venue": "Proceedings of the 33rd ICALP. Springer-Verlag, 2006, pp. 1-12.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "author": ["F.D. McSherry"], "venue": "Proceedings of the SIGMOD International Conference on Management of data. ACM, 2009, pp. 19- 30.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "ICML, pp. 928-936, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "A social network is referred as a structure of \u201cInternet users\u201d interconnected through a variety of relations [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "They can rent cloud computing services from other third part due to their actual needs and scale up and down at any time without taking additional cost in infrastructure [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "Naturally, for social data analysis in cloud, a distributed online learning algorithm is needed to handle the massive social data in distributed scenarios [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 3, "context": "In theory, this approach is a distributed optimization technology and many researches [4]\u2013[6] have been devoted to it.", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "In theory, this approach is a distributed optimization technology and many researches [4]\u2013[6] have been devoted to it.", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "To estimate the utility of the proposed model, we use the notion \u201cregret\u201d [7] in online learning (see Definition 3).", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "In this paper, we introduce two classical groups of effective methods for sparse online learning [8]\u2013[10].", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "In this paper, we introduce two classical groups of effective methods for sparse online learning [8]\u2013[10].", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": ", [11]) induces sparsity in the weights of online learning algorithms via truncated gradient.", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "The second group studies on sparse online learning follows the dual averaging algorithm [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "In this paper, we will exploit online mirror descent [13] and Lasso-L1 norm [14] to make the parameter updated in algorithm sparse.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "In this paper, we will exploit online mirror descent [13] and Lasso-L1 norm [14] to make the parameter updated in algorithm sparse.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "Finally, we choose the \u201cdifferential privacy\u201d [15] technology to guarantee the safety of data centers in cloud.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": ", [4]\u2013[6]) about distributed optimization.", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [4]\u2013[6]) about distributed optimization.", "startOffset": 6, "endOffset": 9}, {"referenceID": 13, "context": "Lasso [14] is a famous method to produce some coefficients that are exactly 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "Dwork [15] first proposed the definition of differential privacy which makes a data miner be able to release some statistic of its database without revealing sensitive information about a particular value itself.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "Dwork [15] proposed that the magnitude of the noise depends on the largest change that a single entry in data source could", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "McSherry [16] has proposed that the privacy guarantee does not degrade across rounds as the samples used in the rounds are disjoint.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "For details of proof of Theorem 1, readers are advised to [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 8, "context": "[9], we know", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "According to Theorem 2, the regret bound becomes the classical square root regret O( \u221a T ) [17], which means less mistakes are made in social recommendations as the algorithm runs.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "Each data point is labeled with a value into [0, 1] according to its classification attribute.", "startOffset": 45, "endOffset": 51}], "year": 2016, "abstractText": "With the rapid growth of Internet technologies, cloud computing and social networks have become ubiquitous. An increasing number of people participate in social networks and massive online social data are obtained. In order to exploit knowledge from copious amounts of data obtained and predict social behavior of users, we urge to realize data mining in social networks. Almost all online websites use cloud services to effectively process the large scale of social data, which are gathered from distributed data centers. These data are so largescale, high-dimension and widely distributed that we propose a distributed sparse online algorithm to handle them. Additionally, privacy-protection is an important point in social networks. We should not compromise the privacy of individuals in networks, while these social data are being learned for data mining. Thus we also consider the privacy problem in this article. Our simulations shows that the appropriate sparsity of data would enhance the performance of our algorithm and the privacy-preserving method does not significantly hurt the performance of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}