{"id": "1602.06654", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Structured Learning of Binary Codes with Column Generation", "abstract": "Hashing involves aim move learn a to among hash purposes over map the produced incorporates to ipod three-dimensional addresses of derived significance, the Hamming space. Hashing previously proven a well programs next large - affect knowledge retrieval. We approve comes winkeljohn promising based binary code learning formulated for data - dependent hash exponential involves. Given put will its triplets idea subroutines second pairwise similarity probably concerning, understand newspaper well included method knowing trays analogous turn maintain second hence mean relations within the large - 3-to-1 learning establishing. Our conventional iteratively loved the best hash functions march next log larger diagnosis. Existing preprogrammed methodologies automation over perfectly coordination these still all reconstruction error or graph Laplacian particularly total purposes, as of followed performance evaluation mandated however reason - - - multivariate performance plan such called was AUC besides NDCG. Our column generation provided method 'll how further generalized from both triplet caused to right was devised learning largest framework that allows for keep directly optimize parametric performance measures. For optimizing affairs ranking measures, the forms mathematical solve can multiple exponentially or unimportant many correlation and constraints, such. say considering than introduced structured consumption interaction. We use every easy of reader 1970s that create - plane using all solve new clustering complicated. To full - so the successful so extended explore stage - does training and initiate without introducing a simplified NDCG defeat over ideal supposition. We readiness the generality of our method after applicants clear to ranking fundamentals and invisible on-line, where scenes has another outperforms this usually legislature - of - the - science hashing methods.", "histories": [["v1", "Mon, 22 Feb 2016 06:02:25 GMT  (318kb,D)", "http://arxiv.org/abs/1602.06654v1", "20 pages"]], "COMMENTS": "20 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guosheng lin", "fayao liu", "chunhua shen", "jianxin wu", "heng tao shen"], "accepted": false, "id": "1602.06654"}, "pdf": {"name": "1602.06654.pdf", "metadata": {"source": "CRF", "title": "Structured Learning of Binary Codes with Column Generation", "authors": ["Guosheng Lin", "Fayao Liu", "Chunhua Shen", "Jianxin Wu", "Heng Tao Shen"], "emails": [], "sections": [{"heading": null, "text": "Existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest\u2014multivariate performance measures such as the AUC and NDCG. Our column generation based method can be further generalized from the triplet loss to a general structured learning based framework that allows one to directly optimize multivariate performance measures. For optimizing general ranking measures, the resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. We use a combination of column generation and cutting-plane techniques to solve the optimization problem. To speed-up the training we further explore stage-wise training and propose to use a simplified NDCG loss for efficient inference. We demonstrate the generality of our method by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.\nKeywords Binary code, Hashing, Nearest neighbor search, Ranking, Structured learning"}, {"heading": "1 Introduction", "text": "The ever increasing volumes of imagery available, and the benefits reaped through the interrogation of large image datasets, have increased enthusiasm for large-scale approaches to vision. One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6]. Hashing methods construct a set of hash functions that map the original features into compact binary code. Hashing enables fast nearest neighbor search by using look-up tables or Hamming distance based ranking. Compact binary code are also extremely efficient for large-scale data storage or network transfer. Applications include image retrieval [7, 8], image matching [9], large-scale object detection [10], etc.\nHash function learning aims to preserve some notion of similarity. We first focus on a type of similarity information that is generally presented in a set of triplet-based relations. The triplet relations used for training can be generated in an either supervised or unsupervised fashion. The fundamental idea is to learn hash functions such that the Hamming distance between two similar data points is smaller than that between two dissimilar data points. This type of relative similarity comparisons have been successfully applied to learn quadratic distance metrics [11, 12].\nCorresponding author: C. Shen. E-mail: chunhua.shen@adelaide.edu.au).\nG. Lin, F. Liu and C. Shen The University of Adelaide, Australia\nJ. Wu Nanjing University, China\nH. T. Shen The University of Queensland, Australia\nar X\niv :1\n60 2.\n06 65\n4v 1\n[ cs\n.L G\n] 2\n2 Fe\nb 20\nUsually this type of similarity relations do not require explicit class labels and thus are easier to obtain than either the class labels or the actual distances between data points. For instance, in content based image retrieval, to collect feedback, users may be required to report whether one image looks more similar to another image than it is to a third one. This task is typically much easier than to label each individual image. Formally, let x denote one data point, we are given a set of triplets:\nT = {(i, j, k) | d(xi,xj) < d(xi,xk)}, (1)\nwhere d(\u00b7, \u00b7) is some distance measure (e.g., Euclidean distance in the original space; or semantic similarity measure provided by a user). As explained, one may not explicitly know d(\u00b7, \u00b7); instead, one may only be able to provide sparse similarity relations. Using such a set of constraints, we formulate a large-margin learning problem which is a convex optimization problem but with an exponentially large number of variables. Column generation is thus employed to efficiently solve the formulated optimization problem.\nOur column generation based method can be further generalized to optimize more general multivariate ranking measures, not limited to the simple triplet loss. Depending on applications, specific measures are used to evaluate the performance of the generated hash codes. For example, information retrieval and ranking criteria [13] such as the Area Under the ROC Curve (AUC) [14], Normalized Discounted Cumulative Gain (NDCG) [15], Precision-at-K, Precision-Recall and Mean Average Precision (mAP) have been widely adopted to evaluate the success of hashing methods. However, to date, most hashing methods are usually learned by optimizing simple errors such as the reconstruction error (e.g., binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17]. To our knowledge, none of the existing hashing methods has tried to learn hash codes that directly optimize a multivariate performance criterion. In this work, we seek to reduce the discrepancy between existing learning criteria and the evaluation criteria (such as retrieval quality measures).\nThe proposed framework accommodates various complex multivariate measures as well as the simple triplet loss. By observing that the hash codes learning problem is essentially an information retrieval problem, various ranking loss functions can and should be applied, rather than merely pairwise distance comparisons in the triplet loss. This framework also allows to introduce more general definitions of \u201csimilarity\u201d to hashing beyond existing ones.\nIn summary, our main contributions are as follows.\n1. We explore the column generation optimization technique and the large-margin learning framework for hash function learning. We first propose a learning framework to optimize the conventional triplet loss, which is referred to as Column Generation Hashing (CGHash). Then we extend this framework to optimize complex multivariate evaluation measures (e.g., ranking measures: AUC and NDCG), which is referred to as StructHash. This framework, for the first time, exploits the gains made in structured output learning for the purposes of hashing. 2. In our column generation based method for optimizing ranking measures, we develop column generation and cutting-plane algorithms to efficiently solve the resulting optimization problem, which may involve exponentially or even infinitely many variables and constraints. 3. We propose a new stage-wise training protocol to speedup the training procedure of the proposed StructHash. With this stage-wise learning approach, we are able to use the efficient unweighted hamming distance on the learned hash functions. Experimental evaluations show that the stage-wise learning approach brings orders of magnitude speedup in training while being equally or even more effective in retrieval accuracy. 4. The proposed StructHash learning procedure requires an inference algorithm for finding the most violated ranking, which is the most time consuming part in the training procedure. We propose to optimize a new ranking measure, termed as Simplified NDCG (SNDCG), which allow efficient inference in the training procedure, and thus significantly speedup the training. Experimental results show that optimizing this new ranking measure leads to around 2 times faster inference. 5. Applied to ranking prediction for image retrieval, the proposed method demonstrates state-of-the-art performance on hash function learning.\nWe have released the training code of our CGHash 1 and StructHash2 on-line, which also includes the recent extensions of StructHash on efficient stage-wise training and using simplified NDCG loss.\nThis paper is organized as follows: we first present our method for optimizing triplet loss in Sec. 3, then we generalize our method for optimizing complex ranking loss in Sec. 4, finally we present empirical evaluation in Sec. 5.\n1 CGHash is available at https://bitbucket.org/guosheng/column-generation-hashing 2 StructHash is available at https://bitbucket.org/guosheng/structhash"}, {"heading": "2 Related work", "text": "Our method provides a unified framework of the column generation technique and large-margin based structured learning for binary code learning. Preliminary results of our work appeared in [18] and [19]. In the following, we give a brief introduction to the most closely related work.\nBinary code learning Compact binary code learning, or hashing aims to preserve some notation of similarity in the Hamming space. These methods can be roughly categorized into unsupervised and (semi-) supervised approaches. Unsupervised methods attempt to preserve the similarities calculated in the original feature space. Examples fall into this category are locality sensitive hashing (LSH) [20], spectral hashing (SPH) [2], anchor graph hashing (AGH) [5], iterative quantization hashing (ITQ) [21]. Specifically, LSH [20] uses random projection to generate binary codes; SPH [2] aims to preserve the neighbourhood relation by optimizing the Laplacian affinity; AGH [5] makes the original SPH much more scalable; ITH [21] first performs linear dimensionality reduction and then conduct binary quantization in the resulting space.\nAs for the supervised approaches, they aim to preserve the label based similarities. Binary reconstruction embedding (BRE) [1] aims to minimize the expected distances; semi-supervised sequential projection learning hashing (SPLH) [8] enforces the smoothness of similar data points and the separability of dissimilar data points; kernelized LSH, proposed by Kulis and Grauman [22], randomly samples training data as support vectors, and randomly draws the dual coefficients from a Gaussian distribution. Later on, Liu et al.[17] extended kernelized LSH to kernelized supervised hashing (KSH). Lin et al. [3, 6] present a general two step framework for hashing learning. In [23], Norouzi et al. propose a latent variables based structured SVM formulation to optimize a hinge-like loss function. Their method attempts to preserve similarities between pairs of training exemplars. They further generalize the method in [23] to optimize a triplet ranking loss designed to preserve relative similarities [24]. Our method belongs to supervised approaches. Unlike existing approaches, we formulate the binary code learning as a structured output learning problem, in order to directly optimize a wide variety of ranking evaluation measures. The hashing method in [25] proposes to optimizes the NDCG ranking loss with a gradient decent method, which comes out after the publication of our preliminary version of StructHash in [19].\nLearning to rank Our method is primarily inspired by recent advances in metric learning for ranking [13, 26, 27]. In [13], McFee et al. propose a structured SVM based method to directly optimize several different ranking measures. However, it can not be scaled to large, high-dimensional datasets due to the spectral decomposition at each iteration and the expensive constraint generation step. Later on, Shalit et al. [26] propose a scalable method for optimizing a ranking loss, though they only consider the Area Under the ROC Curve (AUC) loss. In [27], Lim et al. propose to optimize a Mahalanobis metric with respect to a top-heavy ranking loss, i.e., the Weighted Approximate Pairwise Ranking (WARP) loss [28]. We extend the structured learning based ranking optimization to hash function learning.\nColumn generation Column generation is widely applied in boosting methods [29, 30, 31]. LPBoost [29] is a linear programming boosting method that iteratively learn weak classifiers to form a strong classifier. StructBoost [31] provides a general structured learning framework using column generation for structured prediction problems. We here exploit the column generation technique for hash functions learning."}, {"heading": "3 Hashing for optimizing the triplet loss", "text": "We first describe our column generation based approach for optimizing the triplet loss. We refer to this approach as CGHash. Given a set of training examples X = {x1,x2, . . . ,xn} \u2282 Rd, the task is to learn a set of hash functions [h1(x), h2(x), . . . , hm(x)]. The domain of hash functions is denoted by C: h(\u00b7) \u2208 C. The output of one hash function is a binary value: h(x) \u2208 {0, 1}. With the learned functions, an input x is mapped into a binary code of length m. We use x\u0303 \u2208 {0, 1}m to denote the hashed values of x, i.e.,\nx\u0303 = [h1(x), h2(x), . . . , hm(x)] >. (2)\nThe resulting binary code are supposed to preserve the similarity information. Formally, suppose that we are given a set of triplets T = {(i, j, k)} as the supervision information for learning. These triplets encode the similarity comparison information in which the distance/dissimilarity between xi and xj is smaller than that between xi and xk. We define the weighted Hamming distance for the learned binary codes as:\ndhm(xi,xj ;w) = m\u2211 r=1 wr|hr(xi)\u2212 hr(xj)|, (3)\nwhere wr is a non-negative weighting coefficient associated with the r-th hash function. Such weighted hamming distance is used in multi-dimension spectral hashing [32]. It is expected that after hashing, the distance between relevant data points should be smaller than the distance between irrelevant data points, that is\ndhm(xi,xj) < dhm(xi,xk). (4)\nFor notational simplicity, we define\n\u03b4h(i, j, k) = |h(xi)\u2212 h(xk)| \u2212 |h(xi)\u2212 h(xj)| (5)\nand\n\u03b4\u03a6(i, j, k) = [\u03b4h1(i, j, k), \u03b4h2(i, j, k), . . . , \u03b4hm(i, j, k)]. (6)\nWith the above definitions, the weighted Hamming distance comparison of a triplet can be written as:\ndhm(xi,xk)\u2212 dhm(xi,xj) = w>\u03b4\u03a6(i, j, k). (7)\nWe propose a large-margin learning framework to optimize for the weighting parameter w as well as the hash functions. In what follows, we describe the details of our hashing algorithm using different types of convex loss functions and regularization norms.\n3.1 Learning hash functions using column generation\nAs a starting point, we first discuss using the squared hinge loss function and `1 norm regularization for hash function learning. Using the squared hinge loss, we define the following large-margin optimization problem:\nmin w,\u03be\n1>w + C \u2211\n(i,j,k)\u2208T\n\u03be2(i,j,k) (8)\ns.t. \u2200(i, j, k) \u2208 T : dhm(xi,xk;w)\u2212 dhm(xi,xj ;w) \u2265 1\u2212 \u03be(i,j,k), w \u2265 0, \u03be \u2265 0.\nHere we have used the `1 norm on w as the regularization term to control the complexity of the learned model; the weighting vector w is defined as:\nw = [w1, w2, . . . , wm] >; (9)\n\u03be is the slack variable; C is a parameter controlling the trade-off between the training error and model complexity. With the definition of weighted Hamming distance in (3) and the notation in (6), the optimization problem in (8) can be rewritten as:\nmin w,\u03be\n1>w + C \u2211\n(i,j,k)\u2208T\n\u03be2(i,j,k) (10)\ns.t. \u2200(i, j, k) \u2208 T : w>\u03b4\u03a6(i, j, k) \u2265 1\u2212 \u03be(i,j,k) w \u2265 0, \u03be \u2265 0.\nWe aim to solve the above optimization to obtain the weighting vector w and the set of hash functions [h1, h2, . . . ]. If the hash functions are obtained, the optimization can be easily solved for w, e.g., using LBFGS-B [33]. In our approach, we apply the column generation technique to alternatively solve for w and learn hash functions. Basically, we construct a working set of hash functions and repeat the following two steps until converge: first we solve for the weighting vector using the current working set of hash functions, and then generate new hash function and add to the working set.\nColumn generation is a technique originally used for large scale linear programming problems. LPBoost [34] applies this technique to design boosting algorithms. In each iteration, one column\u2014a variable in the primal or a constraint in the dual problem\u2014is added. Till one cannot find any violating constraints in the dual, the current solution is the optimal solution. In theory, if we run the column generation with a sufficient number of iterations, one can obtain a sufficiently accurate solution. Here we only need to run a small number of column generation iteration (e.g, 60) to learn a compact set of hash functions.\nTo apply column generation technique for learning hash functions, we derive the dual problem of the optimization in (10). The optimization in (10) can be equally written as:\nmin w,\u03c1\n1>w + C \u2211\n(i,j,k)\u2208T\n[ max(1\u2212 \u03c1(i,j,k), 0) ]2 (11)\ns.t. \u2200(i, j, k) \u2208 T : \u03c1(i,j,k) = w>\u03b4\u03a6(i, j, k), (12) w \u2265 0.\nThe Lagrangian of (11) can be written as:\nL(w,\u03c1,\u00b5,\u03b1) =1>w + C \u2211\n(i,j,k)\u2208T\n[ max(1\u2212 \u03c1(i,j,k), 0) ]2 +\n\u2211 (i,j,k)\u2208T \u00b5(i,j,k) [ \u03c1(i,j,k) \u2212w>\u03b4\u03a6(i, j, k) ] \u2212 \u03b1>w, (13)\nwhere \u00b5, \u03b1 are Lagrange multipliers and \u03b1 \u2265 0. For the optimal primal solution, the following must hold: \u2202L\u2202w = 0 and \u2202L\u2202\u03c1 = 0. Therefore we have:\n\u2202L \u2202w = 0 =\u21d2 1\u2212 \u2211 (i,j,k)\u2208T \u00b5(i,j,k)\u03b4\u03a6(i, j, k)\u2212 \u03b1 = 0. (14)\n\u2202L\n\u2202\u03c1(i,j,k) = 0 =\u21d2 \u22122C max(1\u2212 \u03c1(i,j,k), 0) + \u00b5(i,j,k) = 0\n=\u21d2 \u00b5(i,j,k) = 2C max(1\u2212 \u03c1(i,j,k), 0). (15)\nWith Eq. (14), Eq. (15) and Eq. (13), we can derive the dual problem as:\nmax \u00b5 \u2211 (i,j,k)\u2208T \u00b5(i,j,k) \u2212 \u00b52(i,j,k) 4C (16)\ns.t. \u2200h(\u00b7) \u2208 C : \u2211\n(i,j,k)\u2208T\n\u00b5(i,j,k)\u03b4h(i, j, k) \u2264 1.\nHere \u00b5 is one dual variable, which corresponds to one constraint in (12). The core idea of column generation is to generate a small subset of dual constraints by finding the most violated dual constraint in (16). This process is equivalent to adding primal variables into the primal optimization problem (23). Here finding the most violated dual constraint is learning one hash function, which can be written as:\nh?(\u00b7) = argmax h(\u00b7)\u2208C \u2211 (i,j,k)\u2208T \u00b5(i,j,k)\u03b4h(i, j, k)\n= argmax h(\u00b7)\u2208C \u2211 (i,j,k)\u2208T \u00b5(i,j,k) [ |h(xi)\u2212 h(xk)| \u2212 |h(xi)\u2212 h(xj)| ] . (17)\nIn each column generation iteration, we solve the above optimization to generate one hash function. Now we give an overview of our approach. Basically, we repeat the following two steps until converge:\n1. Solve the reduced primal problem in (11) using the current working set of hash functions. We obtain the primal solution w and the dual solution \u00b5 in this step. 2. With the dual solution \u00b5, we solve the subproblem in (17) to learn one hash function, and add to the working set of hash functions.\nOur method is summarized in Algorithm 1. We describe more details for running these two steps as follows. In the first step, we need to obtain the dual solution \u00b5, which is required for solving the subproblem in (17) of the second step to learn one hash function. In each column generation iteration, we can easily solve the optimization in (11) using the current working set of hash functions to obtain the primal solution w, for example, using the efficient\nAlgorithm 1: CGHash: Hashing using column generation (with squared hinge loss)\nInput: training triplets: T = {(i, j, k)}, training examples: x1,x2, . . ., the number of bits: m. Output: Learned hash functions {h1, h2, . . . , hm} and the associated weights w.\n1 Initialize: \u00b5\u2190 1|T| . 2 for r = 1 to m do 3 find a new hash function hr(\u00b7) by solving the subproblem: (17); 4 add hr(\u00b7) to the working set of hash functions; 5 solve the primal problem in (11) for w (using LBFGS-B[33]), and calculate the dual solution \u00b5 by (18);\nLBFGS-B solver [33]. According to the Karush-Kuhn-Tucker (KKT) conditions in Eq. (15), we have the following relation: \u2200(i, j, k) \u2208 T : \u00b5?(i,j,k) = 2C max [ 1\u2212w?>\u03b4\u03a6(i, j, k), 0 ] . (18)\nFrom the above, we are able to obtain the dual solution \u00b5? for the primal solution w?. In the second step, we solve the subproblem in (17) for learning one hash function. The form of hash function h(\u00b7) can be any function that have binary output value. When using a decision stump as the hash function, usually we can exhaustively enumerate all possibility and find the globally best one. However, for many other types of hash functions, e.g., perceptron and kernel functions, globally solving (17) is difficult. In our experiments, we use the perceptron hash function:\nh(x) = 0.5(sign(v>x+ b) + 1). (19)\nIn order to obtain a smoothly differentiable objective function, we reformulate (17) into the following equivalent form:\nh?(\u00b7) = argmax h(\u00b7)\u2208C \u2211 (i,j,k)\u2208T \u00b5(i,j,k) [( h(xi)\u2212 h(xk) )2 \u2212 (h(xi)\u2212 h(xj))2]. (20) The non-smooth sign function in (19) brings the difficulty for optimization. We replace the sign function by a smooth sigmoid function, and then locally solve the above optimization (20) (e.g., using LBFGS) for learning the parameters of a hash function. We can apply a few initialization heuristics for solving (20). For example, similar to LSH, we can generate a number of random planes and choose the best one, which maximizes the objective in (20), as the initial solution. We can also train a decision stump by searching a best dimension and threshold to maximize the objective on the quantized data. Alternatively, we can employ the spectral relaxation method [5] which drops the sign function and solves a generalized eigenvalue problem to obtain a solution for initialization. In our experiments, we use the spectral relaxation method for initialization.\n3.2 Hashing with general smooth convex loss functions\nThe previous discussion for squared hinge loss is an example of using smooth convex loss function in our framework. To take a step forward, here we describe how to incorporate general smooth convex loss functions. We encourage the following constraints to be satisfied as far as possible:\n\u2200(i, j, k) \u2208 T : dhm(xi,xk)\u2212 dhm(xi,xj) = w>\u03b4\u03a6(i, j, k) \u2265 0 (21)\nThese constraints do not have to be all strictly satisfied. Here we define the margin:\n\u03c1(i,j,k) = w >\u03b4\u03a6(i, j, k), (22)\nand we want to maximize the margin with regularization. We denote by f(\u00b7) as a general convex loss function which is assumed to be smooth (e.g., exponential, logistic, squared hinge loss). Using `1 norm for regularization, we define the primal optimization problem as:\nmin w\n1>w + C \u2211\n(i,j,k)\u2208T\nf(\u03c1(i,j,k)) (23)\ns.t. \u2200(i, j, k) \u2208 T : \u03c1(i,j,k) = w>\u03b4\u03a6(i, j, k), w \u2265 0.\nC is a parameter controlling the trade-off between the training error and model complexity. Without the regularization, one can always make w arbitrarily large to make the convex loss approach zero when all constraints are satisfied.\nThe squared hinge loss which we discussed before is an example of f(\u00b7). We can easily recover the formulation in (11) for squared hinge loss by using the following definition:\nf(\u03c1(i,j,k)) = [ max(1\u2212 \u03c1(i,j,k), 0) ]2 . (24)\nFor applying column generation, we derive the dual problem of (23). The Lagrangian of (23) can be written as: L(w,\u03c1,\u00b5,\u03b1) =1>w + C \u2211\n(i,j,k)\u2208T\nf(\u03c1(i,j,k))\n+ \u2211\n(i,j,k)\u2208T\n\u00b5(i,j,k) [ \u03c1(i,j,k) \u2212w>\u03b4\u03a6(i, j, k) ] \u2212 \u03b1>w, (25)\nwhere \u00b5, \u03b1 are Lagrange multipliers and \u03b1 \u2265 0. With the definition of Fenchel conjugate [35]: f?(z) := sup x\u2208domf x>z\u2212 f(x) ( here f?(\u00b7) is the Fenchel conjugate of the function f(\u00b7) ), we have the following dual objective:\ninf w,\u03c1 L(w,\u03c1,\u00b5,\u03b1) = inf \u03c1\n( C \u2211 (i,j,k)\u2208T f(\u03c1(i,j,k)) + \u2211 (i,j,k)\u2208T \u00b5(i,j,k)\u03c1(i,j,k) )\n=\u2212 sup \u03c1\n( \u2212 C \u2211 (i,j,k)\u2208T f(\u03c1(i,j,k))\u2212 \u2211 (i,j,k)\u2208T \u00b5(i,j,k)\u03c1(i,j,k) )\n=\u2212 C sup \u03c1 ( \u2211 (i,j,k)\u2208T \u2212\u00b5(i,j,k) C \u03c1(i,j,k) \u2212 \u2211 (i,j,k)\u2208T f(\u03c1(i,j,k)) )\n=\u2212 C \u2211\n(i,j,k)\u2208T\nf? (\u2212\u00b5(i,j,k)\nC\n) . (26)\nFor the optimal primal solution, the condition: \u2202L\u2202w = 0 must hold; hence we have the following relation:\n1\u2212 \u03b1> \u2212 \u2211\n(i,j,k)\u2208T\n\u00b5(i,j,k)\u03b4\u03a6(i, j, k) = 0. (27)\nConsequently, the corresponding dual problem of (23) can be written as:\nmax \u00b5 \u2212 \u2211 (i,j,k)\u2208T f? (\u2212\u00b5(i,j,k) C ) (28)\ns.t. \u2200h(\u00b7) \u2208 C : \u2211\n(i,j,k)\u2208T\n\u00b5(i,j,k)\u03b4h(i, j, k) \u2264 1. (29)\nWith the above dual problem for general smooth convex loss functions, we generate a new hash function by finding the most violating constraints in (29), which is the same as that for squared hinge loss. Hence, we solve the optimization in (17) to generate a new hash function. Using different loss functions will result in different dual solutions. The dual solution is required for generating hash functions.\nAs aforementioned, in each column generation iteration, we need to obtain the dual solution before solving (17) to generate a hash function. Since we assume that f(\u00b7) is smooth, the Karush-Kuhn-Tucker (KKT) conditions establish the connection between the primal solution of (23) and the dual solution of (28):\n\u2200(i, j, k) \u2208 T : \u00b5?(i,j,k) = \u2212Cf \u2032 (\u03c1?(i,j,k)) (30)\nin which, \u03c1?(i,j,k) = w ?>\u03b4\u03a6(i, j, k). (31)\nIn other words, the dual variable is determined by the gradient of the loss function in the primal. According to (30), we are able to obtain the dual solution \u00b5? using the primal solution w?.\n3.3 Discussion on extensions\nWe can easily incorporate different kinds of loss functions and regularization in our learning framework. In this section, we discuss the case of using the logistic loss and the `\u221e norm regularization."}, {"heading": "3.3.1 Hashing with logistic loss", "text": "It has been shown in (24) that formulation for the squared hinge loss is an example of the general formulation in (23) with smooth convex loss functions. Here we describe using the logistic loss as another example of the general formulation. The learning algorithm is similar to the case of using the squared hinge loss which is described before. We have the following definition for the logistic loss:\nf(\u03c1(i,j,k)) = log (1 + exp (\u2212\u03c1(i,j,k))). (32)\nThe general result for smooth convex loss function can be applied here. The primal optimization problem can be written as:\nmin w,\u03c1\n1>w + C \u2211\n(i,j,k)\u2208T\nlog (1 + exp (\u2212\u03c1(i,j,k))) (33)\ns.t. \u2200(i, j, k) \u2208 T : \u03c1(i,j,k) = w>\u03b4\u03a6(i, j, k), w \u2265 0.\nThe corresponding dual problem can be written as:\nmax \u00b5 \u2211 (i,j,k)\u2208T (\u00b5(i,j,k) \u2212 C) log (C \u2212 \u00b5(i,j,k))\u2212 \u00b5(i,j,k) log (\u00b5(i,j,k)) (34)\ns.t. \u2200h(\u00b7) \u2208 C : \u2211\n(i,j,k)\u2208T\n\u00b5(i,j,k)\u03b4h(i, j, k) \u2264 1.\nThe dual solution can be calculated by:\n\u2200(i, j, k) \u2208 T : \u00b5?(i,j,k) = C\nexp (w?>\u03b4\u03a6(i, j, k)) + 1 . (35)"}, {"heading": "3.3.2 Hashing with `\u221e norm regularization", "text": "The proposed method is flexible that it is easy to incorporate different types of regularizations. Here we discuss the `\u221e norm regularization as an example. For general convex loss, the optimization can be written as:\nmin w,\u03c1\n\u2016w\u2016\u221e + C \u2211\n(i,j,k)\u2208T\nf(\u03c1(i,j,k)) (36)\ns.t. \u2200(i, j, k) \u2208 T : \u03c1(i,j,k) = w>\u03b4\u03a6(i, j, k), w \u2265 0.\nThis optimization problem can be equivalently written as:\nmin w,\u03c1 \u2211 (i,j,k)\u2208T f(\u03c1(i,j,k)) (37)\ns.t. \u2200(i, j, k) \u2208 T : \u03c1(i,j,k) = w>\u03b4\u03a6(i, j, k), 0 \u2264 w \u2264 C\u20321,\nwhere C\u2032 is a constant that controls the regularization trade-off. This optimization can be efficiently solved using quasi-Newton methods such as LBFGS-B by eliminating the auxiliary variable \u03c1. The Lagrangian can be written as:\nL(w,\u03c1,\u00b5,\u03b1,\u03b2) = \u2211\n(i,j,k)\u2208T\nf(\u03c1(i,j,k))\u2212 \u03b1>w + \u03b2>(w \u2212 C\u20321)\n+ \u2211\n(i,j,k)\u2208T\n\u00b5(i,j,k) [ \u03c1(i,j,k) \u2212w>\u03b4\u03a6(i, j, k) ] , (38)\nwhere \u00b5, \u03b1, \u03b2 are Lagrange multipliers and \u03b1 \u2265 0, \u03b2 \u2265 0. Similar to the case for `1 norm, the dual problem can be written as:\nmax \u00b5,\u03b2\n\u2212 C\u20321>\u03b2 \u2212 \u2211\n(i,j,k)\u2208T\nf?(\u2212\u00b5(i,j,k)) (39)\ns.t. \u2200h(\u00b7) \u2208 C : \u2211\n(i,j,k)\u2208T\n\u00b5(i,j,k)\u03b4h(i, j, k) \u2264 \u03b2h,\n\u03b2 \u2265 0.\nAs the same with the case of `1 norm, the dual solution \u00b5 can be calculated using (30), and the rule for generating one hash function is to solve the subproblem in (17).\nSimilar to the discussion for `1 norm, different loss functions, including the squared hinge loss in (24) and the logistic loss in (32), can be applied here to incorporate the `\u221e norm regularization. As the flexibility of our framework, we also can use the non-smooth hinge loss with the `\u221e norm regularization."}, {"heading": "4 Hashing for optimizing ranking loss", "text": "Our column generation based approach CGHash can be extended to optimize the more general ranking loss, which is more complex than the simple triplet loss. This extension is a structured learning based approach for binary code learning. Hence we referred to this extension as StructHash in this paper. Before describing details of StructHash, we first present a preliminary technique which applies large-margin based structured learning for optimize ranking loss.\n4.1 Structured SVM for learning to rank\nFirst we provide a brief overview of structured SVM. Let {(xi;yi)}, i = 1, 2 \u00b7 \u00b7 \u00b7 , denote a set of input-output pairs. The discriminative function for structured output prediction is F (x,y) : X \u00d7 Y 7\u2192 R, which measures the compatibility of the input and output pair (x,y). Structured SVM enforces that the score of the \u201ccorrect\u201d model y\u2032 should be larger than all other \u201cincorrect\u201d model y, \u2200y 6= y\u2032, which writes:\n\u2200y \u2208 Y : w>[\u03a8(x,y\u2032)\u2212 \u03a8(x,y)] \u2265 \u2206(y,y\u2032)\u2212 \u03be. (40)\nHere \u03be is a slack variable (soft margin) corresponding to the hinge loss. \u03a8(x,y) is a vector-valued joint feature mapping. It plays a key role in structured learning and specifies the relationship between an input x and output y. w is the model parameter. The label loss \u2206(y,y\u2032) \u2208 R measures the discrepancy of the predicted y and the true label y\u2032. A typical assumption is that \u2206(y,y) = 0,\u2206(y,y\u2032) > 0 for any y 6= y\u2032, and \u2206(y,y\u2032) is upper bounded. The prediction y? of an input x is achieved by\ny? = argmax y\u2208Y F (x,y) = w>\u03a8(x,y). (41)\nFor structured problems, the size of the output |Y| is typically very large or infinite. Considering all possible constraints in (40) is generally intractable. The cutting-plane method [36] is commonly employed, which allows to maintain a small working-set of constraints and obtain an approximate solution of the original problem up to a pre-set precision. To speed up, the 1-slack reformulation is proposed [37]. Nonetheless the cutting-plane method needs to find the most violated label (equivalent to an inference problem) by solving the following optimization:\ny? = argmax y\u2208Y w>\u03a8(x,y) +\u2206(y,y\u2032). (42)\nStructured SVM typically requires: 1) a well-designed feature representation \u03a8(\u00b7, \u00b7); 2) an appropriate label loss \u2206(\u00b7, \u00b7); 3) solving inference problems (41) and (42) efficiently.\nIn a retrieval system, given a test data point x, the goal is to predict a ranking of data points in the database. For a \u201ccorrect\u201d ranking, relevant data points are expected to be placed in front of irrelevant data points. A ranking output is denoted by y. Given a query xi, we use X + i and X \u2212 i to denote the subsets of relevant and irrelevant data points in the training data. Given two data points: xi and xj , xi\u227ayxj (xi yxj) means that xi is placed before (after) xj in the ranking y. Let us introduce a symbol yjk = 1 if xj\u227ayxk and yjk = \u22121 if xj yxk. The ranking can\nbe evaluated by various measures such as AUC, NDCG, mAP. These evaluation measures can be optimized directly as label loss \u2206 [13, 14]. Here \u03a8(xi,y) can be defined as:\n\u03a8(xi,y) = \u2211 xj\u2208X+i \u2211 xk\u2208X\u2212i yjk [\u03c6(xi,xj)\u2212 \u03c6(xi,xk) |X+i | \u00b7 |X \u2212 i | ] . (43)\nX+i and X \u2212 i are the sets of relevant and irrelevant neighbours of data point xi respectively. Here | \u00b7 | is the set size. The feature map \u03c6(xi,xj) captures the relation between a query xi and point xj . We have briefly reviewed how to optimize ranking criteria using structured prediction. Now we review some basic concepts of hashing before introducing our framework. For the time being, let us assume that we have already learned all the hashing functions. In other words, given a data point x, we assume that we have access to its corresponding hashed values x\u0303, as defined in (2). Later we will show how this mapping can be explicitly learned using column generation. Now let us focus on how to optimize for the weight w. When the weighted hamming distance is used, we aim to learn an optimal weight w. Distances are calculated in the learned space and ranked accordingly. A natural choice for the vector-valued mapping function \u03c6 in (43) is\n\u03c6(xi,xj) = \u2212|x\u0303i \u2212 x\u0303j |. (44)\nNote that we have flipped the sign, which preserves the ordering in the standard structured SVM. Due to this change of sign, sorting the data by ascending dhm(xi,xj) is equivalent to sorting by descending w\n>\u03c6(xi,xj) = \u2212w>|x\u0303i\u2212x\u0303j |. The loss function \u2206(\u00b7, \u00b7) depends on the metric, which we will discuss in detail in the next section. For ease of\nexposition, let us define\n\u03b4\u03a8i(y) = \u03a8(xi,yi)\u2212 \u03a8(xi,y), (45)\nwith \u03a8(xi,y) defined in (43). We consider the following problem,\nmin w\u22650,\u03be\u22650 \u2016w\u20161 + Cm m\u2211 i=1 \u03bei (46)\ns.t. \u2200i = 1, . . . ,m and \u2200y \u2208 Y :\nw>\u03b4\u03a8i(y) \u2265 \u2206(yi,y)\u2212 \u03bei. (47)\nUnlike standard structured SVM, here we use the `1 regularisation (instead of `2) and enforce that w is non-negative. This is aligned with boosting methods [34, 38], and enables us to learn hash functions efficiently.\n4.2 Weighting learning via cutting-plane\nHere we show how to learn the weighting coefficient w. Inspired by [37], we first derive the 1-slack formulation of the original n-slack formulation (46):\nmin w\u22650,\u03be\u22650\n\u2016w\u20161 + C\u03be (48)\ns.t. \u2200c \u2208 {0, 1}m and \u2200y \u2208 Y, i = 1, \u00b7 \u00b7 \u00b7 ,m :\n1 m w> [ m\u2211 i=1 ci \u00b7 \u03b4\u03a8i(y) ] \u2265 1 m m\u2211 i=1 ci\u2206(yi,y)\u2212 \u03be. (49)\nHere c enumerates all possible c \u2208 {0, 1}n. As in [37], cutting-plane methods can be used to solve the 1-slack primal problem (48) efficiently. Specifically, we need to solve a maximization for every xi in each cutting-plane iteration to find the most violated constraint of (49), given a solution w:\ny?i = argmax y \u2206(yi,y)\u2212w >\u03b4\u03a8i(y). (50)\nThe cutting-plane algorithm is summarized in Algorithm 2. We now know how to efficiently learn w using cutting-plane methods. However, it remains unclear how to learn hash functions (or features). Thus far, we have taken for granted that the hashed values x\u0303 are given. We would like to learn the hash functions and w in a single optimization framework. Next we show how this is possible using the column generation technique from boosting.\nAlgorithm 2: Cutting planes for solving the 1-slack primal\nInput: cutting-plane tolerance: cp; inputs from Algorithm 3. Output: w and \u00b5.\n1 Initialize: working set: W\u2190 \u2205; ci = 1, y\u2032i \u2190 any element in Y, for i = 1, . . . , n. 2 repeat 3 W\u2190W \u222a {(c1, . . . , cn, y\u20321, . . . ,y\u2032n)}; 4 obtain primal and dual solutions w, \u03be; \u03bb by solving (48) (e.g., using MOSEK [39]) on current working set W ; 5 for i = 1, . . . , n do 6 y\u2032i = argmax y \u2206(yi, y)\u2212w>\u03b4\u03a8i(y);\n7 ci =\n{ 1 \u2206(yi, y \u2032 i)\u2212w>\u03b4\u03a8i(y\u2032i) > 0\n0 otherwise ; 8 until 1 n w> [ n\u2211 i=1 ci\u03b4\u03a8i(y \u2032 i) ] \u2265 1 n n\u2211 i=1 ci\u2206(yi, y \u2032 i)\u2212 \u03be \u2212 cp;\n9 update \u00b5(i,y) = \u2211 c \u03bb(c,y)ci for \u2200(c, y) \u2208W ;\n4.3 Learning hash functions using column generation\nNote that the dimension of w is the same as the dimension of x\u0303 (and of \u03c6(\u00b7, \u00b7), see Equ. (44)), which is the number of hash bits by the definition (2). If we were able to access all hash functions, it may be possible to select a subset of them and learn the corresponding w due to the sparsity introduced by the `1 regularization in (46). Unfortunately, the number of possible hash functions can be infinitely large. In this case it is in general infeasible to solve the optimization problem exactly. We here develop a column generation algorithm for StructHash to iteratively learn the hash functions and weights, which is similar to CGHash.\nTo learn hash functions via column generation, we derive the dual problem of the above 1-slack optimization, which is,\nmax \u03bb\u22650 \u2211 c,y \u03bb(c,y) m\u2211 i=1 ci\u2206(yi,y) (51)\ns.t. 1\nm \u2211 c,y \u03bb(c,y) [ m\u2211 i=1 ci \u00b7 \u03b4\u03a8i(y) ] \u2264 1, (52)\n0 \u2264 \u2211 c,y \u03bb(c,y) \u2264 C.\nWe denote by \u03bb(c,y) the 1-slack dual variable associated with one constraint in (49). Note that (52) is a set of constraints because \u03b4\u03a8(\u00b7) is a vector of the same dimension as \u03c6(\u00b7, \u00b7) as well as x\u0303, which can be infinitely large. One dimension in the vector \u03b4\u03a8(\u00b7) corresponds to one constraint in (52). Finding the most violated constraint in the dual form (51) of the 1-slack formulation for generating one hash function is to maximize the l.h.s. of (52).\nThe calculation of \u03b4\u03a8(\u00b7) in (45) can be simplified as follows. Because of the subtraction of \u03a8(\u00b7) (defined in (43)), only those incorrect ranking pairs will appear in the calculation. Recall that the true ranking is yi for xi. We define Si(y) as a set of incorrectly ranked pairs: (j, k) \u2208 Si(y), in which the incorrectly ranked pair (j, k) means that the true ranking is xj\u227ayixk but xj yxk. So we have\n\u03b4\u03a8i(y) = 2\n|X+i ||X \u2212 i | \u2211 (j,k)\u2208Si(y) [ \u03c6(xi,xj)\u2212 \u03c6(xi,xk) ] = 2|X+i ||X \u2212 i | \u2211 (j,k)\u2208Si(y) ( |x\u0303i \u2212 x\u0303k| \u2212 |x\u0303i \u2212 x\u0303j | ) . (53)\nWith the above equations and the definition of x\u0303 in (2), the most violated constraint in (52) can be found by solving the following problem:\nh?(\u00b7) = argmax h(\u00b7) \u2211 c,y \u03bb(c,y) \u2211 i\n2ci\n|X+i ||X \u2212 i | \u00b7\n\u2211 (j,k)\u2208Si(y) ( |h(xi)\u2212 h(xk)| \u2212 |h(xi)\u2212 h(xj)| ) . (54)\nBy exchanging the order of summations, the above optimization can be further written in a compact form:\nh?(\u00b7) = argmax h(\u00b7) \u2211 i,y \u2211 (j,k)\u2208Si(y) \u00b5(i,y) ( |h(xi)\u2212 h(xk)| \u2212 |h(xi)\u2212 h(xj)| ) , (55)\nwhere, \u00b5(i,y) = 2\n|X+i ||X \u2212 i | \u2211 c \u03bb(c,y)ci. (56)\nThe objective in the above optimization is a summation of weighted triplet (i, j, k) ranking scores, in which \u00b5(i,y) is the triplet weighting value. Solving the above optimization provides the best hash function for the current solution w. Once a hash function is generated, we learn w using cutting-plane in Sec. 4.2. The column generation procedure for hash function learning is summarised in Algorithm 3.\nAlgorithm 3: StructHash: Column generation for hash function learning\nInput: training examples: (x1; y1), (x2; y2), \u00b7 \u00b7 \u00b7 ; trade-off parameter: C; the maximum iteration number (bit length m). Output: learned hash functions [h1, . . . , hm] and weighting coefficients w.\n1 Initialize: working set of hashing functions WH \u2190 \u2205; for each i, (i = 1, . . . , n), randomly pick any y (0) i \u2208 Y, initialize\n\u00b5(i,y) = C n for y = y (0) i , and \u00b5(i,y) = 0 for all y \u2208 Y\\y (0) i .\n2 repeat 3 Find a new hashing function h?(\u00b7) by solving Equ. (55); add h? into the working set of hashing functions: WH; 4 Solve the structured SVM problem (46) or the equivalent (48) using cutting-plane in Algorithm 2, to obtain w and \u00b5; 5 until the maximum number of iterations is reached ;\nIn most of our experiments, we use the linear perceptron hash function with the output in {0, 1}:\nh(x) = 0.5(sign(v>x+ b) + 1). (57)\nWe apply a similar way as CGHash for learning the hash function. Please refer to the learning procedure of CGHash in Sec. 3.1 for details. Basically, we replace the sign(\u00b7) function by a smooth sigmoid function, and then locally solve the above optimization (55) (e.g., LBFGS [33]) for learning the parameters of a hash function. We apply the spectral relaxation [5] to obtain an initial point for solving (55), which drops the sign(\u00b7) function and solves a generalized eigenvalue problem.\nNext, we discuss some widely-used information retrieval evaluation criteria, and show how they can be seamlessly incorporated into StructHash.\n4.4 Ranking measures\nHere we discuss a few ranking measures for loss functions, including AUC and NDCG. Following [13], we define the loss function over two rankings \u2206 \u2208 [0 1] as:\n\u2206(y,y\u2032) = 1\u2212 score(y,y\u2032). (58)\nHere y\u2032 is the ground truth ranking and y is the prediction. We define X+y\u2032 and X \u2212 y\u2032 as the indexes of relevant and irrelevant neighbours respectively in the ground truth ranking y\u2032. AUC. The area under the ROC curve is to evaluate the performance of correct ordering of data pairs, which can be computed by counting the proportion of correctly ordered data pairs:\nscoreAUC(y,y \u2032) =\n1\n|X+y\u2032 ||X \u2212 y\u2032 | \u2211 i\u2208X+\ny\u2032\n\u2211 j\u2208X\u2212\ny\u2032\n\u03b4(i \u227ay j). (59)\n\u03b4(\u00b7) \u2208 {0, 1} is the indicator function. For using this AUC loss, the maximization inference in (50) can be solved efficiently by sorting the distances of data pairs, as described in [14]. Note that the loss of a wrongly ordered pair is not related to their positions in the ranking list, thus AUC is a position insensitive measure. It clearly shows that AUC loss is to calculate the portion of correctly ranked triplets. Hence optimizing AUC loss in StructHash is equivalent to optimize the triplet loss in CGHash.\nNDCG. Normalized Discounted Cumulative Gain [15] is to measure the ranking quality of the first K returned neighbours. A similar measure is Precision-at-K which is the proportion of top-K relevant neighbours. NDCG is a\nposition-sensitive measure which considers the positions of the top-K relevant neighbours. Compared to the positioninsensitive measure: AUC, NDCG assigns different importances on the ranking positions, which is a more favorable measure for a general notion of a \u201cgood\u201d ranking in real-world applications. In NDCG, each position of the ranking is assigned a score in a decreasing way. NDCG can be computed by accumulating the scores of top-K relevant neighbours:\nscoreNDCG(y,y \u2032) = 1\u2211K i=1 S(i) K\u2211 i=1 S(i)\u03b4(y(i) \u2208 X+y\u2032). (60)\nHere y(i) is the example index on the i-th position of the ranking y. S(i) is the score assigned to the i-th position in the ranking. S(1) = 1, S(i) = 0 for i > K and S(i) = 1/ log 2(i) for other cases. A dynamic programming algorithm is proposed in [40] for solving the maximization inference in (50).\n4.5 Speedup training\nIn this section, we propose two strategies to speedup the training procedure of our StructHash model, both from the aspects of training and inference."}, {"heading": "4.5.1 Stage-wise training", "text": "When learning a new hash function, the original StructHash model needs to solve for all the weights of all hash functions in each column generation iteration. As the number of hashing functions increase, the dimension of the weights which need to learn is also increase, When the dimension of weights increases, we usually need to perform a large number of inference operations (see (50)) in the cutting-plane algorithm for the convergence, which is generally computation expensive. The learning procedure becomes more and more expensive as the number of bits increases.\nHere we exploit the stage-wise learning strategy to speedup the training. In column generation based totallycorrective boosting methods [29, 30, 31], all the hash function weights w are updated during each column generation iteration. In contrast, in stage-wise boosting, e.g., AdaBoost, only the weight of the newly added weak learner is updated in the current boosting iteration and weights of all previous weak learners are fixed. This leads to more efficient training and is less prone to overfit. Inspired by the stage-wise boosting, we here exploit a new training protocol based on the stage-wise training to speedup the training of StructHash. Specifically, in the t-th column generation iteration, we only learn two weight variables, i.e., wt and wt\u22121, where wt is the weight of the current newly added hash function, and wt\u22121 is the weight shared by all previous hash functions.\nUsing this stage-wise training, we only need to solve for two variables (wt and wt\u22121) for learning one hashing function using the cutting-plane algorithm (Algo .2). With much less variables, the cutting-plane algorithm is able to converge much faster, therefore significantly reduce the number of inference (solving (50)) need to perform. Since we solve the optimization problem with only two variables for every hash bit, the optimization complexity of learning new hash function is not increasing with the number of bits. Thus this stage-wise training could easily scale to learning for large number of bits. In the experiment part, as shown in Table 4, this new training protocol largely reduce the total number of inference iterations for learning one hash function and this is not increased with the number of bits, hence brings orders of magnitude training speedup.\nOne more advantage of using this stage-wise training is that by forcing all the hash functions to share the same weights, we can use unweighted hamming distance to calculate similarities of the learned binary codes. We observe that the unweighted hamming distance is more efficient and has better generalization performance, as demonstrated later in the experiment section (Sec. 5.2), This also indicates that the hash functions are more important to the performance than the weights of hash functions in the StructHash model."}, {"heading": "4.5.2 Optimizing Simplified NDCG (SNDCG) score", "text": "As discussed before, we need to solve the maximization inference in (50) for finding most violated constraints. The computational complexity for solving this inference problem mainly depends on the definition of \u2206(y,y\u2032) in (58), of which some examples are discussed in Sec. 4.4. Usually when using position sensitive loss functions, such as mAP, NDCG, it is computational expensive to solve the maximization inference [40, 41], which might limit its application on large-scale learning. Inspired by the efficient metric learning method in [27], here we discuss a form of positionsensitive ranking loss which is capable for fast inference. Basically, we construct a simplified NDCG (referred to as\nSNDCG) score, based on a number of NDCG scores which are calculated from \u201csimple\u201d rankings.\nscoreSNDCG(y,y \u2032) =\n1\n|X+y\u2032 | \u2211 i\u2208X+\ny\u2032\nN(i,y) (61)\nwhere,\nN(i,y) = |X\u2212y |+1\u2211 j=1 S(j)\u03b4(y(j) = i). (62)\nHere y(j) is the example index on the j-th position of the ranking y. S(j) is the score assigned to the j-th position in the ranking. S(j) = 1/ log 2(1 + j).\nIt clearly shows that the loss is decomposed over all relevant examples. N(i) represents the NDCG score corresponding to the i-th relevant example, which is calculated from a simple ranking: a ranking only involves one relevant example and all irrelevant examples. The summation over relevant examples in (61) allow independent inference calculation for each relevant example. For solving the inference on the simple ranking for each relevant example, we only need to perform a simple sorting of the hamming distances which is very efficient. Hence the maximization inference in (50) can be independently and efficiently solved for each relevant example, which is much more efficient than using the original NDCG loss. In the experiment section, we evaluate training efficiency and ranking accuracy of the proposed SNDCG loss in Sec. 5.2."}, {"heading": "5 Experiments", "text": "We evaluate our column generation learning framework for binary code learning in this section. Specifically, we evaluate the proposed method CGHash for optimizing triplet loss and the more general method StructHash for optimizing ranking loss. We first compare our models with state-of-the-art methods in Sec. 5.1, and then in Sec. 5.2 we evaluate the more efficient models proposed in Sec. 4.5.\nNine datasets are used here for evaluation, including one UCI dataset: ISOLET, 4 image datasets: CIFAR103, STL104, MNIST, USPS, and another 4 large image datasets: Tiny-580K [21], Flickr-1M5, SIFT-1M [8] and GIST1M6. CIFAR10 is a subset of the 80-million tiny images and STL10 is a subset of Image-Net. Tiny-580K consists of 580, 000 tiny images. Flick-1M dataset consists of 1 million thumbnail images. SIFT-1M and GIST-1M datasets contain 1 million SIFT and GIST features respectively.\nFor the hashing performance evaluation, we follow the common setting in many supervised methods [1, 17]. For multi-class datasets, we use class labels to define the relevant and irrelevant semantic neighbours by label agreement. For large datasets: Flickr-1M, SIFT-1M, GIST-1M and Tiny-580K, the semantic ground truth is defined according to the `2 distance [8]. Specifically, a data point is labeled as a relevant data point of the query if it lies in the top 2 percentile points in the whole dataset. We generated GIST features for all image datasets except MNIST and USPS. we randomly select 2000 examples for testing queries, and the rest is used as database. We sample 2000 examples from the database as training data for learning models. For large datasets, we use 5000 examples for training. To evaluate the performance of compact bits, the maximum bit length is set to 64, as similar to the evaluation settings in other supervised hashing methods [1].\n3 http://www.cs.toronto.edu/\u02dckriz/cifar.html 4 http://www.stanford.edu/\u02dcacoates/stl10/ 5 http://press.liacs.nl/mirflickr/ 6 http://corpus-texmex.irisa.fr/\n5.1 State-of-the-art comparisons\nOur method is in the category of supervised method for learning compact binary codes. Thus we mainly compare with 3 supervised methods: supervised binary reconstructive embeddings (BREs) [1], supervised self-taught hashing (STHs) [16], semi-supervised sequential projection learning hashing (SPLH) [8]. We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21]. We carefully follow the original authors\u2019 instruction for parameter setting. For SPLH, the regularization parameter is picked from 0.01 to 1. We use the hierarchical variant of AGH. The bandwidth parameters of Gaussian affinity in MDSH is set as \u03c3 = td\u0304. Here d\u0304 is the average Euclidean distance of top 100 nearest neighbours and t is picked from 0.01 to 50. For supervised training of our StructHash and CGHash, we use 50 relevant and 100 irrelevant examples to construct similarity information for each data point.\nWe report the result of the NDCG measure in Table 1. We compare our StructHash using AUC and NDCG loss functions, and our CGHash for triplet loss with other supervised and unsupervised methods. StructHash using NDCG loss function performs the best in most cases. We also report the result of other common measures in Table 2, including the result of Precision-at-K, Mean Average Precision (mAP) and Precision-Recall. Precision-at-K is the proportion of true relevant data points in the returned top-K results. The Precision-Recall curve measures the overall performance in all positions of the prediction ranking, which is computed by varying the number of nearest neighbours. It shows that our method generally performs better than other methods on these evaluation measures. As described before, compared to the AUC measure which is position insensitive, the NDCG measure assigns different importance on ranking positions, which is closely related to many other position sensitive ranking measures (e.g., mAP). As expected, the result shows that on the Precision-at-K, mAP and Precision-recall measures, optimizing the position sensitive NDCG loss performs better than the AUC loss. StructHash with AUC loss actually minimize the triplet loss, hence it achieve similar performance with our triplet loss based method CGHash. StructHash with the NDCG loss which is position insensitive is able to outperform CGHash in these measures. We also plot the NDCG results on several datasets in Fig. 1 by varying the number of bits. Some retrieval examples are shown in Fig. 4.\nWe further evaluate our method on 4 large-scale datasets (Flickr-1M, SIFT-1M, GIST-1M and Tiny-580K). The results of NDCG, mAP and the precision of top-K neighbours are shown in Fig. 2. The NDCG and mAP results are shown by varying the number of bits. The precision of top-K neighbours is shown by varying the number of retrieved examples. In most cases, our method outperforms other competitors. Our method with NDCG loss function succeeds to achieve good performance both on NDCG and other measures.\nApplying the kernel technique in KLSH [22] and KSH [17] further improves the performance of our method. As describe in [17], we perform a pre-processing step to generate the kernel mapping features: we randomly select a number of support vectors (300) then compute the kernel response on data points as input features. Note that here we simply follow KSH for the kernel parameter setting. We evaluate this kernel version of our method in Fig. 3 and compare to KSH. Our kernel version is able to achieve better results.\n5.2 Evaluation of the extensions of StructHash for efficient learning\nIn this section, we evaluate the two extensions of StructHash proposed in Sec. 4.5 for efficient learning. Specifically, we denote the extension of using efficient stage-wise training as StructH-NDCG-Stage, which uses the original NDCG loss; we denote the second extension as StructH-SNDCG-stage which also applies stage-wise training but uses the proposed efficient Simplified NDCG loss instead. We mainly compare this two extensions with the original version of the StructHash with the NDCG loss, denoted as StructH-NDCG.\nTable 3 reports the compared results on 5 datasets using different ranking measures. As we can see, the two efficient extensions, the StructH-NDCG-Stage and the StructH-SNDCG-Stage, generally performs better or comparable with the original method StructH-NDCG.\nWe further compare these two efficient models against the original model in terms of training time. The experiments are conducted on a standard PC machine with 16G memory. Fig. 5 shows the compared results. It clearly reveals that the StructHash model with stage-wise training is orders of magnitude faster than the original StructHash model. Furthermore, compared to optimizing the NDCG score, optimizing the simplified NDCG (SNDCG) score generally reduces the training time by half, which shows the efficient inference of SNDCG significantly improve the training speed.\nWe also present the number of inference iterations performed in different hashing bits and the average time for each inference iteration, as well as the total training time (64-bit) in Table 4. As can be observed, the stage-\nwise training vastly reduces the inference iterations in each bit, therefore bringing orders of magnitude training speedup. As for the average time for each inference iteration, by using unweighted hamming distances in the stagewise training, StructHash-NDCG-Stage consumes less computation time than the StructHash-NDCG. Compared to optimizing the NDCG score, optimizing the SNDCG score further reduces the inference time. Fig. 6 plots the number of inference (in log scale) performed in different hashing bits. It explains that the speedup of the stage-wise training is brought by the greatly reduced inference iterations performed in each bit."}, {"heading": "5.2.1 Training on large-scale datasets", "text": "We further evaluate the more efficient models, i.e., the StructH-NDCG-Stage and the StructH-SNDCG-stage, on two large-scale datasets, namely, the TINY-580K and the SIFT-1M. The results are presented in Table 3. As can be observed, the StructHash with stage-wise training outperforms the original StructHash model. Given that stagewise StructHash uses unweighted hamming distance, this may indicate that the learned hash functions are more\nimportant than the weights. We also observe that, optimizing the SNDCG loss with stage-wise training performs on par with optimizing the original NDCG loss."}, {"heading": "5.2.2 Computational complexity", "text": "To show the scalability of the two more efficient extensions of StructHash, we present the training time by varying the number of training examples in Fig. 7. We report the training time of learning 32-bit hash functions. We also compare StructHash using stage-wise training with the original StructHash, in the left plot of Fig. 7. As we can see, compared to the original StructHash model, the stage-wise training brings orders of magnitude speedup. The right plot in Fig. 7 compares using simplified NDCG loss and the original NDCG loss, and clearly simplified NDCG loss is significantly more efficient."}, {"heading": "6 Conclusion", "text": "We have developed a flexible column generation based hashing framework that is able to optimize general multivariate ranking measures as well as the triplet loss. We have shown that column generation optimization is able to learn high-quality binary codes for supervised hashing. The fact that the proposed method for optimizing ranking loss usually outperforms comparable hashing approaches is to be expected, as it more directly optimizes the required loss function. It is anticipated that the success of the approach may lead to a range of new hashing-based applications with task-specified targets. We also present two extensions of learning framework for efficient learning which are based on stage-wise training and using the proposed simplified NDCG for efficient ranking inference."}], "references": [{"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Proc. Adv. Neural Info. Process. Syst.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "In Proc. Adv. Neural Info. Process. Syst.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A general two-step approach to learning-based hashing", "author": ["G. Lin", "C. Shen", "D. Suter", "A. van den Hengel"], "venue": "In Proc. Int. Conf. Comp. Vis., Sydney, Australia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Inductive hashing on manifolds", "author": ["F. Shen", "C. Shen", "Q. Shi", "A. van den Hengel", "Z. Tang"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn., Oregon,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.F. Chang"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Fast supervised hashing with decision trees for high-dimensional data", "author": ["G. Lin", "C. Shen", "Q. Shi", "A. van den Hengel", "D. Suter"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Small codes and large image databases for recognition", "author": ["A. Torralba", "R. Fergus", "Y. Weiss"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Semi-supervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.F. Chang"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "LDAHash: Improved matching with smaller descriptors", "author": ["C. Strecha", "A. Bronstein", "M. Bronstein", "P. Fua"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["T. Dean", "M.A. Ruzon", "M. Segal", "J. Shlens", "S. Vijayanarasimhan", "J. Yagnik"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Learning a distance metric from relative comparisons", "author": ["M. Schultz", "T. Joachims"], "venue": "In Proc. Adv. Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Positive semidefinite metric learning using boosting-like algorithms", "author": ["C. Shen", "J. Kim", "L. Wang", "A. van den Hengel"], "venue": "J. Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Metric learning to rank", "author": ["B. McFee", "G.R.G. Lanckriet"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "IR evaluation methods for retrieving highly relevant documents", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "In Proc. ACM Conf. SIGIR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Extensions to self-taught hashing: kernelisation and supervision", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. ACM Conf. SIGIR Workshop,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.G. Jiang", "S.F. Chang"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Learning hash functions using column generation", "author": ["X. Li", "G. Lin", "C. Shen", "A. van den Hengel", "Anthony Dick"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Optimizing ranking measures for compact binary code learning", "author": ["G. Lin", "C. Shen", "J. Wu"], "venue": "In Proc. Eur. Conf. Comp. Vis.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In Proc. Int. Conf. Very Large Data Bases,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Iterative quantization: a procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Kernelized locality-sensitive hashing", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Mohammad Norouzi", "David J. Fleet"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Hamming distance metric learning", "author": ["Mohammad Norouzi", "David J. Fleet", "Ruslan Salakhutdinov"], "venue": "In Proc. Adv. Neural Info. Process. Syst.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Ranking preserving hashing for fast similarity search", "author": ["Qifan Wang", "Zhiwei Zhang", "Luo Si"], "venue": "In Proc. Int. Joint Conf. Artif. Intelli.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Online learning in the embedded manifold of low-rank matrices", "author": ["Uri Shalit", "Daphna Weinshall", "Gal Chechik"], "venue": "Journal of Mach. Learn. Res.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Efficient learning of mahalanobis metrics for ranking", "author": ["Daryl Lim", "Gert Lanckriet"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Linear programming boosting via column generation", "author": ["Ayhan Demiriz", "Kristin P. Bennett", "John Shawe-Taylor"], "venue": "Mach. Learn.,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "On the dual formulation of boosting algorithms", "author": ["Chunhua Shen", "Hanxi Li"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "StructBoost: Boosting methods for predicting structured output variables", "author": ["C. Shen", "G. Lin", "A. van den Hengel"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Multidimensional spectral hashing", "author": ["Y. Weiss", "R. Fergus", "A. Torralba"], "venue": "In Proc. Eur. Conf. Comp. Vis.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM T. Math. Softw.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "Linear programming boosting via column generation", "author": ["A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor"], "venue": "Mach. Learn.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2002}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "The cutting-plane method for solving convex programs", "author": ["J.E. Jr. Kelley"], "venue": "J. Society for Industrial & Applied Math.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1960}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "In Proc. ACM Knowledge Discovery & Data Mining,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "On the dual formulation of boosting algorithms", "author": ["C. Shen", "H. Li"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "The MOSEK optimization toolbox for MATLAB manual", "author": ["MOSEK ApS"], "venue": "Version 7.1 (Revision", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Structured learning for non-smooth ranking losses", "author": ["S. Chakrabarti", "R. Khanna", "U. Sawant", "C. Bhattacharyya"], "venue": "In Proc. ACM Knowledge Discovery & Data Mining,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "A support vector method for optimizing average precision", "author": ["Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "In Proc. ACM Conf. SIGIR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Spherical hashing", "author": ["J. Heo", "Y. Lee", "J. He", "S. Chang", "S. Yoon"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 1, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 2, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 3, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 4, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 5, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 6, "context": "Applications include image retrieval [7, 8], image matching [9], large-scale object detection [10], etc.", "startOffset": 37, "endOffset": 43}, {"referenceID": 7, "context": "Applications include image retrieval [7, 8], image matching [9], large-scale object detection [10], etc.", "startOffset": 37, "endOffset": 43}, {"referenceID": 8, "context": "Applications include image retrieval [7, 8], image matching [9], large-scale object detection [10], etc.", "startOffset": 60, "endOffset": 63}, {"referenceID": 9, "context": "Applications include image retrieval [7, 8], image matching [9], large-scale object detection [10], etc.", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "This type of relative similarity comparisons have been successfully applied to learn quadratic distance metrics [11, 12].", "startOffset": 112, "endOffset": 120}, {"referenceID": 11, "context": "This type of relative similarity comparisons have been successfully applied to learn quadratic distance metrics [11, 12].", "startOffset": 112, "endOffset": 120}, {"referenceID": 12, "context": "For example, information retrieval and ranking criteria [13] such as the Area Under the ROC Curve (AUC) [14], Normalized Discounted Cumulative Gain (NDCG) [15], Precision-at-K, Precision-Recall and Mean Average Precision (mAP) have been widely adopted to evaluate the success of hashing methods.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "For example, information retrieval and ranking criteria [13] such as the Area Under the ROC Curve (AUC) [14], Normalized Discounted Cumulative Gain (NDCG) [15], Precision-at-K, Precision-Recall and Mean Average Precision (mAP) have been widely adopted to evaluate the success of hashing methods.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "For example, information retrieval and ranking criteria [13] such as the Area Under the ROC Curve (AUC) [14], Normalized Discounted Cumulative Gain (NDCG) [15], Precision-at-K, Precision-Recall and Mean Average Precision (mAP) have been widely adopted to evaluate the success of hashing methods.", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 81, "endOffset": 91}, {"referenceID": 4, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 81, "endOffset": 91}, {"referenceID": 15, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 81, "endOffset": 91}, {"referenceID": 16, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "Preliminary results of our work appeared in [18] and [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "Preliminary results of our work appeared in [18] and [19].", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "Examples fall into this category are locality sensitive hashing (LSH) [20], spectral hashing (SPH) [2], anchor graph hashing (AGH) [5], iterative quantization hashing (ITQ) [21].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "Examples fall into this category are locality sensitive hashing (LSH) [20], spectral hashing (SPH) [2], anchor graph hashing (AGH) [5], iterative quantization hashing (ITQ) [21].", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "Examples fall into this category are locality sensitive hashing (LSH) [20], spectral hashing (SPH) [2], anchor graph hashing (AGH) [5], iterative quantization hashing (ITQ) [21].", "startOffset": 131, "endOffset": 134}, {"referenceID": 20, "context": "Examples fall into this category are locality sensitive hashing (LSH) [20], spectral hashing (SPH) [2], anchor graph hashing (AGH) [5], iterative quantization hashing (ITQ) [21].", "startOffset": 173, "endOffset": 177}, {"referenceID": 19, "context": "Specifically, LSH [20] uses random projection to generate binary codes; SPH [2] aims to preserve the neighbourhood relation by optimizing the Laplacian affinity; AGH [5] makes the original SPH much more scalable; ITH [21] first performs linear dimensionality reduction and then conduct binary quantization in the resulting space.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "Specifically, LSH [20] uses random projection to generate binary codes; SPH [2] aims to preserve the neighbourhood relation by optimizing the Laplacian affinity; AGH [5] makes the original SPH much more scalable; ITH [21] first performs linear dimensionality reduction and then conduct binary quantization in the resulting space.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "Specifically, LSH [20] uses random projection to generate binary codes; SPH [2] aims to preserve the neighbourhood relation by optimizing the Laplacian affinity; AGH [5] makes the original SPH much more scalable; ITH [21] first performs linear dimensionality reduction and then conduct binary quantization in the resulting space.", "startOffset": 166, "endOffset": 169}, {"referenceID": 20, "context": "Specifically, LSH [20] uses random projection to generate binary codes; SPH [2] aims to preserve the neighbourhood relation by optimizing the Laplacian affinity; AGH [5] makes the original SPH much more scalable; ITH [21] first performs linear dimensionality reduction and then conduct binary quantization in the resulting space.", "startOffset": 217, "endOffset": 221}, {"referenceID": 0, "context": "Binary reconstruction embedding (BRE) [1] aims to minimize the expected distances; semi-supervised sequential projection learning hashing (SPLH) [8] enforces the smoothness of similar data points and the separability of dissimilar data points; kernelized LSH, proposed by Kulis and Grauman [22], randomly samples training data as support vectors, and randomly draws the dual coefficients from a Gaussian distribution.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "Binary reconstruction embedding (BRE) [1] aims to minimize the expected distances; semi-supervised sequential projection learning hashing (SPLH) [8] enforces the smoothness of similar data points and the separability of dissimilar data points; kernelized LSH, proposed by Kulis and Grauman [22], randomly samples training data as support vectors, and randomly draws the dual coefficients from a Gaussian distribution.", "startOffset": 145, "endOffset": 148}, {"referenceID": 21, "context": "Binary reconstruction embedding (BRE) [1] aims to minimize the expected distances; semi-supervised sequential projection learning hashing (SPLH) [8] enforces the smoothness of similar data points and the separability of dissimilar data points; kernelized LSH, proposed by Kulis and Grauman [22], randomly samples training data as support vectors, and randomly draws the dual coefficients from a Gaussian distribution.", "startOffset": 290, "endOffset": 294}, {"referenceID": 16, "context": "[17] extended kernelized LSH to kernelized supervised hashing (KSH).", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3, 6] present a general two step framework for hashing learning.", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[3, 6] present a general two step framework for hashing learning.", "startOffset": 0, "endOffset": 6}, {"referenceID": 22, "context": "In [23], Norouzi et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "They further generalize the method in [23] to optimize a triplet ranking loss designed to preserve relative similarities [24].", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "They further generalize the method in [23] to optimize a triplet ranking loss designed to preserve relative similarities [24].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "The hashing method in [25] proposes to optimizes the NDCG ranking loss with a gradient decent method, which comes out after the publication of our preliminary version of StructHash in [19].", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "The hashing method in [25] proposes to optimizes the NDCG ranking loss with a gradient decent method, which comes out after the publication of our preliminary version of StructHash in [19].", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "Learning to rank Our method is primarily inspired by recent advances in metric learning for ranking [13, 26, 27].", "startOffset": 100, "endOffset": 112}, {"referenceID": 25, "context": "Learning to rank Our method is primarily inspired by recent advances in metric learning for ranking [13, 26, 27].", "startOffset": 100, "endOffset": 112}, {"referenceID": 26, "context": "Learning to rank Our method is primarily inspired by recent advances in metric learning for ranking [13, 26, 27].", "startOffset": 100, "endOffset": 112}, {"referenceID": 12, "context": "In [13], McFee et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "[26] propose a scalable method for optimizing a ranking loss, though they only consider the Area Under the ROC Curve (AUC) loss.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "In [27], Lim et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": ", the Weighted Approximate Pairwise Ranking (WARP) loss [28].", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "Column generation Column generation is widely applied in boosting methods [29, 30, 31].", "startOffset": 74, "endOffset": 86}, {"referenceID": 29, "context": "Column generation Column generation is widely applied in boosting methods [29, 30, 31].", "startOffset": 74, "endOffset": 86}, {"referenceID": 30, "context": "Column generation Column generation is widely applied in boosting methods [29, 30, 31].", "startOffset": 74, "endOffset": 86}, {"referenceID": 28, "context": "LPBoost [29] is a linear programming boosting method that iteratively learn weak classifiers to form a strong classifier.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "StructBoost [31] provides a general structured learning framework using column generation for structured prediction problems.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "Such weighted hamming distance is used in multi-dimension spectral hashing [32].", "startOffset": 75, "endOffset": 79}, {"referenceID": 32, "context": ", using LBFGS-B [33].", "startOffset": 16, "endOffset": 20}, {"referenceID": 33, "context": "LPBoost [34] applies this technique to design boosting algorithms.", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": "2 for r = 1 to m do 3 find a new hash function hr(\u00b7) by solving the subproblem: (17); 4 add hr(\u00b7) to the working set of hash functions; 5 solve the primal problem in (11) for w (using LBFGS-B[33]), and calculate the dual solution \u03bc by (18);", "startOffset": 191, "endOffset": 195}, {"referenceID": 32, "context": "LBFGS-B solver [33].", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Alternatively, we can employ the spectral relaxation method [5] which drops the sign function and solves a generalized eigenvalue problem to obtain a solution for initialization.", "startOffset": 60, "endOffset": 63}, {"referenceID": 34, "context": "With the definition of Fenchel conjugate [35]: f(z) := sup x\u2208domf x>z\u2212 f(x) ( here f(\u00b7) is the Fenchel conjugate of the function f(\u00b7) ), we have the following dual objective:", "startOffset": 41, "endOffset": 45}, {"referenceID": 35, "context": "The cutting-plane method [36] is commonly employed, which allows to maintain a small working-set of constraints and obtain an approximate solution of the original problem up to a pre-set precision.", "startOffset": 25, "endOffset": 29}, {"referenceID": 36, "context": "To speed up, the 1-slack reformulation is proposed [37].", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "These evaluation measures can be optimized directly as label loss \u2206 [13, 14].", "startOffset": 68, "endOffset": 76}, {"referenceID": 13, "context": "These evaluation measures can be optimized directly as label loss \u2206 [13, 14].", "startOffset": 68, "endOffset": 76}, {"referenceID": 33, "context": "This is aligned with boosting methods [34, 38], and enables us to learn hash functions efficiently.", "startOffset": 38, "endOffset": 46}, {"referenceID": 37, "context": "This is aligned with boosting methods [34, 38], and enables us to learn hash functions efficiently.", "startOffset": 38, "endOffset": 46}, {"referenceID": 36, "context": "Inspired by [37], we first derive the 1-slack formulation of the original n-slack formulation (46):", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "As in [37], cutting-plane methods can be used to solve the 1-slack primal problem (48) efficiently.", "startOffset": 6, "endOffset": 10}, {"referenceID": 38, "context": ", using MOSEK [39]) on current working set W ; 5 for i = 1, .", "startOffset": 14, "endOffset": 18}, {"referenceID": 32, "context": ", LBFGS [33]) for learning the parameters of a hash function.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "We apply the spectral relaxation [5] to obtain an initial point for solving (55), which drops the sign(\u00b7) function and solves a generalized eigenvalue problem.", "startOffset": 33, "endOffset": 36}, {"referenceID": 12, "context": "Following [13], we define the loss function over two rankings \u2206 \u2208 [0 1] as: \u2206(y,y) = 1\u2212 score(y,y).", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "Following [13], we define the loss function over two rankings \u2206 \u2208 [0 1] as: \u2206(y,y) = 1\u2212 score(y,y).", "startOffset": 66, "endOffset": 71}, {"referenceID": 13, "context": "For using this AUC loss, the maximization inference in (50) can be solved efficiently by sorting the distances of data pairs, as described in [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "Normalized Discounted Cumulative Gain [15] is to measure the ranking quality of the first K returned neighbours.", "startOffset": 38, "endOffset": 42}, {"referenceID": 39, "context": "A dynamic programming algorithm is proposed in [40] for solving the maximization inference in (50).", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "In column generation based totallycorrective boosting methods [29, 30, 31], all the hash function weights w are updated during each column generation iteration.", "startOffset": 62, "endOffset": 74}, {"referenceID": 29, "context": "In column generation based totallycorrective boosting methods [29, 30, 31], all the hash function weights w are updated during each column generation iteration.", "startOffset": 62, "endOffset": 74}, {"referenceID": 30, "context": "In column generation based totallycorrective boosting methods [29, 30, 31], all the hash function weights w are updated during each column generation iteration.", "startOffset": 62, "endOffset": 74}, {"referenceID": 39, "context": "Usually when using position sensitive loss functions, such as mAP, NDCG, it is computational expensive to solve the maximization inference [40, 41], which might limit its application on large-scale learning.", "startOffset": 139, "endOffset": 147}, {"referenceID": 40, "context": "Usually when using position sensitive loss functions, such as mAP, NDCG, it is computational expensive to solve the maximization inference [40, 41], which might limit its application on large-scale learning.", "startOffset": 139, "endOffset": 147}, {"referenceID": 26, "context": "Inspired by the efficient metric learning method in [27], here we discuss a form of positionsensitive ranking loss which is capable for fast inference.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "Nine datasets are used here for evaluation, including one UCI dataset: ISOLET, 4 image datasets: CIFAR10, STL10, MNIST, USPS, and another 4 large image datasets: Tiny-580K [21], Flickr-1M, SIFT-1M [8] and GIST1M.", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "Nine datasets are used here for evaluation, including one UCI dataset: ISOLET, 4 image datasets: CIFAR10, STL10, MNIST, USPS, and another 4 large image datasets: Tiny-580K [21], Flickr-1M, SIFT-1M [8] and GIST1M.", "startOffset": 197, "endOffset": 200}, {"referenceID": 0, "context": "For the hashing performance evaluation, we follow the common setting in many supervised methods [1, 17].", "startOffset": 96, "endOffset": 103}, {"referenceID": 16, "context": "For the hashing performance evaluation, we follow the common setting in many supervised methods [1, 17].", "startOffset": 96, "endOffset": 103}, {"referenceID": 7, "context": "For large datasets: Flickr-1M, SIFT-1M, GIST-1M and Tiny-580K, the semantic ground truth is defined according to the `2 distance [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "To evaluate the performance of compact bits, the maximum bit length is set to 64, as similar to the evaluation settings in other supervised hashing methods [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "Thus we mainly compare with 3 supervised methods: supervised binary reconstructive embeddings (BREs) [1], supervised self-taught hashing (STHs) [16], semi-supervised sequential projection learning hashing (SPLH) [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 15, "context": "Thus we mainly compare with 3 supervised methods: supervised binary reconstructive embeddings (BREs) [1], supervised self-taught hashing (STHs) [16], semi-supervised sequential projection learning hashing (SPLH) [8].", "startOffset": 144, "endOffset": 148}, {"referenceID": 7, "context": "Thus we mainly compare with 3 supervised methods: supervised binary reconstructive embeddings (BREs) [1], supervised self-taught hashing (STHs) [16], semi-supervised sequential projection learning hashing (SPLH) [8].", "startOffset": 212, "endOffset": 215}, {"referenceID": 19, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 88, "endOffset": 92}, {"referenceID": 4, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 121, "endOffset": 124}, {"referenceID": 41, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 198, "endOffset": 202}, {"referenceID": 20, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 237, "endOffset": 241}, {"referenceID": 21, "context": "Applying the kernel technique in KLSH [22] and KSH [17] further improves the performance of our method.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "Applying the kernel technique in KLSH [22] and KSH [17] further improves the performance of our method.", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "As describe in [17], we perform a pre-processing step to generate the kernel mapping features: we randomly select a number of support vectors (300) then compute the kernel response on data points as input features.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "3 Comparison on large datasets of our kernel StructHash (StructHash-Kernel) with our non-kernel StructHash and the relevant method KSH [17].", "startOffset": 135, "endOffset": 139}], "year": 2016, "abstractText": "Hashing methods aim to learn a set of hash functions which map the original features to compact binary codes with similarity preserving in the Hamming space. Hashing has proven a valuable tool for large-scale information retrieval. We propose a column generation based binary code learning framework for data-dependent hash function learning. Given a set of triplets that encode the pairwise similarity comparison information, our column generation based method learns hash functions that preserve the relative comparison relations within the large-margin learning framework. Our method iteratively learns the best hash functions during the column generation procedure. Existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest\u2014multivariate performance measures such as the AUC and NDCG. Our column generation based method can be further generalized from the triplet loss to a general structured learning based framework that allows one to directly optimize multivariate performance measures. For optimizing general ranking measures, the resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. We use a combination of column generation and cutting-plane techniques to solve the optimization problem. To speed-up the training we further explore stage-wise training and propose to use a simplified NDCG loss for efficient inference. We demonstrate the generality of our method by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.", "creator": "LaTeX with hyperref package"}}}