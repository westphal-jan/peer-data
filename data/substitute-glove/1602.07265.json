{"id": "1602.07265", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2016", "title": "Search Improves Label for Active Learning", "abstract": "We reports level learning turned service to made similarities ephesian: Label (which becomes using) them Search (which form not ). The Search browser suvs which solve where a human searches a interface to seed they backronym, existing solution. Search is keeping long Label also being depends that requires then but difficult. We entertainment others own algorithm equipment both oracles are improve exponentially variety rather - relies consolidation over Label nearly.", "histories": [["v1", "Tue, 23 Feb 2016 19:05:09 GMT  (31kb)", "https://arxiv.org/abs/1602.07265v1", "22 pages"], ["v2", "Mon, 24 Oct 2016 06:29:08 GMT  (62kb)", "http://arxiv.org/abs/1602.07265v2", "32 pages; NIPS 2016"]], "COMMENTS": "22 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alina beygelzimer", "daniel j hsu", "john langford", "chicheng zhang"], "accepted": true, "id": "1602.07265"}, "pdf": {"name": "1602.07265.pdf", "metadata": {"source": "CRF", "title": "Search Improves Label for Active Learning", "authors": ["Alina Beygelzimer", "Daniel Hsu", "John Langford", "Chicheng Zhang"], "emails": ["beygel@yahoo-inc.com", "djhsu@cs.columbia.edu", "jcl@microsoft.com", "chichengzhang@ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n07 26\n5v 2\n[ cs\n.L G\n] 2"}, {"heading": "1 Introduction", "text": "Most active learning theory is based on interacting with a Label oracle: An active learner observes unlabeled examples, each with a label that is initially hidden. The learner provides an unlabeled example to the oracle, and the oracle responds with the label. Using Label in an active learning algorithm is known to give (sometimes exponentially large) problem-dependent improvements in label complexity, even in the agnostic setting where no assumption is made about the underlying distribution [e.g., Balcan et al., 2006, Hanneke, 2007, Dasgupta et al., 2007, Hanneke, 2014].\nA well-known deficiency of Label arises in the presence of rare classes in classification problems, frequently the case in practice [Attenberg and Provost, 2010, Simard et al., 2014]. Class imbalance may be so extreme that simply finding an example from the rare class can exhaust the labeling budget. Consider the problem of learning interval functions in [0, 1]. Any Label-only active learner needs at least \u2126(1/\u01eb) Label queries to learn an arbitrary target interval with error at most \u01eb [Dasgupta, 2005]. Given any positive example from the interval, however, the query complexity of learning intervals collapses to O(log(1/\u01eb)), as we can just do a binary search for each of the end points.\nA natural approach used to overcome this hurdle in practice is to search for known examples of the rare class [Attenberg and Provost, 2010, Simard et al., 2014]. Domain experts are often adept at finding examples of a class by various, often clever means. For instance, when building a hate speech filter, a simple web search can readily produce a set of positive examples. Sending a random batch of unlabeled text to Label is unlikely to produce any positive examples at all.\nAnother form of interaction common in practice is providing counterexamples to a learned predictor. When monitoring the stream filtered by the current hate speech filter, a human editor may spot a clear-cut\n\u2217beygel@yahoo-inc.com \u2020djhsu@cs.columbia.edu \u2021jcl@microsoft.com \u00a7chichengzhang@ucsd.edu\nexample of hate speech that seeped through the filter. The editor, using all the search tools available to her, may even be tasked with searching for such counterexamples. The goal of the learning system is then to interactively restrict the searchable space, guiding the search process to where it is most effective.\nCounterexamples can be ineffective or misleading in practice as well. Reconsidering the intervals example above, a counterexample on the boundary of an incorrect interval provides no useful information about any other examples. What is a good counterexample? What is a natural way to restrict the searchable space? How can the intervals problem be generalized?\nWe define a new oracle, Search, that provides counterexamples to version spaces. Given a set of possible classifiers H mapping unlabeled examples to labels, a version space V \u2286 H is the subset of classifiers still under consideration by the algorithm. A counterexample to a version space is a labeled example which every classifier in the version space classifies incorrectly. When there is no counterexample to the version space, Search returns nothing.\nHow can a counterexample to the version space be used? We consider a nested sequence of hypothesis classes of increasing complexity, akin to Structural Risk Minimization (SRM) in passive learning [see, e.g., Vapnik, 1982, Devroye et al., 1996]. When Search produces a counterexample to the version space, it gives a proof that the current hypothesis class is too simplistic to solve the problem effectively. We show that this guided increase in hypothesis complexity results in a radically lower Label complexity than directly learning on the complex space. Sample complexity bounds for model selection in Label-only active learning were studied by Balcan et al. [2010], Hanneke [2011].\nSearch can easily model the practice of seeding discussed earlier. If the first hypothesis class has just the constant always-negative classifier h(x) = \u22121, a seed example with label +1 is a counterexample to the version space. Our most basic algorithm uses Search just once before using Label, but it is clear from inspection that multiple seeds are not harmful, and they may be helpful if they provide the proof required to operate with an appropriately complex hypothesis class.\nDefining Search with respect to a version space rather than a single classifier allows us to formalize \u201ccounterexample far from the boundary\u201d in a general fashion which is compatible with the way Label-based active learning algorithms work.\nRelated work. The closest oracle considered in the literature is the Class Conditional Query (CCQ) [Balcan and Hanneke, 2012] oracle. A query to CCQ specifies a finite set of unlabeled examples and a label while returning an example in the subset with the specified label, if one exists.\nIn contrast, Search has an implicit query set that is an entire region of the input space rather than a finite set. Simple searches over this large implicit domain can more plausibly discover relevant counterexamples: When building a detector for penguins in images, the input to CCQ might be a set of images and the label \u201cpenguin\u201d. Even if we are very lucky and the set happens to contain a penguin image, a search amongst image tags may fail to find it in the subset because it is not tagged appropriately. Search is more likely to discover counterexamples\u2014surely there are many images correctly tagged as having penguins.\nWhy is it natural to define a query region implicitly via a version space? There is a practical reason\u2014it is a concise description of a natural region with an efficiently implementable membership filter [Beygelzimer et al., 2010, 2011, Huang et al., 2015]. (Compare this to an oracle call that has to explicitly enumerate a large set of examples. The algorithm of Balcan and Hanneke [2012] uses samples of size roughly d\u03bd/\u01eb2.)\nThe use of Search in this paper is also substantially different from the use ofCCQ by Balcan and Hanneke [2012]. Our motivation is to use Search to assist Label, as opposed to using Search alone. This is especially useful in any setting where the cost of Search is significantly higher than the cost of Label\u2014we hope to avoid using Search queries whenever it is possible to make progress using Label queries. This is consistent with how interactive learning systems are used in practice. For example, the Interactive Classification and Extraction system of Simard et al. [2014] combines Label with search in a production environment.\nThe final important distinction is that we require Search to return the label of the optimal predictor in the nested sequence. For many natural sequences of hypothesis classes, the Bayes optimal classifier is eventually in the sequence, in which case it is equivalent to assuming that the label in a counterexample is the most probable one, as opposed to a randomly-drawn label from the conditional distribution (as in CCQ\nand Label). Is this a reasonable assumption? Unlike with Label queries, where the labeler has no choice of what to label, here the labeler chooses a counterexample. If a human editor finds an unquestionable example of hate speech that seeped through the filter, it is quite reasonable to assume that this counterexample is consistent with the Bayes optimal predictor for any sensible feature representation.\nOrganization. Section 2 formally introduces the setting. Section 3 shows that Search is at least as powerful as Label. Section 4 shows how to use Search and Label jointly in the realizable setting where a zero-error classifier exists in the nested sequence of hypothesis classes. Section 5 handles the agnostic setting where Label is subject to label noise, and shows an amortized approach to combining the two oracles with a good guarantee on the total cost."}, {"heading": "2 Definitions and Setting", "text": "In active learning, there is an underlying distribution D over X \u00d7 Y, where X is the instance space and Y := {\u22121,+1} is the label space. The learner can obtain independent draws from D, but the label is hidden unless explicitly requested through a query to the Label oracle. Let DX denote the marginal of D over X .\nWe consider learning with a nested sequence of hypotheses classes H0 \u2282 H1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 Hk \u00b7 \u00b7 \u00b7 , where Hk \u2286 YX has VC dimension dk. For a set of labeled examples S \u2286 X \u00d7Y, let Hk(S) := {h \u2208 Hk : \u2200(x, y) \u2208 S h(x) = y} be the set of hypotheses in Hk consistent with S. Let err(h) := Pr(x,y)\u223cD[h(x) 6= y] denote the error rate of a hypothesis h with respect to distribution D, and err(h, S) be the error rate of h on the labeled examples in S. Let h\u2217k = argminh\u2208Hk err(h) breaking ties arbitrarily and let k \u2217 := argmink\u22650 err(h \u2217 k) breaking ties in favor of the smallest such k. For simplicity, we assume the minimum is attained at some finite k\u2217. Finally, define h\u2217 := h\u2217k\u2217 , the optimal hypothesis in the sequence of classes. The goal of the learner is to learn a hypothesis with error rate not much more than that of h\u2217.\nIn addition to Label, the learner can also query Search with a version space.\nOracle SearchH(V ) (where H \u2208 {Hk}\u221ek=0) input: Set of hypotheses V \u2282 H output: Labeled example (x, h\u2217(x)) s.t. h(x) 6= h\u2217(x) for all h \u2208 V , or \u22a5 if there is no such example.\nThus if SearchH(V ) returns an example, this example is a systematic mistake made by all hypotheses in V . (If V = \u2205, we expect Search to return some example, i.e., not \u22a5.)\nOur analysis is given in terms of the disagreement coefficient of Hanneke [2007], which has been a central parameter for analyzing active learning algorithms. Define the region of disagreement of a set of hypotheses V as Dis(V ) := {x \u2208 X : \u2203h, h\u2032 \u2208 V s.t. h(x) 6= h\u2032(x)}. The disagreement coefficient of V at scale r is \u03b8V (r) := suph\u2208V,r\u2032\u2265r PrDX [Dis(BV (h, r \u2032))]/r\u2032, where BV (h, r \u2032) = {h\u2032 \u2208 V : Prx\u223cDX [h\u2032(x) 6= h(x)] \u2264 r\u2032} is the ball of radius r\u2032 around h. The O\u0303(\u00b7) notation hides factors that are polylogarithmic in 1/\u03b4 and quantities that do appear, where \u03b4 is the usual confidence parameter."}, {"heading": "3 The Relative Power of the Two Oracles", "text": "Although Search cannot always implement Label efficiently, it is as effective at reducing the region of disagreement. The clearest example is learning threshold classifiers H := {hw : w \u2208 [0, 1]} in the realizable case, where hw(x) = +1 if w \u2264 x \u2264 1, and \u22121 if 0 \u2264 x < w. A simple binary search with Label achieves an exponential improvement in query complexity over passive learning. The agreement region of any set of threshold classifiers with thresholds in [wmin, wmax] is [0, wmin)\u222a [wmax, 1]. Since Search is allowed to return any counterexample in the agreement region, there is no mechanism for forcing Search to return the label of a particular point we want. However, this is not needed to achieve logarithmic query complexity with\nSearch: If binary search starts with querying the label of x \u2208 [0, 1], we can query SearchH(Vx), where Vx := {hw \u2208 H : w < x} instead. If Search returns \u22a5, we know that the target w\u2217 \u2264 x and can safely reduce the region of disagreement to [0, x). If Search returns a counterexample (x0,\u22121) with x0 \u2265 x, we know that w\u2217 > x0 and can reduce the region of disagreement to (x0, 1].\nThis observation holds more generally. In the proposition below, we assume that Label(x) = h\u2217(x) for simplicity. If Label(x) is noisy, the proposition holds for any active learning algorithm that doesn\u2019t eliminate any h \u2208 H : h(x) = Label(x) from the version space.\nProposition 1. For any call x \u2208 X to Label such that Label(x) = h\u2217(x), we can construct a call to Search that achieves a no lesser reduction in the region of disagreement.\nProof. For any V \u2286 H , let HSearch(V ) be the hypotheses in H consistent with the output of SearchH(V ): if SearchH(V ) returns a counterexample (x, y) to V , then HSearch(V ) := {h \u2208 H : h(x) = y}; otherwise, HSearch(V ) := V . Let HLabel(x) := {h \u2208 H : h(x) = Label(x)}. Also, let Vx := H+1(x) := {h \u2208 H : h(x) = +1}. We will show that Vx is such that HSearch(Vx) \u2286 HLabel(x), and hence Dis(HSearch(Vx)) \u2286 Dis(HLabel(x)).\nThere are two cases to consider: If h\u2217(x) = +1, then SearchH(Vx) returns \u22a5. In this case, HLabel(x) = HSearch(Vx) = H+1(x), and we are done. If h\n\u2217(x) = \u22121, Search(Vx) returns a valid counterexample (possibly (x,\u22121)) in the region of agreement of H+1(x), eliminating all of H+1(x). Thus HSearch(Vx) \u2282 H \\H+1(x) = HLabel(x), and the claim holds also.\nAs shown by the problem of learning intervals on the line, SEARCH can be exponentially more powerful than LABEL."}, {"heading": "4 Realizable Case", "text": "We now turn to general active learning algorithms that combine Search and Label. We focus on algorithms using both Search and Label since Label is typically easier to implement than Search and hence should be used where Search has no significant advantage. (Whenever Search is less expensive than Label, Section 3 suggests a transformation to a Search-only algorithm.)\nThis section considers the realizable case, in which we assume that the hypothesis h\u2217 = h\u2217k\u2217 \u2208 Hk\u2217 has err(h\u2217) = 0. This means that Label(x) returns h\u2217(x) for any x in the support of DX ."}, {"heading": "4.1 Combining LABEL and SEARCH", "text": "Our algorithm (shown as Algorithm 1) is called Larch, because it combines Label and Search. Like many selective sampling methods, Larch uses a version space to determine its Label queries.\nFor concreteness, we use (a variant of) the algorithm of Cohn et al. [1994], denoted by CAL, as a subroutine in Larch. The inputs to CAL are: a version space V , the Label oracle, a target error rate, and a confidence parameter; and its output is a set of labeled examples (implicitly defining a new version space). CAL is described in Appendix B; its essential properties are specified in Lemma 1.\nLarch differs from Label-only active learners (like CAL) by first calling Search in Step 3. If Search returns \u22a5, Larch checks to see if the last call to CAL resulted in a small-enough error, halting if so in Step 6, and decreasing the allowed error rate if not in Step 8. If Search instead returns a counterexample, the hypothesis class Hk must be impoverished, so in Step 12, Larch increases the complexity of the hypothesis class to the minimum complexity sufficient to correctly classify all known labeled examples in S. After the Search, CAL is called in Step 14 to discover a sufficiently low-error (or at least low-disagreement) version space with high probability.\nWhen Larch advances to index k (for any k \u2264 k\u2217), its set of labeled examples S may imply a version space Hk(S) \u2286 Hk that can be actively-learned more efficiently than the whole of Hk. In our analysis, we quantify this through the disagreement coefficient of Hk(S), which may be markedly smaller than that of the full Hk.\nAlgorithm 1 Larch\ninput: Nested hypothesis classes H0 \u2282 H1 \u2282 \u00b7 \u00b7 \u00b7 ; oracles Label and Search; learning parameters \u01eb, \u03b4 \u2208 (0, 1)\n1: initialize S \u2190 \u2205, (index) k \u2190 0, \u2113 \u2190 0 2: for i = 1, 2, . . . do 3: e \u2190 SearchHk(Hk(S)) 4: if e = \u22a5 then # no counterexample found 5: if 2\u2212\u2113 \u2264 \u01eb then 6: return any h \u2208 Hk(S) 7: else 8: \u2113 \u2190 \u2113+ 1 9: end if 10: else # counterexample found 11: S \u2190 S \u222a {e} 12: k \u2190 min{k\u2032 : Hk\u2032 (S) 6= \u2205} 13: end if 14: S \u2190 S \u222a CAL(Hk(S),Label, 2\u2212\u2113, \u03b4/(i2 + i)) 15: end for\nThe following theorem bounds the oracle query complexity of Algorithm 1 for learning with both Search and Label in the realizable setting. The proof is in section 4.2.\nTheorem 1. Assume that err(h\u2217) = 0. For each k\u2032 \u2265 0, let \u03b8k\u2032 (\u00b7) be the disagreement coefficient of Hk\u2032(S[k\u2032]), where S[k\u2032] is the set of labeled examples S in Larch at the first time that k \u2265 k\u2032. Fix any \u01eb, \u03b4 \u2208 (0, 1). If Larch is run with inputs hypothesis classes {Hk}\u221ek=0, oracles Label and Search, and learning parameters \u01eb, \u03b4, then with probability at least 1\u2212\u03b4: Larch halts after at most k\u2217+log2(1/\u01eb) for-loop iterations and returns a classifier with error rate at most \u01eb; furthermore, it draws at most O\u0303(k\u2217dk\u2217/\u01eb) unlabeled examples from DX , makes at most k\u2217+log2(1/\u01eb) queries to Search, and at most O\u0303 ( ( k\u2217 + log(1/\u01eb) )\n\u00b7 (maxk\u2032\u2264k\u2217 \u03b8k\u2032 (\u01eb)) \u00b7 dk\u2217 \u00b7 log2(1/\u01eb)) queries to Label.\nUnion-of-intervals example. We now show an implication of Theorem 1 in the case where the target hypothesis h\u2217 is the union of non-trivial intervals in X := [0, 1], assuming that DX is uniform. For k \u2265 0, let Hk be the hypothesis class of the union of up to k intervals in [0, 1] with H0 containing only the alwaysnegative hypothesis. (Thus, h\u2217 is the union of k\u2217 non-empty intervals.) The disagreement coefficient of H1 is \u2126(1/\u01eb), and hence Label-only active learners like CAL are not very effective at learning with such classes. However, the first Search query by Larch provides a counterexample to H0, which must be a positive example (x1,+1). Hence, H1(S[1]) (where S[1] is defined in Theorem 1) is the class of intervals that contain x1 with disagreement coefficient \u03b81 \u2264 4.\nNow consider the inductive case. Just before Larch advances its index to a value k (for any k \u2264 k\u2217), Search returns a counterexample (x, h\u2217(x)) to the version space; every hypothesis in this version space (which could be empty) is a union of fewer than k intervals. If the version space is empty, then S must already contain positive examples from at least k different intervals in h\u2217 and at least k\u22121 negative examples separating them. If the version space is not empty, then the point x is either a positive example belonging to a previously uncovered interval in h\u2217 or a negative example splitting an existing interval. In either case, S[k] contains positive examples from at least k distinct intervals separated by at least k \u2212 1 negative examples. The disagreement coefficient of the set of unions of k intervals consistent with S[k] is at most 4k, independent of \u01eb.\nThe VC dimension of Hk is O(k), so Theorem 1 implies that with high probability, Larch makes at most k\u2217 + log(1/\u01eb) queries to Search and O\u0303((k\u2217)3 log(1/\u01eb) + (k\u2217)2 log3(1/\u01eb)) queries to Label."}, {"heading": "4.2 Proof of Theorem 1", "text": "The proof of Theorem 1 uses the following lemma regarding the CAL subroutine, proved in Appendix B. It is similar to a result of Hanneke [2011], but an important difference here is that the input version space V is not assumed to contain h\u2217.\nLemma 1. Assume Label(x) = h\u2217(x) for every x in the support of DX . For any hypothesis set V \u2286 YX with VC dimension d < \u221e, and any \u01eb, \u03b4 \u2208 (0, 1), the following holds with probability at least 1 \u2212 \u03b4. CAL(V,Label, \u01eb, \u03b4) returns labeled examples T \u2286 {(x, h\u2217(x)) : x \u2208 X} such that for any h in V (T ), Pr(x,y)\u223cD[h(x) 6= y \u2227 x \u2208 Dis(V (T ))] \u2264 \u01eb; furthermore, it draws at most O\u0303(d/\u01eb) unlabeled examples from DX , and makes at most O\u0303 (\u03b8V (\u01eb) \u00b7 d \u00b7 log2(1/\u01eb)) queries to Label.\nWe now prove Theorem 1. By Lemma 1 and a union bound, there is an event with probability at least 1\u2212\u2211i\u22651 \u03b4/(i2+i) \u2265 1\u2212\u03b4 such that each call to CAL made by Larch satisfies the high-probability guarantee from Lemma 1. We henceforth condition on this event.\nWe first establish the guarantee on the error rate of a hypothesis returned by Larch. By the assumed properties of Label and Search, and the properties of CAL from Lemma 1, the labeled examples S in Larch are always consistent with h\u2217. Moreover, the return property of CAL implies that at the end of any loop iteration, with the present values of S, k, and \u2113, we have Pr(x,y)\u223cD[h(x) 6= y \u2227 x \u2208 Dis(Hk(S))] \u2264 2\u2212\u2113 for all h \u2208 Hk(S). (The same holds trivially before the first loop iteration.) Therefore, if Larch halts and returns a hypothesis h \u2208 Hk(S), then there is no counterexample to Hk(S), and Pr(x,y)\u223cD[h(x) 6= y \u2227 x \u2208 Dis(Hk(S))] \u2264 \u01eb. These consequences and the law of total probability imply err(h) = Pr(x,y)\u223cD[h(x) 6= y \u2227 x \u2208 Dis(Hk(S))] \u2264 \u01eb.\nWe next consider the number of for-loop iterations executed by Larch. Let Si, ki, and \u2113i be, respectively, the values of S, k, and \u2113 at the start of the i-th for-loop iteration in Larch. We claim that if Larch does not halt in the i-th iteration, then one of k and \u2113 is incremented by at least one. Clearly, if there is no counterexample to Hki(Si) and 2\n\u2212\u2113i > \u01eb, then \u2113 is incremented by one (Step 8). If, instead, there is a counterexample (x, y), then Hki(Si \u222a {(x, y)}) = \u2205, and hence k is incremented to some index larger than ki (Step 12). This proves that ki+1 + \u2113i+1 \u2265 ki + \u2113i + 1. We also have ki \u2264 k\u2217, since h\u2217 \u2208 Hk\u2217 is consistent with S, and \u2113i \u2264 log2(1/\u01eb), as long as Larch does not halt in for-loop iteration i. So the total number of for-loop iterations is at most k\u2217 + log2(1/\u01eb). Together with Lemma 1, this bounds the number of unlabeled examples drawn from DX .\nFinally, we bound the number of queries to Search and Label. The number of queries to Search is the same as the number of for-loop iterations\u2014this is at most k\u2217+log2(1/\u01eb). By Lemma 1 and the fact that V (S\u2032 \u222a S\u2032\u2032) \u2286 V (S\u2032) for any hypothesis space V and sets of labeled examples S\u2032, S\u2032\u2032, the number of Label queries made by CAL in the i-th for-loop iteration is at most O\u0303(\u03b8ki(\u01eb) \u00b7 dki \u00b7 \u21132i \u00b7 polylog(i)). The claimed bound on the number of Label queries made by Larch now readily follows by taking a max over i, and using the facts that i \u2264 k\u2217 and dk\u2032 \u2264 dk\u2217 for all k\u2032 \u2264 k."}, {"heading": "4.3 An Improved Algorithm", "text": "Larch is somewhat conservative in its use of Search, interleaving just one Search query between sequences of Label queries (from CAL). Often, it is advantageous to advance to higher complexity hypothesis classes quickly, as long as there is justification to do so. Counterexamples from Search provide such justification, and a \u22a5 result from Search also provides useful feedback about the current version space: outside of its disagreement region, the version space is in complete agreement with h\u2217 (even if the version space does not contain h\u2217). Based on these observations, we propose an improved algorithm for the realizable setting, which we call Seabel. Due to space limitations, we present it in Appendix C. We prove the following performance guarantee for Seabel.\nTheorem 2. Assume that err(h\u2217) = 0. Let \u03b8k(\u00b7) denote the disagreement coefficient of V kii at the first iteration i in Seabel where ki \u2265 k. Fix any \u01eb, \u03b4 \u2208 (0, 1). If Seabel is run with inputs hypothesis classes {Hk}\u221ek=0, oracles Search and Label, and learning parameters \u01eb, \u03b4 \u2208 (0, 1), then with probability 1 \u2212 \u03b4:\nSeabel halts and returns a classifier with error rate at most \u01eb; furthermore, it draws at most O\u0303((dk\u2217 + log k\u2217)/\u01eb) unlabeled examples from DX , makes at most k \u2217 + O (log(dk\u2217/\u01eb) + log log k \u2217) queries to Search, and at most O\u0303 (maxk\u2264k\u2217 \u03b8k(2\u01eb) \u00b7 (dk\u2217 log2(1/\u01eb) + log k\u2217)) queries to Label.\nIt is not generally possible to directly compare Theorems 1 and 2 on account of the algorithm-dependent disagreement coefficient bounds. However, in cases where these disagreement coefficients are comparable (as in the union-of-intervals example), the Search complexity in Theorem 2 is slightly higher (by additive log terms), but the Label complexity is smaller than that from Theorem 1 by roughly a factor of k\u2217. For the union-of-intervals example, Seabel would learn target union of k\u2217 intervals with k\u2217 +O(log(k\u2217/\u01eb)) queries to Search and O\u0303((k\u2217)2 log2(1/\u01eb)) queries to Label."}, {"heading": "5 Non-Realizable Case", "text": "In this section, we consider the case where the optimal hypothesis h\u2217 may have non-zero error rate, i.e., the non-realizable (or agnostic) setting. In this case, the algorithm Larch, which was designed for the realizable setting, is no longer applicable. First, examples obtained by Label and Search are of different quality: those returned by Search always agree with h\u2217, whereas the labels given by Label need not agree with h\u2217. Moreover, the version spaces (even when k = k\u2217) as defined by Larch may always be empty due to the noisy labels.\nAnother complication arises in our SRM setting that differentiates it from the usual agnostic active learning setting. When working with a specific hypothesis class Hk in the nested sequence, we may observe high error rates because (i) the finite sample error is too high (but additional labeled examples could reduce it), or (ii) the current hypothesis class Hk is impoverished. In case (ii), the best hypothesis in Hk may have a much larger error rate than h\u2217, and hence lower bounds [Ka\u0308a\u0308ria\u0308inen, 2006] imply that active learning on Hk instead of Hk\u2217 may be substantially more difficult.\nThese difficulties in the SRM setting are circumvented by an algorithm that adaptively estimates the error of h\u2217. The algorithm, A-Larch (Algorithm 5), is presented in Appendix D.\nTheorem 3. Assume err(h\u2217) = \u03bd. Let \u03b8k(\u00b7) denote the disagreement coefficient of V kii at the first iteration i in A-Larch where ki \u2265 k. Fix any \u01eb, \u03b4 \u2208 (0, 1). If A-Larch is run with inputs hypothesis classes {Hk}\u221ek=0, oracles Search and Label, learning parameter \u03b4, and unlabeled example budget O\u0303((dk\u2217 +log k\n\u2217)(\u03bd+ \u01eb)/\u01eb2), then with probability 1 \u2212 \u03b4: A-Larch returns a classifier with error rate \u2264 \u03bd + \u01eb; it makes at most k\u2217 + O (log(dk\u2217/\u01eb) + log log k\n\u2217) queries to Search, and O\u0303 (maxk\u2264k\u2217 \u03b8k(2\u03bd + 2\u01eb) \u00b7 (dk\u2217 log2(1/\u01eb) + log k\u2217) \u00b7 (1 + \u03bd2/\u01eb2)) queries to Label.\nThe proof is in Appendix D. The Label query complexity is at least a factor of k\u2217 better than that in Hanneke [2011], and sometimes exponentially better thanks to the reduced disagreement coefficient of the version space when consistency constraints are incorporated."}, {"heading": "5.1 AA-Larch: an Opportunistic Anytime Algorithm", "text": "In many practical scenarios, termination conditions based on quantities like a target excess error rate \u01eb are undesirable. The target \u01eb is unknown, and we instead prefer an algorithm that performs as well as possible until a cost budget is exhausted. Fortunately, when the primary cost being considered are Label queries, there are many Label-only active learning algorithms that readily work in such an \u201canytime\u201d setting [see, e.g., Dasgupta et al., 2007, Hanneke, 2014].\nThe situation is more complicated when we consider both Search and Label: we can often make substantially more progress with Search queries than with Label queries (as the error rate of the best hypothesis in Hk\u2032 for k\n\u2032 > k can be far lower than in Hk). AA-Larch (Algorithm 2) shows that although these queries come at a higher cost, the cost can be amortized.\nAA-Larch relies on several subroutines: Sample-and-Label, Error-Check, Prune-Version-Space and Upgrade-Version-Space (Algorithms 6, 7, 8, and 9). The detailed descriptions are deferred to\nAlgorithm 2 AA-Larch\ninput: Nested hypothesis set H0 \u2286 H1 \u2286 \u00b7 \u00b7 \u00b7 ; oracles Label and Search; learning parameter \u03b4 \u2208 (0, 1); Search-to-Label cost ratio \u03c4 , dataset size upper bound N . output: hypothesis h\u0303. 1: Initialize: consistency constraints S \u2190 \u2205, counter c \u2190 0, k \u2190 0, verified labeled dataset L\u0303 \u2190 \u2205, working\nlabeled dataset L0 \u2190 \u2205, unlabeled examples processed i \u2190 0, Vi \u2190 Hk(S). 2: loop 3: Reset counter c \u2190 0. 4: repeat 5: if Error-Check(Vi, Li, \u03b4i) then 6: (k, S, Vi) \u2190 Upgrade-Version-Space(k, S, \u2205) 7: Vi \u2190 Prune-Version-Space(Vi, L\u0303, \u03b4i) 8: Li \u2190 L\u0303 9: continue loop\n10: end if 11: i \u2190 i+ 1 12: (Li, c) \u2190 Sample-and-Label(Vi\u22121,Label, Li\u22121, c) 13: Vi \u2190 Prune-Version-Space(Vi\u22121, Li, \u03b4i) 14: until c = \u03c4 or li = N 15: e \u2190 SearchHk(Vi) 16: if e 6= \u22a5 then 17: (k, S, Vi) \u2190 Upgrade-Version-Space(k, S, {e}) 18: Vi \u2190 Prune-Version-Space(Vi, L\u0303, \u03b4i) 19: Li \u2190 L\u0303 20: else 21: Update verified dataset L\u0303 \u2190 Li. 22: Store temporary solution h\u0303 = argminh\u2032\u2208Vi err(h\n\u2032, L\u0303). 23: end if 24: end loop\nAppendix E. Sample-and-Label performs standard disagreement-based selective sampling using oracle Label; labels of examples in the disagreement region are queried, otherwise inferred. Prune-Version-Space prunes the version space given the labeled examples collected, based on standard generalization error bounds. Error-Check checks if the best hypothesis in the version space has large error; Search is used to find a systematic mistake for the version space; if either event happens, AA-Larch calls Upgrade-Version-Space to increase k, the level of our working hypothesis class.\nTheorem 4. Assume err(h\u2217) = \u03bd. Let \u03b8k\u2032(\u00b7) denote the disagreement coefficient of Vi at the first iteration i after which k \u2265 k\u2032. Fix any \u01eb \u2208 (0, 1). Let n\u01eb = O\u0303(maxk\u2264k\u2217 \u03b8k(2\u03bd + 2\u01eb)dk\u2217(1 + \u03bd2/\u01eb2)) and define C\u01eb = 2(n\u01eb + k\n\u2217\u03c4). Run Algorithm 2 with a nested sequence of hypotheses {Hk}\u221ek=0, oracles Label and Search, confidence parameter \u03b4, cost ratio \u03c4 \u2265 1, and upper bound N = O\u0303(dk\u2217/\u01eb2). If the cost spent is at least C\u01eb, then with probability 1\u2212 \u03b4, the current hypothesis h\u0303 has error at most \u03bd + \u01eb.\nThe proof is in Appendix E. A comparison to Theorem 3 shows that AA-Larch is adaptive: for any cost complexity C, the excess error rate \u01eb is roughly at most twice that achieved by A-Larch."}, {"heading": "6 Discussion", "text": "The Search oracle captures a powerful form of interaction that is useful for machine learning. Our theoretical analyses of Larch and variants demonstrate that Search can substantially improve Label-based active learners, while being plausibly cheaper to implement than oracles like CCQ.\nAre there examples where CCQ is substantially more powerful than Search? This is a key question, because a good active learning system should use minimally powerful oracles. Another key question is: Can the benefits of Search be provided in a computationally efficient general purpose manner?"}, {"heading": "A Basic Facts and Notations Used in Proofs", "text": ""}, {"heading": "A.1 Concentration Inequalities", "text": "Lemma 2 (Bernstein\u2019s Inequality). Let X1, . . . , Xn be independent zero-mean random variables. Suppose that |Xi| \u2264 M almost surely. Then for all positive t,\nPr\n\n\nn \u2211\ni=1\nXi > t\n  \u2264 exp ( \u2212 t 2/2\n\u2211n j=1 E[X 2 j ] +Mt/3\n)\n.\nLemma 3. Let Z1, . . . , Zn be independent Bernoulli random variables with mean p. Let Z\u0304 = 1 n\n\u2211n\ni=1 Zi."}, {"heading": "Then with probability 1\u2212 \u03b4,", "text": "Z\u0304 \u2264 p+ \u221a 2p ln(1/\u03b4)\nn +\n2 ln(1/\u03b4)\n3n .\nProof. Let Xi = Zi \u2212 p for all i, note that |Xi| \u2264 1. The lemma follows from Bernstein\u2019s Inequality and algebra.\nLemma 4 (Freedman\u2019s Inequality). Let X1, . . . , Xn be a martingale difference sequence, and |Xi| \u2264 M almost surely. Let V be the sum of the conditional variances, i.e.\nV = n \u2211\ni=1\nE[X2i |X1, . . . .Xi\u22121]"}, {"heading": "Then, for every t, v > 0,", "text": "Pr\n\n\nn \u2211\ni=1\nXi > t and V \u2264 v\n  \u2264 exp ( \u2212 t 2/2\nv +Mt/3\n)\n.\nLemma 5. Let Z1, . . . , Zn be a sequence of Bernoulli random variables, where E[Zi|Z1, . . . , Zi\u22121] = pi. Then, for every \u03b4 > 0, with probability 1\u2212 \u03b4:\nn \u2211\ni=1\nZi \u2264 2vn + \u221a 4vn ln log 4n\n\u03b4 +\n2 3 ln log 4n \u03b4 .\nwhere vn = max( \u2211n i=1 pi, 1). Proof. Let Xi = Zi \u2212 pi for all i, note that {Xi} is a martingale difference sequence and |Xi| \u2264 1. From Freedman\u2019s Inequality and algebra, for any v,\nPr\n\n \n1\nn\nn \u2211\ni=1\nZi > v +\n\u221a\n2v ln log 4n \u03b4\nn +\n2 ln log 4n \u03b4\n3n and\nn \u2211\ni=1\npi \u2264 v\n\n  \u2264 \u03b4\nlogn+ 2 .\nThe proof follows by taking union bound over v = 2i, i = 0, 1, . . . , \u2308logn\u2309. Define\n\u03c6(d,m, \u03b4) := 1\nm\n(\nd log em2 + log 2\n\u03b4\n)\n. (1)\nTheorem 5 (Vapnik and Chervonenkis, 1971). Let F be a family of functions f : Z \u2192 {0, 1} on a domain Z with VC dimension at most d, and let P be a distribution on Z. Let Pn denote the empirical measure from an iid sample of size n from P . For any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, for all f \u2208 F ,\n\u2212min { \u03b5+ \u221a Pf\u03b5, \u221a Pnf\u03b5 } \u2264 Pf \u2212 Pnf \u2264 min { \u03b5+ \u221a Pnf\u03b5, \u221a Pf\u03b5 }\nwhere \u03b5 := \u03c6(d, n, \u03b4)."}, {"heading": "A.2 Notations", "text": "For convenience, we define \u03c3(d,m, \u03b4) := \u03c6(d,m, \u03b4/3) , (2)\nas we will often split the overall allowed failure probability \u03b4 across two or three separate events. Because we apply the deviation inequalities to the hypothesis classes from {Hk}\u221ek=0, we also define:\n\u03c3k(m, \u03b4) := \u03c3(dk,m, \u03b4), (3)\nwhere dk is the VC dimension of Hk. We have the following simple fact.\nFact 1.\n\u03c3\n(\nd,m, \u03b4\n2 logm(logm+ 1)\n)\n\u2265 \u01eb =\u21d2 m \u2264 64 \u01eb\n(\nd log 512\n\u01eb + log\n24\n\u03b4\n)\n.\nFor integers i \u2265 1 and k \u2265 0, define\n\u03b4i := \u03b4\ni(i+ 1) , \u03b4i,k := \u03b4i (k + 1)(k + 2)\nNote that \u2211\u221e i=1 \u03b4i = \u03b4 and \u2211\u221e k=0 \u03b4i,k = \u03b4i.\nFinally, for any distribution D\u0303 overX\u00d7Y and any hypothesis h : X \u2192 Y, we use err(h, D\u0303) := Pr(x,y)\u223cD\u0303[h(x) 6= y] to denote the probability with respect to D\u0303 that h makes a classification error."}, {"heading": "B Active Learning Algorithm CAL", "text": "In this section, we describe and analyze a variant of the Label-only active learning algorithm of Cohn et al. [1994], which we refer to as CAL. Note that Hanneke [2011] provides a label complexity analysis of CAL in terms of the disagreement coefficient under the assumption that the Label oracle is consistent with some hypothesis in the hypothesis class used by CAL. We cannot use that analysis because we call CAL as a subroutine in Larch with sets of hypotheses V that do not necessarily contain the optimal hypothesis h\u2217."}, {"heading": "B.1 Description of CAL", "text": "CAL takes as input a set of hypotheses V , the Label oracle (which always returns h\u2217(x) when queried with a point x), and learning parameters \u01eb, \u03b4 \u2208 (0, 1).\nThe pseudocode for CAL is given in Algorithm 3 below, where we use the notation\nU\u2264i :=\ni \u22c3\nj=1\nUj\nfor any sequence of sets (Uj)j\u2208N."}, {"heading": "B.2 Proof of Lemma 1", "text": "We now give the proof of Lemma 1. Let V0 := V and Vi := V (T\u2264i) for each i \u2265 1. Clearly V0 \u2287 V1 \u2287 \u00b7 \u00b7 \u00b7 , and hence Dis(V0) \u2287 Dis(V1) \u2287 \u00b7 \u00b7 \u00b7 as well. Let Ei be the event in which the following hold:\n1. If CAL executes iteration i, then every h \u2208 Vi satisfies\nPr x\u223cDX\n[h(x) 6= h\u2217(x) \u2227 x \u2208 Dis(Vi)] \u2264 \u03c6(d, 2i, \u03b4i/2) .\nAlgorithm 3 CAL\ninput: Hypothesis set V with VC dimension \u2264d; oracle Label; learning parameters \u01eb, \u03b4 \u2208 (0, 1) output: Labeled examples T 1: for i = 1, 2, . . . do 2: Ti \u2190 \u2205 3: for j = 1, 2, . . . , 2i do 4: xi,j \u2190 independent draw from DX (the corresponding label is hidden) 5: if xi,j \u2208 Dis(V (T\u2264i\u22121)) then 6: Ti \u2190 Ti \u222a {(xi,j ,Label(xi,j))} 7: end if\n8: end for\n9: if \u03c6(d, 2i, \u03b4i/2) \u2264 \u01eb or V (T\u2264i) = \u2205 then 10: return T\u2264i 11: end if\n12: end for\n2. If CAL executes iteration i, then the number of Label queries in iteration i is at most\n2i\u00b5i +O ( \u221a 2i\u00b5i log(2/\u03b4i) + log(2/\u03b4i) ) ,\nwhere \u00b5i := \u03b8Vi\u22121(\u01eb) \u00b7 2\u03c6(d, 2i\u22121, \u03b4i\u22121/2) .\nWe claim that E0 \u2229 E1 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Ei holds with probability at least 1 \u2212 \u2211i i\u2032=1 \u03b4i\u2032 \u2265 1 \u2212 \u03b4. The proof is by induction. The base case is trivial, as E0 holds deterministically. For the inductive case, we just have to show that Pr(Ei | E0 \u2229 E1 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Ei\u22121) \u2265 1\u2212 \u03b4i.\nCondition on the event E0 \u2229 E1 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Ei\u22121. Suppose CAL executes iteration i. For all x /\u2208 Dis(Vi\u22121), let Vi\u22121(x) denote the label assigned by every h \u2208 Vi\u22121 to x. Define\nS\u0302i := { (xi,j , y\u0302i,j) : j \u2208 {1, 2, . . . , 2i} , xi,j /\u2208 Dis(Vi\u22121), y\u0302i,j = Vi\u22121(xi,j) } .\nObserve that S\u0302i\u222aTi is an iid sample of size 2i from a distribution (which we call Di\u22121) over labeled examples (x, y), where x \u223c DX and y is given by\ny :=\n{\nVi\u22121(x) if x /\u2208 Dis(Vi\u22121) , h\u2217(x) if x \u2208 Dis(Vi\u22121) .\nIn fact, for any h \u2208 Vi\u22121, we have\nerrDi\u22121(h) = Pr (x,y)\u223cDi\u22121 [h(x) 6= y] = Pr x\u223cDX [h(x) 6= h\u2217(x) \u2227 x \u2208 Dis(Vi\u22121)] . (4)\nThe VC inequality (Theorem 5) implies that, with probability at least 1\u2212 \u03b4i/2,\n\u2200h \u2208 V ( err(h, S\u0302i \u222a Ti) = 0 =\u21d2 errDi\u22121(h) \u2264 \u03c6(d, 2i, \u03b4i/2) ) . (5)\nConsider any h \u2208 Vi. We have err(h, Ti) = 0 by definition of Vi. We also have err(h, S\u0302i) = 0 since h \u2208 Vi \u2286 Vi\u22121. So in the event that (5) holds, we have\nPr x\u223cDX [h(x) 6= h\u2217(x) \u2227 x \u2208 Dis(Vi)] \u2264 Pr x\u223cDX [h(x) 6= h\u2217(x) \u2227 x \u2208 Dis(Vi\u22121)]\n= errDi\u22121(h) \u2264 \u03c6(d, 2i, \u03b4i/2) ,\nwhere the first inequality follows because Dis(Vi) \u2286 Dis(Vi\u22121), and the equality follows from (4). Now we prove the Label query bound.\nClaim 1. On event Ei\u22121 for every h, h \u2032 \u2208 Vi\u22121,\nPr x\u223cDX\n[h(x) 6= h\u2032(x)] \u2264 2\u03c6(d, 2i\u22121, \u03b4i\u22121/2)\nProof. On event Ei\u22121, every h \u2208 Vi\u22121 satisfies\nPr x\u223cDX\n[h(x) 6= h\u2217(x), x \u2208 Dis(Vi\u22121)] \u2264 \u03c6(d, 2i\u22121, \u03b4i\u22121/2) .\nTherefore, for any h, h\u2032 \u2208 Vi\u22121, we have\nPr x\u223cDX [h(x) 6= h\u2032(x)] = Pr x\u223cDX [h(x) 6= h\u2032(x), x \u2208 Dis(Vi\u22121)]\n\u2264 Pr x\u223cDX [h(x) 6= h\u2217(x), x \u2208 Dis(Vi\u22121)]\n+ Pr x\u223cDX\n[h\u2032(x) 6= h\u2217(x), x \u2208 Dis(Vi\u22121)]\n\u2264 2\u03c6(d, 2i\u22121, \u03b4i\u22121/2) .\nSince CAL does not halt before iteration i, we have 2\u03c6(d, 2i\u22121, \u03b4i\u22121/2) \u2265 \u01eb, and hence the above claim and the definition of the disagreement coefficient imply\nPr x\u223cDX\n[x \u2208 Dis(Vi\u22121)] \u2264 \u03b8Vi\u22121(\u01eb) \u00b7 2\u03c6(d, 2i\u22121, \u03b4i\u22121/2) = \u00b5i .\nTherefore, \u00b5i is an upper bound on the probability that Label is queried on xi,j , for each j = 1, 2, . . . , 2 i. By Lemma 3, the number of queries to Label is at most\n2i\u00b5i + O ( \u221a 2i\u00b5i log(2/\u03b4i) + log(2/\u03b4i) ) .\nwith probability at least 1\u2212 \u03b4i/2. We conclude by a union bound that Pr(Ei | E0 \u2229E1 \u2229 \u00b7 \u00b7 \u00b7 \u2229Ei\u22121) \u2265 1\u2212 \u03b4i as required.\nWe now show that in the event E0 \u2229 E1 \u2229 \u00b7 \u00b7 \u00b7 , which holds with probability at least 1 \u2212 \u03b4, the required consequences from Lemma 1 are satisfied. The definition of \u03c6 from (1) and the halting condition in CAL imply that the number of iterations I executed by CAL satisfies\n\u03c3(d, 2I\u22121, \u03b4I\u22121/2) \u2265 \u01eb .\nThus by Fact 1,\n2I \u2264 O ( 1\n\u01eb\n(\nd log 1\n\u01eb + log\n1\n\u03b4\n)\n)\n,\nwhich immediately gives the required bound on the number of unlabeled points drawn from DX . Moreover, I can be bounded as\nI = O ( log(d/\u01eb) + log log(1/\u03b4) ) .\nTherefore, in the event E0 \u2229 E1 \u2229 \u00b7 \u00b7 \u00b7 \u2229 EI , CAL returns a set of labeled examples T := T\u2264I in which every h \u2208 V (T ) satisfies\nPr x\u223cDX\n[h(x) 6= h\u2217(x) \u2227 x \u2208 Dis(V (T ))] \u2264 \u01eb ,\nand the number of Label queries is bounded by\nI \u2211\ni=1\n(\n2i\u00b5i +O ( \u221a 2i\u00b5i log(2/\u03b4i) + log(2/\u03b4i) )\n)\n=\nI \u2211\ni=1\nO\n 2i \u00b7 ( \u03b8Vi\u22121(\u01eb) d log 2i + log(2/\u03b4i)\n2i\n)\n+ log(2/\u03b4i)\n\n\n=\nI \u2211\ni=1\nO (\n\u03b8Vi\u22121(\u01eb) \u00b7 ( d \u00b7 i+ log(1/\u03b4) )\n)\n= O\n(\n\u03b8V (\u01eb) \u00b7 ( d \u00b7 ( log(d/\u01eb) + log log(1/\u03b4) )2 + ( log(d/\u01eb) + log log(1/\u03b4) ) \u00b7 log(1/\u03b4) )\n)\n= O\u0303 ( \u03b8V (\u01eb) \u00b7 d \u00b7 log2(1/\u01eb) )\nas claimed."}, {"heading": "C An Improved Algorithm for the Realizable Case", "text": "In this section, we present an improved algorithm for using Search and Label in the realizable section. We call this algorithm Seabel (Algorithm 4)."}, {"heading": "C.1 Description of Seabel", "text": "Seabel proceeds in iterations like Larch, but takes more advantage of Search. Each iteration is split into two stages: the verification stage, and the sampling stage.\nIn the verification stage (Steps 4\u201313), Seabel makes repeated calls to Search to advance to as high of a complexity class as possible, until \u22a5 is returned. When \u22a5 is returned, it guarantees that whenever the latest version space completely agrees on an unlabeled point, then it is also in agreement with h\u2217, even if it does not contain h\u2217.\nIn the sampling stage (Steps 14\u201319), Seabel performs selective sampling, querying and infering labels based on disagreement over the new version space V kii . The preceding verification stage ensures that whenever a label is inferred, it is guaranteed to be in agreement with h\u2217.\nThe algorithm calls Algorithm 6 in Appendix E, where we slightly abuse the notation in Sample-and-Label that if the counter parameter is missing then it simply does not get updated."}, {"heading": "C.2 Proof of Theorem 2", "text": "Observe that Ti+1 is an iid sample of size 2 i+1 from a distribution (which we call Di) over labeled examples (x, y), where x \u223c DX , and\ny :=\n{\nV kii (x) if x /\u2208 Dis(V kii ) , h\u2217(x) if x \u2208 Dis(V kii ) ,\nfor every x in the support of DX . (T1 is an iid sample from D0 := D; also note k0 = 0 and S0 = \u2205.)\nLemma 6. Algorithm 4 maintains the following invariants:\n1. The loop in the verification stage of iteration i terminates for all i \u2265 1.\n2. ki \u2264 k\u2217 for all i \u2265 0.\n3. h\u2217(x) = V kii (x) for all x /\u2208 Dis(V kii ) for all i \u2265 1.\nAlgorithm 4 Seabel\ninput: Nested hypothesis classes H0 \u2286 H1 \u2286 \u00b7 \u00b7 \u00b7 ; oracles Search and Label; learning parameters \u01eb, \u03b4 \u2208 (0, 1)\n1: initialize S0 \u2190 \u2205, k0 \u2190 0. 2: Draw x1,1, x1,2 at random from DX , T1 \u2190 { (x1,1,Label(x1,1)), (x1,2,Label(x1,2)) } 3: for iteration i = 1, 2, . . . do 4: S \u2190 Si\u22121, k \u2190 min { k\u2032 \u2265 ki\u22121 : Hk\u2032(Si\u22121 \u222a Ti) 6= \u2205 }\n# Verification stage (Steps 4\u201313) 5: loop 6: e \u2190 SearchHk(Hk(S \u222a Ti)) 7: if e 6= \u22a5 then 8: S \u2190 S \u222a {e} 9: k \u2190 min { k\u2032 > k : Hk\u2032(S \u222a Ti) 6= \u2205 }\n10: else 11: break\n12: end if\n13: end loop 14: Si \u2190 S, ki \u2190 k # Sampling stage (Steps 14\u201319) 15: Define new version space V kii = Hki(Si \u222a Ti) 16: Ti+1 \u2190 \u2205 17: for j = 1, 2, . . . , 2i+1 do 18: Ti+1 \u2190 Sample-and-Label(V kii ,Label, Ti+1) 19: end for 20: if \u03c3ki(2 i, \u03b4i,ki) \u2264 \u01eb then 21: return any h\u0302 \u2208 V kii 22: end if\n23: end for\n4. h\u2217 is consistent with Si \u222a Ti+1 for all i \u2265 0.\nProof. It is easy to see that S only contains examples provided by Search, and hence the labels are consistent with h\u2217.\nNow we prove that the invariants hold by induction on i, starting with i = 0. For the base case, only the last invariant needs checking, and it is true because the labels in T1 are obtained from Label.\nFor the inductive step, fix any i \u2265 1, and assume that ki\u22121 \u2264 k\u2217, and that h\u2217 is consistent with Ti. Now consider the verification stage in iteration i. We first prove that the loop in the verification stage will terminate and establish some properties upon termination. Observe that k and S are initially ki\u22121 and Si\u22121, respectively. Throughout the loop, the examples added to S are obtained from Search, and hence are consistent with h\u2217. Thus, h\u2217 \u2208 Hk\u2217(S \u222a Ti), implying Hk\u2217(S \u222a Ti) 6= \u2205. If k = k\u2217, then SearchHk\u2217 (Hk\u2217(S\u222aTi)) would return\u22a5 and Algorithm 4 would exit the loop. If SearchHk(Hk(S\u222aTi)) 6= \u22a5, then k < k\u2217, and k cannot be increased beyond k\u2217 since Hk\u2217(S\u222aTi) 6= \u2205. Thus, the loop must terminate with k \u2264 k\u2217, implying ki \u2264 k\u2217. This establishes invariants 1 and 2. Moreover, because the loop terminates with SearchHk(Hk(S \u222a Ti)) returning \u22a5 (and here, k = ki and Hk(S \u222a Ti) = V kii ), there is no counterexample x \u2208 X such that h\u2217 disagrees with every h \u2208 V kii . This implies that h\u2217(x) = V kii (x) for all x /\u2208 Dis(V kii ), i.e., invariant 3.\nNow consider any (x, y) added to Ti+1 in the sampling stage. If x \u2208 Dis(V kii ), the label is obtained from Label, and hence is consistent with h\u2217; if x /\u2208 Dis(V kii ), the label is V kii (x), which is the same as h\u2217(x) as previously argued. So h\u2217 is consistent with all examples in Ti+1, and hence also all examples in Si \u222a Ti+1, proving invariant 4. This completes the induction.\nLet Ei be the event in which the following hold:\n1. For every k \u2265 0, every h \u2208 Hk satisfies\nerr(h,Di\u22121) \u2264 err(h, Ti) + \u221a err(h, Ti)\u03c3k(2i, \u03b4i,k) + \u03c3k(2 i, \u03b4i,k) .\n2. The number of Label queries in iteration i (to form Ti+1) is at most\n2i+1 Pr x\u223cDX\n[x \u2208 Dis(V kii )] +O ( \u221a\n2i+1 Pr x\u223cDX\n[x \u2208 Dis(V kii )] log(1/\u03b4i) + log(1/\u03b4i) ) ,\nUsing Theorem 5 and Lemma 3, along with the union bound, Pr(Ei) \u2265 1\u2212 \u03b4i. Define E := \u2229\u221ei=1Ei; a union bound implies that Pr(E) \u2265 1\u2212 \u03b4.\nWe now prove Theorem 2, starting with the error rate guarantee. Condition on the event E. Since ki \u2264 k\u2217 (Lemma 6), the definition of \u03c3k from (3), the halting condition in Algorithm 4, and Fact 1 imply that the algorithm must halt after at most I iterations, where\n2I \u2264 O ( 1\n\u01eb\n(\ndk\u2217 log 1\n\u01eb + log\nk\u2217\n\u03b4\n)\n)\n. (6)\nSo let I denote the iteration in which Algorithm 4 halts. By definition of EI , we have\nerr(h\u0302, Di\u22121) \u2264 err(h\u0302, Ti) + \u221a err(h\u0302, Ti)\u03c3ki (2 i, \u03b4i,ki) + \u03c3ki(2 i, \u03b4i,ki)\n= \u03c3ki(2 i, \u03b4i,ki) \u2264 \u01eb ,\nwhere the second inequality follows from the termination condition. By Lemma 6, h\u2217(x) = V ki\u22121 i\u22121 (x) for all x /\u2208 Dis(V ki\u22121i\u22121 ). Therefore, D(\u00b7 | x) = Di\u22121(\u00b7 | x) for every x in the support of DX , and\nerr(h\u0302, D) = err(h\u0302, Di\u22121) \u2264 \u01eb .\nNow we bound the unlabeled, Label, and Search complexities, all conditioned on event E. First, as argued above, the algorithm halts after at most I iterations, where 2I is bounded as in (6). The number of unlabeled examples drawn from DX across all iterations is within a factor of two of the number of examples drawn in the final sampling stage, which is O(2I). Thus (6) also gives the bound on the number of unlabeled examples drawn.\nNext, we consider the Search complexity. For each iteration i, each call to Search either returns a counterexample that forces k to increment (but never past k\u2217, as implied by Lemma 6), or returns \u22a5 which causes an exit from the verification stage loop. Therefore, the total number of Search calls is at most\nk\u2217 + I = k\u2217 +O\n(\nlog dk\u2217\n\u01eb + log log\nk\u2217\n\u03b4\n)\n.\nFinally, we consider the Label complexity. For i \u2264 I, we first show that the version space V kii is always contained in a ball of small radius (with respect to the disagreement pseudometric). Specifically, for every h, h\u2032 in V kii , err(h, Ti) = 0 and err(h, Ti) = 0. By definition of Ei, this implies that\nerr(h,Di\u22121) \u2264 \u03c3ki (2i, \u03b4i,ki) and err(h\u2032, Di\u22121) \u2264 \u03c3ki(2i, \u03b4i,ki).\nTherefore, by the triangle inequality and the fact ki \u2264 k\u2217,\nPr x\u223cD\n[h(x) 6= h\u2032(x)] \u2264 2\u03c3ki(2i, \u03b4i,ki) \u2264 2\u03c3k\u2217(2i, \u03b4i,k\u2217).\nAlso, the upper bound 2I \u2264 O\u0303(dk\u2217/\u01eb) from (6) implies the lower bound \u03c3k\u2217(2i, \u03b4i,k\u2217) \u2265 \u01eb/2 for i \u2264 I. Thus, the probability mass of the disagreement region can be bounded as\nPr x\u223cDX\n[x \u2208 Dis(V kii )] \u2264 \u03b8ki(\u01eb) \u00b7 2\u03c3k\u2217(2i, \u03b4i,k\u2217).\nBy definition of Ei, the number of queries to Label at iteration i is at most\n2i+1 Pr x\u223cDX\n[x \u2208 Dis(V kii )] +O ( \u221a\n2i+1 Pr x\u223cDX\n[x \u2208 Dis(V kii )] log(1/\u03b4i,k) + log(1/\u03b4i) ) ,\nwhich is at most O ( 2i \u00b7 \u03b8ki(\u01eb) \u00b7 \u03c3k\u2217(2i, \u03b4i,k\u2217) ) .\nWe conclude that the total number of Label queries by Algorithm 4 is bounded by\n2 + I \u2211\ni=1\nO ( 2i \u00b7 \u03b8ki(\u01eb) \u00b7 \u03c3k\u2217(2i, \u03b4i,k\u2217) )\n= 2 +\nI \u2211\ni=1\nO\n(\n2i \u00b7 max k\u2264k\u2217\n\u03b8k(\u01eb) \u00b7 \u03c3k\u2217(2i, \u03b4i,k\u2217) )\n= O\n\n  max k\u2264k\u2217 \u03b8k(\u01eb) \u00b7\n\n\nI \u2211\ni=1\n2i \u00b7 d ln(2 i) + ln( (i\n2+i)(k\u2217)2\n\u03b4 )\n2i\n\n\n\n \n= O\n(\nmax k\u2264k\u2217\n\u03b8k(\u01eb) \u00b7 ( dk\u2217I 2 + I log k\u2217\n\u03b4\n)\n)\n= O\n\nmax k\u2264k\u2217\n\u03b8k(\u01eb) \u00b7 ( dk\u2217 ( log dk\u2217\n\u01eb + log log\nk\u2217\n\u03b4\n)2\n+\n(\nlog dk\u2217\n\u01eb + log log\nk\u2217\n\u03b4\n)\nlog k\u2217\n\u03b4\n)\n\n\n= O\u0303\n(\nmax k\u2264k\u2217\n\u03b8k(\u01eb) \u00b7 ( dk\u2217 \u00b7 log2 1\n\u01eb + log k\u2217\n)\n)\nas claimed."}, {"heading": "D A-Larch: An Adaptive Agnostic Algorithm", "text": "In this section, we present a generalization of Seabel that works in the agnostic setting. We call this algorithm A-Larch (Algorithm 5)."}, {"heading": "D.1 Description of A-Larch", "text": "A-Larch proceeds in iterations like Seabel. Each iteration is split into three stages: the error estimation stage, the verification stage, and the sampling stage.\nIn the error estimation stage, A-Larch uses a structural risk minimization approach (Step 4) to compute \u03b3i\u22121, a (tight) upper bound on Pr[h\n\u2217(x) 6= y, x \u2208 Dis(Vi\u22121)]. (See item 1 of Lemma 7 for justification.) The verification stage (Steps 5\u201318) and sampling stage (Steps 20\u201323) in A-Larch are similar to the corresponding stages in Seabel. Same as Seabel, the algorithm calls Algorithm 6, 8 and 9 in Appendix E (Sample-and-Label, Prune-Version-Space andUpgrade-Version-Space, respectively), where we slightly abuse the notation in Sample-and-Label that if the counter parameter is missing then it simply does not get updated.\nAlgorithm 5 A-Larch\ninput: Nested hypothesis set H0 \u2286 H1 \u2286 \u00b7 \u00b7 \u00b7 ; oracles Label and Search; learning parameter \u03b4 \u2208 (0, 1); unlabeled examples budget m = 2I+2. output: hypothesis h\u0302. 1: initialize S \u2190 \u2205, k0 \u2190 0. 2: Draw x1,1, x1,2 at random from DX , T1 \u2190 { (x1,1,Label(x1,1)), (x1,2,Label(x1,2)) }\n3: for i = 1, 2, . . . , I do 4: \u03b3i\u22121 \u2190 mink\u2032\u2265ki\u22121,h\u2208Hk\u2032 { err(h, Ti) + \u221a err(h, Ti)\u03c3k\u2032 (2i, \u03b4i,k\u2032 ) + \u03c3k\u2032 (2 i, \u03b4i,k\u2032) }\n# Error estimation\nstage (Step 4) 5: S \u2190 Si\u22121, k \u2190 ki\u22121. # Verification stage (Steps 5\u201318) 6: V ki \u2190 Prune-Version-Space(Hk(S), Ti, \u03b4i) 7: loop 8: if minh\u2208Hk(S) err(h, Ti) > \u03b3i\u22121 + \u221a \u03b3i\u22121\u03c3k(2i, \u03b4i,k) + \u03c3k(2 i, \u03b4i,k) then\n9: (k, S, V ki ) \u2190 Upgrade-Version-Space(k, S, \u2205) 10: else 11: e \u2190 SearchHk(V ki ) 12: if e 6= \u22a5 then 13: (k, S, V ki ) \u2190 Upgrade-Version-Space(k, S, {e}) 14: else\n15: break 16: end if\n17: end if 18: end loop 19: Si \u2190 S, ki \u2190 k 20: Ti+1 \u2190 \u2205 # Sampling stage (Steps 20\u201323) 21: for j = 1, 2, . . . , 2i+1 do 22: Ti+1 \u2190 Sample-and-Label(V kii ,Label, Ti+1) 23: end for 24: end for 25: return any h\u0302 \u2208 V kII ."}, {"heading": "D.2 Proof of Theorem 3", "text": "Let\nM(\u03bd, k\u2217, \u01eb, \u03b4) := min\n{\n2n : n \u2208 N, 6 \u221a \u03bd\u03c3k\u2217(2n, \u03b4n,k\u2217) + 21\u03c3k\u2217(2 n, \u03b4n,k\u2217) \u2264 \u01eb\n}\n= O\n(\n(dk\u2217 log(1/\u01eb) + log(k \u2217/\u03b4))(\u03bd + \u01eb)\n\u01eb2\n)\n.\nwhere the second line is from Fact 1.\nTheorem 6 (Restatement of Theorem 3). Assume err(h\u2217) = \u03bd. If Algorithm 5 is run with inputs hypothesis classes {Hk}\u221ek=0, oracles Search and Label, learning parameter \u03b4, unlabeled examples budget m = M(\u03bd, k\u2217, \u01eb, \u03b4) and the disagreement coefficient of Hk(S) is at most \u03b8k(\u00b7), then, with probability 1\u2212 \u03b4: (1) The returned hypothesis h\u0302 satisfies err(h\u0302) \u2264 \u03bd + \u01eb . (2) The total number of queries to oracle Search is at most\nk\u2217 + logm \u2264 k\u2217 +O ( log dk\u2217\n\u01eb + log log\nk\u2217\n\u03b4\n)\n.\n(3) The total number of queries to oracle Label is at most\nO\u0303\n\nmax k\u2264k\u2217\n\u03b8k(2\u03bd + 2\u01eb) \u00b7 dk\u2217 ( log 1\n\u01eb\n)2 \u00b7 ( 1 + \u03bd2\n\u01eb2\n)\n\n .\nThe proof relies on an auxillary lemma. First, we need to introduce the following notation. Observe that Ti+1 is an iid sample of size 2\ni+1 from a distribution (which we call Di) over labeled examples (x, y), where x \u223c DX and the conditional distribution is\nDi(y | x) := { 1{y = V kii (x)} if x /\u2208 Dis(V kii ) , D(y | x) if x \u2208 Dis(V kii ) .\nT1 is a sample of size 2 from D0 := D. Let Ei be the event in which the following hold:\n1. For every k \u2265 0, every h \u2208 Hk satisfies\nerr(h,Di\u22121) \u2264 err(h, Ti) + \u221a err(h, Ti)\u03c3k(2i, \u03b4i,k) + \u03c3k(2 i, \u03b4i,k) ,\nerr(h, Ti) \u2264 err(h,Di\u22121) + \u221a err(h,Di\u22121)\u03c3k(2i, \u03b4i,k) + \u03c3k(2 i, \u03b4i,k) .\n2. The number of Label queries at iteration i is at most\n2i+1 Pr x\u223cDX\n[x \u2208 Dis(V kii )] +O ( \u221a\n2i+1 Pr x\u223cDX\n[x \u2208 Dis(V kii )] log(1/\u03b4i) + log(1/\u03b4i) ) .\nUsing Theorem 5 and Lemma 3, along with the union bound, Pr(Ei) \u2265 1 \u2212 \u03b4i. Define E := \u2229\u221ei=1Ei, by union bound, Pr(E) \u2265 1\u2212 \u03b4. Recall that ki is the value of k at the end of iteration i.\nLemma 7. On event E, Algorithm 5 maintains the following invariants:\n1. For all i \u2265 1, \u03b3i\u22121 is such that\nerr(h\u2217, Di\u22121) \u2264 \u03b3i\u22121 \u2264 err(h\u2217, Di\u22121) + 2 \u221a err(h\u2217, Di\u22121)\u03c3k\u2217(2i, \u03b4i,k\u2217) + 3\u03c3k\u2217(2 i, \u03b4i,k\u2217).\n2. The loop in the verification stage of iteration i terminates for all i \u2265 1.\n3. ki \u2264 k\u2217 for all i \u2265 0.\n4. h\u2217(x) = V kii (x) for all x /\u2208 Dis(V kii ) for all i \u2265 1.\n5. For all i \u2265 0, for every hypothesis h, err(h,Di)\u2212 err(h\u2217, Di) \u2265 err(h,D)\u2212 err(h\u2217, D). Therefore, h\u2217 is the optimal hypothesis among \u222akHk with respect to Di.\nProof. Throughout, we assume the event E holds. It is easy to see that S only contains examples provided by Search, and hence the labels are consistent with h\u2217. Now we prove that the invariants hold by induction on i, starting with i = 0. For the base case, invariant 3 holds since k0 = 0 \u2264 k\u2217, and invariant 5 holds since D0 = D and h\u2217 is the optimal hypothesis in \u222akHk. Now consider the inductive step. We first prove that invariant 1 holds.\n(1) By definition of Ei, for all k \u2032 \u2265 ki\u22121, we have for all h \u2208 Hk\u2032 ,\nerr(h,Di\u22121) \u2264 err(h, Ti) + \u221a err(h, Ti)\u03c3k\u2032 (2i, \u03b4i,k\u2032 ) + \u03c3k\u2032 (2 i, \u03b4i,k\u2032) .\nThus,\nmin h\u2208H\nk\u2032\nerr(h,Di\u22121) \u2264 min h\u2208H\nk\u2032\nerr(h, Ti) + \u221a err(h, Ti)\u03c3k\u2032 (2i, \u03b4i,k\u2032) + \u03c3k\u2032 (2 i, \u03b4i,k\u2032) .\nTaking minimum over k\u2032 \u2265 ki\u22121 on both sides, notice that h\u2217 is the optimal hypothesis with respect to Di\u22121 and recall the definition of \u03b3i\u22121, we get\nerr(h\u2217, Di\u22121) \u2264 \u03b3i\u22121 .\n(2) By definition of \u03b3i\u22121, we have\n\u03b3i\u22121 = min k\u2032\u2265ki\u22121,h\u2208Hk\u2032\n{\nerr(h, Ti) + \u221a err(h, Ti)\u03c3k\u2032 (2i, \u03b4i,k\u2032 ) + \u03c3k\u2032 (2 i, \u03b4i,k\u2032)\n}\nTaking k\u2032 = k\u2217, h = h\u2217, we get\n\u03b3i\u22121 \u2264 err(h\u2217, Ti) + \u221a err(h\u2217, Ti)\u03c3k\u2217 (2i, \u03b4i,k\u2217) + \u03c3k\u2217(2 i, \u03b4i,k\u2217)\nIn conjunction with the fact that by definition of Ei,\nerr(h\u2217, Ti) \u2264 err(h\u2217, Di\u22121) + \u221a err(h\u2217, Di\u22121)\u03c3k\u2217(2i, \u03b4i,k\u2217) + \u03c3k\u2217(2 i, \u03b4i,k\u2217)\nWe get\n\u03b3i\u22121 \u2264 err(h\u2217, Di\u22121) + 2 \u221a err(h\u2217, Di\u22121)\u03c3k\u2217(2i, \u03b4i,k\u2217) + 3\u03c3k\u2217(2 i, \u03b4i,k\u2217) .\nThus, invariant 1 is established for iteration i. Now consider the verification stage in iteration i. We first prove that the loop in the verification stage will terminate and establish some properties upon termination. Observe that k and S are initially ki\u22121 and Si\u22121, respectively. Throughout the loop, the examples added to S are obtained from Search, and hence are consistent with h\u2217. In addition, we have the following claim regarding k\u2217.\nClaim 2. If invariants 1\u20135 holds for iteration i\u2212 1, then for iteration i, the following holds:\n(a) minh\u2208Hk\u2217 (S) err(h, Ti) \u2264 \u03b3i\u22121 + \u221a \u03b3i\u22121\u03c3k\u2217(2i, \u03b4i,k\u2217) + \u03c3k\u2217(2 i, \u03b4i,k\u2217)\n(b)\nh\u2217 \u2208 V k\u2217i = { h \u2208 Hk\u2217(S) :\nerr(h, Ti) \u2264 min h\u2032\u2208Hk\u2217 (S)\nerr(h\u2032, Ti) + 2 \u221a err(h\u2032, Ti)\u03c3k\u2217(2i, \u03b4i,k\u2217) + 3\u03c3k\u2217(2 i, \u03b4i,k\u2217) } .\nProof. Recall that h\u2217 is the optimal hypothesis under distribution Di\u22121. We have already shown above that err(h\u2217, Di\u22121) \u2264 \u03b3i\u22121. By the definition of Ei,\nmin h\u2208Hk\u2217 (S)\nerr(h, Ti) \u2264 err(h\u2217, Ti)\n\u2264 err(h\u2217, Di\u22121) + \u221a err(h\u2217, Di\u22121)\u03c3(2i, \u03b4i,k\u2217) + \u03c3(2 i, \u03b4i,k\u2217)\n\u2264 \u03b3i\u22121 + \u221a \u03b3i\u22121\u03c3k\u2217(2i, \u03b4i,k\u2217) + \u03c3k\u2217(2 i, \u03b4i,k\u2217)\nwhere the last inequality is from that err(h\u2217, Di\u22121) \u2264 \u03b3i\u22121. This proves item (a). On the other hand, for all h\u2032 in Hk\u2217(S),\nerr(h\u2217, Ti) \u2264 err(h\u2217, Di\u22121) + \u221a err(h\u2217, Di\u22121)\u03c3k\u2217(2i, \u03b4i,k\u2217) + \u03c3(2 i, \u03b4i,k\u2217)\n\u2264 err(h\u2032, Di\u22121) + \u221a err(h\u2032, Di\u22121)\u03c3k\u2217 (2i, \u03b4i,k\u2217) + \u03c3(2 i, \u03b4i,k\u2217)\n\u2264 err(h\u2032, Ti) + 2 \u221a err(h\u2032, Ti)\u03c3k\u2217 (2i, \u03b4i,k\u2217) + 3\u03c3(2 i, \u03b4i,k\u2217) .\nwhere the first inequality is from the definition of Ei, the second inequality is from Invariant 5 of iteration i\u2212 1, the third inequality is from the definition of Ei.\nThus, err(h\u2217, Ti) \u2264 minh\u2032\u2208Hk\u2217 (S) err(h\u2032, Ti) + 2 \u221a err(h\u2032, Ti)\u03c3k\u2217(2i, \u03b4i,k\u2217) + 3\u03c3k\u2217(2 i, \u03b4i,k\u2217), proving item\n(b).\nClaim 2 implies that k cannot increase beyond k\u2217. To see this, observe that Claim 2(a) implies the condition in Step 8 is not satisfied for k = k\u2217. In addition, Claim 2(b) implies that h\u2217 \u2208 V k\u2217i 6= \u2205, which in turn means that SearchHk\u2217 (V k\u2217\ni ) = \u22a5. Hence, the loop in the verification stage would terminate if k ever reaches k\u2217. Because iteration i starts with k \u2264 k\u2217 (as invariant 3 holds in iteration i \u2212 1), invariants 2 and 3 must also hold for iteration i.\nFinally, we can establish invariants 4 and 5 for iteration i. Because the loop terminates with SearchHk(V ki i )\nreturning \u22a5, there is no counterexample x \u2208 X such that h\u2217 disagrees with every h \u2208 V kii . This implies that h\u2217(x) = V kii (x) for all x /\u2208 Dis(V kii ) (i.e., invariant 4). Hence, for any hypothesis h,\nerr(h,Di) = Pr[h(x) 6= h\u2217(x), x /\u2208 Dis(V kii )] + Pr[h(x) 6= y, x \u2208 Dis(V kii )] .\nTherefore,\nerr(h,Di)\u2212 err(h\u2217, Di) = Pr[h(x) 6= h\u2217(x), x /\u2208 Dis(V kii )] + Pr[h(x) 6= y, x \u2208 Dis(V kii )]\n\u2212 Pr[h\u2217(x) 6= y, x \u2208 Dis(V kii )] \u2265 Pr[h(x) 6= y, x /\u2208 Dis(V kii )]\u2212 Pr[h\u2217(x) 6= y, x /\u2208 Dis(V kii )]\n+ Pr[h(x) 6= y, x \u2208 Dis(V kii )]\u2212 Pr[h\u2217(x) 6= y, x \u2208 Dis(V kii )] = err(h,D)\u2212 err(h\u2217, D) ,\nwhich proves invariant 5 for iteration i.\nProof of Theorem 6. Supose event E happens. We first show a claim regarding the error of hypotheses in current version spaces.\nClaim 3. On event E, for all i \u2265 1, for all h \u2208 V kii ,\nerr(h,D) \u2264 err(h\u2217, Di\u22121) + 6 \u221a err(h\u2217, Di\u22121)\u03c3k\u2217(2i, \u03b4i,k\u2217) + 21\u03c3k\u2217(2 i, \u03b4i,k\u2217).\nProof. First, for every h in V kii ,\nerr(h, Ti) \u2264 min h\u2032\u2208Hki (S)\nerr(h\u2032, Ti) + 2 \u221a err(h\u2032, Ti)\u03c3ki(2 i, \u03b4i,ki) + 3\u03c3ki(2 i, \u03b4i,ki) ,\nand since the condition in step 8 is not satisfied for k = ki, we know that\nmin h\u2032\u2208Hki (S)\nerr(h\u2032, Ti) \u2264 \u03b3i\u22121 + \u221a \u03b3i\u22121\u03c3ki(2 i, \u03b4i,ki) + \u03c3ki(2 i, \u03b4i,ki) .\nThus,\nerr(h, Ti) \u2264 \u03b3i\u22121 + 3 \u221a \u03b3i\u22121\u03c3ki(2 i, \u03b4i,ki) + 6\u03c3ki(2 i, \u03b4i,ki) . (7)\nBy definition of event Ei, we also have\nerr(h,Di\u22121) \u2264 err(h, Ti) + \u221a err(h, Ti)\u03c3ki (2 i, \u03b4i,ki) + \u03c3ki(2 i, \u03b4i,ki) .\nHence,\nerr(h,Di\u22121) \u2264 \u03b3i\u22121 + 4 \u221a \u03b3i\u22121\u03c3ki(2 i, \u03b4i,ki) + 10\u03c3ki(2 i, \u03b4i,ki) .\nFurthermore, by item 1 of Lemma 7,\n\u03b3i\u22121 \u2264 err(h\u2217, Di\u22121) + 2 \u221a err(h\u2217, Di\u22121)\u03c3k\u2217 (2i, \u03b4i,k\u2217) + 3\u03c3k\u2217(2 i, \u03b4i,k\u2217) .\nThis implies that\nerr(h,Di\u22121) \u2264 err(h\u2217, Di\u22121) + 6 \u221a err(h\u2217, Di\u22121)\u03c3ki (2 i, \u03b4i,ki) + 21\u03c3ki(2 i, \u03b4i,ki)\n\u2264 err(h\u2217, Di\u22121) + 6 \u221a err(h\u2217, Di\u22121)\u03c3k\u2217 (2i, \u03b4i,k\u2217) + 21\u03c3k\u2217(2 i, \u03b4i,k\u2217) .\nwhere the second inequality is from item 3 of Lemma 7.\nWe first prove the error rate guarantee. Suppose iteration i = I = log2 M(\u03bd, k \u2217, \u01eb, \u03b4) has been reached.\nObserve that from Claim 3, for h\u0302 \u2208 V kII ,\nerr(h\u0302, DI\u22121)\u2212 err(h\u2217, DI\u22121) \u2264 6 \u221a err(h\u2217, DI\u22121)\u03c3k\u2217(2I , \u03b4I,k\u2217) + 21\u03c3k\u2217(2 I , \u03b4I,k\u2217) \u2264 \u01eb\nwhere the second inequality is from that m = 2I = M(\u03bd, k\u2217, \u01eb, \u03b4). Thus, by item 5 of Lemma 7,\nerr(h\u0302, D)\u2212 err(h\u2217, D) \u2264 err(h\u0302, DI\u22121)\u2212 err(h\u2217, DI\u22121) \u2264 \u01eb .\nNext, we prove the bound on the number of Search queries. From Lemma 7, Algorithm 5 maintains the invariant that k \u2264 k\u2217. For each iteration i, each call to Search either returns an example forcing k to increment, or returns \u22a5 which causes an exit from the verification stage loop. Therefore, the total number of Search calls is at most\nk\u2217 + I \u2264 k\u2217 +O ( log dk\u2217\n\u01eb2 + log log\nk\u2217\n\u03b4\n)\n.\nFinally, we prove the bound on the number of Label queries. This is done in a few steps.\n1. We first show that the version space V kii is always contained in a ball of small radius (with respect to the\ndisagreement pseudometric Prx\u223cDX [h(x) 6= h\u2032(x)]). Specifically, for 1 \u2264 i \u2264 I, for any h, h\u2032 \u2208 V kii , from Claim 3, in conjunction with triangle inequality, and that err(h\u2217, Di\u22121) \u2264 err(h,D) = \u03bd,\nPr x\u223cDX\n[h(x) 6= h\u2032(x)]\n\u2264 2 err(h\u2217, Di\u22121) + 12 \u221a err(h\u2217, Di\u22121)\u03c3k\u2217(2i, \u03b4i,k\u2217) + 42\u03c3k\u2217(2 i, \u03b4i,k\u2217)\n\u2264 2\u03bd + 12 \u221a \u03bd\u03c3k\u2217(2i, \u03b4i,k\u2217) + 42\u03c3k\u2217(2 i, \u03b4i,k\u2217)\nThus, V kii is contained in BHki (S)(h, 2\u03bd + 12 \u221a \u03bd\u03c3k\u2217 (2i, \u03b4i,k\u2217) + 42\u03c3k\u2217(2 i, \u03b4i,k\u2217)) for some h in Hki(S).\n2. Next we bound the label complexity per iteration. Note that by the choice of m = 2I = M(\u03bd, k\u2217, \u01eb, \u03b4), 6 \u221a \u03bd\u03c3k\u2217(2I\u22121, \u03b4I\u22121,k\u2217) + 21\u03c3k\u2217(2 I\u22121, \u03b4I\u22121,k\u2217) \u2265 \u01eb, therefore for all 1 \u2264 i \u2264 I, 6 \u221a\n\u03bd\u03c3k\u2217(2i, \u03b4i,k\u2217) + 21\u03c3k\u2217(2 i, \u03b4i,k\u2217) \u2265 \u01eb/2. Thus, the size of the disagreement region can be bounded as\nPr x\u223cDX\n[x \u2208 Dis(V kii )] \u2264 \u03b8k(2\u03bd + \u01eb) \u00b7 ( 2\u03bd + 12 \u221a \u03bd\u03c3k\u2217 (2i, \u03b4i,k\u2217) + 42\u03c3k\u2217(2 i, \u03b4i,k\u2217) )\n\u2264 \u03b8k(2\u03bd + \u01eb) \u00b7 ( 8\u03bd + 48\u03c3k\u2217(2 i, \u03b4i,k\u2217) ) . (8)\nBy definition of Ei, the number of queries to Label at iteration i is at most\n2i+1 Pr x\u223cDX\n[x \u2208 Dis(V kii )] +O ( \u221a\n2i+1 Pr x\u223cDX\n[x \u2208 Dis(V kii )] log(1/\u03b4i,k\u2217) + log(1/\u03b4i,k\u2217) ) .\nCombining this with (8) gives\n# Label queries in iteration i = O ( 2i \u00b7 \u03b8ki(2\u03bd + \u01eb) \u00b7 (\u03bd + \u03c3k\u2217(2i, \u03b4i,k\u2217)) ) . (9)\n3. From the setting of m = 2I = O\u0303(dk\u2217(\u03bd + \u01eb)/\u01eb 2), we get that\nI = O\n(\nlog dk\u2217\n\u01eb + log log\nk\u2217\n\u03b4\n)\n.\nNow, using (9), we get that the total number of Label queries by Algorithm 5 is bounded by\n2 +\nI \u2211\ni=1\nO ( 2i \u00b7 \u03b8ki(2\u03bd + \u01eb) \u00b7 (\u03bd + \u03c3k\u2217(2i, \u03b4i,k\u2217)) )\n= 2 +\nI \u2211\ni=1\nO\n(\n2i \u00b7 max k\u2264k\u2217\n\u03b8k(2\u03bd + \u01eb) \u00b7 (\u03bd + \u03c3k\u2217(2i, \u03b4i,k\u2217)) )\n= O\n\n  max k\u2264k\u2217 \u03b8k(2\u03bd + \u01eb) \u00b7\n\n\nI \u2211\ni=1\n2i(\u03bd + \u03c3k\u2217(2 i, \u03b4i,k\u2217))\n\n\n\n \n= O\n\n  max k\u2264k\u2217 \u03b8k(2\u03bd + \u01eb) \u00b7\n \u03bd2I + I \u2211\ni=1\n2i d ln(2i) + ln( (i\n2+i)(k\u2217)2\n\u03b4 )\n2i\n\n\n\n \n= O\n(\nmax k\u2264k\u2217\n\u03b8k(2\u03bd + \u01eb) \u00b7 ( \u03bd2I + dk\u2217I 2 + I log k\u2217\n\u03b4\n)\n)\n= O\n\nmax k\u2264k\u2217\n\u03b8k(2\u03bd + \u01eb) \u00b7 ( \u03bd2 + \u01eb\u03bd\n\u01eb2\n(\ndk\u2217 log 1\n\u01eb + log\nk\u2217\n\u03b4\n)\n+ dk\u2217\n(\nlog dk\u2217\n\u01eb + log log\nk\u2217\n\u03b4\n)2\n+\n(\nlog dk\u2217\n\u01eb + log log\nk\u2217\n\u03b4\n)\nlog k\u2217\n\u03b4\n)\n\n\n= O\u0303\n\nmax k\u2264k\u2217\n\u03b8k(2\u03bd + \u01eb) \u00b7 ( dk\u2217(log 1\n\u01eb )2 + log\nk\u2217\n\u03b4\n) \u00b7 ( 1 + \u03bd2\n\u01eb2\n)\n\n ."}, {"heading": "E Performance Guarantees of AA-Larch", "text": ""}, {"heading": "E.1 Detailed Description of Subroutines", "text": "Subroutine Sample-and-Label performs standard disagreement-based selective sampling. Specifically, it draws an unlabeled example x from the DX . If x is in the agreement region of version space V , its label is inferred as V (x); otherwise, we query the Label oracle to get its label. The counter c is incremented when Label is called.\nAlgorithm 6 Sample-and-Label\ninput: Version space V \u2282 H , oracle Label, labeled dataset L, counter c. output: New labeled dataset L\u2032, new counter c\u2032. 1: x \u2190 independent draw from DX (the corresponding label is hidden). 2: if x \u2208 Dis(V ) then 3: L\u2032 \u2190 L \u222a { (x,Label(x)) }\n4: c\u2032 \u2190 c+ 1 5: else 6: L\u2032 \u2190 L \u222a { (x, V (x)) } 7: c\u2032 \u2190 c 8: end if\nSubroutine Error-Check checks if the version space has high error, based on item 2 of Lemma 9 \u2013 that is, if k = k\u2217, then Error-Check should never fail. Furthermore, if version space Vi fails Error-Check, then Vi should have small radius \u2013 see Lemma 8 for details.\nAlgorithm 7 Error-Check\ninput: Version space V \u2282 Hk, labeled dataset L of size l, confidence \u03b4. output: Boolean variable b indicating if V has high error. 1: Let \u03b4k := \u03b4/((k + 1)(k + 2)) for all k \u2265 0. 2: \u03b3 \u2190 mink\u2032\u2265k,h\u2208H\nk\u2032\n{ err(h, L) + 2 \u221a err(h, L)\u03c3k\u2032(l, \u03b4k\u2032) + 3\u03c3k\u2032(l, \u03b4k\u2032) }\n3: if minh\u2208V err(h, L) > \u03b3 + 2 \u221a\n\u03b3\u03c3k(l, \u03b4k) + 3\u03c3k(l, \u03b4k) then 4: b \u2190 true 5: else 6: b \u2190 false 7: end if\nSubroutine Prune-Version-Space performs update on our version space based on standard generalization error bounds. The version space never eliminates the optimal hypothesis in Hk(S) when working with Hk. Claim 4 shows that, if at step i, k = k \u2217, then h\u2217 \u2208 Vi from then on.\nAlgorithm 8 Prune-Version-Space\ninput: Version space V \u2282 Hk, labeled dataset L of size l, confidence \u03b4. output: Pruned version space V \u2032. 1: Update version space:\nV \u2032 \u2190 {\nh \u2208 V : err(h, L) \u2264 min h\u2032\u2208V err(h\u2032, L) + 2 \u221a err(h\u2032, L)\u03c3k(l, \u03b4k) + 3\u03c3k(l, \u03b4k) } ,\nwhere \u03b4k := \u03b4\n(k+1)(k+2) .\nSubroutine Upgrade-Version-Space is called when (1) a systematic mistake of the version space Vi has been found by Search; or (2) Error-Check detects that the error of Vi is high. In either case, k can be increased to the minimum level such that the updated Hk(S) is nonempty. This still maintains the invariant that k \u2264 k\u2217.\nAlgorithm 9 Upgrade-Version-Space\ninput: Current level of hypothesis class k, seed set S, seed to be added s. output: New level of hypothesis class k, new seed set S, updated version space V . 1: S \u2190 S \u222a s 2: k \u2190 min { k\u2032 > k : Hk\u2032(S) 6= \u2205 }\n3: V \u2190 Hk(S)"}, {"heading": "E.2 Proof of Theorem 4", "text": "This section uses the following definition of \u03c3:\n\u03c3k(m, \u03b4) = \u03c6(dk,m, \u03b4/3) = 1\nm (d log em2 + log\n6 \u03b4 ).\nWe restate Theorem 4 here for convenience.\nTheorem 7. There exist constants c1, c2 > 0 such that the following holds. Assume err(h \u2217) = \u03bd. Let \u03b8k\u2032(\u00b7) denote the disagreement coefficient of Vi at the first step i after which k \u2265 k\u2032. Fix any \u01eb, \u03b4 \u2208 (0, 1). Let n\u01eb = c1 maxk\u2264k\u2217 \u03b8k(2\u03bd+2\u01eb)(dk\u2217 log 1 \u01eb +log 1 \u03b4 )(1+\u03bd2/\u01eb2) and define C\u01eb = 2(n\u01eb+k\n\u2217\u03c4). Run Algorithm 2 with a nested sequence of hypotheses {Hk}\u221ek=0, oracles Label and Search, confidence parameter \u03b4, cost ratio \u03c4 \u2265 1, and upper bound N = c2(dk\u2217 log 1\u01eb + log 1\u03b4 )/\u01eb2. If the cost spent is at least C\u01eb, then with probability 1\u2212 \u03b4, the current hypothesis h\u0303 has error at most \u03bd + \u01eb.\nRemark. The purpose of having a bound on unlabeled examples, N , is rather technical\u2014 to deter the algorithm from getting into an infinite loop due to its blind self-confidence. Suppose that AA-Larch starts with H0 that has a single element h. Then, without such an N -based condition, it will incorrectly infer the labels of all the unlabeled examples drawn and end up with an infinite loop between lines 4 and 14. The condition on N is very mild\u2014any N satisfying N = poly(dk\u2217 , 1/\u01eb) and N = \u2126(dk\u2217/\u01eb 2) is sufficient.\nProof of Theorem 7. For integer j \u2265 0, define step j as the execution period in AA-Larch when the value of i is j.\nLet li = |Li|. Denote by LDi the dataset containing unlabeled examples in Li labeled entirely by Label, i.e., LDi = { (x,Label(x)) : (x, y) \u2208 Li }\n. Note that LDi is an iid sample from D. We call dataset Li has favorable bias, if the following holds for any hypothesis h:\nerr(h, LDi )\u2212 err(h\u2217, LDi ) \u2264 err(h, Li)\u2212 err(h\u2217, Li). (10) Let Ei be the event that the following conditions hold:\n1. For every k \u2265 0, every h \u2208 Hk satisfies\nerr(h,D) \u2264 err(h, LDi ) + \u221a err(h, LDi )\u03c3k(li, \u03b4i,k) + \u03c3k(li, \u03b4i,k) ,\nerr(h, LDi ) \u2264 err(h,D) + \u221a err(h,D)\u03c3k(li, \u03b4i,k) + \u03c3k(li, \u03b4i,k) .\nFor every h, h\u2032 \u2208 Hk, (err(h, LDi )\u2212 err(h\u2032, LDi ))\u2212 (err(h,D)\u2212 err(h\u2032, D))\n\u2264 \u221a\ndLD i\n(h, h\u2032) \u00b7 \u03c3k(li, \u03b4i,k) + \u03c3k(li, \u03b4i,k) .\nwhere dLD i (h, h\u2032) = 1 li\n\u2211\n(x,y)\u2208LD i\n[h(x) 6= h\u2032(x)], fraction of LDi where h and h\u2032 disagree.\n2. For every 1 \u2264 i\u2032 < i, the number of Label queries from step i\u2032 to step i is at most\ni \u2211\nj=i\u2032\nPr x\u223cDX\n[x \u2208 Dis(Vj\u22121)] +O\n\n \n\u221a \u221a \u221a \u221a i \u2211\nj=i\u2032\nPr x\u223cDX\n[x \u2208 Dis(Vj\u22121)] log(1/\u03b4i) + log(1/\u03b4i)\n\n  ,\nwhere Vj denotes its final value in Algorithm 2.\nUsing Theorem 5 and Lemma 5, along with the union bound, Pr(Ei) \u2265 1\u2212 \u03b4i. Define E := \u2229\u221ei=1Ei, by union bound, Pr(E) \u2265 1\u2212 \u03b4. We henceforth condition on E holding.\nDefine\nM(\u03bd, k\u2217, \u01eb, \u03b4,N) := min\n{\nm \u2208 N : 8 \u221a \u03bd\u03c3k\u2217(m, \u03b4m+k\u2217N,k\u2217) + 35\u03c3k\u2217(m, \u03b4m+k\u2217N,k\u2217) \u2264 \u01eb }\n\u2264 O ( (dk\u2217 log(1/\u01eb) + log(Nk \u2217/\u03b4))(\u03bd + \u01eb)\n\u01eb2\n)\nWe say that an iteration of the loop is verified if Step 20 is triggered; all other iterations are unverified. Let \u0393 be the set of i\u2019s where xi gets added to the final set L, and \u2206 be the set of i\u2019s where xi gets discarded. It is easy to see that if i is in \u0393 (resp. \u2206), then the i is in a verified (resp. unverified) iteration.\nDefine i\u2217 := min { i \u2208 \u0393 : li \u2265 M(\u03bd, k\u2217, \u01eb, \u03b4,N) }\n. Denote by ki the final value of k after i unlabeled examples are processed.\nWe need to prove two claims:\n1. For i \u2265 i\u2217, err(h\u0303i) \u2264 \u03bd + \u01eb, where h\u0303i is the hypothesis h\u0303 stored at the end of step i. 2. The total cost spent by Algorithm 2 up to step i\u2217 is at most C\u01eb.\nTo prove the first claim, fix any i \u2265 i\u2217. The stored hypothesis h\u0303i is updated only when i \u2208 \u0393, so it suffices to consider only i \u2208 \u0393. From Lemma 10, i \u2264 li + k\u2217N . We also have li \u2265 M(\u03bd, k\u2217, \u01eb, \u03b4,N). Since h\u0303i \u2208 Vi, Lemma 8 gives\nerr(h\u0303i) \u2264 \u03bd + 8 \u221a \u03bd\u03c3k\u2217(li, \u03b4i,k\u2217) + 35\u03c3k\u2217(li, \u03b4i,k\u2217)\n\u2264 \u03bd + 8 \u221a\n\u03bd\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217) + 35\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217)\n\u2264 \u03bd + \u01eb, as desired.\nFor the second claim, we first show that for i in \u0393, the version space is contained in a ball of small radius (with respect to the disagreement pseudometric), thus bounding the size of its disagreement region. Lemma 8 shows that for i \u2208 \u0393, every hypothesis h \u2208 Vi has error at most \u03bd + 8 \u221a\n\u03bd\u03c3k\u2217 (li, \u03b4i,k\u2217) + 35\u03c3k\u2217(li, \u03b4i,k\u2217). Thus, by the triangle inequality and Lemma 10,\nVi \u2286 BHki (h, 2\u03bd + 16 \u221a \u03bd\u03c3k\u2217(li, \u03b4i,k\u2217) + 70\u03c3k\u2217(li, \u03b4i,k\u2217))\n\u2286 BHki (h, 2\u03bd + 16 \u221a \u03bd\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217) + 70\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217)).\nfor some h in Hki(S). This shows that for i \u2208 \u0393, i \u2264 i\u2217, Prx\u223cDX [x \u2208 Dis(Vi)]\n\u2264 \u03b8ki(2\u03bd + 2\u01eb) \u00b7 ( 2\u03bd + 16 \u221a \u03bd\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217) + 70\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217) )\n\u2264 maxk\u2264k\u2217 \u03b8k(2\u03bd + 2\u01eb) \u00b7 ( 2\u03bd + 16 \u221a \u03bd\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217) + 70\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217) ) , (11)\nwhere the first inequality is from the definition of \u03b8ki(\u00b7) and 8 \u221a \u03bd\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217)+35\u03c3k\u2217(li, \u03b4li+k\u2217N,k\u2217) \u2265 \u01eb for i \u2264 i\u2217, the second inequality is from ki \u2264 k\u2217.\nFor i \u2265 1, let Zi be the indicator of whether Label is queried with xi in Step 12, i.e.,\nZi = 1{xi \u2208 Dis(Vi\u22121)} .\nFor 0 \u2264 k \u2264 ki\u2217 , define\ni0k := min {i \u2264 i\u2217 : ki \u2265 k} , the first step when the hypothesis class reaches \u2265 k, ik := max {i \u2264 i\u2217 : ki \u2264 k} , the last step when the hypothesis class is still \u2264 k by the end of that step, i\u2032k := max { i0k \u2264 i \u2264 ik : ki \u2264 k, i \u2208 \u0393 } , the last verified step for hypothesis class \u2264 k (if exists).\nWe call class k skipped if there is no step i such that ki = k. If level k is skipped, then ik = ik\u22121 = i 0 k \u2212 1, and i\u2032k is undefined. Let\nWk :=\ni\u2032 k\n\u2211\ni=i0 k +1\nZi\nbe the number of verified queried examples when working with hypothesis class Hk. Note that Wk/\u03c4 is the number of verified iterations when working with Hk. If level k is skipped, then Wk := 0.\nLet\nYk :=\nik+1 \u2211\ni=i\u2032 k +1\nZi\nbe the number of unverified queried examples when working with hypothesis class Hk. Note that Yk \u2264 \u03c4 , and there is at most one unverified iteration when working with Hk. If level k is skipped,then Yk := 0.\nTherefore, the total cost when working with Hk is at most\nWk \u03c4 \u00b7 2\u03c4 + Yk + \u03c4 \u2264 2\u03c4 + 2Wk\nFurthermore, Claim 4 implies that there is no unverified iteration when working with Hk\u2217 . Hence the total cost when working with Hk\u2217 has a tighter upper bound, that is, 2Wk\u2217 .\nAs a shorthand, let m = M(\u03bd, k\u2217, \u01eb, \u03b4,N). We now bound the total cost incurred up to time i\u2217 as\nk\u2217\u22121 \u2211\nk=0\n(2\u03c4 + 2Wk) + 2Wk\u2217 = 2\u03c4k \u2217 + 2\nki\u2217 \u2211\nk=0\nWk\n= 2\u03c4k\u2217 + 2\nki\u2217 \u2211\nk=0\ni\u2032 k\n\u2211\ni=i0 k +1\nZi\n= 2\u03c4k\u2217 +O\n\n2 \u2211\ni\u2208\u0393:i\u2264i\u2217\nPr x\u223cDX\n[x \u2208 Dis(Vi\u22121)]\n\n+O\n(\nk\u2217 ln 1\n\u03b4i\u2217\n)\n\u2264 2\n\n  \u03c4k\u2217 +O\n\n\nm\u22121 \u2211\nl=1\nmax k\u2264k\u2217 \u03b8k(2\u03bd + 2\u01eb)(\u03bd + \u03c3k\u2217(l, \u03b4l+k\u2217N,k\u2217))\n\n\n\n \n\u2264 2\n\n  \u03c4k\u2217 +O\n\nmax k\u2264k\u2217 \u03b8k(2\u03bd + 2\u01eb)\nm\u22121 \u2211\nl=1\n(\u03bd + \u03c3k\u2217(l, \u03b4l+k\u2217N,k\u2217)))\n\n\n\n \n\u2264 2\n\n  \u03c4k\u2217 + O\u0303\n\nmax k\u2264k\u2217 \u03b8k(2\u03bd + 2\u01eb)dk\u2217\n(\n1 + \u03bd2\n\u01eb2\n)\n\n\n\n \n\u2264 2 (\u03c4k\u2217 + n\u01eb) = C\u01eb, where the first equality is by algebra, the second equality is from the definition of Wk, and the third equality is from the definition of E. The first inequalty is from Lemma 8, using Equation (11) to bound Prx\u223cDX [x \u2208 Dis(Vi\u22121)] and noting that {li : i \u2208 \u0393, i \u2264 i\u2217} = [m].\nNow we provide the proof of our two key lemmas(Lemmas 8 and 10). Consider the last call of Prune-Version-Space in step i. Define \u03b3i as the value of \u03b3 in line 2 of\nError-Check:\n\u03b3i = min k\u2032\u2265ki,h\u2208Hk\u2032\n{\nerr(h, Li) + 2 \u221a err(h, Li)\u03c3k\u2032 (l, \u03b4i,k\u2032) + 3\u03c3k\u2032 (l, \u03b4i,k\u2032)\n}\n(12)\nMeanwhile, from line 1 of Prune-Version-Space, we have for all h \u2208 Vi,\nerr(h, Li) \u2264 min h\u2032\u2208Vi\nerr(h\u2032, Li) + 2 \u221a err(h\u2032, Li)\u03c3ki(li, \u03b4i,ki) + 3\u03c3ki(li, \u03b4i,ki)\nwhere Vi denotes its final value.\nLemma 8. Assume that the following conditions hold:\n1. The dataset Li has favorable bias, i.e. it satisfies Equation (10).\n2. The version space Vi is such that Error-Check(Vi, Li, \u03b4i) returns false, i.e. it has a low empirical error on Li:\nmin h\u2032\u2208Vi\nerr(h\u2032, Li) \u2264 \u03b3i + 2 \u221a \u03b3i\u03c3ki(li, \u03b4i,ki) + 3\u03c3ki(li, \u03b4i,ki). (13)\nThen, every h \u2208 Vi is such that\nerr(h) \u2264 \u03bd + 8 \u221a\n\u03bd\u03c3k\u2217(li, \u03b4i,k\u2217) + 35\u03c3k\u2217(li, \u03b4i,k\u2217). (14)\nwhere Vi and Li denote their final values, respectively. Specifically, Equation (14) holds for any h \u2208 Vi such that i \u2208 \u0393 or i+ 1 \u2208 \u0393.\nProof. Lemma 10 shows that ki \u2264 k\u2217, which we will use below. Start with Equation (13):\nmin h\u2032\u2208Vi\nerr(h\u2032, Li) \u2264 \u03b3i + 2 \u221a \u03b3i\u03c3ki(li, \u03b4i,ki) + 3\u03c3ki(li, \u03b4i,ki).\nSince ki \u2264 k\u2217, \u03c3ki(li, \u03b4i,ki) \u2264 \u03c3k\u2217(li, \u03b4i,k\u2217). From the definition of \u03b3i (Equation (12)), taking k = k \u2217 \u2265 ki, h = h\u2217 \u2208 Hk\u2217 ,\n\u03b3i \u2264 err(h\u2217, Li) + 2 \u221a err(h\u2217, Li)\u03c3k\u2217 (li, \u03b4i,k\u2217) + 3\u03c3k\u2217(li, \u03b4i,k\u2217).\nPlugging the latter into the former and using \u03c3 as a shorthand for \u03c3k\u2217(li, \u03b4i,k\u2217), we have\nmin h\u2032\u2208Vi\nerr(h\u2032, Li) \u2264 err(h\u2217, Li) + 2 \u221a err(h\u2217, Li)\u03c3 + 3\u03c3 + 2\n\u221a\n(err(h\u2217, Li) + 2 \u221a err(h\u2217, Li)\u03c3 + 3\u03c3)\u03c3 + 3\u03c3\n\u2264 err(h\u2217, Li) + 2 \u221a err(h\u2217, Li)\u03c3 + 6\u03c3 + 2( \u221a err(h\u2217, Li)\u03c3 + \u221a 3\u03c3) \u2264 err(h\u2217, Li) + 4 \u221a err(h\u2217, Li)\u03c3 + 10\u03c3 .\nFix any h \u2208 Vi. By construction,\nerr(h, Li) \u2264 min h\u2032\u2208Vi\nerr(h\u2032, Li) + 2 \u221a err(h\u2032, Li)\u03c3ki (li, \u03b4i,ki) + 3\u03c3ki(li, \u03b4i,ki).\nPlugging the former into the latter (recalling that \u03c3ki (li, \u03b4i,ki) \u2264 \u03c3) gives\nerr(h, Li)\u2212 err(h\u2217, Li) \u22644 \u221a err(h\u2217, Li)\u03c3 + 10\u03c3 + 2( \u221a err(h\u2217, Li)\u03c3 + \u221a 10\u03c3) + 3\u03c3\n\u22646 \u221a err(h\u2217, Li)\u03c3 + 20\u03c3.\nCombined with Equation (10), we have,\nerr(h, LDi )\u2212 err(h\u2217, LDi ) \u2264 6 \u221a err(h\u2217, Li)\u03c3 + 20\u03c3.\nSince err(h\u2217, Li) \u2264 err(h\u2217, LDi ),\nerr(h, LDi )\u2212 err(h\u2217, LDi ) \u2264 6 \u221a err(h\u2217, LDi )\u03c3 + 20\u03c3.\nFrom the definition of Ei, err(h\u2217, LDi ) \u2264 \u03bd + \u221a \u03bd\u03c3 + \u03c3.\nerr(h) \u2264 err(h, LDi ) + \u221a err(h, LDi )\u03c3 + \u03c3.\nPlugging in and simplifying algebratically gives\nerr(h) \u2264 \u03bd + 8\u221a\u03bd\u03c3 + 35\u03c3.\nNow, if i \u2208 \u0393, the dataset Li has favorable bias from lemma 11; if i /\u2208 \u0393 and i + 1 \u2208 \u0393, the final value of Li equals some Lj for some j \u2208 \u0393, therefore also has favorable bias.\nMeanwhile, if i \u2208 \u0393, Algorithm 2 fails Error-Check(Vi, Li, \u03b4i) for k = ki. If i /\u2208 \u0393 and i+ 1 \u2208 \u0393, then i + 1 is the start of some verified iteration, i.e. i + 1 = i0k for some k. Hence the final value of Vi also fails Error-Check(Vi, Li, \u03b4i) for k = ki. In both cases, Equation (13) holds.\nTherefore, if i \u2208 \u0393 or i+ 1 \u2208 \u0393, then Equation (14) holds for every h in Vi.\nLemma 9. For step i, suppose Li has favorable bias, i.e. Equation (10) holds. Then for any k and any h \u2208 Hk,\nerr(h\u2217, Li)\u2212 err(h, Li) \u2264 2 \u221a err(h, Li)\u03c3k\u0304(li, \u03b4i,k\u0304) + 3\u03c3k\u0304(li, \u03b4i,k\u0304),\nwhere k\u0304 = max(k\u2217, k). Specifically:\n1. for any h \u2208 Hk\u2217 ,\nerr(h\u2217, Li)\u2212 err(h, Li) \u2264 2 \u221a err(h, Li)\u03c3k\u2217 (li, \u03b4i,k\u2217) + 3\u03c3k\u2217(li, \u03b4i,k\u2217), (15)\n2. The empirical error of h\u2217 on Li can be bounded as follows:\nerr(h\u2217, Li) \u2264 \u03b3i + 2 \u221a \u03b3i\u03c3k\u2217(li, \u03b4i,k\u2217) + 3\u03c3k\u2217(li, \u03b4i,k\u2217) (16)\nProof. Fix any k and h \u2208 Hk. Since k\u0304 \u2265 k, \u03c3k(li, \u03b4i,k) \u2264 \u03c3k\u0304(li, \u03b4i,k\u0304). Similarly, \u03c3k\u2217(li, \u03b4i,k\u2217) \u2264 \u03c3k\u0304(li, \u03b4i,k\u0304). Using the shorthand \u03c3 := \u03c3k\u0304(li, \u03b4i,k\u0304) and noting that h, h \u2217 \u2208 Hk\u0304,\nerr(h\u2217, Li)\u2212 err(h, Li) \u2264 err(h\u2217, LDi )\u2212 err(h, LDi ) \u2264 \u221a\ndLD i\n(h\u2217, h) \u00b7 \u03c3 + \u03c3\n\u2264 \u221a (err(h\u2217, Li) + err(h, Li)) \u00b7 \u03c3 + \u03c3. \u2264 \u221a err(h\u2217, Li)\u03c3 + \u221a err(h, Li)\u03c3 + \u03c3.\nwhere the first inequality is from Equation (10), the second inequality is from the definition of Ei and the optimality of h\u2217, and the third inequality is from the triangle inequality. Letting A = err(h\u2217, Li), B = err(h, Li), and C = B + \u221a B\u03c3 + \u03c3, we can rewrite the above inequality as A \u2264 C + \u221a A\u03c3. Solving the\nresulting quadratic equation in terms of A, we have A \u2264 C + \u03c3 + \u221a C\u03c3, or\nA \u2264 B + \u221a B\u03c3 + 2\u03c3 + \u221a \u03c3(B + \u221a B\u03c3 + \u03c3)\n\u2264 B + \u221a B\u03c3 + 2\u03c3 + \u221a \u03c3( \u221a B + \u221a \u03c3) \u2264 B + 2 \u221a B\u03c3 + 3\u03c3,\nor err(h\u2217, Li) \u2264 err(h, Li) + 2 \u221a err(h, Li)\u03c3 + 3\u03c3.\nSpecifically:\n1. Taking k = k\u2217, we get that Equation (15) holds for any h \u2208 Hk\u2217 , establishing item 1.\n2. Define\n(k\u0302i, h\u0302i) := arg min k\u2032\u2265k\u2217,h\u2208H\nk\u2032\n{\nerr(h, Li) + 2 \u221a err(h, Li)\u03c3k\u2032 (li, \u03b4i,k\u2032) + 3\u03c3k\u2032(li, \u03b4i,k\u2032)\n}\n.\nIn this notation, \u03b3i = err(h\u0302i, Li) + 2 \u221a\nerr(h\u0302i, Li)\u03c3k\u0302i (li, \u03b4i,k\u0302i) + 3\u03c3k\u0302i(li, \u03b4i,k\u0302i). We have\n\u03b3i + 2 \u221a \u03b3i\u03c3k\u2217(li, \u03b4i,k\u2217) + 3\u03c3k\u2217(li, \u03b4i,k\u2217) \u2265 err(h\u0302i, Li) + 2 \u221a err(h\u0302i, Li)\u03c3k\u0304(li, \u03b4i,k\u0304) + 3\u03c3k\u0304(li, \u03b4i,k\u0304)\n\u2265 err(h\u2217, Li),\nwhere k\u0304 = max(k\u2217, k\u0302i) and the last inequality comes from applying Lemma 9 for h \u2032 = h\u0302i \u2208 Hk\u0302i and k\u0304. This establishes Equation (16), proving item 2.\nLemma 10. At any step of AA-Larch, k \u2264 k\u2217. Consequently, for every i, i \u2264 li + k\u2217N .\nProof. We prove the lemma in two steps.\n1. Notice that there are two places where k is incremented in AA-Larch, line 6 and line 17. If k < k\u2217, neither line would increment it beyond k\u2217 as h\u2217 \u2208 Hk\u2217 and h\u2217 is consistent with S. If k = k\u2217, Claim 4 below shows that k will stay at k\u2217. This proves the first part of the claim.\n2. An iteration becomes unverified only if k gets incremented, and Algorithm 2 maintains the invariant that ki \u2264 k\u2217. Thus, the number of unverified iterations is at most k\u2217. In addition, each newly sampled set is of size at most N . So the number of unverified examples is at most k\u2217N .\nHence, i\u2014the total number of examples processed up to step i\u2014equals the sum of the number of verified examples li, plus the number of unverified examples, which is at most k\n\u2217N . This proves the second part of the claim.\nWe show a technical claim used in the proof of Lemma 10 which guarantees that, on event E, when k has reached k\u2217, it will remain k\u2217 from then on. Recall that ki is defined as the final value of k at the end of step i; i0k = min {i : ki \u2265 k} is the step at the end of which the working hypothesis space reaches level \u2265 k.\nClaim 4. If i0k\u2217 is finite, then the following hold for all i \u2265 i0k\u2217 :\n(C1) Li has favorable bias.\n(C2) Step i terminates with ki = k \u2217.\n(C3) h\u2217 \u2208 Vi.\nAbove, Li and Vi denote their final values in AA-Larch.\nProof. By induction on i.\nBase Case. Let i = i0k\u2217 . Consider the execution of AA-Larch at the start of step i 0 k\u2217 (line 11). Since by definition of ik, the final value of k at step i 0 k\u2217 \u2212 1 is < k\u2217, at step i0k\u2217 , line 5 or line 16 is triggered. Hence the dataset Li0 k\u2217\nequals some verified labeled dataset L stored by AA-Larch, i.e. Lj for some j \u2208 \u0393. Thus, applying Lemma 11, Claim C1 holds.\nWe focus on the moment in step i = i0k\u2217 when k increases to k \u2217 in Upgrade-Version-Space(line 6\nor 17). Now consider the temporary Vi0 k\u2217 computed in the next line (Prune-Version-Space). Item 2 of Lemma 9 implies that the version space Vi0 k\u2217 is such that Error-Check(Vi0 k\u2217 , Li0 k\u2217 , \u03b4i0 k\u2217 ) returns false. Therefore the final value of k in step i0k\u2217 is exactly k \u2217. Claim C2 follows.\nClaim C2 implies the temporary Vi0 k\u2217 is final. Item 1 of Lemma 9 implies that h\u2217 \u2208 Vi0 k\u2217 , establishing\nClaim C3.\nInductive Case. Now consider i \u2265 i0k\u2217 +1. The inductive hypothesis says that Claims C1\u20133 hold for step i\u2212 1.\nClaim C1 follows from Claim C3 in step i\u22121. Indeed, the newly added xi either comes from the agreement region of Vi\u22121, in which case label yi agrees with h\n\u2217(xi), or is from the disagreement region of Vi\u22121, in which case the inferred label yi is queried from Label. Following the same reasoning as the proof of Lemma 11, Claim C1 is true.\nClaims C2 and C3 follows the same reasoning as the proof for the base case.\nLemma 11. If i is in \u0393, then Li has favorable bias. That is, for any hypothesis h,\nerr(h, LDi )\u2212 err(h\u2217, LDi ) \u2264 err(h, Li)\u2212 err(h\u2217, Li).\nProof. We can split LDi into two subsets, the subset where L D i agrees with Li and the subset Q D i = {(x, y) \u2208 LDi : h \u2217(x) 6= y} where LDi disagrees with Li. On the former subset, LDi is identical to Li, thus we just need to show that err(h,QDi )\u2212 err(h\u2217, QDi ) \u2264 err(h,Qi)\u2212 err(h\u2217, Qi),\nwhere Qi = {(x, y) : (x,\u2212y) \u2208 QDi }. Since err(h\u2217, QDi ) = 1 and err(h\u2217, Qi) = 0, this reduces to showing that err(h,QDi ) \u2264 1 + err(h,Qi), which is easily seen to hold for any h as err(h,QDi ) \u2264 1 and err(h,Qi) \u2265 0."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We investigate active learning with access to two distinct oracles: Label (which is standard) and Search (which is not). The Search oracle models the situation where a human searches a database to seed or counterexample an existing solution. Search is stronger than Label while being natural to implement in many situations. We show that an algorithm using both oracles can provide exponentially large problem-dependent improvements over Label alone.", "creator": "LaTeX with hyperref package"}}}