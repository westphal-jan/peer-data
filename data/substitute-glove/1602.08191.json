{"id": "1602.08191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "DeepSpark: A Spark-Based Distributed Deep Learning Framework for Commodity Clusters", "abstract": "The continue logic of deep neural networks (DNNs) has made did challenging taken weaknesses additionally large - impact data any crossings for insufficient massive data and physiological involved in DNN field. Distributed desktop operating including GPGPU - established thrust specific rather mostly possibility leave meant optimisation make. In called paper, cannot measures DeepSpark, a available and sequence seen learn fully yet these inspiration Apache Spark including large - component previously signal medical and Caffe two GPU - it speeds. DeepSpark directly willing Caffe variable modifying, for delicate compatibility only existing designs brought hub structures. To would simultaneously naval, DeepSpark automatically sonatrach outgoings and parameters time Caffe - put multiply into Spark others hydrolyzed aggregates warfare reasons but a novel block - on asynchronous variant but the successful layering averaging calculus gradient descent (SGD) photo means, effectively complementing the 50m processing surveillance while Spark. DeepSpark is part down - because private, and soon new february become offering at", "histories": [["v1", "Fri, 26 Feb 2016 04:18:21 GMT  (3754kb,D)", "http://arxiv.org/abs/1602.08191v1", null], ["v2", "Tue, 8 Mar 2016 08:32:16 GMT  (3755kb,D)", "http://arxiv.org/abs/1602.08191v2", null], ["v3", "Sat, 1 Oct 2016 02:44:07 GMT  (439kb,D)", "http://arxiv.org/abs/1602.08191v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hanjoo kim", "jaehong park", "jaehee jang", "sungroh yoon"], "accepted": false, "id": "1602.08191"}, "pdf": {"name": "1602.08191.pdf", "metadata": {"source": "CRF", "title": "DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility", "authors": ["Hanjoo Kim", "Jaehong Park", "Jaehee Jang", "Sungroh Yoon"], "emails": ["sryoon@snu.ac.kr", "uwanggood@snu.ac.kr", "hukla@snu.ac.kr", "sryoon@snu.ac.kr", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS Concepts \u2022Networks \u2192 Cloud computing; \u2022Theory of computation\u2192Distributed computing models; \u2022Computing methodologies \u2192 Neural networks;\nKeywords distributed computing; deep learning; asynchronous SGD"}, {"heading": "1. INTRODUCTION", "text": "Deep neural networks (DNNs) continue to push the boundaries of their application territories. For instance, convolutional neural networks (CNNs) have become the de facto standard method for image/object recognition in computer vision [1]. Other types of DNNs have also shown outstanding\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD \u201916 August 13\u201317, 2016, San Francisco, CA, USA c\u00a9 2017 ACM. ISBN xxx-xxxx-xx-xxx/xx/xx. . . $15.00\nDOI: xx.xxx/xxx x\nperformance in various machine learning problems including speech recognition [2] and image classification [1, 3].\nDNNs deliver a sophisticated modeling capability underpinned by multiple hidden layers (e.g., a recent CNN model called ResNet has over 150 layers [1]), which effectively provide intermediate representations of the original input data. Leveraged by this, DNNs are able to better handle complications in machine learning problems than previous techniques. Although having multiple hidden layers allows DNNs to have powerful non-linear modeling capability, training such DNNs generally requires a large volume of data and a huge amount of computation resources for training. This leads to the training time ranging from several hours to days even with GPGPU-based acceleration [1, 3, 4, 5].\nIn order to improve the efficiency of deep learning training, a variety of approaches have been proposed. Highly optimized GPGPU implementations have significantly shortened the time spent in training DNNs, often showing dozens of times speedup [6, 7]. However, acceleration on a single machine has limitations due to the limited resources such as the host machine\u2019s GPU memory or main memory [5]. To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12]. Those examples exploit data parallelism and/or model parallelism and can potentially provide scalability on deep learning training.\nSeamless integration of DNN training with existing data processing pipelines is also an important practical point. Many real-world datasets used for DNN training (such as raw images or speech signals) need to be converted into a trainable format on deep learning platforms and often require preprocessing to improve robustness [5, 13]. The data for training DNNs are typically huge in scale, which makes the preprocessing procedure demand a great deal of time and resources and require carefully designed software to process. SparkNet [14] was motivated by this and combined deep learning algorithms with existing data analytic pipelines on Apache Spark [15]. Nevertheless, as Spark was not originally devised for DNN training, it lacks useful (and often inevitable) techniques and optimizations. For instance, Spark underperforms on the jobs that require updating shared parameters in asynchronous manner, which is the general scheme of distributed deep learning systems [16].\nTo address the limitations of existing approaches, we propose DeepSpark, a new deep learning framework on Spark for addressing the issues encountered in large-scale data handling and accelerating DNN training. Specifically, our contributions include the following:\nar X\niv :1\n60 2.\n08 19\n1v 1\n[ cs\n.L G\n] 2\n6 Fe\nb 20\n16\n1. Seamless integration of scalable data management capability with deep learning: We implemented our deep learning framework interface and the parameter exchanger on Apache Spark, which provides a straightforward but effective data parallelism layer.\n(a) Enhancing both training and communication using asynchrony: We implemented an asynchronous stochastic gradient descent (SGD) method for better DNN training in Spark. We implemented an adaptive variant of the elastic averaging SGD (EASGD), which accelerated parameter updates and improved the overall convergence rate.\n(b) Adopting the handy and qualified Caffe engine: To integrate the widely used Caffe framework [17] into Spark, we designed a novel Java Caffe wrapper. Using this interfacing component, users can continue to use their existing Caffe models without making numerous adjustments to integrate DeepSpark with their analysis pipeline.\n2. Experimental evaluations that demonstrate the effectiveness of our system scaling and asynchrony for expediting DNN training: We tested DeepSpark with popular benchmarks including CIFAR-10 [18] and ILSVRC 2012 ImageNet [19] datasets and observed consistent speedups over existing scale-up and scale-out approaches.\n3. Availability: The proposed DeepSpark library is freely available at http://deepspark.snu.ac.kr."}, {"heading": "2. RELATED WORK", "text": ""}, {"heading": "2.1 Deep learning training", "text": "Training deep neural network is composed of two steps, feed-forward and back propagation [20]. Feed-forward produces output using previous activations and hidden parameters. Then the error compared to the target is computed at the output classifier and is back propagated to the previous layer through the entire network. Optimization is executed during back propagation step in order to better approximate target distribution. This process is iterated until the end of training. Although complex neural network model successfully approximates input data distribution, it inherently leads to a large amount of parameters to learn. This accompanies huge amount of time and computational resources, which is one of the major concerns in deep learning research.\nWorkers Driver node\nload\nRaw data RDD\npreprocessfilter(), map(), \u2026\n#1 #2 #3 #4\nProcessed RDD\nrepartition()\nRepartitioned RDD\nrepartition\nLocal Storage\nLocal Storage\nLocal Storage\nLocal Storage\nspill to local foreach()\n1 2 3 4 Dummy RDD\n1 3 2 4 parallelize\ntrainingforeachPartition\nParameter Exchanger\nCaffe Trainer\nCaffe Trainer\nCaffe Trainer\nCaffe Trainer\nload\nTrained model\nData source (e.g. HDFS)\nFigure 2: Spark workflow of DeepSpark learning process"}, {"heading": "2.2 Stochastic Gradient Descent", "text": "Gradient descent is the most commonly used optimization method for training deep neural network practically [21]. Minibatch stochastic gradient descent (SGD) [22] is the sequential variant of the batch gradient descent in that it only takes the gradient of the randomly sampled data at each update rather than the gradient of the whole data. Due to its sequential property, minibatch SGD is more suited to process large datasets compared to its batch version. In deep learning settings, the optimization objective is usually given as follows:\nL(w) = 1 N N\u2211 i=1 Jw(x i) + \u03bbReg(w) (1)\nwhere w is the parameters to learn, N is the minibatch size of SGD, i is the index of sampled data, Jw(x\ni) is the loss on sample xi, Reg(w) is the regularization term [22] and \u03bb is the associated regularization weight. The optimization objective L(w) basically consists of average loss on data mini batch and the regularization term. Then the parameter update\nrule by SGD is given by\nw := w \u2212 \u03b7\u2207L(w) (2)\nwhere \u03b7 is the learning rate of the network. We added a momentum factor in this equation. Momentum factor parameterizes the amount of previous update reflected in the current update so that the training can converge faster. With the momentum factor \u00b5 and weight update v, the SGD update rule is modified as follows:\nv = \u00b5v + \u03b7\u2207L(w) w := w \u2212 v\n(3)\nIn addition, since the high dimensionality of DNN parameter space reduces the risk of being trapped in local minima [23], thus SGD generally runs appropriately in optimizing DNN parameters.\n2.3 Distributed Deep Learning\nA na\u0308\u0131ve parallelization SGD can be implemented by splitting batch calculations over multiple worker nodes [24, 25]. Globally shared parameters are initialized and broadcast from master and each worker derives gradients from its local data. Since the throughput of each node can differ, we can think of two strategies to parallelize gradient computing: synchronized or non-synchronized.\n2.3.1 Asynchronous SGD Synchronous SGD waits for every worker node to finish its\ncomputation and reach the barrier. Once all worker nodes has completed their tasks, the master node collects all the gradients, averages them and applies them to the center parameter. Worker nodes then pull the updated parameters from the master. While synchronous SGD is the most straightforward form of parallelizing SGD, its performance is highly degraded by the slowest worker. It also suffers from network overhead while aggregating and broadcasting network parameters.\nAsynchronous SGD has been suggested to resolve the inefficiency caused by the synchronous barrier locking. In the lock-free asynchronous SGD, each worker node independently communicates with the central parameter server without waiting for other nodes to finish. This can give rise to stale gradients, but it is theoretically and empirically investigated that asynchronous SGD can converge faster than the SGD on a single machine. [9, 26, 27]\n2.3.2 Parameter Server The notion of parameter server is a framework aiming for\nlarge scale machine learning training and inference. [8, 10] It has master slave architecture, while data and tasks are distributed over workers and server nodes manage the global parameters. Communication between nodes is performed asynchronously and the framework provides fault tolerance and flexible consistency over independent tasks. In previous works, distributed parameter server has been successfully used in training various machine learning algorithms such as logistic regression and latent Dirichlet allocation [28] on petabytes of real data.\n2.3.3 Reducing communication overhead The distributed training of deep neural networks consists\nof two steps: exploration and exploitation. [27] The former is to explore the parameter space to find the optimal weight parameters and the latter is to update center parameter weights using local worker\u2019s training results and proceed to the next step. In distributed settings, since the parameter exchanging causes\nnetwork overhead, there exists a performance tradeoff between worker\u2019s exploration and master\u2019s exploitation. SparkNet presented the iteration hyperparameter \u03c4 , which is the number of processed minibatches before the next communication [14]. Distributed training system can benefit from large value of \u03c4 under the high cost communication scenario by reducing communication overhead. However, large \u03c4 may end up requiring more iterations to attain convergence, which slows down the learning process. [14, 27]\nZhang et al. (2015) suggested the elastic averaging SGD (EASGD) strategy to maximize the benefit of exploring. [27] In EASGD, master and local workers exchange their weight parameters like the original SGD methods. When updating parameters, however, they compute elastic difference be-\nAlgorithm 1 Pseudo-procedure of DeepSpark for p workers and master node 1: Input: communication period \u03c4 , iterations imax, learn-\ning rate \u03b7, data D on HDFS 2: 3: // Data preprocessing and partitioning 4: RDDD = Load From HDFS(D) 5: RDDprocessed = Preprocess(RDDD) 6: PartitionD = Repartition(RDDprocessed, k) 7: 8: // Initialize for worker node p 9: Dp = spill To Local(PartitionD),\n10: 11: // a variant of asynchronous EASGD update 12: parameter weights xk = xmaster 13: for i = 0 to imax do 14: xk = xk \u2212 \u03b7g(x(i)k ;Dk) 15: for every \u03c4 iteration do 16: \u03b1i = modify Moving Rate(\u03b10, i) 17: EASGD Update(xk, xmaster, \u03b1i) 18: end for 19: end for\ntween them and apply on both master and worker weight parameters. To compute the elastic force, moving rate \u03b1 is involved. At every communication, each worker and the master node update their parameter weights as follows 4.\nwworker = wworker \u2212 \u03b1(wworker \u2212 wmaster) wmaster = wmaster + \u03b1(wworker \u2212 wmaster)\n(4)\nThis method is different from downpour SGD [9], where gradients of local workers are shipped to the master and updated center parameters are sent back to workers at every update. EASGD shows faster convergence of the training even with large value of \u03c4 , with which downpour SGD shows slow convergence rate or even cannot converge. [27]"}, {"heading": "2.4 Apache Hadoop and Spark", "text": "Apache Hadoop YARN [29] and Spark [15] are cluster frameworks that allows large scale data processing on commodity hardware. In Hadoop, data sets are split into multiple blocks in Hadoop Distributed File System (HDFS) [30]. HDFS provides the overall control of these blocks and maintains fault tolerance. Hadoop YARN, which is the framework for resource management and job monitoring, is responsible for containers working in parallel.\nSpark is the cluster computing engine running on YARN, Apache Mesos or EC2. The core of Spark is the in-memory distributed processing using resilient distributed dataset (RDD). RDD is the read-only collection of data partitioned across a set of machines. On RDD, parallel actions are performed to produce actual outcome or transformations can be applied in order to convert RDD into other type of RDD. For further reuse, RDD can be cached in cluster memory, which prevents unnecessary storage I/O and thus accelerates data processing. Hadoop and Spark are originally built for batchsynchronized analysis on large data. They are, however, less suited for jobs that requires asynchronous actions on parallel workers [16].\n3. PROPOSED DEEPSPARK FRAMEWORK"}, {"heading": "3.1 Motivations", "text": "Apache Spark is an attractive platform for data-processing pipelines such as a database query processing. However, Spark RDD is well suited mostly for batch synchronous actions, providing limited asynchronous operations between the master and workers.\nAddressing Spark\u2019s disadvantages, we implemented a new asynchronous SGD solver with a custom parameter exchanger on the Spark environment. To improve asynchronous performance, we noticeably improved the elastic averaging stochastic gradient descent algorithm (EASGD) [27] by considering adaptive parameter updates, delivering faster convergence."}, {"heading": "3.2 Structure overview", "text": "DeepSpark consists of three main parts, which are Apache Spark, a parameter exchanger for asynchronous SGD, and the Caffe software, as shown in Figure 1. Apache Spark manages workers and available resources assigned by resource manager.\nFigure 2 depicts how the Spark workflow progresses for asynchronous SGD, and we give more detailed descriptions in Section 3.3. Subsequently, we explain the parameter exchanger and asynchronous SGD process exploiting Spark in Sections 3.4\u20136 and Figure 3. We clarify how to integrate Caffe as our SGD computing engine with Spark using Java Native Access (JNA)1 interface in Section 3.7. The overall procedure of DeepSpark is summarized in Algorithm 1. We assumed that DeepSpark runs on Hadoop YARN [29] and the Hadoop distributed file system (HDFS) [30] in the following explanation of our workflow."}, {"heading": "3.3 Distributed setup for Spark", "text": "In this section, we explain that DeepSpark\u2019s distributed workflow from the data preparation to asynchronous SGD, which is corresponding to lines 3\u20139 in Algorithm 1 and from load to spilling phase in Figure 2. Since DeepSpark is running on top of Spark framework, it needs to load and transform data in form of Spark RDD [16].\nThe first step of DeepSpark training is to create RDD for training and inference. We defined a container class, which stores label information and corresponding data for each sample. Data specific loader then creates the RDD of this data container class, which is followed by preprocessing phase. In the preprocessing phase, data containers connected to the preprocessing pipeline such as filtering, mapping, transforming, or shuffling. The processed RDD repartitioning is, then, performed in order to make the number of partitions matches the number of worker executors.\nCaffe, the actual computing engine, however, cannot access RDD in a direct way. In DeepSpark, whole dataset is distributed across all workers and each worker is able to cache the dataset it holds or convert it into LMDB2 file format if the data is relatively larger than the memory size.\nFor relatively small dataset, RDD foreachPartition action is executed, where every data partition is loaded in local worker\u2019s memory as a list. This data then becomes available by Caffe\u2019s neural network model using memory data layer in Caffe library. In this case, we need to set the batch size, the number of channels, image width and height according to the Caffe memory data layer specification.\n1https://github.com/java-native-access/jna 2http://lmdb.readthedocs.org/en/release/\nThe other way to feed the data into Caffe is spilling the dataset on worker node\u2019s storage. For large dataset that is hard to fit in cluster memory, RDD foreach operation is performed and each data partition is converted to LMDB file format and stored in temporary local repository of the node it belongs to. Once the LMDB files are created, Caffe compute data dimension automatically and finally complete neural network model parameter. We used LMDB JNI3 to manipulate LMDB on Spark."}, {"heading": "3.4 Asynchronous EASGD operation", "text": "Inherently, Spark does not support step-wise asynchronous operations for asynchronous SGD updates. We adopt the method exploiting Spark RDD operations for overcoming the limitation of Spark. It is possible to mimic the asynchrony by the dummy RDD represented in Figure 2.\nOnce the LMDB local repository for each worker have been prepared, dummy RDD is created and distributed across every workers. This dummy data has a particular role to launch distributed action where parallel model training is performed. This exploits the property of Spark that Spark scheduler reuses the pre-existed worker node session. The size of dummy RDD is set to the number of workers explicitly for full parallel operation and foreach action is executed on this dummy RDD. Inside foreachPartition process, each worker is able to use local data repository which was created in the previous job and starts training step.\n3https://github.com/chirino/lmdbjni\nDuring training process, Spark driver program serves as a central parameter exchanger, which performs asynchronous EASGD update. At the initial step, Driver program broadcasts its network address and neural network setup files to all workers. Each worker then creates their own model and starts training using broadcasted data.\nThe hyper parameters for learning neural network is basically identical to that of Caffe. This includes learning rate, learning rate policy, momentum, the number of max iterations and etc."}, {"heading": "3.5 Adaptive update rules", "text": "In this section, we explain two variants EASGD update rules for fast convergence. For model parameters in master node, the moving rate \u03b1 acts like the learning rate for sequential SGD process. As the adaptive learning rate policy [31] in the single node SGD process, we expect to improved the converge rate of training result during training process by adjusting the moving rate adaptively. In DeepSpark framework, we also explored the effect of modifying \u03b1 during training. Equation 5 describes our implementation to decay \u03b1 with power rate \u03c1 when the update count t arrives at each step size s.\n\u03b1t = \u03b10\u03c1 bt/sc (5)\nAs the training proceeds, the elastic force between master and workers gradually decreases. We expected that this helps to converge the training with more stability. Option-\nally, it is possible to stop decay \u03b1 by setting decay count limit.\nFurthermore, we study the effect of asymmetric moving rate between workers and master. This is to ensure that the master\u2019s training convergence is not interfered with worker\u2019s ill-posed exploration after a specific step. In our implementation, we first start with the same moving rate for both master and workers. The moving rate for workers keep identical throughout the training while for master, however, the moving rate gradually decreased at the left iterations. Equation 6 describes the asymmetric updates between master and workers. The decaying moving rate for master follows the same process as Equation 5.\nwworker = wworker \u2212 \u03b1(wworker \u2212 wmaster) wmaster = wmaster + \u03b1 \u2032 (wworker \u2212 wmaster)\n(6)"}, {"heading": "3.6 Parameter exchanger", "text": "The parameter exchanger is the DeepSpark implementation of parameter server concepts, which is essential for asynchronous update. In DeepSpark, the application driver node serves as the parameter exchanger to enable worker nodes to update their weights asynchronously. Figure 3 represents the outline of learning cycle with parameter exchanger. The\ndriver node starts the parameter exchanger as a separate thread before worker nodes begin training the model.\nFor when there are multiple weight exchange requests from worker nodes, we implemented a thread pool to handle the requests at the same time. For each connection request, the thread pool allocates the pre-created threads that process the weight exchange requests. The size of the thread pool is fixed in the program and we set this up to eight threads because of limited memory and network bandwidth. If the number of requests exceeds the size, the unallocated requests wait in a queue until the preceding requests are finished as shown in Figure 3(a).\nExchange threads asynchronously access to and update the neural net model in the parameter exchanger according to EASGD algorithm. In figure 3(b), since it is a lockfree system, weights can be overwritten by simultaneous updates. Nevertheless, training results accumulate successfully as proved in [32]. After the parameter exchange action, each worker returns to SGD phase to exploring the parameter space asynchronously, as shown in Figure 3(c)."}, {"heading": "3.7 SGD engine", "text": "In DeepSpark, each worker node uses Caffe library as the GPU accelerated SGD engine. However, Spark application is written in java, scala and python, which are not able to\nuse native Caffe library directly in source code level. We implemented our code with JNA so that Spark executors are able to reference Caffe native library.\nTo parallelize the Caffe models, the Caffe model parameters should be possible to access to read and write. The original SGDSolver class of Caffe is not provide interface for that, so we derived an custom Solver class from the SGDSolver. We defined some operations of the derived Solver class in order to perform an atomic iteration action, acquire current trained parameter weights , and modify in parameter weights. This custom solver class provides an interface to control Caffe library for the DeepSpark application. Therefore, we can use current Caffe models in a distributed environment without changing the Caffe network specifications a lot. The line 16 in Algorithm 1 is corresponding to the SGD engine operation."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1 Experimental setup", "text": "We prepared a single-machine environment and a distributed\ncluster environment. The distributed cluster shown in Figure 9 comprised of 25 identical machines and one of them was used for the experiments on a single machine. Each machine had Intel Core i7-4790 processor with 16GB of main memory and an NVIDIA GTX970 GPU with 4GB memory and they communicated with each other via Gigabit Ethernet connections. We did not examined only DeepSpark on the cluster,\nbut also Caffe [17] (single machine) and SparkNet (cluster), as a sequential and parallel opponents, respectively.\nTo manage each node, we set up a separate Hadoop server machine as well. This server machine did not take part in computing but only ran YARN which performs as a manager of cluster nodes and HDFS namenode daemon."}, {"heading": "4.2 Dataset and neural network models", "text": "To observe the advantages of parallelization in terms of data scalability, we used two datasets: CIFAR10 and ImageNet.\n4.2.1 CIFAR10 The CIFAR10 dataset contains 60,000 images with 32x32\nsize, 3 channels, and 10 classes [18]. It consists of 50,000 images for training and 10,000 images for test.\nOn this CIFAR10 dataset, we trained a convolutional neural network model which is a variant of Cuda-convnet [5]. In this model, there are 3 sets including a convolution layer (5x5, stride 1), a pooling layer, and a rectified linear unit (ReLU) activation layer, followed by a fully connected layer and a softmax layer to calculate output (128 feature maps). This model achieved \u223c 75% test accuracy.\n4.2.2 ImageNet ILSVRC 2012 ImageNet dataset consists of 1,281,167 color\nimages with 1,000 classes of different image concepts [19]. To reduce the experiment time, we composed a training set from ImageNet which incudes 383,690 images of 300 classes and a validation set with images that were not used in training. As pre-processing, we unified the size of images (256x256), converted them into a readable format for Apache Spark and saved them on HDFS.\nTo train the ImageNet dataset, we used a Caffe model which is a replication of Google\u2019s GoogLeNet [4]. GoogLeNet is a 22-layer deep network made with Hebbian principle and multi-scale processing intuition with 9 of its Inception modules for classification. The replica GoogLeNet Caffe model, however, has several differences: it does not train with the relighting and scale/aspect-ratio data-augmentation and its weight initialization method is Xavier [33] instead of Gaussian."}, {"heading": "4.3 Experimental results", "text": "4.3.1 Training on CIFAR10 We examined the time that a single Caffe machine and the\ndistributed DeepSpark and SparkNet cluster took to reach target accuracies (.72, .73, .74, .75) for different numbers of nodes and communication period \u03c4 . For all experiments in this section, we set learning rate \u03b7 = 0.001, momentum \u03b4 = 0.9, weight decay as 0.004 and maximum iterations up to 20,000.\nFrom the results shown in Table 1 and Figure 6, we first compared DeepSpark cluster with Caffe and the result is shown in Table 1. From the results shown in Table 1, we confirmed about 66% of the experiment cases converged faster than Caffe, excluding ones that were not able to achieve the target accuracy within maximum iterations. The best speedup case for .75 test accuracy took 28% less time than Caffe, when there were 8 nodes and \u03c4 = 40. Figure 6 illustrates test accuracy versus wall-clock time on Caffe (single), DeepSpark and SparkNet clusters. DeepSpark had a\nsimilar converging tendency to Caffe, which was faster than SparkNet with 25% higher accuracies in average. We could not see SparkNet terminate for large \u03c4s (\u03c4 \u2265 40).\n4.3.2 Training on ImageNet We observed the training tendencies on ImageNet of three\ndifferent methods: a single Caffe machine and the distributed DeepSpark and SparkNet cluster. All the experiments were done in the same configuration, which includes 100,000 maximum iterations, learning rate \u03b7 = 0.05, and weight decay 0.002. Figure 7 draws the accuracies of 3 outputs versus training time. As it is shown in Figure 7, DeepSpark converged faster and more accurately than Caffe on a single node. Among the 3 outputs, output 3 showed the highest accuracy. For the maximum achievements of single node Caffe within the same wall-clock time, DeepSpark showed speed up by 1.62. Finally, DeepSpark attained higher test accuracy by 13% in average against Caffe on single node at full iterations.\n4.3.3 Training with different moving rates For all experiments in this section, we applied moving\nrates on the same experiment setup as the best speedup case of .75 test accuracy from 4.3.1 which includes 8 nodes and communication period \u03c4 = 40. First we used different fixed moving rates from the following set \u03b1 \u2208 {0.1, 0.3, 0.5, 0.7, 0.9, 0.99} and the accuracies and speedups are depicted in Figure 4 and 5, respectively. Figure 4 have blank space because experiments with low moving rates (\u03b1 \u2264 0.3) converged before reaching the target accuracies as it is shown in Figure 4. As \u03b1 grew, the final test accuracies increased but converged soft.\nFinally we conducted additional experiment with adaptive moving rates. We set the initial moving rate \u03b10 as 0.99 and\nchanged the value of \u03b1 at 6 different time points when 1k, 3k, 6k, 8k, 12k and 16k iterations were done. When adapting \u03b1, we employed two schemes which are decaying and asymmetric. In Figure 8 it can be learned that if we change \u03b1 early, the model converges faster but less accurately. If \u03b1 is changed after the model converged, it didn\u2019t make any significant change or even hindered learning. DeepSpark achieved the best 1.97x speedup with decaying \u03b1 at 6,000th iteration."}, {"heading": "5. DISCUSSION", "text": ""}, {"heading": "5.1 Speedup analysis", "text": "DeepSpark achieved high speedup compared to the sequential Caffe and the distributed SparkNet by the virtue of asynchronous adaptive EASGD which alleviate communication overhead, and the distributed settings which reduces disk I/O.\nWe witnessed that communication overhead played a crucial role in slowdown. According to SparkNet [14], speedup of parallelization versus a single sequential machine is given as the following Equation 7, where Na(b) and Ma(\u03c4, b,K) are the required number of iterations to achieve accuracy level a for sequential and parallel, b is the size of minibatch, C(b) is time for a batch computation, S is the communication overhead, and K represents the number of nodes.\nSpeedUp = Na(b)C(b)\n(\u03c4C(b) + S)Ma(\u03c4, b,K) (7)\nWe attempted to relax communication overhead S by asynchronous update and succeeded in accelerating DNN training. The time DeepSpark spent to aggregate and broadcast was less than 1/10 of the time SparkNet spent. Taking CIFAR10 training on DeepSpark (8 nodes, \u03c4 = 40) for example, the time spent in parameter exchange is 2x - 5x of the time spent in minibatch training. Parameter exchanging period can grow bigger as we increase the number of nodes and this affects the overall training time. We observed this in Table 1, where the speedup for target accuracy .75 with \u03c4 = 40 dropped as the number of nodes grew from 8 to 16.\nIn addition, the adaptive EASGD update rules relieved the share of S even more, while suppressing the increase in Ma(\u03c4, b,K) [27], and we confirmed this from the results in Figure 8.\nFor the dataset which is too large to hold in memory, the distributed environment was helpful for accelerating by reducing the disk operation load. Disk I/O overhead transforms Equation 7 into the following Equation 8 for large dataset.\nSpeedUp = Na(b)C(b) + L(|D|)\n(\u03c4C(b) + S)Ma(\u03c4, b,K) + L(|D|/K) (8)\nwhere L(|D|) implies the disk I/O overhead for the size of whole dataset D. In the distributed system, dataset is divided into K shards, and the shared data can arouse reducing disk I/O. In the ImageNet result, the disk I/O overhead was estimated approximately an hour for 32 batch size and 100k iterations in a single node. The overhead decreased by 42% for 8 nodes.\n5.2 Moving rate\nInterestingly, higher test accuracy is acquired from higher moving rate setting as shown in Figure 4. On the other hand, the models with low moving rate tended to converge more softly. And as shown in the Figure 8, adaptive adjusting moving rate \u03b1 was effective to improve convergence.\nIf we set the initial moving rate \u03b10 high, model parameters of the master node can escape from the local optima near the initial model parameter. As the training progresses, however, the master node should wobble less to converge, and lower elasticity between the master node and worker nodes represented by decayed \u03b1 can be helpful.\nAsymmetrically adapted moving rate also showed positive effects on convergence but with less stability. Figure 8 implies that asymmetric \u03b1 may caused degradation and lability of the convergence."}, {"heading": "5.3 Data spilling", "text": "To implement asynchronous EASGD, the scheduling that Spark provides had to be sacrificed. Originally, Spark operates data in memory, which leverages performance up to 100 times faster for certain applications [34] and if data is bigger than memory, Spark stores lingering data in disk and schedules operations. Nevertheless, to run asynchronous operations which are not suitable for Spark, we mimicked the spilling process of register allocation and spilled data into local storage.\nTime delay caused by disk operations however, didn\u2019t affect overall performance of DeepSpark. On 8 worker nodes, it spent approximately a few seconds and 8 minutes to spill CIFAR10 and ImageNet dataset, respectively. These delays accounted for only below 2% of the entire running time, which were negligible."}, {"heading": "5.4 Further improvements", "text": "There remains room for further improving DeepSpark by reducing the network overhead. Our analysis assumes that the weight parameter serialization during network communication could have caused additional network overhead. In our experiments, we used the basic serialization method on Java for convenience and optimized object creation speed. The basic object serialization process in Java is known to be inferior to many other advanced libraries [35]. By replacing the serialization procedure with more sophisticated libraries, we expect the reduction in object de/serialization time and the size of serialized data. Furthermore, changing network protocols may also be helpful for additional performance boosts. For convenience and reliability, our asynchronous update communication structure was based on TCP sockets. By replacing the current communication structure by the UDP datagram communication [36], additional speedups may become possible. Although the reliability of the UDP datagram is inherently not guaranteed, the use of local (usually reliable) networks and the robustness of the asynchronous EASGD can make it irrelevant to DNN training."}, {"heading": "6. CONCLUSION", "text": "We have described our new deep learning framework named DeepSpark, which provides seamless integration with existing large-scale data processing pipelines as well as highly accelerated DNN training procedure. DeepSpark is an example of successful combination of diverse components including Apache Spark, asynchronous parameter updates, and\nGPGPU-based Caffe framework. According to our experiments with popular benchmarks, DeepSpark demonstrated its effectiveness by showing faster convergence than the alternative parallelization schemes compared."}], "references": [{"title": "et al", "author": ["K. He"], "venue": "Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["J.K. Chorowski"], "venue": "Attention-based models for speech recognition. In NIPS, pages 577\u2013585", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "et al", "author": ["C. Szegedy"], "venue": "Going deeper with convolutions. In CVPR, pages 1\u20139", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["A. Krizhevsky"], "venue": "Imagenet classification with deep convolutional neural networks. In NIPS, pages 1097\u2013 1105", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["S. Chetlur"], "venue": "cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": "arXiv preprint arXiv:1404.5997", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["M. Li"], "venue": "Scaling distributed machine learning with the parameter server. In OSDI, pages 583\u2013598", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["J. Dean"], "venue": "Large scale distributed deep networks. In NIPS, pages 1223\u20131231", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["Q. Ho"], "venue": "More effective distributed ml via a stale synchronous parallel parameter server. In NIPS, pages 1223\u20131231", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["E.P. Xing"], "venue": "Petuum: A new platform for distributed machine learning on big data. In SIGKDD, KDD \u201915, pages 1335\u20131344, New York, NY, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["B.C. Ooi"], "venue": "Singa: A distributed deep learning platform. In Proceedings of the ACM International Conference on Multimedia, pages 685\u2013688. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["J.T. Geiger"], "venue": "Investigating nmf speech enhancement for neural network based acoustic models. In IN- TERSPEECH, pages 2405\u20132409", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["P. Moritz"], "venue": "Sparknet: Training deep networks in spark. arXiv preprint arXiv:1511.06051", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["M. Zaharia"], "venue": "Spark: cluster computing with working sets. In Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing, volume 10, page 10", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["M. Zaharia"], "venue": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation, pages 2\u20132. USENIX Association", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["Y. Jia"], "venue": "Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675\u2013678. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "et al", "author": ["O. Russakovsky"], "venue": "ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211\u2013252", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Theory of the backpropagation neural network", "author": ["R. Hecht-Nielsen"], "venue": "IJCNN, pages 593\u2013605. IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "et al", "author": ["Y. LeCun"], "venue": "Deep learning. Nature, 521(7553):436\u2013 444", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Pattern Recognition & Machine Learning", "author": ["Y. Anzai"], "venue": "Elsevier", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["Y.N. Dauphin"], "venue": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, pages 2933\u20132941", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["C.H. Teo"], "venue": "A scalable modular convex solver for regularized risk minimization. In SIGKDD, pages 727\u2013 736. ACM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "et al", "author": ["M. Zinkevich"], "venue": "Parallelized stochastic gradient descent. In NIPS, pages 2595\u20132603", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["X. Lian"], "venue": "Asynchronous parallel stochastic gradient for nonconvex optimization. In NIPS, pages 2719\u2013 2727", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["S. Zhang"], "venue": "Deep learning with elastic averaging sgd. In NIPS, pages 685\u2013693", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["D.M. Blei"], "venue": "Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u20131022", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "et al", "author": ["V.K. Vavilapalli"], "venue": "Apache hadoop yarn: Yet another resource negotiator. In Proceedings of the 4th Annual Symposium on Cloud Computing, page 5. ACM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["K. Shvachko"], "venue": "The hadoop distributed file system. In MSST, pages 1\u201310. IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["R.A. Jacobs"], "venue": "Neural networks, 1(4):295\u2013 307", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1988}, {"title": "et al", "author": ["B. Recht"], "venue": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In NIPS, pages 693\u2013701", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot"], "venue": "In AISTATS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "et al", "author": ["R.S. Xin"], "venue": "Shark: Sql and rich analytics at scale. In Proceedings of the 2013 ACM SIGMOD International Conference on Management of data, pages 13\u201324. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparison of data serialization formats for optimal efficiency on a mobile platform", "author": ["A. Sumaray"], "venue": "In IMCOM,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Profiling and reducing processing overheads in tcp/ip", "author": ["J. Kay"], "venue": "IEEE/ACM TON,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "For instance, convolutional neural networks (CNNs) have become the de facto standard method for image/object recognition in computer vision [1].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "xxx/xxx x performance in various machine learning problems including speech recognition [2] and image classification [1, 3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "xxx/xxx x performance in various machine learning problems including speech recognition [2] and image classification [1, 3].", "startOffset": 117, "endOffset": 123}, {"referenceID": 2, "context": "xxx/xxx x performance in various machine learning problems including speech recognition [2] and image classification [1, 3].", "startOffset": 117, "endOffset": 123}, {"referenceID": 0, "context": ", a recent CNN model called ResNet has over 150 layers [1]), which effectively provide intermediate representations of the original input data.", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "This leads to the training time ranging from several hours to days even with GPGPU-based acceleration [1, 3, 4, 5].", "startOffset": 102, "endOffset": 114}, {"referenceID": 2, "context": "This leads to the training time ranging from several hours to days even with GPGPU-based acceleration [1, 3, 4, 5].", "startOffset": 102, "endOffset": 114}, {"referenceID": 3, "context": "This leads to the training time ranging from several hours to days even with GPGPU-based acceleration [1, 3, 4, 5].", "startOffset": 102, "endOffset": 114}, {"referenceID": 4, "context": "This leads to the training time ranging from several hours to days even with GPGPU-based acceleration [1, 3, 4, 5].", "startOffset": 102, "endOffset": 114}, {"referenceID": 5, "context": "Highly optimized GPGPU implementations have significantly shortened the time spent in training DNNs, often showing dozens of times speedup [6, 7].", "startOffset": 139, "endOffset": 145}, {"referenceID": 6, "context": "Highly optimized GPGPU implementations have significantly shortened the time spent in training DNNs, often showing dozens of times speedup [6, 7].", "startOffset": 139, "endOffset": 145}, {"referenceID": 4, "context": "However, acceleration on a single machine has limitations due to the limited resources such as the host machine\u2019s GPU memory or main memory [5].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 8, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 9, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 10, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 11, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 4, "context": "Many real-world datasets used for DNN training (such as raw images or speech signals) need to be converted into a trainable format on deep learning platforms and often require preprocessing to improve robustness [5, 13].", "startOffset": 212, "endOffset": 219}, {"referenceID": 12, "context": "Many real-world datasets used for DNN training (such as raw images or speech signals) need to be converted into a trainable format on deep learning platforms and often require preprocessing to improve robustness [5, 13].", "startOffset": 212, "endOffset": 219}, {"referenceID": 13, "context": "SparkNet [14] was motivated by this and combined deep learning algorithms with existing data analytic pipelines on Apache Spark [15].", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "SparkNet [14] was motivated by this and combined deep learning algorithms with existing data analytic pipelines on Apache Spark [15].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "For instance, Spark underperforms on the jobs that require updating shared parameters in asynchronous manner, which is the general scheme of distributed deep learning systems [16].", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "(b) Adopting the handy and qualified Caffe engine: To integrate the widely used Caffe framework [17] into Spark, we designed a novel Java Caffe wrapper.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "Experimental evaluations that demonstrate the effectiveness of our system scaling and asynchrony for expediting DNN training: We tested DeepSpark with popular benchmarks including CIFAR-10 [18] and ILSVRC 2012 ImageNet [19] datasets and observed consistent speedups over existing scale-up and scale-out approaches.", "startOffset": 189, "endOffset": 193}, {"referenceID": 18, "context": "Experimental evaluations that demonstrate the effectiveness of our system scaling and asynchrony for expediting DNN training: We tested DeepSpark with popular benchmarks including CIFAR-10 [18] and ILSVRC 2012 ImageNet [19] datasets and observed consistent speedups over existing scale-up and scale-out approaches.", "startOffset": 219, "endOffset": 223}, {"referenceID": 19, "context": "Training deep neural network is composed of two steps, feed-forward and back propagation [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "Gradient descent is the most commonly used optimization method for training deep neural network practically [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 21, "context": "Minibatch stochastic gradient descent (SGD) [22] is the sequential variant of the batch gradient descent in that it only takes the gradient of the randomly sampled data at each update rather than the gradient of the whole data.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "where w is the parameters to learn, N is the minibatch size of SGD, i is the index of sampled data, Jw(x ) is the loss on sample x, Reg(w) is the regularization term [22] and \u03bb is the associated regularization weight.", "startOffset": 166, "endOffset": 170}, {"referenceID": 22, "context": "In addition, since the high dimensionality of DNN parameter space reduces the risk of being trapped in local minima [23], thus SGD generally runs appropriately in optimizing DNN parameters.", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "3 Distributed Deep Learning A n\u00e4\u0131ve parallelization SGD can be implemented by splitting batch calculations over multiple worker nodes [24, 25].", "startOffset": 134, "endOffset": 142}, {"referenceID": 24, "context": "3 Distributed Deep Learning A n\u00e4\u0131ve parallelization SGD can be implemented by splitting batch calculations over multiple worker nodes [24, 25].", "startOffset": 134, "endOffset": 142}, {"referenceID": 8, "context": "[9, 26, 27]", "startOffset": 0, "endOffset": 11}, {"referenceID": 25, "context": "[9, 26, 27]", "startOffset": 0, "endOffset": 11}, {"referenceID": 26, "context": "[9, 26, 27]", "startOffset": 0, "endOffset": 11}, {"referenceID": 7, "context": "[8, 10] It has master slave architecture, while data and tasks are distributed over workers and server nodes manage the global parameters.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[8, 10] It has master slave architecture, while data and tasks are distributed over workers and server nodes manage the global parameters.", "startOffset": 0, "endOffset": 7}, {"referenceID": 27, "context": "In previous works, distributed parameter server has been successfully used in training various machine learning algorithms such as logistic regression and latent Dirichlet allocation [28] on petabytes of real data.", "startOffset": 183, "endOffset": 187}, {"referenceID": 26, "context": "[27] The former is to explore the parameter space to find the optimal weight parameters and the latter is to update center parameter weights using local worker\u2019s training results and proceed to the next step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "SparkNet presented the iteration hyperparameter \u03c4 , which is the number of processed minibatches before the next communication [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "[14, 27] Zhang et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[14, 27] Zhang et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[27] In EASGD, master and local workers exchange their weight parameters like the original SGD methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This method is different from downpour SGD [9], where gradients of local workers are shipped to the master and updated center parameters are sent back to workers at every update.", "startOffset": 43, "endOffset": 46}, {"referenceID": 26, "context": "[27]", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Apache Hadoop YARN [29] and Spark [15] are cluster frameworks that allows large scale data processing on commodity hardware.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "Apache Hadoop YARN [29] and Spark [15] are cluster frameworks that allows large scale data processing on commodity hardware.", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "In Hadoop, data sets are split into multiple blocks in Hadoop Distributed File System (HDFS) [30].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "They are, however, less suited for jobs that requires asynchronous actions on parallel workers [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "To improve asynchronous performance, we noticeably improved the elastic averaging stochastic gradient descent algorithm (EASGD) [27] by considering adaptive parameter updates, delivering faster convergence.", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "We assumed that DeepSpark runs on Hadoop YARN [29] and the Hadoop distributed file system (HDFS) [30] in the following explanation of our workflow.", "startOffset": 46, "endOffset": 50}, {"referenceID": 29, "context": "We assumed that DeepSpark runs on Hadoop YARN [29] and the Hadoop distributed file system (HDFS) [30] in the following explanation of our workflow.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "Since DeepSpark is running on top of Spark framework, it needs to load and transform data in form of Spark RDD [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 30, "context": "As the adaptive learning rate policy [31] in the single node SGD process, we expect to improved the converge rate of training result during training process by adjusting the moving rate adaptively.", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "Nevertheless, training results accumulate successfully as proved in [32].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "but also Caffe [17] (single machine) and SparkNet (cluster), as a sequential and parallel opponents, respectively.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "The CIFAR10 dataset contains 60,000 images with 32x32 size, 3 channels, and 10 classes [18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "On this CIFAR10 dataset, we trained a convolutional neural network model which is a variant of Cuda-convnet [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 18, "context": "ILSVRC 2012 ImageNet dataset consists of 1,281,167 color images with 1,000 classes of different image concepts [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": "To train the ImageNet dataset, we used a Caffe model which is a replication of Google\u2019s GoogLeNet [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 32, "context": "The replica GoogLeNet Caffe model, however, has several differences: it does not train with the relighting and scale/aspect-ratio data-augmentation and its weight initialization method is Xavier [33] instead of Gaussian.", "startOffset": 195, "endOffset": 199}, {"referenceID": 13, "context": "According to SparkNet [14], speedup of parallelization versus a single sequential machine is given as the following Equation 7, where Na(b) and Ma(\u03c4, b,K) are the required number of iterations to achieve accuracy level a for sequential and parallel, b is the size of minibatch, C(b) is time for a batch computation, S is the communication overhead, and K represents the number of nodes.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "In addition, the adaptive EASGD update rules relieved the share of S even more, while suppressing the increase in Ma(\u03c4, b,K) [27], and we confirmed this from the results in Figure 8.", "startOffset": 125, "endOffset": 129}, {"referenceID": 33, "context": "Originally, Spark operates data in memory, which leverages performance up to 100 times faster for certain applications [34] and if data is bigger than memory, Spark stores lingering data in disk and schedules operations.", "startOffset": 119, "endOffset": 123}, {"referenceID": 34, "context": "The basic object serialization process in Java is known to be inferior to many other advanced libraries [35].", "startOffset": 104, "endOffset": 108}, {"referenceID": 35, "context": "By replacing the current communication structure by the UDP datagram communication [36], additional speedups may become possible.", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "The increasing complexity of deep neural networks (DNNs) has made it challenging to exploit existing large-scale data process pipelines for handling massive data and parameters involved in DNN training. Distributed computing platforms and GPGPU-based acceleration provide a mainstream solution to this computational challenge. In this paper, we propose DeepSpark, a distributed and parallel deep learning framework that simultaneously exploits Apache Spark for large-scale distributed data management and Caffe for GPU-based acceleration. DeepSpark directly accepts Caffe input specifications, providing seamless compatibility with existing designs and network structures. To support parallel operations, DeepSpark automatically distributes workloads and parameters to Caffe-running nodes using Spark and iteratively aggregates training results by a novel lock-free asynchronous variant of the popular elastic averaging stochastic gradient descent (SGD) update scheme, effectively complementing the synchronized processing capabilities of Spark. DeepSpark is an on-going project, and the current release is available at http://deepspark.snu.ac.kr. CCS Concepts \u2022Networks \u2192 Cloud computing; \u2022Theory of computation\u2192Distributed computing models; \u2022Computing methodologies \u2192 Neural networks;", "creator": "LaTeX with hyperref package"}}}