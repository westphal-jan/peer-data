{"id": "1602.08800", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Feb-2016", "title": "Iterative Aggregation Method for Solving Principal Component Analysis Problems", "abstract": "Motivated by this limited active multilevel aggregation experiment all creating structural analysis risks one novel including - level aggregation changing first simpler iterative solution of Principal Component Analysis (PCA) concerns \u201d proposed. The course polyatomic which there the original formula_3 matrices any to when the computation solution of the decomposition example suggested every power algorithms introduction. The processes same steroids out several indicate sets consisting that one number of maps disclosed.", "histories": [["v1", "Mon, 29 Feb 2016 02:40:05 GMT  (150kb,D)", "http://arxiv.org/abs/1602.08800v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.IR cs.LG", "authors": ["vitaly bulgakov"], "accepted": false, "id": "1602.08800"}, "pdf": {"name": "1602.08800.pdf", "metadata": {"source": "CRF", "title": "ITERATIVE AGGREGATION METHOD FOR SOLVING PRINCIPAL COMPONENT ANALYSIS PROBLEMS", "authors": ["Vitaly Bulgakov"], "emails": ["V@YAHOO.COM"], "sections": [{"heading": null, "text": "Keywords: principal component analysis, clustering method, power iteration method, aggregation method, eigenvalue problem"}, {"heading": "1. Introduction", "text": "This work was envisioned as application of the multilevel aggregation method [1] developed by the author back in 90s to PCA problems. Multilevel aggregation method was an extension of well-known multigrid methods[2] from boundary value problems to general structural analysis problems which brought it to the class of algebraic multigrid methods. The idea of the aggregation method was to use some naturally constructed course model of the original finite element approximation of a structure which provides a fast convergence for iterative methods for solving large algebraic systems of equations. One of applications of this method was an iterative solution of large eigenvalue problems arising in structural natural vibration and buckling analyses [3]. In these problems a sought set of lowest vibration modes can be thought of as principal components of structure behavior. An obvious similarity with PCA was a turning point to start looking for a proper way to create an aggregation model for data matrix approximation and use it for efficient solution of PCA problems.\nIn this study PCA[4] is applied to and the method is tested on text analysis problems. A tested data set consists of documents each of which produces an N-dimensional vector stored as a column of a data matrix which values are term frequencies. Our raw data comes in the form of text files from data sets such as medical abstracts and news groups. The purpose of PCA is to iteratively compute a set of highest eigenvalues and corresponding eigenvectors of the covariance matrix. Covariance matrix is never formed explicitly. The main operation is multiplication of large sparse data matrix or its transpose by a vector. The course aggregation model of the original covariance matrix is used in the iterative solution of the eigenvalue problem. Original covariance matrix and its approximation of small size assumes similarity of leading eigenvalues and eigenvectors. This fact allows fast convergence of subspace iterations at minimal additional computational cost.\nFor numerical experiments we use R language which is rich of linear algebra, statistical and graphical packages."}, {"heading": "2. PCA problem formulation", "text": "PCA in multivariate statistics is widely used as an effective way to perform unsupervised dimension reduction. The essence of this method lies in using Singular Value Decomposition (SVD)\nar X\niv :1\n60 2.\n08 80\n0v 1\n[ cs\n.N A\n] 2\n9 Fe\nb 20\n16\nwhich provides the best low rank approximation to original data according to Eckart-Young theorem [5]. Let n data points inm dimensional space be contained in the data matrix which is assumed already centered around the origin for computational stability\n(x1, x2, ..., xn) = X (1)\nThen covariance matrix is\nA = XXT (2)\nLet (\u03bbk, \u03c6k) be an eigenpair of A, where eigenvectors \u03c6k define principal directions."}, {"heading": "3. Aggregation model", "text": "In order to create an aggregation model we divide the entire set of data vectors xi into n0 clusters using some similarity criteria where n0 << n. We will explain later how we do clustering. We assume that all vectors within the cluster are similar and a single representative of a cluster is an average of all vectors xi where i \u2208 clusterk or for cluster k we have\nx0k = 1/dimk \u2217 \u2211\ni\u2208clusterk\nxi (3)\nTransformation of matrix X to X0 is done using matrix R which we call aggregator\nX0 = XR (4)\nwhere R[i, k] = if i \u2208 clusterk then 1/dimk else 0. X0 is of size (m,n0). Approximation A0 of covariance matrix A is\nA0 = X0X T 0 = XRR TXT (5)\nFormally matrix A0 is of the same size as A but has a much lower rank. We do not need to use form (5) for computations. For matrix vector multiplication we rather use sparse matrix X0 which according to (3) is constructed by simple averaging of vectors inside a cluster and\nA0v = X0X T 0 v (6)\nTherefore A0v requires O(mn0) operations which is much lower than O(mn) operations required for Av. We also expect and this is confirmed by numerical experiments that convergence of iterative methods for solving partial eigenvalue problem for A0 is faster than that for A.\nThere are quite a few clustering techniques known as computationally efficient. Besides since we need clustering as an auxiliary procedure we do not need highly accurate clustering results. In this study we use K-means clustering algorithm [6] which became very popular in data mining, unsupervised classification, etc. and which converges quickly to a local optimum. Our experience says that the aggregated problem with a small number of clusters provides a good resemblance of the original and approximated covariance matrices in terms of first (highest) eigenvalues which is important for the iterative method described below. In Figure 1 this resemblance is demonstrated where we show distribution of first 10 eigenvalues of both matrices where the data matrix X was obtained by processing 2014 documents of \u201dCardiovascular Diseases Abstracts\u201d corpus. Matrix X0 was obtained by K-means method with 10 clusters."}, {"heading": "4. Iterative method", "text": "We use power iteration method [7] for for solving auxiliary aggregated eigenvalue problem and a modified power method for solving the original eigenvalue problem. This method is also known as subspace iteration when used to simultaneously iterate a set of eigenvectors. One iteration of the power algorithm consists of the following steps:\nfor i = 1 to l : u\u0303i k+1 =\n1\n\u2016Auki \u2016 \u2217 Auki\nuk+11 , ..., u k+1 l = orthonorm(u\u03031 k+1, ..., u\u0303l k+1)\nwith approximation of eigenvalues\n\u03bbki = (Auki , u k i )\n(uki , u k i )\n(7)\nwhich starts with a set of l initial approximations of first eigenvectors (u01, u 0 2, ..., u 0 l ) = U 0. The key property of the power method is that if approximation u0i is spanned by matrix A eigenvectors subspace, then after k multiplications of matrix A by this vector the linear combination of eigenvectors will be weighted by \u03bbi to the power k which gives boost to terms corresponding to highest eigenvalues:\nAku = \u2211 k ci\u03bb k i \u03c6i (8)\nIn the method proposed for the first l principal directions of PCA we will need first k orthonormal eigenvectors of A0 q1, q2, ..., qk where k >= l. These vectors can be obtained by algorithms (7). We will also need matrix Pi\nPi = qiq T i (9)\nSince qTi qj = \u03b4i,j and PiPi = Pi, it is a projector to the subspace of i-th eigenvector of A0. We will modify method (7) using this projector in the following manner:\nfor i = 1 to l : u\u0303k+1i = 1\n\u2016Buki \u2016 \u2217Buki where\nBuki = Au k i + \u03b1i \u2217 PiAPiuki and \u03b1i => min\u2016Au\u0303k+1i \u2212 \u03bbki u\u0303k+1i \u2016\nuk+11 , ..., u k+1 l = orthonorm(u\u03031 k+1, ..., u\u0303l k+1)\nwith approximation of eigenvalues\n\u03bbki = (Auki , u k i )\n(uki , u k i )\n(10)\nThis approach can be thought of as \u201dhelp\u201d to the power iteration method to converge on the subspace of eigenvectors of the aggregated problem. The intuition for that is similarity of first eigenvectors and eigenvalues of the original and aggregated problem if clustering is done properly. Let u = \u2211 ci\u03c6i where \u03c6i are eigenvectors of the original covariance matrixA and Pk be a projector on subspace of \u03c6k. Then\nAu+ \u03b1PkAPku = \u2211 i 6=k ci\u03bbi\u03c6i + ck\u03bbk(1 + \u03b1)\u03c6k (11)\nIf \u03b1 is chosen big then the second term of this expression dominates over the first term thus providing convergence for \u03c6k in one iteration step. \u03b1 can be derived from the condition stated in (10):\n\u03a6(u\u0303k+1i ) = \u2016Au\u0303k+1i \u2212 \u03bbki u\u0303k+1i \u2016\n\u03b1 => min\u03a6 => d\u03a6\nd\u03b1 = 0\n(12)\nThis equation leads to the quadratic equation for \u03b1. Omitting indexes and skipping details we arrive at the following expression for \u03b1\n\u03b1 = \u2212(A 2u,AFu)\u2212 2\u03bb(A2u, Fu) + \u03bb2(Au, Fu)\n(Au,AFu)\u2212 2\u03bb(Au, Fu) + \u03bb2(Fu, Fu) (13)\nwhere F = PAP . We note that as you can see from (12) \u03b1 is chosen from the previous step to simplify computations. This can also be justified by the fact that eigenvalues converge faster than eigenvectors. Detailed algorithm discussion is out of scope of this paper. We just mention here that all operations with matrix A are reduced to the matrix vector multiplications of the sparse data matrix X or its transpose XT ."}, {"heading": "5. Numerical experiments", "text": "For numerical experiments we used two data sets. The fist one is \u201dCardiovascular Diseases Abstracts\u201d which is a set where each abstract is an individual document. The data matrix X size is 16058 by 2014 where the first value is the total number of terms and the second one is the number of documents. We searched for 10 first eigenvalues of the covariance matrix A = XXT and used 10 clusters for constructing auxiliary aggregation problem A0 = X0XT0 . So the size of this problem is more than 201 times lower than that for the original problem.\nThe problem is solved using algorithm (10). Figure 2 shows changes of parameter \u03b1 for the first three eigenvectors. As expected the biggest contribution of projectors (9) is observed in first\niterations to suppress errors caused by initial eigenvector guesses. After some number of iteration contribution of projectors is getting smaller while eigenvectors are getting more accurate.\nWe measure convergence of eigenvalues through Error1 = \u2016\u039bk+1 \u2212 \u039bk\u2016F/\u2016\u039bk\u2016F and convergence of eigenvectors by the residual matrix through Error2 = \u2016AUk \u2212 Uk\u039bk\u2016F where \u2016\u2016F is a matrix Frobenius norm, Uk consists of orthonormal vectors uk1, ..., u k l which are approximations of the eigenvectors and \u039bk is a diagonal matrix of approximations of eigenvalues. Errors graph is demonstrated in Figure 3.\nA good convergence rate of the iterative process is demonstrated. After 40 iterations we got Error1 = 0.00038 and Error2 = 0.0017.\nThe second corpus was \u201dtalk politics\u201d set from the news groups. Size of this problem is 13511 (terms) by 1171 (documents). We searched for 10 first eigenvalues of the covariance matrix and used 10 clusters again. The quality of the clustering aggregated model can be viewed by comparing eigenvalues of the original and aggregated covariance matrices. Figure 4 demonstrates a good resemblance of eigenvalues distribution. Convergence graph is demonstrated in Figure 5. After 40 iterations we got Error1 = 0.00044 and Error2 = 0.00049."}], "references": [{"title": "Multi-Grid Methods and Applications", "author": ["W. Hackbush"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1985}, {"title": "Principal Component Analysis, Series: Springer Series in Statistics, 2nd ed", "author": ["I.T. Jolliffe"], "venue": "XXIX,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "The approximation of one matrix by another of lower rank", "author": ["C. Eckart", "G. Young"], "venue": "Psychometrika, Volume", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1936}, {"title": "Some Methods for classification and Analysis of Multivariate Observations", "author": ["J.B. MacQueen"], "venue": "Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. University of California Press", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1967}, {"title": "Simultaneous iteration method for symmetric matrices", "author": ["H. Rutishauser"], "venue": "Numer. Math.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1970}], "referenceMentions": [{"referenceID": 0, "context": "Multilevel aggregation method was an extension of well-known multigrid methods[2] from boundary value problems to general structural analysis problems which brought it to the class of algebraic multigrid methods.", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "In this study PCA[4] is applied to and the method is tested on text analysis problems.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "which provides the best low rank approximation to original data according to Eckart-Young theorem [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "In this study we use K-means clustering algorithm [6] which became very popular in data mining, unsupervised classification, etc.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "Iterative method We use power iteration method [7] for for solving auxiliary aggregated eigenvalue problem and a modified power method for solving the original eigenvalue problem.", "startOffset": 47, "endOffset": 50}], "year": 2016, "abstractText": "Motivated by the previously developed multilevel aggregation method for solving structural analysis problems a novel two-level aggregation approach for efficient iterative solution of Principal Component Analysis (PCA) problems is proposed. The course aggregation model of the original covariance matrix is used in the iterative solution of the eigenvalue problem by a power iterations method. The method is tested on several data sets consisting of large number of text documents.", "creator": "LaTeX with hyperref package"}}}