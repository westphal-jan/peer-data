{"id": "1603.00748", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Continuous Deep Q-Learning with Model-based Acceleration", "abstract": "Model - any coordination philosophy has been unsuccessfully studied able a of, manner challenges, although years whose been line to handle sized mammalian network limiting and value computation. However, early sample element for unique - making methodology, particularly his example high - dimensional expression approximators, hence may guarantee either dictates to patient applications. In fact paper, nobody explore modeling and contexts to prevent all specific richness of little concerted basic for resulting establish tasks. We propose left advantageous technology for critical the efficiency main such algorithms. First, sure derive short depends distinguishing \u2014 the Q - learning algorithm, , we call normalized adantage structure (NAF ), has an change already the longer commonly some aimed formula_1 and musician - richard calculations. NAF analogous allows us with allows Q - emphasizes same experience relegation that effect needs, different reduced improves performance on a set of simulated inflatable control choices. To further manage during efficiency present come means, let explore the allowing mainly learned layout for slows older - free stresses learning. We show likely iteratively refitted local linear brand unlike unlike effective take it, various commitment reduce faster aspects on cytoplasmic few such custom handful applicable.", "histories": [["v1", "Wed, 2 Mar 2016 15:28:25 GMT  (3028kb,D)", "http://arxiv.org/abs/1603.00748v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO cs.SY", "authors": ["shixiang gu", "timothy p lillicrap", "ilya sutskever", "sergey levine"], "accepted": true, "id": "1603.00748"}, "pdf": {"name": "1603.00748.pdf", "metadata": {"source": "CRF", "title": "Continuous Deep Q-Learning with Model-based Acceleration", "authors": ["Shixiang Gu", "Timothy Lillicrap", "Ilya Sutskever", "Sergey Levine"], "emails": ["SG717@CAM.AC.UK", "COUNTZERO@GOOGLE.COM", "ILYASU@GOOGLE.COM", "SLEVINE@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "Model-free reinforcement learning (RL) has been successfully applied to a range of challenging problems (Kober & Peters, 2012; Deisenroth et al., 2013), and has recently been extended to handle large neural network policies and\nvalue functions (Mnih et al., 2015; Lillicrap et al., 2016; Wang et al., 2015; Heess et al., 2015; Hausknecht & Stone, 2015; Schulman et al., 2015). This makes it possible to train policies for complex tasks with minimal feature and policy engineering, using the raw state representation directly as input to the neural network. However, the sample complexity of model-free algorithms, particularly when using very high-dimensional function approximators, tends to be high (Schulman et al., 2015), which means that the benefit of reduced manual engineering and greater generality is not felt in real-world domains where experience must be collected on real physical systems, such as robots and autonomous vehicles. In such domains, the methods of choice have been efficient model-free algorithms that use more suitable, task-specific representations (Peters et al., 2010; Deisenroth et al., 2013), as well as model-based algorithms that learn a model of the system with supervised learning and optimize a policy under this model (Deisenroth & Rasmussen, 2011; Levine et al., 2015). Using task-specific representations dramatically improves efficiency, but limits the range of tasks that can be learned and requires greater domain knowledge. Using model-based RL also improves efficiency, but limits the policy to only be as good as the learned model. For many real-world tasks, it may be easier to represent a good policy than to learn a good model. For example, a simple robotic grasping behavior might only require closing the fingers at the right moment, while the corresponding dynamics model requires learning the complexities of rigid and deformable bodies undergoing frictional contact. It is therefore desirable to bring the generality of model-free deep reinforcement learning into real-world domains by reducing their sample complexity.\nIn this paper, we propose two complementary techniques for improving the efficiency of deep reinforcement learning in continuous control domains: we derive a variant of Q-learning that can be used in continuous domains, and we propose a method for combining this continuous Qlearning algorithm with learned models so as to accelerate learning while preserving the benefits of model-free RL. Model-free reinforcement learning in domains with contin-\nar X\niv :1\n60 3.\n00 74\n8v 1\n[ cs\n.L G\n] 2\nM ar\n2 01\nuous actions is typically handled with policy search methods (Peters & Schaal, 2006; Peters et al., 2010). Integrating value function estimation into these techniques results in actor-critic algorithms (Hafner & Riedmiller, 2011; Lillicrap et al., 2016; Schulman et al., 2015), which combine the benefits of policy search and value function estimation, but at the cost of training two separate function approximators. Our proposed Q-learning algorithm for continuous domains, which we call normalized advantage functions (NAF), avoids the need for a second actor or policy function, resulting in a simpler algorithm. The simpler optimization objective and the choice of value function parameterization result in an algorithm that is substantially more sample-efficient when used with large neural network function approximators on a range of continuous control domains.\nBeyond deriving an improved model-free deep reinforcement learning algorithm, we also seek to incorporate elements of model-based RL to accelerate learning, without giving up the strengths of model-free methods. One approach is for off-policy algorithms such as Q-learning to incorporate off-policy experience produced by a model-based planner. However, while this solution is a natural one, our empirical evaluation shows that it is ineffective at accelerating learning. As we discuss in our evaluation, this is due in part to the nature of value function estimation algorithms, which must experience both good and bad state transitions to accurately model the value function landscape. We propose an alternative approach to incorporating learned models into our continuous-action Q-learning algorithm based on imagination rollouts: on-policy samples generated under the learned model, analogous to the Dyna-Q method (Sutton, 1990). We show that this is extremely effective when the learned dynamics model perfectly matches the true one, but degrades dramatically with imperfect learned models. However, we demonstrate that iteratively fitting local linear models to the latest batch of on-policy or offpolicy rollouts provides sufficient local accuracy to achieve substantial improvement using short imagination rollouts in the vicinity of the real-world samples.\nOur paper provides three main contributions: first, we derive and evaluate a Q-function representation that allows for effective Q-learning in continuous domains. Second, we evaluate several na\u0131\u0308ve options for incorporating learned models into model-free Q-learning, and we show that they are minimally effective on our continuous control tasks. Third, we propose to combine locally linear models with local on-policy imagination rollouts to accelerate modelfree continuous Q-learning, and show that this produces a large improvement in sample complexity. We evaluate our method on a series of simulated robotic tasks and compare to prior methods."}, {"heading": "2. Related Work", "text": "Deep reinforcement learning has received considerable attention in recent years due to its potential to automate the design of representations in RL. Deep reinforcement learning and related methods have been applied to learn policies to play Atari games (Mnih et al., 2015; Schaul et al., 2015) and perform a wide variety of simulated and real-world robotic control tasks (Hafner & Riedmiller, 2011; Lillicrap et al., 2016; Levine & Koltun, 2013; de Bruin et al., 2015; Hafner & Riedmiller, 2011). While the majority of deep reinforcement learning methods in domains with discrete actions, such as Atari games, are based around value function estimation and Q-learning (Mnih et al., 2015), continuous domains typically require explicit representation of the policy, for example in the context of a policy gradient algorithm (Schulman et al., 2015). If we wish to incorporate the benefits of value function estimation into continuous deep reinforcement learning, we must typically use two networks: one to represent the policy, and one to represent the value function (Schulman et al., 2015; Lillicrap et al., 2016). In this paper, we instead describe how the simplicity and elegance of Q-learning can be ported into continuous domains, by learning a single network that outputs both the value function and policy. Our Q-function representation is related to dueling networks (Wang et al., 2015), though our approach applies to continuous action domains. Our empirical evaluation demonstrates that our continuous Q-learning algorithm achieves faster and more effective learning on a set of benchmark tasks compared to continuous actor-critic methods, and we believe that the simplicity of this approach will make it easier to adopt in practice. Our Q-learning method is also related to the work of Rawlik et al. (2013), but the form of our Q-function update is more standard.\nAs in standard RL, model-based deep reinforcement learning methods have generally been more efficient (Li & Todorov, 2004; Watter et al., 2015; Li & Todorov, 2004; Wahlstro\u0308m et al., 2015; Levine & Koltun, 2013), while model-free algorithms tend to be more generally applicable but substantially slower (Schulman et al., 2015; Lillicrap et al., 2016). Combining model-based and model-free learning has been explored in several ways in the literature. The method closest to our imagination rollouts approach is Dyna-Q (Sutton, 1990), which uses simulated experience in a learned model to supplement real-world on-policy rollouts. As we show in our evaluation, using Dyna-Q style methods to accelerate model-free RL is very effective when the learned model perfectly matches the true model, but degrades rapidly as the model becomes worse. We demonstrate that using iteratively refitted local linear models achieves substantially better results with imagination rollouts than more complex neural network models. We hypothesize that this is likely due to the fact that the more\nexpressive models themselves require substantially more data, and that otherwise efficient algorithms like Dyna-Q are vulnerable to poor model approximations."}, {"heading": "3. Background", "text": "In reinforcement learning, the goal is to learn a policy to control a system with states x \u2208 X and actions u \u2208 U in environment E, so as to maximize the expected sum of returns according to a reward function r(x,u). The dynamical system is defined by an initial state distribution p(x1) and a dynamics distribution p(xt+1|xt,ut). At each time step t \u2208 [1, T ], the agent chooses an action ut according to its current policy \u03c0(ut|xt), and observes a reward r(xt,ut). The agent then experiences a transition to a new state sampled from the dynamics distribution, and we can express the resulting state visitation frequency of the policy \u03c0 as \u03c1\u03c0(xt). Define Rt = \u2211T i=t \u03b3\n(i\u2212t)r(xi,ui), the goal is to maximize the expected sum of returns, given by R = Eri\u22651,xi\u22651\u223cE,ui\u22651\u223c\u03c0[R1], where \u03b3 is a discount factor that prioritizes earlier rewards over later ones. With \u03b3 < 1, we can also set T =\u221e, though we use a finite horizon for all of the tasks in our experiments. The expected return R can be optimized using a variety of model-free and model-based algorithms. In this section, we review several of these methods that we build on in our work.\nModel-Free Reinforcement Learning. When the system dynamics p(xt+1|xt,ut) are not known, as is often the case with physical systems such as robots, policy gradient methods (Peters & Schaal, 2006) and value function or Q-function learning with function approximation (Sutton et al., 1999) are often preferred. Policy gradient methods provide a simple, direct approach to RL, which can succeed on high-dimensional problems, but potentially requires a large number of samples (Schulman et al., 2015; 2016). Off-policy algorithms that use value or Q-function approximation can in principle achieve better data efficiency (Lillicrap et al., 2016). However, adapting such methods to continuous tasks typically requires optimizing two function approximators on different objectives. We instead build on standard Q-learning, which has a single objective. We summarize Q-learning in this section. The Q function Q\u03c0(xt,ut) corresponding to a policy \u03c0 is defined as the expected return from xt after taking action ut and following the policy \u03c0 thereafter:\nQ\u03c0(xt,ut) = Eri\u2265t,xi>t\u223cE,ui>t\u223c\u03c0[Rt|xt,ut] (1)\nQ-learning learns a greedy deterministic policy \u00b5(xt) = argmaxuQ(xt,ut), which corresponds to \u03c0(ut|xt) = \u03b4(ut = \u00b5(xt)). Let \u03b8Q parametrize the action-value function, and \u03b2 be an arbitrary exploration policy, the learning objective is to minimize the Bellman\nerror, where we fix the target yt:\nL(\u03b8Q) = Ext\u223c\u03c1\u03b2 ,ut\u223c\u03b2,rt\u223cE [(Q(xt,ut|\u03b8 Q)\u2212 yt)2]\nyt = r(xt,ut) + \u03b3Q(xt+1,\u00b5(xt+1)) (2)\nFor continuous action problems, Q-learning becomes difficult, because it requires maximizing a complex, nonlinear function at each update. For this reason, continuous domains are often tackled using actor-critic methods (Konda & Tsitsiklis, 1999; Hafner & Riedmiller, 2011; Silver et al., 2014; Lillicrap et al., 2016), where a separate parameterized \u201cactor\u201d policy \u03c0 is learned in addition to the Qfunction or value function \u201ccritic,\u201d such as Deep Deterministic Policy Gradient (DDPG) algorithm (Lillicrap et al., 2016).\nIn order to describe our method in the following sections, it will be useful to also define the value function V \u03c0(xt,ut) and advantage function A\u03c0(xt,ut) of a given policy \u03c0:\nV \u03c0(xt) = Eri\u2265t,xi>t\u223cE,ui\u2265t\u223c\u03c0[Rt|xt,ut] A\u03c0(xt,ut) = Q \u03c0(xt,ut)\u2212 V \u03c0(xt). (3)\nModel-Based Reinforcement Learning. If we know the dynamics p(xt+1|xt,ut), or if we can approximate them with some learned model p\u0302(xt+1|xt,ut), we can use model-based RL and optimal control. While a wide range of model-based RL and control methods have been proposed in the literature (Deisenroth et al., 2013; Kober & Peters, 2012), two are particularly relevant for this work: iterative LQG (iLQG) (Li & Todorov, 2004) and DynaQ (Sutton, 1990). The iLQG algorithm optimizes trajectories by iteratively constructing locally optimal linear feedback controllers under a local linearization of the dynamics p\u0302(xt+1|xt,ut) = N (fxtxt + futut,Ft) and a quadratic expansion of the rewards r(xt,ut) (Tassa et al., 2012). Under linear dynamics and quadratic rewards, the action-value function Q(xt,ut) and value function V (xt) are locally quadratic and can be computed by dynamics programming. The optimal policy can be derived analytically from the quadratic Q(xt,ut) and V (xt) functions, and corresponds to a linear feedback controller g(xt) = u\u0302t + kt +Kt(xt \u2212 x\u0302t), where kt is an openloop term, Kt is the closed-loop feedback matrix, and x\u0302t and u\u0302t are the states and actions of the nominal trajectory, which is the average trajectory of the controller. Employing the maximum entropy objective (Levine & Koltun, 2013), we can also construct a linear-Gaussian controller, where c is a scalar to adjust for arbitrary scaling of the reward magnitudes:\n\u03c0iLQGt (ut|xt) = N (u\u0302t + kt +Kt(xt \u2212 x\u0302t),\u2212cQ\u22121u,ut) (4)\nWhen the dynamics are not known, a particularly effective way to use iLQG is to combine it with learned time-varying\nlinear models p\u0302(xt+1|xt,ut). In this variant of the algorithm, trajectories are sampled from the controller in Equation (4) and used to fit time-varying linear dynamics with linear regression. These dynamics are then used with iLQG to obtain a new controller, typically using a KL-divergence constraint to enforce a trust region, so that the new controller doesn\u2019t deviate too much from the region in which the samples were generated (Levine & Abbeel, 2014).\nBesides enabling iLQG and other planning-based algorithms, a learned model of the dynamics can allow a modelfree algorithm to generate synthetic experience by performing rollouts in the learned model. A particularly relevant method of this type is Dyna-Q (Sutton, 1990), which performs real-world rollouts using the policy \u03c0, and then generates synthetic rollouts using a model learned from these samples. The synthetic rollouts originate at states visited by the real-world rollouts, and serve as supplementary data for a variety of possible reinforcement learning algorithms. However, most prior Dyna-Q methods have focused on relatively small, discrete domains. In Section 5, we describe how our method can be extended into a variant of Dyna-Q to achieve substantially faster learning on a range of continuous control tasks with complex neural network policies, and in Section 6, we empirically analyze the sensitivity of this method to imperfect learned dynamics models."}, {"heading": "4. Continuous Q-Learning with Normalized Advantage Functions", "text": "We first propose a simple method to enable Q-learning in continuous action spaces with deep neural networks, which we refer to as normalized advantage functions (NAF). The idea behind normalized advantage functions is to represent the Q-function Q(xt,ut) in Q-learning in such a way that its maximum, argmaxuQ(xt,ut), can be determined easily and analytically during the Q-learning update. While a number of representations are possible that allow for analytic maximization, the one we use in our implementation is based on a neural network that separately outputs a value function term V (x) and an advantage term A(x,u), which is parameterized as a quadratic function of nonlinear features of the state:\nQ(x,u|\u03b8Q) = A(x,u|\u03b8A) + V (x|\u03b8V )\nA(x,u|\u03b8A) = \u22121 2 (u\u2212 \u00b5(x|\u03b8\u00b5))TP (x|\u03b8P )(u\u2212 \u00b5(x|\u03b8\u00b5))\nP (x|\u03b8P ) is a state-dependent, positive-definite square matrix, which is parametrized by P (x|\u03b8P ) = L(x|\u03b8P )L(x|\u03b8P )T , where L(x|\u03b8P ) is a lower-triangular matrix whose entries come from a linear output layer of a neural network, with the diagonal terms exponentiated. While this representation is more restrictive than a general neural network function, since the Q-function is quadratic\nin u, the action that maximizes the Q-function is always given by \u00b5(x|\u03b8\u00b5). We use this representation with a deep Q-learning algorithm analogous to Mnih et al. (2015), using target networks and a replay buffers as described by (Lillicrap et al., 2016). NAF, given by Algorithm 1, is considerably simpler than DDPG.\nAlgorithm 1 Continuous Q-Learning with NAF\nRandomly initialize normalized Q network Q(x,u|\u03b8Q). Initialize target network Q\u2032 with weight \u03b8Q \u2032 \u2190 \u03b8Q. Initialize replay buffer R\u2190 \u2205. for episode=1,M do\nInitialize a random processN for action exploration Receive initial observation state x1 \u223c p(x1) for t=1, T do\nSelect action ut = \u00b5(xt|\u03b8\u00b5) +Nt Execute ut and observe rt and xt+1 Store transition (xt,ut, rt,xt+1) in R for iteration=1, I do\nSample a random minibatch of m transitions from R Set yi = ri + \u03b3V \u2032(xi+1|\u03b8Q \u2032 ) Update \u03b8Q by minimizing the loss: L = 1 N \u2211 i(yi \u2212 Q(xi,ui|\u03b8Q))2 Update the target network: \u03b8Q \u2032 \u2190 \u03c4\u03b8Q + (1\u2212 \u03c4)\u03b8Q \u2032\nend for end for\nend for\nDecomposing Q into an advantage term A and a statevalue term V was suggested by Baird III (1993); Harmon & Baird III (1996), and was recently explored by Wang et al. (2015) for discrete action problems. Normalized action-value functions have also been proposed by Rawlik et al. (2013) in the context of an alternative temporal difference learning algorithm. However, our method is the first to combine such representations with deep neural networks into an algorithm that can be used to learn policies for a range of challenging continuous control tasks. In general, A does not need to be quadratic, and exploring other parametric forms such as multimodal distributions is an interesting avenue for future work. The appendix provides details on adaptive exploration rule derivation with experimental results, and a variational interpretation of Qlearning which gives an intuitive explanation of the behavior of NAF that conforms with empirical results."}, {"heading": "5. Accelerating Learning with Imagination Rollouts", "text": "While NAF provides some advantages over actor-critic model-free RL methods in continuous domains, we can improve their data efficiency substantially under some additional assumptions by exploiting learned models. We will show that incorporating a particular type of learned model into Q-learning with NAFs significantly improves\nsample efficiency, while still allowing the final policy to be finetuned with model-free learning to achieve good performance without the limitations of imperfect models."}, {"heading": "5.1. Model-Guided Exploration", "text": "One natural approach to incorporating a learned model into an off-policy algorithm such as Q-learning is to use the learned model to generate good exploratory behaviors using planning or trajectory optimization. To evalaute this idea, we utilize the iLQG algorithm to generate good trajectories under the model, and then mix these trajectories together with on-policy experience by appending them to the replay buffer. Interestingly, we show in our evaluation that, even when planning under the true model, the improvement obtained from this approach is often quite small, and varies significantly across domains and choices of exploration noise. The intuition behind this result is that offpolicy iLQG exploration is too different from the learned policy, and Q-learning must consider alternatives in order to ascertain the optimality of a given action. That is, it\u2019s not enough to simply show the algorithm good actions, it must also experience bad actions to understand which actions are better and which are worse."}, {"heading": "5.2. Imagination Rollouts", "text": "As discussed in the previous section, incorporating offpolicy exploration from good, narrow distributions, such as those induced by iLQG, often does not result in significant improvement for Q-learning. These results suggest that Q-learning, which learns a policy based on minimizing temporal differences, inherently requires noisy on-policy actions to succeed. In real-world domains such as robots and autonomous vehicles, this can be undesirable for two reasons: first, it suggests that large amounts of on-policy experience are required in addition to good off-policy samples, and second, it implies that the policy must be allowed to make \u201cits own mistakes\u201d during training, which might involve taking undesirable or dangerous actions that can damage real-world hardware.\nOne way to avoid these problems while still allowing for a large amount of on-policy exploration is to generate synthetic on-policy trajectories under a learned model. Adding these synthetic samples, which we refer to as imagination rollouts, to the replay buffer effectively augments the amount of experience available for Q-learning. The particular approach we use is to perform rollouts in the real world using a mixture of planned iLQG trajectories and onpolicy trajectories, with various mixing coefficients evaluated in our experiments, and then generate additional synthetic on-policy rollouts using the learned model from each state visited along the real-world rollouts. This approach can be viewed as a variant of the Dyna-Q algorithm (Sut-\nton, 1990). However, while Dyna-Q has primarily been used with small and discrete systems, we show that using iteratively refitted linear models allows us to extend the approach to deep reinforcement learning on a range of continuous control domains. In some scenarios, we can even generate all or most of the real rollouts using off-policy iLQG controllers, which is desirable in safety-critic domains where poorly trained policies might take dangerous actions. The algorithm is given as Algorithm 2, and is an extension on Algorithm 1 combining model-based RL.\nAlgorithm 2 Imagination Rollouts with Fitted Dynamics and Optional iLQG Exploration\nRandomly initialize normalized Q network Q(x,u|\u03b8Q). Initialize target network Q\u2032 with weight \u03b8Q \u2032 \u2190 \u03b8Q. Initialize replay buffer R\u2190 \u2205 and fictional buffer Rf \u2190 \u2205. Initialize additional buffers B \u2190 \u2205, Bold \u2190 \u2205 with size nT . Initialize fitted dynamics modelM\u2190 \u2205. for episode = 1,M do\nInitialize a random processN for action exploration Receive initial observation state x1 Select \u00b5\u2032(x, t) from {\u00b5(x|\u03b8\u00b5), \u03c0iLQGt (ut|xt)} with probabilities {p, 1\u2212 p} for t = 1, T do\nSelect action ut = \u00b5\u2032(xt, t) +Nt Execute ut and observe rt and xt+1 Store transition (xt,ut, rt,xt+1, t) in R and B if mod (episode \u00b7 T + t,m) = 0 andM 6= \u2205 then\nSample m (xi,ui, ri,xi+1, i) from Bold UseM to simulate l steps from each sample Store all fictional transitions in Rf\nend if Sample a random minibatch of m transitions I \u00b7 l times from Rf and I times from R, and update \u03b8Q, \u03b8Q \u2032 as in\nAlgorithm 1 per minibatch. end for if Bf is full then M\u2190 FitLocalLinearDynamics(Bf ) (see Section 5.3) \u03c0iLQG \u2190 iLQG OneStep(Bf ,M) (see appendix) Bold \u2190 Bf , Bf \u2190 \u2205\nend if end for\nImagination rollouts can suffer from severe bias when the learned model is inaccurate. For example, we found it very difficult to train nonlinear neural network models for the dynamics that would actually improve the efficiency of Qlearning when used for imagination rollouts. As discussed in the following section, we found that using iteratively refitted time-varying linear dynamics produced substantially better results. In either case, we would still like to preserve the generality and optimality of model-free RL while deriving the benefits of model-based learning. To that end, we observe that most of the benefit of model-based learning is derived in the early stages of the learning process, when the policy induced by the neural network Q-function is poor. As the Q-function becomes more accurate, on-policy be-\nhavior tends to outperform model-based controllers. We therefore propose to switch off imagination rollouts after a given number of iterations.1 In this framework, the imagination rollouts can be thought of as an inexpensive way to pretrain the Q-function, such that fine-tuning using real world experience can quickly converge to an optimal solution."}, {"heading": "5.3. Fitting the Dynamics Model", "text": "In order to obtain good imagination rollouts and improve the efficiency of Q-learning, we needed to use an effective and data-efficient model learning algorithm. While prior methods propose a variety of model classes, including neural networks (Heess et al., 2015), Gaussian processes (Deisenroth & Rasmussen, 2011), and locally-weighted regression (Atkeson et al., 1997), we found that we could obtain good results by using iteratively refitted time-varying linear models, as proposed by Levine & Abbeel (2014). In this approach, instead of learning a good global model for all states and actions, we aim only to obtain a good local model around the latest set of samples. This approach requires a few additional assumptions: namely, it requires the initial state to be either deterministic or low-variance Gaussian, and it requires the states and actions to all be continuous. To handle domains with more varied initial states, we can use a mixture of Gaussian initial states with separate time-varying linear models for each one. The model itself is given by pt(xt+1|xt,ut) = N (Ft[xt;ut]+ft,Nt). Every n episodes, we refit the parameters Ft, ft, and Nt by fitting a Gaussian distribution at each time step to the vectors [xit;u i t;x i t+1], where i indicates the sample index, and conditioning this Gaussian on [xt;ut] to obtain the parameters of the linear-Gaussian dynamics at that step. We use n = 5 in our experiments. Although this approach introduces additional assumptions beyond the standard modelfree RL setting, we show in our evaluation that it produces impressive gains in sample efficiency on tasks where it can be applied."}, {"heading": "6. Experiments", "text": "We evaluated our approach on a set of simulated robotic tasks using the MuJoCo simulator (Todorov et al., 2012). The tasks were based on the benchmarks described by Lillicrap et al. (2016). Although we attempted to replicate the tasks in previous work as closely as possible, discrepancies in the simulator parameters and the contact model produced results that deviate slightly from those reported in prior work. In all experiments, the input to the policy consisted of the state of the system, defined in terms of joint\n1In future work, it would be interesting to select this iteration adaptively based on the expected relative performance of the Qfunction policy and model-based planning.\nangles and root link positions. Angles were often converted to sine and cosine encoding.\nFor both our method and the prior DDPG (Lillicrap et al., 2016) algorithm in the comparisons, we used neural networks with two layers of 200 rectified linear units (ReLU) to produce each of the output parameters \u2013 the Q-function and policy in DDPG, and the value function V , the advantage matrix L, and the mean \u00b5 for NAF. Since Q-learning was done with a replay buffer, we applied the Q-learning update 5 times per each step of experience to accelerate learning (I = 5). To ensure a fair comparison, DDPG also updates both the Q-function and policy parameters 5 times per step."}, {"heading": "6.1. Normalized Advantage Functions", "text": "In this section, we compare NAF and DDPG on 10 representative domains from Lillicrap et al. (2016), with three additional domains: a four-legged 3D ant, a six-joint 2D swimmer, and a 2D peg (see the appendix for the descriptions of task domains). We found the most sensitive hyperparameters to be presence or absence of batch normalization, base learning rate for ADAM (Kingma & Ba, 2014) \u2208 {1e\u22124, 1e\u22123, 1e\u22122}, and exploration noise scale \u2208 {0.1, 0.3, 1.0}. We report the best performance for each domain. We were unable to achieve good results with the method of Rawlik et al. (2013) on our domains, likely due to the complexity of high-dimensional neural network function approximators.\nFigure 1b, Figure 1c, and additional figures in the appendix show the performances on the three-joint reacher, peg insertion, and a gripper with mobile base. While the numerical gap in reacher may be small, qualitatively there is also a very noticeable difference between NAF and DDPG. DDPG converges to a solution where the deterministic policy causes the tip to fluctuate continuously around the target, and does not reach it precisely. NAF, on the other hand, learns a smooth policy that makes the tip slow down and stabilize at the target. This difference is more noticeable in peg insertion and moving gripper, as shown by the much faster convergence rate to the optimal solution. Precision is very important in many real-world robotic tasks, and these result suggest that NAF may be preferred in such domains.\nOn locomotion tasks, the performance of the two methods is relatively similar. On the six-joint swimmer task and four-legged ant, NAF slightly outperforms DDPG in terms of the convergence speed; however, DDPG is faster on cheetah and finds a better policy on walker2d. The loss in performance of NAF can potentially be explained by downside of the mode-seeking behavior as analyzed in the appendix, where it is hard to explore other modes once the quadratic advantage function finds a good one. Choosing a parametric form that is more expressive than a quadratic\ncould be used to address this limitation in future work.\nThe results on all of the domains are summarized in Table 1. Overall, NAF outperformed DDPG on the majority of tasks, particularly manipulation tasks that require precision and suffer less from the lack of multimodal Qfunctions. This makes this approach particularly promising for efficient learning of real-world robotic tasks."}, {"heading": "6.2. Evaluating Best-Case Model-Based Improvement with True Models", "text": "In order to determine how best to incorporate model-based components to accelerate model-free Q-learning, we tested several approaches using the ground truth dynamics, to control for challenges due to model fitting. We evaluated both of the methods discussed in Section 5: the use of model-based planning to generate good off-policy rollouts in the real world, and the use of the model to generate onpolicy synthetic rollouts.\nFigure 2a shows the effect of mixing off-policy iLQG experience and imagination rollouts on the three-joint reacher. It is noticeable that mixing the good off-policy experience does not significantly improve data-efficiency, while imagination rollouts always improve data-efficiency or final performance significantly. In the context of Q-learning, this\nresult is not entirely surprising: Q learning must experience both good and bad actions in order to determine which actions are preferred, while the good model-based rollouts are so far removed from the policy in the early stages of learning that they provide little useful information. Figure 2a also evaluates two different variants of the imagination rollouts approach, where the rollouts in the real world are performed either using the learned policy, or using model-based planning with iLQG. In the case of this task, the iLQG rollouts achieve slightly better results, since the on-policy imagination rollouts sampled around these off-policy states provide Q-learning with additional information about alternative action not taken by the iLQG planner. In general, we did not find that off-policy rollouts were consistently better than on-policy rollouts across all tasks, but they did consistently produce good results. Performing off-policy rollouts with iLQG may be desirable in realworld domains, where a partially learned policy might take undesirable or dangerous actions. Further details of these experiments are provided in the appendix."}, {"heading": "6.3. Guided Imagination Rollouts with Fitted Dynamics", "text": "In this section, we evaluated the performance of imagination rollouts with learned dynamics. As seen in Figure 2b, we found that fitting time-varying linear models following the imagination rollout algorithm is substantially better than fitting neural network dynamics models for the tasks we considered. There is a fundamental tension between efficient learning and expressive models like neural nets. We cannot hope to learn useful neural network models with a small number of samples for complex tasks, which makes it difficult to acquire a good model with fewer samples than are necessary to acquire a good policy. While the model is trained with supervised learning, which is typically more sample efficient, it often needs to represent a more complex function (e.g. rigid body physics). However, having such expressive models is more crucial as we move to improve model accuracy. Figure 2b presents results that compare\nfitted neural network models with the true dynamics when combined with imagination rollouts. These results indicate that the learned neural network models negate the benefits of imagination rollouts on our domains.\nTo evaluate imagination rollouts with fitted time-varying linear dynamics, we chose single-target variants of two of the manipulation tasks: the reacher and the gripper task. The results are shown in Figure 2b and 2c. We found that imagination rollouts of length 5 to 10 were sufficient for these tasks to achieve significant improvement over the fully model-free variant of NAF.\nAdding imagination rollouts in these domains provided 2-5 factors of improvement in data efficiency. In order to retain the benefit of model-free learning and allow the policy to continue improving once it exceeds the quality possible under the learned model, we switch off the imagination rollouts after 130 episodes (20,000 steps) on the gripper domain. This produces a small transient drop in the performance of the policy, but the results quickly improve again. Switching off the imagination rollouts also ensures that Qlearning does not diverge after it reaches good values, as were often observed in the gripper. This suggests that imagination rollouts, in contrast to off-policy exploration discussed in the previous section, is an effective method for bootstrapping model-free deep RL.\nIt should be noted that, although time-varying linear models combined with imagination rollouts provide a substantial boost in sample efficiency, this improvement is provided at some cost in generality, since effective fitting of time-varying linear models requires relatively small initial state distributions. With more complex initial state distributions, we might cluster the trajectories and fit multiple models to account for different modes. Extending the benefits of time-varying linear models to less restrictive settings is a promising direction and build on prior work (Levine et al., 2015; Fu et al., 2015). That said, our results show that imagination rollouts are a very promising approach to\naccelerating model-free learning when combined with the right kind of dynamics model."}, {"heading": "7. Discussion", "text": "In this paper, we explored several methods for improving the sample efficiency of model-free deep reinforcement learning. We first propose a method for applying standard Q-learning methods to high-dimensional, continuous domains, using the normalized advantage function (NAF) representation. This allows us to simplify the more standard actor-critic style algorithms, while preserving the benefits of nonlinear value function approximation, and allows us to employ a simple and effective adaptive exploration method. We show that, in comparison to recently proposed deep actor-critic algorithms, our method tends to learn faster and acquires more accurate policies. We further explore how model-free RL can be accelerated by incorporating learned models, without sacrificing the optimality of the policy in the face of imperfect model learning. We show that, although Q-learning can incorporate off-policy experience, learning primarily from off-policy exploration (via modelbased planning) only rarely improves the overall sample efficiency of the algorithm. We postulate that this caused by the need to observe both successful and unsuccessful actions, in order to obtain an accurate estimate of the Qfunction. We demonstrate that an alternative method based on synthetic on-policy rollouts achieves substantially improved sample complexity, but only when the model learning algorithm is chosen carefully. We demonstrate that training neural network models does not provide substantive improvement in our domains, but simple iteratively refitted time-varying linear models do provide substantial improvement on domains where they can be applied."}, {"heading": "Acknowledgement", "text": "We thank Nicholas Heess for helpful discussion and Tom Erez, Yuval Tassa, Vincent Vanhoucke, and the Google Brain and DeepMind teams for their support."}, {"heading": "8. Appendix", "text": ""}, {"heading": "8.1. iLQG", "text": "The iLQG algorithm optimizes trajectories by iteratively constructing locally optimal linear feedback controllers under a local linearization of the dynamics p(xt+1|xt,ut) = N (fxtxt + futut,Ft) and a quadratic expansion of the rewards r(xt,ut) (Tassa et al., 2012). Under linear dynamics and quadratic rewards, the action-value function Q(xt,ut) and value function V (xt) are locally quadratic and can be computed by dynamics programming.2\nQxu,xut = rxu,xut + f T xutVx,xt+1fxut\nQxut = rxut + f T xutVx,xt+1 Vx,xt = Qx,xt \u2212QTu,xtQ\u22121u,utQu,xt Vxt = Qxt \u2212QTu,xtQ\u22121u,utQut\nQx,xT = Vx,xT = rx,xT\n(5)\nThe time-varying linear feedback controller g(xt) = u\u0302t + kt + Kt(xt \u2212 x\u0302t) maximizes the locally quadratic Q, where kt = \u2212Q\u22121u,utQut,Kt = \u2212Q\u22121u,utQu,xt, and x\u0302t, u\u0302t denote states and actions of the current trajectory around which the partial derivatives are computed. Employing the maximum entropy objective (Levine & Koltun, 2013), we can also construct a linear-Gaussian controller, where c is a scalar to adjust for arbitrary scaling of the reward magnitudes,\n\u03c0iLQGt (ut|xt) = N (u\u0302t + kt +Kt(xt \u2212 x\u0302t),\u2212cQ\u22121u,ut) (6)\nWhen the dynamics are not known, a particularly effective way to use iLQG is to combine it with learned time-varying linear models p\u0302(xt+1|xt,ut). In this variant of the algorithm, trajectories are sampled from the controller in Equation (6) and used to fit time-varying linear dynamics with linear regression. These dynamics are then used with iLQG to obtain a new controller, typically using a KL-divergence constraint to enforce a trust region, so that the new controller doesn\u2019t deviate too much from the region in which the samples were generated (Levine & Abbeel, 2014)."}, {"heading": "8.2. Locally-Invariant Exploration for Normalized Advantage Functions", "text": "Exploration is an essential component of reinforcement learning algorithms. The simplest and most common type of exploration involves randomizing the actions according to some distribution, either by taking random actions with some probability (Mnih et al., 2015), or adding Gaussian noise in continuous action spaces (Schulman et al.,\n2While standard iLQG notation denotes Q,V as discounted sum of costs, we denote them as sum of rewards to make them consistent with the rest of the paper\n2015). However, choosing the magnitude of the random exploration noise can be difficult, particularly in highdimensional domains where different action dimensions require very different exploration scales. Furthermore, independent (spherical) Gaussian noise may be inappropriate for tasks where the optimal behavior requires correlation between action dimensions, as for example in the case of the swimming snake described in our experiments, which must coordinate the motion of different body joints to produce a synchronized undulating gait.\nThe NAF provides us with a simple and natural avenue to obtain an adaptive exploration strategy, analogously to Boltzmann exploration. The idea is to use the matrix in the quadratic component of the advantage function as the precision for a Gaussian action distribution. This naturally causes the policy to become more deterministic along directions where the advantage function varies steeply, and more random along directions where it is flat. The corresponding policy is given by\n\u03c0(u|x) = expQ(x,u|\u03b8 Q) / \u222b expQ(x,u|\u03b8 Q) du\n= N (\u00b5(x|\u03b8\u00b5), cP (x|\u03b8P )\u22121). (7)\nPrevious work also noted that generating Gaussian exploration noise independently for each time step was not wellsuited for many continuous control tasks, particularly simulated robotic tasks where the actions correspond to torques or velocities (Lillicrap et al., 2016). The intuition is that, as the length of the time-step decreases, temporally independent Gaussian exploration will cancel out between time steps. Instead, prior work proposed to sample noise from an Ornstein-Uhlenbech (OU) process to generate a temporally correlated noise sequence (Lillicrap et al., 2016). We adopt the same approach in our work, but sample the innovations for the OU process from the Gaussian distribution in Equation 7. Lastly, we note that the overall scale of P (x|\u03b8P ) could vary significantly through the learning, and depends on the magnitude of the cost, which introduces an undesirable additional degree of freedom. We therefore use a heuristic adaptive-scaling trick to stabilize the noise magnitudes.\nUsing the learned precision as the noise covariance for exploration allowed for convergence to a better policy on the \u201ccanada2d\u201d task, which requires using an arm to strike a puck toward a target, as shown in Figure 3, but did not make a significant difference on the other domains."}, {"heading": "8.3. Q-Learning as Variational Inference", "text": "NAF, with our choice of parametrization, can only fit a locally quadratic Q-function. To understand its implications and expected behaviors, one approach is to approximately view the Q-learning objective as minimizing the exclusive\nKullback-Leibler divergence (KL) between policy distributions induced by the fitted normalized Q-function Q\u0302 and the true Q-function Q. This can be derived by assuming (1) no bootstrapping and the exact target Q is provided per step, (2) no additive exploration noise, i.e. fully on-policy, and (3) use KL loss instead of the least-square. Specifically, let \u03c0 and \u03c0\u0302 be corresponding policies of Q and Q\u0302 respectively, an alternative form of Q-learning could be optimizing the following objective:\nLe(Q\u0302) = Ext\u223c\u03c1\u03c0\u0302 [K\u0303L(\u03c0\u0302||\u03c0)] = Ext\u223c\u03c1\u03c0\u0302,ut\u223c\u03c0\u0302[Q\u0302\u2212Q] (8)\nWe can thus intuitively interpret NAF as doing variational inference to fit a Gaussian to a distribution, and it has mode-seeking behavior. Empirically such behavior enables NAF to learn smoother and more precise controllers, as most effectively illustrated by three-joint reacher and peg insertion experiments, and substantial improvements in terms of convergence speeds in many other representative domains explored in the main paper."}, {"heading": "8.4. Descriptions of Task Domains", "text": "Table 3 describes the task domains used in the experiments."}, {"heading": "8.5. More Results on Normalized Advantage Functions", "text": "Figures 4a, 4b, and 4c provide additional results on the comparison experiments between DDPG and NAF. As shown in the main paper, NAF generally outperforms DDPG. In certain tasks that require precision, such as peg insertion, the difference is very noticeable. However, there are also few cases where NAF underperforms DDPG. The most consistent of such cases is cheetah. While both DDPG and NAF enable cheetah to run decent distances, it is often observed that the cheetah movements learned in NAF are\nlittle less natural than those from DDPG. We speculate such behaviors come from the mode-seeking behavior that is explained in Section 8.3, and thus exploring other parametric forms of NAF, such as multi-modal variants, is a promising avenue for future work."}, {"heading": "8.6. More Results on Evaluating Best-Case", "text": "Model-Based Improvement with True Models\nIn the main paper, iLQG with true dynamics is used to generate guided exploration trajectories. While iLQG works for simple manipulation tasks with small number of initial states, it does not work well for random target reacher or complex locomotion tasks such as cheetah. We therefore run iLQG in model-predictive control (MPC) mode for the experiments reported in Figures 5c, 5b, and 5a, and Table 2. It is important to note that for those experiments, the hyperparameters were fixed (batch normalization is on, learning rate is 10\u22123, and exploration size is 0.3), and thus the results differ slightly from the experiments in the previous section.\nIn cheetah and other complex locomotion tasks, MPC policy is usually sub-optimal, and thus poor performance of mixing MPC experience in Figure 5b is expected. On the other hand, MPC policy works reasonably in hard manipulation tasks such as canada2d, and there is significant gain from mixing MPC experience as Figure 5c shows. However, the most consistent gain comes from using imagination rollouts in all three domains. In particular, Figure 5c shows that in canada2d, MPC experiences gives very good trajectories, i.e. those that hit balls in roughly the right directions, and doing rollouts can generate more of this useful experience, enabling canada2d to learn very quickly. While with true dynamics having the imagination experience directly means more experience and such result may be trivial, it is still important to see the benefits of rollouts which only explore up to l = 10 steps away from the real experience, as reported here. This is an interesting result, since this means the dynamics model only needs to be accurate around the data trajectories and this significantly lessens the requirement on fitted models."}], "references": [{"title": "Locally weighted learning for control", "author": ["Atkeson", "Christopher G", "Moore", "Andrew W", "Schaal", "Stefan"], "venue": "In Lazy learning,", "citeRegEx": "Atkeson et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Atkeson et al\\.", "year": 1997}, {"title": "Advantage updating", "author": ["III Baird", "C. Leemon"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Baird and Leemon,? \\Q1993\\E", "shortCiteRegEx": "Baird and Leemon", "year": 1993}, {"title": "The importance of experience replay database composition in deep reinforcement learning", "author": ["de Bruin", "Tim", "Kober", "Jens", "Tuyls", "Karl", "Babu\u0161ka", "Robert"], "venue": "Deep Reinforcement Learning Workshop,", "citeRegEx": "Bruin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bruin et al\\.", "year": 2015}, {"title": "Pilco: A modelbased and data-efficient approach to policy search", "author": ["Deisenroth", "Marc", "Rasmussen", "Carl E"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Deisenroth et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2011}, {"title": "A survey on policy search for robotics", "author": ["Deisenroth", "Marc Peter", "Neumann", "Gerhard", "Peters", "Jan"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "Deisenroth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "One-shot learning of manipulation skills with online dynamics adaptation and neural network priors", "author": ["Fu", "Justin", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1509.06841,", "citeRegEx": "Fu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2015}, {"title": "Reinforcement learning in feedback control", "author": ["Hafner", "Roland", "Riedmiller", "Martin"], "venue": "Machine learning,", "citeRegEx": "Hafner et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hafner et al\\.", "year": 2011}, {"title": "Multi-player residual advantage learning with general function approximation", "author": ["Harmon", "E Mance", "III Baird", "C. Leemon"], "venue": null, "citeRegEx": "Harmon et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Harmon et al\\.", "year": 1996}, {"title": "Deep reinforcement learning in parameterized action space", "author": ["Hausknecht", "Matthew", "Stone", "Peter"], "venue": "arXiv preprint arXiv:1511.04143,", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Heess", "Nicolas", "Wayne", "Gregory", "Silver", "David", "Lillicrap", "Tim", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Kober", "Jens", "Peters", "Jan"], "venue": "In Reinforcement Learning,", "citeRegEx": "Kober et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2012}, {"title": "Actor-critic algorithms", "author": ["Konda", "Vijay R", "Tsitsiklis", "John N"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Konda et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Konda et al\\.", "year": 1999}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["Levine", "Sergey", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Levine et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2014}, {"title": "Guided policy search", "author": ["Levine", "Sergey", "Koltun", "Vladlen"], "venue": "In International Conference on Machine Learning (ICML), pp", "citeRegEx": "Levine et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2013}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Policy gradient methods for robotics", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Peters et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2006}, {"title": "Relative entropy policy search", "author": ["Peters", "Jan", "M\u00fclling", "Katharina", "Altun", "Yasemin"], "venue": "In AAAI. Atlanta,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "On stochastic optimal control and reinforcement learning by approximate inference", "author": ["Rawlik", "Konrad", "Toussaint", "Marc", "Vijayakumar", "Sethu"], "venue": null, "citeRegEx": "Rawlik et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rawlik et al\\.", "year": 2013}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["Silver", "David", "Lever", "Guy", "Heess", "Nicolas", "Degris", "Thomas", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["Sutton", "Richard S"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Sutton and S.,? \\Q1990\\E", "shortCiteRegEx": "Sutton and S.", "year": 1990}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Synthesis and stabilization of complex behaviors through online trajectory optimization", "author": ["Tassa", "Yuval", "Erez", "Tom", "Todorov", "Emanuel"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Tassa et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tassa et al\\.", "year": 2012}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "From pixels to torques: Policy learning with deep dynamical models", "author": ["Wahlstr\u00f6m", "Niklas", "Sch\u00f6n", "Thomas B", "Deisenroth", "Marc Peter"], "venue": "arXiv preprint arXiv:1502.02251,", "citeRegEx": "Wahlstr\u00f6m et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wahlstr\u00f6m et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "de Freitas", "Nando", "Lanctot", "Marc"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["Watter", "Manuel", "Springenberg", "Jost", "Boedecker", "Joschka", "Riedmiller", "Martin"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": ", 2013), and has recently been extended to handle large neural network policies and value functions (Mnih et al., 2015; Lillicrap et al., 2016; Wang et al., 2015; Heess et al., 2015; Hausknecht & Stone, 2015; Schulman et al., 2015).", "startOffset": 100, "endOffset": 231}, {"referenceID": 29, "context": ", 2013), and has recently been extended to handle large neural network policies and value functions (Mnih et al., 2015; Lillicrap et al., 2016; Wang et al., 2015; Heess et al., 2015; Hausknecht & Stone, 2015; Schulman et al., 2015).", "startOffset": 100, "endOffset": 231}, {"referenceID": 9, "context": ", 2013), and has recently been extended to handle large neural network policies and value functions (Mnih et al., 2015; Lillicrap et al., 2016; Wang et al., 2015; Heess et al., 2015; Hausknecht & Stone, 2015; Schulman et al., 2015).", "startOffset": 100, "endOffset": 231}, {"referenceID": 21, "context": ", 2013), and has recently been extended to handle large neural network policies and value functions (Mnih et al., 2015; Lillicrap et al., 2016; Wang et al., 2015; Heess et al., 2015; Hausknecht & Stone, 2015; Schulman et al., 2015).", "startOffset": 100, "endOffset": 231}, {"referenceID": 21, "context": "However, the sample complexity of model-free algorithms, particularly when using very high-dimensional function approximators, tends to be high (Schulman et al., 2015), which means that the benefit of reduced manual engineering and greater generality is not felt in real-world domains where experience must be collected on real physical systems, such as robots and autonomous vehicles.", "startOffset": 144, "endOffset": 167}, {"referenceID": 18, "context": "In such domains, the methods of choice have been efficient model-free algorithms that use more suitable, task-specific representations (Peters et al., 2010; Deisenroth et al., 2013), as well as model-based algorithms that learn a model of the system with supervised learning and optimize a policy under this model (Deisenroth & Rasmussen, 2011; Levine et al.", "startOffset": 135, "endOffset": 181}, {"referenceID": 4, "context": "In such domains, the methods of choice have been efficient model-free algorithms that use more suitable, task-specific representations (Peters et al., 2010; Deisenroth et al., 2013), as well as model-based algorithms that learn a model of the system with supervised learning and optimize a policy under this model (Deisenroth & Rasmussen, 2011; Levine et al.", "startOffset": 135, "endOffset": 181}, {"referenceID": 15, "context": ", 2013), as well as model-based algorithms that learn a model of the system with supervised learning and optimize a policy under this model (Deisenroth & Rasmussen, 2011; Levine et al., 2015).", "startOffset": 140, "endOffset": 191}, {"referenceID": 18, "context": "ods (Peters & Schaal, 2006; Peters et al., 2010).", "startOffset": 4, "endOffset": 48}, {"referenceID": 16, "context": "Integrating value function estimation into these techniques results in actor-critic algorithms (Hafner & Riedmiller, 2011; Lillicrap et al., 2016; Schulman et al., 2015), which combine the benefits of policy search and value function estimation, but at the cost of training two separate function approximators.", "startOffset": 95, "endOffset": 169}, {"referenceID": 21, "context": "Integrating value function estimation into these techniques results in actor-critic algorithms (Hafner & Riedmiller, 2011; Lillicrap et al., 2016; Schulman et al., 2015), which combine the benefits of policy search and value function estimation, but at the cost of training two separate function approximators.", "startOffset": 95, "endOffset": 169}, {"referenceID": 20, "context": "Deep reinforcement learning and related methods have been applied to learn policies to play Atari games (Mnih et al., 2015; Schaul et al., 2015) and perform a wide variety of simulated and real-world robotic control tasks (Hafner & Riedmiller, 2011; Lillicrap et al.", "startOffset": 104, "endOffset": 144}, {"referenceID": 16, "context": ", 2015) and perform a wide variety of simulated and real-world robotic control tasks (Hafner & Riedmiller, 2011; Lillicrap et al., 2016; Levine & Koltun, 2013; de Bruin et al., 2015; Hafner & Riedmiller, 2011).", "startOffset": 85, "endOffset": 209}, {"referenceID": 21, "context": ", 2015), continuous domains typically require explicit representation of the policy, for example in the context of a policy gradient algorithm (Schulman et al., 2015).", "startOffset": 143, "endOffset": 166}, {"referenceID": 21, "context": "If we wish to incorporate the benefits of value function estimation into continuous deep reinforcement learning, we must typically use two networks: one to represent the policy, and one to represent the value function (Schulman et al., 2015; Lillicrap et al., 2016).", "startOffset": 218, "endOffset": 265}, {"referenceID": 16, "context": "If we wish to incorporate the benefits of value function estimation into continuous deep reinforcement learning, we must typically use two networks: one to represent the policy, and one to represent the value function (Schulman et al., 2015; Lillicrap et al., 2016).", "startOffset": 218, "endOffset": 265}, {"referenceID": 29, "context": "Our Q-function representation is related to dueling networks (Wang et al., 2015), though our approach applies to continuous action domains.", "startOffset": 61, "endOffset": 80}, {"referenceID": 2, "context": ", 2016; Levine & Koltun, 2013; de Bruin et al., 2015; Hafner & Riedmiller, 2011). While the majority of deep reinforcement learning methods in domains with discrete actions, such as Atari games, are based around value function estimation and Q-learning (Mnih et al., 2015), continuous domains typically require explicit representation of the policy, for example in the context of a policy gradient algorithm (Schulman et al., 2015). If we wish to incorporate the benefits of value function estimation into continuous deep reinforcement learning, we must typically use two networks: one to represent the policy, and one to represent the value function (Schulman et al., 2015; Lillicrap et al., 2016). In this paper, we instead describe how the simplicity and elegance of Q-learning can be ported into continuous domains, by learning a single network that outputs both the value function and policy. Our Q-function representation is related to dueling networks (Wang et al., 2015), though our approach applies to continuous action domains. Our empirical evaluation demonstrates that our continuous Q-learning algorithm achieves faster and more effective learning on a set of benchmark tasks compared to continuous actor-critic methods, and we believe that the simplicity of this approach will make it easier to adopt in practice. Our Q-learning method is also related to the work of Rawlik et al. (2013), but the form of our Q-function update is more standard.", "startOffset": 34, "endOffset": 1402}, {"referenceID": 21, "context": ", 2015; Levine & Koltun, 2013), while model-free algorithms tend to be more generally applicable but substantially slower (Schulman et al., 2015; Lillicrap et al., 2016).", "startOffset": 122, "endOffset": 169}, {"referenceID": 16, "context": ", 2015; Levine & Koltun, 2013), while model-free algorithms tend to be more generally applicable but substantially slower (Schulman et al., 2015; Lillicrap et al., 2016).", "startOffset": 122, "endOffset": 169}, {"referenceID": 25, "context": "When the system dynamics p(xt+1|xt,ut) are not known, as is often the case with physical systems such as robots, policy gradient methods (Peters & Schaal, 2006) and value function or Q-function learning with function approximation (Sutton et al., 1999) are often preferred.", "startOffset": 231, "endOffset": 252}, {"referenceID": 21, "context": "Policy gradient methods provide a simple, direct approach to RL, which can succeed on high-dimensional problems, but potentially requires a large number of samples (Schulman et al., 2015; 2016).", "startOffset": 164, "endOffset": 193}, {"referenceID": 16, "context": "approximation can in principle achieve better data efficiency (Lillicrap et al., 2016).", "startOffset": 62, "endOffset": 86}, {"referenceID": 23, "context": "For this reason, continuous domains are often tackled using actor-critic methods (Konda & Tsitsiklis, 1999; Hafner & Riedmiller, 2011; Silver et al., 2014; Lillicrap et al., 2016), where a separate parameterized \u201cactor\u201d policy \u03c0 is learned in addition to the Qfunction or value function \u201ccritic,\u201d such as Deep Deterministic Policy Gradient (DDPG) algorithm (Lillicrap et al.", "startOffset": 81, "endOffset": 179}, {"referenceID": 16, "context": "For this reason, continuous domains are often tackled using actor-critic methods (Konda & Tsitsiklis, 1999; Hafner & Riedmiller, 2011; Silver et al., 2014; Lillicrap et al., 2016), where a separate parameterized \u201cactor\u201d policy \u03c0 is learned in addition to the Qfunction or value function \u201ccritic,\u201d such as Deep Deterministic Policy Gradient (DDPG) algorithm (Lillicrap et al.", "startOffset": 81, "endOffset": 179}, {"referenceID": 16, "context": ", 2016), where a separate parameterized \u201cactor\u201d policy \u03c0 is learned in addition to the Qfunction or value function \u201ccritic,\u201d such as Deep Deterministic Policy Gradient (DDPG) algorithm (Lillicrap et al., 2016).", "startOffset": 185, "endOffset": 209}, {"referenceID": 4, "context": "While a wide range of model-based RL and control methods have been proposed in the literature (Deisenroth et al., 2013; Kober & Peters, 2012), two are particularly relevant for this work: iterative LQG (iLQG) (Li & Todorov, 2004) and DynaQ (Sutton, 1990).", "startOffset": 94, "endOffset": 141}, {"referenceID": 26, "context": "The iLQG algorithm optimizes trajectories by iteratively constructing locally optimal linear feedback controllers under a local linearization of the dynamics p\u0302(xt+1|xt,ut) = N (fxtxt + futut,Ft) and a quadratic expansion of the rewards r(xt,ut) (Tassa et al., 2012).", "startOffset": 246, "endOffset": 266}, {"referenceID": 16, "context": "(2015), using target networks and a replay buffers as described by (Lillicrap et al., 2016).", "startOffset": 67, "endOffset": 91}, {"referenceID": 28, "context": "Decomposing Q into an advantage term A and a statevalue term V was suggested by Baird III (1993); Harmon & Baird III (1996), and was recently explored by Wang et al. (2015) for discrete action problems.", "startOffset": 154, "endOffset": 173}, {"referenceID": 19, "context": "Normalized action-value functions have also been proposed by Rawlik et al. (2013) in the context of an alternative temporal difference learning algorithm.", "startOffset": 61, "endOffset": 82}, {"referenceID": 9, "context": "While prior methods propose a variety of model classes, including neural networks (Heess et al., 2015), Gaussian processes (Deisenroth & Rasmussen, 2011), and locally-weighted regression (Atkeson et al.", "startOffset": 82, "endOffset": 102}, {"referenceID": 0, "context": ", 2015), Gaussian processes (Deisenroth & Rasmussen, 2011), and locally-weighted regression (Atkeson et al., 1997), we found that we could obtain good results by using iteratively refitted time-varying linear models, as proposed by Levine & Abbeel (2014).", "startOffset": 92, "endOffset": 114}, {"referenceID": 0, "context": ", 2015), Gaussian processes (Deisenroth & Rasmussen, 2011), and locally-weighted regression (Atkeson et al., 1997), we found that we could obtain good results by using iteratively refitted time-varying linear models, as proposed by Levine & Abbeel (2014). In this approach, instead of learning a good global model for all states and actions, we aim only to obtain a good local model around the latest set of samples.", "startOffset": 93, "endOffset": 255}, {"referenceID": 27, "context": "We evaluated our approach on a set of simulated robotic tasks using the MuJoCo simulator (Todorov et al., 2012).", "startOffset": 89, "endOffset": 111}, {"referenceID": 16, "context": "The tasks were based on the benchmarks described by Lillicrap et al. (2016). Although we attempted to replicate the tasks in previous work as closely as possible, discrep-", "startOffset": 52, "endOffset": 76}, {"referenceID": 16, "context": "For both our method and the prior DDPG (Lillicrap et al., 2016) algorithm in the comparisons, we used neural net-", "startOffset": 39, "endOffset": 63}, {"referenceID": 16, "context": "In this section, we compare NAF and DDPG on 10 representative domains from Lillicrap et al. (2016), with three additional domains: a four-legged 3D ant, a six-joint 2D swimmer, and a 2D peg (see the appendix for the descriptions of task domains).", "startOffset": 75, "endOffset": 99}, {"referenceID": 16, "context": "In this section, we compare NAF and DDPG on 10 representative domains from Lillicrap et al. (2016), with three additional domains: a four-legged 3D ant, a six-joint 2D swimmer, and a 2D peg (see the appendix for the descriptions of task domains). We found the most sensitive hyperparameters to be presence or absence of batch normalization, base learning rate for ADAM (Kingma & Ba, 2014) \u2208 {1e\u22124, 1e\u22123, 1e\u22122}, and exploration noise scale \u2208 {0.1, 0.3, 1.0}. We report the best performance for each domain. We were unable to achieve good results with the method of Rawlik et al. (2013) on our domains, likely due to the complexity of high-dimensional neural network function approximators.", "startOffset": 75, "endOffset": 585}, {"referenceID": 15, "context": "is a promising direction and build on prior work (Levine et al., 2015; Fu et al., 2015).", "startOffset": 49, "endOffset": 87}, {"referenceID": 5, "context": "is a promising direction and build on prior work (Levine et al., 2015; Fu et al., 2015).", "startOffset": 49, "endOffset": 87}], "year": 2016, "abstractText": "Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of modelfree algorithms, particularly when using highdimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.", "creator": "TeX"}}}