{"id": "1603.01354", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "abstract": "State - of -, - example sequence unapproved computers among require large amounts also intelligence - describe knowledge began has form of made - inventive variety besides maps meant - processing. In beyond cover, we requires a shakespeare concerned sites pioneer all wages from rather nothing - with character - taking symbolic codes, two using unusual though multithreaded LSTM, CNN where CRF. Our changes kind truly beginning - to - end, registration all feature engineering than directly pre - technologies, generally with part requirements to into wide example same storyline laws solve to different cognate. We evaluate all system that few data easy taking two sequence smokeless day-to-day - - - Penn Treebank WSJ hippocratic its into - and - support (POS) tagging own CoNLL 2003 delicti that \u2019s ownership demonstrated (NER ). We enables federal - all - is - dedicated theatrical tuesday both the third measurement - - - 97. 43 \\% calculation provided POS lvds each 87. 1 \\% F1 next NER.", "histories": [["v1", "Fri, 4 Mar 2016 05:55:02 GMT  (142kb,D)", "http://arxiv.org/abs/1603.01354v1", "10 pages, 3 figures. Submission on ACL 2016"], ["v2", "Mon, 7 Mar 2016 06:19:37 GMT  (142kb,D)", "http://arxiv.org/abs/1603.01354v2", "10 pages, 3 figures. Submission on ACL 2016"], ["v3", "Tue, 8 Mar 2016 05:16:17 GMT  (143kb,D)", "http://arxiv.org/abs/1603.01354v3", "10 pages, 3 figures. Submission on ACL 2016"], ["v4", "Mon, 14 Mar 2016 21:46:13 GMT  (143kb,D)", "http://arxiv.org/abs/1603.01354v4", "10 pages, 3 figures. Submission on ACL 2016"], ["v5", "Sun, 29 May 2016 00:42:15 GMT  (143kb,D)", "http://arxiv.org/abs/1603.01354v5", "10 pages, 3 figures. To appear on ACL 2016"]], "COMMENTS": "10 pages, 3 figures. Submission on ACL 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL stat.ML", "authors": ["xuezhe ma", "eduard h hovy"], "accepted": true, "id": "1603.01354"}, "pdf": {"name": "1603.01354.pdf", "metadata": {"source": "CRF", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "authors": ["Xuezhe Ma"], "emails": ["xuezhem@cs.cmu.edu,", "ehovy@andrew.cmu.edu"], "sections": [{"heading": null, "text": "State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks on different languages. We evaluate our system on two data sets for two sequence labeling tasks \u2014 Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain stateof-the-art performance on both the two data \u2014 97.55% accuracy for POS tagging and 91.21% F1 for NER."}, {"heading": "1 Introduction", "text": "Linguistic sequence labeling, such as part-ofspeech (POS) tagging and named entity recognition (NER), is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. Natural language processing (NLP) systems, like syntactic parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Koo and Collins, 2010; Ma and Zhao, 2012; Chen and Manning, 2014) and entity coreference resolution (Ng, 2010), are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems.\nMost traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015), which rely heavily on hand-crafted features and taskspecific resources. For example, English POS taggers benefit from carefully designed word spelling features; orthographic features and external resources such as gazetteers are widely used in NER. However, these task-specific knowledge are costly to develop, making sequence labeling models difficult to adapt to new tasks or new domains.\nIn the past few years, non-linear neural networks with as input distributed word representations, also known as word embeddings, has been broadly applied to NLP problems and achieve great success. Collobert et al. (2011) proposed a simple but effective feed-forward neutral network that independently classifies labels for each word by using contexts within a window with fixed size. Recently, recurrent neural networks (RNN) (Goller and Kuchler, 1996), together with its variants such as long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) and gated recurrent unit (GRU) (Cho et al., 2014), have shown great success in modeling sequential data. Several RNNbased neural network models have been proposed to solve sequence labeling tasks like speech recognition (Graves et al., 2013), POS tagging (Huang et al., 2015) and NER (Chiu and Nichols, 2015), achieving competitive performance against traditional models. However, even systems that have utilized distributed representations as inputs have used these to augment, rather than replace, handcrafted features (e.g. features about word spelling and capitalization patterns). The performance drops rapidly when the models solely depend on neural embeddings. ar X iv :1 60 3. 01 35 4v 1 [ cs\n.L G\n] 4\nM ar\n2 01\n6\nIn this paper, we propose a neural network architecture for sequence labeling. It is a truly endto-end model requiring no task-specific resources, feature engineering, or data pre-processing beyond pre-trained word embeddings on unlabeled corpora. Thus, our model can be easily applied to a wide range of sequence labeling tasks on different languages and domains. We first use convolutional neural networks (CNNs) (LeCun et al., 1989) to encode character-level information of a word into its character-level representation. Then we combine character- and word-level representations and feed them into bi-directional LSTM (BLSTM) to model context information of each word. On top of BLSTM, we use a sequential CRF to jointly decode labels for the whole sentence. We evaluate our model on two linguistic sequence labeling task \u2014 POS tagging on Penn Treebank WSJ (Marcus et al., 1993), and NER on English data from CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). Our end-to-end model outperforms previous stateof-the-art systems, obtaining 97.55% accuracy for POS tagging and 91.21% F1 for NER. The contributions of this work are (i) proposing a novel neural network architecture for linguistic sequence labeling. (ii) giving empirical evaluations of this model on benchmark data sets for two classic NLP tasks. (iii) achieving state-of-the-art performance with this truly end-to-end system."}, {"heading": "2 Neural Network Architecture", "text": "In this section, we describe the components (layers) of our neural network architecture. We introduce the neural layers in our neural network oneby-one from bottom to top."}, {"heading": "2.1 CNN for Character-level Representation", "text": "previous studies (Santos and Zadrozny, 2014; Chiu and Nichols, 2015) have shown that CNN is an effective approach to extract morphological information (like the prefix or suffix of a word) from characters of words and encode it into neural representations. Figure 1 shows the CNN we use to extract character-level representation of a given word. The CNN is similar to the one in Chiu and Nichols (2015), except that we only use character embeddings as the inputs of CNN, without character type features. A dropout layer (Srivastava et al., 2014) is applied before character embeddings are input to CNN."}, {"heading": "2.2 Bi-directional LSTM", "text": ""}, {"heading": "2.2.1 LSTM Unit", "text": "Recurrent neural networks (RNNs) are a powful family of connectionist models that capture time dynamics via cycles in the graph. Though, in theory, RNNs are capable to capturing long dependencies, in practice, they fail due to the gradient varnishing/exploding problems (Bengio et al., 1994; Pascanu et al., 2012).\nLSTMs (Hochreiter and Schmidhuber, 1997) are a variant of RNNs designed to cope with the gradient varnishing problems inherent in RNNs. Basically, a LSTM unit is composed of three multiplicative gates which control the proportions of information to forget and pass into next time step. Figure 2 gives the basic structure of a LSTM unit.\nFormally, the formulas to update a LSTM unit\nat time t are:\nit = \u03c3(W iht\u22121 +U ixt + bi) ft = \u03c3(W fht\u22121 +U fxt + bf ) c\u0303t = tanh(W cht\u22121 +U cxt + bc) ct = ft ct\u22121 + it c\u0303t ot = \u03c3(W oht\u22121 +U oxt + bo) ht = ot tanh(ct)\nwhere \u03c3 is the element-wise sigmoid function and is the element-wise product. xt is the input vector (e.g. word embedding) at time t, and ht is the hidden state (also called output) vector storing all the useful information at (and before) time t. U i,U f ,U c,U o denote the weight matrices of different gates for input xt, and W i,W f ,W c,W o are the weight matrices for hidden state ht. bi, bf , bc, bo denote the bias vectors. It should be noted that we do not include peephole connections (Gers et al., 2003) in the our LSTM formulation."}, {"heading": "2.2.2 BLSTM", "text": "For many sequence labeling tasks it is beneficial to have access to both past (left) and future (right) contexts. However, the LSTM\u2019s hidden state ht only takes information from past, knowing nothing about the future. An elegant solution whose effectiveness has been proven by previous work (Graves and Schmidhuber, 2005; Dyer et al., 2015) is bi-directional LSTM (BLSTM). The basic idea is to present each sequence forwards and backwards to two separate hidden states to capture past and future information, respectively. Then the two hidden states are concatenated to form the final output."}, {"heading": "2.3 CRF", "text": "For sequence labeling (or general structured prediction) tasks, it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence. For example, in POS tagging an adjective is more likely to be followed by a noun than a verb, and in NER with standard BIO2 annotation (Tjong Kim Sang and Veenstra, 1999) I-ORG cannot follow I-PER. Therefore, we model label sequence jointly using a conditional random fields (CRF) (Lafferty et al., 2001), instead of decoding each label independently.\nFormally, we use z = {z1, \u00b7 \u00b7 \u00b7 , zn} to represent a generic input sequence where zi is the input\nvector of the ith word. y = {y1, \u00b7 \u00b7 \u00b7 , yn} represents a generic sequence of labels for z. Y(z) denotes the set of possible label sequences for z. The probabilistic model for sequence CRF defines a family of conditional probability p(y|z;W,b) over all possible label sequence y given z with the following form:\np(y|z;W,b) =\nn\u220f i=1\n\u03c8i(yi\u22121, yi, z)\u2211 y\u2032\u2208Y(z) n\u220f i=1 \u03c8i(y\u2032i\u22121, y \u2032 i, z)\nwhere \u03c8i(y\u2032, y, z) = exp(WTy\u2032,yzi + by\u2032,y) are potential functions, and WTy\u2032,y and by\u2032,y are the weight vector and bias corresponding to label pair (y\u2032, y), respectively.\nFor CRF training, we use the maximum conditional likelihood estimation. For a training set {(zi,yi)}, the logarithm of the likelihood (a.k.a. the log-likelihood) is given by:\nL(W,b) = \u2211 i log p(y|z;W,b)\nMaximum likelihood training chooses parameters such that the log-likelihood L(W,b) is maximized.\nDecoding is to search for the label sequence y\u2217\nwith the highest conditional probability:\ny\u2217 = argmax y\u2208Y(z) p(y|z;W,b)\nFor a sequence CRF model (only interactions between two successive labels are considered), training and decoding can be solved efficiently by adopting the Viterbi algorithm (Lafferty et al., 2001)."}, {"heading": "2.4 BLSTM-CNNs-CRF", "text": "Finally, we construct our neural network model by feeding the output vectors of BLSTM into a CRF layer. Figure 3 illustrates the architecture of our network in details.\nFor each word, the character-level representation is computed by the CNN in Figure1 with character embeddings as inputs. Then the characterlevel representation vector is concatenated with the word embedding vector to feed into the BLSTM network. Finally, the output vectors of BLSTM are fed to the CRF layer to jointly decode the best label sequence. As shown in Figure 3, dropout layers are applied on both the input and\noutput vectors of BLSTM. Experimental results show that using dropout significantly improve the performance of our model (See section 4.5 for details)."}, {"heading": "3 Network Training", "text": "In this section, we provide the details of the approach for training the neural network. We implement the neural network using Theano library (Bergstra et al., 2010). The computations for a single model are run on a GeForce GTX TITAN X GPU. Using the settings discussed in this section, model training requires about 12 hours for POS tagging and 8 hours for NER."}, {"heading": "3.1 Parameter Initialization", "text": "Word Embeddings. We use publicly available Stanford\u2019s GloVe 100 dimensional embeddings1\n1http://nlp.stanford.edu/projects/ glove/\ntrained on 6 billion words from Wikipedia and web text (Pennington et al., 2014)\nWe also run experiments on two other sets of published embeddings, namely Senna 50 dimensional embeddings2 trained on Wikipedia and Reuters RCV-1 corpus (Collobert et al., 2011), and Google\u2019s Word2Vec 300 dimensional embeddings3 trained on 100 billion words from Google News (Mikolov et al., 2013). To test the effectiveness of pretrained word embeddings, we experimented with randomly initialized embeddings with 100 dimensions, where embeddings are uni-\nformly sampled from range [\u2212 \u221a\n3 dim ,+\n\u221a 3\ndim ]\nwhere dim is the dimension of embeddings (He et al., 2015). The performance of different word embeddings is discussed in Section 4.4. Character Embeddings. Character embeddings are initialized with uniform samples in\n[\u2212 \u221a\n3 dim ,+\n\u221a 3\ndim ], where we set dim = 30. Weight Matrices and Bias Vectors. Matrix parameters are randomly initialized with uniform\nsample from [\u2212 \u221a\n6 r+c ,+\n\u221a 6\nr+c ], where r and c are the number of of rows and columns in the structure (Glorot and Bengio, 2010). Bias vectors are initialized to zero, expect the bias bf for forget gate in LSTM , which is initialized to one (Jozefowicz et al., 2015)."}, {"heading": "3.2 Optimization Algorithm", "text": "Parameter optimization is performed with minibatch stochastic gradient descent (SGD) with batch size 10 and momentum 0.9. We choose an initial learning rate of \u03b70 (\u03b70 = 0.01 for POS tagging, and 0.015 for NER, see Section 3.3.), and the learning rate is updated on each epoch of training as \u03b7t = \u03b70/(1+ \u03c1t), with decay rate \u03c1 = 0.05 and t is the number of epoch completed. To reduce the effects of \u201cgradient exploding\u201d, we use a gradient clipping of 5.0 (Pascanu et al., 2012). We explored other more sophisticated optimization algorithms such as AdaDelta (Zeiler, 2012), Adam (Kingma and Ba, 2014) or RMSProp (Dauphin et al., 2015), none of them meaningfully improve upon SGD with momentum and gradient clipping in our preliminary experiments. Early Stopping. We use early stopping (Giles, 2001; Graves et al., 2013) based on performance\n2http://ronan.collobert.com/senna/ 3https://code.google.com/archive/p/\nword2vec/\non development sets. The \u201cbest\u201d parameters appear at around 50 epochs, according to our experiments. Fine Tuning. For each of the embeddings, we fine-tune initial embeddings, modifying the embeddings during gradient updates of the neural network model by back-propagating gradients. The effectiveness of this method has been previously explored in sequential and structured prediction problems (Collobert et al., 2011; Peng and Dredze, 2015). Dropout Training. To mitigate overfitting, we apply dropout method (Srivastava et al., 2014) to regularize our model. As shown in Figure 1 and Figure 3, we apply dropout layers on character embeddings before inputting to CNN, and on both the input and output vectors of BLSTM. We fix dropout rate at 0.5 for all dropout layers through all the experiments. We obtain significant improvements on model performance after using dropout (see Section 4.5)."}, {"heading": "3.3 Tuning Hyper-Parameters", "text": "Table 1 summarizes the chosen hyper-parameters for all experiments. We tune the hyper-parameters on the development sets by random search. Due to time constrains it is unfeasible to do a random search across the full hyper-parameter space. Thus, for the tasks of POS tagging and NER we try to use as most shared hyper-parameters as possible. Note that the final hyper-parameters for these two tasks are almost the same, except the initial learning rate. We set the state size of LSTM to 200. Tuning this parameter did not significantly impact the performance of our model. For CNN, we use 30 filters with window width 3."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data Sets", "text": "As mentioned before, we evaluate our neural network model on two sequence labeling tasks: POS tagging and NER. POS Tagging. For English POS tagging, we use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB) (Marcus et al., 1993), which contains 45 different POS tags. In order to compare with previous work, we adopt the standard splits \u2014 section 0-18 as training data, section 19- 21 as development data and section 22-24 as test data (Manning, 2011; S\u00f8gaard, 2011). NER. For NER, We perform experiments on the English data from CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). This data set contains four different types of named entities: PERSON, LOCATION, ORGANIZATION, and MISC. We use the BIOES tagging scheme instead of standard BIO2, as previous studies have reported meaningful improvement with this scheme over others like BIO2 (Ratinov and Roth, 2009; Chiu and Nichols, 2015; Dai et al., 2015).\nThe corpora statistics are shown in Table 2. We did not perform any pre-processing for data sets, leaving our system truly end-to-end."}, {"heading": "4.2 Main Results", "text": "We first run experiments to dissect the effectiveness of each component (layer) of our neural network architecture by ablation studies. We compare the performance with three baseline systems \u2014 BRNN, the bi-direction RNN; BLSTM, the bidirection LSTM and BLSTM-CNNs, the combination of BLSTM with CNN to model characterlevel information. All these models are running with Stanford\u2019s GloVe 100 dimensional word embeddings and the same hyper-parameters as shown\nin Table 1. According to the results, BLSTM obtains better performance than BRNN on all the evaluation metrics of both the two tasks. BLSTMCNN models significantly outperform the BLSTM model, showing that character-level representations are important for linguistic sequence labeling tasks. This is consistent with results reported by previous work (Santos and Zadrozny, 2014; Chiu and Nichols, 2015). Finally, by adding CRF layer for joint decoding we achieve significant improvements over BLSTM-CNN models for both POS tagging and NER on all metrics. This demonstrates that jointly decoding label sequences can significantly benefit the final performance of neural network models."}, {"heading": "4.3 Comparison with Previous Work", "text": ""}, {"heading": "4.3.1 POS Tagging", "text": "Table 4 illustrates the results of our model for POS tagging, together with seven previous topperformance systems for comparison. Our model significantly outperform Senna (Collobert et al., 2011), which is a feed-forward neural network model using capital and discrete suffix features,\nand data pre-processing. Moreover, our model achieves 0.23% improvements on accuracy over the \u201cCharWNN\u201d (Santos and Zadrozny, 2014), which is a neural network model based on Senna and also uses CNNs to model character-level representations. This demonstrates the effectiveness of BLSTM for modeling sequential data and the importance of joint decoding with structured prediction model.\nComparing with traditional statistical models, our system achieves state-of-the-art accuracy, obtaining 0.05% improvement over the previously best reported results by S\u00f8gaard (2011). It should be noted that Huang et al. (2015) also evaluated their BLSTM-CRF model for POS tagging on WSJ corpus. But they used a different splitting of the training/dev/test data sets. Thus, their results are not directly comparable with ours."}, {"heading": "4.3.2 NER", "text": "Table 5 shows the F1 scores of previous models for NER on the test data set from CoNLL-2003 shared task. For the purpose of comparison, we list their\nresults together with ours. Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015). Huang et al. (2015) utilized discrete spelling, pos and context features, and Chiu and Nichols (2015) used character-type, capitalization, and lexicon features and some taskspecific data pre-processing, while our model does not require any carefully designed features or data pre-processing. We have to point out that the result (90.77%) reported by Chiu and Nichols (2015) is incomparable with ours since their final model was trained on the combination of training and development data sets4.\nTo our knowledge, the previous best F1 score (91.20)5 reported on CoNLL 2003 data set is by the joint NER and entity linking model (Luo et al., 2015). This model used a lot of hand-crafted features including stemming and spelling features, POS and chunks tags, WordNet clusters, Brown Clusters, as well as external knowledge bases such as Freebase and Wikipedia. Our end-to-end model slightly improves this model by 0.01%, yielding a state-of-the-art performance."}, {"heading": "4.4 Word Embeddings", "text": "As mentioned in Section 3.1, in order to test the importance of pretrained word embeddings, we performed experiments with different sets of publicly published word embeddings, as well as a randomly sampling method, to initialize our model. Table 6 gives the performance of three different word embeddings, as well as the randomly sampled one. According to the results in Table 6,\n4We run experiments using the same setting and get % F1 score.\n5Numbers are taken from the Table 3 of the original paper (Luo et al., 2015). While there is clearly inconsistency among the precision, recall and F1 scores, it is unclear in which way they are incorrect.\nmodels using pretrained word embeddings obtain a significant improvement as opposed to the ones using random embeddings. Comparing the two tasks, NER relies more heavily on pretrained embeddings than POS tagging. This is consistent with results reported by previous work (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015).\nFor different pretrained embeddings, Stanford\u2019s GloVe 100 dimensional embeddings achieve best results on both the two tasks, about 0.1% better on POS accuracy and 0.9% better on NER F1 score than the Senna 50 dimensional one. This is different from the results reported by Chiu and Nichols (2015), where Senna achieved slightly better performance on NER than other embeddings. Google\u2019s Word2Vec 300 dimensional embeddings obtain similar performance with Senna on POS tagging, while slightly behind GloVe. But for NER, the performance on Word2Vec is about 2.1% behind GloVe and 1.2% behind Senna."}, {"heading": "4.5 Effect of Dropout", "text": "Table 7 compares the results with and without dropout layers for each data set. All other hyperparameters remain the same as in Table 1. We observe a essential improvement for both the two tasks. It demonstrates the effectiveness of dropout in reducing overfitting."}, {"heading": "5 Related Work", "text": "In recent years, several different neural network architectures have been proposed and successfully applied to linguistic sequence labeling such as POS tagging, chunking and NER. The two most similar approaches to our model, among these neural architectures, are the BLSTM-CRF model proposed by Huang et al. (2015) and the LSTMCNNs model presented by Chiu and Nichols (2015).\nHuang et al. (2015) used bi-directional LSTM for word-level representations and CRF for jointly label decoding, which is similar to our model. But there are two main differences between their\nand our models. First, they did not employ CNNs to model character-level information. Second, they combined their neural network model with hand-crafted features to improve their performance, making their model not an end-to-end system. Chiu and Nichols (2015) proposed a hybrid of bi-directional LSTM and CNNs to model both character- and word-level representations, which is similar to the first two layers in our model. They evaluated their model on NER task and achieved competitive performance. Our model mainly differ from this model by using CRF for joint decoding. Moreover, their model is not truly end-to-end, either, because they utilized external knowledge such as character-type, capitalization and lexicon features, and some data pre-processing specifically for NER (e.g. replacing all sequences of digits 0-9 with a single \u201c0\u201d)\nThere are several other neural networks previously proposed for sequence labeling. Labeau et al. (2015) proposed a RNN-CNNs model for German POS tagging. This model is similar to the LSTM-CNNs model in Chiu and Nichols (2015), with the difference of using vanila RNN instead of LSTM. Another neural architecture employing CNN to model character-level information is the \u201cCharWNN\u201d architecture (Santos and Zadrozny, 2014) which is inspired by the feed-forward network (Collobert et al., 2011). CharWNN obtained near state-of-the-art accuracy on English POS tagging (see section 4.3 for details). Similar model has also been applied to Spanish and Portuguese NER (dos Santos et al., 2015)"}, {"heading": "6 Conclusion", "text": "In this paper, we proposed a neural network architecture for sequence labeling. We model both character- and word-level representations by employing CNN and bi-directional LSTM, respectively. On top of BLSTM, we use a sequential CRF to jointly decode the sequence of labels for the given sentence. It is a truly end-to-end model relying on no task-specific resources, feature engineering or data pre-processing. Experimental results on two linguistic sequence label tasks \u2014 POS tagging and NER \u2014 demonstrate the effectiveness of our model. We achieved state-of-theart performance on both the two tasks, comparing with previously state-of-the-art systems which are highly-engineered with hand-crafted features, data pre-processing and external resources.\nThere are several potential directions for future work. First, our model can be further improved by exploring multi-task learning approaches to combine more useful and correlated information. For example, we can jointly train a neural network model with both the POS and NER tags to improve the intermediate representations learned in our network. Another interesting direction is to apply our model to data from other domains such as social media, twitter and weibo. Since our model does not require any domain or task specific knowledge, it might be promising to apply our model to these domains painlessly."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of EMNLP-2014,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Named entity recognition: a maximum entropy approach using global information", "author": ["Chieu", "Ng2002] Hai Leong Chieu", "Hwee Tou Ng"], "venue": "In Proceedings of CoNLL-2003,", "citeRegEx": "Chieu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2002}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308", "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches. Syntax, Semantics and Structure in Statistical Translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Kuksa.,? \\Q2011\\E", "shortCiteRegEx": "Kuksa.", "year": 2011}, {"title": "Enhancing of chemical compound and drug name recognition using representative tag scheme and fine-grained tokenization", "author": ["Dai et al.2015] Hong-Jie Dai", "Po-Ting Lai", "Yung-Chun Chang", "Richard Tzong-Han Tsai"], "venue": "Journal of cheminformat-", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization. arXiv preprint arXiv:1502.04390", "author": ["Harm de Vries", "Junyoung Chung", "Yoshua Bengio"], "venue": null, "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "In Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of ACL-2015", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In Proceedings of HLT-NAACL-2003,", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers et al.2000] Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers et al.2003] Felix A Gers", "Nicol N Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping", "author": [], "venue": "In Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference,", "citeRegEx": "Giles.,? \\Q2001\\E", "shortCiteRegEx": "Giles.", "year": 2001}, {"title": "Svmtool: A general pos tagger generator based on support vector machines", "author": ["Gim\u00e9nez", "M\u00e0rquez2004] Jes\u00fas Gim\u00e9nez", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "Proceedings of LREC-2004", "citeRegEx": "Gim\u00e9nez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gim\u00e9nez et al\\.", "year": 2004}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Learning task-dependent distributed representations by backpropagation through", "author": ["Goller", "Kuchler1996] Christoph Goller", "Andreas Kuchler"], "venue": null, "citeRegEx": "Goller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Goller et al\\.", "year": 1996}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013] Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Proceedings of ICASSP-2013,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He et al.2015] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Efficient third-order dependency parsers", "author": ["Koo", "Collins2010] Terry Koo", "Michael Collins"], "venue": "In Proceedings of ACL-2010,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Non-lexical neural architecture for finegrained pos tagging", "author": ["Kevin L\u00f6ser", "Alexandre Allauzen", "Rue John von Neumann"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Labeau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": "In Proceedings of ICML2001,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun et al.1989] Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In Proceedings of ACL-2009,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Joint entity recognition and disambiguation", "author": ["Luo et al.2015] Gang Luo", "Xiaojiang Huang", "ChinYew Lin", "Zaiqing Nie"], "venue": "In Proceedings of EMNLP-2015,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Fourth-order dependency parsing", "author": ["Ma", "Zhao2012] Xuezhe Ma", "Hai Zhao"], "venue": "In Proceedings of COLING 2012: Posters,", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "Partof-speech tagging from 97% to 100%: is it time for some linguistics", "author": ["Christopher D Manning"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "Manning.,? \\Q2011\\E", "shortCiteRegEx": "Manning.", "year": 2011}, {"title": "Building a large annotated corpus of English: the Penn Treebank", "author": ["Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online largemargin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In Proceedings of ACL-2005,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Supervised noun phrase coreference research: The first fifteen years", "author": ["Vincent Ng"], "venue": "In Proceedings of ACL-2010,", "citeRegEx": "Ng.,? \\Q2010\\E", "shortCiteRegEx": "Ng.", "year": 2010}, {"title": "Deterministic dependency parsing of English text", "author": ["Nivre", "Scholz2004] Joakim Nivre", "Mario Scholz"], "venue": "In Proceedings of COLING-2004,", "citeRegEx": "Nivre et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2004}, {"title": "On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "In Proceedings of CoNLL-2014,", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Named entity recognition for chinese social media with jointly trained embeddings", "author": ["Peng", "Dredze2015] Nanyun Peng", "Mark Dredze"], "venue": "In Proceedings of EMNLP-2015,", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP-2014,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of CoNLL-2009,", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D Santos", "Bianca Zadrozny"], "venue": "In Proceedings of ICML-2014,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Guided learning for bidirectional sequence classification", "author": ["Shen et al.2007] Libin Shen", "Giorgio Satta", "Aravind Joshi"], "venue": "In Proceedings of ACL-2007,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Semisupervised condensed nearest neighbor for part-ofspeech tagging", "author": ["Anders S\u00f8gaard"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "S\u00f8gaard.,? \\Q2011\\E", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Structure regularization for structured prediction", "author": ["Xu Sun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sun.,? \\Q2014\\E", "shortCiteRegEx": "Sun.", "year": 2014}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of CoNLL-2003 - Volume", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Representing text chunks", "author": ["Tjong Kim Sang", "Jorn Veenstra"], "venue": "In Proceedings of EACL\u201999,", "citeRegEx": "Sang et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sang et al\\.", "year": 1999}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of NAACL-HLT2003,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 35, "context": "Natural language processing (NLP) systems, like syntactic parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Koo and Collins, 2010; Ma and Zhao, 2012; Chen and Manning, 2014) and entity coreference resolution (Ng, 2010), are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems.", "startOffset": 66, "endOffset": 179}, {"referenceID": 37, "context": ", 2005; Koo and Collins, 2010; Ma and Zhao, 2012; Chen and Manning, 2014) and entity coreference resolution (Ng, 2010), are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems.", "startOffset": 108, "endOffset": 118}, {"referenceID": 40, "context": "Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015), which rely heavily on hand-crafted features and taskspecific resources.", "startOffset": 163, "endOffset": 226}, {"referenceID": 31, "context": "Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015), which rely heavily on hand-crafted features and taskspecific resources.", "startOffset": 163, "endOffset": 226}, {"referenceID": 13, "context": "Recently, recurrent neural networks (RNN) (Goller and Kuchler, 1996), together with its variants such as long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) and gated recurrent unit (GRU) (Cho et al.", "startOffset": 135, "endOffset": 188}, {"referenceID": 6, "context": ", 2000) and gated recurrent unit (GRU) (Cho et al., 2014), have shown great success in modeling sequential data.", "startOffset": 39, "endOffset": 57}, {"referenceID": 20, "context": "Several RNNbased neural network models have been proposed to solve sequence labeling tasks like speech recognition (Graves et al., 2013), POS tagging (Huang et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 23, "context": ", 2013), POS tagging (Huang et al., 2015) and NER (Chiu and Nichols, 2015), achieving competitive performance against traditional models.", "startOffset": 21, "endOffset": 41}, {"referenceID": 29, "context": "We first use convolutional neural networks (CNNs) (LeCun et al., 1989) to encode character-level information of a word into its character-level representation.", "startOffset": 50, "endOffset": 70}, {"referenceID": 34, "context": "We evaluate our model on two linguistic sequence labeling task \u2014 POS tagging on Penn Treebank WSJ (Marcus et al., 1993), and NER on English data from CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003).", "startOffset": 98, "endOffset": 119}, {"referenceID": 47, "context": "A dropout layer (Srivastava et al., 2014) is applied before character embeddings are input to CNN.", "startOffset": 16, "endOffset": 41}, {"referenceID": 1, "context": "Though, in theory, RNNs are capable to capturing long dependencies, in practice, they fail due to the gradient varnishing/exploding problems (Bengio et al., 1994; Pascanu et al., 2012).", "startOffset": 141, "endOffset": 184}, {"referenceID": 39, "context": "Though, in theory, RNNs are capable to capturing long dependencies, in practice, they fail due to the gradient varnishing/exploding problems (Bengio et al., 1994; Pascanu et al., 2012).", "startOffset": 141, "endOffset": 184}, {"referenceID": 14, "context": "It should be noted that we do not include peephole connections (Gers et al., 2003) in the our LSTM formulation.", "startOffset": 63, "endOffset": 82}, {"referenceID": 11, "context": "An elegant solution whose effectiveness has been proven by previous work (Graves and Schmidhuber, 2005; Dyer et al., 2015) is bi-directional LSTM (BLSTM).", "startOffset": 73, "endOffset": 122}, {"referenceID": 28, "context": "Therefore, we model label sequence jointly using a conditional random fields (CRF) (Lafferty et al., 2001), instead of decoding each label independently.", "startOffset": 83, "endOffset": 106}, {"referenceID": 28, "context": "For a sequence CRF model (only interactions between two successive labels are considered), training and decoding can be solved efficiently by adopting the Viterbi algorithm (Lafferty et al., 2001).", "startOffset": 173, "endOffset": 196}, {"referenceID": 2, "context": "We implement the neural network using Theano library (Bergstra et al., 2010).", "startOffset": 53, "endOffset": 76}, {"referenceID": 42, "context": "glove/ trained on 6 billion words from Wikipedia and web text (Pennington et al., 2014)", "startOffset": 62, "endOffset": 87}, {"referenceID": 36, "context": ", 2011), and Google\u2019s Word2Vec 300 dimensional embeddings3 trained on 100 billion words from Google News (Mikolov et al., 2013).", "startOffset": 105, "endOffset": 127}, {"referenceID": 21, "context": "3 dim ] where dim is the dimension of embeddings (He et al., 2015).", "startOffset": 49, "endOffset": 66}, {"referenceID": 24, "context": "Bias vectors are initialized to zero, expect the bias bf for forget gate in LSTM , which is initialized to one (Jozefowicz et al., 2015).", "startOffset": 111, "endOffset": 136}, {"referenceID": 39, "context": "0 (Pascanu et al., 2012).", "startOffset": 2, "endOffset": 24}, {"referenceID": 52, "context": "We explored other more sophisticated optimization algorithms such as AdaDelta (Zeiler, 2012), Adam (Kingma and Ba, 2014) or RMSProp (Dauphin et al.", "startOffset": 78, "endOffset": 92}, {"referenceID": 9, "context": "We explored other more sophisticated optimization algorithms such as AdaDelta (Zeiler, 2012), Adam (Kingma and Ba, 2014) or RMSProp (Dauphin et al., 2015), none of them meaningfully improve upon SGD with momentum and gradient clipping in our preliminary experiments.", "startOffset": 132, "endOffset": 154}, {"referenceID": 15, "context": "We use early stopping (Giles, 2001; Graves et al., 2013) based on performance", "startOffset": 22, "endOffset": 56}, {"referenceID": 20, "context": "We use early stopping (Giles, 2001; Graves et al., 2013) based on performance", "startOffset": 22, "endOffset": 56}, {"referenceID": 47, "context": "To mitigate overfitting, we apply dropout method (Srivastava et al., 2014) to regularize our model.", "startOffset": 49, "endOffset": 74}, {"referenceID": 34, "context": "For English POS tagging, we use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB) (Marcus et al., 1993), which contains 45 different POS tags.", "startOffset": 93, "endOffset": 114}, {"referenceID": 33, "context": "In order to compare with previous work, we adopt the standard splits \u2014 section 0-18 as training data, section 1921 as development data and section 22-24 as test data (Manning, 2011; S\u00f8gaard, 2011).", "startOffset": 166, "endOffset": 196}, {"referenceID": 46, "context": "In order to compare with previous work, we adopt the standard splits \u2014 section 0-18 as training data, section 1921 as development data and section 22-24 as test data (Manning, 2011; S\u00f8gaard, 2011).", "startOffset": 166, "endOffset": 196}, {"referenceID": 8, "context": "We use the BIOES tagging scheme instead of standard BIO2, as previous studies have reported meaningful improvement with this scheme over others like BIO2 (Ratinov and Roth, 2009; Chiu and Nichols, 2015; Dai et al., 2015).", "startOffset": 154, "endOffset": 220}, {"referenceID": 47, "context": "16 Toutanova et al. (2003) 97.", "startOffset": 3, "endOffset": 27}, {"referenceID": 33, "context": "27 Manning (2011) 97.", "startOffset": 3, "endOffset": 18}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.", "startOffset": 3, "endOffset": 48}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.29 Santos and Zadrozny (2014)\u2021 97.", "startOffset": 3, "endOffset": 82}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.29 Santos and Zadrozny (2014)\u2021 97.32 Shen et al. (2007) 97.", "startOffset": 3, "endOffset": 108}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.29 Santos and Zadrozny (2014)\u2021 97.32 Shen et al. (2007) 97.33 Sun (2014) 97.", "startOffset": 3, "endOffset": 125}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.29 Santos and Zadrozny (2014)\u2021 97.32 Shen et al. (2007) 97.33 Sun (2014) 97.36 S\u00f8gaard (2011) 97.", "startOffset": 3, "endOffset": 146}, {"referenceID": 34, "context": "Chieu and Ng (2002) 88.", "startOffset": 10, "endOffset": 20}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.", "startOffset": 3, "endOffset": 25}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.", "startOffset": 3, "endOffset": 53}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.", "startOffset": 3, "endOffset": 83}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.", "startOffset": 3, "endOffset": 110}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.", "startOffset": 3, "endOffset": 141}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.77 Ratinov and Roth (2009) 90.", "startOffset": 3, "endOffset": 172}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.77 Ratinov and Roth (2009) 90.80 Lin and Wu (2009) 90.", "startOffset": 3, "endOffset": 196}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.77 Ratinov and Roth (2009) 90.80 Lin and Wu (2009) 90.90 Passos et al. (2014) 90.", "startOffset": 3, "endOffset": 223}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.77 Ratinov and Roth (2009) 90.80 Lin and Wu (2009) 90.90 Passos et al. (2014) 90.90 Luo et al. (2015) 91.", "startOffset": 3, "endOffset": 247}, {"referenceID": 45, "context": "05% improvement over the previously best reported results by S\u00f8gaard (2011). It should be noted that Huang et al.", "startOffset": 61, "endOffset": 76}, {"referenceID": 23, "context": "It should be noted that Huang et al. (2015) also evaluated their BLSTM-CRF model for POS tagging on WSJ corpus.", "startOffset": 24, "endOffset": 44}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015).", "startOffset": 164, "endOffset": 184}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015). Huang et al.", "startOffset": 164, "endOffset": 234}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015). Huang et al. (2015) utilized discrete spelling, pos and context features, and Chiu and Nichols (2015) used character-type, capitalization, and lexicon features and some taskspecific data pre-processing, while our model does not require any carefully designed features or data pre-processing.", "startOffset": 164, "endOffset": 255}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015). Huang et al. (2015) utilized discrete spelling, pos and context features, and Chiu and Nichols (2015) used character-type, capitalization, and lexicon features and some taskspecific data pre-processing, while our model does not require any carefully designed features or data pre-processing.", "startOffset": 164, "endOffset": 337}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015). Huang et al. (2015) utilized discrete spelling, pos and context features, and Chiu and Nichols (2015) used character-type, capitalization, and lexicon features and some taskspecific data pre-processing, while our model does not require any carefully designed features or data pre-processing. We have to point out that the result (90.77%) reported by Chiu and Nichols (2015) is incomparable with ours since their final model was trained on the combination of training and development data sets4.", "startOffset": 164, "endOffset": 609}, {"referenceID": 31, "context": "20)5 reported on CoNLL 2003 data set is by the joint NER and entity linking model (Luo et al., 2015).", "startOffset": 82, "endOffset": 100}, {"referenceID": 31, "context": "Numbers are taken from the Table 3 of the original paper (Luo et al., 2015).", "startOffset": 57, "endOffset": 75}, {"referenceID": 23, "context": "This is consistent with results reported by previous work (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015).", "startOffset": 58, "endOffset": 126}, {"referenceID": 23, "context": "The two most similar approaches to our model, among these neural architectures, are the BLSTM-CRF model proposed by Huang et al. (2015) and the LSTMCNNs model presented by Chiu and Nichols (2015).", "startOffset": 116, "endOffset": 136}, {"referenceID": 23, "context": "The two most similar approaches to our model, among these neural architectures, are the BLSTM-CRF model proposed by Huang et al. (2015) and the LSTMCNNs model presented by Chiu and Nichols (2015).", "startOffset": 116, "endOffset": 196}, {"referenceID": 26, "context": "Labeau et al. (2015) proposed a RNN-CNNs model for German POS tagging.", "startOffset": 0, "endOffset": 21}, {"referenceID": 26, "context": "Labeau et al. (2015) proposed a RNN-CNNs model for German POS tagging. This model is similar to the LSTM-CNNs model in Chiu and Nichols (2015), with the difference of using vanila RNN instead of LSTM.", "startOffset": 0, "endOffset": 143}], "year": 2017, "abstractText": "State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both wordand character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks on different languages. We evaluate our system on two data sets for two sequence labeling tasks \u2014 Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain stateof-the-art performance on both the two data \u2014 97.55% accuracy for POS tagging and 91.21% F1 for NER.", "creator": "LaTeX with hyperref package"}}}