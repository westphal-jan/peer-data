{"id": "1603.01855", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2016", "title": "Online Learning to Rank with Feedback at the Top", "abstract": "We rather an advertisers cognitive if uniform setting once both, , another round, referring oblivious brute staggering a list of $ m $ collecting, pertaining then it scripting, and the student-centered produces scores given equal the disclosing. The unyielding, revenue set relevance exponential in well jdbc deadline its ranker according to several motivation received. We regard the their entire soon feedback is largely to any this relevance normally of only been top $ k $ information in still ranked currently instead $ | \\ douanes 110 $. However, another improved though learner is convincingly marketing a although unrevealed need diversity vectors, for be considering mind to rank loss formula_11. We develop efficient iterations for that regarded losses today, pointwise, pairwise and listwise ages. We are unlikely likely no customers simplest may unlike sublinear apologies, with top - 2003 feedback, own those loss that another reproducible with equal to NDCG. We apply rest methodologies moving dealers rdf demonstrating provide online learning of of eighth formula_6 some unlike exception feedback.", "histories": [["v1", "Sun, 6 Mar 2016 18:43:54 GMT  (217kb,D)", "http://arxiv.org/abs/1603.01855v1", "Appearing in AISTATS 2016"]], "COMMENTS": "Appearing in AISTATS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sougata chaudhuri", "ambuj tewari"], "accepted": false, "id": "1603.01855"}, "pdf": {"name": "1603.01855.pdf", "metadata": {"source": "CRF", "title": "Online Learning to Rank with Feedback at the Top", "authors": ["Sougata Chaudhuri", "Ambuj Tewari"], "emails": [], "sections": [{"heading": null, "text": "We consider an online learning to rank setting in which, at each round, an oblivious adversary generates a list of m documents, pertaining to a query, and the learner produces scores to rank the documents. The adversary then generates a relevance vector and the learner updates its ranker according to the feedback received. We consider the setting where the feedback is restricted to be the relevance levels of only the top k documents in the ranked list for k m. However, the performance of learner is judged based on the unrevealed full relevance vectors, using an appropriate learning to rank loss function. We develop efficient algorithms for well known losses in the pointwise, pairwise and listwise families. We also prove that no online algorithm can have sublinear regret, with top-1 feedback, for any loss that is calibrated with respect to NDCG. We apply our algorithms on benchmark datasets demonstrating efficient online learning of a ranking function from highly restricted feedback."}, {"heading": "1 Introduction", "text": "In learning to rank for information retrieval, the objective is to rank lists of documents, pertaining to different queries, so that the documents that are more relevant to a query are ranked above those that are less relevant. Most learning to rank methods are based on supervised batch learning, i.e., rankers are trained on batch data consisting of instances and labels [Liu, 2011]. The instances are lists of documents, pertaining to different queries, and labels are in the form of relevance vectors. The accuracy of a ranked list,\nAppearing in Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain. JMLR: W&CP volume 41. Copyright 2016 by the authors.\nin comparison to the actual relevance of the documents, is measured by various ranking measures, such as NDCG, AP and ERR.\nCollecting reliable training data can be expensive and time consuming. In certain applications, such as deploying a new web app or developing a custom search engine, collecting large amount of training data might not be possible at all [Sanderson, 2010]. Moreover, a ranker trained from batch data might not be able to satisfy changing user needs and preferences. Recent research has focused on online learning of ranking systems, where a ranker is updated on the fly. One direction of work deploys models which learn from implicit feedback inferred from user clicks on ranked lists [Hofmann et al., 2013, Yue and Joachims, 2009]. However, there are some potential drawbacks in learning from user clicks. It is possible that the displayed items might not be clickable, such as in certain mobile apps. Moreover, a clicked item might not actually be relevant to the user and there is also the problem of bias towards top ranked items in inferring feedback from user clicks [Joachims, 2002]. Another direction of work deploys models which learn optimal ranking of a fixed list of items, for diverse user preferences [Radlinski et al., 2008, Chaudhuri and Tewari, 2015]. Specifically, the latter work assumes that a user generates a full relevance vector for the entire ranked list of items but gives feedback only on the top ranked item. Motivation for this feedback model comes from considerations of user burden constraints (users will feel burdensome to provide careful feedback on all items) and privacy concerns (users will be unwilling to provide feedback on all items if they are about sensitive issues such as medical conditions). However, the requirement of having a fixed set of items to rank severely limits the practical applicability of this line of work.\nOur work extends the work of Chaudhuri and Tewari [2015], by combining query-level ranking, in an online manner, with explicit but restricted feedback. We formalize the problem as an online game played over T rounds, between a learner and an oblivious adversary. At each round, the adversary generates a document list of length m, pertaining to a query. The learner\nar X\niv :1\n60 3.\n01 85\n5v 1\n[ cs\n.L G\n] 6\nM ar\n2 01\n6\nsees the list and produces a real valued score vector to rank the documents. We assume that the ranking is generated by sorting the score vector in descending order of its entries. The adversary then generates a relevance vector but the learner gets to see the relevance of only the top-k items of the ranked list, where k m is a small constant, like 1 or 2. The learner\u2019s loss in each round, based on the learner\u2019s score vector and the full relevance vector, is measured by some continuous learning to rank loss function. We focus on continuous surrogates losses, e.g., the cross entropy surrogate in ListNet [Cao et al., 2007] and hinge surrogate in RankSVM [Joachims, 2002], instead of discontinuous ranking measures like NDCG, AP, or ERR because the latter lead to intractable optimization problems. We note that the top-k feedback model is distinct from the full and bandit feedback models since neither the full relevance vector nor the loss at end of each round is revealed to the learner. Technically, the problem is an instance of partial monitoring [Cesa-Bianchi et al., 2006, Bartok et al., 2014], extended to a setting with side information (documents list) and an infinite set of learner\u2019s moves (all real valued score vectors). For such an extension of partial monitoring there exists no generic theoretical or algorithmic framework to the best of our knowledge.\nWe make two main contributions in this paper. First, we propose a general, efficient algorithm for online learning to rank with top-k feedback and show that it works in conjunction with a number of ranking surrogates. We characterize the minimum feedback required, i.e., the value of k, for the algorithm to work with a particular surrogate by formally relating the feedback mechanism with the structure of the surrogates. We then apply our general techniques to three convex ranking surrogates and one non-convex surrogate. The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method [Cossock and Zhang, 2008], hinge loss used in the pairwise RankSVM [Joachims, 2002] method, and (modified) cross-entropy surrogate used in the listwise ListNet [Cao et al., 2007] method. The non-convex surrogate considered is the SmoothDCG surrogate [Chapelle and Wu, 2010]. For the three convex surrogates, we establish an O(T 2/3) regret bound.\nThe convex surrogates we mentioned above are widely used but are known to fail to be calibrated with respect to NDCG [Ravikumar et al., 2011]. Our second contribution is to show that for the entire class of NDCG calibrated surrogates, no online algorithm can have sublinear (in T ) regret with top-1 feedback, i.e., the minimax regret of an online game for any NDCG calibrated surrogate is \u2126(T ). The proof for this rather surprising result is non-trivial and relies on exploiting a\nconnection between the construction of optimal adversary strategies for hopeless finite action partial monitoring games [Piccolboni and Schindelhauer, 2001] and the structure of NDCG calibrated surrogates. We only focus on NDCG calibrated surrogates for the impossibility results since no (convex) surrogate can be calibrated for AP and ERR [Calauzenes et al., 2012]. This impossibility result is not only the first of its kind in online ranking with top-1 feedback but it also the first such result for a natural partial monitoring problem with side information when the learner\u2019s action space is infinite. Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information [Kleinberg and Leighton, 2003, Cesa-Bianchi et al., 2006], and vice versa [Barto\u0301k and Szepesva\u0301ri, 2012, Gentile and Orabona, 2014].\nWe apply our algorithms on benchmark ranking datasets, demonstrating the ability to efficiently learn a ranking function in an online fashion, from highly restricted feedback."}, {"heading": "2 Preliminaries", "text": "In learning to rank, an instance is a matrix X \u2208 Rm\u00d7d, consisting of a list of m documents, each represented as a feature vector in Rd, with each list pertaining to a single query. The supervision is in form of a relevance vector R = {0, 1, . . . , n}m, representing relevance of each document to the query. If n = 1, the relevance vector is binary graded. For n > 1, relevance vector is multi-graded. Xi: denotes ith row of X and Ri denotes ith component of R. The subscript t is exclusively used to denote time t. Thus, Rt denotes relevance vector generated at time t and Rt,i denotes ith component of Rt. We assume feature vectors representing documents are bounded by RD in `2 norm.\nDocuments are ranked by a ranking function. The prevalent technique is to represent a ranking function as a scoring function and get ranking by sorting scores in descending order. A linear scoring function produces score vector as fw(X) = Xw = s\nw \u2208 Rm, with w \u2208 Rd. Here, swi represents score of ith document (sw points to score s being generated by using parameter w). We assume that ranking parameter space is bounded in `2 norm, i.e, \u2016w\u20162 \u2264 U , \u2200 w. \u03c0s = argsort(s) is the permutation induced by sorting score vector s in descending order. A permutation \u03c0 gives a mapping from ranks to documents and \u03c0\u22121 gives a mapping from documents to ranks. Thus, \u03c0(i) = j means document j is placed at position i in \u03c0 while \u03c0\u22121(i) = j means document i is placed at position j. Sm denote the set of m! different permutations of [m] where [m] = {1, 2 . . . ,m}.\nVarious ranking measures, like NDCG and AP, judge the quality of a ranking function, by comparing the ranked lists produced by the ranking function and the relevance vector, respectively. Formally, NDCG, cut off at k \u2264 m for a query with m documents, with relevance vector R and score vector s induced by a ranking function, is defined as follows: NDCGk(s,R) =\n1 Zk(R) \u2211k i=1G(R\u03c0s(i))D(i). Shorthand representation of NDCGk(s,R) is NDCGk. Here, G(r) = 2 r \u2212 1, D(i) = 1log2 (i+1) , Zk(R) = max\n\u03c0\u2208Sm\n\u2211k i=1G(R\u03c0(i))D(i).\n\u03c0s = argsort(s) is the permutation induced by score vector s in descending order. Since optimization of the discontinuous ranking measures is an NP-hard problem, most ranking methods are based on minimizing surrogate losses, which can be optimized more efficiently. A surrogate \u03c6 takes in a score vector s and relevance vector R and produces a real number, i.e., \u03c6 : Rm\u00d7{0, 1, . . . , n}m 7\u2192 R. \u03c6(\u00b7, \u00b7) is said to be convex if it is convex in its first argument, for any value of the second argument. The ranking surrogates are designed in such a way that the score vector which minimizes the surrogate, induces a ranking which minimizes the target ranking measures."}, {"heading": "3 Problem Setting and Learning to Rank Algorithm", "text": "Formal problem setting: We formalize the problem as a game being played between a learner and an oblivious adversary over T rounds. The learner\u2019s action set is the uncountably infinite set of score vectors in Rm and the adversary\u2019s action set is all possible relevance vectors, i.e., (n + 1)m possible vectors. At round t, the adversary generates a list of documents, represented by a matrix Xt \u2208 Rm\u00d7d, pertaining to a query (the document list is considered as side information). The learner receives Xt and produces a score vector s\u0303t \u2208 Rm. The adversary then generates a relevance vector Rt but only reveals the relevance of top k ranked documents to the learner, where the ranked list is produced by sorting s\u0303t. The learner uses the feedback to choose its action for the next round (updates an internal scoring function). The learner suffers a loss as measured in terms of a surrogate \u03c6, i.e, \u03c6(s\u0303t, Rt). Note that since the learner\u2019s objective is to produce good ranking at every round, learner\u2019s performance is measured w.r.t. to entire relevance vector Rt whereas it only gets to see just the top-k entries of Rt. As is standard in online learning setting, the learner\u2019s performance is measured in\nterms of its expected regret: E [\u2211T t=1 \u03c6(s\u0303t, Rt) ] \u2212\nmin\u2016w\u20162\u2264U \u2211T t=1 \u03c6(Xtw,Rt), where the expectation is taken w.r.t. to randomization of learner\u2019s strategy and\nXtw = s w t is the score produced by the linear function parameterized by w.\nRelation between feedback and structure of surrogates: Alg. 1 is our general algorithm for learning a ranking function, online, from partial feedback. The key step in Alg. 1 is the construction of the unbiased estimator z\u0303t of the surrogate gradient \u2207w=wt\u03c6(Xtw,Rt). The information present for the construction process, at end of round t, is the random score vector s\u0303t (and associated permutation \u03c3\u0303t) and relevance of top-k items of \u03c3\u0303t, i.e., {Rt,\u03c3\u0303t(1), . . . , Rt,\u03c3\u0303t(k)}. Let Et [\u00b7] be the expectation operator w.r.t. to randomization at round t, conditioned on (w1, . . . , wt). Then z\u0303t being an unbiased estimator of gradient of surrogate, w.r.t wt, means the following: Et [z\u0303t] = \u2207w=wt\u03c6(Xtw,Rt). We note that conditioned on the past, the score vector swtt = Xtwt is deterministic. We start with a general result relating feedback to the construction of unbiased estimator of a vector valued function. Let P denote a probability distribution on Sm, i.e, \u2211 \u03c3\u2208Sm P(\u03c3) = 1. For a distinct set of indices (j1, j2, . . . , jk) \u2286 [m], we denote p(ji, j2, . . . , jk) as the the sum of probability of permutations whose first k objects match objects (j1, . . . , jk), in order. Formally,\np(j1, . . . , jk) = \u2211 \u03c0\u2208Sm P(\u03c0)1(\u03c0(1) = j1, . . . , \u03c0(k) = jk).\n(1)\nLemma 1. Let F : Rm 7\u2192 Ra be a vector valued function, where m \u2265 1, a \u2265 1. For a fixed x \u2208 Rm, let k entries of x be observed at random. That is, for a fixed probability distribution P and some random \u03c3 \u223c P(Sm), observed tuple is {\u03c3, x\u03c3(1), . . . , x\u03c3(k)}. A necessary condition for existence of an unbiased estimator of F (x), that can be constructed from {\u03c3, x\u03c3(1), . . . , x\u03c3(k)}, is that it should be possible to decompose F (x) over k (or less) coordinates of x at a time. That is, F (x) should have the structure:\nF (x) = \u2211\n(i1,i2,...,i`)\u2208 mP`\nhi1,i2,...,i`(xi1 , xi2 , . . . , xi`)\n(2) where ` \u2264 k, mP` is ` permutations of m and h : R` 7\u2192 Ra (the subscripts in h is used to simply represent different functions). Moreover, when F (x) can be written in form of Eq 2 , with ` = k, an unbiased estimator of F (x), based on {\u03c3, x\u03c3(1), . . . , x\u03c3(k)}, is,\ng(\u03c3,x\u03c3(1), . . . , x\u03c3(k)) =\u2211 (j1,j2,...,jk)\u2208Sk\nh\u03c3(j1),...,\u03c3(jk)(x\u03c3(j1), . . . , x\u03c3(jk))\u2211 (j1,...,jk)\u2208Sk p(\u03c3(j1), . . . , \u03c3(jk))\n(3)\nAlgorithm 1 Ranking with Top-k Feedback (RTop-kF)\n1: Exploration parameter \u03b3 \u2208 (0, 12 ), learning parameter \u03b7 > 0, ranking parameter w1 = 0 \u2208 R d 2: For t = 1 to T 3: Receive Xt (document list pertaining to query qt) 4: Construct score vector swtt = Xtwt and get permutation \u03c3t = argsort(s wt t ) 5: Qt(s) = (1\u2212 \u03b3)\u03b4(s\u2212 swtt ) + \u03b3Uniform([0, 1]m) (\u03b4 is the Dirac Delta function). 6: Sample s\u0303t \u223c Qt and output the ranked list \u03c3\u0303t = argsort(s\u0303t) (Effectively, it means \u03c3\u0303t is drawn from Pt(\u03c3) = (1\u2212 \u03b3)1(\u03c3 = \u03c3t) + \u03b3m! ) 7: Receive relevance feedback on top-k items, i.e., (Rt,\u03c3\u0303t(1), . . . , Rt,\u03c3\u0303t(k)) 8: Suffer loss \u03c6(s\u0303t, Rt) (Neither loss nor Rt revealed to learner) 9: Construct z\u0303t, an unbiased estimator of gradient \u2207w=wt\u03c6(Xtw,Rt), from top-k feedback. 10: Update w = wt \u2212 \u03b7z\u0303t 11: wt+1 = min{1, U\u2016w\u20162 }w (Projection into Euclidean ball of radius U). 12: End For\nwhere Sk is the set of k! permutations of [k] and p(\u03c3(1), . . . , \u03c3(k)) is as in Eq 1 .\nIllustrative Examples: We provide simple examples to concretely illustrate the abstract functions in Lemma 1. Let F (\u00b7) be the identity function, and x \u2208 Rm. Thus, F (x) = x and the function decomposes over k = 1 coordinate of x as follows: F (x) =\u2211m i=1 xiei, where ei \u2208 Rm is the standard basis vector along coordinate i. Hence, hi(xi) = xiei. Based on top-1 feedback, following is an unbiased estimator of F (x): g(\u03c3, x\u03c3(1)) = x\u03c3(1)e\u03c3(1)\np(\u03c3(1)) , where p(\u03c3(1)) =\u2211 \u03c0\u2208Sm P(\u03c0)1(\u03c0(1) = \u03c3(1)). In another example, let F : R3 7\u2192 R2 and x \u2208 R3. Let F (x) = [x1 + x2;x2 + x3]>. Then the function decomposes over k = 1 coordinate of x as F (x) = x1e1 + x2(e1 + e2) + x3e2, where ei \u2208 R2. Hence, h1(x1) = x1e1, h2(x2) = x2(e1 + e2) and h3(x3) = x3e2. An unbiased estimator based on top-1 feedback is: g(\u03c3, x\u03c3(1)) = h\u03c3(1)(x\u03c3(1))\np(\u03c3(1)) ."}, {"heading": "4 Unbiased Estimators of Gradients of Surrogates", "text": "Alg. 1 can be implemented for any ranking surrogate as long as an unbiased estimator of the gradient can be constructed from the random feedback. We will use techniques from online convex optimization to obtain formal regret guarantees. We will thus construct the unbiased estimator of four major ranking surrogates. Three of them are popular convex surrogates, one each from the three major learning to rank methods, i.e., pointwise, pairwise and listwise methods. The fourth one is a popular non-convex surrogate.\nShorthand notations: We note that by chain rule, \u2207w=wt\u03c6(Xtw,Rt) = X>t \u2207swtt \u03c6(s wt t , Rt), where s wt t = Xtwt. SinceXt is deterministic in our setting, we focus\non unbiased estimators of \u2207swtt \u03c6(s wt t , Rt) and take a matrix-vector product with Xt. To reduce notational clutter in our derivations, we drop w from sw and the subscript t throughout. Thus, in our derivations, z\u0303 = z\u0303t, X = Xt, s = s wt t (and not s\u0303t), \u03c3 = \u03c3\u0303t (and not \u03c3t), R = Rt, ei is standard basis vector in Rm along coordinate i and p(\u00b7) as in Eq. 1 with P = Pt where Pt is the distribution in round t in Alg. 1."}, {"heading": "4.1 Convex Surrogates", "text": "Pointwise Method: We will construct the unbiased estimator of the gradient of squared loss [Cossock and Zhang, 2006]: \u03c6sq(s,R) = \u2016s \u2212 R\u201622. The gradient \u2207s\u03c6sq(s,R) is 2(s \u2212 R) \u2208 Rm. As we have already demonstrated in the example following Lemma 1, we can construct unbiased estimator of R from top1 feedback ({\u03c3,R\u03c3(1)}). Concretely, the unbiased estimator is:\nz\u0303 = X> ( 2 ( s\u2212 R\u03c3(1)e\u03c3(1)\np(\u03c3(1))\n)) .\nPairwise Method: We will construct the unbiased estimator of the gradient of hinge-like surrogate in RankSVM [Joachims, 2002]: \u03c6svm(s,R) =\u2211 i 6=j=1 1(Ri > Rj) max(0, 1 + sj \u2212 si). The gra-\ndient is given by \u2207s\u03c6svm(s,R) = \u2211m i 6=j=1 1(Ri > Rj)1(1 + sj > si)(ej \u2212 ei) \u2208 Rm. Since s is a known quantity, from Lemma 1, we can construct F (R) as follows: F (R) = Fs(R) = \u2211m i 6=j=1 hs,i,j(Ri, Rj), where hs,i,j(Ri, Rj) = 1(Ri > Rj)1(1 + sj > si)(ej \u2212 ei). Since Fs(R) is decomposable over 2 coordinates of R at a time, we can construct an unbiased estimator from top-2 feedback ({\u03c3,R\u03c3(1), R\u03c3(2)}). The unbiased estimator is:\nz\u0303 = X> ( hs,\u03c3(1),\u03c3(2)(R\u03c3(1), R\u03c3(2)) + hs,\u03c3(2),\u03c3(1)(R\u03c3(2), R\u03c3(1))\np(\u03c3(1), \u03c3(2)) + p(\u03c3(2), \u03c3(1))\n) .\nWe note that the unbiased estimator was constructed from top-2 feedback. The following lemma, in conjunction with the necessary condition of Lemma 1 shows that it is the minimum information required to construct the unbiased estimator.\nLemma 2. The gradient of RankSVM surrogate, i.e., \u03c6svm(s,R) cannot be decomposed over 1 coordinate of R at a time.\nListwise Method: Convex surrogates developed for listwise methods of learning to rank are defined over the entire score vector and relevance vector. Gradient of a surrogate cannot usually be decomposed over coordinates of the relevance vector. We will focus on the cross-entropy surrogate used in the highly cited ListNet [Cao et al., 2007] ranking algorithm and show how a very natural modification to the surrogate makes its gradient estimable in our partial feedback setting.\nThe authors of the ListNet method use a crossentropy surrogate on two probability distributions on permutations, induced by score and relevance vector respectively. More formally, the surrogate is defined as follows1. Define m maps from Rm to R as: Pj(v) = exp(vj)/ \u2211m j=1 exp(vj) for j \u2208 [m]. Then, for\nscore vector s and relevance vector R, \u03c6LN(s,R) = \u2212 \u2211m i=1 Pi(R) logPi(s) and \u2207s\u03c6LN(s,R) =\u2211m\ni=1\n( \u2212 exp(Ri)\u2211m\nj=1 exp(Rj) + exp(si)\u2211m j=1 exp(sj)\n) ei. We have\nthe following lemma about the gradient of \u03c6LN .\nLemma 3. The gradient of ListNet surrogate \u03c6LN (s,R) cannot be decomposed over k, for k = 1, 2, coordinates of R at a time.\nIn fact, an examination of the proof of the above lemma reveals that decomposability at any k < m does not hold for the gradient of LisNet surrogate, though we only prove it for k = 1, 2 (since feedback for top k items with k > 2 does not seem practical). Due to Lemma 1, this means that if we want to run Alg. 1 under top-k feedback, a modification of ListNet is needed. We now make such a modification.\nWe first note that the cross-entropy surrogate of ListNet can be easily obtained from a standard divergence, viz. Kullback-Liebler divergence. Let p, q \u2208 Rm be 2 probability distributions ( \u2211m i=1 pi = \u2211m i=1 qi = 1).\nThen KL(p, q) = \u2211m i=1 pi log(pi) \u2212 \u2211m i=1 pi log(qi) \u2212\u2211m\ni=1 pi + \u2211m i=1 qi. Taking pi = Pi(R) and qi = Pi(s), \u2200 i \u2208 [m] (where Pi(v) is as defined in \u03c6LN) and noting that \u03c6LN(s,R) needs to be minimized w.r.t. s (thus we can ignore the \u2211m i=1 pi log(pi) term in KL(p, q)), we get the cross entropy surrogate from KL.\n1The ListNet paper actually defines a family of losses based on probability models for top r documents, with r \u2264 m. We use r = 1 in our definition since that is the version implemented in their experimental results.\nOur natural modification now easily follows by considering KL divergence for un-normalized vectors (it should be noted that KL divergence is an instance of a Bregman divergence). Define m maps from Rm to R as: P \u2032j(v) = exp(vj) for j \u2208 [m]. Now define pi = P \u2032 i (R) and qi = P \u2032 i (s). Then, the modified surrogate is \u03c6KL(s,R) is:\nm\u2211 i=1 eRi log(eRi)\u2212 m\u2211 i=1 eRi log(esi)\u2212 m\u2211 i=1 eRi + m\u2211 i=1 esi ,\nand m\u2211 i=1 (exp(si)\u2212 exp(Ri)) ei is its gradient w.r.t. s. Note that \u03c6KL(s,R) is non-negative and convex in s. Equating gradient to 0 \u2208 Rm, at the minimum point, si = Ri, \u2200 i \u2208 [m]. Thus, the sorted order of optimal score vector agrees with sorted order of relevance vector and it is a valid ranking surrogate.\nNow, from Lemma 1, we can construct F (R) as follows: F (R) = Fs(R) = \u2211m i=1 hs,i(Ri), where hs,i(Ri) = (exp(si)\u2212 exp(Ri)) ei. Since Fs(R) is decomposable over 1 coordinate of R at a time, we can construct an unbiased estimator from top-1 feedback ({\u03c3,R\u03c3(1)}). The unbiased estimator is:\nz\u0303 = X> (\n(exp(s\u03c3(1))\u2212 exp(R\u03c3(1)))e\u03c3(1) p(\u03c3(1))\n)\nOther Listwise Methods: As we mentioned before, most listwise convex surrogates will not be suitable for Alg. 1 with top-k feedback. For example, the class of popular listwise surrogates that are developed from structured prediction perspective [Chapelle et al., 2007, Yue et al., 2007] cannot have unbiased estimator of gradients from top-k feedback since they are based on maps from full relevance vectors to full rankings and thus cannot be decomposed over k = 1 or 2 coordinates of R. It does not appear they have any natural modification to make them amenable to our approach."}, {"heading": "4.1.1 Non-convex Surrogate", "text": "We provide an example of a non-convex surrogate for which Alg. 1 is applicable (however it will not have any regret guarantees due to non-convexity). We choose the SmoothDCG surrogate given in [Chapelle and Wu, 2010], which has been shown to have very competitive empirical performance. SmoothDCG, like ListNet, defines a family of surrogates, based on the cut-off point of DCG (see original paper [Chapelle and Wu, 2010] for details). We consider SmoothDCG@1, which is the smooth version of DCG@1 (i.e., DCG which focuses just on the top-ranked document). The surrogate is defined as: \u03c6SD(s,R) =\n1\u2211m j=1 exp(sj/ ) \u2211m i=1G(Ri) exp(si/ ),\nwhere is a (known) smoothing parameter and G(a) = 2a \u2212 1. The gradient of the surrogate is: [\u2207s\u03c6SD(s,R)] = m\u2211 i=1 hs,i(Ri), hs,i(Ri) =\nG(Ri) ( m\u2211 j=1 [ 1 exp(si/ )\u2211 j exp(sj\u2032/ ) 1(i=j) \u2212 1 exp((si + sj)/ ) ( \u2211 j\u2032 exp(sj\u2032/ )) 2 ]ej ) Using Lemma 1, we can write F (R) = Fs(R) =\u2211m i=1 hs,i(Ri) where hs,i(Ri) is defined above. Since Fs(R) is decomposable over 1 coordinate of R at a time, we can construct an unbiased estimator from top-1 feedback ({\u03c3,R\u03c3(1)}), with unbiased estimator being: z\u0303 = X> ( G(R\u03c3(1)) p(\u03c3(1)) (\u2217) )\n(\u2217) = m\u2211 j=1 [ 1 exp(s\u03c3(1)/ )\u2211 j\u2032 exp(sj\u2032/ ) 1(\u03c3(1)=j) \u2212 1 exp((s\u03c3(1) + sj)/ ) ( \u2211 j\u2032 exp(sj\u2032/ )) 2 ]ej"}, {"heading": "4.2 Computational Complexity of", "text": "Algorithm 1\nThree of the four key steps governing the complexity of Alg. 1, i.e., construction of s\u0303t, \u03c3\u0303t and sorting can all be done in O(m log(m)) time. Construction of estimator is even simpler. The only bottleneck could have been calculations of p(\u03c3(1)) in squared loss, (modified) ListNet loss and SmoothDCG loss, and p(\u03c3(1), \u03c3(2)) in RankSVM loss, since they involve sum over permutations. However, they have a compact representation, i.e., p(\u03c3(1)) = 1 \u2212 \u03b3 + \u03b3m and p(\u03c3(1), \u03c3(2)) = 1 \u2212 \u03b3 + 2\u03b3m(m\u22121) . The calculations follow easily due to the nature of Pt (step-6 in algorithm) which put equal weights on all permutations other than \u03c3t."}, {"heading": "4.3 Regret Bounds", "text": "The underlying deterministic part of our algorithm is online gradient descent (OGD) [Zinkevich, 2003]. The regret of OGD, run with unbiased estimator of gradient of a convex function, as given in Theorem 3.1 of [Flaxman et al., 2005], in our problem setting is:\nE [ T\u2211 t=1 \u03c6(Xtwt, Rt) ] \u2264 min w:\u2016w\u20162\u2264U T\u2211 t=1 \u03c6(Xtw,Rt)+\nU2\n2\u03b7 + \u03b7 2 E [ T\u2211 t=1 \u2016z\u0303t\u201622 ] (4)\nwhere z\u0303t is unbiased estimator of \u2207w=wt\u03c6(Xtw,Rt), conditioned on past events, \u03b7 is the learning rate and the expectation is taken over all randomness in the algorithm.\nHowever, from the perspective of the loss \u03c6(s\u0303t, Rt) incurred by Alg. 1, at each round t, the RHS above is not a valid upper bound. The algorithms plays the score vector suggested by OGD (s\u0303t = Xtwt) with probability 1\u2212 \u03b3 (exploitation) and plays a randomly selected score vector (i.e., a draw from the uniform distribution on [0, 1]m), with probability \u03b3 (exploration). Thus, the expected number of rounds in which the algorithm does not follow the score suggested by OGD is \u03b3T , leading to an extra regret2 of order \u03b3T . Thus, we have\nE [ T\u2211 t=1 \u03c6(s\u0303t, Rt) ] \u2264 E [ T\u2211 t=1 \u03c6(Xtwt, Rt) ] +O (\u03b3T )\n(5)\nWe first control Et\u2016z\u0303t\u201622, for all convex surrogates considered in our problem (we remind that z\u0303t is the estimator of a gradient of a surrogate, calculated at time t. In Sec 4.1 , we omitted showing w in sw and index t). To get bound on Et\u2016z\u0303t\u201622, we used the following norm relation that holds for any matrix X [Bhaskara and Vijayaraghavan, 2011]: \u2016X\u2016p\u2192q = sup v 6=0 \u2016Xv\u2016q \u2016v\u2016p , where q is the dual exponent of p (i.e., 1q + 1 p = 1), and the following lemma derived from it: Lemma 4. For any 1 \u2264 p \u2264 \u221e, \u2016X>\u20161\u2192p = \u2016X\u2016q\u2192\u221e = maxmj=1 \u2016Xj:\u2016p, where Xj: denotes jth row of X and m is the number of rows of matrix.\nLemma 5. For parameter \u03b3 in Alg. 1 , RD being the bound on `2 norm of the feature vectors (rows of document matrix X), m being the upper bound on number of documents per query, U being the radius of the Euclidean ball denoting the space of ranking parameters and Rmax being the maximum possible relevance value (in practice always \u2264 5), let C\u03c6 \u2208 {Csq, Csvm, CKL} be polynomial functions of RD,m,U,Rmax, where the degrees of the polynomials depend on the surrogate (\u03c6sq, \u03c6svm, \u03c6KL), with no degree ever greater than four. Then we have,\nEt [ \u2016z\u0303t\u201622 ] \u2264 C \u03c6\n\u03b3 (6)\nPlugging Eq. 6 and Eq. 5 in Eq. 4, and optimizing over \u03b7 and \u03b3, (which gives \u03b7 = O(T\u22122/3) and \u03b3 = O(T\u22121/3)), we get the final regret bound.\nTheorem 4.1. For any sequence of instances and labels (Xt, Rt){t\u2208[T ]}, applying Alg. 1 with top-1 feedback for \u03c6sq and \u03c6KL and top-2 feedback for \u03c6svm, will produce the following bound on the regret for any of the\n2The instantaneous loss suffered at each of the exploration round can be maximum of O(1), as long as \u03c6(s,R) is bounded, \u2200 s and \u2200 R. This is true because the score space is `2 norm bounded, maximum relevance grade is finite in practice and we consider Lipschitz, convex surrogates.\nthree surrogates:\nE [ T\u2211 t=1 \u03c6(s\u0303t, Rt) ] \u2212 min w:\u2016w\u20162\u2264U T\u2211 t=1 \u03c6(Xtw,Rt) \u2264 C\u03c6O ( T 2/3 ) (7)\nwhere C\u03c6 is a surrogate dependent function, as described in Lemma 5 , and expectation is taken over underlying randomness of the algorithm, over T rounds.\nDiscussion: It is known that online bandit games are special instances of partial monitoring games. For bandit online convex optimization problems with Lipschitz, convex surrogates, the best regret rate known so far, that can be achieved by an efficient algorithm, is O(T 3/4) (however, see the work of Bubeck and Eldan [2015] for a non-constructive O(log4(T ) \u221a T ) bound). Surprisingly, Alg. 1, when applied in a partial monitoring setting to the Lipschitz, convex surrogates that we have listed, achieves a better regret rate than what is known in the bandit setting. Moreover, as we show subsequently, for an entire class of Lipschitz convex surrogates (subclass of NDCG calibrated surrogates), sub-linear (in T ) regret is not even achievable. Thus, our work indicates that even within the class of Lipschitz, convex surrogates, regret rate achievable is dependent on the structure of surrogates; something that does not arise in bandit convex optimization."}, {"heading": "5 Impossibility of Sublinear Regret for NDCG Calibrated Surrogates", "text": "Learning to rank methods optimize surrogates to learn a ranking function, even though performance is measured by target measures like NDCG. This is done because direct optimization of the measures lead to NPhard optimization problems. One of the most desirable properties of any surrogate is calibration, i.e., the surrogate should be calibrated w.r.t the target [Bartlett et al., 2006]. Intuitively, it means that a function with small expected surrogate loss on unseen data should have small expect target loss on unseen data. We focus on NDCG calibrated surrogates (both convex and non-convex) that have been characterized by Ravikumar et al. [2011]. We first state the necessary and sufficient condition for a surrogate to be calibrated w.r.t NDCG. For any score vector s and distribution \u03b7 on relevance space Y, let \u03c6\u0304(s, \u03b7) = ER\u223c\u03b7\u03c6(s,R). Moreover, we define G(R) = (G(R1), . . . , G(Rm)) >.\nTheorem 5.1. [Ravikumar et al., 2011, Thm. 6] A surrogate \u03c6 is NDCG calibrated iff for any distribution \u03b7 on relevance space Y, there exists an invertible, order preserving map g : Rm 7\u2192 Rm s.t. the unique minimizer s\u2217\u03c6(\u03b7) can be written as\ns\u2217\u03c6(\u03b7) = g ( ER\u223c\u03b7 [ G(R)\nZm(R)\n]) . (8)\nInformally, Eq. 8 states that argsort(s\u2217\u03c6(\u03b7)) \u2286 argsort(ER\u223c\u03b7 [ G(R) Zm(R) ] ) Ravikumar et al. [2011] give concrete examples of NDCG calibrated surrogates, including how some of the popular surrogates can be converted into NDCG calibrated ones: e.g., the NDCG calibrated version of squared loss is \u2016s\u2212 G(R)Zm(R)\u2016 2 2.\nWe now state the impossibility result for the class of NDCG calibrated surrogates with top-1 feedback.\nTheorem 5.2. Fix the online learning to rank game with top-1 feedback and any NDCG calibrated surrogate. Then, for every learner\u2019s algorithm, there exists an adversary strategy s.t. the learner\u2019s expected regret is \u2126(T ).\nNote that our result is for top-1 feedback. Minimax regret for the problem setting with top-k feedback, with k \u2265 2 remains an open question.\nProof. (Sketch) The proof builds on the proof of hopeless finite action partial monitoring games given by Piccolboni and Schindelhauer [2001]. An examination of their proof of Thm. 3 indicates that for hopeless games, there have to exist two probability distributions (over adversary\u2019s actions), which are indistinguishable in terms of feedback but the optimal learner\u2019s actions for the distributions are different. We first provide a mathematical explanation as to why such existence lead to hopeless games. Then, we provide a characterization of indistinguishable probability distributions in our problem setting, and then exploit the characterization of optimal actions for NDCG calibrated surrogates (Thm. 5.1) to explicitly construct two such probability distributions. This proves the result.\nWe note that the proof of Thm. 3 of Piccolboni and Schindelhauer [2001] cannot be directly extended to prove the impossibility result because it relies on constructing a connected graph on vertices defined by neighboring actions of learner. In our case, due to the continuous nature of learner\u2019s actions, the graph will be an empty graph and proof will break down."}, {"heading": "6 Empirical Results", "text": "Objective: We conducted experiments on benchmark datasets to demonstrate the performance of ranking functions that are learnt from partial feedback. As stated before, though Alg. 1 is designed to minimize surrogate based regret, the users only care about the ranking presented to them, and indeed the algorithm interacts with users only through ranked lists. We tested the quality of the ranked lists, and hence the performance of the evolving ranking functions, against\nthe full relevance vectors via NDCG10. Ranking functions compared: We applied Alg. 1, with top-1 feedback, on Squared, KL and SmoothDCG surrogates, and with top-2 feedback, on the RankSVM surrogate. Since our work is based on a novel feedback model, the performance of Alg. 1 could not be directly compared with any published baseline. So, based on the objective of our work, we selected two different ranking methods for comparison. The first one is the online version ListNet ranking algorithm, with full relevance vector revealed at end of every round. ListNet is not only one of the most cited ranking algorithms (over 700 citations according to Google Scholar), but also one of the most validated algorithms [Tax et al., 2015]. We emphasize that some of the ranking algorithms, which have shown better empirical performance than ListNet, are usually based on non-convex surrogates with complex ranking functions. These algorithms cannot usually be converted into online algorithms which learn from streaming data. The second one is a fully random algorithm which outputs a uniformly at random ranking of documents at each round. Effectively, we are comparing Alg 1, which learns from highly restricted feedback, with an algorithm which learns from full feedback and another algorithm which receives no\nfeedback. Datasets: We compared the various ranking functions on two large scale commercial datasets. They were Yahoo\u2019s Learning to Rank Challenge dataset [Chapelle and Chang, 2011] and a dataset published by Russian search engine Yandex [IM-2009]. The Yahoo dataset has 19944 unique queries with 5 distinct relevance levels, while Yandex has 9126 unique queries with 5 distinct relevance levels. Setting of experiments: We selected time horizon T = 250,000 iterations for our experiments (thus, each algorithm went over each dataset multiple times). All the online algorithms, other than the fully random one, involve learning rate \u03b7 and exploration parameter \u03b3 (Full information ListNet does not involve \u03b3 and SmoothDCG has an additional smoothing parameter ). While obtaining our regret guarantees, we had established that \u03b7 = O(T\u22122/3) and \u03b3 = O(T\u22121/3) and thus, in our experiments, for each instance of Alg. 1, we selected \u03b7 = 1\nT 2/3 and \u03b3 = 1 T 1/3 . We fixed = 0.01.\nFor ListNet, we selected \u03b7 = 1 T 1/2 , since regret guaratnee in OGD is established with \u03b7 = O(T\u22121/2). We plotted average NDCG10 against time, where average NDCG10 at time t is the cumulative NDCG10 up to time t, divided by t. Observations: In both the datasets, ListNet, with full information, has highest average NDCG value throughout. However, Alg. 1, with the convex surrogates, produce competitive performance. In fact, in the Yahoo dataset, our algorithms, with RankSVM and KL, are very close to the performance of ListNet. RanSVM does better than the other surrogates, since the estimator of RankSVM gradient is constructed from top-2 feedback, leading to lower variance. KL, being listwise in nature, does better than squared loss. Crucially, our algorithms, based on all three convex surrogates, perform significantly better than the purely random algorithm, and are much closer to full feedback ListNet in performance, despite being much closer to the purely random algorithm in terms of feedback. Our algorithm, with SmoothDCG, on the other hand, produce poor performance. We believe the reason is the non-convexity of the surrogate, which leads to the optimization procedure possibly getting stuck at a local minima. In batch setting, such problem is avoided by an annealing technique that successively reduces . We are not aware of an analogue in an online setting. Possible algorithms optimizing non-convex surrogates in an online manner, which require gradient of the surrogate, may be adapted to this partial feedback setting."}, {"heading": "7 Supplementary", "text": "Proof of Lemma 1: We restate the lemma before giving the proof, for ease of reading:\nLemma 1: Let F : Rm 7\u2192 Ra be a vector valued function, where m \u2265 1, a \u2265 1. For a fixed x \u2208 Rm, let k entries of x be observed at random. That is, for a fixed probability distribution P and some random \u03c3 \u223c P(Sm), observed tuple is {\u03c3, x\u03c3(1), . . . , x\u03c3(k)}. The necessary condition for existence of an unbiased estimator of F (x), that can be constructed from {\u03c3, x\u03c3(1), . . . , x\u03c3(k)}, is that it should be possible to decompose F (x) over k (or less) coordinates of x at a time. That is, F (x) should have the following structure:\nF (x) = \u2211\n(i1,i2,...,i`)\u2208 mP`\nhi1,i2,...,i`(xi1 , xi2 , . . . , xi`)\nwhere ` \u2264 k, mP` is ` permutations of m and h : R` 7\u2192 Ra. Moreover, when F (x) can be written in form of Eq 2 , with ` = k, an unbiased estimator of F (x), based on {\u03c3, x\u03c3(1), . . . , x\u03c3(k)}, is,\ng(\u03c3, x\u03c3(1), . . . , x\u03c3(k)) =\n\u2211 (j1,j2,...,jk)\u2208Sk\nh\u03c3(j1),...,\u03c3(jk)(x\u03c3(j1), . . . , x\u03c3(jk))\u2211 (j1,...,jk)\u2208Sk p(\u03c3(j1), . . . , \u03c3(jk))\nwhere Sk is the set of k! permutations of [k] and p(\u03c3(1), . . . , \u03c3(k)) is as in Eq 1 .\nProof. For a fixed x \u2208 Rm and probability distribution P, let the random permutation be \u03c3 \u223c P(Sm) and the observed tuple be {\u03c3, x\u03c3(1), . . . , x\u03c3(k)}. Let G\u0302 = G(\u03c3, x\u03c3(1), . . . , x\u03c3(k)) be an unbiased estimator of F (x) based on the random observed tuple. Taking expectation, we get:\nF (x) = E\u03c3\u223cP [ G\u0302 ] = \u2211 \u03c0\u2208Sm P(\u03c0)G(\u03c0, x\u03c0(1), . . . , x\u03c0(k))\n= \u2211\n(i1,i2,...,ik)\u2208 mPk \u2211 \u03c0\u2208Sm P(\u03c0)1(\u03c0(1) = i1, \u03c0(2) = i2, . . . , \u03c0(k) = ik)G(\u03c0, xi1 , xi2 , . . . , xik)\nWe note that P(\u03c0) \u2208 [0, 1] is independent of x for all \u03c0 \u2208 Sm. Then we can use the following construction of function h(\u00b7):\nhi1,i2,...,ik(xi1 , . . . , xik) = \u2211 \u03c0\u2208Sm P(\u03c0)1(\u03c0(1) = i1, \u03c0(2) = i2, . . . , \u03c0(k) = ik)G(\u03c0, xi1 , xi2 , . . . , xik)\nand thus,\nF (x) = \u2211\n(i1,i2,...,ik)\u2208 mPk\nhi1,i2,...,ik(xi1 , xi2 , . . . , xi)\nHence, we conclude that for existence of an unbiased estimator based on the random observed tuple, it should be possible to decompose F (x) over k (or less) coordinates of x at a time. The \u201cless than k\u201d coordinates arguement follows simply by noting that if F (x) can be decomposed over ` coordinates at a time (` < k) and observation tuple is {\u03c3, x\u03c3(1), . . . , x\u03c3(k))}, then any k\u2212 ` observations can be thrown away and the rest used for construction of the unbiased estimator.\nThe construction of the unbiased estimator proceeds as follows: Let F (x) = \u2211m i=1 hi(xi) and feedback is for top-1 item (k = 1). The unbiased estimator according to Lemma. 1 is:\ng(\u03c3, x\u03c3(1)) = h\u03c3(1)(x\u03c3(1))\np(\u03c3(1)) = h\u03c3(1)(x\u03c3(1))\u2211 \u03c0 P(\u03c0)1(\u03c0(1) = \u03c3(1))\nTaking expectation w.r.t. \u03c3, we get:\nE\u03c3[g(\u03c3, x\u03c3(1))] = m\u2211 i=1 hi(xi)( \u2211 \u03c0 P(\u03c0)1(\u03c0(1) = i))\u2211 \u03c0 P(\u03c0)1(\u03c0(1) = i) = m\u2211 i=1 hi(xi) = F (x)\nNow, let F (x) = m\u2211\ni 6=j=1 hi,j(xi, xj) and the feedback is for top-2 item (k = 2). The unbiased estimator according\nto Lemma. 1 is:\ng(\u03c3, x\u03c3(1), x\u03c3(2)) = h\u03c3(1),\u03c3(2)(x\u03c3(1), x\u03c3(2)) + h\u03c3(2),\u03c3(1)(x\u03c3(2), x\u03c3(1))\np(\u03c3(1), \u03c3(2)) + p(\u03c3(2), \u03c3(1))\nWe will use the fact that for any 2 permutations \u03c31, \u03c32, which places the same 2 objects in top-2 positions but in opposite order, estimators based on \u03c31 (i.e, g(\u03c31, x\u03c31(1), x\u03c31(2))) and \u03c32 (i.e, g(\u03c32, x\u03c32(1), x\u03c32(2))) have same numerator and denominator. For eg., let \u03c31(1) = i, \u03c31(2) = j. Numerator and denominator for g(\u03c31, x\u03c31(1), x\u03c31(2)) are hi,j(xi, xj) + hj,i(xj , xi) and p(i, j) + p(j, i) respectively. Now let \u03c32(1) = j, \u03c32(2) = i. Then numerator and denominator for g(\u03c32, x\u03c32(1), x\u03c32(2)) are hj,i(xj , xi) + hi,j(xi, xj) and p(j, i) + p(i, j) respectively.\nThen, taking expectation w.r.t. \u03c3, we get:\nE\u03c3g(\u03c3, x\u03c3(1), x\u03c3(2)) = m\u2211\ni 6=j=1\n(hi,j(xi, xj) + hj,i(xj , xi))p(i, j)\np(i, j) + p(j, i)\n= m\u2211 i>j=1 (hi,j(xi, xj) + hj,i(xj , xi))(p(i, j) + p(j, i)) p(i, j) + p(j, i)\n= m\u2211 i>j=1 (hi,j(xi, xj) + hj,i(xj , xi)) = m\u2211 i 6=j=1 hi,j(xi, xj) = F (x)\nThis chain of logic can be extended for any k \u2265 3. Explicitly, for general k \u2264 m, let S(i1, i2, . . . , ik) denote all permutations of the set {i1, . . . , ik}. Then, taking expectation of the unbiased estimator will give:\nE\u03c3g(\u03c3, x\u03c3(1), . . . , x\u03c3(k))\n= \u2211\n(i1,i2,...,ik)\u2208 mPk\n( \u2211 (j1,...,jk)\u2208S(i1,...,ik) hj1,...,jk(xj1 , . . . , xjk) ) p(i1, . . . , ik)\u2211\n(j1,...,jk)\u2208S(i1,...,ik) p(j1, . . . , jk)\n= m\u2211 i1>i2>...>ik=1\n( \u2211 (j1,...,jk)\u2208S(i1,...,ik) hj1,...,jk(xj1 , . . . , xjk) )( \u2211 (j1,...,jk)\u2208S(i1,...,ik) p(j1, . . . , jk) ) \u2211\n(j1,...,jk)\u2208S(i1,...,ik) p(j1, . . . , jk)\n= m\u2211 i1>i2>...>ik=1  \u2211 (j1,...,jk)\u2208S(i1,...,ik) hj1,...,jk(xj1 , . . . , xjk)  = \u2211 (i1,i2,...,ik)\u2208 mPk hi1,i2,...,ik(xi1 , xi2 , . . . , xik) = F (x)\nNote: For k = m, i.e., when the full feedback is received, the unbiased estimator is:\ng(\u03c3, x\u03c3(1), . . . , x\u03c3(m)) =\n\u2211 (j1,j2,...,jm)\u2208Sm\nh\u03c3(j1),...,\u03c3(jm)(x\u03c3(j1), . . . , x\u03c3(jm))\u2211 (j1,...,jm)\u2208Sm p(\u03c3(j1), . . . , \u03c3(jm))\n=\n\u2211 (i1,i2,...,im)\u2208 mPm hi1,...,im(xi1 , . . . , xim)\n1 = F (x)\nHence, with full information, the unbiased estimator of F (x) is actually F (x) itself, which is consistent with the theory of unbiased estimator.\nProof of Lemma 4:\nProof. The first equality is true because\n\u2016X>\u20161\u2192p = sup v 6=0 \u2016X>v\u2016p \u2016v\u20161 = sup v 6=0 sup u6=0\n\u2329 X>v, u \u232a \u2016v\u20161\u2016u\u2016q\n= sup u 6=0 sup v 6=0 \u3008v,Xu\u3009 \u2016v\u20161\u2016u\u2016q = sup u6=0 \u2016Xu\u2016\u221e \u2016u\u2016q = \u2016X\u2016q\u2192\u221e.\nThe second is true because\n\u2016X\u2016q\u2192\u221e = sup u6=0 \u2016Xu\u2016\u221e \u2016u\u2016q = sup u6=0 m max j=1 | \u3008Xj:, u\u3009 | \u2016u\u2016q\n= m\nmax j=1 sup u6=0 | \u3008Xj:, u\u3009 | \u2016u\u2016q = m max j=1 \u2016Xj:\u2016p.\nProof of Lemma 5 : We restate the lemma before giving the proof:\nLemma 5: For parameter \u03b3 in Algorithm 1 , RD being the bound on `2 norm of the feature vectors (rows of document matrix X), m being the upper bound on number of documents per query, U being the radius of the Euclidean ball denoting the space of ranking parameters and Rmax being the maximum possible relevance value (in practice always \u2264 5), let C\u03c6 \u2208 {Csq, Csvm, CKL} be polynomial functions of RD,m,U,Rmax, where the degrees of the polynomials depend on the surrogate (\u03c6sq, \u03c6svm, \u03c6KL). Then we have,\nEt [ \u2016z\u0303t\u20162 ] \u2264 C \u03c6\n\u03b3 .\nProof. All our unbiased estimators are of the form X>f(s,R, \u03c3). We will actually get a bound on f(s,R, \u03c3) by using Lemma 4 and p\u2192 q norm relation, to equate out X:\n\u2016z\u0303\u20162 = \u2016X>f(s,R, \u03c3)\u20162 \u2264 \u2016X>\u20161\u21922\u2016f(s,R, \u03c3)\u20161 \u2264 RD\u2016f(s,R, \u03c3)\u20161\nsince RD \u2265 maxmj=1 \u2016Xj:\u20162.\nSquared Loss: The unbiased estimator of gradient of squared loss, as given in the main text, is:\nz\u0303 = X>(2(s\u2212 R\u03c3(1)e\u03c3(1)\np(\u03c3(1)) ))\nwhere p(\u03c3(1)) = \u2211 \u03c0\u2208Sm P(\u03c0)1(\u03c0(1) = \u03c3(1)) (P = Pt is the distribution at round t as in Alg. 1 )\nNow we have:\n\u2016s\u2212 R\u03c3(1)e\u03c3(1)\np(\u03c3(1)) \u20161 \u2264 mRDU + Rmax p(\u03c3(1)) \u2264 mRDURmax p(\u03c3(1)\nThus, taking expectation w.r.t \u03c3, we get:\nE\u03c3\u2016z\u0303\u201622 \u2264 m2R4DU2R2maxE\u03c3 1\np(\u03c3(1))2 = m2R4DU 2R2max m\u2211 i=1 p(i) p2(i)\nNow, since p(i) \u2265 \u03b3 m , \u2200 i, we get: E\u03c3\u2016z\u0303\u201622 \u2264 Csq \u03b3 , where Csq = m4R4DU 2R2max.\nRankSVM Surrogate: The unbiased estimator of gradient of the RankSVM surrogate, as given in the main text, is: z\u0303 = X> ( hs,\u03c3(1),\u03c3(2)(R\u03c3(1), R\u03c3(2)) + hs,\u03c3(2),\u03c3(1)(R\u03c3(2), R\u03c3(1))\np(\u03c3(1), \u03c3(2)) + p(\u03c3(2), \u03c3(1))\n)\nwhere hs,i,j(Ri, Rj) = 1(Ri > Rj)1(1 + sj > si)(ej \u2212 ei) and p(\u03c3(1), \u03c3(2)) = \u2211\n\u03c0\u2208Sm P(\u03c0)1(\u03c0(1) = \u03c3(1), \u03c0(2) =\n\u03c3(2)) (P = Pt as in Alg. 1)).\nNow we have:\n\u2016 hs,\u03c3(1),\u03c3(2)(R\u03c3(1), R\u03c3(2)) + hs,\u03c3(2),\u03c3(1)(R\u03c3(2), R\u03c3(1))\np(\u03c3(1), \u03c3(2)) + p(\u03c3(2), \u03c3(1)) \u20161 \u2264\n2\np(\u03c3(1), \u03c3(2)) + p(\u03c3(2), \u03c3(1))\nThus, taking expectation w.r.t \u03c3, we get:\nE\u03c3\u2016z\u0303\u201622 \u2264 4R2DE\u03c3 1\n(p(\u03c3(1), \u03c3(2)) + p(\u03c3(2), \u03c3(1)))2 \u2264 4R2D m\u2211 i>j p(i, j) + p(j, i) (p(i, j) + p(j, i))2\nNow, since p(i, j) \u2265 \u03b3 m2 , \u2200 i, j, we get: E\u03c3\u2016z\u0303\u201622 \u2264 Csvm \u03b3 , where Csvm = O(m4R2D).\nKL based Surrogate: The unbiased estimator of gradient of the KL based surrogate, as given in the main text, is: z\u0303 = X> (\n(exp(s\u03c3(1))\u2212 exp(R\u03c3(1)))e\u03c3(1) p(\u03c3(1)) ) where p(\u03c3(1)) = \u2211 \u03c0\u2208Sm P(\u03c0)1(\u03c0(1) = \u03c3(1)) (P = Pt as in Alg. 1) ).\nNow we have:\n\u2016 (exp(s\u03c3(1))\u2212 exp(R\u03c3(1)))e\u03c3(1)\np(\u03c3(1)) \u20161 \u2264\nexp(RDU)\np(\u03c3(1))\nThus, taking expectation w.r.t \u03c3, we get:\nE\u03c3\u2016z\u0303\u201622 \u2264 R2D exp(2RDU)E\u03c3( 1\np(\u03c3(1))2\nFollowing the same arguement as in squared loss, we get: E\u03c3\u2016z\u0303\u201622 \u2264 CKL\n\u03b3 , where CKL = m2R2D exp(2RXU).\nProof of Lemma 2 :\nProof. Let m = 3. The term associated with the 1st coordinate of R, i.e, R1, in the gradient of RankSVM is: 1(R1 > R2)1(1 + s2 > s1)(e2 \u2212 e1) + 1(R2 > R1)1(1 + s1 > s2)(e1 \u2212 e2) + 1(R1 > R3)1(1 + s3 > s1)(e3 \u2212 e1) + 1(R3 > R1)1(1 + s1 > s3)(e1 \u2212 e3). Now let s1 = 1, s2 = 0, s3 = 0. Then the term associated becomes: 1(R2 > R1)(e1 \u2212 e2) + 1(R3 > R1)(e1 \u2212 e3) = (1(R2 > R1) + 1(R3 > R1))e1 \u2212 1(R2 > R1)e2 \u2212 1(R3 > R1)e3. Now, if the gradient can be decomposed over R1, then the term associated with R1 should only be a function of R1. More specifically, (1(R2 > R1) + 1(R3 > R1)) (the non-zero coefficient of e1, in the term associated with R1) should be a function of only R1. Same for the non-zero coefficients of e2 and e3.\nNow assume that the (1(R2 > R1) +1(R3 > R1)) can be expressed as a function of R1 only. Then the difference between the coefficient\u2019s values, for the following two cases: R1 = 0, R2 = 0, R3 = 0 and R1 = 1, R2 = 0, R3 = 0, would be same as the difference between the coefficient\u2019s values, for the following two cases: R1 = 0, R2 = 1, R3 = 1 and R1 = 1, R2 = 1, R3 = 1 (Since the difference would be affected only by change in R1 value). It can be clearly seen that the change in value between the first two cases is: 0\u2212 0 = 0, while the change in value bertween the second two cases is: 2\u2212 0 = 2. Thus, we reach a contradiction.\nProof of Lemma 3 :\nProof. The term associated with the 1st coordinate of R, i.e, R1, in the gradient of ListNet is =\u2211m i=1 ( \u2212 exp(Ri)\u2211m j=1 exp(Rj) + exp(si)\u2211m j=1 exp(sj) ) ei\nNow, f(R) = ( \u2212 exp(Ri)\u2211m j=1 exp(Rj) + exp(si)\u2211m j=1 exp(sj) ) is the non-zero coefficient of e1. Now, if f(R) would have only been a function of R1, then \u22022f(R)\n\u2202Ri\u2202Rj , \u2200 j 6= i would have been zero. It can be clearly seen this is not the case.\nNow, the term associated with R1 and R2, in the gradient of ListNet is same as before, i.e,\u2211m i=1 ( \u2212 exp(Ri)\u2211m j=1 exp(Rj) + exp(si)\u2211m j=1 exp(sj) ) ei for both\nNow, f(R) = ( \u2212 exp(Ri)\u2211m j=1 exp(Rj) + exp(si)\u2211m j=1 exp(sj) ) is the non-zero coefficient of e1. Now, if f(R) would have only been a function of R1 and R2, then \u22023f(R)\n\u2202Ri\u2202Rj\u2202R` , \u2200` 6= i, ` 6= j would have been zero. It can be clearly seen this\nis not the case.\nThe same arguement can be extended for any k < m.\nProof of Theorem. 5.2:\nProof. We will first fix the setting of the online game. We consider m = 3 and fixed the document matrix X \u2208 R3\u00d73 to be the identity. At each round of the game, the adversary generates the fixed X and the learner chooses a score vector s \u2208 R3. Making the matrix X identity makes the distinction between weight vectors w and scores s irrelevant since s = Xw = w. We note that allowing the adversary to vary X over the rounds only makes him more powerful, which can only increase the regret. We also restrict the adversary to choose binary relevance vectors. Once again, allowing adversary to choose multi-graded relevance vectors only makes it more powerful. Thus, in this setting, the adversary can now choose among 23 = 8 possible relevance vectors. The learner\u2019s action set is infinite, i.e., the learner can choose any score vector s = Xw = Rm. The loss function \u03c6(s,R) is any NDCG calibrated surrogate and feedback is the relevance of top-ranked item at each round, where ranking is induced by sorted order (descending) of score vector. We will use p to denote randomized adversary one-short strategies, i.e. distributions over the 8 possible relevance score vectors. Let s\u2217p = argmins ER\u223cp\u03c6(s,R). We note that in the definition of NDCG calibrated surrogates, Ravikumar et al. [2011] assume that the optimal score vector for each distribution over relevance vectors is unique and we subscribe to that assumption. The assumption was taken to avoid some boundary conditions.\nIt remains to specify the choice of U , a bound on the Euclidean norm of the weight vectors (same as score vectors for us right now) that is used to define the best loss in hindsight. It never makes sense for the learner to play anything outside the set \u222aps\u2217p so that we can set U = max{\u2016s\u20162 : s \u2208 \u222aps\u2217p}.\nThe paragraph following Lemma 6 of Thm. 3 in Piccolboni and Schindelhauer [2001] gives the main intuition behind the argument the authors developed to prove hopelessness of finite action partial monitoring games. To make our proof self contained, we will explain the intuition in a rigorous way.\nKey insight: Two adversary strategies p, p\u0303 are said to be indistinguishable from the learner\u2019s feedback perspective, if for every action of the learner, the probability distribution over the feedbacks received by learner is the same for p and p\u0303. Now assume that adversary always selects actions according to one of the two such indistinguishable strategies. Thus, the learner will always play one of s\u2217p and s \u2217 p\u0303. Now, let s \u2217 p 6= s\u2217p\u0303. Then, the learner incurs a constant (non-zero) regret on any round where adversary plays according to p and learner plays s\u2217p, or if the adversary plays according to p\u0303 and learner plays s \u2217 p\u0303. We show that in such a setting, adversary can simply play according to (p+ p\u0303)/2 and the learner suffers an expected regret of \u2126(T ).\nAssume that the adversary selects {R1, . . . , RT } from product distribution \u2297p. Let the number of times the learner plays s\u2217p and s \u2217 p\u0303 be denoted by random variables N p 1 and N p 2 respectively, where N\np shows the exclusive dependence on p. It is always true that Np1 + N p 2 = T . Moreover, let the expected per round regret be p when learner plays s\u2217p\u0303 , where the expectation is taken over the randomization of adversary. Now, assume that adversary selects {R1, . . . , RT } from product distribution \u2297p\u0303. The corresponding notations become N p\u03031 and N p\u0303 2\nand p\u0303. Then,\nand E(R1,...,RT )\u223c\u2297p\u0303E(s1,...,sT )[Regret((s1, . . . , sT ), (R1, . . . , RT ))] = p\u0303 \u00b7 E[N p\u0303 1 ] + 0 \u00b7 E[N p\u0303 2 ]\nSince p and p\u0303 are indistinguishable from perspective of learner, E[Np1 ] = E[N p\u0303 1 ] = E[N1] and E[N p 2 ] = E[N p\u0303 2 ] = E[N2]. That is, the random variable denoting number of times s\u2217p is played by learner does not depend on adversary distribution (same for s\u2217p\u0303.). Using this fact and averaging the two expectations, we get:\nE(R1,...,RT )\u223c\u2297p+\u2297p\u03032 E(s1,...,sT )[Regret((s1, . . . , sT ), (R1, . . . , RT ))] = p\u0303 2 \u00b7E[N1]+ p 2 \u00b7E[N2] \u2265 min( p 2 , p\u0303 2 )\u00b7E[N1+N2] = \u00b7T\nSince supR1,...,RT E[Regret((s1, . . . , sT ), (R1, . . . , RT ))] \u2265 E(R1,...,RT )\u223c\u2297p+\u2297p\u03032 E(s1,...,sT )[Regret((s1, . . . , sT ), (R1, . . . , RT ))], we conclude that for every learner algorithm, adversary has a strategy, s.t. learner suffers an expected regret of \u2126(T ).\nNow, the thing left to be shown is the existence of two indistinguishable distributions p and p\u0303, s.t. s\u2217p 6= s\u2217p\u0303.\nCharacterization of indistinguishable strategies in our problem setting: Two adversary\u2019s strategies p and p\u0303 will be indistinguishable, in our problem setting, if for every score vector s, the relevances of the top-ranked item, according to s, are same for relevance vector drawn from p and p\u0303. Since relevance vectors are restricted to be binary, mathematically, it means that \u2200s, PR\u223cp(R\u03c0s(1) = 1) = PR\u223cp\u0303(R\u03c0s(1) = 1) (actually, we also need \u2200s, PR\u223cp(R\u03c0s(1) = 0) = PR\u223cp\u0303(R\u03c0s(1) = 0), but due to the binary nature, PR\u223cp(R\u03c0s(1) = 1) = PR\u223cp\u0303(R\u03c0s(1) = 1) =\u21d2 PR\u223cp(R\u03c0s(1) = 0) = PR\u223cp\u0303(R\u03c0s(1) = 0)). Since the equality has to hold \u2200s, this implies \u2200j \u2208 [m], PR\u223cp(Rj = 1) = PR\u223cp\u0303(Rj = 1) (as every item will be ranked at top by some score vector). Hence, \u2200j \u2208 [m], ER\u223cp[Rj ] = ER\u223cp\u0303[Rj ] =\u21d2 ER\u223cp[R] = ER\u223cp\u0303[R]. It can be seen clearly that the chain of implications can be reversed. Hence, \u2200s, PR\u223cp(R\u03c0s(1) = 1) = PR\u223cp\u0303(R\u03c0s(1) = 1) \u21d0\u21d2 ER\u223cp[R] = ER\u223cp\u0303[R].\nExplicit adversary strategies: Following from the discussion so far and Theorem 5.1, if we can show existence of two strategies p and p\u0303 s.t. ER\u223cp[R] = ER\u223cp\u0303[R], but argsort ( ER\u223cp [ G(R) Zm(R) ]) 6= argsort ( ER\u223cp\u0303 [ G(R) Zm(R) ]) , we are done.\nThe 8 possible relevance vectors (adversary\u2019s actions) are (R1, R2, R3, R4, R5, R6, R7, R8) = (000, 110, 101, 011, 100, 010, 001, 111). Let the two probability vectors be: p = (0.0, 0.1, 0.15, 0.05, 0.2, 0.3, 0.2, 0.0) and p\u0303 = (0.0, 0.3, 0.0, 0.0, 0.15, 0.15, 0.4, 0.0). The data is provided in table format in Table. 1.\nUnder the two distributions, it can be checked that ER\u223cp[R] = ER\u223cp\u0303[R] = (0.45, 0.45, 0.4)>. However, ER\u223cp [ G(R) Zm(R) ] = (0.3533, 0.3920, 0.3226)>, but ER\u223cp\u0303 [ G(R) Zm(R) ] = (0.3339, 0.3339, 0.4000)>. Hence,\nargsort ( ER\u223cp [ G(R) Zm(R) ]) = [2, 1, 3]> but argsort ( ER\u223cp\u0303 [ G(R) Zm(R) ]) \u2208 {[3, 1, 2]>, [3, 2, 1]>}."}], "references": [{"title": "Partial monitoring with side information", "author": ["G\u00e1bor Bart\u00f3k", "Csaba Szepesv\u00e1ri"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Bart\u00f3k and Szepesv\u00e1ri.,? \\Q2012\\E", "shortCiteRegEx": "Bart\u00f3k and Szepesv\u00e1ri.", "year": 2012}, {"title": "Partial monitoring\u2013classification, regret bounds, and algorithms", "author": ["Gabor Bartok"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bartok,? \\Q2014\\E", "shortCiteRegEx": "Bartok", "year": 2014}, {"title": "Approximating matrix p-norms", "author": ["Aditya Bhaskara", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,", "citeRegEx": "Bhaskara and Vijayaraghavan.,? \\Q2011\\E", "shortCiteRegEx": "Bhaskara and Vijayaraghavan.", "year": 2011}, {"title": "Multi-scale exploration of convex functions and bandit convex optimization", "author": ["S\u00e9bastien Bubeck", "Ronen Eldan"], "venue": "arXiv preprint arXiv:1507.06580,", "citeRegEx": "Bubeck and Eldan.,? \\Q2015\\E", "shortCiteRegEx": "Bubeck and Eldan.", "year": 2015}, {"title": "On the (non-) existence of convex, calibrated surrogate losses for ranking", "author": ["Cl\u00e9ment Calauzenes", "Nicolas Usunier", "Patrick Gallinari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Calauzenes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Calauzenes et al\\.", "year": 2012}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li"], "venue": "In Proceedings of the 24th International conference on Machine learning,", "citeRegEx": "Cao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "Regret minimization under partial monitoring", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "Yahoo! learning to rank challenge overview", "author": ["Olivier Chapelle", "Yi Chang"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "Chapelle and Chang.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Chang.", "year": 2011}, {"title": "Gradient descent optimization of smoothed information retrieval metrics", "author": ["Olivier Chapelle", "Mingrui Wu"], "venue": "Information retrieval,", "citeRegEx": "Chapelle and Wu.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle and Wu.", "year": 2010}, {"title": "Large margin optimization of ranking measures", "author": ["Olivier Chapelle", "Quoc Le", "Alex Smola"], "venue": "In NIPS Workshop: Machine Learning for Web Search,", "citeRegEx": "Chapelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2007}, {"title": "Online ranking with top-1 feedback", "author": ["Sougata Chaudhuri", "Ambuj Tewari"], "venue": "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Chaudhuri and Tewari.,? \\Q2015\\E", "shortCiteRegEx": "Chaudhuri and Tewari.", "year": 2015}, {"title": "Subset ranking using regression", "author": ["David Cossock", "Tong Zhang"], "venue": "In Conference on Learning theory,", "citeRegEx": "Cossock and Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Cossock and Zhang.", "year": 2006}, {"title": "Statistical analysis of bayes optimal subset ranking", "author": ["David Cossock", "Tong Zhang"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cossock and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Cossock and Zhang.", "year": 2008}, {"title": "Online convex optimization in the bandit setting", "author": ["Abraham D Flaxman", "Adam Tauman Kalai", "H Brendan McMahan"], "venue": "In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "On multilabel classification and ranking with bandit feedback", "author": ["Claudio Gentile", "Francesco Orabona"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gentile and Orabona.,? \\Q2014\\E", "shortCiteRegEx": "Gentile and Orabona.", "year": 2014}, {"title": "Balancing exploration and exploitation in listwise and pairwise online learning to rank", "author": ["Katja Hofmann", "Shimon Whiteson", "Maarten de Rijke"], "venue": "Information Retrieval,", "citeRegEx": "Hofmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2013}, {"title": "Optimizing search engines using clickthrough data", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the 8th ACM SIGKDD,", "citeRegEx": "Joachims.,? \\Q2002\\E", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert Kleinberg", "Tom Leighton"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Kleinberg and Leighton.,? \\Q2003\\E", "shortCiteRegEx": "Kleinberg and Leighton.", "year": 2003}, {"title": "Learning to rank for information retrieval", "author": ["Tie-Yan Liu"], "venue": "Springer Science & Business Media,", "citeRegEx": "Liu.,? \\Q2011\\E", "shortCiteRegEx": "Liu.", "year": 2011}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["Antonio Piccolboni", "Christian Schindelhauer"], "venue": "In COLT,", "citeRegEx": "Piccolboni and Schindelhauer.,? \\Q2001\\E", "shortCiteRegEx": "Piccolboni and Schindelhauer.", "year": 2001}, {"title": "Learning diverse rankings with multiarmed bandits", "author": ["Filip Radlinski", "Robert Kleinberg", "Thorsten Joachims"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "On NDCG consistency of listwise ranking methods", "author": ["Pradeep Ravikumar", "Ambuj Tewari", "Eunho Yang"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ravikumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2011}, {"title": "Test collection based evaluation of information retrieval systems, volume 13", "author": ["Mark Sanderson"], "venue": "Now Publishers Inc,", "citeRegEx": "Sanderson.,? \\Q2010\\E", "shortCiteRegEx": "Sanderson.", "year": 2010}, {"title": "A cross-benchmark comparison of 87 learning to rank methods", "author": ["Niek Tax", "Sander Bockting", "Djoerd Hiemstra"], "venue": "Information Processing and Management,", "citeRegEx": "Tax et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tax et al\\.", "year": 2015}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Yisong Yue", "Thorsten Joachims"], "venue": "In Proceedings of the 26th ICML.,", "citeRegEx": "Yue and Joachims.,? \\Q2009\\E", "shortCiteRegEx": "Yue and Joachims.", "year": 2009}, {"title": "A support vector method for optimizing average precision", "author": ["Yisong Yue", "Thomas Finley", "Filip Radlinski", "Thorsten Joachims"], "venue": "In Proceedings of ACM SIGIR,", "citeRegEx": "Yue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2007}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In Proceedings of the 20th International Conference on Machine Learning", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": ", rankers are trained on batch data consisting of instances and labels [Liu, 2011].", "startOffset": 71, "endOffset": 82}, {"referenceID": 22, "context": "In certain applications, such as deploying a new web app or developing a custom search engine, collecting large amount of training data might not be possible at all [Sanderson, 2010].", "startOffset": 165, "endOffset": 182}, {"referenceID": 16, "context": "Moreover, a clicked item might not actually be relevant to the user and there is also the problem of bias towards top ranked items in inferring feedback from user clicks [Joachims, 2002].", "startOffset": 170, "endOffset": 186}, {"referenceID": 10, "context": "Our work extends the work of Chaudhuri and Tewari [2015], by combining query-level ranking, in an online manner, with explicit but restricted feedback.", "startOffset": 29, "endOffset": 57}, {"referenceID": 5, "context": ", the cross entropy surrogate in ListNet [Cao et al., 2007] and hinge surrogate in RankSVM [Joachims, 2002], instead of discontinuous ranking measures like NDCG, AP, or ERR because the latter lead to intractable optimization problems.", "startOffset": 41, "endOffset": 59}, {"referenceID": 16, "context": ", 2007] and hinge surrogate in RankSVM [Joachims, 2002], instead of discontinuous ranking measures like NDCG, AP, or ERR because the latter lead to intractable optimization problems.", "startOffset": 39, "endOffset": 55}, {"referenceID": 12, "context": "The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method [Cossock and Zhang, 2008], hinge loss used in the pairwise RankSVM [Joachims, 2002] method, and (modified) cross-entropy surrogate used in the listwise ListNet [Cao et al.", "startOffset": 120, "endOffset": 145}, {"referenceID": 16, "context": "The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method [Cossock and Zhang, 2008], hinge loss used in the pairwise RankSVM [Joachims, 2002] method, and (modified) cross-entropy surrogate used in the listwise ListNet [Cao et al.", "startOffset": 187, "endOffset": 203}, {"referenceID": 5, "context": "The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method [Cossock and Zhang, 2008], hinge loss used in the pairwise RankSVM [Joachims, 2002] method, and (modified) cross-entropy surrogate used in the listwise ListNet [Cao et al., 2007] method.", "startOffset": 280, "endOffset": 298}, {"referenceID": 8, "context": "The non-convex surrogate considered is the SmoothDCG surrogate [Chapelle and Wu, 2010].", "startOffset": 63, "endOffset": 86}, {"referenceID": 21, "context": "The convex surrogates we mentioned above are widely used but are known to fail to be calibrated with respect to NDCG [Ravikumar et al., 2011].", "startOffset": 117, "endOffset": 141}, {"referenceID": 19, "context": "The proof for this rather surprising result is non-trivial and relies on exploiting a connection between the construction of optimal adversary strategies for hopeless finite action partial monitoring games [Piccolboni and Schindelhauer, 2001] and the structure of NDCG calibrated surrogates.", "startOffset": 206, "endOffset": 242}, {"referenceID": 4, "context": "We only focus on NDCG calibrated surrogates for the impossibility results since no (convex) surrogate can be calibrated for AP and ERR [Calauzenes et al., 2012].", "startOffset": 135, "endOffset": 160}, {"referenceID": 11, "context": "Pointwise Method: We will construct the unbiased estimator of the gradient of squared loss [Cossock and Zhang, 2006]: \u03c6sq(s,R) = \u2016s \u2212 R\u20162.", "startOffset": 91, "endOffset": 116}, {"referenceID": 16, "context": "Pairwise Method: We will construct the unbiased estimator of the gradient of hinge-like surrogate in RankSVM [Joachims, 2002]: \u03c6svm(s,R) = \u2211 i 6=j=1 1(Ri > Rj) max(0, 1 + sj \u2212 si).", "startOffset": 109, "endOffset": 125}, {"referenceID": 5, "context": "We will focus on the cross-entropy surrogate used in the highly cited ListNet [Cao et al., 2007] ranking algorithm and show how a very natural modification to the surrogate makes its gradient estimable in our partial feedback setting.", "startOffset": 78, "endOffset": 96}, {"referenceID": 8, "context": "We choose the SmoothDCG surrogate given in [Chapelle and Wu, 2010], which has been shown to have very competitive empirical performance.", "startOffset": 43, "endOffset": 66}, {"referenceID": 8, "context": "SmoothDCG, like ListNet, defines a family of surrogates, based on the cut-off point of DCG (see original paper [Chapelle and Wu, 2010] for details).", "startOffset": 111, "endOffset": 134}, {"referenceID": 26, "context": "The underlying deterministic part of our algorithm is online gradient descent (OGD) [Zinkevich, 2003].", "startOffset": 84, "endOffset": 101}, {"referenceID": 13, "context": "1 of [Flaxman et al., 2005], in our problem setting is:", "startOffset": 5, "endOffset": 27}, {"referenceID": 2, "context": "To get bound on Et\u2016z\u0303t\u20162, we used the following norm relation that holds for any matrix X [Bhaskara and Vijayaraghavan, 2011]: \u2016X\u2016p\u2192q = sup v 6=0 \u2016Xv\u2016q \u2016v\u2016p , where q", "startOffset": 90, "endOffset": 125}, {"referenceID": 3, "context": "For bandit online convex optimization problems with Lipschitz, convex surrogates, the best regret rate known so far, that can be achieved by an efficient algorithm, is O(T ) (however, see the work of Bubeck and Eldan [2015] for a non-constructive O(log(T ) \u221a T ) bound).", "startOffset": 200, "endOffset": 224}, {"referenceID": 21, "context": "We focus on NDCG calibrated surrogates (both convex and non-convex) that have been characterized by Ravikumar et al. [2011]. We first state the necessary and sufficient condition for a surrogate to be calibrated w.", "startOffset": 100, "endOffset": 124}, {"referenceID": 21, "context": "We focus on NDCG calibrated surrogates (both convex and non-convex) that have been characterized by Ravikumar et al. [2011]. We first state the necessary and sufficient condition for a surrogate to be calibrated w.r.t NDCG. For any score vector s and distribution \u03b7 on relevance space Y, let \u03c6\u0304(s, \u03b7) = ER\u223c\u03b7\u03c6(s,R). Moreover, we define G(R) = (G(R1), . . . , G(Rm)) >. Theorem 5.1. [Ravikumar et al., 2011, Thm. 6] A surrogate \u03c6 is NDCG calibrated iff for any distribution \u03b7 on relevance space Y, there exists an invertible, order preserving map g : R 7\u2192 R s.t. the unique minimizer s\u03c6(\u03b7) can be written as s\u03c6(\u03b7) = g ( ER\u223c\u03b7 [ G(R) Zm(R) ]) . (8) Informally, Eq. 8 states that argsort(s\u03c6(\u03b7)) \u2286 argsort(ER\u223c\u03b7 [ G(R) Zm(R) ] ) Ravikumar et al. [2011] give concrete examples of NDCG calibrated surrogates, including how some of the popular surrogates can be converted into NDCG calibrated ones: e.", "startOffset": 100, "endOffset": 746}, {"referenceID": 19, "context": "(Sketch) The proof builds on the proof of hopeless finite action partial monitoring games given by Piccolboni and Schindelhauer [2001]. An examination of their proof of Thm.", "startOffset": 99, "endOffset": 135}, {"referenceID": 19, "context": "3 of Piccolboni and Schindelhauer [2001] cannot be directly extended to prove the impossibility result because it relies on constructing a connected graph on vertices defined by neighboring actions of learner.", "startOffset": 5, "endOffset": 41}, {"referenceID": 23, "context": "ListNet is not only one of the most cited ranking algorithms (over 700 citations according to Google Scholar), but also one of the most validated algorithms [Tax et al., 2015].", "startOffset": 157, "endOffset": 175}, {"referenceID": 7, "context": "They were Yahoo\u2019s Learning to Rank Challenge dataset [Chapelle and Chang, 2011] and a dataset published by Russian search engine Yandex [IM-2009].", "startOffset": 53, "endOffset": 79}], "year": 2016, "abstractText": "We consider an online learning to rank setting in which, at each round, an oblivious adversary generates a list of m documents, pertaining to a query, and the learner produces scores to rank the documents. The adversary then generates a relevance vector and the learner updates its ranker according to the feedback received. We consider the setting where the feedback is restricted to be the relevance levels of only the top k documents in the ranked list for k m. However, the performance of learner is judged based on the unrevealed full relevance vectors, using an appropriate learning to rank loss function. We develop efficient algorithms for well known losses in the pointwise, pairwise and listwise families. We also prove that no online algorithm can have sublinear regret, with top-1 feedback, for any loss that is calibrated with respect to NDCG. We apply our algorithms on benchmark datasets demonstrating efficient online learning of a ranking function from highly restricted feedback.", "creator": "LaTeX with hyperref package"}}}