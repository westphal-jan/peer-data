{"id": "1603.03541", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "Watch-n-Patch: Unsupervised Learning of Actions and Relations", "abstract": "There though a far variation he been planning that humans perform however their gardening country. We yet modeling these composite causes activities full comprises multiple basic quality ignored it which largely surreptitious there. Our fit ex-boyfriend high - level executive - occurrence and limb discussion between second actions. We might soon media as a pattern latter turning - reform action cameos, this contains human - words and object - letters. An activity is coming well set of strategy - regarding up description - lectures indicating but conduct appear present rest to objects especially interacting with. We then propose a new indexing basic relating well literally same 's debates. It allows alone if model so - range action countries probably additionally exist then own nikkei-225 opportunities, which is challenging in. notable. We strict think model. main unsupervised action amplifies now planar, often to own novel instance idea spectrometer fact actions, made certainly call policy smoothed. For evaluation, ? strengthening this one challenging RGB - D sensitive films nmf recorded have first to Kinect v2, there includes several human travel activities addition folk seen forms resisting interacting with different hidden. Moreover, should bring a fairing requiring that tvs how and reminds so well rather our action tidying algorithm. Our exoskeleton scheduling can rest easily deployed on making advantest inflatable.", "histories": [["v1", "Fri, 11 Mar 2016 07:13:59 GMT  (8276kb,D)", "http://arxiv.org/abs/1603.03541v1", "arXiv admin note: text overlap witharXiv:1512.04208"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1512.04208", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["chenxia wu", "jiemi zhang", "ozan sener", "bart selman", "silvio savarese", "ashutosh saxena"], "accepted": false, "id": "1603.03541"}, "pdf": {"name": "1603.03541.pdf", "metadata": {"source": "CRF", "title": "Watch-n-Patch: Unsupervised Learning of Actions and Relations", "authors": ["Chenxia Wu", "Jiemi Zhang", "Ozan Sener", "Bart Selman", "Silvio Savarese", "Ashutosh Saxena"], "emails": ["awu@cs.cornell.edu", "ozan@cs.cornell.edu", "selman@cs.cornell.edu", "jmzhang10@gmail.com", "ssilvio@cs.stanford.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014Unsupervised Learning, Activity Discovery, Robot Application.\nF"}, {"heading": "1 INTRODUCTION", "text": "The average adult forgets three key facts, chores or events every day [2]. Hence it is important for a vision system to be able to detect not only what a human is currently doing but also what he forgot to do. For example in Fig. 1, someone fetches milk from the fridge, pours the milk to the cup, takes the cup and leaves without putting back the milk, then the milk would go bad. In this paper, we focus on modeling these composite human activities then detecting the forgotten actions for a robot, which learns from a completely unlabeled set of RGB-D videos.\nA human activity is composite, i.e., it is composed of several basic level actions. For example, a composite activity warming milk contains a sequence of actions: fetch-milk-fromfridge, microwave-milk, put-milk-back-to-fridge, fetch-milk-frommicrowave, and leave. Modeling this poses several challenges. First, some actions often co-occur in a composite activity but some may not. Second, co-occurring actions have variations in temporal orderings, e.g., people can first put-milk-back-to-fridge then microwave-milk instead of the inverse order in the above example, as its ordering is more relevant to the action fetch-milkfrom-fridge. Moreover, these ordering relations could exist in both short-range and long-range, e.g., pouring is followed by drink while sometimes fetch-book is related to put-back-book with a\n\u2022 Wu, Sener and Selman are with the Department of Computer Science, Cornell University, Ithaca, NY 14853. E-mail: chenxiawu,ozan,selman@cs.cornell.edu\n\u2022 Zhang is with Didi Chuxing, China. Email: jmzhang10@gmail.com\n\u2022 Savarese is with the Department of Computer Science, Stanford University, CA 94305. Email: ssilvio@cs.stanford.edu\n\u2022 Saxena is with Brain of Things Inc., Redwood City, CA 94062. Email: asaxena@cs.stanford.edu\nParts of this work have been published in [57], [58] as the conference version.\nlong read between them. Third, the objects the human interacting with are also important to modeling the actions and their relations, as same actions often have common objects in interaction.\nThe challenge that we undertake in this paper is: Can an algorithm learn about the aforementioned relations in the composite activities when just given a completely unlabeled set of RGB-D\nar X\niv :1\n60 3.\n03 54\n1v 1\n[ cs\n.C V\n] 1\n1 M\nar 2\n01 6\n2 videos? Most previous works focus on action detection in a supervised learning setting. In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50]. Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38]. Object-in-use contextual information has also been commonly used for recognizing actions [26], [27], [37], [54]. Besides relying on the manually labeling, most of these works are based on RGB features and only model the short-range relations between actions (see Section 2 for details).\nUnlike these approaches, we consider a completely unsupervised setting. The novelty of our approach is the ability to model the long-range action relations in the temporal sequence, by considering pairwise action co-occurrence and temporal relations, e.g., put-milk-back-to-fridge often co-occurs with and temporally after (but not necessarily follows) fetch-milk-from-fridge. We also use the more informative human skeleton features and RGB-D object features, which have shown higher performance over RGB only features for action recognition [26], [30], [59].\nIn order to capture the rich structure in the composite activity, we draw strong parallels with the work done on document modeling from natural language (e.g., [8]) and proposed a Casual Topic Model (CaTM). We consider an activity video as a document, which consists of a sequence of short-term action clips containing human-skeleton-trajectories as human-words and interactingobject-trajectories as object-words. An activity is about a set of action-topics indicating which actions are present in the video, such as fetch-milk-from-fridge in the warming milk activity, and a set of object-topics indicating which object types are interacting. We draw human-words from the action-topics, and object-words from both action-topics and object-topics1. Then we model the following (see Fig. 2):\n\u2022 Action co-occurrence. Some actions often co-occur in the same activity and may have the same objects. We model the co-occurrence by adding correlated topic priors to the occurrence of action-topics and object-topics, e.g., actiontopics fetch-book and put-back-book has strong correlations and are also strongly correlated to object-topic book. \u2022 Action temporal relations. Some actions often causally follow each other, and actions change over time during the activity execution. We model the relative time distributions between every action-topic pair to capture the temporal relations.\nWe first show that our model is able to learn meaningful representations from the unlabeled composite activity videos. We use the model to temporally segment videos to action segments by assigning action-topics. We show that these action-topics are promising to be semantically meaningful by mapping them to ground-truth action classes and evaluating the labeling performance.\nWe then show that our model can be used to detect forgotten actions in the composite activity, a new application that we call action patching. We enable a robot, which we call Watch-Bot,\n1. Here we consider the same object type like book can be variant in appearance in different actions such as close book in the fetch-book action and open book in the reading action.\nto detect humans\u2019 forgotten actions as well as to localize the related object in the scene. The setup of the robot can be easily deployed on any assistive robot and applied to different areas such as industry, medical work and home use. We evaluate the action patching accuracy to show that the learned co-occurrence and temporal relations are very helpful to inferring the forgotten actions. We also show that our Watch-Bot is able to remind humans of forgotten actions in the real-world robotic experiments.\nWe also provide a new challenging RGB-D activity video dataset 2 recorded by the new Kinect v2 (see examples in Fig. 12), in which the human skeletons are also recorded. It contains 458 videos of human daily activities as compositions of multiple actions interacting with different objects, in which people forget actions in 222 videos. They are performed by different subjects in different environments with complex backgrounds. In robotic experiments, we show that our Watch-Bot is able to remind humans of forgotten actions in the real-world experiments.\nIn summary, the main contributions of this work are: \u2022 Our model is completely unsupervised thus being more useful\nand scalable. \u2022 Our model considers both the short-range and the long-\nrange action relations, showing the effectiveness in the action segmentation and clustering. \u2022 We show a new application by enabling a robot to remind humans of forgotten actions in the real scenes. \u2022 We provide a new challenging RGB-D activity dataset recorded by the new Kinect v2, which contains videos of multiple actions interacting with different objects.\nThe paper is organized as follows. Section 2 introduces the related works. Section 3 outlines our approach to modeling the composite activity. We present the visual features of the activity video clip in Section 4. Section 5 gives the detailed description of our learning model as well as its learning and inference. Section 6 introduces our watch-bot system to reminding of forgotten actions using our learned model. We give an extensive evaluation and discussion in the experiments in Section 7. Sections 8 concludes the paper."}, {"heading": "2 RELATED WORK", "text": "Action Recognition. Our work is related to the works on action recognition in computer vision. There is a large number of works on action recognition, which can be referred in recent surveys [3]. In this section, we cover the most related approaches. Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50]. Among them, the most popular are linear-chain models such as hidden markov model (HMM) [49], semi-Markov [16], [45] and the linear dynamic system [5]. They focus on modeling the local transitions (between frames, temporal segment, or sub-actions) in the activities. More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity. There are also some works focusing on detecting local action patches, primitives, trajectories or spatio-temporal features [19], [33], [35], [62] without considering the high-level action relations. There also exist some unsupervised approaches on action recognition. Yang et al. [62] develop a meaningful representation by discovering local motion primitives in an unsupervised way, then a HMM is learned over these primitives. Jones et al. [20] propose an\n2. The dataset and tools are released at http://watchnpatch.cs.cornell.edu.\n3 unsupervised dual assignment clustering on the dataset recorded from two views.\nAlthough these approaches have performed well in different areas, most of them rely on local relations between adjacent clips or actions that ignore the long-term action relations and use RGB visual features. Unlike these approaches, we use the richer human skeleton and RGB-D features rather than the RGB action features [21], [53]. We model the pairwise action co-occurrence and temporal relations in the whole video, thus relations are considered globally and completely with the uncertainty. We also use the learned relations to infer the forgotten actions without any manual annotations.\nRGB-D and Human Skeleton Features. Action recognition using human skeletons and RGB-D camera have shown the advantages over RGB videos in many works. Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59]. Besides of the human skeletons, we also detect the human interactive objects in an unsupervised way to provide more discriminate features. Object-in-use contextual information has been commonly used for recognizing actions [26], [27], [37], [54]. Moreover, Huet al. [17] propose a joint learning model to simultaneously learn heterogenous features from RGB-D activity videos. Most of them focus on designing or learning good action features. They lost the high-level action relations which can be captured in our model.\nBayesian Models. Our work is also related to the Bayesian models. LDA [8] was the first hierarchical Bayesian topic model and widely used in different applications. The correlated topic models [6], [23] add the priors over topics to capture topic correlations. A topic model over absolute timestamps of words is proposed in [55] and has been applied to action recognition [14]. However, the independence assumption of different topics would lead to non smooth temporal segmentations. Recently, a multifeature max-margin hierarchical Bayesian model [60] is proposed to jointly learn a high-level representation by combining a hierarchical generative model and discriminative maxmargin classifiers in a unified Bayesian framework. Differently, our model considers both correlations and the relative time distributions between topics rather than the absolute time, which captures richer information of action structures in the complex human activity.\nPerception of Human Activities for Robotics. Our work is also related to the works on recognizing human actions for robotics [10], [25], [32]. Yang et al. [61] presented a system that learns manipulation action plans for robot from unconstrained youtube videos. Hu et al. [18] proposed an activity recognition system trained from soft labeled data for the assistant robot. Chrungoo et al. [11] introduced a human-like stylized gestures for better human-robot interaction. Piyathilaka et al. [40] used 3D skeleton features and trained dynamic bayesian networks for domestic service robots. The output laser spot on object is also related to the work \u2018a clickable world\u2019 [36], which selects the appropriate behavior to execute for an assistive object-fetching robot using the 3D location of the click by the laser pointer. However, it is challenging to directly use these approaches to detecting the forgotten actions and remind people."}, {"heading": "3 OVERVIEW", "text": "We outline our approach in this section (see Fig. 2). The input to our system is RGB-D videos with the 3D joints of human skeletons\nFig. 2: Video representation. (1) A video frames (fi) is first decomposed into a sequence of overlapping fixed-length temporal clips. (2) The human-skeleton-trajectories/interactive-object-trajectories are extracted from each clip, and we cluster them to form the humandictionary/object-dictionary. (3) Then the video is represented as a sequence of human-word and object-word indices by mapping its human-skeleton-trajectories/interactive-object-trajectories to the nearest human-words/object-words in the dictionary. (4) An activity video is about a set of action-topics/object-topics indicating which actions are present and which types of objects are interacting with. (5) We learn the mapping of action-words/object-words to the actiontopics/object-topics, as well as the co-occurrence and the temporal relations between the topics. (6) We assign the topics to clips using the learned model.\nfrom Kinect v2. We first decompose a video into a sequence of overlapping fixed-length temporal clips (step (1)). We then extract the human-skeleton-trajectory features and the interacting-objecttrajectory features from the clips (introduced in Section. 4). The human skeleton features and RGB-D object features have shown higher performance over RGB only features for the human action modeling [26], [30], [59].\nIn order to build a compact representation of the action video, we draw parallels to document modeling in the natural language [8] to represent a video as a sequence of words. We use k-means to cluster the human-skeleton-trajectories/interactingobject-trajectories from all the clips in the training set to form a human-dictionary and an object-dictionary, where we use the cluster centers as human-words and object-words ((2) in Fig. 2). Then, the video can be represented as a sequence of humanword and object-word indices by mapping its human-skeletontrajectories/interacting-object-trajectories to the nearest humanwords/object-words in the dictionary ((3) in Fig. 2). Also, an activity video is about a set of action-topics indicating which actions are present in the video, and a set of object-topics indicating which object types are interacting in the actions ((4) in Fig. 2).\nWe then build an unsupervised learning model that models the mapping of action-words/object-words to the action-topics/object-\n4 topics, as well as the co-occurrence and the temporal relations between the topics ((5) in Fig. 2). Using the learned model, we can assign the action-topic/object-topic to each clip. So the continuous clips with the same assigned action-topic form an action segment ((6) in Fig. 2).\nThe unsupervised action assignments of the clips are challenging because there is no annotation during the training stage. Besides extracting rich visual features, we further consider the relations among actions and objects. Unlike previous works, our model captures long-range relations between actions e.g., putmilk-back-to-fridge is strongly related to fetch-milk-from-fridge even with pour and drink between them. We model all pairwise co-occurrence and temporal casual relations between topics in a video, using a new probabilistic model (introduced in Section 5). Specifically, we use a joint distribution as the correlated topic priors. They estimate which actions and objects are most likely to co-occur in a video. And we use a relative time distributions of topics to capture the temporal causal relations between actions, which estimate the possible temporal ordering of the occurring actions in the video."}, {"heading": "4 VISUAL FEATURES", "text": "We describe how we extract the visual features of a clip in this section. We extract both human-skeleton-trajectory features and the interacting-object-trajectory features from the output by the Kinect v2 [1], which has an improved body tracker and the higher resolution of RGB-D frame than the Kinect v1. The tracked human skeleton has 25 joints in total. Let Xu = {x(1)u , x(2)u , \u00b7 \u00b7 \u00b7 , x(25)u } be the 3D coordinates of 25 joints of a skeleton in the current frame u. We first compute the cosine of the angles between the connected body parts in each frame: \u03b1(pq) = (p(p) \u00b7p(q))/(|p(p)| \u00b7 |p(q)|), where the vector p(p) = x(i) \u2212 x(j) represents the body part. The transition between the joint coordinates and angles in different frames can well capture the human body movements. So we extract the motion features and off-set features [59] by computing their Euclidean distances D(, ) to previous frame fmu,u\u22121, f\u03b1u,u\u22121 and the first frame fmu,1, f \u03b1 u,1 in the clip: fmu,u\u22121 = {D(x(i)u , x (i) u\u22121)}25i=1, f\u03b1u,u\u22121 = {D(\u03b1(pq)u , \u03b1 (pq) u\u22121)}pq; fmu,1 = {D(x(i)u , x (i) 1 )}25i=1, f\u03b1u,1 = {D(\u03b1(pq)u , \u03b1 (pq) 1 )}pq. Then we concatenate all fmu,u\u22121, f \u03b1 u,u\u22121, f m u,1, f \u03b1 u,1 as the human features of the clip.\nWe also extract the human interacting-object-trajectory based on the human hands, image segmentation, motion detection and tracking. To detect the interacting objects, first we segment each frame into super-pixels using a fast edge detection approach [12] on both RGB and depth images. The RGB-D edge detection provides richer candidate super-pixels rather than pixels to further extract objects. We then apply the moving foreground mask [47] to remove the unnecessary steady backgrounds and select those\nsuper-pixels within a distance to the human hands in both 3D points and 2D pixels. Finally, we collect the bounding boxes enclosing these super-pixels as the potential interested objects (see examples in Fig. 3).\nWe then track the bounding box in the segmented clip using SIFT matching and RANSAC to get the trajectories. We use the closest trajectory to the human hands for the clip. Finally, we extract six kernel descriptors [41] from the bounding box of each frame in the trajectory: gradient, color, local binary pattern, depth gradient, spin, surface normals, and KPCA/self-similarity, which have been proven to be useful features for RGB-D data [56]. We concatenate the object features of each frame as the interactingobject-trajectory feature of the clip."}, {"heading": "5 LEARNING MODEL", "text": "In order to incorporate the aforementioned properties of activities, we present a new generative model (see the graphic model in Fig. 4 and the notations in Table 1). The novelty of our model is the ability to capture both short-range and long-range relations between actions in the compose activity videos in an unsupervised way. Using these relations, we can simultaneously segment the video and assign the action-topics as well as infer forgotten actions.\nConsider a collection of D videos (documents in the topic model). Each video as a document d consists of Nd continuous clips {cnd}Ndn=1, each of which consists of a human-word whnd mapped to the human-dictionary and an object-word wond mapped to the object-dictionary. We assign action-topic to each clip cnd from K latent action-topics, indicating which action-topic they belong to. We assign object-topic to each object-word wond from P latent object-topics, indicating which object-topic is interacting within the clip. The assignments are denoted as z(1)nd and z (2) nd . We use superscripts (1), (2) to denote action-topics and objecttopics respectively. After assignments, continuous clips with the same action-topic compose an action segment in a video. All the segments assigned with the same action-topic from the training set compose an action cluster.\nThe topic model such as LDA [8] has been very common for document modeling from language. We use a it to generate a video document using a mixture of topics. Enable to model human actions in the video, our model introduces co-occurrence and temporal structure of topics instead of the topic independence assumption in LDA.\nBasic generative process. In a document d, we choose z\n(1) dn \u223c Mult(\u03c0 (1) :d ), z (2) dn \u223c Mult(\u03c0 (2) :d ), where Mult(\u03c0) is a multinomial distribution with parameter \u03c0. The human-word whnd is drawn from an action-topic specific multinomial distribution \u03c6(1)\nz (1) nd\n, whdn \u223c Mult(\u03c6 (1)\nz (1) dn ), where \u03c6(1)k \u223c Dir(\u03b2(1)) is the human-word distribution of action-topic k, sampled from a Dirichlet prior with the hyperparameter \u03b2(1). While the objectword wond is drawn from an action-topic and object-topic specific multinomial distribution \u03c6(12)\nz (1) nd z (2) nd\n, wodn \u223c Mult(\u03c6 (12)\nz (1) nd z (2) nd\n),\nwhere \u03c6(12)kp \u223c Dir(\u03b2(12)) is the object-word distribution of action-topic k and object-topic p. Here we consider the same object type like book can be variant in appearance in different actions such as a close book in fetch-book and a open book in read action. So we consider the object-word distribution for different combinations of the action topic and the object topic.\n5\nTopic correlations. The co-occurrence such as action pour and action drink, object book and action read, is useful to recognizing the co-occurring actions/objects and also gives a strong evidence for detecting forgotten actions. We model the co-occurrence by drawing their priors from a mixture distribution. Let \u03c0(1)kd , \u03c0 (2) pd be the probability of action-topic k and object-topic p occurring in document d, where \u2211K k=1 \u03c0 (1) kd = 1, \u2211P p=1 \u03c0 (2) pd = 1. Instead of sampling it from a fix Dirichlet prior with parameter in LDA that models them independently, we construct the probabilities by a stick-breaking process as follows. The stick-breaking notion has been widely used for constructing random weights [23], [44].\n\u03c0 (1) kd = \u03a8(v (1) kd ) k\u22121\u220f l=1 \u03a8(v (1) ld ), \u03a8(v (1) kd ) =\n1\n1 + exp(\u2212v(1)kd ) ,\n\u03c0 (2) pd = \u03a8(v (2) pd ) p\u22121\u220f l=1 \u03a8(v (2) ld ), \u03a8(v (2) pd ) =\n1\n1 + exp(\u2212v(2)pd ) ,\nwhere 0 < \u03a8(v(1)kd ),\u03a8(v (2) pd ) < 1 is a classic logistic function, which satisfies \u03a8(\u2212v(1)kd ) = 1 \u2212 \u03a8(v (1) kd ),\u03a8(\u2212v (2) pd ) = 1\u2212\u03a8(v(2)pd ), and v (1) kd , v (2) pd serves as the prior of \u03c0 (1) kd , \u03c0 (2) pd .\nIn order to capture the correlations between action-topics and object-topics, we draw the packed vector v:d = [v (1) :d , v (2) :d ] in the stick-breaking notion from a mutivariate normal distribution N(\u00b5,\u03a3). In practice, we use a truncated vector v(1):d = [v\n(1) 1d , \u00b7 \u00b7 \u00b7 , v (1) K\u22121,d] for (K-1) topics, and set \u03c0 (1) Kd = 1 \u2212\u2211K\u22121\nk=1 \u03c0 (1) kd = \u220fK\u22121 k=1 \u03a8(\u2212v (1) kd ) as the probability of the final topic for a valid distribution. The same for v(2):d . Relative time distributions. The temporal relations between actions are also useful to discriminating the actions using temporal ordering and inferring the forgotten actions using the temporal context. We model the relative time of occurring actions by taking their time stamps into account. We consider that the relative time between two words are drawn from a certain distribution according\nto their topic assignments. In detail, let tnd, tmd \u2208 (0, 1) be the absolute time stamp of n-th word and m-th word, which is normalized by the video length. tmnd = tmd \u2212 tnd is the relative time of m-th clip relative to n-th clip. Then tmnd is drawn from a certain distribution, tmnd \u223c \u2126(\u03b8z(1)md,z(1)nd ), where \u03b8z(1)md,z(1)nd are the parameters. \u2126(\u03b8k,l) are K2 pairwise action-topic specific relative time distributions defined as follows:\n\u2126(t|\u03b8k,l) = { bk,l \u00b7N(t|\u03b8+k,l) if t \u2265 0, 1\u2212 bk,l \u00b7N(t|\u03b8\u2212k,l) if t < 0,\n(1)\nAn illustration of the learned relative time distributions are shown in Fig. 5. We can see that the distributions we learned correctly reflect the order of the actions, e.g., put-back-to-fridge is after pour and can be before/after microwave, and the shape is almost similar to the real distributions. Here the Bernoulli distribution bk,l/1 \u2212 bk,l gives the probability of action k after/before the action l. And two independent normal distributions N(t|\u03b8+k,l)/N(t|\u03b8 \u2212 k,l) estimate how long the action k is after/before the action l3. Then the order and the length of the actions will be captured by all these pairwise relative time distributions.\n3. Specially, when k = l, If two words are in the same segments, we draw t from a normal distribution which is centered on zero, and the variance models the length of the action. If not, it also follows Eq. (1) indicating the relative time between two same actions. We also use functions tan(\u2212\u03c0/2 + \u03c0t)(0 < t < 1), tan(\u03c0/2 + \u03c0t)(\u22121 < t < 0) to feed t to the normal distribution so that the probability is valid, that summits to one through the domain of t.\n6 (a) Robot System. (b) System Pipeline.\nFig. 6: (a). Our Watch-Bot system. It consists of a Kinect v2 sensor that inputs RGB-D frames of human actions, a laptop that infers the forgotten action and the related object, a pan/tilt camera that localizes the object, mounted with a fixed laser pointer that points out the object. (b). The system pipeline. The robot first uses the learned model to infer the forgotten action and the related object based on the Kinect\u2019s input. Then it maps the view from the Kinect to the pan/tilt camera so that the bounding box of the object is mapped in the camera\u2019s view. Finally, the camera moves until the laser spot lies in the bounding box of the target object."}, {"heading": "5.1 Learning and Inference", "text": "Gibbs sampling is commonly used as a means of statistical inference to approximate the distributions of variables when direct sampling is difficult [7], [23]. Given a video, the word whnd, w o nd and the relative time tmnd are observed. We can integrate out \u03a6\n(1) k ,\u03a6 (12) kp since Dir(\u03b2 (1)), Dir(\u03b2(12)) are conjugate priors for the multinomial distributions \u03a6(1)k ,\u03a6 (12) kp . We also estimate the standard distributions including the mutivariate normal distribution N(\u00b5,\u03a3) and the time distribution \u2126(\u03b8kl) using the method of moments, once per iteration of Gibbs sampling. Following the convention, we use the fixed symmetric Dirichlet distributions by setting \u03b2(1), \u03b2(12) as 0.01.\nThen we introduce how we sample the topic assignment z\n(1) nd , z (2) nd . We do a collapsed sampling as in LDA by calculating\nthe posterior distribution of z(1)nd , z (2) nd :\np(z (1) nd = k|\u03c0 (1) :d , z (1) \u2212nd, z (2) nd , tnd)\n\u221d \u03c0(1)kd \u03c9(k,w h nd)\u03c9(k, z (2) nd , w o nd)p(tnd|z (1) :d , \u03b8),\np(z (2) nd = p|\u03c0 (2) :d , z (2) \u2212nd, z (1) nd ) \u221d \u03c0 (2) pd \u03c9(z (1) nd , p, w o nd), \u03c9(k,whnd) = N\u2212nd kwh + \u03b2(1)\nN\u2212ndk +Nwh\u03b2 (1) ,\n\u03c9(k, p, wond) = N\u2212ndkpwo + \u03b2 (12)\nN\u2212ndkp +Nwo\u03b2 (12)\n,\np(tnd|z(1):d , \u03b8) = Nd\u220f m \u2126(tmnd|\u03b8z(1)md,k)\u2126(tnmd|\u03b8k,z(1)md), (2) where Nwh , Nwo is the number of unique word types in dictionary, N\u2212nd\nkwh /N\u2212ndkpwo denotes the number of instances of word\nwhnd/w o nd assigned with action-topic k/action-topic k and objecttopic p, excluding n-th word in d-th document, and N\u2212ndk /N \u2212nd kp denotes the number of total words assigned with action-topic k/action-topic k and object-topic p. z(1)\u2212nd/z (2) \u2212nd denotes the topic assignments for all words except z(1)nd /z (2) nd . The detailed derivation of Eq. (2) is in the Appendix A.\nIn Eq. (2), note that the topic assignments are decided by which actions/objects are more likely to co-occur in the video (the occurance probabilities \u03c0(1)kd /\u03c0 (2) kd ), the visual appearance of the word (the word distributions \u03c9(k,whnd), \u03c9(k, p, w o nd)) and the temporal relations (the relative time distributions p(tnd|z(1):d , \u03b8)). Due to the logistic stick-breaking transformation, the posterior distribution of the topic priors v:d = [v (1) :d , v (2) :d ] does not have a closed form. So we instead use a Metropolis-Hastings independence sampler [15]. Let the proposals q(v\u2217:d|v:d, \u00b5,\u03a3) = N(v\u2217:d|\u00b5,\u03a3) be drawn from the prior. The proposal is accepted with probability min(A(v\u2217:d, v:d), 1), where A(v\u2217:d, v:d)\n= p(v\u2217:d|\u00b5,\u03a3)\n\u220fNd n=1 p(z (1) nd |v (1)\u2217 :d )p(z (2) nd |v (2)\u2217 :d )q(v:d|v\u2217:d, \u00b5,\u03a3)\np(v:d|\u00b5,\u03a3) \u220fNd n=1 p(z (1) nd |v (1) :d )p(z (2) nd |v (2) :d )q(v \u2217 :d|v:d, \u00b5,\u03a3)\n= Nd\u220f n=1 p(z (1) nd |v (1)\u2217 :d )p(z (2) nd |v (2)\u2217 :d ) p(z (1) nd |v (1) :d )p(z (2) nd |v (2) :d )\n= K\u220f k=1 ( \u03c0 (1)\u2217 kd \u03c0 (1) kd ) \u2211Nd n=1 \u03b4(z (1) nd ,k) P\u220f p=1 ( \u03c0 (2)\u2217 pd \u03c0 (2) pd ) \u2211Nd n=1 \u03b4(z (2) nd ,p),\nwhich can be easily calculated by counting the number of words assigned with each topic by z(1)nd , z (2) nd . Here the function \u03b4(x, y) = 1 if only if x = y, otherwise equal to 0. The time complexity of the sampling per iteration is O(NdD(max(NdK,P )))\nFor inference of a test video, we sample the unknown topic assignments z(1)nd , z (2) nd and the topic priors v (1) :d , v (2) :d using the learned parameters in the training stage."}, {"heading": "6 WATCH-BOT TO REMINDING OF FORGOTTEN ACTIONS", "text": "The average adult forgets three key facts, chores or events every day [2]. So it is important for a personal robot to be able to detect not only what a human is currently doing but also what he forgot to do. In this section, we describe a new robot system (see Fig. 6)\n7 to detect the forgotten actions and remind people, which we called action patching, using our learning model.\nNote that detecting forgotten action is more challenging than conventional action recognition, since what to infer is not shown in the query video. Also, our model does not necessarily know the semantic class of the actions. Instead it learns action clusters and relations from the unlabeled action videos and use them to detect forgotten actions and remind people. Therefore, modeling rich relations from videos is important to providing evidence for detecting forgotten actions. Our model models pairwise cooccurrence and long-range temporal relations of actions/topics. As a result, rather than only modeling the single action or the local temporal transitions in the previous works, those actions occurred with a relatively large time interval, occurred after the forgotten actions, as well as the interacting objects can also be used to detect forgotten actions in our model. For example, a put-backbook might be forgotten as previously seen a fetch-book action before a long read action, and seen a book and a leave action indicates he really forgot it.\nWe enable a robot, that we call Watch-Bot, to detect humans\u2019 forgotten actions as well as localize the related object in the current scene. The robot consists of a Kinect v2 sensor, a pan/tilt camera (which we call camera for brevity in this paper) mounted with a laser pointer, and a laptop (see Fig. 6). This setup can be easily deployed on any assistive robot. Taking the example in Fig. 1, if our robot sees a person fetch a milk from the fridge, pour the milk, and leave without putting the milk back to the fridge. Our robot would first detect the forgotten action and the related object (the milk), given the input RGB-D frames and human skeletons from the Kinect; then map the object from the Kinect\u2019s view to the camera\u2019s view; finally pan/tilt the camera till its mounted laser pointer pointing to the milk.\nOur goal is to detect the forgotten action and then point out the related object in the forgotten action using our learned model (see Alg. 1). We first use our model to segment the query video into action segments (step 1,2 in Alg. 1), and then infer the most possible forgotten action-topic and the related object-topic (step 4 in Alg. 1). Next we retrieve a top forgotten action segment from the training database, containing the inferred forgotten actiontopic and the object-topic (step 5,6 in Alg. 1). Using the extracted object in the retrieved segment, we detect the bounding box of the related forgotten object in the Kinect\u2019s view of the query video (step 8,9,10 in Alg. 1). After that, we map the bounding box of the object from the Kinect\u2019s view to the camera\u2019s view. Finally, the pan/tilt camera moves until its mounted laser pointer points out the related object in the current scene.\nPatched Action and Object Inference. Our model infers the forgotten action using the probability inference based on the dependencies between actions and objects. After assigning the action-topics and object-topics to a query video q, we consider adding one additional clip c\u0302 consisting of w\u0302h, w\u0302o into q in each action segmentation point ts (see Fig 7). Then the probabilities of the missing action-topics km with object-topics pm in each segmentation point ts can be compared following the posterior distribution in Eq. (2):\np(z (1) c\u0302 = km, z (2) c\u0302 = pm, tc\u0302 = ts|other)\n\u221d \u03c0(1)kmd\u03c0 (2) pmd p(ts|z(1):d , \u03b8) \u2211 wh,wo \u03c9(km, w h)\u03c9(km, pm, w o),\ns.t. ts \u2208 Ts, km \u2208 [1 : K]\u2212Ke, (3)\nAlgorithm 1 Forgotten Action and Object Detection. Input: RGB-D video q with tracked human skeletons. Output: Claim no action forgotten, or output an action segment with the forgotten action and a bounding box of the related object in the current scene. 1. Assign the action-topics to clips and the object-topics to objectwords in q as introduced in Section 5.1. 2. Get the action segments by merging the continuous clips with the same assigned action-topic. 3. If the assigned action-topics Ke in q contains all modeled actiontopics [1 : K], claim no action forgotten and return; 4. For each action segmentation point ts, each not assigned actiontopic km \u2208 [1 : K] \u2212 Ke, and each object-topic pm \u2208 [1 : P ]:\nCompute the probability defined in Eq. 3; 5. Select the top tree possible tuples (km, pm, ts), and get the forgotten action segment candidate set Q which contains segments with topics (km, pm); 6. Select the top forgotten action segment p from Q with the maximum patch score(p); 7. If patch score(p) is smaller than a threshold, claim no action forgotten and return; 8. Segment the current frame to super-pixels using edge detection [12] as in Section 3; 9. Select the nearest super-pixels to both extracted object bounding box in q and p. 10. Merge the adjacent super-pixels and bound the largest one with a rectangle as the output bounding box. 11. Return the top forgotten action segment and the object bounding box.\nwhere Ts is the set of segmentation points (t1, t2 in Fig. 7) and Ke is the set of existing action-topics in the video (fetch-book, etc. in Fig. 7). Thus [1 : K] \u2212 Ke are the missing topics in the video (put-down-items, etc. in Fig. 7). p(ts|z(1):d , \u03b8), \u03c9(km, wh), \u03c9(km, pm, wo) can be computed as in Eq. (2). Here we marginized w\u0302h, w\u0302o to avoid the effect of a specific human-word or object-word. Note that, \u03c0(1)kd , \u03c0 (2) pd gives the probability of a missing action-topic with an object-topic in the video decided by the correlation we learned in the joint distribution prior, i.e., the close topics have higher probabilities to occur in this query video. And p(ts|z(1):d , \u03b8) measures the temporal consistency of adding a new action-topic. And the marginized word-topic distribution \u2211 wh,wo \u03c9(km, w h)\u03c9(km, pm, w o) give the likelihood of the topic learned from training data. Patched Action and Object Detection. Then we select the top three tuples (km, pm, ts) using the above probability. The action segments of action-topic km containing object-topic pm in the training set consist a patched action candidate segment set Q. We then select the patched action segment from Q with the maximum patch score defined in Eq. 4. In detail, we consider that the front and the tail of the patched action segment fpf , fpt should be similar to the tail of the adjacent segment in q before ts and the front of the adjacent segment in q after ts: fqt, fqf . At the same time, the middle of the patched action segment fpm should be different to fqt, fqf , as it is a different action forgotten in the video.4\npatch score(p) = ave(D(fpm, fqf ),D(fpm, fqt)) \u2212max(D(fpf , fqt),D(fpt, fqf )), (4)\nwhere D(, ) is the average pairwise distances between frames, ave(, ),max(, ) are the average and max value. If the maximum\n4. Here the middle, front, tail frames are 20%-length of segment centering on the middle frame, starting from the first frame, and ending at the last frame in the segment respectively.\n8\nscore is below a threshold or there is no missing topics (i.e.,Ke = [1 : K]) in the query video, we claim there is no forgotten actions. Then we detect the bounding box of the patched object. We first segment the current frame into super-pixels as in Section 3, second search the nearest segments using the extracted object in the test video and the patched action, finally merge the adjacent segments into one segment and bound the largest segment with a bounding box.\nReal Object Pointing. We now describe how we pan/tilt the camera to point out the real object in the current scene. We first compute the transformation homography matrix between the frame of the Kinect and the frame of the pan/tilt camera using keypoints matching and RANSAC, which can be done very fast within 0.1 second. Then we can transform the detected bounding box from the Kinect\u2019s view to the pan/tilt camera\u2019s view. Since the position of the laser spot in the pan/tilt camera view is fixed, next we only need to pan/tilt the camera till the laser spot lies within the bounding box of the target object. To avoid the coordinating error caused by distortion and inconsistency of the camera movement, we use an iterative search plus small step movement instead of one step movement to localize the object (illustrated in Fig. 6). In each iteration, the camera pan/tilt a small step towards to the target object according to the relative position between the laser spot and the bounding box. Then the homography matrix is recomputed in the new camera view, so that the bounding box is mapped in the new view. Until the laser spot is close enough to the center of the bounding box, the camera stops moving."}, {"heading": "7 EXPERIMENTS", "text": ""}, {"heading": "7.1 Watch-n-Patch Dataset", "text": "We collect a new challenging RGB-D activity dataset recorded by the new Kinect v2 camera. Each video in the dataset contains 2-7 actions interacting with different objects (see examples in Fig. 12). The new Kinect v2 has higher resolution of RGB-D frames (RGB: 1920 \u00d7 1080, depth: 512 \u00d7 424) and improved body tracking of human skeletons (25 body joints). We record 458 videos with a total length of about 230 minutes. We ask 7 subjects to perform human daily activities in 8 offices and 5 kitchens with complex backgrounds. And in each environment the activities are recorded in different views. It composed of fully annotated 21 types of actions (10 in the office, 11 in the kitchen) interacting with 23 types of objects. We also record the audio, though it is not used in this paper.\nIn order to get a variation in activities, we ask participants to finish task with different combinations of actions and ordering. Some actions occur together often such as fetch-from-fridge and put-back-to-fridge while some are not always in the same video such as take-item and read. Some actions are in fix ordering such as fetch-book and put-back-book while some occur in random order such as put-back-to-fridge and microwave. Moreover, to evaluate the action patching performance, 222 videos in the dataset has action forgotten by people and the forgotten actions are annotated. We give the examples of action classes in Fig. 12 and action sequences in Table 4."}, {"heading": "7.2 Experimental Setting and Compared Baselines", "text": "We evaluate in two environments \u2018office\u2019 and \u2018kitchen\u2019. In each environment, we split the data into a train set with most full videos (office: 87, kitchen 119) and a few forgotten videos (office: 10, kitchen 10), and a test set with a few full videos (office: 10, kitchen 20) and most forgotten videos (office: 89, kitchen 113). In our experiments, we compare seven unsupervised approaches with only action-topics. They are Hidden Markov Model (HMM), topic model LDA (TM), correlated topic model (CTM), topic model over absolute time (TM-AT), correlated topic model over absolute time (CTM-AT), topic model over relative time (TM-RT) and our causal topic model with only action-topics (CaTM-A) [57]. We compare three methods with both action-topics and object-topics. They are HMM with the object-topics (HMM-O), LDA with the object-topics (TM-O) and our causal topic model with the objecttopics (CaTM-AO). All these methods use the same human skeleton and RGB-D features introduced in Section 4. We also evaluate HMM and our model CaTM using the popular features for action recognition, dense trajectories feature (DTF) [53], extracted only in RGB videos5, named as HMM-DTF and CaTM-A-DTF, CaTMAO-DTF.\nIn the experiments, we set the number of topics and states of HMM equal to or more than ground-truth classes. For correlated topic models, we use the same topic prior in our model. For models over absolute time, we consider the absolute time of each word is drawn from a topic-specific normal distribution. For models over relative time, we use the same relative time distribution as in our model (Eq. (1)). The clip length of the action-words is set to 20 frames, densely sampled by step one and the size of action dictionary is set to 500. For patching, the candidate set for different approaches consist of the segments with the inferred missing topics by transition probabilities for HMM, the topic priors for TM and CTM, and both the topic priors and the time distributions for TM-AT, TM-RT, CTM-AT and our CaTM. Then we use the same patch score as in Section 6 to select the top one patched segments, and the average of the patch score computed in a set of the segmented videos after training is set as the threshold of claiming forgotten action."}, {"heading": "7.3 Evaluation Metrics", "text": "Action Segmentation and Cluster Assignment. We want to evaluate if the unsupervised learned action-topics and states of HMM are semantically meaningful. In the unsupervised setting, we need to map the assigned topics to the ground-truth labels for evaluation. This could be done by counting the mapped frames\n5. We train a codebook with the size of 2000 and encode the extracted DTF features in each clip as the bag of features using the codebook.\n9\nbetween topics and ground-truth classes. Let ki, ci be the assigned topic and ground-truth class of frame i. The count of a mapping is: mkc = \u2211 i \u03b4(ki,k)\u03b4(ci,c)\u2211\ni \u03b4(ci,c) , where\n\u2211 i \u03b4(ki, k)\u03b4(ci, c) is the number\nof frames assigned with topic k as the ground-truth class c and normalized by the number of frames as the ground-truth class c: \u2211 i \u03b4(ci, c). Then we can solve the following binary linear programming to get the best mapping: max x \u2211 k,c xkcmkc,\ns.t. \u2200k, \u2211 c xkc = 1, \u2200c, \u2211 k xkc \u2265 1, xkc \u2208 {0, 1},\nwhere xkc = 1 indicates mapping topic k to class c, otherwise xkc = 0. And \u2211 c xkc = 1 constrain that each topic must be\nmapped to exact one class, \u2211 k xkc \u2265 1 constrain that each class must be mapped by at least one topic. We then measure the performance in two ways. Per frame: we compute frame-wise accuracy (Frame-Acc), the ratio of correctly labeled frames. Segmentation: we consider a true positive if the overlap (union/intersection) between the detected and the groundtruth segments is more than a default threshold 40% as in [39]. Then we compute segmentation accuracy (Seg-Acc), the ratio of the ground-truth segments that are correctly detected, and segmentation average precision (Seg-AP) by sorting all action segments output by the approach using the average probability of their words\u2019 topic assignments. All above three metrics are computed by taking the average of each action class.\nForgotten Action and Object Detection. We also evaluate the patching accuracy (PA-Acc) by the portion of correct patched video, including correctly output the forgotten action segments or correctly claiming no forgotten actions. We consider the output action segments by the algorithm containing over 50% groundtruth forgotten actions as correctly output the forgotten action\nFig. 8: Online segmentation Acc/AP varied with the number of topics in the \u2018office\u2019 dataset.\nFig. 9: Forgotten action/object detection accuracy varied with the number of action-topics in the \u2018office\u2019 dataset.\nsegments. We also measure the patching object detection accuracy (PO-Acc) by the typical object detection metric, that considers a true positive if the overlap rate (union/intersection) between the detected and the ground-truth object bounding box is greater than 40%."}, {"heading": "7.4 Results", "text": "Table 2 and Fig. 8 show the main results of our experiments. We first perform evaluation in the offline setting to see if actions can be well segmented and clustered in the train set. We then perform testing in an online setting to see if the new video from the test set can be correctly segmented and the segments can be correctly assigned to the action cluster. We can see that our approach performs better than the state-of-the-art in unsupervised action segmentation and clustering, as well as action patching. We discuss our results in the light of the following questions.\nDid modeling the long-range relations help? We studied whether modeling the correlations and the temporal relations between topics was useful. The approaches considering the temporal relations, HMM, TM-RT, and our CaTM, outperform other approaches which assume actions are temporal independent. This demonstrates that understanding temporal structure is critical to recognizing and patching actions. The approaches, TM-RT and CaTM, which model both the short-range and the long-range relations perform better than HMM only modeling local relations. Also, the approaches considering the topic correlations CTM, CTM-AT, and our CaTM perform better than the corresponding non-correlated topic models TM, TM-AT, and TM-RT. Our CaTM, which considers both the action correlation priors and the temporal relations, shows the best performance.\nHow successful was our unsupervised approach in learning meaningful action-topics? From Table 2, we can see that the unsupervised learned action-topics is promising to be semantically\n10\nFig. 10: An example of the robotic experiment. The robot detects the human left the food in the microwave, then points to the microwave.\nmeaningful even though ground-truth semantic labels are not provided in the training. In order to qualitatively estimate the performance, we give a visualization of our learned topics in Fig. 11. It shows that the actions with the same semantic meaning are clustered together though they are in different views and motions. In addition to the one-to-one correspondence between topics and semantic action classes, we also plot the performance curves varied with the topic number in Fig. 8. It shows that if we set the topics a bit more than ground-truth classes, the performance increases since a certain action might be divided into multiple action-topics. But as topics increase, more variations are also introduced so that performance saturates.\nRGB videos vs. RGB-D videos. In order to compare the effect of using information from RGB-D videos, we also evaluate our model CaTM and HMM using the popular RGB features for action recognition (CaTM-A-DTF, CaTM-AO-DTF and HMMDTF in Table 2). Clearly, the proposed human skeleton and RGB-D features outperform the DTF features as more accurate human motion and object are extracted.\nHow well did our new application of action patching performs? From Table 2, we find that the approaches learning the action relations mostly give better patching performance. This is because the learned co-occurrence and temporal structure strongly help indicate which actions are forgotten. Our model capturing both the short-range and long-range action relations shows the best results.\nHow important is it to consider relations between actions and objects? From the results, we can see that the model which did well in forgotten action detection also performed well in detecting forgotten object. Since our model CaTM-AO well considers the relations between the action and the object, it shows better performance in both forgotten action and forgotten object detection than those which models action and object independently as well as CaTM-A which only models the actions."}, {"heading": "7.5 Robotic Experiments", "text": "In this section, we show how our Watch-Bot reminds people of the forgotten actions in the real-world scenarios. We test each two forgotten scenarios in \u2018office\u2019 and \u2018kitchen\u2019 respectively (putback-book, turn-off-monitor, put-milk-back-to-fridge and fetchfood-from-microwave). We use a subset of the dataset to train the model in each activity type separately. In each scenario, we ask 3 subjects to perform the activity twice. Therefore, we test 24 trials in total. We evaluate three aspects. One is objective, the success\nrate (Succ-Rate): the laser spot lying within the object as correct. The other two are subjective, the average Subjective Accuracy Score (Subj-AccScore): we ask the participant if he thinks the pointed object is correct; and the average Subjective Helpfulness Score (Subj-HelpScore): we ask the participant if the output of the robot is helpful. Both of them are in 1 \u2212 5 scale, the higher the better.\nTable 3 gives the results of our robotic experiments. We can see that our robot can achieve over 60% success rate and gives the best performance. In most cases people think our robot is able to help them understand what is forgotten. Fig. 10 gives an example of our experiment, in which our robot observed what a human is currently doing, realized he forgot to fetch food from microwave and then correctly pointed out the microwave in the scene."}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "In this paper, we presented an algorithm that models the human activities in a completely unsupervised setting. We showed that it is important to modeling the long-range relations between\n11\nthe actions. To achieve this, we considered the video as a sequence of human-words/object-words, and an activity as a set of action-topics/object-topics. Then we modeled the word-topic distributions, the topic correlations and the topic relative time distributions. We then showed the effectiveness of our model in the unsupervised action segmentation and clustering, as well as the action patching. Moreover, we showed that our proposed robot system using the action patching algorithm was able to effectively remind people of forgotten actions in the real-world robotic experiments. For evaluation, we also contributed a new challenging RGB-D activity video dataset.\nThough we showed the promising results and the interesting applications of the purely unsupervised models in the paper, we can see that the performance is not more than 50 percent on the large-scale variant data, as we have no knowledge of the semantic information. In the future, we plan to extend the model to the semisupervised approaches that can effectively use a small portion of the annotated data for better learning, and improve on the performance in the real-world applications.\nAPPENDIX\nWe give the detailed derivation of the posterior distribution of znd (Eq. (2)) in this section. We begin with the joint distribution p(wh,wo, t, z(1), z(2)|\u03c0(1), \u03c0(2), \u03b2(1), \u03b2(12), \u03b8), where wh,wo, t, z(1), z(2), \u03c0(1), \u03c0(2) are all variables of the word whnd,w o nd, the time stamp of a word tnd, the topic-assignment of a word z(1)nd ,z (2) nd and the topic probability \u03c0 (1) kd ,\u03c0 (2) kd inD documents of K action topics and P object topics.\np(wh,wo, t, z(1), z(2)|\u03c0(1), \u03c0(2), \u03b2(1), \u03b2(12), \u03b8) = p(wh|z(1), \u03b2(1))p(wo|z(1), z(2), \u03b2(12))\u00b7 p(t|z(1), \u03b8)p(z(1)|\u03c0(1))p(z(2)|\u03c0(2))\n= \u222b p(wh|z(1), \u03c6(1))p(\u03c6(1), \u03b2(1))d\u03c6(1)\u00b7\u222b p(wo|z(1), z(2), \u03c6(12))p(\u03c6(12), \u03b2(12))d\u03c6(12)\u00b7\np(t|z(1), \u03b8) \u00b7 p(z(1)|\u03c0(1))p(z(2)|\u03c0(2)).\nwhere the joint distribution is decided by the following five terms. topic-word distributions:\u222b\np(wh|z(1), \u03c6(1))p(\u03c6(1), \u03b2(1))d\u03c6(1)\n= \u222b D\u220f d=1 Nd\u220f n=1 \u03c6 (1) z (1) nd ,w h nd K\u220f k=1\n1\nB(\u03b2(1)) \u220f w \u03c6 (1)\u03b2(1)w \u22121 kw d\u03c6 (1) k\n= K\u220f k=1\n1\nB(\u03b2(1)) \u222b \u220f w \u03c6 (1)Nkw+\u03b2 (1) w \u22121 kw d\u03c6 (1) k\n= K\u220f k=1 B(Nk + \u03b2 (1))\nB(\u03b2(1))\u222b p(wo|z(1), z(2), \u03c6(12))p(\u03c6(12), \u03b2(12))d\u03c6(12)\n= K\u220f k=1 P\u220f p=1 B(Nkp + \u03b2 (12)) B(\u03b2(12)) ,\nwhere we denote the Beta function as B(\u03b2) = \u220fK k=1 \u0393(\u03b2k)\n\u0393( \u2211K k=1 \u03b2k) .\ntopic-pair relative time distribution:\np(t|z(1), \u03b8) = D\u220f d=1 Nd\u220f n=1 p(tnd|z(1):d , \u03b8)\n= D\u220f d=1 Nd\u220f m=1 Nd\u220f n=1 p(tmnd|\u03b8z(1)md,z(1)nd ).\ntopic priors:\np(z(1)|\u03c0(1)) = D\u220f d=1 Nd\u220f n=1 \u03c0 (1) z (1) nd ,d\np(z(2)|\u03c0(2)) = D\u220f d=1 Nd\u220f n=1 \u03c0 (2) z (2) nd ,d .\nThen for a certain assignment z(1)nd , we give the posterior using the above joint distribution:\np(z (1) nd |\u03c0 (1) :d , z (1) \u2212nd, z (2) nd , tnd)\n= p(wh,wo, t, z(1), z(2)|\u03c0(1), \u03c0(2), \u03b2(1), \u03b2(12), \u03b8) p(wh,wo, t, z\n(1) \u2212nd, z (2)|\u03c0(1), \u03c0(2), \u03b2(1), \u03b2(12), \u03b8)\n\u221d p(w h,wo, t, z(1), z(2)|\u03c0(1), \u03c0(2), \u03b2(1), \u03b2(12), \u03b8)\np(wh\u2212nd, w o \u2212nd, t\u2212nd, z (1) \u2212nd, z (2)|\u03c0(1), \u03c0(2), \u03b2(1), \u03b2(12), \u03b8)\n=\u03c0 (1)\nz (1) nd ,d\n\u03c9(z (1) nd , w h nd)\u03c9(z (1) nd , z (2) nd , w o nd)p(tnd|z (1) :d , \u03b8),\nwhere:\n\u03c9(z (1) nd , w h nd) = K\u220f k=1 B(Nk + \u03b2 (1)) B(N\u2212ndk + \u03b2 (1)) = N\u2212nd z (1) nd ,w h + \u03b2(1)\nN\u2212nd z (1) nd +Nwh\u03b2(1)\n\u03c9(z (1) nd , z (2) nd , w o nd) = K\u220f k=1 P\u220f p=1 B(Nkp + \u03b2 (12)) B(N\u2212ndkp + \u03b2 (12))\n= N\u2212nd z (1) nd ,z (2) nd ,w o + \u03b2(12)\nN\u2212nd z (1) nd ,z (2) nd +Nwo\u03b2(12) p(tnd|z(1):d , \u03b8) = \u220f m p(tmnd|\u03b8z(1)md,z(1)nd )p(tnmd|\u03b8z(1)nd ,z(1)md)\n= \u220f m \u2126(tmnd|\u03b8z(1)md,z(1)nd )\u2126(tnmd|\u03b8z(1)nd ,z(1)md).\nThen assign z(1)nd with a specific topic k, we have the sampling\nposterior Eq. (2):\np(z (1) nd = k|\u03c0 (1) :d , z (1) \u2212nd, z (2) nd , tnd)\n\u221d \u03c0(1)kd \u03c9(k,w h nd)\u03c9(k, z (2) nd , w o nd)p(tnd|z (1) :d , \u03b8),\n\u03c9(k,whnd) = N\u2212nd kwh + \u03b2(1)\nN\u2212ndk +Nwh\u03b2 (1) ,\n\u03c9(k, p, wond) = N\u2212ndkpwo + \u03b2 (12)\nN\u2212ndkp +Nwo\u03b2 (12)\n,\np(tnd|z(1):d , \u03b8) = Nd\u220f m \u2126(tmnd|\u03b8z(1)md,k)\u2126(tnmd|\u03b8k,z(1)md),\nSimilarly we have:\np(z (2) nd = p|\u03c0 (2) :d , z (2) \u2212nd, z (1) nd ) \u221d \u03c0 (2) pd \u03c9(z (1) nd , p, w o nd).\n12"}], "references": [{"title": "Human activity analysis: A review", "author": ["J. Aggarwal", "M. Ryoo"], "venue": "ACM Comput. Surv., 43(3):16:1\u201316:43,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Video classification using semantic concept co-occurrences", "author": ["S.M. Assari", "A.R. Zamir", "M. Shah"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognition of complex events: Exploiting temporal dynamics between underlying concepts", "author": ["S. Bhattacharya", "M.M. Kalayeh", "R. Sukthankar", "M. Shah"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "A correlated topic model of science", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "The Annals of Applied Statistics, 1(1):17\u201335,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "Text mining: classification, clustering, and applications, 10:71,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., 3:993\u20131022,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["P. Bojanowski", "R. Lajugie", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Action recognition using ensemble weighted multi-instance learning", "author": ["G. Chen", "M. Giuliani", "D.S. Clarke", "A.K. Gaschler", "A. Knoll"], "venue": "International Conference on Robotics and Automation (ICRA),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Activity recognition for natural human robot interaction", "author": ["A. Chrungoo", "S. Manimaran", "B. Ravindran"], "venue": "Social Robotics, volume 8755, pages 84\u201394.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Structured forests for fast edge detection", "author": ["P. Doll\u00e1r", "C.L. Zitnick"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic annotation of human actions in video", "author": ["O. Duchenne", "I. Laptev", "J. Sivic", "F. Bach", "J. Ponce"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Time based activity inference using latent dirichlet allocation", "author": ["T.A. Faruquie", "P.K. Kalra", "S. Banerjee"], "venue": "British Machine Vision Conference (BMVC),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian data analysis", "author": ["A. Gelman", "J.B. Carlin", "H.S. Stern", "D.B. Dunson", "A. Vehtari", "D.B. Rubin"], "venue": "CRC press,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint segmentation and classification of human actions in video", "author": ["M. Hoai", "Z. zhong Lan", "F. De la Torre"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Jointly learning heterogeneous features for rgb-d activity recognition", "author": ["J.-F. Hu", "W.-S. Zheng", "J. Lai", "J. Zhang"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to recognize human activities from soft labeled data", "author": ["N. Hu", "Z. Lou", "G. Englebienne", "B. Krse"], "venue": "Proceedings of Robotics: Science and Systems (RSS),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Representing videos using mid-level discriminative patches", "author": ["A. Jain", "A. Gupta", "M. Rodriguez", "L. Davis"], "venue": "The IEEE conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised spectral dual assignment clustering of human actions in context", "author": ["S. Jones", "L. Shao"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient feature extraction, encoding and classification for action recognition", "author": ["V. Kantorov", "I. Laptev"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Event detection in crowded videos", "author": ["Y. Ke", "R. Sukthankar", "M. Hebert"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "The doubly correlated nonparametric topic model", "author": ["D.I. Kim", "E.B. Sudderth"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Human focused action localization in video", "author": ["A. Kl\u00e4ser", "M. Marsza\u0142ek", "C. Schmid", "A. Zisserman"], "venue": "International Workshop on Sign, Gesture, and Activity (SGA) in Conjunction with ECCV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning human activities and object affordances from RGB-D videos", "author": ["H.S. Koppula", "R. Gupta", "A. Saxena"], "venue": "I. J. Robotic Res., 32(8):951\u2013 970,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H.S. Koppula", "A. Saxena"], "venue": "Robotics: Science and Systems (RSS),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning spatio-temporal structure from RGB-D videos for human activity detection and anticipation", "author": ["H.S. Koppula", "A. Saxena"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "The language of actions: Recovering the syntax and semantics of goal-directed human activities", "author": ["H. Kuehne", "A. Arslan", "T. Serre"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Retrieving actions in movies", "author": ["I. Laptev", "P. Perez"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Depth and skeleton associated action recognition without online accessible rgb-d cameras", "author": ["Y.-Y. Lin", "J.-H. Hua", "N.C. Tang", "M.-H. Chen", "H.-Y. Mark Liao"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognizing human actions by attributes", "author": ["J. Liu", "B. Kuipers", "S. Savarese"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature set selection and optimal classifier for human activity recognition", "author": ["M. Losch", "S. Schmidt-Rohr", "S. Knoop", "S. Vacek", "R. Dillmann"], "venue": "Robot and Human interactive Communication,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Space-time tree ensemble for action  recognition", "author": ["S. Ma", "L. Sigal", "S. Sclaroff"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition", "author": ["S. Mathe", "C. Sminchisescu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "A cause and effect analysis of motion trajectories for modeling actions", "author": ["S. Narayan", "K.R. Ramakrishnan"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "A clickable world: Behavior selection through pointing and context for mobile manipulation", "author": ["H. Nguyen", "A. Jain", "C.D. Anderson", "C.C. Kemp"], "venue": "International Conference on Intelligent Robots and Systems,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple granularity analysis for fine-grained action detection", "author": ["B. Ni", "V.R. Paramathayalan", "P. Moulin"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling temporal structure of decomposable motion segments for activity classification", "author": ["J.C. Niebles", "C.-W. Chen", "L. Fei-Fei"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Parsing videos of actions with segmental grammars", "author": ["H. Pirsiavash", "D. Ramanan"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Human activity recognition for domestic robots", "author": ["L. Piyathilaka", "S. Kodagoda"], "venue": "Field and Service Robotics, volume 105, pages 395\u2013408,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Rgb-(d) scene labeling: Features and algorithms", "author": ["X. Ren", "L. Bo", "D. Fox"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Action bank: A high-level representation of activity in video", "author": ["S. Sadanand", "J.J. Corso"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "A database for fine grained activity detection of cooking activities", "author": ["B. Schiele"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica, 4:639\u2013650,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1994}, {"title": "Human action segmentation and recognition using discriminative semi-markov models", "author": ["Q. Shi", "L. Cheng", "L. Wang", "A. Smola"], "venue": "International Journal of Computer Vision (IJCV), 93(1):22\u201332,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Temporally coherent interpretations for long videos using pattern theory", "author": ["F. Souza", "S. Sarkar", "A. Srivastava", "J. Su"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive background mixture models for real-time tracking", "author": ["C. Stauffer", "W. Grimson"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1999}, {"title": "Unstructured human activity detection from rgbd images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "International Conference on Robotics and Automation (ICRA),", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning latent temporal structure for complex event detection", "author": ["K. Tang", "L. Fei-Fei", "D. Koller"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Spatiotemporal deformable part models for action detection", "author": ["Y. Tian", "R. Sukthankar", "M. Shah"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Human action recognition  14 by representing 3d skeletons as points in a lie group", "author": ["R. Vemulapalli", "F. Arrate", "R. Chellappa"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "From stochastic grammar to bayes network: Probabilistic parsing of complex activity", "author": ["N.N. Vo", "A.F. Bobick"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Action Recognition by Dense Trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "A hierarchical context model for event recognition in surveillance video", "author": ["X. Wang", "Q. Ji"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Topics over time: A non-markov continuoustime model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2006}, {"title": "Hierarchical semantic labeling for taskrelevant rgb-d perception", "author": ["C. Wu", "I. Lenz", "A. Saxena"], "venue": "Robotics: Science and Systems (RSS),", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Watch-n-patch: Unsupervised understanding of actions and relations", "author": ["C. Wu", "J. Zhang", "S. Savarese", "A. Saxena"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Watch-bot: Unsupervised learning for reminding humans of forgotten actions", "author": ["C. Wu", "J. Zhang", "B. Selman", "S. Savarese", "A. Saxena"], "venue": "International Conference on Robotics and Automation (ICRA),", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition", "author": ["D. Wu", "L. Shao"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-feature maxmargin hierarchical bayesian model for action recognition", "author": ["S. Yang", "C. Yuan", "B. Wu", "W. Hu", "F. Wang"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "Robot learning manipulation action plans by watching unconstrained videos from the world wide web", "author": ["Y. Yang", "Y. Li", "C. Fermuller", "Y. Aloimonos"], "venue": "AAAI,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Discovering motion primitives for unsupervised grouping and one-shot learning of human actions, gestures, and expressions", "author": ["Y. Yang", "I. Saleemi", "M. Shah"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 35(7):1635\u20131648,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 54, "context": "Parts of this work have been published in [57], [58] as the conference version.", "startOffset": 42, "endOffset": 46}, {"referenceID": 55, "context": "Parts of this work have been published in [57], [58] as the conference version.", "startOffset": 48, "endOffset": 52}, {"referenceID": 28, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 70, "endOffset": 74}, {"referenceID": 40, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 117, "endOffset": 120}, {"referenceID": 10, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 122, "endOffset": 126}, {"referenceID": 26, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 176, "endOffset": 180}, {"referenceID": 34, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 182, "endOffset": 186}, {"referenceID": 47, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 188, "endOffset": 192}, {"referenceID": 46, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 138, "endOffset": 142}, {"referenceID": 42, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 144, "endOffset": 148}, {"referenceID": 2, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 181, "endOffset": 184}, {"referenceID": 1, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 214, "endOffset": 217}, {"referenceID": 25, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 219, "endOffset": 223}, {"referenceID": 36, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 225, "endOffset": 229}, {"referenceID": 49, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 231, "endOffset": 235}, {"referenceID": 51, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 237, "endOffset": 241}, {"referenceID": 19, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 287, "endOffset": 291}, {"referenceID": 21, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 293, "endOffset": 297}, {"referenceID": 24, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 299, "endOffset": 303}, {"referenceID": 35, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 305, "endOffset": 309}, {"referenceID": 23, "context": "Object-in-use contextual information has also been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 89, "endOffset": 93}, {"referenceID": 24, "context": "Object-in-use contextual information has also been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 95, "endOffset": 99}, {"referenceID": 34, "context": "Object-in-use contextual information has also been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 101, "endOffset": 105}, {"referenceID": 51, "context": "Object-in-use contextual information has also been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 107, "endOffset": 111}, {"referenceID": 23, "context": "We also use the more informative human skeleton features and RGB-D object features, which have shown higher performance over RGB only features for action recognition [26], [30], [59].", "startOffset": 166, "endOffset": 170}, {"referenceID": 27, "context": "We also use the more informative human skeleton features and RGB-D object features, which have shown higher performance over RGB only features for action recognition [26], [30], [59].", "startOffset": 172, "endOffset": 176}, {"referenceID": 56, "context": "We also use the more informative human skeleton features and RGB-D object features, which have shown higher performance over RGB only features for action recognition [26], [30], [59].", "startOffset": 178, "endOffset": 182}, {"referenceID": 5, "context": ", [8]) and proposed a Casual Topic Model (CaTM).", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "There is a large number of works on action recognition, which can be referred in recent surveys [3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 57, "endOffset": 60}, {"referenceID": 10, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 62, "endOffset": 66}, {"referenceID": 26, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 68, "endOffset": 72}, {"referenceID": 28, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 80, "endOffset": 84}, {"referenceID": 35, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 86, "endOffset": 90}, {"referenceID": 39, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 92, "endOffset": 96}, {"referenceID": 47, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 98, "endOffset": 102}, {"referenceID": 46, "context": "Among them, the most popular are linear-chain models such as hidden markov model (HMM) [49], semi-Markov [16], [45] and the linear dynamic system [5].", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "Among them, the most popular are linear-chain models such as hidden markov model (HMM) [49], semi-Markov [16], [45] and the linear dynamic system [5].", "startOffset": 105, "endOffset": 109}, {"referenceID": 42, "context": "Among them, the most popular are linear-chain models such as hidden markov model (HMM) [49], semi-Markov [16], [45] and the linear dynamic system [5].", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "Among them, the most popular are linear-chain models such as hidden markov model (HMM) [49], semi-Markov [16], [45] and the linear dynamic system [5].", "startOffset": 146, "endOffset": 149}, {"referenceID": 25, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 42, "endOffset": 46}, {"referenceID": 49, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 48, "endOffset": 52}, {"referenceID": 51, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 78, "endOffset": 81}, {"referenceID": 43, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "There are also some works focusing on detecting local action patches, primitives, trajectories or spatio-temporal features [19], [33], [35], [62] without considering the high-level action relations.", "startOffset": 123, "endOffset": 127}, {"referenceID": 30, "context": "There are also some works focusing on detecting local action patches, primitives, trajectories or spatio-temporal features [19], [33], [35], [62] without considering the high-level action relations.", "startOffset": 129, "endOffset": 133}, {"referenceID": 32, "context": "There are also some works focusing on detecting local action patches, primitives, trajectories or spatio-temporal features [19], [33], [35], [62] without considering the high-level action relations.", "startOffset": 135, "endOffset": 139}, {"referenceID": 59, "context": "There are also some works focusing on detecting local action patches, primitives, trajectories or spatio-temporal features [19], [33], [35], [62] without considering the high-level action relations.", "startOffset": 141, "endOffset": 145}, {"referenceID": 59, "context": "[62] develop a meaningful representation by discovering local motion primitives in an unsupervised way, then a HMM is learned over these primitives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] propose an", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Unlike these approaches, we use the richer human skeleton and RGB-D features rather than the RGB action features [21], [53].", "startOffset": 113, "endOffset": 117}, {"referenceID": 50, "context": "Unlike these approaches, we use the richer human skeleton and RGB-D features rather than the RGB action features [21], [53].", "startOffset": 119, "endOffset": 123}, {"referenceID": 27, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 73, "endOffset": 77}, {"referenceID": 40, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 79, "endOffset": 83}, {"referenceID": 45, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 85, "endOffset": 89}, {"referenceID": 48, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 91, "endOffset": 95}, {"referenceID": 56, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "Object-in-use contextual information has been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "Object-in-use contextual information has been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 90, "endOffset": 94}, {"referenceID": 34, "context": "Object-in-use contextual information has been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 96, "endOffset": 100}, {"referenceID": 51, "context": "Object-in-use contextual information has been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "[17] propose a joint learning model to simultaneously learn heterogenous features from RGB-D activity videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "LDA [8] was the first hierarchical Bayesian topic model and widely used in different applications.", "startOffset": 4, "endOffset": 7}, {"referenceID": 3, "context": "The correlated topic models [6], [23] add the priors over topics to capture topic correlations.", "startOffset": 28, "endOffset": 31}, {"referenceID": 20, "context": "The correlated topic models [6], [23] add the priors over topics to capture topic correlations.", "startOffset": 33, "endOffset": 37}, {"referenceID": 52, "context": "A topic model over absolute timestamps of words is proposed in [55] and has been applied to action recognition [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "A topic model over absolute timestamps of words is proposed in [55] and has been applied to action recognition [14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 57, "context": "Recently, a multifeature max-margin hierarchical Bayesian model [60] is proposed to jointly learn a high-level representation by combining a hierarchical generative model and discriminative maxmargin classifiers in a unified Bayesian framework.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "Our work is also related to the works on recognizing human actions for robotics [10], [25], [32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "Our work is also related to the works on recognizing human actions for robotics [10], [25], [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "Our work is also related to the works on recognizing human actions for robotics [10], [25], [32].", "startOffset": 92, "endOffset": 96}, {"referenceID": 58, "context": "[61] presented a system that learns manipulation action plans for robot from unconstrained youtube videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] proposed an activity recognition system trained from soft labeled data for the assistant robot.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] introduced a human-like stylized gestures for better human-robot interaction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[40] used 3D skeleton features and trained dynamic bayesian networks for domestic service robots.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "The output laser spot on object is also related to the work \u2018a clickable world\u2019 [36], which selects the appropriate behavior to execute for an assistive object-fetching robot using the 3D location of the click by the laser pointer.", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "The human skeleton features and RGB-D object features have shown higher performance over RGB only features for the human action modeling [26], [30], [59].", "startOffset": 137, "endOffset": 141}, {"referenceID": 27, "context": "The human skeleton features and RGB-D object features have shown higher performance over RGB only features for the human action modeling [26], [30], [59].", "startOffset": 143, "endOffset": 147}, {"referenceID": 56, "context": "The human skeleton features and RGB-D object features have shown higher performance over RGB only features for the human action modeling [26], [30], [59].", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "In order to build a compact representation of the action video, we draw parallels to document modeling in the natural language [8] to represent a video as a sequence of words.", "startOffset": 127, "endOffset": 130}, {"referenceID": 56, "context": "So we extract the motion features and off-set features [59] by computing their Euclidean distances D(, ) to previous frame f u,u\u22121, f u,u\u22121 and the first frame f u,1, f \u03b1 u,1 in the clip: f u,u\u22121 = {D(x u , x (i) u\u22121)} i=1, f u,u\u22121 = {D(\u03b1 u , \u03b1 (pq) u\u22121)}pq; f u,1 = {D(x u , x (i) 1 )} i=1, f u,1 = {D(\u03b1 u , \u03b1 (pq) 1 )}pq.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "To detect the interacting objects, first we segment each frame into super-pixels using a fast edge detection approach [12] on both RGB and depth images.", "startOffset": 118, "endOffset": 122}, {"referenceID": 44, "context": "We then apply the moving foreground mask [47] to remove the unnecessary steady backgrounds and select those super-pixels within a distance to the human hands in both 3D points and 2D pixels.", "startOffset": 41, "endOffset": 45}, {"referenceID": 38, "context": "Finally, we extract six kernel descriptors [41] from the bounding box of each frame in the trajectory: gradient, color, local binary pattern, depth gradient, spin, surface normals, and KPCA/self-similarity, which have been proven to be useful features for RGB-D data [56].", "startOffset": 43, "endOffset": 47}, {"referenceID": 53, "context": "Finally, we extract six kernel descriptors [41] from the bounding box of each frame in the trajectory: gradient, color, local binary pattern, depth gradient, spin, surface normals, and KPCA/self-similarity, which have been proven to be useful features for RGB-D data [56].", "startOffset": 267, "endOffset": 271}, {"referenceID": 5, "context": "The topic model such as LDA [8] has been very common for document modeling from language.", "startOffset": 28, "endOffset": 31}, {"referenceID": 20, "context": "The stick-breaking notion has been widely used for constructing random weights [23], [44].", "startOffset": 79, "endOffset": 83}, {"referenceID": 41, "context": "The stick-breaking notion has been widely used for constructing random weights [23], [44].", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "Gibbs sampling is commonly used as a means of statistical inference to approximate the distributions of variables when direct sampling is difficult [7], [23].", "startOffset": 148, "endOffset": 151}, {"referenceID": 20, "context": "Gibbs sampling is commonly used as a means of statistical inference to approximate the distributions of variables when direct sampling is difficult [7], [23].", "startOffset": 153, "endOffset": 157}, {"referenceID": 12, "context": "So we instead use a Metropolis-Hastings independence sampler [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "Segment the current frame to super-pixels using edge detection [12] as in Section 3; 9.", "startOffset": 63, "endOffset": 67}, {"referenceID": 54, "context": "They are Hidden Markov Model (HMM), topic model LDA (TM), correlated topic model (CTM), topic model over absolute time (TM-AT), correlated topic model over absolute time (CTM-AT), topic model over relative time (TM-RT) and our causal topic model with only action-topics (CaTM-A) [57].", "startOffset": 279, "endOffset": 283}, {"referenceID": 50, "context": "We also evaluate HMM and our model CaTM using the popular features for action recognition, dense trajectories feature (DTF) [53], extracted only in RGB videos5, named as HMM-DTF and CaTM-A-DTF, CaTMAO-DTF.", "startOffset": 124, "endOffset": 128}, {"referenceID": 36, "context": "Segmentation: we consider a true positive if the overlap (union/intersection) between the detected and the groundtruth segments is more than a default threshold 40% as in [39].", "startOffset": 171, "endOffset": 175}], "year": 2016, "abstractText": "There is a large variation in the activities that humans perform in their everyday lives. We consider modeling these composite human activities which comprises multiple basic level actions in a completely unsupervised setting. Our model learns high-level co-occurrence and temporal relations between the actions. We consider the video as a sequence of short-term action clips, which contains human-words and object-words. An activity is about a set of action-topics and object-topics indicating which actions are present and which objects are interacting with. We then propose a new probabilistic model relating the words and the topics. It allows us to model long-range action relations that commonly exist in the composite activities, which is challenging in previous works. We apply our model to the unsupervised action segmentation and clustering, and to a novel application that detects forgotten actions, which we call action patching. For evaluation, we contribute a new challenging RGB-D activity video dataset recorded by the new Kinect v2, which contains several human daily activities as compositions of multiple actions interacting with different objects. Moreover, we develop a robotic system that watches people and reminds people by applying our action patching algorithm. Our robotic setup can be easily deployed on any assistive robot.", "creator": "LaTeX with hyperref package"}}}