{"id": "1603.05201", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units", "abstract": "Recently, convolutional cognitive networks (CNNs) been been particular and comes larger tools did question few problems instance machine ability the computer universal. In meant printing, cannot aim to maintain insight on that properties of spiral-shaped sensory networks, as make considered a generic methodology did improve the combination such being CNN architectures. Specifically, we 2008 examine existing CNN wheels and observe only intriguing fees that the devices before and stock textures different pairs (anyway. addresses. , filters same opposite complex ). Inspired by able astronomical, simply propose a preface, context seem should necrosis terms name deterministically ReLU (CRelu) are prevents detect its reconstruction funds which CNNs. We assimilate CRelu turning several provincial - taken - given - painter CNN architectures and demonstrate confidence recently their recognition impressive opening CIFAR - 48 / 135 different ImageNet morphometric with affected drudges parameters. Our earlier suggest likely better meaningful brought to smaller present CNNs instead lead even significant performance contribution when though mind required.", "histories": [["v1", "Wed, 16 Mar 2016 18:17:36 GMT  (2926kb,D)", "http://arxiv.org/abs/1603.05201v1", null], ["v2", "Tue, 19 Jul 2016 05:18:36 GMT  (2962kb,D)", "http://arxiv.org/abs/1603.05201v2", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["wenling shang", "kihyuk sohn", "diogo almeida", "honglak lee"], "accepted": true, "id": "1603.05201"}, "pdf": {"name": "1603.05201.pdf", "metadata": {"source": "META", "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units", "authors": ["Wenling Shang", "Kihyuk Sohn", "Diogo Almeida", "Honglak Lee"], "emails": ["SHANGW@UMICH.EDU", "KSOHN@NEC-LABS.COM", "DIOGO@ENLITIC.COM", "HONGLAK@EECS.UMICH.EDU"], "sections": [{"heading": "1. Introduction", "text": "In recent years, convolutional neural networks (CNNs) have achieved great success in many problems of machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2014). In addition, a wide range of techniques has been developed to enhance the performance or ease the training of CNNs (Lin et al., 2013; Zeiler & Fergus, 2013; Maas et al., 2013; Ioffe & Szegedy, 2015). Despite\nPlease contact Wenling Shang for questions or comments.\nFigure 1. Visualization of conv1 filters from AlexNet. Each filter and its pairing filter (wi and w\u0304i next to each other) appear surprisingly opposite (in phase) to each other. See text for details.\nthe great empirical success, fundamental understanding of CNNs is still lagging behind. Towards addressing this issue, this paper aims to provide an insight on the intrinsic property of convolutional neural networks.\nTo better comprehend the internal operations of CNNs, we investigate the well-known AlexNet (Krizhevsky et al., 2012) and thereafter discover that the network learns highly negatively-correlated pairs of filters for the first few convolution layers (Section 2.1). Following our preliminary findings, we hypothesize that the lower convolution layers of AlexNet learn redundant filters to extract both positive and negative phase information of an input signal (Section 2.1). Based on the premise of our conjecture, we propose a novel, simple yet effective activation scheme called Concatenated Rectified Linear Unit (CRelu). The proposed activation scheme preserves both positive and negative phase information while enforcing non-saturated nonlinearity. The unique nature of CRelu allows a mathematical characterization of convolution layers in terms of reconstruction property, which is an important indicator of how expressive and generalizable the corresponding CNN features are (Section 2.2).\nIn experiments, we evaluate the CNN models with CRelu in comparison to Relu on benchmark object recognition datasets, such as CIFAR-10/100 and ImageNet (Section 3). We demonstrate that simply replacing Relu with CRelu for\nar X\niv :1\n60 3.\n05 20\n1v 1\n[ cs\n.L G\n] 1\n6 M\nar 2\nthe lower convolution layers of an existing state-of-the-art CNN architecture yields a substantial improvement in classification performance. In addition, CRelu allows to attain notable parameter reduction without sacrificing classification performance when applied appropriately.\nWe analyze our experimental results from several viewpoints, such as regularization (Section 4.1) and invariant representation learning (Section 4.2). Retrospectively, we provide empirical evaluations on the reconstruction property of CRelu models; we also confirm that by integrating CRelu, the original \u201cpair-grouping\u201d phenomenon vanishes as expected (Section 4.3). Overall, our results suggest that by better understanding the nature of CNNs, we are able to reach their higher potential with a simple modification of the architecture."}, {"heading": "2. CRelu and Reconstruction Property", "text": ""}, {"heading": "2.1. Conjecture on Convolution Layers", "text": "During our initial exploration of classic CNNs trained on natural images such as AlexNet (Krizhevsky et al., 2012), we have noted a curious tendency of the first convolution layer filters: these filters tend to form \u201cpairs\u201d. More precisely, assuming unit length vector for each filter \u03c6i, we define a pairing filter of \u03c6i in the following way: \u03c6\u0304i = argmin\u03c6j \u3008\u03c6i, \u03c6j\u3009. We also define their correlation \u00b5\u03c6i = \u3008\u03c6i, \u03c6\u0304i\u3009.\nIn Figure 1, we show each normalized filter of the first convolution layer from AlexNet with its pairing filter. Interestingly, they appear surprisingly opposite to each other, i.e., for each filter, there does exist another filter that is almost on the opposite phase. Indeed, AlexNet employs the popular non-saturated activation function, rectified linear unit (Relu) (Nair & Hinton, 2010), which zeros out negative values and produces sparse activation. As a consequence, if both the positive phase and negative phase along a specific direction participate in representing the input space, the network then needs to learn two linearly dependent filters of both phases.\nTo systematically study the pairing phenomenon in higher\nlayers as well, we graph the histograms of \u00b5\u0304wi \u2019s for conv1conv5 filters from AlexNet in Figure 2. For comparison, we generate random Gaussian filters ri\u2019s of unit norm1 and plot the histograms of \u00b5\u0304ri \u2019s together. For conv1 layer, we observe that the distribution of \u00b5\u0304wi is negatively centered; by contrast, the mean of \u00b5\u0304ri is only slightly negative with a small standard deviation. Then the center of \u00b5\u0304wi shifts towards zero gradually when going deeper into the network. This implies that convolution filters of the lower layers tend to be paired up with one or a few others that represent their opposite phase, while the phenomenon gradually lessens as they go deeper.\nFollowing these observations, we hypothesize that despite Relu erasing negative linear responses, the first few convolution layers of a deep CNN manage to capture both negative and positive phase information through learning pairs or groups of negatively correlated filters. This conjecture implies that there exists a redundancy among the filters from the lower convolution layers.\nIn fact, for a very special class of deep architecture, the invariant scattering convolutional network (Bruna & Mallat, 2013), it is well-known that its set of convolution filters, which are wavelets, is overcomplete in order to be able to fully recover the original input signals. On the one hand, similar to Relu, each individual activation within the scattering network only preserves partial information of the input. On the other hand, different from Relu, scattering network activation preserves the energy information, i.e. keeping only the modulus of the convolution responses and erasing the phase information; Relu from a generic CNN, as a matter of fact, retains the phase information but eliminates the modulus information when the phase of a response is negative. In addition, while the wavelets for scattering networks are manually engineered, convolution filters from CNNs must be learned, which challenges any rigorous theoretical justification.\n1We sample each entry from standard normal distribution independently and normalize the resulting vector to have unit l2 norm.\nNow suppose we can leverage the pairing prior and design a simple scheme to explicitly allow both positive and negative activation, we will be able to alleviate the redundancy among convolution filters caused by Relu non-linearity and make more efficient use of the trainable parameters.\nWe propose a novel activation scheme, Concatenated Rectified Linear Units, or CRelu, with no additional hyperparameters. It simply makes an identical copy of the linear responses after convolution, negate them, concatenate both parts of activation, and then apply Relu altogether. More precisely, we denote Relu as [\u00b7]+ , max(\u00b7, 0), and define CRelu as the following:\nDefinition 2.1. Define the CRelu activation, denoted by \u03c1c : R \u2192 R2, as the following: \u2200x \u2208 R, \u03c1c(x) , ([x]+, [\u2212x]+).\nThe main rationale of our activation scheme is to allow a filter to be activated in both positive and negative direction while maintaining the same degree of non-saturated nonlinearity.\nAnother way to allow negative activation is to employ the broader class of non-saturated activation functions including Leaky Relu and its variants (Xu et al., 2015; Maas et al., 2013). Leaky Relu assigns a small slope to the negative part instead of completely dropping it. These activation functions share similar motivation with CRelu in the sense that they both tackle the two potential problems caused by the hard zero thresholding: (1) the weights of a filter will not be adjusted if it is never activated, and (2) truncating all negative information can potentially hamper the learning. However, CRelu is an activation scheme rather than a function, which fundamentally differentiates itself from Leaky Relu and its variants. In our version, we apply Relu after separating the negative and positive part to compose CRelu, but it is not the only feasible non-linearity. For example, CRelu can also be combined with other activation functions, such as Leaky Relu, to add more diversity to the architecture (possibly with additional tuning)."}, {"heading": "2.2. Reconstruction Property", "text": "A notable property of CRelu is its information preservation nature: CRelu conserves both negative and positive linear responses after convolution. A direct consequence of information preserving is the reconstruction power of the convolution layers equipped with CRelu.\nReconstruction property of a CNN implies that its features are representative of the input data. This aspect of CNNs has gained interest recently: Mahendran & Vedaldi (2015) invert CNN features back to the input under simple natural image priors; Zhao et al. (2015) stack autoencoders with reconstruction objective to build better classifiers. Bruna et al. (2013) theoretically investigate general conditions un-\nder which the max-pooling layer followed by Relu is injective and measure the stability of the inverting process by computing the Lipschitz lower bound. However, their bounds are non-trivial only when the number of filters significantly outnumbers the input dimension, which is not realistic.\nIn our case, since CRelu preserves all the information after convolution, it becomes more straightforward to analyze the reconstruction property. The rest of this section mathematically characterizes the reconstruction property of a single convolution layer followed by CRelu with or without max-pooling layer.\nLet x \u2208 RD be an input vector2 andwi \u2208 R`, i = 1, . . . ,K be convolution filters. We denote wji \u2208 RD the jth coordinate shift of the convolution filter wi with a fixed stride length of s, i.e., wji [(j\u2212 1)s+k] = wi[k] for k = 1, . . . , `, and 0\u2019s for the rest of entries in the vector. Here, we assume D \u2212 ` is divisible by s and thus there are n = D\u2212`s + 1 shifts for each wi. We define W to be the D \u00d7 nK matrix whose columns are the shifts wji , j = 1, . . . , n, for wi; the columns of W are divided into K blocks, with each block consisting of n shifts of a single filter. The conv + CRelu + max-pooling layer can be defined by first multiplying an input signal x by the matrix WT (conv), separating positive and negative phases then applying the Relu non-linearity (CRelu), and selecting the maximum value in each of the K block (max-pooling). The operation is denoted as fcnn : RD \u2192 R2K such that fcnn(x) , g ( WTx ) , where g , pool \u25e6 CRelu. Figure 3 illustrates an example of the problem setting.\nFirst, we analyze for the case of convolution followed by CRelu without max-pooling (or equivalently, with maxpooling and n = 1).\nProposition 2.1. Let x \u2208 RD and x = x\u2032 + (x \u2212 x\u2032), where x\u2032 \u2208 range(W ) and x\u2212x\u2032 \u2208 ker(W ). Then we can\n2For the sake of clarity, we assume the input signals are vectors (1D) rather than images (2D); however, similar analysis can be done for 2D case.\nreconstruct x\u2032 with fcnn(x).\nProposition A.1 (see Section A in the supplementary materials for proof) states that the part of an input signal spanned by the shifts of the filters is well preserved.\nNext, we add max-pooling into the picture. To reach a non-trivial bound, we put a constraint on the input space V: \u2200x \u2208 V , there exists {cji} j=1,\u00b7\u00b7\u00b7 ,n i=1,\u00b7\u00b7\u00b7 ,K such that\nx = K\u2211 i=1 n\u2211 j=1 cjiw j i , where n\u2211 j=1 1{cji > 0} \u2264 1, \u2200i. (1)\nIn other words, we assume that an input x is a linear combination of the shifted convolution filters {wji } j=1,\u00b7\u00b7\u00b7 ,n i=1,\u00b7\u00b7\u00b7 ,K such that over a single max-pooling region, only one of the shifts participates: \u2211n j=1 1{c j i > 0} \u2264 1: a slight translation of an object or viewpoint change does not alter the nature of a natural image, which is how max-pooling generates shift invariant features by taking away some fine-scaled locality information.\nNext, we denote the matrix consisting of the shifts whose corresponding cji \u2019s are non-zero by Wx , and the vector consisting of the non-zero cji \u2019s by cx, i.e. Wxcx = x. Also, we denote the matrix consisting of the shifts whose activation is positive and selected after max-pooling operation by W\u0302+x , negative by W\u0302 \u2212 x . Let W\u0302x , [ W\u0302+x , W\u0302 \u2212 x ] . Finally,\nwe give notation, W\u0303x, to the matrix consisting of a subset of W\u0302x, such that the ith column comes from W\u0302+x if c j i \u2265 0 or from W\u0302\u2212x if otherwise. Now we obtain the following theorem (see Section A in the supplementary materials for proof.) that characterizes the reconstruction property of a convolution layer followed by CRelu and max-pooling.\nTheorem 2.2. Let x \u2208 V and satisfy the assumption from Equation (1). Then we can obtain x\u2032, the reconstruction of x using fcnn(x) such that\n\u2016x\u2212 x\u2032\u20162 \u2016x\u20162 \u2264\n\u221a \u03bb\u0303max \u2212 \u03bbmin\n\u03bbmin ,\nwhere \u03bbmin and \u03bb\u0303max are square of the minimum and maximum singular values of Wx and W\u0303x respectively.\nWe refer to the term \u2016x\u2212x \u2032\u20162\n\u2016x\u20162 as the reconstruction ratio in later sections. We will revisit this subject after the experiment section (Section 4.3)."}, {"heading": "3. Benchmark Results", "text": "We evaluate the effectiveness of our proposed CRelu activation scheme on three benchmark datasets: CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and ImageNet (Deng et al.,\n2009). To directly assess the impact of CRelu, we employ existing CNN architectures with Relu that have already shown a good recognition performance and demonstrate improved performance on top by replacing Relu into CRelu. Note that the models with CRelu activation doesn\u2019t need significant hyperparameter tuning from the baseline Relu model, and in most of our experiments, we only tune dropout rate while other hyperparameters (e.g., learning rate, mini-batch size) remain the same. We put details of network architecture and training procedure in Section E of the supplementary materials."}, {"heading": "3.1. CIFAR-10 and CIFAR-100", "text": "The CIFAR-10 and 100 dataset (Krizhevsky, 2009) consists of 50, 000 training and 10, 000 testing examples of 32\u00d7 32 images evenly drawn from 10 and 100 classes, respectively. We subtract the mean and divide by the standard deviation for preprocessing, and for data augmentation, we only use random horizontal flip.\nWe use the ConvPool-CNN-C model (Springenberg et al., 2014) as our baseline model, which is composed of convolution and pooling followed by Relu without fullyconnected layers. This baseline model serves our purpose well since it has clearly outlined network architecture only with convolution, pooling, and Relu. It has also shown competitive recognition performance to state-of-the-art using a fairly small number of model parameters.\nFirst, we integrate CRelu into the baseline model by simply replacing Relu while keeping the number of convolution filters the same. This doubles the number of output channels at each convolution layer and the total number of model parameters is doubled. To see whether the performance gain comes from the increased model capacity, we conduct additional experiments with the baseline model while doubling the number of filters and the CRelu model while halving the number of filters.\nSince the dataset doesn\u2019t provide pre-defined validation set, we conduct two different cross-validation schemes:\n1. \u201cSingle\u201d: we hold out a small subset of training set for initial training and retrain the network from scratch using whole training set until we reach at the same loss on a hold out set (Goodfellow et al., 2013). For this case, we also report the corresponding train error rates. 2. 10-folds: we divide training set into 10 folds and do validation on each of 10 folds while training the networks on the rest of 9 folds. The mean error rate of each single network (\u201cAverage\u201d) and the error rate with model averaging of 10 networks (\u201cVote\u201d) are evaluated on the test set.\nThe recognition results are summarized in Table 1. On\nCIFAR-10, we observe significant improvement with the proposed CRelu activation over Relu. Especially, CRelu models consistently improve over Relu models with the same number of neurons (or activations) while reducing the number of model parameters by half (e.g., CRelu + half model and the baseline model have the same number of neurons while the number of model parameters are 0.7M and 1.4M, respectively). On CIFAR-100, the models with larger capacity generally improve the performance for both activation schemes. Nevertheless, we can still find that there is a clear benefit of using CRelu activation that shows significant performance gain when it is compared to the model with the same number of neurons, i.e., half the number of model parameters. One possible explanation for the benefit of using CRelu is its regularization effect, as can be confirmed in Table 1 that the CRelu models showed significantly lower gap between train and test set error rates than those of the baseline Relu models.\nExperiments on Deeper Networks. Motivated by the importance of model capacity from experiments on CIFAR-100, we conduct experiments with very deep CNN that has a similar network architecture to the VGG network (Simonyan & Zisserman, 2014). Specifically, we follow the model architecture and training procedure described by Zagoruyko (2015). Besides the convolution and pooling layers, this network also contains batch normalization (Ioffe & Szegedy, 2015) and fully connected layers. Due to the sophistication of the network composition which may introduce complicated interaction with CRelu, we only integrate CRelu into the first few layers. Similarly to the previous experiments, we subtract the mean and divide by the standard deviation for preprocessing, and for data augmentation, we use random horizontal flip as well as random shifts.\nIn this experiment, we gradually replace Relu after the first, third, and the fifth convolution layers3 with CRelu while halving the number of filters, resulting in a reduced number of model parameters. We report the test set error rates\n3Integrating CRelu into the second or fourth layer before max-pooling layers does not improve the performance.\nusing the same cross-validation schemes as in the previous experiments. As shown by Table 2, there is substantial performance lift in both CIFAR-10 and CIFAR-100 datasets by replacing Relu with CRelu for the first few layers after convolution. Overall, the proposed CRelu activation improves the performance of the state-of-the-art VGG network significantly, achieving highly competitive error rates to other state-of-the-art methods, as summarized in Table 3."}, {"heading": "3.2. ImageNet", "text": "To assess the impact of CRelu on large scale dataset, we perform experiments on ImageNet dataset (Deng et al., 2009), which contains about 1.3M images for training and 50, 000 for validation from 1, 000 object categories. For preprocessing, we subtract the mean and divide by the stan-\ndard deviation for each input channel, and follow the data augmentation as described in (Krizhevsky et al., 2012).\nWe take the All-CNN-B model (Springenberg et al., 2014) as our baseline model. The network architecture of AllCNN-B is similar to that of AlexNet (Krizhevsky et al., 2012), where the max-pooling layer is replaced by convolution with the same kernel size and stride, the fully connected layer is replaced by 1 \u00d7 1 convolution layers followed by average pooling, and the local response normalization layers are discarded. In sum, the layers other than convolution layers are replaced or discarded and finally the network consists of convolution layers only. We choose\nthis model since it reduces the potential complication introduced by CRelu interacting with other types of layers, such as batch normalization or fully connected layers.\nSimilarly to the previous experiments, we gradually integrate more convolution layers with CRelu (e.g., conv1\u2013 4, conv1\u20137, conv1\u20139), while keeping the same number of filters. These models contain more parameters than the baseline model. We also evaluate two models where one replaces all Relu layers into CRelu and the other conv1,conv4 and conv7 only, and both reduce the number of convolution layers before CRelu by half. Hence, these models contain fewer parameters than the baseline model. The network architectures and the training details are in Section D of supplementary material.\nThe summarized results are provided in Table 4. We report the top-1 and top-5 error rates with center crop only and by averaging scores over 10 patches from the center crop and four corners and with horizontal flip (Krizhevsky et al., 2012). Interestingly, integrating CRelu to conv1-4 achieves the best results, whereas going deeper with higher model capacity does not further benefit the classification performance. In fact, this parallels with our initial observation on AlexNet (Figure 2 in Section 2.1)\u2014there exists less \u201cpairing\u201d in the deeper convolution layers and thus there is not much gain by decomposing the phase in the deeper layers. Another interesting observation, which we will discuss further in Section 4.2, is that the model integrating CRelu into conv1, conv4 and conv7 layers also achieve highly competitive recognition results with even fewer parameters than the baseline model. In sum, we believe that such a notable gain over the baseline model by simply modifying the activation scheme is a pleasantly surprising result.4\nWe also compare our best models with AlexNet and other variants in Table 5. Even though reducing the number of\n4We note that Springenberg et al. (2014) reported slightly better result (41.2% top-1 error rate with center crop only) than our replication result, but still the improvement is significant.\nparameters is not our primary goal, it is worth noting that our model with only 4.6M parameters (CRelu + all) outperforms FastFood-32-AD (FriedNet) (Yang et al., 2015) and Pruned AlexNet (PrunedNet) (Han et al., 2015), whose designs directly aim at parameter reduction. Therefore, besides the performance boost, another significance of CRelu activation scheme is in designing more parameter and computationally efficient deep neural networks."}, {"heading": "4. Discussion", "text": "In this section, we discuss qualitative properties of CRelu activation scheme in several viewpoints, such as regularization of the network and learning invariant representation."}, {"heading": "4.1. A View from Regularization", "text": "In general, a model with more trainable parameters is more prone to overfitting. However, somewhat counterintuitively, for the all-conv CIFAR experiments, the models with CRelu display much less overfitting issue comparing to the baseline models with Relu, even though it has twice more parameters (Table 1). We contemplate that keeping both positive and negative phase information makes the training more challenging, and such effect has been leveraged to better regularize deep networks, especially when working on small datasets.\nNot only from empirical evidence, but also, we can describe the regularization effect by deriving a bound for the Rademacher complexity of the CRelu layer followed by linear transformation as follows:\nTheorem 4.1. Let G be the class of real functions Rdin \u2192 R with input dimension F , that is, G = [F ]dinj=1. Let H be a linear transformation function from R2din to R, parametrized by W , where \u2016W\u20162 \u2264 B. Then, we have\nR\u0302L(H \u25e6 \u03c1c \u25e6 G) \u2264 \u221a dinBR\u0302L(F).\nThe proof is in Section B of the supplementary materials. Theorem 4.1 says that the complexity bound of CRelu + linear transformation is the same as that of Relu + linear\ntransformation, which is proved by Wan et al. (2013). In other words, although the number of model parameters are doubled by CRelu, the model complexity does not necessarily increase."}, {"heading": "4.2. Towards Learning Invariant Features", "text": "We measure the invariance scores using the evaluation metrics from (Goodfellow et al., 2009) and draw another comparison between the CRelu models and the Relu models. For a fair evaluation, we compare all 7 conv layers from allconv Relu model with those from all-conv CRelu model trained on CIFAR-10/100; in the case of ImageNet experiments, we choose the model where CRelu replaces Relu for the first 7 conv layers and compare the invariance scores with the first 7 conv layers from the baseline Relu model. Section C in the supplementary materials details how the invariance scores are measured.\nFigure 4 plots the invariance scores for networks trained on CIFAR-10, CIFAR-100, and ImageNet respectively. The invariance scores of CRelu models are consistently higher than those of Relu models. For CIFAR-10 and CIFAR-100, there is a big increase between conv2 and conv3 then again between conv4 and conv6, which are due to max-pooling layer extracting shift invariance features. We also observe that although as a general trend, the invariance scores increase while going deeper into the networks\u2013consistent with the observations from (Goodfellow et al., 2009), rather unexpectedly, the progression is not monotonic. This interesting observation suggests the potentially diverse functionality of different layers in the CNN, which would be worthwhile for future investigation.\nIn particularly, the scores of ImageNet Relu model attain local maximum at conv1, conv4 and conv7 layers. It inspires us to design the architecture where CRelu are placed after conv1, 4, and 7 layers to encourage invariance representations while halving the number of filters to limit model capacity. Surprisingly yet as hoped, this architecture achieves the best top1 and top5 recognition results when averaging scores from 10 patches."}, {"heading": "4.3. Revisiting the Reconstruction Property", "text": "In Section 2.1, we observe that lower layer convolution filters from Relu models form negatively-correlated pairs. Does the pairing phenomenon still exist for CRelu models? We take our best CRelu model trained on ImageNet (where the first 4 conv layers are integrated with CRelu) and repeat the histogram experiments to generate Figure 5. In clear contrast to Figure 2, the distributions of \u00b5\u0304wi from CRelu model well align with the distributions of \u00b5\u0304ri from random Gaussian filters. In other words, each lower layer convolution filter now uniquely spans its own direction without a negatively correlated pairing filter, while CRelu implicitly plays the role of \u201cpair-grouping\u201d.\nIn Section 2.2, we mathematically characterize the reconstruction property of convolution layers with CRelu. Proposition A.1 claims that the part of input spanned by the shifts of the filters can be fully recovered. ImageNet contains a large number of training images from a wide variety of categories; the convolution filters learned from ImageNet are thus expected to be diverse enough to describe the domain of natural images. Hence, to empirically verify the result from Proposition A.1, we can directly invert features from our best CRelu model trained on ImageNet via the linear reconstruction algorithm described in the proof of Proposition A.1 (Algorithm 1). Figure 6 shows an image from the validation set along with its reconstructions using conv1-conv4 features (see Section F in\nthe supplementary materials for more reconstruction examples). Unlike other reconstruction methods (Dosovitskiy & Brox, 2015; Mahendran & Vedaldi, 2015), our simple linear reconstruction algorithm does not involve any additional learning. Nevertheless, it still produces reasonable reconstructions, which qualitatively supports our theoretical claim in Proposition A.1.\nTheorem 2.2 characterizes the reconstruction property when max-pooling is added after CRelu. As an example, we study the all-conv CRelu (half) models used for CIFAR-10/100 experiments. In this model, conv2 and conv5 layers are followed by max-pooling. CIFAR images are much less diverse than those from ImageNet. Instead of directly inverting features all the way back to the original images, we empirically calculate the reconstruction ratio, \u2016x \u2212 x\u2032\u20162/\u2016x\u20162. We sample testing examples, extract pooled features after conv2(conv5) layer and reconstruct features from the previous layer via Algorithm 2. To compare, we perform the same procedures on random convolution filters5. Essentially, convolution imposes structured zeros to the random W\u0303x; there has not been published results on random subspace projection with such structured zeros. In a simplified setting without structured zeros, i.e. no convolution, it is straightforward to show that the ex-\npected reconstruction ratio is \u221a\nD\u2212K D (see Theorem A.6\nin the supplementary materials), where, in our case, D = 48(96)\u00d75\u00d75 andK = 48(96) for conv2(conv5) layer. Table 6 compares between the empirical mean of reconstruction ratios using learned filters and random filters: random filters only recover 1% of the original input, whereas the learned filters span more of the input domain."}, {"heading": "5. Conclusion", "text": "We propose a new activation scheme, CRelu, which conserves both positive and negative linear responses after convolution so that each filter can efficiently represent its unique direction. Our work demonstrates CRelu improves\n5Each entry is sampled from standard normal distribution.\ndeep networks with classification objective. Since CRelu preserves the available information from input yet maintaining the non-saturated non-linearity at the same time, it can potentially benefit more complicated machine learning tasks such as structured output predication and image generation. Another venue for future research involves engaging CRelu to the abundant set of existing deep neural network techniques and frameworks.We hope to investigate along these directions in the near future."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Erik Brinkman, Harry Altman and Mark Rudelson for their helpful comments. We also thank Yuting Zhang and Anna Gilbert for discussions during the preliminary stage of this work. We acknowledge Technicolor Research for providing resources and support and NVIDIA for the donation of GPUs."}, {"heading": "A. Reconstruction Property Proofs", "text": "Proposition A.1. Let x \u2208 RD, x = x\u2032 + (x \u2212 x\u2032), where x\u2032 \u2208 range(W ) and x \u2212 x\u2032 \u2208 ker(W ). Then we can reconstruct x\u2032 with fcnn(x).\nProof. We show x\u2032 can be reconstructed from fcnn(x) by providing a simple linear reconstruction algorithm described by Algorithm 1. First, apply the inverse function of CRelu on fcnn(x): z = \u03c1\u22121c (fcnn(x)). Then, compute the Moore Penrose pseudoinverse of WT , denote by (WT )+. By definition Q = (WT )+WT is the orthogonal projector onto range(W ), therefore we can obtain x\u2032 = (WT )+z.\nDefinition A.1. A frame is a set of elements of a vector space V , {\u03c6k}k=1,\u00b7\u00b7\u00b7 ,K , which satisfies the frame condition: there exist two real numbers C1 and C2, the frame bounds, such that 0 < C1 \u2264 C2 <\u221e, and \u2200v \u2208 V\nC1\u2016v\u201622 \u2264 K\u2211 k=1 |\u3008v, \u03c6i\u3009|2 \u2264 C2\u2016v\u201622.\n(Christensen, 2003)\nProposition A.2. Let {\u03c6k}k=1,...,K be a sequence in V , then {\u03c6k} is a frame for span{\u03c6k}. Hence, {\u03c6k} is a frame for V if and only if V = span{\u03c6k}6. (Christensen, 2003)\n6There exist infinite spanning sets that are not frames, but we will not be concerned with those here since we only deal with finite dimensional vector spaces.\nDefinition A.2. Consider now V equipped with a frame {\u03c6k}k=1,...,K . The Analysis Operator, T : V \u2192 RK , is defined by T v = {\u3008v, \u03c6k\u3009}k=1,...,K . The Synthesis Operator, T \u2217 : RK \u2192 V , is defined by T \u2217{ck}k=1,...,K =\u2211K k=1 ck\u03c6k, which is the adjoint of the Analysis Operator. The Frame Operator, S : V \u2192 V , is defined to be the composition of T with its adjoint:\nSv = T \u2217T v.\nThe Frame Operator is always invertible. (Christensen, 2003) Theorem A.3. The optimal lower frame bound C1 is the smallest eigenvalue of S; the optimal upper frame bound C2 is the largest eigenvalue of S. (Christensen, 2003)\nWe would also like to investigate the matrix representation of the operators T , T \u2217 and S. Consider V , a subspace of RD, equipped with a frame {\u03c6k}k=1,\u00b7\u00b7\u00b7 ,K . Let U \u2208 RD\u00d7d be a matrix whose column vectors form an orthonormal basis for V (here d is the dimension of V ). Choosing U as the basis for V and choosing the standard basis {ek}k=1,\u00b7\u00b7\u00b7 ,K as the basis for RK , the matrix representation of T is T\u0303 = WTU , whereW is the matrix whose column vectors are {\u03c6Tk }k=1,\u00b7\u00b7\u00b7 ,K . Its transpose, T\u0303 \u2217, is the matrix representation for T \u2217; the matrix representation for S is S\u0303 = T\u0303 \u2217T\u0303 . Lemma A.4. Let x \u2208 RD and W an D-by-K matrix. If x \u2208 range(W ), then \u03c3min\u2016x\u20162 \u2264 \u2016WTx\u20162 \u2264 \u03c3max\u2016x\u20162, where \u03c3min and \u03c3max are the least and largest singular value of V respectively.\nProof. By Proposition A.2, the columns in W form a frame for range(W ). Let U be an orthonormal basis for range(W ). Then the matrix representation under U for the Analysis Operator, T , is T\u0303 = WTU , and the corresponding representation for x under U is UTx. Now, by Theorem A.3, we have:\n\u03bbmin\u2016x\u201622 \u2264 \u2016T\u0303 x\u201622 = \u2016WTUUTx\u201622 = \u2016WTx\u201622,\nwhere \u03bb1 is the least eigenvalue of S\u0303. Therefore, we have \u03c3min\u2016x\u20162 \u2264 \u2016WTx\u20162, where \u03c3min is the least singular value of W . Lastly, by the definition of operatorinduced matrix norm, we have the upper bound \u2016WTx\u20162 \u2264 \u03c3max\u2016x\u20162\nTheorem A.5. 2.2 Let x \u2208 V and satisfy the assumption from Equation (1). Then we can obtain x\u2032, the reconstruction of x using fcnn(x) such that\n\u2016x\u2212 x\u2032\u20162 \u2016x\u20162 \u2264\n\u221a \u03bb\u0303max \u2212 \u03bbmin\n\u03bbmin ,\nwhere \u03bbmin and \u03bb\u0303max are square of the minimum and maximum singular values of Wx and W\u0303x respectively.\nProof. We use similar method to reconstruct as described by Algorithm 2: first reverse the CRelu activation and obtain z = \u03c1\u22121c (fcnn(x)); then compute the Moore Penrose pseudoinverse of W\u0302Tx , denote by (W\u0302 T x )\n+; finally, obtain x\u2032 = (W\u0302Tx ) +z, since by definition, Q = (W\u0302Tx ) +W\u0302Tx is the orthogonal projector onto range(W\u0302x). To proceed the proof, we denote the subset of z which matches the corresponding activation of the filters from W\u0303x by z\u0303, compute the Morre Penrose pseudoinverse of W\u0303x and obtain x\u0303 = (W\u0303Tx )\n+z\u0303. Note that since range(W\u0303x) is a subspace of range(W\u0302x), therefore, the reconstruction x\u2032 will always be equal or better than x\u0303, i.e. \u2016x \u2212 x\u2032\u20162 \u2264 \u2016x \u2212 x\u0303\u20162. From Lemma A.4, the nature of max-pooling and the assumption on x (Equation 1), we derive the following inequality\n\u03bbmin\u2016x\u201622 \u2264 \u2016WTx x\u20162 \u2264 \u2016W\u0303Tx x\u201622 = \u2016W\u0303Tx x\u0303\u201622 \u2264 \u03bb\u0303max\u2016x\u0303\u201622,\nwhere \u03bbmin and \u03bb\u0303max are square of the minimum and maximum singular values of Wx and W\u0303x respectively.\nBecause x\u0303 is the orthogonal projection of x on to range(W\u0303x), thus \u2016x\u201622 = \u2016x\u0303\u201622 +\u2016x\u2212 x\u0303\u201622. Now substitute \u2016x\u201622 with \u2016x\u0303\u201622 + \u2016x\u2212 x\u0303\u201622, we have:\n\u03bbmin(\u2016x\u0303\u201622 + \u2016x\u2212 x\u0303\u201622) \u2264 \u03bb\u0303max\u2016x\u0303\u201622\n\u2016x\u2212 x\u0303\u201622 \u2264 \u03bb\u0303max \u2212 \u03bbmin\n\u03bb\u0303min \u2016x\u0303\u201622\n\u2016x\u2212 x\u2032\u201622 \u2264 \u03bb\u0303max \u2212 \u03bbmin\n\u03bbmin \u2016x\u201622\n\u2016x\u2212 x\u2032\u20162 \u2264\n\u221a \u03bb\u0303max \u2212 \u03bbmin\n\u03bbmin \u2016x\u20162\n\u2016x\u2212 x\u2032\u20162 \u2016x\u20162 \u2264\n\u221a \u03bb\u0303max \u2212 \u03bbmin\n\u03bbmin .\nTheorem A.6. Let x \u2208 RD, and let xs \u2208 RD be its projection onto a random subspace of dimension D2, then\nE [ \u2016xs\u20162 \u2016x\u20162 ] = \u221a Ds D\nProof. Without loss of generality, let \u2016x\u20162 = 1. Projecting a fixed x onto a random subspace of dimension Ds is equivalent of projecting a random unit-norm vector z = (z1, z2, \u00b7 \u00b7 \u00b7 , zD)T onto a fixed subspace of dimension Ds thanks to the rotational invariance of inner product. Without loss of generality, assume the fixed subspace here is spanned by the first Ds standard basis covering the first D2 coordinates of z. Then the resulting projection is zs = (z1, z2, \u00b7 \u00b7 \u00b7 , zDs , 0, \u00b7 \u00b7 \u00b7 , 0).\nBecause z is unit norm, we have\nE [ \u2016z\u201622 ] = E [ D\u2211 i=1 z2i ] = 1.\nBecause each entry of z, zi, is identically distributed, we have\nE [ \u2016zs\u201622 ] = E [ Ds\u2211 i=1 z2i ] = Ds D .\nTogether we have\nE [ \u2016xs\u20162 \u2016x\u20162 ] = E [ \u2016zs\u20162 \u2016z\u20162 ] = \u221a Ds D ."}, {"heading": "B. Proof of Model Complexity Bound", "text": "Definition B.1. (Rademacher Complexity) For a sample S = {x1, \u00b7 \u00b7 \u00b7 , xL} generated by a distribution D on set X and a real-valued function classF in domainX , the empirical Rademacher complexity of F is the random variable:\nR\u0302L(F) = E\u03c3 \u2211 f\u2208F | 2 L \u03c3if(xi)| \u2223\u2223\u2223\u2223x1, \u00b7 \u00b7 \u00b7 , xL  ,\nwhere \u03c3i\u2019s are independent uniform {\u00b11}-valued (Rademacher) random variables. The Rademacher complexity of F is RL(F) = ES [ R\u0302L(F) ] Lemma B.1. (Composition Lemma) Assume \u03c1 : R\u2192 R is a L\u03c1-Lipschitz continuous function, i.e. , |\u03c1(x) \u2212 \u03c1(y)| \u2264 L\u03c1|x\u2212 y|. Then R\u0302L(\u03c1 \u25e6 F) = L\u03c1R\u0302L(F).\nProposition B.2. (Network Layer Bound) Let G be the class of real functions Rdin \u2192 R with input dimension F , that is, G = [F ]dinj=1 and H is a linear transform function parametrized by W with \u2016W\u20162 \u2264 B, then R\u0302L(H \u25e6 G) \u2264\u221a dinBR\u0302L(F). (Wan et al., 2013)\nCorollary B.3. By Lemma B.1, Proposition B.2, and the fact that Relu is 1-Lipschitz, we know that R\u0302L(Relu\u25e6G) = R\u0302L(G) and that R\u0302L(H \u25e6 Relu \u25e6 G) \u2264 \u221a dinBR\u0302L(F).\nTheorem B.4. 4.1 Let G be the class of real functions Rdin \u2192 R with input dimension F , that is, G = [F ]dinj=1. Let H be a linear transform function from R2din to R, parametrized by W , where \u2016W\u20162 \u2264 B. Then R\u0302L(H \u25e6 \u03c1c \u25e6 G) \u2264 \u221a dinBR\u0302L(F).\nRecall from Definition 2.1, \u03c1c is the CRelu formulation.\nProof.\nR\u0302L(H \u25e6 \u03c1c \u25e6 G) = E\u03c3 [ sup\nh\u2208H,g\u2208G | 2 L L\u2211 i=1 \u03c3ih \u25e6 \u03c1c \u25e6 g(xi)| ] (S1)\n= E\u03c3\n[ sup\n\u2016W\u2016\u2264B,g\u2208G |\u3008W, 2 L L\u2211 i=1 \u03c3i\u03c1c \u25e6 g(xi)\u3009|\n] (S2)\n\u2264 BE\u03c3 sup f\u2208F \u2016 [ 2 L L\u2211 i=1 \u03c3ji \u03c1c \u25e6 f j(xi) ]din j=1 \u20162  (S3) = BE\u03c3 sup f\u2208F \u2016 [ 2 L L\u2211 i=1 \u03c3ji f j(xi) ]din j=1 \u20162\n (S4) = B \u221a dinE\u03c3 [ sup f\u2208F | 2 L L\u2211 i=1 \u03c3if(xi)| ] (S5)\n= \u221a dinBR\u0302L(F). (S6)\nFrom (S1) to (S2), use the definition of linear transformation and inner product. From (S2) to (S3), use CauchySchwarz inequality and the assumption that \u2016W\u20162 \u2264 B. From (S3) to (S4), use the definition of CRelu and l2 norm. From (S4) to (S5), use the definition of l2 norm and sup operator. From (S5) to (S6), use the definition of R\u0302L\nWe see that CRelu followed by linear transformation reaches the same Rademacher complexity bound as Relu followed by linear transformation with the same input dimension.\nC. Invariance Score We use consistent terminology employed by Goodfellow et al. (2009) to illustrate the calculation of the invariance scores.\nFor CIFAR-10/100, we utilize all 50k testing images to calculate the invariance scores; for ImageNet, we take the center crop from 5k randomly sampled validation images\nFor each individual filter, we calculate its own firing threshold, such that it is fired one percent of the time, i.e. the global firing rate is 0.01. For Relu models, we zero out all the negative negative responses when calculating the threshold; for CRelu models, we take the absolute value.\nTo build the set of semantically similar stimuli for each testing image x, we apply horizontal flip, 15 degree rotation and translation. For CIFAR-10/100, translation is composed of horizontal/vertical shifts by 3 pixels; for ImageNet, translation is composed of cropping from the 4 corners.\nBecause our setup is convolutional, we consider a filter to\nbe fired only if both the transformed stimulus and the original testing example fire the same convolution filter at the same spatial location.\nAt the end, for each convolution layer, we average the invariance scores of all the filters at this layer to form the final score.\nD. Implementation Details on ImageNet Models\nThe networks from Table S7, S8, S9,and S10, where the number of convolution filters after CRelu are kept the same, are optimized using SGD with mini-batch size of 64 examples and fixed momentum 0.9. The learning rate and weight decay is adapted using the following schedule: epoch 1-10, 1e\u22122 and 5e\u22124; epoch 11-20, 1e\u22123 and 5e\u22124; epoch 21-25, 1e\u22124 and 5e\u22124; epoch 26-30, 5e\u22125 and 0; epoch 31-35, 1e\u22125 and 0; epoch 36-40, 5e\u22126 and 0; epoch 41-45, 1e\u22126 and 0.\nThe networks from Table S11 and S12, where the number of convolution filters after CRelu are reduced by half, are optimized using Adam with an initial learning rate 0.0002 and mini-batch size of 64 examples for 100 epochs."}, {"heading": "E. Details of Network Architecture", "text": "Baseline Baseline (double) Layer kernel, stride, padding activation kernel, stride, padding activation conv1 3\u00d73\u00d73\u00d796, 1, 1 Relu 3\u00d73\u00d73\u00d7192, 1, 1 Relu conv2 3\u00d73\u00d796\u00d796, 1, 1 Relu 3\u00d73\u00d7192\u00d7192, 1, 1 Relu pool1 3\u00d73, 2, 0 max 3\u00d73, 2, 0 max conv3 3\u00d73\u00d796\u00d7192, 1, 1 Relu 3\u00d73\u00d7192\u00d7384, 1, 1 Relu conv4 3\u00d73\u00d7192\u00d7192, 1, 1 Relu 3\u00d73\u00d7384\u00d7384, 1, 1 Relu conv5 3\u00d73\u00d7192\u00d7192, 1, 1 Relu 3\u00d73\u00d7384\u00d7384, 1, 1 Relu pool2 3\u00d73, 2, 0 max 3\u00d73, 2, 0 max conv6 3\u00d73\u00d7192\u00d7192, 1, 1 Relu 3\u00d73\u00d7384\u00d7384, 1, 1 Relu conv7 1\u00d71\u00d7192\u00d7192, 1, 1 Relu 1\u00d71\u00d7384\u00d7384, 1, 1 Relu conv8 1\u00d71\u00d7192\u00d710/100, 1, 0 Relu 1\u00d71\u00d7384\u00d710/100, 1, 0 Relu pool3 10\u00d710 (100 for CIFAR-100) avg 10\u00d710 (100 for CIFAR-100) avg\nTable S1. (Left) Baseline and (right) baseline (double) models used for CIFAR-10/100 experiment. \u201cavg\u201d refers average pooling.\nCRelu CRelu (half) Layer kernel, stride, padding activation kernel, stride, padding activation conv1 3\u00d73\u00d73\u00d796, 1, 1 CRelu 3\u00d73\u00d73\u00d748, 1, 1 CRelu conv2 3\u00d73\u00d7192\u00d796, 1, 1 CRelu 3\u00d73\u00d796\u00d748, 1, 1 CRelu pool1 3\u00d73, 2, 0 max 3\u00d73, 2, 0 max conv3 3\u00d73\u00d7192\u00d7192, 1, 1 CRelu 3\u00d73\u00d796\u00d748, 1, 1 CRelu conv4 3\u00d73\u00d7384\u00d7192, 1, 1 CRelu 3\u00d73\u00d796\u00d796, 1, 1 CRelu conv5 3\u00d73\u00d7384\u00d7192, 1, 1 CRelu 3\u00d73\u00d7192\u00d796, 1, 1 CRelu pool2 3\u00d73, 2, 0 max 3\u00d73, 2, 0 max conv6 3\u00d73\u00d7384\u00d7192, 1, 1 CRelu 3\u00d73\u00d7192\u00d796, 1, 1 CRelu conv7 1\u00d71\u00d7384\u00d7192, 1, 1 CRelu 1\u00d71\u00d7192\u00d796, 1, 1 CRelu conv8 1\u00d71\u00d7384\u00d710/100, 1, 0 Relu 1\u00d71\u00d7192\u00d710/100, 1, 0 Relu pool3 10\u00d710 (100 for CIFAR-100) avg 10\u00d710 (100 for CIFAR-100) avg\nTable S2. (Left) CRelu and (right) CRelu (half) models used for CIFAR-10/100 experiment.\nLayer kernel, stride, padding activation conv1 3\u00d73\u00d73\u00d764, 1, 1 BN+Relu dropout with ratio 0.3 conv2 3\u00d73\u00d764\u00d764, 1, 1 BN+Relu pool1 2\u00d72, 2, 0 conv3 3\u00d73\u00d764\u00d7128, 1, 1 BN+Relu dropout with ratio 0.4 conv4 3\u00d73\u00d7128\u00d7128, 1, 1 BN+Relu pool2 2\u00d72, 2, 0 conv5 3\u00d73\u00d7128\u00d7256, 1, 1 BN+Relu dropout with ratio 0.4 conv6 3\u00d73\u00d7256\u00d7256, 1, 1 BN+Relu dropout with ratio 0.4 conv7 3\u00d73\u00d7256\u00d7256, 1, 1 BN+Relu pool3 2\u00d72, 2, 0 conv8 3\u00d73\u00d7256\u00d7512, 1, 1 BN+Relu dropout with ratio 0.4 conv9 3\u00d73\u00d7512\u00d7512, 1, 1 BN+Relu\ndropout with ratio 0.4 conv10 3\u00d73\u00d7512\u00d7512, 1, 1 BN+Relu pool4 2\u00d72, 2, 0 conv11 3\u00d73\u00d7512\u00d7512, 1, 1 BN+Relu dropout with ratio 0.4 conv12 3\u00d73\u00d7512\u00d7512, 1, 1 BN+Relu dropout with ratio 0.4 conv13 3\u00d73\u00d7512\u00d7512, 1, 1 BN+Relu pool5 2\u00d72, 2, 0\ndropout with ratio 0.5 fc14 512\u00d7512 BN+Relu dropout with ratio 0.5 fc15 512\u00d710/100\nTable S3. VGG for CIFAR-10/100\nLayer kernel, stride, padding activation conv1 3\u00d73\u00d73\u00d732, 1, 1 CRelu dropout with ratio 0.1 conv2 \u00b7 \u00b7 \u00b7\nTable S4. VGG + (conv1) for CIFAR-10/100\nLayer kernel, stride, padding activation conv1 3\u00d73\u00d73\u00d732, 1, 1 CRelu dropout with ratio 0.1 conv2 3\u00d73\u00d764\u00d764, 1, 1 BN+Relu pool1 2\u00d72, 2, 0 conv3 3\u00d73\u00d764\u00d764, 1, 1 CRelu dropout with ratio 0.2 conv4 \u00b7 \u00b7 \u00b7\nTable S5. VGG + (conv1, 3) for CIFAR-10/100\nLayer kernel, stride, padding activation conv1 3\u00d73\u00d73\u00d732, 1, 1 CRelu dropout with ratio 0.1 conv2 3\u00d73\u00d764\u00d764, 1, 1 BN+Relu pool1 2\u00d72, 2, 0 conv3 3\u00d73\u00d764\u00d764, 1, 1 CRelu dropout with ratio 0.2 conv4 3\u00d73\u00d7128\u00d7128, 1, 1 BN+Relu pool2 2\u00d72, 2, 0 conv5 3\u00d73\u00d7128\u00d7128, 1, 1 CRelu dropout with ratio 0.2 conv6 3\u00d73\u00d7256\u00d7256, 1, 1 BN+Relu dropout with ratio 0.2 conv7 \u00b7 \u00b7 \u00b7\nTable S6. VGG + (conv1, 3, 5) for CIFAR-10/100\nLayer kernel, stride, padding activation conv1 11\u00d711\u00d73\u00d796, 4,0 Relu conv2 1\u00d71\u00d796\u00d796, 1,0 Relu conv3 3\u00d73\u00d796\u00d796, 2,0 Relu conv4 5\u00d75\u00d796\u00d7256, 1, 2 Relu conv5 1\u00d71\u00d7256\u00d7256, 1,0 Relu conv6 3\u00d73\u00d7256\u00d7256, 2,0 Relu conv7 3\u00d73\u00d7256\u00d7384, 1 ,1 Relu conv8 1\u00d71\u00d7384\u00d7384, 1,0 Relu conv9 3\u00d73\u00d7384\u00d7384, 2,1 Relu\nno dropout conv10 3\u00d73\u00d7384\u00d71024, 1,1 Relu conv11 1\u00d71\u00d71024\u00d71024, 1,0 Relu conv12 1\u00d71\u00d71024\u00d71000, 1 Relu\npool 6\u00d76 average-pooling\nTable S7. Baseline for ImageNet\nLayer kernel, stride, padding activation conv1 11\u00d711\u00d73\u00d796, 4,0 CRelu conv2 1\u00d71\u00d7192\u00d796, 1,0 CRelu conv3 3\u00d73\u00d7192\u00d796, 2,0 CRelu conv4 5\u00d75\u00d7192\u00d7256, 1, 2 CRelu conv5 1\u00d71\u00d7512\u00d7256, 1,0 Relu conv6 3\u00d73\u00d7256\u00d7256, 2,0 Relu conv7 3\u00d73\u00d7256\u00d7384, 1 ,1 Relu conv8 1\u00d71\u00d7384\u00d7384, 1,0 Relu conv9 3\u00d73\u00d7384\u00d7384, 2,1 Relu\nno dropout conv10 3\u00d73\u00d7384\u00d71024, 1,1 Relu conv11 1\u00d71\u00d71024\u00d71024, 1,0 Relu conv12 1\u00d71\u00d71024\u00d71000, 1 Relu\npool 6\u00d76 average-pooling\nTable S8. CRelu (conv1-4) for ImageNet\nLayer kernel, stride, padding activation conv1 11\u00d711\u00d73\u00d796, 4,0 CRelu conv2 1\u00d71\u00d7192\u00d796, 1,0 CRelu conv3 3\u00d73\u00d7192\u00d796, 2,0 CRelu conv4 5\u00d75\u00d7192\u00d7256, 1, 2 CRelu conv5 1\u00d71\u00d7512\u00d7256, 1,0 CRelu conv6 3\u00d73\u00d7512\u00d7256, 2,0 CRelu conv7 3\u00d73\u00d7512\u00d7384, 1 ,1 CRelu conv8 1\u00d71\u00d7768\u00d7384, 1,0 Relu conv9 3\u00d73\u00d7384\u00d7384, 2,1 Relu\ndropout with ratio 0.25 conv10 3\u00d73\u00d7384\u00d71024, 1,1 Relu conv11 1\u00d71\u00d71024\u00d71024, 1,0 Relu conv12 1\u00d71\u00d71024\u00d71000, 1 Relu\npool 6\u00d76 average-pooling\nTable S9. CRelu (conv1-7) for ImageNet\nLayer kernel, stride, padding activation conv1 11\u00d711\u00d73\u00d796, 4,0 CRelu conv2 1\u00d71\u00d7192\u00d796, 1,0 CRelu conv3 3\u00d73\u00d7192\u00d796, 2,0 CRelu conv4 5\u00d75\u00d7192\u00d7256, 1, 2 CRelu conv5 1\u00d71\u00d7512\u00d7256, 1,0 CRelu conv6 3\u00d73\u00d7512\u00d7256, 2,0 CRelu conv7 3\u00d73\u00d7512\u00d7384, 1 ,1 CRelu conv8 1\u00d71\u00d7768\u00d7384, 1,0 CRelu conv9 3\u00d73\u00d7768\u00d7384, 2,1 CRelu\ndropout with ratio 0.25 conv10 3\u00d73\u00d7768\u00d71024, 1,1 Relu conv11 1\u00d71\u00d71024\u00d71024, 1,0 Relu conv12 1\u00d71\u00d71024\u00d71000, 1 Relu\npool 6\u00d76 average-pooling\nTable S10. CRelu (conv1-9) for ImageNet\nLayer kernel, stride, padding activation conv1 11\u00d711\u00d73\u00d748, 4,0 CRelu conv2 1\u00d71\u00d796\u00d748, 1,0 CRelu conv3 3\u00d73\u00d796\u00d748, 2,0 CRelu conv4 5\u00d75\u00d796\u00d7128, 1, 2 CRelu conv5 1\u00d71\u00d7256\u00d7128, 1,0 CRelu conv6 3\u00d73\u00d7256\u00d7128, 2,0 CRelu conv7 3\u00d73\u00d7256\u00d7192, 1 ,1 CRelu conv8 1\u00d71\u00d7384\u00d7192, 1,0 CRelu conv9 3\u00d73\u00d7384\u00d7192, 2,1 CRelu\ndropout with ratio 0.25 conv10 3\u00d73\u00d7384\u00d7512, 1,1 CRelu conv11 1\u00d71\u00d7512\u00d7512, 1,0 CRelu conv12 1\u00d71\u00d7512\u00d71000, 1 CRelu\npool 6\u00d76 average-pooling\nTable S11. CRelu (all) for ImageNet\nLayer kernel, stride, padding activation conv1 11\u00d711\u00d73\u00d748, 4,0 CRelu conv2 1\u00d71\u00d796\u00d796, 1,0 Relu conv3 3\u00d73\u00d796\u00d796, 2,0 Relu conv4 5\u00d75\u00d796\u00d7128, 1, 2 CRelu conv5 1\u00d71\u00d7256\u00d7256, 1,0 Relu conv6 3\u00d73\u00d7256\u00d7256, 2,0 Relu conv7 3\u00d73\u00d7256\u00d7192, 1 ,1 CRelu conv8 1\u00d71\u00d7384\u00d7384, 1,0 Relu conv9 3\u00d73\u00d7384\u00d7384, 2,1 Relu\ndropout with ratio 0.25 conv10 3\u00d73\u00d7384\u00d71024, 1,1 Relu conv11 1\u00d71\u00d71024\u00d71024, 1,0 Relu conv12 1\u00d71\u00d71024\u00d71000, 1 Relu\npool 6\u00d76 average-pooling\nTable S12. CRelu (conv1,4,7) for ImageNet\nF. Image Reconstruction In this section, we provide more image reconstruction examples.\n(a) Original image (b) CRelu conv1 (c) CRelu conv2 (d) CRelu conv3 (e) CRelu conv4\n(f) Original image (g) conv1 Recon (h) conv2 Recon (i) conv3 Recon (j) conv4 Recon\n(k) Original image (l) conv1 Recon (m) conv2 Recon (n) conv3 Recon (o) conv4 Recon\n(p) Original image (q) conv1 Recon (r) conv2 Recon (s) conv3 Recon (t) conv4 Recon"}], "references": [{"title": "Invariant scattering convolution networks", "author": ["Bruna", "Joan", "Mallat", "St\u00e9phane"], "venue": null, "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "Signal recovery from pooling representations", "author": ["Bruna", "Joan", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "In ICML,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "An introduction to frames and Riesz bases", "author": ["Christensen", "Ole"], "venue": "Birkhuser Basel,", "citeRegEx": "Christensen and Ole.,? \\Q2003\\E", "shortCiteRegEx": "Christensen and Ole.", "year": 2003}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-jia", "Kai", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Inverting convolutional networks with convolutional networks", "author": ["Dosovitskiy", "Alexey", "Brox", "Thomas"], "venue": "In CVPR,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Measuring invariances in deep networks", "author": ["Goodfellow", "Ian", "Lee", "Honglak", "Le", "Quoc V", "Saxe", "Andrew", "Ng"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William J"], "venue": "In NIPS,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": null, "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "lya", "Hinton", "Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree", "author": ["Lee", "Cen-yu", "Gallagher", "Patrick W", "Tu", "Zhuowen"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["Liang", "Ming", "Hu", "Xiaolin"], "venue": "In CVPR,", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew", "Hannun", "Awni Y", "Ng"], "venue": "In ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Understanding deep image representations by inverting them", "author": ["Mahendran", "Aravindh", "Vedaldi", "Andrea"], "venue": "In CVPR,", "citeRegEx": "Mahendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Spectral representations for convolutional neural networks", "author": ["Rippel", "Oren", "Snoek", "Jasper", "Adams", "Ryan"], "venue": "In NIPS,", "citeRegEx": "Rippel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Snoek", "Jasper", "Rippel", "Oren", "Swersky", "Kevin", "Kiros", "Ryan", "Satish", "Nadathur", "Sundaram", "Narayanan", "Patwary", "Md. Mostofa Ali", "Adams"], "venue": "In ICML,", "citeRegEx": "Snoek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2015}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Springenberg", "Jost", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": "In ICLR Workshop,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Training very deep networks", "author": ["Srivastava", "Rupesh", "Greff", "Klaus", "Schmidhuber", "Jurgen"], "venue": "In NIPS,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["Xu", "Bing", "Wang", "Naiyan", "Chen", "Tianqi", "Li", "Mu"], "venue": "In ICML Workshop,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Deep fried convnets", "author": ["Yang", "Zichao", "Moczulski", "Marcin", "Denil", "Misha", "de Freitas", "Nndo de", "Smola", "Alex", "Song", "Le", "Wang", "Ziyu"], "venue": "In ICCV,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In ICLR,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}, {"title": "Stacked what-where auto-encoders", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "Goroshin", "Ross", "Lecun", "Yann"], "venue": "In ICLR,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "In recent years, convolutional neural networks (CNNs) have achieved great success in many problems of machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2014).", "startOffset": 139, "endOffset": 237}, {"referenceID": 21, "context": "In recent years, convolutional neural networks (CNNs) have achieved great success in many problems of machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2014).", "startOffset": 139, "endOffset": 237}, {"referenceID": 5, "context": "In recent years, convolutional neural networks (CNNs) have achieved great success in many problems of machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2014).", "startOffset": 139, "endOffset": 237}, {"referenceID": 13, "context": "In addition, a wide range of techniques has been developed to enhance the performance or ease the training of CNNs (Lin et al., 2013; Zeiler & Fergus, 2013; Maas et al., 2013; Ioffe & Szegedy, 2015).", "startOffset": 115, "endOffset": 198}, {"referenceID": 10, "context": "To better comprehend the internal operations of CNNs, we investigate the well-known AlexNet (Krizhevsky et al., 2012) and thereafter discover that the network learns highly negatively-correlated pairs of filters for the first few convolution layers (Section 2.", "startOffset": 92, "endOffset": 117}, {"referenceID": 10, "context": "During our initial exploration of classic CNNs trained on natural images such as AlexNet (Krizhevsky et al., 2012), we have noted a curious tendency of the first convolution layer filters: these filters tend to form \u201cpairs\u201d.", "startOffset": 89, "endOffset": 114}, {"referenceID": 23, "context": "Another way to allow negative activation is to employ the broader class of non-saturated activation functions including Leaky Relu and its variants (Xu et al., 2015; Maas et al., 2013).", "startOffset": 148, "endOffset": 184}, {"referenceID": 13, "context": "Another way to allow negative activation is to employ the broader class of non-saturated activation functions including Leaky Relu and its variants (Xu et al., 2015; Maas et al., 2013).", "startOffset": 148, "endOffset": 184}, {"referenceID": 24, "context": "This aspect of CNNs has gained interest recently: Mahendran & Vedaldi (2015) invert CNN features back to the input under simple natural image priors; Zhao et al. (2015) stack autoencoders with reconstruction objective to build better classifiers.", "startOffset": 150, "endOffset": 169}, {"referenceID": 0, "context": "Bruna et al. (2013) theoretically investigate general conditions unstride size: s 0.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "We evaluate the effectiveness of our proposed CRelu activation scheme on three benchmark datasets: CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and ImageNet (Deng et al., 2009).", "startOffset": 151, "endOffset": 170}, {"referenceID": 19, "context": "We use the ConvPool-CNN-C model (Springenberg et al., 2014) as our baseline model, which is composed of convolution and pooling followed by Relu without fullyconnected layers.", "startOffset": 32, "endOffset": 59}, {"referenceID": 16, "context": "Model CIFAR-10 CIFAR-100 (Rippel et al., 2015) 8.", "startOffset": 25, "endOffset": 46}, {"referenceID": 18, "context": "60 (Snoek et al., 2015) 6.", "startOffset": 3, "endOffset": 23}, {"referenceID": 11, "context": "75 (Lee et al., 2016) 6.", "startOffset": 3, "endOffset": 21}, {"referenceID": 20, "context": "37 (Srivastava et al., 2015) 7.", "startOffset": 3, "endOffset": 28}, {"referenceID": 3, "context": "To assess the impact of CRelu on large scale dataset, we perform experiments on ImageNet dataset (Deng et al., 2009), which contains about 1.", "startOffset": 97, "endOffset": 116}, {"referenceID": 24, "context": "We compare with AlexNet and other variants, such as FastFood-32-AD (FriedNet) (Yang et al., 2015) and pruned AlexNet (PrunedNet) (Han et al.", "startOffset": 78, "endOffset": 97}, {"referenceID": 7, "context": ", 2015) and pruned AlexNet (PrunedNet) (Han et al., 2015), which are modifications of AlexNet aiming at reducing the number of parameters, as well as All-CNN-B, the baseline model (Springenberg et al.", "startOffset": 39, "endOffset": 57}, {"referenceID": 19, "context": ", 2015), which are modifications of AlexNet aiming at reducing the number of parameters, as well as All-CNN-B, the baseline model (Springenberg et al., 2014).", "startOffset": 130, "endOffset": 157}, {"referenceID": 10, "context": "dard deviation for each input channel, and follow the data augmentation as described in (Krizhevsky et al., 2012).", "startOffset": 88, "endOffset": 113}, {"referenceID": 19, "context": "We take the All-CNN-B model (Springenberg et al., 2014) as our baseline model.", "startOffset": 28, "endOffset": 55}, {"referenceID": 10, "context": "The network architecture of AllCNN-B is similar to that of AlexNet (Krizhevsky et al., 2012), where the max-pooling layer is replaced by convolution with the same kernel size and stride, the fully connected layer is replaced by 1 \u00d7 1 convolution layers followed by average pooling, and the local response normalization layers are discarded.", "startOffset": 67, "endOffset": 92}, {"referenceID": 10, "context": "We report the top-1 and top-5 error rates with center crop only and by averaging scores over 10 patches from the center crop and four corners and with horizontal flip (Krizhevsky et al., 2012).", "startOffset": 167, "endOffset": 192}, {"referenceID": 19, "context": "We note that Springenberg et al. (2014) reported slightly better result (41.", "startOffset": 13, "endOffset": 40}, {"referenceID": 24, "context": "6M parameters (CRelu + all) outperforms FastFood-32-AD (FriedNet) (Yang et al., 2015) and Pruned AlexNet (PrunedNet) (Han et al.", "startOffset": 66, "endOffset": 85}, {"referenceID": 7, "context": ", 2015) and Pruned AlexNet (PrunedNet) (Han et al., 2015), whose designs directly aim at parameter reduction.", "startOffset": 39, "endOffset": 57}, {"referenceID": 22, "context": "1 says that the complexity bound of CRelu + linear transformation is the same as that of Relu + linear transformation, which is proved by Wan et al. (2013). In other words, although the number of model parameters are doubled by CRelu, the model complexity does not necessarily increase.", "startOffset": 138, "endOffset": 156}, {"referenceID": 6, "context": "We measure the invariance scores using the evaluation metrics from (Goodfellow et al., 2009) and draw another comparison between the CRelu models and the Relu models.", "startOffset": 67, "endOffset": 92}, {"referenceID": 6, "context": "We also observe that although as a general trend, the invariance scores increase while going deeper into the networks\u2013consistent with the observations from (Goodfellow et al., 2009), rather unexpectedly, the progression is not monotonic.", "startOffset": 156, "endOffset": 181}], "year": 2016, "abstractText": "Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.", "creator": "LaTeX with hyperref package"}}}