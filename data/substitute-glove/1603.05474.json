{"id": "1603.05474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Neural Aggregation Network for Video Face Recognition", "abstract": "In this box, indeed far a Neural Aggregation Network (NAN) work video face inclusion. The networks hope taking strong clips could too personality day and one each when variable least one look frames as its and/or, as manufactured a compact and fixed - 3-dimensional combines appropriate though similar someone. The creating network is number of two semantic. The introduction unidirectional modules an hand CNN which pages each face piece left a feature representing. The phonological aggregation rendezvous is composed latter two content based making blocks not it driven eventually a brain terabytes actually created features extracted from was take cartoon from that scenes embeds modules. The output from the this media along bureaucratically during second, whose output true opposed as two annotating constitute taken the recordings row. Due not under attention regulation, this representation another invariant turn instead order seen the face frames. The developed music done their propose NAN unfairly outperforms once - clever trachytic possible all average sanitizing, and realization union - of - the - art accuracy on fifteen picture face greater datasets: the YouTube Face, IJB - A even Celebrity - 11,000 querying.", "histories": [["v1", "Thu, 17 Mar 2016 13:30:45 GMT  (392kb,D)", "http://arxiv.org/abs/1603.05474v1", "TR"], ["v2", "Fri, 18 Nov 2016 12:38:10 GMT  (2187kb,D)", "http://arxiv.org/abs/1603.05474v2", "TR"], ["v3", "Wed, 12 Apr 2017 06:02:06 GMT  (2274kb,D)", "http://arxiv.org/abs/1603.05474v3", "accepted by CVPR 2017 (IEEE Conference on Computer Vision and Pattern Recognition)"], ["v4", "Wed, 2 Aug 2017 08:08:14 GMT  (1593kb,D)", "http://arxiv.org/abs/1603.05474v4", "Post CVPR2017 version with minor typo fix"]], "COMMENTS": "TR", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["jiaolong yang", "peiran ren", "dongqing zhang", "dong chen", "fang wen", "hongdong li", "gang hua"], "accepted": false, "id": "1603.05474"}, "pdf": {"name": "1603.05474.pdf", "metadata": {"source": "CRF", "title": "Neural Aggregation Network for Video Face Recognition", "authors": ["Jiaolong Yang", "Peiran Ren", "Dong Chen", "Fang Wen", "Hongdong Li", "Gang Hua"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23]. Compared to image-based face recognition, more information of the subjects can be exploited from input videos, which naturally incorporate faces of the same subject in varying poses and illumination conditions. The key issue in video face recognition is to build an appropriate visual representation of the video face, such that it can effectively integrate the information across different frames together, maintaining beneficial while discarding\n\u2217This work was done when J. Yang was an intern at MSR supervised by G. Hua.\nnoisy information. A naive approach would be representing a video face as a set of frame-level face features [26, 23]. Such a representation comprehensively maintains all the information across all frames. However, in the recognition phase, to compare two video faces, one needs to fuse the matching results across all pairs of frames between the two face videos. Let n be the average number of video frames. The computational complexity is O(n2) per match operation, which is obviously not desirable. Besides, such a set-based representation would incur O(n) space complexity per video face example, which demands a lot of memory storage and confronts efficient indexing.\nWe argue that it is more desirable to come with a compact, fixed-size visual representation at the clip level for video faces, irrespective of the varied length of the video clips. Such a representation would allow direct computation of the similarity or distance between two face videos without the need for frame-to-frame matching. A straight-\nar X\niv :1\n60 3.\n05 47\n4v 1\n[ cs\n.C V\n] 1\n7 M\nar 2\nforward solution might be to extract a feature representation at each frame, and then conduct a certain type of pooling to aggregate the frame-level features together to form a video level representation.\nThe most commonly adopted pooling strategy may be average and max pooling. There were some successes applying such pooling strategy in building video face representations. For example, the Eigen-PEP representations [18] takes the average of a part-based representation across different video frames and then conducts a PCA dimension reduction. Other works such as the Video Fisher Vector Faces (VF2) [21] have attempted to use a more general feature encoding schemes, i.e., Fisher Vector coding, to aggregate local features across different video frames together to form a video-level representation.\nNotwithstanding the demonstrated success of these methods, we argue that a good pooling strategy should adaptively weight and select the frame-level features across all frames. To this end, we look for an adaptive weighting scheme to linearly combine all frame-level features from a video together to form a compact and discriminative face representation. Different from previous methods, we neither fix the weights nor rely on any particular heuristics to handcraft them. Instead, we designed a neural network to adaptively calculate the weights at runtime. We named our network the neural aggregation network (NAN), whose coefficients can be trained through supervised learning.\nThe proposed NAN is composed of two major modules that could be trained end-to-end or one by one separately. The feature embedding module is a deep CNN model that serves as a frame-level feature extractor. The other is the aggregation module that adaptively fuses the features of all the video frames together.\nOur neural aggregation network is designed to inherit the main advantages of pooling techniques, including the ability of handling arbitrary input size and producing ordering invariant representation. The key component of this network is inspired by the Neural Turing Machine [7] and the Orderless Set Network [27], both of which applied an attention mechanism to organize the input through a memory. This mechanism can take input of arbitrary size and work as a tailor emphasizing or suppressing each input element just as in a weighted averaging, and very importantly it is order independent. Different from the Orderless Set Network [27], we designed a much simpler network structure for our task of video face recognition, instead of the bulky Long Short Term Memory network (LSTM) [9] used in their paper.\nApart from video level representation, our neural aggregation network can also serve as a subject level feature extractor to fuse multiple data sources. For example, one can feed it with all available images and videos and obtain a feature representation with fixed size. In this way, the face\nrecognition system not only enjoys the time and memory efficiency due to the compact representation, but also exhibits superior performance, as we will show in our experiments.\nWe evaluated the proposed NAN for both the tasks of video face verification and identification. We observed consistent margins in three challenging datasets, including the YouTube face dataset [29], the IJB-A dataset [15], and the Celebrity-1000 dataset [19] compared to the baseline solutions using average pooling and other works. Therefore, our research contributes in the following aspects:\n\u2022 We proposed a neural aggregation network, which learns an adaptive, content-aware pooling strategy for a set of visual features.\n\u2022 We applied it for the task of video face recognition, which leads to a comprehensive, compact, and yet discriminative video face representation.\n\u2022 We achieved state-of-the-art recognition accuracy over three challenging video face recognition benchmarks.\nLast but not least, we shall point out though, that our proposed NAN can serve as a general framework for learning content-adaptive pooling. Therefore, it may also serve for feature aggregation for other computer vision tasks such as image recognition and video event recognition [13, 6, 14, 1]. We leave it as our future work to evaluate its performance against other hand-crafted pooling schemes on these tasks, such as VLAD [13, 6, 14] and scene aligned pooling [1]."}, {"heading": "2. Neural aggregation network", "text": "The whole network of our method is composed of two modules. The feature embedding part is a CNN model for face feature extraction from a simple image. The aggregation module fuses all feature vectors to a fixed-dimension representation, which can be used in any decision model. For face identification, the framework of our method is illustrated in Fig. 1. For face verification, we use a modified version employing a Siamese structure [4].\nOur new representation stands on the shoulders of stateof-the-art deep CNN model, and is more powerful by adaptively aggregating all frames in the input video into a compact 128 dimensional representation. In this section, we will first introduce the aggregation and feature embedding modules in Section 2.1 and 2.2, respectively, then present the training strategies in Section 2.3."}, {"heading": "2.1. Aggregation module", "text": "The aggregation module is designed to take benefits from all frames in a video, potentially containing more discriminative information than a single image, and handle arbitrary video size in an unified form, producing an order invariant representation.\nConsider the video face identification task training on n pairs of labeled data (Xi, yi)ni=1, where X\ni is a video of varying size ti: Xi = {xi1,xi2, ...,xiti}, in which x i k, k = 1, ..., ti is the kth frame in the video, and yi is the corresponding subject ID of Xi. Each frame xik has a corresponding feature representation f ik extracted from the feature embedding module. For better readability, we omit the upper index where appropriate in the following text. Our goal is to utilize all feature vectors from a video to generate a set of linear weights {ak}tk=1, so that the aggregated feature representation becomes\nr = \u2211 k akfk. (1)\nObviously, the key of Eq. 1 is its weights {ak}. If ak \u2261 1t , Eq. 1 will degrades to naive averaging, which is usually non-optimal, as we will show in our experiments. We instead try to let the data itself to help generate better weights. For this purpose, we employed a content based attention mechanism [7] in our new network structure for face set representation.\nThe structure of the aggregation module is shown in Fig. 1. The crux of this module is two attention blocks. Each of them reads all feature vectors from the feature embedding module, filters them with a kernel q and generates a fixed length representation r whose dimension is the same as the feature representation fk of a single image. The first attention block is used to obtain a content adaptive filter kernel, which we argue that it is better to be varying for different subjects. This way, we invented an optimized content aware video face aggregation structure."}, {"heading": "2.1.1 Attention blocks", "text": "The attention block in the aggregation module takes a feature set {fk} as input, filters them with a kernel q via dot product, yielding a set of corresponding significances {ek}. They are then passed to a softmax operator to generate normalized weights {ak}, \u2211 k ak = 1. Both of these opera-\ntions are described by the following equations, respectively:\nek = q T fk (2) ak = exp(ek)\u2211 j exp(ej) . (3)\nThe attention block is modulated solely by a filter kernel q. One key advantage of the attention block is that its output is invariant to the input order of fk. It can be seen from Eq. 2, 3 and 1 that, permuting fk and fk\u2032 has no effects on the aggregated representation r. Another appealing property is that the number of inputs {fk} does not affect the size of output r which is of the same dimension with a single fk.\nWe argue that a content adaptive kernel can produce better results. So we employed two attention blocks, with the first to adapt the second\u2019s filter kernel, through a transfer layer:\nq1 = tanh(Wr0 + b) (4)\nwhere W and b are the weight matrix and bias vector of the neurons respectively, and tanh(x) = e\nx\u2212e\u2212x ex+e\u2212x imposes the\nhyperbolic tangent nonlinearity. The kernel of the first attention block q0 and the coefficients of the transfer layer (W,b) serve as the parameters of the aggregation module, and can be trained by supervised learning via standard gradient descent.\nNote that the roles of the first and second attention blocks are different. The first block which is combined with the transfer layer is responsible for generating an input adaptive filter kernel q1, while the second is to apply this kernel to produce the final aggregated representation r1. The attention mechanism is suitable to handle orderless inputs of arbitrary size, which is an expected property both in kernel adaptation and result generation."}, {"heading": "2.1.2 Discussion", "text": "The structure of the aggregation module looks like an unfolded recurrent neural network (RNN) used in the Orderless Set Network [27] due to the coincidence that there are sequentially two attention blocks working in this module. However, there are at least two differences between the proposed network and the one used in their paper.\nFirstly, we use a feed forward network, with no weight sharing nor any evolving internal state. Actually, once trained, the first filter kernel q0 and coefficients of the transfer layer (W,b) are fixed. Secondly, adding more attention blocks in the aggregation modules sounds like a reasonable extension at first glance. However, that will introduce more non-linearity which is in doubt whether they are beneficial or not, but guarantee to bring more difficulties in training. On trial, we did not observe any improvements using three or four attention blocks, either with or without weight sharing among multiple transfer layers, using complex LSTM unit or not."}, {"heading": "2.2. Feature embedding module", "text": "The image embedding module of our NAN is a deep Convolution Neural Network (CNN), which embeds each frame of a video to a face feature representation. To leverage modern deep CNN networks with high-end performances, in this paper we follow the GoogLeNet [25] network structure, and equip it with the Batch Normalization (BN) technique [12]. The output image features from the GoogLeNet are of 128 dimensions which are then fed into the aggregation module. Consequently, our NAN network produces a 128-d feature representation for each input video. In the rest of this paper we will simply refer to the employed GoogLeNet-BN network as CNN."}, {"heading": "2.3. Network training", "text": "The proposed network can be trained for both face verification and identification tasks described respectively as follows.\nFace identification. Face identification attempts to recognize the identity of a probe face video provided a set of gallery face videos/images with known identities. When training, we optimize the coefficients of our network and a fully-connected prediction layer by minimizing the average classification loss: li = \u2212 log pi,yi where yi is the target label of the i-th (video) instance, pi,yi = exp(pi,yi )\u2211 z exp(pi,z)\n, and pi,z is the zth output of the FC layer. In the testing phase, the identity of a probe video will be determined either by feeding it into the trained network (for close-set tests), or choosing from gallery subjects based on the closest L2 distances of the aggregated features (for open-set tests).\nFace verification. In face verification, pairs of face videos are given and the decisions are made to determine whether each video pair is from the same subject or not. To train our network for this task, we build a siamese neural aggregation network structure [4] with two NANs sharing their weights, and minimize the average of the contrastive loss [8]: li,j = yi,j ||r1i \u2212 r1j ||22 +(1\u2212yi,j)max(0,m\u2212 ||r1i \u2212 r1j ||22), where yi,j = 1 if the pair (i, j) is from the same identity and yi,j = 0 otherwise. The constant m is set to 2 in all our experiments. In the testing phase, the decision is made by comparing the L2 distance of the aggregated features with a scalar threshold tuned on the training set."}, {"heading": "2.3.1 Module training", "text": "The two modules can either be trained simultaneously in an end-to-end fashion, or separately one by one. In this paper, we take the latter option to train the NAN network. Specifically, we first train the CNN on single images with\nthe identification task, then we train the aggregation module on top of the features extracted by CNN.\nWe chose this separate training strategy mainly for two reasons. First, in this work we would like to focus on analyzing the effectiveness and performance of the aggregation module with the attention mechanism. Despite the huge success of applying deep CNN in image-based face recognition task, few attention has been drawn to CNN feature aggregation to our knowledge. Second, training a deep CNN usually necessitates a large volume of labeled data. While millions of still images can be obtained for training nowadays [26, 23], it appears not practical to collect such amount of face videos. We leave an end-to-end training of the NAN as our future work."}, {"heading": "3. Experiments", "text": "This section evaluates the performance of the proposed Neural Aggregation Network for video face recognition tasks. We will begin with introducing our training details and the baseline methods for the NAN. Then we will report results on three video face recognition datasets: the YouTube Face dataset [29], the IARPA Janus Benchmark A (IJB-A) [15], and the Celebrity-1000 dataset [19]."}, {"heading": "3.1. Training details", "text": "As mentioned in Section 2.3, two networks are trained separately in this paper. To train the CNN, we use around 3M face images of 50K identities crawled from the internet to perform image-based identification. The faces are detected using the JDA method[2], and aligned with the LBR method [22]. The input image size is 224x224. After training, the CNN in the feature embedding module is fixed and we focus on analyzing the effectiveness of the neural aggregation module.\nThe aggregation module is trained on the video face datasets we use with standard backpropagation and gradient descent. As the network is quite simple and image features are compact (128-d), the training process is quite efficient: training on 5K video pairs with \u223c1M images in total only takes approximately 3 minutes on CPU of a desktop PC."}, {"heading": "3.2. Baseline Methods", "text": "The performance of our method is evaluated against a few baseline methods. Since our goal is to build a fixed-size compact representation for video faces, we mainly compared our results with aggregation strategies such as average pooling. Moreover, we also compared it with some set-toset similarity measurements leveraging pairwise comparison on the image level.\nCNN+Mean L2 measures the similarity of two video faces via averaging the feature distances of all frame pairs. It necessitates storing all image features of a video or sub-\nject, and has O(n2) complexity for similarity computation.\nCNN+Min L2 is similar to the above but uses the smallest pairwise distance.\nCNN+AvePool is average-pooling along each feature dimension for aggregation.\nCNN+MaxPool is max-pooling along each feature dimension for aggregation.\nCNN+AvePool, CNN+MaxPool and our NAN all produce a 128-d feature representation for each video (or subject of multiple videos), and compute the similarity (L2 distance) in O(1) time."}, {"heading": "3.3. Results on YouTube Face dataset", "text": "We first tested our method on the YouTube Face dataset which is designed for unconstrained face verification in videos. It contains 3,425 videos of 1,595 different people,\nand an average of 2.15 videos are available for each subject. The video clip lengths vary from 48 to 6,070 frames, with an average length 181.3 frames per video. Ten folds of 500 video pairs are available, and we follow the standard verification protocol to report the average accuracy with crossvalidation. As facial landmarks are not provided in this dataset, we first apply the LBR method [22] to locate the landmarks and align every image before image-level feature extraction with CNN.\nThe results of our NAN, its baselines and other methods are presented in Table 1, with their ROC curves shown in Fig. 3. It can be observed that our baselines, except CNN+MaxPool, achieve similar accuracies to the stateof-the-art method FaceNet [23], which has an accuracy of 95.12%\u00b10.39. Note that FaceNet is also based on a GoogLeNet style network, and the average similarity of all pairs of 100 frames in each video (i.e., 10K pairs) was used [23]. Our NAN outperforms all the baselines and other methods. It achieves 95.52%\u00b10.06 accuracy, reducing the error of FaceNet by 8.2% and exhibiting a much smaller variance.\nThough our NAN has surpassed all the baselines and previous methods on the YouTube Face dataset, we believe that the margin to the baseline performances could be larger. This is because the face variations in the videos of the YouTube Face dataset are relatively small, thus no much beneficial information can be extracted compared to naive average pooling or computing mean L2 distances. This motivates us to test the network on more challenging datasets such as the IJB-A dataset and the Celebrity-1000 dataset."}, {"heading": "3.4. Results on IJB-A dataset", "text": "The IJB-A dataset contains face images and videos captured from unconstrained environments. There are 500 subjects with 5,397 images and 2,042 videos sampled to 20,412\nframes in total, 11.4 images and 4.2 videos per subject on average. The IJB-A dataset features full pose variation and wide variations in imaging conditions, which makes the face recognition very challenging. In this dataset, each training and testing instance, which is called a \u2018template\u2019, comprises a mixture of still images and sampled video frames. The numbers of images in the templates range from 1 to 190 with approximately 10 images per template. In our experiments, the faces are aligned with the provided facial landmarks (two eyes and nose base) before image-level feature extraction with the CNN.\nWe tested the proposed method on the \u2018compare\u2019 (1:1 matching) protocol for face verification on IJB-A. There are 10 training and testing splits. Each of them contains 333 subjects, and its corresponding testing split takes the other 167 subjects. The testing splits contain 11,748 pairs of templates (1,756 genuine and 9,992 imposter pairs) on average. The evaluation metric consists of True Accept Rates (TAR) at different False Accept Rates (FAR).\nThe results of our method, compared with other methods as well as our baselines are presented in Table 2. THese results show that both NAN and the baselines outperform previous methods such as [28] and [3], and NAN outperforms its baselines especially on the low FAR cases. The TARs of NAN at FARs of 0.001, 0.01 and 0.1 are 0.785,\n0.897 and 0.959 respectively. The errors are reduced by about 56%, 51% and 22% respectively compared to the state-of-the-art method [3], or about 51%, 28% and 5% respectively compared to the best results by CNN+Mean L2 and CNN+AvePool1. Note that averaged image features was used by the method presented in [3] for image set representation. Our NAN has learned a discriminative aggregation mechanism for the templates, which is better in verification accuracy than the baseline aggregations and set-distance measurements.\nThe ROC curves of NAN and its two baselines are presented in Fig. 4. NAN outperforms both baselines with large margins when the FAR is low (<0.03), which suggests that it is more robust and reliable."}, {"heading": "3.5. Results on Celebrity-1000 dataset", "text": "The Celebrity-1000 dataset is designed to study the unconstrained video-based face identification problem. This dataset contains 159,726 video sequences of 1,000 human subjects, with 2.4M frames in total (15 frames per sequence on average). This dataset provides the face regions and 5 facial landmarks from the face detector. We use these landmarks to align the faces before image feature extraction\n1Results by CNN+Min L2 and CNN+MaxPool are rather poor thus not presented.\nthrough our CNN module. Two types of protocols \u2013 open-set and close-set \u2013 exist on this dataset and we evaluated our NAN on both of them. In the open-set protocol, 200 subjects are used for training, while videos sequences of the rest 800 subjects are used as the gallery set and probe set at testing stage. There are 4 different experimental settings with different number of probe and gallery subjects: 100, 200, 400 and 800. In the closeset protocol, the video sequences from all 1,000 subjects are divided into a training (gallery) subset and a testing (probe) subset. There are also 4 settings for close-set: 100, 200, 500 and 1000 subjects.\nClose-set tests. For the close-set protocol, we first trained the network on the video sequences by minimizing the identification loss. The FC layer output values are taken as the identification scores, and the subject with maximum score is the identification result. We also trained a linear classifier for our baseline method CNN+AvePool to classify each video feature to the subjects. As the features are built on the video sequences, we call this approach \u2018VideoAggr\u2019 to distinguish it from another approach to be described next. Each subject in the dataset has multiple video sequences, thus we can naturally build a single representation for the subject by aggregating all the available images in all the training (gallery) video sequences. We call this approach \u2018SubjectAggr\u2019. In this way, the linear classifier can be bypassed, and identification can be achieved simply by comparing the L2 distances between the aggregated probe video representation and each aggregated subject representation.\nThe identification accuracies of our method on the closeset tests are presented in Table 3. All the baseline methods and our NAN outperformed previous methods by large mar-\ngins, and the NAN consistently outperformed the baseline methods on all these tasks. For the \u2018VideoAggr\u2019 approach, NAN reduces the errors of baseline CNN+AvePool by 14%, 9%, 8% and 3% for the settings of 100, 200, 500, and 1000 subjects, respectively. For the \u2018SubjectAggr\u2019 approach, it brings significant improvements upon the baseline: the errors are reduced by 38%, 21%, 21% and 14% respectively. It is interesting to see that, \u2018SubjectAggr\u2019 leads to a clear performance drop by CNN+AvePool. This indicates that the hand-crafted aggregation gets even worse when applied on the subject level with multiple videos. However, our NAN can benefit from \u2018SubjectAggr\u2019, yielding a result consistently better than or on par with the \u2018VideoAggr\u2019 approach, delivering a considerable accuracy boost (e.g. for 100 subjects the error is reduced by 20%). This indicates that our NAN works very well on handling large data variations and can generate highly-compact subject level representation.\nOpen-set tests. We then tested our NAN with the closeset protocol. We first trained the network on the provided training video sequences. In the testing stage, we took the \u2018SubjectAggr\u2019 approach described before to build a highlycompact face representation for each gallery subject. Identification was performed similarly by comparing the L2 distances between aggregated face representations.\nThe results in Table 4 shows that our NAN reduce the error of the baseline CNN+AvePool by 29%, 29%, 20%, 19% for the settings of 100, 200, 400, and 800 subjects, respectively. This again suggests that in the presence of large face variances, the widely used strategies such as average pooling aggregation and the pairwise distance computation are far from optimal. In such cases, the learned NAN model is powerful and can yield much superior results."}, {"heading": "4. Closing Remarks", "text": "We have presented a Neural Aggregation Network, which is based on CNN and attention mechanism, for video face representation and recognition. It fuses all input frames with a set of content adaptive weights, resulting in a compact (128-d) representation that is invariant to the input frame order, which is an important property in face recognition. The structure of the aggregation module is simple with small computation and memory footprints, but can generate a comprehensive face representation after trained through supervised learning. The experiments have shown than the proposed NAN network consistently outperforms the baselines of hand-crafted aggregations, and it sets new state-ofthe-art on the three tested video face recognition datasets.\nIt should be noted that the proposed method can support general set representation, and therefore can be used in applications other than video face recognition. In addition, we foresee a further performance boost of NAN when trained in an end-to-end fashion in the presence of enough video data."}], "references": [{"title": "Scene aligned pooling for complex video recognition", "author": ["L. Cao", "Y. Mu", "A. Natsev", "S.-F. Chang", "G. Hua", "J.R. Smith"], "venue": "Computer Vision\u2013ECCV 2012, pages 688\u2013701. Springer,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint cascade face detection and alignment", "author": ["D. Chen", "S. Ren", "Y. Wei", "X. Cao", "J. Sun"], "venue": "Computer Vision\u2013ECCV 2014, pages 109\u2013122. Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An end-to-end system for unconstrained face verification with deep convolutional neural networks", "author": ["J.-C. Chen", "R. Ranjan", "A. Kumar", "C.-H. Chen", "V. Patel", "R. Chellappa"], "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 118\u2013126,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 539\u2013546. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Fusing robust face region descriptors via multiple metric learning for face recognition in the wild", "author": ["Z. Cui", "W. Li", "D. Xu", "S. Shan", "X. Chen"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Stable hyper-pooling and query expansion for event detection", "author": ["M. Douze", "J. Revaud", "C. Schmid", "H. J\u00e9gou"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1825\u20131832,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "CoRR, abs/1410.5401,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pages 1735\u20131742. IEEE,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Discriminative deep metric learning for face verification in the wild", "author": ["J. Hu", "J. Lu", "Y.-P. Tan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1875\u20131882,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Large margin multimetric learning for face and kinship verification in the wild", "author": ["J. Hu", "J. Lu", "J. Yuan", "Y.-P. Tan"], "venue": "Computer Vision\u2013ACCV 2014, pages 252\u2013267. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 3304\u20133311. IEEE,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Triangulation embedding and democratic aggregation for image search", "author": ["H. J\u00e9gou", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3310\u20133317,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a", "author": ["B.F. Klare", "B. Klein", "E. Taborsky", "A. Blanton", "J. Cheney", "K. Allen", "P. Grother", "A. Mah", "M. Burge", "A.K. Jain"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 1931\u20131939. IEEE,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Open source biometric recognition", "author": ["J.C. Klontz", "B.F. Klare", "S. Klum", "A.K. Jain", "M.J. Burge"], "venue": "Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference on, pages 1\u20138. IEEE,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic elastic matching for pose variant face verification", "author": ["H. Li", "G. Hua", "Z. Lin", "J. Brandt", "J. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3499\u20133506,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Eigen-pep for video face recognition", "author": ["H. Li", "G. Hua", "X. Shen", "Z. Lin", "J. Brandt"], "venue": "Computer Vision\u2013ACCV 2014, pages 17\u201333. Springer,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward largepopulation face identification in unconstrained videos", "author": ["L. Liu", "L. Zhang", "H. Liu", "S. Yan"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on, 24(11):1874\u20131884,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Volume structured ordinal features with background similarity measure for video face recognition", "author": ["H. Mendez-Vazquez", "Y. Martinez-Diaz", "Z. Chai"], "venue": "International Conf. on Biometrics (ICB),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A compact and discriminative face track descriptor", "author": ["O.M. Parkhi", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Face alignment at 3000 fps via regressing local binary features", "author": ["S. Ren", "X. Cao", "Y. Wei", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1685\u20131692,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 815\u2013823,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeply learned face representations are sparse, selective, and robust", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "Proceedings of the 8  IEEE Conference on Computer Vision and Pattern Recognition, pages 2892\u20132900,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1701\u20131708,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Order matters: sequence to sequence for sets", "author": ["O. Vinyals", "S. Bengio", "M. Kudlur"], "venue": "Proc. International Conf. on Learning Representation, San Juan, Puerto Rico, May", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Face search at scale: 80 million gallery", "author": ["D. Wang", "C. Otto", "A.K. Jain"], "venue": "arXiv preprint arXiv:1507.07242,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Face recognition in unconstrained videos with matched background similarity", "author": ["L. Wolf", "T. Hassner", "I. Maoz"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 529\u2013534. IEEE,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "The svm-minus similarity score for video face recognition", "author": ["L. Wolf", "N. Levy"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 16, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 29, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 4, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 19, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 17, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 18, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 20, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 9, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 25, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 22, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 25, "context": "A naive approach would be representing a video face as a set of frame-level face features [26, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 22, "context": "A naive approach would be representing a video face as a set of frame-level face features [26, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 17, "context": "For example, the Eigen-PEP representations [18] takes the average of a part-based representation across different video frames and then conducts a PCA dimension reduction.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "Other works such as the Video Fisher Vector Faces (VF) [21] have attempted to use a more general feature encoding schemes, i.", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "The key component of this network is inspired by the Neural Turing Machine [7] and the Orderless Set Network [27], both of which applied an attention mechanism to organize the input through a memory.", "startOffset": 75, "endOffset": 78}, {"referenceID": 26, "context": "The key component of this network is inspired by the Neural Turing Machine [7] and the Orderless Set Network [27], both of which applied an attention mechanism to organize the input through a memory.", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": "Different from the Orderless Set Network [27], we designed a much simpler network structure for our task of video face recognition, instead of the bulky Long Short Term Memory network (LSTM) [9] used in their paper.", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "Different from the Orderless Set Network [27], we designed a much simpler network structure for our task of video face recognition, instead of the bulky Long Short Term Memory network (LSTM) [9] used in their paper.", "startOffset": 191, "endOffset": 194}, {"referenceID": 28, "context": "We observed consistent margins in three challenging datasets, including the YouTube face dataset [29], the IJB-A dataset [15], and the Celebrity-1000 dataset [19] compared to the baseline solutions using average pooling and other works.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "We observed consistent margins in three challenging datasets, including the YouTube face dataset [29], the IJB-A dataset [15], and the Celebrity-1000 dataset [19] compared to the baseline solutions using average pooling and other works.", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "We observed consistent margins in three challenging datasets, including the YouTube face dataset [29], the IJB-A dataset [15], and the Celebrity-1000 dataset [19] compared to the baseline solutions using average pooling and other works.", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "Therefore, it may also serve for feature aggregation for other computer vision tasks such as image recognition and video event recognition [13, 6, 14, 1].", "startOffset": 139, "endOffset": 153}, {"referenceID": 5, "context": "Therefore, it may also serve for feature aggregation for other computer vision tasks such as image recognition and video event recognition [13, 6, 14, 1].", "startOffset": 139, "endOffset": 153}, {"referenceID": 13, "context": "Therefore, it may also serve for feature aggregation for other computer vision tasks such as image recognition and video event recognition [13, 6, 14, 1].", "startOffset": 139, "endOffset": 153}, {"referenceID": 0, "context": "Therefore, it may also serve for feature aggregation for other computer vision tasks such as image recognition and video event recognition [13, 6, 14, 1].", "startOffset": 139, "endOffset": 153}, {"referenceID": 12, "context": "We leave it as our future work to evaluate its performance against other hand-crafted pooling schemes on these tasks, such as VLAD [13, 6, 14] and scene aligned pooling [1].", "startOffset": 131, "endOffset": 142}, {"referenceID": 5, "context": "We leave it as our future work to evaluate its performance against other hand-crafted pooling schemes on these tasks, such as VLAD [13, 6, 14] and scene aligned pooling [1].", "startOffset": 131, "endOffset": 142}, {"referenceID": 13, "context": "We leave it as our future work to evaluate its performance against other hand-crafted pooling schemes on these tasks, such as VLAD [13, 6, 14] and scene aligned pooling [1].", "startOffset": 131, "endOffset": 142}, {"referenceID": 0, "context": "We leave it as our future work to evaluate its performance against other hand-crafted pooling schemes on these tasks, such as VLAD [13, 6, 14] and scene aligned pooling [1].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "For face verification, we use a modified version employing a Siamese structure [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "For this purpose, we employed a content based attention mechanism [7] in our new network structure for face set representation.", "startOffset": 66, "endOffset": 69}, {"referenceID": 26, "context": "The structure of the aggregation module looks like an unfolded recurrent neural network (RNN) used in the Orderless Set Network [27] due to the coincidence that there are sequentially two attention blocks working in this module.", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "To leverage modern deep CNN networks with high-end performances, in this paper we follow the GoogLeNet [25] network structure, and equip it with the Batch Normalization (BN) technique [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "To leverage modern deep CNN networks with high-end performances, in this paper we follow the GoogLeNet [25] network structure, and equip it with the Batch Normalization (BN) technique [12].", "startOffset": 184, "endOffset": 188}, {"referenceID": 3, "context": "To train our network for this task, we build a siamese neural aggregation network structure [4] with two NANs sharing their weights, and minimize the average of the contrastive loss [8]: li,j = yi,j ||ri \u2212 rj ||2 +(1\u2212yi,j)max(0,m\u2212 ||ri \u2212 rj ||2), where yi,j = 1 if the pair (i, j) is from the same identity and yi,j = 0 otherwise.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "To train our network for this task, we build a siamese neural aggregation network structure [4] with two NANs sharing their weights, and minimize the average of the contrastive loss [8]: li,j = yi,j ||ri \u2212 rj ||2 +(1\u2212yi,j)max(0,m\u2212 ||ri \u2212 rj ||2), where yi,j = 1 if the pair (i, j) is from the same identity and yi,j = 0 otherwise.", "startOffset": 182, "endOffset": 185}, {"referenceID": 25, "context": "While millions of still images can be obtained for training nowadays [26, 23], it appears not practical to collect such amount of face videos.", "startOffset": 69, "endOffset": 77}, {"referenceID": 22, "context": "While millions of still images can be obtained for training nowadays [26, 23], it appears not practical to collect such amount of face videos.", "startOffset": 69, "endOffset": 77}, {"referenceID": 28, "context": "Then we will report results on three video face recognition datasets: the YouTube Face dataset [29], the IARPA Janus Benchmark A (IJB-A) [15], and the Celebrity-1000 dataset [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "Then we will report results on three video face recognition datasets: the YouTube Face dataset [29], the IARPA Janus Benchmark A (IJB-A) [15], and the Celebrity-1000 dataset [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "Then we will report results on three video face recognition datasets: the YouTube Face dataset [29], the IARPA Janus Benchmark A (IJB-A) [15], and the Celebrity-1000 dataset [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 1, "context": "The faces are detected using the JDA method[2], and aligned with the LBR method [22].", "startOffset": 43, "endOffset": 46}, {"referenceID": 21, "context": "The faces are detected using the JDA method[2], and aligned with the LBR method [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "LM3L [11] 81.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "3 DDML (combined) [10] 82.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "EigenPEP [18] 84.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "6 DeepFace-single [26] 91.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "DeepID2+ [24] 93.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "2 \u2013 FaceNet [23] 95.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "As facial landmarks are not provided in this dataset, we first apply the LBR method [22] to locate the landmarks and align every image before image-level feature extraction with CNN.", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "It can be observed that our baselines, except CNN+MaxPool, achieve similar accuracies to the stateof-the-art method FaceNet [23], which has an accuracy of 95.", "startOffset": 124, "endOffset": 128}, {"referenceID": 22, "context": ", 10K pairs) was used [23].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "OpenBR [16] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "LSFS [28] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "013 DCNNmanual+metric [3] \u2013 0.", "startOffset": 22, "endOffset": 25}, {"referenceID": 27, "context": "THese results show that both NAN and the baselines outperform previous methods such as [28] and [3], and NAN outperforms its baselines especially on the low FAR cases.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "THese results show that both NAN and the baselines outperform previous methods such as [28] and [3], and NAN outperforms its baselines especially on the low FAR cases.", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "The errors are reduced by about 56%, 51% and 22% respectively compared to the state-of-the-art method [3], or about 51%, 28% and 5% respectively compared to the best results by CNN+Mean L2 and CNN+AvePool1.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Note that averaged image features was used by the method presented in [3] for image set representation.", "startOffset": 70, "endOffset": 73}, {"referenceID": 18, "context": "MTJSR [19] 50.", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "Eigen-PEP [18] 50.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "MTJSR [19] 46.", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "Eigen-PEP [18] 51.", "startOffset": 10, "endOffset": 14}], "year": 2016, "abstractText": "In this paper, we present a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with variable number of face frames as its input, and produces a compact and fixeddimension visual representation of that person. The whole network is composed of two modules. The feature embedding module is a CNN which maps each face frame into a feature representation. The neural aggregation module is composed of two content based attention blocks which is driven by a memory storing all the features extracted from the face video through the feature embedding module. The output of the first attention block adapts the second, whose output is adopted as the aggregated representation of the video faces. Due to the attention mechanism, this representation is invariant to the order of the face frames. The experiments show that the proposed NAN consistently outperforms hand-crafted aggregations such as average pooling, and achieves state-of-the-art accuracies on three video face recognition datasets: the YouTube Face, IJB-A and Celebrity-1000 datasets.", "creator": "LaTeX with hyperref package"}}}