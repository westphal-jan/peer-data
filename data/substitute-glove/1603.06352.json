{"id": "1603.06352", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Online Learning with Low Rank Experts", "abstract": "We consider entire changes of correlation a laboratory your although the recover that their whether well steady - visualization structure: always as limit to form unknown $ congressman $ - element symmetric. We devise methodology. anguish corner that are independent now, many fact experts other appropriate but began entered distinguished $ n $. For one interpolation ford simply show giving stiff apart of $ \\ Theta (\\ sqrt {dT} ) $, including allowing to to gives make of the densities $ tex. $ bijective. For itself humorless designed maybe showing an eastern away though $ O (levin \\ sqrt {T} ) $ taken man dipped traveling and $ \\ Omega (\\ sqrt {dT} ) $.", "histories": [["v1", "Mon, 21 Mar 2016 08:29:05 GMT  (20kb)", "https://arxiv.org/abs/1603.06352v1", null], ["v2", "Mon, 23 May 2016 06:47:33 GMT  (20kb)", "http://arxiv.org/abs/1603.06352v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad hazan", "tomer koren", "roi livni", "yishay mansour"], "accepted": false, "id": "1603.06352"}, "pdf": {"name": "1603.06352.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Low Rank Experts \u030a", "authors": ["Elad Hazan", "Tomer Koren", "Roi Livni", "Yishay Mansour"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n06 35\n2v 2\n[ cs\n.L G\n] 2\n3 M\nay 2\n01 6\n? dT q,\nand extend it to a setting of an approximate d subspace. For the adversarial model we show an upper bound of Opd ? T q and a lower bound of \u2126p ? dT q."}, {"heading": "1 Introduction", "text": "Arguably the most well known problem in online learning theory is the so called prediction with experts advice problem. In its simplest form, a learner wishes to make an educated decision and at each round chooses to take the advice of one of N experts. The learner then suffers a loss between 0 and 1.\nIt is a standard result in online learning that, without further assumptions, the best strategy for the learner will incur \u0398p ? T logNq regret (Cesa-Bianchi and Lugosi, 2006). However, it is natural to assume that while experts are abundant, their decisions are based on common paradigms and that their decision making is based on few degrees of freedom \u2013 for example, if experts are indeed experts, their political bias, social background or school of thought largely dominates their decision making. Experts can also be assets on which the learner wishes to distribute her wealth. In this setting, weather, market condition and interests are dominant factors.\nIt is also sensible to assume that one can exploit this structure to achieve better regret bounds, potentially independent of the actual number of experts while still maintaining a strategy of picking an expert\u2019s advice at each round. Our main result is of this flavor and we show how a learner can exploit hidden structure in the problem in an online setting.\nWe model the problem as follows: We assume that each expert corresponds to a vector ui in Rd space where d is potentially small. Then at each round the experts loss corresponds to a scalar product with a vector vt chosen arbitrarily, and possibly in an adversarial manner. The learner does not observe the chosen embedding of the experts in Euclidean space nor the vectors vt, and can only observe the loss of each expert.\nTo further motivate our setting, let us consider the low rank expert model in the stochastic case. It is well known that for linear predictors in d-dimensional space the regret will be Op ? dT q, independent of the number of experts. Indeed, we show that a simple follow the leader algorithm will achieve this regret bound. In fact, one novelty of this paper is a regret\n\u02daAccepted for presentation at Conference on Learning Theory (COLT) 2016. :Princeton University; email: ehazan@cs.princeton.edu. Parts of this work were done while at Microsoft Research, Herzliya. ;Technion\u2014Israel Institute of Technology and Microsoft Research, Herzliya; email: tomerk@technion.ac.il. \u00a7The Hebrew University of Jerusalem and Microsoft Research, Herzliya; email: roi.livni@mail.huji.ac.il. \u00b6Microsoft Research, Herzliya and Tel Aviv University; email: mansour.yishay@gmail.com.\nbound that depends on an approximate rank \u2013 formally we show that one can improve on the Op ? T logNq regret bound and derive bounds that depend on the approximate rank rather than the number of experts. The non-stochastic setting is more challenging. It is true that for linear predictors in d-\ndimension one can achieve Op ? dT q regret bound even in the non-stochastic case. But the result assumes that learner has access to the geometric structure of the problem, namely, the embedding of the experts in the Euclidean space. Given the embedding one can apply a Follow the Regularized Leader approach with proper regularization to derive the desired regret bound.\nOur main result is a regret minimization algorithm that achieves an Opd ? T q regret in this low d-rank setting, when the learner does not have access to the experts\u2019 embedding in Euclidean space. Our algorithm does not need to know the value of the rank d, and adaptively adapts to it. Thus we demonstrate a regret bound that is independent of number of experts. We accompany this upper bound with an \u2126p ? dT q lower bound.\nOur results are part of a larger agenda in online learning. A working premise in Online Learning is that in most cases the stochastic case is the hardest case. Indeed, the literature is filled with generalization bounds and their analogue regret bounds. However, a striking difference is that the statistical bounds are often achieved using simple ERM algorithms, that are oblivious to any structure in the problem, even if the structure is required for the generalization bounds to be valid. In contrast, to achieve the analogue regret bound, one has to work harder. For finite hypothesis class the logN factor is achieved by a sophisticated algorithm, and for more general convex problems in Euclidean space a problem-specific regularization needs to be invoked in order to achieve optimality. Thus, a key difference is that online algorithms need to be tailored to the structure of the problem. This leads to the disappointing fact that to achieve optimal regret bounds, it is not enough for the problem to be structured but the learner needs to actively understand the structure.\nOur current research is an attempt to better understand this key difference: we wish to understand whether an online linear predictor can somehow exploit the geometry of the problem in an implicit manner, similarly to batch ERM algorithms, and how. For this, we invoke a setting where the learner must choose its predictor without the a-priori ability to devise a regularizer. Our findings so far indeed demonstrate that even without access to the structure the learner can indeed overcome her dependence on the irrelevant parameter N .\nTechnically, one should compare our regret bound of Opd ? T q to the standard regret bound\nof Op ? T logNq. For our bound to be superior one needs that d \u201c op ? logNq; while this can indeed be the case in various settings, our result can be better seen as a first step in a more general research direction. We aim to understand how online algorithms can take advantage of structural assumptions in the losses, without being given any explicit information about it."}, {"heading": "1.1 Related Work", "text": "Low rank assumptions are ubiquitous in the Machine Learning literature. They have been successfully applied to various problems, most notably to matrix completion (Cande\u0300s and Recht, 2009; Foygel and Srebro, 2011; Srebro et al., 2004) but also in the context of classification with missing data (Goldberg et al., 2010; Hazan et al., 2015) and large scale optimization (Shalev-Shwartz et al., 2011).\nA similar problem that was studied in the literature is the Branching Experts Problem (Gofer et al., 2013). In the branching expert problem N potential experts are effectively only k distinct experts, but the clustering of the experts to the k clusters is unknown a-priori. This case can be considered as a special instance of our setting as indeed we can embed each expert as a kdimensional vector. Gofer et al. (2013) proved a sharp \u0398p ? kT q regret (the bound is tight only when k \u0103 c logN for some constant c \u0105 0). It is perhaps worth noting that when effectively only k experts appear, the stochastic bound is Op ? T log kq, thus showing that in this similar problem, it is not true that the stochastic case is the hardest case.\nComplexity measures for online learning. We are not the first to try and understand what is the proper analogue for ERM in the online setting. Notions like the VC-dimension and Rademacher complexities have been extended to notions of Littlestone-Dimension (Littlestone, 1988; Shalev-Shwartz, 2011), and Sequential Rademacher Complexity (Rakhlin et al., 2010) respectively.\nThe SOA algorithm suggested by Ben-David et al. (2009) is a general framework for regret minimization that depends solely on the Littlestone dimension. However, the SOA algorithm is conceptually distinct from an ERM algorithm within our framework: to implement the SOA algorithm, one has to have access to the structure of the class (specifically, one needs to compute the Littlestone dimension of subclasses within the algorithm).\nSequential Rademacher complexity seems like a powerful tool for improving our bounds and answering some of our open problems. There are also advances in constructing effective algorithms within this framework (Rakhlin et al., 2012). However, as the branching expert example shows, there is no general argument that show that structure in the problem leads to stochastic\u2013analogue bounds on the complexity.\nLearning from easy data. In another line of research, which is similar in spirit to ours, several authors attempt to go beyond worst-case analysis in online learning, and provide algorithms and bounds that can exploit deficiencies in the data. Work in this direction includes the study of worst-case robust online algorithms that can also adapt to stochastic i.i.d. data (e.g., Hazan and Kale, 2009; Rakhlin et al., 2013; De Rooij et al., 2014; Sani et al., 2014), as well as the exploration of various structural assumptions that can be leveraged for obtaining improved regret guarantees (e.g., Cesa-Bianchi et al., 2007; Hazan and Kale, 2010, 2011; Chiang et al., 2012; Rakhlin and Sridharan, 2013). However, to the best of our knowledge, low rank assumptions in online learning have not been explored in this context.\nAdaptive online algorithms. Online adaptive learning methods have recently been the topic of extensive study and are effective for large scale stochastic optimization in practice. One of the earliest and most widely used methods in this family is the AdaGrad algorithm (Duchi et al., 2011), a subgradient descent method that dynamically incorporate knowledge of the geometry of the data from earlier iterations. Our problem can be cast into an online linear optimization problem and subgradient descent methods are indeed applicable. It might seem at first sight that adapting the regularization via AdaGrad can lead to desired results. However, the analysis of the AdaGrad algorithm can only yield an Op ? dNT q bound on the regret in our\nlow-rank setting. In fact, a closer inspection reveals that the ? N factor in the latter bound is unavoidable for AdaGrad: as we show in Appendix B, in our setting the regret of AdaGrad is lower bounded by \u2126pmint ? N,T uq."}, {"heading": "2 Problem Setup and Main Results", "text": "We recall the standard adversarial online experts model for T rounds with N experts. At each round t \u201c 1, . . . , T , the learner chooses a probability vector xt P \u2206N , where \u2206N denotes the N -simplex, namely the set of all possible distributions over N experts,\n\u2206N \u201c ! x P RN : @i, xpiq \u011b 0 and \u00ffN i\u201c1 xpiq \u201c 1 ) .\nAn adversary replies by choosing a loss vector \u2113t P r\u00b41, 1sN ,1 and the learner suffers a loss xtp\u2113tq \u201c xt \u00a8\u2113t. The objective of the learner is to minimize her regret, which is defined as follows,\nRegretT \u201c T \u00ff\nt\u201c1 xt \u00a8 \u2113t \u00b4 min iPrNs\nT \u00ff t\u201c1 \u2113tpiq.\nIn the stochastic online experts model, the adversary selects a distribution D over the loss vectors in r\u00b41, 1sN , and at time t a random \u2113t P r\u00b41, 1sN is selected from D. The regret is.\nRegretT \u201c T \u00ff\nt\u201c1 xt \u00a8 Er\u2113ts \u00b4 min iPrNs\nT \u00ff t\u201c1 Er\u2113tpiqs ,\nwhere the expectations are taken over the random loss vectors selected from D. In our setting, we wish to assume that there is a structure over the experts which implies that the loss vectors are structured, and are derived from a low rank subspace. Therefore we will add the following constraint over the adversary: let L P RN\u02c6T be the loss matrix obtained in hindsight (i.e., the t\u2019th column of L is \u2113t). We restrict the feasible strategies for the adversary to only such that satisfy: rankpLq \u201c d . An equivalent formulation of our model is as follows: An adversary chooses at the beginning of the game a matrix U P RN\u02c6d, where each row corresponds to an expert. At round t the adversary chooses a vector vt, and the learner gets to observe \u2113t where \u2113t \u201c Uvt. The objective of the learner remains the same: to choose at each round a probability distribution xt that minimizes the regret. We stress that the learner observes only the loss vectors \u2113t, and does not have access to either U or the vectors vt."}, {"heading": "2.1 Main Results", "text": "We next state the main results of this paper:\nTheorem 1. The T -round regret of Algorithm 2 (described in Section 4 below) is at most Opd ? T q, where d \u201c rankpLq.\nWe remark that a regret upper bound of Op ? T mintd, ? logNuq is attainable by combining the standard multiplicative-updates algorithm with our algorithm.2 Our upper bound is accompanied by the following lower bound.\nTheorem 2. For any online learning algorithm, T and d \u010f log2N , there exists a sequence of loss vectors \u21131, . . . \u2113T P r\u00b41, 1sN such that\nRegretT \u201c T \u00ff\nt\u201c1 xt \u00a8 \u2113t \u00b4 min iPrNs\nT \u00ff t\u201c1 \u2113tpiq \u011b\nc\ndT\n8 ,\nand rankpLq \u201c d. 1As will become apparent later, in our setup it is more natural to consider symmetric r\u00b41, 1s loss values rather than the typical r0, 1s losses. The two variants of the problem are equivalent up to a simple shift and scaling of the losses\u2014a transformation that preserves the rank of the loss matrix.\n2A standard way to accomplish this is by running the two online algorithms in parallel, and choosing between their predictions by treating them as two meta-experts in another multiplicative-weights algorithm."}, {"heading": "3 Preliminaries", "text": ""}, {"heading": "3.1 Notation", "text": "Let In be the n \u02c6 n identity matrix. Let 1n be a vector of length n with all 1 entries. The columns of a matrix U are denoted by u1, u2, . . .. The i\u2019th coordinate of a vector x is denoted by xpiq. For a matrix M , we denote by M : the Moore-Penrose pseudo-inverse of M . For a positive definite matrix H \u0105 0 we will denote its corresponding norm }x}H \u201c ? xTHx, and its\ndual norm }x}\u02daH \u201c ? xTH\u00b41x. Given a positive semi-definite matrix M \u013e 0 its corresponding Ellipsoid is defined as: EpMq \u201c tx : xTM :x \u010f 1u ."}, {"heading": "3.2 Ellipsoidal Approximation of Convex Bodies", "text": "A main tool in our algorithm is an Ellipsoid approximation of convex bodies. Recall John\u2019s theorem for symmetric zero-centered convex bodies.\nTheorem 3 (John\u2019s Theorem; e.g, Ball, 1997). Let K be a convex body in Rd that is symmetric around zero (i.e., K \u201c \u00b4K). Let E be an ellipsoid with minimum volume enclosing K. Then:\n1? d E \u010e K \u010e E .\nWhile computing the minimum volume enclosed ellipsoid is computationally hard, for symmetric convex bodies it can be approximated to within 1` \u01eb factor in polynomial time. Specifically, given as input a matrix A P RN\u02c6d, consider the polytope PA \u201c tx : }Ax}8 \u010f 1u. We have the following.\nTheorem 4 (Gro\u0308tschel et al., 2012, Theorem 4.6.5). There exists a poly-time procedure MVEEpAq that receives as input a matrix A P RN\u02c6d and returns a matrix M such that\n1? 2d EpMq \u010e PA \u010e EpMq."}, {"heading": "3.3 Online Mirror Descent", "text": "Another main tool in our analysis is the well-known Online Mirror Descent algorithm for online convex optimization. The Online mirror descent is a subgradient descent method for optimization over a convex set in Rd that implies a regularization factor, chosen a-priori. In Algorithm 1 we describe the algorithm for the special case where the convex set is \u2206N and the regularization function is chosen to be } \u00a8 }2H for some input matrix H \u0105 0:\nAlgorithm 1 OMD: Online Mirror Descent\n1: input: H \u0105 0, t\u03b7tuTt\u201c1, x1 P \u2206N . 2: for t \u201c 1 to T do 3: Play xt 4: Suffer cost xt \u00a8 \u2113t and observe \u2113t 5: Update\nxt`1 \u201c arg min xP\u2206N \u2113t \u00a8 x` \u03b7\u00b41t }x\u00b4 xt}2H .\n6: end for\nThe regret bound of the algorithm is dependent on the choice of regularization and is given as follows:\nLemma 5 (e.g., Hazan, 2015). The T -round regret of the OMD algorithm (Algorithm 1) is bounded as follows:\nT \u00ff t\u201c1 \u2113t \u00a8 xt \u00b4 T \u00ff t\u201c1 \u2113t \u00a8 x\u02da \u010f 1 \u03b7T }x1 \u00b4 x\u02da}2H ` 1 2 T \u00ff t\u201c1 \u03b7tp}\u2113t}\u02daHq2 ."}, {"heading": "3.4 Rademacher Complexity", "text": "Our tool to analyze the stochastic case will be the Rademacher Complexity, specifically we will use it to bound the regret of a \u201cFollow The Leader\u201d algorithm (FTL). Recall that the FTL algorithm selection rule is defined as follows:\nxt \u201c arg min xP\u2206N\nt\u00b41 \u00ff i\u201c1 \u2113i \u00a8 x.\nOne way to bound the regret of the FTL algorithm in the stochastic case is by bounding the Rademacher complexity of the feasible samples. Recall that the Rademacher Complexity of a class of target function F over a sample St \u201c t\u21131, . . . , \u2113tu is defined as follows\nRpF , Stq \u201c E\u03c3 \u00ab\nsup fPF\n1\nt\nt \u00ff i\u201c1 \u03c3ifp\u2113iq\nff\n,\nwhere \u03c3 P t\u00b41, 1ut are i.i.d. Rademacher distributed random variables. The following bound is standard and well known, and for completeness we provide a proof in Appendix A.1.3\nLemma 6. Let K be a symmetric convex set centered around zero in Rd. Recall that the dual set K\u02da is defined as follows:\nK\u02da \u201c tx : sup yPK |y \u00a8 x| \u010f 1u.\nLet St \u201c t\u21131, . . . , \u2113tu \u010e K and let F \u010e \u03b1K\u02da be a subclass of linear functions, then:\nRpF , Stq \u010f \u03b1 c d\nt .\nAnother standard bound applies to the case where F is bounded in the l1-norm.\nLemma 7 (Kakade et al., 2009). Let St \u201c t\u2113\u03021, . . . , \u2113\u0302tu P RN and let F1 be a subclass of linear functions such that supt}f}1 : f P Fu \u010f 1, then:\nRpF1, Stq \u010f max i\n}\u2113\u0302i}8 c 2 logN\nt .\nThe Rademacher complexity is a powerful tool in statistical learning theory and it allows us to bound the generalization error of an FTL algorithm. Namely, for every sample St \u201c t\u21131, . . . , \u2113tu denote:\nfS \u201c argmin fPF\nt \u00ff i\u201c1 fp\u2113iq.\nThen we have the following bound for every f\u02da P F (for i.i.d. loss vectors; see for example Shalev-Shwartz and Ben-David, 2014):\nE St\u201eD E \u2113\u201eD rfStp\u2113q \u00b4 f\u02dap\u2113qs \u010f 2 E St\u201eD rRpF , Stqs .\n3Surprisingly, we could not find any specific reference that precisely derives it.\nApplying this to FTL in the experts setting we have, in terms of regret, that for any x\u02da:\nE\n\u00ab\nT \u00ff t\u201c1 \u2113t \u00a8 xt \u00b4 \u2113t \u00a8 x\u02da\nff\n\u201c T \u00ff\nt\u201c1 E \u21131,...,\u2113t\u00b41\u201eD E\n\u2113t\u201eD r\u2113t \u00a8 xt \u00b4 \u2113t \u00a8 x\u02das\n\u010f 2 T \u00ff\nt\u201c1 E St\u00b41\u201eD rRp\u2206N , St\u00b41qs . (1)"}, {"heading": "4 Upper Bound", "text": "In this section we discuss our online algorithm for the adversarial model, which is given in Algorithm 2. The algorithm is a version of Online Mirror Descent with adaptive regularization. It maintains a positive-definite matrix H, which is being updated whenever the newly observed loss vector \u2113t is not in the span of previously appeared losses. In all other time steps\u2014i.e., when \u2113t remains in the previous span\u2014the algorithm preforms an Online Mirror Descent type update (see Algorithm 1), with the function }x}2H \u201c xTHx as a regularizer.\nThe algorithm updates the regularization matrix H so as to adapt to the low-dimensional geometry of the set of feasible loss vectors. Indeed, as our analysis below reveals, H is an ellipsoidal approximation of a certain low-dimensional convex set in RN to which the loss vectors \u2113t can be localized. This low-dimensional set is the intersection of the unit cube in N dimensions\u2014 in which the loss vectors \u2113t reside by definition\u2014and the low dimensional subspace spanned by previously observed loss vectors, given by spanpUq. Whenever the latter subspace changes, namely, once a newly observed loss vector leaves the span of previous vectors, the ellipsoidal approximation is recomputed and the matrix H is updated accordingly.\nAlgorithm 2 Online Low Rank Experts\n1: Initialize: x1 \u201c 1N 1N , \u03c4 \u201c 0, k \u201c 0, U \u201c tu 2: for t \u201c 1 to T do 3: Observe \u2113t, suffer cost xt \u00a8 \u2113t. 4: if \u2113t R spanpUq then 5: Add \u2113t as a new column of U , reset \u03c4 \u201c 0, and set k \u00d0 k ` 1. 6: Compute M \u201c MVEEpUTq and H \u201c In ` UTMU . 7: end if 8: let \u03c4 \u00d0 \u03c4 ` 1 and \u03b7t \u201c 4 a k{\u03c4 , and set:\nxt`1 \u201c arg min xP\u2206N \u2113t \u00a8 x` \u03b7\u00b41t }x\u00b4 xt}2H .\n9: end for\nTo derive Theorem 1, we begin with analyzing a simpler case where the learner is aware of the subspace from which losses are derived. Specifically, assume that at the beginning of the rounds, the learner is equipped with a rank d matrix U such that for all losses \u21131, \u21132, . . . P spanpUq where we denote by spanpUq the span of the columns of the matrix U .\nIn this simplified setting, we can obtain a regret bound of Op ? dT q via John\u2019s theorem (Theorem 3).4 As discussed above, the loss vectors \u21131, . . . , \u2113T can be localized to the intersection of the unit cube in N dimensions with the d-dimensional subspace spanned by the columns of U . Then, John\u2019s theorem asserts that the minimal-volume enclosing ellipsoid of the intersection is a ? d-approximation to the set of feasible loss vectors.\n4We remark that for the simplified setting, the Op ? dT q regret bound is in fact tight, as our \u2126p ? dT q lower bound (given in Section 5) applies in a setting where the subspace of the loss vectors is known a-priori to the learner.\nTheorem 8. Run Algorithm 1 with Input H, t\u03b7tu and x1 defined as follows: (i) H \u201c In ` UTMU , where M \u201c MVEEpUTq, (ii) \u03b7t \u201c 4 a\nd{t, where d \u201c rankpUq, and (iii) x1 P \u2206 is arbitrary. If \u21131, . . . , \u2113T P spanpUq , then the expected T -round regret of the algorithm is at most 8 ? dT .\nProof. Consider the d-dimensional polytope\nP \u201c tv P Rd : }UTv}8 \u010f 1u.\nThen by John\u2019s Theorem (Theorem 3), we have,\nEp 1 2d Mq \u010e P \u010e EpMq . (2)\nIn order to apply Lemma 5, we need to bound both }\u2113t}\u02daH and }x1 \u00b4 x\u02da}2H . We first bound the norms }\u2113t}\u02daH . Notice that for each loss vector \u2113t there exists vt P P such that \u2113t \u201c UTvt (as \u2113t P spanpUq and }\u2113t}8 \u010f 1). Thus, we can write,\np}\u2113t}\u02daHq2 \u201c \u2113Tt H\u00b41\u2113t \u201c vTt UpIn ` UTMUq\u00b41UTvt \u010f vTt UpUTMUq:UTvt \u201c vTt M\u00b41vt ,\nwhere we have used Lemma 11 (see Appendix A). Now, since vt P P and EpMq is enclosing P , we obtain vTt M \u00b41vt \u010f 1. This proves that\np}\u2113t}\u02daHq2 \u010f 1.\nNext we bound }x1 \u00b4 x\u02da}H \u010f 2 Since }x1 \u00b4 x\u02da}H \u010f 2maxxP\u2206n }x}H , it suffices to bound maxxP\u2206n }x}H . Hence, our goal is to show that }x}H \u010f 2 ? d for all x P \u2206n. Since }x}2H \u201c 1` 2d }x}2H 1 with H 1 \u201c 12dUTMU , it is enough to bound the norm }x}2H 1 . Given a convex set in R d, recall that the dual set is given by\nP \u02da \u201c tx : sup pPP |x \u00a8 p| \u010f 1u.\nThe dual of an ellipsoid EpMq is given by pEpMqq\u02da \u201c EpM\u00b41q and it is standard to show that Eq. (2) implies in the dual:\npEpMqq\u02da \u010e P \u02da \u010e pEp 1 2d Mqq\u02da.\nTaken together we obtain that P \u02da \u010e Ep2dM\u00b41q. Note that by definition the columns of U are in P \u02da, hence, for every ui, }ui}2M \u010f 2d. Since x P \u2206N ,\n}x}2H 1 \u201c 12d}Ux} 2 M \u010f 12d maxi }ui} 2 M \u010f 1 .\nEquipped with the bounds }x}H \u010f ? 1` 2d \u010f 2 ? d for all x P \u2206n and }\u2113t}\u02daH \u010f 1 for all t, we are now ready to analyze the regret of the algorithm, which via Lemma 5 can be bounded as follows:\nRegretT \u201c T \u00ff\nt\u201c1 \u2113t \u00a8 xt \u00b4\nT \u00ff t\u201c1 \u2113t \u00a8 x\u02da\n\u010f 1 \u03b7T }x1 \u00b4 x\u02da}2H ` 1 2\nT \u00ff t\u201c1 \u03b7tp}\u2113t}\u02daHq2 7 Lemma 5\n\u010f 4 \u03b7T max xP\u2206n }x}2H ` 1 2\nT \u00ff t\u201c1 \u03b7tp}\u2113t}\u02daHq2 7 }x1 \u00b4 x\u02da}H \u010f 4 max xP\u2206n }x}H\n\u010f 16d \u03b7T ` 1 2\nT \u00ff t\u201c1 \u03b7t 7 max xP\u2206n }x}2H \u010f 4d, }\u2113t}\u02daH \u010f 1.\nA choice of \u03b7t \u201c 4 a d{t, together with the inequality \u0159Tt\u201c1 1{ ? t \u010f 2 ? T , gives the theorem.\nThe d-low rank setting does not assume that the learner has access to the subspace U , and potentially an adversary may adapt her choice of subspace to the learner\u2019s strategy. However, the learner can still obtain regret bounds that are independent of the number of experts. We are now ready to prove Theorem 1.\nProof of Theorem 1. Let t0 \u201c 1, td`1 \u201c T and for all 1 \u010f k \u010f d let tk be the round where the k\u2019th column is added to U . Also, let Tk \u201c tk`1 \u00b4 tk the length of the k\u2019th epoch. Notice that between rounds tk and tk`1 the algorithm\u2019s execution is identical to Algorithm 1 with input depicted in Theorem 8. Therefore its regret in this time period is at most 8 ? kTk. The total regret is then bounded by\n8 d \u00ff\nk\u201c0\na\nkTk \u010f 8\ng f f e d \u00ff\nk\u201c0 k \u00a8\ng f f e d \u00ff\nk\u201c0 Tk \u010f 8d\n? T ,\nand the theorem follows."}, {"heading": "4.1 Stochastic Online Experts", "text": "We now turn to analyze the regret in the stochastic model, where the loss vectors \u2113t are chosen i.i.d. from some unknown distribution. In this case we can achieve a right regret bound of Op ? dT q using a simple \u201cFollow The Leader\u201d (FTL) algorithm. We will in fact show an even stronger result for the stochastic case, that an approximate rank is enough to bound the complexity. Recall that the approximate rank, rank\u01ebpLq, of a matrix is defined as follows (see Alon et al., 2013): rank\u01ebpLq \u201c mintrankpL1q : }L1 \u00b4 L}8 \u0103 \u01ebu. The following statement is the main result for this section:\nTheorem 9. Assume that an adversary chooses her losses t\u2113tu i.i.d. from some distribution D supported on r\u00b41, 1sN . Then the T -round regret of the FTL algorithm is bounded by:\nRegretT \u010f 8E \u201d a T \u00a8 rank\u01ebpLq \u0131 ` \u01eb a T logN,\nfor every 0 \u010f \u01eb \u0103 1. In particular, if rankpLq \u010f d almost surely, then RegretT \u201c Op ? dT q.\nProof. Our proof relies on Eq. (1) and a bound for Rp\u2206N , Stq. Fix a sequence ST \u201c t\u21131, . . . , \u2113T u and let d \u201c rank\u01ebpLq and let U be N \u02c6 d matrix such that\nL \u201c UV ` L\u0302 ,\nwhere maxi,j |L\u0302i,j| \u0103 \u01eb. We will denotes the columns of L\u0302 by \u2113\u03021, . . . \u2113\u0302N . We define a symmetric convex set centered around zero in Rd as follows:\nK \u201c tv : supi |ui \u00a8 v| \u010f 2u .\nNote that for every vt we have that vt P K if \u01eb \u010f 1. By definition of the set we have: ui P 2K\u02da for every i. One can verify that K\u02da is convex, hence if we let F \u201c convpu1, . . . , uN q we have that F \u010e 2K\u02da. We can think of F as a linear function space, where fupvq \u201c u \u00a8 v. It follows by Lemma 6 that RpF , Stq \u010f a 2d{t. Finally,\nRp\u2206N , Stq \u201c E \u00ab\nsup xP\u2206N\nt \u00ff\ni\u201c1\n1 t \u03c3ix \u00a8 \u2113i\nff \u010f E \u00ab\nsup xP\u2206N\n1\nt\nt \u00ff i\u201c1 \u03c3ix \u00a8 Uvi\nff ` E \u00ab\nsup xP\u2206N\n1\nt\nt \u00ff i\u201c1 \u03c3ix \u00a8 \u2113\u0302\nff\n.\nNext, we have:\nE\n\u00ab\nsup xP\u2206N\n1\nt\nt \u00ff i\u201c1 \u03c3ix \u00a8 Uvi\nff \u201c E \u00ab\nsup xP\u2206N\n1\nt\nt \u00ff i\u201c1 \u03c3ipUTxq \u00a8 vi\nff\n\u201c sup fPconvpuiq E\n\u00ab\n1\nt\nt \u00ff i\u201c1 \u03c3ifpviq\nff\n\u201c RpF , Stq \u0103 2 c d\nt . (3)\nand by Lemma 7,\nE sup xP\u2206N\n\u00ab\n1\nt\nt \u00ff i\u201c1 \u03c3ix \u00a8 \u2113\u0302i\nff\n\u010f \u01eb c 2 logN\nt . (4)\nTaking Eq. (3) and Eq. (4) together, we have:\nRp\u2206N , Stq \u010f 2 c rank\u01eb L\nt ` \u01eb\nc\n2 logN\nt .\nThe statement now follows from Eq. (1)."}, {"heading": "5 Lower Bound", "text": "We now prove Theorem 2. For our proof we will rely on lower bounds for online learning of hypotheses classes with respect to the Littlestone dimension (see Shalev-Shwartz, 2011). For a class H of target functions h : X \u00d1 t0, 1u, the Littlestone dimension LdimpHq measures the complexity, or online learnability, of the class.\nTo define LdimpHq one considers trees whose internal nodes are labeled by instances. Any branch in such a tree can be described as a sequence of examples px1, y1q, . . . , pxd, ydq where xi is the instance associated with the ith node in the path, and yi is 1 if xi`1 is the right child of the i\u2013th node, and yi \u201c 0 otherwise. LdimpHq is then defined as the depth of the largest binary tree that is shattered by H. An instance-labeled tree is said to be shattered by a class H if for any root-to-leaf path px1, y1q, . . . , pxd, ydq there is some h P H such that hpxiq \u201c yi.\nTo prove Theorem 2, we need the following result about the Littlestone dimension.\nLemma 10 (Ben-David et al., 2009). Let H be any hypothesis class with finite LdimpHq, where Ldim is the Littlestone-dimension of a class H. For any (possibly randomized) algorithm, there exists a sequence of labeled instances pv1, y1q, . . . , pvT , yT q with yt P t0, 1u such that\nE\n\u00ab\nT \u00ff t\u201c1 |y\u0302t \u00b4 yt|\nff\n\u00b4min hPH\nT \u00ff i\u201c1 |hpxtq \u00b4 yt| \u011b\nc\nLdimpHqT 8 .\nwhere y\u0302t is the algorithm\u2019s output at iteration t.\nProof of Theorem 2. We let H be the 2d vertices of the d-dimensional hypercube. We define a function class F over the domain X \u201c te1, . . . , edu of standard basis vectors. A function fu P F , is labeled by u P H, and defined over the set of basis vector ej , as follows,\nfupejq \u201c # 0 if upjq \u201c \u00b41, 1 if upjq \u201c 1.\nOne can verify that LdimpFq \u201c d. For each ui P H and y P t0, 1u, we can write\n|fuipejq \u00b4 y| \u201c 1\u00b4 p2y \u00b4 1q \u00a8 ui \u00a8 ej\n2 .\nBy Lemma 10, we deduce that for any algorithm, there exists a sequence pv1, y\u03041q, . . . , pvT , y\u0304T q of standard basis vectors v1, . . . , vT and y\u03041, . . . y\u0304T P t\u00b41, 1u such that:\nT \u00ff\nt\u201c1\n\u00ff\ni\nxtpiqui \u00a8 py\u0304tvtq \u00b4min u\nT \u00ff i\u201c1 u \u00a8 py\u0304tvtq \u011b 2\nc\ndT\n8 . (5)\nWe now consider an adversary that chooses U as his expert matrix, and at round t the learner observes \u2113t \u201c Upy\u0304tvtq. The lower bound now follows from Eq. (5); the fact that rankpLq \u201c d follows from the fact that our experts are embedded in Rd."}, {"heading": "6 Discussion and Open Problems", "text": "We considered the problem of experts with a hidden low rank structure. Our findings are that in the non-stochastic case, similar to the stochastic case, the regret bounds are independent of the number of experts. The most natural question is then to bridge the gap between the upper and lower bounds: Open Problem 1. Is there an algorithm that can achieve regret Op ? dT q for any sequence\n\u21131, . . . , \u2113T such that rankpLq \u201c d? Alternatively, can one prove a lower bound of \u2126pd ? T q?\nAs discussed, our agenda is more general than the low-rank setting. Our aim is to construct new online algorithms that can exploit structure in the data, without explicit information on the structure. Other settings can also be considered within our framework.\nAnother interesting setting, that avoids dependence in dimension, is to assume that experts are embedded in a Hilbert space. By isomorphisms of Hilbert spaces this is equivalent to an adversary that chooses an expert embedding matrix U P RN\u02c6N such that for every ui we have }ui}2 \u010f 1 and correspondingly at each time point we receive a vector vt such that }vt}2 \u010f 1 as a result we have a factorization:\nL \u201c UVT, }U}2,8, }V }2,8 \u010f 1,\nwhere }X}2,8 \u201c sup}y}\u010f1 }Xy}8. Recall the definition of the max-norm, also called the \u03b32-norm (Srebro and Shraibman, 2005):\n}L}max \u201c min UVT\u201cL }U}2,8 \u00a8 }V }2,8.\nThus, similar to the low rank setting we can define this setting as follows: At each round a learner chooses xt P \u2206N , an adversary replies by choosing a loss vector \u2113t, and the learner incurs the corresponding loss. The adversary is restricted to strategies such that\n}L}max \u010f 1.\nThe importance of this setting is that the proper generalization bound for this case is dimension independent (e.g., Kakade et al., 2009). Hence, we ask the following question: Open Problem 2. Is there an algorithm that can achieve regret Op ? T q for any sequence \u21131, . . . , \u2113T such that }L}max \u010f 1? We can also generalize this setting to any pair of norms, } \u00a8 } and its dual } \u00a8 }\u02da, where the description of the game remains the same. The adversary chooses an embedding U of the experts with bounded } \u00a8 } norm. Then, at each round he chooses a set of vectors vt with } \u00a8 }\u02da bounded norm.\nFinally, a different interesting direction to pursue in future work is to extend the noisy result to the adversarial setting. Namely, Open Problem 3. Is there an algorithm that can achieve regret Op ? dT ` \u01eb ? T logNq for any sequence \u21131, . . . , \u2113T such that rank\u01ebpLq \u010f d?"}, {"heading": "A Technical Proofs", "text": "Lemma 11. Let M P Rd\u02c6d, U P Rd\u02c6n such that M \u0105 0 and U . Then\nUpUTMUq:UT \u201c M\u00b41.\nProof. Let N \u201c M1{2U . Then, we have NpNTNq:NT \u201c Id. To see this, write the SVD decomposition N \u201c O\u03a3VT with diagonal non-singular \u03a3 P Rd\u02c6d and OOT \u201c OTO \u201c VTV \u201c Id. Then, NpNTNq:NT \u201c O\u03a3VTpV \u03a32VTq:V \u03a3OT \u201c O\u03a3VTpV \u03a3\u00b42VTqV \u03a3OT \u201c Id. Expanding the definition of N , we get M1{2UpUTMUq:UTM1{2 \u201c Id, and since M1{2 is nonsingular, we can multiply by M\u00b41{2 on both sides and obtain the lemma.\nA.1 Proof of Lemma 6\nThe proof relies on the following corollary of John\u2019s Theorem:\nLemma 12. Let K be a symmetric convex set centered around zero in Rd. There exists a positive semi-definite matrix \u03a3 such that for every x P K:\nxT\u03a3x \u010f sup fPK\u02da |fpxq|2 \u010f dpxT\u03a3xq .\nProof. (of Lemma 6). wlog we assume \u03b1 \u201c 1, the general case follows since Rp\u03b1F , Sq \u201c \u03b1RpF , Sq. We have\nRpF , Sq \u201c E\u03c3 \u00ab\nsup fPF\n1\nt\nt \u00ff i\u201c1 \u03c3ifp\u2113iq\nff \u201c E\u03c3 \u00ab\nsup fPF fp1 t\nt \u00ff i\u201c1 \u03c3i\u2113iq\nff\n\u010f\ng f f eE\u03c3 \u00ab\nsup fPF f2p1 t\nt \u00ff i\u201c1 \u03c3ip\u2113iqq\nff\n.\nNext, we take \u03a3 whose existence follows from Lemma 12. Note that \u03a3 defines a scalar product. Specifically let us denote x\u2113i, \u2113jy \u201c \u2113Ti \u03a3\u2113j, and also we let }\u2113i}22 \u201c \u2113Ti \u03a3\u2113i. Then we have\ng f f eE\u03c3 \u00ab\nsup fPK\u02da f2\n\u02dc\n1\nt\nt \u00ff i\u201c1 \u03c3i\u2113i\n\u00b8ff\n\u010f\ng f f f edE\u03c3 \u00bb \u2013 1\nt2\n\u203a \u203a \u203a \u203a \u203a t \u00ff\ni\u201c1 \u03c3i\u2113i\n\u203a \u203a \u203a \u203a \u203a 2\n2\nfi\nfl\n\u201c\ng f f edE\u03c3 \u00ab 1\nt2\nt \u00ff\ni,j\u201c1 \u03c3i\u03c3jx\u2113i, \u2113jy\nff\n\u201c\ng f f edE\u03c3 \u00ab 1\nt2\nt \u00ff i\u201c1 \u03c32}\u2113i}22\nff\n\u201c\ng f f e d\nt2\nt \u00ff i\u201c1 }\u2113i}22\n\u010f c d\nt max i }\u2113i}22 \u010f\nd\nd t max i sup fPK\u02da\nf2p\u2113iq \u010f c d\nt ,\nas claimed."}, {"heading": "B Lower Bounds for the AdaGrad Algorithm", "text": "AdaGrad (see Algorithm 3) is an algorithm that adapts the regularization matrix with respect to prior losses. Our aim in this section is to show that this learning scheme of the regularization cannot lead to a regret bound that is independent of the number of experts. Our strategy is as follows: since the AdaGrad algorithm depends on a learning rate parameter \u03b7 we consider two cases: either \u03b7 scales with N and becomes smaller, but then we show that for some sequence the algorithm\u2019s update is \u201ctoo slow\u201d. On the other hand, we show that if \u03b7 does not scale with N , the algorithm becomes less stable, and we can again inflict damage. Taken together we prove the following statement:\nTheorem 13. Consider Algorithm 3. For concreteness we assume that x1 \u201c 1N 1. For sufficiently large N , if T \u0103 ? N{6 then there exist a sequence t\u21131, . . . , \u2113T u such that RegretT \u011b T {2 and rankpLq \u201c 1.\nLemma 14. Consider Algorithm 3 with arbitrary \u03b7 and \u03b4. For concreteness we assume that x1 \u201c 1N 1. For sufficiently large N , if T \u010f max ` 1 36\u03b72 `2 ? \u03b4 6\u03b7 , \u03b72N\u00b4\u03b4 \u02d8 then there exist a sequence \u21131, . . . , \u2113T such that RegretT \u011b T {2 and rankpLq \u201c 1. Proof. We prove each bound separately.\nAlgorithm 3 AdaGrad\n1: Input: \u03b7, \u03b4, x1 P \u2206N . 2: Initialize: S0 \u201c G0 \u201c \u03b4I 3: for t \u201c 1 to T do 4: Observe \u2113t, suffer cost xt \u00a8 \u2113t. 5: set\nSt \u201c St\u00b41 ` \u2113t\u2113Tt , Gt \u201c S 1{2 t yt`1 \u201c xt \u00b4 \u03b7G\u00b41t \u2113t xt`1 \u201c arg min\nxP\u2206N }yt`1 \u00b4 x}2Gt\n6: end for\nCase 1: T \u0103 1 36\u03b72\n` 2 ? \u03b4\n6\u03b7 . We let \u2113t \u201c e \u201c p\u00b41, 1N\u00b41 , 1N\u00b41 , 1N\u00b41 , . . . 1N\u00b41q for all t. For every t\nwe have that Gt \u201c a\nteeT ` \u03b4I and\n\u03b7G\u00b41t \u2113t \u201c \u03b7?\nt` \u03b4}e} e.\nNext we use the inequality:\nT \u00ff\nt\u201c1\n1? t` \u03b4 \u010f \u017c T\n0\n1? t` \u03b4\ndt \u201c 2 \u00b4? T ` \u03b4 \u00b4 ? \u03b4 \u00af .\nFor T \u010f \u00b4 1 6\u03b7 ` ? \u03b4 \u00af2 \u00b4 \u03b4 \u201c 1 36\u03b72 ` 2 ? \u03b4 6\u03b7 , we have that:\n\u00b4? T ` \u03b4 \u00b4 ? \u03b4 \u00af\n\u0103 1 6\u03b7 ;\n1 N ` \u03b7}e}\nT \u00ff\ni\u201c1\n1? t` \u03b4 \u010f 1 N ` 2\u03b7}e} \u00b4? T ` \u03b4 \u00b4 ? \u03b4 \u00af \u010f 1 2 ,\nwhere last inequality follows since }e} \u0105 1 and we assume N \u011b 6. One can observe that our update rule does not take yt out of the simplex \u2206N and we have\nxt \u201c 1 N 1\u00b4 \u03b7}e} \u00ff 1? t` \u03b4 e,\nand further, xtp1q \u0103 12 . In hindsight xtp1q suffers loss \u00b4T while all other experts suffer positive loss. Hence the algorithm\u2019s regret is at least\nRegretT \u011b T\n2 .\nCase 2: T \u0103 \u03b72N \u00b4 \u03b4. We now choose e \u201c p`1,`1,`1,`1,`1,`1 loooooooooooooomoooooooooooooon\nN{2 times\n,\u00b41,\u00b41,\u00b41,\u00b41,\u00b41,\u00b41 loooooooooooooomoooooooooooooon\nN{2 times\nq.\nand let \u2113t \u201c p\u00b41qt`1e.\nAs before note that\n\u03b7G\u00b41t \u2113t \u201c p\u00b41qt`1\u03b7? t` \u03b4}e} e \u201c p\u00b41q t`1\u03b7 a Npt` \u03b4q e.\nWe claim that for ? t` \u03b4 \u0103 \u03b7 ? N and t \u0105 1 we have that:\nxt \u201c 2\nN\n$\n\u2019 \u2019 \u2019 &\n\u2019 \u2019 \u2019 %\np1, 1, 1, 1, 1, 1 loooooomoooooon\nN{2 times\n, 0, 0, 0, 0, 0, 0 loooooomoooooon\nN{2 times\nq t is even,\np0, 0, 0, 0, 0, 0 loooooomoooooon\nN{2 times\n, 1, 1, 1, 1, 1, 1 loooooomoooooon\nN{2 times\nq t is odd. (6)\nHence xt\u2113t \u201c 1 and since the cumulative loss of each expert is at most 1 we have that:\nRegretT \u011b T\n2 .\nTo see that Eq. (6) holds, we will show the statement for x2 other cases are easier and follow the same proof: y1 \u201c 1 1N ` \u03b1e, where |\u03b1| \u0105 1?N , hence it has the form\nyt \u201c pa, a, a, a, a, a loooooomoooooon\nN{2 times\n,\u00b4b,\u00b4b,\u00b4b,\u00b4b,\u00b4b loooooooooomoooooooooon\nN{2 times\nq\nwhere a\u00b4 b \u201c 2{N and a, b \u0105 0. The statement now follows from Lemma 15 (see below).\nProof of Theorem 13. By Lemma 14 we need to show that\nmin \u03b7,\u03b4 max\n\u02dc\n1\n9\u03b72 ` 2\n? \u03b4 3\u03b7 , \u03b72N \u00b4 \u03b4 \u00b8 \u0105 ? N 6 .\nTo prove this, we note that since both terms in the max are monotone in both variables, the minimum is attained when there is equality, i.e., the minimal \u03b7, \u03b4 satisfy:\n1\n36\u03b72 ` 2\n? \u03b4 6\u03b7 \u201c \u03b72N \u00b4 \u03b4.\nSince 1 36\u03b72\n` 2 ? \u03b4\n6\u03b7 ` \u03b4 \u201c\n\u00b4\n1 6\u03b7 `\n? \u03b4 \u00af2 , we get:\n? N \u201c 1\n\u03b7\n\u02c6\n1 6\u03b7 ` ? \u03b4 \u02d9 ,\nand we have that: ? N\n6 \u201c\n? 2\n36\u03b72 `\n? \u03b4 6\u03b7 \u0103 ? 2 36\u03b72 ` 2 ? \u03b4 6\u03b7 .\nIt remains to prove Lemma 15, that was used for the proof of Lemma 14.\nLemma 15. Let y \u201c pa, a, a, a, a, a loooooomoooooon\nN{2 times\n,\u00b4b,\u00b4b,\u00b4b,\u00b4b,\u00b4b loooooooooomoooooooooon\nN{2 times\nq where a, b \u011b 0 and assume that a\u00b4 b \u201c\n2{N . Let Gt \u201c ? \u03b4I ` \u03b1eeT\u00b41{2 for some \u03b1 \u0105 0, where\ne \u201c p`1,`1,`1,`1,`1,`1 loooooooooooooomoooooooooooooon\nN{2 times\n,\u00b41,\u00b41,\u00b41,\u00b41,\u00b41,\u00b41 loooooooooooooomoooooooooooooon\nN{2 times\nq .\nThen\nmin xP\u2206N\n1 2 }y \u00b4 x}2Gt \u201c 2 N p1, 1, 1, 1, 1, 1 loooooomoooooon\nN{2 times\n, 0, 0, 0, 0, 0 loooomoooon\nN{2 times\nq .\nProof. Considering the Lagrangian and KKT conditions, we observe that x minimizes the distance iff the following hold:\n1. x P \u2206N (primal feasibility)\n2. \u03bb \u0105 0 and \u03b8p1q \u201c \u03b8p2q \u201c \u00a8 \u00a8 \u00a8 \u201c \u03b8pNq. (dual feasibility)\n3. x \u201c y `G\u00b41t p\u03bb` \u03b8q (stationarity)\n4. xpiq \u2030 0 \u00f1 \u03bbpiq \u201c 0 and \u03bbpiq \u2030 0 \u00f1 xpiq \u201c 0. (complementary slackness)\nNext note that e is an eigenvector of Gt and we have for some c \u0103 0 that\nG\u00b41t ce \u201c p\u00b4b,\u00b4b,\u00b4b,\u00b4b,\u00b4b,\u00b4blooooooooooooomooooooooooooon N{2 times ,`b,`b,`b,`b,`b loooooooooomoooooooooon N{2 times q.\nNow we can write\nce \u201c p0, 0, . . . , 0, 0 loooooomoooooon\nN{2 times\n,\u00b42c,\u00b42c, . . . ,\u00b42c,\u00b42c loooooooooooooomoooooooooooooon\nN{2 times\nq\nloooooooooooooooooooooooomoooooooooooooooooooooooon\n\u03bb\n`pc, c, c, c, c, c looooomooooon\nN{2 times\n, c, c, c, c, c, c looooomooooon\nN{2 times\nq\nlooooooooooooooomooooooooooooooon\n\u03b8\n,\nthat concludes the proof."}], "references": [{"title": "The approximate rank of a matrix and its algorithmic applications: approximate rank", "author": ["N. Alon", "T. Lee", "A. Shraibman", "S. Vempala"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Alon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2013}, {"title": "An elementary introduction to modern convex geometry", "author": ["K. Ball"], "venue": "Flavors of geometry,", "citeRegEx": "Ball.,? \\Q1997\\E", "shortCiteRegEx": "Ball.", "year": 1997}, {"title": "Agnostic online learning", "author": ["S. Ben-David", "D. P\u00e1l", "S. Shalev-Shwartz"], "venue": "In COLT. Citeseer,", "citeRegEx": "Ben.David et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2009}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "Online optimization with gradual variations", "author": ["C.-K. Chiang", "T. Yang", "C.-J. Lee", "M. Mahdavi", "C.-J. Lu", "R. Jin", "S. Zhu"], "venue": "In COLT,", "citeRegEx": "Chiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2012}, {"title": "Follow the leader if you can, hedge if you must", "author": ["S. De Rooij", "T. Van Erven", "P.D. Gr\u00fcnwald", "W.M. Koolen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Rooij et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rooij et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Concentration-based guarantees for low-rank matrix reconstruction", "author": ["R. Foygel", "N. Srebro"], "venue": "arXiv preprint arXiv:1102.3923,", "citeRegEx": "Foygel and Srebro.,? \\Q2011\\E", "shortCiteRegEx": "Foygel and Srebro.", "year": 2011}, {"title": "Regret minimization for branching experts", "author": ["E. Gofer", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Gofer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gofer et al\\.", "year": 2013}, {"title": "Transduction with matrix completion: Three birds with one stone", "author": ["A. Goldberg", "B. Recht", "J. Xu", "R. Nowak", "X. Zhu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goldberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2010}, {"title": "Geometric algorithms and combinatorial optimization, volume 2", "author": ["M. Gr\u00f6tschel", "L. Lov\u00e1sz", "A. Schrijver"], "venue": "Springer Science & Business Media,", "citeRegEx": "Gr\u00f6tschel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00f6tschel et al\\.", "year": 2012}, {"title": "Introduction to Onlne Convex Optimization, Draft", "author": ["E. Hazan"], "venue": "Publishers Inc.,", "citeRegEx": "Hazan.,? \\Q2015\\E", "shortCiteRegEx": "Hazan.", "year": 2015}, {"title": "On stochastic and worst-case models for investing", "author": ["E. Hazan", "S. Kale"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hazan and Kale.,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2009}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["E. Hazan", "S. Kale"], "venue": "Machine learning,", "citeRegEx": "Hazan and Kale.,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2010}, {"title": "Better algorithms for benign bandits", "author": ["E. Hazan", "S. Kale"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Classification with low rank and missing data", "author": ["E. Hazan", "R. Livni", "Y. Mansour"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2015}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T.S. Jaakkola"], "venue": null, "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "It is a standard result in online learning that, without further assumptions, the best strategy for the learner will incur \u0398p ? T logNq regret (Cesa-Bianchi and Lugosi, 2006).", "startOffset": 143, "endOffset": 174}, {"referenceID": 3, "context": "They have been successfully applied to various problems, most notably to matrix completion (Cand\u00e8s and Recht, 2009; Foygel and Srebro, 2011; Srebro et al., 2004) but also in the context of classification with missing data (Goldberg et al.", "startOffset": 91, "endOffset": 161}, {"referenceID": 9, "context": "They have been successfully applied to various problems, most notably to matrix completion (Cand\u00e8s and Recht, 2009; Foygel and Srebro, 2011; Srebro et al., 2004) but also in the context of classification with missing data (Goldberg et al.", "startOffset": 91, "endOffset": 161}, {"referenceID": 11, "context": ", 2004) but also in the context of classification with missing data (Goldberg et al., 2010; Hazan et al., 2015) and large scale optimization (Shalev-Shwartz et al.", "startOffset": 68, "endOffset": 111}, {"referenceID": 17, "context": ", 2004) but also in the context of classification with missing data (Goldberg et al., 2010; Hazan et al., 2015) and large scale optimization (Shalev-Shwartz et al.", "startOffset": 68, "endOffset": 111}, {"referenceID": 10, "context": "A similar problem that was studied in the literature is the Branching Experts Problem (Gofer et al., 2013).", "startOffset": 86, "endOffset": 106}, {"referenceID": 3, "context": "They have been successfully applied to various problems, most notably to matrix completion (Cand\u00e8s and Recht, 2009; Foygel and Srebro, 2011; Srebro et al., 2004) but also in the context of classification with missing data (Goldberg et al., 2010; Hazan et al., 2015) and large scale optimization (Shalev-Shwartz et al., 2011). A similar problem that was studied in the literature is the Branching Experts Problem (Gofer et al., 2013). In the branching expert problem N potential experts are effectively only k distinct experts, but the clustering of the experts to the k clusters is unknown a-priori. This case can be considered as a special instance of our setting as indeed we can embed each expert as a kdimensional vector. Gofer et al. (2013) proved a sharp \u0398p ? kT q regret (the bound is tight only when k \u0103 c logN for some constant c \u0105 0).", "startOffset": 92, "endOffset": 746}, {"referenceID": 6, "context": ", 2014), as well as the exploration of various structural assumptions that can be leveraged for obtaining improved regret guarantees (e.g., Cesa-Bianchi et al., 2007; Hazan and Kale, 2010, 2011; Chiang et al., 2012; Rakhlin and Sridharan, 2013).", "startOffset": 133, "endOffset": 244}, {"referenceID": 8, "context": "One of the earliest and most widely used methods in this family is the AdaGrad algorithm (Duchi et al., 2011), a subgradient descent method that dynamically incorporate knowledge of the geometry of the data from earlier iterations.", "startOffset": 89, "endOffset": 109}, {"referenceID": 2, "context": "The SOA algorithm suggested by Ben-David et al. (2009) is a general framework for regret minimization that depends solely on the Littlestone dimension.", "startOffset": 31, "endOffset": 55}, {"referenceID": 2, "context": "Lemma 10 (Ben-David et al., 2009).", "startOffset": 9, "endOffset": 33}], "year": 2016, "abstractText": "We consider the problem of prediction with expert advice when the losses of the experts have low-dimensional structure: they are restricted to an unknown d-dimensional subspace. We devise algorithms with regret bounds that are independent of the number of experts and depend only on the rank d. For the stochastic model we show a tight bound of \u0398p ? dT q, and extend it to a setting of an approximate d subspace. For the adversarial model we show an upper bound of Opd ? T q and a lower bound of \u03a9p ? dT q.", "creator": "LaTeX with hyperref package"}}}