{"id": "1603.08293", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis", "abstract": "Principal Component Analysis (PCA) indeed place and the example specific face-to-face theoretical while handle wide - three-dimensional user. However, due leave held high computational particular common its eigen formula_1 create, we right to apply PCA have that cover - bringing data with raised 90-cent. Meanwhile, the diameters L2 - norm primarily commitment look still sensitive to detailed guanidine. In recent research, for L1 - sheldon maximization leading PCA tool until endorsed only efficient concepts being has anemic actually sorbs. However, this involved equipment turn amoral strategy to solve made noz \u03bb. Moreover, the L1 - norm impracticality department objective soon cannot. that regardless bolstered PCA reasoning, though even loses the theoretical connection to the formalism given data reconstruction error, which is one followed the only similar intuitions few goals of PCA. In instance cut, we propose to calculate place L21 - norm focus robust PCA solution, which one theoretically connected well the minimizes only steps estimation. More enough, something proposes with suited country - stupid optimal calculations might solve wish objective the however _ ministry L21 - o'connell compactness has been theoretically guarantees convergence. Experimental results opened now worldwide data advance nbc similar compliance of the proposed tool for consultant solutions examination.", "histories": [["v1", "Mon, 28 Mar 2016 03:37:26 GMT  (399kb)", "http://arxiv.org/abs/1603.08293v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["feiping nie", "heng huang"], "accepted": false, "id": "1603.08293"}, "pdf": {"name": "1603.08293.pdf", "metadata": {"source": "CRF", "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis", "authors": ["Feiping Nie", "Heng Huang"], "emails": ["feipingnie@gmail.com,", "heng@uta.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n08 29\n3v 1\n[ cs\n.L G\n] 2\n8 M\nar 2\n01 6\nIndex Terms\u2014 Principal component analysis, robust dimensionality reduction, L21-norm maximization.\nI. INTRODUCTION\nIn many real-world applications, the dimensionality of data are very high. Directly handle the high-dimensional data is computationally expensive. At the same time, the performance could be poor because the number of available data is often limited and the noise in the data would increase dramatically when the dimensionality increases. To solve these problems, dimensionality reduction is one of the most important and effective methods. Among the dimensionality reduction algorithms, Principal Component Analysis (PCA) [1] is one of the most widely used algorithms due to its simplicity and effectiveness. The main goal of PCA is to preserve the structure of the original data in the projected low-dimensional space. To this end, given a data set, PCA finds a projection matrix to minimize the reconstruction error of the projected data points under this projection matrix.\nThe traditional PCA has been successfully applied in many problems [2] in the past decades. However, the traditional PCA algorithm has several drawbacks. First, it need perform Singular Vector Decomposition (SVD) on input data matrix or eigendecomposition on the covariance matrix, which is computationally expensive and difficult to be used when the number and the dimensionality of data are both very high. Second, it is sensitive to data outliers, because its objective function is intrinsically based on squared L2-norm and the outliers with large variation values can be exaggerated by the squared L2-norm. Many recent\nFeiping Nie and Heng Huang are with the Department of Computer Science and Engineering, University of Texas at Arlington, USA. Email: feipingnie@gmail.com, heng@uta.edu\nresearch works [3], [4], [5], [6], [7], [8] have devoted effort to alleviate this problem and improve the robustness to outliers. [3], [6] proposed to find the subspace such that the sum of L1-norm distances of data points to the subspace is minimized. Although the robustness to outliers is improved in these methods, their algorithms are computationally expensive. Moreover, the used L1-norm in objective is not invariant to rotation. Thus, the performance is usually poor when their L1-norm based PCA is combined with K-means clustering [7]. To solve this problem, the R1-PCA was proposed with rotational invariant property and demonstrated good performance [7]. However, the R1-PCA iteratively performs the subspace iteration algorithm [9] in the high dimensional original space, which is computationally expensive. The extension of R1-PCA to tensor version can be found in [10].\nRecently, a PCA method based on L1-norm maximization was proposed in [11], and a similar work can be found in [12]. This method is invariant to rotation and is also robust to outliers. An efficient algorithm was proposed to solve the L1norm maximization problem in [11]. This algorithm only need perform matrix-vector multiplication, and thus can be applied in the case that both the number and the dimensionality of data are very high. Several works on its tensor version and supervised version can be found in [13], [14], [15]. Due to the difficulty of directly solving the L1-norm maximization problem, all these works use a greedy strategy to solve it. Specifically, the projection directions are optimized one by one sequentially. Such a kind of greedy method is easy to get stuck in a local solution.\nMoreover, the L1-norm maximization based PCA method is not theoretically connected to minimization of the reconstruction error, which is the important goal of traditional PCA. In this paper, we propose a novel principal component analysis method based on the L21-norm maximization. The proposed method is robust to data outliers and also invariant to rotation. More importantly, our new method is theoretically connected to the minimization of reconstruction error, and thus is more suitable for principal component analysis than previous method in [11]. To solve the derived L21-norm PCA objective, we propose a new non-greedy and efficient optimization algorithm to optimize all the projection directions simultaneously. Meanwhile, our algorithm will be extended to solve the more general maximization problems. We provide the theoretical analysis to guarantee the convergence of our algorithms. All experimental results on real world data sets show that the proposed method is effective for principal component analysis, and always obtains smaller reconstruction error than the method in [11] under the same reduced dimension.\nThe rest of this paper is organized as follows: We give a brief review of the related work in Section 2. In Section 3, we propose the L21-norm maximization based principal component analysis and solve the derived optimization problem through a new nongreedy and efficient algorithm which can solve the more general L21-norm maximization problem. In Section 4, we extend our\n2 algorithm to solve the general maximization problem which can be used to derive solutions for many other statistical learning models. In Section 5, we present experimental results to verify the effectiveness of the proposed method. Finally, we draw the conclusions in Section 6."}, {"heading": "II. RELATED WORK", "text": "Given data X = [x1, x2, \u00b7 \u00b7 \u00b7 , xn] \u2208 \u211cd\u00d7n, where d and n are the dimensionality and number of data points respectively, without loss of generality, we can assume the data {xi}ni=1 are centralized, i.e.,\n\u2211n i=1 xi = 0.\nWe denote the projection matrix W = [w1, w2, \u00b7 \u00b7 \u00b7 , wm] \u2208 \u211cd\u00d7m. The traditional PCA method minimizes the reconstruction error under the projected subspace, which is to solve the following optimization problem:\nmin WT W=I\nn \u2211\ni=1\n\u2225 \u2225 \u2225 xi \u2212WWTxi \u2225 \u2225 \u2225 2\n2 , (1)\nwhere I is the identity matrix, \u2016 \u00b7 \u20162 is the L2-norm of vector. Equivalently, the traditional PCA method can also be formulated as maximizing the variance of data in the projected subspace, which is to solve the following optimization problem:\nmax WTW=I Tr(WTStW ) = max WT W=I\nn \u2211\ni=1\n\u2225 \u2225 \u2225 WTxi \u2225 \u2225 \u2225\n2 2 , (2)\nwhere St = XXT is the covariance matrix and Tr(\u00b7) is the trace operator of a matrix. The equivalence of Eq. (1) and Eq. (2) is based on the following equation for any matrix W with WTW = I:\nn \u2211\ni=1\n\u2225 \u2225 \u2225 xi \u2212WWTxi \u2225 \u2225 \u2225 2\n2 +\nn \u2211\ni=1\n\u2225 \u2225 \u2225 WTxi \u2225 \u2225 \u2225 2\n2 =\nn \u2211\ni=1\n\u2016xi\u201622 (3)\nBased on Eq. (1), R1-PCA was proposed to solve the following problem [7]:\nmin WT W=I\nn \u2211\ni=1\n\u2225 \u2225 \u2225 xi \u2212WWTxi \u2225 \u2225 \u2225\n2 . (4)\nR1-PCA minimizes the L2-norm loss instead of the squared L2norm loss in traditional PCA, and thus the robustness to outliers is improved. The other important property of R1-PCA is that it is invariant to rotation, which is also preserved by traditional PCA.\nMotivated by Eq. (2), a recent work named PCA-L1 [11] was proposed to maximize the L1-norm instead of the squared L2norm in traditional PCA by solving the following problem:\nmax WT W=I\nn \u2211\ni=1\n\u2225 \u2225 \u2225 WTxi \u2225 \u2225 \u2225\n1 , (5)\nwhere \u2016\u00b7\u20161 is the L1-norm of vector. PCA-L1 also has the rotation invariant property. Directly solving this problem is difficult, thus the author used a greedy strategy to solve it. Specifically, the m projection directions {w1, w2, \u00b7 \u00b7 \u00b7 , wm} are optimized one by one. The first projection direction w1 is optimized by solving:\nmax wT\n1 w1=1\nn \u2211\ni=1\n|wT1 xi|. (6)\nAfter the (k \u2212 1)-th projection direction wk\u22121 has been obtained, the data matrix X is transformed to X = X \u2212\nwk\u22121(wk\u22121) TX, and then the k-th projection direction wk is optimized by solving:\nmax wT\nk wk=1\nn \u2211\ni=1\n|wTk xi|. (7)\nIn this greedy method, the problem (7) is the key function to be solved for each k. The work in [11] proposed an iterative algorithm to solve this problem. In order to guarantee the algorithm converges to a local maximum, the algorithm adds an additional judgement after the convergence to wtk. If there exists i such that (wtk) Txi = 0, then let wtk = (w t k + \u25b3w)/\u2016wtk + \u25b3w\u20162 and re-run the iterative algorithm, where \u25b3w is a small nonzero random vector. However, such operation might make the algorithm interminable. For example, if there is a data point x that exactly locates on the mean of the data set, then x will be zero after centralization. As a result, (wt)Tx is always zero for any wt). Moreover, it is possible that there exists i such that (wtk)\nTxi = 0 at the global maximum. In this case, the algorithm doesn\u2019t have the chance to find the global maximum."}, {"heading": "III. PRINCIPAL COMPONENT ANALYSIS WITH NON-GREEDY L21-NORM MAXIMIZATION", "text": ""}, {"heading": "A. L21-norm principal component analysis", "text": "Motivated by Eq. (2), we propose to solve the following problem:\nmax WTW=I\nn \u2211\ni=1\n\u2016WTxi\u20162 = max WTW=I \u2016XTW\u20162,1, (8)\nwhere \u2016\u00b7\u20162,1 is the L21-norm of a matrix defined as \u2016M\u20162,1 = \u2211\ni ( \u2211 j m 2 ij)\n1\n2 . Contrast to the name of PCA-L1 in [11] that solves problem (5), we call our PCA method with solving problem (8) as PCA-L21. The PCA-L21 maximizes the L2-norm instead of the squared L2-norm in PCA, and thus the robustness to outliers is also improved. Similarly to R1-PCA and PCA-L1, PCA-L21 is also a rotation invariant method.\nIt is conjectured in [11] that problem (4) and problem (5) are closely related. However, no theoretical analysis was provided in [11] and it seems not the case according to our extensively experimental results. In contrast, we will show from both theoretical and experimental results that the proposed problem (8) is indeed closely related to the problem (4), thus PCA-L21 is more suitable for the principal component analysis than PCA-L1.\nFirst, we have the following lemma: Lemma 1: If a2 + b2 = c2, then |c| \u2264 |a|+ |b| \u2264 \u221a 2 |c|. Proof: Starting from the condition, we have\na2 + b2 = c2\n\u21d2 a2 + b2 + 2 |a| |b| \u2265 c2 \u21d2 (|a|+ |b|)2 \u2265 c2\n\u21d2 |a|+ |b| \u2265 |c| . (9)\nOn the other hand, we have\n(|a| \u2212 |b|)2 \u2265 0 \u21d2 (|a|+ |b|)2 \u2264 2(a2 + b2) \u21d2 (|a|+ |b|)2 \u2264 2c2 \u21d2 |a|+ |b| \u2264 \u221a 2 |c| (10)\nCombining Eq. (9) and Eq. (10), we complete the proof.\n3\nAccording to Eq. (3) and Lemma 1, we have the following relationship: n \u2211\ni=1\n\u2016xi\u20162 \u2264 n \u2211\ni=1\n\u2225 \u2225 \u2225 xi \u2212WWTxi \u2225 \u2225 \u2225\n2 +\nn \u2211\ni=1\n\u2225 \u2225 \u2225 WTxi \u2225 \u2225 \u2225\n2 \u2264\n\u221a 2 n \u2211\ni=1\n\u2016xi\u20162,\n(11) which can be written in matrix form as\n\u2016X\u20162,1 \u2264 \u2016XT \u2212XTWWT\u20162,1 + \u2016WTX\u20162,1 \u2264 \u221a 2 \u2016X\u20162,1 .\n(12) Therefore, the proposed problem (8) is theoretically connected to the problem (4), which indicates that maximizing the L21 norm as in problem (8) also makes sense to minimize the reconstruction error, and thus is suitable for the principal component analysis.\nTo solve the problem (8), we first propose an efficient algorithm to solve the more general L21-norm maximization problem. Utilizing this general algorithm, we can solve the problem (8) directly without using the greedy strategy as in [11]."}, {"heading": "B. Efficient algorithm to solve the general L21-norm maximization problem", "text": "Consider a general L21-norm maximization problem as follows:\nmax v\u2208C\nf(v) + \u2211\ni\n\u2016gi(v)\u20162, (13)\nwhere f(v) is an arbitrary scatter-output function, gi(v) (for each i) is an arbitrary vector-output function, and v \u2208 C is an arbitrary constraint. We assume that the objective in problem (13) has an upper bound.\nWe re-write the problem (13) as the following problem:\nmax v\u2208C\nf(v) + \u2211\ni\n(\u03b1i) T gi(v), (14)\nwhere\n\u03b1i =\n{\ngi(v) \u2016gi(v)\u20162 if \u2016gi(v)\u20162 6= 0 ; 0 if \u2016gi(v)\u20162 = 0 .\n(15)\nNote that \u03b1i depends on v and thus is also an unknown variable. Based on Eqs. (14) and (15), we propose an iterative algorithm to solve the problem (13). The algorithm is described in Algorithm 1. In each iteration, \u03b1i is updated by current solution v, and the solution v is updated with the updated \u03b1i. The iterative procedure is repeated till the algorithm converges.\nInitialize v1 \u2208 C, t = 1 ; while not converge do\n1. For each i, calculate \u03b1ti according to Eq. (15) ; 2. vt+1 = argmax\nv\u2208C f(v) +\n\u2211\ni\n(\u03b1ti) T gi(v) ;\n3. t = t+ 1 ; end Output: vt.\nAlgorithm 1: An efficient algorithm to solve the general L21norm maximization problem (13).\nNext, we prove that the proposed iterative algorithm will monotonically increase the objective function value of the problem (13) in each iteration, and will converge to a local solution.\nThe convergence of the Algorithm 1 is demonstrated in the following theorem:\nTheorem 1: The Algorithm 1 monotonically increases the objective function value of the problem (13) in each iteration. Proof: For each iteration t, according to the Step 2 in Algorithm 1, we have\nf(vt+1) + \u2211\ni\n(\u03b1ti) T gi(v t+1) \u2265 f(vt) + \u2211\ni\n(\u03b1ti) T gi(v t). (16)\nOn the other hand, for each i, according to the Cauchy-Schwarz inequality, we know:\n\u2016gi(vt)\u20162\u2016gi(vt+1)\u20162 \u2265 gi(vt)T gi(vt+1).\nBased on this inequality, Eq. (15), and\n\u2016gi(vt)\u20162 \u2212 (\u03b1ti)T gi(vt) = 0,\nwe have\n\u2016gi(vt)\u20162\u2016gi(vt+1)\u20162 \u2265 gi(vt)T gi(vt+1) \u21d2 \u2016gi(vt+1)\u20162 \u2265 (\u03b1ti)T gi(vt+1) \u21d2 \u2016gi(vt+1)\u20162 \u2212 (\u03b1ti)T gi(vt+1) \u2265 0 \u21d2 \u2016gi(vt+1)\u20162 \u2212 (\u03b1ti)T gi(vt+1) \u2265 \u2016gi(vt)\u20162 \u2212 (\u03b1ti)T gi(vt).\nThe above inequality holds for every i, thus we have \u2211\ni\n\u2016gi(vt+1)\u20162 \u2212 \u2211\ni\n(\u03b1ti) T gi(v t+1)\n\u2265 \u2211\ni\n\u2016gi(vt)\u20162 \u2212 \u2211\ni\n(\u03b1ti) T gi(v\nt) . (17)\nCombining Eq. (16) and Eq. (17), we arrive at\nf(vt+1) + \u2211\ni\n\u2016gi(vt+1)\u20162 \u2265 f(vt) + \u2211\ni\n\u2016gi(vt)\u20162 . (18)\nThus, the Algorithm 1 monotonically increases the objective of the problem (13) in each iteration t.\nAs the objective of the problem (13) has an upper bound, Theorem 1 indicates that the Algorithm 1 converges. The following theorem shows that the Algorithm 1 will converge to a local solution.\nTheorem 2: The Algorithm 1 will converge to a local solution of the problem (13). Proof: The Lagrangian function of the problem (13) is\nL(v, \u03bb) = f(v) + \u2211\ni\n\u2016gi(v)\u20162 \u2212 r(v, \u03bb), (19)\nwhere r(\u03bb, v) is the Lagrangian term to encode the constraint v \u2208 C in problem (13).\nTaking the derivative1 of L(v, \u03bb) w.r.t. v, and setting the derivative to zero, we have:\n\u2202L(v, \u03bb) \u2202v = \u2202f(v) \u2202v + \u2211\ni\nJi(v)\u03b1i \u2212 \u2202r(v, \u03bb)\n\u2202v = 0, (20)\nwhere \u03b1i is defined in Eq. (15) and Ji(v) is a matrix with the (j, k)-th element as \u2202g k i (v) \u2202vj\n, gki (v) denotes the k-th element of the vector gi(v).\nSuppose the Algorithm 1 converges to a solution v\u2217. From the Step 2 in Algorithm 1 we have\nv\u2217 = argmax v\u2208C f(v\u2217) + \u2211\ni\n(\u03b1\u2217i ) T gi(v \u2217). (21)\n1When gi(v) = 0, then 0 is a subgradient of function \u2016gi(v)\u20162 , so the \u03b1i defined in Eq. (15) is the gradient or a subgradient of the function \u2016gi(v)\u20162 in all the cases.\n4 According to the KKT condition [16] of the problem (21), we know that the solution v\u2217 for problem (21) satisfies Eq. (20). Note that Eq. (20) is the KKT condition of the problem (13), hence the solution v\u2217 satisfies the KKT condition of the problem (13). Therefore, the converged solution v\u2217 is a local solution of the problem (13)."}, {"heading": "C. Non-greedy maximization algorithm to solve L21-Norm Principal component analysis", "text": "Obviously the proposed problem (8) is a special case of the problem (13), thus we can use the proposed Algorithm 1 to solve the objective of L21-Norm PCA.\nIn Algorithm 1, Step 2 is the key step. Thus, to solve the problem (8), the key step is to solve the following problem:\nmax WTW=I\nn \u2211\ni=1\n\u03b1Ti W Txi , (22)\nwhere the vector \u03b1i \u2208 \u211cm\u00d71 is defined as:\n\u03b1i =\n{\nWT xi \u2016WT xi\u20162 if \u2016WTxi\u20162 6= 0 ; 0 if \u2016WTxi\u20162 = 0 .\n(23)\nDenoting the matrix M = \u2211n i=1 xi\u03b1 T i \u2208 \u211cd\u00d7m, we can re-\nwrite the problem (22) as:\nmax WTW=I\nTr(WTM). (24)\nSuppose the SVD result of M is M = U\u039bV T , then Tr(WTM) can be re-written as:\nTr(WTM)\n= Tr(WTU\u039bV T )\n= Tr(\u039bV TWTU)\n= Tr(\u039bZ) = \u2211\ni\n\u03bbiizii , (25)\nwhere Z = V TWTU , \u03bbii and zii are the (i, i)-th element of matrix \u03bb and Z, respectively.\nNote that Z is an orthonormal matrix, i.e. ZTZ = I , so zii \u2264 1. On the other hand, \u03bbii \u2265 0, since \u03bbii is singular value of M . Therefore, Tr(WTM) = \u2211\ni\n\u03bbiizii \u2264 \u2211\ni\n\u03bbii, and when zii =\n1 (1 \u2264 i \u2264 c), the equality holds. That is to say, Tr(WTM) reaches the maximum, when Z = I . Recall that Z = V TWTU , thus the optimal solution to the problem Eq. (24) is\nW = UZT V T = UV T . (26)\nBased on the Algorithm 1, the algorithm of PCA-L21 to solve problem (8) is described in Algorithm 2. According to Theorem 2, we can obtain a local solution with the algorithm. Contrast to the PCA-L1 algorithm in [11], the PCA-L21 algorithm directly solves the projection matrix W (i.e. optimizes all projection directions simultaneously), but the PCA-L1 algorithm solves the projection directions one by one using a greedy strategy.\nFrom Algorithm 2 we can see that the computational complexity of the algorithm is O(ndmt), where n, d,m, t is the number of data points, the original dimensionality, the reduced dimensionality and the iteration number, respectively. In practice,\nthe algorithm usually converges in ten iterations. Therefore, the computational complexity of the algorithm is linear w.r.t. either data number or data dimension, which indicates the algorithm is applicable in the case that both data number and data dimension are high. If the data are sparse, the computational complexity is further reduced to O(nsmt), where s is the averaged number of nonzero elements in one data point.\nInput: X, m, where X is centralized Initialize W 1 \u2208 \u211cd\u00d7m such that WTW = I , t = 1 ; while not converge do\n1. For each i, calculate \u03b1ti according to Eq. (23), M = n \u2211\ni=1\nxi\u03b1 T i ;\n2. Calculate the SVD of M as M = U\u039bV T , Let W t+1 = UV T ; 3. t = t+ 1 ;\nend Output: W t \u2208 \u211cd\u00d7m.\nAlgorithm 2: The non-greedy optimization algorithm to solve the L21-norm principal component analysis."}, {"heading": "D. Kernel and tensor extensions of L21-norm PCA", "text": "Similar to traditional PCA, the proposed PCA-L21 is also a linear method, and is not suitable to handle data under the non-Gaussian distribution. A popular technique to deal with this problem is to extend the linear method to kernel method. Obviously, the PCA-L21 is invariant to rotation and shift, so this linear method satisfies the conditions of the general kernelization framework in [17]. Thus, the PCA-L21 can be kernelized using this framework. Specifically, the given data are transformed by KPCA [18], and then Algorithm 2 is performed on the transformed data.\nAnother problem of PCA is that it can only handle data points with vector format. For high-order tensor data, we have to vectorize the data to very high-dimensional vectors before applying PCA. This approach destroys the spacial information of tensor data and makes the computational burden very heavy. A popular technique to deal with this problem is to extend the vector based method to tensor based method. As the problem (8) in PCA-L21 only includes linear operator WTxi, it can be easily extended to the tensor method to handle high-order data directly. For simplicity, we only briefly discuss the case of 2D tensor, the higher order tensor cases can be readily extended by replacing the linear operator WTxi with tensor operator [19].\nGiven data X = [X1, X2, \u00b7 \u00b7 \u00b7 , Xn] \u2208 \u211cr\u00d7c\u00d7n, where each data Xi \u2208 \u211cr\u00d7c is a 2D matrix, n is the number of data points, we assume that {Xi}ni=1 are centralized, i.e., \u2211n i=1 Xi = 0. To handle the tensor case, linear operator WTxi is replaced by UTXiV , where U \u2208 \u211cr\u00d7k1 and V \u2208 \u211cc\u00d7k2 are two projection matrices (k1 < r and k2 < c are the reduced dimensions of two projection subspaces). Correspondingly, the problem (8) becomes:\nmax UTU=Ik1 ,V TV =Ik2\nn \u2211\ni=1\n\u2016UTXiV \u2016F , (27)\nwhere \u2016 \u00b7 \u2016F is the Frobenius norm of matrix. Similar to other tensor methods, problem (27) can be solved by alternative optimization technique. Specifically, when fix U , the problem\n5 (27) is reduced to the problem (8), and thus the V can be optimized by Algorithm 2. In turn, U can also be optimized by Algorithm 2 when fix V . The procedure is iteratively performed until converges."}, {"heading": "IV. THE EXTENSION OF OUR ALGORITHM FOR GENERAL MAXIMIZATION PROBLEM", "text": "Besides solving the L21-norm maximization problem in above section, to provide the useful and efficient algorithm for related research problems, we extend our idea to solve the more general maximization problem as follows:\nmax v\u2208C\nf(v) + \u2211\ni\nhi(gi(v)), (28)\nwhere f(v) is an arbitrary scatter-output function, gi(v) (for each i) is an arbitrary scatter, vector, or matrix-output function, and hi (for each i) is an arbitrary convex function, v \u2208 C is an arbitrary constraint. We assume that the objective in problem (28) has an upper bound.\nWe propose an iterative algorithm to solve the problem (13). The algorithm is described in Algorithm 3. Similar to Algorithm 1, in each iteration, \u03b1i is updated by current solution v, and the solution v is updated with the updated \u03b1i. The iterative procedure is repeated till the algorithm converges.\nInitialize v1 \u2208 C, t = 1 ; while not converge do\n1. For each i, calculate \u03b1ti = h \u2032 i(gi(v t)) = \u2202hi(gi(v t)) \u2202(gi(vt)) ; 2. vt+1 = argmax v\u2208C f(v) + \u2211\ni\nTr((\u03b1ti) T gi(v)) ;\n3. t = t+ 1 ; end Output: vt.\nAlgorithm 3: An efficient algorithm to solve the more general maximization problem (28).\nThe convergence of the Algorithm 3 is guaranteed by the following theorem:\nTheorem 3: The Algorithm 3 monotonically increases the objective of the problem (28) in each iteration. Proof: For each iteration t, according to the Step 2 in Algorithm 3, we have\nf(vt+1) + \u2211\ni\nTr((\u03b1ti) T gi(v t+1)) \u2265 f(vt) + \u2211\ni\nTr((\u03b1ti) T gi(v t)).\n(29) For each i, since hi is convex, according to the property of convex function, we know hi(gi(v t+1)) \u2212 hi(gi(vt)) \u2265 Tr((gi(v t+1) \u2212 gi(vt))Th\u2032i(gi(vt)). According to the definition of \u03b1ti in Step 1, we have\nhi(gi(v t+1))\u2212 Tr((\u03b1ti)T gi(vt+1)) \u2265 hi(gi(vt))\u2212 Tr((\u03b1ti)T gi(vt)). (30)\nCombining Eq. (29) and Eq. (30), we arrive at\nf(vt+1) + \u2211\ni\nhi(gi(v t+1)) \u2265 f(vt) +\n\u2211\ni\nhi(gi(v t)). (31)\nThus the Algorithm 3 monotonically increases the objective of the problem (28) in each iteration.\nBecause the objective of the problem (28) has an upper bound, Theorem 3 indicates that the Algorithm 3 converges. The following theorem shows that the Algorithm 3 will converge to a local solution of the problem (28).\nTheorem 4: The Algorithm 3 will converge to a local solution of the problem (28). Proof: The Lagrangian function of the problem (28) is\nL(v, \u03bb) = f(v) + \u2211\ni\nhi(gi(v))\u2212 r(v, \u03bb), (32)\nwhere r(\u03bb, v) is the Lagrangian term to encode the constraint v \u2208 C in problem (28).\nTaking the derivative 2 of L(v, \u03bb) w.r.t. v, and setting the derivative to zero, we have:\n\u2202L(v, \u03bb) \u2202v = \u2202f(v) \u2202v + \u2211\ni\nTr ( (\u03b1i) T \u2202gi(v) )\n\u2202v \u2212 \u2202r(v, \u03bb) \u2202v = 0,\n(33) where \u03b1i = h \u2032 i(gi(v)).\nSuppose the Algorithm 3 converges to a solution v\u2217. From the Step 2 in Algorithm 3 we have\nv\u2217 = argmax v\u2208C f(v\u2217) + \u2211\ni\nTr((\u03b1\u2217i ) T gi(v \u2217)). (34)\nAccording to the KKT condition of the problem (34), we know that the solution v\u2217 to problem (34) satisfies Eq. (33). Note that Eq. (33) is the KKT condition of the problem (28), so the converged solution v\u2217 of Algorithm 3 satisfies the KKT condition of the problem (28). Therefore, the converged solution v\u2217 is a local solution of the problem (28).\nAlgorithm 3 is very useful to solve the general maximization problems. For example, we can directly use the algorithm to solve the following two important problems:\nmax V TV=I\nTr(V TAV ) , (35)\nmax vT v=1,\u2016v\u2016\n0 \u2264k\nvTAv . (36)\nIt is interesting to point out that the derived algorithms for Eq. (35) and Eq. (36) based on Algorithm 3 are exactly the classical power method (or subspace iteration method) and the recently proposed truncated power method [20], respectively."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we will present the experimental results to demonstrate the effectiveness of the proposed PCA-L21 compared to traditional PCA, R1-PCA and PCA-L1."}, {"heading": "A. Reconstruction errors with occlusions", "text": "We use six image benchmark data sets to perform our experiments. A brief descriptions of the data sets are shown in Table I, and the samples from each data sets are shown in Figure 1.\nIn each image data set, we randomly select 10, 20, 30 percent images respectively, and each selected image is occluded with a randomly located square. The width of these squares are the half of the width of the images.\n2When x = 0, then 0 is a subgradient of function \u2016x\u20162, so \u03b1i is the gradient or a subgradient of the function \u2016x\u20162 in all the cases, where \u03b1i = x/\u2016x\u20162 if \u2016x\u20162 6= 0 and \u03b1i = 0 otherwise.\n6\nWe use the following reconstruction error to measure the quality of dimensionality reduction methods:\ne(m) = 1\nn\nn \u2211\ni=1\n\u2225 \u2225 \u2225 xorgi \u2212WW Txi \u2225 \u2225 \u2225\n2 , (37)\nwhere n is the number of training data, W \u2208 \u211cd\u00d7m is the learned projection matrix by PCA, R1-PCA, PCA-L1, or PCA-L21, xorgi and xi are the i-th original non-occluded image and the i-th image used in the training respectively.\nIn the experiments, the projected dimension m varies from 21 to 69. The results of the reconstruction error by PCA, R1PCA, PCA-L1 and PCA-L21 are shown in Figures 2-4. From the figures, we can see that PCA, R1-PCA and the proposed PCAL21 are suitable for principal component analysis from the view of data reconstruction. When there are occlusions in the data, R1PCA and PCA-L21 outperform PCA in terms of reconstruction error. We can also observed that in this experimental setting, PCAL1 doesn\u2019t perform as good as PCA, which indicates that PCAL1 is not closely connected to the minimization of reconstruction error, thus is not a good option for principal component analysis in some cases."}, {"heading": "B. Reconstruction errors with noise images", "text": "In this experiment, two image data sets XM2VTS and Coil20 are used. For each data set, we add 10, 20, 30 percent images from the Palm image data set as the noise images, respectively. Some samples from the Palm data set are shown in Figure 5.\nWe use the following reconstruction error to measure the\nquality of dimensionality reduction methods:\ne(m) = 1\nn\nn \u2211\ni=1\n\u2225 \u2225 \u2225 xorgi \u2212WW Txorgi \u2225 \u2225 \u2225\n2 , (38)\nwhere n is the number of training data (not including the noise image data from the Palm data set), W \u2208 \u211cd\u00d7m is the learned projection matrix by PCA, R1-PCA, PCA-L1 or PCA-L21, xorgi is the i-th original training data (not including the noise image data from the Palm data set). Under this experimental setting, if a method is robust to the data outliers, its reconstruction error should be smaller than other non-robust methods.\nIn the experiments, the projected dimension m varies from 21 to 69. The results of the reconstruction error by PCA, R1-PCA, PCA-L1 and PCA-L21 are shown in Figure 6.\nWe can see from the figures that, in this experimental setting, R1-PCA, PCA-L1 and the proposed PCA-L21 all outperform PCA in terms of reconstruction error, and our PCA-L21 consistently outperforms R1PCA, PCA-L1 and performs best in this case. The experimental results clearly indicates that the proposed PCA-L21 is more suitable for principal component analysis than the traditional PCA when there are outliers in the data."}, {"heading": "VI. CONCLUSIONS", "text": "A principal component analysis with L21-norm maximization was proposed in this paper. The L21-norm maximization based PCA is theoretically connected to the minimization of the reconstruction error, and thus is more suitable for principal component analysis than the L1-norm maximization based PCA proposed in [11]. To avoid the greedy strategy used in [11] for solving the L1norm maximization problem, we propose an efficient optimization algorithm to solve a more general L21-norm maximization problem, which is non-greedy and is guaranteed to converge to a local solution. Moreover, we extend our algorithm to solve the more general maximization problem which can derive solutions for many related statistical learning models. Experimental results on real world data sets show that the proposed method is effective for principal component analysis, and always obtains smaller reconstruction error than the related methods under the same reduced dimension."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was partially supported by nsf-iis 1117965, nsf-ccf 0830780, nsf-dms 0915228, nsf-ccf 0917274."}], "references": [{"title": "Principal Component Analysis,2nd Edition", "author": ["I.T. Jolliffe"], "venue": "New York: Springer-Verlag,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Robust factorization", "author": ["H. Aanas", "R. Fisker", "K. Astrom", "J. Carstensen"], "venue": "IEEE Transactions on PAMI, vol. 24, no. 9, pp. 1215\u20131225, 2002.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "A framework for robust subspace learning", "author": ["F. De La Torre", "M. Black"], "venue": "International Journal of Computer Vision, vol. 54, no. 1, pp.  117\u2013142, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "R1-PCA: rotational invariant L1-norm principal component analysis for robust subspace  factorization", "author": ["C.H.Q. Ding", "D. Zhou", "X. He", "H. Zha"], "venue": "ICML, 2006, pp. 281\u2013288.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust principal component  9 analysis: Exact recovery of corrupted low-rank matrices via convex optimization", "author": ["J. Wright", "A. Ganesh", "S. Rao", "Y. Ma"], "venue": "NIPS, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Matrix Computations, 3rd Edition", "author": ["G.H. Golub", "C.F. van Loan"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Robust tensor factorization using r1 norm", "author": ["H. Huang", "C.H.Q. Ding"], "venue": "CVPR, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Principal component analysis based on L1-norm maximization", "author": ["N. Kwak"], "venue": "IEEE Transactions on PAMI, vol. 30, no. 9, pp. 1672\u20131680, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Methods of L1 estimation of a covariance matrix", "author": ["J. Galpin", "D. Hawkins"], "venue": "Computational Statistics & Data Analysis, vol. 5, no. 4, pp. 305\u2013319, 1987.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1987}, {"title": "L1-norm-based 2DPCA", "author": ["X. Li", "Y. Pang", "Y. Yuan"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 38, no. 4, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Multilinear maximum distance embedding via l1-norm optimization", "author": ["Y. Liu", "Y. Liu", "K.C.C. Chan"], "venue": "AAAI, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust tensor analysis with L1-norm", "author": ["Y. Pang", "X. Li", "Y. Yuan"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 20, no. 2, pp. 172\u2013178, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "A general kernelization framework for learning algorithms based on kernel PCA", "author": ["C. Zhang", "F. Nie", "S. Xiang"], "venue": "Neurocomputing, vol. 73, no. 4-6, pp. 959\u2013967, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "K.-R. M\u00fcller"], "venue": "Neural Computation, vol. 10, no. 5, pp. 1299\u20131319, 1998.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Signal processing based on multilinear algebra", "author": ["L.D. Lathauwer"], "venue": "Ph.D. dissertation, Faculteit der Toegepaste Wetenschappen. Katholieke Universiteit Leuven, 1997.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["X. Yuan", "T. Zhang"], "venue": "Technical Report, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Among the dimensionality reduction algorithms, Principal Component Analysis (PCA) [1] is one of the most widely used algorithms due to its simplicity and effectiveness.", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "edu research works [3], [4], [5], [6], [7], [8] have devoted effort to alleviate this problem and improve the robustness to outliers.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "edu research works [3], [4], [5], [6], [7], [8] have devoted effort to alleviate this problem and improve the robustness to outliers.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "edu research works [3], [4], [5], [6], [7], [8] have devoted effort to alleviate this problem and improve the robustness to outliers.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "edu research works [3], [4], [5], [6], [7], [8] have devoted effort to alleviate this problem and improve the robustness to outliers.", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "Thus, the performance is usually poor when their L1-norm based PCA is combined with K-means clustering [7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "To solve this problem, the R1-PCA was proposed with rotational invariant property and demonstrated good performance [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "However, the R1-PCA iteratively performs the subspace iteration algorithm [9] in the high dimensional original space, which is computationally expensive.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "The extension of R1-PCA to tensor version can be found in [10].", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "Recently, a PCA method based on L1-norm maximization was proposed in [11], and a similar work can be found in [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "Recently, a PCA method based on L1-norm maximization was proposed in [11], and a similar work can be found in [12].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "An efficient algorithm was proposed to solve the L1norm maximization problem in [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "Several works on its tensor version and supervised version can be found in [13], [14], [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "Several works on its tensor version and supervised version can be found in [13], [14], [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "Several works on its tensor version and supervised version can be found in [13], [14], [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "More importantly, our new method is theoretically connected to the minimization of reconstruction error, and thus is more suitable for principal component analysis than previous method in [11].", "startOffset": 188, "endOffset": 192}, {"referenceID": 7, "context": "All experimental results on real world data sets show that the proposed method is effective for principal component analysis, and always obtains smaller reconstruction error than the method in [11] under the same reduced dimension.", "startOffset": 193, "endOffset": 197}, {"referenceID": 3, "context": "(1), R1-PCA was proposed to solve the following problem [7]:", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "(2), a recent work named PCA-L1 [11] was proposed to maximize the L1-norm instead of the squared L2norm in traditional PCA by solving the following problem:", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "The work in [11] proposed an iterative algorithm to solve this problem.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "Contrast to the name of PCA-L1 in [11] that solves problem (5), we call our PCA method with solving problem (8) as PCA-L21.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "It is conjectured in [11] that problem (4) and problem (5) are closely related.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "However, no theoretical analysis was provided in [11] and it seems not the case according to our extensively experimental results.", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "Utilizing this general algorithm, we can solve the problem (8) directly without using the greedy strategy as in [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 7, "context": "Contrast to the PCA-L1 algorithm in [11], the PCA-L21 algorithm directly solves the projection matrix W (i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "Obviously, the PCA-L21 is invariant to rotation and shift, so this linear method satisfies the conditions of the general kernelization framework in [17].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "Specifically, the given data are transformed by KPCA [18], and then Algorithm 2 is performed on the transformed data.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "For simplicity, we only briefly discuss the case of 2D tensor, the higher order tensor cases can be readily extended by replacing the linear operator Wxi with tensor operator [19].", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "(36) based on Algorithm 3 are exactly the classical power method (or subspace iteration method) and the recently proposed truncated power method [20], respectively.", "startOffset": 145, "endOffset": 149}], "year": 2016, "abstractText": "Principal Component Analysis (PCA) is one of the most important unsupervised methods to handle highdimensional data. However, due to the high computational complexity of its eigen decomposition solution, it hard to apply PCA to the large-scale data with high dimensionality. Meanwhile, the squared L2-norm based objective makes it sensitive to data outliers. In recent research, the L1-norm maximization based PCA method was proposed for efficient computation and being robust to outliers. However, this work used a greedy strategy to solve the eigen vectors. Moreover, the L1-norm maximization based objective may not be the correct robust PCA formulation, because it loses the theoretical connection to the minimization of data reconstruction error, which is one of the most important intuitions and goals of PCA. In this paper, we propose to maximize the L21-norm based robust PCA objective, which is theoretically connected to the minimization of reconstruction error. More importantly, we propose the efficient non-greedy optimization algorithms to solve our objective and the more general L21-norm maximization problem with theoretically guaranteed convergence. Experimental results on real world data sets show the effectiveness of the proposed method for principal component analysis.", "creator": "LaTeX with hyperref package"}}}