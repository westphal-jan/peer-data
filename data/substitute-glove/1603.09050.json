{"id": "1603.09050", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Robustness of Bayesian Pool-Based Active Learning Against Prior Misspecification", "abstract": "We theoretical the repeatability of active means (AL) algorithmic attempted prior misspecification: even that initialization broadest similar amazing using one perturbed 2006 as compared to forms 's true early. In had long ranged one losses number years the width talk setting, we aware yet all $ \\ alpha $ - formula_3 algorithms are remarkably (i. e. , on $ \\ nucleus $ - formula_15) yet that controls is Lipschitz thus far came prior. We future film thought robustness or not be achieved mean the utility latter longer - Lipschitz. This significant never it use new Lipschitz utility addition AL whatever predictability most full. For within minimum required steps, everyone rather from automatically small robustness continuing of approximate AL algorithms. Our surprising imply same these typically or AL algorithms are somewhat defeat unperturbed cnidarian. We rest authorize for available of a glaze 1987 to shortage the problem of. misspecification. We rely while robustness according taken uniform stir limited but so experimentally that much classes reasonably well although not.", "histories": [["v1", "Wed, 30 Mar 2016 06:21:42 GMT  (233kb,D)", "http://arxiv.org/abs/1603.09050v1", "This paper is published at AAAI Conference on Artificial Intelligence (AAAI 2016)"]], "COMMENTS": "This paper is published at AAAI Conference on Artificial Intelligence (AAAI 2016)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nguyen viet cuong", "nan ye", "wee sun lee"], "accepted": true, "id": "1603.09050"}, "pdf": {"name": "1603.09050.pdf", "metadata": {"source": "CRF", "title": "Robustness of Bayesian Pool-based Active Learning Against Prior Misspecification", "authors": ["Nguyen Viet Cuong", "Nan Ye", "Wee Sun Lee"], "emails": ["nvcuong@nus.edu.sg", "n.ye@qut.edu.au", "leews@comp.nus.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "In pool-based active learning (AL), training examples are sequentially selected and labeled from a pool of unlabeled data, with the aim of obtaining a good classifier using as few labeled examples as possible (McCallum and Nigam 1998). To achieve computational efficiency, most commonly used methods greedily select one example at a time based on some criterion.\nIn this paper, we consider Bayesian pool-based AL that assumes data labels are generated from a prior distribution. In theory, the true prior is generally assumed to be known (Golovin and Krause 2011; Cuong et al. 2013; Cuong, Lee, and Ye 2014). In practice, it is often unknown and misspecified; that is, the prior used is different from the true one. This work is the first one investigating the robustness of AL algorithms against prior misspecification \u2013 that is, whether an algorithm achieves similar performance using a perturbed prior as compared to using the true prior.\nWe focus on the analysis of approximate algorithms for two commonly studied problems: the maximum coverage problem which aims to maximize the expected or worstcase utility of the chosen examples given a fixed budget of queries, and the minimum cost problem which aims to minimize the expected number of queries needed to identify the\ntrue labeling of all examples. We focus on approximate algorithms because previous works have shown that, in general, it is computationally intractable to find the optimal strategy for choosing the examples, while some commonly used AL algorithms can achieve good approximation ratios compared to the optimal strategies (Golovin and Krause 2011; Chen and Krause 2013; Cuong et al. 2013; Cuong, Lee, and Ye 2014). For example, with the version space reduction utility, the maximum Gibbs error algorithm achieves a (1 \u2212 1/e)-approximation of the optimal expected utility (Cuong et al. 2013), while the least confidence algorithm achieves the same approximation of the optimal worst-case utility (Cuong, Lee, and Ye 2014).\nOur work shows that many commonly used AL algorithms are robust. In the maximum coverage setting, our main result is that if the utility function is Lipschitz continuous in the prior, all \u03b1-approximate algorithms are robust, i.e., they are near \u03b1-approximate when using a perturbed prior. More precisely, their performance guarantee on the expected or worst-case utility degrades by at most a constant factor of the `1 distance between the perturbed prior and the true prior. It follows from this result that the maximum Gibbs error and the least confidence algorithms are near (1 \u2212 1/e)-approximate. Our result also implies the robustness of the batch and generalized versions of the maximum Gibbs error algorithm. If the utility is non-Lipschitz, we show that even an optimal algorithm for the perturbed prior may not be robust. This suggests we should use a Lipschitz continuous utility for AL in order to achieve robustness. Similarly, we prove a robustness result for the minimum cost setting that implies the robustness of the generalized binary search AL algorithm (Dasgupta 2004; Nowak 2008).\nWe also address the difficulty of choosing a good prior in practice. Practically, it is often easier to come up with a set of distributions and combine them using a mixture. Our theoretical results imply robustness when the mixture prior is close to the true prior. In the mixture setting, another interesting question is robustness when the true prior is one of the components of the mixture. In this case, the mixture prior may not necessarily be close to the true prior in terms of `1 distance. We prove that for the uniform mixture prior, approximate AL algorithms are still robust in the sense that they are competitive with the optimum performance of the\nar X\niv :1\n60 3.\n09 05\n0v 1\n[ cs\n.L G\n] 3\n0 M\nar 2\n01 6\nmixture, which is the performance we expect when modeling. Our experiments show that the uniform mixture performs well in practice.\nRelated Works: Greedy algorithms for pool-based AL usually optimize some measure of uncertainty of the selected examples (Settles 2009; Cuong et al. 2013). In the Bayesian setting, they can be viewed as greedily optimizing some corresponding average-case or worst-case objective. For instance, the maximum entropy algorithm (Settles and Craven 2008), which maximizes the Shannon entropy of the selected examples, attempts to greedily maximize the policy entropy in the average case (Cuong et al. 2013). The maximum Gibbs error algorithm, which maximizes the Gibbs error of the selected examples, attempts to greedily maximize the version space reduction in the average case (Cuong et al. 2013); and the least confidence algorithm, which minimizes the probability of the most likely label of the selected examples, attempts to maximize the version space reduction in the worst case (Cuong, Lee, and Ye 2014).\nAnalyses of these algorithms typically investigate their near-optimality guarantees in the average or worst case. The maximum entropy algorithm was shown to have no constant factor approximation guarantee in the average case (Cuong, Lee, and Ye 2014). In contrast, the maximum Gibbs error algorithm has a (1 \u2212 1/e)-factor approximation guarantee for the average version space reduction objective (Cuong et al. 2013). This algorithm is a probabilistic version of the generalized binary search algorithm (Dasgupta 2004; Golovin and Krause 2011). It can also be applied to the batch mode setting (Hoi et al. 2006), and was shown to provide a (1 \u2212 e\u2212(e\u22121)/e)-factor approximation to the optimal batch AL algorithm (Cuong et al. 2013). In the noiseless case, this batch maximum Gibbs error algorithm is equivalent to the BatchGreedy algorithm (Chen and Krause 2013).\nCuong, Lee, and Ye (2014) showed the least confidence algorithm (Culotta and McCallum 2005) has a (1 \u2212 1/e)factor approximation guarantee with respect to the worstcase version space reduction objective. A similar result in the worst case was also shown for the generalized maximum Gibbs error algorithm with an arbitrary loss (Cuong, Lee, and Ye 2014). These results are due to the pointwise submodularity of version space reduction. AL that exploits submodularity was also studied in (Guillory and Bilmes 2010; Wei, Iyer, and Bilmes 2015)."}, {"heading": "2 Preliminaries", "text": "Let X be a finite set (a pool) of examples and Y be a finite set of labels. Consider the hypothesis space H def= YX consisting of all functions from X to Y . Each hypothesis h \u2208 H is a labeling ofX . In the Bayesian setting, we assume an unknown true labeling htrue drawn from a prior p0[h] on H. After observing a labeled set D, we obtain the posterior pD[h] def= p0[h | D] using Bayes\u2019 rule.\nThe true labeling htrue may be generated by a complex process rather than directly drawn from a prior p0. For instance, if probabilistic models (e.g., naive Bayes) are used to generate labels for the examples and a prior is imposed on these models instead of the labelings, we can convert this\nprior into an equivalent prior on the labelings and work with this induced prior. The construction of the induced prior involves computing the probability of labelings with respect to the original prior (Cuong et al. 2013). In practice, we do not need to compute or maintain the induced prior explicitly as this process is very expensive. Instead, we compute or approximate the AL criteria directly from the original prior on the probabilistic models (Cuong et al. 2013; Cuong, Lee, and Ye 2014).\nFor any distribution p[h], any example sequence S \u2286 X , and any label sequence y with the same length, p[y;S] denotes the probability that y is the label sequence of S. Formally, p[y;S] def= \u2211 h p[h]P[h(S) = y | h], where h(S) = (h(x1), . . . , h(xi)) if S = (x1, . . . , xi). In the above, P[h(S) = y | h] is the probability that S has label sequence y given the hypothesis h. If h is deterministic as in our setting, P[h(S) = y | h] = 1(h(S) = y), where 1(\u00b7) is the indicator function. Note that p[ \u00b7 ;S] is a probability distribution on the label sequences of S. We also write p[y;x] to denote p[{y}; {x}] for x \u2208 X and y \u2208 Y .\nGiven a prior, a pool-based AL algorithm is equivalent to a policy for choosing training examples from X . A policy is a mapping from a partial labeling (labeling of a subset of X ) to the next unlabeled example to query. It can be represented by a policy tree whose nodes correspond to unlabeled examples to query, and edges from a node correspond to its labels. When an unlabeled example is queried, we receive its true label according to htrue. We will focus on adaptive policies, which use the observed labels of previously chosen examples to query the next unlabeled example."}, {"heading": "3 Robustness: Maximum Coverage Problem", "text": "We now consider the robustness of AL algorithms for the maximum coverage problem: find an adaptive policy maximizing the expected or worst-case utility given a budget of k queries (Cuong et al. 2013; Cuong, Lee, and Ye 2014). The utility is a non-negative function f(S, h) : 2X \u00d7H \u2192 R\u22650. Intuitively, a utility measures the value of querying examples S when the true labeling is h. Utilities for AL usually depend on the prior, so we shall use the notation fp(S, h) to denote that the utility fp depends on a distribution p overH. fp is said to be Lipschitz continuous (in the prior) with a Lipschitz constant L if for any S, h, and two priors p, p\u2032,\n|fp(S, h)\u2212 fp\u2032(S, h)| \u2264 L\u2016p\u2212 p\u2032\u2016, (1)\nwhere \u2016p\u2212p\u2032\u2016def= \u2211 h |p[h]\u2212p\u2032[h]| is the `1 distance between p and p\u2032. Lipschitz continuity implies boundedness.1\nAn AL algorithm is a mapping from a utility and a prior to a policy. Let x\u03c0h denote the set of examples selected by a policy \u03c0 when the true labeling is h. We now analyze the robustness of AL algorithms for both the average case and the worst case.\n1Choose an arbitrary p\u2032, then for any p, S, h, we have fp(S, h) \u2264 fp\u2032(S, h) + L\u2016p \u2212 p\u2032\u2016 \u2264 fp\u2032(S, h) + 2L \u2264 maxS,h fp\u2032(S, h) + 2L. Similarly, a lower bound exists."}, {"heading": "3.1 The Average Case", "text": "In this case, our objective is to find a policy with maximum expected utility. If p0 is the true prior, the expected utility of a policy \u03c0 is f avgp0 (\u03c0) def= Eh\u223cp0 [fp0(x\u03c0h, h)].\nWe consider the case where we have already chosen a utility, but still need to choose the prior. In practice, the choice is often subjective and may not be the true prior. A natural question is: if we choose a perturbed prior p1 (i.e., a prior not very different from the true prior p0 in terms of `1 distance), can an AL algorithm achieve performance competitive to that obtained using the true prior?\nOur first robustness result is for \u03b1-approximate algorithms that return an \u03b1-approximate policy of the optimal one. Formally, an average-case \u03b1-approximate (0 < \u03b1 \u2264 1) algorithmA outputs, for any prior p, a policyA(p) satisfying\nf avgp (A(p)) \u2265 \u03b1max \u03c0 f avgp (\u03c0).\nWhen \u03b1 = 1, we call A an exact algorithm. For notational convenience, we drop the dependency of A on the utility as we assumed a fixed utility here. We consider approximate algorithms because practical algorithms are generally approximate due to computational intractability of the problem. We have the following robustness result. The proof of this theorem is given in Appendix A. Theorem 1. Assume the utility fp is Lipschitz continuous with a Lipschitz constant L. LetM be an upper bound of fp. If A is an average-case \u03b1-approximate algorithm, then for any true prior p0 and any perturbed prior p1,\nf avgp0 (A(p1)) \u2265 \u03b1max\u03c0 f avg p0 (\u03c0)\u2212 (\u03b1+1)(L+M)\u2016p1\u2212p0\u2016.\nThus, A is robust in the sense that it returns a near \u03b1approximate policy when using a perturbed prior. f avgp0 (A(p1)) is the expected utility of the policy returned by A using p1 as prior. The expected utility is always computed with respect to the true prior p0. Theorem 1 shows that when we use a perturbed prior p1, the expected utility achieved by an average-case \u03b1-approximate algorithm degrades by at most a constant factor of the `1 distance between the perturbed prior and the true prior.\nApplication to Maximum Gibbs Error: Theorem 1 implies the robustness of the maximum Gibbs error algorithm (Cuong et al. 2013). This algorithm greedily selects the next example x\u2217 satisfying x\u2217 = argmaxx Ey\u223cpD[\u00b7;x][1\u2212 pD[y;x]], where pD is the current posterior and pD[y;x] is the probability (w.r.t. pD) that x has label y. In the binary-class and noiseless setting, it is equivalent to the generalized binary search algorithm (Dasgupta 2004; Nowak 2008; Golovin and Krause 2011). Consider the version space reduction utility fp(S, h) def= 1\u2212 p[h(S);S], where p[h(S);S] is the probability (w.r.t. p) that S has the labels h(S). We have the following corollary about the algorithm. The proof of this corollary is given in Appendix B. Corollary 1. If A is the maximum Gibbs error algorithm, then for any true prior p0 and any perturbed prior p1,\nf avgp0 (A(p1)) \u2265 ( 1\u2212 1\ne ) max \u03c0 f avgp0 (\u03c0)\u2212 ( 4\u2212 2 e ) \u2016p1\u2212 p0\u2016.\nApplication to Batch Maximum Gibbs Error: We can also obtain the robustness result for the batch version of the maximum Gibbs error algorithm. In the batch setting, the AL algorithm queries a batch of examples in each iteration instead of only one example (Hoi et al. 2006). The batch maximum Gibbs error algorithm is described in Algorithm 1 of (Cuong et al. 2013), and by Theorem 5 of the same work, it is a (1\u2212e\u2212(e\u22121)/e)-approximate algorithm for the version space reduction utility above. If we restrict the policies to only those in the batch setting, then from Theorem 1, we have the following corollary. Note that the range of the max operation in the corollary is restricted to only batch policies. Corollary 2. If A is the batch maximum Gibbs error algorithm, for any true prior p0 and any perturbed prior p1,\nf avgp0 (A(p1)) \u2265 ( 1\u2212 e\u2212(e\u22121)/e ) max \u03c0 f avgp0 (\u03c0)\n\u2212 ( 4\u2212 2e\u2212(e\u22121)/e ) \u2016p1 \u2212 p0\u2016."}, {"heading": "3.2 The Worst Case", "text": "In this case, our objective is to find a policy with maximum worst-case utility. If p0 is the true prior, the worst-case utility of a policy \u03c0 is fworstp0 (\u03c0) def= minh [fp0(x \u03c0 h, h)].\nAn algorithm A is a worst-case \u03b1-approximate algorithm (0 < \u03b1 \u2264 1) if for any prior p, we have fworstp (A(p)) \u2265 \u03b1max\u03c0 fworstp (\u03c0). When \u03b1 = 1, A is an exact algorithm.\nFor worst-case \u03b1-approximate algorithms, we can obtain a robustness result similar to Theorem 1. The proof of the following theorem is given in Appendix C. Theorem 2. Assume fp is Lipschitz continuous with a Lipschitz constant L. If A is a worst-case \u03b1-approximate algorithm, then for any true prior p0 and perturbed prior p1, fworstp0 (A(p1)) \u2265 \u03b1max\u03c0 f worst p0 (\u03c0)\u2212 (\u03b1+ 1)L\u2016p1 \u2212 p0\u2016.\nThe worst-case utility is also computed with respect to the true prior p0 (i.e., using fworstp0 instead of f worst p1 ). Theorem 2 shows that when we use a perturbed prior, the worst-case utility achieved by a worst-case \u03b1-approximate algorithm degrades by at most a constant factor of the `1 distance between the perturbed prior and the true prior.\nApplication to Least Confidence: Theorem 2 implies the robustness of the well-known least confidence AL algorithm (Lewis and Gale 1994; Culotta and McCallum 2005) with perturbed priors. This algorithm greedily selects the next example x\u2217 satisfying x\u2217 = argminx{maxy\u2208Y pD[y;x]}. If fp is the version space reduction utility (considered previously for the maximum Gibbs error algorithm), we have the following corollary. The proof of this corollary is given in Appendix D. Corollary 3. If A is the least confidence algorithm, then for any true prior p0 and any perturbed prior p1,\nfworstp0 (A(p1)) \u2265 ( 1\u22121\ne ) max \u03c0 fworstp0 (\u03c0)\u2212 ( 2\u22121 e ) \u2016p1\u2212p0\u2016.\nApplication to Generalized Maximum Gibbs Error: Theorem 2 also implies the robustness of the worst-case generalized Gibbs error algorithm (Cuong, Lee, and Ye 2014)\nwith a bounded loss. Intuitively, the algorithm greedily maximizes in the worst case the total generalized version space reduction, which is defined as\ntp(S, h) def= \u2211\nh\u2032,h\u2032\u2032:h\u2032(S)6=h(S) or h\u2032\u2032(S)6=h(S)\np[h\u2032]L(h\u2032, h\u2032\u2032) p[h\u2032\u2032],\nwhere L is a non-negative loss function between labelings that satisfies L(h, h\u2032) = L(h\u2032, h) and L(h, h) = 0 for all h, h\u2032. The worst-case generalized Gibbs error algorithm attempts to greedily maximize tworstp0 (\u03c0) def= minh tp0(x \u03c0 h, h), and it is a worst-case (1 \u2212 1/e)-approximate algorithm for this objective (Cuong, Lee, and Ye 2014).\nIf we assume L is upper bounded by a constant m, we have the following corollary about this algorithm. The proof of this corollary is given in Appendix E. Note that the bounded loss assumption is reasonable since it holds for various practical loss functions such as Hamming loss or F1 loss, which is 1\u2212 F1(h, h\u2032) where F1(h, h\u2032) is the F1 score between h and h\u2032. Corollary 4. If A is the worst-case generalized Gibbs error algorithm and the loss function of interest is upper bounded by a constant m \u2265 0, then for any true prior p0 and any perturbed prior p1,\ntworstp0 (A(p1)) \u2265 ( 1\u2212 1\ne ) max \u03c0 tworstp0 (\u03c0)\u2212m ( 4\u2212 2 e ) \u2016p1\u2212p0\u2016."}, {"heading": "3.3 Discussions", "text": "We emphasize that our results are important as they enhance our understanding and confidence about existing AL algorithms. Furthermore, if the utility we want to maximize is not Lipschitz continuous, then even an exact AL algorithm for perturbed priors may not be robust, both in the average and worst cases. We prove this in Theorem 3 below (see Appendix F for proof). Theorem 3. For both the average and worst cases, there is an AL problem with a non-Lipschitz utility such that: for any C,\u03b1, > 0, there exist a perturbed prior p1 satisfying 0 < \u2016p1 \u2212 p0\u2016 < and an exact algorithm A\u2217 satisfying\nf cp0(A \u2217(p1)) < \u03b1max \u03c0 f cp0(\u03c0)\u2212 C\u2016p1 \u2212 p0\u2016,\nwhere f cp0 \u2208 {f avg p0 , f worst p0 } respectively.\nThis theorem and our results above suggest we should use a Lipschitz utility for AL to maintain the robustness.\nBy taking p1 = p0, Corollaries 1 and 2 can recover the approximation ratios for the maximum Gibbs error and batch maximum Gibbs error algorithms in Theorems 4 and 5 of (Cuong et al. 2013) respectively. Similarly, Corollaries 3 and 4 can recover the ratios for the least confidence and generalized Gibbs error algorithms in Theorems 5 and 8 of (Cuong, Lee, and Ye 2014) respectively. Thus, our corollaries are generalizations of these previous theorems.\nIf A is \u03b1-approximate (in the average or worst case) with an optimal constant \u03b1 under some computational complexity assumption (Golovin and Krause 2011), then it is also optimal in our theorems under the same assumption. This can be proven easily by contradiction and setting p1 = p0.\nIf we are only interested in some particular prior p0 and the perturbed priors within a neighborhood of p0, we can relax the Lipschitz assumption (1) to the locally Lipschitz assumption at p0: there exist L and \u03b4 such that for all S, h, and p, if \u2016p0\u2212p\u2016 < \u03b4, then |fp0(S, h)\u2212fp(S, h)| \u2264 L\u2016p0\u2212p\u2016. Under this relaxed assumption, the theorems and corollaries above still hold for any p1 satisfying \u2016p0 \u2212 p1\u2016 < \u03b4."}, {"heading": "4 Robustness: Minimum Cost Problem", "text": "In this section, we investigate the robustness of AL algorithms for the minimum cost problem in the average case: find an adaptive policy minimizing the expected number of queries to identify the true labeling htrue (Golovin and Krause 2011). This problem assumes htrue is drawn from a prior p0 on a small hypothesis space H (i.e., H does not need to contain all functions from X to Y). After we make a query and observe a label, all the hypotheses inconsistent with the observed label are removed from the current hypothesis space (also called the version space). We stop when there is only one hypothesis htrue left.\nWe do not consider this problem in the worst case because even the optimal worst-case algorithm may not be robust.2 For instance, if the true prior gives probability 1 to one correct hypothesis but the perturbed prior gives positive probabilities to all the hypotheses, then the cost of using the true prior is 0 while the cost of using the perturbed prior is |X |.\nFor any policy \u03c0 and hypothesis h, let c(\u03c0, h) be the cost of identifying h when running \u03c0. This is the length of the path corresponding to h in the policy tree of \u03c0. For any prior p0 and policy \u03c0, the expected cost of \u03c0 with respect to the prior p0 is defined as c avg p0 (\u03c0)\ndef= Eh\u223cp0 [c(\u03c0, h)]. We will consider \u03b1(p)-approximate algorithms that return a policy whose expected cost is within an \u03b1(p)-factor of the optimal one. Formally, for any prior p, an \u03b1(p)-approximate (\u03b1(p) \u2265 1) algorithm A outputs a policy A(p) satisfying\ncavgp (A(p)) \u2264 \u03b1(p)min \u03c0 cavgp (\u03c0).\nNote that \u03b1(p) may depend on the prior p. When \u03b1(p) = 1, A is an exact algorithm. We have the following robustness result for the minimum cost problem in the average case. The proof of this theorem is given in Appendix G. Theorem 4. Assume c(\u03c0, h) is upper bounded by a constant K for all \u03c0, h. If A is an \u03b1(p)-approximate algorithm, then for any true prior p0 and any perturbed prior p1,\ncavgp0 (A(p1)) \u2264 \u03b1(p1)min\u03c0 c avg p0 (\u03c0)+(\u03b1(p1)+1)K\u2016p1\u2212p0\u2016.\nThe assumption c(\u03c0, h) \u2264 K for all \u03c0, h is reasonable since c(\u03c0, h) \u2264 |X | for all \u03c0, h. WhenH is small, K can be much smaller than |X |.\nApplication to Generalized Binary Search: Theorem 4 implies the robustness of the generalized binary search algorithm, which is known to be (ln 1minh p[h] + 1)-approximate (Golovin and Krause 2011). The result is stated in the corollary below. By taking p1 = p0, this corollary can recover the previous result by Golovin and Krause (2011) for the generalized binary search algorithm.\n2The Lipschitz assumption is not satisfied in this setting.\nCorollary 5. Assume c(\u03c0, h) is upper bounded by K for all \u03c0, h. If A is the generalized binary search algorithm, then for any true prior p0 and any perturbed prior p1,\ncavgp0 (A(p1)) \u2264 ( ln\n1\nminh p1[h] + 1 ) min \u03c0 cavgp0 (\u03c0)\n+ ( ln\n1\nminh p1[h] + 2\n) K\u2016p1 \u2212 p0\u2016.\nTheorem 4 can also provide the robustness of algorithms for problems other than AL. For instance, it can provide the robustness of the RAId algorithm for the adaptive informative path planning problem (Lim, Hsu, and Lee 2015)."}, {"heading": "5 Mixture Prior", "text": "Let us consider methods that minimize a regularized loss. These methods are commonly used and known to be equivalent to finding the maximum a posteriori hypothesis with an appropriate prior. In practice, the best regularization constant is usually unknown, and a common technique (in passive learning) is to split the available data set into a training and a validation set, which is used to select the best regularization constant based on performance of the algorithm trained on the training set. As this method is effective in practice, we construct a Bayesian version and study its performance, particularly the robustness, when used with AL. We assume that we have a candidate set of prior distributions corresponding to different regularization constants, and the true hypothesis is randomly generated by first randomly selecting a distribution and then selecting a hypothesis using that distribution. This corresponds to assuming that the prior distribution is the mixture distribution. For simplicity, we consider the uniform mixture in this work.\nFirst, we note that optimizing the expected cost of the mixture directly has a lower expected cost than trying to separately identify the appropriate component (corresponding to using a validation set in passive learning) and the best hypothesis given the component (corresponding to using the training set). Hence, we would expect the method to perform favorably in comparison to passive learning when the mixture prior is the true prior.\nResults in earlier sections assure us that the method is near optimal when the mixture prior is incorrect but generates hypotheses with probabilities similar to the true prior. What if the true prior is far from the mixture distribution in the `1 distance? In particular, we are interested in the case where the true distribution is one of the mixture components, rather than the mixture itself. Theorem 5 below provides bounds on the performance in such cases (see Appendix H for proof). We note that the theorem holds for general priors that may vary in form (e.g., with different probability mass functions) and is not restricted to priors corresponding to regularization constants.\nThe bounds show that the performance of the mixture is competitive with that of the optimal algorithm, although the constant can be large if some hypotheses have small probabilities under the true distribution. We also provide an absolute bound (instead of competitive bound) which may be more informative in cases where there are hypotheses with\nAlgorithm 1 Active learning for the mixture prior model Input: A set of n priors {p1, p2, . . . , pn}, the initial normalized weights for the priors {w1, w2, . . . , wn}, and the budget of k queries. pi0 \u2190 pi; wi0 \u2190 wi; for all i = 1, 2, . . . , n; for t = 1 to k do\nChoose an unlabeled example x\u2217 based on an AL criterion; y\u2217 \u2190 Query-label(x\u2217); Update and normalize weights:\nwit \u221d wit\u22121 pit\u22121[y\u2217;x\u2217] for all i = 1, 2, . . . , n; Update each posterior individually using Bayes\u2019 rule:\npit[h] \u221d pit\u22121[h]P[h(x\u2217) = y\u2217 | h] for each i = 1, 2, . . . , n and h \u2208 H;\nend for return {p1k, p2k, . . . , pnk} and {w1k, w2k, . . . , wnk};\nsmall probabilities. The bound (the first bound in Theorem 5) shows that the expected cost is within a constant factor of the optimal expected cost of the mixture, which is the expected cost we would have to pay if our model was correct. The optimal expected cost of the mixture is in turn better than the expected cost of any two-stage identification procedure that identifies the component and the hypothesis given the component separately, assuming the expectation is taken with respect to the mixture. Theorem 5. If A is an \u03b1(p)-approximate algorithm for the minimum cost problem, then for any true prior p0 and any k component uniform mixture prior p1 = \u2211k i=1 1 kp1,i such that p0 \u2208 {p1,i}ki=1, we have cavgp0 (A(p1)) \u2264 k\u03b1(p1)min\u03c0 c avg p1 (\u03c0), and\ncavgp0 (A(p1)) \u2264 \u03b1(p1) ( k \u2212 1 minh p0[h] + 1 ) min \u03c0 cavgp0 (\u03c0)."}, {"heading": "As a result, if A is generalized binary search, then", "text": "cavgp0 (A(p1)) \u2264 k ( ln kminh p0[h] + 1 ) min \u03c0 cavgp1 (\u03c0), and\ncavgp0 (A(p1)) \u2264 ( ln kminh p0[h]+1 )( k\u22121 minh p0[h] +1 ) min \u03c0 cavgp0 (\u03c0).\nThe algorithm for greedy AL with the mixture model is shown in Algorithm 1. In the algorithm, the unlabeled example x\u2217 can be chosen using any AL criterion. The criterion can be computed from the weights and posteriors obtained from the previous iteration. For instance, if the maximum Gibbs error algorithm is used, then at iteration t, we have x\u2217 = argmaxx Ey\u223cp[\u00b7;x][1\u2212 p[y;x]], where p[y;x] = \u2211n i=1 w i t\u22121 p i t\u22121[y;x]. After x\n\u2217 is chosen, we query its label y\u2217 and update the new weights and posteriors, which are always normalized so that \u2211 i w i t = 1 for all\nt and \u2211 h p i t[h] = 1 for all i and t. The algorithm returns the final weights and posteriors that can be used to make predictions on new examples. More specifically, the predicted label of a new example x is argmaxy \u2211n i=1 w i k p i k[y;x].\nWe note that Algorithm 1 does not require the hypotheses to be deterministic. In fact, the algorithm can be used with\nTable 1: AUCs of maximum Gibbs error algorithm with 1/\u03c32 = 0.01, 0.1, 1, 10 and the mixture prior model on 20 Newsgroups data set (upper half) and UCI data set (lower half). Double asterisks (**) indicate the best score, while an asterisk (*) indicates the second best score on a row (without the last column). The last column shows the AUCs of passive learning with the mixture prior model for comparison.\nData set 0.01 0.1 1 10 Mixture Mixture (Passive)\nalt.atheism/comp.graphics 87.60** 87.25 84.94 81.46 87.33* 83.92 talk.politics.guns/talk.politics.mideast 80.71** 79.28 74.57 66.76 79.49* 76.34 comp.sys.mac.hardware/comp.windows.x 78.75** 78.21* 75.07 70.54 78.21* 75.02 rec.motorcycles/rec.sport.baseball 86.20** 85.39 82.23 77.35 85.59* 81.56\nsci.crypt/sci.electronics 78.08** 77.35 73.92 68.72 77.42* 73.08 sci.space/soc.religion.christian 86.09** 85.12 81.48 75.51 85.50* 80.31 soc.religion.christian/talk.politics.guns 86.16** 85.01 80.91 74.03 85.46* 81.81 Average (20 Newsgroups) 83.37** 82.52 79.02 73.48 82.71* 78.86\nAdult 79.38 80.15 80.39** 79.68 80.18* 77.41 Breast cancer 88.28* 88.37** 86.95 83.82 88.14 89.07\nDiabetes 65.09* 64.53 64.39 65.48** 64.82 64.24 Ionosphere 82.80* 82.76 81.48 77.88 82.95** 81.91 Liver disorders 66.31** 64.16 61.42 58.42 64.73* 65.89 Mushroom 90.73** 89.56 84.14 82.94 90.33* 73.38\nSonar 66.75** 65.45* 63.74 60.81 65.00 66.53 Average (UCI) 77.05** 76.43 74.64 72.72 76.59* 74.06\nprobabilistic hypotheses where P[h(x) = y | h] \u2208 [0, 1]. We also note that computing pit[y;x] for a posterior p i t can be expensive. In this work, we approximate it using the MAP hypothesis. In particular, we assume pit[y;x] \u2248 piMAP[y;x], the probability that x has label y according to the MAP hypothesis of the posterior pit. This is used to approximate both the AL criterion and the predicted label of a new example."}, {"heading": "6 Experiments", "text": "We report experimental results with different priors and the mixture prior. We use the logistic regression model with different L2 regularizers, which are well-known to impose a Gaussian prior with mean zero and variance \u03c32 on the parameter space. Thus, we can consider different priors by varying the variance \u03c32 of the regularizer. We consider two experiments with the maximum Gibbs error algorithm. Since our data sets are all binary classification, this algorithm is equivalent to the least confidence and the maximum entropy algorithms. In our first experiment, we compare models that use different priors (equivalently, regularizers). In the second experiment, we run the uniform mixture prior model and compare it with models that use only one prior. For AL, we randomly choose the first 10 examples as a seed set. The scores are averaged over 100 runs of the experiments with different seed sets."}, {"heading": "6.1 Experiment with Different Priors", "text": "We run maximum Gibbs error with 1/\u03c32 = 0.01, 0.1, 0.2, 1, 10 on tasks from the 20 Newsgroups and UCI data sets (Joachims 1996; Bache and Lichman 2013) shown in the first column of Table 1. Figure 1 shows the average areas under the\n78.88 76.99 75.58\n69.43\n58.62\n83.37 82.52 81.78 79.02\n73.48\n0.01 0.1 0.2 1 10\n(a)\nPassive Active\n74.07 72.48\n71.22\n66.80\n61.50\n77.05 76.43 75.86 74.64\n72.72\n0.01 0.1 0.2 1 10\n(b)\nPassive Active\n. . .\n.\n.\n. . . .\n.\n. . .\n( )\ni i\n. .\n.\n.\n.\n. . . .\n.\n. . .\n( )\ni ti\nFigure 1: Average AUCs for passive learning and maximum Gibbs error AL algorithms with 1/\u03c32 = 0.01, 0.1, 0.2, 1, and 10 on the 20 Newsgroups (a) and UCI (b) data sets.\naccuracy curves (AUC) on the first 150 selected examples for the different regularizers. Figures 1a and 1b give the average AUCs (computed on a separate test set) for the 20 Newsgroups and UCI data sets respectively. We also compare the scores for AL with passive learning.\nFrom Figure 1, AL is better than passive learning for all the regularizers. When the regularizers are close to each other (e.g., 1/\u03c32 = 0.1 and 0.2), the corresponding scores tend to be close. When they are farther apart (e.g., 1/\u03c32 = 0.1 and 10), the scores also tend to be far from each other. In\nsome sense, this confirms our results in previous sections."}, {"heading": "6.2 Experiment with Mixture Prior", "text": "We investigate the performance of the mixture prior model proposed in Algorithm 1. For AL, it is often infeasible to use a validation set to choose the regularizers beforehand because we do not initially have any labeled data. So, using the mixture prior is a reasonable choice in this case.\nWe run the uniform mixture prior with regularizers 1/\u03c32 = 0.01, 0.1, 1, 10 and compare it with models that use only one of these regularizers. Table 1 shows the AUCs of the first 150 selected examples for these models on the 20 Newsgroups and the UCI data sets.\nFrom the results, the mixture prior model achieves the second best AUCs for all tasks in the 20 Newsgroups data set. For the UCI data set, the model achieves the best score on Ionosphere and the second best scores on three other tasks. For the remaining three tasks, it achieves the third best scores. On average, the mixture prior model achieves the second best scores for both data sets. Thus, the model performs reasonably well given the fact that we do not know which regularizer is the best to use for the data. We also note that if a bad regularizer is used (e.g., 1/\u03c32 = 10), AL may be even worse than passive learning with mixture prior."}, {"heading": "7 Conclusion", "text": "We proved new robustness bounds for AL with perturbed priors that can be applied to various AL algorithms used in practice. We showed that if the utility is not Lipschitz, an optimal algorithm on perturbed priors may not be robust. Our results suggest that we should use a Lipschitz utility for AL if robustness is required. We also proved novel robustness bounds for a uniform mixture prior and showed experimentally that this prior is reasonable in practice.\nAcknowledgments. We gratefully acknowledge the support of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS), and of QUT through a Vice Chancellor\u2019s Research Fellowship. We also gratefully acknowledge the support of Singapore MOE AcRF Tier Two grant R-265-000-443-112.\nAppendix"}, {"heading": "A Proof of Theorem 1", "text": "Let C = L+M . For any policy \u03c0, note that:\n|f avgp0 (\u03c0)\u2212 f avg p1 (\u03c0)| = |( \u2211 h p0[h]fp0(x \u03c0 h, h)\u2212 \u2211 h p0[h]fp1(x \u03c0 h, h))\n+( \u2211 h p0[h]fp1(x \u03c0 h, h)\u2212 \u2211 h p1[h]fp1(x \u03c0 h, h))|\n\u2264 C\u2016p1 \u2212 p0\u2016,\nwhere the last inequality holds due to the Lipschitz continuity and boundedness of the utility function fp. Thus, if\n\u03c01 = argmax\u03c0 f avg p1 (\u03c0) and \u03c00 = argmax\u03c0 f avg p0 (\u03c0), it follows that:\nf avgp1 (\u03c01) \u2265 f avg p1 (\u03c00) \u2265 f avg p0 (\u03c00)\u2212 C\u2016p1 \u2212 p0\u2016, and\nf avgp0 (\u03c0) \u2265 f avg p1 (\u03c0)\u2212 C\u2016p1 \u2212 p0\u2016 for all \u03c0.\nHence,\nf avgp0 (A(p1)) \u2265 f avg p1 (A(p1))\u2212 C\u2016p1 \u2212 p0\u2016\n\u2265 \u03b1f avgp1 (\u03c01)\u2212 C\u2016p1 \u2212 p0\u2016 \u2265 \u03b1(f avgp0 (\u03c00)\u2212 C\u2016p1 \u2212 p0\u2016)\u2212 C\u2016p1 \u2212 p0\u2016 = \u03b1max\n\u03c0 f avgp0 (\u03c0)\u2212 C(\u03b1+ 1)\u2016p1 \u2212 p0\u2016,\nwhere the first and third inequalities are from the above discussions and the second inequality holds as A is \u03b1approximate."}, {"heading": "B Proof of Corollary 1", "text": "Cuong et al. (2013) showed that the maximum Gibbs error algorithm provides a constant factor approximation to the optimal policy Gibbs error, which is equivalent to the expected version space reduction f avgp (\u03c0). Formally, they showed that, for any prior p,\nf avgp (A(p)) \u2265 ( 1\u2212 1\ne ) max \u03c0 f avgp (\u03c0),\nwhere A is the maximum Gibbs error algorithm. That is, the algorithm is average-case (1\u2212 1/e)-approximate.\nFurthermore, the version space reduction utility is upper bounded by M = 1; and for any priors p, p\u2032, we also have\n|fp(S, h)\u2212 fp\u2032(S, h)| = |p\u2032[h(S);S]\u2212 p[h(S);S]| = |\n\u2211 h\u2032 p\u2032[h\u2032]P[h\u2032(S) = h(S)|h\u2032]\n\u2212 \u2211 h\u2032 p[h\u2032]P[h\u2032(S) = h(S)|h\u2032]|\n\u2264 \u2016p\u2212 p\u2032\u2016.\nThus, the version space reduction utility is Lipschitz continuous with L = 1 and is upper bounded by M = 1. Hence, Corollary 1 follows from Theorem 1."}, {"heading": "C Proof of Theorem 2", "text": "Let \u03c00 = argmax\u03c0 fworstp0 (\u03c0) and \u03c01 = argmax\u03c0 f worst p1 (\u03c0). We have fworstp1 (\u03c01) \u2265 f worst p1 (\u03c00) = fp1(x \u03c00 h0 , h0), where h0 = argminh fp1(x \u03c00 h , h). Using the Lipschitz continuity of fp and the definition of fworstp0 , we have\nfp1(x \u03c00 h0 , h0) \u2265 fp0(x \u03c00 h0 , h0)\u2212 L\u2016p0 \u2212 p1\u2016\n\u2265 min h fp0(x \u03c00 h , h)\u2212 L\u2016p0 \u2212 p1\u2016 = fworstp0 (\u03c00)\u2212 L\u2016p0 \u2212 p1\u2016.\nThus, fworstp1 (\u03c01) \u2265 f worst p0 (\u03c00)\u2212 L\u2016p0 \u2212 p1\u2016.\nLet \u03c0 = A(p1) and h\u2217 = argminh fp0(x \u03c0 h, h). We have\nfworstp0 (\u03c0) = minh fp0(x \u03c0 h, h) = fp0(x \u03c0 h\u2217 , h \u2217). By the Lipschitz continuity of fp, we have\nfp0(x \u03c0 h\u2217 , h \u2217) \u2265 fp1(x\u03c0h\u2217 , h\u2217)\u2212 L\u2016p0 \u2212 p1\u2016 \u2265 min\nh fp1(x\n\u03c0 h, h)\u2212 L\u2016p0 \u2212 p1\u2016\n= fworstp1 (\u03c0)\u2212 L\u2016p0 \u2212 p1\u2016 \u2265 \u03b1max\n\u03c0 fworstp1 (\u03c0)\u2212 L\u2016p0 \u2212 p1\u2016\n= \u03b1fworstp1 (\u03c01)\u2212 L\u2016p0 \u2212 p1\u2016,\nwhere the last inequality holds asA is \u03b1-approximate. Using the inequality relating fworstp1 (\u03c01) and f worst p0 (\u03c00) above, we now have\nfworstp0 (\u03c0) \u2265 \u03b1(f worst p0 (\u03c00)\u2212 L\u2016p0 \u2212 p1\u2016)\u2212 L\u2016p0 \u2212 p1\u2016\n= \u03b1max \u03c0\nfworstp0 (\u03c0)\u2212 (\u03b1+ 1)L\u2016p0 \u2212 p1\u2016."}, {"heading": "D Proof of Corollary 3", "text": "Cuong, Lee, and Ye (2014) have shown that using the least confidence algorithm can achieve a constant factor approximation to the optimal worst-case version space reduction. Formally, if fp(S, h) is the version space reduction utility (that was considered previously for the maximum Gibbs error algorithm), then fworstp (\u03c0) is the worst-case version space reduction of \u03c0, and it was shown (Cuong, Lee, and Ye 2014) that, for any prior p,\nfworstp (A(p)) \u2265 ( 1\u2212 1\ne ) max \u03c0 fworstp (\u03c0),\nwhere A is the least confidence algorithm. That is, the least confidence algorithm is worst-case (1\u2212 1/e)-approximate.\nSince the version space reduction utility is Lipschitz continuous with L = 1 as shown in the proof of Corollary 1, Corollary 3 follows from Theorem 2."}, {"heading": "E Proof of Corollary 4", "text": "It was shown by Cuong, Lee, and Ye (2014) that, for any prior p,\ntworstp (A(p)) \u2265 ( 1\u2212 1\ne ) max \u03c0 tworstp (\u03c0),\nwhere A is the worst-case generalized Gibbs error algorithm. That is, the worst-case generalized Gibbs error algorithm is worst-case (1\u2212 1/e)-approximate.\nIf we assume the loss function L is upper bounded by a constant m, then tp is Lipschitz continuous with L = 2m.\nIndeed, for any S, h, p, and p\u2032, we have |tp(S, h)\u2212 tp\u2032(S, h)|\n= | \u2211\nh\u2032(S)6=h(S) or h\u2032\u2032(S) 6=h(S)\nL(h\u2032, h\u2032\u2032)(p[h\u2032]p[h\u2032\u2032]\u2212 p\u2032[h\u2032]p\u2032[h\u2032\u2032])|\n\u2264 m \u2211\nh\u2032(S)6=h(S) or h\u2032\u2032(S) 6=h(S)\n|p[h\u2032]p[h\u2032\u2032]\u2212 p\u2032[h\u2032]p\u2032[h\u2032\u2032]|\n= m \u2211\nh\u2032(S)6=h(S) or h\u2032\u2032(S) 6=h(S)\n|(p[h\u2032]\u2212 p\u2032[h\u2032])p[h\u2032\u2032]\n+ p\u2032[h\u2032](p[h\u2032\u2032]\u2212 p\u2032[h\u2032\u2032])| \u2264 m \u2211 h\u2032,h\u2032\u2032 (|p[h\u2032]\u2212 p\u2032[h\u2032]|p[h\u2032\u2032] + p\u2032[h\u2032]|p[h\u2032\u2032]\u2212 p\u2032[h\u2032\u2032]|)\n= 2m\u2016p\u2212 p\u2032\u2016. Thus, Corollary 4 follows from Theorem 2."}, {"heading": "F Proof of Theorem 3", "text": "For both the average and worst cases, consider the AL problem with budget k = 1 and the utility fp(S, h) = |{h\u2032 : p[h\u2032] > \u00b5 and h\u2032(S) 6= h(S)}|, for some very small \u00b5 > 0 in the worst case and \u00b5 = 0 in the average case.\nThis utility returns the number of hypotheses that have a significant probability (greater than \u00b5) and are not consistent with h on S. When \u00b5 = 0, it is the number of hypotheses pruned from the version space. So, this is a reasonable utility to maximize for AL. It is easy to see that this utility is nonLipschitz.\nConsider the case where there are two examples x0, x1 and 4 hypotheses h1, . . . , h4 with binary labels given according to the following table.\nHypothesis x0 x1 h1 0 0 h2 1 0 h3 0 1 h4 1 1\nConsider the true prior p0 where p0[h1] = p0[h2] = 12\u2212\u00b5 and p0[h3] = p0[h4] = \u00b5, and a perturbed prior p1 where p1[h1] = p1[h2] = 1 2 \u2212 \u00b5\u2212 \u03b4 and p1[h3] = p1[h4] = \u00b5+ \u03b4, for some small \u03b4 > 0. With budget k = 1, there are two possible policies: the policy \u03c00 which chooses x0 and the policy \u03c01 which chooses x1. Let A\u2217(p1) = \u03c01. Note that f avg p1 (\u03c01) = 2( 1 2 \u2212 \u00b5\u2212 \u03b4) + 2( 12 \u2212 \u00b5 \u2212 \u03b4) + 2(\u00b5 + \u03b4) + 2(\u00b5 + \u03b4) = 2, and f avg p1 (\u03c00) = 2( 12 \u2212 \u00b5\u2212 \u03b4) + 2( 1 2 \u2212 \u00b5\u2212 \u03b4) + 2(\u00b5+ \u03b4) + 2(\u00b5+ \u03b4) = 2. Thus, \u03c01 is an average-case optimal policy for p1 and A\u2217 is an exact algorithm for p1 in the average case.\nSimilarly, fworstp1 (\u03c01) = 2 = f worst p1 (\u03c00). Thus, \u03c01 is a worst-case optimal policy for p1 and A\u2217 is an exact algorithm for p1 in the worst case. Hence, A\u2217 is an exact algorithm for p1 in both average and worst cases.\nConsidering p0, we have f avg p0 (\u03c01) = 0( 1 2 \u2212 \u00b5) + 0( 1 2 \u2212 \u00b5) + 2\u00b5 + 2\u00b5 = 0 since \u00b5 = 0 in the average case. On the other hand, f avgp0 (\u03c00) = 1( 1 2 \u2212\u00b5)+1( 1 2 \u2212\u00b5)+1\u00b5+1\u00b5 = 1. Similarly, in the worst case, we also have fworstp0 (\u03c01) = 0 and fworstp0 (\u03c00) = 1. Thus, \u03c00 is the optimal policy for p0 in both average and worst cases. Now given any C,\u03b1, > 0, we can choose a small enough \u03b4 such that \u2016p1 \u2212 p0\u2016 < and \u03b1\u2212 C\u2016p1 \u2212 p0\u2016 > 0. Hence, Theorem 3 holds."}, {"heading": "G Proof of Theorem 4", "text": "For any policy \u03c0, note that\n|cavgp0 (\u03c0)\u2212 c avg p1 (\u03c0)| = | \u2211 h p0[h]c(\u03c0, h)\u2212 \u2211 h p1[h]c(\u03c0, h)|\n= | \u2211 h (p0[h]\u2212 p1[h])c(\u03c0, h)|\n\u2264 K\u2016p0 \u2212 p1\u2016,\nwhere the last inequality holds as c(\u03c0, h) is upper bounded by K. Thus,\ncavgp0 (\u03c0) \u2264 c avg p1 (\u03c0) +K\u2016p0 \u2212 p1\u2016, for all \u03c0, p0, p1.\nLet \u03c00 = argmin\u03c0 c avg p0 (\u03c0). We have\ncavgp0 (A(p1)) \u2264 c avg p1 (A(p1)) +K\u2016p0 \u2212 p1\u2016\n\u2264 \u03b1(p1)min \u03c0 cavgp1 (\u03c0) +K\u2016p0 \u2212 p1\u2016 \u2264 \u03b1(p1)cavgp1 (\u03c00) +K\u2016p0 \u2212 p1\u2016 \u2264 \u03b1(p1)(cavgp0 (\u03c00) +K\u2016p0 \u2212 p1\u2016) +K\u2016p0 \u2212 p1\u2016 = \u03b1(p1)c avg p0 (\u03c00) + (\u03b1(p1) + 1)K\u2016p0 \u2212 p1\u2016 = \u03b1(p1)min \u03c0 cavgp0 (\u03c0) + (\u03b1(p1) + 1)K\u2016p0 \u2212 p1\u2016,\nwhere the first and fourth inequalities are from the discussion above, and the second inequality is from the fact that A is \u03b1(p)-approximate."}, {"heading": "H Proof of Theorem 5", "text": "If k = 1, then p1 = p0 and Theorem 5 trivially holds. Consider k \u2265 2. For any h, since\np0[h] p1[h] = p0[h]\u2211k i=1 1 kp1,i[h] \u2264 p0[h]1 kp0[h] = k,\nwe have k\u22121 \u2265 1\u2212 p0[h]p1[h] \u2265 1\u2212k. Thus, |1\u2212 p0[h] p1[h] | \u2264 k\u22121.\nHence, for any policy \u03c0,\n|cavgp1 (\u03c0)\u2212 c avg p0 (\u03c0)| = | \u2211 h p1[h](1\u2212 p0[h] p1[h] )c(\u03c0, h)|\n\u2264 (k \u2212 1) \u2211 h p1[h]c(\u03c0, h) = (k \u2212 1)cavgp1 (\u03c0).\nTherefore, cavgp0 (\u03c0) \u2264 kc avg p1 (\u03c0).\nOn the other hand, for any h, we have\np1[h] p0[h] =\n\u2211k i=1 1 kp1,i[h]\np0[h] =\n1 kp0[h] + \u2211 i:p1,i 6=p0 1 kp1,i[h]\np0[h]\n\u2264 1 k +\nk\u22121 k\nminh p0[h] =\n1 k + k \u2212 1 kminh p0[h] .\nThus, 1 \u2212 1k \u2265 1 \u2212 p1[h] p0[h] \u2265 1 \u2212 1k \u2212 k\u22121 kminh p0[h] . When H contains at least 2 hypothesis, minh p0[h] \u2264 1/2, and 1 k + k\u22121 kminh p0[h]\n\u22121 \u2265 1\u2212 1k (the case whenH is a singleton is equivalent to k = 1). Hence,\n|1\u2212 p1[h] p0[h] | \u2264 1 k + k \u2212 1 kminh p0[h] \u2212 1.\nWe have\n|cavgp0 (\u03c0)\u2212 c avg p1 (\u03c0)| = | \u2211 h p0[h](1\u2212 p1[h] p0[h] )c(\u03c0, h)|\n\u2264 ( 1 k + k \u2212 1 kminh p0[h] \u2212 1)cavgp0 (\u03c0).\nTherefore, cavgp1 (\u03c0) \u2264 ( 1k + k\u22121 kminh p0[h] )cavgp0 (\u03c0).\nNow let \u03c00 = argmin\u03c0 c avg p0 (\u03c0). We have\ncavgp0 (A(p1)) \u2264 kc avg p1 (A(p1))\n\u2264 k\u03b1(p1)min \u03c0 cavgp1 (\u03c0) (first part) \u2264 k\u03b1(p1)cavgp1 (\u03c00)\n\u2264 k\u03b1(p1)( 1\nk + k \u2212 1 kminh p0[h] )cavgp0 (\u03c00)\n= \u03b1(p1)(1 + k \u2212 1\nminh p0[h] )min \u03c0 cavgp0 (\u03c0),\nwhere the first and fourth inequalities are from the discussions above, and the second inequality is from the fact that A is \u03b1(p)-approximate.\nIf A is the generalized binary search algorithm, then \u03b1(p1) = ln 1minh p1[h] + 1. Note that\nminh p1[h] = minh \u2211k i=1 1 kp1,i[h] \u2265 minh 1 kp0[h]. Thus, \u03b1(p1) \u2264 ln kminh p0[h] + 1. Therefore, cavgp0 (A(p1)) \u2264 (ln kminh p0[h]+1)( k\u22121 minh p0[h] +1)min \u03c0 cavgp0 (\u03c0)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We study the robustness of active learning (AL) algorithms against prior misspecification: whether an algorithm achieves similar performance using a perturbed prior as compared to using the true prior. In both the average and worst cases of the maximum coverage setting, we prove that all \u03b1-approximate algorithms are robust (i.e., near \u03b1-approximate) if the utility is Lipschitz continuous in the prior. We further show that robustness may not be achieved if the utility is non-Lipschitz. This suggests we should use a Lipschitz utility for AL if robustness is required. For the minimum cost setting, we can also obtain a robustness result for approximate AL algorithms. Our results imply that many commonly used AL algorithms are robust against perturbed priors. We then propose the use of a mixture prior to alleviate the problem of prior misspecification. We analyze the robustness of the uniform mixture prior and show experimentally that it performs reasonably well in practice.", "creator": "LaTeX with hyperref package"}}}