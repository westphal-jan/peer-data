{"id": "1604.00466", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Automatic Annotation of Structured Facts in Images", "abstract": "Motivated being for documentation of be - level image aim, we present of charging applied full data archives of evaluating cognitive factual, appear the captions. Example structured authenticity include indicate containing (directory. 1. , & :4; fruits, shirt & coup\u00e9; ), regard (+. 16. , & lt; pregnant, smile & gt; ), behaviours (blog. q. , & eric@hearstdc.com; thing, always, stray & mustang; ), rest positional concerning (address. k. , & sg; moldings, on, sides & performa; ). The books contextual types beginning on be of fact - object skater (aol. [. , & lt; man, walking, gets & z3; and man pictures border cardboard in as ). With, definition approach, also to method now should over giving hundreds although demonstrators seen contemporary fact cryptic close inaccurate on 83 \\% local even such truth. % fact indeed presented unlike shared Amazon Mechanical Turk already scout forensic in means evaluation. Our therapeutic switching collected lot are 248, 000 cognitive idea transliterations by could now 205, 000 form physical questions before photo full captions been side-effects must for invisible however getting than one while of processing in on used CPU ported. % Our work enabled many - improvements doubt - interest regard in 3-d. % Based over intellectual j analytical besides those as turn 53 \\% present both must nine annotation% We policy next collecting expected act measurement truth annotations directly include suggests objects (msn. g. & eric@hearstdc.com; gun, seen & lisboa; , & lt; coloured, hat & coupes; , no & rb; 8-year, smirk & gto; , two interactions & jp; beast, carriage, wave & gt;.", "histories": [["v1", "Sat, 2 Apr 2016 06:35:45 GMT  (8911kb,D)", "https://arxiv.org/abs/1604.00466v1", null], ["v2", "Tue, 5 Apr 2016 18:58:10 GMT  (8911kb,D)", "http://arxiv.org/abs/1604.00466v2", null], ["v3", "Fri, 8 Apr 2016 00:04:22 GMT  (8911kb,D)", "http://arxiv.org/abs/1604.00466v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["mohamed elhoseiny", "scott cohen", "walter chang", "brian price", "ahmed elgammal"], "accepted": false, "id": "1604.00466"}, "pdf": {"name": "1604.00466.pdf", "metadata": {"source": "CRF", "title": "Automatic Annotation of Structured Facts in Images", "authors": ["Mohamed Elhoseiny", "Scott Cohen", "Walter Chang", "Brian Price", "Ahmed Elgammal"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "People generally acquire visual knowledge by exposure to both visual facts and to semantic or language-based representations of these facts, e.g., by seeing an image of \u201ca person petting dog\u201d and observing this visual fact associated with its language representation . In this work, we focus on methods for collecting structured facts that we define as structures that provide attributes about an object, and/or the actions and interactions this object may have with other objects. We introduce the idea of automatically collecting annotations for second order visual facts and third order visual facts where second order facts <S,P> are at-\ntributed objects (e.g., <S: car, P: red>) and singleframe actions (e.g., <S: person, P: jumping>), and third order facts specify interactions (i.e., <boy, petting, dog>). This structure is helpful for designing machine learning algorithms that learn deeper image semantics from caption data and allow us to model the relationships between facts. In order to enable such a setting, we need to collect these structured fact annotations in the form of (language view, visual view) pairs (e.g., <baby, sitting on, chair> as the language view and an image with this fact as a visual view) to train models.\n(Chen et al., 2013) showed that visual concepts, from a predefined ontology, can be learned by querying the web about these concepts using image-web search engines. More recently, (Divvala et al., 2014) presented an approach to learn concepts related to a particular object by querying the web with Google-N-gram data that has the concept name. There are three limitations to these approaches. (1) It is difficult to define the space of visual knowledge and then search for it. It is further restricting to define it based on a predefined ontology such as (Chen et al., 2013) or a particular object such as (Divvala et al., 2014). (2) Using image search is not reliable to collect data for concepts with few images on the web. These methods assume that the top retrieved examples by imageweb search are positive examples and that there are images available that are annotated with the searched concept. (3) These concepts/facts are not structured and hence annotations lacks information like \u201cjumping\u201d is the action part in <person, jumping >, or \u201cman\u2019 and \u201chorse\u201d are interacting in <person, riding, horse >. This structure is important for deeper understanding of visual data, which is one of the main motivations of this work.\nThe problems in the prior work motivate us to propose a method to automatically annotate structured facts by processing image caption data since\nar X\niv :1\n60 4.\n00 46\n6v 3\n[ cs\n.C L\n] 8\nA pr\n2 01\n6\nfacts in image captions are highly likely to be located in the associated images. We show that a large quantity of high quality structured visual facts could be extracted from caption datasets using natural language processing methods. Caption writing is free-form and an easier task for crowd-sourcing workers than labeling second- and third-order tasks, and such free-form descriptions are readily available in existing image caption datasets. We focused on collecting facts from the MS COCO image caption dataset (Lin et al., 2014) and the newly collected Flickr30K entities (Plummer et al., 2015). We automatically collected more than 380,000 structured fact annotations in high quality from both the 120,000 MS COCO scenes and 30,000 Flickr30K scenes.\nThe main contribution of this paper is an accurate, automatic, and efficient method for extraction of structured fact visual annotations from image-caption datasets, as illustrated in Fig. 1. Our approach (1) extracts facts from captions associated with images and then (2) localizes the extracted facts in the image. For fact extraction from captions, We propose a new method called SedonaNLP for fact extraction to fill gaps in existing fact extraction from sentence methods like Clausie (Del Corro and Gemulla, 2013). SedonaNLP produces more facts than Clausie, especially <subject,attribute> facts, and thus enables collecting more visual annotations than using Clausie alone. The final set of automatic annotations are the set of successfully localized facts in the associated images. We show that these facts are extracted with more than 80% accuracy according to human judgment."}, {"heading": "2 Motivation", "text": "Our goal by proposing this automatic method is to generate language&vision annotations at the factlevel to help study language&vision for the sake of structured understanding of visual facts. Existing\nsystems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015). This gives rise to a key question about our work: why it is useful to collect such a large quantity of structured facts compared to caption-level systems?\nWe illustrate the difference between captionlevel learning fact-level learning that motivates this work by the example in Fig 1. Caption-level learning systems correlate captions like those on top of Fig. 1(top-left) to the whole image that includes all objects. Structured Fact-level learning systems are instead fed with localized annotations for each fact extracted form the image caption; see in Fig. 1(right), Fig. 6, and 7 in Sec. 6. Fact level annotations are less confusing training data than sentences because they provide more precise information for both the language and the visual views. (1) From the language view, the annotations we generate is precise to list a particular fact (e.g., <bicycle,parked between, parking posts>). (2) From the visual view, it provide the bounding box of this fact; see Fig 1. (3) A third unique part about our annotations is the structure: e.g., <bicycle,parked between, parking posts> instead of \u201ca bicycle parked between parking posts\u201d.\nOur collected data has been used to develop methods that learn hundreds of thousands of image facts, as we introduced and studied in (Mohamed Elhoseiny, 2016). The results shows that fact-level learning is superior compared to caption-level learning like (Kiros et al., 2015), as shown in Table 4 in (Mohamed Elhoseiny, 2016) (16.39% accuracy versus 3.48% for (Kiros et al., 2015)). It further shows the value of the associated structure in the (16.39% accuracy versus 8.1%) in Table 4(Mohamed Elhoseiny, 2016)). Similar results also shown on a smaller scale in Table 3 in (Mohamed Elhoseiny, 2016)."}, {"heading": "3 Approach Overview", "text": "We propose a two step automatic annotation of structured facts: (i) Extraction of structured fact from captions, and (ii) Localization of these facts in images. First, the captions associated with the given image are analyzed to extract sets of clauses that are considered as candidate <S,P>, and <S,P,O> facts.\nCaptions can provide a tremendous amount of information to image understanding systems. However, developing NLP systems to accurately and completely extract structured knowledge from free-form text is an open problem. We extract structured facts using two methods: Clausie (Del Corro and Gemulla, 2013) and Sedona( detailed later in Sec 4); also see Fig 1. We found Clausie (Del Corro and Gemulla, 2013) missed many visual facts in the captions which motivated us to develop Sedona to fill this gap as detailed in Sec. 4.\nSecond, we localize these facts within the image (see Fig. 1). The successfully located facts in the images are saved as fact-image annotations that could be used to train visual perception models to learn attributed objects, actions, and interactions. We managed to collect 380.409 highquality second- and third-order fact annotations (146,515 from Flickr30K Entities, 157,122 from the MS COCO training set, and 76,772 from the MS COCO validation set). We present statistics of the automatically collected facts in the Experiments section. Note that the process of localizing facts in an image is constrained by information in the dataset.\nFor MS COCO, the dataset contains object annotations for about 80 different objects as provided by the training and validation sets. Although this provides abstract information about objects in each image (e.g., \u201cperson\u201d), it is usually mentioned in different ways in the caption. For the \u201cperson\u201d object, \u201cman\u201d, \u201cgirl\u201d, \u201ckid\u201d, or \u201cchild\u201d could instead appear in the caption. In order to locate second- and third-order facts in images, we started by defining visual entities. For the MS COCO dataset (Lin et al., 2014), we define a visual entity as any noun that is either (1) one of the MS COCO dataset objects, (2) a noun in the WordNet ontology (Miller, 1995; Leacock and Chodorow, 1998) that is an immediate or indirect hyponym of one of the MS COCO objects (since WordNet is searchable by a sense and not a word, we perform\nword sense disambiguation on the sentences using a state-of-the-art method (Zhong and Ng, 2010)), or (3) one of scenes the SUN dataset (Xiao et al., 2010) (e.g., a \u201crestaurant\u201d). We expect visual entities to appear either in the S or the O part (if exists) of a candidate fact. This allows us to then localize facts for images in the MS COCO dataset. Given a candidate third-order fact, we first try to assign each S and O to one of the visual entities. If S and O elements are not visual entities, then the fact is ignored. Otherwise, the facts are processed by several heuristics, detailed in Sec 5. For instance, our method takes into account that grounding the plural \u201dmen\u201d in the fact <S:men, P: chasing, O: soccer ball > may require the union of multiple \u201dman\u201d bounding boxes.\nIn the Flickr30K Entities dataset (Plummer et al., 2015), the bounding box annotations are presented as phrase labels for sentences (for each phrase in a caption that refers to an entity in the scene). A visual entity is considered to be a phrase with a bounding box annotation or one of the SUN scenes. Several heuristics were developed and applied to collect these fact annotations, e.g. grounding a fact about a scene to the entire image; detailed in Sec 5."}, {"heading": "4 Fact Extraction from Captions", "text": "We extract facts from captions using Clausie (Del Corro and Gemulla, 2013) and our proposed SedonaNLP system. In contrast to Clausie, we address several challenging linguistic issues by evolving our NLP pipeline to: 1) correct many common spelling and punctuation mistakes, 2) resolve word sense ambiguity within clauses, and 3) learn a common spatial preposition lexicon (e.g., \u201cnext to\u201d, \u201con top of\u201d, \u201cin front of\u201d) that consists of over 110 such terms, as well as a lexicon of over two dozen collection phrase adjectives (e.g., \u201dgroup of\u201d, \u201dbunch of\u201d, \u201dcrowd of\u201d, \u201dherd of\u201d). For our purpose, these strategies allowed us to extract more interesting structured facts that Clausie fails at which include (1) more discrimination between single versus plural terms, (2) extracting positional facts (e.g., next to). Additionally, SedonaNLP produces attribute facts that we denote as <S, A>; see Fig 4. Similar to some existing systems OpenNLP (Baldridge, 2014) and ClearNLP (Choi, 2014), the SedonaNLP platform also performs many common NLP\ntasks: e.g., sentence segmentation, tokenization, part-of-speech tagging, named entity extraction, chunking, dependency and constituency-based parsing, and coreference resolution. SedonaNLP itself employs both open-source components such as NLTK and WordNet, as well as internallydeveloped annotation algorithms for POS and clause tagging. These tasks are used to create more advanced functions such as structured fact annotation of images via semantic triple extraction. In our work, we found SedonaNLP and Clausie to be complementary for producing a set of candidate facts for possible localization in the image that resulted in successful annotations.\nVarying degrees of success have been achieved in extracting and representing structured triples from sentences using <subject, predicate, object> triples. For instance, (Rusu et al., 2007) describe a basic set of methods based on traversing the parse graphs generated by various commonly available parsers. Larger scale text mining methods for learning structured facts for question answering have been developed in the IBM Watson PRISMATIC framework (Fan et al., 2010). While parsers such as CoreNLP (Manning et al., 2014) are available to generate comprehensive dependency graphs, these have historically required significant processing time for each sentence or have traded accuracy for performance. In contrast, SedonaNLP currently employs a shallow dependency parsing method that runs in some cases 8-9X faster than earlier cited methods running on identical hardware. We choose a shallow approach with high, medium, and low confidence cutoffs after observing that roughly 80%\nof all captions consisted of 0 or 1 Verb expressions (VX); see Fig. 3 for MSCOCO dataset (Lin et al., 2014). The top 500 image caption syntactic patterns we observed can be found on our supplemental materials. These syntactic patterns are used to learn rules for automatic extraction for not only <S,P,O>, but also <S,P>, and <S,A>, where <S,P>, are subject-action facts and <S,A> are subject-attribute facts. Pattern examples and statistics for MS COCO are shown in Fig. 5.\nIn SedonaNLP, structured fact extraction was accomplished by learning a subset of abstract syntactic patterns consisting of basic noun, verb, and preposition expressions by analyzing 1.6M caption examples provided by the MS COCO, Flickr30K, and Stony Brook University Im2Text caption datasets. Our approach mirrors existing known art with the addition of internallydeveloped POS and clause tagging accuracy improvements through the use of heuristics listed\nbelow to reduce higher occurrence errors due to systematic parsing errors: (i) Mapping past participles to adjectives (e.g., stained glass), (ii) Denesting existential facts (e.g., this is a picture of a cat watching a tv.), (iii) Identifying auxiliary verbs (e.g., do verb forms).\nIn Fig. 4, we show an example of extracted <S,P,O> structured facts useful for image annotation for a small sample of MS COCO captions. Our initial experiments empirically confirmed the findings of IBM Watson PRISMATIC researchers who indicated big complex parse trees tend to have more wrong parses. By limiting a frame to be only a small subset of a complex parse tree, we reduce the chance of error parse in each frame (Fan et al., 2010). In practice, we observed many correctly extracted structured facts for the more complex sentences (i.e., sentences with multiple VX verb expressions and multiple spatial prepositional expressions) \u2013 these facts contained useful information that could have been used in our joint learning model but were conservatively filtered to help ensure the overall accuracy of the facts being presented to our system. As improvements are made to semantic triple extraction and confidence evaluation systems, we see potential in several areas to exploit more structured facts and to filter less information. Our full <S,P,O> triple and related\ntuple extractions for MS COCO and Flickr30K datasets are available in the supplemental material."}, {"heading": "5 Locating facts in the Image", "text": "In this section, we present details about the second step of our automatic annotation process introduced in Sec. 3. After the candidate facts are extracted from the sentences, we end up with a set Fs = {f il }, i = 1 : Ns for statement s, where Ns is the number of extracted candidate fact f il , \u2200i from the statement s using either Clausie (Del Corro and Gemulla, 2013) or Sedona-3.0. The localization step is further divided into two steps. The mapping step maps nouns in the facts to candidate boxes in the image. The grounding step processes each fact associated with the candidate boxes and outputs a final bounding box if localization is successful. The two steps are detailed in the following subsections."}, {"heading": "5.1 Mapping", "text": "The mapping step starts with a pre-processing step that filters out a non-useful subset of Fs and produces a more useful set F\u2217s that we try to locate/ground in the image. We perform this step by performing word sense disambiguation using the state-of-the-art method (Zhong and Ng, 2010). The word sense disambiguation method provides\neach word in the statement with a word sense in the wordNet ontology (Leacock and Chodorow, 1998). It also assigns for each word a part of speech tag. Hence, for each extracted candidate fact in Fs we can verify if it follows the expected part of speech according to (Zhong and Ng, 2010). For instance, all S should be nouns, all P should be either verbs or adjectives, and O should be nouns. This results in a filtered set of facts F\u2217s. Then, each S is associated with a set of candidate boxes in the image for second- and third-order facts and each O associated with a set or candidate boxes in the image for third-order facts only. Since entities in MSCOCO dataset and Flickr30K are annotated differently, we present how the candidate boxes are determined in each of these datasets.\nMS COCO Mapping: Mapping to candidate boxes for MS COCO reduces to assigning the S for second-order and third-order facts, and S and O for third-order facts. Either S or O is assigned to one of the MSCOCO objects or SUN scenes classes. Given the word sense of the given part (S or O), we check if the given sense is a descendant of MSCOCO objects senses in the wordNet ontology. If it is, the given part (S or O) is associated with the set of candidate bounding boxes that belongs to the given object (e.g., all boxes that contain the \u201cperson\u201d MSCOCO object is under the \u201cperson\u201d wordnet node like \u201cman\u201d, \u2019girl\u2019, etc). If the given part (S or O) is not an MSCOCO object or one of its descendants under wordNet, we further check if the given part is one of the SUN dataset scenes. If this condition holds, the given part is associated with a bounding box of the whole image.\nFlickr30K Mapping: In contrast to MSCOCO dataset, the bounding box annotation comes for each entity in each statement in Flickr30K dataset. Hence, we compute the candidate bounding box annotations for each candidate fact by searching the entities in the same statement from which the clause is extracted. Candidate boxes are those that have the same name. Similarly, this process assigns S for second-order facts and assigns S and O for second- and third-order facts.\nHaving finished the mapping process, whether for MSCOCO or Flickr30K, each candidate fact f il \u2208 F\u2217s, is associated with candidate boxes depending on its type as follows.\n<S,P> : Each f il \u2208 F\u2217s of second-order type is associated with one set of bounding boxes biS , which are the candidate boxes for the S part. biO\ncould be assumed to be always an empty set for second-order facts. <S,P,O> : Each f il \u2208 F\u2217s of third-order type is associated with two sets of bounding boxes biS and biS as candidate boxes for the S and P parts, respectively."}, {"heading": "5.2 Grounding", "text": "The grounding process is the process of associating each f il \u2208 F\u2217s with an image fv by assigning fl to a bounding box in the given MS COCO image scene given the biS and b i O candidate boxes. The grounding process is relatively different for the two dataset due to the difference of the entity annotations.\nGrounding: MS COCO dataset (Training and Validation sets)\nIn the MS COCO dataset, one challenging aspect is that the S or O can be singular, plural, or referring to the scene. This means that one S could map to multiple boxes in the image. For example, \u201cpeople\u201d maps to multiple boxes of \u201cperson\u201d. Furthermore, this case could exist for both the S and the O. In cases where either S or O is plural, the bounding box assigned is the union of all candidate bounding boxes in biS . The grounding then proceeds as follows. <S,P> facts: (1) If the computed biS = \u2205 for the given f il , then f il fails to ground and is discarded. (2) If S singular, f iv is the image region that with the largest candidate bounding box in biS . (3) If S is plural, f iv is the image region that with union of the candidate bounding boxes in biS . <S,P, O> facts: (1) If biS = \u2205 and biO = \u2205, f il fails to ground and is ignored. (2) If biS 6= \u2205 and biO 6= \u2205, then bounding boxes are assigned to S and O such that the distance between them is minimized (though if S or O is plural, the assigned bounding box is the union of all bounding boxes for biS or b i O respectively), and the grounding is assigned the union of the bounding boxes assigned to S and O.\n(3) If either biS = \u2205 or biO = \u2205, then a bounding box is assigned to the present object (the largest bounding box if singular, or the union of all bounding boxes if plural). If the area of this region compared to the area of the whole scene is greater than a threshold th = 0.3, then the f iv is associated to the whole image of the scene. Otherwise,\nf il fails to ground and is ignored. Grounding: Flickr30K dataset The main difference in Flickr30K is that for each entity phrase in a sentence, there is a box in the image. This means there is no need to have cases for single and plural. Since in this case, the word \u201cmen\u201d in the sentence will be associated with the set of boxes referred to by \u201cmen\u201d in the sentences. We union these boxes for plural words as one candidate box for \u201cmen\u201d\nWe can also use the information that the object box has to refer to a word that is after the subject word, since subject usually occurs earlier in the sentence compared to object. We union these boxes for plural words. <S,P> facts: If the computed biS = \u2205 for the given f il , then f il fails to ground and is discarded. Otherwise, the fact is assigned to the largest candidate box in if there are multiple boxes. <S,P, O> facts: <S,P, O> facts are handled very similar to MSCOCO dataset with two main differences.\na) The candidate boxes are computed as described for the case of Flickr30K dataset.\nb) All cases are handled as single case, since even plural words are assigned one box based on the nature of the annotations in this dataset."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Human Subject Evaluation", "text": "We propose three questions to evaluate each annotation: (Q1) Is the extracted fact correct (Yes/No)? The purpose of this question is to evaluate errors captured by the first step, which extracts facts by Sedona or Clausie. (Q2) Is the fact located in the image (Yes/No)? In some cases, there might be a fact mentioned in the caption that does not exist in\nthe image and is mistakenly considered as an annotation. (Q3) How accurate is the box assigned to a given fact (a to g)? a (about right), b (a bit big), c (a bit small), d (too small), e (too big), f (totally wrong box), g (fact does not exist or other). Our instructions on these questions to the participants can be found in this anonymous url (Eval, 2016).\nWe evaluate these three questions for the facts that were successfully assigned a box in the image, because the main purpose of this evaluation is to measure the usability of the collected annotations as training data for our model. We created an Amazon Mechanical Turk form to ask these three questions. So far, we collected a total of 10,786 evaluation responses, which are an evaluation of 3,595 (fv, fl) pairs (3 responses/ pair). Table 2 shows the evaluation results, which indicate that the data is useful for training, since\u224883.1% of them are correct facts with boxes that are either about right, or a bit big or small (a,b,c). We further some evaluation responses that we collected from volunteer researchers in Table 2 showing similar results.\nFig. 6 shows some successful qualitative results that include four extracted structured facts from MS COCO dataset (e.g., <person, using, phone>, <person, standing>, etc). Fig 7 also show a negative example where there is a wrong fact among the extracted facts (i.e., <house, ski>). The main reason for this failure case is that \u201chow\u201d is mistyped as \u201chouse\u201d; see Fig 7. The supplementary materials includes all the captions of these examples and also additional qualitative examples."}, {"heading": "6.2 Hardness Evaluation of the collected data", "text": "In order to study how the method behave in both easy and hard examples. This section present statistics of the successfully extracted facts and re-\nlate it to the hardness of the extraction of these facts. We start by defining hardness of an extracted fact in our case and its dependency on the fact type. Our method collect both second- and third-order facts. We refer to candidate subjects as all instances of the entity in the image that match the subject type of either a second-order fact <S,P> or a third-order fact <S,P,O>. We refer to candidate objects as all instances in the image that match the object type of a third-order fact <S,P,O>. The selection of the candidate subjects and candidate objects is a part of our method that we detailed in Sec 5. We define the hardness for second order facts by the number of candidate subjects and the hardness of third order facts by the number of candidate subjects multiplied by the number of candidate objects.\nIn Fig 16 and 17, the Y axis is the number of facts for each bin. The X axis shows the bins that correspond to hardness that we defined for both\nFigure 6: Se6eral Facts successfully extracted by our method from two MS COCO scenes\nFigure 7: An example where one of the extracted facts are not correct due to a spelling mistake\nsecond and third order fats. Figure 16 shows a histogram of the difficulties for all Mturk evaluated examples including both the successful and the failure cases. Figure 17 shows a similar histogram but for but for subset of facts verified by the Turkers with Q3 as (about right). The figures show that the method is able to handle difficulty cases even with more than 150 possibilities for grounding. We show these results broken out for MSCOCO and Flickr30K Entities datasets and for each fact types in the supplementary materials.\nFigure 8: (All MTurk Data) Hardness histogram after candidate box selection using our method\nFigure 9: (MTurk Data with Q3=about right)Hardness histogram after our candidate box selection"}, {"heading": "7 Conclusion", "text": "We present a new method whose main purpose to collect visual fact annotation by a language approach. The collected data help train visual system systems on the fact level with the diversity of facts captured by any fact described by an image caption. We showed the effectiveness of the proposed methodology by extracting hundreds of thousands of fact-level annotations from MSCOCO and Flickr30K datasets. We verified and analyzed the collected data and showed that more than 80% of the collected data are good for training visual systems."}, {"heading": "8 SAFA Qualitative Results", "text": "SAFA successful cases On the left is the input, and on the right is the output.\nSAFA failure cases"}, {"heading": "8.1 Statistics on collected data (MTurk subset)", "text": "In order to study how the method behave in both easy and hard examples. This section present statistics of the successfully extracted facts and relate it to the hardness of the extraction of these facts. We start by defining hardness of an extracted fact in our case and its dependency on the fact type. Our method collect both second- and third-order facts. We refer to candidate subjects as all instances of the entity in the image that match the subject type of either a second-order fact <S,P> or a third-order fact <S,P,O>. We refer to candidate objects as all instances in the image that match the object type of a third-order fact <S,P,O>. The selection of the candidate subjects and candidate objects is a part of our method that we detailed in Sec. 4 in the paper. We define the hardness for second order facts by the number of candidate subjects and the hardness of third order facts by the number of candidate subjects multiplied by the number of candidate objects.\nIn all the following figures the Y axis is the number of facts for each bin. The X axes are bins that correspond to\n(1) the number of candidate subjects for second and third order facts.\n(2) the number of candidate objects for third order facts.\n(3) the number of candidate objects multiplied by number of candidate subjects for third-order facts (which is all possible pairs of entities in an image that match the given <S,P,O> fact)\nWe here show the number of possible candidates as the level of difficulty/hardness on the x axis, y axis is the number of facts in each case."}, {"heading": "8.1.1 Statistics on both all MTurk data compared to the subset where Q3 = about right for each dataset", "text": "The following figures show statistics on the facts verified by MTurkers (from the union of all datasets). Figure 16 shows a histogram of the difficulties for all Mturk evaluated examples. Figure 17 shows a similar histogram but for but for subset of facts verified by the Turkers by Q3 as (about right) by MTurker. The figures show that the method is able to handle difficulty cases even with more than 150 possibilities for grounding."}, {"heading": "8.1.2 Statistics on Q3 = about right for each dataset", "text": ""}, {"heading": "9 SAFA collected data statistics (the whole 380K collected data)", "text": "This section shows the number of candidate subject and object statistics for all successfully grounded facts for all MS COCO (union of training and validation subsets) and Flickr30K datasets. SAFA collects second- and third-order facts. We refer to candidate subjects as all instances of the entity that match the subject of either a second-order fact <S,P> or a third-order fact <S,P,O>. We refer to candidate objects as all instances of the entity that match the object of a third-order fact <S,P,O>. The selection of the candidate subjects and candidate objects is a\npart of our method that we detailed in this paper. Our method was designed to achieve high precision such that the grounded facts are as accurate as possible as we showed in our experiments.\nIn all the following figures the Y axis is the\nnumber of facts for each bin. The X axes are bins that correspond to\n(1) the number of candidate subjects for second and third order facts.\n(2) the number of candidate objects for third order facts.\n(3) the number of candidate objects multiplied by number of candidate subjects for third-order facts (which is all possible pairs of entities in an image that match the given <S,P,O> fact)\n9.1 Second-Order Facts <S,P>: Candidate Subjects\n9.2 Third-Order Facts <S,P,O>: Candidate Subjects\n9.3 Third-Order Facts <S,P,O>: Candidate Objects\n9.4 Third-Order Facts <S,P,O>: Candidate Subjects/Object Combination"}], "references": [{"title": "Vqa: Visual question answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": null, "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Neil: Extracting visual knowledge from web data", "author": ["Chen et al.2013] Xinlei Chen", "Ashish Shrivastava", "Arpan Gupta"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Clausie: clause-based open information extraction", "author": ["Del Corro", "Gemulla2013] Luciano Del Corro", "Rainer Gemulla"], "venue": null, "citeRegEx": "Corro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Corro et al\\.", "year": 2013}, {"title": "Learning everything about anything: Webly-supervised visual concept learning", "author": ["Alireza Farhadi", "Carlos Guestrin"], "venue": null, "citeRegEx": "Divvala et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Divvala et al\\.", "year": 2014}, {"title": "Prismatic: Inducing knowledge from a large scale lexicalized relation resource", "author": ["Fan et al.2010] James Fan", "David Ferrucci", "David Gondek", "Aditya Kalyanpur"], "venue": "In NAACL HLT. Association for Computational Linguistics", "citeRegEx": "Fan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2010}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Armand Joulin", "Fei Fei F Li"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models. TACL", "author": ["Kiros et al.2015] Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Combining local context and wordnet similarity for word sense identification", "author": ["Leacock", "Chodorow1998] Claudia Leacock", "Martin Chodorow"], "venue": null, "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Marcus Rohrbach", "Mario Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR", "author": ["Mao et al.2015] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan Yuille"], "venue": null, "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Sherlock: Scalable fact learning", "author": ["Scott Cohen"], "venue": null, "citeRegEx": "Elhoseiny and Cohen.,? \\Q2016\\E", "shortCiteRegEx": "Elhoseiny and Cohen.", "year": 2016}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["Liwei Wang", "Chris Cervantes", "Juan Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": null, "citeRegEx": "Plummer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Ren et al.2015] Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Triplet extraction from sentences", "author": ["Rusu et al.2007] Delia Rusu", "Lorand Dali", "Blaz Fortuna", "Marko Grobelnik", "Dunja Mladenic"], "venue": "In International Multiconference\u201d Information Society-IS", "citeRegEx": "Rusu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2007}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["Xiao et al.2010] Jianxiong Xiao", "James Hays", "Krista Ehinger", "Aude Oliva", "Antonio Torralba"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhong", "Ng2010] Zhi Zhong", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Zhong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "(Chen et al., 2013) showed that visual concepts, from a predefined ontology, can be learned by querying the web about these concepts using image-web search engines.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "More recently, (Divvala et al., 2014) presented an approach to learn", "startOffset": 15, "endOffset": 37}, {"referenceID": 1, "context": "It is further restricting to define it based on a predefined ontology such as (Chen et al., 2013) or a particular object such as (Divvala et al.", "startOffset": 78, "endOffset": 97}, {"referenceID": 3, "context": ", 2013) or a particular object such as (Divvala et al., 2014).", "startOffset": 39, "endOffset": 61}, {"referenceID": 8, "context": "MS COCO image caption dataset (Lin et al., 2014) and the newly collected Flickr30K entities (Plummer et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 14, "context": ", 2014) and the newly collected Flickr30K entities (Plummer et al., 2015).", "startOffset": 51, "endOffset": 73}, {"referenceID": 5, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 6, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 17, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 19, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 11, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 0, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 9, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 15, "context": "Existing systems already work on relating captions directly to the whole image such as (Karpathy et al., 2014; Kiros et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Mao et al., 2015; Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015).", "startOffset": 87, "endOffset": 250}, {"referenceID": 6, "context": "The results shows that fact-level learning is superior compared to caption-level learning like (Kiros et al., 2015), as shown in Table 4 in (Mohamed Elhoseiny, 2016) (16.", "startOffset": 95, "endOffset": 115}, {"referenceID": 6, "context": "48% for (Kiros et al., 2015)).", "startOffset": 8, "endOffset": 28}, {"referenceID": 8, "context": "For the MS COCO dataset (Lin et al., 2014), we define a visual entity as any noun that is either (1) one of the MS COCO dataset objects, (2) a noun in the WordNet ontology (Miller, 1995; Leacock and Chodorow, 1998) that is an immediate or indirect hyponym of one of the MS COCO objects (since WordNet is searchable by a sense and not a word, we perform word sense disambiguation on the sentences using a state-of-the-art method (Zhong and Ng, 2010)), or (3) one of scenes the SUN dataset (Xiao et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 12, "context": ", 2014), we define a visual entity as any noun that is either (1) one of the MS COCO dataset objects, (2) a noun in the WordNet ontology (Miller, 1995; Leacock and Chodorow, 1998) that is an immediate or indirect hyponym of one of the MS COCO objects (since WordNet is searchable by a sense and not a word, we perform word sense disambiguation on the sentences using a state-of-the-art method (Zhong and Ng, 2010)), or (3) one of scenes the SUN dataset (Xiao et al.", "startOffset": 137, "endOffset": 179}, {"referenceID": 18, "context": ", 2014), we define a visual entity as any noun that is either (1) one of the MS COCO dataset objects, (2) a noun in the WordNet ontology (Miller, 1995; Leacock and Chodorow, 1998) that is an immediate or indirect hyponym of one of the MS COCO objects (since WordNet is searchable by a sense and not a word, we perform word sense disambiguation on the sentences using a state-of-the-art method (Zhong and Ng, 2010)), or (3) one of scenes the SUN dataset (Xiao et al., 2010) (e.", "startOffset": 453, "endOffset": 472}, {"referenceID": 16, "context": "For instance, (Rusu et al., 2007) describe a basic set of methods based on traversing the parse graphs generated by various commonly available parsers.", "startOffset": 14, "endOffset": 33}, {"referenceID": 4, "context": "Larger scale text mining methods for learning structured facts for question answering have been developed in the IBM Watson PRISMATIC framework (Fan et al., 2010).", "startOffset": 144, "endOffset": 162}, {"referenceID": 10, "context": "While parsers such as CoreNLP (Manning et al., 2014) are available to generate comprehensive dependency graphs, these have historically required significant processing time for each sentence or have traded accuracy for performance.", "startOffset": 30, "endOffset": 52}, {"referenceID": 8, "context": "3 for MSCOCO dataset (Lin et al., 2014).", "startOffset": 21, "endOffset": 39}, {"referenceID": 4, "context": "By limiting a frame to be only a small subset of a complex parse tree, we reduce the chance of error parse in each frame (Fan et al., 2010).", "startOffset": 121, "endOffset": 139}], "year": 2016, "abstractText": "Motivated by the application of fact-level image understanding, we present an automatic method for data collection of structured visual facts from images with captions. Example structured facts include attributed objects (e.g., <flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man, walking, dog>), and positional information (e.g., <vase, on, table>). The collected annotations are in the form of fact-image pairs (e.g.,<man, walking, dog> and an image region containing this fact). With a language approach, the proposed method is able to collect hundreds of thousands of visual fact annotations with accuracy of 83% according to human judgment. Our method automatically collected more than 380,000 visual fact annotations and more than 110,000 unique visual facts from images with captions and localized them in images in less than one day of processing time on standard CPU platforms.", "creator": "LaTeX with hyperref package"}}}