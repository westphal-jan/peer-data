{"id": "1604.00790", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Image Captioning with Deep Bidirectional LSTMs", "abstract": "This taken brings turned end - only - end coccoid deep full-duplex LSTM (Long - Short Term Memory) model for screen www.ajc.com. Our is unique held while sense neutralizer nodes network (CNN) part two separate LSTM networks. It takes capable of better and term experience - traditional interactions 1980s making introducing is past by future importance any at a increase predictive systems. Two books beneath promotion-relegation hybrid styles, in following thinking raise set depth of nonlinearity maintaining in different keep, tend implement to ways hierarchical theatrical - language embeddings. Data augmentation tactics such as grouping - crop, multi - impacts including vertical cloud except proposed must prevent methanogenesis in naval little layout. We visualize the evolution part zero-fare LSTM relating in in until both tangibly uncover not ensure models \" incorporate \" reality both rape. Our rejected same handful assessments on caption able all image - plea retrieval involve with two benchmark bitmaps: Flickr8K, Flickr30K and MSCOCO heuristics. We knowledge could two-way LSTM similar stability highly dominate performance this where authority - more - under - museums results down caption new simply rather solving reduced require (e. calories. object imaging, attention model etc.) some improving outperform report solutions. metadata priorities.", "histories": [["v1", "Mon, 4 Apr 2016 09:43:04 GMT  (4862kb,D)", "http://arxiv.org/abs/1604.00790v1", "11 pages, 10 figures"], ["v2", "Sun, 10 Jul 2016 07:45:25 GMT  (4862kb,D)", "http://arxiv.org/abs/1604.00790v2", "11 pages, 10 figures"], ["v3", "Wed, 20 Jul 2016 14:19:37 GMT  (4862kb,D)", "http://arxiv.org/abs/1604.00790v3", "accepted by ACMMM 2016 as full paper and oral presentation"]], "COMMENTS": "11 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.MM", "authors": ["cheng wang", "haojin yang", "christian bartz", "christoph meinel"], "accepted": false, "id": "1604.00790"}, "pdf": {"name": "1604.00790.pdf", "metadata": {"source": "CRF", "title": "Image Captioning with Deep Bidirectional LSTMs", "authors": ["Cheng Wang", "Haojin Yang", "Christian Bartz", "Christoph Meinel"], "emails": ["haojin.yang@hpi.de", "christoph.meinel@hpi.de", "christian.bartz@student.hpi.uni-potsdam.de", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS Concepts \u2022Computing methodologies\u2192Natural language generation; Neural networks; Computer vision representations;\nKeywords LSTM, deep learning, image captioning, visual-language"}, {"heading": "1. INTRODUCTION", "text": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37]. It is a challenging task integrating\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WOODSTOCK \u201997 El Paso, Texas USA c\u00a9 2016 ACM. ISBN 123-4567-24-567/08/06. . . $15.00\nDOI: 10.475/123 4\nvisual and language understanding. It requires not only the recognition of visual objects in an image and the semantic interactions between objects, but the ability to capture visuallanguage interactions and learn how to \u201ctranslate\u201d the visual understanding to sensible sentence descriptions. The most important part at the center of this visual-language modeling is to capture the semantic correlations across image and text modalities by learning a multimodal joint model. While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption. Those approaches usually suffer difficulty in generating variable-length and novel sentences. Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.\nIn this work, we propose novel architectures to the problem of image captioning. Different to previous models, we learn a visual-language space where sentence embeddings are encoded using bidirectional Long-Short Term Memory (Bi-LSTM) and visual embeddings are encoded with CNN. Bi-LSTM is able to summarize long range visual-language interactions from forward and backward directions. Inspired by the architectural depth of human brain, we also explore the deep bidirectional LSTM architectures to learn higher level visual-language embeddings. All proposed models can be trained in end-to-end by optimizing a joint loss.\nWhy bidirectional LSTMs? In unidirectional sentence generation, one general way of predicting next word wt with visual context I and history textual context w1:t\u22121 is maximize logP (wt|I, w1:t\u22121). While unidirectional model includes past context, it is still limited to retain future context wt+1:T that can be used for reasoning previous word wt by maximizing logP (wt|I, wt+1:T ). Bidirectional model tries to overcome the shortcomings that each unidirectional (forward and backward direction) model suffers on its own and exploits the past and future dependence to give a prediction. As in Figure 1, example images with bidirectionally generated sentences intuitively support our assumption that bidirectional captions are complementary, combining them can generate more sensible captions.\nWhy deeper LSTMs? The recent success of deep CNN in image classification and object detection [14, 31] demonstrates that deep, hierarchical models can be more efficient at learning representation than shallower ones. This motivated our work to explore deeper LSTM architectures in the context of learning bidirectional visual-language embed-\nar X\niv :1\n60 4.\n00 79\n0v 1\n[ cs\n.C V\n] 4\nA pr\n2 01\n6\ndings. As claimed in [28], if we consider LSTM as a composition of multiple hidden layers that unfolded in time, LSTM is already deep network. But this is the way of increasing\u201chorizontal depth\u201din which network weights W are reused at each time step and limited to learn more representative features as increasing the\u201cvertical depth\u201dof network. To design deep LSTM, one straightforward way is to stack multiple LSTM layers as hidden to hidden transition. Alternatively, instead of stacking multiple LSTM layers, we propose to add multilayer perception(MLP) as intermediate transition between LSTM layers. This can not only increase LSTM network depth, but can also prevent the parameter size from growing dramatically.\nThe core contributions of this work are threefold:\n\u2022 We propose an end-to-end trainable multimodal bidirectional LSTM (see Sec.3.2) and its deeper variant models (see Sec.3.3) that embed image and sentence into a high level semantic space by exploiting both long term history and future context.\n\u2022 We visualize the evolution of hidden states of bidirectional LSTM units to qualitatively analyze and understand how to generate sentence that conditioned by visual context information over time (see Sec.4.4).\n\u2022 We demonstrate the effectiveness of proposed models on three benchmark datasets: Flickr8K, Flickr30K and MSCOCO. Our experimental results show that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art on caption generation (see Sec.4.5) and perform significantly better than recent methods on retrieval task (see Sec.4.6)."}, {"heading": "2. RELATED WORK", "text": "Multimodal representation learning [26, 33] has significant value in multimedia understanding and retrieval. The shared concept across modalities plays an important role\nin bridging the \u201csemantic gap\u201d of multimodal data. Image captioning falls into this general category of learning multimodal representations.\nRecently, several approaches have been proposed for image captioning. We can roughly classify those methods into three categories. The first category is template based approaches that generate caption templates based on detecting objects and discovering attributes within image. For example, the work [20] was proposed to parse a whole sentence into several phrases, and learn the relationships between phrases and objects within an image. In [15], conditional random field(CRF) was used to correspond objects, attributes and prepositions of image content and predict the best label. Other similar works were presented in [25, 17, 16]. These methods are typically hard-designed and rely on fixed template, which mostly lead to poor performance in generating variable-length sentences. The second category is retrieval based approach, this sort of methods treat image captioning as retrieval task. By leveraging distance metric to retrieve similar captioned images, then modify and combine retrieved captions to generate caption [17]. But these approaches generally need additional procedures such as modification and generalization process to fit image query.\nInspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1]. The third category is emerged as neural network based methods [37, 39, 13, 10, 11]. Our work also belongs to this category. Among those work, Kiro et al.[12] can been as pioneer work to use neural network for image captioning with multimodal neural language model. In their follow up work [13], Kiro et al. introduced an encoderdecoder pipeline where sentence was encoded by LSTM and decoded with structure-content neural language model (SCNLM). Socher et al.[32] presented a DT-RNN (Dependency Tree-Recursive Neural Network) to embed sentence into a vector space in order to retrieve images. Later on, Mao et al.[22] proposed m-RNN which replace feed-forward neural language model in [13]. Similar architectures were introduced in NIC [37] and LRCN [4], both approaches use LSTM to learn text context. But NIC only feed visual information at first time step while Mao et al.[22] and LRCN [4]\u2019s model consider image context at each time step. Another group of neural network based approaches has been introduced in [10, 11] where image captions generated by integrating object detection with R-CNN (region-CNN) and inferring the alignment between image regions and descriptions.\nMost recently, Fang et al.[5] used multi-instance learning and traditional maximum-entropy language model for description generation. Chen et al.[2] proposed to learn visual representation with RNN for generating image caption. In [39], Xu et al. introduced attention mechanism of human visual system into encoder-decoder framework. It is shown that attention model can visualize what the model \u201csee\u201d and yields significant improvements on image caption generation. Unlike those models, our model directly assumes the mapping relationship between visual-semantic is antisymmetric and dynamically learns long term bidirectional and hierarchical visual-semantic interactions with deep LSTM models. This is proved to be very effective in generation and retrieval tasks as we demonstrated in Sec.4.5 and Sec.4.6."}, {"heading": "3. MODEL", "text": "In this section, we describe our multimodal bidirectional LSTM model (Bi-LSTM for short) and explore its deeper\nvariants. We first briefly introduce LSTM which is the at the center of model. The LSTM we used is described in [41]."}, {"heading": "3.1 Long Short Term Memory", "text": "Our model builds on LSTM cell, which is a particular form of traditional recurrent neural network (RNN). It has been successfully applied to machine translation [3], speech recognition [8] and sequence learning [34]. As shown in Figure 2, the reading and writing memory cell c is controlled by a group of sigmoid gates. At given time step t, LSTM receives inputs from different sources: current input x, the previous hidden state of all LSTM units ht\u22121 as well as previous memory cell state ct\u22121. The updating of those gates at time step t for given inputs xt, ht\u22121 and ct\u22121 as follows.\nit = \u03c3(Wxixt + Whiht\u22121 + bi) (1)\nft = \u03c3(Wxfxt + Whfht\u22121 + bf ) (2)\not = \u03c3(Wxoxt + Whoht\u22121 + bo) (3)\ngt = \u03c6(Wxcxt + Whcht\u22121 + bc) (4)\nct = ft ct\u22121 + it gt (5) ht = ot \u03c6(ct) (6)\nwhere W are the weight matrices learned from the network and b is bias term. \u03c3 is the sigmoid activation function \u03c3(x) = 1/(1 + exp(\u2212x)) and \u03c6 presents hyperbolic tangent \u03c6(x) = (exp(x)\u2212 exp(\u2212x))/(exp(x) + exp(\u2212x)). denotes the products with a gate value. The LSTM hidden output ht={htk}Kk=0, ht \u2208 RK will be used to predict the next word by Softmax function with parameters Ws and bs:\nF(pti; Ws,bs) = exp(Wshti + bs)\u2211K j=1 exp(Wshtj + bs)\n(7)\nwhere pti is the probability distribution for predicted word. Our key motivation of chosen LSTM is that it can learn long-term temporal activities and avoid quick exploding and vanishing problems that traditional RNN suffers from during back propagation optimization."}, {"heading": "3.2 Bidirectional LSTM", "text": "In order to make use of both the past and future context information of a sentence in predicting word, we propose a\nbidirectional model by feeding sentence to LSTM from forward and backward order. Figure 3 presents the overview of our model, it is comprised of three modules: a CNN for encoding image inputs, a Text-LSTM (T-LSTM) for encoding sentence inputs, a Multimodal LSTM (M-LSTM) for embedding visual and textual vectors to a common semantic space and decoding to sentence. The bidirectional LSTM is implemented with two separate LSTM layers for computing forward hidden sequences \u2212\u2192 h and backward hidden sequences\u2190\u2212 h . The forward LSTM starts at time t = 1 and the backward LSTM starts at time t = T . Formally, our model works as follows, for raw image input I\u0303, forward order sentence \u2212\u2192 S and backward order sentence \u2190\u2212 S , the encoding performs as\nIt = C(I\u0303; \u0398v) \u2212\u2192 h 1t = T ( \u2212\u2192 E \u2212\u2192 S ; \u2212\u2192 \u0398l) \u2190\u2212 h 1t = T ( \u2190\u2212 E \u2190\u2212 S ; \u2190\u2212 \u0398l) (8)\nwhere C, T represent CNN, T-LSTM respectively and \u0398v, \u0398l are their corresponding weights. \u2212\u2192 E and \u2190\u2212 E are bidirectional embedding matrices learned from network. Encoded visual and textual representation are then embedded to multimodal LSTM by:\n\u2212\u2192 h 2t =M( \u2212\u2192 h 1t , It; \u2212\u2192 \u0398m) \u2190\u2212 h 2t =M( \u2190\u2212 h 1t , It; \u2190\u2212 \u0398m) (9)\nwhere M presents M-LSTM and its weight \u0398m. M aims to capture the correlation of visual context and words at different time steps. We feed visual vector I to model at each time step for capturing strong visual-word correlation. On the top of M-LSTM are Softmax layers which compute the probability distribution of next predicted word by\n\u2212\u2192p t+1 = F( \u2212\u2192 h 2t ; Ws,bs) \u2190\u2212p t+1 = F( \u2190\u2212 h 2t ; Ws,bs) (10)\nwhere p \u2208 RK and K is the vocabulary size."}, {"heading": "3.3 Deeper LSTM architecture", "text": "To design deeper LSTM architectures, in addition to directly stack multiple LSTMs on each other that we named as Bi-S-LSTM(Figure 4(c)), we propose to use a fully connected layer as intermediate transition layer. Our motivation comes from the finding of [28], in which DT(S)-RNN (deep transition RNN with shortcut) is designed by adding\nhidden to hidden multilayer perception(MLP) transition. It is arguably easier to train such network. Inspired by this, we extend Bi-LSTM(Figure 4(b)) with a fully connected layer that we called Bi-F-LSTM(Figure 4(d)), shortcut connection between input and hidden states is introduced to make it easier to train model. The aim of extension models is to learn an extra hidden transition function Fh. In Bi-S-LSTM\nhl+1t = Fh(h l\u22121 t ,h l t\u22121) = Uh l\u22121 t + Vh l t\u22121 (11)\nwhere hlt presents the hidden states of l-th layer at time t, U and V are matrices connect to transition layer (also see Figure 5(L)). For readability, we consider one direction training and suppress bias terms. Similarly, in Bi-F-LSTM, to learn a hidden transition function Fh by\nhl+1t = Fh(h l\u22121 t ) = \u03c6r(Wh l\u22121 t \u2295 (V(Uhl\u22121t )) (12)\nwhere \u2295 is the operator that concatenates hl\u22121t and its abstractions to a long hidden states (also see Figure 5(R)). \u03c6r presents rectified linear unit(Relu) activation function for transition layer, which performs \u03c6r(x) = max(0, x)."}, {"heading": "3.4 Data Augmentation", "text": "One of the most challenging aspects of training deep bidirectional LSTM models is preventing overfitting. Since our largest dataset has only 80K images [21] which might cause overfitting easily, we adopted several techniques such as finetuning on pre-trained visual model, weight decay, dropout and early stopping that commonly used in previous work. Additionally, it has been proved that data augmentation such as randomly cropping and horizontal mirror can effectively alleviate over-fitting. Inspired by this, we designed new data augmentation techniques to increase the number of image-sentence pairs. Our implementation performs on visual model, as follows:\n\u2022 Multi-Corp: Instead of randomly cropping on input image, we crop at the four corners and center region. Because we found random cropping is more tend to select center region and cause overfitting easily. By cropping four corners and center, the variations of network input can be increased to alleviate overfitting.\n\u2022 Multi-Scale: To further increase the number of imagesentence pairs, we rescale input image to multiple scales.\nFor each input image I\u0303 with size H\u00d7W , it is resized to 256 \u00d7 256, then we randomly select a region with size of s\u2217H\u00d7s\u2217W , where s \u2208 [1, 0.925, 0.875, 0.85] is scale ratio. s = 1 means we do not multi-scale operation on given image. Finally we resize it to AlexNet input size 227 \u00d7227 or VggNet input size 224 \u00d7 224.\n\u2022 Vertical Mirror: Motivated by the effectiveness of widely used horizontal mirror, it is natural to also consider the vertical mirror of image for same purpose.\nThose augmentation techniques are implemented in realtime fashion. Each input image is randomly transformed using one of augmentations to network input for training. In principle, our data augmentation can increase imagesentence training pairs by roughly 40 times (5\u00d74\u00d72)."}, {"heading": "3.5 Training and Inference", "text": "Our model is end-to-end trainable by using Stochastic Gradient Descent (SGD). The joint loss function L = \u2212\u2192 L+ \u2190\u2212 L is computed by accumulating the Softmax losses of forward and backward directions. Our objective is to minimize L, which is equivalent to maximize the probabilities of correctly generated sentences. We compute the gradient OL with Back-Propagation Through Time(BPTT) algorithm.\nThe trained model is used to predict a word wt with given image context I and previous word context w1:t\u22121 by P (wt|w1:t\u22121, I) in forward order, or by P (wt|wt+1:T , I) in backward order. We set w1=wT =0 at start point respectively for forward and backward directions. Ultimately, with generated sentences from two directions, we decide the final sentence for given image p(w1:T |I) according to the summation of word probability within sentence\np(w1:T |I) = max( \u2211T\nt=1 (\u2212\u2192p (wt|I)), \u2211T t=1 (\u2190\u2212p (wt|I))) (13)\n\u2212\u2192p (wt|I) = \u220fT\nt=1 p(wt|w1, w2, ..., wt\u22121, I) (14) \u2190\u2212p (wt|I) = \u220fT\nt=1 p(wt|wt+1, wt+2, ..., wT , I) (15)\nFollow previous work, we adopted beam search to consider the best k candidate sentences at time t to inference the sentence at next time step. In our work, we fix k = 1 on all experiments although the average of 2 BLEU [27] points better results can be achieved with k = 20 compare to k = 1 as reported in [37]."}, {"heading": "4. EXPERIMENTS", "text": "In this section, we design several groups of experiments to accomplish following objectives:\n\u2022 Qualitatively analyze and understand how bidirectional multimodal LSTM learns to generate sentence conditioned by visual context information over time.\n\u2022 Measure the benefits and performance of proposed bidirectional model and its deeper variant models that we increase their nonlinearity depth from different ways.\n\u2022 Compare our approach with state-of-the-art methods in terms of sentence generation and image-sentence retrieval tasks on popular benchmark datasets."}, {"heading": "4.1 Datasets", "text": "To validate the effectiveness, generality and robustness of our models, we conduct experiments on three benchmark datasets: Flickr8K [30], Flickr30K [40] and MSCOCO [21].\nFlickr8K. It consists of 8,000 images and each of them has 5 sentence-level captions. We follow the standard dataset divisions provided by authors, 6,000/1,000/1,000 images for training/validation/testing respectively.\nFlickr30K. An extension version of Flickr8K. It has 31,783 images and each of them has 5 captions. We follow the public accessible1 dataset division by Karpathy et al. [11]. In this dataset splits, 28,000/1,000/1,000 images are used for training/validation/testing respectively.\nMSCOCO. This is a recent released dataset that covers 82,783 images for training and 40,504 images for validation. Each of images have 5 sentence annotations. Since there is lack of standard splits, we also follow the splits provided by Karpathy et al. [11]. Namely, 80,000 training images and 5,000 images for both validation and testing.\n1http://cs.stanford.edu/people/karpathy/deepimagesent/"}, {"heading": "4.2 Implementation Details", "text": "Visual feature. We used two visual models for encoding image: Caffe [9] reference model which is pre-trained with AlexNet [14] and 16-layer VggNet model [31]. The fc-7 and fc-15 features are extracted and feed to train visuallanguage model with LSTM. Previous work [37, 22] have demonstrated that with more powerful image models such as GoogleNet [35] and VggNet [31] can achieve promising improvements. To make a fair comparison with recent works, we selected the widely used two models for experiments.\nTextual feature. We first represent each word w within sentence as one-hot vector, w \u2208 RK , K is vocabulary size built on training sentences and different for different datasets. By performing basic tokenization and removing the words that occurs less than 5 times in the training set, we have 2028, 7400 and 8801 words for Flickr8K, Flickr30K and MSCOCO dataset vocabularies respectively.\nOur work uses the LSTM implementation of [4] on Caffe framework. All of our experiments were conducted on Ubuntu 14.04, 16G RAM, single Titan X GPU with 12G memory. Our LSTMs use 1000 hidden units and weights initialized uniformly from [-0.08, 0.08]. The batch sizes are 150, 100, 100, 32 for Bi-LSTM, Bi-S-LSTM, Bi-F-LSTM and BiLSTM(VGG) models respectively. Models are trained with learning rate \u03b7 = 0.01 (except \u03b7 = 0.005 for Bi-LSTM(VGG)), weight decay \u03bb is 0.0005 and we used momentum 0.9. Each model is trained for 18\u223c35 epochs with early stopping."}, {"heading": "4.3 Evaluation Metrics", "text": "We evaluate our models on two tasks: caption generation and image-sentence retrieval. In caption generation, we follow previous work to use BLEU-N (N=1,2,3,4) scores [27]:\nBN = min(1, e 1\u2212 r c ) \u00b7 e 1 N \u2211N n=1 log pn (16)\nwhere r, c are the length of reference sentence and generated sentence, pn is the modified n-gram precisions. We also report METETOR [18] and CIDEr [36] scores for further comparison. In image-sentence retrieval (image query sentence and vice versa), we adopt R@K (K=1,5,10) and Med r as evaluation metrics. R@K is the recall rate R at top K candidates and Med r is the median rank of the first retrieved ground-truth image and sentence. All mentioned\nmetric scores are computed by MSCOCO caption evaluation server2, which is commonly used for image caption challenge3."}, {"heading": "4.4 Visualization and Qualitative Analysis", "text": "The aim of this set experiment is to visualize the properties of proposed bidirectional LSTM model and explain how it works in generating sentence word by word over time.\nFirst, we examine the temporal evolution of internal gate states and understand how bidirectional LSTM units retain valuable context information and attenuate unimportant information. Figure 6 shows input and output data, the pattern of three sigmoid gates (input, forget and output) as well as cell states. We can clearly see that dynamic states are periodically distilled to units from time step t = 0 to t = 11. At t = 0, the input data are sigmoid modulated to input gate i(t) where values lie within in [0,1]. At this step, the values of forget gates f(t) of different LSTM units are zeros. Along with the increasing of time step, forget gate starts to decide which unimportant information should be forgot, meanwhile, decide to retain those useful information. Then the memory cell states c(t) and output gate o(t) gradually absorb the valuable context information over time and make a rich representation h(t) of the output data.\nNext, we examine how visual and textual features are embedded to common semantic space and used to predict word over time. Figure 7 shows the evolution of hidden units at different layers. For T-LSTM layer where LSTM units are conditioned by textual context from the past and future. It performs as the encoder of forward and backward sentences. At M-LSTM layer, LSTM units are conditioned by both vi-\n2https://github.com/tylin/coco-caption 3http://mscoco.org/home/\nsual and textual context. It learns the correlations between input word sequence and visual information that encoded by CNN. At given time step, by removing unimportant information that make less contribution to correlate input word and visual context, the units tend to appear sparsity pattern and learn more discriminative representations from inputs. At higher layer, embedded multimodal representations are used to compute the probability distribution of next predict word with Softmax. It should be noted, for given image, the number of words in generated sentence from forward and backward direction can be different.\nFigure 8 presents some example images with generated captions. From it we found some interesting patterns of bidirectional captions: (1) Cover different semantics, for example, in (b) forward sentence captures \u201ccouch\u201d and \u201ctable\u201d while backward one describes \u201cchairs\u201d and \u201ctable\u201d. (2) Describe static scenario and inference dynamics, in (a) and (d), one caption describes the static scene, and the other one presents the potential action or motion that possibly happen in the next time step. (3) Generate novel sentences, from generated captions, we found that a significant proportion (88% by randomly select 1000 images on MSCOCO validation set) of generated sentences are novel (not appear in training set). But generated sentences are highly similar to ground-truth captions, for example in (d), forward caption is similar to one of ground-truth captions (\u201cA passenger train that is pulling into a station\u201d) and backward caption is similar to ground-truth caption (\u201ca train is in a tunnel by a station\u201d). It illustrates that our model has strong capability in learning visual-language correlation and generating novel sentences.\n4.5 Results on Caption Generation\nNow, we compare with state-of-the-art methods. Table 1 presents the comparison results in terms of BLEU-N. Our approach achieves very competitive performance on evaluated datasets although with less powerful AlexNet visual model. We can see that increase the depth of LSTM is beneficial on generation task. Deeper variant models mostly obtain better performance compare to Bi-LSTM, but they are inferior to latter one in B-3 and B-4 on Flickr8K. We conjecture it should be the reason that Flick8K is a relatively small dataset which suffers difficulty in training deep models with limited data. One of interesting facts we found is that by stacking multiple LSTM layers is generally superior to LSTM with fully connected transition layer although Bi-S-LSTM needs more training time. By replacing AlexNet with VggNet results in significant improvement on all BLEU evaluation metrics. We should be aware of that a recent interesting work [39] achieves the best results by integrating attention mechanism [19, 39] on this task. Although we believe incorporating such powerful mechanism into our framework can make further improvements, note that our current model Bi-LSTMV achieves the best or second best results on most of metrics while the small gap in performance between our model and Hard-AttentionV [39] is existed.\nThe further comparison on METEOR and CIDEr scores\n4On MSCOCO dataset, NIC uses 4K images for validation and test. LRCN randomly selects 5K images from MSCOCO validation set for validation and test. m-RNN uses 4K images for validation and 1K as test.\nis plotted in Figure 9. Without integrating object detection and more powerful vision model, our model (Bi-LSTMA) outperforms DeepVSV [11] in a certain margin. It achieves 19.4/49.6 on Flickr 8K (compare to 16.7/31.8 of DeepVSV ) and 16.2/28.2 on Flickr30K (15.3/24.7 of DeepVSV ). On MSCOCO, our Bi-S-LSTMV obtains 20.8/66.6 for METEOR/CIDEr, which exceeds 19.5/66.0 in DeepVSV ."}, {"heading": "4.6 Results on Image-Sentence Retrieval", "text": "For retrieval evaluation, we focus on image to sentence retrieval and vice versa. This is an instance of cross-modal retrieval [6, 29, 38] which has been a hot research subject in multimedia field. Table 2 illustrates our results on different datasets. The performance of our models exceed those compared methods on most of metrics or matching existing results. In a few metrics, our model didn\u2019t show better result than Mind\u2019s Eye [2] which combined image and text\nfeatures in ranking (it makes this task more like multimodal retrieval) and NIC [37] which employed more powerful vision model, large beam size and model ensemble. While adopting more powerful visual model VggNet results in significant improvements across all metrics, with less powerful AlexNet model, our results are still competitive on some metrics, e.g. R@1, R@5 on Flickr8K and Flickr30K. We also note that on relatively small dataset Filckr8K, shallow model performs slightly better than deeper ones on retrieval task, which in contrast with the results on the other two datasets. As we explained before, we think deeper LSTM architectures are better suited for ranking task on large datasets which provides enough training data for more complicate model training, otherwise, overfitting occurs. By increasing data variations with our implemented data augmentation techniques can alleviate it in a certain degree. But we foresee further significant improvement gains as training example grows, by reducing reliance on augmentation with fresh data. Figure 10 presents some examples of retrieval experiments. For each caption (image) query, sensible images and descriptive captions are retrieved. It shows our models captured the visual-textual correlation for image and sentence ranking."}, {"heading": "4.7 Discussion", "text": "Efficiency. In addition to showing superior performance, our models also possess high computational efficiency. Table 3 presents the computational costs of proposed models. We\nrandomly select 10 images from Flickr8K validation set, and perform caption generation and image to sentence retrieval test for 5 times respectively. The table shows the averaged time costs across 5 test results. The time costs of network initialization is excluded. The costs of caption generation includes: computing image feature, sampling bidirectional captions, computing the final caption. The time costs for retrieval considers: computing image-sentence pair scores (totally 10 \u00d7 50 pairs), ranking sentences for each image query. As can be seen from Table 1, 2 and 3, deep models have only slightly higher time consumption but yield significant improvements and proposed Bi-F-LSTM can strike the balance between performance and efficiency.\nChallenges in exact comparison. It is challenging to make a direct, extract comparison with relate methods due to the differences in dataset division on MSCOCO. In principle, testing on smaller validation set can lead to better results, particularly in retrieval task. Since we strictly follow dataset splits as in [11], we compare to it in most cases. Another challenge is the visual model that used for encoding image inputs. Different models are employed in different works, to make a fair and comprehensive comparison, we se-\nlect commonly used AlexNet and VggNet in our work. Limitations. Although our models have showed superiorities to related methods, they also have some limitations. The first one is the training complexity that introduced by adding backward direction LSTM and increasing the depth of bidirectional LSTM. Those factors make our end-to-end model training take more time. Secondly, our proposed algorithm for selecting final captions based on forward and backward captions sometimes cannot make right decision if the length of two generated captions are quite different. Because the caption with many more words generally has higher score that computed by \u2211T t=1 p(wt|I). An alternative option is to apply the selection before Softmax layer through average or max-out bidirectional word portabilities. But our experiments did not show promising result on it. As a future concern, a more efficient word-level selection need to be explored for avoiding this problem."}, {"heading": "5. CONCLUSIONS", "text": "We proposed a bidirectional LSTM model that generates descriptive sentence for image by taking both history and future context into account. We further designed deep bidirectional LSTM architectures to embed image and sentence at high semantic space for learning visual-language model. We also qualitatively visualized internal states of proposed model to understand how multimodal LSTM generates word at consecutive time steps. The effectiveness, generality and robustness of proposed models were evaluated on numerous datasets. Our models achieve highly completive or stateof-the-art results on both generation and retrieval tasks. Our future work will focus on exploring more sophisticated\nlanguage representation (e.g. word2vec) and incorporating multitask learning and attention mechanism into our model. We also plan to apply our model to other sequence learning tasks such as text recognition and video captioning."}, {"heading": "6. REFERENCES", "text": "[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine\ntranslation by jointly learning to align and translate. ICLR, 2015.\n[2] X. Chen and C. Lawrence Zitnick. Mind\u2019s eye: A recurrent visual representation for image caption generation. In CVPR, pages 2422\u20132431, 2015.\n[3] K. Cho, B. Van Merrie\u0308nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. EMNLP, 2014.\n[4] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, pages 2625\u20132634, 2015.\n[5] H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dolla\u0301r, J. Gao, X. He, M. Mitchell, and J. Platt. From captions to visual concepts and back. In CVPR, pages 1473\u20131482, 2015.\n[6] F. Feng, X. Wang, and R. Li. Cross-modal retrieval with correspondence autoencoder. In ACMMM, pages 7\u201316. ACM, 2014.\n[7] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, and T. Mikolov. Devise: A deep visual-semantic\nembedding model. In NIPS, pages 2121\u20132129, 2013.\n[8] A. Graves, A. Mohamed, and G. E. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, pages 6645\u20136649. IEEE, 2013.\n[9] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACMMM, pages 675\u2013678. ACM, 2014.\n[10] A. Karpathy, A. Joulin, and F-F. Li. Deep fragment embeddings for bidirectional image sentence mapping. In NIPS, pages 1889\u20131897, 2014.\n[11] A. Karpathy and F-F. Li. Deep visual-semantic alignments for generating image descriptions. In CVPR, pages 3128\u20133137, 2015.\n[12] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural language models. In ICML, pages 595\u2013603, 2014.\n[13] R. Kiros, R. Salakhutdinov, and R. Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.\n[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1097\u20131105, 2012.\n[15] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. Berg. Babytalk: Understanding and generating simple image descriptions. IEEE Trans. on Pattern Analysis and Machine Intelligence(PAMI), 35(12):2891\u20132903, 2013.\n[16] P. Kuznetsova, V. Ordonez, A. C. Berg, T. Berg, and Y. Choi. Collective generation of natural image descriptions. In ACL, volume 1, pages 359\u2013368. ACL, 2012.\n[17] P. Kuznetsova, V. Ordonez, T. Berg, and Y. Choi. Treetalk: Composition and compression of trees for image descriptions. Trans. of the Association for Computational Linguistics(TACL), 2(10):351\u2013362, 2014.\n[18] M. Lavie. Meteor universal: language specific translation evaluation for any target language. ACL, page 376, 2014.\n[19] Y. LeCun, Y. Bengio, and G. E. Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.\n[20] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi. Composing simple image descriptions using web-scale n-grams. In CoNLL, pages 220\u2013228. ACL, 2011.\n[21] T-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla\u0301r, and C. Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740\u2013755. Springer, 2014.\n[22] J. H. Mao, W. Xu, Y. Yang, J. Wang, Z. H. Huang, and A. Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR, 2015.\n[23] T. Mikolov, M. Karafia\u0301t, L. Burget, J. Cernocky\u0300, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH, volume 2, page 3, 2010.\n[24] T. Mikolov, S. Kombrink, L. Burget, J. H. C\u030cernocky\u0300, and S. Khudanpur. Extensions of recurrent neural network language model. In ICASSP, pages\n5528\u20135531. IEEE, 2011.\n[25] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. Berg, K. Yamaguchi, T. Berg, K. Stratos, and H. Daume\u0301 III. Midge: Generating image descriptions from computer vision detections. In ACL, pages 747\u2013756. ACL, 2012.\n[26] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng. Multimodal deep learning. In ICML, pages 689\u2013696, 2011.\n[27] K. Papineni, S. Roukos, T. Ward, and W. Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, pages 311\u2013318. ACL, 2002.\n[28] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026, 2013.\n[29] J. C. Pereira, E. Coviello, G. Doyle, N. Rasiwasia, G. Lanckriet, R. Levy, and N. Vasconcelos. On the role of correlation and abstraction in cross-modal multimedia retrieval. IEEE Trans. on Pattern Analysis and Machine Intelligence(PAMI), 36(3):521\u2013535, 2014.\n[30] C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier. Collecting image annotations using amazon\u2019s mechanical turk. In NAACL HLT Workshop, pages 139\u2013147. Association for Computational Linguistics, 2010.\n[31] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[32] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng. Grounded compositional semantics for finding and describing images with sentences. Trans. of the Association for Computational Linguistics(TACL), 2:207\u2013218, 2014.\n[33] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep boltzmann machines. In NIPS, pages 2222\u20132230, 2012.\n[34] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, pages 3104\u20133112, 2014.\n[35] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, pages 1\u20139, 2015.\n[36] R. Vedantam, Z. Lawrence, and D. Parikh. Cider: Consensus-based image description evaluation. In CVPR, pages 4566\u20134575, 2015.\n[37] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, pages 3156\u20133164, 2015.\n[38] X.Jiang, F. Wu, X. Li, Z. Zhao, W. Lu, S. Tang, and Y. Zhuang. Deep compositional cross-modal learning to rank via local-global alignment. In ACMMM, pages 69\u201378. ACM, 2015.\n[39] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. ICML, 2015.\n[40] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Trans. of the Association for\nComputational Linguistics(TACL), 2:67\u201378, 2014.\n[41] W. Zaremba and I. Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.\n[42] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In ECCV, pages 818\u2013833. Springer, 2014."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "CVPR, pages 2422\u20132431", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, pages 2625\u20132634", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt"], "venue": "CVPR, pages 1473\u20131482", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-modal retrieval with correspondence autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "ACMMM, pages 7\u201316. ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Devise: A deep visual-semantic  embedding model", "author": ["A. Frome", "G. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "NIPS, pages 2121\u20132129", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "ICASSP, pages 6645\u20136649. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACMMM, pages 675\u2013678. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "F-F. Li"], "venue": "NIPS, pages 1889\u20131897", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F-F. Li"], "venue": "CVPR, pages 3128\u20133137", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "ICML, pages 595\u2013603", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T. Berg"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence(PAMI), 35(12):2891\u20132903", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T. Berg", "Y. Choi"], "venue": "ACL, volume 1, pages 359\u2013368. ACL", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "T. Berg", "Y. Choi"], "venue": "Trans. of the Association for Computational Linguistics(TACL), 2(10):351\u2013362", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Meteor universal: language specific translation evaluation for any target language", "author": ["M. Lavie"], "venue": "ACL, page 376", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G.E. Hinton"], "venue": "Nature, 521(7553):436\u2013444", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "CoNLL, pages 220\u2013228. ACL", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft coco: Common objects in context", "author": ["T-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J.H. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z.H. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, volume 2, page 3", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J.H. \u010cernock\u1ef3", "S. Khudanpur"], "venue": "ICASSP, pages  5528\u20135531. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "ACL, pages 747\u2013756. ACL", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A. Ng"], "venue": "ICML, pages 689\u2013696", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": "ACL, pages 311\u2013318. ACL", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1312.6026", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "On the role of correlation and abstraction in cross-modal multimedia retrieval", "author": ["J.C. Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence(PAMI), 36(3):521\u2013535", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "NAACL HLT Workshop, pages 139\u2013147. Association for Computational Linguistics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "Trans. of the Association for Computational Linguistics(TACL), 2:207\u2013218", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "NIPS, pages 2222\u20132230", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, pages 3104\u20133112", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR, pages 1\u20139", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "Z. Lawrence", "D. Parikh"], "venue": "CVPR, pages 4566\u20134575", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, pages 3156\u20133164", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compositional cross-modal learning to rank via local-global alignment", "author": ["X.Jiang", "F. Wu", "X. Li", "Z. Zhao", "W. Lu", "S. Tang", "Y. Zhuang"], "venue": "In ACMMM,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. ICML", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Trans. of the Association for  Computational Linguistics(TACL), 2:67\u201378", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV, pages 818\u2013833. Springer", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 9, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 12, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 16, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 15, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 21, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 31, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 36, "context": "Automatically describe an image using sentence-level captions has been receiving much attention recent years [11, 10, 13, 17, 16, 22, 32, 37].", "startOffset": 109, "endOffset": 141}, {"referenceID": 19, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 14, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 24, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 16, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 15, "context": "While some previous works [20, 15, 25, 17, 16] have been proposed to address the problem of image captioning, they are rely on either use sentence templates, or treat it as retrieval task through ranking the best matching sentence in database as caption.", "startOffset": 26, "endOffset": 46}, {"referenceID": 10, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 9, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 12, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 21, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 31, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 36, "context": "Recent work [11, 10, 13, 22, 32, 37] indicate that embedding visual and language to common semantic space with relatively shallow recurrent neural network (RNN) yield promising results.", "startOffset": 12, "endOffset": 36}, {"referenceID": 13, "context": "Why deeper LSTMs? The recent success of deep CNN in image classification and object detection [14, 31] demonstrates that deep, hierarchical models can be more efficient at learning representation than shallower ones.", "startOffset": 94, "endOffset": 102}, {"referenceID": 30, "context": "Why deeper LSTMs? The recent success of deep CNN in image classification and object detection [14, 31] demonstrates that deep, hierarchical models can be more efficient at learning representation than shallower ones.", "startOffset": 94, "endOffset": 102}, {"referenceID": 27, "context": "As claimed in [28], if we consider LSTM as a composition of multiple hidden layers that unfolded in time, LSTM is already deep network.", "startOffset": 14, "endOffset": 18}, {"referenceID": 25, "context": "Multimodal representation learning [26, 33] has significant value in multimedia understanding and retrieval.", "startOffset": 35, "endOffset": 43}, {"referenceID": 32, "context": "Multimodal representation learning [26, 33] has significant value in multimedia understanding and retrieval.", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "For example, the work [20] was proposed to parse a whole sentence into several phrases, and learn the relationships between phrases and objects within an image.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "In [15], conditional random field(CRF) was used to correspond objects, attributes and prepositions of image content and predict the best label.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Other similar works were presented in [25, 17, 16].", "startOffset": 38, "endOffset": 50}, {"referenceID": 16, "context": "Other similar works were presented in [25, 17, 16].", "startOffset": 38, "endOffset": 50}, {"referenceID": 15, "context": "Other similar works were presented in [25, 17, 16].", "startOffset": 38, "endOffset": 50}, {"referenceID": 16, "context": "By leveraging distance metric to retrieve similar captioned images, then modify and combine retrieved captions to generate caption [17].", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 35, "endOffset": 43}, {"referenceID": 41, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 35, "endOffset": 43}, {"referenceID": 22, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 73, "endOffset": 84}, {"referenceID": 23, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 73, "endOffset": 84}, {"referenceID": 0, "context": "Inspired by the success use of CNN [14, 42] and Recurrent Neural Network [23, 24, 1].", "startOffset": 73, "endOffset": 84}, {"referenceID": 36, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 38, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 12, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 9, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 10, "context": "The third category is emerged as neural network based methods [37, 39, 13, 10, 11].", "startOffset": 62, "endOffset": 82}, {"referenceID": 11, "context": "[12] can been as pioneer work to use neural network for image captioning with multimodal neural language model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In their follow up work [13], Kiro et al.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "[32] presented a DT-RNN (Dependency Tree-Recursive Neural Network) to embed sentence into a vector space in order to retrieve images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] proposed m-RNN which replace feed-forward neural language model in [13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[22] proposed m-RNN which replace feed-forward neural language model in [13].", "startOffset": 72, "endOffset": 76}, {"referenceID": 36, "context": "Similar architectures were introduced in NIC [37] and LRCN [4], both approaches use LSTM to learn text context.", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "Similar architectures were introduced in NIC [37] and LRCN [4], both approaches use LSTM to learn text context.", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "[22] and LRCN [4]\u2019s model consider image context at each time step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[22] and LRCN [4]\u2019s model consider image context at each time step.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "Another group of neural network based approaches has been introduced in [10, 11] where image captions generated by integrating object detection with R-CNN (region-CNN) and inferring the alignment between image regions and descriptions.", "startOffset": 72, "endOffset": 80}, {"referenceID": 10, "context": "Another group of neural network based approaches has been introduced in [10, 11] where image captions generated by integrating object detection with R-CNN (region-CNN) and inferring the alignment between image regions and descriptions.", "startOffset": 72, "endOffset": 80}, {"referenceID": 4, "context": "[5] used multi-instance learning and traditional maximum-entropy language model for description generation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] proposed to learn visual representation with RNN for generating image caption.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "In [39], Xu et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 40, "context": "The LSTM we used is described in [41].", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "It has been successfully applied to machine translation [3], speech recognition [8] and sequence learning [34].", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "It has been successfully applied to machine translation [3], speech recognition [8] and sequence learning [34].", "startOffset": 80, "endOffset": 83}, {"referenceID": 33, "context": "It has been successfully applied to machine translation [3], speech recognition [8] and sequence learning [34].", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "Our motivation comes from the finding of [28], in which DT(S)-RNN (deep transition RNN with shortcut) is designed by adding", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "[22, 4].", "startOffset": 0, "endOffset": 7}, {"referenceID": 3, "context": "[22, 4].", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "Since our largest dataset has only 80K images [21] which might cause overfitting easily, we adopted several techniques such as finetuning on pre-trained visual model, weight decay, dropout and early stopping that commonly used in previous work.", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "In our work, we fix k = 1 on all experiments although the average of 2 BLEU [27] points better results can be achieved with k = 20 compare to k = 1 as reported in [37].", "startOffset": 76, "endOffset": 80}, {"referenceID": 36, "context": "In our work, we fix k = 1 on all experiments although the average of 2 BLEU [27] points better results can be achieved with k = 20 compare to k = 1 as reported in [37].", "startOffset": 163, "endOffset": 167}, {"referenceID": 29, "context": "To validate the effectiveness, generality and robustness of our models, we conduct experiments on three benchmark datasets: Flickr8K [30], Flickr30K [40] and MSCOCO [21].", "startOffset": 133, "endOffset": 137}, {"referenceID": 39, "context": "To validate the effectiveness, generality and robustness of our models, we conduct experiments on three benchmark datasets: Flickr8K [30], Flickr30K [40] and MSCOCO [21].", "startOffset": 149, "endOffset": 153}, {"referenceID": 20, "context": "To validate the effectiveness, generality and robustness of our models, we conduct experiments on three benchmark datasets: Flickr8K [30], Flickr30K [40] and MSCOCO [21].", "startOffset": 165, "endOffset": 169}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "We used two visual models for encoding image: Caffe [9] reference model which is pre-trained with AlexNet [14] and 16-layer VggNet model [31].", "startOffset": 52, "endOffset": 55}, {"referenceID": 13, "context": "We used two visual models for encoding image: Caffe [9] reference model which is pre-trained with AlexNet [14] and 16-layer VggNet model [31].", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "We used two visual models for encoding image: Caffe [9] reference model which is pre-trained with AlexNet [14] and 16-layer VggNet model [31].", "startOffset": 137, "endOffset": 141}, {"referenceID": 36, "context": "Previous work [37, 22] have demonstrated that with more powerful image models such as GoogleNet [35] and VggNet [31] can achieve promising improvements.", "startOffset": 14, "endOffset": 22}, {"referenceID": 21, "context": "Previous work [37, 22] have demonstrated that with more powerful image models such as GoogleNet [35] and VggNet [31] can achieve promising improvements.", "startOffset": 14, "endOffset": 22}, {"referenceID": 34, "context": "Previous work [37, 22] have demonstrated that with more powerful image models such as GoogleNet [35] and VggNet [31] can achieve promising improvements.", "startOffset": 96, "endOffset": 100}, {"referenceID": 30, "context": "Previous work [37, 22] have demonstrated that with more powerful image models such as GoogleNet [35] and VggNet [31] can achieve promising improvements.", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "Our work uses the LSTM implementation of [4] on Caffe framework.", "startOffset": 41, "endOffset": 44}, {"referenceID": 26, "context": "In caption generation, we follow previous work to use BLEU-N (N=1,2,3,4) scores [27]:", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "We also report METETOR [18] and CIDEr [36] scores for further comparison.", "startOffset": 23, "endOffset": 27}, {"referenceID": 35, "context": "We also report METETOR [18] and CIDEr [36] scores for further comparison.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "At t = 0, the input data are sigmoid modulated to input gate i(t) where values lie within in [0,1].", "startOffset": 93, "endOffset": 98}, {"referenceID": 36, "context": "Models B-1 B-2 B-3 B-4 B-1 B-2 B-3 B-4 B-1 B-2 B-3 B-4 NIC[37]G,\u2021 63 41 27.", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "6 LRCN[4]A,\u2021 - - - 58.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "4 DeepVS[11]V 57.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "1 23 m-RNN[22]A,\u2021 56.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "0 54 36 23 15 - - - m-RNN[22]V,\u2021 - - - 60 41 28 19 67 49 35 25 Hard-Attention[39]V 67 45.", "startOffset": 25, "endOffset": 29}, {"referenceID": 38, "context": "0 54 36 23 15 - - - m-RNN[22]V,\u2021 - - - 60 41 28 19 67 49 35 25 Hard-Attention[39]V 67 45.", "startOffset": 77, "endOffset": 81}, {"referenceID": 38, "context": "We should be aware of that a recent interesting work [39] achieves the best results by integrating attention mechanism [19, 39] on this task.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "We should be aware of that a recent interesting work [39] achieves the best results by integrating attention mechanism [19, 39] on this task.", "startOffset": 119, "endOffset": 127}, {"referenceID": 38, "context": "We should be aware of that a recent interesting work [39] achieves the best results by integrating attention mechanism [19, 39] on this task.", "startOffset": 119, "endOffset": 127}, {"referenceID": 38, "context": "Although we believe incorporating such powerful mechanism into our framework can make further improvements, note that our current model Bi-LSTM achieves the best or second best results on most of metrics while the small gap in performance between our model and Hard-Attention [39] is existed.", "startOffset": 276, "endOffset": 280}, {"referenceID": 10, "context": "Without integrating object detection and more powerful vision model, our model (Bi-LSTM) outperforms DeepVS [11] in a certain margin.", "startOffset": 108, "endOffset": 112}, {"referenceID": 5, "context": "This is an instance of cross-modal retrieval [6, 29, 38] which has been a hot research subject in multimedia field.", "startOffset": 45, "endOffset": 56}, {"referenceID": 28, "context": "This is an instance of cross-modal retrieval [6, 29, 38] which has been a hot research subject in multimedia field.", "startOffset": 45, "endOffset": 56}, {"referenceID": 37, "context": "This is an instance of cross-modal retrieval [6, 29, 38] which has been a hot research subject in multimedia field.", "startOffset": 45, "endOffset": 56}, {"referenceID": 1, "context": "In a few metrics, our model didn\u2019t show better result than Mind\u2019s Eye [2] which combined image and text", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "Flickr 8K DeViSE[7] 4.", "startOffset": 16, "endOffset": 19}, {"referenceID": 31, "context": "6 29 SDT-RNN[32] 4.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "0 29 DeFrag[10]+O 12.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "[13]A 13.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13]V 18 40.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "5 10 m-RNN[22]A 14.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "4 15 Mind\u2019s Eye[2]V 17.", "startOffset": 15, "endOffset": 18}, {"referenceID": 10, "context": "1 8 DeepVS[11]+O,V 16.", "startOffset": 10, "endOffset": 14}, {"referenceID": 36, "context": "4 NIC[37]G 20 60 6 19 64 5 Bi-LSTMA 21.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "Flickr 30K DeViSE[7] 4.", "startOffset": 17, "endOffset": 20}, {"referenceID": 31, "context": "7 25 SDT-RNN[32] 9.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "[13]A 14.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13]V 23.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "5 8 LRCN[4]A 14 34.", "startOffset": 8, "endOffset": 11}, {"referenceID": 36, "context": "9 47 11 NIC[37]G 17 56 7 17 57 8 m-RNN[22]A 18.", "startOffset": 11, "endOffset": 15}, {"referenceID": 21, "context": "9 47 11 NIC[37]G 17 56 7 17 57 8 m-RNN[22]A 18.", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "5 16 Mind\u2019s Eye[2]V 18.", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "9 8 DeFrag [10]+O 16.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "5 13 DeepVS[11]+O,V 22.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "MSCOCO DeepVS[11]+O,V 16.", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "features in ranking (it makes this task more like multimodal retrieval) and NIC [37] which employed more powerful vision model, large beam size and model ensemble.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Since we strictly follow dataset splits as in [11], we compare to it in most cases.", "startOffset": 46, "endOffset": 50}], "year": 2016, "abstractText": "This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our model builds on a deep convolutional neural network (CNN) and two separate LSTM networks. It is capable of learning long term visual-language interactions by making use of history and future context information at high level semantic space. Two novel deep bidirectional variant models, in which we increase the depth of nonlinearity transition in different way, are proposed to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale and vertical mirror are proposed to prevent overfitting in training deep models. We visualize the evolution of bidirectional LSTM internal states over time and qualitatively analyze how our models \u201ctranslate\u201dimage to sentence. Our proposed models are evaluated on caption generation and image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art results on caption generation even without integrating additional mechanism (e.g. object detection, attention model etc.) and significantly outperform recent methods on retrieval task.", "creator": "LaTeX with hyperref package"}}}