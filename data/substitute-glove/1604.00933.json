{"id": "1604.00933", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Entity Type Recognition using an Ensemble of Distributional Semantic Models to Enhance Query Understanding", "abstract": "We having an ensemble change for tachyons search database entities in was awareness domain. Understanding saw many of interests indicated end old through instructions (Company, Skill, Job Title, etc.) applications more insightful agencies end-user based upon those groups compared to a symbols 1h03 - limited users. Because search forwarded are addition fairly short, optimise but enjoy pack - example - referred wheel to others creating example able be motivated continuing well for lack of behavioral information. Our approach instead uses troubling three different have of varying complexity well order to seize real - success own about encrypted establishing. We professionals underlies terminological geometric of server entities both as systems: minutes) morphological topologies generated from encyclopedic corpora more Wikipedia, under 1) college dimensional word formula_9 vectors reduced from jobs of did interviews can word2vec. Additionally, sense approach amplifier both entity defining properties prior then WordNet and crux construct deposits from DBpedia. We evaluate what approach days into online for same to CareerBuilder; later sector yet national present form US. The web second contains entities extracted year 250,000 same time authorities / colleges carry faqs, work postings, and resume documents. After constructing the dichotomous vectors of searched councils, we rely implementing machine curriculum made supposing search wholly consist. Empirical analysts show as still clearly corniness making regional - especially - full - history word2vec distributional semantics is worked place Wikipedia. Moreover, we sustain networking - averaged F 31 score of 97% standard before possible asymptotic motivations acoustic.", "histories": [["v1", "Mon, 4 Apr 2016 16:18:44 GMT  (497kb,D)", "http://arxiv.org/abs/1604.00933v1", "A short version of this paper has been accepted in \"COMPSAC 2016: The 40th IEEE Computer Society International Conference on Computers, Software &amp; Applications\""]], "COMMENTS": "A short version of this paper has been accepted in \"COMPSAC 2016: The 40th IEEE Computer Society International Conference on Computers, Software &amp; Applications\"", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["walid shalaby", "khalifeh al jadda", "mohammed korayem", "trey grainger"], "accepted": false, "id": "1604.00933"}, "pdf": {"name": "1604.00933.pdf", "metadata": {"source": "CRF", "title": "Entity Type Recognition using an Ensemble of Distributional Semantic Models to Enhance Query Understanding", "authors": ["Walid Shalaby", "Khalifeh Al Jadda", "Mohammed Korayem", "Trey Grainger"], "emails": ["wshalaby@uncc.edu", "trey.grainger@careerbuilder.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nEntity Recognition (ER) is an information extraction task which refers to identifying regions of text corresponding to entities. A sub-task related to ER is the Entity Type Recognition (ETR) which refers to categorizing these entities into a predefined set of types [1]. The focus of the majority of ETR research has been on Named Entity Recognition (NER), which typically limits entity types to Person, Location, and Organization [2]\u2013[5]. Most techniques used in ETR rely on a mix of local information about the context of the entity and external knowledge usually gained through learning on training data. ETR in search queries is considered extremely important; a Microsoft\u2019s study reported that 71% of queries submitted to their Bing search engine contain named entities somewhere, while 20 \u2212 30% are purely named entities [6]. Recognizing\nthe type of named entities in queries enables a search engine to understand the intent of users, which subsequently leads to more accurate results being returned. ETR in search queries is very challenging, however, due to the lack of textual context surrounding the query. Search queries are usually made of just a few words, which is typically not enough context to independently and accurately recognize the types of the entities within a search query. Our research is specifically targeted at the problem of ETR within the job search and recruitment domain. Unfortunately, none of the published ETR datasets fully resemble the entity categories within the job search and recruitment domain. Some of the specific entity categories within this domain include Company, Job Title, School, and Skill, which all aren\u2019t found explicitly within existing ETR datasets. As a result, we can\u2019t leverage any existing gazetteers for these entity types.\nIn this paper we introduce a novel system for ETR in search queries which has been applied successfully within the job search and recruitment domain. The proposed system utilizes features collected from Wikipedia, DBpedia , WordNet , and a corpus of more than 60 million job postings provided by Careerbuilder. We integrated this model within CareerBuilder\u2019s semantic search engine [7]\u2013[9], which improved the quality of search results for tens of millions of job seekers every month.\nThe system is used within the search engine in two ways: 1) offline, to classify a list of pre-recognized entities extracted from popular queries found in CareerBuilder\u2019s search logs, and 2) online, to dynamically classify the search entities within new, previously unseen queries as part of CareerBuilder\u2019s semantic query parser.\nTo the best of our knowledge we are the first group targeting ETR of queries within the job search and recruitment domain. We evaluated this system using a data set provided by CareerBuilder which contains more than 177K labeled entities. The results demonstrate that our system achieves a 97% microaveraged F1 score over all the categories.\nThe main contributions of this paper are:\n1) We introduce a novel approach for generating distributional semantic vectors of named entities in search queries\nar X\niv :1\n60 4.\n00 93\n3v 1\n[ cs\n.C L\n] 4\nA pr\nusing Wikipedia as an intermediate corpus. 2) Our approach is simple and efficient. It outperforms state-\nof-the-art techniques for distributional representations like word2vec. 3) We evaluate our method on the largest labeled entity type data set within the recruitment domain achieving a 97% micro-averaged F1 score. 4) We demonstrate increases in overall system accuracy through an ensemble of features leveraging distributional semantic representations, entity ontologies, and entity linguistic properties."}, {"heading": "II. RELATED WORK", "text": "Both ETR and NER have experienced a surge in the research community in recent years [10]\u2013[16]. David et al. [2] and Mansouri et al. [17] presented comprehensive reviews about different approaches for NER including several representations that leverage dictionaries, corpora, and various classification methods.\nGuo et al. [18] presented a formulation for both NER and ETR in search queries using a probabilistic approach and Latent Dirichlet Allocation (LDA). They represented query terms as words in documents and modeled the entity type classes as topics. They proposed using a weakly supervised learning algorithm to learn the topics, while impressive, their approach was limited to recognizing only one entity per query. Our approach, instead, can accurately identify multiple entities per search query and recognize their types.\nOther approaches which utilize knowledge bases to link named entities in text with corresponding entities in the knowledge bases were presented in [1], [19]\u2013[22]. Wikipedia has been used extensively as a knowledge base for ETR. Many researchers have utilized Wikipedia-based features such as wikilinks, article titles and categories, and graph representations of the inner links between Wikipedia pages.\nHan et al. [1] proposed a methodology which relies on having a Wikipedia page whose title is similar to the given entity. After looking up that page, if any, they extracted the category of that entity from the first line in that page. In our case, we couldn\u2019t find a Wikipedia page for most of the popular queries we have, for example, java developer has no corresponding page in Wikipedia. Our methodology can handle such cases by looking in Wikipedia content not titles for the occurrences of that entity and using the context as a representation in order to recognize the entity type.\nRichman and Schone proposed a novel system for multilingual NER [23] . They utilize wikilinks to identify words and phrases that might be entities within text. Once they recognize the entities, they use category links or interlinks to map those entities with English phrases or categories.\nUsing Wikipedia concepts as a representation space for query\u2019s intent was introduced in [24]. In this paper each intent domain is represented as a set of Wikipedia articles and categories, then each query intent is predicted by mapping the query into the Wikipedia representation space.\nThe system introduced in [25] transforms links to Wikipedia articles into named entity annotations by classifying\nthe target articles into the classic named entity types Person, Location, and Organization.\nUtilizing Wikipedia infobox for ETR was presented in [26]. The proposed model classifies entities by matching entity attributes extracted from the relevant article infobox with core entity attributes built from Wikipedia infobox templates.\nThe system introduced in [27] converted Wikipedia into a structured knowledge base (KB). In this work, the authors converted Wikipedia graph structure into a taxonomy. This was done by finding a single main lineage, called the primary lineage, for each concept. This KB is used later to extract, link, and classify entities mentioned in a Twitter stream.\nWe consider [28] as the most related work to ours. In this work, the authors proposed a system that utilizes Wikipedia as an intermediate corpus to categorize search queries. The system works through two phases; in the first phase, a query is mapped to its relevant Wikipedia pages by searching an index of Wikipedia articles. In the second phase, concepts representing retrieved Wikipedia pages are mapped into categories. Though we also utilize a Wikipedia search index to retrieve articles related to query entities, our approach utilizes totally different features and entity representation to infer entity type."}, {"heading": "III. METHODOLOGY", "text": "In this section we detail our methodology for recognizing search query entity types. Our approach employs two distributional semantic representations of search entities. Moreover, we utilize ontological properties as well as linguistic properties of search entities to improve overall system performance. The ultimate goal of our system is to categorize a given search entity into one of four categories: Company, Job Title, Skill, and School. We do plan to expand these categories in the future, but these four represent the most important to initially target."}, {"heading": "A. System Overview", "text": "Prior to performing ETR, it is of course necessary that we first perform ER on incoming search queries so that we know the entities for which we are trying to identify an entity type. Our methodology for recognizing known entities and performing Entity Extraction from queries was previously described in [29]. In essence, we perform data mining on historical search query logs, perform collaborative filtering to determine which queries are used commonly together across many users, and build a semantic knowledge base containing the entities and related entities found from within the mined search logs.\nBased upon this semantic knowledge base, we are able to perform entity extraction on future queries for known entities, but we are missing two important components:\n1) Identification of entities not found in our semantic knowledge base. 2) Knowledge of the entity type of each identified entity.\nTo solve the first problem, we implemented a language model of unigrams, bigrams, and trigrams across a corpus of millions of job posting documents. Leveraging Bayes algorithm, we are able to dynamically calculate probabilities as to\nwhether any combination of keywords entered into a search query constitute a single phrase or multiple phrases. Based upon the combination of our semantic knowledge base, our Bayes-based phrase identifier, and our query parser, we are able to successfully identify the correct query parsing including the constituent named entities with accuracy of greater than 92%.\nThe last stage needed to truly interpret the user\u2019s query correctly is ETR. If a user searches for google software engineer java, it is critical to understand that the user is looking for a job at Google (Company) as a software engineer (Job Title) programming in Java (Skill). Without this knowledge of entity types, we will not be able to fully represent the information need of our users within the search system. The following sections will describe our methodology for performing ETR on our identified entities."}, {"heading": "B. Entity Type Recognition Process", "text": "The proposed system combines features from different sources in order to make accurate entity type predictions for a given search entity. This ensemble of features represents our domain-specific knowledge as well as real-world knowledge about the search entity. We call these features clues. Figure 1 shows the system design for how a user\u2019s query is parsed, as well as how the system leverages these feature clues to accurately perform ETR.\nThe first clue models real-world contextual information about the query entity by searching for that entity inside Wikipedia using a customized search index. The second clue models domain-specific knowledge by building synonym vectors of search entities using the word2vec model [30]. These vectors are generated using millions of job postings from CareerBuilder.\nTwo other clues, leveraging DBpedia and WordNet, are collected to increase the accuracy and coverage over the Company and Job Title categories specifically. After collecting all the clues for every known query entity, we combine these features and use them to train an entity classifier over labeled entity samples. The classifier can then be used to categorize new search entities, thus improving our understanding of the query intent for future searches."}, {"heading": "C. Constructing Contextual Vectors", "text": "The purpose of this phase is to enrich the contextless search entities with contextual information. In order to do so we map each entity into a distributional semantic vector representation. The vector dimensions represent entity contexts in an intermediate corpus. We use Wikipedia as the source for these contextual vectors for all of the search entities which are represented.\nAs query entities need to be categorized in an online fashion, context vectors are required to be constructed as efficiently as possible. Therefore, we build an inverted index of all Wikipedia articles as a preprocessing step. We build the index using Apache Lucene1, an open-source indexing and search engine. For each article we index the title, content, length, and categories. We exclude all disambiguation, list of, and redirect pages.\nAs shown in Equation 1, given an entity ej we construct its context vector Xej by first searching for that entity in the search index. Then, from the top n search hits, we retrieve all content words Wi that occur in the same context of ej within a specific window size in each search hit i. We also retrieve category words Ci of search hits and add them to Xej .\nXej =<w1, w2, ..., c1, c2, ...> : w\u2208Wi, c\u2208Ci, i=[1..n] (1) 1https://lucene.apache.org/\nThese context vectors represent available real-world knowledge about the given entity. Table I shows example search entities along with their context vectors. We can notice that contextual words are very representative for the given entity. Moreover, words from search hits categories augment context words and thus enrich the contextual representation of each entity."}, {"heading": "D. Constructing Synonymy Vectors", "text": "The purpose of this phase is to enrich the search entities with domain-specific knowledge. CareerBuilder has millions of job openings that are posted or modified on daily basis. These postings contain many representative features relevant to the recruitment domain. For example, a typical job posting might contain a job title, job description, required skills, salary information, company information, required experience and education, location...etc.\nIn order to utilize this information, we use the job postings as an intermediate corpus to train a word2vec model. For a given search entity ej , we generate its synonyms vector Sej from words that have closest distributional representations in the trained word2vec model.\nDistributional semantic vectors generated in this phase represent domain-specific knowledge about a given entity. Table II shows the same search entities as in Table I along with corresponding synonymy vectors. We can notice that the Company and School entity vectors are somewhat poor and unrepresentative. This is because many job postings are missing company information or sometimes company name is only provided without any context. The same problem arises for school information. On the other hand, synonymy vectors of Job Title and Skill entities are very rich and representative. This observation motivated us to combine features for search entities from both contextual and synonymy vectors in a combined vector space."}, {"heading": "E. Entity Ontological Features", "text": "Another representative feature is extracted from DBpedia by linking search hits (representing Wikipedia concepts) to their corresponding entries in the DBpedia ontology. We use the type property to determine whether the retrieved concept type is one of our targeted categories, specifically Company.\nAfter searching for a given entity ej in the Wikipedia index, we retrieve the top n search hits (concepts). Then, we check whether the title of any of these concepts is the same as ej . If any, we check whether the type of this concept in DBpedia ontology is Company and subsequently add a new binary feature indicating that finding.\nGiven that companies are already found explicitly in DBPedia, why don\u2019t we just use the DBpedia type feature exclusively for categorizing into the Company entity type? There are five reasons we instead choose to combine multiple feature types:\n1) DBpedia ontology suffers from low coverage where many companies in Wikipedia don\u2019t have a type of Company in DBpedia (e.g., Boonton Iron Works2, SalesforceIQ3).\n2https://en.wikipedia.org/wiki/Boonton_Iron_Works 3https://en.wikipedia.org/wiki/SalesforceIQ\n2) DBpedia provides categories for the canonical form of company name only. If an entity is searched for using a surface form, the DBpedia lookup will fail. In contrast, Wikipedia will generally contain surface forms in the same context as the canonical form (e.g., International Turnkey Systems Group vs. ITS Group4) 3) As DBpedia covers only Wikipedia concepts, it fails to catch companies that do not have a Wikipedia page. Alternatively, these companies will be correctly categorized using their contextual vectors if mentioned in a representative context within Wikipedia (e.g., Nutonian). 4) Some companies have a type of Organization instead of Company in DBpedia. Unfortunately, entities belonging to one of our other entity types (School) can also be categorized as Organization in DBpedia (e.g., Athens College). This means that we cannot reliably categorize concepts with the type of Organization as Company. 5) Finally, there is a time lag between DBpedia and Wikipedia. So, DBpedia does not contain the most recent snapshot of Wikipedia concepts in its ontology."}, {"heading": "F. Entity Linguistic Features", "text": "We utilize the lexical properties of search entities to determine whether they belong to one of the target categories, specifically Job Title. The motivation behind this approach is the fact that almost all Job Title entities contain an agent noun (e.g., director, developer, nurse, manager...etc). To determine whether an entity might represent a Job Title, we search its words inside the WordNet dictionary where all agent nouns are stored at the <noun.person> lexical file. Upon finding any, we add a new binary feature indicating that finding.\nWhile it might be tempting to rely exclusively on the agent noun feature from the WordNet lexicon for categorizing Job Title entities, two challenges prevent this:\n1) CareerBuilder operates job boards in many countries and in many different languages. Therefore, we\u2019re biased toward using language independent models where possible. Depending solely on the WordNet lexicon for categorizing Job Title entities would pose limitations on the ETR system for non-English job boards. 2) Not all Job Title entities have an agent noun (e.g., staff, faculty)."}, {"heading": "G. Building the Prediction Model", "text": "To build the ETR model, we use supervised machine learning on a very large labeled set of search entities obtained from CareerBuilder\u2019s search logs. For each discovered search entity ej , we generate:\n1) A Contextual vector (Xej ) using the Wikipedia index. 2) A Synonyms vector (Sej ) using the word2vec model. 3) An Ontological type (ontej ) if the entity refers to a\nDBpedia concept. This is a binary feature which is true if DBpedia type is company. 4) A Lexical type (lexej ). This is a binary feature which is true if one of the entity terms has a <noun.person> type in WordNet, i.e., it is an agent noun.\n4https://en.wikipedia.org/wiki/International_Turnkey_Systems_Group\nTo combine all those features, we follow a simple yet effective approach. First we utilize the vector space model to generate an entity-word matrix using the distributional semantic vectors (Xej , Sej ). The generated distributional vectors represent semantically-related words to the identified query entities, so it is straightforward to then map each entity as a document of words contained in the entity\u2019s contextual and synonymy vectors. Rows in the entity-word matrix represent entities and columns represent corresponding related words. Secondly, we transform this matrix using term frequencyinverse document frequency (tf-idf) weights. Thirdly, we append ontej and lexej as two additional binary columns to the tf-idf entity-word matrix. Finally, we train an entity type classifier on the produced matrix to generate the ETR model."}, {"heading": "IV. EXPERIMENTS AND RESULTS", "text": "In this section we present our empirical results. We start by describing the data set used in experiments and then detail different models developed for ETR along with their results."}, {"heading": "A. Data set", "text": "We build our ETR models using the largest labeled entity data set owned by CareerBuilder. The data set contains more than 177K labeled entities distributed over four categories as shown in Table III. These entities were obtained from CareerBuilder\u2019s search logs, job postings, and resume postings, and were manually reviewed by annotators working at CareerBuilder."}, {"heading": "B. Experimental Setup", "text": "We conducted several experiments in order to evaluate the performance of the ETR system with different models. We started by evaluating models built from a single feature source i.e., contextual vectors or word synonymy vectors. Then we\nevaluated a model built using an ensemble of both of these distributional vectors. Finally, we evaluated a model which combines both distributional vectors plus the entity\u2019s ontological and lexical features (i.e., ontej and lexej respectively).\nTo assess the effectiveness of our approach, we built two baseline models. The first one is the bag-of-words (bow) model which depends solely on words that appear in search entities as features without any contextual enrichment. The second model (wikiw) is a distributional semantic model built by training word2vec on Wikipedia. After word2vec produces word distributional vectors, word synonymy vectors of search entities are generated as described in Section III.D. We then generate a tf-idf entity-word matrix from these vectors as described in Section III.G.\nWe built the Wikipedia search index using the English Wikipedia dump of March 20155. The total uncompressed XML dump size was about 52GB representing about 7 million articles. We extracted the articles using a modified version of the Wikipedia Extractor6. Our version7 extracts articles as plain text, discarding images and tables. We discarded the References and External Links sections (if any). We pruned all articles which are not under the main namespace, and excluded all disambiguation, list of, and redirect pages as well. Eventually, our index contained about 4 million documents.\nWhile searching the Wikipedia index, we search both content and title fields. For efficiency, we limit retrieved results to the top 3 hits which have a minimum length of 100 bytes.\nTo build the word embedding vectors, we trained word2vec on more than 60 million job postings from CareerBuilder. We used Apache Spark\u2019s scalable machine learning library (MLlib8) which has an implementation of word2vec in Scala9. We configured the parameters of the word2vec model as follows: minimum word count = 50, number of iterations (epoch)=1, vector size = 300, and number of partitions = 5000. The model took about 32 hours to fit on one of CareerBuilder\u2019s Hadoop clusters with 69 data nodes, each having a 2.6 GHz\n5https://dumps.wikimedia.org/enwiki/20150304/ 6http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 7https://github.com/walid-shalaby/wikiextractor 8https://spark.apache.org/mllib/ 9http://www.scala-lang.org/\nAMD Opteron Processor with 12 to 32 cores and 32GB to 128GB RAM.\nFinally, we evaluate all the ETR models using a Support Vector Machine (SVM) classifier with a linear kernel, leveraging the scikit-learn machine learning library [31]. Because entity instance frequencies over categories is a bit skewed and to avoid overfitting, we configured the classifier to use a different regularization value for each category relative to category frequencies. For each model we report Precision (P), Recall (R), and their harmonic mean (F1) scores. All results are calculated using 10-fold cross-validation over the labeled entities data set. Folds were randomly generated using stratified sampling."}, {"heading": "C. Results", "text": "Table IV shows the results obtained from the baseline models compared to the contextual vectors model using 10- fold cross-validation on the labeled entities data set.\nThe first baseline model is the bow. This model gives relatively lower F1 scores on all categories as shown in Table IV. Due to the absence of contextual information, this model fails to generalize well with unseen entities, as they contain terms that are not in the model\u2019s feature space. This is very clear with categories that have high naming variations (i.e., Company and Skill). bow performs relatively well on Job Title as it has limited naming variations. It also performs very well on School entities as they have common naming conventions (e.g., university, school, institute...etc).\nThe second baseline model is wikiw which is built by training word2vec on Wikipedia. This model utilizes contextual features inferred from word distributional properties, hence it performs better than bow on all categories. As shown in Table IV, the wikiw F1 score is higher than bow by more than 5% on Company, 2% on Job Title, 1% on School, and 11% on Skill. Those results indicate the viability of distributional semantic representations for ETR of short search entities.\nThe third model is wikix which is built using contextual vectors generated by searching the Wikipedia index. It retrieves search entity contexts and category information from search hits and then utilizes them as learning features. As shown in Table IV, this novel approach outperforms both bow and wikiw models substantially on Company and Skill. It also performs slightly better on School. These results indicate\nthe effectiveness of the wikix model in recognizing these categories accurately.\nIt is important to mention that, though both the wikix and wikiw models use Wikipedia as an intermediate corpus to learn distributional representations of words, the wikix representations are more successful for the ETR task. Compared with the wikiw model, the F1 scores of the wikix model increased on the Company class by 5%, on the School class by 1%, and on the Skill class by 5%.\nThe Job Title category is the only example where the wikiw model performed better (by 2%) than the wikix model. A closer look at the scores reveals that, the wikix model is more accurate than the wikiw model as it has a higher P score. The wikiw model, however, has better coverage as it has a higher R score. Considering the small size of the Job Title category (~3,600 entities), that difference in recall cannot be considered substantial.\nThe results in Table IV prove empirically that, for ETR of search entities, our novel approach for modeling realworld knowledge using contextual distributional representations outperforms word2vec, the state-of-the-art for distributional semantic representations, even though both use the same intermediate corpus (Wikipedia). Moreover, our method is much simpler and more efficient than word2vec as it doesn\u2019t require optimizing an objective function for learning word embedding vectors.\nIn order to increase overall system performance, we built four ETR models that combine features from different sources as described in Section III.G. We first built jobw which models domain-specific knowledge of search entities. The jobw model is built by training word2vec on the textual content of millions of job postings.\nAs shown in Table V, we combined both contextual vector (wikix) and synonyms vector (jobw) representations and built an ensemble of the two models (wikix,jobw). The ensemble improved the results over wikix across all categories. the largest improvement was on Job Title, which saw a 3% improvement in F1 score. More importantly, this ensemble outperforms the wikiw and bow models on all categories.\nTo further increase system accuracy on Company class, we incorporated the DBpedia ontological type of search entity (ont) with contextual and synonymy vectors as described in Section III.E. This ensemble (wikix,jobw,ont), as shown in\nTable V, increased F1 score on Company by ~0.4%.\nThe third ensemble is (wikix,jobw,lex). It aims at increasing system accuracy on Job Title class by incorporating entity\u2019s linguistic features (lex) as described in Section III.F. As shown in Table V, the F1 score on Job Title increased by ~0.6% when incorporating this feature.\nFinally, we combined all features generating an ensemble of contextual vectors, synonymy vectors, ontological features, and linguistics features (wikix, jobw, lex, ont). As shown in Table V, this model produced the best F1 scores on all categories among all the aforementioned models."}, {"heading": "V. CONCLUSION", "text": "In this paper we presented an effective approach for ETR of search query entities in the job search and recruitment domain. We proposed a novel ensemble of features which enrich short query entities with real-world and domain-specific knowledge. The ensemble entity representation model contains features representing: 1) contextual information in Wikipedia, 2) embedding information in millions of job postings, 3) class type in DBpedia for Company entities, and 4) linguistic properties in WordNet for Job Title entities.\nOur approach is novel and distinct from other ETR approaches. To our knowledge, generating distributional semantic vectors of query entities using contextual information from Wikipedia as a search index was not reported before in the literature.\nEvaluation results on a data set of more than 177K search entities were very promising. The results showed that our Wikipedia-based model outperforms the state-of-the-art word2vec model trained on Wikipedia on three out of four target entity categories. Moreover, our ensemble representation could achieve 97% micro-averaged F1 score on the four entity types outperforming the word2vec baseline by 6% on Company, 1% on Job Title, 1% on School, and 5% on Skill.\nIn terms of performance, our system takes 30ms per entity type request, making it efficient and appropriate for serving online search queries.\nOur system has been integrated within CareerBuilder\u2019s semantic search engine, which improved the quality of search results for tens of millions of job seekers every month."}], "references": [{"title": "Exploiting wikipedia as external knowledge for named entity recognition", "author": ["J. Kazama", "K. Torisawa"], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 698\u2013 707, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of named entity recognition and classification", "author": ["D. Nadeau", "S. Sekine"], "venue": "Lingvisticae Investigationes, vol. 30, no. 1, pp. 3\u201326, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Extracting personal names from email: Applying named entity recognition to informal text", "author": ["E. Minkov", "R.C. Wang", "W.W. Cohen"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pp. 443\u2013450, Association for Computational Linguistics, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Person name entity recognition for arabic", "author": ["K. Shaalan", "H. Raza"], "venue": "Proceedings of the 2007 Workshop on Computational Approaches to Semitic Languages: Common Issues and Resources, pp. 17\u201324, Association for Computational Linguistics, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Named entity recognition using an hmm-based chunk tagger", "author": ["G. Zhou", "J. Su"], "venue": "proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pp. 473\u2013480, Association for Computational Linguistics, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Building taxonomy of web search intents for name entity queries", "author": ["X. Yin", "S. Shah"], "venue": "Proceedings of the 19th international conference on World wide web, pp. 1001\u20131010, ACM, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Pgmhd: A scalable probabilistic graphical model for massive hierarchical data problems", "author": ["K. AlJadda", "M. Korayem", "C. Ortiz", "T. Grainger", "J.A. Miller", "W.S. York"], "venue": "Big Data (Big Data), 2014 IEEE International Conference on, pp. 55\u201360, IEEE, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving the quality of semantic relationships extracted from massive user behavioral data", "author": ["K. AlJadda", "M. Korayem", "T. Grainger"], "venue": "Big Data (Big Data), 2015 IEEE International Conference on, pp. 2951\u20132953, IEEE, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Query sense disambiguation leveraging large scale user behavioral data", "author": ["M. Korayem", "C. Ortiz", "K. AlJadda", "T. Grainger"], "venue": "Big Data (Big Data), 2015 IEEE International Conference on, pp. 1230\u20131237, IEEE, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Introduction to the conll- 2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pp. 142\u2013147, Association for Computational Linguistics, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["L. Ratinov", "D. Roth"], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pp. 147\u2013155, Association for Computational Linguistics, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Erd\u201914: entity recognition and disambiguation challenge", "author": ["D. Carmel", "M.-W. Chang", "E. Gabrilovich", "B.-J.P. Hsu", "K. Wang"], "venue": "ACM SIGIR Forum, vol. 48, pp. 63\u201377, ACM, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Semeval-2015 task 13: Multilingual all-words sense disambiguation and entity linking", "author": ["A. Moro", "R. Navigli"], "venue": "Proceedings of SemEval-2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustype: Effective entity recognition and typing by relation phrasebased clustering", "author": ["X. Ren", "A. El-Kishky", "C. Wang", "F. Tao", "C.R. Voss", "J. Han"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 995\u2013 1004, ACM, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to link with wikipedia", "author": ["D. Milne", "I.H. Witten"], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management, pp. 509\u2013518, ACM, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "author": ["S. Cucerzan"], "venue": "EMNLP-CoNLL, vol. 7, pp. 708\u2013716, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Named entity recognition approaches", "author": ["A. Mansouri", "L.S. Affendey", "A. Mamat"], "venue": "International Journal of Computer Science and Network Security, vol. 8, no. 2, pp. 339\u2013344, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Named entity recognition in query", "author": ["J. Guo", "G. Xu", "X. Cheng", "H. Li"], "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pp. 267\u2013274, ACM, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Collective annotation of wikipedia entities in web text", "author": ["S. Kulkarni", "A. Singh", "G. Ramakrishnan", "S. Chakrabarti"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 457\u2013466, ACM, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Collective entity linking in web text: a graph-based method", "author": ["X. Han", "L. Sun", "J. Zhao"], "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pp. 765\u2013774, ACM, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "An entity-topic model for entity linking", "author": ["X. Han", "L. Sun"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 105\u2013115, Association for Computational Linguistics, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Entity linking at web scale", "author": ["T. Lin", "O. Etzioni"], "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pp. 84\u201388, Association for Computational Linguistics, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining wiki resources for multilingual named entity recognition", "author": ["A.E. Richman", "P. Schone"], "venue": "ACL, pp. 1\u20139, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Understanding user\u2019s query intent with wikipedia", "author": ["J. Hu", "G. Wang", "F. Lochovsky", "J.-t. Sun", "Z. Chen"], "venue": "Proceedings of the 18th international conference on World wide web, pp. 471\u2013480, ACM, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Transforming wikipedia into named entity training data", "author": ["J. Nothman", "J.R. Curran", "T. Murphy"], "venue": "Proceedings of the Australian Language Technology Workshop, pp. 124\u2013132, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Identifying and extracting named entities from wikipedia database using entity infoboxes", "author": ["M. Mohamed", "M. Oussalah"], "venue": "International Journal of Advanced Computer Science and Applications (IJACSA), vol. 5, no. 7, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Entity extraction, linking, classification, and tagging for social media: a wikipedia-based approach", "author": ["A. Gattani", "D.S. Lamba", "N. Garera", "M. Tiwari", "X. Chai", "S. Das", "S. Subramaniam", "A. Rajaraman", "V. Harinarayan", "A. Doan"], "venue": "Proceedings of the VLDB Endowment, vol. 6, no. 11, pp. 1126\u20131137, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Search query categorization at scale", "author": ["M. Laclav\u00edk", "M. Ciglan", "S. Steingold", "M. Seleng", "A. Dorman", "S. Dlugolinsky"], "venue": "Proceedings of the 24th International Conference on World Wide Web Companion, pp. 1281\u20131286, International World Wide Web Conferences Steering Committee, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Crowdsourced query augmentation through semantic discovery of domain-specific jargon", "author": ["K. AlJadda", "M. Korayem", "T. Grainger", "C. Russell"], "venue": "Big Data (Big Data), 2014 IEEE International Conference on, pp. 808\u2013815, IEEE, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "A sub-task related to ER is the Entity Type Recognition (ETR) which refers to categorizing these entities into a predefined set of types [1].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "The focus of the majority of ETR research has been on Named Entity Recognition (NER), which typically limits entity types to Person, Location, and Organization [2]\u2013[5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 4, "context": "The focus of the majority of ETR research has been on Named Entity Recognition (NER), which typically limits entity types to Person, Location, and Organization [2]\u2013[5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 5, "context": "ETR in search queries is considered extremely important; a Microsoft\u2019s study reported that 71% of queries submitted to their Bing search engine contain named entities somewhere, while 20 \u2212 30% are purely named entities [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 6, "context": "We integrated this model within CareerBuilder\u2019s semantic search engine [7]\u2013[9], which improved the quality of search results for tens of millions of job seekers every month.", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "We integrated this model within CareerBuilder\u2019s semantic search engine [7]\u2013[9], which improved the quality of search results for tens of millions of job seekers every month.", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "Both ETR and NER have experienced a surge in the research community in recent years [10]\u2013[16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "Both ETR and NER have experienced a surge in the research community in recent years [10]\u2013[16].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "[2] and Mansouri et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] presented comprehensive reviews about different approaches for NER including several representations that leverage dictionaries, corpora, and various classification methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] presented a formulation for both NER and ETR in search queries using a probabilistic approach and Latent Dirichlet Allocation (LDA).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Other approaches which utilize knowledge bases to link named entities in text with corresponding entities in the knowledge bases were presented in [1], [19]\u2013[22].", "startOffset": 147, "endOffset": 150}, {"referenceID": 18, "context": "Other approaches which utilize knowledge bases to link named entities in text with corresponding entities in the knowledge bases were presented in [1], [19]\u2013[22].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "Other approaches which utilize knowledge bases to link named entities in text with corresponding entities in the knowledge bases were presented in [1], [19]\u2013[22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "[1] proposed a methodology which relies on having a Wikipedia page whose title is similar to the given entity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Richman and Schone proposed a novel system for multilingual NER [23] .", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "Using Wikipedia concepts as a representation space for query\u2019s intent was introduced in [24].", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "The system introduced in [25] transforms links to Wikipedia articles into named entity annotations by classifying the target articles into the classic named entity types Person, Location, and Organization.", "startOffset": 25, "endOffset": 29}, {"referenceID": 25, "context": "Utilizing Wikipedia infobox for ETR was presented in [26].", "startOffset": 53, "endOffset": 57}, {"referenceID": 26, "context": "The system introduced in [27] converted Wikipedia into a structured knowledge base (KB).", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "We consider [28] as the most related work to ours.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "Our methodology for recognizing known entities and performing Entity Extraction from queries was previously described in [29].", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "The second clue models domain-specific knowledge by building synonym vectors of search entities using the word2vec model [30].", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "Finally, we evaluate all the ETR models using a Support Vector Machine (SVM) classifier with a linear kernel, leveraging the scikit-learn machine learning library [31].", "startOffset": 163, "endOffset": 167}], "year": 2016, "abstractText": "We present an ensemble approach for categorizing search query entities in the recruitment domain. Understanding the types of entities expressed in a search query (Company, Skill, Job Title, etc.) enables more intelligent information retrieval based upon those entities compared to a traditional keyword-based search. Because search queries are typically very short, leveraging a traditional bag-of-words model to identify entity types would be inappropriate due to the lack of contextual information. Our approach instead combines clues from different sources of varying complexity in order to collect real-world knowledge about query entities. We employ distributional semantic representations of query entities through two models: 1) contextual vectors generated from encyclopedic corpora like Wikipedia, and 2) high dimensional word embedding vectors generated from millions of job postings using word2vec. Additionally, our approach utilizes both entity linguistic properties obtained from WordNet and ontological properties extracted from DBpedia. We evaluate our approach on a data set created at CareerBuilder; the largest job board in the US. The data set contains entities extracted from millions of job seekers/recruiters search queries, job postings, and resume documents. After constructing the distributional vectors of search entities, we use supervised machine learning to infer search entity types. Empirical results show that our approach outperforms the state-of-the-art word2vec distributional semantics model trained on Wikipedia. Moreover, we achieve microaveraged F1 score of 97% using the proposed distributional representations ensemble.", "creator": "LaTeX with hyperref package"}}}