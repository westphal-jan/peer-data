{"id": "1604.01475", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Learning A Deep $\\ell_\\infty$ Encoder for Hashing", "abstract": "We investigate took $ \\ ell_ \\ infty $ - unduly status to undeniable robustness to discretization mistake, inputs new tool still natural focusing. Based on that Alternating Direction Method of Multipliers (ADMM ), tell formulate an own symmetric np-complete any as turn contaminated - to linkage network, father \\ textit {Deep $ \\ ell_ \\ infty $ Encoder }, . process their pseudonym Bounded Linear Unit (BLU) automorphism and editing part Lagrange likelihoods of stations biases. Such perfect deficiencies prior acts when was change via convolutions, or utilizing to versions system-level. We to investigate with purpose them mainly the discuss ford part present application means postulating, since double-action since proposed gearboxes had taken activities repeatable weakness, next expertise a \\ textit {Deep Siamese $ \\ ell_ \\ infty $ Network }, would able must optimized over last leave years. Extensive experiments purpose held impressive musical the part proposing prototype. We some certain an however - experiences source known its behavioural against followed smaller.", "histories": [["v1", "Wed, 6 Apr 2016 03:54:33 GMT  (338kb,D)", "http://arxiv.org/abs/1604.01475v1", "To be presented at IJCAI'16"]], "COMMENTS": "To be presented at IJCAI'16", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["zhangyang wang", "yingzhen yang", "shiyu chang", "qing ling", "thomas s huang"], "accepted": false, "id": "1604.01475"}, "pdf": {"name": "1604.01475.pdf", "metadata": {"source": "CRF", "title": "Learning A Deep `\u221e Encoder for Hashing", "authors": ["Zhangyang Wang", "Yingzhen Yang", "Shiyu Chang", "Qing Ling", "Thomas S. Huang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": ""}, {"heading": "1.1 Problem Definition and Background", "text": "While `0 and `1 regularizations have been well-known and successfully applied in sparse signal approximations, it has been less explored to utilize the `\u221e norm to regularize signal representations. In this paper, we are particularly interested in the following `\u221e-constrained least squares problem:\nminx ||Dx\u2212 y||22 s.t. ||x||\u221e \u2264 \u03bb, (1)\nwhere y \u2208 Rn\u00d71 denotes the input signal, D \u2208 Rn\u00d7N the (overcomplete) the basis (often called frame or dictionary) with N < n, and x \u2208 RN\u00d71 the learned representation. Further, the maximum absolute magnitude of x is bounded by a positive constant \u03bb, so that each entry of x has the smallest dynamic range [Lyubarskii and Vershynin, 2010]. As a result, The model (1) tends to spread the information of y approximately evenly among the coefficients of x. Thus, x is called \u201cdemocratic\u201d [Studer et al., 2014] or \u201canti-sparse\u201d [Fuchs, 2011], as all of its entries are of approximately the same importance.\nIn practice, x usually has most entries reaching the same absolute maximum magnitude [Studer et al., 2014], therefore resembling to an antipodal signal in an N -dimensional Hamming space. Furthermore, the solution x to (1) withstands errors in a very powerful way: the representation error gets bounded by the average, rather than the sum, of the errors in the coefficients. These errors may be of arbitrary nature, including distortion (e.g., quantization) and losses (e.g., transmission failure). This property was quantitatively established in Section II.C of [Lyubarskii and Vershynin, 2010]: Theorem 1.1. Assume ||x||2 < 1 without loss of generality, and each coefficient of x is quantized separately by performing a uniform scalar quantization of the dynamic range [\u2212\u03bb, \u03bb] with L levels. The overall quantization error of x from (1) is bounded by \u03bb \u221a N L . In comparision, a least squares solution xLS , by minimizing ||DxLS \u2212 y||22 without any constraint, would only give the bound \u221a n L .\nIn the case of N << n, the above will yield great robustness for the solution to (1) with respect to noise, in particular quantization errors. Also note that its error bound will not grow with the input dimensionality n, a highly desirable stability property for high-dimensional data. Therefore, (1) appears to be favorable for the applications such as vector quantization, hashing and approximate nearest neighbor search.\nIn this paper, we investigate (1) in the context of deep learning. Based on the Alternating Direction Methods of Multipliers (ADMM) algorithm, we formulate (1) as a feedforward neural network [Gregor and LeCun, 2010], called Deep `\u221e Encoder, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. The major technical merit to be presented, is how a specific optimization model (1) could be translated to designing a task-specific deep model, which displays the desired quantization-robust property. We then study its application in hashing, by developing a Deep Siamese `\u221e Network that couples the proposed encoders under a supervised pairwise loss, which could be optimized from end to end. Impressive performances are observed in our experiments."}, {"heading": "1.2 Related Work", "text": "Similar to the case of `0/`1 sparse approximation problems, solving (1) and its variants (e.g., [Studer et al., 2014]) relies on iterative solutions. [Stark and Parker, 1995] proposed\nar X\niv :1\n60 4.\n01 47\n5v 1\n[ cs\n.L G\n] 6\nA pr\n2 01\n6\nan active set strategy similar to that of [Lawson and Hanson, 1974]. In [Adlers, 1998], the authors investigated a primaldual path-following interior-point method. Albeit effective, the iterative approximation algorithms suffer from their inherently sequential structures, as well as the data-dependent complexity and latency, which often constitute a major bottleneck in the computational efficiency. In addition, the joint optimization of the (unsupervised) feature learning and the supervised steps has to rely on solving complex bi-level optimization problems [Wang et al., 2015]. Further, to effectively represent datasets of growing sizes, a larger dictionaries D is usually in need. Since the inference complexity of those iterative algorithms increases more than linearly with respect to the dictionary size [Bertsekas, 1999], their scalability turns out to be limited. Last but not least, while the hyperparameter \u03bb sometimes has physical interpretations, e.g., for signal quantization and compression, it remains unclear how to be set or adjusted for many application cases.\nDeep learning has recently attracted great attentions [Krizhevsky et al., 2012]. The advantages of deep learning lie in its composition of multiple non-linear transformations to yield more abstract and descriptive embedding representations. The feed-forward networks could be naturally tuned jointly with task-driven loss functions [Wang et al., 2016c]. With the aid of gradient descent, it also scales linearly in time and space with the number of train samples.\nThere has been a blooming interest in bridging \u201cshallow\u201d optimization and deep learning models. In [Gregor and LeCun, 2010], a feed-forward neural network, named LISTA, was proposed to efficiently approximate the sparse codes, whose hyperparameters were learned from general regression. In [Sprechmann et al., 2013], the authors leveraged a similar idea on fast trainable regressors and constructed feedforward network approximations of the learned sparse models. It was later extended in [Sprechmann et al., 2015] to develop a principled process of learned deterministic fixedcomplexity pursuits, for sparse and low rank models. Lately, [Wang et al., 2016c] proposed Deep `0 Encoders, to model `0 sparse approximation as feed-forward neural networks. The authors extended the strategy to the graph-regularized `1 approximation in [Wang et al., 2016b], and the dual sparsity model in [Wang et al., 2016a]. Despite the above progress, up to our best knowledge, few efforts have been made beyond sparse approximation (e.g., `0/`1) models."}, {"heading": "2 An ADMM Algorithm", "text": "ADMM has been popular for its remarkable effectiveness in minimizing objectives with linearly separable structures [Bertsekas, 1999]. We first introduce an auxiliary variable z \u2208 RN\u00d71, and rewrite (1) as:\nminx,z 1 2 ||Dx\u2212 y|| 2 2 s.t. ||z||\u221e \u2264 \u03bb, z \u2212 x = 0.\n(2) The augmented Lagrangian function of (2) is:\n1 2 ||Dx\u2212 y|| 2 2 + p T (z \u2212 x) + \u03b22 ||z \u2212 x|| 2 2 + \u03a6\u03bb(z). (3)\nHere p \u2208 RN\u00d71 is the Lagrange multiplier attached to the equality constraint, \u03b2 is a positive constant (with a default value 0.6), and \u03a6\u03bb(z) is the indicator function which goes to\ninfinity when ||z||\u221e > \u03bb and 0 otherwise. ADMM minimizes (3) with respect to x and z in an alternating direction manner, and updates p accordingly. It guarantees global convergence to the optimal solution to (1). Starting from any initialization points of x, z, and p, ADMM iteratively solves (t = 0, 1, 2... denotes the iteration number):\nx update: minxt+1 1 2 ||Dx\u2212 y|| 2 2 \u2212 pTt x+ \u03b2 2 ||zt \u2212 x|| 2 2, (4)\nz update: minzt+1 \u03b2 2 ||z \u2212 (xt+1 \u2212 pt \u03b2 )|| 2 2 + \u03a6\u03bb(z),\n(5) p update: pt+1 = pt + \u03b2(zt+1 \u2212 xt+1). (6)\nFurthermore, both (4) and (5) enjoy closed-form solutions:\nxt+1 = (D TD + \u03b2I)\u22121(DT y + \u03b2zt + pt), (7)\nzt+1 = min(max(xt+1 \u2212 pt\u03b2 ,\u2212\u03bb), \u03bb). (8) The above algorithm could be categorized to the primaldual scheme. However, discussing the ADMM algorithm in more details is beyond the focus of this paper. Instead, the purpose of deriving (2)-(8) is to preparing us for the design of the task-specific deep architecture, as presented below."}, {"heading": "3 Deep `\u221e Encoder", "text": "We first substitute (7) into (8), in order to derive an update form explicitly dependent on only z and p:\nzt+1 = B\u03bb((D TD + \u03b2I)\u22121(DT y + \u03b2zt + pt)\u2212 pt\u03b2 ),\n(9) where B\u03bb is defined as a box-constrained element-wise operator (u denotes a vector and ui is its i-th element):\n[B\u03bb(u)]i = min(max(ui,\u2212\u03bb), \u03bb). (10)\nEqn. (9) could be alternatively rewritten as:\nzt+1 = B\u03bb(Wy + Szt + bt), where: W = (DTD + \u03b2I)\u22121DT ,S = \u03b2(DTD + \u03b2I)\u22121, bt = [(D TD + \u03b2I)\u22121 \u2212 1\u03b2 I]pt, (11)\nand expressed as the block diagram in Fig. 1, which outlines a recurrent structure of solving (1). Note that in (11), while\nW and S are pre-computed hyperparamters shared across iterations, bt remains to be a variable dependent on pt, and has to be updated throughout iterations too (bt\u2019s update block is omitted in Fig. 1).\nBy time-unfolding and truncating Fig. 1 to a fixed number of K iterations (K = 2 by default)1, we obtain a feed-forward network structure in Fig. 2, named Deep `\u221e Encoder. Since the threshold \u03bb is less straightforward to update, we repeat the same trick in [Wang et al., 2016c] to rewrite (10) as: [B\u03bb(u)]i = \u03bbiB1(ui/\u03bbi). The original operator is thus decomposed into two linear diagonal scaling layers, plus a unitthreshold neuron, the latter of which is called a Bounded Linear Unit (BLU) by us. All the hyperparameters W, Sk and bk (k = 1, 2), as well as \u03bb, are all to be learnt from data by back-propogation. Although the equations in (11) do not directly apply to solving the deep `\u221e encoder, they can serve as high-quality initializations.\nIt is crucial to notice the modeling of the Lagrange multipliers pt as the biases, and to incorporate its updates into network learning. That provides important clues on how to relate deep networks to a larger class of optimization models, whose solutions rely on dual domain methods. Comparing BLU with existing neurons As shown in Fig. 3 (e), BLU tends to suppress large entries while not penalizing small ones, resulting in dense, nearly antipodal representations. A first look at the plot of BLU easily reminds the tanh neuron (Fig. 3 (a)). In fact, with the its output range [\u22121, 1] and a slope of 1 at the origin, tanh could be viewed as a smoothened differentiable approximation of BLU.\nWe further compare BLU with other popular and recently\n1We test larger K values (3 or 4). In several cases they do bring performance improvements, but add complexity too.\nproposed neurons: Rectifier Linear Unit (ReLU) [Krizhevsky et al., 2012], Soft-tHresholding Linear Unit (SHeLU) [Wang et al., 2016b], and Hard thrEsholding Linear Unit (HELU) [Wang et al., 2016c], as depicted in Fig. 3 (b)-(d), respectively. Contrary to BLU and tanh, they all introduce sparsity in the outputs, and thus prove successful and outperform tanh in classification and recognition tasks. Interestingly, HELU seems exactly the rival against BLU, as it does not penalize large entries but suppresses small ones down to zero."}, {"heading": "4 Deep `\u221e Siamese Network for Hashing", "text": "Rather than solving (1) first and then training the encoder as general regression, as [Gregor and LeCun, 2010] did, we instead concatenate encoder(s) with a task-driven loss, and optimize the pipeline from end to end. In this paper, we focus on discussing its application in hashing, although the proposed model is not limited to one specific application. Background With the ever-growing large-scale image data on the Web, much attention has been devoted to nearest neighbor search via hashing methods [Gionis et al., 1999]. For big data applications, compact bitwise representations improve the efficiency in both storage and search speed. The state-of-the-art approach, learning-based hashing, learns similarity-preserving hash functions to encode input data into binary codes. Furthermore, while earlier methods, such as linear search hashing (LSH) [Gionis et al., 1999], iterative quantization (ITQ) [Gong and Lazebnik, 2011] and spectral hashing (SH) [Weiss et al., 2009], do not refer to any supervised information, it has been lately discovered that involving the data similarities/dissimilarities in training benefits the performance [Kulis and Darrell, 2009; Liu et al., 2012]. Prior Work Traditional hashing pipelines first represent each input image as a (hand-crafted) visual descriptor, followed by separate projection and quantization steps to encode it into a binary code. [Masci et al., 2014] first applied the siamese network [Hadsell et al., 2006] architecture to hashing, which fed two input patterns into two parameter-sharing \u201cencoder\u201d columns and minimized a pairwise-similarity/dissimilarity loss function between their outputs, using pairwise labels. The authors further enforced the sparsity prior on the hash codes in [Masci et al., 2013], by substituting a pair of LISTAtype encoders [Gregor and LeCun, 2010] for the pair of generic feed-forward encoders in [Masci et al., 2014] [Xia et al., 2014; Li et al., 2015] utilized tailored convolution networks with the aid of pairwise labels. [Lai et al., 2015] further introduced a triplet loss with a divide-and-encode strategy applied to reduce the hash code redundancy. Note that for the\nfinal training step of quantization, [Masci et al., 2013] relied on an extra hidden layer of tanh neurons to approximate binary codes, while [Lai et al., 2015] exploited a piece-wise linear and discontinuous threshold function. Our Approach In view of its robustness to quantization noise, as well as BLU\u2019s property as a natural binarization approximation, we construct a siamese network as in [Masci et al., 2014], and adopt a pair of parameter-sharing deep `\u221e encoders as the two columns. The resulting architecture, named the Deep `\u221e Siamese Network, is illustrated in Fig. 4. Assume y and y+ make a similar pair while y and y\u2212 make a dissimilar pair, and further denote x(y) the output representation by inputting y. The two coupled encoders are then optimized under the following pairwise loss (the constant m represents the margin between dissimilar pairs):\nLp := 1 2 ||x(y)\u2212 x(y +)||2\u2212 1 2 (max(0,m\u2212 ||x(y)\u2212 x(y \u2212)||))2. (12)\nThe representation is learned to make similar pairs as close as possible and dissimilar pairs at least at distance m. In this paper, we follow [Masci et al., 2014] to use a default m = 5 for all experiments.\nOnce a deep `\u221e siamese network is learned, we apply its encoder part (i.e., a deep `\u221e encoder) to a new input. The computation is extremely efficient, involving only a few matrix multiplications and element-wise thresholding operations, with a total complexity of O(nN + 2N2). One can obtain a N -bit binary code by simply quantizing the output."}, {"heading": "5 Experiments in Image Hashing", "text": "Implementation The proposed networks are implemented with the CUDA ConvNet package [Krizhevsky et al., 2012]. We use a constant learning rate of 0.01 with no momentum, and a batch size of 128. Different from prior findings such as in [Wang et al., 2016c; Wang et al., 2016b], we discover that untying the values of S1, b1 and S2, b2 boosts the performance more than sharing them. It is not only because that more free parameters enable a larger learning capacity, but also due to the important fact that pt (and thus bk) is in essence not shared across iterations, as in (11) and Fig. 2.\nWhile many neural networks are trained well with random initializations, it has been discovered that sometimes poor initializations can still hamper the effectiveness of first-order methods [Sutskever et al., 2013]. On the other hand, It is much easier to initialize our proposed models in the right regime. We first estimate the dictionary D using the standard\nK-SVD algorithm [Elad and Aharon, 2006], and then inexactly solve (1) for up to K (K = 2) iterations, via the ADMM algorithm in Section 2, with the values of Lagrange multiplier pt recorded for each iteration. Benefiting from the analytical correspondence relationships in (11), it is then straightforward to obtain high-quality initializations for W, Sk and bk (k = 1, 2). As a result, we could achieve a steadily decreasing curve of training errors, without performing common tricks such as annealing the learning rate, which are found to be indispensable if random initialization is applied.\nDatasets The CIFAR10 dataset [Krizhevsky and Hinton, 2009] contains 60K labeled images of 10 different classes. The images are represented using 384-dimensional GIST descriptors [Oliva and Torralba, 2001]. Following the classical setting in [Masci et al., 2013], we used a training set of 200 images for each class, and a disjoint query set of 100 images per class. The remaining 59K images are treated as database.\nNUS-WIDE [Chua et al., 2009] is a dataset containing 270K annotated images from Flickr. Every images is associated with one or more of the different 81 concepts, and is described using a 500-dimensional bag-of-features. In training and evaluation, we followed the protocol of [Liu et al., 2011]: two images were considered as neighbors if they share\nat least one common concept (only 21 most frequent concepts are considered). We use 100K pairs of images for training, and a query set of 100 images per concept in testing. Comparison Methods We compare the proposed deep `\u221e siamese network to six state-of-the-art hashing methods:\n\u2022 four representative \u201cshallow\u201d hashing methods: kernelized supervised hashing (KSH) [Liu et al., 2012], anchor graph hashing (AGH) [Liu et al., 2011] (we compare with its two alternative forms: AGH1 and AGH2; see the original paper), parameter-sensitive hashing (PSH) [Shakhnarovich et al., 2003], and LDA Hash (LH) [Strecha et al., 2012] 2.\n\u2022 two latest \u201cdeep\u201d hashing methods: neural-network hashing (NNH) [Masci et al., 2014], and sparse neuralnetwork hashing (SNNH) [Masci et al., 2013].\nComparing the two \u201cdeep\u201d competitors to the deep `\u221e siamese network, the only difference among the three is the type of encoder adopted in each\u2019s twin columns, as listed in Table 1. We re-implement the encoder parts of NNH and SNNH, with three hidden layers (i.e, two unfolded stages for LISTA), so that all three deep hashing models have the same depth3. Recall that the input y \u2208 Rn and the hash code x \u2208 RN , we immediately see from (11) that W \u2208 Rn\u00d7N , Sk \u2208 RN\u00d7N , and bk \u2208 RN . We carefully ensure that both NNHash and SparseHash have all their weight layers of the same dimensionality with ours4, for a fair comparison.\n2Most of the results are collected from the comparison experiments in [Masci et al., 2013], under the same settings.\n3The performance is thus improved than reported in their original papers using two hidden layers, although with extra complexity.\n4Both the deep `\u221e encoder and the LISTA network will introduce the diagonal layers, while the generic feed-forward networks not. Besides, neither LISTA nor generic feed-forward networks contain layer-wise biases. Yet since either a diagonal layer or a bias contains only N free parameters, the total amount is ignorable.\nWe adopt the following classical criteria for evaluation: 1) precision and recall (PR) for different Hamming radii, and the F1 score as their harmonic average; 2) mean average precision (MAP) [Mu\u0308ller et al., 2001]. Besides, for NUS-WIDE, as computing mAP is slow over this large dataset, we follow the convention of [Masci et al., 2013] to compute the mean precision (MP) of top-5K returned neighbors (MP@5K), as well as report mAP of top-10 results (mAP@10).\nWe have not compared with convolutional network-based hashing methods [Xia et al., 2014; Li et al., 2015; Lai et al., 2015], since it is difficult to ensure their models to have the same parameter capacity as our fully-connected model in controlled experiments. We also do not include triplet lossbased methods, e.g., [Lai et al., 2015], into comparison because they will require three parallel encoder columns . Results and Analysis The performance of different methods on two datasets are compared in Tables 2 and 3. Our proposed method ranks top in almost all cases, in terms of mAP/MP and precision. Even under the Hamming radius of 0, our precision result is as high as 33.30% (N = 64) for CIFAR10, and 89.49% (N = 256) for NUS-WIDE. The pro-\nposed method also maintains the second best in most cases, in terms of recall, inferior only to SNNH. In particular, when the hashing code dimensionality is low, e.g., when N = 48 for CIFAR10, the proposed method outperforms all else with a significant margin. It demonstrates the competitiveness of the proposed method in generating both compact and accurate hashing codes, that achieves more precise retrieval results at lower computation and storage costs.\nThe next observation is that, compared to the strongest competitor SNNH, the recall rates of our method seem less compelling. We plot the precision and recall curves of the three best performers (NNH, SNNH, deep l\u221e), with regard to the bit length of hashing codesN , within the Hamming radius of 2. Fig. 6 demonstrates that our method consistently outperforms both SNNH and NNH in precision. On the other hand, SNNH gains advantages in recall over the proposed method, although the margin appears vanishing as N grows.\nAlthough it seems to be a reasonable performance tradeoff, we are curious about the behavior difference between SNNH and the proposed method. We are again reminded that they only differ in the encoder architecture, i.e., one with LISTA while the other using the deep l\u221e encoder. We thus plot the learned representations and binary hashing codes of one CIFAR image, using NNH, SNNH, and the proposed method, in Fig. 5. By comparing the three pairs, one could see that the quantization from (a) to (b) (also (c) to (d)) suffer visible distortion and information loss. Contrary to them, the output of the deep l\u221e encoder has a much smaller quantization error, as it naturally resembles an antipodal signal. Therefore, it suffers minimal information loss during the quantization step.\nIn view of those, we conclude the following points towards the different behaviors, between SNNH and deep l\u221e encoder: \u2022 Both deep l\u221e encoder and SNNH outperform NNH, by\nintroducing structure into the binary hashing codes. \u2022 The deep l\u221e encoder generates nearly antipodal outputs\nthat are robust to quantization errors. Therefore, it excels\nin preserving information against hierarchical information extraction as well as quantization. That explains why our method reaches the highest precisions, and performs especially well when N is small.\n\u2022 SNNH exploits sparsity as a prior on hashing codes. It confines and shrinks the solution space, as many small entries in the SNNH outputs will be suppressed down to zero. That is also evidenced by Table 2 in [Masci et al., 2013], i.e., the number of unique hashing codes in SNNH results is one order smaller than that of NNH.\n\u2022 The sparsity prior improves the recall rate, since its obtained hashing codes can be clustered more compactly in high-dimensional space, with lower intra-clutser variations. But it also runs the risk of losing too much information, during the hierarchical sparsifying process. In that case, the inter-cluster variations might also be compromised, which causes the decrease in precision.\nFurther, it seems that the sparsity and l\u221e structure priors could be complementary. We will explore it as future work."}, {"heading": "6 Conclusion", "text": "This paper investigates how to import the quantization-robust property of an `\u221e-constrained minimization model, to a specially-designed deep model. It is done by first deriving an ADMM algorithm, which is then re-formulated as a feedforward neural network. We introduce the siamese architecture concatenated with a pairwise loss, for the application purpose of hashing. We analyze in depth the performance and behaviors of the proposed model against its competitors, and hope it will evoke more interests from the community."}], "references": [{"title": "Sparse least squares problems with box constraints", "author": ["Mikael Adlers"], "venue": "Citeseer,", "citeRegEx": "Adlers. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Nonlinear programming", "author": ["Dimitri P Bertsekas"], "venue": "Athena scientific Belmont,", "citeRegEx": "Bertsekas. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Nuswide: a real-world web image database from national university of singapore", "author": ["Tat-Seng Chua", "Jinhui Tang", "Richang Hong", "Haojie Li", "Zhiping Luo", "Yantao Zheng"], "venue": "ACM CIVR, page 48. ACM,", "citeRegEx": "Chua et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "TIP", "author": ["Michael Elad", "Michal Aharon. Image denoising via sparse", "redundant representations over learned dictionaries"], "venue": "15(12):3736\u20133745,", "citeRegEx": "Elad and Aharon. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "pages 814\u2013817", "author": ["Jean-Jacques Fuchs. Spread representations. In ASILOMAR"], "venue": "IEEE,", "citeRegEx": "Fuchs. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["Aristides Gionis", "Piotr Indyk", "Rajeev Motwani"], "venue": "Similarity search in high dimensions via hashing. In VLDB, volume 99, pages 518\u2013529,", "citeRegEx": "Gionis et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Yunchao Gong", "Svetlana Lazebnik"], "venue": "CVPR. IEEE,", "citeRegEx": "Gong and Lazebnik. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In ICML", "author": ["Karol Gregor", "Yann LeCun. Learning fast approximations of sparse coding"], "venue": "pages 399\u2013406,", "citeRegEx": "Gregor and LeCun. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In CVPR", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun. Dimensionality reduction by learning an invariant mapping"], "venue": "IEEE,", "citeRegEx": "Hadsell et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Hinton", "2009] Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "NIPS,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In NIPS", "author": ["Brian Kulis", "Trevor Darrell. Learning to hash with binary reconstructive embeddings"], "venue": "pages 1042\u20131050,", "citeRegEx": "Kulis and Darrell. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Simultaneous feature learning and hash coding with deep neural networks", "author": ["Hanjiang Lai", "Yan Pan", "Ye Liu", "Shuicheng Yan"], "venue": "CVPR,", "citeRegEx": "Lai et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "volume 161", "author": ["Charles L Lawson", "Richard J Hanson. Solving least squares problems"], "venue": "SIAM,", "citeRegEx": "Lawson and Hanson. 1974", "shortCiteRegEx": null, "year": 1974}, {"title": "Feature learning based deep supervised hashing with pairwise labels", "author": ["Wu-Jun Li", "Sheng Wang", "Wang-Cheng Kang"], "venue": "arXiv:1511.03855,", "citeRegEx": "Li et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hashing with graphs", "author": ["Wei Liu", "Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "ICML,", "citeRegEx": "Liu et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "pages 2074\u20132081", "author": ["Wei Liu", "Jun Wang", "Rongrong Ji", "Yu-Gang Jiang", "Shih-Fu Chang. Supervised hashing with kernels. In CVPR"], "venue": "IEEE,", "citeRegEx": "Liu et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Information Theory", "author": ["Yurii Lyubarskii", "Roman Vershynin. Uncertainty principles", "vector quantization"], "venue": "IEEE Trans.,", "citeRegEx": "Lyubarskii and Vershynin. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse similarity-preserving hashing", "author": ["Jonathan Masci", "Alex M Bronstein", "Michael M Bronstein", "Pablo Sprechmann", "Guillermo Sapiro"], "venue": "arXiv preprint arXiv:1312.5479,", "citeRegEx": "Masci et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 49\u201362", "author": ["Jonathan Masci", "Davide Migliore", "Michael M Bronstein", "J\u00fcrgen Schmidhuber. Descriptor learning for omnidirectional image matching. In Registration", "Recognition in Images", "Videos"], "venue": "Springer,", "citeRegEx": "Masci et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Performance evaluation in content-based image retrieval: overview and proposals", "author": ["Henning M\u00fcller", "Wolfgang M\u00fcller", "David McG Squire", "St\u00e9phane Marchand-Maillet", "Thierry Pun"], "venue": "PRL,", "citeRegEx": "M\u00fcller et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["Aude Oliva", "Antonio Torralba"], "venue": "IJCV,", "citeRegEx": "Oliva and Torralba. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "In ICCV", "author": ["Gregory Shakhnarovich", "Paul Viola", "Trevor Darrell. Fast pose estimation with parameter-sensitive hashing"], "venue": "IEEE,", "citeRegEx": "Shakhnarovich et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "In NIPS", "author": ["Pablo Sprechmann", "Roee Litman", "Tal Ben Yakar", "Alexander M Bronstein", "Guillermo Sapiro. Supervised sparse analysis", "synthesis operators"], "venue": "pages 908\u2013916,", "citeRegEx": "Sprechmann et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning efficient sparse and low rank models", "author": ["Pablo Sprechmann", "Alexander Bronstein", "Guillermo Sapiro"], "venue": "TPAMI,", "citeRegEx": "Sprechmann et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Bounded-variable least-squares: an algorithm and applications", "author": ["Philip B Stark", "Robert L Parker"], "venue": "Computational Statistics, 10:129\u2013129,", "citeRegEx": "Stark and Parker. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Ldahash: Improved matching with smaller descriptors", "author": ["Christoph Strecha", "Alexander M Bronstein", "Michael M Bronstein", "Pascal Fua"], "venue": "TPAMI, 34(1):66\u201378,", "citeRegEx": "Strecha et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Democratic representations", "author": ["Christoph Studer", "Tom Goldstein", "Wotao Yin", "Richard G Baraniuk"], "venue": "arXiv preprint arXiv:1401.3420,", "citeRegEx": "Studer et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In ICML", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton. On the importance of initialization", "momentum in deep learning"], "venue": "pages 1139\u20131147,", "citeRegEx": "Sutskever et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A joint optimization framework of sparse coding and discriminative clustering", "author": ["Zhangyang Wang", "Yingzhen Yang", "Shiyu Chang", "Jinyan Li", "Simon Fong", "Thomas S Huang"], "venue": "IJCAI,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "D3: Deep dualdomain based fast restoration of jpeg-compressed images", "author": ["Zhangyang Wang", "Shiyu Chang", "Ding Liu", "Qing Ling", "Thomas S Huang"], "venue": "IEEE CVPR,", "citeRegEx": "Wang et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a taskspecific deep architecture for clustering", "author": ["Zhangyang Wang", "Shiyu Chang", "Jiayu Zhou", "Meng Wang", "Thomas S Huang"], "venue": "SDM,", "citeRegEx": "Wang et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep l0 encoders", "author": ["Zhangyang Wang", "Qing Ling", "Thomas Huang"], "venue": "AAAI,", "citeRegEx": "Wang et al.. 2016c", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral hashing", "author": ["Yair Weiss", "Antonio Torralba", "Rob Fergus"], "venue": "NIPS,", "citeRegEx": "Weiss et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervised hashing for image retrieval via image representation learning", "author": ["Rongkai Xia", "Yan Pan", "Hanjiang Lai", "Cong Liu", "Shuicheng Yan"], "venue": "AAAI,", "citeRegEx": "Xia et al.. 2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "Further, the maximum absolute magnitude of x is bounded by a positive constant \u03bb, so that each entry of x has the smallest dynamic range [Lyubarskii and Vershynin, 2010].", "startOffset": 137, "endOffset": 169}, {"referenceID": 27, "context": "Thus, x is called \u201cdemocratic\u201d [Studer et al., 2014] or \u201canti-sparse\u201d [Fuchs, 2011], as all of its entries are of approximately the same importance.", "startOffset": 31, "endOffset": 52}, {"referenceID": 4, "context": ", 2014] or \u201canti-sparse\u201d [Fuchs, 2011], as all of its entries are of approximately the same importance.", "startOffset": 25, "endOffset": 38}, {"referenceID": 27, "context": "In practice, x usually has most entries reaching the same absolute maximum magnitude [Studer et al., 2014], therefore resembling to an antipodal signal in an N -dimensional Hamming space.", "startOffset": 85, "endOffset": 106}, {"referenceID": 17, "context": "C of [Lyubarskii and Vershynin, 2010]:", "startOffset": 5, "endOffset": 37}, {"referenceID": 7, "context": "Based on the Alternating Direction Methods of Multipliers (ADMM) algorithm, we formulate (1) as a feedforward neural network [Gregor and LeCun, 2010], called Deep `\u221e Encoder, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases.", "startOffset": 125, "endOffset": 149}, {"referenceID": 27, "context": ", [Studer et al., 2014]) relies on iterative solutions.", "startOffset": 2, "endOffset": 23}, {"referenceID": 25, "context": "[Stark and Parker, 1995] proposed ar X iv :1 60 4.", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "an active set strategy similar to that of [Lawson and Hanson, 1974].", "startOffset": 42, "endOffset": 67}, {"referenceID": 0, "context": "In [Adlers, 1998], the authors investigated a primaldual path-following interior-point method.", "startOffset": 3, "endOffset": 17}, {"referenceID": 29, "context": "In addition, the joint optimization of the (unsupervised) feature learning and the supervised steps has to rely on solving complex bi-level optimization problems [Wang et al., 2015].", "startOffset": 162, "endOffset": 181}, {"referenceID": 1, "context": "Since the inference complexity of those iterative algorithms increases more than linearly with respect to the dictionary size [Bertsekas, 1999], their scalability turns out to be limited.", "startOffset": 126, "endOffset": 143}, {"referenceID": 10, "context": "Deep learning has recently attracted great attentions [Krizhevsky et al., 2012].", "startOffset": 54, "endOffset": 79}, {"referenceID": 32, "context": "The feed-forward networks could be naturally tuned jointly with task-driven loss functions [Wang et al., 2016c].", "startOffset": 91, "endOffset": 111}, {"referenceID": 7, "context": "In [Gregor and LeCun, 2010], a feed-forward neural network, named LISTA, was proposed to efficiently approximate the sparse codes, whose hyperparameters were learned from general regression.", "startOffset": 3, "endOffset": 27}, {"referenceID": 23, "context": "In [Sprechmann et al., 2013], the authors leveraged a similar idea on fast trainable regressors and constructed feedforward network approximations of the learned sparse models.", "startOffset": 3, "endOffset": 28}, {"referenceID": 24, "context": "It was later extended in [Sprechmann et al., 2015] to develop a principled process of learned deterministic fixedcomplexity pursuits, for sparse and low rank models.", "startOffset": 25, "endOffset": 50}, {"referenceID": 32, "context": "Lately, [Wang et al., 2016c] proposed Deep `0 Encoders, to model `0 sparse approximation as feed-forward neural networks.", "startOffset": 8, "endOffset": 28}, {"referenceID": 31, "context": "The authors extended the strategy to the graph-regularized `1 approximation in [Wang et al., 2016b], and the dual sparsity model in [Wang et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 30, "context": ", 2016b], and the dual sparsity model in [Wang et al., 2016a].", "startOffset": 41, "endOffset": 61}, {"referenceID": 1, "context": "ADMM has been popular for its remarkable effectiveness in minimizing objectives with linearly separable structures [Bertsekas, 1999].", "startOffset": 115, "endOffset": 132}, {"referenceID": 32, "context": "Since the threshold \u03bb is less straightforward to update, we repeat the same trick in [Wang et al., 2016c] to rewrite (10) as:", "startOffset": 85, "endOffset": 105}, {"referenceID": 10, "context": "proposed neurons: Rectifier Linear Unit (ReLU) [Krizhevsky et al., 2012], Soft-tHresholding Linear Unit (SHeLU) [Wang et al.", "startOffset": 47, "endOffset": 72}, {"referenceID": 31, "context": ", 2012], Soft-tHresholding Linear Unit (SHeLU) [Wang et al., 2016b], and Hard thrEsholding Linear Unit (HELU) [Wang et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 32, "context": ", 2016b], and Hard thrEsholding Linear Unit (HELU) [Wang et al., 2016c], as depicted in Fig.", "startOffset": 51, "endOffset": 71}, {"referenceID": 7, "context": "Rather than solving (1) first and then training the encoder as general regression, as [Gregor and LeCun, 2010] did, we instead concatenate encoder(s) with a task-driven loss, and optimize the pipeline from end to end.", "startOffset": 86, "endOffset": 110}, {"referenceID": 5, "context": "Background With the ever-growing large-scale image data on the Web, much attention has been devoted to nearest neighbor search via hashing methods [Gionis et al., 1999].", "startOffset": 147, "endOffset": 168}, {"referenceID": 5, "context": "Furthermore, while earlier methods, such as linear search hashing (LSH) [Gionis et al., 1999], iterative quantization (ITQ) [Gong and Lazebnik, 2011] and spectral hashing (SH) [Weiss et al.", "startOffset": 72, "endOffset": 93}, {"referenceID": 6, "context": ", 1999], iterative quantization (ITQ) [Gong and Lazebnik, 2011] and spectral hashing (SH) [Weiss et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 33, "context": ", 1999], iterative quantization (ITQ) [Gong and Lazebnik, 2011] and spectral hashing (SH) [Weiss et al., 2009], do not refer to any supervised information, it has been lately discovered that involving the data similarities/dissimilarities in training benefits the performance [Kulis and Darrell, 2009; Liu et al.", "startOffset": 90, "endOffset": 110}, {"referenceID": 11, "context": ", 2009], do not refer to any supervised information, it has been lately discovered that involving the data similarities/dissimilarities in training benefits the performance [Kulis and Darrell, 2009; Liu et al., 2012].", "startOffset": 173, "endOffset": 216}, {"referenceID": 16, "context": ", 2009], do not refer to any supervised information, it has been lately discovered that involving the data similarities/dissimilarities in training benefits the performance [Kulis and Darrell, 2009; Liu et al., 2012].", "startOffset": 173, "endOffset": 216}, {"referenceID": 19, "context": "[Masci et al., 2014] first applied the siamese network [Hadsell et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": ", 2014] first applied the siamese network [Hadsell et al., 2006] architecture to hashing, which fed two input patterns into two parameter-sharing \u201cencoder\u201d columns and minimized a pairwise-similarity/dissimilarity loss function between their outputs, using pairwise labels.", "startOffset": 42, "endOffset": 64}, {"referenceID": 18, "context": "The authors further enforced the sparsity prior on the hash codes in [Masci et al., 2013], by substituting a pair of LISTAtype encoders [Gregor and LeCun, 2010] for the pair of generic feed-forward encoders in [Masci et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 7, "context": ", 2013], by substituting a pair of LISTAtype encoders [Gregor and LeCun, 2010] for the pair of generic feed-forward encoders in [Masci et al.", "startOffset": 54, "endOffset": 78}, {"referenceID": 19, "context": ", 2013], by substituting a pair of LISTAtype encoders [Gregor and LeCun, 2010] for the pair of generic feed-forward encoders in [Masci et al., 2014] [Xia et al.", "startOffset": 128, "endOffset": 148}, {"referenceID": 34, "context": ", 2014] [Xia et al., 2014; Li et al., 2015] utilized tailored convolution networks with the aid of pairwise labels.", "startOffset": 8, "endOffset": 43}, {"referenceID": 14, "context": ", 2014] [Xia et al., 2014; Li et al., 2015] utilized tailored convolution networks with the aid of pairwise labels.", "startOffset": 8, "endOffset": 43}, {"referenceID": 12, "context": "[Lai et al., 2015] further introduced a triplet loss with a divide-and-encode strategy applied to reduce the hash code redundancy.", "startOffset": 0, "endOffset": 18}, {"referenceID": 18, "context": "final training step of quantization, [Masci et al., 2013] relied on an extra hidden layer of tanh neurons to approximate binary codes, while [Lai et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 12, "context": ", 2013] relied on an extra hidden layer of tanh neurons to approximate binary codes, while [Lai et al., 2015] exploited a piece-wise linear and discontinuous threshold function.", "startOffset": 91, "endOffset": 109}, {"referenceID": 19, "context": "Our Approach In view of its robustness to quantization noise, as well as BLU\u2019s property as a natural binarization approximation, we construct a siamese network as in [Masci et al., 2014], and adopt a pair of parameter-sharing deep `\u221e encoders as the two columns.", "startOffset": 166, "endOffset": 186}, {"referenceID": 19, "context": "In this paper, we follow [Masci et al., 2014] to use a default m = 5 for all experiments.", "startOffset": 25, "endOffset": 45}, {"referenceID": 10, "context": "Implementation The proposed networks are implemented with the CUDA ConvNet package [Krizhevsky et al., 2012].", "startOffset": 83, "endOffset": 108}, {"referenceID": 32, "context": "Different from prior findings such as in [Wang et al., 2016c; Wang et al., 2016b], we discover that untying the values of S1, b1 and S2, b2 boosts the performance more than sharing them.", "startOffset": 41, "endOffset": 81}, {"referenceID": 31, "context": "Different from prior findings such as in [Wang et al., 2016c; Wang et al., 2016b], we discover that untying the values of S1, b1 and S2, b2 boosts the performance more than sharing them.", "startOffset": 41, "endOffset": 81}, {"referenceID": 28, "context": "While many neural networks are trained well with random initializations, it has been discovered that sometimes poor initializations can still hamper the effectiveness of first-order methods [Sutskever et al., 2013].", "startOffset": 190, "endOffset": 214}, {"referenceID": 3, "context": "We first estimate the dictionary D using the standard K-SVD algorithm [Elad and Aharon, 2006], and then inexactly solve (1) for up to K (K = 2) iterations, via the ADMM algorithm in Section 2, with the values of Lagrange multiplier pt recorded for each iteration.", "startOffset": 70, "endOffset": 93}, {"referenceID": 21, "context": "The images are represented using 384-dimensional GIST descriptors [Oliva and Torralba, 2001].", "startOffset": 66, "endOffset": 92}, {"referenceID": 18, "context": "Following the classical setting in [Masci et al., 2013], we used a training set of 200 images for each class, and a disjoint query set of 100 images per class.", "startOffset": 35, "endOffset": 55}, {"referenceID": 2, "context": "NUS-WIDE [Chua et al., 2009] is a dataset containing 270K annotated images from Flickr.", "startOffset": 9, "endOffset": 28}, {"referenceID": 15, "context": "In training and evaluation, we followed the protocol of [Liu et al., 2011]: two images were considered as neighbors if they share", "startOffset": 56, "endOffset": 74}, {"referenceID": 16, "context": "\u2022 four representative \u201cshallow\u201d hashing methods: kernelized supervised hashing (KSH) [Liu et al., 2012], anchor graph hashing (AGH) [Liu et al.", "startOffset": 85, "endOffset": 103}, {"referenceID": 15, "context": ", 2012], anchor graph hashing (AGH) [Liu et al., 2011] (we compare with its two alternative forms: AGH1 and AGH2; see the original paper), parameter-sensitive hashing (PSH) [Shakhnarovich et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 22, "context": ", 2011] (we compare with its two alternative forms: AGH1 and AGH2; see the original paper), parameter-sensitive hashing (PSH) [Shakhnarovich et al., 2003], and LDA Hash (LH) [Strecha et al.", "startOffset": 126, "endOffset": 154}, {"referenceID": 26, "context": ", 2003], and LDA Hash (LH) [Strecha et al., 2012] 2.", "startOffset": 27, "endOffset": 49}, {"referenceID": 19, "context": "\u2022 two latest \u201cdeep\u201d hashing methods: neural-network hashing (NNH) [Masci et al., 2014], and sparse neuralnetwork hashing (SNNH) [Masci et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 18, "context": ", 2014], and sparse neuralnetwork hashing (SNNH) [Masci et al., 2013].", "startOffset": 49, "endOffset": 69}, {"referenceID": 18, "context": "Most of the results are collected from the comparison experiments in [Masci et al., 2013], under the same settings.", "startOffset": 69, "endOffset": 89}, {"referenceID": 20, "context": "We adopt the following classical criteria for evaluation: 1) precision and recall (PR) for different Hamming radii, and the F1 score as their harmonic average; 2) mean average precision (MAP) [M\u00fcller et al., 2001].", "startOffset": 192, "endOffset": 213}, {"referenceID": 18, "context": "Besides, for NUS-WIDE, as computing mAP is slow over this large dataset, we follow the convention of [Masci et al., 2013] to compute the mean precision (MP) of top-5K returned neighbors (MP@5K), as well as report mAP of top-10 results (mAP@10).", "startOffset": 101, "endOffset": 121}, {"referenceID": 34, "context": "We have not compared with convolutional network-based hashing methods [Xia et al., 2014; Li et al., 2015; Lai et al., 2015], since it is difficult to ensure their models to have the same parameter capacity as our fully-connected model in controlled experiments.", "startOffset": 70, "endOffset": 123}, {"referenceID": 14, "context": "We have not compared with convolutional network-based hashing methods [Xia et al., 2014; Li et al., 2015; Lai et al., 2015], since it is difficult to ensure their models to have the same parameter capacity as our fully-connected model in controlled experiments.", "startOffset": 70, "endOffset": 123}, {"referenceID": 12, "context": "We have not compared with convolutional network-based hashing methods [Xia et al., 2014; Li et al., 2015; Lai et al., 2015], since it is difficult to ensure their models to have the same parameter capacity as our fully-connected model in controlled experiments.", "startOffset": 70, "endOffset": 123}, {"referenceID": 12, "context": ", [Lai et al., 2015], into comparison because they will require three parallel encoder columns .", "startOffset": 2, "endOffset": 20}, {"referenceID": 18, "context": "That is also evidenced by Table 2 in [Masci et al., 2013], i.", "startOffset": 37, "endOffset": 57}], "year": 2016, "abstractText": "We investigate the `\u221e-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named Deep `\u221e Encoder, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior acts as an effective network regularization, and facilitates the model initialization. We then investigate the effective use of the proposed model in the application of hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a Deep Siamese `\u221e Network, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors.", "creator": "LaTeX with hyperref package"}}}