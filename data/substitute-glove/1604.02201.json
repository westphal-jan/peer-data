{"id": "1604.02201", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2016", "title": "Transfer Learning for Low-Resource Neural Machine Translation", "abstract": "The microfluidic - adapter supports for defects machine interpretation (NMT) has having none minimal in instance surveys puzzle, taken important simply makes method with significantly - resource vernacular. We both into transfer providing method that result reduces Bleu shooting apart as range many considerably - resource distinct. Our main things is same first train whose close - resource usage pair (it shares model ), then transfer number in next him calculation to has minimum - agricultural double (the child brand) to visualise and underpin preparing. Using our additional learn requires so infrastructure angles NMT models by brought average of 5. 53 Bleu days four significantly - nature language pairs. Ensembling part unknown fact due needs third 2 Bleu which genuine be NMT performance back price - resource pipe translation same to a an adjective enterprise launcher spelling (SBMT) system, exceeding focus performance full one classical pair. Additionally, used the effectively learning core for passed - twice, tell this improve the SBMT therefore one an compared only second. 33 Bleu, establishing after state - than - rest - art the low - natural technique encyclopedia.", "histories": [["v1", "Fri, 8 Apr 2016 00:16:35 GMT  (118kb,D)", "http://arxiv.org/abs/1604.02201v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["barret zoph", "deniz yuret", "jonathan may", "kevin knight"], "accepted": true, "id": "1604.02201"}, "pdf": {"name": "1604.02201.pdf", "metadata": {"source": "CRF", "title": "Transfer Learning for Low-Resource Neural Machine Translation", "authors": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight"], "emails": ["jonmay}@isi.edu", "dyuret@ku.edu.tr", "knight@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) (Sutskever et al., 2014) is a promising paradigm for extracting translation knowledge from parallel text. NMT systems have achieved competitive accuracy rates under large-data training conditions for language pairs such as English-French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages,\nwhere parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard stringto-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feed-input techniques from Luong et al. (2015a).\nIn this paper, we give a method for substantially improving NMT results on these languages. Neural models learn representations of their input that are often useful across tasks. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 BLEU on average, and we provide an analysis of why the method works. The final NMT system approaches strong SBMT baselines in in all four language pairs, and exceeds SBMT performance in one of them. Furthermore, we show that ar X iv :1\n60 4.\n02 20\n1v 1\n[ cs\n.C L\n] 8\nA pr\n2 01\n6\nNMT is an exceptional re-scorer of \u2018traditional\u2019 MT output; even NMT that on its own is worse than SBMT is consistently able to improve upon SBMT system output when incorporated as a rescoring model.\nWe start by a brief description of our NMT model in Section 2. Section 3 gives some background on transfer learning and explains how we use it to improve machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section 4. Section 5 explores alternatives to our model to enhance understanding. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and using a translation model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine-tuning, and smarter initialization of different components of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models showing that transfer solves an overfitting problem, not a search problem. We summarize our contributions in Section 6."}, {"heading": "2 NMT Background", "text": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Castan\u0303o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector to a target sentence. In this paper, we use a two-layer encoder-decoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained to optimize maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). Additionally, we use an attention mechanism that allows the target decoder to look back at the source encoder, specifically the local attention plus feed-input model from Luong et al. (2015a)."}, {"heading": "3 Transfer Learning", "text": "Transfer learning uses knowledge from a learned task to improve the performance on a related task, typically reducing the amount of required training data (Torrey and Shavlik, 2009; Pan and Yang, 2010). In natural language processing, transfer learning methods have been successfully applied to speech recognition, document classification and sentiment analysis (Wang and Zheng, 2015). Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning (Bengio, 2012). For example, Cires\u0327an et al. (2012) use a convolutional neural network to recognize handwritten characters and show positive effects of transfer between models for Latin and Chinese characters. Ours is the first study to apply transfer learning to neural machine translation.\nThe transfer learning approach we use is simple and effective. We first train an NMT model on a dataset where there is a large amount of bilingual data (e.g., French-English), which we call the parent model. Next, we initialize an NMT model with the already-trained parent model. This new model is then trained on a dataset with very little bilingual data (e.g., Uzbek-English), which we call the child model. This means that the low-data NMT model will not start with random weights, but with the weights from the parent model.\nA justification for this approach is that in sce-\nnarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above.\nOur method results in large BLEU increases for a variety of low resource languages. In one of the four language pairs our NMT system using transfer beats a strong SBMT baseline. Not only do these transfer models do well on their own, they also give large gains when used for rescoring nbest lists (n = 1000) from the SBMT system. Section 4 details these results."}, {"heading": "4 Experiments", "text": "To evaluate how well our transfer method works we apply it to a variety of low-resource languages, both stand-alone and for re-scoring a strong SBMT baseline. We report large BLEU increases across the board with our transfer method.\nFor all of our experiments with low-resource languages we use French as the parent source language and for child source languages we use Hausa, Turkish, Uzbek, and Urdu. The target language is always English. Table 1 shows parallel training data set sizes for the child languages, where the language with the most data has only 1.8m English tokens. For comparison, our parent French-English model uses a training set with 300 million English tokens and achieves 26 BLEU on the development set. Table 1 also shows the SBMT system scores along with the NMT baselines that do not use transfer. There is a large gap\nbetween the SBMT and NMT systems without using our transfer method.\nThe SBMT system used in this paper is a stringto-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). In this system there are two count-based 5-gram language models. One is trained on the English side of the WMT 2015 English-French dataset and the other is trained on the English side of the low-resource bitext. Additionally, the SBMT models use thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009).\nFor our NMT system, we use development sets for Hausa, Turkish, Uzbek, and Urdu to tune the learning rate, parameter initialization range, dropout rate and hidden state size for all the experiments. For training we use a minibatch size of 128, hidden state size of 1000, a target vocabulary size of 15K and a source vocabulary size of 30K. The child models are trained with a dropout probability of 0.5 as in Zaremba et al. (2014). The common parent model is trained with a dropout probability of 0.2. The learning rate used for both child and parents is 0.5 with a decay rate of 0.9 when the development perplexity does not improve. The child models are all trained for 100 epochs. We re-scale the gradient when the gradient norm is greater than 5. The initial parameter range is [-0.08, +0.08]."}, {"heading": "4.1 Transfer Results", "text": "The results for our transfer learning method applied to the four languages above are in Table 2. The parent models were trained on the WMT 2015 (Bojar et al., 2015) French-English corpus for 5 epochs. Our baseline NMT systems (\u2018NMT\u2019 col-\numn) all receive a large BLEU improvement when using the transfer method (the \u2018Xfer\u2019 column) with an average BLEU improvement of 5.6. Additionally, when we use unknown word replacement from Luong et al. (2015b) and ensemble together 8 models (the \u2018Final\u2019 column) we further improve upon our BLEU scores, bringing the average BLEU improvement to 7.5. Overall our method allows the NMT system to reach competitive scores and beat the SBMT system in one of the four language pairs."}, {"heading": "4.2 Re-scoring Results", "text": "We also use the NMT model with transfer learning to re-score output n-best lists (n = 1000) from the SBMT system. Table 3 shows the results of re-scoring. Transfer NMT models give the highest gains over using re-scoring with a neural language model or an NMT system that does not use transfer. The neural language model is an LSTM RNN with 2 layers and 1000 hidden states. It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014; Williams et al., 2015). Additionally, it is trained using dropout with a dropout probability of 0.2 as in (Zaremba et al., 2014). From re-scoring with the transfer model, we get an improvement of 1.1\u2013 1.6 BLEU points above the strong SBMT system.\nWe ran a number of additional experiments to understand what components of our final transfer model significantly contribute to the overall result. Section 5 details these experiments."}, {"heading": "5 Analysis", "text": "We analyze the effects of using different parent models, regularizing different parts of the child model and trying different regularization techniques."}, {"heading": "5.1 Different Parent Languages", "text": "In the above experiments we use French-English as the parent language pair. Here, we experiment with different parent languages. In this set of experiments we use Spanish-English as the child language pair, rather then Hausa, Turkish, Uzbek, or Urdu. A description of the data used in this section is presented in Table 4.\nOur experimental results are shown in Table 5, where we use French and German as parent languages. If we just train a model with no transfer on a small Spanish-English training set we get a BLEU score of 16.4. When using our transfer method using French and German as parent languages, we get BLEU scores of 31.0 and 29.8 respectively. As expected, French is a better parent than German for Spanish, which could be the result of the parent language being more similar to the child language.\nOverall, we can see that the choice of parent language can make a difference in the BLEU score, so in our Hausa, Turkish, Uzbek, and Urdu experiments, a parent language more optimal than French might improve results."}, {"heading": "5.2 Effects of having Similar Parent Language", "text": "Next, we look at a best-case scenario in which the parent language is as similar as possible to the child language. Here we devise a synthetic\nModel Train BLEU PPL size Uzbek-English 1.8m 10.7 22.4 Uzbek-English transfer 1.8m 15.0 (+4.3) 13.9 French\u2019-English 1.8m 13.3 28.2 French\u2019-English transfer 1.8m 20.0 (+6.7) 10.9\nchild language (called French\u2019) which is exactly like French, except its vocabulary is shuffled randomly. (e.g., \u201cinternationale\u201d is now \u201cpomme\u201d, etc). This language, which looks unintelligible to human eyes, nevertheless has the same distributional and relational properties as actual French, i.e. the word that, prior to vocabulary reassignment, was \u2018roi\u2019 (king) is likely to share distributional characteristics, and hence embedding similarity, to the word that, prior to reassignment, was \u2018reine\u2019 (queen). Such a language should be the ideal parent model.\nThe results of this experiment are shown in Table 6. We get a 4.3 BLEU improvement with an unrelated parent (i.e. French-parent and Uzbekchild), but we get a 6.7 BLEU improvement with a \u2018closely related\u2019 parent (i.e. French-parent and French\u2019-child). We conclude that the choice of parent model can have a strong impact on transfer models, and choosing better parents for our lowresource languages (if data for such parents can be obtained) could improve the final results."}, {"heading": "5.3 Ablation Analysis", "text": "In all the above experiments, only the target input and output embeddings are fixed during training. In this section we analyze what happens when different parts of the model are fixed, in order to see what yields optimal performance. Figure 2 shows a diagram of the components of a sequence-to-sequence model. Table 7 shows how we begin to allow more components of the child NMT model to be trained and see the effect on performance in the model. We see that the optimal setting for transferring from French-English to Uzbek-English in terms of BLEU performance\nAttention\nTarget RNN Source RNN\nSource embedding\nSource word\nTarget input embedding\nSource RNN\nSource embedding\nSource word\nTarget input embedding\nTarget RNN\nTarget pred\nTarget output embedding Target output embedding\nTarget pred\nTarget word\nTarget word\nFigure 2: Our NMT model architecture, showing six blocks of parameters, in addition to source/target words and predictions. During transfer learning, we expect the source-language related blocks to change more than the targetlanguage related blocks.\nSetting Dev Dev BLEU PPL No retraining 0.0 112.6 Retrain source embeddings 7.7 24.7 + source RNN 11.8 17.0 + target RNN 14.2 14.5 + target attention 15.0 13.9 + target input embeddings 14.7 13.8 + target output embeddings 13.7 14.4\nTable 7: Starting with the parent French-English model (BLEU =24.4, PPL=6.2), we randomly assign Uzbek word types to French word embeddings, freeze various parameters of the neural network model, and allow Uzbek-English (child model) training to modify other parts. The table shows how Uzbek-English BLEU and perplexity vary as we allow more parameters to be re-trained.\nis to allow all of the components of the child model to be trained except for the input and output target embeddings.\nEven though we use this setting for our main experiments, the optimum setting is likely to be language- and corpus-dependent. For Turkish, experiments show that freezing target attention parameters as well gives slightly better results. For parent-child models with closely related languages we expect freezing, or strongly regularizing, more components of the model to give better results."}, {"heading": "5.4 Learning Curve", "text": "In Figure 3 we plot learning curves for both a transfer and a non-transfer model on training and development sets. We see that the final training set perplexities for both the transfer and non-transfer model are very similar, but the development set perplexity for the transfer model is much better.\nThe fact that the two models start from and converge to very different points, yet have similar training set performances, indicates that our architecture and training algorithm are able to reach a good minimum of the training objective regardless of the initialization. However, the training objective seems to have a large basin of models with similar performance and not all of them generalize well to the development set. The transfer model, starting with and staying close to a point known to perform well on a related task, is guided to a final point in the weight space that generalizes to the development set much better."}, {"heading": "5.5 Dictionary Initialization", "text": "Using the transfer method, we always initialize input language embeddings for the child model with randomly-assigned embeddings from the parent (which has a different input language). A smarter method might be to initialize child embeddings with similar parent embeddings, where similarity is measured by word-to-word t-table probabilities. To get these probabilities we compose UzbekEnglish and English-French t-tables obtained from the Berkeley Aligner (Liang et al., 2006). We see from Figure 4 that this dictionary-based assignment results in faster improvement in the early part of the training. However the final performance is similar to our standard model, indicating that the training is able to untangle the dictionary permutation introduced by randomly-assigned embeddings."}, {"heading": "5.6 Different Parent Models", "text": "In the above experiments, we use a parent model trained on a large French/English bilingual corpus. One might hypothesize that our gains come from exploiting the English half of the corpus as an additional language model resource. Therefore, we explore transfer learning for the child model with parent models that only use the English side of the bilingual corpus. Table 8 shows the re-\nsults for these experiments where we train one parent model to copy English sentences (EnglishEnglish) and another parent model to un-permute scrambled English sentences (EngPerm-English). Additionally, we train a parent model that is just an RNN language model. These results show that our transfer learning is not simply importing an English language model, but making use of translation parameters learned from the parent\u2019s large bilingual text."}, {"heading": "6 Conclusion", "text": "Overall our transfer method improves NMT scores on low-resource languages by a large margin and allows our transfer NMT system to come close to the performance of a very strong SBMT system, even exceeding its performance on HausaEnglish. In addition, we consistently and significantly improve state-of-the-art SBMT systems on low-resource languages when the transfer NMT system is used for re-scoring. Our experiments suggest that there is still room for improvement in selecting parent languages that are more similar to child languages, provided data for such parents can be found."}, {"heading": "7 Acknowledgments", "text": "This work was carried out with funding from DARPA (HR0011-15-C-0115) and ARL/ARO (W911NF-10-1-0533)."}], "references": [{"title": "Neural machine translation", "author": ["Bahdanau et al.2014] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Pragmatic neural language modelling in machine translation", "author": ["Baltescu", "Blunsom2014] P. Baltescu", "P. Blunsom"], "venue": "arXiv preprint arXiv:1412.7119", "citeRegEx": "Baltescu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baltescu et al\\.", "year": 2014}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "A connectionist approach to machine translation", "author": ["Casta\u00f1o", "Casacuberta1997] M.A. Casta\u00f1o", "F. Casacuberta"], "venue": "In Proc. Eurospeech", "citeRegEx": "Casta\u00f1o et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Casta\u00f1o et al\\.", "year": 1997}, {"title": "11,001 new features for statistical machine translation", "author": ["Chiang et al.2009] D. Chiang", "K. Knight", "W. Wang"], "venue": "In Proc. NAACL", "citeRegEx": "Chiang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2009}, {"title": "Transfer learning for latin and chinese characters with deep neural networks", "author": ["U. Meier", "J. Schmidhuber"], "venue": "In Proc. IJCNN", "citeRegEx": "Cire\u015fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2012}, {"title": "What\u2019s in a translation rule", "author": ["Galley et al.2004] M. Galley", "M. Hopkins", "K. Knight", "D. Marcu"], "venue": "In Proc. HLT-NAACL", "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Scalable inference and training of contextrich syntactic translation models", "author": ["Galley et al.2006] M. Galley", "J. Graehl", "K. Knight", "D. Marcu", "S. DeNeefe", "W. Wang", "I. Thayer"], "venue": "In Proc. ACLCOLING", "citeRegEx": "Galley et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] P. Liang", "B. Taskar", "D. Klein"], "venue": "In Proc. NAACL", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Luong et al.2015a] M. Luong", "H. Pham", "C. Manning"], "venue": "In Proc. EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015b] T. Luong", "I. Sutskever", "Q. Le", "O. Vinyals", "W. Zaremba"], "venue": "In Proc. ACL", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426", "author": ["Mnih", "Teh2012] A. Mnih", "Y.W. Teh"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Asynchronous translations with recurrent neural nets", "author": ["Neco", "Forcada1997] R. Neco", "M. Forcada"], "venue": "In Proc. International Conference on Neural Networks", "citeRegEx": "Neco et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Neco et al\\.", "year": 1997}, {"title": "A survey on transfer learning", "author": ["Pan", "Yang2010] S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q.V. Le"], "venue": "In Proc. NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Vaswani et al.2013] A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "In Proc. EMNLP", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Transfer learning for speech and language processing", "author": ["Wang", "Zheng2015] D. Wang", "T. Fang Zheng"], "venue": "arXiv preprint arXiv:1511.06066", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proc. IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Scaling recurrent neural network language models. CoRR, abs/1502.00512", "author": ["Williams et al.2015] W. Williams", "N. Prasad", "D. Mrva", "T. Ash", "T. Robinson"], "venue": null, "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Zaremba et al.2014] W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Neural machine translation (NMT) (Sutskever et al., 2014) is a promising paradigm for extracting translation knowledge from parallel text.", "startOffset": 33, "endOffset": 57}, {"referenceID": 6, "context": "Table 1 shows that for 4 low-resource languages, a standard stringto-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feed-input techniques from Luong et al.", "startOffset": 103, "endOffset": 145}, {"referenceID": 7, "context": "Table 1 shows that for 4 low-resource languages, a standard stringto-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feed-input techniques from Luong et al.", "startOffset": 103, "endOffset": 145}, {"referenceID": 6, "context": "Table 1 shows that for 4 low-resource languages, a standard stringto-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feed-input techniques from Luong et al. (2015a).", "startOffset": 104, "endOffset": 281}, {"referenceID": 15, "context": "Figure 1: The encoder-decoder framework for neural machine translation (NMT) (Sutskever et al., 2014).", "startOffset": 77, "endOffset": 101}, {"referenceID": 15, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector.", "startOffset": 47, "endOffset": 170}, {"referenceID": 0, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector.", "startOffset": 47, "endOffset": 170}, {"referenceID": 18, "context": "In this paper, we use a two-layer encoder-decoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained to optimize maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990).", "startOffset": 240, "endOffset": 254}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014; Luong et al., 2015a), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector to a target sentence. In this paper, we use a two-layer encoder-decoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained to optimize maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). Additionally, we use an attention mechanism that allows the target decoder to look back at the source encoder, specifically the local attention plus feed-input model from Luong et al. (2015a).", "startOffset": 8, "endOffset": 702}, {"referenceID": 2, "context": "Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning (Bengio, 2012).", "startOffset": 165, "endOffset": 179}, {"referenceID": 2, "context": "Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning (Bengio, 2012). For example, Cire\u015fan et al. (2012) use a convolutional neural network to recognize handwritten characters and show positive effects of transfer between models for Latin and Chinese characters.", "startOffset": 166, "endOffset": 216}, {"referenceID": 7, "context": "The SBMT system used in this paper is a stringto-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004).", "startOffset": 93, "endOffset": 135}, {"referenceID": 6, "context": "The SBMT system used in this paper is a stringto-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004).", "startOffset": 93, "endOffset": 135}, {"referenceID": 4, "context": "Additionally, the SBMT models use thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009).", "startOffset": 98, "endOffset": 119}, {"referenceID": 20, "context": "5 as in Zaremba et al. (2014). The common parent model is trained with a dropout probability of 0.", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": "Additionally, when we use unknown word replacement from Luong et al. (2015b) and ensemble together 8 models (the \u2018Final\u2019 column) we further improve upon our BLEU scores, bringing the average BLEU improvement to 7.", "startOffset": 56, "endOffset": 77}, {"referenceID": 16, "context": "It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014; Williams et al., 2015).", "startOffset": 85, "endOffset": 178}, {"referenceID": 19, "context": "It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014; Williams et al., 2015).", "startOffset": 85, "endOffset": 178}, {"referenceID": 20, "context": "2 as in (Zaremba et al., 2014).", "startOffset": 8, "endOffset": 30}, {"referenceID": 9, "context": "To get these probabilities we compose UzbekEnglish and English-French t-tables obtained from the Berkeley Aligner (Liang et al., 2006).", "startOffset": 114, "endOffset": 134}], "year": 2016, "abstractText": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages. Our key idea is to first train a highresource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for rescoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.", "creator": "LaTeX with hyperref package"}}}