{"id": "1604.03209", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Disfluency Detection using a Bidirectional LSTM", "abstract": "We introduce it set effective five disfluency detectors using an Bidirectional Long - Short Term Memory neural computer (BLSTM ). In one which form spelling sequence, soon comparable good as direct pattern defending page come were incorporating to prevent characteristic alone concepts size in students, which next to continued combined took the tell i.e. mean. The BLSTM takes improved and characterization maintenance states in use way was additionally reparandum state. The final domestic ispl integer linear programming even incorporate reducing known disfluency structure. In technique opened was Switchboard sangha, the model achieves post - of - part - abstract excellent more both soon specific disfluency calibration task when set outlook detected task. Analysis shows how the model has it real-time of free - corrects disfluencies, the changing out be way ability must detect.", "histories": [["v1", "Tue, 12 Apr 2016 02:34:00 GMT  (84kb,D)", "http://arxiv.org/abs/1604.03209v1", "Submitted to INTERSPEECH 2016"]], "COMMENTS": "Submitted to INTERSPEECH 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vicky zayats", "mari ostendorf", "hannaneh hajishirzi"], "accepted": false, "id": "1604.03209"}, "pdf": {"name": "1604.03209.pdf", "metadata": {"source": "CRF", "title": "Disfluency Detection using a Bidirectional LSTM", "authors": ["Vicky Zayats", "Mari Ostendorf", "Hannaneh Hajishirzi"], "emails": ["vzayats@uw.edu", "ostendor@uw.edu", "hannaneh@uw.edu"], "sections": [{"heading": "1. Introduction", "text": "A characteristic of spontaneous speech that makes it different from written text \u2013 including informal text \u2013 is the presence of disfluencies. Disfluencies include filled pauses, repetitions, repairs and false starts. Disfluencies are frequent in all forms of spontaneous speech, whether casual discussions or formal arguments [1]. They present significant challenges for some natural language processing (NLP) tasks on spoken transcripts, such as parsing and machine translation [2, 3, 4]. On the other hand, disfluencies also reflect speaker interaction [5]. Disfluency detection is most often used as a preprocessing step for NLP, where the goal is removal of the non-fluent word sequences. For extracting information about the interaction, the detection of both disfluent and correction parts can be important.\nA standard annotation of disfluency structure [6] indicates the reparandum (word or words that the speaker intends to be replaced or ignored), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.)\n[reparandum + {interregnum} repair] Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. Table 1 gives a few examples. In this work, we use a slightly modified representation from [7] that distinguishes repetitions (marked by \u2018S\u2019) and flattens the nested structure in a sequence of repetitions, which has has led to improved disfluency detection in prior work [7, 1].\nMost work on automatic disfluency detection is aimed at cleaning transcripts for further processing, where only reparandum detection is of interest. In this study we are interested in both the reparandum and repair, motivated by a long term goal\nof understanding variability in disfluency production related to cognitive load and social context. We introduce a new approach to disfluency detection given text transcripts that leverages a Bidirectional Long-Short Term Memory (BLSTM) neural network and integer linear programming. The model achieves state-of-the-art performance on the standard Switchboard task, and analyses show contributions from including pattern match features in the input."}, {"heading": "2. Related Work", "text": "Approaches to automatic disfluency detection generally fall into two categories: sequence tagging and parsing-based models. Many studies have used a sequence tagging model with begininside-outside (BIO) style states that label words as being inside or outside of a reparandum word sequence. The most successful approaches have leveraged discriminative models, including conditional random fields (CRFs) as a classifier [8, 9, 7, 1, 10]. In [9], Integer Linear Programming is integrated with the CRF for optimizing over the prediction sequences. An alternative for improving the CR uses an F-score matching objective, multistep learner and Max-Margin Markov Networks (M3N) [11]; the objective change had the highest impact. The current best performing system uses a Semi-Markov CRF [12].\nAnother set of approaches leverage parsing and represent a noisy channel relationship between the reparandum and the repair [13, 14]. The noisy channel parsing models are not well suited to detecting restarts (where there is no repair), but they could be used for identifying the repair, though results on that task have not been reported. Incremental dependency parsing combined with disfluency removal has also been explored [15, 3]. Because incremental models do not benefit from reparandum/repair similarity cues, they tend to have lower performance than delayed decision models. Depending on the downstream application, an advantage of parsing models in general is that they jointly optimize for both parsing and disfluency detection performance. A disadvantage is that they require treebank annotation for training. Since we are ultimately interested\nar X\niv :1\n60 4.\n03 20\n9v 1\n[ cs\n.C L\n] 1\n2 A\npr 2\n01 6\nin applying disfluency detection to a broad set of domains, we will leverage a sequence tagging approach, but we extend the label state space to separately model repetitions and repairs, as in [1].\nTwo recent studies have applied recurrent neural networks (RNNs) to disfluency detection. One approach explores incremental detection [16], with an objective that combines detection performance with minimal latency. Because of the latency constraints, this approach has weak performance in comparison to other studies on disfluency detection. Word embeddings learned by an RNN have also been used as features in a CRF classifier [10]. In our current study, we also use an RNN, particularly the LSTM framework, but in the standard disfluency detection paradigm (non-incremental), which allows us to use a bidirection architecture and leverage the relatedness of repair and reparandum for repetition and correction disfluencies. Unlike [10], the RNN is the classifier, so our feature embeddings are trained in an end-to-end manner, and they also leverage pattern matching features.\nWhile most studies of disfluency detection focus on using only text transcripts as input, it is well known that prosodic cues are useful in combination with lexical cues [17, 18, 8, 12]. Prosody can carry information that is not represented in transcripts (e.g. length of pauses, fundamental frequency trends), which is relevant for detecting interruption points. However, most studies find that the gain from combining prosodic features with lexical features is relatively small, so our current study focuses on lexical features alone. Adding prosodic information to the existing features is an easy modification with the neural network framework, and we hope to explore this in future work."}, {"heading": "3. General Framework", "text": "The standard disfluency detection task involving reparandum detection is often called \u201cedit detection\u201d. The typical sequence tagging model represents 5 states: beginning of the edit region BE, inside edit IE, the word before the interruption point IP, one word edit BE IP and outside of the edit (including both repairs and fluent regions) O. For evaluation of edit detection, all words with labels other than O are considered edit words. We also consider two extensions of the state space. The first extension (called explicit repair modeling) includes 8 states, adding: C for the repair word, and C IE, C IP for words in nested disfluencies that belong to both a reparandum and a repair. For the edit detection task, the C IE, C IP states are considered part of an edit region. Note that having explicit repair states does not allow correction detection, as defined in [1], since the repairs associated with repetitions vs. corrections are not distinguished. The expanded state space uses the extent of the correction to improve edit detection. The second extension includes 17 states for joint reparandum and correction detection, expanding all non-O states to separately represent repetition and nonrepetition disfluencies, as in [1]. With this model, we can detect corrections and take advantage of the fact that repetitions tend to benefit from different features than other disfluencies.\nAs reviewed in section 2, CRFs have been used widely for disfluency detection and therefore represent a strong baseline for comparison to the new LSTMs developed here. In our work we use the CRF++ toolkit.1 Starting from a core feature set of lexical, distance-based pattern match features and disfluency language model features used in [1] (listed in Table 2), the CRF features are generated by applying feature functions provided\n1CRF++ available at https://taku910.github.io/crfpp\nby CRF++ templates to create new features within a relative time frame. For example, using the core feature \u2019word index,\u2019 we can construct n-gram features by applying feature functions across a local time window. A total of 258 features are generated, including combinations of different core features as well as n-grams and POS n-grams."}, {"heading": "4. Proposed Method", "text": ""}, {"heading": "4.1. RNN Architectures", "text": "We use Long-Short Term Memory (LSTM) RNNs for the task of disfluency detection. LSTMs are deep neural networks to model sequences, achieving good performance in a variety of NLP tasks [19, 20, 21]. A typical memory cell includes gates to weigh input and history impact at a particular time, allowing the model to determine their relative importance and alleviating the vanishing gradient problem [22]. As a result, LSTMs can effectively represent longer phrases, which is useful for the disfluency detection task. For disfluency state sequence tagging, we use a softmax layer at the top layer of the LSTM.\nAn LSTM is a directional model and predicts a state given its previous states. For disfluency detection based on text alone, it is difficult to predict a word as disfluent by only observing the previous words prior to the occurrence of the interruption point. Unexpected word sequences following the interruption point and similarity between the repair and the reparandum are important indicators. Therefore, we make use of LSTMs with the input sentence in reverse order. In addition, we explore use of a bidirectional LSTM (BLSTM) [23]. As shown in Figure 1, the BLSTM uses past and future states in predicting the disfluency tag of a given word. This is particularly useful for predicting both repairs and corrections (8-state and 17-state disfluency)."}, {"heading": "4.2. Feature Embeddings", "text": "As shown in Figure 1, the input vector consists of three main components: word index, POS tag, and disfluency-based features, as listed in Table 2. The disfluency features provide useful information about word identity (filler or incomplete words) and patterns (if the exact word appeared previously in a fixed length window). We separately map one-hot representations of these features to embeddings for a dense representation, and then concatenate them to use as the input to LSTM cells. For initialization of the word embeddings, we train a backward-LSTM language model on the Switchboard corpus with disfluencies removed. The initialization for POS tag embeddings is similar, with the training text mapped to POS tags. All other parameters have random initialization. During the training of the whole neural network, embeddings are updated through back propagation similar to all the other parameters."}, {"heading": "4.3. ILP post-processing", "text": "While the hidden states of LSTM and BLSTM are connected through time, the outputs from the softmax layer are not. This often leads to inconsistencies between neighboring labels, sometimes resulting in label sequences that are not valid paths in the state space (i.e. \u2018illegal\u2019). For example, the model can output the sequence of labels O IE IE IP, which is not a valid sequence since disfluencies always start with BE tag. As such, an additional smoothing over LSTM predictions is needed. Some of the possible approaches include LSTM-CRF [24], Markov model [16] or Integer Linear Programming (ILP) [9]. In this work we use the ILP solution previously presented in [9] for disfluency detection. Since [9] uses constraints for 5 states only, we collapse the softmax proportions from the larger state space to 5 states, as in\nP (yt = O) = P (yt = O) + P (yt = C)\nP (yt = IE) = P (yt = IE) + P (yt = C IE)\nP (yt = IP ) = P (yt = IP ) + P (yt = C IP )\n(1)\nfor the 8-state model."}, {"heading": "5. Experiments", "text": "We assess the proposed BLSTM model for disfluency detection in experiments on the Switchboard corpus of conversational speech [25], using the standard division of the disfluencyannotated subset into training, development and test sets. The flattened version of repetition annotation provided in [7] is used. As in other studies on disfluency detection, performance is measured using precision/recall of words in edit regions. In addition, we use the same measure on finer grain labels, including different types of edit regions (repetition vs. non-repetition disfluencies) and corrections.\nAll code is written in theano.2 LSTM (including BLSTM) parameter optimization is done using Adadelta [26] with a minibatch size of 50. We use the Switchboard development set to tune the LSTM parameters (number of dimensions) and to find an optimal stopping point for LSTM training. The dimensions of the word embeddings and the hidden dimension are separately tuned for each variation, and the best result is presented in the table. The best number of dimensions for the BLSTM with 17 states is 100, and for all other models it is 150. The POS embedding dimension is chosen to 5 for all models. As mentioned previously, word and POS embeddings are initialized using a backwards LSTM language model trained on cleanedup Switchboard text, and other model parameters have random initialization. All models are trained using only sentences that have 50 or fewer words due to the high computational complexity of longer histories in the LSTM. Fewer than 1% of Switchboard sentences have length greater than 50 words. In testing, all sentences are processed."}, {"heading": "5.1. Model Performance", "text": "Table 3 shows the CRF and BLSTM performance on the development set for the edit detection task using all three state space alternatives presented in the Section 3. As shown in the table, the BLSTM has much better performance in edit detection compared to the CRF in all cases. Moreover, the BLSTM with explicit repair states achieves the best result in the edit detection task, which we hypothesize is related to the success of the noisy channel model approach: explicitly representing the extent of the repair allows the model to match the repair to the reparandum for improved detection. Table 3 also gives the result of the ILP post-processing on the best model. Although some corrections to illegal sequences do not impact edit detection performance, many do. ILP improves the precision of our BLSTM predictions without hurting the recall.\nThe 17-state models give slightly worse performance for\n2We modify theano code for LSTM available at https:// github.com/JonathanRaiman/theano_lstm\nedit detection, but they enable correction detection, the results of which are shown in Table 4. While the BLSTM gives significantly better overall F-score, the two models have very different precision-recall tradeoffs. Augmenting the 17-state BLSTM with ILP post-processing could potentially recover some of the precision lost in moving to the BLSTM."}, {"heading": "5.2. Method Comparison", "text": "We evaluate our best models on the test set and compare them to recent methods in the literature leveraging only transcripts. The results on edit detection are shown in Table 5. For edit detection, we use the explicit repair state space (8 states), which achieves the best results on the development set, including results both with and without ILP post-processing. Both systems beat the best prior result with lexical cues only, achieving stateof-the-art performance of 85.9. Looking at the results in terms of the reduction of the performance gap from 1-F of 15.2 to 14.1, this corresponds to a 7% improvement. The BLSTMs also beat the higher performing version in [12] that leverages prosodic features (F=85.4). Incorporating prosodic features in a neural network framework is straightforward and will likely lead to an additional gain.\nThe 17-state BLSTM model also leads to a significant performance gain in correction detection on the test set, achieving an F-score of 57.7 compared to 49.6 for the CRF result reported in [1], corresponding to a 16% improvement."}, {"heading": "6. Analysis", "text": ""}, {"heading": "6.1. Ablation study: CRF vs LSTM", "text": "We also conducted an ablation study to study the effect of the CRF vs. LSTM vs. BLSTM models in combination with different feature embeddings, with results shown in Table 6 for edit detection (5 states) on the development set. We compare the performance of all systems with words alone to the setting when all the features are used. The CRF word features include 1-3 grams within a window of 8 around the word, whereas the LSTM and BLSTM use only the current word index and incorporate longer context through the recurrent structure. For the words-only case, the LSTM and BLSTM give much better results than the CRF. When we add POS and pattern-match features, all systems improve, but the CRF benefits much more."}, {"heading": "6.2. Repetitions vs. Non-repetitions", "text": "Repetition disfluencies are much easier to detect than other disfluencies, although not trivial since some repetitions can be fluent. In order to better understand model performance, we evaluate the 17-state models in terms of their ability to detect repetition vs. non-repetition (other) reparanda. The results are shown in Table 7, showing that the BLSTM is much better in predicting non-repetitions compared to the CRF, allowing better modeling of more complex disfluencies. We conjecture that the dense word representation in the BLSTM captures more of the reparandum/repair \u201crough copy\u201d similarities than the simple POS pattern match features."}, {"heading": "7. Conclusion and Future Work", "text": "In summary, this paper introduces a Bidirectional Long-Short Term Memory (BLSTM) neural network approach to disfluency detection, achieving state-of-the-art performance of 85.9 F-score on the standard disfluency detection task using explicit repair states, lexical feature embeddings and integer linear programming post-processing. In addition, we improve the stateof-the-art in correction detection. Analysis shows that performance gain is for cases that are hardest to detect: restarts and repairs.\nThe best case BLSTM models leverage hand-crafted pattern match features, indicating that the BLSTM architecture is not sufficiently powerful to learn these cues automatically with the amount of available annotated training data. An open question for future work is whether other neural network architectures might more effectively learn these cues and thus have a better fit to disfluency detection task. A related question is whether the pattern match features improve performance in cross-domain scenarios, as observed in [1]."}, {"heading": "8. References", "text": "[1] V. Zayats, M. Ostendorf, and H. Hajishirzi, \u201cMultidomain\ndisfluency and repair detection,\u201d in Proceedings of Interspeech, 2014.\n[2] M. Johnson and E. Charniak, \u201cA tag-based noisy channel model of speech repairs,\u201d in Proc. ACL, 2004.\n[3] M. Honnibal and M. Johnson, \u201cJoint incremental disfluency detection and dependency parsing,\u201d Transactions\nof the Association for Computational Linguistics, vol. 2, no. 1, pp. 131\u2013142, 2014.\n[4] W. Wang, G. Tur, J. Zheng, and N. F. Ayan, \u201cAutomatic disfluency removal for improving spoken language translation,\u201d in Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 5214\u20135217.\n[5] E. Shriberg, \u201cTo errrris human: ecology and acoustics of speech disfluencies,\u201d Journal of the International Phonetic Association, vol. 31, no. 01, pp. 153\u2013169, 2001.\n[6] E. Shriberg, \u201cPreliminaries to a theory of speech disfluencies,\u201d Ph.D. dissertation, Department of Psychology, University of California, Berkeley, CA, 1994.\n[7] M. Ostendorf and S. Hahn, \u201cA sequential repetition model for improved disfluency detection,\u201d in Proc. Interspeech, 2013.\n[8] Y. Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf, and M. Harper, \u201cEnriching speech recognition with automatic detection of sentence boundaries and disfluencies,\u201d IEEE Trans. Audio, Speech and Language Processing, vol. 14, pp. 1526\u20131540, 2006.\n[9] K. Georgila, \u201cUsing integer linear programming for detecting speech disfluencies,\u201d in Proc. NAACL HLT, 2009.\n[10] E. Cho, T.-L. Ha, and A. Waibel, \u201cCrf-based disfluency detection using semantic features for german to english spoken language translation,\u201d 2013.\n[11] X. Qian and Y. Liu, \u201cDisuency detection using multi-step stacked learning,\u201d in Proc. NAACL HLT, 2013.\n[12] J. Ferguson, G. Durrett, and D. Klein, \u201cDisfluency detection with a semi-markov model and prosodic features,\u201d Proc. NAACL HLT, 2015.\n[13] E. Charniak and M. Johnson, \u201cEdit detection and parsing for transcribed speech,\u201d in Proc. NAACL, 2001, pp. 118\u2013 126.\n[14] S. Zwarts, M. Johnson, and R. Dale, \u201cDetecting speech repairs incrementally using a noisy channel approach,\u201d in Proc. Coling, 2010, pp. 1371\u20131378.\n[15] M. S. Rasooli and J. R. Tetreault, \u201cJoint parsing and disfluency detection in linear time.\u201d in EMNLP, 2013, pp. 124\u2013129.\n[16] J. Hough and D. Schlangen, \u201cRecurrent neural networks for incremental disfluency detection,\u201d in Proc. Interspeech, 2015.\n[17] E. Shriberg and A. Stolcke, \u201cA prosody-only decision-tree model for disfluency detection,\u201d in Eurospeech, 1997, pp. 2383\u20132386.\n[18] E. Shriberg, \u201cPhonetic consequences of speech disfluency,\u201d in Proc. International conference of Phonetics Sciences, 1999, pp. 619\u2013622.\n[19] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.\n[20] M. Sundermeyer, R. Schlu\u0308ter, and H. Ney, \u201cLstm neural networks for language modeling.\u201d in INTERSPEECH, 2012.\n[21] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u2013 3112.\n[22] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber, \u201cGradient flow in recurrent nets: the difficulty of learning long-term dependencies,\u201d 2001.\n[23] A. Graves and J. Schmidhuber, \u201cFramewise phoneme classification with bidirectional lstm and other neural network architectures,\u201d Neural Networks, vol. 18, no. 5, pp. 602\u2013610, 2005.\n[24] Z. Huang, W. Xu, and K. Yu, \u201cBidirectional lstmcrf models for sequence tagging,\u201d arXiv preprint arXiv:1508.01991, 2015.\n[25] J. J. Godfrey, E. C. Holliman, and J. McDaniel, \u201cSwitchboard: Telephone speech corpus for research and development,\u201d in Proc. ACL, vol. I, 1992, pp. 517\u2013520.\n[26] M. D. Zeiler, \u201cAdadelta: An adaptive learning rate method,\u201d arXiv preprint arXiv:1212.5701, 2012."}], "references": [{"title": "Multidomain disfluency and repair detection", "author": ["V. Zayats", "M. Ostendorf", "H. Hajishirzi"], "venue": "Proceedings of Interspeech, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A tag-based noisy channel model of speech repairs", "author": ["M. Johnson", "E. Charniak"], "venue": "Proc. ACL, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Joint incremental disfluency detection and dependency parsing", "author": ["M. Honnibal", "M. Johnson"], "venue": "Transactions  of the Association for Computational Linguistics, vol. 2, no. 1, pp. 131\u2013142, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic disfluency removal for improving spoken language translation", "author": ["W. Wang", "G. Tur", "J. Zheng", "N.F. Ayan"], "venue": "Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 5214\u20135217.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "To errrris human: ecology and acoustics of speech disfluencies", "author": ["E. Shriberg"], "venue": "Journal of the International Phonetic Association, vol. 31, no. 01, pp. 153\u2013169, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Preliminaries to a theory of speech disfluencies", "author": ["E. Shriberg"], "venue": "Ph.D. dissertation, Department of Psychology, University of California, Berkeley, CA, 1994.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "A sequential repetition model for improved disfluency detection", "author": ["M. Ostendorf", "S. Hahn"], "venue": "Proc. Interspeech, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Enriching speech recognition with automatic detection of sentence boundaries and disfluencies", "author": ["Y. Liu", "E. Shriberg", "A. Stolcke", "D. Hillard", "M. Ostendorf", "M. Harper"], "venue": "IEEE Trans. Audio, Speech and Language Processing, vol. 14, pp. 1526\u20131540, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Using integer linear programming for detecting speech disfluencies", "author": ["K. Georgila"], "venue": "Proc. NAACL HLT, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Crf-based disfluency detection using semantic features for german to english spoken language translation", "author": ["E. Cho", "T.-L. Ha", "A. Waibel"], "venue": "2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Disuency detection using multi-step stacked learning", "author": ["X. Qian", "Y. Liu"], "venue": "Proc. NAACL HLT, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Disfluency detection with a semi-markov model and prosodic features", "author": ["J. Ferguson", "G. Durrett", "D. Klein"], "venue": "Proc. NAACL HLT, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Edit detection and parsing for transcribed speech", "author": ["E. Charniak", "M. Johnson"], "venue": "Proc. NAACL, 2001, pp. 118\u2013 126.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Detecting speech repairs incrementally using a noisy channel approach", "author": ["S. Zwarts", "M. Johnson", "R. Dale"], "venue": "Proc. Coling, 2010, pp. 1371\u20131378.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Joint parsing and disfluency detection in linear time.", "author": ["M.S. Rasooli", "J.R. Tetreault"], "venue": "in EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Recurrent neural networks for incremental disfluency detection", "author": ["J. Hough", "D. Schlangen"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A prosody-only decision-tree model for disfluency detection", "author": ["E. Shriberg", "A. Stolcke"], "venue": "Eurospeech, 1997, pp. 2383\u20132386.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Phonetic consequences of speech disfluency", "author": ["E. Shriberg"], "venue": "Proc. International conference of Phonetics Sciences, 1999, pp. 619\u2013622.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Lstm neural networks for language modeling.", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "in INTERSPEECH,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u2013 3112.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "2001.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, vol. 18, no. 5, pp. 602\u2013610, 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Bidirectional lstmcrf models for sequence tagging", "author": ["Z. Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1991}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "Proc. ACL, vol. I, 1992, pp. 517\u2013520.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1992}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Disfluencies are frequent in all forms of spontaneous speech, whether casual discussions or formal arguments [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "They present significant challenges for some natural language processing (NLP) tasks on spoken transcripts, such as parsing and machine translation [2, 3, 4].", "startOffset": 148, "endOffset": 157}, {"referenceID": 2, "context": "They present significant challenges for some natural language processing (NLP) tasks on spoken transcripts, such as parsing and machine translation [2, 3, 4].", "startOffset": 148, "endOffset": 157}, {"referenceID": 3, "context": "They present significant challenges for some natural language processing (NLP) tasks on spoken transcripts, such as parsing and machine translation [2, 3, 4].", "startOffset": 148, "endOffset": 157}, {"referenceID": 4, "context": "On the other hand, disfluencies also reflect speaker interaction [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "A standard annotation of disfluency structure [6] indicates the reparandum (word or words that the speaker intends to be replaced or ignored), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "In this work, we use a slightly modified representation from [7] that distinguishes repetitions (marked by \u2018S\u2019) and flattens the nested structure in a sequence of repetitions, which has has led to improved disfluency detection in prior work [7, 1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "In this work, we use a slightly modified representation from [7] that distinguishes repetitions (marked by \u2018S\u2019) and flattens the nested structure in a sequence of repetitions, which has has led to improved disfluency detection in prior work [7, 1].", "startOffset": 241, "endOffset": 247}, {"referenceID": 0, "context": "In this work, we use a slightly modified representation from [7] that distinguishes repetitions (marked by \u2018S\u2019) and flattens the nested structure in a sequence of repetitions, which has has led to improved disfluency detection in prior work [7, 1].", "startOffset": 241, "endOffset": 247}, {"referenceID": 7, "context": "The most successful approaches have leveraged discriminative models, including conditional random fields (CRFs) as a classifier [8, 9, 7, 1, 10].", "startOffset": 128, "endOffset": 144}, {"referenceID": 8, "context": "The most successful approaches have leveraged discriminative models, including conditional random fields (CRFs) as a classifier [8, 9, 7, 1, 10].", "startOffset": 128, "endOffset": 144}, {"referenceID": 6, "context": "The most successful approaches have leveraged discriminative models, including conditional random fields (CRFs) as a classifier [8, 9, 7, 1, 10].", "startOffset": 128, "endOffset": 144}, {"referenceID": 0, "context": "The most successful approaches have leveraged discriminative models, including conditional random fields (CRFs) as a classifier [8, 9, 7, 1, 10].", "startOffset": 128, "endOffset": 144}, {"referenceID": 9, "context": "The most successful approaches have leveraged discriminative models, including conditional random fields (CRFs) as a classifier [8, 9, 7, 1, 10].", "startOffset": 128, "endOffset": 144}, {"referenceID": 8, "context": "In [9], Integer Linear Programming is integrated with the CRF for optimizing over the prediction sequences.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "An alternative for improving the CR uses an F-score matching objective, multistep learner and Max-Margin Markov Networks (MN) [11]; the objective change had the highest impact.", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "The current best performing system uses a Semi-Markov CRF [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "Another set of approaches leverage parsing and represent a noisy channel relationship between the reparandum and the repair [13, 14].", "startOffset": 124, "endOffset": 132}, {"referenceID": 13, "context": "Another set of approaches leverage parsing and represent a noisy channel relationship between the reparandum and the repair [13, 14].", "startOffset": 124, "endOffset": 132}, {"referenceID": 14, "context": "Incremental dependency parsing combined with disfluency removal has also been explored [15, 3].", "startOffset": 87, "endOffset": 94}, {"referenceID": 2, "context": "Incremental dependency parsing combined with disfluency removal has also been explored [15, 3].", "startOffset": 87, "endOffset": 94}, {"referenceID": 0, "context": "in applying disfluency detection to a broad set of domains, we will leverage a sequence tagging approach, but we extend the label state space to separately model repetitions and repairs, as in [1].", "startOffset": 193, "endOffset": 196}, {"referenceID": 15, "context": "One approach explores incremental detection [16], with an objective that combines detection performance with minimal latency.", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "Word embeddings learned by an RNN have also been used as features in a CRF classifier [10].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "Unlike [10], the RNN is the classifier, so our feature embeddings are trained in an end-to-end manner, and they also leverage pattern matching features.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "While most studies of disfluency detection focus on using only text transcripts as input, it is well known that prosodic cues are useful in combination with lexical cues [17, 18, 8, 12].", "startOffset": 170, "endOffset": 185}, {"referenceID": 17, "context": "While most studies of disfluency detection focus on using only text transcripts as input, it is well known that prosodic cues are useful in combination with lexical cues [17, 18, 8, 12].", "startOffset": 170, "endOffset": 185}, {"referenceID": 7, "context": "While most studies of disfluency detection focus on using only text transcripts as input, it is well known that prosodic cues are useful in combination with lexical cues [17, 18, 8, 12].", "startOffset": 170, "endOffset": 185}, {"referenceID": 11, "context": "While most studies of disfluency detection focus on using only text transcripts as input, it is well known that prosodic cues are useful in combination with lexical cues [17, 18, 8, 12].", "startOffset": 170, "endOffset": 185}, {"referenceID": 0, "context": "Note that having explicit repair states does not allow correction detection, as defined in [1], since the repairs associated with repetitions vs.", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "The second extension includes 17 states for joint reparandum and correction detection, expanding all non-O states to separately represent repetition and nonrepetition disfluencies, as in [1].", "startOffset": 187, "endOffset": 190}, {"referenceID": 0, "context": "Starting from a core feature set of lexical, distance-based pattern match features and disfluency language model features used in [1] (listed in Table 2), the CRF features are generated by applying feature functions provided", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "3 language model features described in [1]", "startOffset": 39, "endOffset": 42}, {"referenceID": 18, "context": "LSTMs are deep neural networks to model sequences, achieving good performance in a variety of NLP tasks [19, 20, 21].", "startOffset": 104, "endOffset": 116}, {"referenceID": 19, "context": "LSTMs are deep neural networks to model sequences, achieving good performance in a variety of NLP tasks [19, 20, 21].", "startOffset": 104, "endOffset": 116}, {"referenceID": 20, "context": "LSTMs are deep neural networks to model sequences, achieving good performance in a variety of NLP tasks [19, 20, 21].", "startOffset": 104, "endOffset": 116}, {"referenceID": 21, "context": "A typical memory cell includes gates to weigh input and history impact at a particular time, allowing the model to determine their relative importance and alleviating the vanishing gradient problem [22].", "startOffset": 198, "endOffset": 202}, {"referenceID": 22, "context": "In addition, we explore use of a bidirectional LSTM (BLSTM) [23].", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "Some of the possible approaches include LSTM-CRF [24], Markov model [16] or Integer Linear Programming (ILP) [9].", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "Some of the possible approaches include LSTM-CRF [24], Markov model [16] or Integer Linear Programming (ILP) [9].", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "Some of the possible approaches include LSTM-CRF [24], Markov model [16] or Integer Linear Programming (ILP) [9].", "startOffset": 109, "endOffset": 112}, {"referenceID": 8, "context": "In this work we use the ILP solution previously presented in [9] for disfluency detection.", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "Since [9] uses constraints for 5 states only, we collapse the softmax proportions from the larger state space to 5 states, as in", "startOffset": 6, "endOffset": 9}, {"referenceID": 24, "context": "We assess the proposed BLSTM model for disfluency detection in experiments on the Switchboard corpus of conversational speech [25], using the standard division of the disfluencyannotated subset into training, development and test sets.", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "The flattened version of repetition annotation provided in [7] is used.", "startOffset": 59, "endOffset": 62}, {"referenceID": 25, "context": "LSTM (including BLSTM) parameter optimization is done using Adadelta [26] with a minibatch size of 50.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "[11] - - 84.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] - - 84.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] (lexical) 90.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The BLSTMs also beat the higher performing version in [12] that leverages prosodic features (F=85.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "6 for the CRF result reported in [1], corresponding to a 16% improvement.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "CRF [1] 17 states 94.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "A related question is whether the pattern match features improve performance in cross-domain scenarios, as observed in [1].", "startOffset": 119, "endOffset": 122}], "year": 2016, "abstractText": "We introduce a new approach for disfluency detection using a Bidirectional Long-Short Term Memory neural network (BLSTM). In addition to the word sequence, the model takes as input pattern match features that were developed to reduce sensitivity to vocabuary size in training, which lead to improved performance over the word sequence alone. The BLSTM takes advantage of explicit repair states in addition to the standard reparandum states. The final output leverages integer linear programming to incorporate constraints of disluency structure. In experiments on the Switchboard corpus, the model achieves state-of-the-art performance for both the standard disfluency detection task and the correction detection task. Analysis shows that the model has better detection of non-repetition disfluencies, which tend to be much harder to detect.", "creator": "LaTeX with hyperref package"}}}