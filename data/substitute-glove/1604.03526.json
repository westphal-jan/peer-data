{"id": "1604.03526", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Spatiotemporal Articulated Models for Dynamic SLAM", "abstract": "We propose an online spatiotemporal specialization model estimation cooperation still reports the articulated integrated however come known a facial prediction available solely using passive sighting. The massive changed can inevitably future mera - selles of source articulated simple time at confidence because from brought regression and well-defined structure. We demonstrate the guidance of same predictive simplified by sections it within a quality halting localization and mapping (SLAM) pipeline a two-dimensional especially robot sequencer when officially unexplored dynamic terrain. Our conventional \u201d anyone failed enliven the robot brought map. dynamic incident by comments both different judges in the ever. We demonstrate after effectiveness now after proposed framework over notably long-range them might - making dynamic forests.", "histories": [["v1", "Tue, 12 Apr 2016 19:00:48 GMT  (942kb,D)", "http://arxiv.org/abs/1604.03526v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["suren kumar", "vikas dhiman", "madan ravi ganesh", "jason j corso"], "accepted": false, "id": "1604.03526"}, "pdf": {"name": "1604.03526.pdf", "metadata": {"source": "CRF", "title": "Spatiotemporal Articulated Models for Dynamic SLAM", "authors": ["Suren Kumar", "Vikas Dhiman", "Madan Ravi Ganesh", "Jason J. Corso"], "emails": ["jjcorso}@umich.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nSimultaneous localization and mapping (SLAM) has significantly improved over the last few decades, moving from sparse, 2D, slow feature-based maps [23] to dense, 3D, fast maps [16]. For example, KinectFusion [16] impressively estimates a full 6 degrees-of-freedom (DOF) camera motion while building a dense, metrically accurate 3D map of an environment in real-time [16]. However, most of these compelling developments in SLAM operate under the assumption that the environment is static; i.e., any motion in the environment is considered to be noise. Yet, most environments in our world are dynamic\u2014roadways, hospitals, factories, etc. The ability for robotic systems to deal with such motion is increasingly critical for the advancement of robotic systems used in autonomous driving [14], interaction with articulated objects [3] and tracking [21], among others.\nEarly attempts at extending SLAM to dynamic environments either explicitly track the moving objects [29], remove the moving landmarks [10] from observations, or, alternatively, build two different maps for static and dynamic parts of the environment as Wolf et al. [30] do. However, all of these methods rely strictly on the static parts of the environment to localize the robot. As a consequence, these approaches would fail to extend to environments with significant motion. Fundamentally, neglecting the dynamic part of the environment for robot localization implies that observing the motion provides no information about the location of the robot.\nOn the contrary, we observe that motion has a rich structure that can be leveraged for improving SLAM. We will use the running example of a typical, hinged door throughout this paper. The motion of this door has just a single DOF\u2014the angle about the hinge\u2014rather than the full 6 DOF. Intuitively,\nobserving the relative motion of a door constrains the possible locations of a robot, even though the door itself may be moving. More concretely, explaining away (removing) the motion of a door by explicitly modeling it provides the same constraint for the localization problem as observing a static landmark.\nTo that end, we propose a two-part method for joint articulated motion estimation and SLAM in dynamic environments. Our proposed method first estimates the articulated structure, such as, axis of the motion of a door and its motion parameters, such as, current angle of the door; and, it does so spatiotemporally in an online manner. Essentially, the first step finds the best subspace of the special euclidean manifold (SE(3)) to represent the motion of an object. For our door example, which is a 1 DOF revolute joint, the subspace is a unit circle. All such observed motion in the scene is eventually associated with motion subspace, and each such subspace further constrains the SLAM problem. This first part of our method is inspired by psychophysical experiments on human motion understanding that have demonstrated humans first distinguish between competing articulations (translation, rotation, and expansion) and then estimate the motion conditioned on the motion model [31].\nIn the second part, we build motion-continuity-based temporal prediction models to capture the temporal evolution of the motion parameters in the established subspace. To summarize, the first part finds a 1 DOF subspace of the special euclidean (SE(3)) manifold and the second part models the evolution of an object\u2019s motion in the resulting subspace. The resulting framework allows us to estimate articulated structure along with the motion propagation model and hence, allows us to include articulated motions in an online SLAM framework. Indeed, we show the application of temporal predictions of the resulting model for SLAM in dynamic environments.\nTo the best of our knowledge, this is the first method that explicitly estimates spatiotemporal articulated motion and integrates it while performing SLAM in a dynamic environment. The proposed method has two major contributions. First, we propose an online articulated structure with an explicit temporal model estimation from a low number of feature tracks. Theoretically, our method can estimate articulated structure of a prismatic or revolute articulated joint using just a single feature track. Second, we show how the spatiotemporal articulated motion prediction can be integrated within\nar X\niv :1\n60 4.\n03 52\n6v 1\n[ cs\n.R O\n] 1\n2 A\npr 2\n01 6\nSLAM to estimate the motion of the dynamic objects while building a map of an unknown dynamic environment. We evaluate the proposed framework with regards to articulation structure, temporal predictions, and SLAM capabilities in both simulation and real-life experiments."}, {"heading": "II. RELATED WORK", "text": "We review previous work related to both spatiotemporal articulated structure estimation as well as SLAM in dynamic environments."}, {"heading": "A. Articulation Structure", "text": "Articulation structure estimation methods attempt to recover the linking structure, or kinematic chain, of rigid bodies, essentially discovering the articulated joints that constrain the motion of rigid bodies to a subspace of SE(3). The early approaches to address this problem extended multibody structure from motion [4] ideas to understand articulation by clustering feature trajectories into individual rigid bodies [33]. With the introduction of the commercial depth camera, the feature trajectories could be directly represented in 3D and thus avoiding the need to estimate shape [20, 13]. However, these methods implicitly assume that large number of features trajectories are available whereas our model can estimate the articulation structure of a single trajectory.\nTo avoid the problem of dense feature tracking, some methods achieve direct sensing of articulated motion via placement of markers, such as, ARToolKit [7], checker-board markers etc. [9, 26, 25, 11]. The placement of markers removes the need for an otherwise noisy feature-tracking process from the structure estimation problem. Another way to get better estimation of articulated motion is via active interaction of a robot manipulating an articulated object [12, 11]. In contrast, our method requires no such markers nor active manipulation; in fact, using our method, a robotic system could observe a different agent manipulating the environment to infer its articulation structure.\nThese prior works in estimating articulation structure have mostly relied on collecting data from demonstrations and performing articulation estimation offline, e.g., [26, 25]. In contrast to these state-of-the-art methods, we do online articulation estimation. Online estimation not only enables evolving beliefs with more observations but also allows for inclusion in online tracking and mapping algorithms. The closest work to ours is Martin et al. [15] who propose a framework for online estimation; however, their method has no explicit probabilistic measure for model confidence to select an articulation model. Furthermore, the state-of-the-art in articulation estimation does not model the temporal evolution of motion.\nOur model directly addresses this lack of temporal modeling (for example, acceleration/deceleration of a door) in articulation estimation. We propose an explicit temporal model for each articulation type, which is necessary to make good long-term future predictions. Temporal modeling of arbitrary order allows us to: i) track new parts/objects that enter/exit the scene [15]; ii) model the entire scene and as a result\nexplore dependencies between neighboring objects; and, iii) assimilate articulated object motion in SLAM [5]. Apart from the applications presented in this paper, temporal models associated with articulated structure will help in robot-environment interaction, specifically obtaining dynamic characteristics of the objects in the environment [26]."}, {"heading": "B. SLAM in Dynamic Environments", "text": "Previous literature to handle dynamic environments can be divided into two predominant approaches: i) detect moving objects and ignore them [10], and ii) track moving objects as landmarks [2]. In the first approach, using the fact that the conventional SLAM map is highly redundant, the moving landmarks can be removed from the map building process [1]. In contrast, moving-object tracking-based approaches explicitly track moving objects by adding them to the estimation state [29]. Hahnel et al. [10] propose tracking humans in dense populated environments and removing the areas corresponding to humans to build static maps. Nieuwenhuisen et al. [18] proposed using a parametrized model of a door to localize doors in a moving environment while simultaneously estimating a static map. Recent work has focused on updating the map by removing dynamic parts of the scene without considering the nature of motion in the environment [6]. However, these methods assume that the measurement of moving objects carries no useful information for the robot localization. Furthermore, these method do not strive to build maps of dynamic environments that can be used to discover articulated structure or novel objects. Our method demonstrates the utility of estimating motion and incorporating it into SLAM in both simulated and real-world cases.\nStachniss and Burgard [24] considered a graphic model similar to ours to update the map of a dynamic environment by using local patch maps and modeling transitions between patch maps. However, such an approach is only suitable for quasi-static environments as the number of maps required would increase exponentially with the number and pose characterizations of dynamic objects in the world. A recent work has extended dense tracking and mapping to dynamic scenes by estimating a dense warp field [17]. However, the approach disregards the object level rigid nature which limits its application to topological changes, such as, open to closed door. In the context of manipulating doors, Pretrovskaya et al. [19] used a model of the door and a prior low-resolution static map of environment to track objects for manipulation tasks. In contrast, our approach does not need any prior maps of the environment or articulated objects, and does not suffer from exponential increase in the number of maps for a dynamic environment."}, {"heading": "III. SPATIOTEMPORAL ARTICULATION MODELS", "text": ""}, {"heading": "A. Articulation Models", "text": "We represent all articulated motion in the environment as\nzt = fM (C, q(t)) + \u03bd (1)\nwhere zt is the observed motion of an object (a trajectory in 3D at time t), M \u2208 {Mj}rj=1 is one of the r possible motion models, C is the configuration space (e.g. axis vector in case of a door), q(t) represents the time-varying motion variables (e.g. length of prismatic joint, angle of door) associated with the motion model M and \u03bd is the noise in observations. We assume that all observed motion can be explained by one of the pre-defined r possible motion models. Indeed, there are only finite articulated joint models in man-made environments.\nThis kind of representation, where non-time varying configuration parameters are separated from time-varying motion variables, is beneficial for a variety of reasons: First, it allows for the unified treatment of various types of articulation due to its single and consistent representation of motion variables. Second, this representation can be robustly estimated from experimental data given the reduced number of parameters to be estimated. Furthermore, it often makes the estimation problem linear, and as a consequence, convex.\nA notable omission from our modeling of articulation model, Eq. 1 is the input to the system such as torque acting on a door, force on a drawer etc. This modeling limitation is due to the passive nature of our sensing approach in addition to no prior information about the agents in the scene. To probabilistically predict motion at the next time step P (zt+1|zt) (under Markovian assumption) without modeling the input forces/torques (thus not using a dynamics model), we model the propagation of motion variables P (qt+1|qt).\nThe configuration parameters in Eq. 1 are entirely dependent on the type of articulated joint. In this section, we consider the problem of articulation identification from point correspondences over time. Rigid bodies can move in 3D space with SE(3) configuration, which is the product space of SO(3) (Rotation Group for 3D rotation) and E(3) (Translation using 3D movement). The full SE(3) has 6 DOF, which are reduced when a rigid body is connected to another rigid body via a joint. For example, the configuration space for a revolute joint (1 DOF joint) can be assumed to be a connected subset of the unit circle. Figure 1 shows some of the articulated joints modeled in this work.\nWe model the environment as collection of landmarks which can move according to three different articulation models: static, prismatic and revolute. To find a relevant subspace of SE(3), we fit a circle and line in 3D using least square formulation (optimal for Gaussian noise) to estimate the subspace corresponding to revolute and prismatic joint respectively. For the static joint, we model a perturbation from the previous position as motion variable. We provide more details on the articulated models used in the current paper in Appendix VIII-A."}, {"heading": "B. Temporal Structure", "text": "Articulation estimation provides us with configuration parameters of the articulated motion but one still needs to estimate the evolution of motion variables over time (e.g. position of the object along an axis for a prismatic joint). The goal of our approach is to enforce a structure on the evolution\nof articulated motion without using any prior information specific to the current articulated body. We take our inspiration from neuroscience literature which posits that humans produce smooth trajectories to plan movements from one point to another in environment [8]. This smoothness assumption can be leveraged by using motion models that use limited number of position derivatives.\nLet us assume that q(t) is the articulated motion variable (e.g. extension of a prismatic joint, angle of door along a hinge ). The system model for a finite order motion in discrete time domain with X(t) = [q, q(1), ., ., q(n\u22121)] (dropping the explicit time dependence of q and using superscript to denote the order of derivative) as the state can be written as,\nX(t+ 1) = AX(t) +B\u03b7\nA =  1 \u03b4t \u03b4t 2\n2 . . 0 1 \u03b4t . . 0 0 1 . . 0 0 1 . . 0 0 . . . 0 0 0 0 1 B =  \u03b4tn n! \u03b4tn\u22121 (n\u22121)! . . \u03b4t2 2! \u03b4t  (2)\nwhere q(n) denotes nth order derivative of the motion variable and \u03b7 is the noise.\nThe model considered in Eq. 2 attempts to predict the motion variable q(t + 1) using the information at time step t. It is a finite order Taylor series expansion of the motion variable q(t + 1). We initialize the state X by obtaining q(t) via inverse kinematics based on Eq. 1 and then evaluating numerical derivatives via Gaussian convolved motion parameter estimates. Our representation assumes the differentiability of the motion variable. The approximation error is of the order O((\u03b4t)n). Various convergence studies can be done to choose the right order n for a given time duration \u03b4t, but here we study the physical aspects of the problem.\na) Choosing Order: Ideally, one would want to choose the motion variable order as high as possible to reduce the approximation error in Taylor series, especially, for long-term\nbehavior prediction which is necessary for motion planning or when sensors go blind. But, higher order motion models will result in over-fitting because of the need to estimate more parameters from few initial samples. It also increases the filtering problem complexity as Kalman filtering involves matrix multiplication. Hence, the computational complexity is atleast O(N2), where N is the length of the state vector. Furthermore, the error in estimating higher-order derivatives of a noisy signal increases exponentially with respect to derivative order.\nFor the current work, we found the first order motion models to be adequate for most of the experimental use cases."}, {"heading": "IV. ARTICULATION MODEL ESTIMATION", "text": "We now consider the task of estimating the type of articulated model M \u2208 {Mj}rj=1 out of r different models. This does not automatically follow from the configuration and motion variables estimation. For example, consider the case of a point particle moving in 3D space: one can fit a line, circle or assume it to be static. One can potentially use goodnessof-fit measures to estimate the appropriate model along with some heuristics. However, there are various limitations in comparing goodness-of-fit measures related to the number of free parameters in different models, noise in the data, overfitting and the number of data samples required [22]. Instead of picking a model at the initial time-step, we use a filteringbased multiple model approach to correctly pick the model for a given object.\nWe assume that our target object/particle obeys one of the r different motion models. In the current formulation, we assume a uniform prior \u00b5j(0) = P (Mj), \u2211r j=1 \u00b5j(0) = 1, over different spatiotemporal articulation models for each individual object. This prior can be modified appropriately by object detection. For example, a door is more likely to have a revolute joint. Motion model probabilities are updated as more and more observations are received using laws of total probability [32].\n\u00b5j(t) \u2261 P (Mj |Z0:t) = P (zt|Z0:t\u22121,Mj)P (Mj |Z0:t\u22121)\nP (zt|Z0:t\u22121)\n\u00b5j(t) = P (zt|Z0:t\u22121,Mj)\u00b5j(t\u2212 1)\u2211r j=1 P (zt|Z0:t\u22121,Mj)\u00b5j(t\u2212 1)\n(3)\nThe probability of the current observation zt (the entire trajectory of observation up to time t is denoted by Z0:t\u22121) conditioned over a specific articulated motion model and all the previous observations can be represented by various methods. This probability for an Extended Kalman Filter (EKF) based filtering algorithm is the probability of observation residual as sampled from a normal distribution distributed with zero mean and innovation covariance [32]. One could pick a model by simply picking the model with maximum probability. However, in a online estimation framework, outliers might change the probability significantly in short term. We pick a model based on probabilistic threshold, which is decided based on the total number of the models r. As more and more\nobservations are received, our estimation algorithm (Algorithm 1) chooses a specific model for each target object.\nData: {Mj}rj=1, zt, \u03c4 Result: M\u0302 \u2208 {Mj}rj=1, C, P (q(t+ 1)|q(t)) initialization: Cj = {}, M = {} ; while M = {} do\nforall the M \u2208 {Mj}rj=1, do if Cj is {} then\nEstimate Cj ; Estimate Temporal Structure ;\nelse Propagate state using EKF ; Estimate P (zt|Z0:t\u22121,Mj) ;\nend end forall the M \u2208 {Mj}rj=1, do\nNormalize to obtain \u00b5j(t) ; if \u00b5j(t) > \u03c4 then\nM\u0302 = Mj end\nend end\nAlgorithm 1: Estimating the correct articulation model, associated configuration parameters, and motion variables"}, {"heading": "V. SLAM FOR DYNAMIC ENVIRONMENT", "text": "To demonstrate the need for articulation estimation with explicit time dependence, we propose an algorithm for performing SLAM in a dynamic scene. Figure 2 shows the graphical model of our general SLAM problem where xt, ut, zt, mt, vt represent the robot state, input to the robot, observation by the robot, state of the environment, and action\nof various agents in the environment, respectively. Please note that m in lower case is used to refer to the landmark while M is used to refer to the type of motion model.\nBasic SLAM algorithms assume the map mt\u22121 \u2261 mt \u2261 m to be static and model the combination of robot state and map ({xt,m}) as the state of the estimation problem [5]. The associated estimation problem only requires a motion model P (xt|xt\u22121, ut) and an observation model P (zt|xt,m). The observation model assumes the observations to be conditionally independent given the map and the current vehicle state. The goal of the estimation process is to produce unbiased and consistent estimates (the expectation of mean squared errors should match the filter-calculated covariance) [32].\nIn contrast, for the dynamic SLAM problem, the state consists of a time-varying map and the robot state. Hence, the full estimation problem is:\nP (xt,mt|Z0:t,U0:t,V0:t, x0,m0), (4)\nwhere Z0:t, U0:t and V0:t represent the set of observations, robot control inputs and map control inputs from the start time to time step k. It is assumed that the map is Markovian in nature which implies that the start state of the map m0 contains all the information needed to make future prediction if actions of various agents in the world vt\u22121, ..., vt+1 and their impact on the map are known."}, {"heading": "A. Motion Propagation", "text": "The motion propagation update models the evolution of state according to the motion model. To write the equation concisely, let A = {Z0:t\u22121,U0:t,V0:t, x0,m0}, then\nP (xt,mt|A) = \u222b \u222b P (xt, xt\u22121,mt,mt\u22121|A)dxt\u22121dmt\u22121\n= \u222b \u222b P (xt|xt\u22121, ut)P (mt|mt\u22121, vt\u22121)\nP (xt\u22121,mt\u22121|A)dxt\u22121dmt\u22121 (5)\nThe independence relationship in the derivation of the time update in Eq. 5 is due to the Bayesian network in Fig. 2 in which each node is independent of its non-descendants given the parents of that node. Given the structure of time update, we need two motion models, one for the robot, P (xt|xt\u22121, ut) and another one for the world, P (mt|mt\u22121, vt\u22121). It can be clearly observed that P (mt|mt\u22121, vt\u22121) for a static map is a Dirac Delta function and integrates out in Eq. 5."}, {"heading": "B. Measurement Update", "text": "The measurement update uses Bayes formula to update the state of the estimation problem given a new observation zt at time step t. To write the equations concisely, let B = {Z0:t,U0:t,V0:t, x0,m0}, then\nP (xt,mt|B) = P (zt|xt,mt, A)P (xt,mt|A)\nP (zt|A)\n= P (zt|xt,mt)P (xt,mt|A)\nP (zt|A) (6)\nEq. 6, together with Eq. 5, defines the complete recursive form of the SLAM algorithm for a dynamic environment. The focus of current work is the representation of nonstatic environment using articulation motion model in order to extend the standard SLAM algorithm with its static world assumption to a dynamic world."}, {"heading": "C. Dynamic World Representation", "text": "In this work, we demonstrate the SLAM algorithm for a feature based map. The overall framework is extensible to other kinds of mapping algorithms such as, dense maps. In feature based mapping, motion of each feature is assumed to be independent, given the location of the feature at the previous time step. The state of the map, mt is the collection of motion parameters of all the landmarks observed in the scene. The true motion model for each landmark in the scene is assumed to be one of the motion models M \u2208 {Mj}rj=1."}, {"heading": "VI. ARTICULATED EKF SLAM", "text": ""}, {"heading": "A. Robot Motion Model", "text": "We consider a robot with state xt = (x, y, \u03b8)T at time t moving with constant linear velocity vt and angular velocity \u03c9t. The state of the robot at the next time step can be represented as\nxt+1 = x\u2212 vt\u03c9t sin \u03b8 + vt\u03c9t sin(\u03b8 + \u03c9t\u03b4t)y + vt\u03c9t cos \u03b8 \u2212 vt\u03c9t cos(\u03b8 + \u03c9k\u03b4t) \u03b8 + \u03c9k\u03b4t +N (0, Nt), (7)\n, where \u03b4t is the width of the time step and Nt is the error covariance of the noise (zero mean Gaussian). Error covariance can be derived by propagating the noise through the robot motion model and projecting the input noise to the state space [28].\nIf the angular velocity is close to zero, the robot model as represented in Eq. 8 will be ill-conditioned. The model with zero angular velocity is given by\nxt+1 = x+ vt\u03b4t cos(\u03b8)y + vt\u03b4t sin(\u03b8) \u03b8 +N (0, Nt). (8) Following the approximations proposed by Thrun et al. [28], the angular and linear velocities are generated by a motion control unit u\u0302t = (v\u0302t, \u03c9\u0302t)T with zero mean additive Gaussian noise. (\nvt \u03c9t\n) = ( v\u0302t \u03c9\u0302t ) +N (0, St) (9)\nSt =\n( \u03b11v\u0302 2 t + \u03b12\u03c9\u0302 2 t 0\n0 \u03b13v\u0302 2 t + \u03b14\u03c9\u0302 2 t\n) (10)\nwhere \u03b11, \u03b12, \u03b13, \u03b14 are the noise coefficients."}, {"heading": "B. Observation Model", "text": "The robot uses an ASUS Xtion camera which provides depth estimates for all the features. Let the jth landmark be located at position mj = (mj,x,mj,y,mjz )\nT within robot\u2019s sensor field of view. Each observation can be written as\nzit = R T t mj,x \u2212 xmj,y \u2212 y mj,z +N (0, Qk) (11) where zit is the i\nth observation of the jth landmark at time step t disturbed by zero mean Gaussian noise with covariance matrix Qk and Rt is the rotation matrix corresponding to rotation of angle \u03b8k around z axis. It is assumed that the robot moves in the same plane and hence its position in the z axis is not a part of the state. However, our model is readily extensible to general robot motion cases."}, {"heading": "C. Jacobian Computation", "text": "Extended Kalman Filtering (EKF) requires linearization of the robot\u2019s motion model to ensure that the state propagation maintains Gaussianity of the state distribution. In order to propagate the state, we estimate the Jacobian of the state propagation model with respect to the state at time step t. The state Jacobian can be represented as\nJxt+1xt = 1 0 \u2212 vt\u03c9t cos \u03b8 + vt\u03c9t cos(\u03b8 + \u03c9t\u03b4t)0 1 \u2212 vt\u03c9t sin \u03b8 + vt\u03c9t sin(\u03b8 + \u03c9t\u03b4t) 0 0 1  (12) Furthermore, the error in the control space is projected onto the state space, for which we compute the Jacobian of the state propagation model with respect to the input ut.\nTo assimilate each observation zti , we compute the Jacobian of the observation model with respect to the overall SLAM state, consisting of the robot state as well as the motion parameters state associated with each landmark. However, for the ith observation at time step t of landmark j, the only relevant entries in the Jacobian matrix are the derivative of observation with respect to the robot states and the motion parameters state associated with landmark j.\nJ zit q(t) =  cos \u03b8 sin \u03b8 0\u2212 sin \u03b8 cos \u03b8 0 0 0 1  Jmjq(t) (13) where Jmjq(t) is the Jacobian of landmark observation with respect to the motion parameters state q(t) which is calculated based on the articulated motion models as described in Section VIII-A.\nWe summarize the entire articulated EKF SLAM approach to dynamic environments in Algorithm 2."}, {"heading": "VII. EXPERIMENTS", "text": "We analyze components of the proposed spatiotemporal articulation estimation framework and also evaluate its predictive performance to explore dynamic environments."}, {"heading": "A. Articulated Structure", "text": "To elucidate the effectiveness of separating motion parameters from structure parameters, we consider the structure estimation of a 2D landmark undergoing revolute motion. In the current case, articulation structure refers to the center location and radius of the landmark, while the motion parameter is \u03b8, the time-varying angle. For joint estimation, we choose a commonly used constant-velocity motion model,\nzt = Xc + r[cos(\u03b80 + \u03b4T\u03c9), sin(\u03b80 + \u03b4T\u03c9)] T (14)\nwhere zt is the observed landmark position, \u03b80 is the initial angle, Xc is the landmark\u2019s center location and \u03c9 is the constant angular velocity of the point. The estimation problem resulting from Eq. 14 is non-linear in \u03b80 and \u03c9. On the other hand, the proposed articulated structure estimation approach solves a well-conditioned problem of estimating a circle from points lying on a circle. We performed a Monte Carlo simulation and averaged errors across all the trials. Figure 3 shows the estimation results using least squares fitting between observed and predicted values of the landmark for one specific run. Table I shows results from 500 Monte Carlo runs of both methods. It can be observed that separating structure estimation from temporal modeling has significantly less error compared to joint estimation problem."}, {"heading": "B. Temporal Order", "text": "After the evaluation of structure parameters, various orders of temporal models can be estimated using the approach outlined in Section III-B. To evaluate performance of various motion continuity orders, we obtain raw angular trajectory of a spring-loaded door and fit zeroth, first, and second order temporal models. We use the temporal model in a EKF filtering framework with direct sensing of motion parameter as observation model. This is equivalent to observation of the landmark zt after the articulated structure is estimated mapped via inverse kinematics. Figure 4 shows the motion parameter for different orders. It can be observed that increasing the\norder adds further flexibility. We choose a first order continuity temporal model for rest of the experiments."}, {"heading": "C. Articulation Estimation", "text": "To test the articulation estimation framework, we simulated an environment with one static, prismatic and revolute points each. We used a minimum of 7 samples to estimate configuration and initialize motion parameters. Figure 5 shows the results for the articulation estimation. Given sufficient observation, all the articulation models are estimated correctly. However, static articulation takes the longest time to be correctly estimated. This is because of the difficulty in separating static landmark from a revolute and prismatic landmark with zero velocities."}, {"heading": "D. Dynamic World SLAM", "text": "In order to test our dynamic SLAM framework, we simulated a map with landmarks that are either static, prismatic or\nrevolute. A robot with limited field of view (90 degree cone of radius 4 meters) simulated readings from a depth sensor which were then used to simultaneously localize the robot as well as to map the environment. We used a total of 42 landmarks in the environment with 1 revolute, 1 prismatic and 40 static landmarks. Both, the revolute and prismatic landmarks were correctly identified with a threshold \u03c4 = 0.6. Figure 6 shows the robot localization using both the standard EKF slam algorithm, a dynamic variant of standard EKF slam based on xt = xt\u22121 + (xt\u22121 \u2212 xt\u22122) and the articulated EKF (A-SLAM) algorithm.\nTable II compares standard EKF SLAM algorithm and its dynamic variant to the proposed A-SLAM algorithm. However, the standard EKF SLAM algorithm includes the landmark position in the state as opposed to to our algorithm where the SLAM state includes the motion parameters. As a result, we only compare the resulting localization estimates of the robot using community accepted error metrics [27]. It is evident from the results that articulated structure estimation significantly improves robot localization error. Notably, we obtain such improvements in performance despite having two non-static landmarks which violates the assumption of standard SLAM algorithms.\nWe also demonstrate our articulation model selection algorithm on the 3 different landmarks as visualized in Figure 6. The proposed algorithm needs a minimum of 7 samples to fit\narticulated structure and initialize temporal modeling. From the prismatic joint selection (Fig 7 (b)), it can be observed that a maximum probability selection approach would fail at frame number 13.\nE. Indoor Objects\nWe acquired a RGBD video of an indoor scene with a refrigerator door being articulated. We detect and track features by detecting good features to track and match them in consecutive frames, using a pipeline similar to the one proposed by Pillai et al. [20]. We use the proposed articulation estimation framework and obtain the refrigerator axis by clustering all the features that are classified as revolute. The visual result is illustrated in Figure 8 which demonstrates correct prediction of the articulated axis. Further, we present a comparison of localization estimates , similar to the previous experiment, in\nTable III."}, {"heading": "VIII. CONCLUSION", "text": "We presented a principled approach to articulated structure estimation, which is essential for articulated motion representation in unexplored environments. Our spatiotemporal articulation models can be estimated by tracking a single landmark for 1-DOF joint. The proposed framework also presents a passive approach to model temporal motion propagation. We demonstrated that our approach outperforms the traditional SLAM algorithms by integrating articulated structure estimation. Natural extensions of the current framework will be to model rigid bodies and articulated chains.\nAPPENDIX"}, {"heading": "A. Articulated Motion Models", "text": "We briefly describe the articulated structure and motion parameters of the articulated joints modeled in this paper.\n1) Revolute Joint: For a landmark moving according to revolute motion, the articulation structure consists of the plane of motion P , location of center and radius of the circle in that plane. The motion parameter to characterize the motion is simply the angle \u03c6t of the current point with respect to the horizontal. The equation of motion can be written as\nmj = (x0 + r cos\u03c6t)v1 + (y0 + r sin\u03c6t)v2 + P0, (15)\nwhere mj is the current location of j the landmark. v1, v2 and P0 are two perpendicular vectors in the plane and a point on the plane P respectively. r is the radius of the circle and \u03b8 is the angle with respect to horizontal. To estimate the articulated structure, we first fit a plane in 3D and then fit a circle to the projection of points on that plane using least squares.\n2) Prismatic Joint: Articulated structure estimation of a prismatic joint simply involves fitting a line to the landmark positions over time. The equation of motion is given by\nmj = ltn\u0302, (16)\nwhere lt is the time-varying motion parameter that measures length along a line represented by n\u0302.\n3) Static Joint: For a static joint, we simply take the position of the static joint P0 as the structure and the motion variables models a perturbation about that location. The equation of motion is given by\nmj = P0 + [1, 1, 1] T lt, (17)\nwhere lt is the perturbation of the landmark at time step t around the static location."}], "references": [{"title": "Mobile robot localisation and mapping in extensive outdoor environments", "author": ["Tim Bailey"], "venue": "PhD thesis, Citeseer,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Simultaneous localization and mapping (slam): Part ii", "author": ["Tim Bailey", "Hugh Durrant-Whyte"], "venue": "IEEE RA Magazine,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Whole-body motion planning for manipulation of articulated objects", "author": ["Felix Burget", "Anja Hornung", "Maren Bennewitz"], "venue": "In ICRA. IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A multibody factorization method for independently moving objects", "author": ["Jo\u00e3o Paulo Costeira", "Takeo Kanade"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Simultaneous localization and mapping: part i", "author": ["Hugh Durrant-Whyte", "Tim Bailey"], "venue": "IEEE RA Magazine,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Dynamic obstacles detection and 3d map updating", "author": ["Federico Ferri", "Mario Gianni", "Matteo Menna", "Fiora Pirri"], "venue": "In IROS. IEEE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Comparing artag and artoolkit plus fiducial marker systems", "author": ["Mark Fiala"], "venue": "In Haptic Audio Visual Environments and their Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "The coordination of arm movements: an experimentally confirmed mathematical model", "author": ["Tamar Flash", "Neville Hogan"], "venue": "The journal of Neuroscience,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1985}, {"title": "A single planner for a composite task of approaching, opening and navigating through nonspring and spring-loaded doors", "author": ["Steven Gray", "Subhashini Chitta", "Vipin Kumar", "Maxim Likhachev"], "venue": "In ICRA. IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Mobile robot mapping in populated environments", "author": ["Dirk H\u00e4hnel", "Dirk Schulz", "Wolfram Burgard"], "venue": "Advanced Robotics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Active articulation model estimation through interactive perception", "author": ["Karol Hausman", "Scott Niekum", "Sarah Osentoski", "G Sukhatme"], "venue": "In ICRA,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Extracting planar kinematic models using interactive perception", "author": ["Dov Katz", "Oliver Brock"], "venue": "In Unifying Perspec-  tives in Computational and Robot Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Interactive segmentation, tracking, and kinematic modeling of unknown 3d articulated objects", "author": ["Dov Katz", "Moslem Kazemi", "J. Andrew (Drew) Bagnell", "Anthony (Tony) Stentz"], "venue": "In ICRA,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Towards fully autonomous driving: Systems and algorithms", "author": ["Jesse Levinson", "Jake Askeland", "Jan Becker", "Jennifer Dolson", "David Held", "Soeren Kammel", "J Zico Kolter", "Dirk Langer", "Oliver Pink", "Vaughan Pratt"], "venue": "In Intelligent Vehicles Symposium (IV),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Online interactive perception of articulated objects with multi-level recursive estimation based on task-specific priors", "author": ["Roberto Martin Martin", "Oliver Brock"], "venue": "In IROS. IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Kinectfusion: Real-time dense surface mapping and tracking", "author": ["Richard A Newcombe", "Shahram Izadi", "Otmar Hilliges", "David Molyneaux", "David Kim", "Andrew J Davison", "Pushmeet Kohi", "Jamie Shotton", "Steve Hodges", "Andrew Fitzgibbon"], "venue": "In Mixed and augmented reality (ISMAR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time", "author": ["Richard A Newcombe", "Dieter Fox", "Steven M Seitz"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Improving indoor navigation of autonomous robots by an explicit representation of doors", "author": ["Matthias Nieuwenhuisen", "J\u00f6rg St\u00fcckler", "Sven Behnke"], "venue": "In ICRA. IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Probabilistic mobile manipulation in dynamic environments, with application to opening doors", "author": ["Anna Petrovskaya", "Andrew Y Ng"], "venue": "In IJCAI,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Learning articulated motions from visual demonstration", "author": ["Sudeep Pillai", "Matthew Walter", "Seth Teller"], "venue": "In RSS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Dart: Dense articulated real-time tracking", "author": ["Tanner Schmidt", "Richard Newcombe", "Dieter Fox"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Evaluating goodness-of-fit in comparison of models to data. Psychologie der Kognition: Reden and vortr\u00e4ge anl\u00e4sslich der emeritierung von Werner Tack", "author": ["Christian D Schunn", "Dieter Wallach"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "On the representation and estimation of spatial uncertainty", "author": ["Randall C Smith", "Peter Cheeseman"], "venue": "IJRR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1986}, {"title": "Mobile robot mapping and localization in non-static environments", "author": ["Cyrill Stachniss", "Wolfram Burgard"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Learning kinematic models of articulated objects. In Approaches to Probabilistic Model Learning for Mobile Manipulation Robots, pages 65\u2013111", "author": ["J\u00fcrgen Sturm"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "A  probabilistic framework for learning kinematic models of articulated objects", "author": ["J\u00fcrgen Sturm", "Cyrill Stachniss", "Wolfram Burgard"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "A benchmark for the evaluation of rgb-d slam systems", "author": ["J\u00fcrgen Sturm", "Nikolas Engelhard", "Felix Endres", "Wolfram Burgard", "Daniel Cremers"], "venue": "In IROS. IEEE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Online simultaneous localization and mapping with detection and tracking of moving objects: Theory and results from a ground vehicle in crowded urban areas", "author": ["Chieh-Chih Wang", "Charles Thorpe", "Sebastian Thrun"], "venue": "In ICRA. IEEE,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "Mobile robot simultaneous localization and mapping in dynamic environments", "author": ["Denis F Wolf", "Gaurav S Sukhatme"], "venue": "Autonomous Robots,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Model selection and velocity estimation using novel priors for motion patterns", "author": ["Shuang Wu", "Hongjing Lu", "Alan L Yuille"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Estimation with applications to tracking and navigation", "author": ["Bar-Shalom Yaakov", "XR Li", "Kirubarajan Thiagalingam"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Automatic kinematic chain building from feature trajectories of articulated objects", "author": ["Jingyu Yan", "M. Pollefeys"], "venue": "In CVPR,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}], "referenceMentions": [{"referenceID": 22, "context": "Simultaneous localization and mapping (SLAM) has significantly improved over the last few decades, moving from sparse, 2D, slow feature-based maps [23] to dense, 3D, fast maps [16].", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "Simultaneous localization and mapping (SLAM) has significantly improved over the last few decades, moving from sparse, 2D, slow feature-based maps [23] to dense, 3D, fast maps [16].", "startOffset": 176, "endOffset": 180}, {"referenceID": 15, "context": "For example, KinectFusion [16] impressively estimates a full 6 degrees-of-freedom (DOF) camera motion while building a dense, metrically accurate 3D map of an environment in real-time [16].", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "For example, KinectFusion [16] impressively estimates a full 6 degrees-of-freedom (DOF) camera motion while building a dense, metrically accurate 3D map of an environment in real-time [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "The ability for robotic systems to deal with such motion is increasingly critical for the advancement of robotic systems used in autonomous driving [14], interaction with articulated objects [3] and tracking [21], among others.", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "The ability for robotic systems to deal with such motion is increasingly critical for the advancement of robotic systems used in autonomous driving [14], interaction with articulated objects [3] and tracking [21], among others.", "startOffset": 191, "endOffset": 194}, {"referenceID": 20, "context": "The ability for robotic systems to deal with such motion is increasingly critical for the advancement of robotic systems used in autonomous driving [14], interaction with articulated objects [3] and tracking [21], among others.", "startOffset": 208, "endOffset": 212}, {"referenceID": 27, "context": "Early attempts at extending SLAM to dynamic environments either explicitly track the moving objects [29], remove the moving landmarks [10] from observations, or, alternatively, build two different maps for static and dynamic parts of the environment as Wolf et al.", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "Early attempts at extending SLAM to dynamic environments either explicitly track the moving objects [29], remove the moving landmarks [10] from observations, or, alternatively, build two different maps for static and dynamic parts of the environment as Wolf et al.", "startOffset": 134, "endOffset": 138}, {"referenceID": 28, "context": "[30] do.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "This first part of our method is inspired by psychophysical experiments on human motion understanding that have demonstrated humans first distinguish between competing articulations (translation, rotation, and expansion) and then estimate the motion conditioned on the motion model [31].", "startOffset": 282, "endOffset": 286}, {"referenceID": 3, "context": "The early approaches to address this problem extended multibody structure from motion [4] ideas to understand articulation by clustering feature trajectories into individual rigid bodies [33].", "startOffset": 86, "endOffset": 89}, {"referenceID": 31, "context": "The early approaches to address this problem extended multibody structure from motion [4] ideas to understand articulation by clustering feature trajectories into individual rigid bodies [33].", "startOffset": 187, "endOffset": 191}, {"referenceID": 19, "context": "With the introduction of the commercial depth camera, the feature trajectories could be directly represented in 3D and thus avoiding the need to estimate shape [20, 13].", "startOffset": 160, "endOffset": 168}, {"referenceID": 12, "context": "With the introduction of the commercial depth camera, the feature trajectories could be directly represented in 3D and thus avoiding the need to estimate shape [20, 13].", "startOffset": 160, "endOffset": 168}, {"referenceID": 6, "context": "To avoid the problem of dense feature tracking, some methods achieve direct sensing of articulated motion via placement of markers, such as, ARToolKit [7], checker-board markers etc.", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "[9, 26, 25, 11].", "startOffset": 0, "endOffset": 15}, {"referenceID": 25, "context": "[9, 26, 25, 11].", "startOffset": 0, "endOffset": 15}, {"referenceID": 24, "context": "[9, 26, 25, 11].", "startOffset": 0, "endOffset": 15}, {"referenceID": 10, "context": "[9, 26, 25, 11].", "startOffset": 0, "endOffset": 15}, {"referenceID": 11, "context": "Another way to get better estimation of articulated motion is via active interaction of a robot manipulating an articulated object [12, 11].", "startOffset": 131, "endOffset": 139}, {"referenceID": 10, "context": "Another way to get better estimation of articulated motion is via active interaction of a robot manipulating an articulated object [12, 11].", "startOffset": 131, "endOffset": 139}, {"referenceID": 25, "context": ", [26, 25].", "startOffset": 2, "endOffset": 10}, {"referenceID": 24, "context": ", [26, 25].", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": "[15] who propose a framework for online estimation; however, their method has no explicit probabilistic measure for model confidence to select an articulation model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Temporal modeling of arbitrary order allows us to: i) track new parts/objects that enter/exit the scene [15]; ii) model the entire scene and as a result explore dependencies between neighboring objects; and, iii) assimilate articulated object motion in SLAM [5].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "Temporal modeling of arbitrary order allows us to: i) track new parts/objects that enter/exit the scene [15]; ii) model the entire scene and as a result explore dependencies between neighboring objects; and, iii) assimilate articulated object motion in SLAM [5].", "startOffset": 258, "endOffset": 261}, {"referenceID": 25, "context": "Apart from the applications presented in this paper, temporal models associated with articulated structure will help in robot-environment interaction, specifically obtaining dynamic characteristics of the objects in the environment [26].", "startOffset": 232, "endOffset": 236}, {"referenceID": 9, "context": "Previous literature to handle dynamic environments can be divided into two predominant approaches: i) detect moving objects and ignore them [10], and ii) track moving objects as landmarks [2].", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "Previous literature to handle dynamic environments can be divided into two predominant approaches: i) detect moving objects and ignore them [10], and ii) track moving objects as landmarks [2].", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "In the first approach, using the fact that the conventional SLAM map is highly redundant, the moving landmarks can be removed from the map building process [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 27, "context": "In contrast, moving-object tracking-based approaches explicitly track moving objects by adding them to the estimation state [29].", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "[10] propose tracking humans in dense populated environments and removing the areas corresponding to humans to build static maps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] proposed using a parametrized model of a door to localize doors in a moving environment while simultaneously estimating a static map.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Recent work has focused on updating the map by removing dynamic parts of the scene without considering the nature of motion in the environment [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 23, "context": "Stachniss and Burgard [24] considered a graphic model similar to ours to update the map of a dynamic environment by using local patch maps and modeling transitions between patch maps.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "A recent work has extended dense tracking and mapping to dynamic scenes by estimating a dense warp field [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "[19] used a model of the door and a prior low-resolution static map of environment to track objects for manipulation tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We take our inspiration from neuroscience literature which posits that humans produce smooth trajectories to plan movements from one point to another in environment [8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 21, "context": "However, there are various limitations in comparing goodness-of-fit measures related to the number of free parameters in different models, noise in the data, overfitting and the number of data samples required [22].", "startOffset": 210, "endOffset": 214}, {"referenceID": 30, "context": "Motion model probabilities are updated as more and more observations are received using laws of total probability [32].", "startOffset": 114, "endOffset": 118}, {"referenceID": 30, "context": "This probability for an Extended Kalman Filter (EKF) based filtering algorithm is the probability of observation residual as sampled from a normal distribution distributed with zero mean and innovation covariance [32].", "startOffset": 213, "endOffset": 217}, {"referenceID": 4, "context": "Basic SLAM algorithms assume the map mt\u22121 \u2261 mt \u2261 m to be static and model the combination of robot state and map ({xt,m}) as the state of the estimation problem [5].", "startOffset": 161, "endOffset": 164}, {"referenceID": 30, "context": "The goal of the estimation process is to produce unbiased and consistent estimates (the expectation of mean squared errors should match the filter-calculated covariance) [32].", "startOffset": 170, "endOffset": 174}, {"referenceID": 26, "context": "As a result, we only compare the resulting localization estimates of the robot using community accepted error metrics [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "mj = P0 + [1, 1, 1] T lt, (17)", "startOffset": 10, "endOffset": 19}, {"referenceID": 0, "context": "mj = P0 + [1, 1, 1] T lt, (17)", "startOffset": 10, "endOffset": 19}, {"referenceID": 0, "context": "mj = P0 + [1, 1, 1] T lt, (17)", "startOffset": 10, "endOffset": 19}], "year": 2010, "abstractText": "We propose an online spatiotemporal articulation model estimation framework that estimates both articulated structure as well as a temporal prediction model solely using passive observations. The resulting model can predict future motions of an articulated object with high confidence because of the spatial and temporal structure. We demonstrate the effectiveness of the predictive model by incorporating it within a standard simultaneous localization and mapping (SLAM) pipeline for mapping and robot localization in previously unexplored dynamic environments. Our method is able to localize the robot and map a dynamic scene by explaining the observed motion in the world. We demonstrate the effectiveness of the proposed framework for both simulated and real-world dynamic environments.", "creator": "LaTeX with hyperref package"}}}