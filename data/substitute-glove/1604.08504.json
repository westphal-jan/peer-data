{"id": "1604.08504", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2016", "title": "Detecting \"Smart\" Spammers On Social Network: A Topic Model Approach", "abstract": "Spammer detection after environment network is a nonetheless failure. The insulated extremist - ads implemented none sustained new emergence result \" make \" spammers. They resemble self mail friends although cannot to instance. In because that, does alone into novel 240-volt rankings approach based on Latent Dirichlet Allocation (LDA ), its familiar model. Our approach extracts addition the some took same ongoing security brought topic furthermore observed, such capture the virtue of counterpropaganda. Tested only put jumped sub-system made one self - collected predefined, your considering method outshines included declared - of - the - painter methods since beyond and 7.2 F1 - straight.", "histories": [["v1", "Thu, 28 Apr 2016 16:36:35 GMT  (328kb,D)", "http://arxiv.org/abs/1604.08504v1", "NAACL-HLT 2016, Student Research Workshop"], ["v2", "Thu, 9 Jun 2016 06:50:36 GMT  (154kb,D)", "http://arxiv.org/abs/1604.08504v2", "NAACL-HLT 2016, Student Research Workshop"]], "COMMENTS": "NAACL-HLT 2016, Student Research Workshop", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["linqing liu", "yao lu", "ye luo", "renxian zhang", "laurent itti", "jianwei lu"], "accepted": true, "id": "1604.08504"}, "pdf": {"name": "1604.08504.pdf", "metadata": {"source": "CRF", "title": "Detecting \"Smart\" Spammers On Social Network: A Topic Model Approach", "authors": ["Linqing Liu", "Yao Lu", "Ye Luo", "Renxian Zhang", "Laurent Itti", "Jianwei Lu"], "emails": ["likicode@gmail.com,", "95luyao@tongji.edu.cn,", "rxzhang@tongji.edu.cn,", "kennyluo2008@hotmail.com,", "jwlu33@hotmail.com,", "itti@usc.edu"], "sections": [{"heading": "1 Introduction", "text": "Microblogging such as Twitter and Weibo is a popular social networking service, which allows users to post messages up to 140 characters. There are millions of active users on the platform who stay connected with friends. Unfortunately, spammers also use it as a tool to post malicious links, send unsolicited messages to legitimate users, etc. A certain amount of spammers could sway the public opinion and cause distrust of the social platform. Despite the use of rigid anti-spam rules, human-like spammers whose homepages having photos, detailed profiles etc. have emerged. Unlike previous \"simple\" spammers, whose tweets contain only malicious links, those \"smart\" spammers are more difficult to distinguish from legitimate users via content-based features alone (Ferrara et al., 2014).\n\u2217Corresponding Author\nThere is a considerable amount of previous work on spammer detection on social platforms. Researcher from Twitter Inc. (Chu et al., 2010) collect bot accounts and perform analysis on the user behavior and user profile features. Lee et al. (2011) use the so-called social honeypot by alluring social spammers\u2019 retweet to build a benchmark dataset, which has been extensively explored in our paper. Some researchers focus on the clustering of urls in tweets and network graph of social spammers (Yang et al., 2012; Wang et al., 2015; Wang, 2010; Yang et al., 2011), showing the power of social relationship features.As for content information modeling, (Hu et al., 2013) apply improved sparse learning methods. However, few studies have adopted topic-based features. Some researchers (Liu et al., 2014) discuss topic characteristics of spamming posts, indicating that spammers are highly likely to dwell on some certain topics such as promotion. But this may not be applicable to the current scenario of smart spammers.\nIn this paper, we propose an efficient feature extraction method. In this method, two new topicbased features are extracted and used to discriminate human-like spammers from legitimate users. We consider the historical tweets of each user as a document and use the Latent Dirichlet Allocation (LDA) model to compute the topic distribution for each user. Based on the calculated topic probability, two topic-based features, the Local Outlier Standard Score (LOSS) which captures the user\u00e2A\u0306Z\u0301s interests on different topics and the Global Outlier Standard Score (GOSS) which reveals the user\u00e2A\u0306Z\u0301s interests on specific topic in comparison with other users\u00e2A\u0306Z\u0301,\nar X\niv :1\n60 4.\n08 50\n4v 1\n[ cs\n.C L\n] 2\n8 A\npr 2\nare extracted. The two features contain both local and global information, and the combination of them can distinguish human-like spammers effectively.\nTo the best of our knowledge, it is the first time that features based on topic distributions are used in spammer classification. Experimental results on one public dataset and one self-collected dataset further validate that the two sets of extracted topicbased features get excellent performance on humanlike spammer classification problem compared with other state-of-the-art methods. In addition, we build a Weibo dataset, which contains both legitimate users and spammers.\nTo summarize, our major contributions are twofold:\n\u2022 We extract topic-based features (GOSS and LOSS) for spammer detection, which outperform state-of-the-art methods. \u2022 We build a dataset of Chinese microblogs for\nspammer detection.\nIn the following sections, we first propose the topic-based features extraction method in Section 2, and then introduce the two datasets in Section 3. Experimental results are discussed in Section 4, and we conclude the paper in Section 5. Future work is presented in Section 6."}, {"heading": "2 Methodology", "text": "In this section, we first provide some observations we obtained after carefully exploring the social network, then the LDA model is introduced. Based on the LDA model, the ways to obtain the topic probability vector for each user and the two topic-based features are provided."}, {"heading": "2.1 Observation", "text": "After exploring the homepages of a substantial number of spammers, we have two observations. 1) social spammers can be divided into two categories. One is content polluters, and their tweets are all about certain kinds of advertisement and campaign. The other is fake accounts, and their tweets resemble legitimate users\u00e2A\u0306Z\u0301 but it seems they are simply random copies of others to avoid being detected by anti-spam rules. 2) For legitimate users, content polluters and fake accounts, they show different patterns on topics which interest them.\n\u2022 Legitimate users mainly focus on limited topics which interest him. They seldom post contents unrelated to their concern.\n\u2022 Content polluters concentrate on certain topics.\n\u2022 Fake accounts focus on a wide range of topics due to random copying and retweeting of other users\u00e2A\u0306Z\u0301 tweets.\n\u2022 Spammers and legitimate users show different interests on some topics e.g. commercial, weather, etc.\nTo better illustrate our observation, Figure. 1 shows the topic distribution of spammers and legitimate users in two employed datasets(the Honeypot dataset and Weibo dataset). We can see that on both topics (topic-3 and topic-11) there exists obvious difference between the red bars and green bars, representing spammers and legitimate users. On the Honeypot dataset, spammers have a narrower shape of distribution (the outliers on the red bar tail are not counted) than that of legitimate users. This is because there are more content polluters than fake accounts. In other word, spammers in this dataset tend to concentrate on limited topics. While on the Weibo dataset, fake accounts who are interested in different topics take large proportion of spammers. Their distribution is more flat (i.e. red bars) than that of the legitimate users. Therefore we can detect spammers by means of the difference of their topic distribution patterns."}, {"heading": "2.2 LDA model", "text": "Blei et al.(2003) first presented Latent Dirichlet Allocation(LDA) as an example of topic model.\nEach document i is deemed as a bag of words W = {wi1, wi2, ..., wiM} and M is the number of words. Each word is attributable to one of the document\u2019s topics Z = {zi1, zi2, ..., ziK} and K is the number of topics. \u03c8k is a multinomial distribution over words for topic k. \u03b8i is another multinomial distribution over topics for document i. The smoothed generative model is illustrated in Figure. 2. \u03b1 and \u03b2 are hyper parameter that affect scarcity of the document-topic and topic-word distributions. In this paper, \u03b1, \u03b2 and K are empirically set to 0.3, 0.01 and 15. The entire content of each Twitter user is regarded as one document. We adopt Gibbs Sampling (Griffiths and Steyvers, 2004) to speed up the inference of LDA. Based on LDA, we can get the topic probabilities for all users in the employed dataset as: X = [Xi;X2; \u00b7 \u00b7 \u00b7 ;Xn] \u2208 Rn\u00d7K , where n is the number of users. Each element Xi = [p (z1) p (z2) \u00b7 \u00b7 \u00b7 p (zK)] \u2208 R1\u00d7K is a topic probability vector for the ith document. Xi is the raw topic probability vector and our features are developed on top of it."}, {"heading": "2.3 Topic-based Features", "text": "Using the LDA model, each person in the dataset is with a topic probability vector Xi. Assume xik \u2208 Xi denotes the likelihood that the ith tweet account favors kth topic in the dataset. Our topic based features can be calculated as below.\nGlobal Outlier Standard Score measures the degree that a user\u2019s tweet content is related to a certain topic compared to the other users. Specifically, the \"GOSS\" score of user i on topic k can be calculated as Eq.(1):\n\u00b5 (xk) = \u2211n i=1 xik n ,\nGOSS (xik) = xik\u2212\u00b5(xk)\u221a\u2211 i (xik\u2212\u00b5(xk))2 . (1)\nThe value of GOSS (xik) indicates the interest-\ning degree of this person to the kth topic. Specifically, if GOSS (xik) > GOSS (xjk), it means that the ith person has more interest in topic k than the jth person. If the value GOSS (xik) is extremely high or low, the ith person showing extreme interest or no interest on topic k which will probably be a distinctive pattern in the fowllowing classfication. Therefore, the topics interested or disliked by the ith person can be manifested by f iGOSS = [GOSS(xi1) \u00b7 \u00b7 \u00b7GOSS(xiK)], from which the pattern of the interested topics with regarding to this person is found. Denote f iGOSS = [GOSS(xi1) \u00b7 \u00b7 \u00b7GOSS(xiK)] our first topic-based feature, and it hopefully can get good performance on spammer detection.\nLocal Outlier Standard Score measures the degree of interest someone shows to a certain topic by considering his own homepage content only. For instance, the \"LOSS\" score of account i on topic k can be calculated as Eq.( 2):\n\u00b5 (xi) = \u2211K k=1 xik K ,\nLOSS (xik) = xik\u2212\u00b5(xi)\u221a\u2211\nk\n(xik\u2212\u00b5(xi))2 . (2)\n\u00b5(xi) represents the averaged interesting degree for all topics with regarding to ith user and his tweet content. Similarly to GOSS, the topics interested or disliked by the ith person via considering his single post information can be manifested by f iLOSS = [LOSS(xi1) \u00b7 \u00b7 \u00b7LOSS(xiK)], and LOSS becomes our second topic-based features for the ith person."}, {"heading": "3 Dataset", "text": "We use one public dataset Social Honeypot dataset and one self-collected dataset Weibo dataset to validate the effectiveness of our proposed features.\nSocial Honeypot Dataset: Lee et al. (2010) created and deployed 60 seed social accounts on Twitter to attract spammers by reporting back what accounts interact with them. They collected 19,276 legitimate users and 22,223 spammers in their datasets along with their tweet content in 7 months. This is our first test dataset.\nOur Weibo Dataset: Sina Weibo is one of the most famous social platforms in China. It has implemented many features from Twitter. The 2197 legitimate user accounts in this dataset are provided\nby the Tianchi Competition1 held by Sina Weibo. The spammers are all purchased commercially from multiple vendors on the Internet. We checked them manually and collected 802 suitable \"smart\" spammers accounts.\nPreprocessing: Before directly performing the experiments on the employed datasets, we first delete some accounts with few posts in the two employed since the number of tweets is highly indicative of spammers. For the English Honeypot dataset, we remove stopwords, punctuations, nonASCII words and apply stemming. For the Chinese Weibo dataset, we perform segmentation with \"Jieba\"2, a Chinese text segmentation tool. After preprocessing steps, the Weibo dataset contains 2197 legitimate users and 802 spammers, and the honeypot dataset contains 2218 legitimate users and 2947 spammers. It is worth mentioning that the Honeypot dataset has been slashed because most of the Twitter accounts only have limited number of posts, which are not enough to show their interest inclination.\n1Tianchi Site http://tianchi.aliyun.com 2Jieba Project Page https://github.com/fxsjy/jieba"}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Evaluation Metrics", "text": "The evaluating indicators in our model are show in Table 2 . We calculate precision, recall and F1-score (i.e. F1 score) as in Eq. (3). Precision is the ratio of selected accounts that are spammers. Recall is the ratio of spammers that are detected so. F1-score is the harmonic mean of precision and recall.\nprecision = TP\nTP + FP , recall =\nTP\nTP + FN\nF1\u2212 score = 2\u00d7 precision\u00d7 recall precision+ recall (3)"}, {"heading": "4.2 Performance Comparisons with Baseline", "text": "Three baseline classification methods: Support Vector Machines (SVM), Adaboost, and Random Forests are adopted to evaluate our extracted features. We test each classification algorithm with scikit-learn (Pedregosa et al., 2011) and run a 10- fold cross validation. On each dataset, the em-\nployed classifiers are trained with individual feature first, and then with the combination of the two features. From Table 1, we can see that GOSS+LOSS achieves the best performance on F1-score among all others. Besides, the classification by combination of LOSS and GOSS can increase accuracy by more than 3% compared with raw topic distribution probability."}, {"heading": "4.3 Comparison with Other Features", "text": "To compare our extracted features with previously used features for spammer detection, we use three most discriminative feature sets according to Lee et al. (2011)(Table 4). Two classifiers (Adaboost and SVM) are selected to conduct feature performance comparisons. Using Adaboost, our LOSS+GOSS features outperform all other features except for UFN which is 2% higher than ours with regard to precision on the Honeypot dataset. It is caused by the incorrectly classified spammers who are mostly news source after our manual check. They keep posting all kinds of news pieces covering diverse topics, which is similar to the behavior of fake accounts. However, UFN based on friendship networks is more useful for public accounts who possess large number of followers. The best recall value of our LOSS+GOSS features using SVM is up to 6% higher than the results by other feature groups. Regarding F1-score, our features outperform all other features. To further show the advantages of our proposed features, we compare our combined LOSS+GOSS with the combination of all the features from Lee et al. (2011) (UFN+UC+UH). It\u2019s obvious that LOSS+GOSS have a great advantage over UFN+UC+UH in terms of recall and F1score. Moreover, by combining our LOSS+GOSS features and UFN+UC+UH features together, we\nobtained another 7.1% and 2.3% performance gain with regard to precision and F1-score by Adaboost. Though there is a slight decline in terms of recall. By SVM, we get comparative results on recall and F1-score but about 3.5% improvement on precision."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a novel feature extraction method to effectively detect \"smart\" spammers who post seemingly legitimate tweets and are thus difficult to identify by existing spammer classification methods. Using the LDA model, we obtain the topic probability for each Twitter user. By utilizing the topic probability result, we extract our two topicbased features: GOSS and LOSS which represent the account with global and local information. Experimental results on a public dataset and a self-built Chinese microblog dataset validate the effectiveness of the proposed features."}, {"heading": "6 Future Work", "text": "In future work, the combination method of local and global information can be further improved to maximize their individual strengths. We will also apply decision theory to enhancing the performance of our proposed features. Moreover, we are also building larger datasets on both Twitter and Weibo to validate our method. Moreover, larger datasets on both Twitter and Weibo will be built to further validate our method."}], "references": [{"title": "Who is tweeting on twitter: human, bot, or cyborg", "author": ["Chu et al.2010] Zi Chu", "Steven Gianvecchio", "Haining Wang", "Sushil Jajodia"], "venue": "In Proceedings of the 26th annual computer security applications conference,", "citeRegEx": "Chu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2010}, {"title": "Finding scientific topics", "author": ["Griffiths", "Steyvers2004] Thomas L Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Social spammer detection in microblogging", "author": ["Hu et al.2013] Xia Hu", "Jiliang Tang", "Yanchao Zhang", "Huan Liu"], "venue": "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,", "citeRegEx": "Hu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Devils, angels, and robots: Tempting destructive users in social media", "author": ["Lee et al.2010] Kyumin Lee", "Brian David Eoff", "James Caverlee"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "Seven months with the devils: A long-term study of content polluters on twitter", "author": ["Lee et al.2011] Kyumin Lee", "Brian David Eoff", "James Caverlee"], "venue": "In ICWSM. Citeseer", "citeRegEx": "Lee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Sdhm: A hybrid model for spammer detection in weibo", "author": ["Yu Liu", "Bin Wu", "Bai Wang", "Guanchen Li"], "venue": "In Advances in Social Networks Analysis and Mining (ASONAM),", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Making the most of tweet-inherent features for social spam detection on twitter", "author": ["Wang et al.2015] Bo Wang", "Arkaitz Zubiaga", "Maria Liakata", "Rob Procter"], "venue": "arXiv preprint arXiv:1503.07405", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Don\u2019t follow me: Spam detection in twitter. In Security and Cryptography (SECRYPT)", "author": ["Alex Hai Wang"], "venue": "Proceedings of the 2010 International Conference on,", "citeRegEx": "Wang.,? \\Q2010\\E", "shortCiteRegEx": "Wang.", "year": 2010}, {"title": "Die free or live hard? empirical evaluation and new design for fighting evolving twitter spammers", "author": ["Yang et al.2011] Chao Yang", "Robert Chandler Harkreader", "Guofei Gu"], "venue": "In Recent Advances in Intrusion Detection,", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Analyzing spammers\u2019 social networks for fun and profit: a case study of cyber criminal ecosystem on twitter", "author": ["Yang et al.2012] Chao Yang", "Robert Harkreader", "Jialong Zhang", "Seungwon Shin", "Guofei Gu"], "venue": "In Proceedings of the 21st international conference on", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "(Chu et al., 2010) collect bot accounts and perform analysis on the user behavior and user profile features.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Some researchers focus on the clustering of urls in tweets and network graph of social spammers (Yang et al., 2012; Wang et al., 2015; Wang, 2010; Yang et al., 2011), showing the power of social relationship features.", "startOffset": 96, "endOffset": 165}, {"referenceID": 6, "context": "Some researchers focus on the clustering of urls in tweets and network graph of social spammers (Yang et al., 2012; Wang et al., 2015; Wang, 2010; Yang et al., 2011), showing the power of social relationship features.", "startOffset": 96, "endOffset": 165}, {"referenceID": 7, "context": "Some researchers focus on the clustering of urls in tweets and network graph of social spammers (Yang et al., 2012; Wang et al., 2015; Wang, 2010; Yang et al., 2011), showing the power of social relationship features.", "startOffset": 96, "endOffset": 165}, {"referenceID": 8, "context": "Some researchers focus on the clustering of urls in tweets and network graph of social spammers (Yang et al., 2012; Wang et al., 2015; Wang, 2010; Yang et al., 2011), showing the power of social relationship features.", "startOffset": 96, "endOffset": 165}, {"referenceID": 2, "context": "As for content information modeling, (Hu et al., 2013) apply improved sparse learning methods.", "startOffset": 37, "endOffset": 54}, {"referenceID": 5, "context": "Some researchers (Liu et al., 2014) discuss topic characteristics of spamming posts, indicating that spammers are highly likely to dwell on some certain topics such as promotion.", "startOffset": 17, "endOffset": 35}, {"referenceID": 0, "context": "(Chu et al., 2010) collect bot accounts and perform analysis on the user behavior and user profile features. Lee et al. (2011) use the so-called social honeypot by alluring social spammers\u2019 retweet to build a benchmark dataset, which has been extensively explored in our paper.", "startOffset": 1, "endOffset": 127}, {"referenceID": 3, "context": "Social Honeypot Dataset: Lee et al. (2010) created and deployed 60 seed social accounts on Twitter to attract spammers by reporting back what accounts interact with them.", "startOffset": 25, "endOffset": 43}, {"referenceID": 3, "context": "To compare our extracted features with previously used features for spammer detection, we use three most discriminative feature sets according to Lee et al. (2011)(Table 4).", "startOffset": 146, "endOffset": 164}, {"referenceID": 3, "context": "To compare our extracted features with previously used features for spammer detection, we use three most discriminative feature sets according to Lee et al. (2011)(Table 4). Two classifiers (Adaboost and SVM) are selected to conduct feature performance comparisons. Using Adaboost, our LOSS+GOSS features outperform all other features except for UFN which is 2% higher than ours with regard to precision on the Honeypot dataset. It is caused by the incorrectly classified spammers who are mostly news source after our manual check. They keep posting all kinds of news pieces covering diverse topics, which is similar to the behavior of fake accounts. However, UFN based on friendship networks is more useful for public accounts who possess large number of followers. The best recall value of our LOSS+GOSS features using SVM is up to 6% higher than the results by other feature groups. Regarding F1-score, our features outperform all other features. To further show the advantages of our proposed features, we compare our combined LOSS+GOSS with the combination of all the features from Lee et al. (2011) (UFN+UC+UH).", "startOffset": 146, "endOffset": 1105}], "year": 2017, "abstractText": "Spammer detection on social network is a challenging problem. The rigid anti-spam rules have resulted in emergence of \"smart\" spammers. They resemble legitimate users who are difficult to identify. In this paper, we present a novel spammer classification approach based on Latent Dirichlet Allocation (LDA), a topic model. Our approach extracts both the local and the global information of topic distribution patterns, which capture the essence of spamming. Tested on one benchmark dataset and one self-collected dataset, our proposed method outperforms other stateof-the-art methods in terms of averaged F1score.", "creator": "LaTeX with hyperref package"}}}