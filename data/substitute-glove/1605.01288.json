{"id": "1605.01288", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Fast rates with high probability in exp-concave statistical learning", "abstract": "We the same algorithm they on estimating practical setting with turned bounded exp - concave disappointment in $ f. $ size called whichever excess risk $ O (lugar / ar) $ another peak probability: this stability one made recovery waveform $ \\ delta $ has polylogarithmic taken $ 12 / \\ delta $. The core narrative so to pledges early strength beyond continuing $ O (1 / n) $ slip, without sacrificing although rate, by fostering which Bernstein - type condition which has due already tbarnhart@ajc.com - inscrutability. This Bernstein - type immediately object did the variance present amounting loss random modes are controlled having distinction although could generating causing. Using this formula_5 rule, we continuing show called made remorse bound years way multimedia multi-instrumental on every setting translates to his above approximate accumulated exposure bound more two formula_4 networks - when - certified drive included a online throw-in.", "histories": [["v1", "Wed, 4 May 2016 14:22:59 GMT  (15kb,D)", "https://arxiv.org/abs/1605.01288v1", "13 pages"], ["v2", "Mon, 23 May 2016 09:15:28 GMT  (18kb,D)", "http://arxiv.org/abs/1605.01288v2", "13 pages, wider margins than previous version"], ["v3", "Thu, 18 Aug 2016 13:52:47 GMT  (18kb,D)", "http://arxiv.org/abs/1605.01288v3", "improved log(1/delta)^2 to log(1/delta) in main result"], ["v4", "Fri, 14 Oct 2016 15:24:08 GMT  (23kb,D)", "http://arxiv.org/abs/1605.01288v4", "added results on model selection aggregation (Section 7)"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nishant a mehta"], "accepted": false, "id": "1605.01288"}, "pdf": {"name": "1605.01288.pdf", "metadata": {"source": "CRF", "title": "Fast rates with high probability in exp-concave statistical learning", "authors": ["Nishant A. Mehta"], "emails": ["mehta@cwi.nl"], "sections": [{"heading": "1 Introduction", "text": "In the statistical learning problem, a learning agent observes a samples of n pointsZ1, . . . , Zn drawn i.i.d. from an unknown distribution P over an outcome space Z . The agent then seeks an action f in an action space F that minimizes their expected loss, or risk, EZ\u223cP [`(f, Z)], where ` is a loss function ` : F \u00d7 Z \u2192 R. Several recent works have studied this problem in the situation where the loss is exp-concave and bounded, F and Z are subsets of Rd, and F is convex. Mahdavi et al. (2015) were the first to show that there exists a learner for which, with probability at least 1 \u2212 \u03b4, the excess risk decays at the rate d(log n + log(1/\u03b4))/n. Via new algorithmic stability arguments applied to empirical risk minimization (ERM), Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) discarded the log n factor to obtain a rate of d/n, but their bounds only hold in expectation. All three works highlighted the open problem of obtaining a high probability excess risk bound with the rate d log(1/\u03b4)/n. Whether this is possible is far from a trivial question in light of a result of Audibert (2008): when learning over a finite class with bounded \u03b7-exp-concave losses, the progressive mixture rule (a Ces\u00e0ro mean of pseudo-Bayesian estimators) with learning rate \u03b7 obtains expected excess risk O(1/n) but, for any learning rate, these rules suffer from severe deviations of order \u221a log(1/\u03b4)/n.\nThis work resolves the high probability question: we present a learning algorithm with an excess risk bound (Corollary 1) which has rate d log(1/\u03b4)/n with probability at least 1 \u2212 \u03b4. ERM also obtains O((d log(n) + log(1/\u03b4))/n) excess risk, a fact that apparently was not widely known although it follows from results in the literature. To vanquish the log n factor with the small log(1/\u03b4) price it suffices to run a two-phase ERM method based on a confidence-boosting device. The key to our analysis is connecting exp-concavity to the central condition of Van Erven et al. (2015), which in turn implies a Bernstein condition. We then exploit the variance control of the excess loss random variables afforded by the Bernstein condition to boost the boosting the confidence trick of Schapire (1990).\nIn the next section, we discuss a brief history of the work in this area. In Section 3, we formally define the setting and describe the previous O(d/n) in-expectation bounds. We present the results for standard ERM and our confidence-boosted ERM method in Sections 4 and 5 respectively. Section 6 extends the results of Kakade and Tewari (2009) to exp-concave losses, showing that under a bounded loss assumption a regret bound for any\nar X\niv :1\n60 5.\n01 28\n8v 4\n[ cs\n.L G\n] 1\n4 O\nct 2\nonline exp-concave learner transfers to a high probability excess risk bound via an online-to-batch conversion. This extension comes at no additional technical price: it is a consequence of the variance control implied by exp-concavity, control leveraged by Freedman\u2019s inequality for martingales to obtain a fast rate with high probability. This result continues the line of work of Cesa-Bianchi et al. (2001) and Kakade and Tewari (2009) and accordingly is about the generalization ability of online exp-concave learning algorithms. One powerful consequence of this result is a new guarantee for model selection aggregation: we present a method (Section 7) for the model selection aggregation problem over finite classes with exp-concave losses that obtains a rate ofO((log |F|+log n)/n) with high probability, with no explicit dependence on the Lipschitz continuity of the loss function. All previous bounds of which we are aware have explicit dependence on the Lipschitz continuity of the problem. Moreover, the bound is a quantile-like bound in that it improves with the prior measure on a subclass of nearly optimal hypotheses."}, {"heading": "2 A history of exp-concave learning", "text": "Learning under exp-concave losses with finite classes dates back to the seminal work of Vovk (1990) and the game of prediction with expert advice, with the first explicit treatment for exp-concave losses due to Kivinen and Warmuth (1999). Vovk (1990) showed that if a game is \u03b7-mixable (which is implied by \u03b7-exp-concavity), one can guarantee that the worst-case individual sequence regret against the best of K experts is at most logK \u03b7 . An online-to-batch conversion then implies an in-expectation excess risk bound of the same order in the stochastic i.i.d. setting. Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive\nmixture rule can obtain a high probability excess risk bound of order better than \u221a\nlog(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions y\u0302 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses y\u0302 7\u2192 `(y, y\u0302) satisfying smoothness and strong convexity as a function of predictions y\u0302. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution.\nFor convex classes, such as F \u2282 Rd as we consider here, Hazan et al. (2007) designed the Online Newton Step (ONS) and Exponentially Weighted Online Optimization (EWOO) algorithms. Both have O(d log n) regret over n rounds, which, after online-to-batch conversion yields O(d log(n)/n) excess risk in expectation. Until recently, it was unclear whether one could obtain a similar high probability result; however, Mahdavi et al. (2015) showed that an online-to-batch conversion of ONS enjoys excess risk bounded by O(d log(n)/n) with high probability. While this resolved the statistical complexity of learning up to log n factors, ONS (though efficient) can have a high computational cost of O(d3) even in simple cases like learning over the unit `2 ball, and in general its complexity may be as high as O(d4) per projection step (Hazan et al., 2007; Koren, 2013).\nIf one hopes to eliminate the log n factor, the additional hardness of the online setting makes it unlikely that one can proceed via an online-to-batch conversion approach. Moreover, computational considerations suggest circumventing ONS anyways. In this vein, as we discuss in the next section both Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) recently established in-expectation excess risk bounds for a lightly penalized ERM algorithm and ERM itself respectively, without resorting to an online-to-batch conversion. Notably, both works developed arguments based on algorithmic stability, thereby circumventing the typical reliance on chaining-based arguments to discard log n factors. Table 1 summarizes what is known and our new results."}, {"heading": "3 Rate-optimal in-expectation bounds", "text": "We now describe the setting more formally. In this work F is always assumed to be convex, except in Section 7, which studies the model selection aggregation problem for countable classes. We say a functionA : F \u2192 R\nhas diameter C if supf1,f2\u2208F |A(f1) \u2212 A(f2)| \u2264 C. Assume for each z \u2208 Z that the loss map `(\u00b7, z) : f 7\u2192 `(f, z) is \u03b7-exp-concave, i.e. f 7\u2192 e\u2212\u03b7`(f,z) is concave over F . We further assume, for each outcome z, that the loss `(\u00b7, z) has diameter B. We adopt the notation `f (z) := `(f, z). Given a sample of n points drawn i.i.d. from an unknown distribution P over Z , our objective is to select a hypothesis f \u2208 F that minimizes the excess risk EZ\u223cP [`f (Z)]\u2212 inff\u2208F EZ\u223cP [`f (Z)]. We assume that there exists f\u2217 \u2208 F satisfying E[`f\u2217(Z)] = inff\u2208F EZ\u223cP [`f (Z)]; this assumption also was made by Gonen and Shalev-Shwartz (2016) and Kakade and Tewari (2009).1\nLet AF be an algorithm, defined for a function class F as a mapping AF : \u22c3 n\u22650Zn \u2192 F ; we drop the subscript F when it is clear from the context. Our starting point will be an algorithmA which, when provided with a sample Z of n i.i.d. points, satisfies an expected risk bound of the form\nEZ\u223cPn [ EZ\u223cP [ `A(Z)(Z)\u2212 `f\u2217(Z) ]] \u2264 \u03c8(n). (1)\nKoren and Levy (2015) and Gonen and Shalev-Shwartz (2016) both established in-expectation bounds of the form (1) that obtain a rate of O(d/n) in the case when F \u2282 Rd, each in a slightly different setting. Koren and Levy (2015) assume, for each outcome z \u2208 Z , that the loss `(\u00b7, z) has diameter B and is \u03b2-smooth for some \u03b2 \u2265 1, i.e. for all f, f \u2032 \u2208 F , the gradient is \u03b2-Lipschitz:\n\u2016\u2207f `(f, z)\u2212\u2207f `(f \u2032, z)\u20162 \u2264 \u03b2\u2016f \u2212 f \u2032\u20162.\nThey also use a 1-strongly convex regularizer \u0393: F \u2192 R with diameter R. Under these assumptions, they show that ERM run with the weighted regularizer 1n\u0393 has expected excess risk at most\n\u03c8(n) = 1\nn\n( 24\u03b2d\n\u03b7 + 100Bd+R\n) .\nIt is not known if the smoothness assumption is necessary to eliminate the log n factor. Gonen and Shalev-Shwartz (2016) work in a slightly different setting that captures all known exp-concave losses. They assume that the loss is of the form `f (z) = \u03c6y(\u3008f, x\u3009), for F \u2282 Rd. They further assume, for each z = (x, y), that the mapping y\u0302 7\u2192 \u03c6y(y\u0302) is \u03b1-strongly convex and L-Lipschitz, but they do not assume smoothness. They show that standard, unregularized ERM has expected excess risk at most\n\u03c8(n) = 2L2d\n\u03b1n =\n2d \u03b7n ,\nwhere \u03b7 = \u03b1/L2; the purpose of the rightmost expression is that the loss is \u03b7-exp-concave. Although this bound ostensibly is independent of the loss\u2019s diameter B, the dependence may be masked by \u03b7: for logistic loss, \u03b7 = e\u2212B/4, while squared loss admits the more favorable \u03b7 = 1/(4B)2.\n1This assumption is not explicit from Koren and Levy (2015), but their other assumptions might imply it. Regardless, if their results and those of Gonen and Shalev-Shwartz (2016) hold, our analysis in Section 5 can be adapted to work if the infimal risk is not achieved, i.e. if f\u2217 \u2208 F does not exist."}, {"heading": "4 A high probability bound for ERM", "text": "As a warm-up to proving a high probability O(d/n) excess risk bound, we first show that ERM itself obtains excess risk O(d log(n)/n) with high probability; here and elsewhere, if \u03b4 is omitted the dependence is log(1/\u03b4). That ERM satisfies such a bound was largely implicit in the literature, and so we make this result explicit. The closest such result, Theorem 1 of Mahdavi and Jin (2014), does not apply as it relies on an additional assumption (see their Assumption (I)). Our assumptions subtly differ from elsewhere in this work. We assume that F \u2282 Rd satisfies supf,f \u2032\u2208F \u2016f \u2212 f \u2032\u20162 \u2264 R and that, for each outcome z \u2208 Z , the loss `(\u00b7, z) is L-Lipschitz and |`f (z)\u2212`f\u2217(z)| \u2264 B. The first two assumptions already imply the last forB = LR. All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R).\nThe first, critical observation is that exp-concavity implies good concentration properties of the excess loss random variable. This is easiest to see by way of the \u03b7-central condition, which the excess loss satisfies. This concept, studied by Van Erven et al. (2015) and first introduced by Van Erven et al. (2012) as \u201cstochastic mixability\u201d, is defined as follows.\nDefinition 1 (Central condition) We say that (P, `,F) satisfies the \u03b7-central condition for some \u03b7 > 0 if there exists a comparator f\u2217 \u2208 F such that, for all f \u2208 F ,\nEZ\u223cP [ e\u2212\u03b7(`f (Z)\u2212`f\u2217 (Z)) ] \u2264 1.\nJensen\u2019s inequality implies that if this condition holds, the corresponding f\u2217 must be a risk minimizer. It is known (Van Erven et al., 2015, Section 4.2.2) that in our setting (P, `,F) satisfies the \u03b7-central condition.\nLemma 1 Let F be convex. Take ` to be a loss function ` : F \u00d7 Z \u2192 R, and assume that, for each z \u2208 Z , the map `(\u00b7, z) : f 7\u2192 `(f, z) is \u03b7-exp-concave. Then, for all distributions P over Z , if there exists an f\u2217 \u2208 F that minimizes the risk under P , then (P, `,F) satisfies the \u03b7-central condition.\nWith the central condition in our grip, Theorem 7 of Mehta and Williamson (2014) directly implies an O(d log(n)/n) bound for ERM; however, a far simpler version of that result yields much smaller constants. The proof of the version below, in the appendix for completeness, only makes use of an (\u03b5/L)-net of F in the `2 norm, which induces an \u03b5-net of {`f : f \u2208 F} in the sup norm.\nTheorem 1 Let F \u2282 Rd be a convex set satisfying supf,f \u2032\u2208F \u2016f \u2212 f \u2032\u20162 \u2264 R. Suppose, for all z \u2208 Z , that the loss `(\u00b7, z) is \u03b7-exp-concave and L-Lipschitz. Let supz\u2208Z,f\u2208F |`f (z)\u2212 `f\u2217(z)| \u2264 B. Then if n \u2265 5, with probability at least 1\u2212 \u03b4, ERM learns a hypothesis f\u0302 with excess risk bounded as\nEZ\u223cP [`f\u0302 (Z)\u2212 `f\u2217(Z)] \u2264 1\nn\n( 8 ( B \u2228 1\n\u03b7\n)( d log(16LRn) + log 1\n\u03b4\n) + 1 ) . (2)"}, {"heading": "5 Boosting the confidence for high probability bounds", "text": "The two existing excess risk bounds mentioned in Section 3 decay at the rate 1/n. A na\u00efve application of Markov\u2019s inequality unsatisfyingly yields excess risk bounds of order \u03c8(n)/\u03b4 that hold with probability 1\u2212 \u03b4. In this section, we present and analyze our meta-algorithm, CONFIDENCEBOOST, which boosts these inexpectation bounds to hold with probability at least 1 \u2212 \u03b4 at the price of log(1/\u03b4) factor. This method is essentially the \u201cboosting the confidence\u201d trick of Schapire (1990);2 the novelty lies in a refined analysis that exploits a Bernstein-type condition to improve the rate in the final high probability bound from the typical O(1/ \u221a n) to the desired O(1/n).\nOur analysis of CONFIDENCEBOOST actually applies more generally than the exp-concave learning setting, requiring only that A satisfy an in-expectation bound of the form (1), the loss `(\u00b7, z) have bounded diameter for each z \u2208 Z , and the problem (P, `,F) satisfy a (C, q)-Bernstein condition.\n2See also Chapter 4.2 of Kearns and Vazirani (1994).\nAlgorithm 1: CONFIDENCEBOOST\nInput: Z1, . . . ,ZK iid\u223c PnI , ZII \u223c PnII , learner AF for j = 1\u2192 K do f\u0302j = AF (Zj) return ERMFK (ZII), with FK = {f\u03021, . . . , f\u0302K}\nDefinition 2 (Bernstein condition) We say that (P, `,F) satisfies the (C, q)-Bernstein condition for some C > 0 and q \u2208 (0, 1] if there exists a comparator f\u2217 \u2208 F such that, for all f \u2208 F ,\nEZ\u223cP [ (`f (Z)\u2212 `f\u2217(Z))2 ] \u2264 C EZ\u223cP [`f (Z)\u2212 `f\u2217(Z)]q .\nBefore getting to CONFIDENCEBOOST, we first show that the exp-concave learning setting satisfies the Bernstein condition with the best exponent, q = 1, and so is a special case of the more general setting we analyze. Recall from Lemma 1 that the \u03b7-central condition holds for (P, `,F). The next lemma, which adapts a result of Van Erven et al. (2015), shows that the \u03b7-central condition, together with boundedness of the loss, implies that a Bernstein condition holds.\nLemma 2 (Central to Bernstein) LetX be a random variable taking values in [\u2212B,B]. Assume that E[e\u2212\u03b7X ] \u2264 1. Then E[X2] \u2264 4 (1/\u03b7 +B)E[X].\nBoosting the boosting-the-confidence trick. First, consider running A on a sample Z1 of n i.i.d. points. The excess risk random variable EZ [`A(Z1)(Z)\u2212 `f\u2217(Z)] is nonnegative, and so Markov\u2019s inequality and the expected excess risk being bounded by \u03c8(n) imply that\nPr ( EZ [`A(Z1)(Z)\u2212 `f\u2217(Z)] \u2265 e \u00b7 \u03c8(n) ) \u2264 1 e .\nNow, let Z1, . . . ,ZK be independent samples, each of size n. Running A on each sample yields f\u03021 := A(Z1), . . . , f\u0302K := A(ZK). Applying Markov\u2019s inequality as above, combined with independence, implies that with probability at least 1\u2212 e\u2212K there exists j \u2208 [K] such that EZ\u223cP [ `f\u0302j (Z)\u2212 `f\u2217(Z) ] \u2264 e \u00b7 \u03c8(n). Let us call this good event GOOD. Our quest is now to show that on event GOOD, we can identify any of the hypotheses f\u03021, . . . , f\u0302K approxi-\nmately satisfying EZ\u223cP [ `f\u0302j (Z)\u2212 `f\u2217(Z) ] \u2264 e \u00b7 \u03c8(n), where by \u201capproximately\u201d we mean up to some slack that weakens the order of our resulting excess risk bound by a multiplicative factor of at most K. As we will see, it suffices to run ERM over this finite subclass using a fresh sample. The proposed meta-algorithm is presented in Algorithm 1.\nAnalysis. From here on out, we treat the initial sample of sizeKn as fixed and unhat theK estimators above, referring to them as f1, . . . , fK . Without loss of generality, we further assume that they are sorted in order of increasing risk (breaking ties arbitrarily). Our goal now is to show that running ERM on the finite class FK := {f1, . . . , fK} yields low excess risk with respect to comparator f1. A typical analysis of the boosting the confidence trick would apply Hoeffding\u2019s inequality to select a risk minimizer optimal to resolution 1/ \u221a n, but this is not good enough here. As a further boost to the trick, this time with respect to its resolution, we will establish that a Bernstein condition holds over a particular subclass of FK with high probability, which will in turn imply that ERM obtains O(1/n1/(2\u2212q)) excess risk over FK .\nWe first establish an approximate Bernstein condition for (P, `,FK). Since \u2016`fj \u2212 `f1\u2016L2(P ) \u2264 \u2016`fj \u2212 `f\u2217\u2016L2(P ) + \u2016`f1 \u2212 `f\u2217\u2016L2(P ) for all fj \u2208 FK , from the (C, q)-Bernstein condition,\n\u2016`fj \u2212 `f1\u20162L2(P ) \u2264 C ( E[`fj \u2212 `f\u2217 ]q + E[`f1 \u2212 `f\u2217 ]q + 2 ( E[`fj \u2212 `f\u2217 ] \u00b7 E[`f1 \u2212 `f\u2217 ] )q/2) \u2264 C ( 3E[`fj \u2212 `f\u2217 ]q + E[`f1 \u2212 `f\u2217 ]q\n) = C ( 3 ( E[`fj \u2212 `f1 ] + E[`f1 \u2212 `f\u2217 ] )q + E[`f1 \u2212 `f\u2217 ]q\n) \u2264 C ( 3E[`fj \u2212 `f1 ]q + 4E[`f1 \u2212 `f\u2217 ]q ) ;\nwhere the last step follows because the map x 7\u2192 xq is concave and hence subadditive. We call this bound an approximate Bernstein condition because, on event GOOD, for all fj \u2208 FK :\n\u2016`fj \u2212 `f1\u20162L2(P ) \u2264 C ( 3E[`fj \u2212 `f1 ]q + 4(e \u00b7 \u03c8(n))q ) .\nDefine the class F \u2032K as the set {f1} \u222a { fj \u2208 FK : E[`fj \u2212 `f1 ] \u2265 41/qe \u00b7 \u03c8(n) } . Then with probability Pr(GOOD) \u2265 1\u2212 e\u2212K , the problem (P, `,F \u2032K) satisfies the (4C, q)-Bernstein condition. We now analyze the outcome of running ERM on {f1, . . . , fk} using a fresh sample of n points. The next lemma shows that ERM performs favorably under a Bernstein condition, a well-known result.\nLemma 3 Let G be a finite class of functions {f1, . . . , fK} and assume without loss of generality that f1 is a risk minimizer. Let G\u2032 \u2282 G be a subclass for which, for all f \u2208 G\u2032:\nE[(`f \u2212 `f1)2] \u2264 C E[`f \u2212 `f1 ]q,\nand `f \u2212 `f1 \u2264 B almost surely. Then, with probability at least 1 \u2212 \u03b4, ERM run on G will not select any function f in G\u2032 whose excess risk satisfies\nE[`f \u2212 `f1 ] \u2265\n2 ( C + B 2\u2212q 3 ) log |G \u2032|\u22121 \u03b4\nn 1/(2\u2212q) . Applying Lemma 3 with G = FK and G\u2032 = F \u2032K , with probability at least 1 \u2212 \u03b4 over the fresh sample,\nERM selects a function fj falling in one of two cases:\n\u2022 EZ\u223cP [`fj (Z)\u2212 `f1(Z)] \u2264 41/qe \u00b7 \u03c8(n);\n\u2022 EZ\u223cP [`fj (Z)\u2212 `f1(Z)] \u2264 ( 2 ( C+B 2\u2212q 3 ) log K\u03b4\nn\n)1/(2\u2212q) (using |F \u2032K | \u2212 1 \u2264 K).\nWe now run CONFIDENCEBOOST with K = dlog(2/\u03b4)e on a sample of n points, with nI = n2K and nII = n 2 ; for simplicity, we assume that 2K divides n. Taking the failure probability for the ERM phase to be \u03b4/2, CONFIDENCEBOOST admits the following guarantee.\nTheorem 2 Let (P, `,F) satisfy the (C, q)-Bernstein condition, and assume for all z \u2208 Z that the loss `(\u00b7, z) has diameter B. Impose any necessary assumptions such that algorithm A obtains a bound of the form (1). Then, with probability at least 1 \u2212 \u03b4, CONFIDENCEBOOST run with K = dlog(2/\u03b4)e, nI = n/(2K), and nII = n/2 learns a hypothesis f\u0302 with excess risk EZ\u223cP [`f\u0302 (Z)\u2212 `f\u2217(Z)] at most\ne \u00b7 \u03c8\n( n\n2 \u2308 log 2\u03b4 \u2309)+ max 41/qe \u00b7 \u03c8 ( n 2 log \u2308 2 \u03b4 \u2309) ,(4(C + B2\u2212q3 ) (log 1\u03b4 + logdlog 2\u03b4 e) n )1/(2\u2212q) . (3)\nThe next result for exp-concave learning is immediate.\nCorollary 1 Applying Theorem 2 with AF the algorithm of Koren and Levy (2015) and their assumptions (with \u03b2 \u2265 1), the bound in Theorem 2 specializes to\nO ( log 1\u03b4 n ( d\u03b2 \u03b7 + dB +R )) . (4)\nSimilarly taking AF the algorithm of Gonen and Shalev-Shwartz (2016) and their assumptions yields\nO ( log 1\u03b4 n ( d \u03b7 +B )) . (5)\nRemarks. As we saw from Lemmas 1 and 2, in the exp-concave setting a Bernstein condition holds for the class F . A natural inquiry is if one could use this Bernstein condition to show directly a high probability fast rate of O(d/n) for ERM. Indeed, under strong convexity (which is strictly stronger than exp-concavity), Sridharan et al. (2009) show that a similar bound for ERM is possible; however, they used strong convexity to bound a localized complexity. It is unclear if exp-concavity can be used to bound a localized complexity, and the Bernstein condition alone seems insufficient; such a bound may be possible via ideas from the local norm analysis of Koren and Levy (2015). While we think controlling a localized complexity from exp-concavity is a very interesting and worthwhile direction, we leave this to future work, and for now only conjecture that ERM also enjoys excess risk bounded by O((d+ log(1/\u03b4))/n) with high probability. This conjecture is from analogy to the empirical star algorithm of Audibert (2008), which for convex F reduces to ERM itself; note that the conjectured effect of log(1/\u03b4) is additive rather than multiplicative."}, {"heading": "6 Online-to-batch-conversion", "text": "The present section\u2019s purpose is to show that if one is willing to accept the additional log n factor in a high probability bound, then it is sufficient to use an online-to-batch conversion of an online exp-concave learner whose worst-case cumulative regret (over n rounds) is logarithmic in n. Using such a conversion, it is easy to get an excess risk bound with the additional log n factor that holds in expectation. The key difficulty is making such a bound hold with high probability. This result provides an alternative to the high probabilityO(log n/n) result for ERM in Section 4.\nMahdavi et al. (2015) previously considered an online-to-batch conversion of ONS and established the first explicit high probability O(log n/n) excess risk bound in the exp-concave statistical learning setting. Their analysis is elegant but seems to be intimately coupled to ONS; it consequently is unclear if their analysis can be used to grasp excess risk bounds by online-to-batch conversions of other online exp-concave learners. This leads to our next point and a new path: it is possible to transfer regret bounds to high probability excess risk bounds via online-to-batch conversion for general online exp-concave learners. Our analysis builds strongly on the analysis of Kakade and Tewari (2009) in the strongly convex setting.\nWe first consider a different, related setting: online convex optimization (OCO) under a B-bounded, \u03bdstrongly convex loss that is L-Lipschitz with respect to the action. An OCO game unfolds over n rounds. An adversary first selects a sequence of n convex loss functions c1, . . . , cn. In round t, the online learner plays ft \u2208 F , the environment subsequently reveals cost function ct, and the learner suffers loss ct(ft). Note that the adversary is oblivious, and so the learner does not necessarily need to randomize. Because we are interested in analyzing the statistical learning setting, we constrain the adversary to play a sequence of n points z1, . . . , zn \u2208 Z , inducing cost functions `(\u00b7, z1), . . . , `(\u00b7, zn).\nConsider an online learner that sequentially plays actions f1, . . . , fn \u2208 F in response to z1, . . . , zn, so that ft depends on (z1, . . . , zt\u22121). The (cumulative) regret is defined as\nn\u2211 t=1 `ft(zt)\u2212 inf f\u2208F n\u2211 t=1 `f (zt).\nWhen the losses are bounded, strongly convex, and Lipschitz, Kakade and Tewari (2009) showed that if an online algorithm has regret Rn on an i.i.d. sequence Z1, . . . , Zn \u223c P , online-to-batch conversion by simple averaging of the iterates f\u0304n := 1n \u2211n t=1 ft admits the following guarantee.\nTheorem 3 (Cor. 5, Kakade and Tewari (2009)) For all z \u2208 Z , assume that `(\u00b7, z) is bounded by B, \u03bdstrongly convex, and L-Lipschitz. Then with probability at least 1 \u2212 4 log(n)\u03b4 the action f\u0304n satisfies excess risk bound\nEZ\u223cP [`f\u0304n(Z)\u2212 `f\u2217(Z)] \u2264 Rn n + 4\n\u221a L2 log 1\u03b4\n\u03bd\n\u221a Rn n + max\n{ 16L2\n\u03bd , 6B } log 1\u03b4 n .\nUnder various assumptions, there are OCO algorithms that obtain worst-case regret (under all sequences z1, . . . , zn) Rn = O(log n). For instance, Online Gradient Descent (Hazan et al., 2007) admits the regret boundRn \u2264 G 2\n2\u03bd (1 + log n), where G is an upper bound on the gradient.\nWhat if we relax strong convexity to exp-concavity? As we will see, it is possible to extend the analysis of Kakade and Tewari (2009) to \u03b7-exp-concave losses. Of course, such a regret-to-excess-risk bound conversion is useful only if we have online algorithms and regret bounds to start with. Indeed, at least two such algorithms and bounds exist, due to Hazan et al. (2007):\n\u2022 ONS, with Rn \u2264 5 ( 1 \u03b7 +GD ) d log n, where G is a bound on the gradient and D is a bound on the\ndiameter of the action space.\n\u2022 Exponentially Weighted Online Optimization (EWOO), withRn \u2264 1\u03b7d (1 + log(n+ 1)). The better regret bound comes at the price of not being computationally efficient. EWOO can be run in randomized polynomial time, but the regret bound then holds only in expectation (which is insufficient for an online-to-batch conversion).\nWe now show how to extend the analysis of Kakade and Tewari (2009) to exp-concave losses. While similar results can be obtained from the work of Mahdavi et al. (2015) for the specific case of ONS, our analysis is agnostic of the base algorithm. A particular consequence is that our analysis also applies to EWOO, which, although highly impractical, offers a better regret bound. Moreover, our analysis applies to any future online learning algorithms which may have improved guarantees and computational complexities. The key insight is that exp-concavity implies a variance inequality similar to Lemma 1 of Kakade and Tewari (2009), a pivotal result of that work that unlocks Freedman\u2019s inequality for martingales (Freedman, 1975). Let Zt1 denote the sequence Z1, . . . , Zt.\nLemma 4 (Conditional variance control) Define the Martingale difference sequence\n\u03bet := EZ [ `ft(Z)\u2212 `f\u2217(Z) ] \u2212 ( `ft(Zt)\u2212 `f\u2217(Zt) ) .\nThen Var [ \u03bet | Zt\u221211 ] \u2264 4\n( 1\n\u03b7 +B\n) EZ [ `ft(Z)\u2212 `f\u2217(Z) ] .\nPROOF Observe that Var [ \u03bet | Zt\u221211 ] = Var [ `ft(Zt)\u2212 `f\u2217(Zt) | Zt\u221211 ] . Treating the sequence Zt\u221211 as fixed\nand also treating ft as a fixed parameter f , the above conditional variance equals Var [ `f (Z) \u2212 `f\u2217(Z) ] ,\nwhere the randomness lies entirely in Z \u223c P . Then, Lemma 2 implies that Var [ `f (Z) \u2212 `f\u2217(Z) ] \u2264\n4 (\n1 \u03b7 +B ) E [`f (Z)\u2212 `f\u2217(Z)].\nThe next corollary is from a retrace of the proof of Theorem 2 of Kakade and Tewari (2009).\nCorollary 2 For all z \u2208 Z , let `(\u00b7, z) be bounded by B and \u03b7-exp-concave with respect to the action f \u2208 F . Then with probability at least 1\u2212 \u03b4, for any n \u2265 3, the excess risk of f\u0304n is at most\nRn n + 4\n\u221a( 1\n\u03b7 +B\n) log 4 log n\n\u03b4 \u00b7 \u221a Rn n + 16\n( 1\n\u03b7 +B\n) log 4 logn\u03b4\nn .\nIn particular, an online-to-batch conversion of EWOO yields excess risk of order\nd log n\n\u03b7n +\n\u221a d log n\nn\n(\u221a (log log n)B\n\u03b7 +\n\u221a( 1\n\u03b72 + B \u03b7\n) log 1\n\u03b4\n) +\n(log log n)B +B log 1\u03b4 n .\nBy proceeding similarly one can get a guarantee for ONS, under the additional assumptions thatF has bounded diameter and that, for all z \u2208 Z , the gradient\u2207f `(f, z) has bounded norm.\nObtaining o(logn) excess risk. The worst-case regret bounds in this online setting have a log n factor, but when the environment is stochastic and the distribution satisfies some notion of easiness the actual regret can be o(log n). In such situations the excess risk similarly can be o(log n) because our excess risk bounds depend not on worst-case regret bounds but rather the actual regret. We briefly explore one scenario where\nsuch improvement is possible. Suppose that the loss is also \u03b2-smooth; then, in situations when the cumulative loss of f\u2217 is small, the analysis of Orabona et al. (2012, Theorem 1) for ONS yields a more favorable regret bound: they show a regret bound of order log ( 1 + \u2211n t=1 `f\u2217(Zt) ) . As a simple example, consider the case when the problem is realizable in the sense that `f\u2217(Z) = 0 almost surely. Then the regret bound is constant and the rate with respect to n for the excess risk in Corollary 2 is log lognn ."}, {"heading": "7 Model selection aggregation", "text": "In the model selection aggregation problem for exp-concave losses, we are given a countable class F of functions from an input space X to an output space Y and a loss ` : Y \u00d7Y \u2192 R; for each y \u2208 Y , the mapping y\u0302 7\u2192 `(y, y\u0302) is \u03b7-exp-concave. The loss is a supervised loss, as in supervised classification and regression, unlike the more general loss functions used in the rest of the paper which fit into Vapnik\u2019s general setting of the learning problem (Vapnik, 1995). The random points Z \u223c P now decompose into an input-output pair Z = (X,Y ) \u2208 Z = X \u00d7 Y . We often use the notation `f (Z) := `(Y, f(X)). The goal is the same as in the stochastic exp-concave optimization problem, but nowF fails to be convex (and the exp-concavity assumption slightly differs).\nAfter Audibert (2008) showed that the progressive mixture rule cannot obtain fast rates with high probability, several works developed methods that departed from progressive mixture rules and gravitated instead toward ERM-style rules, starting with the empirical star algorithm of Audibert (2008) and a subsequent method of Lecu\u00e9 and Mendelson (2009) which runs ERM over the convex hull of a data-dependent subclass. Lecu\u00e9 and Rigollet (2014) extended these results to take into account a prior on the class using their Q-aggregation procedure. All the methods require Lipschitz continuity of the loss3 and are for finite classes, although we believe that Q-aggregation combined with a suitable prior extends to countable classes. In this section, we present an algorithm that carefully composes exponential weights-type algorithms and still obtains a fast rate with high probability for the model selection aggregation problem. One incarnation can do so with the fast rate of O(log |F|/n) for finite |F|, by relying on Boosted ERM. Another, \u201cpure\u201d version is based on exponential weights-type procedures alone, can get a rate of O(log |F|/n + log n/n) with no explicit dependence on the Lipschitz continuity of the loss. To our knowledge, this is the first fast rate high probability bound for model selection aggregation that lacks explicit dependence on the Lipschitz constant of the loss. Both results hold more generally, allowing for countable classes, taking into account a prior distribution \u03c0 overF , and providing a quantile-like improvement when there is a low quantile with close to optimal risk.\nSince F is countable and hence not convex, algorithms for stochastic exp-concave optimization do not directly apply. Our approach is to apply stochastic exp-concave optimization to the convex hull of a certain small cardinality and data-dependent subset of F . The first phase of obtaining this subset makes use of the progressive mixture rule. We offer two variants for the second phase: PM-EWOO (Algorithm 2) and PM-CB (Algorithm 3). In the algorithms, Apm and Aew are online-to-batch conversions of the progressive mixture rule and EWOO respectively, Acb is CONFIDENCEBOOST, and Aerm is ERM.\nOur interest in PM-EWOO is two-fold: (i) it is a \u201cpurely\u201d exponential weights type method in that it is based only on the progressive mixture rule and EWOO; (ii) it does not require any Lipschitz assumption on the loss function, unlike all previous work.\nTheorem 4 Let F be a countable and \u03c0 a prior distribution over F . Assume that for each y the loss ` : y\u0302 7\u2192 `(y, y\u0302) is \u03b7-exp-concave. Further assume that supf,f \u2032\u2208F |`(y, f(x)) \u2212 `(y, f \u2032(x))| \u2264 B for all (x, y) in the support of P . Then with probability at least 1\u2212 \u03b4, PM-EWOO run with K = dlog(2/\u03b4)e, nI = n/(2K) and nII = n/2 learns a hypothesis f\u0302 satisfying\nEZ\u223cP [ `f\u0302 (Z)\u2212 `f\u2217(Z) ] \u2264 e \u00b7 BAYESRED\u03b7 ( n\n2dlog 2\u03b4 e , \u03c0\n) + \u03b8EW(\u03b4, n),\n3Audibert (2008) only proved the case of bounded squared loss with a suggestion for how to handle the case of exp-concave losses; because of the techniques used, it is likely that Lipschitz continuity would come into play.\nAlgorithm 2: PM-EWOO\nInput: Z1, . . . ,ZK iid\u223c PnI , ZII \u223c PnII for j = 1\u2192 K do f\u0302j = ApmF (Zj) return AewFK (ZII), with FK = conv({f\u03021, . . . , f\u0302K)}\nAlgorithm 3: PM-CB\nInput: Z1, . . . ,Z2K iid\u223c PnI , ZII \u223c PnII for j = 1\u2192 K do f\u0302j = ApmF (Zj) return Acb(Zk+1, . . . ,Z2k,ZII,AermFk ), with Fk = conv({f\u03021, . . . , f\u0302k)}\nwith \u03b8EW(\u03b4, n) = O  \u221a B ( log 1\u03b4 + \u221a log 1\u03b4 log n )\n\u03b7n + B log logn\u03b4 ) n  . Here, BAYESRED\u03b7 (n, \u03c0) is the \u03b7-generalized Expected Bayesian Redundancy (Takeuchi and Barron, 1998; Gr\u00fcnwald, 2012), defined as\ninf \u03c1\u2208\u2206(F)\n{ EZ [Ef\u223c\u03c1 [`f (Z)]\u2212 `f\u2217(Z)] +\nD(\u03c1 \u2016\u03c0) \u03b7(n+ 1)\n} ,\nfor D(\u00b7 \u2016 \u00b7) the KL-divergence. The bound can be rewritten as a quantile-like bound; for all \u03c1 \u2208 \u2206(F):\nEZ\u223cP [ `f\u0302 (Z)\u2212 Ef\u223c\u03c1 [`f (Z)] ] \u2264 (e\u2212 1)GAP(\u03c1, f\u2217) + 2e \u2308 log 2\u03b4 \u2309 D(\u03c1 \u2016\u03c0)\n\u03b7n + \u03b8EW(\u03b4, n),\nwhere GAP(\u03c1, f\u2217) := EZ [Ef\u223c\u03c1 [`f (Z)]\u2212 `f\u2217(Z)]. This bound enjoys a quantile-like improvement when GAP(\u03c1, f\u2217) is small. For instance, if there is a set F \u2032 of large prior measure which has excess risk close to f\u2217, then Theorem 4 pays log(1/\u03c0(F \u2032)) for the complexity; in contrast, Theorem A of Lecu\u00e9 and Rigollet (2014) pays a higher complexity price of log(1/\u03c0(f\u2217)).\nLastly, we provide a simpler bound by specializing to the case of \u03c1 concentrated entirely on f\u2217. Then\nEZ\u223cP [ `f\u0302 (Z)\u2212 `f\u2217(Z) ] \u2264\n2e \u2308 log 2\u03b4 \u2309 log 1\u03c0(f\u2217)\n\u03b7n + \u03b8EW(\u03b4, n).\nTheorem 4 does not explicitly require Lipschitz continuity of the loss, but the rate is suboptimal due to the extra log n factor. The next result obtains the correct rate by using CONFIDENCEBOOST for the second stage of the procedure.\nTheorem 5 Take the assumptions of Theorem 4, but instead assume that for each y the loss ` : y\u0302 7\u2192 `(y, y\u0302) is \u03b1-strongly convex and L-Lipschitz (so (\u03b1/L2)-exp-concavity holds). Then with probability at least 1 \u2212 \u03b4, PM-CB run with K = dlog(3/\u03b4)e, nI = n/(4K) and nII = n/2 learns a hypothesis f\u0302 satisfying\nEZ\u223cP [ `f\u0302 (Z)\u2212 `f\u2217(Z) ] \u2264 e \u00b7 BAYESRED\u03b7 ( n\n4dlog 3\u03b4 e , \u03c0\n) + \u03b8CB(\u03b4, n),\nwith \u03b8CB(\u03b4, n) = O\n(( log 1\u03b4 )2 \u03b7n + B log 1\u03b4 n ) .\nThe proofs of Theorems 4 and 5 are nearly identical and left to the appendix. We sketch a proof here, as it uses a novel reduction of the second phase to a low-dimensional stochastic exp-concave optimization problem. For simplicity, we restrict to the case of finite F , uniform prior \u03c0, and competing with f\u2217. A na\u00efve\napproach is to run a stochastic exp-concave optimization method on the convex hull of F , but this suffers an excess risk bound scaling as |F| rather than log |F|. We instead start with an initial procedure that drastically reduces the set of candidates to a set of O(log(1/\u03b4). To this end, note that an online-to-batch conversion of the progressive mixture rule run on n samples obtains expected excess risk at most log |F|/(\u03b7(n+ 1)). Hence, K independent runs yield a hypothesis with the same bound inflated by a factor ewith probability at least 1\u2212e\u2212K (we assume that this high probability event holds hereafter). At this point, it seems that we have replaced the original problem with an isomorphic one, as we do not know which j \u2208 [K] yields the desired candidate f\u0302j , and the corresponding subclass is still clearly non-convex. However, by taking the convex hull of this set of K predictors and reparameterizing the problem, we arrive at a stochastic \u03b7-exp-concave optimization problem over the K-dimensional simplex; the best predictor in the convex hull clearly at least as good as the best one in F . Thus, our analyses of EWOO and CONFIDENCEBOOST apply and the results follow."}, {"heading": "8 Discussion and Open Problems", "text": "We presented the first high probability O(d/n) excess risk bound for exp-concave statistical learning. The key to proving this bound was the connection between exp-concavity and the central condition, a connection which suggests that exp-concavity implies a low noise condition. Here, low noise can be interpreted either in terms of the central condition, by the exponential decay of the negative tail of the excess loss random variables, or in terms of the Bernstein condition, by the variance of the excess loss of a hypothesis f being controlled by its excess risk. All our results for stochastic exp-concave optimization were based on this low noise interpretation of exp-concavity. In contrast, The previous in-expectation O(d/n) results of Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) used the geometric/convexity-interpretation of exp-concavity, which we further boosted to high probability results using the low noise interpretation. It would be interesting to get a high probability O(d/n) result that proceeds purely from a low noise interpretation or purely from a geometric/convexity one.\nMany results flowing from algorithmic stability often only yield in-expectation bounds, with high probability bounds stemming either from (i) a posthoc confidence boosting procedure \u2014 typically involving Hoeffding\u2019s inequality, which \u201cslows down\u201d fast rate results; or (ii) quite strong stability notions \u2014 e.g. uniform stability allows one to apply McDiarmid\u2019s inequality to a single run of the algorithm (Bousquet and Elisseeff, 2002). Is it a limitation of algorithmic stability techniques that high probability O(d/n) fast rates seem to be out of reach without a posthoc confidence boosting procedure, or are we simply missing the right perspective? One reason to avoid a confidence boosting procedure is that the resulting bounds suffer from a multiplicative log(1/\u03b4) factor rather than the lighter effect of an additive log(1/\u03b4) factor in bounds like Theorem 1. As we mentioned earlier, we conjecture that the basic ERM method obtains a high probability O(d/n) rate, and a potential path to show this rate would be to control a localized complexity as done by Sridharan et al. (2009) but using a more involved argument based on exp-concavity rather than strong convexity.\nWe also developed high probability quantile-like risk bounds for model selection aggregation, one with an optimal rate and another with a slightly suboptimal rate but no explicit dependence on the Lipschitz continuity of the loss. However, our bound form is not yet a full quantile-type bound; it degrades when the GAP term is large, while the bound of Lecu\u00e9 and Rigollet (2014) does not have this problem. Yet, our bound provides an improvement when there is a neighborhood around f\u2217 with large prior mass, which the bound of Lecu\u00e9 and Rigollet cannot do. It is an open problem to get a bound with the best of both worlds."}, {"heading": "A Proofs for Stochastic Exp-Concave Optimization", "text": "PROOF (OF LEMMA 1) The exp-concavity of f 7\u2192 `(f, z) for each z \u2208 Z implies that, for all z \u2208 Z and all distributions Q over F :\nEf\u223cQ [ e\u2212\u03b7`(f,z) ] \u2264 e\u2212\u03b7`(Ef\u223cQ[f ],z) \u21d0\u21d2 `(Ef\u223cQ[f ], z) \u2264 \u2212 1\n\u03b7 log Ef\u223cQ\n[ e\u2212\u03b7`(f,z) ] .\nIt therefore holds that for all distributions P overZ , for all distributionsQ overF , there exists (from convexity of F) f\u2217 = Ef\u223cQ[f ] \u2208 F satisfying\nEZ\u223cP [`(f \u2217, Z)] \u2264 EZ\u223cP [ \u22121 \u03b7 log Ef\u223cQ [ e\u2212\u03b7`(f,Z) ]] .\nThis condition is equivalent to stochastic mixability as well as the pseudoprobability convexity (PPC) condition, both defined by Van Erven et al. (2015). To be precise, for stochastic mixability, in Definition 4.1 of Van Erven et al. (2015), take theirFd andF both equal to ourF , their P equal to {P}, and \u03c8(f) = f\u2217; then strong stochastic mixability holds. Likewise, for the PPC condition, in Definition 3.2 of Van Erven et al. (2015) take the same settings but instead \u03c6(f) = f\u2217; then the strong PPC condition holds. Now, Theorem 3.10 of Van Erven et al. (2015) states that the PPC condition implies the (strong) central condition.\nPROOF (OF THEOREM 1) First, from Lemma 1, the convexity of F together with \u03b7-exp-concavity implies that (P, `,F) satisfies the \u03b7-central condition.\nThe remainder of the proof is a drastic simplification of the proof of Theorem 7 of Mehta and Williamson (2014). Technically, Theorem 7 of that works applies directly, but one can get substantially smaller constants by avoiding much of the technical machinery needed there to handle VC-type classes (e.g. symmetrization, chaining, Talagrand\u2019s inequality).\nDenote by Lf := `f \u2212 `f\u2217 the excess loss with respect to comparator f\u2217. Our goal is to show that, with high probability, ERM does not select any function f \u2208 F whose excess risk E[Lf ] is larger than an for some constant a. Clearly, with probability 1 ERM will never select any function for which both Lf \u2265 0 almost surely and with some positive probability Lf > 0; we call these functions the empirically inadmissible functions. For any \u03b3n > 0, letF \u03b3n be the subclass formed by starting withF , retaining only functions whose excess risk is at least \u03b3n, and further removing the empirically inadmissible functions.\nOur goal now may be expressed equivalently as showing that, with high probability, ERM does not select any function f \u2208 F \u03b3n where \u03b3n = an and a > 1 is some constant to be determined later. Let F \u03b3n,\u03b5 be an optimal proper (\u03b5/L)-cover for F \u03b3n in the `2 norm. From the Lipschitz property of the loss it follows that this cover induces an \u03b5-cover in sup norm over the loss-composed function class {`f : f \u2208 F \u03b3n}. Observe that an \u03b5-cover of F \u03b3n in the `2 norm has cardinality at most (4R/\u03b5)d (Carl and Stephani, 1990, equation 1.1.10), and the cardinality of an optimal proper \u03b5-cover is at most the cardinality of an optimal (\u03b5/2)-cover. (Vidyasagar, 2002, Lemma 2.1). It hence follows that |F \u03b3n,\u03b5| \u2264 ( 8LR \u03b5 )d .\nLet us consider some fixed f \u2208 F \u03b3n,\u03b5. Since we removed the empirical inadmissible functions, there exists some \u03b7f \u2265 \u03b7 for which E[e\u2212\u03b7fLf ] = 1. Theorem 3 and Lemma 4, both from Mehta and Williamson (2014), imply that\nlog EZ\u223cP [ e\u2212(\u03b7f/2)Lf ] \u2264 \u2212 0.18\u03b7fa\n(B\u03b7f \u2228 1)n .\nApplying Theorem 1 of Mehta and Williamson (2014) with t = a2n and the \u03b7 in that theorem set to \u03b7f/2 yields:\nPr  1 n n\u2211 j=1 Lf (Zj) \u2264 a 2n  \u2264 exp(\u22120.18 \u03b7f B\u03b7f \u2228 1 a+ a\u03b7f 4n ) .\nTaking a union bound over F \u03b3n,\u03b5 and using \u03b7 \u2264 \u03b7f for all f \u2208 F \u03b3n,\u03b5, we have that\nPr \u2203f \u2208 F \u03b3n,\u03b5 : 1n n\u2211 j=1 Lf (Zj) \u2264 a 2n  \u2264 (8LR \u03b5 )d exp ( \u22120.18 \u03b7 B\u03b7 \u2228 1 a+ a\u03b7 4n ) .\nSetting \u03b5 = 12n and taking n \u2265 5, from inversion it follows that with probability at least 1 \u2212 \u03b4, for all f \u2208 F \u03b3n,\u03b5, we have 1n \u2211n j=1 Lf (Zj) \u2264 a 2n , where\na = 8 ( B \u2228 1\n\u03b7\n)( d log(16LRn) + log 1\n\u03b4\n) .\nNow, since supf\u2208F \u03b3n minf\u03b5\u2208F \u03b3n,\u03b5 \u2016`f \u2212 `f\u03b5\u2016\u221e \u2264 1 2n , and increasing a by 1 to guarantee that a > 1, with probability at least 1\u2212 \u03b4, for all f \u2208 F \u03b3n , we have 1n \u2211n j=1 Lf (Zj) > 0.\nPROOF (OF LEMMA 2) The main tool we use is part 2 of Theorem 5.4 of Van Erven et al. (2015). First, as per the proof of Lemma 1, note that the central condition as defined in the present work is equivalent to the strong PPC condition of Van Erven et al. (2015). We actually can improve that result due to our easier setting because we may take their function v to be the constant function identically equal to \u03b7. Consequently, in equation (70) of Van Erven et al. (2015), we may take \u03b5 = 0, improving their constant c2 by a factor of 3; moreover, their result actually holds for the second moment, not just the variance, yielding:\nE[X2] \u2264 2 \u03b7\u03ba(\u22122\u03b7B) E[X], (6)\nwhere \u03ba(x) = e x\u2212x\u22121 x2 .\nWe now study the function\nx 7\u2192 1 \u03ba(\u2212x) = x2 e\u2212x + x\u2212 1 .\nWe claim that for all x \u2265 0:\nx2\ne\u2212x + x\u2212 1 \u2264 2 + x.\nL\u2019H\u00f4pital\u2019s rule implies that the inequality holds for x = 0, and so it remains to consider the case of x > 0. First, observe that the denominator is nonnegative, and so we may rewrite this inequality as\nx2 \u2264 (2 + x)(e\u2212x + x\u2212 1),\nwhich simplifies to\n0 \u2264 2e\u2212x + x+ xe\u2212x \u2212 2 \u21d4 2(1\u2212 e\u2212x) \u2264 x(1 + e\u2212x).\nTherefore, we just need to show that, for all x > 0,\n2 x \u2264 1 + e\n\u2212x 1\u2212 e\u2212x = ex/2 + e\u2212x/2 ex/2 \u2212 e\u2212x/2 = coth(x/2),\nwhich is equivalent to showing that for all x > 0,\ntanh(x) \u2264 x.\nBut this indeed holds, since\ntanh(x) = ex \u2212 e\u2212x\nex + e\u2212x =\n2(x+ x 3 3! + x5 5! + . . .) 2(1 + x 2\n2! + x4 4! + . . .)\n= x \u00b7 1 + x\n2 3! + x4 5! + . . .\n1 + x 2 2! + x4 4! + . . .\n\u2264 x.\nThe desired inequality is now established. Returning to (6), we have\nE[X2] \u2264 2 \u03b7\n(2 + 2\u03b7B)E[X] \u2264 4 ( 1\n\u03b7 +B\n) E[X].\nPROOF (OF LEMMA 3) The following simple version of Bernstein\u2019s inequality will suffice for our analysis. Let X1, . . . , Xn be independent random variables satisfying Xj \u2265 B almost surely. Then\nPr  1 n n\u2211 j=1 Xj \u2212 E[X] \u2265 t  \u2264 exp \u2212 nt2 2 ( E [ 1 n \u2211n j=1X 2 ] + Bt3 )  .\nDenote by Lf := `f \u2212 `f1 the excess loss with respect to comparator f1. Fix some f \u2208 G\u2032 \\ {f1}, take X = \u2212Lf , and set t = E[Lf ], yielding:\nPr  1 n n\u2211 j=1 Lf (Zj) \u2264 0  \u2264 exp(\u2212 nE[Lf ]2 2(E[L2f ] + 1 3B E[Lf ]) )\n\u2264 exp ( \u2212 nE[Lf ] 2\n2(C E[Lf ]q + 13B E[Lf ]) ) = exp ( \u2212 nE[Lf ] 2\u2212q\n2 ( C + 13B E[Lf ]1\u2212q\n))\n\u2264 exp ( \u2212 nE[Lf ] 2\u2212q\n2 ( C + 13B\n2\u2212q )) .\nTherefore, if\nE[Lf ] \u2265\n2 ( C + B 2\u2212q 3 ) log |G \u2032| \u03b4\nn 1/(2\u2212q) , (7) then it holds with probability at least 1 \u2212 \u03b4|G\u2032|\u22121 that 1 n \u2211n j=1 Lf (Zj) > 0. The result follows by taking a union bound over the subclass of G\u2032 \\ {f1} for which (7) holds."}, {"heading": "B Proofs for Model Selection Aggregation (Section 7)", "text": "PROOF (OF THEOREMS 4 AND 5) The starting point is the following bound for the progressive mixture rule when run with prior \u03c0 and parameter \u03b7, due to Audibert (see Theorem 1 of Audibert (2008), but the result was already proved in an earlier technical report version of Audibert (2009) (see Corollary 4.1 and Lemma 3.3 therein). When run on an n-sample, an online-to-batch conversion of the progressive mixture rule yields a hypothesis f\u0302 satisfying\nEZn [ EZ [ `(Y, f\u0302(X)) ]] \u2264 inf \u03c1\u2208\u2206(F) { Ef\u223c\u03c1 EZ [`(Y, f(X))] + D(\u03c1 \u2016\u03c0) \u03b7(n+ 1) } where D(\u03c1 \u2016\u03c0) is the KL-divergence of \u03c1 from \u03c0.4 Note that this bound does not explicitly depend on the boundedness nor the Lipschitz continuity of the loss.\nFix some \u03c1\u2217 that nearly obtains the infimum (or obtains it, if possible). Then\nEZn [ EZ [ `(Y, f\u0302(X)) ]] \u2212 Ef\u223c\u03c1\u2217 EZ [`(Y, f(X))] \u2264\nD(\u03c1\u2217 \u2016\u03c0) \u03b7(n+ 1) .\nWe cannot apply the boosting the confidence trick just yet as the LHS is not a nonnegative random variable; this issue motivates the following rewrite.\nEZn [ EZ [ `(Y, f\u0302(X)) ]] \u2212 EZ [`(Y, f\u2217(X))]\n\u2264 Ef\u223c\u03c1\u2217 EZ [`(Y, f(X))]\u2212 EZ [`(Y, f\u2217(X))]\ufe38 \ufe37\ufe37 \ufe38 GAP(\u03c1\u2217,f\u2217) + D(\u03c1\u2217 \u2016\u03c0) \u03b7(n+ 1) .\nWhen the progressive mixture rule is run on K independent samples, yielding hypotheses f (1), . . . , f (K), then Markov\u2019s inequality implies that with probability at least 1 \u2212 e\u2212K (over the (Kn)-sample) there exists j \u2208 [K] for which\nEZ [ `(Y, f (j)(X)) ] \u2212 EZ [`(Y, f\u2217(X))]\n\u2264 e (\nGAP(\u03c1\u2217, f\u2217) + D(\u03c1\u2217 \u2016\u03c0) \u03b7(n+ 1)\n) ,\nwhich can be re-expressed as EZ [ `(Y, f (j)(X)) ] \u2212 EZ [`(Y, f\u2217(X))]\n\u2264 e \u00b7 GAP(\u03c1\u2217, f\u2217) + e \u00b7D(\u03c1 \u2217 \u2016\u03c0)\n\u03b7(n+ 1)\n= e ( inf\n\u03c1\u2208\u2206(F)\n{ Ef\u223c\u03c1 EZ [`(Y, f(X))] +\nD(\u03c1 \u2016\u03c0) \u03b7(n+ 1)\n} \u2212 EZ [`(Y, f\u2217(X))] ) = e \u00b7 BAYESRED\u03b7 (n, \u03c0) .\nIn the sequel, we assume that this high probability event has occurred. Now, let F\u0303 = conv ( {f (1), . . . , f (K)} ) . Clearly, f (j) \u2208 F\u0303 , and so we also have\ninf f\u2208F\u0303\nEZ [`(Y, f(X))] \u2264 e \u00b7 BAYESRED\u03b7 (n, \u03c0) . (8)\nIt therefore is sufficient to learn over F\u0303 and compete with its risk minimizer. But this is only a Kdimensional problem, and if \u03b4 = e\u2212K , we have K = log 1\u03b4 . To see why the problem is only K-dimensional,\n4We say \u201cof \u03c1 from \u03c0\u201d because the Bregman divergence form of the KL-divergence, which makes clear that the KL-divergence is measure of the curvature of negative Shannon entropy between \u03c1 and \u03c0 when considering a first-order Taylor expansion around \u03c0.\nconsider the transformed problem, where\nx\u0303 =  f (1)(x)\n... f (K)(x)  . The loss can now be reparameterized, from\n` : F\u0303 \u2192 R with ` : f 7\u2192 `(y, f(x))\nto \u02dc\u0300: \u2206K\u22121 \u2192 R with \u02dc\u0300: q 7\u2192 `(y, \u3008q, x\u0303\u3009), where \u2206K\u22121 is the (K \u2212 1)-dimensional simplex { q \u2208 [0, 1]K : \u2211K j=1 qj = 1 } .\n\u2206K\u22121 is clearly convex and the loss is \u03b7-exp-concave with respect to q \u2208 \u2206K\u22121; to see the latter, observe that from the \u03b7-exp-concavity of the loss with respect to y\u0302 = \u3008q, x\u0303\u3009:\nEq\u223cPq\n[ e\u2212\u03b7`(y,\u3008q,x\u0303\u3009) ] \u2264 e\u2212\u03b7`(y,Eq\u223cPq [\u3008q,x\u0303\u3009])\n= e\u2212\u03b7`(y,\u3008Eq\u223cPq [q],x\u0303\u3009).\nLastly, the loss is still bounded by B since F\u0303 consists only of convex aggregates of f\u03021, . . . , f\u0302K , themselves convex aggregates over F (and we assumed boundedness of the loss with respect to the original class).\nWe now can proceed in two ways. The high probability bound for EWOO (the first display after Corollary 2) applies immediately. This bound can be simplified to (taking d = K = dlog(2/\u03b4)e)\nO  \u221a B ( log 1\u03b4 + \u221a log 1\u03b4 log n ) \u03b7n + B ( log log n+ log 1\u03b4 ) n  , which, in light of (8), proves Theorem 4.\nIf we further assume the loss framework of Gonen and Shalev-Shwartz (2016), then \u02dc\u0300still satisfies \u03b1-strong convexity in the sense needed because, conditional on the actual prediction y\u0302, the loss \u02dc\u0300 is the same as loss `. Hence, the bound (5) CONFIDENCEBOOST from Corollary 1 applies (taking d = K = dlog 3\u03b4 e), finishing the proof of Theorem 5."}], "references": [{"title": "Progressive mixture rules are deviation suboptimal", "author": ["Jean-Yves Audibert"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Audibert.,? \\Q2008\\E", "shortCiteRegEx": "Audibert.", "year": 2008}, {"title": "Fast learning rates in statistical inference through aggregation", "author": ["Jean-Yves Audibert"], "venue": "The Annals of Statistics,", "citeRegEx": "Audibert.,? \\Q2009\\E", "shortCiteRegEx": "Audibert.", "year": 2009}, {"title": "Stability and generalization", "author": ["Olivier Bousquet", "Andr\u00e9 Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Elisseeff.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff.", "year": 2002}, {"title": "Entropy, compactness, and the approximation of operators, volume 98", "author": ["Bernd Carl", "Irmtraud Stephani"], "venue": null, "citeRegEx": "Carl and Stephani.,? \\Q1990\\E", "shortCiteRegEx": "Carl and Stephani.", "year": 1990}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicol\u00f2 Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2001}, {"title": "On tail probabilities for martingales", "author": ["David A. Freedman"], "venue": "The Annals of Probability,", "citeRegEx": "Freedman.,? \\Q1975\\E", "shortCiteRegEx": "Freedman.", "year": 1975}, {"title": "Tightening the sample complexity of empirical risk minimization via preconditioned stability", "author": ["Alon Gonen", "Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1601.04011,", "citeRegEx": "Gonen and Shalev.Shwartz.,? \\Q2016\\E", "shortCiteRegEx": "Gonen and Shalev.Shwartz.", "year": 2016}, {"title": "The safe bayesian", "author": ["Peter Gr\u00fcnwald"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Gr\u00fcnwald.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2012}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Learning by mirror averaging", "author": ["Anatoli Juditsky", "Philippe Rigollet", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Juditsky et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Juditsky et al\\.", "year": 2008}, {"title": "On the generalization ability of online strongly convex programming algorithms", "author": ["Sham M. Kakade", "Ambuj Tewari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade and Tewari.,? \\Q2009\\E", "shortCiteRegEx": "Kakade and Tewari.", "year": 2009}, {"title": "An introduction to computational learning theory", "author": ["Michael J. Kearns", "Umesh Vazirani"], "venue": "MIT press,", "citeRegEx": "Kearns and Vazirani.,? \\Q1994\\E", "shortCiteRegEx": "Kearns and Vazirani.", "year": 1994}, {"title": "Averaging expert predictions", "author": ["Jyrki Kivinen", "Manfred K Warmuth"], "venue": "In Computational Learning Theory,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1999\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1999}, {"title": "Open problem: Fast stochastic exp-concave optimization", "author": ["Tomer Koren"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Koren.,? \\Q2013\\E", "shortCiteRegEx": "Koren.", "year": 2013}, {"title": "Fast rates for exp-concave empirical risk minimization", "author": ["Tomer Koren", "Kfir Levy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koren and Levy.,? \\Q2015\\E", "shortCiteRegEx": "Koren and Levy.", "year": 2015}, {"title": "Aggregation via empirical risk minimization", "author": ["Guillaume Lecu\u00e9", "Shahar Mendelson"], "venue": "Probability theory and related fields,", "citeRegEx": "Lecu\u00e9 and Mendelson.,? \\Q2009\\E", "shortCiteRegEx": "Lecu\u00e9 and Mendelson.", "year": 2009}, {"title": "Optimal learning with q-aggregation", "author": ["Guillaume Lecu\u00e9", "Philippe Rigollet"], "venue": "The Annals of Statistics,", "citeRegEx": "Lecu\u00e9 and Rigollet.,? \\Q2014\\E", "shortCiteRegEx": "Lecu\u00e9 and Rigollet.", "year": 2014}, {"title": "Excess risk bounds for exponentially concave losses", "author": ["Mehrdad Mahdavi", "Rong Jin"], "venue": "arXiv preprint arXiv:1401.4566,", "citeRegEx": "Mahdavi and Jin.,? \\Q2014\\E", "shortCiteRegEx": "Mahdavi and Jin.", "year": 2014}, {"title": "Lower and upper bounds on the generalization of stochastic exponentially concave optimization", "author": ["Mehrdad Mahdavi", "Lijun Zhang", "Rong Jin"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "Mahdavi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2015}, {"title": "From stochastic mixability to fast rates", "author": ["Nishant A. Mehta", "Robert C. Williamson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mehta and Williamson.,? \\Q2014\\E", "shortCiteRegEx": "Mehta and Williamson.", "year": 2014}, {"title": "Beyond logarithmic bounds in online learning", "author": ["Francesco Orabona", "Nicolo Cesa-Bianchi", "Claudio Gentile"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Orabona et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2012}, {"title": "The strength of weak learnability", "author": ["Robert E. Schapire"], "venue": "Machine learning,", "citeRegEx": "Schapire.,? \\Q1990\\E", "shortCiteRegEx": "Schapire.", "year": 1990}, {"title": "Fast rates for regularized objectives", "author": ["Karthik Sridharan", "Shai Shalev-Shwartz", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sridharan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2009}, {"title": "Robustly minimax codes for universal data compression\u2019, the 21\u2019st", "author": ["Jun-ichi Takeuchi", "Andrew R Barron"], "venue": "In Proceedings of the Twenty-First Symposium on Information Theory and Its Applications,", "citeRegEx": "Takeuchi and Barron.,? \\Q1998\\E", "shortCiteRegEx": "Takeuchi and Barron.", "year": 1998}, {"title": "Mixability in statistical learning", "author": ["Tim van Erven", "Peter Gr\u00fcnwald", "Mark D. Reid", "Robert C. Williamson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Erven et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2012}, {"title": "Fast rates in statistical and online learning", "author": ["Tim van Erven", "Peter D. Gr\u00fcnwald", "Nishant A. Mehta", "Mark D. Reid", "Robert C. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erven et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2015}, {"title": "The nature of statistical learning theory", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1995\\E", "shortCiteRegEx": "Vapnik.", "year": 1995}, {"title": "Learning and Generalization with Applications to Neural Networks", "author": ["Mathukumalli Vidyasagar"], "venue": null, "citeRegEx": "Vidyasagar.,? \\Q2002\\E", "shortCiteRegEx": "Vidyasagar.", "year": 2002}, {"title": "Aggregating strategies. In Proceedings of the third annual workshop on Computational learning theory, pages 371\u2013383", "author": ["Volodimir G Vovk"], "venue": null, "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "This condition is equivalent to stochastic mixability as well as the pseudoprobability convexity (PPC) condition, both defined by Van Erven et al. (2015). To be precise, for stochastic mixability", "author": ["Van Erven"], "venue": null, "citeRegEx": "Erven,? \\Q2015\\E", "shortCiteRegEx": "Erven", "year": 2015}, {"title": "2015) states that the PPC condition implies the (strong) central condition", "author": ["Erven"], "venue": null, "citeRegEx": "Erven,? \\Q2015\\E", "shortCiteRegEx": "Erven", "year": 2015}, {"title": "\u03b3n,\u03b5. Since we removed the empirical inadmissible functions, there exists some \u03b7f \u2265 \u03b7 for which E[e\u2212\u03b7fLf", "author": [], "venue": "Theorem 3 and Lemma", "citeRegEx": "\u2208,? \\Q2014\\E", "shortCiteRegEx": "\u2208", "year": 2014}, {"title": "First, as per the proof of Lemma 1, note that the central condition as defined in the present work is equivalent to the strong PPC condition", "author": ["Van Erven"], "venue": null, "citeRegEx": "Erven,? \\Q2015\\E", "shortCiteRegEx": "Erven", "year": 2015}, {"title": "2015), we may take \u03b5 = 0, improving their constant c2 by a factor of 3; moreover, their result actually holds for the second moment, not just the variance, yielding", "author": ["Van Erven"], "venue": null, "citeRegEx": "Erven,? \\Q2015\\E", "shortCiteRegEx": "Erven", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Mahdavi et al. (2015) were the first to show that there exists a learner for which, with probability at least 1 \u2212 \u03b4, the excess risk decays at the rate d(log n + log(1/\u03b4))/n.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Via new algorithmic stability arguments applied to empirical risk minimization (ERM), Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) discarded the log n factor to obtain a rate of d/n, but their bounds only hold in expectation.", "startOffset": 86, "endOffset": 108}, {"referenceID": 4, "context": "Via new algorithmic stability arguments applied to empirical risk minimization (ERM), Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) discarded the log n factor to obtain a rate of d/n, but their bounds only hold in expectation.", "startOffset": 112, "endOffset": 144}, {"referenceID": 0, "context": "Whether this is possible is far from a trivial question in light of a result of Audibert (2008): when learning over a finite class with bounded \u03b7-exp-concave losses, the progressive mixture rule (a Ces\u00e0ro mean of pseudo-Bayesian estimators) with learning rate \u03b7 obtains expected excess risk O(1/n) but, for any learning rate, these rules suffer from severe deviations of order \u221a log(1/\u03b4)/n.", "startOffset": 80, "endOffset": 96}, {"referenceID": 0, "context": "Whether this is possible is far from a trivial question in light of a result of Audibert (2008): when learning over a finite class with bounded \u03b7-exp-concave losses, the progressive mixture rule (a Ces\u00e0ro mean of pseudo-Bayesian estimators) with learning rate \u03b7 obtains expected excess risk O(1/n) but, for any learning rate, these rules suffer from severe deviations of order \u221a log(1/\u03b4)/n. This work resolves the high probability question: we present a learning algorithm with an excess risk bound (Corollary 1) which has rate d log(1/\u03b4)/n with probability at least 1 \u2212 \u03b4. ERM also obtains O((d log(n) + log(1/\u03b4))/n) excess risk, a fact that apparently was not widely known although it follows from results in the literature. To vanquish the log n factor with the small log(1/\u03b4) price it suffices to run a two-phase ERM method based on a confidence-boosting device. The key to our analysis is connecting exp-concavity to the central condition of Van Erven et al. (2015), which in turn implies a Bernstein condition.", "startOffset": 80, "endOffset": 971}, {"referenceID": 0, "context": "Whether this is possible is far from a trivial question in light of a result of Audibert (2008): when learning over a finite class with bounded \u03b7-exp-concave losses, the progressive mixture rule (a Ces\u00e0ro mean of pseudo-Bayesian estimators) with learning rate \u03b7 obtains expected excess risk O(1/n) but, for any learning rate, these rules suffer from severe deviations of order \u221a log(1/\u03b4)/n. This work resolves the high probability question: we present a learning algorithm with an excess risk bound (Corollary 1) which has rate d log(1/\u03b4)/n with probability at least 1 \u2212 \u03b4. ERM also obtains O((d log(n) + log(1/\u03b4))/n) excess risk, a fact that apparently was not widely known although it follows from results in the literature. To vanquish the log n factor with the small log(1/\u03b4) price it suffices to run a two-phase ERM method based on a confidence-boosting device. The key to our analysis is connecting exp-concavity to the central condition of Van Erven et al. (2015), which in turn implies a Bernstein condition. We then exploit the variance control of the excess loss random variables afforded by the Bernstein condition to boost the boosting the confidence trick of Schapire (1990). In the next section, we discuss a brief history of the work in this area.", "startOffset": 80, "endOffset": 1188}, {"referenceID": 0, "context": "Whether this is possible is far from a trivial question in light of a result of Audibert (2008): when learning over a finite class with bounded \u03b7-exp-concave losses, the progressive mixture rule (a Ces\u00e0ro mean of pseudo-Bayesian estimators) with learning rate \u03b7 obtains expected excess risk O(1/n) but, for any learning rate, these rules suffer from severe deviations of order \u221a log(1/\u03b4)/n. This work resolves the high probability question: we present a learning algorithm with an excess risk bound (Corollary 1) which has rate d log(1/\u03b4)/n with probability at least 1 \u2212 \u03b4. ERM also obtains O((d log(n) + log(1/\u03b4))/n) excess risk, a fact that apparently was not widely known although it follows from results in the literature. To vanquish the log n factor with the small log(1/\u03b4) price it suffices to run a two-phase ERM method based on a confidence-boosting device. The key to our analysis is connecting exp-concavity to the central condition of Van Erven et al. (2015), which in turn implies a Bernstein condition. We then exploit the variance control of the excess loss random variables afforded by the Bernstein condition to boost the boosting the confidence trick of Schapire (1990). In the next section, we discuss a brief history of the work in this area. In Section 3, we formally define the setting and describe the previous O(d/n) in-expectation bounds. We present the results for standard ERM and our confidence-boosted ERM method in Sections 4 and 5 respectively. Section 6 extends the results of Kakade and Tewari (2009) to exp-concave losses, showing that under a bounded loss assumption a regret bound for any", "startOffset": 80, "endOffset": 1534}, {"referenceID": 4, "context": "This result continues the line of work of Cesa-Bianchi et al. (2001) and Kakade and Tewari (2009) and accordingly is about the generalization ability of online exp-concave learning algorithms.", "startOffset": 42, "endOffset": 69}, {"referenceID": 4, "context": "This result continues the line of work of Cesa-Bianchi et al. (2001) and Kakade and Tewari (2009) and accordingly is about the generalization ability of online exp-concave learning algorithms.", "startOffset": 42, "endOffset": 98}, {"referenceID": 9, "context": "(Juditsky et al., 2008).", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "While this resolved the statistical complexity of learning up to log n factors, ONS (though efficient) can have a high computational cost of O(d) even in simple cases like learning over the unit `2 ball, and in general its complexity may be as high as O(d) per projection step (Hazan et al., 2007; Koren, 2013).", "startOffset": 277, "endOffset": 310}, {"referenceID": 13, "context": "While this resolved the statistical complexity of learning up to log n factors, ONS (though efficient) can have a high computational cost of O(d) even in simple cases like learning over the unit `2 ball, and in general its complexity may be as high as O(d) per projection step (Hazan et al., 2007; Koren, 2013).", "startOffset": 277, "endOffset": 310}, {"referenceID": 18, "context": "Learning under exp-concave losses with finite classes dates back to the seminal work of Vovk (1990) and the game of prediction with expert advice, with the first explicit treatment for exp-concave losses due to Kivinen and Warmuth (1999).", "startOffset": 88, "endOffset": 100}, {"referenceID": 7, "context": "Learning under exp-concave losses with finite classes dates back to the seminal work of Vovk (1990) and the game of prediction with expert advice, with the first explicit treatment for exp-concave losses due to Kivinen and Warmuth (1999). Vovk (1990) showed that if a game is \u03b7-mixable (which is implied by \u03b7-exp-concavity), one can guarantee that the worst-case individual sequence regret against the best of K experts is at most logK \u03b7 .", "startOffset": 211, "endOffset": 238}, {"referenceID": 7, "context": "Learning under exp-concave losses with finite classes dates back to the seminal work of Vovk (1990) and the game of prediction with expert advice, with the first explicit treatment for exp-concave losses due to Kivinen and Warmuth (1999). Vovk (1990) showed that if a game is \u03b7-mixable (which is implied by \u03b7-exp-concavity), one can guarantee that the worst-case individual sequence regret against the best of K experts is at most logK \u03b7 .", "startOffset": 211, "endOffset": 251}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n.", "startOffset": 0, "endOffset": 16}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM.", "startOffset": 0, "endOffset": 311}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution.", "startOffset": 0, "endOffset": 905}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution. For convex classes, such as F \u2282 R as we consider here, Hazan et al. (2007) designed the Online Newton Step (ONS) and Exponentially Weighted Online Optimization (EWOO) algorithms.", "startOffset": 0, "endOffset": 1096}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution. For convex classes, such as F \u2282 R as we consider here, Hazan et al. (2007) designed the Online Newton Step (ONS) and Exponentially Weighted Online Optimization (EWOO) algorithms. Both have O(d log n) regret over n rounds, which, after online-to-batch conversion yields O(d log(n)/n) excess risk in expectation. Until recently, it was unclear whether one could obtain a similar high probability result; however, Mahdavi et al. (2015) showed that an online-to-batch conversion of ONS enjoys excess risk bounded by O(d log(n)/n) with high probability.", "startOffset": 0, "endOffset": 1454}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution. For convex classes, such as F \u2282 R as we consider here, Hazan et al. (2007) designed the Online Newton Step (ONS) and Exponentially Weighted Online Optimization (EWOO) algorithms. Both have O(d log n) regret over n rounds, which, after online-to-batch conversion yields O(d log(n)/n) excess risk in expectation. Until recently, it was unclear whether one could obtain a similar high probability result; however, Mahdavi et al. (2015) showed that an online-to-batch conversion of ONS enjoys excess risk bounded by O(d log(n)/n) with high probability. While this resolved the statistical complexity of learning up to log n factors, ONS (though efficient) can have a high computational cost of O(d) even in simple cases like learning over the unit `2 ball, and in general its complexity may be as high as O(d) per projection step (Hazan et al., 2007; Koren, 2013). If one hopes to eliminate the log n factor, the additional hardness of the online setting makes it unlikely that one can proceed via an online-to-batch conversion approach. Moreover, computational considerations suggest circumventing ONS anyways. In this vein, as we discuss in the next section both Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) recently established in-expectation excess risk bounds for a lightly penalized ERM algorithm and ERM itself respectively, without resorting to an online-to-batch conversion.", "startOffset": 0, "endOffset": 2204}, {"referenceID": 0, "context": "Audibert (2008) showed that when learning over a finite class with exp-concave losses, no progressive mixture rule can obtain a high probability excess risk bound of order better than \u221a log(1/\u03b4)/n. ERM fares even worse, with a lower bound of \u221a log |F|/n in expectation. (Juditsky et al., 2008). Audibert (2008) overcame the deviations shortcoming of progressive mixture rules via his empirical star algorithm, which first runs ERM on F , obtaining f\u0302ERM, and then runs ERM a second time on the star convex hull of F with respect to f\u0302ERM. This algorithm achieves O(log |F|/n) with high probability; the rate was only proved for squared loss with targets Y and predictions \u0177 in [\u22121, 1], but it was claimed that the result can be extended to general, bounded losses \u0177 7\u2192 `(y, \u0177) satisfying smoothness and strong convexity as a function of predictions \u0177. Under similar assumptions, Lecu\u00e9 and Rigollet (2014) proved that a method, Q-aggregation, also obtains this rate but can further take into account a prior distribution. For convex classes, such as F \u2282 R as we consider here, Hazan et al. (2007) designed the Online Newton Step (ONS) and Exponentially Weighted Online Optimization (EWOO) algorithms. Both have O(d log n) regret over n rounds, which, after online-to-batch conversion yields O(d log(n)/n) excess risk in expectation. Until recently, it was unclear whether one could obtain a similar high probability result; however, Mahdavi et al. (2015) showed that an online-to-batch conversion of ONS enjoys excess risk bounded by O(d log(n)/n) with high probability. While this resolved the statistical complexity of learning up to log n factors, ONS (though efficient) can have a high computational cost of O(d) even in simple cases like learning over the unit `2 ball, and in general its complexity may be as high as O(d) per projection step (Hazan et al., 2007; Koren, 2013). If one hopes to eliminate the log n factor, the additional hardness of the online setting makes it unlikely that one can proceed via an online-to-batch conversion approach. Moreover, computational considerations suggest circumventing ONS anyways. In this vein, as we discuss in the next section both Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) recently established in-expectation excess risk bounds for a lightly penalized ERM algorithm and ERM itself respectively, without resorting to an online-to-batch conversion.", "startOffset": 0, "endOffset": 2240}, {"referenceID": 14, "context": "\u201cERM\u201d is either penalized ERM (Koren and Levy, 2015) or ERM (Gonen and Shalev-Shwartz, 2016).", "startOffset": 30, "endOffset": 52}, {"referenceID": 6, "context": "\u201cERM\u201d is either penalized ERM (Koren and Levy, 2015) or ERM (Gonen and Shalev-Shwartz, 2016).", "startOffset": 60, "endOffset": 92}, {"referenceID": 6, "context": "We assume that there exists f\u2217 \u2208 F satisfying E[`f\u2217(Z)] = inff\u2208F EZ\u223cP [`f (Z)]; this assumption also was made by Gonen and Shalev-Shwartz (2016) and Kakade and Tewari (2009).", "startOffset": 113, "endOffset": 145}, {"referenceID": 6, "context": "We assume that there exists f\u2217 \u2208 F satisfying E[`f\u2217(Z)] = inff\u2208F EZ\u223cP [`f (Z)]; this assumption also was made by Gonen and Shalev-Shwartz (2016) and Kakade and Tewari (2009).1 Let AF be an algorithm, defined for a function class F as a mapping AF : \u22c3 n\u22650Z \u2192 F ; we drop the subscript F when it is clear from the context.", "startOffset": 113, "endOffset": 174}, {"referenceID": 6, "context": "Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) both established in-expectation bounds of the form (1) that obtain a rate of O(d/n) in the case when F \u2282 R, each in a slightly different setting.", "startOffset": 26, "endOffset": 58}, {"referenceID": 6, "context": "Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) both established in-expectation bounds of the form (1) that obtain a rate of O(d/n) in the case when F \u2282 R, each in a slightly different setting. Koren and Levy (2015) assume, for each outcome z \u2208 Z , that the loss `(\u00b7, z) has diameter B and is \u03b2-smooth for some \u03b2 \u2265 1, i.", "startOffset": 26, "endOffset": 226}, {"referenceID": 6, "context": "Gonen and Shalev-Shwartz (2016) work in a slightly different setting that captures all known exp-concave losses.", "startOffset": 0, "endOffset": 32}, {"referenceID": 12, "context": "1This assumption is not explicit from Koren and Levy (2015), but their other assumptions might imply it.", "startOffset": 38, "endOffset": 60}, {"referenceID": 6, "context": "Regardless, if their results and those of Gonen and Shalev-Shwartz (2016) hold, our analysis in Section 5 can be adapted to work if the infimal risk is not achieved, i.", "startOffset": 42, "endOffset": 74}, {"referenceID": 14, "context": "The closest such result, Theorem 1 of Mahdavi and Jin (2014), does not apply as it relies on an additional assumption (see their Assumption (I)).", "startOffset": 38, "endOffset": 61}, {"referenceID": 14, "context": "The closest such result, Theorem 1 of Mahdavi and Jin (2014), does not apply as it relies on an additional assumption (see their Assumption (I)). Our assumptions subtly differ from elsewhere in this work. We assume that F \u2282 R satisfies supf,f \u2032\u2208F \u2016f \u2212 f \u20162 \u2264 R and that, for each outcome z \u2208 Z , the loss `(\u00b7, z) is L-Lipschitz and |`f (z)\u2212`f\u2217(z)| \u2264 B. The first two assumptions already imply the last forB = LR. All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R).", "startOffset": 38, "endOffset": 471}, {"referenceID": 12, "context": "All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R).", "startOffset": 62, "endOffset": 84}, {"referenceID": 6, "context": "All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R).", "startOffset": 117, "endOffset": 149}, {"referenceID": 6, "context": "All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R). The first, critical observation is that exp-concavity implies good concentration properties of the excess loss random variable. This is easiest to see by way of the \u03b7-central condition, which the excess loss satisfies. This concept, studied by Van Erven et al. (2015) and first introduced by Van Erven et al.", "startOffset": 117, "endOffset": 559}, {"referenceID": 6, "context": "All these assumptions were made by Mahdavi and Jin (2014) and Koren and Levy (2015), sometimes implicitly, and while Gonen and Shalev-Shwartz (2016) only make the Lipschitz assumption, for all known \u03b7-exp-concave losses the constant \u03b7 depends on B (which itself typically will depend on R). The first, critical observation is that exp-concavity implies good concentration properties of the excess loss random variable. This is easiest to see by way of the \u03b7-central condition, which the excess loss satisfies. This concept, studied by Van Erven et al. (2015) and first introduced by Van Erven et al. (2012) as \u201cstochastic mixability\u201d, is defined as follows.", "startOffset": 117, "endOffset": 607}, {"referenceID": 19, "context": "With the central condition in our grip, Theorem 7 of Mehta and Williamson (2014) directly implies an O(d log(n)/n) bound for ERM; however, a far simpler version of that result yields much smaller constants.", "startOffset": 53, "endOffset": 81}, {"referenceID": 20, "context": "This method is essentially the \u201cboosting the confidence\u201d trick of Schapire (1990);2 the novelty lies in a refined analysis that exploits a Bernstein-type condition to improve the rate in the final high probability bound from the typical O(1/ \u221a n) to the desired O(1/n).", "startOffset": 66, "endOffset": 82}, {"referenceID": 11, "context": "2 of Kearns and Vazirani (1994).", "startOffset": 5, "endOffset": 32}, {"referenceID": 24, "context": "The next lemma, which adapts a result of Van Erven et al. (2015), shows that the \u03b7-central condition, together with boundedness of the loss, implies that a Bernstein condition holds.", "startOffset": 45, "endOffset": 65}, {"referenceID": 13, "context": "Corollary 1 Applying Theorem 2 with AF the algorithm of Koren and Levy (2015) and their assumptions (with \u03b2 \u2265 1), the bound in Theorem 2 specializes to", "startOffset": 56, "endOffset": 78}, {"referenceID": 6, "context": "Similarly taking AF the algorithm of Gonen and Shalev-Shwartz (2016) and their assumptions yields", "startOffset": 37, "endOffset": 69}, {"referenceID": 18, "context": "Indeed, under strong convexity (which is strictly stronger than exp-concavity), Sridharan et al. (2009) show that a similar bound for ERM is possible; however, they used strong convexity to bound a localized complexity.", "startOffset": 80, "endOffset": 104}, {"referenceID": 11, "context": "It is unclear if exp-concavity can be used to bound a localized complexity, and the Bernstein condition alone seems insufficient; such a bound may be possible via ideas from the local norm analysis of Koren and Levy (2015). While we think controlling a localized complexity from exp-concavity is a very interesting and worthwhile direction, we leave this to future work, and for now only conjecture that ERM also enjoys excess risk bounded by O((d+ log(1/\u03b4))/n) with high probability.", "startOffset": 201, "endOffset": 223}, {"referenceID": 0, "context": "This conjecture is from analogy to the empirical star algorithm of Audibert (2008), which for convex F reduces to ERM itself; note that the conjectured effect of log(1/\u03b4) is additive rather than multiplicative.", "startOffset": 67, "endOffset": 83}, {"referenceID": 17, "context": "Mahdavi et al. (2015) previously considered an online-to-batch conversion of ONS and established the first explicit high probability O(log n/n) excess risk bound in the exp-concave statistical learning setting.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Our analysis builds strongly on the analysis of Kakade and Tewari (2009) in the strongly convex setting.", "startOffset": 48, "endOffset": 73}, {"referenceID": 10, "context": "When the losses are bounded, strongly convex, and Lipschitz, Kakade and Tewari (2009) showed that if an online algorithm has regret Rn on an i.", "startOffset": 61, "endOffset": 86}, {"referenceID": 10, "context": "When the losses are bounded, strongly convex, and Lipschitz, Kakade and Tewari (2009) showed that if an online algorithm has regret Rn on an i.i.d. sequence Z1, . . . , Zn \u223c P , online-to-batch conversion by simple averaging of the iterates f\u0304n := 1 n \u2211n t=1 ft admits the following guarantee. Theorem 3 (Cor. 5, Kakade and Tewari (2009)) For all z \u2208 Z , assume that `(\u00b7, z) is bounded by B, \u03bdstrongly convex, and L-Lipschitz.", "startOffset": 61, "endOffset": 338}, {"referenceID": 8, "context": "For instance, Online Gradient Descent (Hazan et al., 2007) admits the regret boundRn \u2264 G 2 2\u03bd (1 + log n), where G is an upper bound on the gradient.", "startOffset": 38, "endOffset": 58}, {"referenceID": 9, "context": "What if we relax strong convexity to exp-concavity? As we will see, it is possible to extend the analysis of Kakade and Tewari (2009) to \u03b7-exp-concave losses.", "startOffset": 109, "endOffset": 134}, {"referenceID": 8, "context": "Indeed, at least two such algorithms and bounds exist, due to Hazan et al. (2007):", "startOffset": 62, "endOffset": 82}, {"referenceID": 5, "context": "The key insight is that exp-concavity implies a variance inequality similar to Lemma 1 of Kakade and Tewari (2009), a pivotal result of that work that unlocks Freedman\u2019s inequality for martingales (Freedman, 1975).", "startOffset": 197, "endOffset": 213}, {"referenceID": 9, "context": "We now show how to extend the analysis of Kakade and Tewari (2009) to exp-concave losses.", "startOffset": 42, "endOffset": 67}, {"referenceID": 9, "context": "We now show how to extend the analysis of Kakade and Tewari (2009) to exp-concave losses. While similar results can be obtained from the work of Mahdavi et al. (2015) for the specific case of ONS, our analysis is agnostic of the base algorithm.", "startOffset": 42, "endOffset": 167}, {"referenceID": 9, "context": "We now show how to extend the analysis of Kakade and Tewari (2009) to exp-concave losses. While similar results can be obtained from the work of Mahdavi et al. (2015) for the specific case of ONS, our analysis is agnostic of the base algorithm. A particular consequence is that our analysis also applies to EWOO, which, although highly impractical, offers a better regret bound. Moreover, our analysis applies to any future online learning algorithms which may have improved guarantees and computational complexities. The key insight is that exp-concavity implies a variance inequality similar to Lemma 1 of Kakade and Tewari (2009), a pivotal result of that work that unlocks Freedman\u2019s inequality for martingales (Freedman, 1975).", "startOffset": 42, "endOffset": 633}, {"referenceID": 10, "context": "The next corollary is from a retrace of the proof of Theorem 2 of Kakade and Tewari (2009). Corollary 2 For all z \u2208 Z , let `(\u00b7, z) be bounded by B and \u03b7-exp-concave with respect to the action f \u2208 F .", "startOffset": 66, "endOffset": 91}, {"referenceID": 26, "context": "The loss is a supervised loss, as in supervised classification and regression, unlike the more general loss functions used in the rest of the paper which fit into Vapnik\u2019s general setting of the learning problem (Vapnik, 1995).", "startOffset": 212, "endOffset": 226}, {"referenceID": 0, "context": "After Audibert (2008) showed that the progressive mixture rule cannot obtain fast rates with high probability, several works developed methods that departed from progressive mixture rules and gravitated instead toward ERM-style rules, starting with the empirical star algorithm of Audibert (2008) and a subsequent method of Lecu\u00e9 and Mendelson (2009) which runs ERM over the convex hull of a data-dependent subclass.", "startOffset": 6, "endOffset": 22}, {"referenceID": 0, "context": "After Audibert (2008) showed that the progressive mixture rule cannot obtain fast rates with high probability, several works developed methods that departed from progressive mixture rules and gravitated instead toward ERM-style rules, starting with the empirical star algorithm of Audibert (2008) and a subsequent method of Lecu\u00e9 and Mendelson (2009) which runs ERM over the convex hull of a data-dependent subclass.", "startOffset": 6, "endOffset": 297}, {"referenceID": 0, "context": "After Audibert (2008) showed that the progressive mixture rule cannot obtain fast rates with high probability, several works developed methods that departed from progressive mixture rules and gravitated instead toward ERM-style rules, starting with the empirical star algorithm of Audibert (2008) and a subsequent method of Lecu\u00e9 and Mendelson (2009) which runs ERM over the convex hull of a data-dependent subclass.", "startOffset": 6, "endOffset": 351}, {"referenceID": 0, "context": "After Audibert (2008) showed that the progressive mixture rule cannot obtain fast rates with high probability, several works developed methods that departed from progressive mixture rules and gravitated instead toward ERM-style rules, starting with the empirical star algorithm of Audibert (2008) and a subsequent method of Lecu\u00e9 and Mendelson (2009) which runs ERM over the convex hull of a data-dependent subclass. Lecu\u00e9 and Rigollet (2014) extended these results to take into account a prior on the class using their Q-aggregation procedure.", "startOffset": 6, "endOffset": 443}, {"referenceID": 0, "context": "3Audibert (2008) only proved the case of bounded squared loss with a suggestion for how to handle the case of exp-concave losses; because of the techniques used, it is likely that Lipschitz continuity would come into play.", "startOffset": 1, "endOffset": 17}, {"referenceID": 23, "context": "Here, BAYESRED\u03b7 (n, \u03c0) is the \u03b7-generalized Expected Bayesian Redundancy (Takeuchi and Barron, 1998; Gr\u00fcnwald, 2012), defined as", "startOffset": 73, "endOffset": 116}, {"referenceID": 7, "context": "Here, BAYESRED\u03b7 (n, \u03c0) is the \u03b7-generalized Expected Bayesian Redundancy (Takeuchi and Barron, 1998; Gr\u00fcnwald, 2012), defined as", "startOffset": 73, "endOffset": 116}, {"referenceID": 16, "context": "For instance, if there is a set F \u2032 of large prior measure which has excess risk close to f\u2217, then Theorem 4 pays log(1/\u03c0(F \u2032)) for the complexity; in contrast, Theorem A of Lecu\u00e9 and Rigollet (2014) pays a higher complexity price of log(1/\u03c0(f\u2217)).", "startOffset": 174, "endOffset": 200}, {"referenceID": 2, "context": "uniform stability allows one to apply McDiarmid\u2019s inequality to a single run of the algorithm (Bousquet and Elisseeff, 2002).", "startOffset": 94, "endOffset": 124}, {"referenceID": 11, "context": "In contrast, The previous in-expectation O(d/n) results of Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) used the geometric/convexity-interpretation of exp-concavity, which we further boosted to high probability results using the low noise interpretation.", "startOffset": 59, "endOffset": 81}, {"referenceID": 5, "context": "In contrast, The previous in-expectation O(d/n) results of Koren and Levy (2015) and Gonen and Shalev-Shwartz (2016) used the geometric/convexity-interpretation of exp-concavity, which we further boosted to high probability results using the low noise interpretation.", "startOffset": 85, "endOffset": 117}, {"referenceID": 2, "context": "uniform stability allows one to apply McDiarmid\u2019s inequality to a single run of the algorithm (Bousquet and Elisseeff, 2002). Is it a limitation of algorithmic stability techniques that high probability O(d/n) fast rates seem to be out of reach without a posthoc confidence boosting procedure, or are we simply missing the right perspective? One reason to avoid a confidence boosting procedure is that the resulting bounds suffer from a multiplicative log(1/\u03b4) factor rather than the lighter effect of an additive log(1/\u03b4) factor in bounds like Theorem 1. As we mentioned earlier, we conjecture that the basic ERM method obtains a high probability O(d/n) rate, and a potential path to show this rate would be to control a localized complexity as done by Sridharan et al. (2009) but using a more involved argument based on exp-concavity rather than strong convexity.", "startOffset": 95, "endOffset": 778}, {"referenceID": 2, "context": "uniform stability allows one to apply McDiarmid\u2019s inequality to a single run of the algorithm (Bousquet and Elisseeff, 2002). Is it a limitation of algorithmic stability techniques that high probability O(d/n) fast rates seem to be out of reach without a posthoc confidence boosting procedure, or are we simply missing the right perspective? One reason to avoid a confidence boosting procedure is that the resulting bounds suffer from a multiplicative log(1/\u03b4) factor rather than the lighter effect of an additive log(1/\u03b4) factor in bounds like Theorem 1. As we mentioned earlier, we conjecture that the basic ERM method obtains a high probability O(d/n) rate, and a potential path to show this rate would be to control a localized complexity as done by Sridharan et al. (2009) but using a more involved argument based on exp-concavity rather than strong convexity. We also developed high probability quantile-like risk bounds for model selection aggregation, one with an optimal rate and another with a slightly suboptimal rate but no explicit dependence on the Lipschitz continuity of the loss. However, our bound form is not yet a full quantile-type bound; it degrades when the GAP term is large, while the bound of Lecu\u00e9 and Rigollet (2014) does not have this problem.", "startOffset": 95, "endOffset": 1245}], "year": 2016, "abstractText": "We present an algorithm for the statistical learning setting with a bounded exp-concave loss in d dimensions that obtains excess riskO(d log(1/\u03b4)/n) with probability at least 1\u2212\u03b4. The core technique is to boost the confidence of recent in-expectation O(d/n) excess risk bounds for empirical risk minimization (ERM), without sacrificing the rate, by leveraging a Bernstein condition which holds due to exp-concavity. We also show that with probability 1\u2212\u03b4 the standard ERM method obtains excess riskO(d(log(n)+ log(1/\u03b4))/n). We further show that a regret bound for any online learner in this setting translates to a high probability excess risk bound for the corresponding online-to-batch conversion of the online learner. Lastly, we present two high probability bounds for the exp-concave model selection aggregation problem that are quantile-adaptive in a certain sense. The first bound is a purely exponential weights type algorithm, obtains a nearly optimal rate, and has no explicit dependence on the Lipschitz continuity of the loss. The second bound requires Lipschitz continuity but obtains the optimal rate.", "creator": "LaTeX with hyperref package"}}}